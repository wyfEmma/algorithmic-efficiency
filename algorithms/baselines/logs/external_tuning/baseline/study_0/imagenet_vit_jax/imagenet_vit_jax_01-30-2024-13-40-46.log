python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=3682051175 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_01-30-2024-13-40-46.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0130 13:41:06.795738 139863983413056 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax.
I0130 13:41:07.782571 139863983413056 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0130 13:41:07.783275 139863983413056 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0130 13:41:07.783401 139863983413056 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0130 13:41:07.784422 139863983413056 submission_runner.py:542] Using RNG seed 3682051175
I0130 13:41:08.927804 139863983413056 submission_runner.py:551] --- Tuning run 1/5 ---
I0130 13:41:08.928011 139863983413056 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_1.
I0130 13:41:08.928375 139863983413056 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_1/hparams.json.
I0130 13:41:09.112618 139863983413056 submission_runner.py:206] Initializing dataset.
I0130 13:41:09.128468 139863983413056 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:41:09.138708 139863983413056 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 13:41:09.525683 139863983413056 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:41:17.773759 139863983413056 submission_runner.py:213] Initializing model.
I0130 13:41:26.706218 139863983413056 submission_runner.py:255] Initializing optimizer.
I0130 13:41:27.699977 139863983413056 submission_runner.py:262] Initializing metrics bundle.
I0130 13:41:27.700164 139863983413056 submission_runner.py:280] Initializing checkpoint and logger.
I0130 13:41:27.701304 139863983413056 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0130 13:41:27.701444 139863983413056 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0130 13:41:28.035493 139863983413056 logger_utils.py:220] Unable to record git information. Continuing without it.
I0130 13:41:28.339334 139863983413056 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_1/flags_0.json.
I0130 13:41:28.349352 139863983413056 submission_runner.py:314] Starting training loop.
I0130 13:42:11.190883 139701931472640 logging_writer.py:48] [0] global_step=0, grad_norm=0.3351106643676758, loss=6.907756805419922
I0130 13:42:11.208426 139863983413056 spec.py:321] Evaluating on the training split.
I0130 13:42:11.216599 139863983413056 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:42:11.225778 139863983413056 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 13:42:11.308946 139863983413056 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:42:28.685971 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 13:42:28.697822 139863983413056 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:42:28.714952 139863983413056 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 13:42:28.794090 139863983413056 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:42:46.155316 139863983413056 spec.py:349] Evaluating on the test split.
I0130 13:42:46.164530 139863983413056 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 13:42:46.170262 139863983413056 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0130 13:42:46.224336 139863983413056 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 13:42:51.512809 139863983413056 submission_runner.py:408] Time since start: 83.16s, 	Step: 1, 	{'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 42.85896849632263, 'total_duration': 83.16339540481567, 'accumulated_submission_time': 42.85896849632263, 'accumulated_eval_time': 40.30432653427124, 'accumulated_logging_time': 0}
I0130 13:42:51.529157 139668737730304 logging_writer.py:48] [1] accumulated_eval_time=40.304327, accumulated_logging_time=0, accumulated_submission_time=42.858968, global_step=1, preemption_count=0, score=42.858968, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=83.163395, train/accuracy=0.000996, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0130 13:43:55.047252 139700054882048 logging_writer.py:48] [100] global_step=100, grad_norm=0.3815669119358063, loss=6.906069755554199
I0130 13:44:37.777904 139700063274752 logging_writer.py:48] [200] global_step=200, grad_norm=0.4406109154224396, loss=6.89323091506958
I0130 13:45:22.815008 139700054882048 logging_writer.py:48] [300] global_step=300, grad_norm=0.6499064564704895, loss=6.855404853820801
I0130 13:46:07.920022 139700063274752 logging_writer.py:48] [400] global_step=400, grad_norm=0.5664462447166443, loss=6.818679332733154
I0130 13:46:52.801100 139700054882048 logging_writer.py:48] [500] global_step=500, grad_norm=0.624799907207489, loss=6.851456642150879
I0130 13:47:37.902850 139700063274752 logging_writer.py:48] [600] global_step=600, grad_norm=0.7370424866676331, loss=6.746707439422607
I0130 13:48:22.844661 139700054882048 logging_writer.py:48] [700] global_step=700, grad_norm=0.9734796285629272, loss=6.681227684020996
I0130 13:49:08.071249 139700063274752 logging_writer.py:48] [800] global_step=800, grad_norm=1.7228559255599976, loss=6.669015884399414
I0130 13:49:51.609338 139863983413056 spec.py:321] Evaluating on the training split.
I0130 13:50:03.505779 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 13:50:11.664055 139863983413056 spec.py:349] Evaluating on the test split.
I0130 13:50:13.344752 139863983413056 submission_runner.py:408] Time since start: 525.00s, 	Step: 898, 	{'train/accuracy': 0.014863280579447746, 'train/loss': 6.403157711029053, 'validation/accuracy': 0.014499999582767487, 'validation/loss': 6.414474010467529, 'validation/num_examples': 50000, 'test/accuracy': 0.010500000789761543, 'test/loss': 6.4579176902771, 'test/num_examples': 10000, 'score': 462.88109707832336, 'total_duration': 524.9953355789185, 'accumulated_submission_time': 462.88109707832336, 'accumulated_eval_time': 62.0397412776947, 'accumulated_logging_time': 0.02691793441772461}
I0130 13:50:13.361827 139668746123008 logging_writer.py:48] [898] accumulated_eval_time=62.039741, accumulated_logging_time=0.026918, accumulated_submission_time=462.881097, global_step=898, preemption_count=0, score=462.881097, test/accuracy=0.010500, test/loss=6.457918, test/num_examples=10000, total_duration=524.995336, train/accuracy=0.014863, train/loss=6.403158, validation/accuracy=0.014500, validation/loss=6.414474, validation/num_examples=50000
I0130 13:50:14.604075 139668754515712 logging_writer.py:48] [900] global_step=900, grad_norm=1.077932596206665, loss=6.566359996795654
I0130 13:50:54.598109 139668746123008 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.130614995956421, loss=6.537985801696777
I0130 13:51:38.910261 139668754515712 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.1413042545318604, loss=6.633737564086914
I0130 13:52:23.962955 139668746123008 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2914577722549438, loss=6.508334636688232
I0130 13:53:08.801095 139668754515712 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.494978904724121, loss=6.481099605560303
I0130 13:53:53.699138 139668746123008 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.862642765045166, loss=6.705835819244385
I0130 13:54:38.600837 139668754515712 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.3868356943130493, loss=6.665359973907471
I0130 13:55:23.441809 139668746123008 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.8369178771972656, loss=6.259581565856934
I0130 13:56:08.110301 139668754515712 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.5072190761566162, loss=6.425423622131348
I0130 13:56:52.949929 139668746123008 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.927318811416626, loss=6.564933776855469
I0130 13:57:13.591163 139863983413056 spec.py:321] Evaluating on the training split.
I0130 13:57:25.335601 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 13:57:33.488377 139863983413056 spec.py:349] Evaluating on the test split.
I0130 13:57:35.134571 139863983413056 submission_runner.py:408] Time since start: 966.79s, 	Step: 1848, 	{'train/accuracy': 0.040546875447034836, 'train/loss': 5.837361812591553, 'validation/accuracy': 0.039159998297691345, 'validation/loss': 5.86493444442749, 'validation/num_examples': 50000, 'test/accuracy': 0.03280000016093254, 'test/loss': 5.980993270874023, 'test/num_examples': 10000, 'score': 883.0495610237122, 'total_duration': 966.7851572036743, 'accumulated_submission_time': 883.0495610237122, 'accumulated_eval_time': 83.5831470489502, 'accumulated_logging_time': 0.05479598045349121}
I0130 13:57:35.151441 139668754515712 logging_writer.py:48] [1848] accumulated_eval_time=83.583147, accumulated_logging_time=0.054796, accumulated_submission_time=883.049561, global_step=1848, preemption_count=0, score=883.049561, test/accuracy=0.032800, test/loss=5.980993, test/num_examples=10000, total_duration=966.785157, train/accuracy=0.040547, train/loss=5.837362, validation/accuracy=0.039160, validation/loss=5.864934, validation/num_examples=50000
I0130 13:57:56.424594 139668746123008 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.7330856323242188, loss=6.208302974700928
I0130 13:58:38.186007 139668754515712 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.1973748207092285, loss=6.168090343475342
I0130 13:59:23.048616 139668746123008 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.623941421508789, loss=6.201029300689697
I0130 14:00:07.918341 139668754515712 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.3561915159225464, loss=6.689372539520264
I0130 14:00:52.476082 139668746123008 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.8753055334091187, loss=6.107013702392578
I0130 14:01:37.353957 139668754515712 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.035247564315796, loss=6.0506510734558105
I0130 14:02:22.361309 139668746123008 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.4523338079452515, loss=6.424205780029297
I0130 14:03:07.407704 139668754515712 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.428865909576416, loss=6.054582118988037
I0130 14:03:51.983349 139668746123008 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.435422658920288, loss=6.537649154663086
I0130 14:04:35.601194 139863983413056 spec.py:321] Evaluating on the training split.
I0130 14:04:47.331514 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 14:04:55.573191 139863983413056 spec.py:349] Evaluating on the test split.
I0130 14:04:57.223140 139863983413056 submission_runner.py:408] Time since start: 1408.87s, 	Step: 2799, 	{'train/accuracy': 0.0707421824336052, 'train/loss': 5.413792610168457, 'validation/accuracy': 0.06505999714136124, 'validation/loss': 5.469770431518555, 'validation/num_examples': 50000, 'test/accuracy': 0.05170000344514847, 'test/loss': 5.6387457847595215, 'test/num_examples': 10000, 'score': 1303.4351394176483, 'total_duration': 1408.873723268509, 'accumulated_submission_time': 1303.4351394176483, 'accumulated_eval_time': 105.20509386062622, 'accumulated_logging_time': 0.08552002906799316}
I0130 14:04:57.241421 139668754515712 logging_writer.py:48] [2799] accumulated_eval_time=105.205094, accumulated_logging_time=0.085520, accumulated_submission_time=1303.435139, global_step=2799, preemption_count=0, score=1303.435139, test/accuracy=0.051700, test/loss=5.638746, test/num_examples=10000, total_duration=1408.873723, train/accuracy=0.070742, train/loss=5.413793, validation/accuracy=0.065060, validation/loss=5.469770, validation/num_examples=50000
I0130 14:04:58.073786 139668746123008 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.4890145063400269, loss=6.347769737243652
I0130 14:05:38.123647 139668754515712 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.06249737739563, loss=5.953514099121094
I0130 14:06:22.423636 139668746123008 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.4170961380004883, loss=5.9672532081604
I0130 14:07:07.345354 139668754515712 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.3996144533157349, loss=5.9511613845825195
I0130 14:07:52.367805 139668746123008 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.4400320053100586, loss=6.441163063049316
I0130 14:08:36.840673 139668754515712 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.584490418434143, loss=5.819016933441162
I0130 14:09:21.660527 139668746123008 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.4682246446609497, loss=5.876157283782959
I0130 14:10:06.689446 139668754515712 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.4416735172271729, loss=6.000226974487305
I0130 14:10:51.686120 139668746123008 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.474402904510498, loss=5.808602333068848
I0130 14:11:36.350595 139668754515712 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.467938780784607, loss=5.783492088317871
I0130 14:11:57.547596 139863983413056 spec.py:321] Evaluating on the training split.
I0130 14:12:09.476696 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 14:12:17.709502 139863983413056 spec.py:349] Evaluating on the test split.
I0130 14:12:19.365677 139863983413056 submission_runner.py:408] Time since start: 1851.02s, 	Step: 3749, 	{'train/accuracy': 0.10007812082767487, 'train/loss': 5.093623638153076, 'validation/accuracy': 0.09064000099897385, 'validation/loss': 5.134443283081055, 'validation/num_examples': 50000, 'test/accuracy': 0.07300000637769699, 'test/loss': 5.3636393547058105, 'test/num_examples': 10000, 'score': 1723.6793518066406, 'total_duration': 1851.0162580013275, 'accumulated_submission_time': 1723.6793518066406, 'accumulated_eval_time': 127.02318668365479, 'accumulated_logging_time': 0.11602115631103516}
I0130 14:12:19.382513 139668746123008 logging_writer.py:48] [3749] accumulated_eval_time=127.023187, accumulated_logging_time=0.116021, accumulated_submission_time=1723.679352, global_step=3749, preemption_count=0, score=1723.679352, test/accuracy=0.073000, test/loss=5.363639, test/num_examples=10000, total_duration=1851.016258, train/accuracy=0.100078, train/loss=5.093624, validation/accuracy=0.090640, validation/loss=5.134443, validation/num_examples=50000
I0130 14:12:40.185101 139668754515712 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.4031733274459839, loss=5.768342971801758
I0130 14:13:22.937625 139668746123008 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.7456810474395752, loss=5.752711296081543
I0130 14:14:07.744933 139668754515712 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.4726896286010742, loss=6.183769226074219
I0130 14:14:52.493320 139668746123008 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.3826707601547241, loss=6.081013202667236
I0130 14:15:37.270320 139668754515712 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.1008957624435425, loss=6.193567276000977
I0130 14:16:21.964743 139668746123008 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.729198694229126, loss=5.942918300628662
I0130 14:17:06.960561 139668754515712 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.399579644203186, loss=6.513689041137695
I0130 14:17:51.999954 139668746123008 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.589739203453064, loss=5.502022743225098
I0130 14:18:36.800492 139668754515712 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.475459098815918, loss=6.409756183624268
I0130 14:19:19.729506 139863983413056 spec.py:321] Evaluating on the training split.
I0130 14:19:31.537201 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 14:19:39.981907 139863983413056 spec.py:349] Evaluating on the test split.
I0130 14:19:41.628331 139863983413056 submission_runner.py:408] Time since start: 2293.28s, 	Step: 4697, 	{'train/accuracy': 0.14359374344348907, 'train/loss': 4.689797401428223, 'validation/accuracy': 0.13179999589920044, 'validation/loss': 4.742933750152588, 'validation/num_examples': 50000, 'test/accuracy': 0.10000000149011612, 'test/loss': 5.027024269104004, 'test/num_examples': 10000, 'score': 2143.966569185257, 'total_duration': 2293.278913974762, 'accumulated_submission_time': 2143.966569185257, 'accumulated_eval_time': 148.92200636863708, 'accumulated_logging_time': 0.1431868076324463}
I0130 14:19:41.646941 139668746123008 logging_writer.py:48] [4697] accumulated_eval_time=148.922006, accumulated_logging_time=0.143187, accumulated_submission_time=2143.966569, global_step=4697, preemption_count=0, score=2143.966569, test/accuracy=0.100000, test/loss=5.027024, test/num_examples=10000, total_duration=2293.278914, train/accuracy=0.143594, train/loss=4.689797, validation/accuracy=0.131800, validation/loss=4.742934, validation/num_examples=50000
I0130 14:19:43.299480 139668754515712 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.1445404291152954, loss=6.280095100402832
I0130 14:20:23.419837 139668746123008 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.5736757516860962, loss=5.627699375152588
I0130 14:21:07.894288 139668754515712 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.82111394405365, loss=5.467463493347168
I0130 14:21:53.030042 139668746123008 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.5287785530090332, loss=5.382552146911621
I0130 14:22:38.496388 139668754515712 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.793911337852478, loss=5.308679580688477
I0130 14:23:23.876955 139668746123008 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.661536455154419, loss=5.249863624572754
I0130 14:24:09.072895 139668754515712 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.5389666557312012, loss=5.516761779785156
I0130 14:24:54.005893 139668746123008 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.235790252685547, loss=5.260894775390625
I0130 14:25:39.002267 139668754515712 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.6084963083267212, loss=5.398706912994385
I0130 14:26:24.228911 139668746123008 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.701813817024231, loss=5.331378936767578
I0130 14:26:41.930673 139863983413056 spec.py:321] Evaluating on the training split.
I0130 14:26:54.190025 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 14:27:02.349813 139863983413056 spec.py:349] Evaluating on the test split.
I0130 14:27:04.008012 139863983413056 submission_runner.py:408] Time since start: 2735.66s, 	Step: 5641, 	{'train/accuracy': 0.18896484375, 'train/loss': 4.296420097351074, 'validation/accuracy': 0.17146000266075134, 'validation/loss': 4.386421203613281, 'validation/num_examples': 50000, 'test/accuracy': 0.12770000100135803, 'test/loss': 4.731691837310791, 'test/num_examples': 10000, 'score': 2564.190915584564, 'total_duration': 2735.6585891246796, 'accumulated_submission_time': 2564.190915584564, 'accumulated_eval_time': 170.99932527542114, 'accumulated_logging_time': 0.17235016822814941}
I0130 14:27:04.025713 139668754515712 logging_writer.py:48] [5641] accumulated_eval_time=170.999325, accumulated_logging_time=0.172350, accumulated_submission_time=2564.190916, global_step=5641, preemption_count=0, score=2564.190916, test/accuracy=0.127700, test/loss=4.731692, test/num_examples=10000, total_duration=2735.658589, train/accuracy=0.188965, train/loss=4.296420, validation/accuracy=0.171460, validation/loss=4.386421, validation/num_examples=50000
I0130 14:27:28.052144 139668746123008 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.3003325462341309, loss=6.1475982666015625
I0130 14:28:10.488238 139668754515712 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.9912211894989014, loss=5.211799144744873
I0130 14:28:55.660023 139668746123008 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.4847474098205566, loss=5.6116204261779785
I0130 14:29:40.503022 139668754515712 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.600532054901123, loss=5.229280948638916
I0130 14:30:25.666738 139668746123008 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.7639650106430054, loss=5.1074323654174805
I0130 14:31:10.655900 139668754515712 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.4868872165679932, loss=6.296895980834961
I0130 14:31:56.288444 139668746123008 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.7170597314834595, loss=5.089657783508301
I0130 14:32:41.798857 139668754515712 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.8728169202804565, loss=4.987910747528076
I0130 14:33:27.188999 139668746123008 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.4662706851959229, loss=5.25875997543335
I0130 14:34:04.232735 139863983413056 spec.py:321] Evaluating on the training split.
I0130 14:34:16.606539 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 14:34:27.302927 139863983413056 spec.py:349] Evaluating on the test split.
I0130 14:34:28.953736 139863983413056 submission_runner.py:408] Time since start: 3180.60s, 	Step: 6584, 	{'train/accuracy': 0.23142577707767487, 'train/loss': 3.941406726837158, 'validation/accuracy': 0.2192399948835373, 'validation/loss': 4.0269927978515625, 'validation/num_examples': 50000, 'test/accuracy': 0.1648000031709671, 'test/loss': 4.412674903869629, 'test/num_examples': 10000, 'score': 2984.3381164073944, 'total_duration': 3180.604308128357, 'accumulated_submission_time': 2984.3381164073944, 'accumulated_eval_time': 195.720308303833, 'accumulated_logging_time': 0.20057272911071777}
I0130 14:34:28.975324 139668754515712 logging_writer.py:48] [6584] accumulated_eval_time=195.720308, accumulated_logging_time=0.200573, accumulated_submission_time=2984.338116, global_step=6584, preemption_count=0, score=2984.338116, test/accuracy=0.164800, test/loss=4.412675, test/num_examples=10000, total_duration=3180.604308, train/accuracy=0.231426, train/loss=3.941407, validation/accuracy=0.219240, validation/loss=4.026993, validation/num_examples=50000
I0130 14:34:35.781818 139668746123008 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.613820195198059, loss=5.367376327514648
I0130 14:35:16.253344 139668754515712 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.2130203247070312, loss=6.226517677307129
I0130 14:36:01.246083 139668746123008 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.4563177824020386, loss=4.903162002563477
I0130 14:36:46.522295 139668754515712 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.4329655170440674, loss=5.1028642654418945
I0130 14:37:31.581424 139668746123008 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.8351854085922241, loss=4.870098114013672
I0130 14:38:16.732817 139668754515712 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.5069767236709595, loss=6.259661674499512
I0130 14:39:01.543223 139668746123008 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.3332598209381104, loss=5.658273220062256
I0130 14:39:46.621174 139668754515712 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9862061142921448, loss=6.315052032470703
I0130 14:40:32.004968 139668746123008 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.233178734779358, loss=6.121926784515381
I0130 14:41:18.084133 139668754515712 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.3900235891342163, loss=6.04894495010376
I0130 14:41:29.175331 139863983413056 spec.py:321] Evaluating on the training split.
I0130 14:41:41.316192 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 14:41:52.275499 139863983413056 spec.py:349] Evaluating on the test split.
I0130 14:41:53.971636 139863983413056 submission_runner.py:408] Time since start: 3625.62s, 	Step: 7526, 	{'train/accuracy': 0.279296875, 'train/loss': 3.6524882316589355, 'validation/accuracy': 0.2576200067996979, 'validation/loss': 3.760484218597412, 'validation/num_examples': 50000, 'test/accuracy': 0.19610001146793365, 'test/loss': 4.192534446716309, 'test/num_examples': 10000, 'score': 3404.479038000107, 'total_duration': 3625.622143268585, 'accumulated_submission_time': 3404.479038000107, 'accumulated_eval_time': 220.51652264595032, 'accumulated_logging_time': 0.23270916938781738}
I0130 14:41:53.997921 139668746123008 logging_writer.py:48] [7526] accumulated_eval_time=220.516523, accumulated_logging_time=0.232709, accumulated_submission_time=3404.479038, global_step=7526, preemption_count=0, score=3404.479038, test/accuracy=0.196100, test/loss=4.192534, test/num_examples=10000, total_duration=3625.622143, train/accuracy=0.279297, train/loss=3.652488, validation/accuracy=0.257620, validation/loss=3.760484, validation/num_examples=50000
I0130 14:42:24.010137 139668754515712 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.6720548868179321, loss=5.027371406555176
I0130 14:43:06.847188 139668746123008 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.6852829456329346, loss=5.031923770904541
I0130 14:43:51.833780 139668754515712 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.2917786836624146, loss=6.287484169006348
I0130 14:44:36.808811 139668746123008 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.5278329849243164, loss=5.087230682373047
I0130 14:45:21.648034 139668754515712 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.656812310218811, loss=4.773712635040283
I0130 14:46:06.614102 139668746123008 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.6311893463134766, loss=4.778606414794922
I0130 14:46:51.997150 139668754515712 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.664861798286438, loss=4.819816589355469
I0130 14:47:36.715378 139668746123008 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.3989843130111694, loss=5.693272590637207
I0130 14:48:21.957446 139668754515712 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.6590628623962402, loss=4.731575012207031
I0130 14:48:54.077790 139863983413056 spec.py:321] Evaluating on the training split.
I0130 14:49:06.614760 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 14:49:24.278133 139863983413056 spec.py:349] Evaluating on the test split.
I0130 14:49:25.932400 139863983413056 submission_runner.py:408] Time since start: 4077.58s, 	Step: 8473, 	{'train/accuracy': 0.3104882836341858, 'train/loss': 3.4264824390411377, 'validation/accuracy': 0.28867998719215393, 'validation/loss': 3.565650224685669, 'validation/num_examples': 50000, 'test/accuracy': 0.2160000056028366, 'test/loss': 4.029370307922363, 'test/num_examples': 10000, 'score': 3824.4939839839935, 'total_duration': 4077.5829815864563, 'accumulated_submission_time': 3824.4939839839935, 'accumulated_eval_time': 252.37112522125244, 'accumulated_logging_time': 0.2743103504180908}
I0130 14:49:25.956184 139668746123008 logging_writer.py:48] [8473] accumulated_eval_time=252.371125, accumulated_logging_time=0.274310, accumulated_submission_time=3824.493984, global_step=8473, preemption_count=0, score=3824.493984, test/accuracy=0.216000, test/loss=4.029370, test/num_examples=10000, total_duration=4077.582982, train/accuracy=0.310488, train/loss=3.426482, validation/accuracy=0.288680, validation/loss=3.565650, validation/num_examples=50000
I0130 14:49:37.133042 139668754515712 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.7302619218826294, loss=4.978761672973633
I0130 14:50:18.289672 139668746123008 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.7230578660964966, loss=4.587224960327148
I0130 14:51:03.185993 139668754515712 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.7223849296569824, loss=4.652998924255371
I0130 14:51:48.349421 139668746123008 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.4480825662612915, loss=4.627885818481445
I0130 14:52:33.566496 139668754515712 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.8442845344543457, loss=4.467264175415039
I0130 14:53:18.857763 139668746123008 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.5580941438674927, loss=4.570261001586914
I0130 14:54:03.785701 139668754515712 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.629792332649231, loss=4.527457237243652
I0130 14:54:48.985736 139668746123008 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.4918044805526733, loss=4.539021015167236
I0130 14:55:33.892792 139668754515712 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.000466823577881, loss=4.478071212768555
I0130 14:56:18.822002 139668746123008 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.5490764379501343, loss=4.528555870056152
I0130 14:56:26.258883 139863983413056 spec.py:321] Evaluating on the training split.
I0130 14:56:39.182097 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 14:56:50.299746 139863983413056 spec.py:349] Evaluating on the test split.
I0130 14:56:51.967495 139863983413056 submission_runner.py:408] Time since start: 4523.62s, 	Step: 9418, 	{'train/accuracy': 0.3550195097923279, 'train/loss': 3.1795032024383545, 'validation/accuracy': 0.3194199800491333, 'validation/loss': 3.3503851890563965, 'validation/num_examples': 50000, 'test/accuracy': 0.24330000579357147, 'test/loss': 3.854766607284546, 'test/num_examples': 10000, 'score': 4244.734249830246, 'total_duration': 4523.618048667908, 'accumulated_submission_time': 4244.734249830246, 'accumulated_eval_time': 278.07969093322754, 'accumulated_logging_time': 0.3103628158569336}
I0130 14:56:52.001073 139668754515712 logging_writer.py:48] [9418] accumulated_eval_time=278.079691, accumulated_logging_time=0.310363, accumulated_submission_time=4244.734250, global_step=9418, preemption_count=0, score=4244.734250, test/accuracy=0.243300, test/loss=3.854767, test/num_examples=10000, total_duration=4523.618049, train/accuracy=0.355020, train/loss=3.179503, validation/accuracy=0.319420, validation/loss=3.350385, validation/num_examples=50000
I0130 14:57:25.193616 139668746123008 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.6082050800323486, loss=5.103769779205322
I0130 14:58:09.596810 139668754515712 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.5588587522506714, loss=5.339176177978516
I0130 14:58:54.672869 139668746123008 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.8700348138809204, loss=4.592453956604004
I0130 14:59:40.454224 139668754515712 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.133383870124817, loss=5.640322685241699
I0130 15:00:25.718872 139668746123008 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.6216319799423218, loss=4.43828010559082
I0130 15:01:10.895189 139668754515712 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.0281853675842285, loss=6.148358345031738
I0130 15:01:55.883746 139668746123008 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.1675002574920654, loss=5.678889274597168
I0130 15:02:40.927897 139668754515712 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.5963670015335083, loss=4.673770427703857
I0130 15:03:26.413999 139668746123008 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.0141855478286743, loss=5.588418483734131
I0130 15:03:52.220289 139863983413056 spec.py:321] Evaluating on the training split.
I0130 15:04:04.828422 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 15:04:19.331947 139863983413056 spec.py:349] Evaluating on the test split.
I0130 15:04:20.985878 139863983413056 submission_runner.py:408] Time since start: 4972.64s, 	Step: 10359, 	{'train/accuracy': 0.3695117235183716, 'train/loss': 3.0522427558898926, 'validation/accuracy': 0.3451800048351288, 'validation/loss': 3.1714067459106445, 'validation/num_examples': 50000, 'test/accuracy': 0.267300009727478, 'test/loss': 3.692898988723755, 'test/num_examples': 10000, 'score': 4664.890008926392, 'total_duration': 4972.636468410492, 'accumulated_submission_time': 4664.890008926392, 'accumulated_eval_time': 306.845290184021, 'accumulated_logging_time': 0.3585376739501953}
I0130 15:04:21.004809 139668754515712 logging_writer.py:48] [10359] accumulated_eval_time=306.845290, accumulated_logging_time=0.358538, accumulated_submission_time=4664.890009, global_step=10359, preemption_count=0, score=4664.890009, test/accuracy=0.267300, test/loss=3.692899, test/num_examples=10000, total_duration=4972.636468, train/accuracy=0.369512, train/loss=3.052243, validation/accuracy=0.345180, validation/loss=3.171407, validation/num_examples=50000
I0130 15:04:37.779274 139668746123008 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.2333952188491821, loss=5.082204818725586
I0130 15:05:19.711805 139668754515712 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.6849738359451294, loss=4.560508728027344
I0130 15:06:04.858100 139668746123008 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.441372275352478, loss=4.3570380210876465
I0130 15:06:49.925517 139668754515712 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.4421433210372925, loss=4.281802654266357
I0130 15:07:35.043254 139668746123008 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.1796523332595825, loss=6.009422779083252
I0130 15:08:20.060290 139668754515712 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.6436564922332764, loss=4.322518825531006
I0130 15:09:05.133959 139668746123008 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.6271377801895142, loss=4.374972343444824
I0130 15:09:50.152979 139668754515712 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.2611500024795532, loss=5.367645740509033
I0130 15:10:35.316702 139668746123008 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.7842518091201782, loss=4.268561840057373
I0130 15:11:20.715882 139668754515712 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.0653958320617676, loss=5.687185764312744
I0130 15:11:21.230232 139863983413056 spec.py:321] Evaluating on the training split.
I0130 15:11:34.553595 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 15:11:45.892141 139863983413056 spec.py:349] Evaluating on the test split.
I0130 15:11:47.535228 139863983413056 submission_runner.py:408] Time since start: 5419.19s, 	Step: 11303, 	{'train/accuracy': 0.3982031047344208, 'train/loss': 2.9216232299804688, 'validation/accuracy': 0.3621799945831299, 'validation/loss': 3.0729660987854004, 'validation/num_examples': 50000, 'test/accuracy': 0.2752000093460083, 'test/loss': 3.60302472114563, 'test/num_examples': 10000, 'score': 5085.055969715118, 'total_duration': 5419.18580698967, 'accumulated_submission_time': 5085.055969715118, 'accumulated_eval_time': 333.15025997161865, 'accumulated_logging_time': 0.38761043548583984}
I0130 15:11:47.554399 139668746123008 logging_writer.py:48] [11303] accumulated_eval_time=333.150260, accumulated_logging_time=0.387610, accumulated_submission_time=5085.055970, global_step=11303, preemption_count=0, score=5085.055970, test/accuracy=0.275200, test/loss=3.603025, test/num_examples=10000, total_duration=5419.185807, train/accuracy=0.398203, train/loss=2.921623, validation/accuracy=0.362180, validation/loss=3.072966, validation/num_examples=50000
I0130 15:12:27.095979 139668754515712 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.598211407661438, loss=4.394469261169434
I0130 15:13:12.376730 139668746123008 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.098219633102417, loss=5.727847576141357
I0130 15:13:57.653146 139668754515712 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.4085454940795898, loss=4.95017147064209
I0130 15:14:42.937158 139668746123008 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.3931772708892822, loss=4.66832160949707
I0130 15:15:28.285578 139668754515712 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.5678654909133911, loss=4.20898962020874
I0130 15:16:13.472307 139668746123008 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.4130958318710327, loss=4.114032745361328
I0130 15:16:58.432096 139668754515712 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.3415955305099487, loss=4.712893486022949
I0130 15:17:43.239012 139668746123008 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.6770312786102295, loss=4.106456756591797
I0130 15:18:28.419828 139668754515712 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.3016984462738037, loss=4.358662128448486
I0130 15:18:47.914260 139863983413056 spec.py:321] Evaluating on the training split.
I0130 15:19:00.673433 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 15:19:14.949900 139863983413056 spec.py:349] Evaluating on the test split.
I0130 15:19:16.604776 139863983413056 submission_runner.py:408] Time since start: 5868.26s, 	Step: 12245, 	{'train/accuracy': 0.4265234172344208, 'train/loss': 2.743405342102051, 'validation/accuracy': 0.38815999031066895, 'validation/loss': 2.927395820617676, 'validation/num_examples': 50000, 'test/accuracy': 0.3027999997138977, 'test/loss': 3.4700565338134766, 'test/num_examples': 10000, 'score': 5505.355977058411, 'total_duration': 5868.255347728729, 'accumulated_submission_time': 5505.355977058411, 'accumulated_eval_time': 361.84075355529785, 'accumulated_logging_time': 0.4170222282409668}
I0130 15:19:16.624163 139668746123008 logging_writer.py:48] [12245] accumulated_eval_time=361.840754, accumulated_logging_time=0.417022, accumulated_submission_time=5505.355977, global_step=12245, preemption_count=0, score=5505.355977, test/accuracy=0.302800, test/loss=3.470057, test/num_examples=10000, total_duration=5868.255348, train/accuracy=0.426523, train/loss=2.743405, validation/accuracy=0.388160, validation/loss=2.927396, validation/num_examples=50000
I0130 15:19:38.995011 139668754515712 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.0987493991851807, loss=4.22232723236084
I0130 15:20:21.967347 139668746123008 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.389256238937378, loss=4.282748699188232
I0130 15:21:07.931788 139668754515712 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.2281731367111206, loss=4.534379005432129
I0130 15:21:53.141999 139668746123008 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.648882508277893, loss=4.15001106262207
I0130 15:22:38.289529 139668754515712 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.5129715204238892, loss=4.1676483154296875
I0130 15:23:23.581163 139668746123008 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.9234936237335205, loss=4.127056121826172
I0130 15:24:08.554080 139668754515712 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.4822131395339966, loss=4.171364784240723
I0130 15:24:53.680163 139668746123008 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.5467075109481812, loss=4.514885425567627
I0130 15:25:38.846327 139668754515712 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.6182010173797607, loss=4.47763729095459
I0130 15:26:16.654818 139863983413056 spec.py:321] Evaluating on the training split.
I0130 15:26:30.371042 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 15:26:46.228667 139863983413056 spec.py:349] Evaluating on the test split.
I0130 15:26:47.886768 139863983413056 submission_runner.py:408] Time since start: 6319.54s, 	Step: 13186, 	{'train/accuracy': 0.43205076456069946, 'train/loss': 2.7276670932769775, 'validation/accuracy': 0.4016000032424927, 'validation/loss': 2.8690223693847656, 'validation/num_examples': 50000, 'test/accuracy': 0.30970001220703125, 'test/loss': 3.434152841567993, 'test/num_examples': 10000, 'score': 5925.322516679764, 'total_duration': 6319.537341594696, 'accumulated_submission_time': 5925.322516679764, 'accumulated_eval_time': 393.07268619537354, 'accumulated_logging_time': 0.4512176513671875}
I0130 15:26:47.908427 139668746123008 logging_writer.py:48] [13186] accumulated_eval_time=393.072686, accumulated_logging_time=0.451218, accumulated_submission_time=5925.322517, global_step=13186, preemption_count=0, score=5925.322517, test/accuracy=0.309700, test/loss=3.434153, test/num_examples=10000, total_duration=6319.537342, train/accuracy=0.432051, train/loss=2.727667, validation/accuracy=0.401600, validation/loss=2.869022, validation/num_examples=50000
I0130 15:26:53.976849 139668754515712 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4562463760375977, loss=4.083658695220947
I0130 15:27:34.647253 139668746123008 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.7578392028808594, loss=4.239596366882324
I0130 15:28:19.484079 139668754515712 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.1516366004943848, loss=5.4740800857543945
I0130 15:29:05.198959 139668746123008 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3521547317504883, loss=5.318351745605469
I0130 15:29:51.090342 139668754515712 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.8971141576766968, loss=4.233157157897949
I0130 15:30:36.491696 139668746123008 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.6443092823028564, loss=4.137263774871826
I0130 15:31:21.849528 139668754515712 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.8931219577789307, loss=5.916962146759033
I0130 15:32:06.909682 139668746123008 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.3414959907531738, loss=4.844330787658691
I0130 15:32:52.353440 139668754515712 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.4921114444732666, loss=4.192223072052002
I0130 15:33:37.508747 139668746123008 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.5991299152374268, loss=4.026893615722656
I0130 15:33:47.968527 139863983413056 spec.py:321] Evaluating on the training split.
I0130 15:34:01.508474 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 15:34:17.194579 139863983413056 spec.py:349] Evaluating on the test split.
I0130 15:34:18.841747 139863983413056 submission_runner.py:408] Time since start: 6770.49s, 	Step: 14124, 	{'train/accuracy': 0.44664061069488525, 'train/loss': 2.5795199871063232, 'validation/accuracy': 0.4178600013256073, 'validation/loss': 2.742652177810669, 'validation/num_examples': 50000, 'test/accuracy': 0.3222000300884247, 'test/loss': 3.3154313564300537, 'test/num_examples': 10000, 'score': 6345.321208238602, 'total_duration': 6770.492334604263, 'accumulated_submission_time': 6345.321208238602, 'accumulated_eval_time': 423.94589376449585, 'accumulated_logging_time': 0.48592185974121094}
I0130 15:34:18.861190 139668754515712 logging_writer.py:48] [14124] accumulated_eval_time=423.945894, accumulated_logging_time=0.485922, accumulated_submission_time=6345.321208, global_step=14124, preemption_count=0, score=6345.321208, test/accuracy=0.322200, test/loss=3.315431, test/num_examples=10000, total_duration=6770.492335, train/accuracy=0.446641, train/loss=2.579520, validation/accuracy=0.417860, validation/loss=2.742652, validation/num_examples=50000
I0130 15:34:49.599817 139668746123008 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.2678800821304321, loss=4.403782844543457
I0130 15:35:34.308473 139668754515712 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.4231181144714355, loss=4.058321952819824
I0130 15:36:19.779277 139668746123008 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0956873893737793, loss=4.8315229415893555
I0130 15:37:05.224164 139668754515712 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.2234246730804443, loss=4.117580890655518
I0130 15:37:50.131347 139668746123008 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.3573707342147827, loss=4.860769271850586
I0130 15:38:35.320244 139668754515712 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.8697908520698547, loss=5.993257522583008
I0130 15:39:20.687767 139668746123008 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.4520902633666992, loss=4.01857852935791
I0130 15:40:06.573716 139668754515712 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.4007620811462402, loss=4.323616981506348
I0130 15:40:52.171939 139668746123008 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8848775625228882, loss=5.946151256561279
I0130 15:41:19.119626 139863983413056 spec.py:321] Evaluating on the training split.
I0130 15:41:33.266874 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 15:41:48.123639 139863983413056 spec.py:349] Evaluating on the test split.
I0130 15:41:49.779612 139863983413056 submission_runner.py:408] Time since start: 7221.43s, 	Step: 15061, 	{'train/accuracy': 0.46728515625, 'train/loss': 2.460857391357422, 'validation/accuracy': 0.432559996843338, 'validation/loss': 2.633084297180176, 'validation/num_examples': 50000, 'test/accuracy': 0.3351000249385834, 'test/loss': 3.2048323154449463, 'test/num_examples': 10000, 'score': 6765.518880844116, 'total_duration': 7221.430178642273, 'accumulated_submission_time': 6765.518880844116, 'accumulated_eval_time': 454.60587215423584, 'accumulated_logging_time': 0.5157270431518555}
I0130 15:41:49.799496 139668754515712 logging_writer.py:48] [15061] accumulated_eval_time=454.605872, accumulated_logging_time=0.515727, accumulated_submission_time=6765.518881, global_step=15061, preemption_count=0, score=6765.518881, test/accuracy=0.335100, test/loss=3.204832, test/num_examples=10000, total_duration=7221.430179, train/accuracy=0.467285, train/loss=2.460857, validation/accuracy=0.432560, validation/loss=2.633084, validation/num_examples=50000
I0130 15:42:05.811962 139668746123008 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.5005656480789185, loss=4.154141426086426
I0130 15:42:49.022694 139668754515712 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.4336050748825073, loss=4.003810405731201
I0130 15:43:34.558812 139668746123008 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.4660447835922241, loss=4.003971576690674
I0130 15:44:19.884423 139668754515712 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.7844678163528442, loss=4.116735458374023
I0130 15:45:05.472078 139668746123008 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9940482974052429, loss=5.793132781982422
I0130 15:45:51.007213 139668754515712 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.3018769025802612, loss=4.369114875793457
I0130 15:46:36.416779 139668746123008 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.3683828115463257, loss=4.05095911026001
I0130 15:47:21.694242 139668754515712 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.461154818534851, loss=4.026494026184082
I0130 15:48:06.974074 139668746123008 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.1221874952316284, loss=4.470696926116943
I0130 15:48:50.119396 139863983413056 spec.py:321] Evaluating on the training split.
I0130 15:49:04.101906 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 15:49:18.892513 139863983413056 spec.py:349] Evaluating on the test split.
I0130 15:49:20.552059 139863983413056 submission_runner.py:408] Time since start: 7672.20s, 	Step: 15996, 	{'train/accuracy': 0.4797070324420929, 'train/loss': 2.4331743717193604, 'validation/accuracy': 0.428739994764328, 'validation/loss': 2.6840686798095703, 'validation/num_examples': 50000, 'test/accuracy': 0.3368000090122223, 'test/loss': 3.2335305213928223, 'test/num_examples': 10000, 'score': 7185.779655456543, 'total_duration': 7672.20264005661, 'accumulated_submission_time': 7185.779655456543, 'accumulated_eval_time': 485.0385265350342, 'accumulated_logging_time': 0.5460660457611084}
I0130 15:49:20.572590 139668754515712 logging_writer.py:48] [15996] accumulated_eval_time=485.038527, accumulated_logging_time=0.546066, accumulated_submission_time=7185.779655, global_step=15996, preemption_count=0, score=7185.779655, test/accuracy=0.336800, test/loss=3.233531, test/num_examples=10000, total_duration=7672.202640, train/accuracy=0.479707, train/loss=2.433174, validation/accuracy=0.428740, validation/loss=2.684069, validation/num_examples=50000
I0130 15:49:22.593903 139668746123008 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.3964449167251587, loss=4.3423566818237305
I0130 15:50:03.707419 139668754515712 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.1742140054702759, loss=4.479966163635254
I0130 15:50:49.170327 139668746123008 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.307004451751709, loss=5.103544235229492
I0130 15:51:34.763184 139668754515712 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.3784714937210083, loss=3.941539764404297
I0130 15:52:20.099050 139668746123008 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.119537353515625, loss=5.925734996795654
I0130 15:53:05.394759 139668754515712 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.3133400678634644, loss=3.9883952140808105
I0130 15:53:50.880800 139668746123008 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.1036018133163452, loss=5.774896144866943
I0130 15:54:36.305346 139668754515712 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9973496794700623, loss=5.6191864013671875
I0130 15:55:21.844637 139668746123008 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.3751978874206543, loss=4.240373134613037
I0130 15:56:07.255861 139668754515712 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.9301552176475525, loss=5.778552532196045
I0130 15:56:21.018518 139863983413056 spec.py:321] Evaluating on the training split.
I0130 15:56:35.300423 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 15:56:50.088498 139863983413056 spec.py:349] Evaluating on the test split.
I0130 15:56:51.752305 139863983413056 submission_runner.py:408] Time since start: 8123.40s, 	Step: 16932, 	{'train/accuracy': 0.47164061665534973, 'train/loss': 2.5049636363983154, 'validation/accuracy': 0.4402399957180023, 'validation/loss': 2.659813642501831, 'validation/num_examples': 50000, 'test/accuracy': 0.3441000282764435, 'test/loss': 3.238809108734131, 'test/num_examples': 10000, 'score': 7606.167441606522, 'total_duration': 8123.402855873108, 'accumulated_submission_time': 7606.167441606522, 'accumulated_eval_time': 515.7722768783569, 'accumulated_logging_time': 0.5766818523406982}
I0130 15:56:51.783625 139668746123008 logging_writer.py:48] [16932] accumulated_eval_time=515.772277, accumulated_logging_time=0.576682, accumulated_submission_time=7606.167442, global_step=16932, preemption_count=0, score=7606.167442, test/accuracy=0.344100, test/loss=3.238809, test/num_examples=10000, total_duration=8123.402856, train/accuracy=0.471641, train/loss=2.504964, validation/accuracy=0.440240, validation/loss=2.659814, validation/num_examples=50000
I0130 15:57:19.419763 139668754515712 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.3743172883987427, loss=3.996805429458618
I0130 15:58:04.199715 139668746123008 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.2474896907806396, loss=3.8962862491607666
I0130 15:58:48.986531 139668754515712 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.311988115310669, loss=4.013824939727783
I0130 15:59:33.855494 139668746123008 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.351430892944336, loss=3.966507911682129
I0130 16:00:19.492787 139668754515712 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.4741132259368896, loss=3.8010191917419434
I0130 16:01:05.068314 139668746123008 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.386133074760437, loss=3.9607717990875244
I0130 16:01:50.313574 139668754515712 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.4394032955169678, loss=4.372599124908447
I0130 16:02:35.589220 139668746123008 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.2768012285232544, loss=4.152191162109375
I0130 16:03:21.163702 139668754515712 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.4756603240966797, loss=3.8624703884124756
I0130 16:03:52.090256 139863983413056 spec.py:321] Evaluating on the training split.
I0130 16:04:06.122303 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 16:04:23.632232 139863983413056 spec.py:349] Evaluating on the test split.
I0130 16:04:25.266855 139863983413056 submission_runner.py:408] Time since start: 8576.92s, 	Step: 17870, 	{'train/accuracy': 0.49119138717651367, 'train/loss': 2.32535719871521, 'validation/accuracy': 0.4554999768733978, 'validation/loss': 2.496633529663086, 'validation/num_examples': 50000, 'test/accuracy': 0.3556000292301178, 'test/loss': 3.085073232650757, 'test/num_examples': 10000, 'score': 8026.408129453659, 'total_duration': 8576.917443037033, 'accumulated_submission_time': 8026.408129453659, 'accumulated_eval_time': 548.9488704204559, 'accumulated_logging_time': 0.6240944862365723}
I0130 16:04:25.287035 139668746123008 logging_writer.py:48] [17870] accumulated_eval_time=548.948870, accumulated_logging_time=0.624094, accumulated_submission_time=8026.408129, global_step=17870, preemption_count=0, score=8026.408129, test/accuracy=0.355600, test/loss=3.085073, test/num_examples=10000, total_duration=8576.917443, train/accuracy=0.491191, train/loss=2.325357, validation/accuracy=0.455500, validation/loss=2.496634, validation/num_examples=50000
I0130 16:04:37.691635 139668754515712 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.2331799268722534, loss=3.947634696960449
I0130 16:05:19.035818 139668746123008 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1773759126663208, loss=4.066659450531006
I0130 16:06:04.427531 139668754515712 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.4821350574493408, loss=3.7925567626953125
I0130 16:06:49.867048 139668746123008 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.0356405973434448, loss=4.865079402923584
I0130 16:07:35.323448 139668754515712 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.4827929735183716, loss=3.915168285369873
I0130 16:08:20.315072 139668746123008 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.2501955032348633, loss=4.120569705963135
I0130 16:09:05.278497 139668754515712 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.3039731979370117, loss=3.907647132873535
I0130 16:09:50.849293 139668746123008 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.2750295400619507, loss=3.935873031616211
I0130 16:10:36.149635 139668754515712 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.392059087753296, loss=3.9350123405456543
I0130 16:11:21.931766 139668746123008 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.1691133975982666, loss=4.3952107429504395
I0130 16:11:25.301105 139863983413056 spec.py:321] Evaluating on the training split.
I0130 16:11:39.810234 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 16:11:57.529613 139863983413056 spec.py:349] Evaluating on the test split.
I0130 16:11:59.166031 139863983413056 submission_runner.py:408] Time since start: 9030.82s, 	Step: 18809, 	{'train/accuracy': 0.5016992092132568, 'train/loss': 2.3369905948638916, 'validation/accuracy': 0.4569000005722046, 'validation/loss': 2.542475938796997, 'validation/num_examples': 50000, 'test/accuracy': 0.3546000123023987, 'test/loss': 3.1322598457336426, 'test/num_examples': 10000, 'score': 8446.362316608429, 'total_duration': 9030.816624879837, 'accumulated_submission_time': 8446.362316608429, 'accumulated_eval_time': 582.8137938976288, 'accumulated_logging_time': 0.6556878089904785}
I0130 16:11:59.186816 139668754515712 logging_writer.py:48] [18809] accumulated_eval_time=582.813794, accumulated_logging_time=0.655688, accumulated_submission_time=8446.362317, global_step=18809, preemption_count=0, score=8446.362317, test/accuracy=0.354600, test/loss=3.132260, test/num_examples=10000, total_duration=9030.816625, train/accuracy=0.501699, train/loss=2.336991, validation/accuracy=0.456900, validation/loss=2.542476, validation/num_examples=50000
I0130 16:12:35.965003 139668746123008 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.283523440361023, loss=3.7938685417175293
I0130 16:13:20.062880 139668754515712 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9400838613510132, loss=5.787055969238281
I0130 16:14:05.696636 139668746123008 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.338880181312561, loss=3.8271396160125732
I0130 16:14:51.140161 139668754515712 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.4248796701431274, loss=3.8943138122558594
I0130 16:15:36.708680 139668746123008 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0835857391357422, loss=4.430327415466309
I0130 16:16:22.124678 139668754515712 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.3362152576446533, loss=3.872911214828491
I0130 16:17:07.529815 139668746123008 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.1267589330673218, loss=5.152501106262207
I0130 16:17:52.927861 139668754515712 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.9924960732460022, loss=5.214231491088867
I0130 16:18:38.221993 139668746123008 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.4800142049789429, loss=3.8554961681365967
I0130 16:18:59.493035 139863983413056 spec.py:321] Evaluating on the training split.
I0130 16:19:13.861317 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 16:19:35.024691 139863983413056 spec.py:349] Evaluating on the test split.
I0130 16:19:36.654779 139863983413056 submission_runner.py:408] Time since start: 9488.31s, 	Step: 19749, 	{'train/accuracy': 0.4941796660423279, 'train/loss': 2.3316903114318848, 'validation/accuracy': 0.4655799865722656, 'validation/loss': 2.4985291957855225, 'validation/num_examples': 50000, 'test/accuracy': 0.3619000315666199, 'test/loss': 3.080425500869751, 'test/num_examples': 10000, 'score': 8866.605522155762, 'total_duration': 9488.30536866188, 'accumulated_submission_time': 8866.605522155762, 'accumulated_eval_time': 619.9755432605743, 'accumulated_logging_time': 0.6908133029937744}
I0130 16:19:36.674110 139668754515712 logging_writer.py:48] [19749] accumulated_eval_time=619.975543, accumulated_logging_time=0.690813, accumulated_submission_time=8866.605522, global_step=19749, preemption_count=0, score=8866.605522, test/accuracy=0.361900, test/loss=3.080426, test/num_examples=10000, total_duration=9488.305369, train/accuracy=0.494180, train/loss=2.331690, validation/accuracy=0.465580, validation/loss=2.498529, validation/num_examples=50000
I0130 16:19:57.442294 139668746123008 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.4247411489486694, loss=3.820827007293701
I0130 16:20:39.668944 139668754515712 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.395672082901001, loss=3.897474527359009
I0130 16:21:24.900458 139668746123008 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.5582090616226196, loss=3.8065426349639893
I0130 16:22:10.564538 139668754515712 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.256727933883667, loss=3.8107097148895264
I0130 16:22:55.759323 139668746123008 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.0545884370803833, loss=4.896703243255615
I0130 16:23:41.147792 139668754515712 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.0082862377166748, loss=4.7957763671875
I0130 16:24:26.368000 139668746123008 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.2911224365234375, loss=3.7263576984405518
I0130 16:25:11.942886 139668754515712 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.325903058052063, loss=4.240124702453613
I0130 16:25:57.084764 139668746123008 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.2292219400405884, loss=3.9620330333709717
I0130 16:26:36.833885 139863983413056 spec.py:321] Evaluating on the training split.
I0130 16:26:51.075720 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 16:27:13.911053 139863983413056 spec.py:349] Evaluating on the test split.
I0130 16:27:15.559836 139863983413056 submission_runner.py:408] Time since start: 9947.21s, 	Step: 20689, 	{'train/accuracy': 0.5057812333106995, 'train/loss': 2.29396653175354, 'validation/accuracy': 0.4740999937057495, 'validation/loss': 2.452284574508667, 'validation/num_examples': 50000, 'test/accuracy': 0.366100013256073, 'test/loss': 3.067256450653076, 'test/num_examples': 10000, 'score': 9286.701465845108, 'total_duration': 9947.210430145264, 'accumulated_submission_time': 9286.701465845108, 'accumulated_eval_time': 658.7014982700348, 'accumulated_logging_time': 0.7256288528442383}
I0130 16:27:15.581220 139668754515712 logging_writer.py:48] [20689] accumulated_eval_time=658.701498, accumulated_logging_time=0.725629, accumulated_submission_time=9286.701466, global_step=20689, preemption_count=0, score=9286.701466, test/accuracy=0.366100, test/loss=3.067256, test/num_examples=10000, total_duration=9947.210430, train/accuracy=0.505781, train/loss=2.293967, validation/accuracy=0.474100, validation/loss=2.452285, validation/num_examples=50000
I0130 16:27:20.377696 139668746123008 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.3314855098724365, loss=3.7416553497314453
I0130 16:28:01.704421 139668754515712 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.1923307180404663, loss=4.389762878417969
I0130 16:28:46.909713 139668746123008 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.4499872922897339, loss=3.7645652294158936
I0130 16:29:32.004421 139668754515712 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9389362335205078, loss=5.333073616027832
I0130 16:30:17.902042 139668746123008 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.3476651906967163, loss=3.8559930324554443
I0130 16:31:04.053706 139668754515712 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.9709378480911255, loss=5.626082420349121
I0130 16:31:50.154905 139668746123008 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.2932108640670776, loss=3.8009700775146484
I0130 16:32:36.143718 139668754515712 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.3970603942871094, loss=3.8947482109069824
I0130 16:33:22.167418 139668746123008 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.2724465131759644, loss=4.142595291137695
I0130 16:34:07.947740 139668754515712 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.3971260786056519, loss=4.350520133972168
I0130 16:34:15.566097 139863983413056 spec.py:321] Evaluating on the training split.
I0130 16:34:30.113759 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 16:34:52.232534 139863983413056 spec.py:349] Evaluating on the test split.
I0130 16:34:53.864416 139863983413056 submission_runner.py:408] Time since start: 10405.52s, 	Step: 21618, 	{'train/accuracy': 0.5250585675239563, 'train/loss': 2.16995906829834, 'validation/accuracy': 0.48357999324798584, 'validation/loss': 2.3651888370513916, 'validation/num_examples': 50000, 'test/accuracy': 0.3717000186443329, 'test/loss': 2.9770636558532715, 'test/num_examples': 10000, 'score': 9706.624783277512, 'total_duration': 10405.515013217926, 'accumulated_submission_time': 9706.624783277512, 'accumulated_eval_time': 696.9998137950897, 'accumulated_logging_time': 0.7613976001739502}
I0130 16:34:53.882236 139668746123008 logging_writer.py:48] [21618] accumulated_eval_time=696.999814, accumulated_logging_time=0.761398, accumulated_submission_time=9706.624783, global_step=21618, preemption_count=0, score=9706.624783, test/accuracy=0.371700, test/loss=2.977064, test/num_examples=10000, total_duration=10405.515013, train/accuracy=0.525059, train/loss=2.169959, validation/accuracy=0.483580, validation/loss=2.365189, validation/num_examples=50000
I0130 16:35:27.046046 139668754515712 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.898891270160675, loss=5.763083457946777
I0130 16:36:12.209747 139668746123008 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8876218795776367, loss=5.843637943267822
I0130 16:36:57.988916 139668754515712 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.5349743366241455, loss=3.8235719203948975
I0130 16:37:44.162705 139668746123008 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.3235926628112793, loss=3.83538818359375
I0130 16:38:29.352540 139668754515712 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.3384405374526978, loss=3.7541725635528564
I0130 16:39:15.025419 139668746123008 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9477341175079346, loss=5.502039909362793
I0130 16:40:00.838832 139668754515712 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.3806986808776855, loss=3.8184845447540283
I0130 16:40:46.526120 139668746123008 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.3473927974700928, loss=3.707925319671631
I0130 16:41:32.008103 139668754515712 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.9235801100730896, loss=5.477222442626953
I0130 16:41:54.186076 139863983413056 spec.py:321] Evaluating on the training split.
I0130 16:42:05.592673 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 16:42:27.692035 139863983413056 spec.py:349] Evaluating on the test split.
I0130 16:42:29.335118 139863983413056 submission_runner.py:408] Time since start: 10860.99s, 	Step: 22550, 	{'train/accuracy': 0.5533007979393005, 'train/loss': 2.0510404109954834, 'validation/accuracy': 0.4919999837875366, 'validation/loss': 2.332852840423584, 'validation/num_examples': 50000, 'test/accuracy': 0.3823000192642212, 'test/loss': 2.93916916847229, 'test/num_examples': 10000, 'score': 10126.87127161026, 'total_duration': 10860.985714435577, 'accumulated_submission_time': 10126.87127161026, 'accumulated_eval_time': 732.1488399505615, 'accumulated_logging_time': 0.7885289192199707}
I0130 16:42:29.353606 139668746123008 logging_writer.py:48] [22550] accumulated_eval_time=732.148840, accumulated_logging_time=0.788529, accumulated_submission_time=10126.871272, global_step=22550, preemption_count=0, score=10126.871272, test/accuracy=0.382300, test/loss=2.939169, test/num_examples=10000, total_duration=10860.985714, train/accuracy=0.553301, train/loss=2.051040, validation/accuracy=0.492000, validation/loss=2.332853, validation/num_examples=50000
I0130 16:42:49.711441 139668754515712 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.3934669494628906, loss=3.7652788162231445
I0130 16:43:32.579468 139668746123008 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.2854028940200806, loss=4.374561309814453
I0130 16:44:18.280522 139668754515712 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.3701744079589844, loss=3.922325849533081
I0130 16:45:04.256150 139668746123008 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.4658985137939453, loss=3.6705946922302246
I0130 16:45:49.904229 139668754515712 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.4635852575302124, loss=3.7202694416046143
I0130 16:46:35.354743 139668746123008 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.2132662534713745, loss=3.7535994052886963
I0130 16:47:20.930324 139668754515712 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.2080135345458984, loss=3.8287384510040283
I0130 16:48:06.512454 139668746123008 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.3303987979888916, loss=3.8285746574401855
I0130 16:48:51.939770 139668754515712 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.2508811950683594, loss=3.758007049560547
I0130 16:49:29.645748 139863983413056 spec.py:321] Evaluating on the training split.
I0130 16:49:40.860731 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 16:50:03.892859 139863983413056 spec.py:349] Evaluating on the test split.
I0130 16:50:05.555438 139863983413056 submission_runner.py:408] Time since start: 11317.21s, 	Step: 23484, 	{'train/accuracy': 0.5318750143051147, 'train/loss': 2.154400587081909, 'validation/accuracy': 0.49793997406959534, 'validation/loss': 2.32008957862854, 'validation/num_examples': 50000, 'test/accuracy': 0.3897000253200531, 'test/loss': 2.925745725631714, 'test/num_examples': 10000, 'score': 10547.10615158081, 'total_duration': 11317.205980062485, 'accumulated_submission_time': 10547.10615158081, 'accumulated_eval_time': 768.0584897994995, 'accumulated_logging_time': 0.816164493560791}
I0130 16:50:05.578360 139668746123008 logging_writer.py:48] [23484] accumulated_eval_time=768.058490, accumulated_logging_time=0.816164, accumulated_submission_time=10547.106152, global_step=23484, preemption_count=0, score=10547.106152, test/accuracy=0.389700, test/loss=2.925746, test/num_examples=10000, total_duration=11317.205980, train/accuracy=0.531875, train/loss=2.154401, validation/accuracy=0.497940, validation/loss=2.320090, validation/num_examples=50000
I0130 16:50:12.360615 139668754515712 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.3169329166412354, loss=3.73762583732605
I0130 16:50:53.844652 139668746123008 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.0736949443817139, loss=4.676366329193115
I0130 16:51:39.513754 139668754515712 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.3364512920379639, loss=3.7115097045898438
I0130 16:52:25.578733 139668746123008 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.0740313529968262, loss=5.590702533721924
I0130 16:53:11.031220 139668754515712 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.2193197011947632, loss=3.9064712524414062
I0130 16:53:56.453188 139668746123008 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.1734403371810913, loss=4.121294975280762
I0130 16:54:42.239069 139668754515712 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.3955309391021729, loss=3.7726387977600098
I0130 16:55:27.526132 139668746123008 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.0853321552276611, loss=4.982077121734619
I0130 16:56:13.248215 139668754515712 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.1520949602127075, loss=3.886258602142334
I0130 16:56:58.604503 139668746123008 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.0484063625335693, loss=5.416642189025879
I0130 16:57:05.614078 139863983413056 spec.py:321] Evaluating on the training split.
I0130 16:57:16.618761 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 16:57:38.329695 139863983413056 spec.py:349] Evaluating on the test split.
I0130 16:57:39.967971 139863983413056 submission_runner.py:408] Time since start: 11771.62s, 	Step: 24417, 	{'train/accuracy': 0.5354101657867432, 'train/loss': 2.1295690536499023, 'validation/accuracy': 0.49625998735427856, 'validation/loss': 2.312972068786621, 'validation/num_examples': 50000, 'test/accuracy': 0.3899000287055969, 'test/loss': 2.9328761100769043, 'test/num_examples': 10000, 'score': 10967.08280968666, 'total_duration': 11771.618568897247, 'accumulated_submission_time': 10967.08280968666, 'accumulated_eval_time': 802.4123823642731, 'accumulated_logging_time': 0.8501076698303223}
I0130 16:57:39.985793 139668754515712 logging_writer.py:48] [24417] accumulated_eval_time=802.412382, accumulated_logging_time=0.850108, accumulated_submission_time=10967.082810, global_step=24417, preemption_count=0, score=10967.082810, test/accuracy=0.389900, test/loss=2.932876, test/num_examples=10000, total_duration=11771.618569, train/accuracy=0.535410, train/loss=2.129569, validation/accuracy=0.496260, validation/loss=2.312972, validation/num_examples=50000
I0130 16:58:13.546001 139668746123008 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.3225493431091309, loss=3.6833627223968506
I0130 16:58:58.634034 139668754515712 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.1222270727157593, loss=4.151487350463867
I0130 16:59:44.417303 139668746123008 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0488237142562866, loss=4.865769386291504
I0130 17:00:30.127345 139668754515712 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.299754023551941, loss=3.7022480964660645
I0130 17:01:15.704297 139668746123008 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.900119423866272, loss=5.6950578689575195
I0130 17:02:01.181157 139668754515712 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.4135825634002686, loss=3.7366583347320557
I0130 17:02:47.425215 139668746123008 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.199639916419983, loss=4.005987644195557
I0130 17:03:32.932170 139668754515712 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.2952085733413696, loss=3.635674476623535
I0130 17:04:18.392324 139668746123008 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.3153712749481201, loss=3.637310028076172
I0130 17:04:40.102458 139863983413056 spec.py:321] Evaluating on the training split.
I0130 17:04:50.925386 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 17:05:12.304360 139863983413056 spec.py:349] Evaluating on the test split.
I0130 17:05:13.949615 139863983413056 submission_runner.py:408] Time since start: 12225.60s, 	Step: 25349, 	{'train/accuracy': 0.5593945384025574, 'train/loss': 1.9963288307189941, 'validation/accuracy': 0.5059399604797363, 'validation/loss': 2.242471933364868, 'validation/num_examples': 50000, 'test/accuracy': 0.3992000222206116, 'test/loss': 2.854811668395996, 'test/num_examples': 10000, 'score': 11387.141655921936, 'total_duration': 12225.600093364716, 'accumulated_submission_time': 11387.141655921936, 'accumulated_eval_time': 836.2594237327576, 'accumulated_logging_time': 0.8776521682739258}
I0130 17:05:13.978272 139668754515712 logging_writer.py:48] [25349] accumulated_eval_time=836.259424, accumulated_logging_time=0.877652, accumulated_submission_time=11387.141656, global_step=25349, preemption_count=0, score=11387.141656, test/accuracy=0.399200, test/loss=2.854812, test/num_examples=10000, total_duration=12225.600093, train/accuracy=0.559395, train/loss=1.996329, validation/accuracy=0.505940, validation/loss=2.242472, validation/num_examples=50000
I0130 17:05:35.056368 139668746123008 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0004456043243408, loss=5.385926246643066
I0130 17:06:18.319329 139668754515712 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9691386818885803, loss=4.827536582946777
I0130 17:07:05.814243 139668746123008 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.1299333572387695, loss=4.6370697021484375
I0130 17:07:51.997128 139668754515712 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.0513451099395752, loss=4.448267936706543
I0130 17:08:37.839599 139668746123008 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.2966196537017822, loss=3.7396745681762695
I0130 17:09:23.307710 139668754515712 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.1393990516662598, loss=4.814690589904785
I0130 17:10:09.323871 139668746123008 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.20110285282135, loss=4.325382709503174
I0130 17:10:55.050539 139668754515712 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.320777177810669, loss=3.716404676437378
I0130 17:11:40.754150 139668746123008 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.5131508111953735, loss=3.6388158798217773
I0130 17:12:14.065642 139863983413056 spec.py:321] Evaluating on the training split.
I0130 17:12:24.086642 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 17:12:44.059207 139863983413056 spec.py:349] Evaluating on the test split.
I0130 17:12:45.710747 139863983413056 submission_runner.py:408] Time since start: 12677.36s, 	Step: 26274, 	{'train/accuracy': 0.5405468344688416, 'train/loss': 2.104295015335083, 'validation/accuracy': 0.5084199905395508, 'validation/loss': 2.2594079971313477, 'validation/num_examples': 50000, 'test/accuracy': 0.40050002932548523, 'test/loss': 2.8530092239379883, 'test/num_examples': 10000, 'score': 11806.869129419327, 'total_duration': 12677.361342906952, 'accumulated_submission_time': 11806.869129419327, 'accumulated_eval_time': 867.9045441150665, 'accumulated_logging_time': 1.2171745300292969}
I0130 17:12:45.729464 139668754515712 logging_writer.py:48] [26274] accumulated_eval_time=867.904544, accumulated_logging_time=1.217175, accumulated_submission_time=11806.869129, global_step=26274, preemption_count=0, score=11806.869129, test/accuracy=0.400500, test/loss=2.853009, test/num_examples=10000, total_duration=12677.361343, train/accuracy=0.540547, train/loss=2.104295, validation/accuracy=0.508420, validation/loss=2.259408, validation/num_examples=50000
I0130 17:12:56.527857 139668746123008 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.236659049987793, loss=3.601322650909424
I0130 17:13:39.956457 139668754515712 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.2758584022521973, loss=3.774822235107422
I0130 17:14:25.467224 139668746123008 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9930301904678345, loss=5.771501064300537
I0130 17:15:11.108706 139668754515712 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.3482964038848877, loss=3.685220718383789
I0130 17:15:56.612266 139668746123008 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.2242692708969116, loss=3.9442670345306396
I0130 17:16:41.809017 139668754515712 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.3251479864120483, loss=3.495771646499634
I0130 17:17:27.228220 139668746123008 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.2960014343261719, loss=4.000494956970215
I0130 17:18:12.833743 139668754515712 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.3907185792922974, loss=3.5189101696014404
I0130 17:18:58.320535 139668746123008 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.2889752388000488, loss=3.603571653366089
I0130 17:19:43.645881 139668754515712 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.4509848356246948, loss=3.6809182167053223
I0130 17:19:46.194777 139863983413056 spec.py:321] Evaluating on the training split.
I0130 17:19:57.333087 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 17:20:15.579149 139863983413056 spec.py:349] Evaluating on the test split.
I0130 17:20:17.248656 139863983413056 submission_runner.py:408] Time since start: 13128.90s, 	Step: 27207, 	{'train/accuracy': 0.5548046827316284, 'train/loss': 2.044733762741089, 'validation/accuracy': 0.5141599774360657, 'validation/loss': 2.2269556522369385, 'validation/num_examples': 50000, 'test/accuracy': 0.4026000201702118, 'test/loss': 2.8585050106048584, 'test/num_examples': 10000, 'score': 12227.276673793793, 'total_duration': 13128.899226903915, 'accumulated_submission_time': 12227.276673793793, 'accumulated_eval_time': 898.9583828449249, 'accumulated_logging_time': 1.245424747467041}
I0130 17:20:17.275213 139668746123008 logging_writer.py:48] [27207] accumulated_eval_time=898.958383, accumulated_logging_time=1.245425, accumulated_submission_time=12227.276674, global_step=27207, preemption_count=0, score=12227.276674, test/accuracy=0.402600, test/loss=2.858505, test/num_examples=10000, total_duration=13128.899227, train/accuracy=0.554805, train/loss=2.044734, validation/accuracy=0.514160, validation/loss=2.226956, validation/num_examples=50000
I0130 17:20:55.367041 139668754515712 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.4596936702728271, loss=3.6173834800720215
I0130 17:21:40.697915 139668746123008 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.995316743850708, loss=5.520565986633301
I0130 17:22:26.672235 139668754515712 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.1412749290466309, loss=4.321096420288086
I0130 17:23:13.139730 139668746123008 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.4643449783325195, loss=3.5985028743743896
I0130 17:23:58.456409 139668754515712 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.252556324005127, loss=4.238813877105713
I0130 17:24:44.320783 139668746123008 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.9967223405838013, loss=4.561734199523926
I0130 17:25:30.015076 139668754515712 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.4943119287490845, loss=3.6273415088653564
I0130 17:26:15.531456 139668746123008 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.3453009128570557, loss=3.864623546600342
I0130 17:27:01.215267 139668754515712 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.3566032648086548, loss=3.8012807369232178
I0130 17:27:17.277932 139863983413056 spec.py:321] Evaluating on the training split.
I0130 17:27:28.283758 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 17:27:49.121994 139863983413056 spec.py:349] Evaluating on the test split.
I0130 17:27:50.758647 139863983413056 submission_runner.py:408] Time since start: 13582.41s, 	Step: 28137, 	{'train/accuracy': 0.5671679377555847, 'train/loss': 1.9500863552093506, 'validation/accuracy': 0.5212999582290649, 'validation/loss': 2.152677536010742, 'validation/num_examples': 50000, 'test/accuracy': 0.4109000265598297, 'test/loss': 2.7780418395996094, 'test/num_examples': 10000, 'score': 12647.221132278442, 'total_duration': 13582.409242868423, 'accumulated_submission_time': 12647.221132278442, 'accumulated_eval_time': 932.4390978813171, 'accumulated_logging_time': 1.2829210758209229}
I0130 17:27:50.777053 139668746123008 logging_writer.py:48] [28137] accumulated_eval_time=932.439098, accumulated_logging_time=1.282921, accumulated_submission_time=12647.221132, global_step=28137, preemption_count=0, score=12647.221132, test/accuracy=0.410900, test/loss=2.778042, test/num_examples=10000, total_duration=13582.409243, train/accuracy=0.567168, train/loss=1.950086, validation/accuracy=0.521300, validation/loss=2.152678, validation/num_examples=50000
I0130 17:28:16.336778 139668754515712 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.3194010257720947, loss=3.609539270401001
I0130 17:29:00.048452 139668746123008 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.3209960460662842, loss=3.6491899490356445
I0130 17:29:45.784421 139668754515712 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.1737674474716187, loss=4.073282718658447
I0130 17:30:31.589712 139668746123008 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8915265202522278, loss=5.6500468254089355
I0130 17:31:17.169760 139668754515712 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.9648376107215881, loss=5.693258762359619
I0130 17:32:02.465392 139668746123008 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.265014410018921, loss=3.853105068206787
I0130 17:32:48.395094 139668754515712 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.3006014823913574, loss=3.5409109592437744
I0130 17:33:33.998178 139668746123008 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.3114675283432007, loss=3.5323257446289062
I0130 17:34:19.585454 139668754515712 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.1787704229354858, loss=3.916090488433838
I0130 17:34:51.052924 139863983413056 spec.py:321] Evaluating on the training split.
I0130 17:35:02.038213 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 17:35:23.954109 139863983413056 spec.py:349] Evaluating on the test split.
I0130 17:35:25.607248 139863983413056 submission_runner.py:408] Time since start: 14037.26s, 	Step: 29071, 	{'train/accuracy': 0.570507824420929, 'train/loss': 1.9200985431671143, 'validation/accuracy': 0.5317800045013428, 'validation/loss': 2.108691453933716, 'validation/num_examples': 50000, 'test/accuracy': 0.42430001497268677, 'test/loss': 2.731639862060547, 'test/num_examples': 10000, 'score': 13067.440270900726, 'total_duration': 14037.257809400558, 'accumulated_submission_time': 13067.440270900726, 'accumulated_eval_time': 966.9934012889862, 'accumulated_logging_time': 1.3104724884033203}
I0130 17:35:25.629053 139668746123008 logging_writer.py:48] [29071] accumulated_eval_time=966.993401, accumulated_logging_time=1.310472, accumulated_submission_time=13067.440271, global_step=29071, preemption_count=0, score=13067.440271, test/accuracy=0.424300, test/loss=2.731640, test/num_examples=10000, total_duration=14037.257809, train/accuracy=0.570508, train/loss=1.920099, validation/accuracy=0.531780, validation/loss=2.108691, validation/num_examples=50000
I0130 17:35:37.597558 139668754515712 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.0995171070098877, loss=4.185105323791504
I0130 17:36:19.622507 139668746123008 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.1447709798812866, loss=4.468412399291992
I0130 17:37:05.427932 139668754515712 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.2565298080444336, loss=3.757028102874756
I0130 17:37:50.893758 139668746123008 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.298957347869873, loss=3.5344431400299072
I0130 17:38:36.838524 139668754515712 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.5022720098495483, loss=3.66141676902771
I0130 17:39:22.191658 139668746123008 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.3354721069335938, loss=3.555499792098999
I0130 17:40:07.841473 139668754515712 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.2361547946929932, loss=4.237379550933838
I0130 17:40:53.408621 139668746123008 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.1020232439041138, loss=4.711988925933838
I0130 17:41:39.064105 139668754515712 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.3782694339752197, loss=3.5663352012634277
I0130 17:42:24.676926 139668746123008 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.3061192035675049, loss=3.5648021697998047
I0130 17:42:25.769055 139863983413056 spec.py:321] Evaluating on the training split.
I0130 17:42:36.515671 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 17:42:56.326097 139863983413056 spec.py:349] Evaluating on the test split.
I0130 17:42:57.963426 139863983413056 submission_runner.py:408] Time since start: 14489.61s, 	Step: 30004, 	{'train/accuracy': 0.5688085556030273, 'train/loss': 1.989999532699585, 'validation/accuracy': 0.5308399796485901, 'validation/loss': 2.1682181358337402, 'validation/num_examples': 50000, 'test/accuracy': 0.41940000653266907, 'test/loss': 2.765523672103882, 'test/num_examples': 10000, 'score': 13487.520725488663, 'total_duration': 14489.614025115967, 'accumulated_submission_time': 13487.520725488663, 'accumulated_eval_time': 999.1877725124359, 'accumulated_logging_time': 1.343907117843628}
I0130 17:42:57.982591 139668754515712 logging_writer.py:48] [30004] accumulated_eval_time=999.187773, accumulated_logging_time=1.343907, accumulated_submission_time=13487.520725, global_step=30004, preemption_count=0, score=13487.520725, test/accuracy=0.419400, test/loss=2.765524, test/num_examples=10000, total_duration=14489.614025, train/accuracy=0.568809, train/loss=1.990000, validation/accuracy=0.530840, validation/loss=2.168218, validation/num_examples=50000
I0130 17:43:37.460911 139668746123008 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.4093173742294312, loss=3.5449700355529785
I0130 17:44:23.237552 139668754515712 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.0200958251953125, loss=5.689645767211914
I0130 17:45:09.252099 139668746123008 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.3733940124511719, loss=3.683972120285034
I0130 17:45:55.281688 139668754515712 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.1354238986968994, loss=4.738597393035889
I0130 17:46:40.765737 139668746123008 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.1983232498168945, loss=3.804002523422241
I0130 17:47:26.439268 139668754515712 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.2569438219070435, loss=5.639386177062988
I0130 17:48:12.160835 139668746123008 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.571650743484497, loss=3.5502336025238037
I0130 17:48:57.584830 139668754515712 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.414760947227478, loss=3.779207468032837
I0130 17:49:43.252092 139668746123008 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.217154860496521, loss=4.779319763183594
I0130 17:49:58.130773 139863983413056 spec.py:321] Evaluating on the training split.
I0130 17:50:09.273251 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 17:50:28.545359 139863983413056 spec.py:349] Evaluating on the test split.
I0130 17:50:30.192496 139863983413056 submission_runner.py:408] Time since start: 14941.84s, 	Step: 30934, 	{'train/accuracy': 0.5673046708106995, 'train/loss': 1.9936236143112183, 'validation/accuracy': 0.526419997215271, 'validation/loss': 2.189337968826294, 'validation/num_examples': 50000, 'test/accuracy': 0.41370001435279846, 'test/loss': 2.810401201248169, 'test/num_examples': 10000, 'score': 13907.610777139664, 'total_duration': 14941.843090057373, 'accumulated_submission_time': 13907.610777139664, 'accumulated_eval_time': 1031.2494950294495, 'accumulated_logging_time': 1.3736467361450195}
I0130 17:50:30.212519 139668754515712 logging_writer.py:48] [30934] accumulated_eval_time=1031.249495, accumulated_logging_time=1.373647, accumulated_submission_time=13907.610777, global_step=30934, preemption_count=0, score=13907.610777, test/accuracy=0.413700, test/loss=2.810401, test/num_examples=10000, total_duration=14941.843090, train/accuracy=0.567305, train/loss=1.993624, validation/accuracy=0.526420, validation/loss=2.189338, validation/num_examples=50000
I0130 17:50:56.987242 139668746123008 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.2032804489135742, loss=4.574585437774658
I0130 17:51:41.707072 139668754515712 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.4166960716247559, loss=3.825481414794922
I0130 17:52:27.410287 139668746123008 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.5186837911605835, loss=3.637803077697754
I0130 17:53:13.689026 139668754515712 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.3496021032333374, loss=3.6479804515838623
I0130 17:53:59.208131 139668746123008 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.3251268863677979, loss=3.4376351833343506
I0130 17:54:44.966548 139668754515712 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.3474243879318237, loss=3.4471569061279297
I0130 17:55:30.579193 139668746123008 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.3374491930007935, loss=3.5380539894104004
I0130 17:56:16.256740 139668754515712 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.2682081460952759, loss=3.7350106239318848
I0130 17:57:01.900036 139668746123008 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.274080753326416, loss=4.12105655670166
I0130 17:57:30.260701 139863983413056 spec.py:321] Evaluating on the training split.
I0130 17:57:41.059323 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 17:58:02.414997 139863983413056 spec.py:349] Evaluating on the test split.
I0130 17:58:04.058581 139863983413056 submission_runner.py:408] Time since start: 15395.71s, 	Step: 31864, 	{'train/accuracy': 0.5966015458106995, 'train/loss': 1.8194531202316284, 'validation/accuracy': 0.5347200036048889, 'validation/loss': 2.106844902038574, 'validation/num_examples': 50000, 'test/accuracy': 0.4237000346183777, 'test/loss': 2.7296688556671143, 'test/num_examples': 10000, 'score': 14327.601864337921, 'total_duration': 15395.709174633026, 'accumulated_submission_time': 14327.601864337921, 'accumulated_eval_time': 1065.047378540039, 'accumulated_logging_time': 1.4030015468597412}
I0130 17:58:04.079111 139668754515712 logging_writer.py:48] [31864] accumulated_eval_time=1065.047379, accumulated_logging_time=1.403002, accumulated_submission_time=14327.601864, global_step=31864, preemption_count=0, score=14327.601864, test/accuracy=0.423700, test/loss=2.729669, test/num_examples=10000, total_duration=15395.709175, train/accuracy=0.596602, train/loss=1.819453, validation/accuracy=0.534720, validation/loss=2.106845, validation/num_examples=50000
I0130 17:58:18.835838 139668746123008 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.0716731548309326, loss=5.584211349487305
I0130 17:59:01.169656 139668754515712 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.3744962215423584, loss=3.5225026607513428
I0130 17:59:46.897989 139668746123008 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.3740520477294922, loss=3.452446460723877
I0130 18:00:32.730070 139668754515712 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.3136813640594482, loss=3.4740915298461914
I0130 18:01:18.331886 139668746123008 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.0326200723648071, loss=4.500204086303711
I0130 18:02:04.205725 139668754515712 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.3859912157058716, loss=3.5152792930603027
I0130 18:02:49.557585 139668746123008 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.0913631916046143, loss=5.115843296051025
I0130 18:03:35.518486 139668754515712 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0836843252182007, loss=5.692502021789551
I0130 18:04:20.993930 139668746123008 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.3033479452133179, loss=3.728843927383423
I0130 18:05:04.214526 139863983413056 spec.py:321] Evaluating on the training split.
I0130 18:05:15.020680 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 18:05:36.033136 139863983413056 spec.py:349] Evaluating on the test split.
I0130 18:05:37.667942 139863983413056 submission_runner.py:408] Time since start: 15849.32s, 	Step: 32796, 	{'train/accuracy': 0.5743749737739563, 'train/loss': 1.9367910623550415, 'validation/accuracy': 0.5367400050163269, 'validation/loss': 2.105886220932007, 'validation/num_examples': 50000, 'test/accuracy': 0.4262000322341919, 'test/loss': 2.733640193939209, 'test/num_examples': 10000, 'score': 14747.678482532501, 'total_duration': 15849.318536996841, 'accumulated_submission_time': 14747.678482532501, 'accumulated_eval_time': 1098.5008039474487, 'accumulated_logging_time': 1.434175968170166}
I0130 18:05:37.691096 139668754515712 logging_writer.py:48] [32796] accumulated_eval_time=1098.500804, accumulated_logging_time=1.434176, accumulated_submission_time=14747.678483, global_step=32796, preemption_count=0, score=14747.678483, test/accuracy=0.426200, test/loss=2.733640, test/num_examples=10000, total_duration=15849.318537, train/accuracy=0.574375, train/loss=1.936791, validation/accuracy=0.536740, validation/loss=2.105886, validation/num_examples=50000
I0130 18:05:39.690948 139668746123008 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.4368559122085571, loss=3.4703595638275146
I0130 18:06:20.287687 139668754515712 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.443335771560669, loss=3.4777402877807617
I0130 18:07:05.945329 139668746123008 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.964570939540863, loss=5.707094192504883
I0130 18:07:51.550951 139668754515712 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.3958989381790161, loss=3.4814000129699707
I0130 18:08:37.464906 139668746123008 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.526689052581787, loss=3.4932727813720703
I0130 18:09:22.910480 139668754515712 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.3943461179733276, loss=3.4477717876434326
I0130 18:10:08.752279 139668746123008 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.1416107416152954, loss=5.367237567901611
I0130 18:10:54.312012 139668754515712 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.3873292207717896, loss=3.630976915359497
I0130 18:11:39.919460 139668746123008 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.3650720119476318, loss=3.606790781021118
I0130 18:12:25.653728 139668754515712 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.0981115102767944, loss=4.632959365844727
I0130 18:12:37.750458 139863983413056 spec.py:321] Evaluating on the training split.
I0130 18:12:48.794270 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 18:13:09.917148 139863983413056 spec.py:349] Evaluating on the test split.
I0130 18:13:11.553551 139863983413056 submission_runner.py:408] Time since start: 16303.20s, 	Step: 33728, 	{'train/accuracy': 0.5781444907188416, 'train/loss': 1.909422755241394, 'validation/accuracy': 0.536899983882904, 'validation/loss': 2.1106417179107666, 'validation/num_examples': 50000, 'test/accuracy': 0.4223000109195709, 'test/loss': 2.7446610927581787, 'test/num_examples': 10000, 'score': 15167.680012226105, 'total_duration': 16303.204138755798, 'accumulated_submission_time': 15167.680012226105, 'accumulated_eval_time': 1132.3038840293884, 'accumulated_logging_time': 1.4670917987823486}
I0130 18:13:11.573669 139668746123008 logging_writer.py:48] [33728] accumulated_eval_time=1132.303884, accumulated_logging_time=1.467092, accumulated_submission_time=15167.680012, global_step=33728, preemption_count=0, score=15167.680012, test/accuracy=0.422300, test/loss=2.744661, test/num_examples=10000, total_duration=16303.204139, train/accuracy=0.578144, train/loss=1.909423, validation/accuracy=0.536900, validation/loss=2.110642, validation/num_examples=50000
I0130 18:13:40.709530 139668754515712 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.3690602779388428, loss=3.404595136642456
I0130 18:14:25.581216 139668746123008 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.0610688924789429, loss=5.732446193695068
I0130 18:15:10.913672 139668754515712 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.1821792125701904, loss=4.275777339935303
I0130 18:15:56.644580 139668746123008 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.4446803331375122, loss=3.5899229049682617
I0130 18:16:42.117728 139668754515712 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.384880781173706, loss=3.534982919692993
I0130 18:17:27.523172 139668746123008 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.2844047546386719, loss=3.7334327697753906
I0130 18:18:13.070855 139668754515712 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.4448678493499756, loss=3.4556148052215576
I0130 18:18:58.553720 139668746123008 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.6041890382766724, loss=3.48048734664917
I0130 18:19:43.808905 139668754515712 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.4795079231262207, loss=3.4933342933654785
I0130 18:20:11.860993 139863983413056 spec.py:321] Evaluating on the training split.
I0130 18:20:22.767180 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 18:20:42.191212 139863983413056 spec.py:349] Evaluating on the test split.
I0130 18:20:43.851474 139863983413056 submission_runner.py:408] Time since start: 16755.50s, 	Step: 34663, 	{'train/accuracy': 0.5896288752555847, 'train/loss': 1.9078178405761719, 'validation/accuracy': 0.5367599725723267, 'validation/loss': 2.140103340148926, 'validation/num_examples': 50000, 'test/accuracy': 0.427700012922287, 'test/loss': 2.7481539249420166, 'test/num_examples': 10000, 'score': 15587.909015655518, 'total_duration': 16755.502063274384, 'accumulated_submission_time': 15587.909015655518, 'accumulated_eval_time': 1164.2943496704102, 'accumulated_logging_time': 1.4970180988311768}
I0130 18:20:43.875246 139668746123008 logging_writer.py:48] [34663] accumulated_eval_time=1164.294350, accumulated_logging_time=1.497018, accumulated_submission_time=15587.909016, global_step=34663, preemption_count=0, score=15587.909016, test/accuracy=0.427700, test/loss=2.748154, test/num_examples=10000, total_duration=16755.502063, train/accuracy=0.589629, train/loss=1.907818, validation/accuracy=0.536760, validation/loss=2.140103, validation/num_examples=50000
I0130 18:20:59.062905 139668754515712 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.3006157875061035, loss=3.4195971488952637
I0130 18:21:42.052815 139668746123008 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.1695399284362793, loss=4.514423370361328
I0130 18:22:27.236804 139668754515712 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.334886908531189, loss=3.5959041118621826
I0130 18:23:13.026096 139668746123008 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.4246091842651367, loss=3.545135736465454
I0130 18:23:59.088045 139668754515712 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.4025988578796387, loss=3.6096713542938232
I0130 18:24:44.584674 139668746123008 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.363587498664856, loss=3.5134005546569824
I0130 18:25:30.220468 139668754515712 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.4384782314300537, loss=3.396733045578003
I0130 18:26:16.213295 139668746123008 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.2870638370513916, loss=4.314177513122559
I0130 18:27:01.810209 139668754515712 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.9730649590492249, loss=5.305968284606934
I0130 18:27:44.248307 139863983413056 spec.py:321] Evaluating on the training split.
I0130 18:27:55.269011 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 18:28:16.260378 139863983413056 spec.py:349] Evaluating on the test split.
I0130 18:28:17.906244 139863983413056 submission_runner.py:408] Time since start: 17209.56s, 	Step: 35595, 	{'train/accuracy': 0.5802148580551147, 'train/loss': 1.8932175636291504, 'validation/accuracy': 0.5425800085067749, 'validation/loss': 2.0681850910186768, 'validation/num_examples': 50000, 'test/accuracy': 0.42820000648498535, 'test/loss': 2.7046916484832764, 'test/num_examples': 10000, 'score': 16008.223159313202, 'total_duration': 17209.556839942932, 'accumulated_submission_time': 16008.223159313202, 'accumulated_eval_time': 1197.9523015022278, 'accumulated_logging_time': 1.5312883853912354}
I0130 18:28:17.930549 139668746123008 logging_writer.py:48] [35595] accumulated_eval_time=1197.952302, accumulated_logging_time=1.531288, accumulated_submission_time=16008.223159, global_step=35595, preemption_count=0, score=16008.223159, test/accuracy=0.428200, test/loss=2.704692, test/num_examples=10000, total_duration=17209.556840, train/accuracy=0.580215, train/loss=1.893218, validation/accuracy=0.542580, validation/loss=2.068185, validation/num_examples=50000
I0130 18:28:20.328897 139668754515712 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.2534096240997314, loss=3.921022653579712
I0130 18:29:01.064272 139668746123008 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.2064584493637085, loss=4.576861381530762
I0130 18:29:46.521580 139668754515712 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.4917995929718018, loss=3.484668493270874
I0130 18:30:32.173713 139668746123008 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.3538533449172974, loss=3.627058982849121
I0130 18:31:18.188472 139668754515712 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.584904432296753, loss=3.63755464553833
I0130 18:32:04.013386 139668746123008 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.3303933143615723, loss=3.487166404724121
I0130 18:32:49.640233 139668754515712 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.1702736616134644, loss=4.492345333099365
I0130 18:33:35.594692 139668746123008 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.5824432373046875, loss=3.4200737476348877
I0130 18:34:21.137293 139668754515712 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.3646701574325562, loss=3.490511417388916
I0130 18:35:06.580057 139668746123008 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.348184585571289, loss=4.075166702270508
I0130 18:35:18.077915 139863983413056 spec.py:321] Evaluating on the training split.
I0130 18:35:29.148385 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 18:35:49.694346 139863983413056 spec.py:349] Evaluating on the test split.
I0130 18:35:51.333383 139863983413056 submission_runner.py:408] Time since start: 17662.98s, 	Step: 36527, 	{'train/accuracy': 0.5877343416213989, 'train/loss': 1.890945553779602, 'validation/accuracy': 0.545740008354187, 'validation/loss': 2.086134433746338, 'validation/num_examples': 50000, 'test/accuracy': 0.43640002608299255, 'test/loss': 2.7020034790039062, 'test/num_examples': 10000, 'score': 16428.311499357224, 'total_duration': 17662.983982801437, 'accumulated_submission_time': 16428.311499357224, 'accumulated_eval_time': 1231.2077586650848, 'accumulated_logging_time': 1.5664069652557373}
I0130 18:35:51.355324 139668754515712 logging_writer.py:48] [36527] accumulated_eval_time=1231.207759, accumulated_logging_time=1.566407, accumulated_submission_time=16428.311499, global_step=36527, preemption_count=0, score=16428.311499, test/accuracy=0.436400, test/loss=2.702003, test/num_examples=10000, total_duration=17662.983983, train/accuracy=0.587734, train/loss=1.890946, validation/accuracy=0.545740, validation/loss=2.086134, validation/num_examples=50000
I0130 18:36:20.896970 139668746123008 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.0673214197158813, loss=4.682580947875977
I0130 18:37:05.761735 139668754515712 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.2984180450439453, loss=3.7468554973602295
I0130 18:37:51.345707 139668746123008 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.4904571771621704, loss=3.673556327819824
I0130 18:38:37.229592 139668754515712 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.1211259365081787, loss=5.439413547515869
I0130 18:39:22.760662 139668746123008 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.3779326677322388, loss=3.5756607055664062
I0130 18:40:08.465724 139668754515712 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.3826713562011719, loss=3.4365837574005127
I0130 18:40:54.138567 139668746123008 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.1721017360687256, loss=4.961700916290283
I0130 18:41:39.614533 139668754515712 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.253346562385559, loss=4.151926517486572
I0130 18:42:25.332215 139668746123008 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.3387086391448975, loss=3.7604236602783203
I0130 18:42:51.437598 139863983413056 spec.py:321] Evaluating on the training split.
I0130 18:43:02.781239 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 18:43:24.037489 139863983413056 spec.py:349] Evaluating on the test split.
I0130 18:43:25.670629 139863983413056 submission_runner.py:408] Time since start: 18117.32s, 	Step: 37459, 	{'train/accuracy': 0.5893945097923279, 'train/loss': 1.879970908164978, 'validation/accuracy': 0.5448200106620789, 'validation/loss': 2.087385654449463, 'validation/num_examples': 50000, 'test/accuracy': 0.43080002069473267, 'test/loss': 2.711873769760132, 'test/num_examples': 10000, 'score': 16848.334990262985, 'total_duration': 18117.32121515274, 'accumulated_submission_time': 16848.334990262985, 'accumulated_eval_time': 1265.4408011436462, 'accumulated_logging_time': 1.5989644527435303}
I0130 18:43:25.690781 139668754515712 logging_writer.py:48] [37459] accumulated_eval_time=1265.440801, accumulated_logging_time=1.598964, accumulated_submission_time=16848.334990, global_step=37459, preemption_count=0, score=16848.334990, test/accuracy=0.430800, test/loss=2.711874, test/num_examples=10000, total_duration=18117.321215, train/accuracy=0.589395, train/loss=1.879971, validation/accuracy=0.544820, validation/loss=2.087386, validation/num_examples=50000
I0130 18:43:42.454411 139668746123008 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.5315072536468506, loss=3.488659143447876
I0130 18:44:25.404474 139668754515712 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.220806360244751, loss=5.517486572265625
I0130 18:45:11.029664 139668746123008 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.3248538970947266, loss=3.7992262840270996
I0130 18:45:56.414474 139668754515712 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.335339069366455, loss=3.6325855255126953
I0130 18:46:41.974102 139668746123008 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.14582359790802, loss=5.610992908477783
I0130 18:47:27.506637 139668754515712 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.384310007095337, loss=3.5191454887390137
I0130 18:48:13.206740 139668746123008 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.3204317092895508, loss=3.595019817352295
I0130 18:48:58.823090 139668754515712 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.4166330099105835, loss=3.4581360816955566
I0130 18:49:44.547090 139668746123008 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.2516952753067017, loss=4.244344711303711
I0130 18:50:25.849326 139863983413056 spec.py:321] Evaluating on the training split.
I0130 18:50:36.657881 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 18:50:56.491256 139863983413056 spec.py:349] Evaluating on the test split.
I0130 18:50:58.131317 139863983413056 submission_runner.py:408] Time since start: 18569.78s, 	Step: 38392, 	{'train/accuracy': 0.6200585961341858, 'train/loss': 1.7613177299499512, 'validation/accuracy': 0.5546599626541138, 'validation/loss': 2.0597145557403564, 'validation/num_examples': 50000, 'test/accuracy': 0.43700000643730164, 'test/loss': 2.6826961040496826, 'test/num_examples': 10000, 'score': 17268.433282613754, 'total_duration': 18569.78191447258, 'accumulated_submission_time': 17268.433282613754, 'accumulated_eval_time': 1297.7227976322174, 'accumulated_logging_time': 1.631274700164795}
I0130 18:50:58.152951 139668754515712 logging_writer.py:48] [38392] accumulated_eval_time=1297.722798, accumulated_logging_time=1.631275, accumulated_submission_time=17268.433283, global_step=38392, preemption_count=0, score=17268.433283, test/accuracy=0.437000, test/loss=2.682696, test/num_examples=10000, total_duration=18569.781914, train/accuracy=0.620059, train/loss=1.761318, validation/accuracy=0.554660, validation/loss=2.059715, validation/num_examples=50000
I0130 18:51:01.744565 139668746123008 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.3914066553115845, loss=3.510328769683838
I0130 18:51:42.614353 139668754515712 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.4535043239593506, loss=3.602701187133789
I0130 18:52:28.417893 139668746123008 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.2600785493850708, loss=4.377168655395508
I0130 18:53:14.368478 139668754515712 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.3893018960952759, loss=3.5045602321624756
I0130 18:54:00.331670 139668746123008 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.5824435949325562, loss=3.4664900302886963
I0130 18:54:45.545992 139668754515712 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.6109342575073242, loss=3.4681310653686523
I0130 18:55:31.136670 139668746123008 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.3789985179901123, loss=3.4914698600769043
I0130 18:56:16.718894 139668754515712 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.4683339595794678, loss=3.4945976734161377
I0130 18:57:02.009698 139668746123008 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.469101071357727, loss=3.4668383598327637
I0130 18:57:47.655790 139668754515712 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.530789852142334, loss=3.8680408000946045
I0130 18:57:58.146038 139863983413056 spec.py:321] Evaluating on the training split.
I0130 18:58:09.111090 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 18:58:29.086733 139863983413056 spec.py:349] Evaluating on the test split.
I0130 18:58:30.728374 139863983413056 submission_runner.py:408] Time since start: 19022.38s, 	Step: 39325, 	{'train/accuracy': 0.5931640267372131, 'train/loss': 1.8362483978271484, 'validation/accuracy': 0.5575799942016602, 'validation/loss': 2.013503313064575, 'validation/num_examples': 50000, 'test/accuracy': 0.44110003113746643, 'test/loss': 2.634138584136963, 'test/num_examples': 10000, 'score': 17688.36795592308, 'total_duration': 19022.378975868225, 'accumulated_submission_time': 17688.36795592308, 'accumulated_eval_time': 1330.3051433563232, 'accumulated_logging_time': 1.663404941558838}
I0130 18:58:30.751945 139668746123008 logging_writer.py:48] [39325] accumulated_eval_time=1330.305143, accumulated_logging_time=1.663405, accumulated_submission_time=17688.367956, global_step=39325, preemption_count=0, score=17688.367956, test/accuracy=0.441100, test/loss=2.634139, test/num_examples=10000, total_duration=19022.378976, train/accuracy=0.593164, train/loss=1.836248, validation/accuracy=0.557580, validation/loss=2.013503, validation/num_examples=50000
I0130 18:59:01.106580 139668754515712 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.0961003303527832, loss=4.963596820831299
I0130 18:59:46.084151 139668746123008 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.2700716257095337, loss=3.669471502304077
I0130 19:00:31.863998 139668754515712 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.3882620334625244, loss=4.116147518157959
I0130 19:01:17.456088 139668746123008 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.54087495803833, loss=3.4024765491485596
I0130 19:02:03.000637 139668754515712 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.3522889614105225, loss=3.4674928188323975
I0130 19:02:48.515641 139668746123008 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.2790658473968506, loss=3.738487720489502
I0130 19:03:34.062079 139668754515712 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.3410401344299316, loss=3.5736076831817627
I0130 19:04:19.684722 139668746123008 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.3743919134140015, loss=3.751502752304077
I0130 19:05:05.250518 139668754515712 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.4809273481369019, loss=3.3137853145599365
I0130 19:05:31.012784 139863983413056 spec.py:321] Evaluating on the training split.
I0130 19:05:41.804387 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 19:06:02.141350 139863983413056 spec.py:349] Evaluating on the test split.
I0130 19:06:03.790569 139863983413056 submission_runner.py:408] Time since start: 19475.44s, 	Step: 40258, 	{'train/accuracy': 0.5926952958106995, 'train/loss': 1.8907710313796997, 'validation/accuracy': 0.5488799810409546, 'validation/loss': 2.0913960933685303, 'validation/num_examples': 50000, 'test/accuracy': 0.44050002098083496, 'test/loss': 2.6993274688720703, 'test/num_examples': 10000, 'score': 18108.570605278015, 'total_duration': 19475.44114756584, 'accumulated_submission_time': 18108.570605278015, 'accumulated_eval_time': 1363.0829148292542, 'accumulated_logging_time': 1.6961984634399414}
I0130 19:06:03.811662 139668746123008 logging_writer.py:48] [40258] accumulated_eval_time=1363.082915, accumulated_logging_time=1.696198, accumulated_submission_time=18108.570605, global_step=40258, preemption_count=0, score=18108.570605, test/accuracy=0.440500, test/loss=2.699327, test/num_examples=10000, total_duration=19475.441148, train/accuracy=0.592695, train/loss=1.890771, validation/accuracy=0.548880, validation/loss=2.091396, validation/num_examples=50000
I0130 19:06:20.972848 139668754515712 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.2306818962097168, loss=3.993864059448242
I0130 19:07:04.240385 139668746123008 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.4275529384613037, loss=3.392275333404541
I0130 19:07:49.791689 139668754515712 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.359194278717041, loss=3.873018503189087
I0130 19:08:35.433451 139668746123008 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.1394331455230713, loss=5.190893650054932
I0130 19:09:21.486613 139668754515712 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.1035146713256836, loss=5.64424467086792
I0130 19:10:07.249894 139668746123008 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.3462493419647217, loss=3.4904701709747314
I0130 19:10:52.905574 139668754515712 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.02662992477417, loss=5.347705841064453
I0130 19:11:38.461410 139668746123008 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.4532159566879272, loss=3.654877185821533
I0130 19:12:24.320296 139668754515712 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.2768361568450928, loss=3.491900682449341
I0130 19:13:03.973439 139863983413056 spec.py:321] Evaluating on the training split.
I0130 19:13:14.758492 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 19:13:34.196329 139863983413056 spec.py:349] Evaluating on the test split.
I0130 19:13:35.851643 139863983413056 submission_runner.py:408] Time since start: 19927.50s, 	Step: 41188, 	{'train/accuracy': 0.6147655844688416, 'train/loss': 1.7229764461517334, 'validation/accuracy': 0.560259997844696, 'validation/loss': 1.9867902994155884, 'validation/num_examples': 50000, 'test/accuracy': 0.44780001044273376, 'test/loss': 2.6145589351654053, 'test/num_examples': 10000, 'score': 18528.672277212143, 'total_duration': 19927.50222015381, 'accumulated_submission_time': 18528.672277212143, 'accumulated_eval_time': 1394.9611177444458, 'accumulated_logging_time': 1.7287757396697998}
I0130 19:13:35.872137 139668746123008 logging_writer.py:48] [41188] accumulated_eval_time=1394.961118, accumulated_logging_time=1.728776, accumulated_submission_time=18528.672277, global_step=41188, preemption_count=0, score=18528.672277, test/accuracy=0.447800, test/loss=2.614559, test/num_examples=10000, total_duration=19927.502220, train/accuracy=0.614766, train/loss=1.722976, validation/accuracy=0.560260, validation/loss=1.986790, validation/num_examples=50000
I0130 19:13:41.064803 139668754515712 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.3823879957199097, loss=3.5556955337524414
I0130 19:14:22.750490 139668746123008 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.1529124975204468, loss=4.78176212310791
I0130 19:15:08.606955 139668754515712 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.3358638286590576, loss=3.377864360809326
I0130 19:15:54.379927 139668746123008 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.0635437965393066, loss=5.45871114730835
I0130 19:16:40.421307 139668754515712 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.9871537685394287, loss=5.613872051239014
I0130 19:17:25.921643 139668746123008 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.0638647079467773, loss=5.5248517990112305
I0130 19:18:11.466240 139668754515712 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.1384403705596924, loss=4.528485298156738
I0130 19:18:56.879068 139668746123008 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.0148199796676636, loss=5.48133659362793
I0130 19:19:42.170364 139668754515712 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.2124335765838623, loss=4.225886821746826
I0130 19:20:28.045374 139668746123008 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.3816546201705933, loss=3.5338640213012695
I0130 19:20:35.998904 139863983413056 spec.py:321] Evaluating on the training split.
I0130 19:20:47.107260 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 19:21:06.386203 139863983413056 spec.py:349] Evaluating on the test split.
I0130 19:21:08.033338 139863983413056 submission_runner.py:408] Time since start: 20379.68s, 	Step: 42119, 	{'train/accuracy': 0.5964453220367432, 'train/loss': 1.8440462350845337, 'validation/accuracy': 0.558899998664856, 'validation/loss': 2.013521671295166, 'validation/num_examples': 50000, 'test/accuracy': 0.44630002975463867, 'test/loss': 2.636042594909668, 'test/num_examples': 10000, 'score': 18948.740561246872, 'total_duration': 20379.68393588066, 'accumulated_submission_time': 18948.740561246872, 'accumulated_eval_time': 1426.9955496788025, 'accumulated_logging_time': 1.7593953609466553}
I0130 19:21:08.055299 139668754515712 logging_writer.py:48] [42119] accumulated_eval_time=1426.995550, accumulated_logging_time=1.759395, accumulated_submission_time=18948.740561, global_step=42119, preemption_count=0, score=18948.740561, test/accuracy=0.446300, test/loss=2.636043, test/num_examples=10000, total_duration=20379.683936, train/accuracy=0.596445, train/loss=1.844046, validation/accuracy=0.558900, validation/loss=2.013522, validation/num_examples=50000
I0130 19:21:40.808587 139668746123008 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.5113582611083984, loss=3.4003028869628906
I0130 19:22:25.639357 139668754515712 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.8624356985092163, loss=3.373601198196411
I0130 19:23:11.405044 139668746123008 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.4242359399795532, loss=3.5621395111083984
I0130 19:23:57.034055 139668754515712 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.3710360527038574, loss=3.428431510925293
I0130 19:24:42.715041 139668746123008 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.3015555143356323, loss=3.8700509071350098
I0130 19:25:28.467827 139668754515712 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.2457107305526733, loss=4.2985453605651855
I0130 19:26:14.140406 139668746123008 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.4660532474517822, loss=3.539734125137329
I0130 19:26:59.737847 139668754515712 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.3031280040740967, loss=5.614237308502197
I0130 19:27:45.280175 139668746123008 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.5027284622192383, loss=3.3938674926757812
I0130 19:28:08.234779 139863983413056 spec.py:321] Evaluating on the training split.
I0130 19:28:18.770110 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 19:28:38.168360 139863983413056 spec.py:349] Evaluating on the test split.
I0130 19:28:39.819443 139863983413056 submission_runner.py:408] Time since start: 20831.47s, 	Step: 43052, 	{'train/accuracy': 0.6012109518051147, 'train/loss': 1.8032618761062622, 'validation/accuracy': 0.5563399791717529, 'validation/loss': 1.9997272491455078, 'validation/num_examples': 50000, 'test/accuracy': 0.44930002093315125, 'test/loss': 2.613379716873169, 'test/num_examples': 10000, 'score': 19368.863209962845, 'total_duration': 20831.470037698746, 'accumulated_submission_time': 19368.863209962845, 'accumulated_eval_time': 1458.5802025794983, 'accumulated_logging_time': 1.7902934551239014}
I0130 19:28:39.849757 139668754515712 logging_writer.py:48] [43052] accumulated_eval_time=1458.580203, accumulated_logging_time=1.790293, accumulated_submission_time=19368.863210, global_step=43052, preemption_count=0, score=19368.863210, test/accuracy=0.449300, test/loss=2.613380, test/num_examples=10000, total_duration=20831.470038, train/accuracy=0.601211, train/loss=1.803262, validation/accuracy=0.556340, validation/loss=1.999727, validation/num_examples=50000
I0130 19:28:59.430954 139668746123008 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.712032437324524, loss=3.3581278324127197
I0130 19:29:42.812896 139668754515712 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.1623121500015259, loss=4.707111835479736
I0130 19:30:29.104684 139668746123008 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.214981198310852, loss=5.543623924255371
I0130 19:31:14.934355 139668754515712 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.4362798929214478, loss=3.4339537620544434
I0130 19:32:00.829732 139668746123008 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.184244155883789, loss=5.450888156890869
I0130 19:32:46.474814 139668754515712 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.4393031597137451, loss=3.4080722332000732
I0130 19:33:32.090375 139668746123008 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.6350367069244385, loss=3.4603285789489746
I0130 19:34:18.017636 139668754515712 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.9821558594703674, loss=5.253339767456055
I0130 19:35:03.609362 139668746123008 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.3308792114257812, loss=3.4604570865631104
I0130 19:35:39.887981 139863983413056 spec.py:321] Evaluating on the training split.
I0130 19:35:50.637021 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 19:36:11.719517 139863983413056 spec.py:349] Evaluating on the test split.
I0130 19:36:13.370090 139863983413056 submission_runner.py:408] Time since start: 21285.02s, 	Step: 43982, 	{'train/accuracy': 0.6162695288658142, 'train/loss': 1.7325092554092407, 'validation/accuracy': 0.5649799704551697, 'validation/loss': 1.9556329250335693, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.5773203372955322, 'test/num_examples': 10000, 'score': 19788.844311714172, 'total_duration': 21285.02067923546, 'accumulated_submission_time': 19788.844311714172, 'accumulated_eval_time': 1492.062311410904, 'accumulated_logging_time': 1.8297011852264404}
I0130 19:36:13.393018 139668754515712 logging_writer.py:48] [43982] accumulated_eval_time=1492.062311, accumulated_logging_time=1.829701, accumulated_submission_time=19788.844312, global_step=43982, preemption_count=0, score=19788.844312, test/accuracy=0.453300, test/loss=2.577320, test/num_examples=10000, total_duration=21285.020679, train/accuracy=0.616270, train/loss=1.732509, validation/accuracy=0.564980, validation/loss=1.955633, validation/num_examples=50000
I0130 19:36:20.977814 139668746123008 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.330437421798706, loss=3.4551753997802734
I0130 19:37:02.347708 139668754515712 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.38245689868927, loss=3.6892735958099365
I0130 19:37:47.815253 139668746123008 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.3315807580947876, loss=3.7349555492401123
I0130 19:38:33.596331 139668754515712 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.5116298198699951, loss=3.8838741779327393
I0130 19:39:19.471441 139668746123008 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.5149050951004028, loss=3.5164854526519775
I0130 19:40:05.090255 139668754515712 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.4637948274612427, loss=3.4618680477142334
I0130 19:40:50.472069 139668746123008 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.2925481796264648, loss=3.425762414932251
I0130 19:41:36.546932 139668754515712 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.1010494232177734, loss=5.486013889312744
I0130 19:42:21.988709 139668746123008 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.181552767753601, loss=5.5361480712890625
I0130 19:43:07.719049 139668754515712 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.1644279956817627, loss=4.8513689041137695
I0130 19:43:13.378230 139863983413056 spec.py:321] Evaluating on the training split.
I0130 19:43:24.017695 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 19:43:44.460458 139863983413056 spec.py:349] Evaluating on the test split.
I0130 19:43:46.105067 139863983413056 submission_runner.py:408] Time since start: 21737.76s, 	Step: 44914, 	{'train/accuracy': 0.6068750023841858, 'train/loss': 1.7359304428100586, 'validation/accuracy': 0.5651000142097473, 'validation/loss': 1.9321789741516113, 'validation/num_examples': 50000, 'test/accuracy': 0.45260003209114075, 'test/loss': 2.549083709716797, 'test/num_examples': 10000, 'score': 20208.771056890488, 'total_duration': 21737.755645275116, 'accumulated_submission_time': 20208.771056890488, 'accumulated_eval_time': 1524.7891371250153, 'accumulated_logging_time': 1.8625941276550293}
I0130 19:43:46.132220 139668746123008 logging_writer.py:48] [44914] accumulated_eval_time=1524.789137, accumulated_logging_time=1.862594, accumulated_submission_time=20208.771057, global_step=44914, preemption_count=0, score=20208.771057, test/accuracy=0.452600, test/loss=2.549084, test/num_examples=10000, total_duration=21737.755645, train/accuracy=0.606875, train/loss=1.735930, validation/accuracy=0.565100, validation/loss=1.932179, validation/num_examples=50000
I0130 19:44:21.084077 139668754515712 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.103652000427246, loss=5.0320305824279785
I0130 19:45:06.697460 139668746123008 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.3503633737564087, loss=3.722867012023926
I0130 19:45:52.143456 139668754515712 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.5358176231384277, loss=3.329728603363037
I0130 19:46:38.125772 139668746123008 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.119045615196228, loss=4.820452690124512
I0130 19:47:23.460434 139668754515712 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.4405300617218018, loss=3.478304386138916
I0130 19:48:09.040680 139668746123008 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.1287028789520264, loss=4.933419227600098
I0130 19:48:54.495740 139668754515712 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.4600512981414795, loss=3.8888072967529297
I0130 19:49:39.865882 139668746123008 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.5691310167312622, loss=3.3817880153656006
I0130 19:50:25.571947 139668754515712 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.4141794443130493, loss=3.4172439575195312
I0130 19:50:46.196089 139863983413056 spec.py:321] Evaluating on the training split.
I0130 19:50:57.161222 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 19:51:16.546168 139863983413056 spec.py:349] Evaluating on the test split.
I0130 19:51:18.193438 139863983413056 submission_runner.py:408] Time since start: 22189.84s, 	Step: 45847, 	{'train/accuracy': 0.6036913990974426, 'train/loss': 1.7868403196334839, 'validation/accuracy': 0.5648800134658813, 'validation/loss': 1.972473382949829, 'validation/num_examples': 50000, 'test/accuracy': 0.4513000249862671, 'test/loss': 2.5971016883850098, 'test/num_examples': 10000, 'score': 20628.775985956192, 'total_duration': 22189.844033002853, 'accumulated_submission_time': 20628.775985956192, 'accumulated_eval_time': 1556.7864758968353, 'accumulated_logging_time': 1.8994367122650146}
I0130 19:51:18.214752 139668746123008 logging_writer.py:48] [45847] accumulated_eval_time=1556.786476, accumulated_logging_time=1.899437, accumulated_submission_time=20628.775986, global_step=45847, preemption_count=0, score=20628.775986, test/accuracy=0.451300, test/loss=2.597102, test/num_examples=10000, total_duration=22189.844033, train/accuracy=0.603691, train/loss=1.786840, validation/accuracy=0.564880, validation/loss=1.972473, validation/num_examples=50000
I0130 19:51:39.794575 139668754515712 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.1794644594192505, loss=3.98221492767334
I0130 19:52:23.414539 139668746123008 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.676413893699646, loss=3.419908285140991
I0130 19:53:08.971753 139668754515712 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.5339692831039429, loss=3.3594813346862793
I0130 19:53:54.742540 139668746123008 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.2652091979980469, loss=4.773887634277344
I0130 19:54:40.541483 139668754515712 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.2592766284942627, loss=3.755246877670288
I0130 19:55:26.091530 139668746123008 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.3543418645858765, loss=3.8895883560180664
I0130 19:56:11.802770 139668754515712 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.183121681213379, loss=5.666738510131836
I0130 19:56:57.238224 139668746123008 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1883331537246704, loss=5.425638675689697
I0130 19:57:42.542199 139668754515712 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.9920603632926941, loss=5.265227317810059
I0130 19:58:18.349086 139863983413056 spec.py:321] Evaluating on the training split.
I0130 19:58:29.202548 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 19:58:49.879022 139863983413056 spec.py:349] Evaluating on the test split.
I0130 19:58:51.514998 139863983413056 submission_runner.py:408] Time since start: 22643.17s, 	Step: 46780, 	{'train/accuracy': 0.6071484088897705, 'train/loss': 1.7338517904281616, 'validation/accuracy': 0.564520001411438, 'validation/loss': 1.9355089664459229, 'validation/num_examples': 50000, 'test/accuracy': 0.4513000249862671, 'test/loss': 2.5617563724517822, 'test/num_examples': 10000, 'score': 21048.85280585289, 'total_duration': 22643.165594816208, 'accumulated_submission_time': 21048.85280585289, 'accumulated_eval_time': 1589.9524147510529, 'accumulated_logging_time': 1.9305696487426758}
I0130 19:58:51.537160 139668746123008 logging_writer.py:48] [46780] accumulated_eval_time=1589.952415, accumulated_logging_time=1.930570, accumulated_submission_time=21048.852806, global_step=46780, preemption_count=0, score=21048.852806, test/accuracy=0.451300, test/loss=2.561756, test/num_examples=10000, total_duration=22643.165595, train/accuracy=0.607148, train/loss=1.733852, validation/accuracy=0.564520, validation/loss=1.935509, validation/num_examples=50000
I0130 19:58:59.934134 139668754515712 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.4955320358276367, loss=3.399869680404663
I0130 19:59:41.494886 139668746123008 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.4493082761764526, loss=3.436452865600586
I0130 20:00:27.070719 139668754515712 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.4953851699829102, loss=3.392070770263672
I0130 20:01:12.444743 139668746123008 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.4485275745391846, loss=3.4369866847991943
I0130 20:01:57.994661 139668754515712 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.4989452362060547, loss=3.323798894882202
I0130 20:02:43.677134 139668746123008 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.3447555303573608, loss=3.542850971221924
I0130 20:03:28.968312 139668754515712 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.4754024744033813, loss=3.2168643474578857
I0130 20:04:14.651020 139668746123008 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.5365175008773804, loss=3.292069673538208
I0130 20:05:00.164620 139668754515712 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.5327019691467285, loss=3.272306203842163
I0130 20:05:45.528435 139668746123008 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.4977660179138184, loss=3.344194173812866
I0130 20:05:51.550344 139863983413056 spec.py:321] Evaluating on the training split.
I0130 20:06:02.416070 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 20:06:23.790132 139863983413056 spec.py:349] Evaluating on the test split.
I0130 20:06:25.436633 139863983413056 submission_runner.py:408] Time since start: 23097.09s, 	Step: 47715, 	{'train/accuracy': 0.6330664157867432, 'train/loss': 1.6488145589828491, 'validation/accuracy': 0.5689399838447571, 'validation/loss': 1.946183681488037, 'validation/num_examples': 50000, 'test/accuracy': 0.45410001277923584, 'test/loss': 2.5774097442626953, 'test/num_examples': 10000, 'score': 21468.807317256927, 'total_duration': 23097.087222337723, 'accumulated_submission_time': 21468.807317256927, 'accumulated_eval_time': 1623.8386886119843, 'accumulated_logging_time': 1.9631412029266357}
I0130 20:06:25.461507 139668754515712 logging_writer.py:48] [47715] accumulated_eval_time=1623.838689, accumulated_logging_time=1.963141, accumulated_submission_time=21468.807317, global_step=47715, preemption_count=0, score=21468.807317, test/accuracy=0.454100, test/loss=2.577410, test/num_examples=10000, total_duration=23097.087222, train/accuracy=0.633066, train/loss=1.648815, validation/accuracy=0.568940, validation/loss=1.946184, validation/num_examples=50000
I0130 20:06:59.814462 139668746123008 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.1984390020370483, loss=5.595815658569336
I0130 20:07:44.937910 139668754515712 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.3190867900848389, loss=3.973191738128662
I0130 20:08:30.285567 139668746123008 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.4046741724014282, loss=5.446476459503174
I0130 20:09:15.859460 139668754515712 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.478746771812439, loss=3.2987735271453857
I0130 20:10:01.297413 139668746123008 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.31663978099823, loss=4.22357702255249
I0130 20:10:46.816068 139668754515712 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.5529502630233765, loss=3.311065912246704
I0130 20:11:32.288691 139668746123008 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.5371679067611694, loss=3.5310933589935303
I0130 20:12:17.847459 139668754515712 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.3742190599441528, loss=3.4922597408294678
I0130 20:13:03.485354 139668746123008 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.663527011871338, loss=3.5275964736938477
I0130 20:13:25.788620 139863983413056 spec.py:321] Evaluating on the training split.
I0130 20:13:36.683519 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 20:13:55.862658 139863983413056 spec.py:349] Evaluating on the test split.
I0130 20:13:57.505583 139863983413056 submission_runner.py:408] Time since start: 23549.16s, 	Step: 48651, 	{'train/accuracy': 0.6096093654632568, 'train/loss': 1.764771580696106, 'validation/accuracy': 0.5680999755859375, 'validation/loss': 1.94655179977417, 'validation/num_examples': 50000, 'test/accuracy': 0.4523000121116638, 'test/loss': 2.5711467266082764, 'test/num_examples': 10000, 'score': 21889.076536417007, 'total_duration': 23549.156180381775, 'accumulated_submission_time': 21889.076536417007, 'accumulated_eval_time': 1655.5556573867798, 'accumulated_logging_time': 1.9970552921295166}
I0130 20:13:57.527354 139668754515712 logging_writer.py:48] [48651] accumulated_eval_time=1655.555657, accumulated_logging_time=1.997055, accumulated_submission_time=21889.076536, global_step=48651, preemption_count=0, score=21889.076536, test/accuracy=0.452300, test/loss=2.571147, test/num_examples=10000, total_duration=23549.156180, train/accuracy=0.609609, train/loss=1.764772, validation/accuracy=0.568100, validation/loss=1.946552, validation/num_examples=50000
I0130 20:14:17.495166 139668746123008 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.442589521408081, loss=3.339836835861206
I0130 20:15:00.913284 139668754515712 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.3904293775558472, loss=3.746629238128662
I0130 20:15:46.476065 139668746123008 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.2120572328567505, loss=4.185483932495117
I0130 20:16:31.961195 139668754515712 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.247252345085144, loss=4.846534252166748
I0130 20:17:17.654120 139668746123008 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.3098737001419067, loss=5.378956317901611
I0130 20:18:03.313829 139668754515712 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.5358515977859497, loss=3.4339866638183594
I0130 20:18:48.836864 139668746123008 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.5633279085159302, loss=3.521745443344116
I0130 20:19:34.543043 139668754515712 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.4997625350952148, loss=3.3904635906219482
I0130 20:20:20.231415 139668746123008 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.4845558404922485, loss=3.3338215351104736
I0130 20:20:57.889715 139863983413056 spec.py:321] Evaluating on the training split.
I0130 20:21:08.681326 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 20:21:28.629305 139863983413056 spec.py:349] Evaluating on the test split.
I0130 20:21:30.275375 139863983413056 submission_runner.py:408] Time since start: 24001.93s, 	Step: 49585, 	{'train/accuracy': 0.6127734184265137, 'train/loss': 1.7522122859954834, 'validation/accuracy': 0.5688999891281128, 'validation/loss': 1.9498313665390015, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.5811941623687744, 'test/num_examples': 10000, 'score': 22309.38073515892, 'total_duration': 24001.9259724617, 'accumulated_submission_time': 22309.38073515892, 'accumulated_eval_time': 1687.9413216114044, 'accumulated_logging_time': 2.028259038925171}
I0130 20:21:30.297844 139668754515712 logging_writer.py:48] [49585] accumulated_eval_time=1687.941322, accumulated_logging_time=2.028259, accumulated_submission_time=22309.380735, global_step=49585, preemption_count=0, score=22309.380735, test/accuracy=0.453700, test/loss=2.581194, test/num_examples=10000, total_duration=24001.925972, train/accuracy=0.612773, train/loss=1.752212, validation/accuracy=0.568900, validation/loss=1.949831, validation/num_examples=50000
I0130 20:21:36.689771 139668746123008 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.7445515394210815, loss=3.4832706451416016
I0130 20:22:18.600269 139668754515712 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.1874680519104004, loss=5.444152355194092
I0130 20:23:04.267380 139668746123008 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.4021412134170532, loss=3.519047260284424
I0130 20:23:49.911492 139668754515712 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.3947322368621826, loss=5.006082057952881
I0130 20:24:35.789647 139668746123008 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.4508665800094604, loss=5.560525894165039
I0130 20:25:21.324241 139668754515712 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.583526611328125, loss=3.240814447402954
I0130 20:26:06.997014 139668746123008 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.2804539203643799, loss=4.426152229309082
I0130 20:26:52.531629 139668754515712 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.7427387237548828, loss=3.359579086303711
I0130 20:27:38.189711 139668746123008 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2922135591506958, loss=5.5728068351745605
I0130 20:28:23.935097 139668754515712 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.6609606742858887, loss=3.2902603149414062
I0130 20:28:30.418770 139863983413056 spec.py:321] Evaluating on the training split.
I0130 20:28:41.134656 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 20:29:01.773317 139863983413056 spec.py:349] Evaluating on the test split.
I0130 20:29:03.413936 139863983413056 submission_runner.py:408] Time since start: 24455.06s, 	Step: 50516, 	{'train/accuracy': 0.6240820288658142, 'train/loss': 1.6619887351989746, 'validation/accuracy': 0.5757799744606018, 'validation/loss': 1.8856275081634521, 'validation/num_examples': 50000, 'test/accuracy': 0.4613000154495239, 'test/loss': 2.5196902751922607, 'test/num_examples': 10000, 'score': 22729.444053173065, 'total_duration': 24455.064513206482, 'accumulated_submission_time': 22729.444053173065, 'accumulated_eval_time': 1720.9364750385284, 'accumulated_logging_time': 2.0603320598602295}
I0130 20:29:03.443069 139668746123008 logging_writer.py:48] [50516] accumulated_eval_time=1720.936475, accumulated_logging_time=2.060332, accumulated_submission_time=22729.444053, global_step=50516, preemption_count=0, score=22729.444053, test/accuracy=0.461300, test/loss=2.519690, test/num_examples=10000, total_duration=24455.064513, train/accuracy=0.624082, train/loss=1.661989, validation/accuracy=0.575780, validation/loss=1.885628, validation/num_examples=50000
I0130 20:29:37.583507 139668754515712 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.5622365474700928, loss=3.4197428226470947
I0130 20:30:22.730416 139668746123008 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.5382978916168213, loss=3.6509573459625244
I0130 20:31:08.362319 139668754515712 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.555132269859314, loss=3.352938175201416
I0130 20:31:53.783669 139668746123008 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.3820513486862183, loss=5.486690521240234
I0130 20:32:39.231297 139668754515712 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.5229507684707642, loss=3.6232078075408936
I0130 20:33:24.881551 139668746123008 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.4358735084533691, loss=3.5954179763793945
I0130 20:34:10.546724 139668754515712 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.4227505922317505, loss=3.288713216781616
I0130 20:34:56.236088 139668746123008 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.2572427988052368, loss=3.8551406860351562
I0130 20:35:42.073246 139668754515712 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.542060375213623, loss=3.3978519439697266
I0130 20:36:03.688110 139863983413056 spec.py:321] Evaluating on the training split.
I0130 20:36:14.391184 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 20:36:35.026180 139863983413056 spec.py:349] Evaluating on the test split.
I0130 20:36:36.666268 139863983413056 submission_runner.py:408] Time since start: 24908.32s, 	Step: 51449, 	{'train/accuracy': 0.6103906035423279, 'train/loss': 1.7757805585861206, 'validation/accuracy': 0.5678399801254272, 'validation/loss': 1.9511852264404297, 'validation/num_examples': 50000, 'test/accuracy': 0.45740002393722534, 'test/loss': 2.5609421730041504, 'test/num_examples': 10000, 'score': 23149.631596565247, 'total_duration': 24908.316866636276, 'accumulated_submission_time': 23149.631596565247, 'accumulated_eval_time': 1753.9146332740784, 'accumulated_logging_time': 2.098830223083496}
I0130 20:36:36.693268 139668746123008 logging_writer.py:48] [51449] accumulated_eval_time=1753.914633, accumulated_logging_time=2.098830, accumulated_submission_time=23149.631597, global_step=51449, preemption_count=0, score=23149.631597, test/accuracy=0.457400, test/loss=2.560942, test/num_examples=10000, total_duration=24908.316867, train/accuracy=0.610391, train/loss=1.775781, validation/accuracy=0.567840, validation/loss=1.951185, validation/num_examples=50000
I0130 20:36:57.473890 139668754515712 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.5691157579421997, loss=3.518974781036377
I0130 20:37:40.866279 139668746123008 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.5023680925369263, loss=3.4229021072387695
I0130 20:38:26.474729 139668754515712 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.3913030624389648, loss=3.744056224822998
I0130 20:39:12.176883 139668746123008 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.2135510444641113, loss=5.4934797286987305
I0130 20:39:57.880015 139668754515712 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.2854856252670288, loss=5.05279541015625
I0130 20:40:43.289539 139668746123008 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2499423027038574, loss=5.558559417724609
I0130 20:41:28.758799 139668754515712 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.57980215549469, loss=3.3642196655273438
I0130 20:42:14.185707 139668746123008 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.730484962463379, loss=3.306062698364258
I0130 20:43:00.441799 139668754515712 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.7093454599380493, loss=3.3271591663360596
I0130 20:43:36.670993 139863983413056 spec.py:321] Evaluating on the training split.
I0130 20:43:47.615307 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 20:44:08.474412 139863983413056 spec.py:349] Evaluating on the test split.
I0130 20:44:10.113587 139863983413056 submission_runner.py:408] Time since start: 25361.76s, 	Step: 52382, 	{'train/accuracy': 0.6201757788658142, 'train/loss': 1.6868958473205566, 'validation/accuracy': 0.5767799615859985, 'validation/loss': 1.8882721662521362, 'validation/num_examples': 50000, 'test/accuracy': 0.459600031375885, 'test/loss': 2.5248870849609375, 'test/num_examples': 10000, 'score': 23569.55105662346, 'total_duration': 25361.764173030853, 'accumulated_submission_time': 23569.55105662346, 'accumulated_eval_time': 1787.3572108745575, 'accumulated_logging_time': 2.134948968887329}
I0130 20:44:10.135540 139668746123008 logging_writer.py:48] [52382] accumulated_eval_time=1787.357211, accumulated_logging_time=2.134949, accumulated_submission_time=23569.551057, global_step=52382, preemption_count=0, score=23569.551057, test/accuracy=0.459600, test/loss=2.524887, test/num_examples=10000, total_duration=25361.764173, train/accuracy=0.620176, train/loss=1.686896, validation/accuracy=0.576780, validation/loss=1.888272, validation/num_examples=50000
I0130 20:44:17.721977 139668754515712 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.4053161144256592, loss=3.8115856647491455
I0130 20:44:59.276011 139668746123008 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.4845647811889648, loss=3.25443172454834
I0130 20:45:45.435441 139668754515712 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.3050668239593506, loss=4.341087341308594
I0130 20:46:31.189183 139668746123008 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.2310189008712769, loss=5.4868364334106445
I0130 20:47:17.164338 139668754515712 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.5975515842437744, loss=3.383578300476074
I0130 20:48:02.843488 139668746123008 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.3813450336456299, loss=3.171696186065674
I0130 20:48:48.508302 139668754515712 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.584978461265564, loss=3.4062530994415283
I0130 20:49:34.283355 139668746123008 logging_writer.py:48] [53100] global_step=53100, grad_norm=2.0646917819976807, loss=3.3815219402313232
I0130 20:50:20.040437 139668754515712 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.5317426919937134, loss=3.3985085487365723
I0130 20:51:05.813241 139668746123008 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.4925981760025024, loss=3.3990793228149414
I0130 20:51:10.565942 139863983413056 spec.py:321] Evaluating on the training split.
I0130 20:51:21.433351 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 20:51:42.001506 139863983413056 spec.py:349] Evaluating on the test split.
I0130 20:51:43.645173 139863983413056 submission_runner.py:408] Time since start: 25815.30s, 	Step: 53312, 	{'train/accuracy': 0.6251562237739563, 'train/loss': 1.6901180744171143, 'validation/accuracy': 0.5781999826431274, 'validation/loss': 1.9079535007476807, 'validation/num_examples': 50000, 'test/accuracy': 0.46330001950263977, 'test/loss': 2.536857843399048, 'test/num_examples': 10000, 'score': 23989.921848773956, 'total_duration': 25815.295751094818, 'accumulated_submission_time': 23989.921848773956, 'accumulated_eval_time': 1820.4364140033722, 'accumulated_logging_time': 2.1685073375701904}
I0130 20:51:43.674847 139668754515712 logging_writer.py:48] [53312] accumulated_eval_time=1820.436414, accumulated_logging_time=2.168507, accumulated_submission_time=23989.921849, global_step=53312, preemption_count=0, score=23989.921849, test/accuracy=0.463300, test/loss=2.536858, test/num_examples=10000, total_duration=25815.295751, train/accuracy=0.625156, train/loss=1.690118, validation/accuracy=0.578200, validation/loss=1.907954, validation/num_examples=50000
I0130 20:52:19.615473 139668746123008 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.6565160751342773, loss=3.3534085750579834
I0130 20:53:05.066651 139668754515712 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.316027045249939, loss=5.454610824584961
I0130 20:53:51.170158 139668746123008 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.2756563425064087, loss=5.276978015899658
I0130 20:54:37.044611 139668754515712 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.593456745147705, loss=3.3362886905670166
I0130 20:55:22.887702 139668746123008 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.4097394943237305, loss=3.8220925331115723
I0130 20:56:08.457717 139668754515712 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.3838039636611938, loss=4.0213494300842285
I0130 20:56:54.135201 139668746123008 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.4598276615142822, loss=3.34287691116333
I0130 20:57:39.603179 139668754515712 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.5928877592086792, loss=3.370910406112671
I0130 20:58:25.196525 139668746123008 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.5449113845825195, loss=3.276124954223633
I0130 20:58:43.992376 139863983413056 spec.py:321] Evaluating on the training split.
I0130 20:58:55.024261 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 20:59:14.748850 139863983413056 spec.py:349] Evaluating on the test split.
I0130 20:59:16.395544 139863983413056 submission_runner.py:408] Time since start: 26268.05s, 	Step: 54243, 	{'train/accuracy': 0.6433789134025574, 'train/loss': 1.6116437911987305, 'validation/accuracy': 0.5797600150108337, 'validation/loss': 1.8849976062774658, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.5215423107147217, 'test/num_examples': 10000, 'score': 24410.181674718857, 'total_duration': 26268.046141147614, 'accumulated_submission_time': 24410.181674718857, 'accumulated_eval_time': 1852.839579820633, 'accumulated_logging_time': 2.207714319229126}
I0130 20:59:16.419833 139668754515712 logging_writer.py:48] [54243] accumulated_eval_time=1852.839580, accumulated_logging_time=2.207714, accumulated_submission_time=24410.181675, global_step=54243, preemption_count=0, score=24410.181675, test/accuracy=0.462900, test/loss=2.521542, test/num_examples=10000, total_duration=26268.046141, train/accuracy=0.643379, train/loss=1.611644, validation/accuracy=0.579760, validation/loss=1.884998, validation/num_examples=50000
I0130 20:59:39.575668 139668746123008 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.1430953741073608, loss=5.29452657699585
I0130 21:00:23.908298 139668754515712 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.4048014879226685, loss=4.669979572296143
I0130 21:01:09.572699 139668746123008 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.032047986984253, loss=5.15910005569458
I0130 21:01:55.287920 139668754515712 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.483168601989746, loss=3.421945333480835
I0130 21:02:40.940485 139668746123008 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.2499709129333496, loss=4.852504730224609
I0130 21:03:26.706779 139668754515712 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.4873921871185303, loss=3.5876035690307617
I0130 21:04:12.309257 139668746123008 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.5301135778427124, loss=3.6232638359069824
I0130 21:04:57.881444 139668754515712 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.6117039918899536, loss=3.350093126296997
I0130 21:05:43.802713 139668746123008 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.5571813583374023, loss=3.3238306045532227
I0130 21:06:16.438143 139863983413056 spec.py:321] Evaluating on the training split.
I0130 21:06:27.390061 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 21:06:48.119352 139863983413056 spec.py:349] Evaluating on the test split.
I0130 21:06:49.756310 139863983413056 submission_runner.py:408] Time since start: 26721.41s, 	Step: 55173, 	{'train/accuracy': 0.6248632669448853, 'train/loss': 1.6888477802276611, 'validation/accuracy': 0.5834400057792664, 'validation/loss': 1.886579155921936, 'validation/num_examples': 50000, 'test/accuracy': 0.4659000337123871, 'test/loss': 2.519357681274414, 'test/num_examples': 10000, 'score': 24830.142689466476, 'total_duration': 26721.406907081604, 'accumulated_submission_time': 24830.142689466476, 'accumulated_eval_time': 1886.1577606201172, 'accumulated_logging_time': 2.2408063411712646}
I0130 21:06:49.779356 139668754515712 logging_writer.py:48] [55173] accumulated_eval_time=1886.157761, accumulated_logging_time=2.240806, accumulated_submission_time=24830.142689, global_step=55173, preemption_count=0, score=24830.142689, test/accuracy=0.465900, test/loss=2.519358, test/num_examples=10000, total_duration=26721.406907, train/accuracy=0.624863, train/loss=1.688848, validation/accuracy=0.583440, validation/loss=1.886579, validation/num_examples=50000
I0130 21:07:00.966066 139668746123008 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.3616975545883179, loss=5.478108882904053
I0130 21:07:42.996810 139668754515712 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.5652077198028564, loss=3.390563488006592
I0130 21:08:28.794013 139668746123008 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.3335819244384766, loss=3.9123477935791016
I0130 21:09:14.511390 139668754515712 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1968995332717896, loss=5.097231864929199
I0130 21:10:00.370922 139668746123008 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.647888422012329, loss=3.523254632949829
I0130 21:10:46.255838 139668754515712 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.61311936378479, loss=3.3824191093444824
I0130 21:11:31.816208 139668746123008 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.530547857284546, loss=3.8176016807556152
I0130 21:12:17.320156 139668754515712 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.6855493783950806, loss=3.354654312133789
I0130 21:13:02.895758 139668746123008 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.6715070009231567, loss=3.546785593032837
I0130 21:13:48.768256 139668754515712 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.6620980501174927, loss=3.507430076599121
I0130 21:13:50.323308 139863983413056 spec.py:321] Evaluating on the training split.
I0130 21:14:00.880586 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 21:14:20.681655 139863983413056 spec.py:349] Evaluating on the test split.
I0130 21:14:22.330643 139863983413056 submission_runner.py:408] Time since start: 27173.98s, 	Step: 56105, 	{'train/accuracy': 0.6288476586341858, 'train/loss': 1.6663347482681274, 'validation/accuracy': 0.5836799740791321, 'validation/loss': 1.8777925968170166, 'validation/num_examples': 50000, 'test/accuracy': 0.4644000232219696, 'test/loss': 2.5098447799682617, 'test/num_examples': 10000, 'score': 25250.628484487534, 'total_duration': 27173.98124217987, 'accumulated_submission_time': 25250.628484487534, 'accumulated_eval_time': 1918.1650898456573, 'accumulated_logging_time': 2.2733821868896484}
I0130 21:14:22.353882 139668746123008 logging_writer.py:48] [56105] accumulated_eval_time=1918.165090, accumulated_logging_time=2.273382, accumulated_submission_time=25250.628484, global_step=56105, preemption_count=0, score=25250.628484, test/accuracy=0.464400, test/loss=2.509845, test/num_examples=10000, total_duration=27173.981242, train/accuracy=0.628848, train/loss=1.666335, validation/accuracy=0.583680, validation/loss=1.877793, validation/num_examples=50000
I0130 21:15:01.243544 139668754515712 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.3850582838058472, loss=3.3119850158691406
I0130 21:15:47.159922 139668746123008 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.4542466402053833, loss=3.2007410526275635
I0130 21:16:32.670676 139668754515712 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.5023486614227295, loss=3.2415313720703125
I0130 21:17:18.439465 139668746123008 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.6810230016708374, loss=3.3487021923065186
I0130 21:18:03.904081 139668754515712 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.718503713607788, loss=3.3155157566070557
I0130 21:18:49.577865 139668746123008 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.233391523361206, loss=5.033486843109131
I0130 21:19:35.336584 139668754515712 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.5126138925552368, loss=3.3336241245269775
I0130 21:20:21.207674 139668746123008 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.6385096311569214, loss=3.312411308288574
I0130 21:21:06.755162 139668754515712 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.4490611553192139, loss=3.244079828262329
I0130 21:21:22.394787 139863983413056 spec.py:321] Evaluating on the training split.
I0130 21:21:33.177046 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 21:21:53.648548 139863983413056 spec.py:349] Evaluating on the test split.
I0130 21:21:55.286289 139863983413056 submission_runner.py:408] Time since start: 27626.94s, 	Step: 57036, 	{'train/accuracy': 0.644726574420929, 'train/loss': 1.6122007369995117, 'validation/accuracy': 0.5859599709510803, 'validation/loss': 1.8723894357681274, 'validation/num_examples': 50000, 'test/accuracy': 0.4666000306606293, 'test/loss': 2.5017263889312744, 'test/num_examples': 10000, 'score': 25670.61145210266, 'total_duration': 27626.93688774109, 'accumulated_submission_time': 25670.61145210266, 'accumulated_eval_time': 1951.056599855423, 'accumulated_logging_time': 2.3057284355163574}
I0130 21:21:55.309753 139668746123008 logging_writer.py:48] [57036] accumulated_eval_time=1951.056600, accumulated_logging_time=2.305728, accumulated_submission_time=25670.611452, global_step=57036, preemption_count=0, score=25670.611452, test/accuracy=0.466600, test/loss=2.501726, test/num_examples=10000, total_duration=27626.936888, train/accuracy=0.644727, train/loss=1.612201, validation/accuracy=0.585960, validation/loss=1.872389, validation/num_examples=50000
I0130 21:22:21.285457 139668754515712 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.4983749389648438, loss=3.302356004714966
I0130 21:23:05.398560 139668746123008 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.7702935934066772, loss=3.2405645847320557
I0130 21:23:50.976348 139668754515712 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.4506922960281372, loss=3.221621036529541
I0130 21:24:36.700849 139668746123008 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.4190138578414917, loss=3.6207618713378906
I0130 21:25:22.114099 139668754515712 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.2731637954711914, loss=4.60878324508667
I0130 21:26:07.806543 139668746123008 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.6037379503250122, loss=3.3514270782470703
I0130 21:26:53.390963 139668754515712 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.387990117073059, loss=3.587517023086548
I0130 21:27:38.935751 139668746123008 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.5696008205413818, loss=3.2983763217926025
I0130 21:28:24.548502 139668754515712 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.6433076858520508, loss=3.415452003479004
I0130 21:28:55.803751 139863983413056 spec.py:321] Evaluating on the training split.
I0130 21:29:06.493638 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 21:29:27.466310 139863983413056 spec.py:349] Evaluating on the test split.
I0130 21:29:29.113765 139863983413056 submission_runner.py:408] Time since start: 28080.76s, 	Step: 57970, 	{'train/accuracy': 0.6248242259025574, 'train/loss': 1.6605851650238037, 'validation/accuracy': 0.5842999815940857, 'validation/loss': 1.8347177505493164, 'validation/num_examples': 50000, 'test/accuracy': 0.46570003032684326, 'test/loss': 2.4734883308410645, 'test/num_examples': 10000, 'score': 26091.046587705612, 'total_duration': 28080.764357566833, 'accumulated_submission_time': 26091.046587705612, 'accumulated_eval_time': 1984.3666186332703, 'accumulated_logging_time': 2.3380661010742188}
I0130 21:29:29.140810 139668746123008 logging_writer.py:48] [57970] accumulated_eval_time=1984.366619, accumulated_logging_time=2.338066, accumulated_submission_time=26091.046588, global_step=57970, preemption_count=0, score=26091.046588, test/accuracy=0.465700, test/loss=2.473488, test/num_examples=10000, total_duration=28080.764358, train/accuracy=0.624824, train/loss=1.660585, validation/accuracy=0.584300, validation/loss=1.834718, validation/num_examples=50000
I0130 21:29:41.523079 139668754515712 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.4365273714065552, loss=5.420681953430176
I0130 21:30:24.003293 139668746123008 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.3484610319137573, loss=5.546390533447266
I0130 21:31:09.606032 139668754515712 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.54017972946167, loss=3.2712900638580322
I0130 21:31:55.467846 139668746123008 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.3013893365859985, loss=4.3039422035217285
I0130 21:32:41.151700 139668754515712 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.6327223777770996, loss=3.301076889038086
I0130 21:33:26.850985 139668746123008 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.6226491928100586, loss=3.4162869453430176
I0130 21:34:12.561274 139668754515712 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.5347732305526733, loss=3.429593324661255
I0130 21:34:58.350630 139668746123008 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.15107262134552, loss=5.479262828826904
I0130 21:35:44.228466 139668754515712 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.586434245109558, loss=3.266892910003662
I0130 21:36:29.459319 139863983413056 spec.py:321] Evaluating on the training split.
I0130 21:36:40.261560 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 21:37:01.344053 139863983413056 spec.py:349] Evaluating on the test split.
I0130 21:37:02.987485 139863983413056 submission_runner.py:408] Time since start: 28534.64s, 	Step: 58900, 	{'train/accuracy': 0.6384179592132568, 'train/loss': 1.634387731552124, 'validation/accuracy': 0.5914799571037292, 'validation/loss': 1.8405027389526367, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.4767680168151855, 'test/num_examples': 10000, 'score': 26511.307859420776, 'total_duration': 28534.63808321953, 'accumulated_submission_time': 26511.307859420776, 'accumulated_eval_time': 2017.8947920799255, 'accumulated_logging_time': 2.3739945888519287}
I0130 21:37:03.013392 139668746123008 logging_writer.py:48] [58900] accumulated_eval_time=2017.894792, accumulated_logging_time=2.373995, accumulated_submission_time=26511.307859, global_step=58900, preemption_count=0, score=26511.307859, test/accuracy=0.471600, test/loss=2.476768, test/num_examples=10000, total_duration=28534.638083, train/accuracy=0.638418, train/loss=1.634388, validation/accuracy=0.591480, validation/loss=1.840503, validation/num_examples=50000
I0130 21:37:03.418295 139668754515712 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.112356185913086, loss=4.799686431884766
I0130 21:37:44.248196 139668746123008 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.3227025270462036, loss=4.69906759262085
I0130 21:38:29.878527 139668754515712 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.294779658317566, loss=5.042097568511963
I0130 21:39:15.613476 139668746123008 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.5172955989837646, loss=3.4949426651000977
I0130 21:40:01.690530 139668754515712 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.4510732889175415, loss=3.308441162109375
I0130 21:40:47.371711 139668746123008 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.6615175008773804, loss=3.326354742050171
I0130 21:41:33.296733 139668754515712 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.4043824672698975, loss=4.970335960388184
I0130 21:42:19.119657 139668746123008 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.4735207557678223, loss=3.294778823852539
I0130 21:43:04.670370 139668754515712 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.4610260725021362, loss=3.642282485961914
I0130 21:43:50.519857 139668746123008 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.2371892929077148, loss=5.030742645263672
I0130 21:44:03.148323 139863983413056 spec.py:321] Evaluating on the training split.
I0130 21:44:13.968501 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 21:44:34.712837 139863983413056 spec.py:349] Evaluating on the test split.
I0130 21:44:36.349531 139863983413056 submission_runner.py:408] Time since start: 28988.00s, 	Step: 59829, 	{'train/accuracy': 0.634570300579071, 'train/loss': 1.6421008110046387, 'validation/accuracy': 0.58406001329422, 'validation/loss': 1.8689597845077515, 'validation/num_examples': 50000, 'test/accuracy': 0.46700000762939453, 'test/loss': 2.510704517364502, 'test/num_examples': 10000, 'score': 26931.3852558136, 'total_duration': 28988.000118494034, 'accumulated_submission_time': 26931.3852558136, 'accumulated_eval_time': 2051.0959992408752, 'accumulated_logging_time': 2.4093093872070312}
I0130 21:44:36.373190 139668754515712 logging_writer.py:48] [59829] accumulated_eval_time=2051.095999, accumulated_logging_time=2.409309, accumulated_submission_time=26931.385256, global_step=59829, preemption_count=0, score=26931.385256, test/accuracy=0.467000, test/loss=2.510705, test/num_examples=10000, total_duration=28988.000118, train/accuracy=0.634570, train/loss=1.642101, validation/accuracy=0.584060, validation/loss=1.868960, validation/num_examples=50000
I0130 21:45:05.153836 139668746123008 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.5044889450073242, loss=3.3101131916046143
I0130 21:45:50.140631 139668754515712 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.4496084451675415, loss=3.638662576675415
I0130 21:46:36.481160 139668746123008 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.1746717691421509, loss=4.083628177642822
I0130 21:47:22.589121 139668754515712 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.4413377046585083, loss=3.8043460845947266
I0130 21:48:08.286678 139668746123008 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.6229408979415894, loss=3.22379994392395
I0130 21:48:53.901682 139668754515712 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.5181961059570312, loss=4.030170917510986
I0130 21:49:39.810403 139668746123008 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.5803571939468384, loss=3.3420424461364746
I0130 21:50:25.422293 139668754515712 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.4707938432693481, loss=5.121794700622559
I0130 21:51:11.088491 139668746123008 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.640395164489746, loss=3.3382346630096436
I0130 21:51:36.710388 139863983413056 spec.py:321] Evaluating on the training split.
I0130 21:51:47.233725 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 21:52:05.725389 139863983413056 spec.py:349] Evaluating on the test split.
I0130 21:52:07.372290 139863983413056 submission_runner.py:408] Time since start: 29439.02s, 	Step: 60758, 	{'train/accuracy': 0.6297656297683716, 'train/loss': 1.6264452934265137, 'validation/accuracy': 0.5870400071144104, 'validation/loss': 1.8277873992919922, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.478414297103882, 'test/num_examples': 10000, 'score': 27351.664115190506, 'total_duration': 29439.022877693176, 'accumulated_submission_time': 27351.664115190506, 'accumulated_eval_time': 2081.7578916549683, 'accumulated_logging_time': 2.442704200744629}
I0130 21:52:07.399799 139668754515712 logging_writer.py:48] [60758] accumulated_eval_time=2081.757892, accumulated_logging_time=2.442704, accumulated_submission_time=27351.664115, global_step=60758, preemption_count=0, score=27351.664115, test/accuracy=0.471600, test/loss=2.478414, test/num_examples=10000, total_duration=29439.022878, train/accuracy=0.629766, train/loss=1.626445, validation/accuracy=0.587040, validation/loss=1.827787, validation/num_examples=50000
I0130 21:52:24.586766 139668746123008 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.2105286121368408, loss=4.549324035644531
I0130 21:53:08.018106 139668754515712 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.6213351488113403, loss=3.6136133670806885
I0130 21:53:53.643088 139668746123008 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.5450013875961304, loss=3.294992446899414
I0130 21:54:39.323490 139668754515712 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.6457571983337402, loss=3.3736484050750732
I0130 21:55:25.023608 139668746123008 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.6005667448043823, loss=3.245283365249634
I0130 21:56:11.026558 139668754515712 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.6584898233413696, loss=3.706113576889038
I0130 21:56:56.744921 139668746123008 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.4449580907821655, loss=3.3995912075042725
I0130 21:57:42.412871 139668754515712 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.1472764015197754, loss=4.732867240905762
I0130 21:58:28.264026 139668746123008 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.6069116592407227, loss=3.3078267574310303
I0130 21:59:07.594454 139863983413056 spec.py:321] Evaluating on the training split.
I0130 21:59:18.686319 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 21:59:38.158173 139863983413056 spec.py:349] Evaluating on the test split.
I0130 21:59:39.807190 139863983413056 submission_runner.py:408] Time since start: 29891.46s, 	Step: 61688, 	{'train/accuracy': 0.6357226371765137, 'train/loss': 1.6139147281646729, 'validation/accuracy': 0.5892199873924255, 'validation/loss': 1.826127290725708, 'validation/num_examples': 50000, 'test/accuracy': 0.4758000373840332, 'test/loss': 2.4539406299591064, 'test/num_examples': 10000, 'score': 27771.800651311874, 'total_duration': 29891.457787036896, 'accumulated_submission_time': 27771.800651311874, 'accumulated_eval_time': 2113.970644235611, 'accumulated_logging_time': 2.47971773147583}
I0130 21:59:39.831281 139668754515712 logging_writer.py:48] [61688] accumulated_eval_time=2113.970644, accumulated_logging_time=2.479718, accumulated_submission_time=27771.800651, global_step=61688, preemption_count=0, score=27771.800651, test/accuracy=0.475800, test/loss=2.453941, test/num_examples=10000, total_duration=29891.457787, train/accuracy=0.635723, train/loss=1.613915, validation/accuracy=0.589220, validation/loss=1.826127, validation/num_examples=50000
I0130 21:59:45.021308 139668746123008 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.6142535209655762, loss=3.304205894470215
I0130 22:00:26.674169 139668754515712 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.63265061378479, loss=3.4239792823791504
I0130 22:01:12.212276 139668746123008 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.3495430946350098, loss=4.105611324310303
I0130 22:01:57.924448 139668754515712 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.5625656843185425, loss=3.283891201019287
I0130 22:02:43.754373 139668746123008 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.4163269996643066, loss=3.8220674991607666
I0130 22:03:29.449315 139668754515712 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.5175440311431885, loss=3.3031351566314697
I0130 22:04:15.060360 139668746123008 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.8436263799667358, loss=3.261652946472168
I0130 22:05:00.800980 139668754515712 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.6917706727981567, loss=3.3539979457855225
I0130 22:05:46.607115 139668746123008 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.2521775960922241, loss=5.288370132446289
I0130 22:06:32.694185 139668754515712 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.3972828388214111, loss=3.536681652069092
I0130 22:06:40.179427 139863983413056 spec.py:321] Evaluating on the training split.
I0130 22:06:50.800114 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 22:07:12.015131 139863983413056 spec.py:349] Evaluating on the test split.
I0130 22:07:13.664146 139863983413056 submission_runner.py:408] Time since start: 30345.31s, 	Step: 62618, 	{'train/accuracy': 0.6410741806030273, 'train/loss': 1.5769309997558594, 'validation/accuracy': 0.594760000705719, 'validation/loss': 1.7927261590957642, 'validation/num_examples': 50000, 'test/accuracy': 0.4781000316143036, 'test/loss': 2.4324634075164795, 'test/num_examples': 10000, 'score': 28192.091106176376, 'total_duration': 30345.314726114273, 'accumulated_submission_time': 28192.091106176376, 'accumulated_eval_time': 2147.4553532600403, 'accumulated_logging_time': 2.5132803916931152}
I0130 22:07:13.693784 139668746123008 logging_writer.py:48] [62618] accumulated_eval_time=2147.455353, accumulated_logging_time=2.513280, accumulated_submission_time=28192.091106, global_step=62618, preemption_count=0, score=28192.091106, test/accuracy=0.478100, test/loss=2.432463, test/num_examples=10000, total_duration=30345.314726, train/accuracy=0.641074, train/loss=1.576931, validation/accuracy=0.594760, validation/loss=1.792726, validation/num_examples=50000
I0130 22:07:46.848692 139668754515712 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.3280048370361328, loss=3.452719211578369
I0130 22:08:31.671017 139668746123008 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.3042963743209839, loss=4.43781852722168
I0130 22:09:17.450649 139668754515712 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.5541274547576904, loss=3.3629136085510254
I0130 22:10:03.532814 139668746123008 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.2782800197601318, loss=4.229933738708496
I0130 22:10:49.069591 139668754515712 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.604483962059021, loss=3.0840091705322266
I0130 22:11:34.808702 139668746123008 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.3042941093444824, loss=5.525990962982178
I0130 22:12:20.572251 139668754515712 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.3978497982025146, loss=5.325417518615723
I0130 22:13:05.960108 139668746123008 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.4516247510910034, loss=3.4673190116882324
I0130 22:13:51.504429 139668754515712 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.6442564725875854, loss=3.401492118835449
I0130 22:14:14.068917 139863983413056 spec.py:321] Evaluating on the training split.
I0130 22:14:24.610825 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 22:14:45.977385 139863983413056 spec.py:349] Evaluating on the test split.
I0130 22:14:47.617178 139863983413056 submission_runner.py:408] Time since start: 30799.27s, 	Step: 63551, 	{'train/accuracy': 0.6690039038658142, 'train/loss': 1.4578577280044556, 'validation/accuracy': 0.5971800088882446, 'validation/loss': 1.771941065788269, 'validation/num_examples': 50000, 'test/accuracy': 0.4824000298976898, 'test/loss': 2.3906712532043457, 'test/num_examples': 10000, 'score': 28612.40795993805, 'total_duration': 30799.267755031586, 'accumulated_submission_time': 28612.40795993805, 'accumulated_eval_time': 2181.0035922527313, 'accumulated_logging_time': 2.5525503158569336}
I0130 22:14:47.644565 139668746123008 logging_writer.py:48] [63551] accumulated_eval_time=2181.003592, accumulated_logging_time=2.552550, accumulated_submission_time=28612.407960, global_step=63551, preemption_count=0, score=28612.407960, test/accuracy=0.482400, test/loss=2.390671, test/num_examples=10000, total_duration=30799.267755, train/accuracy=0.669004, train/loss=1.457858, validation/accuracy=0.597180, validation/loss=1.771941, validation/num_examples=50000
I0130 22:15:07.618182 139668754515712 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.5976825952529907, loss=3.6927616596221924
I0130 22:15:50.765948 139668746123008 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.5826574563980103, loss=3.4397902488708496
I0130 22:16:36.840066 139668754515712 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.5948762893676758, loss=3.228954553604126
I0130 22:17:22.797220 139668746123008 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.3314672708511353, loss=3.765231132507324
I0130 22:18:08.546022 139668754515712 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1883693933486938, loss=5.2371826171875
I0130 22:18:54.215910 139668746123008 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.317939043045044, loss=5.408944129943848
I0130 22:19:39.865761 139668754515712 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.3073605298995972, loss=4.252234935760498
I0130 22:20:25.668439 139668746123008 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.3676515817642212, loss=5.158435344696045
I0130 22:21:11.129591 139668754515712 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.8171817064285278, loss=3.3310749530792236
I0130 22:21:47.646430 139863983413056 spec.py:321] Evaluating on the training split.
I0130 22:21:58.626137 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 22:22:20.157673 139863983413056 spec.py:349] Evaluating on the test split.
I0130 22:22:21.805341 139863983413056 submission_runner.py:408] Time since start: 31253.46s, 	Step: 64482, 	{'train/accuracy': 0.6347460746765137, 'train/loss': 1.6374387741088867, 'validation/accuracy': 0.5944199562072754, 'validation/loss': 1.8259129524230957, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.455456018447876, 'test/num_examples': 10000, 'score': 29032.35076379776, 'total_duration': 31253.455913305283, 'accumulated_submission_time': 29032.35076379776, 'accumulated_eval_time': 2215.1625061035156, 'accumulated_logging_time': 2.5897204875946045}
I0130 22:22:21.833636 139668746123008 logging_writer.py:48] [64482] accumulated_eval_time=2215.162506, accumulated_logging_time=2.589720, accumulated_submission_time=29032.350764, global_step=64482, preemption_count=0, score=29032.350764, test/accuracy=0.476100, test/loss=2.455456, test/num_examples=10000, total_duration=31253.455913, train/accuracy=0.634746, train/loss=1.637439, validation/accuracy=0.594420, validation/loss=1.825913, validation/num_examples=50000
I0130 22:22:29.429600 139668754515712 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.591913104057312, loss=3.205286979675293
I0130 22:23:11.044498 139668746123008 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.5820024013519287, loss=3.3634047508239746
I0130 22:23:56.497522 139668754515712 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.7824969291687012, loss=3.3544280529022217
I0130 22:24:42.295253 139668746123008 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.5807029008865356, loss=3.2723793983459473
I0130 22:25:28.149752 139668754515712 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.5084148645401, loss=5.35678768157959
I0130 22:26:13.676267 139668746123008 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.362215518951416, loss=5.413336753845215
I0130 22:26:59.646024 139668754515712 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.4742432832717896, loss=5.358309745788574
I0130 22:27:45.587194 139668746123008 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.522854208946228, loss=5.482979774475098
I0130 22:28:31.158994 139668754515712 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.7951576709747314, loss=3.3748505115509033
I0130 22:29:16.564332 139668746123008 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.7109348773956299, loss=3.221858501434326
I0130 22:29:22.210299 139863983413056 spec.py:321] Evaluating on the training split.
I0130 22:29:33.016368 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 22:29:54.198646 139863983413056 spec.py:349] Evaluating on the test split.
I0130 22:29:55.839338 139863983413056 submission_runner.py:408] Time since start: 31707.49s, 	Step: 65414, 	{'train/accuracy': 0.6394921541213989, 'train/loss': 1.6012791395187378, 'validation/accuracy': 0.5922200083732605, 'validation/loss': 1.8196548223495483, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.461789846420288, 'test/num_examples': 10000, 'score': 29452.668427705765, 'total_duration': 31707.489936828613, 'accumulated_submission_time': 29452.668427705765, 'accumulated_eval_time': 2248.791541337967, 'accumulated_logging_time': 2.6287543773651123}
I0130 22:29:55.866607 139668754515712 logging_writer.py:48] [65414] accumulated_eval_time=2248.791541, accumulated_logging_time=2.628754, accumulated_submission_time=29452.668428, global_step=65414, preemption_count=0, score=29452.668428, test/accuracy=0.476200, test/loss=2.461790, test/num_examples=10000, total_duration=31707.489937, train/accuracy=0.639492, train/loss=1.601279, validation/accuracy=0.592220, validation/loss=1.819655, validation/num_examples=50000
I0130 22:30:30.754972 139668746123008 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.6231821775436401, loss=3.1904330253601074
I0130 22:31:16.205250 139668754515712 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.2689961194992065, loss=5.386926651000977
I0130 22:32:02.108537 139668746123008 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.645395040512085, loss=3.2043819427490234
I0130 22:32:48.186088 139668754515712 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.32429838180542, loss=3.9823925495147705
I0130 22:33:33.928621 139668746123008 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.2433321475982666, loss=5.398849010467529
I0130 22:34:19.474673 139668754515712 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.5493175983428955, loss=3.1742935180664062
I0130 22:35:05.286346 139668746123008 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.745343804359436, loss=3.2919366359710693
I0130 22:35:50.838392 139668754515712 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.445946455001831, loss=3.543358087539673
I0130 22:36:36.847983 139668746123008 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.446225881576538, loss=3.5704801082611084
I0130 22:36:56.089813 139863983413056 spec.py:321] Evaluating on the training split.
I0130 22:37:06.832825 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 22:37:28.163445 139863983413056 spec.py:349] Evaluating on the test split.
I0130 22:37:29.811678 139863983413056 submission_runner.py:408] Time since start: 32161.46s, 	Step: 66344, 	{'train/accuracy': 0.659472644329071, 'train/loss': 1.5096582174301147, 'validation/accuracy': 0.6010400056838989, 'validation/loss': 1.7638288736343384, 'validation/num_examples': 50000, 'test/accuracy': 0.48270002007484436, 'test/loss': 2.3863396644592285, 'test/num_examples': 10000, 'score': 29872.83366370201, 'total_duration': 32161.46227788925, 'accumulated_submission_time': 29872.83366370201, 'accumulated_eval_time': 2282.513402938843, 'accumulated_logging_time': 2.6658544540405273}
I0130 22:37:29.837890 139668754515712 logging_writer.py:48] [66344] accumulated_eval_time=2282.513403, accumulated_logging_time=2.665854, accumulated_submission_time=29872.833664, global_step=66344, preemption_count=0, score=29872.833664, test/accuracy=0.482700, test/loss=2.386340, test/num_examples=10000, total_duration=32161.462278, train/accuracy=0.659473, train/loss=1.509658, validation/accuracy=0.601040, validation/loss=1.763829, validation/num_examples=50000
I0130 22:37:52.585849 139668746123008 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.5569477081298828, loss=3.8874402046203613
I0130 22:38:36.225715 139668754515712 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.368937611579895, loss=5.17656946182251
I0130 22:39:21.958430 139668746123008 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.8528704643249512, loss=3.2030723094940186
I0130 22:40:08.167581 139668754515712 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.570230484008789, loss=3.2959837913513184
I0130 22:40:54.037237 139668746123008 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.680133581161499, loss=3.194857597351074
I0130 22:41:39.477589 139668754515712 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.299145221710205, loss=5.065972328186035
I0130 22:42:25.427953 139668746123008 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.2941877841949463, loss=5.389396667480469
I0130 22:43:11.064574 139668754515712 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.3140473365783691, loss=4.43161678314209
I0130 22:43:56.394771 139668746123008 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.6734222173690796, loss=5.4783101081848145
I0130 22:44:30.055977 139863983413056 spec.py:321] Evaluating on the training split.
I0130 22:44:40.813024 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 22:45:01.968656 139863983413056 spec.py:349] Evaluating on the test split.
I0130 22:45:03.606558 139863983413056 submission_runner.py:408] Time since start: 32615.26s, 	Step: 67275, 	{'train/accuracy': 0.6406835913658142, 'train/loss': 1.6189045906066895, 'validation/accuracy': 0.6001200079917908, 'validation/loss': 1.808770775794983, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.4507222175598145, 'test/num_examples': 10000, 'score': 30292.992176771164, 'total_duration': 32615.257148742676, 'accumulated_submission_time': 30292.992176771164, 'accumulated_eval_time': 2316.063986301422, 'accumulated_logging_time': 2.7025163173675537}
I0130 22:45:03.630632 139668754515712 logging_writer.py:48] [67275] accumulated_eval_time=2316.063986, accumulated_logging_time=2.702516, accumulated_submission_time=30292.992177, global_step=67275, preemption_count=0, score=30292.992177, test/accuracy=0.478500, test/loss=2.450722, test/num_examples=10000, total_duration=32615.257149, train/accuracy=0.640684, train/loss=1.618905, validation/accuracy=0.600120, validation/loss=1.808771, validation/num_examples=50000
I0130 22:45:14.008913 139668746123008 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.6060833930969238, loss=3.2637033462524414
I0130 22:45:56.282944 139668754515712 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.4524039030075073, loss=3.612616777420044
I0130 22:46:42.056311 139668746123008 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.4119622707366943, loss=4.089334487915039
I0130 22:47:28.058035 139668754515712 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.2839415073394775, loss=4.148257255554199
I0130 22:48:13.926779 139668746123008 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.5011652708053589, loss=4.272886753082275
I0130 22:48:59.162467 139668754515712 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.523378610610962, loss=3.1648125648498535
I0130 22:49:45.166744 139668746123008 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.582411527633667, loss=3.3536975383758545
I0130 22:50:31.233576 139668754515712 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.3800095319747925, loss=3.938572645187378
I0130 22:51:16.930235 139668746123008 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.5496217012405396, loss=5.240187644958496
I0130 22:52:02.600174 139668754515712 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.5103719234466553, loss=3.165443181991577
I0130 22:52:03.668426 139863983413056 spec.py:321] Evaluating on the training split.
I0130 22:52:14.607352 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 22:52:34.145871 139863983413056 spec.py:349] Evaluating on the test split.
I0130 22:52:35.792319 139863983413056 submission_runner.py:408] Time since start: 33067.44s, 	Step: 68204, 	{'train/accuracy': 0.6403124928474426, 'train/loss': 1.6007063388824463, 'validation/accuracy': 0.5951399803161621, 'validation/loss': 1.8148831129074097, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.447077989578247, 'test/num_examples': 10000, 'score': 30712.97263765335, 'total_duration': 33067.442917346954, 'accumulated_submission_time': 30712.97263765335, 'accumulated_eval_time': 2348.187881231308, 'accumulated_logging_time': 2.735978603363037}
I0130 22:52:35.816721 139668746123008 logging_writer.py:48] [68204] accumulated_eval_time=2348.187881, accumulated_logging_time=2.735979, accumulated_submission_time=30712.972638, global_step=68204, preemption_count=0, score=30712.972638, test/accuracy=0.474500, test/loss=2.447078, test/num_examples=10000, total_duration=33067.442917, train/accuracy=0.640312, train/loss=1.600706, validation/accuracy=0.595140, validation/loss=1.814883, validation/num_examples=50000
I0130 22:53:15.646363 139668754515712 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.5910007953643799, loss=3.428981304168701
I0130 22:54:01.172194 139668746123008 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3642024993896484, loss=4.140329837799072
I0130 22:54:46.937214 139668754515712 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.211397409439087, loss=5.079407691955566
I0130 22:55:33.127053 139668746123008 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.7832807302474976, loss=3.271803379058838
I0130 22:56:18.836584 139668754515712 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.3875420093536377, loss=5.361458778381348
I0130 22:57:04.667743 139668746123008 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.7055851221084595, loss=3.1315104961395264
I0130 22:57:50.289054 139668754515712 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.593474268913269, loss=3.197925567626953
I0130 22:58:36.004542 139668746123008 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.6572346687316895, loss=3.168839693069458
I0130 22:59:22.708078 139668754515712 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.311435580253601, loss=3.8530657291412354
I0130 22:59:36.171997 139863983413056 spec.py:321] Evaluating on the training split.
I0130 22:59:47.400299 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 23:00:07.573513 139863983413056 spec.py:349] Evaluating on the test split.
I0130 23:00:09.207847 139863983413056 submission_runner.py:408] Time since start: 33520.86s, 	Step: 69131, 	{'train/accuracy': 0.6518359184265137, 'train/loss': 1.5784968137741089, 'validation/accuracy': 0.599399983882904, 'validation/loss': 1.8185914754867554, 'validation/num_examples': 50000, 'test/accuracy': 0.4790000319480896, 'test/loss': 2.451775312423706, 'test/num_examples': 10000, 'score': 31133.27097249031, 'total_duration': 33520.85844230652, 'accumulated_submission_time': 31133.27097249031, 'accumulated_eval_time': 2381.223728656769, 'accumulated_logging_time': 2.768976926803589}
I0130 23:00:09.233666 139668746123008 logging_writer.py:48] [69131] accumulated_eval_time=2381.223729, accumulated_logging_time=2.768977, accumulated_submission_time=31133.270972, global_step=69131, preemption_count=0, score=31133.270972, test/accuracy=0.479000, test/loss=2.451775, test/num_examples=10000, total_duration=33520.858442, train/accuracy=0.651836, train/loss=1.578497, validation/accuracy=0.599400, validation/loss=1.818591, validation/num_examples=50000
I0130 23:00:37.160129 139668754515712 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.6760120391845703, loss=3.2181499004364014
I0130 23:01:21.909791 139668746123008 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.283022165298462, loss=4.919506549835205
I0130 23:02:07.574049 139668754515712 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.3812167644500732, loss=5.423937797546387
I0130 23:02:53.169069 139668746123008 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.4620105028152466, loss=3.1344387531280518
I0130 23:03:38.936708 139668754515712 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.839186429977417, loss=3.1056275367736816
I0130 23:04:24.418718 139668746123008 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.7789076566696167, loss=3.0398178100585938
I0130 23:05:10.301707 139668754515712 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.2286471128463745, loss=4.8272857666015625
I0130 23:05:56.086075 139668746123008 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.1619811058044434, loss=4.969931125640869
I0130 23:06:41.691782 139668754515712 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.8499442338943481, loss=3.269784927368164
I0130 23:07:09.413684 139863983413056 spec.py:321] Evaluating on the training split.
I0130 23:07:20.079110 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 23:07:39.545568 139863983413056 spec.py:349] Evaluating on the test split.
I0130 23:07:41.193536 139863983413056 submission_runner.py:408] Time since start: 33972.84s, 	Step: 70062, 	{'train/accuracy': 0.6419140696525574, 'train/loss': 1.6183698177337646, 'validation/accuracy': 0.5989399552345276, 'validation/loss': 1.8223744630813599, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.46195387840271, 'test/num_examples': 10000, 'score': 31553.394397974014, 'total_duration': 33972.844121456146, 'accumulated_submission_time': 31553.394397974014, 'accumulated_eval_time': 2413.0035569667816, 'accumulated_logging_time': 2.803950309753418}
I0130 23:07:41.228297 139668746123008 logging_writer.py:48] [70062] accumulated_eval_time=2413.003557, accumulated_logging_time=2.803950, accumulated_submission_time=31553.394398, global_step=70062, preemption_count=0, score=31553.394398, test/accuracy=0.479400, test/loss=2.461954, test/num_examples=10000, total_duration=33972.844121, train/accuracy=0.641914, train/loss=1.618370, validation/accuracy=0.598940, validation/loss=1.822374, validation/num_examples=50000
I0130 23:07:56.810598 139668754515712 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.3830862045288086, loss=3.8668882846832275
I0130 23:08:39.874929 139668746123008 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.4180045127868652, loss=3.535343885421753
I0130 23:09:25.669320 139668754515712 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.6801806688308716, loss=3.1599011421203613
I0130 23:10:11.642788 139668746123008 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.464447021484375, loss=3.193768262863159
I0130 23:10:57.259960 139668754515712 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.5304937362670898, loss=3.1068358421325684
I0130 23:11:42.844355 139668746123008 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.5004061460494995, loss=3.175168752670288
I0130 23:12:28.733190 139668754515712 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.508579969406128, loss=3.6629509925842285
I0130 23:13:14.488437 139668746123008 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.4841798543930054, loss=4.180624961853027
I0130 23:14:00.373083 139668754515712 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.230486512184143, loss=4.597805500030518
I0130 23:14:41.598155 139863983413056 spec.py:321] Evaluating on the training split.
I0130 23:14:52.308640 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 23:15:12.951888 139863983413056 spec.py:349] Evaluating on the test split.
I0130 23:15:14.591342 139863983413056 submission_runner.py:408] Time since start: 34426.24s, 	Step: 70992, 	{'train/accuracy': 0.64990234375, 'train/loss': 1.5852410793304443, 'validation/accuracy': 0.6050199866294861, 'validation/loss': 1.7876743078231812, 'validation/num_examples': 50000, 'test/accuracy': 0.4846000373363495, 'test/loss': 2.424272298812866, 'test/num_examples': 10000, 'score': 31973.70577263832, 'total_duration': 34426.241938352585, 'accumulated_submission_time': 31973.70577263832, 'accumulated_eval_time': 2445.9967498779297, 'accumulated_logging_time': 2.8490071296691895}
I0130 23:15:14.617723 139668746123008 logging_writer.py:48] [70992] accumulated_eval_time=2445.996750, accumulated_logging_time=2.849007, accumulated_submission_time=31973.705773, global_step=70992, preemption_count=0, score=31973.705773, test/accuracy=0.484600, test/loss=2.424272, test/num_examples=10000, total_duration=34426.241938, train/accuracy=0.649902, train/loss=1.585241, validation/accuracy=0.605020, validation/loss=1.787674, validation/num_examples=50000
I0130 23:15:18.209623 139668754515712 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.385170578956604, loss=5.168827533721924
I0130 23:15:59.398245 139668746123008 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.4256396293640137, loss=4.491454124450684
I0130 23:16:45.331660 139668754515712 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.600843071937561, loss=3.2249808311462402
I0130 23:17:31.157792 139668746123008 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.5081888437271118, loss=3.5644006729125977
I0130 23:18:17.037841 139668754515712 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.4169166088104248, loss=4.859352111816406
I0130 23:19:02.273550 139668746123008 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.6341838836669922, loss=3.3146908283233643
I0130 23:19:47.924288 139668754515712 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.690873146057129, loss=3.209526538848877
I0130 23:20:33.670477 139668746123008 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.5263323783874512, loss=3.5930585861206055
I0130 23:21:19.298187 139668754515712 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.3882299661636353, loss=5.391045570373535
I0130 23:22:04.586934 139668746123008 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.7885148525238037, loss=3.124607563018799
I0130 23:22:14.757962 139863983413056 spec.py:321] Evaluating on the training split.
I0130 23:22:25.290738 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 23:22:46.510721 139863983413056 spec.py:349] Evaluating on the test split.
I0130 23:22:48.160066 139863983413056 submission_runner.py:408] Time since start: 34879.81s, 	Step: 71924, 	{'train/accuracy': 0.6512500047683716, 'train/loss': 1.5644874572753906, 'validation/accuracy': 0.6061800122261047, 'validation/loss': 1.7602328062057495, 'validation/num_examples': 50000, 'test/accuracy': 0.4853000342845917, 'test/loss': 2.3803160190582275, 'test/num_examples': 10000, 'score': 32393.788382530212, 'total_duration': 34879.81064558029, 'accumulated_submission_time': 32393.788382530212, 'accumulated_eval_time': 2479.398825407028, 'accumulated_logging_time': 2.8843657970428467}
I0130 23:22:48.192660 139668754515712 logging_writer.py:48] [71924] accumulated_eval_time=2479.398825, accumulated_logging_time=2.884366, accumulated_submission_time=32393.788383, global_step=71924, preemption_count=0, score=32393.788383, test/accuracy=0.485300, test/loss=2.380316, test/num_examples=10000, total_duration=34879.810646, train/accuracy=0.651250, train/loss=1.564487, validation/accuracy=0.606180, validation/loss=1.760233, validation/num_examples=50000
I0130 23:23:18.964805 139668746123008 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.4639395475387573, loss=5.091148376464844
I0130 23:24:03.574381 139668754515712 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.6438599824905396, loss=3.4575774669647217
I0130 23:24:49.146340 139668746123008 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.9091107845306396, loss=3.4282875061035156
I0130 23:25:34.714221 139668754515712 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.265147089958191, loss=5.1256103515625
I0130 23:26:20.351786 139668746123008 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.7653237581253052, loss=3.2374634742736816
I0130 23:27:05.976457 139668754515712 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.5397264957427979, loss=4.032970428466797
I0130 23:27:51.713098 139668746123008 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.4216035604476929, loss=5.235876560211182
I0130 23:28:37.209926 139668754515712 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.453755259513855, loss=5.011012077331543
I0130 23:29:22.788784 139668746123008 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.587443470954895, loss=5.145655632019043
I0130 23:29:48.478050 139863983413056 spec.py:321] Evaluating on the training split.
I0130 23:29:59.493764 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 23:30:20.902647 139863983413056 spec.py:349] Evaluating on the test split.
I0130 23:30:22.534220 139863983413056 submission_runner.py:408] Time since start: 35334.18s, 	Step: 72858, 	{'train/accuracy': 0.6725976467132568, 'train/loss': 1.4549552202224731, 'validation/accuracy': 0.6046000123023987, 'validation/loss': 1.746518850326538, 'validation/num_examples': 50000, 'test/accuracy': 0.4829000234603882, 'test/loss': 2.3781180381774902, 'test/num_examples': 10000, 'score': 32814.01478791237, 'total_duration': 35334.18482041359, 'accumulated_submission_time': 32814.01478791237, 'accumulated_eval_time': 2513.4549918174744, 'accumulated_logging_time': 2.9265990257263184}
I0130 23:30:22.561318 139668754515712 logging_writer.py:48] [72858] accumulated_eval_time=2513.454992, accumulated_logging_time=2.926599, accumulated_submission_time=32814.014788, global_step=72858, preemption_count=0, score=32814.014788, test/accuracy=0.482900, test/loss=2.378118, test/num_examples=10000, total_duration=35334.184820, train/accuracy=0.672598, train/loss=1.454955, validation/accuracy=0.604600, validation/loss=1.746519, validation/num_examples=50000
I0130 23:30:39.720374 139668746123008 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.7424577474594116, loss=3.259814500808716
I0130 23:31:22.643229 139668754515712 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.2469481229782104, loss=4.992680549621582
I0130 23:32:08.192625 139668746123008 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.6323264837265015, loss=3.178834915161133
I0130 23:32:53.807815 139668754515712 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.6361439228057861, loss=3.2502996921539307
I0130 23:33:39.843194 139668746123008 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.7010600566864014, loss=3.205369472503662
I0130 23:34:25.333953 139668754515712 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.2892038822174072, loss=4.656408309936523
I0130 23:35:10.870353 139668746123008 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.6862514019012451, loss=3.194808006286621
I0130 23:35:56.531165 139668754515712 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.492980718612671, loss=4.841771602630615
I0130 23:36:41.745646 139668746123008 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.3677318096160889, loss=4.1235575675964355
I0130 23:37:22.831631 139863983413056 spec.py:321] Evaluating on the training split.
I0130 23:37:33.492797 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 23:37:54.389658 139863983413056 spec.py:349] Evaluating on the test split.
I0130 23:37:56.037098 139863983413056 submission_runner.py:408] Time since start: 35787.69s, 	Step: 73792, 	{'train/accuracy': 0.6545507907867432, 'train/loss': 1.5467392206192017, 'validation/accuracy': 0.6106799840927124, 'validation/loss': 1.7467910051345825, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.389852523803711, 'test/num_examples': 10000, 'score': 33234.2273080349, 'total_duration': 35787.68767333031, 'accumulated_submission_time': 33234.2273080349, 'accumulated_eval_time': 2546.6604709625244, 'accumulated_logging_time': 2.96217679977417}
I0130 23:37:56.072814 139668754515712 logging_writer.py:48] [73792] accumulated_eval_time=2546.660471, accumulated_logging_time=2.962177, accumulated_submission_time=33234.227308, global_step=73792, preemption_count=0, score=33234.227308, test/accuracy=0.490300, test/loss=2.389853, test/num_examples=10000, total_duration=35787.687673, train/accuracy=0.654551, train/loss=1.546739, validation/accuracy=0.610680, validation/loss=1.746791, validation/num_examples=50000
I0130 23:37:59.667311 139668746123008 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.6199817657470703, loss=3.251164197921753
I0130 23:38:40.571778 139668754515712 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.8154606819152832, loss=3.1006312370300293
I0130 23:39:25.931946 139668746123008 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.516179084777832, loss=3.524963617324829
I0130 23:40:11.736864 139668754515712 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.7020982503890991, loss=3.151348352432251
I0130 23:40:57.431238 139668746123008 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.593897819519043, loss=3.1405513286590576
I0130 23:41:43.041842 139668754515712 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.5009264945983887, loss=3.84584903717041
I0130 23:42:28.565506 139668746123008 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.7347406148910522, loss=3.1874520778656006
I0130 23:43:14.274554 139668754515712 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.5194690227508545, loss=5.447556972503662
I0130 23:43:59.558597 139668746123008 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.6439436674118042, loss=3.4675400257110596
I0130 23:44:45.097713 139668754515712 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.2344841957092285, loss=5.415340900421143
I0130 23:44:56.170169 139863983413056 spec.py:321] Evaluating on the training split.
I0130 23:45:07.091418 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 23:45:27.414800 139863983413056 spec.py:349] Evaluating on the test split.
I0130 23:45:29.061010 139863983413056 submission_runner.py:408] Time since start: 36240.71s, 	Step: 74726, 	{'train/accuracy': 0.6555468440055847, 'train/loss': 1.561237096786499, 'validation/accuracy': 0.6126199960708618, 'validation/loss': 1.7528570890426636, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.3892462253570557, 'test/num_examples': 10000, 'score': 33654.26421165466, 'total_duration': 36240.71160840988, 'accumulated_submission_time': 33654.26421165466, 'accumulated_eval_time': 2579.5513093471527, 'accumulated_logging_time': 3.009035110473633}
I0130 23:45:29.091464 139668746123008 logging_writer.py:48] [74726] accumulated_eval_time=2579.551309, accumulated_logging_time=3.009035, accumulated_submission_time=33654.264212, global_step=74726, preemption_count=0, score=33654.264212, test/accuracy=0.491400, test/loss=2.389246, test/num_examples=10000, total_duration=36240.711608, train/accuracy=0.655547, train/loss=1.561237, validation/accuracy=0.612620, validation/loss=1.752857, validation/num_examples=50000
I0130 23:45:59.042146 139668754515712 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.7530323266983032, loss=3.248243570327759
I0130 23:46:43.955941 139668746123008 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.486614465713501, loss=4.427410125732422
I0130 23:47:29.680786 139668754515712 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.5554380416870117, loss=3.791910171508789
I0130 23:48:15.646894 139668746123008 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.6382616758346558, loss=3.1720023155212402
I0130 23:49:01.112871 139668754515712 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.8266723155975342, loss=3.199040651321411
I0130 23:49:46.821006 139668746123008 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.8411355018615723, loss=3.153506278991699
I0130 23:50:32.665163 139668754515712 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.286048412322998, loss=5.222193241119385
I0130 23:51:18.235882 139668746123008 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.757267951965332, loss=3.2270278930664062
I0130 23:52:03.932428 139668754515712 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.6201311349868774, loss=3.482529640197754
I0130 23:52:29.234902 139863983413056 spec.py:321] Evaluating on the training split.
I0130 23:52:39.853153 139863983413056 spec.py:333] Evaluating on the validation split.
I0130 23:53:00.555219 139863983413056 spec.py:349] Evaluating on the test split.
I0130 23:53:02.204533 139863983413056 submission_runner.py:408] Time since start: 36693.86s, 	Step: 75657, 	{'train/accuracy': 0.6675195097923279, 'train/loss': 1.4419922828674316, 'validation/accuracy': 0.6165199875831604, 'validation/loss': 1.693387746810913, 'validation/num_examples': 50000, 'test/accuracy': 0.49060001969337463, 'test/loss': 2.3438291549682617, 'test/num_examples': 10000, 'score': 34074.34876227379, 'total_duration': 36693.855113983154, 'accumulated_submission_time': 34074.34876227379, 'accumulated_eval_time': 2612.520934343338, 'accumulated_logging_time': 3.050046682357788}
I0130 23:53:02.233901 139668746123008 logging_writer.py:48] [75657] accumulated_eval_time=2612.520934, accumulated_logging_time=3.050047, accumulated_submission_time=34074.348762, global_step=75657, preemption_count=0, score=34074.348762, test/accuracy=0.490600, test/loss=2.343829, test/num_examples=10000, total_duration=36693.855114, train/accuracy=0.667520, train/loss=1.441992, validation/accuracy=0.616520, validation/loss=1.693388, validation/num_examples=50000
I0130 23:53:19.826581 139668754515712 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.7152388095855713, loss=3.1691808700561523
I0130 23:54:03.161688 139668746123008 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.5392255783081055, loss=3.577986240386963
I0130 23:54:48.853353 139668754515712 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.6803911924362183, loss=3.171787738800049
I0130 23:55:34.570617 139668746123008 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.8367581367492676, loss=3.144723653793335
I0130 23:56:20.422131 139668754515712 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.6023439168930054, loss=3.2524876594543457
I0130 23:57:05.917640 139668746123008 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.3110597133636475, loss=5.15652322769165
I0130 23:57:51.540788 139668754515712 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.647637128829956, loss=3.2548842430114746
I0130 23:58:37.336381 139668746123008 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.6233956813812256, loss=3.15841007232666
I0130 23:59:22.768784 139668754515712 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.554665446281433, loss=3.5132064819335938
I0131 00:00:02.508429 139863983413056 spec.py:321] Evaluating on the training split.
I0131 00:00:12.751375 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 00:00:31.670483 139863983413056 spec.py:349] Evaluating on the test split.
I0131 00:00:33.311809 139863983413056 submission_runner.py:408] Time since start: 37144.96s, 	Step: 76588, 	{'train/accuracy': 0.6521288752555847, 'train/loss': 1.591919183731079, 'validation/accuracy': 0.6082599759101868, 'validation/loss': 1.7924443483352661, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.4197306632995605, 'test/num_examples': 10000, 'score': 34494.56536388397, 'total_duration': 37144.96238017082, 'accumulated_submission_time': 34494.56536388397, 'accumulated_eval_time': 2643.3242888450623, 'accumulated_logging_time': 3.088836431503296}
I0131 00:00:33.343707 139668746123008 logging_writer.py:48] [76588] accumulated_eval_time=2643.324289, accumulated_logging_time=3.088836, accumulated_submission_time=34494.565364, global_step=76588, preemption_count=0, score=34494.565364, test/accuracy=0.486900, test/loss=2.419731, test/num_examples=10000, total_duration=37144.962380, train/accuracy=0.652129, train/loss=1.591919, validation/accuracy=0.608260, validation/loss=1.792444, validation/num_examples=50000
I0131 00:00:38.563776 139668754515712 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.4943090677261353, loss=5.383590221405029
I0131 00:01:20.170559 139668746123008 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.6731535196304321, loss=3.26772403717041
I0131 00:02:05.614390 139668754515712 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.5076786279678345, loss=3.7059788703918457
I0131 00:02:51.364833 139668746123008 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.792654037475586, loss=3.1587274074554443
I0131 00:03:37.319318 139668754515712 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.558367371559143, loss=3.186892032623291
I0131 00:04:22.706953 139668746123008 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.7037394046783447, loss=3.413822650909424
I0131 00:05:08.548935 139668754515712 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.749808430671692, loss=5.50388240814209
I0131 00:05:54.320133 139668746123008 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.5244072675704956, loss=4.252226829528809
I0131 00:06:40.016188 139668754515712 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.6849268674850464, loss=3.2023327350616455
I0131 00:07:25.473271 139668746123008 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.4651423692703247, loss=4.07361364364624
I0131 00:07:33.837869 139863983413056 spec.py:321] Evaluating on the training split.
I0131 00:07:44.310361 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 00:08:05.269137 139863983413056 spec.py:349] Evaluating on the test split.
I0131 00:08:06.905715 139863983413056 submission_runner.py:408] Time since start: 37598.56s, 	Step: 77520, 	{'train/accuracy': 0.6513671875, 'train/loss': 1.5582828521728516, 'validation/accuracy': 0.6090599894523621, 'validation/loss': 1.7538148164749146, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.3950002193450928, 'test/num_examples': 10000, 'score': 34915.00139904022, 'total_duration': 37598.55631041527, 'accumulated_submission_time': 34915.00139904022, 'accumulated_eval_time': 2676.3921184539795, 'accumulated_logging_time': 3.1309778690338135}
I0131 00:08:06.932163 139668754515712 logging_writer.py:48] [77520] accumulated_eval_time=2676.392118, accumulated_logging_time=3.130978, accumulated_submission_time=34915.001399, global_step=77520, preemption_count=0, score=34915.001399, test/accuracy=0.486200, test/loss=2.395000, test/num_examples=10000, total_duration=37598.556310, train/accuracy=0.651367, train/loss=1.558283, validation/accuracy=0.609060, validation/loss=1.753815, validation/num_examples=50000
I0131 00:08:39.416792 139668746123008 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.6896967887878418, loss=3.1948506832122803
I0131 00:09:24.207928 139668754515712 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.8347946405410767, loss=3.3470046520233154
I0131 00:10:09.976275 139668746123008 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.701295018196106, loss=3.1536548137664795
I0131 00:10:56.122273 139668754515712 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.7761006355285645, loss=3.1525208950042725
I0131 00:11:41.633492 139668746123008 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.4446793794631958, loss=4.4007062911987305
I0131 00:12:27.472926 139668754515712 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.9434739351272583, loss=3.262406826019287
I0131 00:13:12.937061 139668746123008 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.7338097095489502, loss=3.168043851852417
I0131 00:13:58.659799 139668754515712 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.53889799118042, loss=3.1998729705810547
I0131 00:14:44.323113 139668746123008 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.5385794639587402, loss=4.7864484786987305
I0131 00:15:07.167212 139863983413056 spec.py:321] Evaluating on the training split.
I0131 00:15:17.603991 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 00:15:38.027733 139863983413056 spec.py:349] Evaluating on the test split.
I0131 00:15:39.671278 139863983413056 submission_runner.py:408] Time since start: 38051.32s, 	Step: 78452, 	{'train/accuracy': 0.6617382764816284, 'train/loss': 1.5026801824569702, 'validation/accuracy': 0.6145200133323669, 'validation/loss': 1.7247921228408813, 'validation/num_examples': 50000, 'test/accuracy': 0.48840001225471497, 'test/loss': 2.357494354248047, 'test/num_examples': 10000, 'score': 35335.17891597748, 'total_duration': 38051.32184123993, 'accumulated_submission_time': 35335.17891597748, 'accumulated_eval_time': 2708.896152973175, 'accumulated_logging_time': 3.167029619216919}
I0131 00:15:39.706815 139668754515712 logging_writer.py:48] [78452] accumulated_eval_time=2708.896153, accumulated_logging_time=3.167030, accumulated_submission_time=35335.178916, global_step=78452, preemption_count=0, score=35335.178916, test/accuracy=0.488400, test/loss=2.357494, test/num_examples=10000, total_duration=38051.321841, train/accuracy=0.661738, train/loss=1.502680, validation/accuracy=0.614520, validation/loss=1.724792, validation/num_examples=50000
I0131 00:15:59.300642 139668746123008 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.7817846536636353, loss=3.2638468742370605
I0131 00:16:43.116358 139668754515712 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.7055565118789673, loss=3.083190441131592
I0131 00:17:28.555737 139668746123008 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.7360799312591553, loss=3.107823371887207
I0131 00:18:14.265029 139668754515712 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.7495968341827393, loss=3.4772210121154785
I0131 00:19:00.057676 139668746123008 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.302616834640503, loss=4.173961639404297
I0131 00:19:45.402735 139668754515712 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.8036051988601685, loss=3.3244271278381348
I0131 00:20:31.389166 139668746123008 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.558221459388733, loss=3.1376805305480957
I0131 00:21:17.199868 139668754515712 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.5529711246490479, loss=4.454704761505127
I0131 00:22:02.674690 139668746123008 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.8500981330871582, loss=3.2159788608551025
I0131 00:22:39.901469 139863983413056 spec.py:321] Evaluating on the training split.
I0131 00:22:50.434683 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 00:23:11.766726 139863983413056 spec.py:349] Evaluating on the test split.
I0131 00:23:13.405599 139863983413056 submission_runner.py:408] Time since start: 38505.06s, 	Step: 79383, 	{'train/accuracy': 0.6783593893051147, 'train/loss': 1.4718852043151855, 'validation/accuracy': 0.612280011177063, 'validation/loss': 1.779030442237854, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.4213500022888184, 'test/num_examples': 10000, 'score': 35755.31320667267, 'total_duration': 38505.05619072914, 'accumulated_submission_time': 35755.31320667267, 'accumulated_eval_time': 2742.400264978409, 'accumulated_logging_time': 3.214477300643921}
I0131 00:23:13.437986 139668754515712 logging_writer.py:48] [79383] accumulated_eval_time=2742.400265, accumulated_logging_time=3.214477, accumulated_submission_time=35755.313207, global_step=79383, preemption_count=0, score=35755.313207, test/accuracy=0.486400, test/loss=2.421350, test/num_examples=10000, total_duration=38505.056191, train/accuracy=0.678359, train/loss=1.471885, validation/accuracy=0.612280, validation/loss=1.779030, validation/num_examples=50000
I0131 00:23:20.627941 139668746123008 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.7448862791061401, loss=3.063640594482422
I0131 00:24:02.104847 139668754515712 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.6989119052886963, loss=3.193264961242676
I0131 00:24:47.840722 139668746123008 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.668989896774292, loss=3.3881540298461914
I0131 00:25:33.506641 139668754515712 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.5151275396347046, loss=3.864349603652954
I0131 00:26:19.320505 139668746123008 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.4172210693359375, loss=4.865194320678711
I0131 00:27:04.809940 139668754515712 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.4466547966003418, loss=4.2876787185668945
I0131 00:27:50.531995 139668746123008 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.5422649383544922, loss=3.454085111618042
I0131 00:28:36.414128 139668754515712 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.8048248291015625, loss=3.231046676635742
I0131 00:29:21.990163 139668746123008 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.706590175628662, loss=3.0483899116516113
I0131 00:30:07.997094 139668754515712 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.5021846294403076, loss=3.5116307735443115
I0131 00:30:13.656160 139863983413056 spec.py:321] Evaluating on the training split.
I0131 00:30:23.930842 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 00:30:44.996960 139863983413056 spec.py:349] Evaluating on the test split.
I0131 00:30:46.638901 139863983413056 submission_runner.py:408] Time since start: 38958.29s, 	Step: 80314, 	{'train/accuracy': 0.655468761920929, 'train/loss': 1.5589125156402588, 'validation/accuracy': 0.6153199672698975, 'validation/loss': 1.7471002340316772, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.3878822326660156, 'test/num_examples': 10000, 'score': 36175.472902059555, 'total_duration': 38958.28948068619, 'accumulated_submission_time': 36175.472902059555, 'accumulated_eval_time': 2775.382992506027, 'accumulated_logging_time': 3.256436347961426}
I0131 00:30:46.675873 139668746123008 logging_writer.py:48] [80314] accumulated_eval_time=2775.382993, accumulated_logging_time=3.256436, accumulated_submission_time=36175.472902, global_step=80314, preemption_count=0, score=36175.472902, test/accuracy=0.490300, test/loss=2.387882, test/num_examples=10000, total_duration=38958.289481, train/accuracy=0.655469, train/loss=1.558913, validation/accuracy=0.615320, validation/loss=1.747100, validation/num_examples=50000
I0131 00:31:21.424630 139668754515712 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.4560023546218872, loss=3.9963085651397705
I0131 00:32:06.823475 139668746123008 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.8626242876052856, loss=3.11613130569458
I0131 00:32:52.455100 139668754515712 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.558319091796875, loss=5.327500820159912
I0131 00:33:38.419067 139668746123008 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.931776523590088, loss=3.231048822402954
I0131 00:34:23.905932 139668754515712 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.6433931589126587, loss=3.2843544483184814
I0131 00:35:09.330014 139668746123008 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.6919288635253906, loss=3.4299540519714355
I0131 00:35:54.962800 139668754515712 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.7233973741531372, loss=3.5562384128570557
I0131 00:36:40.716502 139668746123008 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.8441537618637085, loss=3.12144136428833
I0131 00:37:26.542741 139668754515712 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.3130850791931152, loss=4.636488914489746
I0131 00:37:46.715293 139863983413056 spec.py:321] Evaluating on the training split.
I0131 00:37:57.198782 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 00:38:18.813285 139863983413056 spec.py:349] Evaluating on the test split.
I0131 00:38:20.453114 139863983413056 submission_runner.py:408] Time since start: 39412.10s, 	Step: 81246, 	{'train/accuracy': 0.672070324420929, 'train/loss': 1.45991849899292, 'validation/accuracy': 0.6218799948692322, 'validation/loss': 1.682550311088562, 'validation/num_examples': 50000, 'test/accuracy': 0.4992000162601471, 'test/loss': 2.3286523818969727, 'test/num_examples': 10000, 'score': 36595.45416688919, 'total_duration': 39412.10371303558, 'accumulated_submission_time': 36595.45416688919, 'accumulated_eval_time': 2809.1208050251007, 'accumulated_logging_time': 3.3032021522521973}
I0131 00:38:20.479563 139668746123008 logging_writer.py:48] [81246] accumulated_eval_time=2809.120805, accumulated_logging_time=3.303202, accumulated_submission_time=36595.454167, global_step=81246, preemption_count=0, score=36595.454167, test/accuracy=0.499200, test/loss=2.328652, test/num_examples=10000, total_duration=39412.103713, train/accuracy=0.672070, train/loss=1.459918, validation/accuracy=0.621880, validation/loss=1.682550, validation/num_examples=50000
I0131 00:38:42.470584 139668754515712 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.481257677078247, loss=4.794174671173096
I0131 00:39:26.274963 139668746123008 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.5152039527893066, loss=5.3192877769470215
I0131 00:40:12.069598 139668754515712 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.6023967266082764, loss=3.989917278289795
I0131 00:40:57.909893 139668746123008 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.457763433456421, loss=3.805079936981201
I0131 00:41:43.771439 139668754515712 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.3857632875442505, loss=4.372230529785156
I0131 00:42:29.329238 139668746123008 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.7459079027175903, loss=3.268541097640991
I0131 00:43:14.914793 139668754515712 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.032750129699707, loss=3.119398832321167
I0131 00:44:00.257374 139668746123008 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.8124291896820068, loss=3.166088581085205
I0131 00:44:45.862362 139668754515712 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.8316209316253662, loss=3.296138048171997
I0131 00:45:20.555411 139863983413056 spec.py:321] Evaluating on the training split.
I0131 00:45:30.641176 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 00:45:52.070969 139863983413056 spec.py:349] Evaluating on the test split.
I0131 00:45:53.700604 139863983413056 submission_runner.py:408] Time since start: 39865.35s, 	Step: 82178, 	{'train/accuracy': 0.677539050579071, 'train/loss': 1.4339114427566528, 'validation/accuracy': 0.6177999973297119, 'validation/loss': 1.7054755687713623, 'validation/num_examples': 50000, 'test/accuracy': 0.49160003662109375, 'test/loss': 2.343575954437256, 'test/num_examples': 10000, 'score': 37015.47077083588, 'total_duration': 39865.35120391846, 'accumulated_submission_time': 37015.47077083588, 'accumulated_eval_time': 2842.2659935951233, 'accumulated_logging_time': 3.3403608798980713}
I0131 00:45:53.727352 139668746123008 logging_writer.py:48] [82178] accumulated_eval_time=2842.265994, accumulated_logging_time=3.340361, accumulated_submission_time=37015.470771, global_step=82178, preemption_count=0, score=37015.470771, test/accuracy=0.491600, test/loss=2.343576, test/num_examples=10000, total_duration=39865.351204, train/accuracy=0.677539, train/loss=1.433911, validation/accuracy=0.617800, validation/loss=1.705476, validation/num_examples=50000
I0131 00:46:02.914480 139668754515712 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.4416749477386475, loss=4.0707478523254395
I0131 00:46:44.969295 139668746123008 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.5160212516784668, loss=3.763681650161743
I0131 00:47:30.632540 139668754515712 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.8166906833648682, loss=3.218844175338745
I0131 00:48:16.522279 139668746123008 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.8651149272918701, loss=3.1109395027160645
I0131 00:49:02.397692 139668754515712 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.6943498849868774, loss=3.532680034637451
I0131 00:49:47.959280 139668746123008 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.411126732826233, loss=4.20700740814209
I0131 00:50:33.567667 139668754515712 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.329095482826233, loss=4.168240070343018
I0131 00:51:19.367374 139668746123008 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.400754451751709, loss=4.919651508331299
I0131 00:52:05.089576 139668754515712 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.8313885927200317, loss=3.2063918113708496
I0131 00:52:50.645473 139668746123008 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.739504098892212, loss=5.226446151733398
I0131 00:52:54.001684 139863983413056 spec.py:321] Evaluating on the training split.
I0131 00:53:04.435519 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 00:53:21.705852 139863983413056 spec.py:349] Evaluating on the test split.
I0131 00:53:23.353848 139863983413056 submission_runner.py:408] Time since start: 40315.00s, 	Step: 83109, 	{'train/accuracy': 0.6681249737739563, 'train/loss': 1.490073561668396, 'validation/accuracy': 0.619879961013794, 'validation/loss': 1.6944104433059692, 'validation/num_examples': 50000, 'test/accuracy': 0.5033000111579895, 'test/loss': 2.3191699981689453, 'test/num_examples': 10000, 'score': 37435.68727660179, 'total_duration': 40315.00443935394, 'accumulated_submission_time': 37435.68727660179, 'accumulated_eval_time': 2871.618143796921, 'accumulated_logging_time': 3.3766629695892334}
I0131 00:53:23.386611 139668754515712 logging_writer.py:48] [83109] accumulated_eval_time=2871.618144, accumulated_logging_time=3.376663, accumulated_submission_time=37435.687277, global_step=83109, preemption_count=0, score=37435.687277, test/accuracy=0.503300, test/loss=2.319170, test/num_examples=10000, total_duration=40315.004439, train/accuracy=0.668125, train/loss=1.490074, validation/accuracy=0.619880, validation/loss=1.694410, validation/num_examples=50000
I0131 00:54:01.516937 139668746123008 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.2015042304992676, loss=3.1451616287231445
I0131 00:54:47.098345 139668754515712 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.8128232955932617, loss=3.1341490745544434
I0131 00:55:32.546994 139668746123008 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.6397768259048462, loss=3.235239028930664
I0131 00:56:18.325915 139668754515712 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.991044521331787, loss=3.1556942462921143
I0131 00:57:03.812011 139668746123008 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.6131837368011475, loss=3.426882743835449
I0131 00:57:49.642475 139668754515712 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.756808876991272, loss=3.482149839401245
I0131 00:58:35.329397 139668746123008 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.7249394655227661, loss=3.240633487701416
I0131 00:59:21.378468 139668754515712 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.3534255027770996, loss=4.843944072723389
I0131 01:00:07.031531 139668746123008 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.6825964450836182, loss=3.1443543434143066
I0131 01:00:23.603969 139863983413056 spec.py:321] Evaluating on the training split.
I0131 01:00:34.008780 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 01:00:53.737982 139863983413056 spec.py:349] Evaluating on the test split.
I0131 01:00:55.378106 139863983413056 submission_runner.py:408] Time since start: 40767.03s, 	Step: 84038, 	{'train/accuracy': 0.6651171445846558, 'train/loss': 1.465654730796814, 'validation/accuracy': 0.6218000054359436, 'validation/loss': 1.6730984449386597, 'validation/num_examples': 50000, 'test/accuracy': 0.49800002574920654, 'test/loss': 2.3054466247558594, 'test/num_examples': 10000, 'score': 37855.84566473961, 'total_duration': 40767.02870512009, 'accumulated_submission_time': 37855.84566473961, 'accumulated_eval_time': 2903.3922667503357, 'accumulated_logging_time': 3.419344186782837}
I0131 01:00:55.409517 139668754515712 logging_writer.py:48] [84038] accumulated_eval_time=2903.392267, accumulated_logging_time=3.419344, accumulated_submission_time=37855.845665, global_step=84038, preemption_count=0, score=37855.845665, test/accuracy=0.498000, test/loss=2.305447, test/num_examples=10000, total_duration=40767.028705, train/accuracy=0.665117, train/loss=1.465655, validation/accuracy=0.621800, validation/loss=1.673098, validation/num_examples=50000
I0131 01:01:20.581640 139668746123008 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.7878562211990356, loss=3.1371121406555176
I0131 01:02:04.446729 139668754515712 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.6967214345932007, loss=3.339869737625122
I0131 01:02:50.250782 139668746123008 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.8086961507797241, loss=2.9943766593933105
I0131 01:03:36.078588 139668754515712 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.8768419027328491, loss=3.0561814308166504
I0131 01:04:21.537019 139668746123008 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.7875308990478516, loss=3.0413832664489746
I0131 01:05:06.992350 139668754515712 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.887009620666504, loss=3.2257580757141113
I0131 01:05:52.499902 139668746123008 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.3737003803253174, loss=4.316570281982422
I0131 01:06:38.202786 139668754515712 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.7145311832427979, loss=3.264996290206909
I0131 01:07:23.831113 139668746123008 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.7098373174667358, loss=2.956923723220825
I0131 01:07:55.404294 139863983413056 spec.py:321] Evaluating on the training split.
I0131 01:08:05.410164 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 01:08:26.310011 139863983413056 spec.py:349] Evaluating on the test split.
I0131 01:08:27.954966 139863983413056 submission_runner.py:408] Time since start: 41219.61s, 	Step: 84971, 	{'train/accuracy': 0.6802929639816284, 'train/loss': 1.428873062133789, 'validation/accuracy': 0.6230999827384949, 'validation/loss': 1.6761740446090698, 'validation/num_examples': 50000, 'test/accuracy': 0.503000020980835, 'test/loss': 2.307889223098755, 'test/num_examples': 10000, 'score': 38275.783217191696, 'total_duration': 41219.60556221008, 'accumulated_submission_time': 38275.783217191696, 'accumulated_eval_time': 2935.9429478645325, 'accumulated_logging_time': 3.4596714973449707}
I0131 01:08:27.982781 139668754515712 logging_writer.py:48] [84971] accumulated_eval_time=2935.942948, accumulated_logging_time=3.459671, accumulated_submission_time=38275.783217, global_step=84971, preemption_count=0, score=38275.783217, test/accuracy=0.503000, test/loss=2.307889, test/num_examples=10000, total_duration=41219.605562, train/accuracy=0.680293, train/loss=1.428873, validation/accuracy=0.623100, validation/loss=1.676174, validation/num_examples=50000
I0131 01:08:39.977009 139668746123008 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.7097057104110718, loss=3.1790738105773926
I0131 01:09:22.141133 139668754515712 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.464811086654663, loss=3.9206290245056152
I0131 01:10:08.024769 139668746123008 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.7157691717147827, loss=3.174640417098999
I0131 01:10:53.925554 139668754515712 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.536195993423462, loss=4.734055042266846
I0131 01:11:39.751421 139668746123008 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.6142690181732178, loss=4.075531959533691
I0131 01:12:25.427436 139668754515712 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.7302507162094116, loss=3.3477728366851807
I0131 01:13:11.216978 139668746123008 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.5912010669708252, loss=3.5358211994171143
I0131 01:13:56.702135 139668754515712 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.6699784994125366, loss=3.283339262008667
I0131 01:14:42.113332 139668746123008 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.7517013549804688, loss=3.221642017364502
I0131 01:15:27.649692 139668754515712 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.4956419467926025, loss=5.245802879333496
I0131 01:15:28.209583 139863983413056 spec.py:321] Evaluating on the training split.
I0131 01:15:38.437507 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 01:15:58.948971 139863983413056 spec.py:349] Evaluating on the test split.
I0131 01:16:00.590765 139863983413056 submission_runner.py:408] Time since start: 41672.24s, 	Step: 85903, 	{'train/accuracy': 0.6666210889816284, 'train/loss': 1.5058388710021973, 'validation/accuracy': 0.623259961605072, 'validation/loss': 1.7001662254333496, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.3466873168945312, 'test/num_examples': 10000, 'score': 38695.95108413696, 'total_duration': 41672.24136352539, 'accumulated_submission_time': 38695.95108413696, 'accumulated_eval_time': 2968.3241169452667, 'accumulated_logging_time': 3.4983112812042236}
I0131 01:16:00.625128 139668746123008 logging_writer.py:48] [85903] accumulated_eval_time=2968.324117, accumulated_logging_time=3.498311, accumulated_submission_time=38695.951084, global_step=85903, preemption_count=0, score=38695.951084, test/accuracy=0.499500, test/loss=2.346687, test/num_examples=10000, total_duration=41672.241364, train/accuracy=0.666621, train/loss=1.505839, validation/accuracy=0.623260, validation/loss=1.700166, validation/num_examples=50000
I0131 01:16:40.290827 139668754515712 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.3879657983779907, loss=5.273838996887207
I0131 01:17:25.945690 139668746123008 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.5409632921218872, loss=4.781110763549805
I0131 01:18:11.843602 139668754515712 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.5257935523986816, loss=4.785885810852051
I0131 01:18:57.418977 139668746123008 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.7556302547454834, loss=3.166341781616211
I0131 01:19:43.143743 139668754515712 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.7508749961853027, loss=3.058241367340088
I0131 01:20:28.952495 139668746123008 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.6846994161605835, loss=3.3167576789855957
I0131 01:21:14.792765 139668754515712 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.7589017152786255, loss=3.317836284637451
I0131 01:22:00.352764 139668746123008 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.8665415048599243, loss=3.2277565002441406
I0131 01:22:46.120130 139668754515712 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.915900468826294, loss=3.2278218269348145
I0131 01:23:00.780535 139863983413056 spec.py:321] Evaluating on the training split.
I0131 01:23:11.286360 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 01:23:30.499851 139863983413056 spec.py:349] Evaluating on the test split.
I0131 01:23:32.148747 139863983413056 submission_runner.py:408] Time since start: 42123.80s, 	Step: 86834, 	{'train/accuracy': 0.6658398509025574, 'train/loss': 1.5209107398986816, 'validation/accuracy': 0.6223999857902527, 'validation/loss': 1.7161909341812134, 'validation/num_examples': 50000, 'test/accuracy': 0.5, 'test/loss': 2.360891819000244, 'test/num_examples': 10000, 'score': 39116.04857087135, 'total_duration': 42123.79933476448, 'accumulated_submission_time': 39116.04857087135, 'accumulated_eval_time': 2999.6923208236694, 'accumulated_logging_time': 3.542412757873535}
I0131 01:23:32.182555 139668746123008 logging_writer.py:48] [86834] accumulated_eval_time=2999.692321, accumulated_logging_time=3.542413, accumulated_submission_time=39116.048571, global_step=86834, preemption_count=0, score=39116.048571, test/accuracy=0.500000, test/loss=2.360892, test/num_examples=10000, total_duration=42123.799335, train/accuracy=0.665840, train/loss=1.520911, validation/accuracy=0.622400, validation/loss=1.716191, validation/num_examples=50000
I0131 01:23:58.982621 139668754515712 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.9205600023269653, loss=3.052807092666626
I0131 01:24:43.434561 139668746123008 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.5684205293655396, loss=5.457310676574707
I0131 01:25:29.257110 139668754515712 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.5296481847763062, loss=4.208611965179443
I0131 01:26:15.164528 139668746123008 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.7431859970092773, loss=3.1699492931365967
I0131 01:27:00.726843 139668754515712 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.7267533540725708, loss=3.035071849822998
I0131 01:27:46.390558 139668746123008 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.6847261190414429, loss=3.0624070167541504
I0131 01:28:32.125053 139668754515712 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.4852237701416016, loss=4.559154033660889
I0131 01:29:18.081436 139668746123008 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.52043879032135, loss=4.00120735168457
I0131 01:30:03.823858 139668754515712 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.8395719528198242, loss=3.116077423095703
I0131 01:30:32.272060 139863983413056 spec.py:321] Evaluating on the training split.
I0131 01:30:43.027585 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 01:31:04.604267 139863983413056 spec.py:349] Evaluating on the test split.
I0131 01:31:06.251955 139863983413056 submission_runner.py:408] Time since start: 42577.90s, 	Step: 87764, 	{'train/accuracy': 0.6809179782867432, 'train/loss': 1.4363369941711426, 'validation/accuracy': 0.6262999773025513, 'validation/loss': 1.676653504371643, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.3251760005950928, 'test/num_examples': 10000, 'score': 39536.07976317406, 'total_duration': 42577.90255379677, 'accumulated_submission_time': 39536.07976317406, 'accumulated_eval_time': 3033.672209739685, 'accumulated_logging_time': 3.5862622261047363}
I0131 01:31:06.280655 139668746123008 logging_writer.py:48] [87764] accumulated_eval_time=3033.672210, accumulated_logging_time=3.586262, accumulated_submission_time=39536.079763, global_step=87764, preemption_count=0, score=39536.079763, test/accuracy=0.497300, test/loss=2.325176, test/num_examples=10000, total_duration=42577.902554, train/accuracy=0.680918, train/loss=1.436337, validation/accuracy=0.626300, validation/loss=1.676654, validation/num_examples=50000
I0131 01:31:21.073007 139668754515712 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.9788023233413696, loss=3.1161983013153076
I0131 01:32:03.574714 139668746123008 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.5030410289764404, loss=4.890084743499756
I0131 01:32:49.222309 139668754515712 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.4997702836990356, loss=4.875794410705566
I0131 01:33:35.263835 139668746123008 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.0054023265838623, loss=3.302462100982666
I0131 01:34:20.931367 139668754515712 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.8173720836639404, loss=3.045600414276123
I0131 01:35:06.641021 139668746123008 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.9860737323760986, loss=3.057600736618042
I0131 01:35:52.121315 139668754515712 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.6790562868118286, loss=3.173524856567383
I0131 01:36:37.824848 139668746123008 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.6328297853469849, loss=3.4066085815429688
I0131 01:37:23.409783 139668754515712 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.7291321754455566, loss=3.0512359142303467
I0131 01:38:06.598203 139863983413056 spec.py:321] Evaluating on the training split.
I0131 01:38:16.608074 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 01:38:37.613924 139863983413056 spec.py:349] Evaluating on the test split.
I0131 01:38:39.242305 139863983413056 submission_runner.py:408] Time since start: 43030.89s, 	Step: 88696, 	{'train/accuracy': 0.6968359351158142, 'train/loss': 1.3666648864746094, 'validation/accuracy': 0.6245999932289124, 'validation/loss': 1.67709481716156, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.3024938106536865, 'test/num_examples': 10000, 'score': 39956.33709144592, 'total_duration': 43030.892899513245, 'accumulated_submission_time': 39956.33709144592, 'accumulated_eval_time': 3066.3163137435913, 'accumulated_logging_time': 3.6272923946380615}
I0131 01:38:39.273096 139668746123008 logging_writer.py:48] [88696] accumulated_eval_time=3066.316314, accumulated_logging_time=3.627292, accumulated_submission_time=39956.337091, global_step=88696, preemption_count=0, score=39956.337091, test/accuracy=0.503500, test/loss=2.302494, test/num_examples=10000, total_duration=43030.892900, train/accuracy=0.696836, train/loss=1.366665, validation/accuracy=0.624600, validation/loss=1.677095, validation/num_examples=50000
I0131 01:38:41.271829 139668754515712 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.4944264888763428, loss=4.473919868469238
I0131 01:39:22.101192 139668746123008 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.5801105499267578, loss=5.14695930480957
I0131 01:40:07.958464 139668754515712 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.6155824661254883, loss=3.2399840354919434
I0131 01:40:53.430510 139668746123008 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.591050386428833, loss=3.905015230178833
I0131 01:41:39.237785 139668754515712 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.5880236625671387, loss=4.1047892570495605
I0131 01:42:24.543729 139668746123008 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.7452924251556396, loss=3.0426018238067627
I0131 01:43:10.023059 139668754515712 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.910872459411621, loss=3.0852856636047363
I0131 01:43:56.089897 139668746123008 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.8101595640182495, loss=3.1863558292388916
I0131 01:44:41.519115 139668754515712 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.9127085208892822, loss=3.1593315601348877
I0131 01:45:27.318104 139668746123008 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.7778868675231934, loss=3.432938575744629
I0131 01:45:39.368458 139863983413056 spec.py:321] Evaluating on the training split.
I0131 01:45:49.801621 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 01:46:08.635840 139863983413056 spec.py:349] Evaluating on the test split.
I0131 01:46:10.285278 139863983413056 submission_runner.py:408] Time since start: 43481.94s, 	Step: 89628, 	{'train/accuracy': 0.6750390529632568, 'train/loss': 1.4493118524551392, 'validation/accuracy': 0.6331999897956848, 'validation/loss': 1.6430368423461914, 'validation/num_examples': 50000, 'test/accuracy': 0.5107000470161438, 'test/loss': 2.262535572052002, 'test/num_examples': 10000, 'score': 40376.373056173325, 'total_duration': 43481.935858011246, 'accumulated_submission_time': 40376.373056173325, 'accumulated_eval_time': 3097.2331142425537, 'accumulated_logging_time': 3.6694083213806152}
I0131 01:46:10.320950 139668754515712 logging_writer.py:48] [89628] accumulated_eval_time=3097.233114, accumulated_logging_time=3.669408, accumulated_submission_time=40376.373056, global_step=89628, preemption_count=0, score=40376.373056, test/accuracy=0.510700, test/loss=2.262536, test/num_examples=10000, total_duration=43481.935858, train/accuracy=0.675039, train/loss=1.449312, validation/accuracy=0.633200, validation/loss=1.643037, validation/num_examples=50000
I0131 01:46:39.512184 139668746123008 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.948600172996521, loss=3.027982234954834
I0131 01:47:24.779968 139668754515712 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.7565926313400269, loss=3.3730573654174805
I0131 01:48:10.846281 139668746123008 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.6716523170471191, loss=3.0000698566436768
I0131 01:48:57.313921 139668754515712 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.064563751220703, loss=3.002302646636963
I0131 01:49:43.278042 139668746123008 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.7401576042175293, loss=3.0557868480682373
I0131 01:50:29.381845 139668754515712 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.7771263122558594, loss=2.9079596996307373
I0131 01:51:15.134300 139668746123008 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.8766628503799438, loss=3.0717854499816895
I0131 01:52:00.792953 139668754515712 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.7025872468948364, loss=3.352973699569702
I0131 01:52:46.377303 139668746123008 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.108788251876831, loss=3.409970760345459
I0131 01:53:10.302791 139863983413056 spec.py:321] Evaluating on the training split.
I0131 01:53:20.628248 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 01:53:38.631936 139863983413056 spec.py:349] Evaluating on the test split.
I0131 01:53:40.274899 139863983413056 submission_runner.py:408] Time since start: 43931.93s, 	Step: 90554, 	{'train/accuracy': 0.6821093559265137, 'train/loss': 1.3882906436920166, 'validation/accuracy': 0.6303799748420715, 'validation/loss': 1.624939203262329, 'validation/num_examples': 50000, 'test/accuracy': 0.5046000480651855, 'test/loss': 2.265634775161743, 'test/num_examples': 10000, 'score': 40796.29656982422, 'total_duration': 43931.92548966408, 'accumulated_submission_time': 40796.29656982422, 'accumulated_eval_time': 3127.205216407776, 'accumulated_logging_time': 3.715543746948242}
I0131 01:53:40.312549 139668754515712 logging_writer.py:48] [90554] accumulated_eval_time=3127.205216, accumulated_logging_time=3.715544, accumulated_submission_time=40796.296570, global_step=90554, preemption_count=0, score=40796.296570, test/accuracy=0.504600, test/loss=2.265635, test/num_examples=10000, total_duration=43931.925490, train/accuracy=0.682109, train/loss=1.388291, validation/accuracy=0.630380, validation/loss=1.624939, validation/num_examples=50000
I0131 01:53:59.079905 139668746123008 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.6722346544265747, loss=3.5848543643951416
I0131 01:54:42.950022 139668754515712 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.000070810317993, loss=3.145195245742798
I0131 01:55:28.894325 139668746123008 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.6410571336746216, loss=3.198908805847168
I0131 01:56:14.572755 139668754515712 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.6340289115905762, loss=5.287426948547363
I0131 01:57:00.637205 139668746123008 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.781746745109558, loss=3.072930097579956
I0131 01:57:46.243991 139668754515712 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.7265865802764893, loss=3.1570396423339844
I0131 01:58:31.905597 139668746123008 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.7160881757736206, loss=3.449573278427124
I0131 01:59:17.813881 139668754515712 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.75913405418396, loss=3.132490873336792
I0131 02:00:04.115649 139668746123008 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.8364288806915283, loss=3.0843989849090576
I0131 02:00:40.393538 139863983413056 spec.py:321] Evaluating on the training split.
I0131 02:00:50.974360 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 02:01:11.543141 139863983413056 spec.py:349] Evaluating on the test split.
I0131 02:01:13.181513 139863983413056 submission_runner.py:408] Time since start: 44384.83s, 	Step: 91481, 	{'train/accuracy': 0.6850780844688416, 'train/loss': 1.3804093599319458, 'validation/accuracy': 0.6272000074386597, 'validation/loss': 1.6394050121307373, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2855710983276367, 'test/num_examples': 10000, 'score': 41216.31773328781, 'total_duration': 44384.83210873604, 'accumulated_submission_time': 41216.31773328781, 'accumulated_eval_time': 3159.993196964264, 'accumulated_logging_time': 3.765239715576172}
I0131 02:01:13.209555 139668754515712 logging_writer.py:48] [91481] accumulated_eval_time=3159.993197, accumulated_logging_time=3.765240, accumulated_submission_time=41216.317733, global_step=91481, preemption_count=0, score=41216.317733, test/accuracy=0.510100, test/loss=2.285571, test/num_examples=10000, total_duration=44384.832109, train/accuracy=0.685078, train/loss=1.380409, validation/accuracy=0.627200, validation/loss=1.639405, validation/num_examples=50000
I0131 02:01:21.186762 139668746123008 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.5460679531097412, loss=4.2450151443481445
I0131 02:02:02.751401 139668754515712 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.617429494857788, loss=3.2293877601623535
I0131 02:02:48.760921 139668746123008 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.7107309103012085, loss=3.58239483833313
I0131 02:03:34.654432 139668754515712 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.654861330986023, loss=5.326946258544922
I0131 02:04:20.669994 139668746123008 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.5515804290771484, loss=4.953484535217285
I0131 02:05:06.308644 139668754515712 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.748557209968567, loss=2.998013973236084
I0131 02:05:51.845997 139668746123008 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.7835783958435059, loss=3.15962290763855
I0131 02:06:37.576051 139668754515712 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.9795913696289062, loss=3.3771402835845947
I0131 02:07:23.072500 139668746123008 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.6394271850585938, loss=4.998729705810547
I0131 02:08:08.867728 139668754515712 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.7393336296081543, loss=3.1351284980773926
I0131 02:08:13.671285 139863983413056 spec.py:321] Evaluating on the training split.
I0131 02:08:23.757439 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 02:08:44.783932 139863983413056 spec.py:349] Evaluating on the test split.
I0131 02:08:46.415147 139863983413056 submission_runner.py:408] Time since start: 44838.07s, 	Step: 92412, 	{'train/accuracy': 0.6785351634025574, 'train/loss': 1.4298800230026245, 'validation/accuracy': 0.6362599730491638, 'validation/loss': 1.630461573600769, 'validation/num_examples': 50000, 'test/accuracy': 0.5117000341415405, 'test/loss': 2.257552146911621, 'test/num_examples': 10000, 'score': 41636.72108960152, 'total_duration': 44838.06574630737, 'accumulated_submission_time': 41636.72108960152, 'accumulated_eval_time': 3192.737065553665, 'accumulated_logging_time': 3.803889751434326}
I0131 02:08:46.443130 139668746123008 logging_writer.py:48] [92412] accumulated_eval_time=3192.737066, accumulated_logging_time=3.803890, accumulated_submission_time=41636.721090, global_step=92412, preemption_count=0, score=41636.721090, test/accuracy=0.511700, test/loss=2.257552, test/num_examples=10000, total_duration=44838.065746, train/accuracy=0.678535, train/loss=1.429880, validation/accuracy=0.636260, validation/loss=1.630462, validation/num_examples=50000
I0131 02:09:21.982093 139668754515712 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.6945546865463257, loss=3.1528210639953613
I0131 02:10:07.645663 139668746123008 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.6596633195877075, loss=3.5989737510681152
I0131 02:10:53.443195 139668754515712 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.875130295753479, loss=3.013735294342041
I0131 02:11:39.370264 139668746123008 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.7847402095794678, loss=3.0492653846740723
I0131 02:12:25.043964 139668754515712 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.745689034461975, loss=4.415225028991699
I0131 02:13:10.805003 139668746123008 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.7712256908416748, loss=3.2036192417144775
I0131 02:13:56.740405 139668754515712 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.612290620803833, loss=3.732219696044922
I0131 02:14:42.379080 139668746123008 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.8506284952163696, loss=3.0657947063446045
I0131 02:15:27.942841 139668754515712 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.624250888824463, loss=5.2677764892578125
I0131 02:15:46.806787 139863983413056 spec.py:321] Evaluating on the training split.
I0131 02:15:57.084967 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 02:16:16.374636 139863983413056 spec.py:349] Evaluating on the test split.
I0131 02:16:18.034809 139863983413056 submission_runner.py:408] Time since start: 45289.69s, 	Step: 93343, 	{'train/accuracy': 0.68212890625, 'train/loss': 1.4268741607666016, 'validation/accuracy': 0.6340399980545044, 'validation/loss': 1.6396797895431519, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2780954837799072, 'test/num_examples': 10000, 'score': 42057.02818584442, 'total_duration': 45289.685396671295, 'accumulated_submission_time': 42057.02818584442, 'accumulated_eval_time': 3223.965080499649, 'accumulated_logging_time': 3.8404388427734375}
I0131 02:16:18.083456 139668746123008 logging_writer.py:48] [93343] accumulated_eval_time=3223.965080, accumulated_logging_time=3.840439, accumulated_submission_time=42057.028186, global_step=93343, preemption_count=0, score=42057.028186, test/accuracy=0.510100, test/loss=2.278095, test/num_examples=10000, total_duration=45289.685397, train/accuracy=0.682129, train/loss=1.426874, validation/accuracy=0.634040, validation/loss=1.639680, validation/num_examples=50000
I0131 02:16:41.244794 139668754515712 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.6247155666351318, loss=3.6773903369903564
I0131 02:17:25.432963 139668746123008 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.9475758075714111, loss=3.106762647628784
I0131 02:18:11.147527 139668754515712 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.0056447982788086, loss=3.1314918994903564
I0131 02:18:56.910075 139668746123008 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.965593934059143, loss=3.156651258468628
I0131 02:19:42.676878 139668754515712 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.7468055486679077, loss=4.258771896362305
I0131 02:20:28.637708 139668746123008 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.8840066194534302, loss=3.676687717437744
I0131 02:21:14.374604 139668754515712 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.9565688371658325, loss=3.031158447265625
I0131 02:21:59.919445 139668746123008 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.9399739503860474, loss=2.943100690841675
I0131 02:22:45.847829 139668754515712 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.7874789237976074, loss=4.980725288391113
I0131 02:23:18.100996 139863983413056 spec.py:321] Evaluating on the training split.
I0131 02:23:28.493397 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 02:23:48.457944 139863983413056 spec.py:349] Evaluating on the test split.
I0131 02:23:50.113707 139863983413056 submission_runner.py:408] Time since start: 45741.76s, 	Step: 94272, 	{'train/accuracy': 0.6900390386581421, 'train/loss': 1.358396291732788, 'validation/accuracy': 0.632860004901886, 'validation/loss': 1.6014665365219116, 'validation/num_examples': 50000, 'test/accuracy': 0.5105000138282776, 'test/loss': 2.2469406127929688, 'test/num_examples': 10000, 'score': 42476.98532438278, 'total_duration': 45741.764285326004, 'accumulated_submission_time': 42476.98532438278, 'accumulated_eval_time': 3255.977776527405, 'accumulated_logging_time': 3.900721549987793}
I0131 02:23:50.148699 139668746123008 logging_writer.py:48] [94272] accumulated_eval_time=3255.977777, accumulated_logging_time=3.900722, accumulated_submission_time=42476.985324, global_step=94272, preemption_count=0, score=42476.985324, test/accuracy=0.510500, test/loss=2.246941, test/num_examples=10000, total_duration=45741.764285, train/accuracy=0.690039, train/loss=1.358396, validation/accuracy=0.632860, validation/loss=1.601467, validation/num_examples=50000
I0131 02:24:01.727988 139668754515712 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.5454192161560059, loss=4.291221618652344
I0131 02:24:44.040572 139668746123008 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.9817595481872559, loss=2.955510139465332
I0131 02:25:29.696493 139668754515712 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.8679677248001099, loss=2.9884934425354004
I0131 02:26:15.365531 139668746123008 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.5703463554382324, loss=5.112349510192871
I0131 02:27:00.968577 139668754515712 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.8512372970581055, loss=3.0422468185424805
I0131 02:27:46.581561 139668746123008 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.8939239978790283, loss=3.8302218914031982
I0131 02:28:32.345499 139668754515712 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.7627873420715332, loss=4.948428630828857
I0131 02:29:17.996650 139668746123008 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.8145439624786377, loss=5.025979042053223
I0131 02:30:03.976538 139668754515712 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.8922233581542969, loss=3.13254451751709
I0131 02:30:49.626791 139668746123008 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.8555513620376587, loss=2.9118781089782715
I0131 02:30:50.200644 139863983413056 spec.py:321] Evaluating on the training split.
I0131 02:31:00.319027 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 02:31:18.795755 139863983413056 spec.py:349] Evaluating on the test split.
I0131 02:31:20.453458 139863983413056 submission_runner.py:408] Time since start: 46192.10s, 	Step: 95203, 	{'train/accuracy': 0.69740229845047, 'train/loss': 1.3583762645721436, 'validation/accuracy': 0.6399799585342407, 'validation/loss': 1.6108720302581787, 'validation/num_examples': 50000, 'test/accuracy': 0.5120000243186951, 'test/loss': 2.2654266357421875, 'test/num_examples': 10000, 'score': 42896.978048563, 'total_duration': 46192.10404133797, 'accumulated_submission_time': 42896.978048563, 'accumulated_eval_time': 3286.2305703163147, 'accumulated_logging_time': 3.9465410709381104}
I0131 02:31:20.491795 139668754515712 logging_writer.py:48] [95203] accumulated_eval_time=3286.230570, accumulated_logging_time=3.946541, accumulated_submission_time=42896.978049, global_step=95203, preemption_count=0, score=42896.978049, test/accuracy=0.512000, test/loss=2.265427, test/num_examples=10000, total_duration=46192.104041, train/accuracy=0.697402, train/loss=1.358376, validation/accuracy=0.639980, validation/loss=1.610872, validation/num_examples=50000
I0131 02:32:00.777483 139668746123008 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.862052321434021, loss=2.9861814975738525
I0131 02:32:46.217415 139668754515712 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.7864208221435547, loss=3.7003262042999268
I0131 02:33:32.095695 139668746123008 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.7794455289840698, loss=3.0702433586120605
I0131 02:34:18.023123 139668754515712 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.0232317447662354, loss=3.236124038696289
I0131 02:35:03.554350 139668746123008 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.0508556365966797, loss=3.1704134941101074
I0131 02:35:49.030177 139668754515712 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.0129311084747314, loss=3.0270066261291504
I0131 02:36:34.913178 139668746123008 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.9605141878128052, loss=5.213503837585449
I0131 02:37:20.628436 139668754515712 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.8760085105895996, loss=2.9953153133392334
I0131 02:38:06.559268 139668746123008 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.8833287954330444, loss=5.226363182067871
I0131 02:38:20.555278 139863983413056 spec.py:321] Evaluating on the training split.
I0131 02:38:31.063969 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 02:38:49.398685 139863983413056 spec.py:349] Evaluating on the test split.
I0131 02:38:51.046806 139863983413056 submission_runner.py:408] Time since start: 46642.70s, 	Step: 96132, 	{'train/accuracy': 0.6863671541213989, 'train/loss': 1.4062321186065674, 'validation/accuracy': 0.6400399804115295, 'validation/loss': 1.6143362522125244, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.2290306091308594, 'test/num_examples': 10000, 'score': 43316.982568740845, 'total_duration': 46642.69738292694, 'accumulated_submission_time': 43316.982568740845, 'accumulated_eval_time': 3316.7220873832703, 'accumulated_logging_time': 3.99554705619812}
I0131 02:38:51.080333 139668754515712 logging_writer.py:48] [96132] accumulated_eval_time=3316.722087, accumulated_logging_time=3.995547, accumulated_submission_time=43316.982569, global_step=96132, preemption_count=0, score=43316.982569, test/accuracy=0.517500, test/loss=2.229031, test/num_examples=10000, total_duration=46642.697383, train/accuracy=0.686367, train/loss=1.406232, validation/accuracy=0.640040, validation/loss=1.614336, validation/num_examples=50000
I0131 02:39:18.639520 139668746123008 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.7915209531784058, loss=4.376003742218018
I0131 02:40:03.625857 139668754515712 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.919790506362915, loss=3.029633045196533
I0131 02:40:49.665159 139668746123008 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.7882764339447021, loss=3.470837116241455
I0131 02:41:35.371888 139668754515712 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.8221564292907715, loss=4.93398380279541
I0131 02:42:21.093839 139668746123008 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.135840654373169, loss=2.975437879562378
I0131 02:43:06.874715 139668754515712 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.7950001955032349, loss=5.2390031814575195
I0131 02:43:52.301878 139668746123008 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.9624160528182983, loss=3.171614170074463
I0131 02:44:37.941292 139668754515712 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.9658032655715942, loss=3.020134687423706
I0131 02:45:23.488677 139668746123008 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.953570008277893, loss=3.0163753032684326
I0131 02:45:51.132625 139863983413056 spec.py:321] Evaluating on the training split.
I0131 02:46:01.537478 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 02:46:19.312905 139863983413056 spec.py:349] Evaluating on the test split.
I0131 02:46:20.956782 139863983413056 submission_runner.py:408] Time since start: 47092.61s, 	Step: 97062, 	{'train/accuracy': 0.685546875, 'train/loss': 1.4155725240707397, 'validation/accuracy': 0.6375600099563599, 'validation/loss': 1.633133053779602, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.2834768295288086, 'test/num_examples': 10000, 'score': 43736.97590112686, 'total_duration': 47092.60734796524, 'accumulated_submission_time': 43736.97590112686, 'accumulated_eval_time': 3346.5461995601654, 'accumulated_logging_time': 4.039233446121216}
I0131 02:46:20.997772 139668754515712 logging_writer.py:48] [97062] accumulated_eval_time=3346.546200, accumulated_logging_time=4.039233, accumulated_submission_time=43736.975901, global_step=97062, preemption_count=0, score=43736.975901, test/accuracy=0.518300, test/loss=2.283477, test/num_examples=10000, total_duration=47092.607348, train/accuracy=0.685547, train/loss=1.415573, validation/accuracy=0.637560, validation/loss=1.633133, validation/num_examples=50000
I0131 02:46:36.576928 139668746123008 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.5871647596359253, loss=3.9168450832366943
I0131 02:47:19.590487 139668754515712 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.82514226436615, loss=3.133676290512085
I0131 02:48:05.105297 139668746123008 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.7691670656204224, loss=2.8742384910583496
I0131 02:48:50.791652 139668754515712 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.833616852760315, loss=2.9543633460998535
I0131 02:49:36.400564 139668746123008 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.646691918373108, loss=3.4195737838745117
I0131 02:50:22.423120 139668754515712 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.9610822200775146, loss=3.1064419746398926
I0131 02:51:08.215834 139668746123008 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.6063146591186523, loss=4.328991889953613
I0131 02:51:53.888629 139668754515712 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.9543243646621704, loss=4.3727521896362305
I0131 02:52:39.679482 139668746123008 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.5655903816223145, loss=4.271369457244873
I0131 02:53:20.982173 139863983413056 spec.py:321] Evaluating on the training split.
I0131 02:53:31.193939 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 02:53:50.596829 139863983413056 spec.py:349] Evaluating on the test split.
I0131 02:53:52.252838 139863983413056 submission_runner.py:408] Time since start: 47543.90s, 	Step: 97992, 	{'train/accuracy': 0.7066406011581421, 'train/loss': 1.2971094846725464, 'validation/accuracy': 0.6403200030326843, 'validation/loss': 1.582248568534851, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.2080962657928467, 'test/num_examples': 10000, 'score': 44156.90197920799, 'total_duration': 47543.903435230255, 'accumulated_submission_time': 44156.90197920799, 'accumulated_eval_time': 3377.816866159439, 'accumulated_logging_time': 4.090280055999756}
I0131 02:53:52.287708 139668754515712 logging_writer.py:48] [97992] accumulated_eval_time=3377.816866, accumulated_logging_time=4.090280, accumulated_submission_time=44156.901979, global_step=97992, preemption_count=0, score=44156.901979, test/accuracy=0.520500, test/loss=2.208096, test/num_examples=10000, total_duration=47543.903435, train/accuracy=0.706641, train/loss=1.297109, validation/accuracy=0.640320, validation/loss=1.582249, validation/num_examples=50000
I0131 02:53:55.879270 139668746123008 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.9552552700042725, loss=5.17333984375
I0131 02:54:37.173367 139668754515712 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.886859655380249, loss=3.0505895614624023
I0131 02:55:22.684469 139668746123008 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.8911700248718262, loss=3.451421022415161
I0131 02:56:08.570237 139668754515712 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.0230281352996826, loss=3.759427785873413
I0131 02:56:54.491473 139668746123008 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.0189476013183594, loss=3.0284314155578613
I0131 02:57:39.893110 139668754515712 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.7766505479812622, loss=4.352205276489258
I0131 02:58:25.695459 139668746123008 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.9619982242584229, loss=2.942434310913086
I0131 02:59:11.457541 139668754515712 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.0354654788970947, loss=3.086732864379883
I0131 02:59:57.033297 139668746123008 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.5387606620788574, loss=4.843735218048096
I0131 03:00:42.604776 139668754515712 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.0217275619506836, loss=3.0689430236816406
I0131 03:00:52.303811 139863983413056 spec.py:321] Evaluating on the training split.
I0131 03:01:03.013150 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 03:01:24.393425 139863983413056 spec.py:349] Evaluating on the test split.
I0131 03:01:26.042020 139863983413056 submission_runner.py:408] Time since start: 47997.69s, 	Step: 98923, 	{'train/accuracy': 0.6883788704872131, 'train/loss': 1.393426775932312, 'validation/accuracy': 0.641319990158081, 'validation/loss': 1.6070661544799805, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.2544445991516113, 'test/num_examples': 10000, 'score': 44576.86071538925, 'total_duration': 47997.69262051582, 'accumulated_submission_time': 44576.86071538925, 'accumulated_eval_time': 3411.5551047325134, 'accumulated_logging_time': 4.134812593460083}
I0131 03:01:26.071251 139668746123008 logging_writer.py:48] [98923] accumulated_eval_time=3411.555105, accumulated_logging_time=4.134813, accumulated_submission_time=44576.860715, global_step=98923, preemption_count=0, score=44576.860715, test/accuracy=0.509100, test/loss=2.254445, test/num_examples=10000, total_duration=47997.692621, train/accuracy=0.688379, train/loss=1.393427, validation/accuracy=0.641320, validation/loss=1.607066, validation/num_examples=50000
I0131 03:01:57.263468 139668754515712 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.8268805742263794, loss=3.6236960887908936
I0131 03:02:41.807908 139668746123008 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.8191845417022705, loss=3.669111728668213
I0131 03:03:27.483257 139668754515712 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.631456971168518, loss=3.812126874923706
I0131 03:04:13.309902 139668746123008 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.9995348453521729, loss=3.9211924076080322
I0131 03:04:58.928057 139668754515712 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.9564889669418335, loss=4.687739372253418
I0131 03:05:44.560384 139668746123008 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.8932769298553467, loss=3.0744006633758545
I0131 03:06:30.298023 139668754515712 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.0336666107177734, loss=3.060943126678467
I0131 03:07:15.863000 139668746123008 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.191131591796875, loss=3.897141933441162
I0131 03:08:01.262741 139668754515712 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.8445191383361816, loss=5.229978084564209
I0131 03:08:26.111368 139863983413056 spec.py:321] Evaluating on the training split.
I0131 03:08:36.365762 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 03:08:56.652097 139863983413056 spec.py:349] Evaluating on the test split.
I0131 03:08:58.294656 139863983413056 submission_runner.py:408] Time since start: 48449.95s, 	Step: 99856, 	{'train/accuracy': 0.6871289014816284, 'train/loss': 1.3720650672912598, 'validation/accuracy': 0.6365999579429626, 'validation/loss': 1.5965046882629395, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.2395246028900146, 'test/num_examples': 10000, 'score': 44996.839626550674, 'total_duration': 48449.945254564285, 'accumulated_submission_time': 44996.839626550674, 'accumulated_eval_time': 3443.7383959293365, 'accumulated_logging_time': 4.1757354736328125}
I0131 03:08:58.324387 139668746123008 logging_writer.py:48] [99856] accumulated_eval_time=3443.738396, accumulated_logging_time=4.175735, accumulated_submission_time=44996.839627, global_step=99856, preemption_count=0, score=44996.839627, test/accuracy=0.513400, test/loss=2.239525, test/num_examples=10000, total_duration=48449.945255, train/accuracy=0.687129, train/loss=1.372065, validation/accuracy=0.636600, validation/loss=1.596505, validation/num_examples=50000
I0131 03:09:16.300338 139668754515712 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.001145601272583, loss=3.054595470428467
I0131 03:09:59.693900 139668746123008 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.0746636390686035, loss=3.2383201122283936
I0131 03:10:45.952954 139668754515712 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.1032261848449707, loss=3.0752675533294678
I0131 03:11:31.940204 139668746123008 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.6125510931015015, loss=4.2630486488342285
I0131 03:12:17.878839 139668754515712 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.1235578060150146, loss=4.926928997039795
I0131 03:13:03.707928 139668746123008 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.9810011386871338, loss=3.1134090423583984
I0131 03:13:49.196001 139668754515712 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.9262765645980835, loss=3.106369733810425
I0131 03:14:34.939357 139668746123008 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.4979599714279175, loss=4.665385723114014
I0131 03:15:20.443805 139668754515712 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.9591251611709595, loss=3.1954898834228516
I0131 03:15:58.513294 139863983413056 spec.py:321] Evaluating on the training split.
I0131 03:16:08.917367 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 03:16:30.463081 139863983413056 spec.py:349] Evaluating on the test split.
I0131 03:16:32.126456 139863983413056 submission_runner.py:408] Time since start: 48903.78s, 	Step: 100785, 	{'train/accuracy': 0.7017577886581421, 'train/loss': 1.312267780303955, 'validation/accuracy': 0.6451999545097351, 'validation/loss': 1.5605461597442627, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.181353807449341, 'test/num_examples': 10000, 'score': 45416.97100830078, 'total_duration': 48903.77703499794, 'accumulated_submission_time': 45416.97100830078, 'accumulated_eval_time': 3477.3515434265137, 'accumulated_logging_time': 4.214470624923706}
I0131 03:16:32.165771 139668746123008 logging_writer.py:48] [100785] accumulated_eval_time=3477.351543, accumulated_logging_time=4.214471, accumulated_submission_time=45416.971008, global_step=100785, preemption_count=0, score=45416.971008, test/accuracy=0.522600, test/loss=2.181354, test/num_examples=10000, total_duration=48903.777035, train/accuracy=0.701758, train/loss=1.312268, validation/accuracy=0.645200, validation/loss=1.560546, validation/num_examples=50000
I0131 03:16:38.553819 139668754515712 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.107433557510376, loss=3.072758674621582
I0131 03:17:19.906985 139668746123008 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.939349889755249, loss=2.967918634414673
I0131 03:18:05.388367 139668754515712 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.033540725708008, loss=5.200918674468994
I0131 03:18:51.088527 139668746123008 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.8067208528518677, loss=3.8020670413970947
I0131 03:19:36.787487 139668754515712 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.969089150428772, loss=3.081892967224121
I0131 03:20:22.633793 139668746123008 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.7764570713043213, loss=3.2001771926879883
I0131 03:21:08.465137 139668754515712 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.8508294820785522, loss=3.0361313819885254
I0131 03:21:54.293923 139668746123008 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.9839643239974976, loss=3.2253377437591553
I0131 03:22:40.141412 139668754515712 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.9056427478790283, loss=2.979203462600708
I0131 03:23:25.797721 139668746123008 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.1938560009002686, loss=2.9375336170196533
I0131 03:23:32.308539 139863983413056 spec.py:321] Evaluating on the training split.
I0131 03:23:42.783035 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 03:24:04.260492 139863983413056 spec.py:349] Evaluating on the test split.
I0131 03:24:05.903764 139863983413056 submission_runner.py:408] Time since start: 49357.55s, 	Step: 101716, 	{'train/accuracy': 0.69349604845047, 'train/loss': 1.36771821975708, 'validation/accuracy': 0.6436399817466736, 'validation/loss': 1.5866146087646484, 'validation/num_examples': 50000, 'test/accuracy': 0.5249000191688538, 'test/loss': 2.2173092365264893, 'test/num_examples': 10000, 'score': 45837.05639505386, 'total_duration': 49357.55435633659, 'accumulated_submission_time': 45837.05639505386, 'accumulated_eval_time': 3510.946757078171, 'accumulated_logging_time': 4.263065338134766}
I0131 03:24:05.936732 139668754515712 logging_writer.py:48] [101716] accumulated_eval_time=3510.946757, accumulated_logging_time=4.263065, accumulated_submission_time=45837.056395, global_step=101716, preemption_count=0, score=45837.056395, test/accuracy=0.524900, test/loss=2.217309, test/num_examples=10000, total_duration=49357.554356, train/accuracy=0.693496, train/loss=1.367718, validation/accuracy=0.643640, validation/loss=1.586615, validation/num_examples=50000
I0131 03:24:39.893321 139668746123008 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.6158690452575684, loss=4.555120468139648
I0131 03:25:25.407760 139668754515712 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.047489881515503, loss=3.0458405017852783
I0131 03:26:11.148638 139668746123008 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.820142388343811, loss=5.059070587158203
I0131 03:26:57.174667 139668754515712 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.5968860387802124, loss=5.1250996589660645
I0131 03:27:42.789171 139668746123008 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.032646656036377, loss=3.0147531032562256
I0131 03:28:28.237068 139668754515712 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.0328710079193115, loss=3.126781463623047
I0131 03:29:14.245164 139668746123008 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.803225040435791, loss=5.129404067993164
I0131 03:30:00.001575 139668754515712 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.6945812702178955, loss=4.778288841247559
I0131 03:30:46.267719 139668746123008 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.0082151889801025, loss=2.986356019973755
I0131 03:31:06.147251 139863983413056 spec.py:321] Evaluating on the training split.
I0131 03:31:17.036733 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 03:31:35.265348 139863983413056 spec.py:349] Evaluating on the test split.
I0131 03:31:36.924429 139863983413056 submission_runner.py:408] Time since start: 49808.58s, 	Step: 102645, 	{'train/accuracy': 0.6882616877555847, 'train/loss': 1.39771568775177, 'validation/accuracy': 0.6396999955177307, 'validation/loss': 1.6158915758132935, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.250572919845581, 'test/num_examples': 10000, 'score': 46257.20961642265, 'total_duration': 49808.57501959801, 'accumulated_submission_time': 46257.20961642265, 'accumulated_eval_time': 3541.7239258289337, 'accumulated_logging_time': 4.30495023727417}
I0131 03:31:36.969449 139668754515712 logging_writer.py:48] [102645] accumulated_eval_time=3541.723926, accumulated_logging_time=4.304950, accumulated_submission_time=46257.209616, global_step=102645, preemption_count=0, score=46257.209616, test/accuracy=0.522300, test/loss=2.250573, test/num_examples=10000, total_duration=49808.575020, train/accuracy=0.688262, train/loss=1.397716, validation/accuracy=0.639700, validation/loss=1.615892, validation/num_examples=50000
I0131 03:31:59.366405 139668746123008 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.6667896509170532, loss=4.80172872543335
I0131 03:32:43.990224 139668754515712 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.054673194885254, loss=2.997894287109375
I0131 03:33:29.802441 139668746123008 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.7349680662155151, loss=4.739115238189697
I0131 03:34:15.625803 139668754515712 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.215346336364746, loss=3.0894157886505127
I0131 03:35:01.476015 139668746123008 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.9509207010269165, loss=3.2268378734588623
I0131 03:35:47.125551 139668754515712 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.0666232109069824, loss=5.10221529006958
I0131 03:36:32.856058 139668746123008 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.7745492458343506, loss=3.6098852157592773
I0131 03:37:18.589011 139668754515712 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.8647310733795166, loss=2.939756393432617
I0131 03:38:04.447942 139668746123008 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.9840162992477417, loss=3.131467819213867
I0131 03:38:37.097834 139863983413056 spec.py:321] Evaluating on the training split.
I0131 03:38:48.055838 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 03:39:05.403689 139863983413056 spec.py:349] Evaluating on the test split.
I0131 03:39:07.048331 139863983413056 submission_runner.py:408] Time since start: 50258.70s, 	Step: 103573, 	{'train/accuracy': 0.7084375023841858, 'train/loss': 1.2893983125686646, 'validation/accuracy': 0.6509000062942505, 'validation/loss': 1.5381120443344116, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.1615405082702637, 'test/num_examples': 10000, 'score': 46677.277096033096, 'total_duration': 50258.698894262314, 'accumulated_submission_time': 46677.277096033096, 'accumulated_eval_time': 3571.6743774414062, 'accumulated_logging_time': 4.362541913986206}
I0131 03:39:07.085834 139668754515712 logging_writer.py:48] [103573] accumulated_eval_time=3571.674377, accumulated_logging_time=4.362542, accumulated_submission_time=46677.277096, global_step=103573, preemption_count=0, score=46677.277096, test/accuracy=0.532900, test/loss=2.161541, test/num_examples=10000, total_duration=50258.698894, train/accuracy=0.708438, train/loss=1.289398, validation/accuracy=0.650900, validation/loss=1.538112, validation/num_examples=50000
I0131 03:39:18.263478 139668746123008 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.6154060363769531, loss=3.7384724617004395
I0131 03:40:01.267238 139668754515712 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.775205373764038, loss=3.786691427230835
I0131 03:40:47.035344 139668746123008 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.179744005203247, loss=2.92769455909729
I0131 03:41:33.042568 139668754515712 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.9423284530639648, loss=3.0085439682006836
I0131 03:42:18.744359 139668746123008 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.44830322265625, loss=3.1608896255493164
I0131 03:43:04.533839 139668754515712 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.8437286615371704, loss=5.1951680183410645
I0131 03:43:50.207833 139668746123008 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.136683940887451, loss=2.9131641387939453
I0131 03:44:36.024999 139668754515712 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.9705641269683838, loss=3.0234344005584717
I0131 03:45:21.814499 139668746123008 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.8788702487945557, loss=3.4561994075775146
I0131 03:46:07.621536 139668754515712 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.9174230098724365, loss=3.562333822250366
I0131 03:46:07.635307 139863983413056 spec.py:321] Evaluating on the training split.
I0131 03:46:18.103976 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 03:46:36.408703 139863983413056 spec.py:349] Evaluating on the test split.
I0131 03:46:38.067402 139863983413056 submission_runner.py:408] Time since start: 50709.72s, 	Step: 104501, 	{'train/accuracy': 0.7164257764816284, 'train/loss': 1.3079707622528076, 'validation/accuracy': 0.6469599604606628, 'validation/loss': 1.601804494857788, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.236384868621826, 'test/num_examples': 10000, 'score': 47097.76482272148, 'total_duration': 50709.717992305756, 'accumulated_submission_time': 47097.76482272148, 'accumulated_eval_time': 3602.106454372406, 'accumulated_logging_time': 4.4127349853515625}
I0131 03:46:38.105177 139668746123008 logging_writer.py:48] [104501] accumulated_eval_time=3602.106454, accumulated_logging_time=4.412735, accumulated_submission_time=47097.764823, global_step=104501, preemption_count=0, score=47097.764823, test/accuracy=0.526200, test/loss=2.236385, test/num_examples=10000, total_duration=50709.717992, train/accuracy=0.716426, train/loss=1.307971, validation/accuracy=0.646960, validation/loss=1.601804, validation/num_examples=50000
I0131 03:47:18.837207 139668754515712 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.892227292060852, loss=2.915179491043091
I0131 03:48:04.600828 139668746123008 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.032829999923706, loss=2.956136703491211
I0131 03:48:50.611236 139668754515712 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.6692875623703003, loss=4.41910457611084
I0131 03:49:36.751228 139668746123008 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.1184592247009277, loss=2.978610038757324
I0131 03:50:22.217594 139668754515712 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.008763074874878, loss=2.902947187423706
I0131 03:51:08.380521 139668746123008 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.9166619777679443, loss=3.9830636978149414
I0131 03:51:54.640095 139668754515712 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.6772853136062622, loss=3.7507028579711914
I0131 03:52:40.392631 139668746123008 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.035384178161621, loss=2.9823975563049316
I0131 03:53:26.476473 139668754515712 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.3247156143188477, loss=3.313048839569092
I0131 03:53:38.084485 139863983413056 spec.py:321] Evaluating on the training split.
I0131 03:53:48.469876 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 03:54:10.255171 139863983413056 spec.py:349] Evaluating on the test split.
I0131 03:54:11.889931 139863983413056 submission_runner.py:408] Time since start: 51163.54s, 	Step: 105427, 	{'train/accuracy': 0.6813281178474426, 'train/loss': 1.432881474494934, 'validation/accuracy': 0.6276599764823914, 'validation/loss': 1.6766186952590942, 'validation/num_examples': 50000, 'test/accuracy': 0.5055000185966492, 'test/loss': 2.2996857166290283, 'test/num_examples': 10000, 'score': 47517.686259269714, 'total_duration': 51163.54053092003, 'accumulated_submission_time': 47517.686259269714, 'accumulated_eval_time': 3635.9119005203247, 'accumulated_logging_time': 4.460398435592651}
I0131 03:54:11.921943 139668746123008 logging_writer.py:48] [105427] accumulated_eval_time=3635.911901, accumulated_logging_time=4.460398, accumulated_submission_time=47517.686259, global_step=105427, preemption_count=0, score=47517.686259, test/accuracy=0.505500, test/loss=2.299686, test/num_examples=10000, total_duration=51163.540531, train/accuracy=0.681328, train/loss=1.432881, validation/accuracy=0.627660, validation/loss=1.676619, validation/num_examples=50000
I0131 03:54:41.499886 139668754515712 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.1114821434020996, loss=2.873516798019409
I0131 03:55:25.882442 139668746123008 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.7923308610916138, loss=4.845954895019531
I0131 03:56:11.561602 139668754515712 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.6727122068405151, loss=4.989375114440918
I0131 03:56:57.479643 139668746123008 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.8012875318527222, loss=3.7300729751586914
I0131 03:57:42.953300 139668754515712 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.7453521490097046, loss=3.9457597732543945
I0131 03:58:28.554006 139668746123008 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.0563886165618896, loss=3.0474653244018555
I0131 03:59:14.604090 139668754515712 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.865663766860962, loss=3.649339437484741
I0131 04:00:00.100263 139668746123008 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.1256046295166016, loss=2.89738130569458
I0131 04:00:45.504219 139668754515712 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.196552038192749, loss=2.839789867401123
I0131 04:01:11.985233 139863983413056 spec.py:321] Evaluating on the training split.
I0131 04:01:22.416337 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 04:01:42.341109 139863983413056 spec.py:349] Evaluating on the test split.
I0131 04:01:43.986377 139863983413056 submission_runner.py:408] Time since start: 51615.64s, 	Step: 106359, 	{'train/accuracy': 0.7081249952316284, 'train/loss': 1.3071223497390747, 'validation/accuracy': 0.653439998626709, 'validation/loss': 1.5344473123550415, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.1776108741760254, 'test/num_examples': 10000, 'score': 47937.69239234924, 'total_duration': 51615.63697743416, 'accumulated_submission_time': 47937.69239234924, 'accumulated_eval_time': 3667.9130449295044, 'accumulated_logging_time': 4.501140356063843}
I0131 04:01:44.017188 139668746123008 logging_writer.py:48] [106359] accumulated_eval_time=3667.913045, accumulated_logging_time=4.501140, accumulated_submission_time=47937.692392, global_step=106359, preemption_count=0, score=47937.692392, test/accuracy=0.525000, test/loss=2.177611, test/num_examples=10000, total_duration=51615.636977, train/accuracy=0.708125, train/loss=1.307122, validation/accuracy=0.653440, validation/loss=1.534447, validation/num_examples=50000
I0131 04:02:00.836938 139668754515712 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.0915355682373047, loss=2.9846549034118652
I0131 04:02:44.002651 139668746123008 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.122358560562134, loss=3.0399985313415527
I0131 04:03:29.666860 139668754515712 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.125614643096924, loss=2.873957395553589
I0131 04:04:15.556277 139668746123008 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.7226005792617798, loss=4.870388031005859
I0131 04:05:01.274915 139668754515712 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.0160562992095947, loss=3.0902581214904785
I0131 04:05:46.802785 139668746123008 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.005764961242676, loss=2.9067630767822266
I0131 04:06:32.507734 139668754515712 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.053929567337036, loss=3.026257276535034
I0131 04:07:18.337090 139668746123008 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.1536083221435547, loss=3.0016181468963623
I0131 04:08:04.094314 139668754515712 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.7430624961853027, loss=4.787406921386719
I0131 04:08:44.037500 139863983413056 spec.py:321] Evaluating on the training split.
I0131 04:08:54.418114 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 04:09:14.477037 139863983413056 spec.py:349] Evaluating on the test split.
I0131 04:09:16.123022 139863983413056 submission_runner.py:408] Time since start: 52067.77s, 	Step: 107289, 	{'train/accuracy': 0.7215625047683716, 'train/loss': 1.2222944498062134, 'validation/accuracy': 0.6552599668502808, 'validation/loss': 1.523241400718689, 'validation/num_examples': 50000, 'test/accuracy': 0.535800039768219, 'test/loss': 2.1337265968322754, 'test/num_examples': 10000, 'score': 48357.65385222435, 'total_duration': 52067.7736222744, 'accumulated_submission_time': 48357.65385222435, 'accumulated_eval_time': 3699.9985733032227, 'accumulated_logging_time': 4.5414369106292725}
I0131 04:09:16.157665 139668746123008 logging_writer.py:48] [107289] accumulated_eval_time=3699.998573, accumulated_logging_time=4.541437, accumulated_submission_time=48357.653852, global_step=107289, preemption_count=0, score=48357.653852, test/accuracy=0.535800, test/loss=2.133727, test/num_examples=10000, total_duration=52067.773622, train/accuracy=0.721563, train/loss=1.222294, validation/accuracy=0.655260, validation/loss=1.523241, validation/num_examples=50000
I0131 04:09:20.960897 139668754515712 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.1087512969970703, loss=3.057809352874756
I0131 04:10:02.405074 139668746123008 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.897564649581909, loss=3.9596385955810547
I0131 04:10:47.972995 139668754515712 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.080632448196411, loss=5.1903252601623535
I0131 04:11:33.869451 139668746123008 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.071004867553711, loss=3.907937526702881
I0131 04:12:19.394460 139668754515712 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.9054886102676392, loss=3.387281894683838
I0131 04:13:04.853633 139668746123008 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.0975825786590576, loss=2.8123779296875
I0131 04:13:50.385340 139668754515712 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.020109176635742, loss=3.0023856163024902
I0131 04:14:36.146412 139668746123008 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.045727252960205, loss=3.1216800212860107
I0131 04:15:21.755047 139668754515712 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.315782070159912, loss=2.8926327228546143
I0131 04:16:07.236109 139668746123008 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.8984522819519043, loss=3.965830087661743
I0131 04:16:16.376935 139863983413056 spec.py:321] Evaluating on the training split.
I0131 04:16:26.457591 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 04:16:45.638485 139863983413056 spec.py:349] Evaluating on the test split.
I0131 04:16:47.295522 139863983413056 submission_runner.py:408] Time since start: 52518.95s, 	Step: 108222, 	{'train/accuracy': 0.7074999809265137, 'train/loss': 1.2904741764068604, 'validation/accuracy': 0.6606400012969971, 'validation/loss': 1.5038014650344849, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.1375465393066406, 'test/num_examples': 10000, 'score': 48777.81280827522, 'total_duration': 52518.94608283043, 'accumulated_submission_time': 48777.81280827522, 'accumulated_eval_time': 3730.9171195030212, 'accumulated_logging_time': 4.586863279342651}
I0131 04:16:47.335829 139668754515712 logging_writer.py:48] [108222] accumulated_eval_time=3730.917120, accumulated_logging_time=4.586863, accumulated_submission_time=48777.812808, global_step=108222, preemption_count=0, score=48777.812808, test/accuracy=0.534900, test/loss=2.137547, test/num_examples=10000, total_duration=52518.946083, train/accuracy=0.707500, train/loss=1.290474, validation/accuracy=0.660640, validation/loss=1.503801, validation/num_examples=50000
I0131 04:17:18.924202 139668746123008 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.1585330963134766, loss=3.1054561138153076
I0131 04:18:04.135342 139668754515712 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.059267520904541, loss=5.073673248291016
I0131 04:18:49.556188 139668746123008 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.2349467277526855, loss=5.114026069641113
I0131 04:19:35.288341 139668754515712 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.9485114812850952, loss=4.609135627746582
I0131 04:20:21.131926 139668746123008 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.7756061553955078, loss=4.903991222381592
I0131 04:21:06.747608 139668754515712 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.9322706460952759, loss=4.611401081085205
I0131 04:21:52.661944 139668746123008 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.540346384048462, loss=4.296390533447266
I0131 04:22:38.282351 139668754515712 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.6858739852905273, loss=4.350303649902344
I0131 04:23:24.034751 139668746123008 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.0083205699920654, loss=2.9715542793273926
I0131 04:23:47.437108 139863983413056 spec.py:321] Evaluating on the training split.
I0131 04:23:57.516030 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 04:24:18.904228 139863983413056 spec.py:349] Evaluating on the test split.
I0131 04:24:20.547861 139863983413056 submission_runner.py:408] Time since start: 52972.20s, 	Step: 109153, 	{'train/accuracy': 0.7104882597923279, 'train/loss': 1.2939804792404175, 'validation/accuracy': 0.6558799743652344, 'validation/loss': 1.531550407409668, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.1510488986968994, 'test/num_examples': 10000, 'score': 49197.85523939133, 'total_duration': 52972.1984333992, 'accumulated_submission_time': 49197.85523939133, 'accumulated_eval_time': 3764.0278511047363, 'accumulated_logging_time': 4.637696266174316}
I0131 04:24:20.587484 139668754515712 logging_writer.py:48] [109153] accumulated_eval_time=3764.027851, accumulated_logging_time=4.637696, accumulated_submission_time=49197.855239, global_step=109153, preemption_count=0, score=49197.855239, test/accuracy=0.533000, test/loss=2.151049, test/num_examples=10000, total_duration=52972.198433, train/accuracy=0.710488, train/loss=1.293980, validation/accuracy=0.655880, validation/loss=1.531550, validation/num_examples=50000
I0131 04:24:39.750869 139668746123008 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.0588560104370117, loss=2.943552017211914
I0131 04:25:22.714825 139668754515712 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.2758429050445557, loss=2.815394401550293
I0131 04:26:08.373200 139668746123008 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.195168972015381, loss=3.4802303314208984
I0131 04:26:53.916768 139668754515712 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.0166261196136475, loss=3.017275810241699
I0131 04:27:39.571746 139668746123008 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.0069940090179443, loss=4.959683418273926
I0131 04:28:25.473240 139668754515712 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.7104145288467407, loss=5.0073466300964355
I0131 04:29:11.269225 139668746123008 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.8245224952697754, loss=4.985803604125977
I0131 04:29:57.188318 139668754515712 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.7225658893585205, loss=3.969848394393921
I0131 04:30:42.755350 139668746123008 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.770185947418213, loss=5.001861572265625
I0131 04:31:20.650089 139863983413056 spec.py:321] Evaluating on the training split.
I0131 04:31:31.122998 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 04:31:53.102358 139863983413056 spec.py:349] Evaluating on the test split.
I0131 04:31:54.754939 139863983413056 submission_runner.py:408] Time since start: 53426.41s, 	Step: 110084, 	{'train/accuracy': 0.7136523127555847, 'train/loss': 1.3198362588882446, 'validation/accuracy': 0.6524199843406677, 'validation/loss': 1.5822229385375977, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.2205541133880615, 'test/num_examples': 10000, 'score': 49617.85936307907, 'total_duration': 53426.40553641319, 'accumulated_submission_time': 49617.85936307907, 'accumulated_eval_time': 3798.1327052116394, 'accumulated_logging_time': 4.687514781951904}
I0131 04:31:54.790088 139668754515712 logging_writer.py:48] [110084] accumulated_eval_time=3798.132705, accumulated_logging_time=4.687515, accumulated_submission_time=49617.859363, global_step=110084, preemption_count=0, score=49617.859363, test/accuracy=0.531400, test/loss=2.220554, test/num_examples=10000, total_duration=53426.405536, train/accuracy=0.713652, train/loss=1.319836, validation/accuracy=0.652420, validation/loss=1.582223, validation/num_examples=50000
I0131 04:32:01.577677 139668746123008 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.100752353668213, loss=3.1132891178131104
I0131 04:32:43.034793 139668754515712 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.008655548095703, loss=2.926344156265259
I0131 04:33:28.587112 139668746123008 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.6276445388793945, loss=4.542919635772705
I0131 04:34:14.338651 139668754515712 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.021249532699585, loss=3.645531177520752
I0131 04:35:00.410040 139668746123008 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.3969321250915527, loss=3.0025815963745117
I0131 04:35:46.070239 139668754515712 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.848258376121521, loss=3.198232412338257
I0131 04:36:31.610252 139668746123008 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.9024133682250977, loss=3.2768118381500244
I0131 04:37:17.462355 139668754515712 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.0538220405578613, loss=3.011685609817505
I0131 04:38:03.044845 139668746123008 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.1175191402435303, loss=2.9881844520568848
I0131 04:38:48.604803 139668754515712 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.2878000736236572, loss=2.9514219760894775
I0131 04:38:55.188228 139863983413056 spec.py:321] Evaluating on the training split.
I0131 04:39:05.792674 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 04:39:28.057945 139863983413056 spec.py:349] Evaluating on the test split.
I0131 04:39:29.717057 139863983413056 submission_runner.py:408] Time since start: 53881.37s, 	Step: 111016, 	{'train/accuracy': 0.7125585675239563, 'train/loss': 1.2730872631072998, 'validation/accuracy': 0.6649599671363831, 'validation/loss': 1.4868433475494385, 'validation/num_examples': 50000, 'test/accuracy': 0.537600040435791, 'test/loss': 2.1088082790374756, 'test/num_examples': 10000, 'score': 50038.1953496933, 'total_duration': 53881.36761689186, 'accumulated_submission_time': 50038.1953496933, 'accumulated_eval_time': 3832.661499977112, 'accumulated_logging_time': 4.737124443054199}
I0131 04:39:29.754925 139668746123008 logging_writer.py:48] [111016] accumulated_eval_time=3832.661500, accumulated_logging_time=4.737124, accumulated_submission_time=50038.195350, global_step=111016, preemption_count=0, score=50038.195350, test/accuracy=0.537600, test/loss=2.108808, test/num_examples=10000, total_duration=53881.367617, train/accuracy=0.712559, train/loss=1.273087, validation/accuracy=0.664960, validation/loss=1.486843, validation/num_examples=50000
I0131 04:40:03.728109 139668754515712 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.8857346773147583, loss=2.9148306846618652
I0131 04:40:49.233393 139668746123008 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.12369441986084, loss=3.172987461090088
I0131 04:41:35.138362 139668754515712 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.0691397190093994, loss=5.149493217468262
I0131 04:42:21.156892 139668746123008 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.1032888889312744, loss=2.8664987087249756
I0131 04:43:06.706551 139668754515712 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.1314661502838135, loss=3.005966901779175
I0131 04:43:52.567453 139668746123008 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.9088062047958374, loss=4.02064847946167
I0131 04:44:38.147443 139668754515712 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.9460421800613403, loss=4.351136684417725
I0131 04:45:23.691915 139668746123008 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.2202603816986084, loss=4.105709075927734
I0131 04:46:09.285766 139668754515712 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.933577537536621, loss=3.8387861251831055
I0131 04:46:29.947937 139863983413056 spec.py:321] Evaluating on the training split.
I0131 04:46:40.486599 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 04:46:56.623351 139863983413056 spec.py:349] Evaluating on the test split.
I0131 04:46:58.280439 139863983413056 submission_runner.py:408] Time since start: 54329.93s, 	Step: 111947, 	{'train/accuracy': 0.7182812094688416, 'train/loss': 1.2427871227264404, 'validation/accuracy': 0.6640799641609192, 'validation/loss': 1.481744647026062, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.1059060096740723, 'test/num_examples': 10000, 'score': 50458.330828905106, 'total_duration': 54329.931025743484, 'accumulated_submission_time': 50458.330828905106, 'accumulated_eval_time': 3860.9939935207367, 'accumulated_logging_time': 4.784902572631836}
I0131 04:46:58.317697 139668746123008 logging_writer.py:48] [111947] accumulated_eval_time=3860.993994, accumulated_logging_time=4.784903, accumulated_submission_time=50458.330829, global_step=111947, preemption_count=0, score=50458.330829, test/accuracy=0.546900, test/loss=2.105906, test/num_examples=10000, total_duration=54329.931026, train/accuracy=0.718281, train/loss=1.242787, validation/accuracy=0.664080, validation/loss=1.481745, validation/num_examples=50000
I0131 04:47:19.923005 139668754515712 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.143958806991577, loss=3.165837049484253
I0131 04:48:05.420058 139668746123008 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.0350239276885986, loss=2.8472514152526855
I0131 04:48:51.960669 139668754515712 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.9508858919143677, loss=4.361855506896973
I0131 04:49:37.784193 139668746123008 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.328979730606079, loss=2.9070873260498047
I0131 04:50:24.275053 139668754515712 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.015235662460327, loss=3.2004270553588867
I0131 04:51:10.121370 139668746123008 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.8834080696105957, loss=3.6455368995666504
I0131 04:51:56.018308 139668754515712 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.2020068168640137, loss=2.907914876937866
I0131 04:52:42.325970 139668746123008 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.9148893356323242, loss=3.5402700901031494
I0131 04:53:28.232580 139668754515712 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.1572775840759277, loss=2.873852252960205
I0131 04:53:58.622947 139863983413056 spec.py:321] Evaluating on the training split.
I0131 04:54:08.966075 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 04:54:30.161642 139863983413056 spec.py:349] Evaluating on the test split.
I0131 04:54:31.806603 139863983413056 submission_runner.py:408] Time since start: 54783.46s, 	Step: 112868, 	{'train/accuracy': 0.7190234065055847, 'train/loss': 1.2374742031097412, 'validation/accuracy': 0.6615200042724609, 'validation/loss': 1.4908949136734009, 'validation/num_examples': 50000, 'test/accuracy': 0.5367000102996826, 'test/loss': 2.121800184249878, 'test/num_examples': 10000, 'score': 50878.57825565338, 'total_duration': 54783.457184791565, 'accumulated_submission_time': 50878.57825565338, 'accumulated_eval_time': 3894.177620410919, 'accumulated_logging_time': 4.832216739654541}
I0131 04:54:31.847049 139668746123008 logging_writer.py:48] [112868] accumulated_eval_time=3894.177620, accumulated_logging_time=4.832217, accumulated_submission_time=50878.578256, global_step=112868, preemption_count=0, score=50878.578256, test/accuracy=0.536700, test/loss=2.121800, test/num_examples=10000, total_duration=54783.457185, train/accuracy=0.719023, train/loss=1.237474, validation/accuracy=0.661520, validation/loss=1.490895, validation/num_examples=50000
I0131 04:54:45.019623 139668754515712 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.028156280517578, loss=3.301234722137451
I0131 04:55:27.133941 139668746123008 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.9224897623062134, loss=4.715076446533203
I0131 04:56:13.269601 139668754515712 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.1894266605377197, loss=2.9638450145721436
I0131 04:56:59.041003 139668746123008 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.371992349624634, loss=3.1622941493988037
I0131 04:57:45.423279 139668754515712 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.3165764808654785, loss=2.892078161239624
I0131 04:58:30.751273 139668746123008 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.1642019748687744, loss=3.0409913063049316
I0131 04:59:16.392065 139668754515712 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.455598831176758, loss=2.901808500289917
I0131 05:00:02.405598 139668746123008 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.1514227390289307, loss=2.8697891235351562
I0131 05:00:48.194478 139668754515712 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.9867860078811646, loss=3.0650670528411865
I0131 05:01:31.810070 139863983413056 spec.py:321] Evaluating on the training split.
I0131 05:01:42.055298 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 05:02:03.475455 139863983413056 spec.py:349] Evaluating on the test split.
I0131 05:02:05.125171 139863983413056 submission_runner.py:408] Time since start: 55236.78s, 	Step: 113797, 	{'train/accuracy': 0.7408984303474426, 'train/loss': 1.1787163019180298, 'validation/accuracy': 0.666979968547821, 'validation/loss': 1.5026799440383911, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.132467031478882, 'test/num_examples': 10000, 'score': 51298.48287606239, 'total_duration': 55236.77574682236, 'accumulated_submission_time': 51298.48287606239, 'accumulated_eval_time': 3927.492713212967, 'accumulated_logging_time': 4.883016586303711}
I0131 05:02:05.161508 139668746123008 logging_writer.py:48] [113797] accumulated_eval_time=3927.492713, accumulated_logging_time=4.883017, accumulated_submission_time=51298.482876, global_step=113797, preemption_count=0, score=51298.482876, test/accuracy=0.544200, test/loss=2.132467, test/num_examples=10000, total_duration=55236.775747, train/accuracy=0.740898, train/loss=1.178716, validation/accuracy=0.666980, validation/loss=1.502680, validation/num_examples=50000
I0131 05:02:06.757159 139668754515712 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.9229274988174438, loss=3.0308213233947754
I0131 05:02:47.166072 139668746123008 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.1849448680877686, loss=2.893127202987671
I0131 05:03:32.648114 139668754515712 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.1497602462768555, loss=4.712466716766357
I0131 05:04:18.501888 139668746123008 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.2027804851531982, loss=2.8991715908050537
I0131 05:05:04.344339 139668754515712 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.8996942043304443, loss=4.326943397521973
I0131 05:05:50.111301 139668746123008 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.0633366107940674, loss=2.9760897159576416
I0131 05:06:35.680209 139668754515712 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.9826747179031372, loss=4.875320911407471
I0131 05:07:21.293396 139668746123008 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.1601927280426025, loss=3.3857007026672363
I0131 05:08:06.789716 139668754515712 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.2499098777770996, loss=2.9442155361175537
I0131 05:08:52.233158 139668746123008 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.0416972637176514, loss=3.5508267879486084
I0131 05:09:05.126086 139863983413056 spec.py:321] Evaluating on the training split.
I0131 05:09:15.229655 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 05:09:37.139958 139863983413056 spec.py:349] Evaluating on the test split.
I0131 05:09:38.779689 139863983413056 submission_runner.py:408] Time since start: 55690.43s, 	Step: 114730, 	{'train/accuracy': 0.7202734351158142, 'train/loss': 1.223569393157959, 'validation/accuracy': 0.6654999852180481, 'validation/loss': 1.4597338438034058, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.0868093967437744, 'test/num_examples': 10000, 'score': 51718.38898730278, 'total_duration': 55690.430280447006, 'accumulated_submission_time': 51718.38898730278, 'accumulated_eval_time': 3961.1463055610657, 'accumulated_logging_time': 4.928993463516235}
I0131 05:09:38.814689 139668754515712 logging_writer.py:48] [114730] accumulated_eval_time=3961.146306, accumulated_logging_time=4.928993, accumulated_submission_time=51718.388987, global_step=114730, preemption_count=0, score=51718.388987, test/accuracy=0.546900, test/loss=2.086809, test/num_examples=10000, total_duration=55690.430280, train/accuracy=0.720273, train/loss=1.223569, validation/accuracy=0.665500, validation/loss=1.459734, validation/num_examples=50000
I0131 05:10:07.155679 139668746123008 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.954464077949524, loss=4.456230163574219
I0131 05:10:52.232014 139668754515712 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.1433162689208984, loss=3.7282214164733887
I0131 05:11:37.899720 139668746123008 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.081157684326172, loss=3.0136325359344482
I0131 05:12:23.963905 139668754515712 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.1489357948303223, loss=2.8037734031677246
I0131 05:13:09.489899 139668746123008 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.214561700820923, loss=2.888300895690918
I0131 05:13:55.045258 139668754515712 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.2311463356018066, loss=2.872068405151367
I0131 05:14:40.555059 139668746123008 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.0975468158721924, loss=2.958026647567749
I0131 05:15:26.095727 139668754515712 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.0859482288360596, loss=3.162958860397339
I0131 05:16:11.815178 139668746123008 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.057600498199463, loss=2.8914241790771484
I0131 05:16:38.797712 139863983413056 spec.py:321] Evaluating on the training split.
I0131 05:16:49.072878 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 05:17:09.514192 139863983413056 spec.py:349] Evaluating on the test split.
I0131 05:17:11.167656 139863983413056 submission_runner.py:408] Time since start: 56142.82s, 	Step: 115661, 	{'train/accuracy': 0.7249413728713989, 'train/loss': 1.2017278671264648, 'validation/accuracy': 0.6695599555969238, 'validation/loss': 1.4541553258895874, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.075801372528076, 'test/num_examples': 10000, 'score': 52138.31260251999, 'total_duration': 56142.81825685501, 'accumulated_submission_time': 52138.31260251999, 'accumulated_eval_time': 3993.5162620544434, 'accumulated_logging_time': 4.973785161972046}
I0131 05:17:11.202598 139668754515712 logging_writer.py:48] [115661] accumulated_eval_time=3993.516262, accumulated_logging_time=4.973785, accumulated_submission_time=52138.312603, global_step=115661, preemption_count=0, score=52138.312603, test/accuracy=0.544000, test/loss=2.075801, test/num_examples=10000, total_duration=56142.818257, train/accuracy=0.724941, train/loss=1.201728, validation/accuracy=0.669560, validation/loss=1.454155, validation/num_examples=50000
I0131 05:17:27.157402 139668746123008 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.3337855339050293, loss=2.8379807472229004
I0131 05:18:10.250287 139668754515712 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.9788507223129272, loss=4.406853199005127
I0131 05:18:55.749946 139668746123008 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.2014694213867188, loss=2.7806386947631836
I0131 05:19:41.418815 139668754515712 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.105053663253784, loss=3.730146646499634
I0131 05:20:27.210285 139668746123008 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.259187936782837, loss=2.880335569381714
I0131 05:21:12.834617 139668754515712 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.006755828857422, loss=3.8618626594543457
I0131 05:21:58.489726 139668746123008 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.2316701412200928, loss=2.9091973304748535
I0131 05:22:44.559640 139668754515712 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.0368759632110596, loss=3.311664342880249
I0131 05:23:30.098954 139668746123008 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.0926992893218994, loss=2.799149751663208
I0131 05:24:11.547734 139863983413056 spec.py:321] Evaluating on the training split.
I0131 05:24:21.639370 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 05:24:43.218650 139863983413056 spec.py:349] Evaluating on the test split.
I0131 05:24:44.871197 139863983413056 submission_runner.py:408] Time since start: 56596.52s, 	Step: 116592, 	{'train/accuracy': 0.7337695360183716, 'train/loss': 1.1900928020477295, 'validation/accuracy': 0.675059974193573, 'validation/loss': 1.4655356407165527, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 2.100456476211548, 'test/num_examples': 10000, 'score': 52558.60073399544, 'total_duration': 56596.52179288864, 'accumulated_submission_time': 52558.60073399544, 'accumulated_eval_time': 4026.8397274017334, 'accumulated_logging_time': 5.018024444580078}
I0131 05:24:44.908951 139668754515712 logging_writer.py:48] [116592] accumulated_eval_time=4026.839727, accumulated_logging_time=5.018024, accumulated_submission_time=52558.600734, global_step=116592, preemption_count=0, score=52558.600734, test/accuracy=0.551500, test/loss=2.100456, test/num_examples=10000, total_duration=56596.521793, train/accuracy=0.733770, train/loss=1.190093, validation/accuracy=0.675060, validation/loss=1.465536, validation/num_examples=50000
I0131 05:24:48.510077 139668746123008 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.1021182537078857, loss=2.9751858711242676
I0131 05:25:29.492358 139668754515712 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.175349235534668, loss=3.045360803604126
I0131 05:26:15.102157 139668746123008 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.15085768699646, loss=4.162412643432617
I0131 05:27:00.728084 139668754515712 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.4092798233032227, loss=3.0040345191955566
I0131 05:27:46.532209 139668746123008 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.4862589836120605, loss=2.831047534942627
I0131 05:28:31.890647 139668754515712 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.9557247161865234, loss=4.220296859741211
I0131 05:29:17.410454 139668746123008 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.344139814376831, loss=2.869326591491699
I0131 05:30:03.139821 139668754515712 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.1760594844818115, loss=2.9802279472351074
I0131 05:30:48.501614 139668746123008 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.3777408599853516, loss=2.8599793910980225
I0131 05:31:34.140432 139668754515712 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.1986019611358643, loss=2.8276729583740234
I0131 05:31:45.290668 139863983413056 spec.py:321] Evaluating on the training split.
I0131 05:31:55.775729 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 05:32:14.728339 139863983413056 spec.py:349] Evaluating on the test split.
I0131 05:32:16.380866 139863983413056 submission_runner.py:408] Time since start: 57048.03s, 	Step: 117526, 	{'train/accuracy': 0.730664074420929, 'train/loss': 1.1986579895019531, 'validation/accuracy': 0.6748799681663513, 'validation/loss': 1.4364898204803467, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 2.076021194458008, 'test/num_examples': 10000, 'score': 52978.923567056656, 'total_duration': 57048.03144288063, 'accumulated_submission_time': 52978.923567056656, 'accumulated_eval_time': 4057.929902076721, 'accumulated_logging_time': 5.0658793449401855}
I0131 05:32:16.420383 139668746123008 logging_writer.py:48] [117526] accumulated_eval_time=4057.929902, accumulated_logging_time=5.065879, accumulated_submission_time=52978.923567, global_step=117526, preemption_count=0, score=52978.923567, test/accuracy=0.549100, test/loss=2.076021, test/num_examples=10000, total_duration=57048.031443, train/accuracy=0.730664, train/loss=1.198658, validation/accuracy=0.674880, validation/loss=1.436490, validation/num_examples=50000
I0131 05:32:46.361916 139668754515712 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.2243807315826416, loss=3.374924659729004
I0131 05:33:31.722103 139668746123008 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.3876852989196777, loss=2.7356913089752197
I0131 05:34:17.226781 139668754515712 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.222532272338867, loss=2.79304575920105
I0131 05:35:02.990547 139668746123008 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.2175917625427246, loss=2.820286989212036
I0131 05:35:48.683907 139668754515712 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.448853015899658, loss=2.85732364654541
I0131 05:36:34.605229 139668746123008 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.2918596267700195, loss=2.8423566818237305
I0131 05:37:20.334837 139668754515712 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.234830141067505, loss=2.809152603149414
I0131 05:38:05.864810 139668746123008 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.2667236328125, loss=2.8436388969421387
I0131 05:38:51.382959 139668754515712 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.220694065093994, loss=4.936068534851074
I0131 05:39:16.681290 139863983413056 spec.py:321] Evaluating on the training split.
I0131 05:39:26.974292 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 05:39:46.365647 139863983413056 spec.py:349] Evaluating on the test split.
I0131 05:39:48.018697 139863983413056 submission_runner.py:408] Time since start: 57499.67s, 	Step: 118457, 	{'train/accuracy': 0.7255663871765137, 'train/loss': 1.2088725566864014, 'validation/accuracy': 0.6714800000190735, 'validation/loss': 1.4509501457214355, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.084545135498047, 'test/num_examples': 10000, 'score': 53399.125903367996, 'total_duration': 57499.6692841053, 'accumulated_submission_time': 53399.125903367996, 'accumulated_eval_time': 4089.267287492752, 'accumulated_logging_time': 5.115900993347168}
I0131 05:39:48.060957 139668746123008 logging_writer.py:48] [118457] accumulated_eval_time=4089.267287, accumulated_logging_time=5.115901, accumulated_submission_time=53399.125903, global_step=118457, preemption_count=0, score=53399.125903, test/accuracy=0.546600, test/loss=2.084545, test/num_examples=10000, total_duration=57499.669284, train/accuracy=0.725566, train/loss=1.208873, validation/accuracy=0.671480, validation/loss=1.450950, validation/num_examples=50000
I0131 05:40:05.627923 139668754515712 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.9001538753509521, loss=4.076723098754883
I0131 05:40:48.856436 139668746123008 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.4066343307495117, loss=2.842811107635498
I0131 05:41:34.784492 139668754515712 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.1038951873779297, loss=4.381258964538574
I0131 05:42:20.757792 139668746123008 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.180530309677124, loss=2.836730718612671
I0131 05:43:06.765141 139668754515712 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.101961612701416, loss=3.346268892288208
I0131 05:43:52.464330 139668746123008 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.5333142280578613, loss=2.7713534832000732
I0131 05:44:38.344046 139668754515712 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.2205491065979004, loss=2.871964454650879
I0131 05:45:24.109689 139668746123008 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.374955177307129, loss=2.829068660736084
I0131 05:46:09.966401 139668754515712 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.43691349029541, loss=2.841921329498291
I0131 05:46:48.210052 139863983413056 spec.py:321] Evaluating on the training split.
I0131 05:46:58.858600 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 05:47:19.078748 139863983413056 spec.py:349] Evaluating on the test split.
I0131 05:47:20.739262 139863983413056 submission_runner.py:408] Time since start: 57952.39s, 	Step: 119385, 	{'train/accuracy': 0.7369726300239563, 'train/loss': 1.1480770111083984, 'validation/accuracy': 0.6777399778366089, 'validation/loss': 1.4084073305130005, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.038119316101074, 'test/num_examples': 10000, 'score': 53819.215997457504, 'total_duration': 57952.38986158371, 'accumulated_submission_time': 53819.215997457504, 'accumulated_eval_time': 4121.7964906692505, 'accumulated_logging_time': 5.169644355773926}
I0131 05:47:20.771900 139668746123008 logging_writer.py:48] [119385] accumulated_eval_time=4121.796491, accumulated_logging_time=5.169644, accumulated_submission_time=53819.215997, global_step=119385, preemption_count=0, score=53819.215997, test/accuracy=0.551000, test/loss=2.038119, test/num_examples=10000, total_duration=57952.389862, train/accuracy=0.736973, train/loss=1.148077, validation/accuracy=0.677740, validation/loss=1.408407, validation/num_examples=50000
I0131 05:47:27.165111 139668754515712 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.300143241882324, loss=3.096837043762207
I0131 05:48:08.829155 139668754515712 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.0803043842315674, loss=3.1355481147766113
I0131 05:48:54.436576 139668746123008 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.2473509311676025, loss=3.2057037353515625
I0131 05:49:40.123157 139668754515712 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.1316208839416504, loss=2.8828463554382324
I0131 05:50:26.034726 139668746123008 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.197620153427124, loss=3.911573648452759
I0131 05:51:11.553200 139668754515712 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.182934045791626, loss=4.714372158050537
I0131 05:51:57.031828 139668746123008 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.3773086071014404, loss=2.7683722972869873
I0131 05:52:43.045160 139668754515712 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.002953290939331, loss=3.922531843185425
I0131 05:53:28.491895 139668746123008 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.3381242752075195, loss=4.418368339538574
I0131 05:54:14.217712 139668754515712 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.4608821868896484, loss=4.011880874633789
I0131 05:54:20.858949 139863983413056 spec.py:321] Evaluating on the training split.
I0131 05:54:30.987123 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 05:54:53.107908 139863983413056 spec.py:349] Evaluating on the test split.
I0131 05:54:54.737654 139863983413056 submission_runner.py:408] Time since start: 58406.39s, 	Step: 120316, 	{'train/accuracy': 0.7320312261581421, 'train/loss': 1.1968371868133545, 'validation/accuracy': 0.6744199991226196, 'validation/loss': 1.4489213228225708, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 2.078464984893799, 'test/num_examples': 10000, 'score': 54239.2432115078, 'total_duration': 58406.38825464249, 'accumulated_submission_time': 54239.2432115078, 'accumulated_eval_time': 4155.675201416016, 'accumulated_logging_time': 5.214017868041992}
I0131 05:54:54.769823 139668746123008 logging_writer.py:48] [120316] accumulated_eval_time=4155.675201, accumulated_logging_time=5.214018, accumulated_submission_time=54239.243212, global_step=120316, preemption_count=0, score=54239.243212, test/accuracy=0.549900, test/loss=2.078465, test/num_examples=10000, total_duration=58406.388255, train/accuracy=0.732031, train/loss=1.196837, validation/accuracy=0.674420, validation/loss=1.448921, validation/num_examples=50000
I0131 05:55:28.721377 139668754515712 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.53419828414917, loss=2.855194330215454
I0131 05:56:13.771430 139668746123008 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.2309324741363525, loss=2.792872428894043
I0131 05:56:59.394023 139668754515712 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.790569305419922, loss=3.8681678771972656
I0131 05:57:45.414223 139668746123008 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.3205180168151855, loss=2.9286911487579346
I0131 05:58:31.016546 139668754515712 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.3170511722564697, loss=2.811230182647705
I0131 05:59:16.671772 139668746123008 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.3715054988861084, loss=2.894087791442871
I0131 06:00:02.698210 139668754515712 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.403613805770874, loss=2.7090678215026855
I0131 06:00:48.515293 139668746123008 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.253429412841797, loss=2.8398680686950684
I0131 06:01:34.405995 139668754515712 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.3372280597686768, loss=2.6893019676208496
I0131 06:01:55.109816 139863983413056 spec.py:321] Evaluating on the training split.
I0131 06:02:05.922677 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 06:02:24.405220 139863983413056 spec.py:349] Evaluating on the test split.
I0131 06:02:26.057503 139863983413056 submission_runner.py:408] Time since start: 58857.71s, 	Step: 121247, 	{'train/accuracy': 0.7334765195846558, 'train/loss': 1.1736871004104614, 'validation/accuracy': 0.6791200041770935, 'validation/loss': 1.4089908599853516, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 2.0281074047088623, 'test/num_examples': 10000, 'score': 54659.526497364044, 'total_duration': 58857.70808959007, 'accumulated_submission_time': 54659.526497364044, 'accumulated_eval_time': 4186.622891664505, 'accumulated_logging_time': 5.255097150802612}
I0131 06:02:26.101073 139668746123008 logging_writer.py:48] [121247] accumulated_eval_time=4186.622892, accumulated_logging_time=5.255097, accumulated_submission_time=54659.526497, global_step=121247, preemption_count=0, score=54659.526497, test/accuracy=0.556900, test/loss=2.028107, test/num_examples=10000, total_duration=58857.708090, train/accuracy=0.733477, train/loss=1.173687, validation/accuracy=0.679120, validation/loss=1.408991, validation/num_examples=50000
I0131 06:02:47.670624 139668754515712 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.141094446182251, loss=3.9986321926116943
I0131 06:03:32.335596 139668746123008 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.175611734390259, loss=4.219656944274902
I0131 06:04:17.723782 139668754515712 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.269301652908325, loss=2.9098262786865234
I0131 06:05:03.622531 139668746123008 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.2214226722717285, loss=4.267249584197998
I0131 06:05:49.390898 139668754515712 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.3367152214050293, loss=2.9353342056274414
I0131 06:06:35.159031 139668746123008 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.692159414291382, loss=2.7361974716186523
I0131 06:07:20.987492 139668754515712 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.3039610385894775, loss=4.2161545753479
I0131 06:08:06.818878 139668746123008 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.435669422149658, loss=2.724768877029419
I0131 06:08:52.434907 139668754515712 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.404952049255371, loss=2.8638439178466797
I0131 06:09:26.417502 139863983413056 spec.py:321] Evaluating on the training split.
I0131 06:09:37.008974 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 06:09:56.320919 139863983413056 spec.py:349] Evaluating on the test split.
I0131 06:09:57.968629 139863983413056 submission_runner.py:408] Time since start: 59309.62s, 	Step: 122176, 	{'train/accuracy': 0.7381835579872131, 'train/loss': 1.1373696327209473, 'validation/accuracy': 0.6790800094604492, 'validation/loss': 1.3966187238693237, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.0180749893188477, 'test/num_examples': 10000, 'score': 55079.78484630585, 'total_duration': 59309.61920070648, 'accumulated_submission_time': 55079.78484630585, 'accumulated_eval_time': 4218.173988342285, 'accumulated_logging_time': 5.308438777923584}
I0131 06:09:58.005093 139668746123008 logging_writer.py:48] [122176] accumulated_eval_time=4218.173988, accumulated_logging_time=5.308439, accumulated_submission_time=55079.784846, global_step=122176, preemption_count=0, score=55079.784846, test/accuracy=0.554700, test/loss=2.018075, test/num_examples=10000, total_duration=59309.619201, train/accuracy=0.738184, train/loss=1.137370, validation/accuracy=0.679080, validation/loss=1.396619, validation/num_examples=50000
I0131 06:10:08.004237 139668754515712 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.1848723888397217, loss=4.886269569396973
I0131 06:10:49.923729 139668746123008 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.3316266536712646, loss=3.261260509490967
I0131 06:11:35.922080 139668754515712 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.222604274749756, loss=4.914971351623535
I0131 06:12:21.704131 139668746123008 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.305455207824707, loss=2.693209171295166
I0131 06:13:07.702393 139668754515712 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.372697353363037, loss=4.9794020652771
I0131 06:13:53.340902 139668746123008 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.5306758880615234, loss=3.5069007873535156
I0131 06:14:39.124338 139668754515712 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.4242055416107178, loss=4.998726844787598
I0131 06:15:24.812072 139668746123008 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.3192055225372314, loss=3.0309743881225586
I0131 06:16:10.465933 139668754515712 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.6895148754119873, loss=2.7861251831054688
I0131 06:16:56.027841 139668746123008 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.485748052597046, loss=2.7334816455841064
I0131 06:16:58.308461 139863983413056 spec.py:321] Evaluating on the training split.
I0131 06:17:08.600818 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 06:17:26.953674 139863983413056 spec.py:349] Evaluating on the test split.
I0131 06:17:28.609260 139863983413056 submission_runner.py:408] Time since start: 59760.26s, 	Step: 123107, 	{'train/accuracy': 0.7488671541213989, 'train/loss': 1.1549774408340454, 'validation/accuracy': 0.681439995765686, 'validation/loss': 1.463075876235962, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 2.0738844871520996, 'test/num_examples': 10000, 'score': 55500.03019499779, 'total_duration': 59760.25983929634, 'accumulated_submission_time': 55500.03019499779, 'accumulated_eval_time': 4248.474764108658, 'accumulated_logging_time': 5.354437828063965}
I0131 06:17:28.647215 139668754515712 logging_writer.py:48] [123107] accumulated_eval_time=4248.474764, accumulated_logging_time=5.354438, accumulated_submission_time=55500.030195, global_step=123107, preemption_count=0, score=55500.030195, test/accuracy=0.559900, test/loss=2.073884, test/num_examples=10000, total_duration=59760.259839, train/accuracy=0.748867, train/loss=1.154977, validation/accuracy=0.681440, validation/loss=1.463076, validation/num_examples=50000
I0131 06:18:06.974612 139668746123008 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.4942517280578613, loss=4.726225852966309
I0131 06:18:52.422169 139668754515712 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.466315746307373, loss=2.8879802227020264
I0131 06:19:38.277217 139668746123008 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.1721038818359375, loss=3.5919597148895264
I0131 06:20:24.593319 139668754515712 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.303821086883545, loss=4.057112693786621
I0131 06:21:09.906408 139668746123008 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.450575351715088, loss=2.79388689994812
I0131 06:21:55.593346 139668754515712 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.302823781967163, loss=3.238405704498291
I0131 06:22:41.440440 139668746123008 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.2232441902160645, loss=4.932774066925049
I0131 06:23:27.679526 139668754515712 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.2265186309814453, loss=2.855708599090576
I0131 06:24:13.282235 139668746123008 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.6948461532592773, loss=4.443634510040283
I0131 06:24:28.869440 139863983413056 spec.py:321] Evaluating on the training split.
I0131 06:24:39.418344 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 06:24:57.748152 139863983413056 spec.py:349] Evaluating on the test split.
I0131 06:24:59.386534 139863983413056 submission_runner.py:408] Time since start: 60211.04s, 	Step: 124036, 	{'train/accuracy': 0.7421875, 'train/loss': 1.1317152976989746, 'validation/accuracy': 0.6843999624252319, 'validation/loss': 1.3768173456192017, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.0066962242126465, 'test/num_examples': 10000, 'score': 55920.1936044693, 'total_duration': 60211.03713226318, 'accumulated_submission_time': 55920.1936044693, 'accumulated_eval_time': 4278.991844892502, 'accumulated_logging_time': 5.403183937072754}
I0131 06:24:59.420845 139668754515712 logging_writer.py:48] [124036] accumulated_eval_time=4278.991845, accumulated_logging_time=5.403184, accumulated_submission_time=55920.193604, global_step=124036, preemption_count=0, score=55920.193604, test/accuracy=0.557300, test/loss=2.006696, test/num_examples=10000, total_duration=60211.037132, train/accuracy=0.742188, train/loss=1.131715, validation/accuracy=0.684400, validation/loss=1.376817, validation/num_examples=50000
I0131 06:25:25.404101 139668746123008 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.460812568664551, loss=4.25606107711792
I0131 06:26:09.487439 139668754515712 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.4612786769866943, loss=3.0139126777648926
I0131 06:26:55.259030 139668746123008 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.413367509841919, loss=4.862056732177734
I0131 06:27:41.325431 139668754515712 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.5518293380737305, loss=2.800218343734741
I0131 06:28:27.145772 139668746123008 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.447216272354126, loss=2.7276523113250732
I0131 06:29:13.118356 139668754515712 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.0762076377868652, loss=3.3261520862579346
I0131 06:29:59.061034 139668746123008 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.3517072200775146, loss=2.800919532775879
I0131 06:30:44.884483 139668754515712 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.3720290660858154, loss=2.7423207759857178
I0131 06:31:30.626153 139668746123008 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.480308771133423, loss=2.7257676124572754
I0131 06:31:59.517731 139863983413056 spec.py:321] Evaluating on the training split.
I0131 06:32:09.914915 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 06:32:30.825917 139863983413056 spec.py:349] Evaluating on the test split.
I0131 06:32:32.468944 139863983413056 submission_runner.py:408] Time since start: 60664.12s, 	Step: 124965, 	{'train/accuracy': 0.742968738079071, 'train/loss': 1.1672155857086182, 'validation/accuracy': 0.6843799948692322, 'validation/loss': 1.420559048652649, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.050241708755493, 'test/num_examples': 10000, 'score': 56340.23328781128, 'total_duration': 60664.11954545975, 'accumulated_submission_time': 56340.23328781128, 'accumulated_eval_time': 4311.943066358566, 'accumulated_logging_time': 5.446610689163208}
I0131 06:32:32.502218 139668754515712 logging_writer.py:48] [124965] accumulated_eval_time=4311.943066, accumulated_logging_time=5.446611, accumulated_submission_time=56340.233288, global_step=124965, preemption_count=0, score=56340.233288, test/accuracy=0.558600, test/loss=2.050242, test/num_examples=10000, total_duration=60664.119545, train/accuracy=0.742969, train/loss=1.167216, validation/accuracy=0.684380, validation/loss=1.420559, validation/num_examples=50000
I0131 06:32:46.885704 139668746123008 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.2862343788146973, loss=3.5281057357788086
I0131 06:33:29.471546 139668754515712 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.2994868755340576, loss=2.7169179916381836
I0131 06:34:15.597568 139668746123008 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.5157079696655273, loss=2.7329440116882324
I0131 06:35:01.774097 139668754515712 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.525646209716797, loss=2.7017126083374023
I0131 06:35:48.099380 139668746123008 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.615875005722046, loss=3.1481549739837646
I0131 06:36:34.076795 139668754515712 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.4677224159240723, loss=4.592418670654297
I0131 06:37:20.055711 139668746123008 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.4295241832733154, loss=2.708415985107422
I0131 06:38:05.846213 139668754515712 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.42052960395813, loss=3.9676175117492676
I0131 06:38:51.486468 139668746123008 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.608140468597412, loss=2.862795352935791
I0131 06:39:32.788845 139863983413056 spec.py:321] Evaluating on the training split.
I0131 06:39:43.089111 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 06:40:04.341781 139863983413056 spec.py:349] Evaluating on the test split.
I0131 06:40:05.983798 139863983413056 submission_runner.py:408] Time since start: 61117.63s, 	Step: 125892, 	{'train/accuracy': 0.7481640577316284, 'train/loss': 1.098036289215088, 'validation/accuracy': 0.6840199828147888, 'validation/loss': 1.375110149383545, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.99488365650177, 'test/num_examples': 10000, 'score': 56760.46367549896, 'total_duration': 61117.63439536095, 'accumulated_submission_time': 56760.46367549896, 'accumulated_eval_time': 4345.138010501862, 'accumulated_logging_time': 5.488474130630493}
I0131 06:40:06.021357 139668754515712 logging_writer.py:48] [125892] accumulated_eval_time=4345.138011, accumulated_logging_time=5.488474, accumulated_submission_time=56760.463675, global_step=125892, preemption_count=0, score=56760.463675, test/accuracy=0.563800, test/loss=1.994884, test/num_examples=10000, total_duration=61117.634395, train/accuracy=0.748164, train/loss=1.098036, validation/accuracy=0.684020, validation/loss=1.375110, validation/num_examples=50000
I0131 06:40:09.614790 139668746123008 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.206341505050659, loss=3.4907279014587402
I0131 06:40:50.918372 139668754515712 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.218543291091919, loss=4.055125713348389
I0131 06:41:36.462654 139668746123008 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.50754714012146, loss=2.7672529220581055
I0131 06:42:22.321362 139668754515712 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.4796719551086426, loss=2.7850754261016846
I0131 06:43:08.129558 139668746123008 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.3249430656433105, loss=2.792083263397217
I0131 06:43:53.923627 139668754515712 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.3706517219543457, loss=4.507047176361084
I0131 06:44:39.752219 139668746123008 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.3003134727478027, loss=3.2320308685302734
I0131 06:45:25.522523 139668754515712 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.64266300201416, loss=2.6800684928894043
I0131 06:46:11.206099 139668746123008 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.3210208415985107, loss=3.568286418914795
I0131 06:46:56.820878 139668754515712 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.541154623031616, loss=2.7520060539245605
I0131 06:47:06.184916 139863983413056 spec.py:321] Evaluating on the training split.
I0131 06:47:16.828934 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 06:47:39.172897 139863983413056 spec.py:349] Evaluating on the test split.
I0131 06:47:40.808895 139863983413056 submission_runner.py:408] Time since start: 61572.46s, 	Step: 126822, 	{'train/accuracy': 0.7443749904632568, 'train/loss': 1.1106796264648438, 'validation/accuracy': 0.6901800036430359, 'validation/loss': 1.3542922735214233, 'validation/num_examples': 50000, 'test/accuracy': 0.5568000078201294, 'test/loss': 1.9962732791900635, 'test/num_examples': 10000, 'score': 57180.5708527565, 'total_duration': 61572.45949554443, 'accumulated_submission_time': 57180.5708527565, 'accumulated_eval_time': 4379.7619886398315, 'accumulated_logging_time': 5.534748315811157}
I0131 06:47:40.842110 139668746123008 logging_writer.py:48] [126822] accumulated_eval_time=4379.761989, accumulated_logging_time=5.534748, accumulated_submission_time=57180.570853, global_step=126822, preemption_count=0, score=57180.570853, test/accuracy=0.556800, test/loss=1.996273, test/num_examples=10000, total_duration=61572.459496, train/accuracy=0.744375, train/loss=1.110680, validation/accuracy=0.690180, validation/loss=1.354292, validation/num_examples=50000
I0131 06:48:12.404558 139668754515712 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.5151612758636475, loss=2.809297561645508
I0131 06:48:57.499730 139668746123008 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.6844823360443115, loss=2.7533512115478516
I0131 06:49:43.031140 139668754515712 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.517855644226074, loss=2.968010663986206
I0131 06:50:29.177870 139668746123008 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.6215522289276123, loss=2.765170097351074
I0131 06:51:14.600193 139668754515712 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.3542134761810303, loss=3.8426332473754883
I0131 06:52:00.594616 139668746123008 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.2460451126098633, loss=2.7453668117523193
I0131 06:52:46.278172 139668754515712 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.339672327041626, loss=3.121481418609619
I0131 06:53:32.035386 139668746123008 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.71640944480896, loss=2.819570541381836
I0131 06:54:17.676746 139668754515712 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.801866292953491, loss=2.841933250427246
I0131 06:54:40.954917 139863983413056 spec.py:321] Evaluating on the training split.
I0131 06:54:51.603261 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 06:55:13.362931 139863983413056 spec.py:349] Evaluating on the test split.
I0131 06:55:15.004120 139863983413056 submission_runner.py:408] Time since start: 62026.65s, 	Step: 127753, 	{'train/accuracy': 0.7480273246765137, 'train/loss': 1.1173925399780273, 'validation/accuracy': 0.688759982585907, 'validation/loss': 1.3697985410690308, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9796117544174194, 'test/num_examples': 10000, 'score': 57600.62677979469, 'total_duration': 62026.65469145775, 'accumulated_submission_time': 57600.62677979469, 'accumulated_eval_time': 4413.811166524887, 'accumulated_logging_time': 5.5767738819122314}
I0131 06:55:15.043175 139668746123008 logging_writer.py:48] [127753] accumulated_eval_time=4413.811167, accumulated_logging_time=5.576774, accumulated_submission_time=57600.626780, global_step=127753, preemption_count=0, score=57600.626780, test/accuracy=0.561300, test/loss=1.979612, test/num_examples=10000, total_duration=62026.654691, train/accuracy=0.748027, train/loss=1.117393, validation/accuracy=0.688760, validation/loss=1.369799, validation/num_examples=50000
I0131 06:55:34.217012 139668754515712 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.2908246517181396, loss=4.219913482666016
I0131 06:56:17.352650 139668746123008 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.343165397644043, loss=2.658646583557129
I0131 06:57:03.053349 139668754515712 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.402320146560669, loss=2.8706231117248535
I0131 06:57:48.900107 139668746123008 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.458177328109741, loss=2.913874864578247
I0131 06:58:34.646624 139668754515712 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.4190783500671387, loss=2.9526820182800293
I0131 06:59:20.297762 139668746123008 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.6410770416259766, loss=2.7931435108184814
I0131 07:00:06.416634 139668754515712 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.5866005420684814, loss=2.646496534347534
I0131 07:00:52.076363 139668746123008 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.521474599838257, loss=2.916181802749634
I0131 07:01:37.622429 139668754515712 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.6690006256103516, loss=2.8433194160461426
I0131 07:02:15.242655 139863983413056 spec.py:321] Evaluating on the training split.
I0131 07:02:25.684271 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 07:02:46.819983 139863983413056 spec.py:349] Evaluating on the test split.
I0131 07:02:48.470572 139863983413056 submission_runner.py:408] Time since start: 62480.12s, 	Step: 128684, 	{'train/accuracy': 0.75990229845047, 'train/loss': 1.0859085321426392, 'validation/accuracy': 0.6924999952316284, 'validation/loss': 1.37213134765625, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9980629682540894, 'test/num_examples': 10000, 'score': 58020.767980098724, 'total_duration': 62480.12114715576, 'accumulated_submission_time': 58020.767980098724, 'accumulated_eval_time': 4447.039078474045, 'accumulated_logging_time': 5.6262922286987305}
I0131 07:02:48.518858 139668746123008 logging_writer.py:48] [128684] accumulated_eval_time=4447.039078, accumulated_logging_time=5.626292, accumulated_submission_time=58020.767980, global_step=128684, preemption_count=0, score=58020.767980, test/accuracy=0.566100, test/loss=1.998063, test/num_examples=10000, total_duration=62480.121147, train/accuracy=0.759902, train/loss=1.085909, validation/accuracy=0.692500, validation/loss=1.372131, validation/num_examples=50000
I0131 07:02:55.307760 139668754515712 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.6382944583892822, loss=2.9530527591705322
I0131 07:03:36.949291 139668746123008 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.5821521282196045, loss=2.7604711055755615
I0131 07:04:23.050463 139668754515712 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.5402443408966064, loss=2.735635280609131
I0131 07:05:09.165298 139668746123008 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.371868371963501, loss=4.860373497009277
I0131 07:05:54.942238 139668754515712 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.398314952850342, loss=3.492609977722168
I0131 07:06:40.426017 139668746123008 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.4531893730163574, loss=4.423971176147461
I0131 07:07:26.344882 139668754515712 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.111147165298462, loss=2.9779961109161377
I0131 07:08:12.189546 139668746123008 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.130526065826416, loss=3.3149495124816895
I0131 07:08:57.676569 139668754515712 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.495604991912842, loss=2.7182369232177734
I0131 07:09:43.457510 139668746123008 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.611091375350952, loss=2.6549904346466064
I0131 07:09:48.594741 139863983413056 spec.py:321] Evaluating on the training split.
I0131 07:09:59.103425 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 07:10:20.974526 139863983413056 spec.py:349] Evaluating on the test split.
I0131 07:10:22.619915 139863983413056 submission_runner.py:408] Time since start: 62934.27s, 	Step: 129613, 	{'train/accuracy': 0.7639062404632568, 'train/loss': 1.050950288772583, 'validation/accuracy': 0.6924999952316284, 'validation/loss': 1.3538199663162231, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 1.9598753452301025, 'test/num_examples': 10000, 'score': 58440.78542947769, 'total_duration': 62934.27048492432, 'accumulated_submission_time': 58440.78542947769, 'accumulated_eval_time': 4481.064235448837, 'accumulated_logging_time': 5.68550181388855}
I0131 07:10:22.664390 139668754515712 logging_writer.py:48] [129613] accumulated_eval_time=4481.064235, accumulated_logging_time=5.685502, accumulated_submission_time=58440.785429, global_step=129613, preemption_count=0, score=58440.785429, test/accuracy=0.570900, test/loss=1.959875, test/num_examples=10000, total_duration=62934.270485, train/accuracy=0.763906, train/loss=1.050950, validation/accuracy=0.692500, validation/loss=1.353820, validation/num_examples=50000
I0131 07:10:57.839268 139668746123008 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.5849125385284424, loss=2.990262746810913
I0131 07:11:43.144796 139668754515712 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.6609058380126953, loss=4.639714241027832
I0131 07:12:28.860906 139668746123008 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.409623384475708, loss=4.564382553100586
I0131 07:13:14.900764 139668754515712 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.4008593559265137, loss=4.006259918212891
I0131 07:14:00.300826 139668746123008 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.710632085800171, loss=4.693765163421631
I0131 07:14:46.260442 139668754515712 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.5710980892181396, loss=2.7736949920654297
I0131 07:15:31.914311 139668746123008 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.601194381713867, loss=3.018157482147217
I0131 07:16:17.485510 139668754515712 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.511563539505005, loss=3.193895101547241
I0131 07:17:03.171761 139668746123008 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.5193870067596436, loss=2.7076072692871094
I0131 07:17:22.894999 139863983413056 spec.py:321] Evaluating on the training split.
I0131 07:17:33.403159 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 07:17:54.190704 139863983413056 spec.py:349] Evaluating on the test split.
I0131 07:17:55.827634 139863983413056 submission_runner.py:408] Time since start: 63387.48s, 	Step: 130545, 	{'train/accuracy': 0.7498437166213989, 'train/loss': 1.1301133632659912, 'validation/accuracy': 0.693120002746582, 'validation/loss': 1.3789575099945068, 'validation/num_examples': 50000, 'test/accuracy': 0.5597000122070312, 'test/loss': 2.01959228515625, 'test/num_examples': 10000, 'score': 58860.95683288574, 'total_duration': 63387.47819709778, 'accumulated_submission_time': 58860.95683288574, 'accumulated_eval_time': 4513.996830224991, 'accumulated_logging_time': 5.740103721618652}
I0131 07:17:55.862504 139668754515712 logging_writer.py:48] [130545] accumulated_eval_time=4513.996830, accumulated_logging_time=5.740104, accumulated_submission_time=58860.956833, global_step=130545, preemption_count=0, score=58860.956833, test/accuracy=0.559700, test/loss=2.019592, test/num_examples=10000, total_duration=63387.478197, train/accuracy=0.749844, train/loss=1.130113, validation/accuracy=0.693120, validation/loss=1.378958, validation/num_examples=50000
I0131 07:18:18.313292 139668746123008 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.5457050800323486, loss=3.518062114715576
I0131 07:19:02.017139 139668754515712 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.6053569316864014, loss=2.756135940551758
I0131 07:19:47.647154 139668746123008 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.3535256385803223, loss=3.279339551925659
I0131 07:20:33.483373 139668754515712 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.6828508377075195, loss=2.6670327186584473
I0131 07:21:19.103465 139668746123008 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.48366379737854, loss=3.7987253665924072
I0131 07:22:04.638893 139668754515712 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.706571102142334, loss=2.6983237266540527
I0131 07:22:50.253135 139668746123008 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.582294464111328, loss=3.0802013874053955
I0131 07:23:35.989378 139668754515712 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.3786115646362305, loss=3.580084800720215
I0131 07:24:21.967594 139668746123008 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.8353359699249268, loss=2.720463275909424
I0131 07:24:55.904625 139863983413056 spec.py:321] Evaluating on the training split.
I0131 07:25:06.196488 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 07:25:28.297147 139863983413056 spec.py:349] Evaluating on the test split.
I0131 07:25:29.945853 139863983413056 submission_runner.py:408] Time since start: 63841.60s, 	Step: 131476, 	{'train/accuracy': 0.756054699420929, 'train/loss': 1.0679931640625, 'validation/accuracy': 0.6955400109291077, 'validation/loss': 1.3339507579803467, 'validation/num_examples': 50000, 'test/accuracy': 0.5682000517845154, 'test/loss': 1.9795842170715332, 'test/num_examples': 10000, 'score': 59280.941180706024, 'total_duration': 63841.59644985199, 'accumulated_submission_time': 59280.941180706024, 'accumulated_eval_time': 4548.038062334061, 'accumulated_logging_time': 5.784178018569946}
I0131 07:25:29.981259 139668754515712 logging_writer.py:48] [131476] accumulated_eval_time=4548.038062, accumulated_logging_time=5.784178, accumulated_submission_time=59280.941181, global_step=131476, preemption_count=0, score=59280.941181, test/accuracy=0.568200, test/loss=1.979584, test/num_examples=10000, total_duration=63841.596450, train/accuracy=0.756055, train/loss=1.067993, validation/accuracy=0.695540, validation/loss=1.333951, validation/num_examples=50000
I0131 07:25:39.957955 139668746123008 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.4363608360290527, loss=2.930074691772461
I0131 07:26:21.938082 139668754515712 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.541940689086914, loss=2.9235916137695312
I0131 07:27:07.751991 139668746123008 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.454861879348755, loss=2.7026190757751465
I0131 07:27:53.570009 139668754515712 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.8893651962280273, loss=2.7639575004577637
I0131 07:28:39.561395 139668746123008 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.39370059967041, loss=3.7441837787628174
I0131 07:29:25.211717 139668754515712 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.6006031036376953, loss=2.604722738265991
I0131 07:30:11.176614 139668746123008 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.6711795330047607, loss=2.5328481197357178
I0131 07:30:56.717564 139668754515712 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.9202165603637695, loss=2.9985499382019043
I0131 07:31:42.368632 139668746123008 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.4719722270965576, loss=3.7571115493774414
I0131 07:32:28.811535 139668754515712 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.011359691619873, loss=2.7008681297302246
I0131 07:32:30.110250 139863983413056 spec.py:321] Evaluating on the training split.
I0131 07:32:40.733363 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 07:33:01.667128 139863983413056 spec.py:349] Evaluating on the test split.
I0131 07:33:03.310072 139863983413056 submission_runner.py:408] Time since start: 64294.96s, 	Step: 132405, 	{'train/accuracy': 0.7659375071525574, 'train/loss': 1.050911784172058, 'validation/accuracy': 0.6958400011062622, 'validation/loss': 1.3542274236679077, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 1.9746955633163452, 'test/num_examples': 10000, 'score': 59701.01261425018, 'total_duration': 64294.9606654644, 'accumulated_submission_time': 59701.01261425018, 'accumulated_eval_time': 4581.237900733948, 'accumulated_logging_time': 5.829130172729492}
I0131 07:33:03.347684 139668746123008 logging_writer.py:48] [132405] accumulated_eval_time=4581.237901, accumulated_logging_time=5.829130, accumulated_submission_time=59701.012614, global_step=132405, preemption_count=0, score=59701.012614, test/accuracy=0.567000, test/loss=1.974696, test/num_examples=10000, total_duration=64294.960665, train/accuracy=0.765938, train/loss=1.050912, validation/accuracy=0.695840, validation/loss=1.354227, validation/num_examples=50000
I0131 07:33:42.334792 139668754515712 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.5898311138153076, loss=2.8541810512542725
I0131 07:34:28.141213 139668746123008 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.7871670722961426, loss=2.730276107788086
I0131 07:35:14.329770 139668754515712 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.8833529949188232, loss=4.795323371887207
I0131 07:36:00.544768 139668746123008 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.461639404296875, loss=2.6804018020629883
I0131 07:36:46.190800 139668754515712 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.6760079860687256, loss=2.7028517723083496
I0131 07:37:31.925125 139668746123008 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.7698874473571777, loss=2.6305642127990723
I0131 07:38:17.801304 139668754515712 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.2836360931396484, loss=3.2467026710510254
I0131 07:39:03.514910 139668746123008 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.550480365753174, loss=3.4236440658569336
I0131 07:39:49.215081 139668754515712 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.9750916957855225, loss=4.6613922119140625
I0131 07:40:03.670477 139863983413056 spec.py:321] Evaluating on the training split.
I0131 07:40:14.251275 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 07:40:36.296143 139863983413056 spec.py:349] Evaluating on the test split.
I0131 07:40:37.932523 139863983413056 submission_runner.py:408] Time since start: 64749.58s, 	Step: 133333, 	{'train/accuracy': 0.7583202719688416, 'train/loss': 1.063936471939087, 'validation/accuracy': 0.7030799984931946, 'validation/loss': 1.3143028020858765, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.9410356283187866, 'test/num_examples': 10000, 'score': 60121.277137994766, 'total_duration': 64749.58311963081, 'accumulated_submission_time': 60121.277137994766, 'accumulated_eval_time': 4615.49994468689, 'accumulated_logging_time': 5.87558388710022}
I0131 07:40:37.966862 139668746123008 logging_writer.py:48] [133333] accumulated_eval_time=4615.499945, accumulated_logging_time=5.875584, accumulated_submission_time=60121.277138, global_step=133333, preemption_count=0, score=60121.277138, test/accuracy=0.572900, test/loss=1.941036, test/num_examples=10000, total_duration=64749.583120, train/accuracy=0.758320, train/loss=1.063936, validation/accuracy=0.703080, validation/loss=1.314303, validation/num_examples=50000
I0131 07:41:05.133189 139668754515712 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.811068534851074, loss=2.639997959136963
I0131 07:41:49.705245 139668746123008 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.6619811058044434, loss=2.6535489559173584
I0131 07:42:35.456450 139668754515712 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.733182907104492, loss=2.622175455093384
I0131 07:43:21.437356 139668746123008 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.683133125305176, loss=3.2622125148773193
I0131 07:44:07.291043 139668754515712 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.1825079917907715, loss=2.67829966545105
I0131 07:44:53.050075 139668746123008 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.734916925430298, loss=3.0383331775665283
I0131 07:45:38.919445 139668754515712 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.812114715576172, loss=2.691082000732422
I0131 07:46:24.526346 139668746123008 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.444058656692505, loss=2.9855713844299316
I0131 07:47:10.136631 139668754515712 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.6928508281707764, loss=3.2972350120544434
I0131 07:47:38.149173 139863983413056 spec.py:321] Evaluating on the training split.
I0131 07:47:48.505079 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 07:48:09.057986 139863983413056 spec.py:349] Evaluating on the test split.
I0131 07:48:10.704473 139863983413056 submission_runner.py:408] Time since start: 65202.36s, 	Step: 134263, 	{'train/accuracy': 0.7602148056030273, 'train/loss': 1.0567129850387573, 'validation/accuracy': 0.700939953327179, 'validation/loss': 1.3244282007217407, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.947724461555481, 'test/num_examples': 10000, 'score': 60541.402416706085, 'total_duration': 65202.35506153107, 'accumulated_submission_time': 60541.402416706085, 'accumulated_eval_time': 4648.055237054825, 'accumulated_logging_time': 5.918776750564575}
I0131 07:48:10.743204 139668746123008 logging_writer.py:48] [134263] accumulated_eval_time=4648.055237, accumulated_logging_time=5.918777, accumulated_submission_time=60541.402417, global_step=134263, preemption_count=0, score=60541.402417, test/accuracy=0.573300, test/loss=1.947724, test/num_examples=10000, total_duration=65202.355062, train/accuracy=0.760215, train/loss=1.056713, validation/accuracy=0.700940, validation/loss=1.324428, validation/num_examples=50000
I0131 07:48:25.924320 139668754515712 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.6748692989349365, loss=2.5859360694885254
I0131 07:49:09.281281 139668746123008 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.6324141025543213, loss=4.607473373413086
I0131 07:49:55.433315 139668754515712 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.1029465198516846, loss=2.939845085144043
I0131 07:50:41.850087 139668746123008 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.7328622341156006, loss=2.73679256439209
I0131 07:51:27.937716 139668754515712 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.4751627445220947, loss=3.535876512527466
I0131 07:52:14.040665 139668746123008 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.927863359451294, loss=2.7468762397766113
I0131 07:52:59.821641 139668754515712 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.0530598163604736, loss=2.6596240997314453
I0131 07:53:45.825836 139668746123008 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.397930860519409, loss=3.7216882705688477
I0131 07:54:31.799207 139668754515712 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.880678415298462, loss=2.7036733627319336
I0131 07:55:10.918832 139863983413056 spec.py:321] Evaluating on the training split.
I0131 07:55:21.691065 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 07:55:43.697199 139863983413056 spec.py:349] Evaluating on the test split.
I0131 07:55:45.337228 139863983413056 submission_runner.py:408] Time since start: 65656.99s, 	Step: 135186, 	{'train/accuracy': 0.7699609398841858, 'train/loss': 1.0183870792388916, 'validation/accuracy': 0.7020399570465088, 'validation/loss': 1.3113582134246826, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.9366137981414795, 'test/num_examples': 10000, 'score': 60961.52054858208, 'total_duration': 65656.98781871796, 'accumulated_submission_time': 60961.52054858208, 'accumulated_eval_time': 4682.473633766174, 'accumulated_logging_time': 5.967818260192871}
I0131 07:55:45.377288 139668746123008 logging_writer.py:48] [135186] accumulated_eval_time=4682.473634, accumulated_logging_time=5.967818, accumulated_submission_time=60961.520549, global_step=135186, preemption_count=0, score=60961.520549, test/accuracy=0.578100, test/loss=1.936614, test/num_examples=10000, total_duration=65656.987819, train/accuracy=0.769961, train/loss=1.018387, validation/accuracy=0.702040, validation/loss=1.311358, validation/num_examples=50000
I0131 07:55:51.368415 139668754515712 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.788680076599121, loss=4.466559886932373
I0131 07:56:32.706374 139668746123008 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.728863477706909, loss=2.6139700412750244
I0131 07:57:18.417068 139668754515712 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.4903526306152344, loss=3.7046732902526855
I0131 07:58:04.407710 139668746123008 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.865745782852173, loss=4.538633346557617
I0131 07:58:50.471277 139668754515712 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.221095561981201, loss=4.475038528442383
I0131 07:59:36.193565 139668746123008 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.135704278945923, loss=2.6680307388305664
I0131 08:00:22.232423 139668754515712 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.106826066970825, loss=2.658574104309082
I0131 08:01:07.754398 139668746123008 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.6608588695526123, loss=2.564980983734131
I0131 08:01:53.510622 139668754515712 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.089723587036133, loss=2.8251261711120605
I0131 08:02:39.274049 139668746123008 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.9095911979675293, loss=2.7554523944854736
I0131 08:02:45.712568 139863983413056 spec.py:321] Evaluating on the training split.
I0131 08:02:55.885613 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 08:03:17.618547 139863983413056 spec.py:349] Evaluating on the test split.
I0131 08:03:19.261388 139863983413056 submission_runner.py:408] Time since start: 66110.91s, 	Step: 136116, 	{'train/accuracy': 0.768359363079071, 'train/loss': 1.0378899574279785, 'validation/accuracy': 0.705839991569519, 'validation/loss': 1.3026723861694336, 'validation/num_examples': 50000, 'test/accuracy': 0.5782000422477722, 'test/loss': 1.9222419261932373, 'test/num_examples': 10000, 'score': 61381.798147916794, 'total_duration': 66110.9119849205, 'accumulated_submission_time': 61381.798147916794, 'accumulated_eval_time': 4716.022467851639, 'accumulated_logging_time': 6.017577886581421}
I0131 08:03:19.299529 139668754515712 logging_writer.py:48] [136116] accumulated_eval_time=4716.022468, accumulated_logging_time=6.017578, accumulated_submission_time=61381.798148, global_step=136116, preemption_count=0, score=61381.798148, test/accuracy=0.578200, test/loss=1.922242, test/num_examples=10000, total_duration=66110.911985, train/accuracy=0.768359, train/loss=1.037890, validation/accuracy=0.705840, validation/loss=1.302672, validation/num_examples=50000
I0131 08:03:53.259724 139668746123008 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.859408378601074, loss=2.644375801086426
I0131 08:04:38.662862 139668754515712 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.636667251586914, loss=3.0503737926483154
I0131 08:05:24.770514 139668746123008 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.648343563079834, loss=3.1490156650543213
I0131 08:06:10.752812 139668754515712 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.9103026390075684, loss=2.5978000164031982
I0131 08:06:56.559377 139668746123008 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.799492120742798, loss=2.58609676361084
I0131 08:07:42.178691 139668754515712 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.7261242866516113, loss=2.6214685440063477
I0131 08:08:28.098679 139668746123008 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.649160385131836, loss=3.0721004009246826
I0131 08:09:13.803600 139668754515712 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.9348983764648438, loss=2.6333096027374268
I0131 08:09:59.566258 139668746123008 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.0082600116729736, loss=2.6104116439819336
I0131 08:10:19.492404 139863983413056 spec.py:321] Evaluating on the training split.
I0131 08:10:29.776373 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 08:10:48.171461 139863983413056 spec.py:349] Evaluating on the test split.
I0131 08:10:49.828228 139863983413056 submission_runner.py:408] Time since start: 66561.48s, 	Step: 137045, 	{'train/accuracy': 0.7707226276397705, 'train/loss': 0.9996986985206604, 'validation/accuracy': 0.7090199589729309, 'validation/loss': 1.2752629518508911, 'validation/num_examples': 50000, 'test/accuracy': 0.5843000411987305, 'test/loss': 1.8952151536941528, 'test/num_examples': 10000, 'score': 61801.93469142914, 'total_duration': 66561.47880458832, 'accumulated_submission_time': 61801.93469142914, 'accumulated_eval_time': 4746.358287096024, 'accumulated_logging_time': 6.064196825027466}
I0131 08:10:49.872376 139668754515712 logging_writer.py:48] [137045] accumulated_eval_time=4746.358287, accumulated_logging_time=6.064197, accumulated_submission_time=61801.934691, global_step=137045, preemption_count=0, score=61801.934691, test/accuracy=0.584300, test/loss=1.895215, test/num_examples=10000, total_duration=66561.478805, train/accuracy=0.770723, train/loss=0.999699, validation/accuracy=0.709020, validation/loss=1.275263, validation/num_examples=50000
I0131 08:11:12.268670 139668746123008 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.873724937438965, loss=2.9187145233154297
I0131 08:11:56.830662 139668754515712 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.9349782466888428, loss=2.765162944793701
I0131 08:12:42.450772 139668746123008 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.1735999584198, loss=4.648104190826416
I0131 08:13:28.257106 139668754515712 logging_writer.py:48] [137400] global_step=137400, grad_norm=2.615783929824829, loss=3.896061420440674
I0131 08:14:14.096072 139668746123008 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.1114087104797363, loss=2.5726380348205566
I0131 08:14:59.589769 139668754515712 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.0787155628204346, loss=2.841435194015503
I0131 08:15:45.685237 139668746123008 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.7229206562042236, loss=3.708103656768799
I0131 08:16:31.270953 139668754515712 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.7354440689086914, loss=2.964236259460449
I0131 08:17:16.955497 139668746123008 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.7984619140625, loss=2.934483766555786
I0131 08:17:50.265368 139863983413056 spec.py:321] Evaluating on the training split.
I0131 08:18:00.592328 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 08:18:19.501738 139863983413056 spec.py:349] Evaluating on the test split.
I0131 08:18:21.150278 139863983413056 submission_runner.py:408] Time since start: 67012.80s, 	Step: 137975, 	{'train/accuracy': 0.7759569883346558, 'train/loss': 0.9817464351654053, 'validation/accuracy': 0.708899974822998, 'validation/loss': 1.267372965812683, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.8926504850387573, 'test/num_examples': 10000, 'score': 62222.269728422165, 'total_duration': 67012.80084323883, 'accumulated_submission_time': 62222.269728422165, 'accumulated_eval_time': 4777.243156194687, 'accumulated_logging_time': 6.118428707122803}
I0131 08:18:21.198986 139668754515712 logging_writer.py:48] [137975] accumulated_eval_time=4777.243156, accumulated_logging_time=6.118429, accumulated_submission_time=62222.269728, global_step=137975, preemption_count=0, score=62222.269728, test/accuracy=0.582100, test/loss=1.892650, test/num_examples=10000, total_duration=67012.800843, train/accuracy=0.775957, train/loss=0.981746, validation/accuracy=0.708900, validation/loss=1.267373, validation/num_examples=50000
I0131 08:18:31.716110 139668746123008 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.7353017330169678, loss=2.7837486267089844
I0131 08:19:14.009031 139668754515712 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.8255228996276855, loss=3.473712921142578
I0131 08:19:59.688052 139668746123008 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.8611810207366943, loss=2.6028778553009033
I0131 08:20:45.485804 139668754515712 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.111236572265625, loss=2.632751941680908
I0131 08:21:31.052973 139668746123008 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.8321774005889893, loss=3.0638198852539062
I0131 08:22:16.875649 139668754515712 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.7533957958221436, loss=3.440579891204834
I0131 08:23:02.460087 139668746123008 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.0751779079437256, loss=2.556967258453369
I0131 08:23:47.880939 139668754515712 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.246479034423828, loss=4.470737457275391
I0131 08:24:33.653504 139668746123008 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.965630292892456, loss=2.653672218322754
I0131 08:25:19.551030 139668754515712 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.2881808280944824, loss=4.774476528167725
I0131 08:25:21.551560 139863983413056 spec.py:321] Evaluating on the training split.
I0131 08:25:31.687141 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 08:25:53.344203 139863983413056 spec.py:349] Evaluating on the test split.
I0131 08:25:54.980248 139863983413056 submission_runner.py:408] Time since start: 67466.63s, 	Step: 138906, 	{'train/accuracy': 0.7888085842132568, 'train/loss': 0.9418398141860962, 'validation/accuracy': 0.7098599672317505, 'validation/loss': 1.2711541652679443, 'validation/num_examples': 50000, 'test/accuracy': 0.5886000394821167, 'test/loss': 1.8847719430923462, 'test/num_examples': 10000, 'score': 62642.559386491776, 'total_duration': 67466.630849123, 'accumulated_submission_time': 62642.559386491776, 'accumulated_eval_time': 4810.671858549118, 'accumulated_logging_time': 6.181352853775024}
I0131 08:25:55.015835 139668746123008 logging_writer.py:48] [138906] accumulated_eval_time=4810.671859, accumulated_logging_time=6.181353, accumulated_submission_time=62642.559386, global_step=138906, preemption_count=0, score=62642.559386, test/accuracy=0.588600, test/loss=1.884772, test/num_examples=10000, total_duration=67466.630849, train/accuracy=0.788809, train/loss=0.941840, validation/accuracy=0.709860, validation/loss=1.271154, validation/num_examples=50000
I0131 08:26:32.956753 139668754515712 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.2091493606567383, loss=4.71451473236084
I0131 08:27:18.247814 139668746123008 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.9876620769500732, loss=2.578127384185791
I0131 08:28:04.107342 139668754515712 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.9386332035064697, loss=2.499157667160034
I0131 08:28:49.508411 139668746123008 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.1340386867523193, loss=2.646773338317871
I0131 08:29:35.026469 139668754515712 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.18733549118042, loss=2.6710288524627686
I0131 08:30:20.937480 139668746123008 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.8003978729248047, loss=2.51242995262146
I0131 08:31:06.620773 139668754515712 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.9532554149627686, loss=2.5313210487365723
I0131 08:31:52.090276 139668746123008 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.087803602218628, loss=2.6414146423339844
I0131 08:32:37.819007 139668754515712 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.1091623306274414, loss=4.422541618347168
I0131 08:32:55.104541 139863983413056 spec.py:321] Evaluating on the training split.
I0131 08:33:05.200492 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 08:33:26.960695 139863983413056 spec.py:349] Evaluating on the test split.
I0131 08:33:28.612507 139863983413056 submission_runner.py:408] Time since start: 67920.26s, 	Step: 139840, 	{'train/accuracy': 0.770312488079071, 'train/loss': 0.9991475343704224, 'validation/accuracy': 0.7108599543571472, 'validation/loss': 1.2587929964065552, 'validation/num_examples': 50000, 'test/accuracy': 0.5869000554084778, 'test/loss': 1.8614424467086792, 'test/num_examples': 10000, 'score': 63062.58865451813, 'total_duration': 67920.26310777664, 'accumulated_submission_time': 63062.58865451813, 'accumulated_eval_time': 4844.179829597473, 'accumulated_logging_time': 6.227135896682739}
I0131 08:33:28.648584 139668746123008 logging_writer.py:48] [139840] accumulated_eval_time=4844.179830, accumulated_logging_time=6.227136, accumulated_submission_time=63062.588655, global_step=139840, preemption_count=0, score=63062.588655, test/accuracy=0.586900, test/loss=1.861442, test/num_examples=10000, total_duration=67920.263108, train/accuracy=0.770312, train/loss=0.999148, validation/accuracy=0.710860, validation/loss=1.258793, validation/num_examples=50000
I0131 08:33:52.989436 139668754515712 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.872347354888916, loss=3.783384323120117
I0131 08:34:37.001784 139668746123008 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.058622360229492, loss=2.638566732406616
I0131 08:35:22.831694 139668754515712 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.8668603897094727, loss=3.819766044616699
I0131 08:36:09.062376 139668746123008 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.1091017723083496, loss=2.713134527206421
I0131 08:36:54.593999 139668754515712 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.7889950275421143, loss=2.7082679271698
I0131 08:37:40.184361 139668746123008 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.0668246746063232, loss=4.685710906982422
I0131 08:38:25.746008 139668754515712 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.1740505695343018, loss=2.7876524925231934
I0131 08:39:11.414109 139668746123008 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.936310291290283, loss=3.9065983295440674
I0131 08:39:57.225945 139668754515712 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.2742743492126465, loss=4.69147253036499
I0131 08:40:28.894338 139863983413056 spec.py:321] Evaluating on the training split.
I0131 08:40:39.254961 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 08:40:58.965542 139863983413056 spec.py:349] Evaluating on the test split.
I0131 08:41:00.606073 139863983413056 submission_runner.py:408] Time since start: 68372.26s, 	Step: 140771, 	{'train/accuracy': 0.7780663967132568, 'train/loss': 0.9655563831329346, 'validation/accuracy': 0.7139399647712708, 'validation/loss': 1.2472407817840576, 'validation/num_examples': 50000, 'test/accuracy': 0.5897000432014465, 'test/loss': 1.8567043542861938, 'test/num_examples': 10000, 'score': 63482.7752828598, 'total_duration': 68372.2566754818, 'accumulated_submission_time': 63482.7752828598, 'accumulated_eval_time': 4875.891560316086, 'accumulated_logging_time': 6.273644685745239}
I0131 08:41:00.645767 139668746123008 logging_writer.py:48] [140771] accumulated_eval_time=4875.891560, accumulated_logging_time=6.273645, accumulated_submission_time=63482.775283, global_step=140771, preemption_count=0, score=63482.775283, test/accuracy=0.589700, test/loss=1.856704, test/num_examples=10000, total_duration=68372.256675, train/accuracy=0.778066, train/loss=0.965556, validation/accuracy=0.713940, validation/loss=1.247241, validation/num_examples=50000
I0131 08:41:12.614235 139668754515712 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.9794809818267822, loss=2.7770652770996094
I0131 08:41:54.745236 139668746123008 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.0563039779663086, loss=2.6362500190734863
I0131 08:42:40.463649 139668754515712 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.3308961391448975, loss=2.6818687915802
I0131 08:43:26.157509 139668746123008 logging_writer.py:48] [141100] global_step=141100, grad_norm=2.9195168018341064, loss=2.660996913909912
I0131 08:44:12.056756 139668754515712 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.683239698410034, loss=3.041947364807129
I0131 08:44:57.515591 139668746123008 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.960371732711792, loss=2.582190990447998
I0131 08:45:43.525631 139668754515712 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.4942238330841064, loss=4.650073051452637
I0131 08:46:29.521391 139668746123008 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.1969826221466064, loss=2.631901502609253
I0131 08:47:15.472105 139668754515712 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.908918857574463, loss=2.4954705238342285
I0131 08:48:01.147668 139668746123008 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.22166109085083, loss=2.7096776962280273
I0131 08:48:01.161697 139863983413056 spec.py:321] Evaluating on the training split.
I0131 08:48:11.410867 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 08:48:27.643822 139863983413056 spec.py:349] Evaluating on the test split.
I0131 08:48:29.302934 139863983413056 submission_runner.py:408] Time since start: 68820.95s, 	Step: 141701, 	{'train/accuracy': 0.7883398532867432, 'train/loss': 0.9381603598594666, 'validation/accuracy': 0.7156800031661987, 'validation/loss': 1.2484104633331299, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 1.8638124465942383, 'test/num_examples': 10000, 'score': 63903.234132528305, 'total_duration': 68820.95351719856, 'accumulated_submission_time': 63903.234132528305, 'accumulated_eval_time': 4904.032790899277, 'accumulated_logging_time': 6.321924686431885}
I0131 08:48:29.349584 139668754515712 logging_writer.py:48] [141701] accumulated_eval_time=4904.032791, accumulated_logging_time=6.321925, accumulated_submission_time=63903.234133, global_step=141701, preemption_count=0, score=63903.234133, test/accuracy=0.588200, test/loss=1.863812, test/num_examples=10000, total_duration=68820.953517, train/accuracy=0.788340, train/loss=0.938160, validation/accuracy=0.715680, validation/loss=1.248410, validation/num_examples=50000
I0131 08:49:11.196954 139668746123008 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.0179994106292725, loss=2.5975444316864014
I0131 08:49:56.917424 139668754515712 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.1691157817840576, loss=2.5951950550079346
I0131 08:50:42.713652 139668746123008 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.173968553543091, loss=2.6287546157836914
I0131 08:51:28.358534 139668754515712 logging_writer.py:48] [142100] global_step=142100, grad_norm=2.783961772918701, loss=3.405202865600586
I0131 08:52:14.003170 139668746123008 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.5657734870910645, loss=4.672475337982178
I0131 08:52:59.832431 139668754515712 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.861636161804199, loss=2.854314088821411
I0131 08:53:45.649620 139668746123008 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.984818696975708, loss=2.5831079483032227
I0131 08:54:31.334800 139668754515712 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.3421881198883057, loss=4.184171676635742
I0131 08:55:17.080429 139668746123008 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.7594244480133057, loss=4.468339920043945
I0131 08:55:29.674284 139863983413056 spec.py:321] Evaluating on the training split.
I0131 08:55:40.241988 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 08:55:59.979186 139863983413056 spec.py:349] Evaluating on the test split.
I0131 08:56:01.616606 139863983413056 submission_runner.py:408] Time since start: 69273.27s, 	Step: 142629, 	{'train/accuracy': 0.7808007597923279, 'train/loss': 0.9934661388397217, 'validation/accuracy': 0.7153599858283997, 'validation/loss': 1.2737171649932861, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.8806276321411133, 'test/num_examples': 10000, 'score': 64323.499345541, 'total_duration': 69273.26719760895, 'accumulated_submission_time': 64323.499345541, 'accumulated_eval_time': 4935.975115537643, 'accumulated_logging_time': 6.379689931869507}
I0131 08:56:01.654674 139668754515712 logging_writer.py:48] [142629] accumulated_eval_time=4935.975116, accumulated_logging_time=6.379690, accumulated_submission_time=64323.499346, global_step=142629, preemption_count=0, score=64323.499346, test/accuracy=0.592400, test/loss=1.880628, test/num_examples=10000, total_duration=69273.267198, train/accuracy=0.780801, train/loss=0.993466, validation/accuracy=0.715360, validation/loss=1.273717, validation/num_examples=50000
I0131 08:56:30.407490 139668746123008 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.90861439704895, loss=2.9599790573120117
I0131 08:57:14.859729 139668754515712 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.5947089195251465, loss=4.625237464904785
I0131 08:58:00.749043 139668746123008 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.761362314224243, loss=3.152204990386963
I0131 08:58:46.486770 139668754515712 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.049967050552368, loss=2.925615072250366
I0131 08:59:32.055863 139668746123008 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.1021041870117188, loss=4.465156078338623
I0131 09:00:17.855440 139668754515712 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.006425142288208, loss=3.4625000953674316
I0131 09:01:03.719697 139668746123008 logging_writer.py:48] [143300] global_step=143300, grad_norm=2.8861446380615234, loss=4.062432289123535
I0131 09:01:49.242347 139668754515712 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.920741558074951, loss=2.658607244491577
I0131 09:02:35.015336 139668746123008 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.362004280090332, loss=2.541750431060791
I0131 09:03:01.736093 139863983413056 spec.py:321] Evaluating on the training split.
I0131 09:03:12.048341 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 09:03:33.749519 139863983413056 spec.py:349] Evaluating on the test split.
I0131 09:03:35.391070 139863983413056 submission_runner.py:408] Time since start: 69727.04s, 	Step: 143560, 	{'train/accuracy': 0.7847851514816284, 'train/loss': 0.949887216091156, 'validation/accuracy': 0.7196199893951416, 'validation/loss': 1.221478819847107, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.8401914834976196, 'test/num_examples': 10000, 'score': 64743.521449804306, 'total_duration': 69727.04166722298, 'accumulated_submission_time': 64743.521449804306, 'accumulated_eval_time': 4969.630095720291, 'accumulated_logging_time': 6.429242849349976}
I0131 09:03:35.427058 139668754515712 logging_writer.py:48] [143560] accumulated_eval_time=4969.630096, accumulated_logging_time=6.429243, accumulated_submission_time=64743.521450, global_step=143560, preemption_count=0, score=64743.521450, test/accuracy=0.591000, test/loss=1.840191, test/num_examples=10000, total_duration=69727.041667, train/accuracy=0.784785, train/loss=0.949887, validation/accuracy=0.719620, validation/loss=1.221479, validation/num_examples=50000
I0131 09:03:51.815323 139668746123008 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.116215705871582, loss=2.4875285625457764
I0131 09:04:34.235320 139668754515712 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.2052059173583984, loss=2.5653040409088135
I0131 09:05:19.997545 139668746123008 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.522087335586548, loss=4.601512908935547
I0131 09:06:05.976548 139668754515712 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.379427909851074, loss=3.5752861499786377
I0131 09:06:52.083791 139668746123008 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.1643128395080566, loss=4.647687911987305
I0131 09:07:37.724046 139668754515712 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.327256202697754, loss=2.538210391998291
I0131 09:08:23.492255 139668746123008 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.6076483726501465, loss=3.3635292053222656
I0131 09:09:09.342047 139668754515712 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.739888906478882, loss=4.617640495300293
I0131 09:09:55.244757 139668746123008 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.9927961826324463, loss=3.0798540115356445
I0131 09:10:35.597697 139863983413056 spec.py:321] Evaluating on the training split.
I0131 09:10:46.057576 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 09:11:07.654895 139863983413056 spec.py:349] Evaluating on the test split.
I0131 09:11:09.290405 139863983413056 submission_runner.py:408] Time since start: 70180.94s, 	Step: 144490, 	{'train/accuracy': 0.7881640195846558, 'train/loss': 0.9314032793045044, 'validation/accuracy': 0.718239963054657, 'validation/loss': 1.2326812744140625, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.845787763595581, 'test/num_examples': 10000, 'score': 65163.633311748505, 'total_duration': 70180.94100570679, 'accumulated_submission_time': 65163.633311748505, 'accumulated_eval_time': 5003.322789907455, 'accumulated_logging_time': 6.475466012954712}
I0131 09:11:09.329912 139668754515712 logging_writer.py:48] [144490] accumulated_eval_time=5003.322790, accumulated_logging_time=6.475466, accumulated_submission_time=65163.633312, global_step=144490, preemption_count=0, score=65163.633312, test/accuracy=0.595200, test/loss=1.845788, test/num_examples=10000, total_duration=70180.941006, train/accuracy=0.788164, train/loss=0.931403, validation/accuracy=0.718240, validation/loss=1.232681, validation/num_examples=50000
I0131 09:11:13.713730 139668746123008 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.115905523300171, loss=3.862213373184204
I0131 09:11:55.266976 139668754515712 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.9664371013641357, loss=3.041067600250244
I0131 09:12:40.756951 139668746123008 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.1377241611480713, loss=3.375718593597412
I0131 09:13:26.462375 139668754515712 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.1902408599853516, loss=2.8563778400421143
I0131 09:14:12.560999 139668746123008 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.3514280319213867, loss=2.668663740158081
I0131 09:14:58.239719 139668754515712 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.020085096359253, loss=3.1171746253967285
I0131 09:15:43.918151 139668746123008 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.345762014389038, loss=2.6840052604675293
I0131 09:16:29.840925 139668754515712 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.322856903076172, loss=2.524864435195923
I0131 09:17:15.482274 139668746123008 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.8032398223876953, loss=4.570376396179199
I0131 09:18:01.243974 139668754515712 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.425786018371582, loss=2.59560489654541
I0131 09:18:09.700346 139863983413056 spec.py:321] Evaluating on the training split.
I0131 09:18:20.265414 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 09:18:41.718390 139863983413056 spec.py:349] Evaluating on the test split.
I0131 09:18:43.361777 139863983413056 submission_runner.py:408] Time since start: 70635.01s, 	Step: 145420, 	{'train/accuracy': 0.7870116829872131, 'train/loss': 0.9516395926475525, 'validation/accuracy': 0.7205599546432495, 'validation/loss': 1.229424238204956, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.8493428230285645, 'test/num_examples': 10000, 'score': 65583.94645094872, 'total_duration': 70635.012373209, 'accumulated_submission_time': 65583.94645094872, 'accumulated_eval_time': 5036.984225511551, 'accumulated_logging_time': 6.524138689041138}
I0131 09:18:43.398831 139668746123008 logging_writer.py:48] [145420] accumulated_eval_time=5036.984226, accumulated_logging_time=6.524139, accumulated_submission_time=65583.946451, global_step=145420, preemption_count=0, score=65583.946451, test/accuracy=0.593200, test/loss=1.849343, test/num_examples=10000, total_duration=70635.012373, train/accuracy=0.787012, train/loss=0.951640, validation/accuracy=0.720560, validation/loss=1.229424, validation/num_examples=50000
I0131 09:19:15.742633 139668754515712 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.111999034881592, loss=2.852006435394287
I0131 09:20:01.089235 139668746123008 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.191566228866577, loss=3.3755407333374023
I0131 09:20:46.886868 139668754515712 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.2251760959625244, loss=2.460914134979248
I0131 09:21:32.620216 139668746123008 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.1388750076293945, loss=2.628932476043701
I0131 09:22:18.088738 139668754515712 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.320572853088379, loss=4.615932464599609
I0131 09:23:03.710350 139668746123008 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.225942611694336, loss=2.511209487915039
I0131 09:23:49.647347 139668754515712 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.5441393852233887, loss=2.503283977508545
I0131 09:24:35.518211 139668746123008 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.1942062377929688, loss=2.5280873775482178
I0131 09:25:21.289250 139668754515712 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.396475315093994, loss=2.4433646202087402
I0131 09:25:43.428897 139863983413056 spec.py:321] Evaluating on the training split.
I0131 09:25:53.879482 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 09:26:15.322386 139863983413056 spec.py:349] Evaluating on the test split.
I0131 09:26:16.976044 139863983413056 submission_runner.py:408] Time since start: 71088.63s, 	Step: 146350, 	{'train/accuracy': 0.7876952886581421, 'train/loss': 0.9195204973220825, 'validation/accuracy': 0.726419985294342, 'validation/loss': 1.2012972831726074, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.814241647720337, 'test/num_examples': 10000, 'score': 66003.91966462135, 'total_duration': 71088.62664437294, 'accumulated_submission_time': 66003.91966462135, 'accumulated_eval_time': 5070.531363964081, 'accumulated_logging_time': 6.570559024810791}
I0131 09:26:17.015331 139668746123008 logging_writer.py:48] [146350] accumulated_eval_time=5070.531364, accumulated_logging_time=6.570559, accumulated_submission_time=66003.919665, global_step=146350, preemption_count=0, score=66003.919665, test/accuracy=0.601400, test/loss=1.814242, test/num_examples=10000, total_duration=71088.626644, train/accuracy=0.787695, train/loss=0.919520, validation/accuracy=0.726420, validation/loss=1.201297, validation/num_examples=50000
I0131 09:26:37.362566 139668754515712 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.2076382637023926, loss=3.393394947052002
I0131 09:27:21.120613 139668746123008 logging_writer.py:48] [146500] global_step=146500, grad_norm=4.143377780914307, loss=4.608749866485596
I0131 09:28:06.748410 139668754515712 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.331533908843994, loss=2.5419514179229736
I0131 09:28:52.852193 139668746123008 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.7744016647338867, loss=2.587618827819824
I0131 09:29:38.649180 139668754515712 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.8554739952087402, loss=4.57059383392334
I0131 09:30:24.448619 139668746123008 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.816335916519165, loss=4.1153764724731445
I0131 09:31:10.332206 139668754515712 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.1518595218658447, loss=3.2362334728240967
I0131 09:31:55.849687 139668746123008 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.2028095722198486, loss=3.2256147861480713
I0131 09:32:41.317850 139668754515712 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.6300408840179443, loss=2.4824419021606445
I0131 09:33:17.026088 139863983413056 spec.py:321] Evaluating on the training split.
I0131 09:33:27.287515 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 09:33:48.621763 139863983413056 spec.py:349] Evaluating on the test split.
I0131 09:33:50.270579 139863983413056 submission_runner.py:408] Time since start: 71541.92s, 	Step: 147280, 	{'train/accuracy': 0.7960156202316284, 'train/loss': 0.888444185256958, 'validation/accuracy': 0.7275800108909607, 'validation/loss': 1.1892650127410889, 'validation/num_examples': 50000, 'test/accuracy': 0.6032000184059143, 'test/loss': 1.8031319379806519, 'test/num_examples': 10000, 'score': 66423.87331795692, 'total_duration': 71541.92117094994, 'accumulated_submission_time': 66423.87331795692, 'accumulated_eval_time': 5103.775855779648, 'accumulated_logging_time': 6.61857533454895}
I0131 09:33:50.308055 139668746123008 logging_writer.py:48] [147280] accumulated_eval_time=5103.775856, accumulated_logging_time=6.618575, accumulated_submission_time=66423.873318, global_step=147280, preemption_count=0, score=66423.873318, test/accuracy=0.603200, test/loss=1.803132, test/num_examples=10000, total_duration=71541.921171, train/accuracy=0.796016, train/loss=0.888444, validation/accuracy=0.727580, validation/loss=1.189265, validation/num_examples=50000
I0131 09:33:58.694323 139668754515712 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.3791611194610596, loss=4.1305952072143555
I0131 09:34:40.886364 139668746123008 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.297713041305542, loss=2.8037478923797607
I0131 09:35:26.404692 139668754515712 logging_writer.py:48] [147500] global_step=147500, grad_norm=5.695344924926758, loss=4.125307559967041
I0131 09:36:12.392330 139668746123008 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.765472888946533, loss=4.355210304260254
I0131 09:36:58.352015 139668754515712 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.9187216758728027, loss=3.8961713314056396
I0131 09:37:43.880465 139668746123008 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.080209255218506, loss=4.513192176818848
I0131 09:38:29.408841 139668754515712 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.5250887870788574, loss=2.5285117626190186
I0131 09:39:15.271693 139668746123008 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.3439583778381348, loss=2.486485004425049
I0131 09:40:01.193897 139668754515712 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.4558169841766357, loss=4.3289995193481445
I0131 09:40:46.702258 139668746123008 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.8736917972564697, loss=2.6153159141540527
I0131 09:40:50.453663 139863983413056 spec.py:321] Evaluating on the training split.
I0131 09:41:00.749652 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 09:41:21.952951 139863983413056 spec.py:349] Evaluating on the test split.
I0131 09:41:23.597762 139863983413056 submission_runner.py:408] Time since start: 71995.25s, 	Step: 148210, 	{'train/accuracy': 0.7996679544448853, 'train/loss': 0.9016132950782776, 'validation/accuracy': 0.7257199883460999, 'validation/loss': 1.2199366092681885, 'validation/num_examples': 50000, 'test/accuracy': 0.5993000268936157, 'test/loss': 1.8347848653793335, 'test/num_examples': 10000, 'score': 66843.96156454086, 'total_duration': 71995.24836182594, 'accumulated_submission_time': 66843.96156454086, 'accumulated_eval_time': 5136.919964790344, 'accumulated_logging_time': 6.665120840072632}
I0131 09:41:23.638009 139668754515712 logging_writer.py:48] [148210] accumulated_eval_time=5136.919965, accumulated_logging_time=6.665121, accumulated_submission_time=66843.961565, global_step=148210, preemption_count=0, score=66843.961565, test/accuracy=0.599300, test/loss=1.834785, test/num_examples=10000, total_duration=71995.248362, train/accuracy=0.799668, train/loss=0.901613, validation/accuracy=0.725720, validation/loss=1.219937, validation/num_examples=50000
I0131 09:42:00.295957 139668746123008 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.723271608352661, loss=2.469209671020508
I0131 09:42:45.842833 139668754515712 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.3350744247436523, loss=2.556497097015381
I0131 09:43:31.384690 139668746123008 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.5152289867401123, loss=2.425264358520508
I0131 09:44:17.131747 139668754515712 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.61681866645813, loss=4.0291571617126465
I0131 09:45:02.666832 139668746123008 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.483842372894287, loss=3.8233842849731445
I0131 09:45:48.388253 139668754515712 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.652583122253418, loss=3.666670799255371
I0131 09:46:34.479380 139668746123008 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.8881959915161133, loss=3.428717613220215
I0131 09:47:20.013095 139668754515712 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.9795987606048584, loss=4.247979164123535
I0131 09:48:05.568398 139668746123008 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.5086231231689453, loss=2.519352436065674
I0131 09:48:23.792550 139863983413056 spec.py:321] Evaluating on the training split.
I0131 09:48:34.352910 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 09:48:54.557605 139863983413056 spec.py:349] Evaluating on the test split.
I0131 09:48:56.193944 139863983413056 submission_runner.py:408] Time since start: 72447.84s, 	Step: 149141, 	{'train/accuracy': 0.7919726371765137, 'train/loss': 0.9040732979774475, 'validation/accuracy': 0.7277799844741821, 'validation/loss': 1.1872663497924805, 'validation/num_examples': 50000, 'test/accuracy': 0.6061000227928162, 'test/loss': 1.7923401594161987, 'test/num_examples': 10000, 'score': 67264.05994081497, 'total_duration': 72447.8445456028, 'accumulated_submission_time': 67264.05994081497, 'accumulated_eval_time': 5169.321353435516, 'accumulated_logging_time': 6.71377420425415}
I0131 09:48:56.230978 139668754515712 logging_writer.py:48] [149141] accumulated_eval_time=5169.321353, accumulated_logging_time=6.713774, accumulated_submission_time=67264.059941, global_step=149141, preemption_count=0, score=67264.059941, test/accuracy=0.606100, test/loss=1.792340, test/num_examples=10000, total_duration=72447.844546, train/accuracy=0.791973, train/loss=0.904073, validation/accuracy=0.727780, validation/loss=1.187266, validation/num_examples=50000
I0131 09:49:20.199647 139668746123008 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.3119540214538574, loss=2.8642706871032715
I0131 09:50:04.626497 139668754515712 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.7632083892822266, loss=2.5096917152404785
I0131 09:50:50.302763 139668746123008 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.8127529621124268, loss=2.5768144130706787
I0131 09:51:36.301868 139668754515712 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.7213518619537354, loss=2.862776756286621
I0131 09:52:21.948628 139668746123008 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.1662724018096924, loss=2.3995213508605957
I0131 09:53:07.508699 139668754515712 logging_writer.py:48] [149700] global_step=149700, grad_norm=4.027266502380371, loss=2.583786964416504
I0131 09:53:53.211088 139668746123008 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.5008790493011475, loss=2.4703359603881836
I0131 09:54:39.070050 139668754515712 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.4577903747558594, loss=2.540645122528076
I0131 09:55:24.826224 139668746123008 logging_writer.py:48] [150000] global_step=150000, grad_norm=4.081754207611084, loss=4.278159141540527
I0131 09:55:56.490771 139863983413056 spec.py:321] Evaluating on the training split.
I0131 09:56:07.068746 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 09:56:26.765979 139863983413056 spec.py:349] Evaluating on the test split.
I0131 09:56:28.415504 139863983413056 submission_runner.py:408] Time since start: 72900.07s, 	Step: 150071, 	{'train/accuracy': 0.7992578148841858, 'train/loss': 0.882004976272583, 'validation/accuracy': 0.7291399836540222, 'validation/loss': 1.1780247688293457, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.791891098022461, 'test/num_examples': 10000, 'score': 67684.25943183899, 'total_duration': 72900.0660943985, 'accumulated_submission_time': 67684.25943183899, 'accumulated_eval_time': 5201.246497869492, 'accumulated_logging_time': 6.762799263000488}
I0131 09:56:28.452759 139668754515712 logging_writer.py:48] [150071] accumulated_eval_time=5201.246498, accumulated_logging_time=6.762799, accumulated_submission_time=67684.259432, global_step=150071, preemption_count=0, score=67684.259432, test/accuracy=0.604300, test/loss=1.791891, test/num_examples=10000, total_duration=72900.066094, train/accuracy=0.799258, train/loss=0.882005, validation/accuracy=0.729140, validation/loss=1.178025, validation/num_examples=50000
I0131 09:56:40.428649 139668746123008 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.814304828643799, loss=2.4949326515197754
I0131 09:57:23.329411 139668754515712 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.698367118835449, loss=4.21918249130249
I0131 09:58:08.728327 139668746123008 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.187955856323242, loss=4.417367935180664
I0131 09:58:54.868176 139668754515712 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.720122814178467, loss=3.4256398677825928
I0131 09:59:40.713758 139668746123008 logging_writer.py:48] [150500] global_step=150500, grad_norm=4.040747165679932, loss=2.4883086681365967
I0131 10:00:26.539584 139668754515712 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.420984983444214, loss=3.04699444770813
I0131 10:01:12.413752 139668746123008 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.605290412902832, loss=2.7223660945892334
I0131 10:01:58.428649 139668754515712 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.706167221069336, loss=2.460710048675537
I0131 10:02:44.097583 139668746123008 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.749359369277954, loss=2.5993266105651855
I0131 10:03:28.722975 139863983413056 spec.py:321] Evaluating on the training split.
I0131 10:03:38.956585 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 10:03:58.575285 139863983413056 spec.py:349] Evaluating on the test split.
I0131 10:04:00.218625 139863983413056 submission_runner.py:408] Time since start: 73351.87s, 	Step: 151000, 	{'train/accuracy': 0.8058202862739563, 'train/loss': 0.8614989519119263, 'validation/accuracy': 0.7326399683952332, 'validation/loss': 1.1725473403930664, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.7773709297180176, 'test/num_examples': 10000, 'score': 68104.4731631279, 'total_duration': 73351.86920380592, 'accumulated_submission_time': 68104.4731631279, 'accumulated_eval_time': 5232.74213886261, 'accumulated_logging_time': 6.808778524398804}
I0131 10:04:00.257660 139668754515712 logging_writer.py:48] [151000] accumulated_eval_time=5232.742139, accumulated_logging_time=6.808779, accumulated_submission_time=68104.473163, global_step=151000, preemption_count=0, score=68104.473163, test/accuracy=0.609000, test/loss=1.777371, test/num_examples=10000, total_duration=73351.869204, train/accuracy=0.805820, train/loss=0.861499, validation/accuracy=0.732640, validation/loss=1.172547, validation/num_examples=50000
I0131 10:04:00.662021 139668746123008 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.4731605052948, loss=3.0939621925354004
I0131 10:04:41.344103 139668754515712 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.3986940383911133, loss=2.9821038246154785
I0131 10:05:26.991052 139668746123008 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.5878264904022217, loss=2.419990062713623
I0131 10:06:12.662570 139668754515712 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.768071413040161, loss=2.45859694480896
I0131 10:06:58.605775 139668746123008 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.4578945636749268, loss=2.616446018218994
I0131 10:07:44.253901 139668754515712 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.9380767345428467, loss=4.212527275085449
I0131 10:08:29.945169 139668746123008 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.836182117462158, loss=2.500361442565918
I0131 10:09:15.468119 139668754515712 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.9855551719665527, loss=2.6139907836914062
I0131 10:10:01.223824 139668746123008 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.8411881923675537, loss=3.7444655895233154
I0131 10:10:46.833631 139668754515712 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.030843257904053, loss=4.2450852394104
I0131 10:11:00.683544 139863983413056 spec.py:321] Evaluating on the training split.
I0131 10:11:10.943018 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 10:11:32.243742 139863983413056 spec.py:349] Evaluating on the test split.
I0131 10:11:33.877360 139863983413056 submission_runner.py:408] Time since start: 73805.53s, 	Step: 151932, 	{'train/accuracy': 0.8000390529632568, 'train/loss': 0.8734333515167236, 'validation/accuracy': 0.731220006942749, 'validation/loss': 1.1657071113586426, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.7815557718276978, 'test/num_examples': 10000, 'score': 68524.8423511982, 'total_duration': 73805.52795624733, 'accumulated_submission_time': 68524.8423511982, 'accumulated_eval_time': 5265.935954332352, 'accumulated_logging_time': 6.8564043045043945}
I0131 10:11:33.914733 139668746123008 logging_writer.py:48] [151932] accumulated_eval_time=5265.935954, accumulated_logging_time=6.856404, accumulated_submission_time=68524.842351, global_step=151932, preemption_count=0, score=68524.842351, test/accuracy=0.604500, test/loss=1.781556, test/num_examples=10000, total_duration=73805.527956, train/accuracy=0.800039, train/loss=0.873433, validation/accuracy=0.731220, validation/loss=1.165707, validation/num_examples=50000
I0131 10:12:01.492625 139668754515712 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.776339054107666, loss=3.2881555557250977
I0131 10:12:45.932606 139668746123008 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.8408517837524414, loss=2.4456353187561035
I0131 10:13:31.655792 139668754515712 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.594426155090332, loss=3.387082099914551
I0131 10:14:17.397561 139668746123008 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.7331502437591553, loss=4.1888837814331055
I0131 10:15:02.955615 139668754515712 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.717687129974365, loss=4.2852277755737305
I0131 10:15:48.543771 139668746123008 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.6594736576080322, loss=2.4914684295654297
I0131 10:16:34.193035 139668754515712 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.8453917503356934, loss=2.570808172225952
I0131 10:17:19.950020 139668746123008 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.621466636657715, loss=2.6536660194396973
I0131 10:18:05.518307 139668754515712 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.4600107669830322, loss=2.7868106365203857
I0131 10:18:34.231300 139863983413056 spec.py:321] Evaluating on the training split.
I0131 10:18:44.397984 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 10:19:05.719041 139863983413056 spec.py:349] Evaluating on the test split.
I0131 10:19:07.356694 139863983413056 submission_runner.py:408] Time since start: 74259.01s, 	Step: 152865, 	{'train/accuracy': 0.804492175579071, 'train/loss': 0.8603943586349487, 'validation/accuracy': 0.7348399758338928, 'validation/loss': 1.1572157144546509, 'validation/num_examples': 50000, 'test/accuracy': 0.6141000390052795, 'test/loss': 1.7590707540512085, 'test/num_examples': 10000, 'score': 68945.09971499443, 'total_duration': 74259.00728917122, 'accumulated_submission_time': 68945.09971499443, 'accumulated_eval_time': 5299.06134557724, 'accumulated_logging_time': 6.904045104980469}
I0131 10:19:07.397013 139668746123008 logging_writer.py:48] [152865] accumulated_eval_time=5299.061346, accumulated_logging_time=6.904045, accumulated_submission_time=68945.099715, global_step=152865, preemption_count=0, score=68945.099715, test/accuracy=0.614100, test/loss=1.759071, test/num_examples=10000, total_duration=74259.007289, train/accuracy=0.804492, train/loss=0.860394, validation/accuracy=0.734840, validation/loss=1.157216, validation/num_examples=50000
I0131 10:19:21.766115 139668754515712 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.8305599689483643, loss=2.508307933807373
I0131 10:20:04.563340 139668746123008 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.563706398010254, loss=2.448652744293213
I0131 10:20:50.250734 139668754515712 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.6825106143951416, loss=3.296901226043701
I0131 10:21:36.055370 139668746123008 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.088923931121826, loss=3.9792640209198
I0131 10:22:21.645274 139668754515712 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.5894081592559814, loss=3.1353983879089355
I0131 10:23:07.230494 139668746123008 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.564917802810669, loss=3.376823902130127
I0131 10:23:52.835144 139668754515712 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.6978373527526855, loss=3.301685094833374
I0131 10:24:38.440457 139668746123008 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.986483573913574, loss=2.5122339725494385
I0131 10:25:23.939841 139668754515712 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.9937121868133545, loss=2.3948183059692383
I0131 10:26:07.724817 139863983413056 spec.py:321] Evaluating on the training split.
I0131 10:26:18.664549 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 10:26:40.044688 139863983413056 spec.py:349] Evaluating on the test split.
I0131 10:26:41.698609 139863983413056 submission_runner.py:408] Time since start: 74713.35s, 	Step: 153797, 	{'train/accuracy': 0.8055663704872131, 'train/loss': 0.8574345707893372, 'validation/accuracy': 0.7340199947357178, 'validation/loss': 1.1686347723007202, 'validation/num_examples': 50000, 'test/accuracy': 0.6112000346183777, 'test/loss': 1.773018479347229, 'test/num_examples': 10000, 'score': 69365.369992733, 'total_duration': 74713.34918832779, 'accumulated_submission_time': 69365.369992733, 'accumulated_eval_time': 5333.035125255585, 'accumulated_logging_time': 6.953671216964722}
I0131 10:26:41.741719 139668746123008 logging_writer.py:48] [153797] accumulated_eval_time=5333.035125, accumulated_logging_time=6.953671, accumulated_submission_time=69365.369993, global_step=153797, preemption_count=0, score=69365.369993, test/accuracy=0.611200, test/loss=1.773018, test/num_examples=10000, total_duration=74713.349188, train/accuracy=0.805566, train/loss=0.857435, validation/accuracy=0.734020, validation/loss=1.168635, validation/num_examples=50000
I0131 10:26:43.335221 139668754515712 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.859393358230591, loss=2.3869194984436035
I0131 10:27:24.483857 139668746123008 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.7575857639312744, loss=2.672825813293457
I0131 10:28:10.196515 139668754515712 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.864114761352539, loss=2.4316298961639404
I0131 10:28:56.023098 139668746123008 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.793667793273926, loss=2.4143426418304443
I0131 10:29:42.216093 139668754515712 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.041633129119873, loss=2.5575573444366455
I0131 10:30:27.554465 139668746123008 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.7868528366088867, loss=2.543074131011963
I0131 10:31:13.340031 139668754515712 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.472295045852661, loss=2.83318829536438
I0131 10:31:58.855204 139668746123008 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.347123146057129, loss=4.215971946716309
I0131 10:32:44.433478 139668754515712 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.7534282207489014, loss=2.4874887466430664
I0131 10:33:29.904255 139668746123008 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.4959115982055664, loss=2.3610243797302246
I0131 10:33:41.893265 139863983413056 spec.py:321] Evaluating on the training split.
I0131 10:33:52.191777 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 10:34:14.374075 139863983413056 spec.py:349] Evaluating on the test split.
I0131 10:34:16.012599 139863983413056 submission_runner.py:408] Time since start: 75167.66s, 	Step: 154728, 	{'train/accuracy': 0.8135741949081421, 'train/loss': 0.8229332566261292, 'validation/accuracy': 0.737779974937439, 'validation/loss': 1.1446532011032104, 'validation/num_examples': 50000, 'test/accuracy': 0.6133000254631042, 'test/loss': 1.7443084716796875, 'test/num_examples': 10000, 'score': 69785.46291160583, 'total_duration': 75167.66317725182, 'accumulated_submission_time': 69785.46291160583, 'accumulated_eval_time': 5367.154443502426, 'accumulated_logging_time': 7.006609916687012}
I0131 10:34:16.051638 139668754515712 logging_writer.py:48] [154728] accumulated_eval_time=5367.154444, accumulated_logging_time=7.006610, accumulated_submission_time=69785.462912, global_step=154728, preemption_count=0, score=69785.462912, test/accuracy=0.613300, test/loss=1.744308, test/num_examples=10000, total_duration=75167.663177, train/accuracy=0.813574, train/loss=0.822933, validation/accuracy=0.737780, validation/loss=1.144653, validation/num_examples=50000
I0131 10:34:45.233320 139668746123008 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.430502891540527, loss=2.8542938232421875
I0131 10:35:30.033045 139668754515712 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.976519823074341, loss=2.764024257659912
I0131 10:36:15.830996 139668746123008 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.547113418579102, loss=4.367789268493652
I0131 10:37:01.668282 139668754515712 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.344289779663086, loss=2.3756370544433594
I0131 10:37:47.385555 139668746123008 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.7950563430786133, loss=2.6944212913513184
I0131 10:38:33.081275 139668754515712 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.015369892120361, loss=2.4325203895568848
I0131 10:39:18.844459 139668746123008 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.6728363037109375, loss=3.2724101543426514
I0131 10:40:04.854881 139668754515712 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.085319519042969, loss=2.4169750213623047
I0131 10:40:50.364997 139668746123008 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.7700693607330322, loss=3.371976375579834
I0131 10:41:16.213779 139863983413056 spec.py:321] Evaluating on the training split.
I0131 10:41:26.764136 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 10:41:48.190890 139863983413056 spec.py:349] Evaluating on the test split.
I0131 10:41:49.831547 139863983413056 submission_runner.py:408] Time since start: 75621.48s, 	Step: 155658, 	{'train/accuracy': 0.8089257478713989, 'train/loss': 0.8678907752037048, 'validation/accuracy': 0.7383999824523926, 'validation/loss': 1.1707357168197632, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.7738709449768066, 'test/num_examples': 10000, 'score': 70205.56688523293, 'total_duration': 75621.48214673996, 'accumulated_submission_time': 70205.56688523293, 'accumulated_eval_time': 5400.772227048874, 'accumulated_logging_time': 7.056041240692139}
I0131 10:41:49.872460 139668754515712 logging_writer.py:48] [155658] accumulated_eval_time=5400.772227, accumulated_logging_time=7.056041, accumulated_submission_time=70205.566885, global_step=155658, preemption_count=0, score=70205.566885, test/accuracy=0.614300, test/loss=1.773871, test/num_examples=10000, total_duration=75621.482147, train/accuracy=0.808926, train/loss=0.867891, validation/accuracy=0.738400, validation/loss=1.170736, validation/num_examples=50000
I0131 10:42:07.304887 139668746123008 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.013617992401123, loss=2.4233601093292236
I0131 10:42:50.304095 139668754515712 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.150655746459961, loss=3.7762928009033203
I0131 10:43:35.997892 139668746123008 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.6163318157196045, loss=3.1997623443603516
I0131 10:44:22.066912 139668754515712 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.019247531890869, loss=2.4648070335388184
I0131 10:45:07.806256 139668746123008 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.205903053283691, loss=3.809023380279541
I0131 10:45:53.460510 139668754515712 logging_writer.py:48] [156200] global_step=156200, grad_norm=3.7017292976379395, loss=3.3814451694488525
I0131 10:46:39.329102 139668746123008 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.219309329986572, loss=3.534207582473755
I0131 10:47:25.422934 139668754515712 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.075024604797363, loss=3.4626870155334473
I0131 10:48:11.085968 139668746123008 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.9756617546081543, loss=2.522962808609009
I0131 10:48:50.118865 139863983413056 spec.py:321] Evaluating on the training split.
I0131 10:49:00.706832 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 10:49:22.354156 139863983413056 spec.py:349] Evaluating on the test split.
I0131 10:49:24.022246 139863983413056 submission_runner.py:408] Time since start: 76075.67s, 	Step: 156587, 	{'train/accuracy': 0.8149804472923279, 'train/loss': 0.8367795944213867, 'validation/accuracy': 0.7415599822998047, 'validation/loss': 1.1527200937271118, 'validation/num_examples': 50000, 'test/accuracy': 0.6217000484466553, 'test/loss': 1.7479978799819946, 'test/num_examples': 10000, 'score': 70625.50346064568, 'total_duration': 76075.67284274101, 'accumulated_submission_time': 70625.50346064568, 'accumulated_eval_time': 5434.675608158112, 'accumulated_logging_time': 7.35836124420166}
I0131 10:49:24.061680 139668754515712 logging_writer.py:48] [156587] accumulated_eval_time=5434.675608, accumulated_logging_time=7.358361, accumulated_submission_time=70625.503461, global_step=156587, preemption_count=0, score=70625.503461, test/accuracy=0.621700, test/loss=1.747998, test/num_examples=10000, total_duration=76075.672843, train/accuracy=0.814980, train/loss=0.836780, validation/accuracy=0.741560, validation/loss=1.152720, validation/num_examples=50000
I0131 10:49:29.658591 139668746123008 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.176469326019287, loss=2.462765693664551
I0131 10:50:11.470097 139668754515712 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.192956924438477, loss=2.528456211090088
I0131 10:50:57.540741 139668746123008 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.207711696624756, loss=2.4711334705352783
I0131 10:51:43.846396 139668754515712 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.512450695037842, loss=4.116450786590576
I0131 10:52:30.113645 139668746123008 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.7983198165893555, loss=3.136079788208008
I0131 10:53:15.990316 139668754515712 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.761352300643921, loss=2.405956983566284
I0131 10:54:02.150443 139668746123008 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.178080081939697, loss=2.5447187423706055
I0131 10:54:48.437623 139668754515712 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.038157939910889, loss=2.4397976398468018
I0131 10:55:34.197335 139668746123008 logging_writer.py:48] [157400] global_step=157400, grad_norm=3.858445167541504, loss=2.727630615234375
I0131 10:56:20.094074 139668754515712 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.4677581787109375, loss=2.3977103233337402
I0131 10:56:24.310585 139863983413056 spec.py:321] Evaluating on the training split.
I0131 10:56:35.028819 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 10:56:56.577427 139863983413056 spec.py:349] Evaluating on the test split.
I0131 10:56:58.210283 139863983413056 submission_runner.py:408] Time since start: 76529.86s, 	Step: 157511, 	{'train/accuracy': 0.8213085532188416, 'train/loss': 0.7878121137619019, 'validation/accuracy': 0.745199978351593, 'validation/loss': 1.118953824043274, 'validation/num_examples': 50000, 'test/accuracy': 0.6192000508308411, 'test/loss': 1.7231636047363281, 'test/num_examples': 10000, 'score': 71045.69268107414, 'total_duration': 76529.8608827591, 'accumulated_submission_time': 71045.69268107414, 'accumulated_eval_time': 5468.575320243835, 'accumulated_logging_time': 7.409008026123047}
I0131 10:56:58.248189 139668746123008 logging_writer.py:48] [157511] accumulated_eval_time=5468.575320, accumulated_logging_time=7.409008, accumulated_submission_time=71045.692681, global_step=157511, preemption_count=0, score=71045.692681, test/accuracy=0.619200, test/loss=1.723164, test/num_examples=10000, total_duration=76529.860883, train/accuracy=0.821309, train/loss=0.787812, validation/accuracy=0.745200, validation/loss=1.118954, validation/num_examples=50000
I0131 10:57:34.549855 139668754515712 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.095064640045166, loss=2.726378917694092
I0131 10:58:20.384490 139668746123008 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.214110374450684, loss=2.3340728282928467
I0131 10:59:05.995149 139668754515712 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.060462951660156, loss=2.4265971183776855
I0131 10:59:51.990738 139668746123008 logging_writer.py:48] [157900] global_step=157900, grad_norm=3.9575321674346924, loss=2.561497211456299
I0131 11:00:37.684581 139668754515712 logging_writer.py:48] [158000] global_step=158000, grad_norm=3.9915332794189453, loss=2.5499181747436523
I0131 11:01:23.716943 139668746123008 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.253127098083496, loss=2.4509830474853516
I0131 11:02:09.190330 139668754515712 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.8239216804504395, loss=2.5733494758605957
I0131 11:02:54.841399 139668746123008 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.993327617645264, loss=2.6131229400634766
I0131 11:03:40.603688 139668754515712 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.069450855255127, loss=2.6885597705841064
I0131 11:03:58.478169 139863983413056 spec.py:321] Evaluating on the training split.
I0131 11:04:09.033126 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 11:04:30.875781 139863983413056 spec.py:349] Evaluating on the test split.
I0131 11:04:32.505671 139863983413056 submission_runner.py:408] Time since start: 76984.16s, 	Step: 158441, 	{'train/accuracy': 0.8138671517372131, 'train/loss': 0.8201526403427124, 'validation/accuracy': 0.7439999580383301, 'validation/loss': 1.1131176948547363, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.7198753356933594, 'test/num_examples': 10000, 'score': 71465.86521029472, 'total_duration': 76984.15627121925, 'accumulated_submission_time': 71465.86521029472, 'accumulated_eval_time': 5502.602823019028, 'accumulated_logging_time': 7.457210540771484}
I0131 11:04:32.549478 139668746123008 logging_writer.py:48] [158441] accumulated_eval_time=5502.602823, accumulated_logging_time=7.457211, accumulated_submission_time=71465.865210, global_step=158441, preemption_count=0, score=71465.865210, test/accuracy=0.620000, test/loss=1.719875, test/num_examples=10000, total_duration=76984.156271, train/accuracy=0.813867, train/loss=0.820153, validation/accuracy=0.744000, validation/loss=1.113118, validation/num_examples=50000
I0131 11:04:56.505118 139668754515712 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.3476996421813965, loss=2.3684401512145996
I0131 11:05:40.621103 139668746123008 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.297619819641113, loss=2.4531328678131104
I0131 11:06:26.204226 139668754515712 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.001361846923828, loss=2.824343204498291
I0131 11:07:11.794548 139668746123008 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.037139892578125, loss=2.9847733974456787
I0131 11:07:57.654426 139668754515712 logging_writer.py:48] [158900] global_step=158900, grad_norm=3.942565679550171, loss=2.3395323753356934
I0131 11:08:43.653288 139668746123008 logging_writer.py:48] [159000] global_step=159000, grad_norm=3.8648135662078857, loss=3.4517903327941895
I0131 11:09:29.493295 139668754515712 logging_writer.py:48] [159100] global_step=159100, grad_norm=3.97881817817688, loss=3.7297134399414062
I0131 11:10:15.383963 139668746123008 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.576580047607422, loss=3.7024638652801514
I0131 11:11:01.227430 139668754515712 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.090749740600586, loss=2.4084744453430176
I0131 11:11:32.539612 139863983413056 spec.py:321] Evaluating on the training split.
I0131 11:11:43.006168 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 11:12:04.492597 139863983413056 spec.py:349] Evaluating on the test split.
I0131 11:12:06.138816 139863983413056 submission_runner.py:408] Time since start: 77437.79s, 	Step: 159370, 	{'train/accuracy': 0.82093745470047, 'train/loss': 0.8007131218910217, 'validation/accuracy': 0.7448599934577942, 'validation/loss': 1.126524806022644, 'validation/num_examples': 50000, 'test/accuracy': 0.6217000484466553, 'test/loss': 1.7308553457260132, 'test/num_examples': 10000, 'score': 71885.79600262642, 'total_duration': 77437.7894179821, 'accumulated_submission_time': 71885.79600262642, 'accumulated_eval_time': 5536.2020580768585, 'accumulated_logging_time': 7.512519836425781}
I0131 11:12:06.181073 139668746123008 logging_writer.py:48] [159370] accumulated_eval_time=5536.202058, accumulated_logging_time=7.512520, accumulated_submission_time=71885.796003, global_step=159370, preemption_count=0, score=71885.796003, test/accuracy=0.621700, test/loss=1.730855, test/num_examples=10000, total_duration=77437.789418, train/accuracy=0.820937, train/loss=0.800713, validation/accuracy=0.744860, validation/loss=1.126525, validation/num_examples=50000
I0131 11:12:18.547130 139668754515712 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.915530681610107, loss=3.863739013671875
I0131 11:13:00.787140 139668746123008 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.04063606262207, loss=2.8396294116973877
I0131 11:13:46.661204 139668754515712 logging_writer.py:48] [159600] global_step=159600, grad_norm=5.071171283721924, loss=2.3447115421295166
I0131 11:14:32.159282 139668746123008 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.37224006652832, loss=2.571474075317383
I0131 11:15:18.008883 139668754515712 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.7378621101379395, loss=2.412386417388916
I0131 11:16:03.708228 139668746123008 logging_writer.py:48] [159900] global_step=159900, grad_norm=3.9721596240997314, loss=2.386556625366211
I0131 11:16:49.396116 139668754515712 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.504187107086182, loss=2.524015188217163
I0131 11:17:35.101802 139668746123008 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.016417980194092, loss=2.3450815677642822
I0131 11:18:20.912405 139668754515712 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.169461250305176, loss=2.6287693977355957
I0131 11:19:06.568550 139668746123008 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.503909587860107, loss=2.4616873264312744
I0131 11:19:06.582335 139863983413056 spec.py:321] Evaluating on the training split.
I0131 11:19:17.165138 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 11:19:39.020169 139863983413056 spec.py:349] Evaluating on the test split.
I0131 11:19:40.659712 139863983413056 submission_runner.py:408] Time since start: 77892.31s, 	Step: 160301, 	{'train/accuracy': 0.8273046612739563, 'train/loss': 0.7696381211280823, 'validation/accuracy': 0.7482199668884277, 'validation/loss': 1.115172028541565, 'validation/num_examples': 50000, 'test/accuracy': 0.6235000491142273, 'test/loss': 1.7202221155166626, 'test/num_examples': 10000, 'score': 72306.14024019241, 'total_duration': 77892.31031370163, 'accumulated_submission_time': 72306.14024019241, 'accumulated_eval_time': 5570.279438018799, 'accumulated_logging_time': 7.564241647720337}
I0131 11:19:40.702903 139668754515712 logging_writer.py:48] [160301] accumulated_eval_time=5570.279438, accumulated_logging_time=7.564242, accumulated_submission_time=72306.140240, global_step=160301, preemption_count=0, score=72306.140240, test/accuracy=0.623500, test/loss=1.720222, test/num_examples=10000, total_duration=77892.310314, train/accuracy=0.827305, train/loss=0.769638, validation/accuracy=0.748220, validation/loss=1.115172, validation/num_examples=50000
I0131 11:20:21.474223 139668746123008 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.584744453430176, loss=2.431643009185791
I0131 11:21:06.903283 139668754515712 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.754953861236572, loss=3.7556657791137695
I0131 11:21:52.768071 139668746123008 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.901564598083496, loss=3.7382566928863525
I0131 11:22:39.014475 139668754515712 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.434734344482422, loss=3.3823206424713135
I0131 11:23:24.546454 139668746123008 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.811438083648682, loss=3.9162158966064453
I0131 11:24:10.104245 139668754515712 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.594385623931885, loss=4.047446250915527
I0131 11:24:55.721717 139668746123008 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.1256937980651855, loss=2.615631580352783
I0131 11:25:41.299729 139668754515712 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.3924880027771, loss=2.355837821960449
I0131 11:26:27.202639 139668746123008 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.208252906799316, loss=3.4110188484191895
I0131 11:26:41.075375 139863983413056 spec.py:321] Evaluating on the training split.
I0131 11:26:51.627466 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 11:27:09.592666 139863983413056 spec.py:349] Evaluating on the test split.
I0131 11:27:11.244827 139863983413056 submission_runner.py:408] Time since start: 78342.90s, 	Step: 161232, 	{'train/accuracy': 0.8215234279632568, 'train/loss': 0.79811030626297, 'validation/accuracy': 0.7492199540138245, 'validation/loss': 1.1067612171173096, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.7063981294631958, 'test/num_examples': 10000, 'score': 72726.45551586151, 'total_duration': 78342.89540290833, 'accumulated_submission_time': 72726.45551586151, 'accumulated_eval_time': 5600.44885635376, 'accumulated_logging_time': 7.616266965866089}
I0131 11:27:11.292996 139668754515712 logging_writer.py:48] [161232] accumulated_eval_time=5600.448856, accumulated_logging_time=7.616267, accumulated_submission_time=72726.455516, global_step=161232, preemption_count=0, score=72726.455516, test/accuracy=0.630000, test/loss=1.706398, test/num_examples=10000, total_duration=78342.895403, train/accuracy=0.821523, train/loss=0.798110, validation/accuracy=0.749220, validation/loss=1.106761, validation/num_examples=50000
I0131 11:27:38.996692 139668746123008 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.44200325012207, loss=2.403634786605835
I0131 11:28:24.692433 139668754515712 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.765442371368408, loss=2.365245819091797
I0131 11:29:10.456502 139668746123008 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.388218879699707, loss=2.742579936981201
I0131 11:29:56.391002 139668754515712 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.473581314086914, loss=2.321568489074707
I0131 11:30:42.022249 139668746123008 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.197946548461914, loss=2.8148481845855713
I0131 11:31:27.922663 139668754515712 logging_writer.py:48] [161800] global_step=161800, grad_norm=5.36997127532959, loss=4.331996440887451
I0131 11:32:13.858679 139668746123008 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.722134590148926, loss=2.530576467514038
I0131 11:32:59.750612 139668754515712 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.736434459686279, loss=2.982616901397705
I0131 11:33:45.278014 139668746123008 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.796806812286377, loss=3.8067972660064697
I0131 11:34:11.536774 139863983413056 spec.py:321] Evaluating on the training split.
I0131 11:34:21.826400 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 11:34:39.898626 139863983413056 spec.py:349] Evaluating on the test split.
I0131 11:34:41.555217 139863983413056 submission_runner.py:408] Time since start: 78793.21s, 	Step: 162159, 	{'train/accuracy': 0.8233007788658142, 'train/loss': 0.7672076225280762, 'validation/accuracy': 0.7484999895095825, 'validation/loss': 1.0859493017196655, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.673032522201538, 'test/num_examples': 10000, 'score': 73146.63851761818, 'total_duration': 78793.20580601692, 'accumulated_submission_time': 73146.63851761818, 'accumulated_eval_time': 5630.4672927856445, 'accumulated_logging_time': 7.6764256954193115}
I0131 11:34:41.603971 139668754515712 logging_writer.py:48] [162159] accumulated_eval_time=5630.467293, accumulated_logging_time=7.676426, accumulated_submission_time=73146.638518, global_step=162159, preemption_count=0, score=73146.638518, test/accuracy=0.633800, test/loss=1.673033, test/num_examples=10000, total_duration=78793.205806, train/accuracy=0.823301, train/loss=0.767208, validation/accuracy=0.748500, validation/loss=1.085949, validation/num_examples=50000
I0131 11:34:58.384038 139668746123008 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.448052883148193, loss=2.4363040924072266
I0131 11:35:41.902908 139668754515712 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.830474853515625, loss=2.4258885383605957
I0131 11:36:27.529417 139668746123008 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.169002056121826, loss=2.2544655799865723
I0131 11:37:13.365476 139668754515712 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.3698954582214355, loss=2.2784814834594727
I0131 11:37:59.099429 139668746123008 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.6020588874816895, loss=3.228499174118042
I0131 11:38:45.011935 139668754515712 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.712512969970703, loss=2.5114946365356445
I0131 11:39:30.778237 139668746123008 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.5596795082092285, loss=2.3749186992645264
I0131 11:40:16.817877 139668754515712 logging_writer.py:48] [162900] global_step=162900, grad_norm=5.4171977043151855, loss=4.362022399902344
I0131 11:41:02.409150 139668746123008 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.553607940673828, loss=2.8455042839050293
I0131 11:41:41.760740 139863983413056 spec.py:321] Evaluating on the training split.
I0131 11:41:51.858847 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 11:42:13.169121 139863983413056 spec.py:349] Evaluating on the test split.
I0131 11:42:14.807072 139863983413056 submission_runner.py:408] Time since start: 79246.46s, 	Step: 163088, 	{'train/accuracy': 0.8291601538658142, 'train/loss': 0.7714617848396301, 'validation/accuracy': 0.7514199614524841, 'validation/loss': 1.10415518283844, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.7050611972808838, 'test/num_examples': 10000, 'score': 73566.73581504822, 'total_duration': 79246.45765209198, 'accumulated_submission_time': 73566.73581504822, 'accumulated_eval_time': 5663.513606309891, 'accumulated_logging_time': 7.735929727554321}
I0131 11:42:14.854216 139668754515712 logging_writer.py:48] [163088] accumulated_eval_time=5663.513606, accumulated_logging_time=7.735930, accumulated_submission_time=73566.735815, global_step=163088, preemption_count=0, score=73566.735815, test/accuracy=0.631000, test/loss=1.705061, test/num_examples=10000, total_duration=79246.457652, train/accuracy=0.829160, train/loss=0.771462, validation/accuracy=0.751420, validation/loss=1.104155, validation/num_examples=50000
I0131 11:42:20.045164 139668746123008 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.390658855438232, loss=3.3378121852874756
I0131 11:43:00.736456 139668754515712 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.612508773803711, loss=2.4524521827697754
I0131 11:43:46.413615 139668746123008 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.4970245361328125, loss=2.3415443897247314
I0131 11:44:32.246518 139668754515712 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.4900102615356445, loss=2.367044687271118
I0131 11:45:18.111077 139668746123008 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.534743309020996, loss=2.3333797454833984
I0131 11:46:03.548869 139668754515712 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.390928745269775, loss=2.309701442718506
I0131 11:46:48.999511 139668746123008 logging_writer.py:48] [163700] global_step=163700, grad_norm=5.060925483703613, loss=4.190095901489258
I0131 11:47:34.701776 139668754515712 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.63144063949585, loss=3.151397466659546
I0131 11:48:20.539602 139668746123008 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.816125392913818, loss=2.3447961807250977
I0131 11:49:06.145125 139668754515712 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.634270668029785, loss=2.248129367828369
I0131 11:49:14.845779 139863983413056 spec.py:321] Evaluating on the training split.
I0131 11:49:25.156642 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 11:49:47.296228 139863983413056 spec.py:349] Evaluating on the test split.
I0131 11:49:48.928142 139863983413056 submission_runner.py:408] Time since start: 79700.58s, 	Step: 164021, 	{'train/accuracy': 0.83509761095047, 'train/loss': 0.7409867644309998, 'validation/accuracy': 0.7541199922561646, 'validation/loss': 1.079445481300354, 'validation/num_examples': 50000, 'test/accuracy': 0.6335000395774841, 'test/loss': 1.6779024600982666, 'test/num_examples': 10000, 'score': 73986.66831469536, 'total_duration': 79700.57873511314, 'accumulated_submission_time': 73986.66831469536, 'accumulated_eval_time': 5697.595949888229, 'accumulated_logging_time': 7.793646812438965}
I0131 11:49:48.970156 139668746123008 logging_writer.py:48] [164021] accumulated_eval_time=5697.595950, accumulated_logging_time=7.793647, accumulated_submission_time=73986.668315, global_step=164021, preemption_count=0, score=73986.668315, test/accuracy=0.633500, test/loss=1.677902, test/num_examples=10000, total_duration=79700.578735, train/accuracy=0.835098, train/loss=0.740987, validation/accuracy=0.754120, validation/loss=1.079445, validation/num_examples=50000
I0131 11:50:20.933516 139668754515712 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.5861968994140625, loss=2.417202949523926
I0131 11:51:05.244799 139668746123008 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.537180423736572, loss=2.357712745666504
I0131 11:51:50.902585 139668754515712 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.643290042877197, loss=2.3594589233398438
I0131 11:52:36.538381 139668746123008 logging_writer.py:48] [164400] global_step=164400, grad_norm=5.283929824829102, loss=4.069349765777588
I0131 11:53:22.013340 139668754515712 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.716491222381592, loss=2.367363452911377
I0131 11:54:07.583399 139668746123008 logging_writer.py:48] [164600] global_step=164600, grad_norm=5.767025947570801, loss=4.15494441986084
I0131 11:54:53.058477 139668754515712 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.497401714324951, loss=2.2941386699676514
I0131 11:55:38.490097 139668746123008 logging_writer.py:48] [164800] global_step=164800, grad_norm=5.229079723358154, loss=3.7233633995056152
I0131 11:56:24.105033 139668754515712 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.4731268882751465, loss=2.35491681098938
I0131 11:56:49.370318 139863983413056 spec.py:321] Evaluating on the training split.
I0131 11:57:00.002974 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 11:57:22.389794 139863983413056 spec.py:349] Evaluating on the test split.
I0131 11:57:24.022258 139863983413056 submission_runner.py:408] Time since start: 80155.67s, 	Step: 164957, 	{'train/accuracy': 0.8322851657867432, 'train/loss': 0.7366339564323425, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 1.0619897842407227, 'validation/num_examples': 50000, 'test/accuracy': 0.6362000107765198, 'test/loss': 1.6556838750839233, 'test/num_examples': 10000, 'score': 74407.01210308075, 'total_duration': 80155.67283654213, 'accumulated_submission_time': 74407.01210308075, 'accumulated_eval_time': 5732.24786067009, 'accumulated_logging_time': 7.844249725341797}
I0131 11:57:24.063066 139668746123008 logging_writer.py:48] [164957] accumulated_eval_time=5732.247861, accumulated_logging_time=7.844250, accumulated_submission_time=74407.012103, global_step=164957, preemption_count=0, score=74407.012103, test/accuracy=0.636200, test/loss=1.655684, test/num_examples=10000, total_duration=80155.672837, train/accuracy=0.832285, train/loss=0.736634, validation/accuracy=0.755440, validation/loss=1.061990, validation/num_examples=50000
I0131 11:57:41.638349 139668754515712 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.43808126449585, loss=3.3654708862304688
I0131 11:58:24.754387 139668746123008 logging_writer.py:48] [165100] global_step=165100, grad_norm=5.031344890594482, loss=2.3856887817382812
I0131 11:59:10.926182 139668754515712 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.800317287445068, loss=2.3260021209716797
I0131 11:59:56.748503 139668746123008 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.7824554443359375, loss=3.6451382637023926
I0131 12:00:42.483460 139668754515712 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.839137077331543, loss=3.0186681747436523
I0131 12:01:28.338008 139668746123008 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.6580328941345215, loss=2.5134341716766357
I0131 12:02:13.923235 139668754515712 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.665430545806885, loss=2.7268013954162598
I0131 12:02:59.515709 139668746123008 logging_writer.py:48] [165700] global_step=165700, grad_norm=5.89999532699585, loss=4.25893497467041
I0131 12:03:45.313035 139668754515712 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.76444149017334, loss=3.163780450820923
I0131 12:04:24.146492 139863983413056 spec.py:321] Evaluating on the training split.
I0131 12:04:34.367470 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 12:04:53.333087 139863983413056 spec.py:349] Evaluating on the test split.
I0131 12:04:55.002843 139863983413056 submission_runner.py:408] Time since start: 80606.65s, 	Step: 165887, 	{'train/accuracy': 0.8315820097923279, 'train/loss': 0.7409655451774597, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.0678353309631348, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.6705749034881592, 'test/num_examples': 10000, 'score': 74827.03679513931, 'total_duration': 80606.65344071388, 'accumulated_submission_time': 74827.03679513931, 'accumulated_eval_time': 5763.104225158691, 'accumulated_logging_time': 7.89516282081604}
I0131 12:04:55.045957 139668746123008 logging_writer.py:48] [165887] accumulated_eval_time=5763.104225, accumulated_logging_time=7.895163, accumulated_submission_time=74827.036795, global_step=165887, preemption_count=0, score=74827.036795, test/accuracy=0.629400, test/loss=1.670575, test/num_examples=10000, total_duration=80606.653441, train/accuracy=0.831582, train/loss=0.740966, validation/accuracy=0.755600, validation/loss=1.067835, validation/num_examples=50000
I0131 12:05:00.648917 139668754515712 logging_writer.py:48] [165900] global_step=165900, grad_norm=5.066099166870117, loss=2.358616352081299
I0131 12:05:42.485558 139668746123008 logging_writer.py:48] [166000] global_step=166000, grad_norm=5.313916206359863, loss=3.971933126449585
I0131 12:06:28.328752 139668754515712 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.5130391120910645, loss=2.5087010860443115
I0131 12:07:14.365182 139668746123008 logging_writer.py:48] [166200] global_step=166200, grad_norm=5.171040058135986, loss=3.64115834236145
I0131 12:08:00.031003 139668754515712 logging_writer.py:48] [166300] global_step=166300, grad_norm=5.054174900054932, loss=2.3451988697052
I0131 12:08:45.748195 139668746123008 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.676840782165527, loss=3.25960111618042
I0131 12:09:31.451927 139668754515712 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.647468566894531, loss=3.6536102294921875
I0131 12:10:17.327683 139668746123008 logging_writer.py:48] [166600] global_step=166600, grad_norm=5.882762908935547, loss=3.9627199172973633
I0131 12:11:03.027678 139668754515712 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.41289758682251, loss=2.306427001953125
I0131 12:11:48.570754 139668746123008 logging_writer.py:48] [166800] global_step=166800, grad_norm=5.8043107986450195, loss=4.357635021209717
I0131 12:11:55.178808 139863983413056 spec.py:321] Evaluating on the training split.
I0131 12:12:05.637075 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 12:12:25.167455 139863983413056 spec.py:349] Evaluating on the test split.
I0131 12:12:26.817564 139863983413056 submission_runner.py:408] Time since start: 81058.47s, 	Step: 166816, 	{'train/accuracy': 0.8335937261581421, 'train/loss': 0.7405239939689636, 'validation/accuracy': 0.7565000057220459, 'validation/loss': 1.076341986656189, 'validation/num_examples': 50000, 'test/accuracy': 0.6351000070571899, 'test/loss': 1.669582486152649, 'test/num_examples': 10000, 'score': 75247.1132349968, 'total_duration': 81058.46815085411, 'accumulated_submission_time': 75247.1132349968, 'accumulated_eval_time': 5794.742982387543, 'accumulated_logging_time': 7.947040319442749}
I0131 12:12:26.860098 139668754515712 logging_writer.py:48] [166816] accumulated_eval_time=5794.742982, accumulated_logging_time=7.947040, accumulated_submission_time=75247.113235, global_step=166816, preemption_count=0, score=75247.113235, test/accuracy=0.635100, test/loss=1.669582, test/num_examples=10000, total_duration=81058.468151, train/accuracy=0.833594, train/loss=0.740524, validation/accuracy=0.756500, validation/loss=1.076342, validation/num_examples=50000
I0131 12:13:00.822230 139668746123008 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.978140830993652, loss=2.347517967224121
I0131 12:13:46.300796 139668754515712 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.662094593048096, loss=2.2234928607940674
I0131 12:14:32.099905 139668746123008 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.376381874084473, loss=3.5038018226623535
I0131 12:15:18.111975 139668754515712 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.655727386474609, loss=2.276334762573242
I0131 12:16:03.545095 139668746123008 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.154141902923584, loss=2.35829758644104
I0131 12:16:48.980066 139668754515712 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.797354221343994, loss=2.2279348373413086
I0131 12:17:34.506623 139668746123008 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.872809410095215, loss=2.629213333129883
I0131 12:18:20.064874 139668754515712 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.754238128662109, loss=2.375493049621582
I0131 12:19:06.233875 139668746123008 logging_writer.py:48] [167700] global_step=167700, grad_norm=7.238354682922363, loss=4.230075359344482
I0131 12:19:26.897377 139863983413056 spec.py:321] Evaluating on the training split.
I0131 12:19:37.057687 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 12:19:58.227132 139863983413056 spec.py:349] Evaluating on the test split.
I0131 12:19:59.870295 139863983413056 submission_runner.py:408] Time since start: 81511.52s, 	Step: 167747, 	{'train/accuracy': 0.8338671922683716, 'train/loss': 0.7559254765510559, 'validation/accuracy': 0.75791996717453, 'validation/loss': 1.0783684253692627, 'validation/num_examples': 50000, 'test/accuracy': 0.6355000138282776, 'test/loss': 1.6743074655532837, 'test/num_examples': 10000, 'score': 75667.09297275543, 'total_duration': 81511.52088880539, 'accumulated_submission_time': 75667.09297275543, 'accumulated_eval_time': 5827.715883970261, 'accumulated_logging_time': 7.998236179351807}
I0131 12:19:59.912528 139668754515712 logging_writer.py:48] [167747] accumulated_eval_time=5827.715884, accumulated_logging_time=7.998236, accumulated_submission_time=75667.092973, global_step=167747, preemption_count=0, score=75667.092973, test/accuracy=0.635500, test/loss=1.674307, test/num_examples=10000, total_duration=81511.520889, train/accuracy=0.833867, train/loss=0.755925, validation/accuracy=0.757920, validation/loss=1.078368, validation/num_examples=50000
I0131 12:20:21.500869 139668746123008 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.763859748840332, loss=2.3220791816711426
I0131 12:21:04.639277 139668754515712 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.9840240478515625, loss=3.113156795501709
I0131 12:21:50.586356 139668746123008 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.0434370040893555, loss=2.250805616378784
I0131 12:22:36.240100 139668754515712 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.820967197418213, loss=2.2872376441955566
I0131 12:23:21.997010 139668746123008 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.89430046081543, loss=2.290501356124878
I0131 12:24:07.890308 139668754515712 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.829126358032227, loss=2.531820058822632
I0131 12:24:53.790485 139668746123008 logging_writer.py:48] [168400] global_step=168400, grad_norm=5.599942684173584, loss=2.7091729640960693
I0131 12:25:39.451437 139668754515712 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.803707599639893, loss=2.298137903213501
I0131 12:26:25.133473 139668746123008 logging_writer.py:48] [168600] global_step=168600, grad_norm=5.06223201751709, loss=2.50964617729187
I0131 12:27:00.222505 139863983413056 spec.py:321] Evaluating on the training split.
I0131 12:27:10.662503 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 12:27:32.457798 139863983413056 spec.py:349] Evaluating on the test split.
I0131 12:27:34.090517 139863983413056 submission_runner.py:408] Time since start: 81965.74s, 	Step: 168678, 	{'train/accuracy': 0.8360546827316284, 'train/loss': 0.7370734214782715, 'validation/accuracy': 0.7594599723815918, 'validation/loss': 1.0704350471496582, 'validation/num_examples': 50000, 'test/accuracy': 0.636400043964386, 'test/loss': 1.665740728378296, 'test/num_examples': 10000, 'score': 76087.34431004524, 'total_duration': 81965.7411146164, 'accumulated_submission_time': 76087.34431004524, 'accumulated_eval_time': 5861.583888530731, 'accumulated_logging_time': 8.05066990852356}
I0131 12:27:34.136766 139668754515712 logging_writer.py:48] [168678] accumulated_eval_time=5861.583889, accumulated_logging_time=8.050670, accumulated_submission_time=76087.344310, global_step=168678, preemption_count=0, score=76087.344310, test/accuracy=0.636400, test/loss=1.665741, test/num_examples=10000, total_duration=81965.741115, train/accuracy=0.836055, train/loss=0.737073, validation/accuracy=0.759460, validation/loss=1.070435, validation/num_examples=50000
I0131 12:27:43.318362 139668746123008 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.830731391906738, loss=2.273240566253662
I0131 12:28:25.099669 139668754515712 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.651676177978516, loss=3.371180295944214
I0131 12:29:11.352384 139668746123008 logging_writer.py:48] [168900] global_step=168900, grad_norm=5.7778167724609375, loss=4.161726951599121
I0131 12:29:57.295670 139668754515712 logging_writer.py:48] [169000] global_step=169000, grad_norm=5.19004487991333, loss=3.5970406532287598
I0131 12:30:43.177830 139668746123008 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.868297100067139, loss=2.2602527141571045
I0131 12:31:28.757951 139668754515712 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.425215244293213, loss=2.2074508666992188
I0131 12:32:14.533962 139668746123008 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.71776008605957, loss=3.0676565170288086
I0131 12:32:59.955541 139668754515712 logging_writer.py:48] [169400] global_step=169400, grad_norm=5.119131088256836, loss=2.1912055015563965
I0131 12:33:45.444546 139668746123008 logging_writer.py:48] [169500] global_step=169500, grad_norm=5.30922269821167, loss=3.59660005569458
I0131 12:34:31.087094 139668754515712 logging_writer.py:48] [169600] global_step=169600, grad_norm=5.082907676696777, loss=2.68426513671875
I0131 12:34:34.334989 139863983413056 spec.py:321] Evaluating on the training split.
I0131 12:34:44.541312 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 12:35:06.867092 139863983413056 spec.py:349] Evaluating on the test split.
I0131 12:35:08.506341 139863983413056 submission_runner.py:408] Time since start: 82420.16s, 	Step: 169609, 	{'train/accuracy': 0.83984375, 'train/loss': 0.7221209406852722, 'validation/accuracy': 0.760159969329834, 'validation/loss': 1.0561344623565674, 'validation/num_examples': 50000, 'test/accuracy': 0.6401000022888184, 'test/loss': 1.649182677268982, 'test/num_examples': 10000, 'score': 76507.4817943573, 'total_duration': 82420.15692543983, 'accumulated_submission_time': 76507.4817943573, 'accumulated_eval_time': 5895.755210876465, 'accumulated_logging_time': 8.10885739326477}
I0131 12:35:08.546669 139668746123008 logging_writer.py:48] [169609] accumulated_eval_time=5895.755211, accumulated_logging_time=8.108857, accumulated_submission_time=76507.481794, global_step=169609, preemption_count=0, score=76507.481794, test/accuracy=0.640100, test/loss=1.649183, test/num_examples=10000, total_duration=82420.156925, train/accuracy=0.839844, train/loss=0.722121, validation/accuracy=0.760160, validation/loss=1.056134, validation/num_examples=50000
I0131 12:35:45.465617 139668754515712 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.727559566497803, loss=2.4887049198150635
I0131 12:36:30.810978 139668746123008 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.961606502532959, loss=2.342592716217041
I0131 12:37:16.583605 139668754515712 logging_writer.py:48] [169900] global_step=169900, grad_norm=5.52873420715332, loss=2.2612528800964355
I0131 12:38:02.709555 139668746123008 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.72649621963501, loss=2.5155344009399414
I0131 12:38:48.120052 139668754515712 logging_writer.py:48] [170100] global_step=170100, grad_norm=5.221352577209473, loss=2.2698588371276855
I0131 12:39:34.120827 139668746123008 logging_writer.py:48] [170200] global_step=170200, grad_norm=5.027264595031738, loss=2.248811960220337
I0131 12:40:19.845025 139668754515712 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.900644302368164, loss=2.2581732273101807
I0131 12:41:05.654094 139668746123008 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.987719535827637, loss=2.2220020294189453
I0131 12:41:51.165492 139668754515712 logging_writer.py:48] [170500] global_step=170500, grad_norm=6.3174662590026855, loss=4.058385372161865
I0131 12:42:08.732513 139863983413056 spec.py:321] Evaluating on the training split.
I0131 12:42:19.153839 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 12:42:41.269514 139863983413056 spec.py:349] Evaluating on the test split.
I0131 12:42:42.910861 139863983413056 submission_runner.py:408] Time since start: 82874.56s, 	Step: 170540, 	{'train/accuracy': 0.8365820050239563, 'train/loss': 0.7234076261520386, 'validation/accuracy': 0.7600599527359009, 'validation/loss': 1.0516064167022705, 'validation/num_examples': 50000, 'test/accuracy': 0.6373000144958496, 'test/loss': 1.6481930017471313, 'test/num_examples': 10000, 'score': 76927.61034536362, 'total_duration': 82874.5614593029, 'accumulated_submission_time': 76927.61034536362, 'accumulated_eval_time': 5929.933554887772, 'accumulated_logging_time': 8.157514572143555}
I0131 12:42:42.955382 139668746123008 logging_writer.py:48] [170540] accumulated_eval_time=5929.933555, accumulated_logging_time=8.157515, accumulated_submission_time=76927.610345, global_step=170540, preemption_count=0, score=76927.610345, test/accuracy=0.637300, test/loss=1.648193, test/num_examples=10000, total_duration=82874.561459, train/accuracy=0.836582, train/loss=0.723408, validation/accuracy=0.760060, validation/loss=1.051606, validation/num_examples=50000
I0131 12:43:07.330324 139668754515712 logging_writer.py:48] [170600] global_step=170600, grad_norm=5.556307315826416, loss=2.287470817565918
I0131 12:43:51.357445 139668746123008 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.775832653045654, loss=2.174168586730957
I0131 12:44:37.127497 139668754515712 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.88594913482666, loss=2.6353518962860107
I0131 12:45:22.831619 139668746123008 logging_writer.py:48] [170900] global_step=170900, grad_norm=5.123112201690674, loss=3.4189095497131348
I0131 12:46:08.336686 139668754515712 logging_writer.py:48] [171000] global_step=171000, grad_norm=6.009689807891846, loss=4.125308990478516
I0131 12:46:53.764761 139668746123008 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.9815354347229, loss=3.2128520011901855
I0131 12:47:39.384327 139668754515712 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.781745433807373, loss=2.270357608795166
I0131 12:48:25.114811 139668746123008 logging_writer.py:48] [171300] global_step=171300, grad_norm=5.47176456451416, loss=2.4464833736419678
I0131 12:49:10.780118 139668754515712 logging_writer.py:48] [171400] global_step=171400, grad_norm=5.243305206298828, loss=2.9556281566619873
I0131 12:49:43.137487 139863983413056 spec.py:321] Evaluating on the training split.
I0131 12:49:53.928816 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 12:50:15.767893 139863983413056 spec.py:349] Evaluating on the test split.
I0131 12:50:17.410928 139863983413056 submission_runner.py:408] Time since start: 83329.06s, 	Step: 171472, 	{'train/accuracy': 0.8395116925239563, 'train/loss': 0.7189873456954956, 'validation/accuracy': 0.7607799768447876, 'validation/loss': 1.0504368543624878, 'validation/num_examples': 50000, 'test/accuracy': 0.6414000391960144, 'test/loss': 1.6435645818710327, 'test/num_examples': 10000, 'score': 77347.73178625107, 'total_duration': 83329.06152820587, 'accumulated_submission_time': 77347.73178625107, 'accumulated_eval_time': 5964.20698928833, 'accumulated_logging_time': 8.21371054649353}
I0131 12:50:17.455425 139668746123008 logging_writer.py:48] [171472] accumulated_eval_time=5964.206989, accumulated_logging_time=8.213711, accumulated_submission_time=77347.731786, global_step=171472, preemption_count=0, score=77347.731786, test/accuracy=0.641400, test/loss=1.643565, test/num_examples=10000, total_duration=83329.061528, train/accuracy=0.839512, train/loss=0.718987, validation/accuracy=0.760780, validation/loss=1.050437, validation/num_examples=50000
I0131 12:50:29.195098 139668754515712 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.869399070739746, loss=2.2797868251800537
I0131 12:51:11.424573 139668746123008 logging_writer.py:48] [171600] global_step=171600, grad_norm=5.531389236450195, loss=2.417858600616455
I0131 12:51:56.840481 139668754515712 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.809919834136963, loss=2.7020978927612305
I0131 12:52:42.560533 139668746123008 logging_writer.py:48] [171800] global_step=171800, grad_norm=5.258006572723389, loss=2.252964735031128
I0131 12:53:09.902001 139668754515712 logging_writer.py:48] [171861] global_step=171861, preemption_count=0, score=77520.089318
I0131 12:53:10.580695 139863983413056 checkpoints.py:490] Saving checkpoint at step: 171861
I0131 12:53:11.987503 139863983413056 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_1/checkpoint_171861
I0131 12:53:12.005386 139863983413056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_1/checkpoint_171861.
I0131 12:53:12.918621 139863983413056 submission_runner.py:583] Tuning trial 1/5
I0131 12:53:12.918854 139863983413056 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0131 12:53:12.926201 139863983413056 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 42.85896849632263, 'total_duration': 83.16339540481567, 'accumulated_submission_time': 42.85896849632263, 'accumulated_eval_time': 40.30432653427124, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (898, {'train/accuracy': 0.014863280579447746, 'train/loss': 6.403157711029053, 'validation/accuracy': 0.014499999582767487, 'validation/loss': 6.414474010467529, 'validation/num_examples': 50000, 'test/accuracy': 0.010500000789761543, 'test/loss': 6.4579176902771, 'test/num_examples': 10000, 'score': 462.88109707832336, 'total_duration': 524.9953355789185, 'accumulated_submission_time': 462.88109707832336, 'accumulated_eval_time': 62.0397412776947, 'accumulated_logging_time': 0.02691793441772461, 'global_step': 898, 'preemption_count': 0}), (1848, {'train/accuracy': 0.040546875447034836, 'train/loss': 5.837361812591553, 'validation/accuracy': 0.039159998297691345, 'validation/loss': 5.86493444442749, 'validation/num_examples': 50000, 'test/accuracy': 0.03280000016093254, 'test/loss': 5.980993270874023, 'test/num_examples': 10000, 'score': 883.0495610237122, 'total_duration': 966.7851572036743, 'accumulated_submission_time': 883.0495610237122, 'accumulated_eval_time': 83.5831470489502, 'accumulated_logging_time': 0.05479598045349121, 'global_step': 1848, 'preemption_count': 0}), (2799, {'train/accuracy': 0.0707421824336052, 'train/loss': 5.413792610168457, 'validation/accuracy': 0.06505999714136124, 'validation/loss': 5.469770431518555, 'validation/num_examples': 50000, 'test/accuracy': 0.05170000344514847, 'test/loss': 5.6387457847595215, 'test/num_examples': 10000, 'score': 1303.4351394176483, 'total_duration': 1408.873723268509, 'accumulated_submission_time': 1303.4351394176483, 'accumulated_eval_time': 105.20509386062622, 'accumulated_logging_time': 0.08552002906799316, 'global_step': 2799, 'preemption_count': 0}), (3749, {'train/accuracy': 0.10007812082767487, 'train/loss': 5.093623638153076, 'validation/accuracy': 0.09064000099897385, 'validation/loss': 5.134443283081055, 'validation/num_examples': 50000, 'test/accuracy': 0.07300000637769699, 'test/loss': 5.3636393547058105, 'test/num_examples': 10000, 'score': 1723.6793518066406, 'total_duration': 1851.0162580013275, 'accumulated_submission_time': 1723.6793518066406, 'accumulated_eval_time': 127.02318668365479, 'accumulated_logging_time': 0.11602115631103516, 'global_step': 3749, 'preemption_count': 0}), (4697, {'train/accuracy': 0.14359374344348907, 'train/loss': 4.689797401428223, 'validation/accuracy': 0.13179999589920044, 'validation/loss': 4.742933750152588, 'validation/num_examples': 50000, 'test/accuracy': 0.10000000149011612, 'test/loss': 5.027024269104004, 'test/num_examples': 10000, 'score': 2143.966569185257, 'total_duration': 2293.278913974762, 'accumulated_submission_time': 2143.966569185257, 'accumulated_eval_time': 148.92200636863708, 'accumulated_logging_time': 0.1431868076324463, 'global_step': 4697, 'preemption_count': 0}), (5641, {'train/accuracy': 0.18896484375, 'train/loss': 4.296420097351074, 'validation/accuracy': 0.17146000266075134, 'validation/loss': 4.386421203613281, 'validation/num_examples': 50000, 'test/accuracy': 0.12770000100135803, 'test/loss': 4.731691837310791, 'test/num_examples': 10000, 'score': 2564.190915584564, 'total_duration': 2735.6585891246796, 'accumulated_submission_time': 2564.190915584564, 'accumulated_eval_time': 170.99932527542114, 'accumulated_logging_time': 0.17235016822814941, 'global_step': 5641, 'preemption_count': 0}), (6584, {'train/accuracy': 0.23142577707767487, 'train/loss': 3.941406726837158, 'validation/accuracy': 0.2192399948835373, 'validation/loss': 4.0269927978515625, 'validation/num_examples': 50000, 'test/accuracy': 0.1648000031709671, 'test/loss': 4.412674903869629, 'test/num_examples': 10000, 'score': 2984.3381164073944, 'total_duration': 3180.604308128357, 'accumulated_submission_time': 2984.3381164073944, 'accumulated_eval_time': 195.720308303833, 'accumulated_logging_time': 0.20057272911071777, 'global_step': 6584, 'preemption_count': 0}), (7526, {'train/accuracy': 0.279296875, 'train/loss': 3.6524882316589355, 'validation/accuracy': 0.2576200067996979, 'validation/loss': 3.760484218597412, 'validation/num_examples': 50000, 'test/accuracy': 0.19610001146793365, 'test/loss': 4.192534446716309, 'test/num_examples': 10000, 'score': 3404.479038000107, 'total_duration': 3625.622143268585, 'accumulated_submission_time': 3404.479038000107, 'accumulated_eval_time': 220.51652264595032, 'accumulated_logging_time': 0.23270916938781738, 'global_step': 7526, 'preemption_count': 0}), (8473, {'train/accuracy': 0.3104882836341858, 'train/loss': 3.4264824390411377, 'validation/accuracy': 0.28867998719215393, 'validation/loss': 3.565650224685669, 'validation/num_examples': 50000, 'test/accuracy': 0.2160000056028366, 'test/loss': 4.029370307922363, 'test/num_examples': 10000, 'score': 3824.4939839839935, 'total_duration': 4077.5829815864563, 'accumulated_submission_time': 3824.4939839839935, 'accumulated_eval_time': 252.37112522125244, 'accumulated_logging_time': 0.2743103504180908, 'global_step': 8473, 'preemption_count': 0}), (9418, {'train/accuracy': 0.3550195097923279, 'train/loss': 3.1795032024383545, 'validation/accuracy': 0.3194199800491333, 'validation/loss': 3.3503851890563965, 'validation/num_examples': 50000, 'test/accuracy': 0.24330000579357147, 'test/loss': 3.854766607284546, 'test/num_examples': 10000, 'score': 4244.734249830246, 'total_duration': 4523.618048667908, 'accumulated_submission_time': 4244.734249830246, 'accumulated_eval_time': 278.07969093322754, 'accumulated_logging_time': 0.3103628158569336, 'global_step': 9418, 'preemption_count': 0}), (10359, {'train/accuracy': 0.3695117235183716, 'train/loss': 3.0522427558898926, 'validation/accuracy': 0.3451800048351288, 'validation/loss': 3.1714067459106445, 'validation/num_examples': 50000, 'test/accuracy': 0.267300009727478, 'test/loss': 3.692898988723755, 'test/num_examples': 10000, 'score': 4664.890008926392, 'total_duration': 4972.636468410492, 'accumulated_submission_time': 4664.890008926392, 'accumulated_eval_time': 306.845290184021, 'accumulated_logging_time': 0.3585376739501953, 'global_step': 10359, 'preemption_count': 0}), (11303, {'train/accuracy': 0.3982031047344208, 'train/loss': 2.9216232299804688, 'validation/accuracy': 0.3621799945831299, 'validation/loss': 3.0729660987854004, 'validation/num_examples': 50000, 'test/accuracy': 0.2752000093460083, 'test/loss': 3.60302472114563, 'test/num_examples': 10000, 'score': 5085.055969715118, 'total_duration': 5419.18580698967, 'accumulated_submission_time': 5085.055969715118, 'accumulated_eval_time': 333.15025997161865, 'accumulated_logging_time': 0.38761043548583984, 'global_step': 11303, 'preemption_count': 0}), (12245, {'train/accuracy': 0.4265234172344208, 'train/loss': 2.743405342102051, 'validation/accuracy': 0.38815999031066895, 'validation/loss': 2.927395820617676, 'validation/num_examples': 50000, 'test/accuracy': 0.3027999997138977, 'test/loss': 3.4700565338134766, 'test/num_examples': 10000, 'score': 5505.355977058411, 'total_duration': 5868.255347728729, 'accumulated_submission_time': 5505.355977058411, 'accumulated_eval_time': 361.84075355529785, 'accumulated_logging_time': 0.4170222282409668, 'global_step': 12245, 'preemption_count': 0}), (13186, {'train/accuracy': 0.43205076456069946, 'train/loss': 2.7276670932769775, 'validation/accuracy': 0.4016000032424927, 'validation/loss': 2.8690223693847656, 'validation/num_examples': 50000, 'test/accuracy': 0.30970001220703125, 'test/loss': 3.434152841567993, 'test/num_examples': 10000, 'score': 5925.322516679764, 'total_duration': 6319.537341594696, 'accumulated_submission_time': 5925.322516679764, 'accumulated_eval_time': 393.07268619537354, 'accumulated_logging_time': 0.4512176513671875, 'global_step': 13186, 'preemption_count': 0}), (14124, {'train/accuracy': 0.44664061069488525, 'train/loss': 2.5795199871063232, 'validation/accuracy': 0.4178600013256073, 'validation/loss': 2.742652177810669, 'validation/num_examples': 50000, 'test/accuracy': 0.3222000300884247, 'test/loss': 3.3154313564300537, 'test/num_examples': 10000, 'score': 6345.321208238602, 'total_duration': 6770.492334604263, 'accumulated_submission_time': 6345.321208238602, 'accumulated_eval_time': 423.94589376449585, 'accumulated_logging_time': 0.48592185974121094, 'global_step': 14124, 'preemption_count': 0}), (15061, {'train/accuracy': 0.46728515625, 'train/loss': 2.460857391357422, 'validation/accuracy': 0.432559996843338, 'validation/loss': 2.633084297180176, 'validation/num_examples': 50000, 'test/accuracy': 0.3351000249385834, 'test/loss': 3.2048323154449463, 'test/num_examples': 10000, 'score': 6765.518880844116, 'total_duration': 7221.430178642273, 'accumulated_submission_time': 6765.518880844116, 'accumulated_eval_time': 454.60587215423584, 'accumulated_logging_time': 0.5157270431518555, 'global_step': 15061, 'preemption_count': 0}), (15996, {'train/accuracy': 0.4797070324420929, 'train/loss': 2.4331743717193604, 'validation/accuracy': 0.428739994764328, 'validation/loss': 2.6840686798095703, 'validation/num_examples': 50000, 'test/accuracy': 0.3368000090122223, 'test/loss': 3.2335305213928223, 'test/num_examples': 10000, 'score': 7185.779655456543, 'total_duration': 7672.20264005661, 'accumulated_submission_time': 7185.779655456543, 'accumulated_eval_time': 485.0385265350342, 'accumulated_logging_time': 0.5460660457611084, 'global_step': 15996, 'preemption_count': 0}), (16932, {'train/accuracy': 0.47164061665534973, 'train/loss': 2.5049636363983154, 'validation/accuracy': 0.4402399957180023, 'validation/loss': 2.659813642501831, 'validation/num_examples': 50000, 'test/accuracy': 0.3441000282764435, 'test/loss': 3.238809108734131, 'test/num_examples': 10000, 'score': 7606.167441606522, 'total_duration': 8123.402855873108, 'accumulated_submission_time': 7606.167441606522, 'accumulated_eval_time': 515.7722768783569, 'accumulated_logging_time': 0.5766818523406982, 'global_step': 16932, 'preemption_count': 0}), (17870, {'train/accuracy': 0.49119138717651367, 'train/loss': 2.32535719871521, 'validation/accuracy': 0.4554999768733978, 'validation/loss': 2.496633529663086, 'validation/num_examples': 50000, 'test/accuracy': 0.3556000292301178, 'test/loss': 3.085073232650757, 'test/num_examples': 10000, 'score': 8026.408129453659, 'total_duration': 8576.917443037033, 'accumulated_submission_time': 8026.408129453659, 'accumulated_eval_time': 548.9488704204559, 'accumulated_logging_time': 0.6240944862365723, 'global_step': 17870, 'preemption_count': 0}), (18809, {'train/accuracy': 0.5016992092132568, 'train/loss': 2.3369905948638916, 'validation/accuracy': 0.4569000005722046, 'validation/loss': 2.542475938796997, 'validation/num_examples': 50000, 'test/accuracy': 0.3546000123023987, 'test/loss': 3.1322598457336426, 'test/num_examples': 10000, 'score': 8446.362316608429, 'total_duration': 9030.816624879837, 'accumulated_submission_time': 8446.362316608429, 'accumulated_eval_time': 582.8137938976288, 'accumulated_logging_time': 0.6556878089904785, 'global_step': 18809, 'preemption_count': 0}), (19749, {'train/accuracy': 0.4941796660423279, 'train/loss': 2.3316903114318848, 'validation/accuracy': 0.4655799865722656, 'validation/loss': 2.4985291957855225, 'validation/num_examples': 50000, 'test/accuracy': 0.3619000315666199, 'test/loss': 3.080425500869751, 'test/num_examples': 10000, 'score': 8866.605522155762, 'total_duration': 9488.30536866188, 'accumulated_submission_time': 8866.605522155762, 'accumulated_eval_time': 619.9755432605743, 'accumulated_logging_time': 0.6908133029937744, 'global_step': 19749, 'preemption_count': 0}), (20689, {'train/accuracy': 0.5057812333106995, 'train/loss': 2.29396653175354, 'validation/accuracy': 0.4740999937057495, 'validation/loss': 2.452284574508667, 'validation/num_examples': 50000, 'test/accuracy': 0.366100013256073, 'test/loss': 3.067256450653076, 'test/num_examples': 10000, 'score': 9286.701465845108, 'total_duration': 9947.210430145264, 'accumulated_submission_time': 9286.701465845108, 'accumulated_eval_time': 658.7014982700348, 'accumulated_logging_time': 0.7256288528442383, 'global_step': 20689, 'preemption_count': 0}), (21618, {'train/accuracy': 0.5250585675239563, 'train/loss': 2.16995906829834, 'validation/accuracy': 0.48357999324798584, 'validation/loss': 2.3651888370513916, 'validation/num_examples': 50000, 'test/accuracy': 0.3717000186443329, 'test/loss': 2.9770636558532715, 'test/num_examples': 10000, 'score': 9706.624783277512, 'total_duration': 10405.515013217926, 'accumulated_submission_time': 9706.624783277512, 'accumulated_eval_time': 696.9998137950897, 'accumulated_logging_time': 0.7613976001739502, 'global_step': 21618, 'preemption_count': 0}), (22550, {'train/accuracy': 0.5533007979393005, 'train/loss': 2.0510404109954834, 'validation/accuracy': 0.4919999837875366, 'validation/loss': 2.332852840423584, 'validation/num_examples': 50000, 'test/accuracy': 0.3823000192642212, 'test/loss': 2.93916916847229, 'test/num_examples': 10000, 'score': 10126.87127161026, 'total_duration': 10860.985714435577, 'accumulated_submission_time': 10126.87127161026, 'accumulated_eval_time': 732.1488399505615, 'accumulated_logging_time': 0.7885289192199707, 'global_step': 22550, 'preemption_count': 0}), (23484, {'train/accuracy': 0.5318750143051147, 'train/loss': 2.154400587081909, 'validation/accuracy': 0.49793997406959534, 'validation/loss': 2.32008957862854, 'validation/num_examples': 50000, 'test/accuracy': 0.3897000253200531, 'test/loss': 2.925745725631714, 'test/num_examples': 10000, 'score': 10547.10615158081, 'total_duration': 11317.205980062485, 'accumulated_submission_time': 10547.10615158081, 'accumulated_eval_time': 768.0584897994995, 'accumulated_logging_time': 0.816164493560791, 'global_step': 23484, 'preemption_count': 0}), (24417, {'train/accuracy': 0.5354101657867432, 'train/loss': 2.1295690536499023, 'validation/accuracy': 0.49625998735427856, 'validation/loss': 2.312972068786621, 'validation/num_examples': 50000, 'test/accuracy': 0.3899000287055969, 'test/loss': 2.9328761100769043, 'test/num_examples': 10000, 'score': 10967.08280968666, 'total_duration': 11771.618568897247, 'accumulated_submission_time': 10967.08280968666, 'accumulated_eval_time': 802.4123823642731, 'accumulated_logging_time': 0.8501076698303223, 'global_step': 24417, 'preemption_count': 0}), (25349, {'train/accuracy': 0.5593945384025574, 'train/loss': 1.9963288307189941, 'validation/accuracy': 0.5059399604797363, 'validation/loss': 2.242471933364868, 'validation/num_examples': 50000, 'test/accuracy': 0.3992000222206116, 'test/loss': 2.854811668395996, 'test/num_examples': 10000, 'score': 11387.141655921936, 'total_duration': 12225.600093364716, 'accumulated_submission_time': 11387.141655921936, 'accumulated_eval_time': 836.2594237327576, 'accumulated_logging_time': 0.8776521682739258, 'global_step': 25349, 'preemption_count': 0}), (26274, {'train/accuracy': 0.5405468344688416, 'train/loss': 2.104295015335083, 'validation/accuracy': 0.5084199905395508, 'validation/loss': 2.2594079971313477, 'validation/num_examples': 50000, 'test/accuracy': 0.40050002932548523, 'test/loss': 2.8530092239379883, 'test/num_examples': 10000, 'score': 11806.869129419327, 'total_duration': 12677.361342906952, 'accumulated_submission_time': 11806.869129419327, 'accumulated_eval_time': 867.9045441150665, 'accumulated_logging_time': 1.2171745300292969, 'global_step': 26274, 'preemption_count': 0}), (27207, {'train/accuracy': 0.5548046827316284, 'train/loss': 2.044733762741089, 'validation/accuracy': 0.5141599774360657, 'validation/loss': 2.2269556522369385, 'validation/num_examples': 50000, 'test/accuracy': 0.4026000201702118, 'test/loss': 2.8585050106048584, 'test/num_examples': 10000, 'score': 12227.276673793793, 'total_duration': 13128.899226903915, 'accumulated_submission_time': 12227.276673793793, 'accumulated_eval_time': 898.9583828449249, 'accumulated_logging_time': 1.245424747467041, 'global_step': 27207, 'preemption_count': 0}), (28137, {'train/accuracy': 0.5671679377555847, 'train/loss': 1.9500863552093506, 'validation/accuracy': 0.5212999582290649, 'validation/loss': 2.152677536010742, 'validation/num_examples': 50000, 'test/accuracy': 0.4109000265598297, 'test/loss': 2.7780418395996094, 'test/num_examples': 10000, 'score': 12647.221132278442, 'total_duration': 13582.409242868423, 'accumulated_submission_time': 12647.221132278442, 'accumulated_eval_time': 932.4390978813171, 'accumulated_logging_time': 1.2829210758209229, 'global_step': 28137, 'preemption_count': 0}), (29071, {'train/accuracy': 0.570507824420929, 'train/loss': 1.9200985431671143, 'validation/accuracy': 0.5317800045013428, 'validation/loss': 2.108691453933716, 'validation/num_examples': 50000, 'test/accuracy': 0.42430001497268677, 'test/loss': 2.731639862060547, 'test/num_examples': 10000, 'score': 13067.440270900726, 'total_duration': 14037.257809400558, 'accumulated_submission_time': 13067.440270900726, 'accumulated_eval_time': 966.9934012889862, 'accumulated_logging_time': 1.3104724884033203, 'global_step': 29071, 'preemption_count': 0}), (30004, {'train/accuracy': 0.5688085556030273, 'train/loss': 1.989999532699585, 'validation/accuracy': 0.5308399796485901, 'validation/loss': 2.1682181358337402, 'validation/num_examples': 50000, 'test/accuracy': 0.41940000653266907, 'test/loss': 2.765523672103882, 'test/num_examples': 10000, 'score': 13487.520725488663, 'total_duration': 14489.614025115967, 'accumulated_submission_time': 13487.520725488663, 'accumulated_eval_time': 999.1877725124359, 'accumulated_logging_time': 1.343907117843628, 'global_step': 30004, 'preemption_count': 0}), (30934, {'train/accuracy': 0.5673046708106995, 'train/loss': 1.9936236143112183, 'validation/accuracy': 0.526419997215271, 'validation/loss': 2.189337968826294, 'validation/num_examples': 50000, 'test/accuracy': 0.41370001435279846, 'test/loss': 2.810401201248169, 'test/num_examples': 10000, 'score': 13907.610777139664, 'total_duration': 14941.843090057373, 'accumulated_submission_time': 13907.610777139664, 'accumulated_eval_time': 1031.2494950294495, 'accumulated_logging_time': 1.3736467361450195, 'global_step': 30934, 'preemption_count': 0}), (31864, {'train/accuracy': 0.5966015458106995, 'train/loss': 1.8194531202316284, 'validation/accuracy': 0.5347200036048889, 'validation/loss': 2.106844902038574, 'validation/num_examples': 50000, 'test/accuracy': 0.4237000346183777, 'test/loss': 2.7296688556671143, 'test/num_examples': 10000, 'score': 14327.601864337921, 'total_duration': 15395.709174633026, 'accumulated_submission_time': 14327.601864337921, 'accumulated_eval_time': 1065.047378540039, 'accumulated_logging_time': 1.4030015468597412, 'global_step': 31864, 'preemption_count': 0}), (32796, {'train/accuracy': 0.5743749737739563, 'train/loss': 1.9367910623550415, 'validation/accuracy': 0.5367400050163269, 'validation/loss': 2.105886220932007, 'validation/num_examples': 50000, 'test/accuracy': 0.4262000322341919, 'test/loss': 2.733640193939209, 'test/num_examples': 10000, 'score': 14747.678482532501, 'total_duration': 15849.318536996841, 'accumulated_submission_time': 14747.678482532501, 'accumulated_eval_time': 1098.5008039474487, 'accumulated_logging_time': 1.434175968170166, 'global_step': 32796, 'preemption_count': 0}), (33728, {'train/accuracy': 0.5781444907188416, 'train/loss': 1.909422755241394, 'validation/accuracy': 0.536899983882904, 'validation/loss': 2.1106417179107666, 'validation/num_examples': 50000, 'test/accuracy': 0.4223000109195709, 'test/loss': 2.7446610927581787, 'test/num_examples': 10000, 'score': 15167.680012226105, 'total_duration': 16303.204138755798, 'accumulated_submission_time': 15167.680012226105, 'accumulated_eval_time': 1132.3038840293884, 'accumulated_logging_time': 1.4670917987823486, 'global_step': 33728, 'preemption_count': 0}), (34663, {'train/accuracy': 0.5896288752555847, 'train/loss': 1.9078178405761719, 'validation/accuracy': 0.5367599725723267, 'validation/loss': 2.140103340148926, 'validation/num_examples': 50000, 'test/accuracy': 0.427700012922287, 'test/loss': 2.7481539249420166, 'test/num_examples': 10000, 'score': 15587.909015655518, 'total_duration': 16755.502063274384, 'accumulated_submission_time': 15587.909015655518, 'accumulated_eval_time': 1164.2943496704102, 'accumulated_logging_time': 1.4970180988311768, 'global_step': 34663, 'preemption_count': 0}), (35595, {'train/accuracy': 0.5802148580551147, 'train/loss': 1.8932175636291504, 'validation/accuracy': 0.5425800085067749, 'validation/loss': 2.0681850910186768, 'validation/num_examples': 50000, 'test/accuracy': 0.42820000648498535, 'test/loss': 2.7046916484832764, 'test/num_examples': 10000, 'score': 16008.223159313202, 'total_duration': 17209.556839942932, 'accumulated_submission_time': 16008.223159313202, 'accumulated_eval_time': 1197.9523015022278, 'accumulated_logging_time': 1.5312883853912354, 'global_step': 35595, 'preemption_count': 0}), (36527, {'train/accuracy': 0.5877343416213989, 'train/loss': 1.890945553779602, 'validation/accuracy': 0.545740008354187, 'validation/loss': 2.086134433746338, 'validation/num_examples': 50000, 'test/accuracy': 0.43640002608299255, 'test/loss': 2.7020034790039062, 'test/num_examples': 10000, 'score': 16428.311499357224, 'total_duration': 17662.983982801437, 'accumulated_submission_time': 16428.311499357224, 'accumulated_eval_time': 1231.2077586650848, 'accumulated_logging_time': 1.5664069652557373, 'global_step': 36527, 'preemption_count': 0}), (37459, {'train/accuracy': 0.5893945097923279, 'train/loss': 1.879970908164978, 'validation/accuracy': 0.5448200106620789, 'validation/loss': 2.087385654449463, 'validation/num_examples': 50000, 'test/accuracy': 0.43080002069473267, 'test/loss': 2.711873769760132, 'test/num_examples': 10000, 'score': 16848.334990262985, 'total_duration': 18117.32121515274, 'accumulated_submission_time': 16848.334990262985, 'accumulated_eval_time': 1265.4408011436462, 'accumulated_logging_time': 1.5989644527435303, 'global_step': 37459, 'preemption_count': 0}), (38392, {'train/accuracy': 0.6200585961341858, 'train/loss': 1.7613177299499512, 'validation/accuracy': 0.5546599626541138, 'validation/loss': 2.0597145557403564, 'validation/num_examples': 50000, 'test/accuracy': 0.43700000643730164, 'test/loss': 2.6826961040496826, 'test/num_examples': 10000, 'score': 17268.433282613754, 'total_duration': 18569.78191447258, 'accumulated_submission_time': 17268.433282613754, 'accumulated_eval_time': 1297.7227976322174, 'accumulated_logging_time': 1.631274700164795, 'global_step': 38392, 'preemption_count': 0}), (39325, {'train/accuracy': 0.5931640267372131, 'train/loss': 1.8362483978271484, 'validation/accuracy': 0.5575799942016602, 'validation/loss': 2.013503313064575, 'validation/num_examples': 50000, 'test/accuracy': 0.44110003113746643, 'test/loss': 2.634138584136963, 'test/num_examples': 10000, 'score': 17688.36795592308, 'total_duration': 19022.378975868225, 'accumulated_submission_time': 17688.36795592308, 'accumulated_eval_time': 1330.3051433563232, 'accumulated_logging_time': 1.663404941558838, 'global_step': 39325, 'preemption_count': 0}), (40258, {'train/accuracy': 0.5926952958106995, 'train/loss': 1.8907710313796997, 'validation/accuracy': 0.5488799810409546, 'validation/loss': 2.0913960933685303, 'validation/num_examples': 50000, 'test/accuracy': 0.44050002098083496, 'test/loss': 2.6993274688720703, 'test/num_examples': 10000, 'score': 18108.570605278015, 'total_duration': 19475.44114756584, 'accumulated_submission_time': 18108.570605278015, 'accumulated_eval_time': 1363.0829148292542, 'accumulated_logging_time': 1.6961984634399414, 'global_step': 40258, 'preemption_count': 0}), (41188, {'train/accuracy': 0.6147655844688416, 'train/loss': 1.7229764461517334, 'validation/accuracy': 0.560259997844696, 'validation/loss': 1.9867902994155884, 'validation/num_examples': 50000, 'test/accuracy': 0.44780001044273376, 'test/loss': 2.6145589351654053, 'test/num_examples': 10000, 'score': 18528.672277212143, 'total_duration': 19927.50222015381, 'accumulated_submission_time': 18528.672277212143, 'accumulated_eval_time': 1394.9611177444458, 'accumulated_logging_time': 1.7287757396697998, 'global_step': 41188, 'preemption_count': 0}), (42119, {'train/accuracy': 0.5964453220367432, 'train/loss': 1.8440462350845337, 'validation/accuracy': 0.558899998664856, 'validation/loss': 2.013521671295166, 'validation/num_examples': 50000, 'test/accuracy': 0.44630002975463867, 'test/loss': 2.636042594909668, 'test/num_examples': 10000, 'score': 18948.740561246872, 'total_duration': 20379.68393588066, 'accumulated_submission_time': 18948.740561246872, 'accumulated_eval_time': 1426.9955496788025, 'accumulated_logging_time': 1.7593953609466553, 'global_step': 42119, 'preemption_count': 0}), (43052, {'train/accuracy': 0.6012109518051147, 'train/loss': 1.8032618761062622, 'validation/accuracy': 0.5563399791717529, 'validation/loss': 1.9997272491455078, 'validation/num_examples': 50000, 'test/accuracy': 0.44930002093315125, 'test/loss': 2.613379716873169, 'test/num_examples': 10000, 'score': 19368.863209962845, 'total_duration': 20831.470037698746, 'accumulated_submission_time': 19368.863209962845, 'accumulated_eval_time': 1458.5802025794983, 'accumulated_logging_time': 1.7902934551239014, 'global_step': 43052, 'preemption_count': 0}), (43982, {'train/accuracy': 0.6162695288658142, 'train/loss': 1.7325092554092407, 'validation/accuracy': 0.5649799704551697, 'validation/loss': 1.9556329250335693, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.5773203372955322, 'test/num_examples': 10000, 'score': 19788.844311714172, 'total_duration': 21285.02067923546, 'accumulated_submission_time': 19788.844311714172, 'accumulated_eval_time': 1492.062311410904, 'accumulated_logging_time': 1.8297011852264404, 'global_step': 43982, 'preemption_count': 0}), (44914, {'train/accuracy': 0.6068750023841858, 'train/loss': 1.7359304428100586, 'validation/accuracy': 0.5651000142097473, 'validation/loss': 1.9321789741516113, 'validation/num_examples': 50000, 'test/accuracy': 0.45260003209114075, 'test/loss': 2.549083709716797, 'test/num_examples': 10000, 'score': 20208.771056890488, 'total_duration': 21737.755645275116, 'accumulated_submission_time': 20208.771056890488, 'accumulated_eval_time': 1524.7891371250153, 'accumulated_logging_time': 1.8625941276550293, 'global_step': 44914, 'preemption_count': 0}), (45847, {'train/accuracy': 0.6036913990974426, 'train/loss': 1.7868403196334839, 'validation/accuracy': 0.5648800134658813, 'validation/loss': 1.972473382949829, 'validation/num_examples': 50000, 'test/accuracy': 0.4513000249862671, 'test/loss': 2.5971016883850098, 'test/num_examples': 10000, 'score': 20628.775985956192, 'total_duration': 22189.844033002853, 'accumulated_submission_time': 20628.775985956192, 'accumulated_eval_time': 1556.7864758968353, 'accumulated_logging_time': 1.8994367122650146, 'global_step': 45847, 'preemption_count': 0}), (46780, {'train/accuracy': 0.6071484088897705, 'train/loss': 1.7338517904281616, 'validation/accuracy': 0.564520001411438, 'validation/loss': 1.9355089664459229, 'validation/num_examples': 50000, 'test/accuracy': 0.4513000249862671, 'test/loss': 2.5617563724517822, 'test/num_examples': 10000, 'score': 21048.85280585289, 'total_duration': 22643.165594816208, 'accumulated_submission_time': 21048.85280585289, 'accumulated_eval_time': 1589.9524147510529, 'accumulated_logging_time': 1.9305696487426758, 'global_step': 46780, 'preemption_count': 0}), (47715, {'train/accuracy': 0.6330664157867432, 'train/loss': 1.6488145589828491, 'validation/accuracy': 0.5689399838447571, 'validation/loss': 1.946183681488037, 'validation/num_examples': 50000, 'test/accuracy': 0.45410001277923584, 'test/loss': 2.5774097442626953, 'test/num_examples': 10000, 'score': 21468.807317256927, 'total_duration': 23097.087222337723, 'accumulated_submission_time': 21468.807317256927, 'accumulated_eval_time': 1623.8386886119843, 'accumulated_logging_time': 1.9631412029266357, 'global_step': 47715, 'preemption_count': 0}), (48651, {'train/accuracy': 0.6096093654632568, 'train/loss': 1.764771580696106, 'validation/accuracy': 0.5680999755859375, 'validation/loss': 1.94655179977417, 'validation/num_examples': 50000, 'test/accuracy': 0.4523000121116638, 'test/loss': 2.5711467266082764, 'test/num_examples': 10000, 'score': 21889.076536417007, 'total_duration': 23549.156180381775, 'accumulated_submission_time': 21889.076536417007, 'accumulated_eval_time': 1655.5556573867798, 'accumulated_logging_time': 1.9970552921295166, 'global_step': 48651, 'preemption_count': 0}), (49585, {'train/accuracy': 0.6127734184265137, 'train/loss': 1.7522122859954834, 'validation/accuracy': 0.5688999891281128, 'validation/loss': 1.9498313665390015, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.5811941623687744, 'test/num_examples': 10000, 'score': 22309.38073515892, 'total_duration': 24001.9259724617, 'accumulated_submission_time': 22309.38073515892, 'accumulated_eval_time': 1687.9413216114044, 'accumulated_logging_time': 2.028259038925171, 'global_step': 49585, 'preemption_count': 0}), (50516, {'train/accuracy': 0.6240820288658142, 'train/loss': 1.6619887351989746, 'validation/accuracy': 0.5757799744606018, 'validation/loss': 1.8856275081634521, 'validation/num_examples': 50000, 'test/accuracy': 0.4613000154495239, 'test/loss': 2.5196902751922607, 'test/num_examples': 10000, 'score': 22729.444053173065, 'total_duration': 24455.064513206482, 'accumulated_submission_time': 22729.444053173065, 'accumulated_eval_time': 1720.9364750385284, 'accumulated_logging_time': 2.0603320598602295, 'global_step': 50516, 'preemption_count': 0}), (51449, {'train/accuracy': 0.6103906035423279, 'train/loss': 1.7757805585861206, 'validation/accuracy': 0.5678399801254272, 'validation/loss': 1.9511852264404297, 'validation/num_examples': 50000, 'test/accuracy': 0.45740002393722534, 'test/loss': 2.5609421730041504, 'test/num_examples': 10000, 'score': 23149.631596565247, 'total_duration': 24908.316866636276, 'accumulated_submission_time': 23149.631596565247, 'accumulated_eval_time': 1753.9146332740784, 'accumulated_logging_time': 2.098830223083496, 'global_step': 51449, 'preemption_count': 0}), (52382, {'train/accuracy': 0.6201757788658142, 'train/loss': 1.6868958473205566, 'validation/accuracy': 0.5767799615859985, 'validation/loss': 1.8882721662521362, 'validation/num_examples': 50000, 'test/accuracy': 0.459600031375885, 'test/loss': 2.5248870849609375, 'test/num_examples': 10000, 'score': 23569.55105662346, 'total_duration': 25361.764173030853, 'accumulated_submission_time': 23569.55105662346, 'accumulated_eval_time': 1787.3572108745575, 'accumulated_logging_time': 2.134948968887329, 'global_step': 52382, 'preemption_count': 0}), (53312, {'train/accuracy': 0.6251562237739563, 'train/loss': 1.6901180744171143, 'validation/accuracy': 0.5781999826431274, 'validation/loss': 1.9079535007476807, 'validation/num_examples': 50000, 'test/accuracy': 0.46330001950263977, 'test/loss': 2.536857843399048, 'test/num_examples': 10000, 'score': 23989.921848773956, 'total_duration': 25815.295751094818, 'accumulated_submission_time': 23989.921848773956, 'accumulated_eval_time': 1820.4364140033722, 'accumulated_logging_time': 2.1685073375701904, 'global_step': 53312, 'preemption_count': 0}), (54243, {'train/accuracy': 0.6433789134025574, 'train/loss': 1.6116437911987305, 'validation/accuracy': 0.5797600150108337, 'validation/loss': 1.8849976062774658, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.5215423107147217, 'test/num_examples': 10000, 'score': 24410.181674718857, 'total_duration': 26268.046141147614, 'accumulated_submission_time': 24410.181674718857, 'accumulated_eval_time': 1852.839579820633, 'accumulated_logging_time': 2.207714319229126, 'global_step': 54243, 'preemption_count': 0}), (55173, {'train/accuracy': 0.6248632669448853, 'train/loss': 1.6888477802276611, 'validation/accuracy': 0.5834400057792664, 'validation/loss': 1.886579155921936, 'validation/num_examples': 50000, 'test/accuracy': 0.4659000337123871, 'test/loss': 2.519357681274414, 'test/num_examples': 10000, 'score': 24830.142689466476, 'total_duration': 26721.406907081604, 'accumulated_submission_time': 24830.142689466476, 'accumulated_eval_time': 1886.1577606201172, 'accumulated_logging_time': 2.2408063411712646, 'global_step': 55173, 'preemption_count': 0}), (56105, {'train/accuracy': 0.6288476586341858, 'train/loss': 1.6663347482681274, 'validation/accuracy': 0.5836799740791321, 'validation/loss': 1.8777925968170166, 'validation/num_examples': 50000, 'test/accuracy': 0.4644000232219696, 'test/loss': 2.5098447799682617, 'test/num_examples': 10000, 'score': 25250.628484487534, 'total_duration': 27173.98124217987, 'accumulated_submission_time': 25250.628484487534, 'accumulated_eval_time': 1918.1650898456573, 'accumulated_logging_time': 2.2733821868896484, 'global_step': 56105, 'preemption_count': 0}), (57036, {'train/accuracy': 0.644726574420929, 'train/loss': 1.6122007369995117, 'validation/accuracy': 0.5859599709510803, 'validation/loss': 1.8723894357681274, 'validation/num_examples': 50000, 'test/accuracy': 0.4666000306606293, 'test/loss': 2.5017263889312744, 'test/num_examples': 10000, 'score': 25670.61145210266, 'total_duration': 27626.93688774109, 'accumulated_submission_time': 25670.61145210266, 'accumulated_eval_time': 1951.056599855423, 'accumulated_logging_time': 2.3057284355163574, 'global_step': 57036, 'preemption_count': 0}), (57970, {'train/accuracy': 0.6248242259025574, 'train/loss': 1.6605851650238037, 'validation/accuracy': 0.5842999815940857, 'validation/loss': 1.8347177505493164, 'validation/num_examples': 50000, 'test/accuracy': 0.46570003032684326, 'test/loss': 2.4734883308410645, 'test/num_examples': 10000, 'score': 26091.046587705612, 'total_duration': 28080.764357566833, 'accumulated_submission_time': 26091.046587705612, 'accumulated_eval_time': 1984.3666186332703, 'accumulated_logging_time': 2.3380661010742188, 'global_step': 57970, 'preemption_count': 0}), (58900, {'train/accuracy': 0.6384179592132568, 'train/loss': 1.634387731552124, 'validation/accuracy': 0.5914799571037292, 'validation/loss': 1.8405027389526367, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.4767680168151855, 'test/num_examples': 10000, 'score': 26511.307859420776, 'total_duration': 28534.63808321953, 'accumulated_submission_time': 26511.307859420776, 'accumulated_eval_time': 2017.8947920799255, 'accumulated_logging_time': 2.3739945888519287, 'global_step': 58900, 'preemption_count': 0}), (59829, {'train/accuracy': 0.634570300579071, 'train/loss': 1.6421008110046387, 'validation/accuracy': 0.58406001329422, 'validation/loss': 1.8689597845077515, 'validation/num_examples': 50000, 'test/accuracy': 0.46700000762939453, 'test/loss': 2.510704517364502, 'test/num_examples': 10000, 'score': 26931.3852558136, 'total_duration': 28988.000118494034, 'accumulated_submission_time': 26931.3852558136, 'accumulated_eval_time': 2051.0959992408752, 'accumulated_logging_time': 2.4093093872070312, 'global_step': 59829, 'preemption_count': 0}), (60758, {'train/accuracy': 0.6297656297683716, 'train/loss': 1.6264452934265137, 'validation/accuracy': 0.5870400071144104, 'validation/loss': 1.8277873992919922, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.478414297103882, 'test/num_examples': 10000, 'score': 27351.664115190506, 'total_duration': 29439.022877693176, 'accumulated_submission_time': 27351.664115190506, 'accumulated_eval_time': 2081.7578916549683, 'accumulated_logging_time': 2.442704200744629, 'global_step': 60758, 'preemption_count': 0}), (61688, {'train/accuracy': 0.6357226371765137, 'train/loss': 1.6139147281646729, 'validation/accuracy': 0.5892199873924255, 'validation/loss': 1.826127290725708, 'validation/num_examples': 50000, 'test/accuracy': 0.4758000373840332, 'test/loss': 2.4539406299591064, 'test/num_examples': 10000, 'score': 27771.800651311874, 'total_duration': 29891.457787036896, 'accumulated_submission_time': 27771.800651311874, 'accumulated_eval_time': 2113.970644235611, 'accumulated_logging_time': 2.47971773147583, 'global_step': 61688, 'preemption_count': 0}), (62618, {'train/accuracy': 0.6410741806030273, 'train/loss': 1.5769309997558594, 'validation/accuracy': 0.594760000705719, 'validation/loss': 1.7927261590957642, 'validation/num_examples': 50000, 'test/accuracy': 0.4781000316143036, 'test/loss': 2.4324634075164795, 'test/num_examples': 10000, 'score': 28192.091106176376, 'total_duration': 30345.314726114273, 'accumulated_submission_time': 28192.091106176376, 'accumulated_eval_time': 2147.4553532600403, 'accumulated_logging_time': 2.5132803916931152, 'global_step': 62618, 'preemption_count': 0}), (63551, {'train/accuracy': 0.6690039038658142, 'train/loss': 1.4578577280044556, 'validation/accuracy': 0.5971800088882446, 'validation/loss': 1.771941065788269, 'validation/num_examples': 50000, 'test/accuracy': 0.4824000298976898, 'test/loss': 2.3906712532043457, 'test/num_examples': 10000, 'score': 28612.40795993805, 'total_duration': 30799.267755031586, 'accumulated_submission_time': 28612.40795993805, 'accumulated_eval_time': 2181.0035922527313, 'accumulated_logging_time': 2.5525503158569336, 'global_step': 63551, 'preemption_count': 0}), (64482, {'train/accuracy': 0.6347460746765137, 'train/loss': 1.6374387741088867, 'validation/accuracy': 0.5944199562072754, 'validation/loss': 1.8259129524230957, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.455456018447876, 'test/num_examples': 10000, 'score': 29032.35076379776, 'total_duration': 31253.455913305283, 'accumulated_submission_time': 29032.35076379776, 'accumulated_eval_time': 2215.1625061035156, 'accumulated_logging_time': 2.5897204875946045, 'global_step': 64482, 'preemption_count': 0}), (65414, {'train/accuracy': 0.6394921541213989, 'train/loss': 1.6012791395187378, 'validation/accuracy': 0.5922200083732605, 'validation/loss': 1.8196548223495483, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.461789846420288, 'test/num_examples': 10000, 'score': 29452.668427705765, 'total_duration': 31707.489936828613, 'accumulated_submission_time': 29452.668427705765, 'accumulated_eval_time': 2248.791541337967, 'accumulated_logging_time': 2.6287543773651123, 'global_step': 65414, 'preemption_count': 0}), (66344, {'train/accuracy': 0.659472644329071, 'train/loss': 1.5096582174301147, 'validation/accuracy': 0.6010400056838989, 'validation/loss': 1.7638288736343384, 'validation/num_examples': 50000, 'test/accuracy': 0.48270002007484436, 'test/loss': 2.3863396644592285, 'test/num_examples': 10000, 'score': 29872.83366370201, 'total_duration': 32161.46227788925, 'accumulated_submission_time': 29872.83366370201, 'accumulated_eval_time': 2282.513402938843, 'accumulated_logging_time': 2.6658544540405273, 'global_step': 66344, 'preemption_count': 0}), (67275, {'train/accuracy': 0.6406835913658142, 'train/loss': 1.6189045906066895, 'validation/accuracy': 0.6001200079917908, 'validation/loss': 1.808770775794983, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.4507222175598145, 'test/num_examples': 10000, 'score': 30292.992176771164, 'total_duration': 32615.257148742676, 'accumulated_submission_time': 30292.992176771164, 'accumulated_eval_time': 2316.063986301422, 'accumulated_logging_time': 2.7025163173675537, 'global_step': 67275, 'preemption_count': 0}), (68204, {'train/accuracy': 0.6403124928474426, 'train/loss': 1.6007063388824463, 'validation/accuracy': 0.5951399803161621, 'validation/loss': 1.8148831129074097, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.447077989578247, 'test/num_examples': 10000, 'score': 30712.97263765335, 'total_duration': 33067.442917346954, 'accumulated_submission_time': 30712.97263765335, 'accumulated_eval_time': 2348.187881231308, 'accumulated_logging_time': 2.735978603363037, 'global_step': 68204, 'preemption_count': 0}), (69131, {'train/accuracy': 0.6518359184265137, 'train/loss': 1.5784968137741089, 'validation/accuracy': 0.599399983882904, 'validation/loss': 1.8185914754867554, 'validation/num_examples': 50000, 'test/accuracy': 0.4790000319480896, 'test/loss': 2.451775312423706, 'test/num_examples': 10000, 'score': 31133.27097249031, 'total_duration': 33520.85844230652, 'accumulated_submission_time': 31133.27097249031, 'accumulated_eval_time': 2381.223728656769, 'accumulated_logging_time': 2.768976926803589, 'global_step': 69131, 'preemption_count': 0}), (70062, {'train/accuracy': 0.6419140696525574, 'train/loss': 1.6183698177337646, 'validation/accuracy': 0.5989399552345276, 'validation/loss': 1.8223744630813599, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.46195387840271, 'test/num_examples': 10000, 'score': 31553.394397974014, 'total_duration': 33972.844121456146, 'accumulated_submission_time': 31553.394397974014, 'accumulated_eval_time': 2413.0035569667816, 'accumulated_logging_time': 2.803950309753418, 'global_step': 70062, 'preemption_count': 0}), (70992, {'train/accuracy': 0.64990234375, 'train/loss': 1.5852410793304443, 'validation/accuracy': 0.6050199866294861, 'validation/loss': 1.7876743078231812, 'validation/num_examples': 50000, 'test/accuracy': 0.4846000373363495, 'test/loss': 2.424272298812866, 'test/num_examples': 10000, 'score': 31973.70577263832, 'total_duration': 34426.241938352585, 'accumulated_submission_time': 31973.70577263832, 'accumulated_eval_time': 2445.9967498779297, 'accumulated_logging_time': 2.8490071296691895, 'global_step': 70992, 'preemption_count': 0}), (71924, {'train/accuracy': 0.6512500047683716, 'train/loss': 1.5644874572753906, 'validation/accuracy': 0.6061800122261047, 'validation/loss': 1.7602328062057495, 'validation/num_examples': 50000, 'test/accuracy': 0.4853000342845917, 'test/loss': 2.3803160190582275, 'test/num_examples': 10000, 'score': 32393.788382530212, 'total_duration': 34879.81064558029, 'accumulated_submission_time': 32393.788382530212, 'accumulated_eval_time': 2479.398825407028, 'accumulated_logging_time': 2.8843657970428467, 'global_step': 71924, 'preemption_count': 0}), (72858, {'train/accuracy': 0.6725976467132568, 'train/loss': 1.4549552202224731, 'validation/accuracy': 0.6046000123023987, 'validation/loss': 1.746518850326538, 'validation/num_examples': 50000, 'test/accuracy': 0.4829000234603882, 'test/loss': 2.3781180381774902, 'test/num_examples': 10000, 'score': 32814.01478791237, 'total_duration': 35334.18482041359, 'accumulated_submission_time': 32814.01478791237, 'accumulated_eval_time': 2513.4549918174744, 'accumulated_logging_time': 2.9265990257263184, 'global_step': 72858, 'preemption_count': 0}), (73792, {'train/accuracy': 0.6545507907867432, 'train/loss': 1.5467392206192017, 'validation/accuracy': 0.6106799840927124, 'validation/loss': 1.7467910051345825, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.389852523803711, 'test/num_examples': 10000, 'score': 33234.2273080349, 'total_duration': 35787.68767333031, 'accumulated_submission_time': 33234.2273080349, 'accumulated_eval_time': 2546.6604709625244, 'accumulated_logging_time': 2.96217679977417, 'global_step': 73792, 'preemption_count': 0}), (74726, {'train/accuracy': 0.6555468440055847, 'train/loss': 1.561237096786499, 'validation/accuracy': 0.6126199960708618, 'validation/loss': 1.7528570890426636, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.3892462253570557, 'test/num_examples': 10000, 'score': 33654.26421165466, 'total_duration': 36240.71160840988, 'accumulated_submission_time': 33654.26421165466, 'accumulated_eval_time': 2579.5513093471527, 'accumulated_logging_time': 3.009035110473633, 'global_step': 74726, 'preemption_count': 0}), (75657, {'train/accuracy': 0.6675195097923279, 'train/loss': 1.4419922828674316, 'validation/accuracy': 0.6165199875831604, 'validation/loss': 1.693387746810913, 'validation/num_examples': 50000, 'test/accuracy': 0.49060001969337463, 'test/loss': 2.3438291549682617, 'test/num_examples': 10000, 'score': 34074.34876227379, 'total_duration': 36693.855113983154, 'accumulated_submission_time': 34074.34876227379, 'accumulated_eval_time': 2612.520934343338, 'accumulated_logging_time': 3.050046682357788, 'global_step': 75657, 'preemption_count': 0}), (76588, {'train/accuracy': 0.6521288752555847, 'train/loss': 1.591919183731079, 'validation/accuracy': 0.6082599759101868, 'validation/loss': 1.7924443483352661, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.4197306632995605, 'test/num_examples': 10000, 'score': 34494.56536388397, 'total_duration': 37144.96238017082, 'accumulated_submission_time': 34494.56536388397, 'accumulated_eval_time': 2643.3242888450623, 'accumulated_logging_time': 3.088836431503296, 'global_step': 76588, 'preemption_count': 0}), (77520, {'train/accuracy': 0.6513671875, 'train/loss': 1.5582828521728516, 'validation/accuracy': 0.6090599894523621, 'validation/loss': 1.7538148164749146, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.3950002193450928, 'test/num_examples': 10000, 'score': 34915.00139904022, 'total_duration': 37598.55631041527, 'accumulated_submission_time': 34915.00139904022, 'accumulated_eval_time': 2676.3921184539795, 'accumulated_logging_time': 3.1309778690338135, 'global_step': 77520, 'preemption_count': 0}), (78452, {'train/accuracy': 0.6617382764816284, 'train/loss': 1.5026801824569702, 'validation/accuracy': 0.6145200133323669, 'validation/loss': 1.7247921228408813, 'validation/num_examples': 50000, 'test/accuracy': 0.48840001225471497, 'test/loss': 2.357494354248047, 'test/num_examples': 10000, 'score': 35335.17891597748, 'total_duration': 38051.32184123993, 'accumulated_submission_time': 35335.17891597748, 'accumulated_eval_time': 2708.896152973175, 'accumulated_logging_time': 3.167029619216919, 'global_step': 78452, 'preemption_count': 0}), (79383, {'train/accuracy': 0.6783593893051147, 'train/loss': 1.4718852043151855, 'validation/accuracy': 0.612280011177063, 'validation/loss': 1.779030442237854, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.4213500022888184, 'test/num_examples': 10000, 'score': 35755.31320667267, 'total_duration': 38505.05619072914, 'accumulated_submission_time': 35755.31320667267, 'accumulated_eval_time': 2742.400264978409, 'accumulated_logging_time': 3.214477300643921, 'global_step': 79383, 'preemption_count': 0}), (80314, {'train/accuracy': 0.655468761920929, 'train/loss': 1.5589125156402588, 'validation/accuracy': 0.6153199672698975, 'validation/loss': 1.7471002340316772, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.3878822326660156, 'test/num_examples': 10000, 'score': 36175.472902059555, 'total_duration': 38958.28948068619, 'accumulated_submission_time': 36175.472902059555, 'accumulated_eval_time': 2775.382992506027, 'accumulated_logging_time': 3.256436347961426, 'global_step': 80314, 'preemption_count': 0}), (81246, {'train/accuracy': 0.672070324420929, 'train/loss': 1.45991849899292, 'validation/accuracy': 0.6218799948692322, 'validation/loss': 1.682550311088562, 'validation/num_examples': 50000, 'test/accuracy': 0.4992000162601471, 'test/loss': 2.3286523818969727, 'test/num_examples': 10000, 'score': 36595.45416688919, 'total_duration': 39412.10371303558, 'accumulated_submission_time': 36595.45416688919, 'accumulated_eval_time': 2809.1208050251007, 'accumulated_logging_time': 3.3032021522521973, 'global_step': 81246, 'preemption_count': 0}), (82178, {'train/accuracy': 0.677539050579071, 'train/loss': 1.4339114427566528, 'validation/accuracy': 0.6177999973297119, 'validation/loss': 1.7054755687713623, 'validation/num_examples': 50000, 'test/accuracy': 0.49160003662109375, 'test/loss': 2.343575954437256, 'test/num_examples': 10000, 'score': 37015.47077083588, 'total_duration': 39865.35120391846, 'accumulated_submission_time': 37015.47077083588, 'accumulated_eval_time': 2842.2659935951233, 'accumulated_logging_time': 3.3403608798980713, 'global_step': 82178, 'preemption_count': 0}), (83109, {'train/accuracy': 0.6681249737739563, 'train/loss': 1.490073561668396, 'validation/accuracy': 0.619879961013794, 'validation/loss': 1.6944104433059692, 'validation/num_examples': 50000, 'test/accuracy': 0.5033000111579895, 'test/loss': 2.3191699981689453, 'test/num_examples': 10000, 'score': 37435.68727660179, 'total_duration': 40315.00443935394, 'accumulated_submission_time': 37435.68727660179, 'accumulated_eval_time': 2871.618143796921, 'accumulated_logging_time': 3.3766629695892334, 'global_step': 83109, 'preemption_count': 0}), (84038, {'train/accuracy': 0.6651171445846558, 'train/loss': 1.465654730796814, 'validation/accuracy': 0.6218000054359436, 'validation/loss': 1.6730984449386597, 'validation/num_examples': 50000, 'test/accuracy': 0.49800002574920654, 'test/loss': 2.3054466247558594, 'test/num_examples': 10000, 'score': 37855.84566473961, 'total_duration': 40767.02870512009, 'accumulated_submission_time': 37855.84566473961, 'accumulated_eval_time': 2903.3922667503357, 'accumulated_logging_time': 3.419344186782837, 'global_step': 84038, 'preemption_count': 0}), (84971, {'train/accuracy': 0.6802929639816284, 'train/loss': 1.428873062133789, 'validation/accuracy': 0.6230999827384949, 'validation/loss': 1.6761740446090698, 'validation/num_examples': 50000, 'test/accuracy': 0.503000020980835, 'test/loss': 2.307889223098755, 'test/num_examples': 10000, 'score': 38275.783217191696, 'total_duration': 41219.60556221008, 'accumulated_submission_time': 38275.783217191696, 'accumulated_eval_time': 2935.9429478645325, 'accumulated_logging_time': 3.4596714973449707, 'global_step': 84971, 'preemption_count': 0}), (85903, {'train/accuracy': 0.6666210889816284, 'train/loss': 1.5058388710021973, 'validation/accuracy': 0.623259961605072, 'validation/loss': 1.7001662254333496, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.3466873168945312, 'test/num_examples': 10000, 'score': 38695.95108413696, 'total_duration': 41672.24136352539, 'accumulated_submission_time': 38695.95108413696, 'accumulated_eval_time': 2968.3241169452667, 'accumulated_logging_time': 3.4983112812042236, 'global_step': 85903, 'preemption_count': 0}), (86834, {'train/accuracy': 0.6658398509025574, 'train/loss': 1.5209107398986816, 'validation/accuracy': 0.6223999857902527, 'validation/loss': 1.7161909341812134, 'validation/num_examples': 50000, 'test/accuracy': 0.5, 'test/loss': 2.360891819000244, 'test/num_examples': 10000, 'score': 39116.04857087135, 'total_duration': 42123.79933476448, 'accumulated_submission_time': 39116.04857087135, 'accumulated_eval_time': 2999.6923208236694, 'accumulated_logging_time': 3.542412757873535, 'global_step': 86834, 'preemption_count': 0}), (87764, {'train/accuracy': 0.6809179782867432, 'train/loss': 1.4363369941711426, 'validation/accuracy': 0.6262999773025513, 'validation/loss': 1.676653504371643, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.3251760005950928, 'test/num_examples': 10000, 'score': 39536.07976317406, 'total_duration': 42577.90255379677, 'accumulated_submission_time': 39536.07976317406, 'accumulated_eval_time': 3033.672209739685, 'accumulated_logging_time': 3.5862622261047363, 'global_step': 87764, 'preemption_count': 0}), (88696, {'train/accuracy': 0.6968359351158142, 'train/loss': 1.3666648864746094, 'validation/accuracy': 0.6245999932289124, 'validation/loss': 1.67709481716156, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.3024938106536865, 'test/num_examples': 10000, 'score': 39956.33709144592, 'total_duration': 43030.892899513245, 'accumulated_submission_time': 39956.33709144592, 'accumulated_eval_time': 3066.3163137435913, 'accumulated_logging_time': 3.6272923946380615, 'global_step': 88696, 'preemption_count': 0}), (89628, {'train/accuracy': 0.6750390529632568, 'train/loss': 1.4493118524551392, 'validation/accuracy': 0.6331999897956848, 'validation/loss': 1.6430368423461914, 'validation/num_examples': 50000, 'test/accuracy': 0.5107000470161438, 'test/loss': 2.262535572052002, 'test/num_examples': 10000, 'score': 40376.373056173325, 'total_duration': 43481.935858011246, 'accumulated_submission_time': 40376.373056173325, 'accumulated_eval_time': 3097.2331142425537, 'accumulated_logging_time': 3.6694083213806152, 'global_step': 89628, 'preemption_count': 0}), (90554, {'train/accuracy': 0.6821093559265137, 'train/loss': 1.3882906436920166, 'validation/accuracy': 0.6303799748420715, 'validation/loss': 1.624939203262329, 'validation/num_examples': 50000, 'test/accuracy': 0.5046000480651855, 'test/loss': 2.265634775161743, 'test/num_examples': 10000, 'score': 40796.29656982422, 'total_duration': 43931.92548966408, 'accumulated_submission_time': 40796.29656982422, 'accumulated_eval_time': 3127.205216407776, 'accumulated_logging_time': 3.715543746948242, 'global_step': 90554, 'preemption_count': 0}), (91481, {'train/accuracy': 0.6850780844688416, 'train/loss': 1.3804093599319458, 'validation/accuracy': 0.6272000074386597, 'validation/loss': 1.6394050121307373, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2855710983276367, 'test/num_examples': 10000, 'score': 41216.31773328781, 'total_duration': 44384.83210873604, 'accumulated_submission_time': 41216.31773328781, 'accumulated_eval_time': 3159.993196964264, 'accumulated_logging_time': 3.765239715576172, 'global_step': 91481, 'preemption_count': 0}), (92412, {'train/accuracy': 0.6785351634025574, 'train/loss': 1.4298800230026245, 'validation/accuracy': 0.6362599730491638, 'validation/loss': 1.630461573600769, 'validation/num_examples': 50000, 'test/accuracy': 0.5117000341415405, 'test/loss': 2.257552146911621, 'test/num_examples': 10000, 'score': 41636.72108960152, 'total_duration': 44838.06574630737, 'accumulated_submission_time': 41636.72108960152, 'accumulated_eval_time': 3192.737065553665, 'accumulated_logging_time': 3.803889751434326, 'global_step': 92412, 'preemption_count': 0}), (93343, {'train/accuracy': 0.68212890625, 'train/loss': 1.4268741607666016, 'validation/accuracy': 0.6340399980545044, 'validation/loss': 1.6396797895431519, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2780954837799072, 'test/num_examples': 10000, 'score': 42057.02818584442, 'total_duration': 45289.685396671295, 'accumulated_submission_time': 42057.02818584442, 'accumulated_eval_time': 3223.965080499649, 'accumulated_logging_time': 3.8404388427734375, 'global_step': 93343, 'preemption_count': 0}), (94272, {'train/accuracy': 0.6900390386581421, 'train/loss': 1.358396291732788, 'validation/accuracy': 0.632860004901886, 'validation/loss': 1.6014665365219116, 'validation/num_examples': 50000, 'test/accuracy': 0.5105000138282776, 'test/loss': 2.2469406127929688, 'test/num_examples': 10000, 'score': 42476.98532438278, 'total_duration': 45741.764285326004, 'accumulated_submission_time': 42476.98532438278, 'accumulated_eval_time': 3255.977776527405, 'accumulated_logging_time': 3.900721549987793, 'global_step': 94272, 'preemption_count': 0}), (95203, {'train/accuracy': 0.69740229845047, 'train/loss': 1.3583762645721436, 'validation/accuracy': 0.6399799585342407, 'validation/loss': 1.6108720302581787, 'validation/num_examples': 50000, 'test/accuracy': 0.5120000243186951, 'test/loss': 2.2654266357421875, 'test/num_examples': 10000, 'score': 42896.978048563, 'total_duration': 46192.10404133797, 'accumulated_submission_time': 42896.978048563, 'accumulated_eval_time': 3286.2305703163147, 'accumulated_logging_time': 3.9465410709381104, 'global_step': 95203, 'preemption_count': 0}), (96132, {'train/accuracy': 0.6863671541213989, 'train/loss': 1.4062321186065674, 'validation/accuracy': 0.6400399804115295, 'validation/loss': 1.6143362522125244, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.2290306091308594, 'test/num_examples': 10000, 'score': 43316.982568740845, 'total_duration': 46642.69738292694, 'accumulated_submission_time': 43316.982568740845, 'accumulated_eval_time': 3316.7220873832703, 'accumulated_logging_time': 3.99554705619812, 'global_step': 96132, 'preemption_count': 0}), (97062, {'train/accuracy': 0.685546875, 'train/loss': 1.4155725240707397, 'validation/accuracy': 0.6375600099563599, 'validation/loss': 1.633133053779602, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.2834768295288086, 'test/num_examples': 10000, 'score': 43736.97590112686, 'total_duration': 47092.60734796524, 'accumulated_submission_time': 43736.97590112686, 'accumulated_eval_time': 3346.5461995601654, 'accumulated_logging_time': 4.039233446121216, 'global_step': 97062, 'preemption_count': 0}), (97992, {'train/accuracy': 0.7066406011581421, 'train/loss': 1.2971094846725464, 'validation/accuracy': 0.6403200030326843, 'validation/loss': 1.582248568534851, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.2080962657928467, 'test/num_examples': 10000, 'score': 44156.90197920799, 'total_duration': 47543.903435230255, 'accumulated_submission_time': 44156.90197920799, 'accumulated_eval_time': 3377.816866159439, 'accumulated_logging_time': 4.090280055999756, 'global_step': 97992, 'preemption_count': 0}), (98923, {'train/accuracy': 0.6883788704872131, 'train/loss': 1.393426775932312, 'validation/accuracy': 0.641319990158081, 'validation/loss': 1.6070661544799805, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.2544445991516113, 'test/num_examples': 10000, 'score': 44576.86071538925, 'total_duration': 47997.69262051582, 'accumulated_submission_time': 44576.86071538925, 'accumulated_eval_time': 3411.5551047325134, 'accumulated_logging_time': 4.134812593460083, 'global_step': 98923, 'preemption_count': 0}), (99856, {'train/accuracy': 0.6871289014816284, 'train/loss': 1.3720650672912598, 'validation/accuracy': 0.6365999579429626, 'validation/loss': 1.5965046882629395, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.2395246028900146, 'test/num_examples': 10000, 'score': 44996.839626550674, 'total_duration': 48449.945254564285, 'accumulated_submission_time': 44996.839626550674, 'accumulated_eval_time': 3443.7383959293365, 'accumulated_logging_time': 4.1757354736328125, 'global_step': 99856, 'preemption_count': 0}), (100785, {'train/accuracy': 0.7017577886581421, 'train/loss': 1.312267780303955, 'validation/accuracy': 0.6451999545097351, 'validation/loss': 1.5605461597442627, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.181353807449341, 'test/num_examples': 10000, 'score': 45416.97100830078, 'total_duration': 48903.77703499794, 'accumulated_submission_time': 45416.97100830078, 'accumulated_eval_time': 3477.3515434265137, 'accumulated_logging_time': 4.214470624923706, 'global_step': 100785, 'preemption_count': 0}), (101716, {'train/accuracy': 0.69349604845047, 'train/loss': 1.36771821975708, 'validation/accuracy': 0.6436399817466736, 'validation/loss': 1.5866146087646484, 'validation/num_examples': 50000, 'test/accuracy': 0.5249000191688538, 'test/loss': 2.2173092365264893, 'test/num_examples': 10000, 'score': 45837.05639505386, 'total_duration': 49357.55435633659, 'accumulated_submission_time': 45837.05639505386, 'accumulated_eval_time': 3510.946757078171, 'accumulated_logging_time': 4.263065338134766, 'global_step': 101716, 'preemption_count': 0}), (102645, {'train/accuracy': 0.6882616877555847, 'train/loss': 1.39771568775177, 'validation/accuracy': 0.6396999955177307, 'validation/loss': 1.6158915758132935, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.250572919845581, 'test/num_examples': 10000, 'score': 46257.20961642265, 'total_duration': 49808.57501959801, 'accumulated_submission_time': 46257.20961642265, 'accumulated_eval_time': 3541.7239258289337, 'accumulated_logging_time': 4.30495023727417, 'global_step': 102645, 'preemption_count': 0}), (103573, {'train/accuracy': 0.7084375023841858, 'train/loss': 1.2893983125686646, 'validation/accuracy': 0.6509000062942505, 'validation/loss': 1.5381120443344116, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.1615405082702637, 'test/num_examples': 10000, 'score': 46677.277096033096, 'total_duration': 50258.698894262314, 'accumulated_submission_time': 46677.277096033096, 'accumulated_eval_time': 3571.6743774414062, 'accumulated_logging_time': 4.362541913986206, 'global_step': 103573, 'preemption_count': 0}), (104501, {'train/accuracy': 0.7164257764816284, 'train/loss': 1.3079707622528076, 'validation/accuracy': 0.6469599604606628, 'validation/loss': 1.601804494857788, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.236384868621826, 'test/num_examples': 10000, 'score': 47097.76482272148, 'total_duration': 50709.717992305756, 'accumulated_submission_time': 47097.76482272148, 'accumulated_eval_time': 3602.106454372406, 'accumulated_logging_time': 4.4127349853515625, 'global_step': 104501, 'preemption_count': 0}), (105427, {'train/accuracy': 0.6813281178474426, 'train/loss': 1.432881474494934, 'validation/accuracy': 0.6276599764823914, 'validation/loss': 1.6766186952590942, 'validation/num_examples': 50000, 'test/accuracy': 0.5055000185966492, 'test/loss': 2.2996857166290283, 'test/num_examples': 10000, 'score': 47517.686259269714, 'total_duration': 51163.54053092003, 'accumulated_submission_time': 47517.686259269714, 'accumulated_eval_time': 3635.9119005203247, 'accumulated_logging_time': 4.460398435592651, 'global_step': 105427, 'preemption_count': 0}), (106359, {'train/accuracy': 0.7081249952316284, 'train/loss': 1.3071223497390747, 'validation/accuracy': 0.653439998626709, 'validation/loss': 1.5344473123550415, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.1776108741760254, 'test/num_examples': 10000, 'score': 47937.69239234924, 'total_duration': 51615.63697743416, 'accumulated_submission_time': 47937.69239234924, 'accumulated_eval_time': 3667.9130449295044, 'accumulated_logging_time': 4.501140356063843, 'global_step': 106359, 'preemption_count': 0}), (107289, {'train/accuracy': 0.7215625047683716, 'train/loss': 1.2222944498062134, 'validation/accuracy': 0.6552599668502808, 'validation/loss': 1.523241400718689, 'validation/num_examples': 50000, 'test/accuracy': 0.535800039768219, 'test/loss': 2.1337265968322754, 'test/num_examples': 10000, 'score': 48357.65385222435, 'total_duration': 52067.7736222744, 'accumulated_submission_time': 48357.65385222435, 'accumulated_eval_time': 3699.9985733032227, 'accumulated_logging_time': 4.5414369106292725, 'global_step': 107289, 'preemption_count': 0}), (108222, {'train/accuracy': 0.7074999809265137, 'train/loss': 1.2904741764068604, 'validation/accuracy': 0.6606400012969971, 'validation/loss': 1.5038014650344849, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.1375465393066406, 'test/num_examples': 10000, 'score': 48777.81280827522, 'total_duration': 52518.94608283043, 'accumulated_submission_time': 48777.81280827522, 'accumulated_eval_time': 3730.9171195030212, 'accumulated_logging_time': 4.586863279342651, 'global_step': 108222, 'preemption_count': 0}), (109153, {'train/accuracy': 0.7104882597923279, 'train/loss': 1.2939804792404175, 'validation/accuracy': 0.6558799743652344, 'validation/loss': 1.531550407409668, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.1510488986968994, 'test/num_examples': 10000, 'score': 49197.85523939133, 'total_duration': 52972.1984333992, 'accumulated_submission_time': 49197.85523939133, 'accumulated_eval_time': 3764.0278511047363, 'accumulated_logging_time': 4.637696266174316, 'global_step': 109153, 'preemption_count': 0}), (110084, {'train/accuracy': 0.7136523127555847, 'train/loss': 1.3198362588882446, 'validation/accuracy': 0.6524199843406677, 'validation/loss': 1.5822229385375977, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.2205541133880615, 'test/num_examples': 10000, 'score': 49617.85936307907, 'total_duration': 53426.40553641319, 'accumulated_submission_time': 49617.85936307907, 'accumulated_eval_time': 3798.1327052116394, 'accumulated_logging_time': 4.687514781951904, 'global_step': 110084, 'preemption_count': 0}), (111016, {'train/accuracy': 0.7125585675239563, 'train/loss': 1.2730872631072998, 'validation/accuracy': 0.6649599671363831, 'validation/loss': 1.4868433475494385, 'validation/num_examples': 50000, 'test/accuracy': 0.537600040435791, 'test/loss': 2.1088082790374756, 'test/num_examples': 10000, 'score': 50038.1953496933, 'total_duration': 53881.36761689186, 'accumulated_submission_time': 50038.1953496933, 'accumulated_eval_time': 3832.661499977112, 'accumulated_logging_time': 4.737124443054199, 'global_step': 111016, 'preemption_count': 0}), (111947, {'train/accuracy': 0.7182812094688416, 'train/loss': 1.2427871227264404, 'validation/accuracy': 0.6640799641609192, 'validation/loss': 1.481744647026062, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.1059060096740723, 'test/num_examples': 10000, 'score': 50458.330828905106, 'total_duration': 54329.931025743484, 'accumulated_submission_time': 50458.330828905106, 'accumulated_eval_time': 3860.9939935207367, 'accumulated_logging_time': 4.784902572631836, 'global_step': 111947, 'preemption_count': 0}), (112868, {'train/accuracy': 0.7190234065055847, 'train/loss': 1.2374742031097412, 'validation/accuracy': 0.6615200042724609, 'validation/loss': 1.4908949136734009, 'validation/num_examples': 50000, 'test/accuracy': 0.5367000102996826, 'test/loss': 2.121800184249878, 'test/num_examples': 10000, 'score': 50878.57825565338, 'total_duration': 54783.457184791565, 'accumulated_submission_time': 50878.57825565338, 'accumulated_eval_time': 3894.177620410919, 'accumulated_logging_time': 4.832216739654541, 'global_step': 112868, 'preemption_count': 0}), (113797, {'train/accuracy': 0.7408984303474426, 'train/loss': 1.1787163019180298, 'validation/accuracy': 0.666979968547821, 'validation/loss': 1.5026799440383911, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.132467031478882, 'test/num_examples': 10000, 'score': 51298.48287606239, 'total_duration': 55236.77574682236, 'accumulated_submission_time': 51298.48287606239, 'accumulated_eval_time': 3927.492713212967, 'accumulated_logging_time': 4.883016586303711, 'global_step': 113797, 'preemption_count': 0}), (114730, {'train/accuracy': 0.7202734351158142, 'train/loss': 1.223569393157959, 'validation/accuracy': 0.6654999852180481, 'validation/loss': 1.4597338438034058, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.0868093967437744, 'test/num_examples': 10000, 'score': 51718.38898730278, 'total_duration': 55690.430280447006, 'accumulated_submission_time': 51718.38898730278, 'accumulated_eval_time': 3961.1463055610657, 'accumulated_logging_time': 4.928993463516235, 'global_step': 114730, 'preemption_count': 0}), (115661, {'train/accuracy': 0.7249413728713989, 'train/loss': 1.2017278671264648, 'validation/accuracy': 0.6695599555969238, 'validation/loss': 1.4541553258895874, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.075801372528076, 'test/num_examples': 10000, 'score': 52138.31260251999, 'total_duration': 56142.81825685501, 'accumulated_submission_time': 52138.31260251999, 'accumulated_eval_time': 3993.5162620544434, 'accumulated_logging_time': 4.973785161972046, 'global_step': 115661, 'preemption_count': 0}), (116592, {'train/accuracy': 0.7337695360183716, 'train/loss': 1.1900928020477295, 'validation/accuracy': 0.675059974193573, 'validation/loss': 1.4655356407165527, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 2.100456476211548, 'test/num_examples': 10000, 'score': 52558.60073399544, 'total_duration': 56596.52179288864, 'accumulated_submission_time': 52558.60073399544, 'accumulated_eval_time': 4026.8397274017334, 'accumulated_logging_time': 5.018024444580078, 'global_step': 116592, 'preemption_count': 0}), (117526, {'train/accuracy': 0.730664074420929, 'train/loss': 1.1986579895019531, 'validation/accuracy': 0.6748799681663513, 'validation/loss': 1.4364898204803467, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 2.076021194458008, 'test/num_examples': 10000, 'score': 52978.923567056656, 'total_duration': 57048.03144288063, 'accumulated_submission_time': 52978.923567056656, 'accumulated_eval_time': 4057.929902076721, 'accumulated_logging_time': 5.0658793449401855, 'global_step': 117526, 'preemption_count': 0}), (118457, {'train/accuracy': 0.7255663871765137, 'train/loss': 1.2088725566864014, 'validation/accuracy': 0.6714800000190735, 'validation/loss': 1.4509501457214355, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.084545135498047, 'test/num_examples': 10000, 'score': 53399.125903367996, 'total_duration': 57499.6692841053, 'accumulated_submission_time': 53399.125903367996, 'accumulated_eval_time': 4089.267287492752, 'accumulated_logging_time': 5.115900993347168, 'global_step': 118457, 'preemption_count': 0}), (119385, {'train/accuracy': 0.7369726300239563, 'train/loss': 1.1480770111083984, 'validation/accuracy': 0.6777399778366089, 'validation/loss': 1.4084073305130005, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.038119316101074, 'test/num_examples': 10000, 'score': 53819.215997457504, 'total_duration': 57952.38986158371, 'accumulated_submission_time': 53819.215997457504, 'accumulated_eval_time': 4121.7964906692505, 'accumulated_logging_time': 5.169644355773926, 'global_step': 119385, 'preemption_count': 0}), (120316, {'train/accuracy': 0.7320312261581421, 'train/loss': 1.1968371868133545, 'validation/accuracy': 0.6744199991226196, 'validation/loss': 1.4489213228225708, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 2.078464984893799, 'test/num_examples': 10000, 'score': 54239.2432115078, 'total_duration': 58406.38825464249, 'accumulated_submission_time': 54239.2432115078, 'accumulated_eval_time': 4155.675201416016, 'accumulated_logging_time': 5.214017868041992, 'global_step': 120316, 'preemption_count': 0}), (121247, {'train/accuracy': 0.7334765195846558, 'train/loss': 1.1736871004104614, 'validation/accuracy': 0.6791200041770935, 'validation/loss': 1.4089908599853516, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 2.0281074047088623, 'test/num_examples': 10000, 'score': 54659.526497364044, 'total_duration': 58857.70808959007, 'accumulated_submission_time': 54659.526497364044, 'accumulated_eval_time': 4186.622891664505, 'accumulated_logging_time': 5.255097150802612, 'global_step': 121247, 'preemption_count': 0}), (122176, {'train/accuracy': 0.7381835579872131, 'train/loss': 1.1373696327209473, 'validation/accuracy': 0.6790800094604492, 'validation/loss': 1.3966187238693237, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.0180749893188477, 'test/num_examples': 10000, 'score': 55079.78484630585, 'total_duration': 59309.61920070648, 'accumulated_submission_time': 55079.78484630585, 'accumulated_eval_time': 4218.173988342285, 'accumulated_logging_time': 5.308438777923584, 'global_step': 122176, 'preemption_count': 0}), (123107, {'train/accuracy': 0.7488671541213989, 'train/loss': 1.1549774408340454, 'validation/accuracy': 0.681439995765686, 'validation/loss': 1.463075876235962, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 2.0738844871520996, 'test/num_examples': 10000, 'score': 55500.03019499779, 'total_duration': 59760.25983929634, 'accumulated_submission_time': 55500.03019499779, 'accumulated_eval_time': 4248.474764108658, 'accumulated_logging_time': 5.354437828063965, 'global_step': 123107, 'preemption_count': 0}), (124036, {'train/accuracy': 0.7421875, 'train/loss': 1.1317152976989746, 'validation/accuracy': 0.6843999624252319, 'validation/loss': 1.3768173456192017, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.0066962242126465, 'test/num_examples': 10000, 'score': 55920.1936044693, 'total_duration': 60211.03713226318, 'accumulated_submission_time': 55920.1936044693, 'accumulated_eval_time': 4278.991844892502, 'accumulated_logging_time': 5.403183937072754, 'global_step': 124036, 'preemption_count': 0}), (124965, {'train/accuracy': 0.742968738079071, 'train/loss': 1.1672155857086182, 'validation/accuracy': 0.6843799948692322, 'validation/loss': 1.420559048652649, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.050241708755493, 'test/num_examples': 10000, 'score': 56340.23328781128, 'total_duration': 60664.11954545975, 'accumulated_submission_time': 56340.23328781128, 'accumulated_eval_time': 4311.943066358566, 'accumulated_logging_time': 5.446610689163208, 'global_step': 124965, 'preemption_count': 0}), (125892, {'train/accuracy': 0.7481640577316284, 'train/loss': 1.098036289215088, 'validation/accuracy': 0.6840199828147888, 'validation/loss': 1.375110149383545, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.99488365650177, 'test/num_examples': 10000, 'score': 56760.46367549896, 'total_duration': 61117.63439536095, 'accumulated_submission_time': 56760.46367549896, 'accumulated_eval_time': 4345.138010501862, 'accumulated_logging_time': 5.488474130630493, 'global_step': 125892, 'preemption_count': 0}), (126822, {'train/accuracy': 0.7443749904632568, 'train/loss': 1.1106796264648438, 'validation/accuracy': 0.6901800036430359, 'validation/loss': 1.3542922735214233, 'validation/num_examples': 50000, 'test/accuracy': 0.5568000078201294, 'test/loss': 1.9962732791900635, 'test/num_examples': 10000, 'score': 57180.5708527565, 'total_duration': 61572.45949554443, 'accumulated_submission_time': 57180.5708527565, 'accumulated_eval_time': 4379.7619886398315, 'accumulated_logging_time': 5.534748315811157, 'global_step': 126822, 'preemption_count': 0}), (127753, {'train/accuracy': 0.7480273246765137, 'train/loss': 1.1173925399780273, 'validation/accuracy': 0.688759982585907, 'validation/loss': 1.3697985410690308, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9796117544174194, 'test/num_examples': 10000, 'score': 57600.62677979469, 'total_duration': 62026.65469145775, 'accumulated_submission_time': 57600.62677979469, 'accumulated_eval_time': 4413.811166524887, 'accumulated_logging_time': 5.5767738819122314, 'global_step': 127753, 'preemption_count': 0}), (128684, {'train/accuracy': 0.75990229845047, 'train/loss': 1.0859085321426392, 'validation/accuracy': 0.6924999952316284, 'validation/loss': 1.37213134765625, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9980629682540894, 'test/num_examples': 10000, 'score': 58020.767980098724, 'total_duration': 62480.12114715576, 'accumulated_submission_time': 58020.767980098724, 'accumulated_eval_time': 4447.039078474045, 'accumulated_logging_time': 5.6262922286987305, 'global_step': 128684, 'preemption_count': 0}), (129613, {'train/accuracy': 0.7639062404632568, 'train/loss': 1.050950288772583, 'validation/accuracy': 0.6924999952316284, 'validation/loss': 1.3538199663162231, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 1.9598753452301025, 'test/num_examples': 10000, 'score': 58440.78542947769, 'total_duration': 62934.27048492432, 'accumulated_submission_time': 58440.78542947769, 'accumulated_eval_time': 4481.064235448837, 'accumulated_logging_time': 5.68550181388855, 'global_step': 129613, 'preemption_count': 0}), (130545, {'train/accuracy': 0.7498437166213989, 'train/loss': 1.1301133632659912, 'validation/accuracy': 0.693120002746582, 'validation/loss': 1.3789575099945068, 'validation/num_examples': 50000, 'test/accuracy': 0.5597000122070312, 'test/loss': 2.01959228515625, 'test/num_examples': 10000, 'score': 58860.95683288574, 'total_duration': 63387.47819709778, 'accumulated_submission_time': 58860.95683288574, 'accumulated_eval_time': 4513.996830224991, 'accumulated_logging_time': 5.740103721618652, 'global_step': 130545, 'preemption_count': 0}), (131476, {'train/accuracy': 0.756054699420929, 'train/loss': 1.0679931640625, 'validation/accuracy': 0.6955400109291077, 'validation/loss': 1.3339507579803467, 'validation/num_examples': 50000, 'test/accuracy': 0.5682000517845154, 'test/loss': 1.9795842170715332, 'test/num_examples': 10000, 'score': 59280.941180706024, 'total_duration': 63841.59644985199, 'accumulated_submission_time': 59280.941180706024, 'accumulated_eval_time': 4548.038062334061, 'accumulated_logging_time': 5.784178018569946, 'global_step': 131476, 'preemption_count': 0}), (132405, {'train/accuracy': 0.7659375071525574, 'train/loss': 1.050911784172058, 'validation/accuracy': 0.6958400011062622, 'validation/loss': 1.3542274236679077, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 1.9746955633163452, 'test/num_examples': 10000, 'score': 59701.01261425018, 'total_duration': 64294.9606654644, 'accumulated_submission_time': 59701.01261425018, 'accumulated_eval_time': 4581.237900733948, 'accumulated_logging_time': 5.829130172729492, 'global_step': 132405, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7583202719688416, 'train/loss': 1.063936471939087, 'validation/accuracy': 0.7030799984931946, 'validation/loss': 1.3143028020858765, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.9410356283187866, 'test/num_examples': 10000, 'score': 60121.277137994766, 'total_duration': 64749.58311963081, 'accumulated_submission_time': 60121.277137994766, 'accumulated_eval_time': 4615.49994468689, 'accumulated_logging_time': 5.87558388710022, 'global_step': 133333, 'preemption_count': 0}), (134263, {'train/accuracy': 0.7602148056030273, 'train/loss': 1.0567129850387573, 'validation/accuracy': 0.700939953327179, 'validation/loss': 1.3244282007217407, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.947724461555481, 'test/num_examples': 10000, 'score': 60541.402416706085, 'total_duration': 65202.35506153107, 'accumulated_submission_time': 60541.402416706085, 'accumulated_eval_time': 4648.055237054825, 'accumulated_logging_time': 5.918776750564575, 'global_step': 134263, 'preemption_count': 0}), (135186, {'train/accuracy': 0.7699609398841858, 'train/loss': 1.0183870792388916, 'validation/accuracy': 0.7020399570465088, 'validation/loss': 1.3113582134246826, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.9366137981414795, 'test/num_examples': 10000, 'score': 60961.52054858208, 'total_duration': 65656.98781871796, 'accumulated_submission_time': 60961.52054858208, 'accumulated_eval_time': 4682.473633766174, 'accumulated_logging_time': 5.967818260192871, 'global_step': 135186, 'preemption_count': 0}), (136116, {'train/accuracy': 0.768359363079071, 'train/loss': 1.0378899574279785, 'validation/accuracy': 0.705839991569519, 'validation/loss': 1.3026723861694336, 'validation/num_examples': 50000, 'test/accuracy': 0.5782000422477722, 'test/loss': 1.9222419261932373, 'test/num_examples': 10000, 'score': 61381.798147916794, 'total_duration': 66110.9119849205, 'accumulated_submission_time': 61381.798147916794, 'accumulated_eval_time': 4716.022467851639, 'accumulated_logging_time': 6.017577886581421, 'global_step': 136116, 'preemption_count': 0}), (137045, {'train/accuracy': 0.7707226276397705, 'train/loss': 0.9996986985206604, 'validation/accuracy': 0.7090199589729309, 'validation/loss': 1.2752629518508911, 'validation/num_examples': 50000, 'test/accuracy': 0.5843000411987305, 'test/loss': 1.8952151536941528, 'test/num_examples': 10000, 'score': 61801.93469142914, 'total_duration': 66561.47880458832, 'accumulated_submission_time': 61801.93469142914, 'accumulated_eval_time': 4746.358287096024, 'accumulated_logging_time': 6.064196825027466, 'global_step': 137045, 'preemption_count': 0}), (137975, {'train/accuracy': 0.7759569883346558, 'train/loss': 0.9817464351654053, 'validation/accuracy': 0.708899974822998, 'validation/loss': 1.267372965812683, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.8926504850387573, 'test/num_examples': 10000, 'score': 62222.269728422165, 'total_duration': 67012.80084323883, 'accumulated_submission_time': 62222.269728422165, 'accumulated_eval_time': 4777.243156194687, 'accumulated_logging_time': 6.118428707122803, 'global_step': 137975, 'preemption_count': 0}), (138906, {'train/accuracy': 0.7888085842132568, 'train/loss': 0.9418398141860962, 'validation/accuracy': 0.7098599672317505, 'validation/loss': 1.2711541652679443, 'validation/num_examples': 50000, 'test/accuracy': 0.5886000394821167, 'test/loss': 1.8847719430923462, 'test/num_examples': 10000, 'score': 62642.559386491776, 'total_duration': 67466.630849123, 'accumulated_submission_time': 62642.559386491776, 'accumulated_eval_time': 4810.671858549118, 'accumulated_logging_time': 6.181352853775024, 'global_step': 138906, 'preemption_count': 0}), (139840, {'train/accuracy': 0.770312488079071, 'train/loss': 0.9991475343704224, 'validation/accuracy': 0.7108599543571472, 'validation/loss': 1.2587929964065552, 'validation/num_examples': 50000, 'test/accuracy': 0.5869000554084778, 'test/loss': 1.8614424467086792, 'test/num_examples': 10000, 'score': 63062.58865451813, 'total_duration': 67920.26310777664, 'accumulated_submission_time': 63062.58865451813, 'accumulated_eval_time': 4844.179829597473, 'accumulated_logging_time': 6.227135896682739, 'global_step': 139840, 'preemption_count': 0}), (140771, {'train/accuracy': 0.7780663967132568, 'train/loss': 0.9655563831329346, 'validation/accuracy': 0.7139399647712708, 'validation/loss': 1.2472407817840576, 'validation/num_examples': 50000, 'test/accuracy': 0.5897000432014465, 'test/loss': 1.8567043542861938, 'test/num_examples': 10000, 'score': 63482.7752828598, 'total_duration': 68372.2566754818, 'accumulated_submission_time': 63482.7752828598, 'accumulated_eval_time': 4875.891560316086, 'accumulated_logging_time': 6.273644685745239, 'global_step': 140771, 'preemption_count': 0}), (141701, {'train/accuracy': 0.7883398532867432, 'train/loss': 0.9381603598594666, 'validation/accuracy': 0.7156800031661987, 'validation/loss': 1.2484104633331299, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 1.8638124465942383, 'test/num_examples': 10000, 'score': 63903.234132528305, 'total_duration': 68820.95351719856, 'accumulated_submission_time': 63903.234132528305, 'accumulated_eval_time': 4904.032790899277, 'accumulated_logging_time': 6.321924686431885, 'global_step': 141701, 'preemption_count': 0}), (142629, {'train/accuracy': 0.7808007597923279, 'train/loss': 0.9934661388397217, 'validation/accuracy': 0.7153599858283997, 'validation/loss': 1.2737171649932861, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.8806276321411133, 'test/num_examples': 10000, 'score': 64323.499345541, 'total_duration': 69273.26719760895, 'accumulated_submission_time': 64323.499345541, 'accumulated_eval_time': 4935.975115537643, 'accumulated_logging_time': 6.379689931869507, 'global_step': 142629, 'preemption_count': 0}), (143560, {'train/accuracy': 0.7847851514816284, 'train/loss': 0.949887216091156, 'validation/accuracy': 0.7196199893951416, 'validation/loss': 1.221478819847107, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.8401914834976196, 'test/num_examples': 10000, 'score': 64743.521449804306, 'total_duration': 69727.04166722298, 'accumulated_submission_time': 64743.521449804306, 'accumulated_eval_time': 4969.630095720291, 'accumulated_logging_time': 6.429242849349976, 'global_step': 143560, 'preemption_count': 0}), (144490, {'train/accuracy': 0.7881640195846558, 'train/loss': 0.9314032793045044, 'validation/accuracy': 0.718239963054657, 'validation/loss': 1.2326812744140625, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.845787763595581, 'test/num_examples': 10000, 'score': 65163.633311748505, 'total_duration': 70180.94100570679, 'accumulated_submission_time': 65163.633311748505, 'accumulated_eval_time': 5003.322789907455, 'accumulated_logging_time': 6.475466012954712, 'global_step': 144490, 'preemption_count': 0}), (145420, {'train/accuracy': 0.7870116829872131, 'train/loss': 0.9516395926475525, 'validation/accuracy': 0.7205599546432495, 'validation/loss': 1.229424238204956, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.8493428230285645, 'test/num_examples': 10000, 'score': 65583.94645094872, 'total_duration': 70635.012373209, 'accumulated_submission_time': 65583.94645094872, 'accumulated_eval_time': 5036.984225511551, 'accumulated_logging_time': 6.524138689041138, 'global_step': 145420, 'preemption_count': 0}), (146350, {'train/accuracy': 0.7876952886581421, 'train/loss': 0.9195204973220825, 'validation/accuracy': 0.726419985294342, 'validation/loss': 1.2012972831726074, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.814241647720337, 'test/num_examples': 10000, 'score': 66003.91966462135, 'total_duration': 71088.62664437294, 'accumulated_submission_time': 66003.91966462135, 'accumulated_eval_time': 5070.531363964081, 'accumulated_logging_time': 6.570559024810791, 'global_step': 146350, 'preemption_count': 0}), (147280, {'train/accuracy': 0.7960156202316284, 'train/loss': 0.888444185256958, 'validation/accuracy': 0.7275800108909607, 'validation/loss': 1.1892650127410889, 'validation/num_examples': 50000, 'test/accuracy': 0.6032000184059143, 'test/loss': 1.8031319379806519, 'test/num_examples': 10000, 'score': 66423.87331795692, 'total_duration': 71541.92117094994, 'accumulated_submission_time': 66423.87331795692, 'accumulated_eval_time': 5103.775855779648, 'accumulated_logging_time': 6.61857533454895, 'global_step': 147280, 'preemption_count': 0}), (148210, {'train/accuracy': 0.7996679544448853, 'train/loss': 0.9016132950782776, 'validation/accuracy': 0.7257199883460999, 'validation/loss': 1.2199366092681885, 'validation/num_examples': 50000, 'test/accuracy': 0.5993000268936157, 'test/loss': 1.8347848653793335, 'test/num_examples': 10000, 'score': 66843.96156454086, 'total_duration': 71995.24836182594, 'accumulated_submission_time': 66843.96156454086, 'accumulated_eval_time': 5136.919964790344, 'accumulated_logging_time': 6.665120840072632, 'global_step': 148210, 'preemption_count': 0}), (149141, {'train/accuracy': 0.7919726371765137, 'train/loss': 0.9040732979774475, 'validation/accuracy': 0.7277799844741821, 'validation/loss': 1.1872663497924805, 'validation/num_examples': 50000, 'test/accuracy': 0.6061000227928162, 'test/loss': 1.7923401594161987, 'test/num_examples': 10000, 'score': 67264.05994081497, 'total_duration': 72447.8445456028, 'accumulated_submission_time': 67264.05994081497, 'accumulated_eval_time': 5169.321353435516, 'accumulated_logging_time': 6.71377420425415, 'global_step': 149141, 'preemption_count': 0}), (150071, {'train/accuracy': 0.7992578148841858, 'train/loss': 0.882004976272583, 'validation/accuracy': 0.7291399836540222, 'validation/loss': 1.1780247688293457, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.791891098022461, 'test/num_examples': 10000, 'score': 67684.25943183899, 'total_duration': 72900.0660943985, 'accumulated_submission_time': 67684.25943183899, 'accumulated_eval_time': 5201.246497869492, 'accumulated_logging_time': 6.762799263000488, 'global_step': 150071, 'preemption_count': 0}), (151000, {'train/accuracy': 0.8058202862739563, 'train/loss': 0.8614989519119263, 'validation/accuracy': 0.7326399683952332, 'validation/loss': 1.1725473403930664, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.7773709297180176, 'test/num_examples': 10000, 'score': 68104.4731631279, 'total_duration': 73351.86920380592, 'accumulated_submission_time': 68104.4731631279, 'accumulated_eval_time': 5232.74213886261, 'accumulated_logging_time': 6.808778524398804, 'global_step': 151000, 'preemption_count': 0}), (151932, {'train/accuracy': 0.8000390529632568, 'train/loss': 0.8734333515167236, 'validation/accuracy': 0.731220006942749, 'validation/loss': 1.1657071113586426, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.7815557718276978, 'test/num_examples': 10000, 'score': 68524.8423511982, 'total_duration': 73805.52795624733, 'accumulated_submission_time': 68524.8423511982, 'accumulated_eval_time': 5265.935954332352, 'accumulated_logging_time': 6.8564043045043945, 'global_step': 151932, 'preemption_count': 0}), (152865, {'train/accuracy': 0.804492175579071, 'train/loss': 0.8603943586349487, 'validation/accuracy': 0.7348399758338928, 'validation/loss': 1.1572157144546509, 'validation/num_examples': 50000, 'test/accuracy': 0.6141000390052795, 'test/loss': 1.7590707540512085, 'test/num_examples': 10000, 'score': 68945.09971499443, 'total_duration': 74259.00728917122, 'accumulated_submission_time': 68945.09971499443, 'accumulated_eval_time': 5299.06134557724, 'accumulated_logging_time': 6.904045104980469, 'global_step': 152865, 'preemption_count': 0}), (153797, {'train/accuracy': 0.8055663704872131, 'train/loss': 0.8574345707893372, 'validation/accuracy': 0.7340199947357178, 'validation/loss': 1.1686347723007202, 'validation/num_examples': 50000, 'test/accuracy': 0.6112000346183777, 'test/loss': 1.773018479347229, 'test/num_examples': 10000, 'score': 69365.369992733, 'total_duration': 74713.34918832779, 'accumulated_submission_time': 69365.369992733, 'accumulated_eval_time': 5333.035125255585, 'accumulated_logging_time': 6.953671216964722, 'global_step': 153797, 'preemption_count': 0}), (154728, {'train/accuracy': 0.8135741949081421, 'train/loss': 0.8229332566261292, 'validation/accuracy': 0.737779974937439, 'validation/loss': 1.1446532011032104, 'validation/num_examples': 50000, 'test/accuracy': 0.6133000254631042, 'test/loss': 1.7443084716796875, 'test/num_examples': 10000, 'score': 69785.46291160583, 'total_duration': 75167.66317725182, 'accumulated_submission_time': 69785.46291160583, 'accumulated_eval_time': 5367.154443502426, 'accumulated_logging_time': 7.006609916687012, 'global_step': 154728, 'preemption_count': 0}), (155658, {'train/accuracy': 0.8089257478713989, 'train/loss': 0.8678907752037048, 'validation/accuracy': 0.7383999824523926, 'validation/loss': 1.1707357168197632, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.7738709449768066, 'test/num_examples': 10000, 'score': 70205.56688523293, 'total_duration': 75621.48214673996, 'accumulated_submission_time': 70205.56688523293, 'accumulated_eval_time': 5400.772227048874, 'accumulated_logging_time': 7.056041240692139, 'global_step': 155658, 'preemption_count': 0}), (156587, {'train/accuracy': 0.8149804472923279, 'train/loss': 0.8367795944213867, 'validation/accuracy': 0.7415599822998047, 'validation/loss': 1.1527200937271118, 'validation/num_examples': 50000, 'test/accuracy': 0.6217000484466553, 'test/loss': 1.7479978799819946, 'test/num_examples': 10000, 'score': 70625.50346064568, 'total_duration': 76075.67284274101, 'accumulated_submission_time': 70625.50346064568, 'accumulated_eval_time': 5434.675608158112, 'accumulated_logging_time': 7.35836124420166, 'global_step': 156587, 'preemption_count': 0}), (157511, {'train/accuracy': 0.8213085532188416, 'train/loss': 0.7878121137619019, 'validation/accuracy': 0.745199978351593, 'validation/loss': 1.118953824043274, 'validation/num_examples': 50000, 'test/accuracy': 0.6192000508308411, 'test/loss': 1.7231636047363281, 'test/num_examples': 10000, 'score': 71045.69268107414, 'total_duration': 76529.8608827591, 'accumulated_submission_time': 71045.69268107414, 'accumulated_eval_time': 5468.575320243835, 'accumulated_logging_time': 7.409008026123047, 'global_step': 157511, 'preemption_count': 0}), (158441, {'train/accuracy': 0.8138671517372131, 'train/loss': 0.8201526403427124, 'validation/accuracy': 0.7439999580383301, 'validation/loss': 1.1131176948547363, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.7198753356933594, 'test/num_examples': 10000, 'score': 71465.86521029472, 'total_duration': 76984.15627121925, 'accumulated_submission_time': 71465.86521029472, 'accumulated_eval_time': 5502.602823019028, 'accumulated_logging_time': 7.457210540771484, 'global_step': 158441, 'preemption_count': 0}), (159370, {'train/accuracy': 0.82093745470047, 'train/loss': 0.8007131218910217, 'validation/accuracy': 0.7448599934577942, 'validation/loss': 1.126524806022644, 'validation/num_examples': 50000, 'test/accuracy': 0.6217000484466553, 'test/loss': 1.7308553457260132, 'test/num_examples': 10000, 'score': 71885.79600262642, 'total_duration': 77437.7894179821, 'accumulated_submission_time': 71885.79600262642, 'accumulated_eval_time': 5536.2020580768585, 'accumulated_logging_time': 7.512519836425781, 'global_step': 159370, 'preemption_count': 0}), (160301, {'train/accuracy': 0.8273046612739563, 'train/loss': 0.7696381211280823, 'validation/accuracy': 0.7482199668884277, 'validation/loss': 1.115172028541565, 'validation/num_examples': 50000, 'test/accuracy': 0.6235000491142273, 'test/loss': 1.7202221155166626, 'test/num_examples': 10000, 'score': 72306.14024019241, 'total_duration': 77892.31031370163, 'accumulated_submission_time': 72306.14024019241, 'accumulated_eval_time': 5570.279438018799, 'accumulated_logging_time': 7.564241647720337, 'global_step': 160301, 'preemption_count': 0}), (161232, {'train/accuracy': 0.8215234279632568, 'train/loss': 0.79811030626297, 'validation/accuracy': 0.7492199540138245, 'validation/loss': 1.1067612171173096, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.7063981294631958, 'test/num_examples': 10000, 'score': 72726.45551586151, 'total_duration': 78342.89540290833, 'accumulated_submission_time': 72726.45551586151, 'accumulated_eval_time': 5600.44885635376, 'accumulated_logging_time': 7.616266965866089, 'global_step': 161232, 'preemption_count': 0}), (162159, {'train/accuracy': 0.8233007788658142, 'train/loss': 0.7672076225280762, 'validation/accuracy': 0.7484999895095825, 'validation/loss': 1.0859493017196655, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.673032522201538, 'test/num_examples': 10000, 'score': 73146.63851761818, 'total_duration': 78793.20580601692, 'accumulated_submission_time': 73146.63851761818, 'accumulated_eval_time': 5630.4672927856445, 'accumulated_logging_time': 7.6764256954193115, 'global_step': 162159, 'preemption_count': 0}), (163088, {'train/accuracy': 0.8291601538658142, 'train/loss': 0.7714617848396301, 'validation/accuracy': 0.7514199614524841, 'validation/loss': 1.10415518283844, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.7050611972808838, 'test/num_examples': 10000, 'score': 73566.73581504822, 'total_duration': 79246.45765209198, 'accumulated_submission_time': 73566.73581504822, 'accumulated_eval_time': 5663.513606309891, 'accumulated_logging_time': 7.735929727554321, 'global_step': 163088, 'preemption_count': 0}), (164021, {'train/accuracy': 0.83509761095047, 'train/loss': 0.7409867644309998, 'validation/accuracy': 0.7541199922561646, 'validation/loss': 1.079445481300354, 'validation/num_examples': 50000, 'test/accuracy': 0.6335000395774841, 'test/loss': 1.6779024600982666, 'test/num_examples': 10000, 'score': 73986.66831469536, 'total_duration': 79700.57873511314, 'accumulated_submission_time': 73986.66831469536, 'accumulated_eval_time': 5697.595949888229, 'accumulated_logging_time': 7.793646812438965, 'global_step': 164021, 'preemption_count': 0}), (164957, {'train/accuracy': 0.8322851657867432, 'train/loss': 0.7366339564323425, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 1.0619897842407227, 'validation/num_examples': 50000, 'test/accuracy': 0.6362000107765198, 'test/loss': 1.6556838750839233, 'test/num_examples': 10000, 'score': 74407.01210308075, 'total_duration': 80155.67283654213, 'accumulated_submission_time': 74407.01210308075, 'accumulated_eval_time': 5732.24786067009, 'accumulated_logging_time': 7.844249725341797, 'global_step': 164957, 'preemption_count': 0}), (165887, {'train/accuracy': 0.8315820097923279, 'train/loss': 0.7409655451774597, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.0678353309631348, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.6705749034881592, 'test/num_examples': 10000, 'score': 74827.03679513931, 'total_duration': 80606.65344071388, 'accumulated_submission_time': 74827.03679513931, 'accumulated_eval_time': 5763.104225158691, 'accumulated_logging_time': 7.89516282081604, 'global_step': 165887, 'preemption_count': 0}), (166816, {'train/accuracy': 0.8335937261581421, 'train/loss': 0.7405239939689636, 'validation/accuracy': 0.7565000057220459, 'validation/loss': 1.076341986656189, 'validation/num_examples': 50000, 'test/accuracy': 0.6351000070571899, 'test/loss': 1.669582486152649, 'test/num_examples': 10000, 'score': 75247.1132349968, 'total_duration': 81058.46815085411, 'accumulated_submission_time': 75247.1132349968, 'accumulated_eval_time': 5794.742982387543, 'accumulated_logging_time': 7.947040319442749, 'global_step': 166816, 'preemption_count': 0}), (167747, {'train/accuracy': 0.8338671922683716, 'train/loss': 0.7559254765510559, 'validation/accuracy': 0.75791996717453, 'validation/loss': 1.0783684253692627, 'validation/num_examples': 50000, 'test/accuracy': 0.6355000138282776, 'test/loss': 1.6743074655532837, 'test/num_examples': 10000, 'score': 75667.09297275543, 'total_duration': 81511.52088880539, 'accumulated_submission_time': 75667.09297275543, 'accumulated_eval_time': 5827.715883970261, 'accumulated_logging_time': 7.998236179351807, 'global_step': 167747, 'preemption_count': 0}), (168678, {'train/accuracy': 0.8360546827316284, 'train/loss': 0.7370734214782715, 'validation/accuracy': 0.7594599723815918, 'validation/loss': 1.0704350471496582, 'validation/num_examples': 50000, 'test/accuracy': 0.636400043964386, 'test/loss': 1.665740728378296, 'test/num_examples': 10000, 'score': 76087.34431004524, 'total_duration': 81965.7411146164, 'accumulated_submission_time': 76087.34431004524, 'accumulated_eval_time': 5861.583888530731, 'accumulated_logging_time': 8.05066990852356, 'global_step': 168678, 'preemption_count': 0}), (169609, {'train/accuracy': 0.83984375, 'train/loss': 0.7221209406852722, 'validation/accuracy': 0.760159969329834, 'validation/loss': 1.0561344623565674, 'validation/num_examples': 50000, 'test/accuracy': 0.6401000022888184, 'test/loss': 1.649182677268982, 'test/num_examples': 10000, 'score': 76507.4817943573, 'total_duration': 82420.15692543983, 'accumulated_submission_time': 76507.4817943573, 'accumulated_eval_time': 5895.755210876465, 'accumulated_logging_time': 8.10885739326477, 'global_step': 169609, 'preemption_count': 0}), (170540, {'train/accuracy': 0.8365820050239563, 'train/loss': 0.7234076261520386, 'validation/accuracy': 0.7600599527359009, 'validation/loss': 1.0516064167022705, 'validation/num_examples': 50000, 'test/accuracy': 0.6373000144958496, 'test/loss': 1.6481930017471313, 'test/num_examples': 10000, 'score': 76927.61034536362, 'total_duration': 82874.5614593029, 'accumulated_submission_time': 76927.61034536362, 'accumulated_eval_time': 5929.933554887772, 'accumulated_logging_time': 8.157514572143555, 'global_step': 170540, 'preemption_count': 0}), (171472, {'train/accuracy': 0.8395116925239563, 'train/loss': 0.7189873456954956, 'validation/accuracy': 0.7607799768447876, 'validation/loss': 1.0504368543624878, 'validation/num_examples': 50000, 'test/accuracy': 0.6414000391960144, 'test/loss': 1.6435645818710327, 'test/num_examples': 10000, 'score': 77347.73178625107, 'total_duration': 83329.06152820587, 'accumulated_submission_time': 77347.73178625107, 'accumulated_eval_time': 5964.20698928833, 'accumulated_logging_time': 8.21371054649353, 'global_step': 171472, 'preemption_count': 0})], 'global_step': 171861}
I0131 12:53:12.927248 139863983413056 submission_runner.py:586] Timing: 77520.0893175602
I0131 12:53:12.927327 139863983413056 submission_runner.py:588] Total number of evals: 185
I0131 12:53:12.927370 139863983413056 submission_runner.py:589] ====================
I0131 12:53:12.927414 139863983413056 submission_runner.py:542] Using RNG seed 3682051175
I0131 12:53:12.928947 139863983413056 submission_runner.py:551] --- Tuning run 2/5 ---
I0131 12:53:12.929059 139863983413056 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_2.
I0131 12:53:12.934691 139863983413056 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_2/hparams.json.
I0131 12:53:12.936527 139863983413056 submission_runner.py:206] Initializing dataset.
I0131 12:53:12.947150 139863983413056 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0131 12:53:12.957836 139863983413056 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0131 12:53:13.174787 139863983413056 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0131 12:53:17.293261 139863983413056 submission_runner.py:213] Initializing model.
I0131 12:53:23.653523 139863983413056 submission_runner.py:255] Initializing optimizer.
I0131 12:53:24.114708 139863983413056 submission_runner.py:262] Initializing metrics bundle.
I0131 12:53:24.114894 139863983413056 submission_runner.py:280] Initializing checkpoint and logger.
I0131 12:53:24.205860 139863983413056 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_2 with prefix checkpoint_
I0131 12:53:24.206005 139863983413056 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0131 12:53:39.695260 139863983413056 logger_utils.py:220] Unable to record git information. Continuing without it.
I0131 12:53:54.493163 139863983413056 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_2/flags_0.json.
I0131 12:53:54.505532 139863983413056 submission_runner.py:314] Starting training loop.
I0131 12:54:29.510652 139702501852928 logging_writer.py:48] [0] global_step=0, grad_norm=0.2983640730381012, loss=6.9077534675598145
I0131 12:54:29.521291 139863983413056 spec.py:321] Evaluating on the training split.
I0131 12:54:37.802543 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 12:54:55.726244 139863983413056 spec.py:349] Evaluating on the test split.
I0131 12:54:57.371991 139863983413056 submission_runner.py:408] Time since start: 62.87s, 	Step: 1, 	{'train/accuracy': 0.0008593749953433871, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 35.015655517578125, 'total_duration': 62.8664071559906, 'accumulated_submission_time': 35.015655517578125, 'accumulated_eval_time': 27.850645065307617, 'accumulated_logging_time': 0}
I0131 12:54:57.380162 139702510245632 logging_writer.py:48] [1] accumulated_eval_time=27.850645, accumulated_logging_time=0, accumulated_submission_time=35.015656, global_step=1, preemption_count=0, score=35.015656, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=62.866407, train/accuracy=0.000859, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0131 12:56:02.158579 139702543816448 logging_writer.py:48] [100] global_step=100, grad_norm=0.38148215413093567, loss=6.904768943786621
I0131 12:56:47.546758 139702527031040 logging_writer.py:48] [200] global_step=200, grad_norm=0.40988588333129883, loss=6.8811211585998535
I0131 12:57:33.143432 139702543816448 logging_writer.py:48] [300] global_step=300, grad_norm=0.4903113543987274, loss=6.839808464050293
I0131 12:58:19.539371 139702527031040 logging_writer.py:48] [400] global_step=400, grad_norm=0.5257590413093567, loss=6.81116247177124
I0131 12:59:05.919422 139702543816448 logging_writer.py:48] [500] global_step=500, grad_norm=0.5351301431655884, loss=6.847874641418457
I0131 12:59:51.931971 139702527031040 logging_writer.py:48] [600] global_step=600, grad_norm=0.7661167979240417, loss=6.731335163116455
I0131 13:00:37.932301 139702543816448 logging_writer.py:48] [700] global_step=700, grad_norm=0.9413014650344849, loss=6.670276165008545
I0131 13:01:23.865899 139702527031040 logging_writer.py:48] [800] global_step=800, grad_norm=1.136874794960022, loss=6.678386211395264
I0131 13:01:57.620562 139863983413056 spec.py:321] Evaluating on the training split.
I0131 13:02:08.666891 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 13:02:30.474712 139863983413056 spec.py:349] Evaluating on the test split.
I0131 13:02:32.111872 139863983413056 submission_runner.py:408] Time since start: 517.61s, 	Step: 875, 	{'train/accuracy': 0.01748046837747097, 'train/loss': 6.352764129638672, 'validation/accuracy': 0.016339998692274094, 'validation/loss': 6.365495681762695, 'validation/num_examples': 50000, 'test/accuracy': 0.013000000268220901, 'test/loss': 6.417356014251709, 'test/num_examples': 10000, 'score': 455.200243473053, 'total_duration': 517.6062908172607, 'accumulated_submission_time': 455.200243473053, 'accumulated_eval_time': 62.341947078704834, 'accumulated_logging_time': 0.01773810386657715}
I0131 13:02:32.130429 139702543816448 logging_writer.py:48] [875] accumulated_eval_time=62.341947, accumulated_logging_time=0.017738, accumulated_submission_time=455.200243, global_step=875, preemption_count=0, score=455.200243, test/accuracy=0.013000, test/loss=6.417356, test/num_examples=10000, total_duration=517.606291, train/accuracy=0.017480, train/loss=6.352764, validation/accuracy=0.016340, validation/loss=6.365496, validation/num_examples=50000
I0131 13:02:42.496788 139702527031040 logging_writer.py:48] [900] global_step=900, grad_norm=0.8483399748802185, loss=6.5857696533203125
I0131 13:03:24.577384 139702543816448 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2833057641983032, loss=6.575814247131348
I0131 13:04:10.532389 139702527031040 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0452181100845337, loss=6.649188041687012
I0131 13:04:56.532758 139702543816448 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8640939593315125, loss=6.534937858581543
I0131 13:05:43.111119 139702527031040 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.3927342891693115, loss=6.504521369934082
I0131 13:06:29.188692 139702543816448 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0125236511230469, loss=6.741935729980469
I0131 13:07:15.464146 139702527031040 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9115163683891296, loss=6.709257125854492
I0131 13:08:01.561934 139702543816448 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0513585805892944, loss=6.324368000030518
I0131 13:08:47.654776 139702527031040 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9786624312400818, loss=6.483996391296387
I0131 13:09:32.530688 139863983413056 spec.py:321] Evaluating on the training split.
I0131 13:09:43.770951 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 13:10:06.562340 139863983413056 spec.py:349] Evaluating on the test split.
I0131 13:10:08.215981 139863983413056 submission_runner.py:408] Time since start: 973.71s, 	Step: 1799, 	{'train/accuracy': 0.04648437350988388, 'train/loss': 5.830441951751709, 'validation/accuracy': 0.04227999970316887, 'validation/loss': 5.864605903625488, 'validation/num_examples': 50000, 'test/accuracy': 0.03400000184774399, 'test/loss': 5.986781597137451, 'test/num_examples': 10000, 'score': 875.5398058891296, 'total_duration': 973.7104048728943, 'accumulated_submission_time': 875.5398058891296, 'accumulated_eval_time': 98.02724885940552, 'accumulated_logging_time': 0.04830026626586914}
I0131 13:10:08.230547 139702543816448 logging_writer.py:48] [1799] accumulated_eval_time=98.027249, accumulated_logging_time=0.048300, accumulated_submission_time=875.539806, global_step=1799, preemption_count=0, score=875.539806, test/accuracy=0.034000, test/loss=5.986782, test/num_examples=10000, total_duration=973.710405, train/accuracy=0.046484, train/loss=5.830442, validation/accuracy=0.042280, validation/loss=5.864606, validation/num_examples=50000
I0131 13:10:09.041476 139702527031040 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1960188150405884, loss=6.6082000732421875
I0131 13:10:50.025827 139702543816448 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.2162326574325562, loss=6.286698341369629
I0131 13:11:35.867186 139702527031040 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.4334324598312378, loss=6.291985988616943
I0131 13:12:22.054364 139702543816448 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.373983144760132, loss=6.328047752380371
I0131 13:13:08.073044 139702527031040 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7476973533630371, loss=6.73097038269043
I0131 13:13:54.002157 139702543816448 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9172722697257996, loss=6.217766284942627
I0131 13:14:39.807535 139702527031040 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.3038214445114136, loss=6.189153671264648
I0131 13:15:28.055771 139702543816448 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2795809507369995, loss=6.523782253265381
I0131 13:16:27.417392 139702527031040 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.3122458457946777, loss=6.193686485290527
I0131 13:17:08.488376 139863983413056 spec.py:321] Evaluating on the training split.
I0131 13:17:18.980137 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 13:17:42.205872 139863983413056 spec.py:349] Evaluating on the test split.
I0131 13:17:43.854279 139863983413056 submission_runner.py:408] Time since start: 1429.35s, 	Step: 2691, 	{'train/accuracy': 0.07083984464406967, 'train/loss': 5.448312282562256, 'validation/accuracy': 0.06827999651432037, 'validation/loss': 5.485576152801514, 'validation/num_examples': 50000, 'test/accuracy': 0.05140000209212303, 'test/loss': 5.651369571685791, 'test/num_examples': 10000, 'score': 1295.7417635917664, 'total_duration': 1429.3486967086792, 'accumulated_submission_time': 1295.7417635917664, 'accumulated_eval_time': 133.39315724372864, 'accumulated_logging_time': 0.0725252628326416}
I0131 13:17:43.868451 139702543816448 logging_writer.py:48] [2691] accumulated_eval_time=133.393157, accumulated_logging_time=0.072525, accumulated_submission_time=1295.741764, global_step=2691, preemption_count=0, score=1295.741764, test/accuracy=0.051400, test/loss=5.651370, test/num_examples=10000, total_duration=1429.348697, train/accuracy=0.070840, train/loss=5.448312, validation/accuracy=0.068280, validation/loss=5.485576, validation/num_examples=50000
I0131 13:17:47.842182 139702527031040 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7968091368675232, loss=6.598668575286865
I0131 13:18:29.233730 139702543816448 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0613863468170166, loss=6.411977767944336
I0131 13:19:15.052493 139702527031040 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0973256826400757, loss=6.092787265777588
I0131 13:20:00.879849 139702543816448 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9795190095901489, loss=6.074169158935547
I0131 13:20:46.737575 139702527031040 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8128359913825989, loss=6.078932762145996
I0131 13:21:32.469527 139702543816448 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.2623872756958008, loss=6.536799907684326
I0131 13:22:18.015669 139702527031040 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0118567943572998, loss=6.0083327293396
I0131 13:23:03.783477 139702543816448 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9980577230453491, loss=6.006552696228027
I0131 13:23:49.576508 139702527031040 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.21860671043396, loss=6.159245014190674
I0131 13:24:35.569431 139702543816448 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.1369549036026, loss=5.956229209899902
I0131 13:24:44.014177 139863983413056 spec.py:321] Evaluating on the training split.
I0131 13:24:54.326788 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 13:25:16.594312 139863983413056 spec.py:349] Evaluating on the test split.
I0131 13:25:18.246481 139863983413056 submission_runner.py:408] Time since start: 1883.74s, 	Step: 3620, 	{'train/accuracy': 0.1144726499915123, 'train/loss': 5.076627731323242, 'validation/accuracy': 0.1036200001835823, 'validation/loss': 5.128720760345459, 'validation/num_examples': 50000, 'test/accuracy': 0.08130000531673431, 'test/loss': 5.356257915496826, 'test/num_examples': 10000, 'score': 1715.829880952835, 'total_duration': 1883.7408828735352, 'accumulated_submission_time': 1715.829880952835, 'accumulated_eval_time': 167.62544560432434, 'accumulated_logging_time': 0.09573221206665039}
I0131 13:25:18.262705 139702527031040 logging_writer.py:48] [3620] accumulated_eval_time=167.625446, accumulated_logging_time=0.095732, accumulated_submission_time=1715.829881, global_step=3620, preemption_count=0, score=1715.829881, test/accuracy=0.081300, test/loss=5.356258, test/num_examples=10000, total_duration=1883.740883, train/accuracy=0.114473, train/loss=5.076628, validation/accuracy=0.103620, validation/loss=5.128721, validation/num_examples=50000
I0131 13:25:50.796642 139702543816448 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.0589858293533325, loss=5.957033157348633
I0131 13:26:37.023170 139702527031040 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.098748803138733, loss=5.942119598388672
I0131 13:27:22.767313 139702543816448 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.0408209562301636, loss=5.9112935066223145
I0131 13:28:08.972591 139702527031040 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.182937502861023, loss=6.2801313400268555
I0131 13:28:54.758880 139702543816448 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8841158747673035, loss=6.184741020202637
I0131 13:29:40.403398 139702527031040 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9116010665893555, loss=6.316867828369141
I0131 13:30:26.463047 139702543816448 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9857287406921387, loss=6.070313453674316
I0131 13:31:12.367412 139702527031040 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9212960600852966, loss=6.585173606872559
I0131 13:31:58.270089 139702543816448 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0165560245513916, loss=5.681394577026367
I0131 13:32:18.535108 139863983413056 spec.py:321] Evaluating on the training split.
I0131 13:32:29.025827 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 13:32:51.185839 139863983413056 spec.py:349] Evaluating on the test split.
I0131 13:32:52.815912 139863983413056 submission_runner.py:408] Time since start: 2338.31s, 	Step: 4546, 	{'train/accuracy': 0.16103515028953552, 'train/loss': 4.597641944885254, 'validation/accuracy': 0.14226000010967255, 'validation/loss': 4.700864315032959, 'validation/num_examples': 50000, 'test/accuracy': 0.10830000787973404, 'test/loss': 5.006032943725586, 'test/num_examples': 10000, 'score': 2136.0431559085846, 'total_duration': 2338.310334444046, 'accumulated_submission_time': 2136.0431559085846, 'accumulated_eval_time': 201.90626621246338, 'accumulated_logging_time': 0.12220025062561035}
I0131 13:32:52.830538 139702527031040 logging_writer.py:48] [4546] accumulated_eval_time=201.906266, accumulated_logging_time=0.122200, accumulated_submission_time=2136.043156, global_step=4546, preemption_count=0, score=2136.043156, test/accuracy=0.108300, test/loss=5.006033, test/num_examples=10000, total_duration=2338.310334, train/accuracy=0.161035, train/loss=4.597642, validation/accuracy=0.142260, validation/loss=4.700864, validation/num_examples=50000
I0131 13:33:14.779817 139702543816448 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8941743969917297, loss=6.508537292480469
I0131 13:33:58.916114 139702527031040 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.4205955266952515, loss=6.427917957305908
I0131 13:34:44.595982 139702543816448 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.0797849893569946, loss=5.792945384979248
I0131 13:35:30.717446 139702527031040 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.1062211990356445, loss=5.665589809417725
I0131 13:36:16.755159 139702543816448 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.0993820428848267, loss=5.592220306396484
I0131 13:37:02.694363 139702527031040 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.1175647974014282, loss=5.516485691070557
I0131 13:37:48.658935 139702543816448 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.171001672744751, loss=5.495788097381592
I0131 13:38:34.549938 139702527031040 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9510877132415771, loss=5.687098503112793
I0131 13:39:20.370519 139702543816448 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.196373701095581, loss=5.47677755355835
I0131 13:39:53.163378 139863983413056 spec.py:321] Evaluating on the training split.
I0131 13:40:03.583145 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 13:40:25.607892 139863983413056 spec.py:349] Evaluating on the test split.
I0131 13:40:27.242723 139863983413056 submission_runner.py:408] Time since start: 2792.74s, 	Step: 5473, 	{'train/accuracy': 0.2023632824420929, 'train/loss': 4.2548089027404785, 'validation/accuracy': 0.18807999789714813, 'validation/loss': 4.334715366363525, 'validation/num_examples': 50000, 'test/accuracy': 0.14079999923706055, 'test/loss': 4.678713321685791, 'test/num_examples': 10000, 'score': 2556.31773352623, 'total_duration': 2792.7371475696564, 'accumulated_submission_time': 2556.31773352623, 'accumulated_eval_time': 235.98561549186707, 'accumulated_logging_time': 0.14621448516845703}
I0131 13:40:27.257108 139702527031040 logging_writer.py:48] [5473] accumulated_eval_time=235.985615, accumulated_logging_time=0.146214, accumulated_submission_time=2556.317734, global_step=5473, preemption_count=0, score=2556.317734, test/accuracy=0.140800, test/loss=4.678713, test/num_examples=10000, total_duration=2792.737148, train/accuracy=0.202363, train/loss=4.254809, validation/accuracy=0.188080, validation/loss=4.334715, validation/num_examples=50000
I0131 13:40:38.433452 139702543816448 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9726410508155823, loss=5.601649284362793
I0131 13:41:21.016256 139702527031040 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.454885721206665, loss=5.655055046081543
I0131 13:42:06.565923 139702543816448 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8706061244010925, loss=6.273406028747559
I0131 13:42:52.085206 139702527031040 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.2908964157104492, loss=5.456961631774902
I0131 13:43:37.784924 139702543816448 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.1163734197616577, loss=5.797082424163818
I0131 13:44:23.516047 139702527031040 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.1513643264770508, loss=5.486878395080566
I0131 13:45:09.238715 139702543816448 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.3243062496185303, loss=5.37014102935791
I0131 13:45:54.964028 139702527031040 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.864469051361084, loss=6.397096157073975
I0131 13:46:41.177637 139702543816448 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.0161716938018799, loss=5.32228946685791
I0131 13:47:27.117209 139702527031040 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.2228597402572632, loss=5.272566795349121
I0131 13:47:27.277219 139863983413056 spec.py:321] Evaluating on the training split.
I0131 13:47:38.031609 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 13:47:58.135775 139863983413056 spec.py:349] Evaluating on the test split.
I0131 13:47:59.775563 139863983413056 submission_runner.py:408] Time since start: 3245.27s, 	Step: 6402, 	{'train/accuracy': 0.25146484375, 'train/loss': 3.9174740314483643, 'validation/accuracy': 0.2351599931716919, 'validation/loss': 4.009504795074463, 'validation/num_examples': 50000, 'test/accuracy': 0.18300001323223114, 'test/loss': 4.3917012214660645, 'test/num_examples': 10000, 'score': 2976.2778012752533, 'total_duration': 3245.2699744701385, 'accumulated_submission_time': 2976.2778012752533, 'accumulated_eval_time': 268.4839344024658, 'accumulated_logging_time': 0.17171645164489746}
I0131 13:47:59.789870 139702543816448 logging_writer.py:48] [6402] accumulated_eval_time=268.483934, accumulated_logging_time=0.171716, accumulated_submission_time=2976.277801, global_step=6402, preemption_count=0, score=2976.277801, test/accuracy=0.183000, test/loss=4.391701, test/num_examples=10000, total_duration=3245.269974, train/accuracy=0.251465, train/loss=3.917474, validation/accuracy=0.235160, validation/loss=4.009505, validation/num_examples=50000
I0131 13:48:40.277535 139702527031040 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.9604624509811401, loss=5.445347309112549
I0131 13:49:26.139171 139702543816448 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.9313013553619385, loss=5.53904390335083
I0131 13:50:12.367861 139702527031040 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8254275918006897, loss=6.284058570861816
I0131 13:50:58.544641 139702543816448 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.0896927118301392, loss=5.167511940002441
I0131 13:51:44.169575 139702527031040 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8849586248397827, loss=5.354709148406982
I0131 13:52:29.897340 139702543816448 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.0132285356521606, loss=5.165579795837402
I0131 13:53:15.435429 139702527031040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.901755690574646, loss=6.3534417152404785
I0131 13:54:01.158033 139702543816448 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8606855273246765, loss=5.840517044067383
I0131 13:54:47.025322 139702527031040 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7210416793823242, loss=6.420586585998535
I0131 13:54:59.988287 139863983413056 spec.py:321] Evaluating on the training split.
I0131 13:55:10.732133 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 13:55:31.256354 139863983413056 spec.py:349] Evaluating on the test split.
I0131 13:55:32.891902 139863983413056 submission_runner.py:408] Time since start: 3698.39s, 	Step: 7330, 	{'train/accuracy': 0.2987304627895355, 'train/loss': 3.6502153873443604, 'validation/accuracy': 0.2711600065231323, 'validation/loss': 3.7866060733795166, 'validation/num_examples': 50000, 'test/accuracy': 0.20920000970363617, 'test/loss': 4.218708038330078, 'test/num_examples': 10000, 'score': 3396.4188299179077, 'total_duration': 3698.38631939888, 'accumulated_submission_time': 3396.4188299179077, 'accumulated_eval_time': 301.3875472545624, 'accumulated_logging_time': 0.1950387954711914}
I0131 13:55:32.906561 139702543816448 logging_writer.py:48] [7330] accumulated_eval_time=301.387547, accumulated_logging_time=0.195039, accumulated_submission_time=3396.418830, global_step=7330, preemption_count=0, score=3396.418830, test/accuracy=0.209200, test/loss=4.218708, test/num_examples=10000, total_duration=3698.386319, train/accuracy=0.298730, train/loss=3.650215, validation/accuracy=0.271160, validation/loss=3.786606, validation/num_examples=50000
I0131 13:56:01.242353 139702527031040 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8020016551017761, loss=6.234097480773926
I0131 13:56:46.352149 139702543816448 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8061864972114563, loss=6.155999183654785
I0131 13:57:32.240415 139702527031040 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.2433850765228271, loss=5.320230960845947
I0131 13:58:18.117120 139702543816448 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.115861177444458, loss=5.286005973815918
I0131 13:59:03.763811 139702527031040 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7551457285881042, loss=6.366005897521973
I0131 13:59:49.635135 139702543816448 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8673141598701477, loss=5.341492176055908
I0131 14:00:35.764704 139702527031040 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9931930899620056, loss=5.078313827514648
I0131 14:01:21.591888 139702543816448 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0437161922454834, loss=5.083345890045166
I0131 14:02:07.747090 139702527031040 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.0201274156570435, loss=5.105767250061035
I0131 14:02:33.208580 139863983413056 spec.py:321] Evaluating on the training split.
I0131 14:02:43.802624 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 14:03:04.840757 139863983413056 spec.py:349] Evaluating on the test split.
I0131 14:03:06.473119 139863983413056 submission_runner.py:408] Time since start: 4151.97s, 	Step: 8257, 	{'train/accuracy': 0.32369139790534973, 'train/loss': 3.4396770000457764, 'validation/accuracy': 0.3033199906349182, 'validation/loss': 3.5456032752990723, 'validation/num_examples': 50000, 'test/accuracy': 0.2370000183582306, 'test/loss': 4.004726886749268, 'test/num_examples': 10000, 'score': 3816.6637468338013, 'total_duration': 4151.96754193306, 'accumulated_submission_time': 3816.6637468338013, 'accumulated_eval_time': 334.6520891189575, 'accumulated_logging_time': 0.21846985816955566}
I0131 14:03:06.487979 139702543816448 logging_writer.py:48] [8257] accumulated_eval_time=334.652089, accumulated_logging_time=0.218470, accumulated_submission_time=3816.663747, global_step=8257, preemption_count=0, score=3816.663747, test/accuracy=0.237000, test/loss=4.004727, test/num_examples=10000, total_duration=4151.967542, train/accuracy=0.323691, train/loss=3.439677, validation/accuracy=0.303320, validation/loss=3.545603, validation/num_examples=50000
I0131 14:03:24.047661 139702527031040 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.9045565128326416, loss=5.8755717277526855
I0131 14:04:07.462830 139702543816448 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.307772159576416, loss=5.046061992645264
I0131 14:04:53.189951 139702527031040 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.2095388174057007, loss=5.29093074798584
I0131 14:05:39.279757 139702543816448 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.295401692390442, loss=4.935967445373535
I0131 14:06:24.846207 139702527031040 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.0259647369384766, loss=4.942608833312988
I0131 14:07:11.052731 139702543816448 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9826234579086304, loss=4.940645217895508
I0131 14:07:57.097867 139702527031040 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.0816013813018799, loss=4.782705307006836
I0131 14:08:42.801228 139702543816448 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.9614067673683167, loss=4.889350891113281
I0131 14:09:28.623236 139702527031040 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9132895469665527, loss=4.833925247192383
I0131 14:10:06.543988 139863983413056 spec.py:321] Evaluating on the training split.
I0131 14:10:17.052050 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 14:10:38.531840 139863983413056 spec.py:349] Evaluating on the test split.
I0131 14:10:40.169290 139863983413056 submission_runner.py:408] Time since start: 4605.66s, 	Step: 9184, 	{'train/accuracy': 0.3685546815395355, 'train/loss': 3.1528046131134033, 'validation/accuracy': 0.33935999870300293, 'validation/loss': 3.2868564128875732, 'validation/num_examples': 50000, 'test/accuracy': 0.25930002331733704, 'test/loss': 3.76635479927063, 'test/num_examples': 10000, 'score': 4236.656970024109, 'total_duration': 4605.663713693619, 'accumulated_submission_time': 4236.656970024109, 'accumulated_eval_time': 368.2774066925049, 'accumulated_logging_time': 0.2466275691986084}
I0131 14:10:40.184756 139702543816448 logging_writer.py:48] [9184] accumulated_eval_time=368.277407, accumulated_logging_time=0.246628, accumulated_submission_time=4236.656970, global_step=9184, preemption_count=0, score=4236.656970, test/accuracy=0.259300, test/loss=3.766355, test/num_examples=10000, total_duration=4605.663714, train/accuracy=0.368555, train/loss=3.152805, validation/accuracy=0.339360, validation/loss=3.286856, validation/num_examples=50000
I0131 14:10:46.976349 139702527031040 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.0341063737869263, loss=4.851063251495361
I0131 14:11:28.802110 139702543816448 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9950347542762756, loss=4.81447172164917
I0131 14:12:14.469470 139702527031040 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0556763410568237, loss=4.805707931518555
I0131 14:13:00.559612 139702543816448 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9893710613250732, loss=5.381922245025635
I0131 14:13:46.360187 139702527031040 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8090547323226929, loss=5.522906303405762
I0131 14:14:32.020646 139702543816448 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.0206379890441895, loss=4.908974647521973
I0131 14:15:18.055179 139702527031040 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7825295329093933, loss=5.769402027130127
I0131 14:16:03.667877 139702543816448 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9830556511878967, loss=4.726537227630615
I0131 14:16:49.684289 139702527031040 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6821728348731995, loss=6.247550010681152
I0131 14:17:35.729836 139702543816448 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7930017709732056, loss=5.8336381912231445
I0131 14:17:40.416648 139863983413056 spec.py:321] Evaluating on the training split.
I0131 14:17:50.988230 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 14:18:13.012531 139863983413056 spec.py:349] Evaluating on the test split.
I0131 14:18:14.650333 139863983413056 submission_runner.py:408] Time since start: 5060.14s, 	Step: 10112, 	{'train/accuracy': 0.4025000035762787, 'train/loss': 3.0011579990386963, 'validation/accuracy': 0.36711999773979187, 'validation/loss': 3.1657626628875732, 'validation/num_examples': 50000, 'test/accuracy': 0.2835000157356262, 'test/loss': 3.687035322189331, 'test/num_examples': 10000, 'score': 4656.829707622528, 'total_duration': 5060.144756317139, 'accumulated_submission_time': 4656.829707622528, 'accumulated_eval_time': 402.5110983848572, 'accumulated_logging_time': 0.27249979972839355}
I0131 14:18:14.665917 139702527031040 logging_writer.py:48] [10112] accumulated_eval_time=402.511098, accumulated_logging_time=0.272500, accumulated_submission_time=4656.829708, global_step=10112, preemption_count=0, score=4656.829708, test/accuracy=0.283500, test/loss=3.687035, test/num_examples=10000, total_duration=5060.144756, train/accuracy=0.402500, train/loss=3.001158, validation/accuracy=0.367120, validation/loss=3.165763, validation/num_examples=50000
I0131 14:18:50.268950 139702543816448 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9358989000320435, loss=4.989795684814453
I0131 14:19:35.871371 139702527031040 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6697974801063538, loss=5.741494178771973
I0131 14:20:22.178312 139702543816448 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.8285263180732727, loss=5.323274612426758
I0131 14:21:08.050701 139702527031040 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9168576002120972, loss=4.774770736694336
I0131 14:21:53.486582 139702543816448 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.9269749522209167, loss=4.667577743530273
I0131 14:22:39.351721 139702527031040 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.9929578900337219, loss=4.610405921936035
I0131 14:23:25.024767 139702543816448 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6902773380279541, loss=6.101008415222168
I0131 14:24:10.787030 139702527031040 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9229522347450256, loss=4.639388084411621
I0131 14:24:56.718778 139702543816448 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.030887246131897, loss=4.65067195892334
I0131 14:25:14.733599 139863983413056 spec.py:321] Evaluating on the training split.
I0131 14:25:25.371848 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 14:25:46.024277 139863983413056 spec.py:349] Evaluating on the test split.
I0131 14:25:47.667325 139863983413056 submission_runner.py:408] Time since start: 5513.16s, 	Step: 11041, 	{'train/accuracy': 0.4302734136581421, 'train/loss': 2.806786060333252, 'validation/accuracy': 0.3969399929046631, 'validation/loss': 2.9504761695861816, 'validation/num_examples': 50000, 'test/accuracy': 0.3059000074863434, 'test/loss': 3.4966979026794434, 'test/num_examples': 10000, 'score': 5076.838560819626, 'total_duration': 5513.161741495132, 'accumulated_submission_time': 5076.838560819626, 'accumulated_eval_time': 435.44481587409973, 'accumulated_logging_time': 0.2978024482727051}
I0131 14:25:47.683317 139702527031040 logging_writer.py:48] [11041] accumulated_eval_time=435.444816, accumulated_logging_time=0.297802, accumulated_submission_time=5076.838561, global_step=11041, preemption_count=0, score=5076.838561, test/accuracy=0.305900, test/loss=3.496698, test/num_examples=10000, total_duration=5513.161741, train/accuracy=0.430273, train/loss=2.806786, validation/accuracy=0.396940, validation/loss=2.950476, validation/num_examples=50000
I0131 14:26:11.639393 139702543816448 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7808151841163635, loss=5.489930152893066
I0131 14:26:56.060411 139702527031040 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9495526552200317, loss=4.548248291015625
I0131 14:27:42.413688 139702543816448 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6533733606338501, loss=5.7790374755859375
I0131 14:28:28.780111 139702527031040 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.9003775119781494, loss=4.732724666595459
I0131 14:29:14.405721 139702543816448 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7421802282333374, loss=5.85313081741333
I0131 14:30:00.241870 139702527031040 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8258036971092224, loss=5.133234977722168
I0131 14:30:45.978044 139702543816448 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.9395791292190552, loss=4.952604293823242
I0131 14:31:31.642320 139702527031040 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9862304925918579, loss=4.529726028442383
I0131 14:32:17.391099 139702543816448 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9371976256370544, loss=4.437963962554932
I0131 14:32:48.081737 139863983413056 spec.py:321] Evaluating on the training split.
I0131 14:32:58.700201 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 14:33:18.000279 139863983413056 spec.py:349] Evaluating on the test split.
I0131 14:33:19.653638 139863983413056 submission_runner.py:408] Time since start: 5965.15s, 	Step: 11969, 	{'train/accuracy': 0.4562304615974426, 'train/loss': 2.67525053024292, 'validation/accuracy': 0.4208199977874756, 'validation/loss': 2.8304824829101562, 'validation/num_examples': 50000, 'test/accuracy': 0.3264000117778778, 'test/loss': 3.385175943374634, 'test/num_examples': 10000, 'score': 5497.176267147064, 'total_duration': 5965.148057460785, 'accumulated_submission_time': 5497.176267147064, 'accumulated_eval_time': 467.0167169570923, 'accumulated_logging_time': 0.32605648040771484}
I0131 14:33:19.669615 139702527031040 logging_writer.py:48] [11969] accumulated_eval_time=467.016717, accumulated_logging_time=0.326056, accumulated_submission_time=5497.176267, global_step=11969, preemption_count=0, score=5497.176267, test/accuracy=0.326400, test/loss=3.385176, test/num_examples=10000, total_duration=5965.148057, train/accuracy=0.456230, train/loss=2.675251, validation/accuracy=0.420820, validation/loss=2.830482, validation/num_examples=50000
I0131 14:33:32.494834 139702543816448 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9033206105232239, loss=4.9354071617126465
I0131 14:34:15.452518 139702527031040 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.936426043510437, loss=4.4596028327941895
I0131 14:35:01.099714 139702543816448 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8557860255241394, loss=4.6101202964782715
I0131 14:35:46.877641 139702527031040 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9173169136047363, loss=4.509782791137695
I0131 14:36:32.599252 139702543816448 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.1260101795196533, loss=4.607253074645996
I0131 14:37:18.539642 139702527031040 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7716861367225647, loss=4.72397518157959
I0131 14:38:04.190334 139702543816448 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9943759441375732, loss=4.453143119812012
I0131 14:38:49.865764 139702527031040 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.9571236371994019, loss=4.511553764343262
I0131 14:39:35.575599 139702543816448 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9332913756370544, loss=4.448863506317139
I0131 14:40:19.758847 139863983413056 spec.py:321] Evaluating on the training split.
I0131 14:40:30.474698 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 14:40:51.728154 139863983413056 spec.py:349] Evaluating on the test split.
I0131 14:40:53.367795 139863983413056 submission_runner.py:408] Time since start: 6418.86s, 	Step: 12898, 	{'train/accuracy': 0.4791015386581421, 'train/loss': 2.5726089477539062, 'validation/accuracy': 0.43879997730255127, 'validation/loss': 2.7511792182922363, 'validation/num_examples': 50000, 'test/accuracy': 0.33970001339912415, 'test/loss': 3.322092294692993, 'test/num_examples': 10000, 'score': 5917.206485748291, 'total_duration': 6418.862198352814, 'accumulated_submission_time': 5917.206485748291, 'accumulated_eval_time': 500.62565183639526, 'accumulated_logging_time': 0.3518826961517334}
I0131 14:40:53.386153 139702527031040 logging_writer.py:48] [12898] accumulated_eval_time=500.625652, accumulated_logging_time=0.351883, accumulated_submission_time=5917.206486, global_step=12898, preemption_count=0, score=5917.206486, test/accuracy=0.339700, test/loss=3.322092, test/num_examples=10000, total_duration=6418.862198, train/accuracy=0.479102, train/loss=2.572609, validation/accuracy=0.438800, validation/loss=2.751179, validation/num_examples=50000
I0131 14:40:54.586348 139702543816448 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9639437198638916, loss=4.522727012634277
I0131 14:41:35.569039 139702527031040 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7506332993507385, loss=4.715899467468262
I0131 14:42:21.300242 139702543816448 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9259092807769775, loss=4.705798149108887
I0131 14:43:07.136893 139702527031040 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.02122163772583, loss=4.324638366699219
I0131 14:43:52.788474 139702543816448 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.0264257192611694, loss=4.499807834625244
I0131 14:44:38.823437 139702527031040 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6646719574928284, loss=5.544939041137695
I0131 14:45:24.633590 139702543816448 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7041699290275574, loss=5.4540605545043945
I0131 14:46:10.349595 139702527031040 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9297098517417908, loss=4.431266784667969
I0131 14:46:56.294091 139702543816448 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.8835150599479675, loss=4.430917739868164
I0131 14:47:42.484366 139702527031040 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6484888195991516, loss=5.996181488037109
I0131 14:47:53.543332 139863983413056 spec.py:321] Evaluating on the training split.
I0131 14:48:03.984654 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 14:48:25.543734 139863983413056 spec.py:349] Evaluating on the test split.
I0131 14:48:27.186908 139863983413056 submission_runner.py:408] Time since start: 6872.68s, 	Step: 13826, 	{'train/accuracy': 0.5181445479393005, 'train/loss': 2.3554434776306152, 'validation/accuracy': 0.4603399932384491, 'validation/loss': 2.6169192790985107, 'validation/num_examples': 50000, 'test/accuracy': 0.35690000653266907, 'test/loss': 3.201007843017578, 'test/num_examples': 10000, 'score': 6337.3051698207855, 'total_duration': 6872.681324005127, 'accumulated_submission_time': 6337.3051698207855, 'accumulated_eval_time': 534.2692155838013, 'accumulated_logging_time': 0.37982916831970215}
I0131 14:48:27.203209 139702543816448 logging_writer.py:48] [13826] accumulated_eval_time=534.269216, accumulated_logging_time=0.379829, accumulated_submission_time=6337.305170, global_step=13826, preemption_count=0, score=6337.305170, test/accuracy=0.356900, test/loss=3.201008, test/num_examples=10000, total_duration=6872.681324, train/accuracy=0.518145, train/loss=2.355443, validation/accuracy=0.460340, validation/loss=2.616919, validation/num_examples=50000
I0131 14:48:57.170892 139702527031040 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.8131276369094849, loss=5.013982772827148
I0131 14:49:41.777937 139702543816448 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.907850980758667, loss=4.420688629150391
I0131 14:50:28.093470 139702527031040 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.0236382484436035, loss=4.331766605377197
I0131 14:51:13.934266 139702543816448 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.908896803855896, loss=4.656591415405273
I0131 14:51:59.462760 139702527031040 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.9600951075553894, loss=4.300500392913818
I0131 14:52:45.118375 139702543816448 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7697557806968689, loss=4.98765754699707
I0131 14:53:31.088208 139702527031040 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.941428005695343, loss=4.348820686340332
I0131 14:54:16.829207 139702543816448 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.8777529001235962, loss=5.003766059875488
I0131 14:55:02.604278 139702527031040 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6117787957191467, loss=6.022520065307617
I0131 14:55:27.444406 139863983413056 spec.py:321] Evaluating on the training split.
I0131 14:55:37.949957 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 14:55:53.929190 139863983413056 spec.py:349] Evaluating on the test split.
I0131 14:55:55.613253 139863983413056 submission_runner.py:408] Time since start: 7321.11s, 	Step: 14756, 	{'train/accuracy': 0.5152734518051147, 'train/loss': 2.3585195541381836, 'validation/accuracy': 0.47669997811317444, 'validation/loss': 2.536402463912964, 'validation/num_examples': 50000, 'test/accuracy': 0.3725000321865082, 'test/loss': 3.11665415763855, 'test/num_examples': 10000, 'score': 6757.488060712814, 'total_duration': 7321.107630968094, 'accumulated_submission_time': 6757.488060712814, 'accumulated_eval_time': 562.43803191185, 'accumulated_logging_time': 0.4050462245941162}
I0131 14:55:55.643264 139702543816448 logging_writer.py:48] [14756] accumulated_eval_time=562.438032, accumulated_logging_time=0.405046, accumulated_submission_time=6757.488061, global_step=14756, preemption_count=0, score=6757.488061, test/accuracy=0.372500, test/loss=3.116654, test/num_examples=10000, total_duration=7321.107631, train/accuracy=0.515273, train/loss=2.358520, validation/accuracy=0.476700, validation/loss=2.536402, validation/num_examples=50000
I0131 14:56:13.671272 139702527031040 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.148003101348877, loss=4.248897552490234
I0131 14:56:57.546675 139702543816448 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.883594274520874, loss=4.534219741821289
I0131 14:57:43.637992 139702527031040 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7915986776351929, loss=6.038218021392822
I0131 14:58:29.697573 139702543816448 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.8850467801094055, loss=4.399672508239746
I0131 14:59:15.580744 139702527031040 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.8931418657302856, loss=4.213795185089111
I0131 15:00:01.695453 139702543816448 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.8866534233093262, loss=4.261960029602051
I0131 15:00:47.640098 139702527031040 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.0516241788864136, loss=4.365086555480957
I0131 15:01:33.701936 139702543816448 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6798861026763916, loss=5.795732021331787
I0131 15:02:19.416786 139702527031040 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.9159060120582581, loss=4.587523460388184
I0131 15:02:55.737257 139863983413056 spec.py:321] Evaluating on the training split.
I0131 15:03:06.375501 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 15:03:27.136841 139863983413056 spec.py:349] Evaluating on the test split.
I0131 15:03:28.780838 139863983413056 submission_runner.py:408] Time since start: 7774.28s, 	Step: 15681, 	{'train/accuracy': 0.5367968678474426, 'train/loss': 2.26556658744812, 'validation/accuracy': 0.49793997406959534, 'validation/loss': 2.443150281906128, 'validation/num_examples': 50000, 'test/accuracy': 0.38520002365112305, 'test/loss': 3.0521280765533447, 'test/num_examples': 10000, 'score': 7177.518812179565, 'total_duration': 7774.275252819061, 'accumulated_submission_time': 7177.518812179565, 'accumulated_eval_time': 595.4816019535065, 'accumulated_logging_time': 0.44942355155944824}
I0131 15:03:28.796962 139702543816448 logging_writer.py:48] [15681] accumulated_eval_time=595.481602, accumulated_logging_time=0.449424, accumulated_submission_time=7177.518812, global_step=15681, preemption_count=0, score=7177.518812, test/accuracy=0.385200, test/loss=3.052128, test/num_examples=10000, total_duration=7774.275253, train/accuracy=0.536797, train/loss=2.265567, validation/accuracy=0.497940, validation/loss=2.443150, validation/num_examples=50000
I0131 15:03:36.783346 139702527031040 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9542517066001892, loss=4.272071838378906
I0131 15:04:18.442446 139702543816448 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0297389030456543, loss=4.245311260223389
I0131 15:05:04.035201 139702527031040 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.7229664921760559, loss=4.657066822052002
I0131 15:05:49.951184 139702543816448 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9837969541549683, loss=4.584667682647705
I0131 15:06:35.664356 139702527031040 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7849628329277039, loss=4.674222946166992
I0131 15:07:21.334599 139702543816448 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7934421300888062, loss=5.188580513000488
I0131 15:08:07.567058 139702527031040 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9432335495948792, loss=4.178814888000488
I0131 15:08:53.011341 139702543816448 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.7402543425559998, loss=5.964666366577148
I0131 15:09:38.718324 139702527031040 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9513788819313049, loss=4.264399528503418
I0131 15:10:24.668159 139702543816448 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6507198810577393, loss=5.7670722007751465
I0131 15:10:28.936830 139863983413056 spec.py:321] Evaluating on the training split.
I0131 15:10:39.382470 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 15:10:58.601903 139863983413056 spec.py:349] Evaluating on the test split.
I0131 15:11:00.240708 139863983413056 submission_runner.py:408] Time since start: 8225.74s, 	Step: 16611, 	{'train/accuracy': 0.5549218654632568, 'train/loss': 2.1838366985321045, 'validation/accuracy': 0.501039981842041, 'validation/loss': 2.4183454513549805, 'validation/num_examples': 50000, 'test/accuracy': 0.39080002903938293, 'test/loss': 3.024139881134033, 'test/num_examples': 10000, 'score': 7597.600474834442, 'total_duration': 8225.735122203827, 'accumulated_submission_time': 7597.600474834442, 'accumulated_eval_time': 626.7855026721954, 'accumulated_logging_time': 0.4745340347290039}
I0131 15:11:00.261349 139702527031040 logging_writer.py:48] [16611] accumulated_eval_time=626.785503, accumulated_logging_time=0.474534, accumulated_submission_time=7597.600475, global_step=16611, preemption_count=0, score=7597.600475, test/accuracy=0.390800, test/loss=3.024140, test/num_examples=10000, total_duration=8225.735122, train/accuracy=0.554922, train/loss=2.183837, validation/accuracy=0.501040, validation/loss=2.418345, validation/num_examples=50000
I0131 15:11:36.213014 139702543816448 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6667962670326233, loss=5.664669513702393
I0131 15:12:21.941638 139702527031040 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.8364128470420837, loss=4.451310157775879
I0131 15:13:07.695994 139702543816448 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7087385654449463, loss=5.810014724731445
I0131 15:13:53.721082 139702527031040 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.9334904551506042, loss=4.195849895477295
I0131 15:14:39.445592 139702543816448 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9039838314056396, loss=4.164125442504883
I0131 15:15:25.103924 139702527031040 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9229772686958313, loss=4.284212112426758
I0131 15:16:10.857463 139702543816448 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.9034750461578369, loss=4.28999137878418
I0131 15:16:56.452236 139702527031040 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.9857468605041504, loss=4.064066410064697
I0131 15:17:42.325333 139702543816448 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8390213847160339, loss=4.168951034545898
I0131 15:18:00.697973 139863983413056 spec.py:321] Evaluating on the training split.
I0131 15:18:11.403286 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 15:18:31.252545 139863983413056 spec.py:349] Evaluating on the test split.
I0131 15:18:32.908068 139863983413056 submission_runner.py:408] Time since start: 8678.40s, 	Step: 17542, 	{'train/accuracy': 0.5504687428474426, 'train/loss': 2.1938395500183105, 'validation/accuracy': 0.5118199586868286, 'validation/loss': 2.3658089637756348, 'validation/num_examples': 50000, 'test/accuracy': 0.40570002794265747, 'test/loss': 2.9616432189941406, 'test/num_examples': 10000, 'score': 8017.97752737999, 'total_duration': 8678.402475357056, 'accumulated_submission_time': 8017.97752737999, 'accumulated_eval_time': 658.9955780506134, 'accumulated_logging_time': 0.5048494338989258}
I0131 15:18:32.928983 139702527031040 logging_writer.py:48] [17542] accumulated_eval_time=658.995578, accumulated_logging_time=0.504849, accumulated_submission_time=8017.977527, global_step=17542, preemption_count=0, score=8017.977527, test/accuracy=0.405700, test/loss=2.961643, test/num_examples=10000, total_duration=8678.402475, train/accuracy=0.550469, train/loss=2.193840, validation/accuracy=0.511820, validation/loss=2.365809, validation/num_examples=50000
I0131 15:18:56.504750 139702543816448 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.9881796836853027, loss=4.545062065124512
I0131 15:19:40.819689 139702527031040 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.8428460955619812, loss=4.356602191925049
I0131 15:20:26.765110 139702543816448 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.9846513271331787, loss=4.138406753540039
I0131 15:21:12.339506 139702527031040 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.9286254644393921, loss=4.194284439086914
I0131 15:22:00.540287 139702543816448 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.9699769020080566, loss=4.312677383422852
I0131 15:22:46.656778 139702527031040 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9137299060821533, loss=4.094517230987549
I0131 15:23:32.589221 139702543816448 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7960023283958435, loss=4.965676307678223
I0131 15:24:18.518204 139702527031040 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.005982756614685, loss=4.083747863769531
I0131 15:25:04.365089 139702543816448 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.9485464692115784, loss=4.315483093261719
I0131 15:25:32.914659 139863983413056 spec.py:321] Evaluating on the training split.
I0131 15:25:43.619268 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 15:26:05.522616 139863983413056 spec.py:349] Evaluating on the test split.
I0131 15:26:07.166900 139863983413056 submission_runner.py:408] Time since start: 9132.66s, 	Step: 18464, 	{'train/accuracy': 0.5669335722923279, 'train/loss': 2.0964860916137695, 'validation/accuracy': 0.5268599987030029, 'validation/loss': 2.2790541648864746, 'validation/num_examples': 50000, 'test/accuracy': 0.4108000099658966, 'test/loss': 2.897355794906616, 'test/num_examples': 10000, 'score': 8437.904272794724, 'total_duration': 9132.661324977875, 'accumulated_submission_time': 8437.904272794724, 'accumulated_eval_time': 693.2478134632111, 'accumulated_logging_time': 0.5354297161102295}
I0131 15:26:07.183908 139702527031040 logging_writer.py:48] [18464] accumulated_eval_time=693.247813, accumulated_logging_time=0.535430, accumulated_submission_time=8437.904273, global_step=18464, preemption_count=0, score=8437.904273, test/accuracy=0.410800, test/loss=2.897356, test/num_examples=10000, total_duration=9132.661325, train/accuracy=0.566934, train/loss=2.096486, validation/accuracy=0.526860, validation/loss=2.279054, validation/num_examples=50000
I0131 15:26:21.959118 139702543816448 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.9415355920791626, loss=4.124600887298584
I0131 15:27:04.764462 139702527031040 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.9094930291175842, loss=4.221580505371094
I0131 15:27:50.617070 139702543816448 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.978078305721283, loss=4.10402250289917
I0131 15:28:37.135637 139702527031040 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7729892134666443, loss=4.511646270751953
I0131 15:29:22.724880 139702543816448 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9124184250831604, loss=4.049290657043457
I0131 15:30:08.934354 139702527031040 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7036076784133911, loss=5.7907304763793945
I0131 15:30:54.976681 139702543816448 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.0688138008117676, loss=4.031494617462158
I0131 15:31:40.579623 139702527031040 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9036636352539062, loss=4.096724510192871
I0131 15:32:26.405371 139702543816448 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7816851735115051, loss=4.598702907562256
I0131 15:33:07.557716 139863983413056 spec.py:321] Evaluating on the training split.
I0131 15:33:18.210753 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 15:33:41.076133 139863983413056 spec.py:349] Evaluating on the test split.
I0131 15:33:42.713977 139863983413056 submission_runner.py:408] Time since start: 9588.21s, 	Step: 19391, 	{'train/accuracy': 0.5886914134025574, 'train/loss': 1.9833548069000244, 'validation/accuracy': 0.5397199988365173, 'validation/loss': 2.214489698410034, 'validation/num_examples': 50000, 'test/accuracy': 0.4244000315666199, 'test/loss': 2.8181262016296387, 'test/num_examples': 10000, 'score': 8858.220192193985, 'total_duration': 9588.208375692368, 'accumulated_submission_time': 8858.220192193985, 'accumulated_eval_time': 728.4040546417236, 'accumulated_logging_time': 0.5617325305938721}
I0131 15:33:42.735097 139702527031040 logging_writer.py:48] [19391] accumulated_eval_time=728.404055, accumulated_logging_time=0.561733, accumulated_submission_time=8858.220192, global_step=19391, preemption_count=0, score=8858.220192, test/accuracy=0.424400, test/loss=2.818126, test/num_examples=10000, total_duration=9588.208376, train/accuracy=0.588691, train/loss=1.983355, validation/accuracy=0.539720, validation/loss=2.214490, validation/num_examples=50000
I0131 15:33:46.721116 139702543816448 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9655618667602539, loss=4.02849817276001
I0131 15:34:27.708023 139702527031040 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.727523684501648, loss=5.199490070343018
I0131 15:35:13.561734 139702543816448 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7997617125511169, loss=5.214975357055664
I0131 15:35:59.023928 139702527031040 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.9718050360679626, loss=4.114163875579834
I0131 15:36:44.798486 139702543816448 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8958759903907776, loss=4.005195617675781
I0131 15:37:30.589626 139702527031040 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.9353750944137573, loss=4.062991619110107
I0131 15:38:16.512678 139702543816448 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9324842095375061, loss=3.991802215576172
I0131 15:39:02.417831 139702527031040 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.8553676605224609, loss=4.001535415649414
I0131 15:39:48.223738 139702543816448 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7050400376319885, loss=4.959665775299072
I0131 15:40:34.292243 139702527031040 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7477913498878479, loss=4.932042598724365
I0131 15:40:43.040361 139863983413056 spec.py:321] Evaluating on the training split.
I0131 15:40:53.693443 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 15:41:16.608542 139863983413056 spec.py:349] Evaluating on the test split.
I0131 15:41:18.244655 139863983413056 submission_runner.py:408] Time since start: 10043.74s, 	Step: 20321, 	{'train/accuracy': 0.5872656106948853, 'train/loss': 1.982076644897461, 'validation/accuracy': 0.5415999889373779, 'validation/loss': 2.1796276569366455, 'validation/num_examples': 50000, 'test/accuracy': 0.43080002069473267, 'test/loss': 2.7886321544647217, 'test/num_examples': 10000, 'score': 9278.46674156189, 'total_duration': 10043.739041805267, 'accumulated_submission_time': 9278.46674156189, 'accumulated_eval_time': 763.6083111763, 'accumulated_logging_time': 0.5928773880004883}
I0131 15:41:18.268794 139702543816448 logging_writer.py:48] [20321] accumulated_eval_time=763.608311, accumulated_logging_time=0.592877, accumulated_submission_time=9278.466742, global_step=20321, preemption_count=0, score=9278.466742, test/accuracy=0.430800, test/loss=2.788632, test/num_examples=10000, total_duration=10043.739042, train/accuracy=0.587266, train/loss=1.982077, validation/accuracy=0.541600, validation/loss=2.179628, validation/num_examples=50000
I0131 15:41:50.206993 139702527031040 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.8930039405822754, loss=3.962672233581543
I0131 15:42:35.861621 139702543816448 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.900688648223877, loss=4.377899646759033
I0131 15:43:21.933452 139702527031040 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.8604378700256348, loss=4.179896831512451
I0131 15:44:07.982910 139702543816448 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.8858026266098022, loss=3.9894633293151855
I0131 15:44:53.591343 139702527031040 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.8460804224014282, loss=4.54552698135376
I0131 15:45:39.844970 139702543816448 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.9439104795455933, loss=3.9746603965759277
I0131 15:46:25.770877 139702527031040 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7668843865394592, loss=5.334712028503418
I0131 15:47:11.670508 139702543816448 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.9563997387886047, loss=4.10426664352417
I0131 15:47:57.400808 139702527031040 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.827241063117981, loss=5.629220962524414
I0131 15:48:18.648847 139863983413056 spec.py:321] Evaluating on the training split.
I0131 15:48:28.793397 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 15:48:52.864267 139863983413056 spec.py:349] Evaluating on the test split.
I0131 15:48:54.507961 139863983413056 submission_runner.py:408] Time since start: 10500.00s, 	Step: 21248, 	{'train/accuracy': 0.5874413847923279, 'train/loss': 1.9971106052398682, 'validation/accuracy': 0.5496799945831299, 'validation/loss': 2.1746737957000732, 'validation/num_examples': 50000, 'test/accuracy': 0.4305000305175781, 'test/loss': 2.7999267578125, 'test/num_examples': 10000, 'score': 9698.78861618042, 'total_duration': 10500.002382278442, 'accumulated_submission_time': 9698.78861618042, 'accumulated_eval_time': 799.4674112796783, 'accumulated_logging_time': 0.6267457008361816}
I0131 15:48:54.526154 139702543816448 logging_writer.py:48] [21248] accumulated_eval_time=799.467411, accumulated_logging_time=0.626746, accumulated_submission_time=9698.788616, global_step=21248, preemption_count=0, score=9698.788616, test/accuracy=0.430500, test/loss=2.799927, test/num_examples=10000, total_duration=10500.002382, train/accuracy=0.587441, train/loss=1.997111, validation/accuracy=0.549680, validation/loss=2.174674, validation/num_examples=50000
I0131 15:49:15.722888 139702527031040 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.9266296625137329, loss=4.048625469207764
I0131 15:49:59.425341 139702543816448 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9429606199264526, loss=4.044028282165527
I0131 15:50:45.210380 139702527031040 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8079644441604614, loss=4.244744300842285
I0131 15:51:31.289116 139702543816448 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.8121660947799683, loss=4.500384330749512
I0131 15:52:17.094879 139702527031040 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6872727274894714, loss=5.750946521759033
I0131 15:53:02.915286 139702543816448 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6400200724601746, loss=5.788798809051514
I0131 15:53:48.728359 139702527031040 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.9654767513275146, loss=3.9743032455444336
I0131 15:54:34.677365 139702543816448 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.9222590923309326, loss=4.033592224121094
I0131 15:55:20.304828 139702527031040 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9570928812026978, loss=3.99336314201355
I0131 15:55:54.758814 139863983413056 spec.py:321] Evaluating on the training split.
I0131 15:56:05.596115 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 15:56:28.810197 139863983413056 spec.py:349] Evaluating on the test split.
I0131 15:56:30.459140 139863983413056 submission_runner.py:408] Time since start: 10955.95s, 	Step: 22177, 	{'train/accuracy': 0.6080663800239563, 'train/loss': 1.9223328828811646, 'validation/accuracy': 0.5569199919700623, 'validation/loss': 2.1479055881500244, 'validation/num_examples': 50000, 'test/accuracy': 0.44140002131462097, 'test/loss': 2.7625842094421387, 'test/num_examples': 10000, 'score': 10118.962321043015, 'total_duration': 10955.953563451767, 'accumulated_submission_time': 10118.962321043015, 'accumulated_eval_time': 835.1677443981171, 'accumulated_logging_time': 0.6544830799102783}
I0131 15:56:30.476553 139702543816448 logging_writer.py:48] [22177] accumulated_eval_time=835.167744, accumulated_logging_time=0.654483, accumulated_submission_time=10118.962321, global_step=22177, preemption_count=0, score=10118.962321, test/accuracy=0.441400, test/loss=2.762584, test/num_examples=10000, total_duration=10955.953563, train/accuracy=0.608066, train/loss=1.922333, validation/accuracy=0.556920, validation/loss=2.147906, validation/num_examples=50000
I0131 15:56:40.059169 139702527031040 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7923181653022766, loss=5.496382713317871
I0131 15:57:22.130703 139702543816448 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9177666306495667, loss=3.9256045818328857
I0131 15:58:08.040134 139702527031040 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.9839239120483398, loss=3.9353113174438477
I0131 15:58:53.937768 139702543816448 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6859986186027527, loss=5.442223072052002
I0131 15:59:40.046602 139702527031040 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.9163094758987427, loss=3.9720208644866943
I0131 16:00:25.823887 139702543816448 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.867508053779602, loss=4.518326282501221
I0131 16:01:11.836259 139702527031040 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.9137009382247925, loss=4.091423988342285
I0131 16:01:57.502768 139702543816448 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.9273629188537598, loss=3.9045441150665283
I0131 16:02:43.505489 139702527031040 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9492236971855164, loss=3.9803452491760254
I0131 16:03:29.407575 139702543816448 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.944692075252533, loss=3.9597673416137695
I0131 16:03:30.963976 139863983413056 spec.py:321] Evaluating on the training split.
I0131 16:03:41.628922 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 16:04:04.140295 139863983413056 spec.py:349] Evaluating on the test split.
I0131 16:04:05.779676 139863983413056 submission_runner.py:408] Time since start: 11411.27s, 	Step: 23105, 	{'train/accuracy': 0.6371484398841858, 'train/loss': 1.7699031829833984, 'validation/accuracy': 0.5706999897956848, 'validation/loss': 2.06937313079834, 'validation/num_examples': 50000, 'test/accuracy': 0.4481000304222107, 'test/loss': 2.6906943321228027, 'test/num_examples': 10000, 'score': 10539.390924930573, 'total_duration': 11411.274099826813, 'accumulated_submission_time': 10539.390924930573, 'accumulated_eval_time': 869.9834413528442, 'accumulated_logging_time': 0.6817433834075928}
I0131 16:04:05.797784 139702527031040 logging_writer.py:48] [23105] accumulated_eval_time=869.983441, accumulated_logging_time=0.681743, accumulated_submission_time=10539.390925, global_step=23105, preemption_count=0, score=10539.390925, test/accuracy=0.448100, test/loss=2.690694, test/num_examples=10000, total_duration=11411.274100, train/accuracy=0.637148, train/loss=1.769903, validation/accuracy=0.570700, validation/loss=2.069373, validation/num_examples=50000
I0131 16:04:44.817746 139702543816448 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.8982440233230591, loss=4.012619972229004
I0131 16:05:30.566990 139702527031040 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.9860247373580933, loss=3.984490156173706
I0131 16:06:16.340253 139702543816448 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.9902837872505188, loss=3.952336072921753
I0131 16:07:02.159131 139702527031040 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.9259277582168579, loss=3.9815993309020996
I0131 16:07:47.967698 139702543816448 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.9062232971191406, loss=4.831714630126953
I0131 16:08:34.017930 139702527031040 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.9534617066383362, loss=3.943629264831543
I0131 16:09:20.201149 139702543816448 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.756487250328064, loss=5.632547378540039
I0131 16:10:06.342383 139702527031040 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.9487838745117188, loss=4.0856828689575195
I0131 16:10:51.893564 139702543816448 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.8413971662521362, loss=4.263676166534424
I0131 16:11:05.828686 139863983413056 spec.py:321] Evaluating on the training split.
I0131 16:11:16.475673 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 16:11:38.888533 139863983413056 spec.py:349] Evaluating on the test split.
I0131 16:11:40.539663 139863983413056 submission_runner.py:408] Time since start: 11866.03s, 	Step: 24032, 	{'train/accuracy': 0.6188671588897705, 'train/loss': 1.8651227951049805, 'validation/accuracy': 0.5744799971580505, 'validation/loss': 2.0639655590057373, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.681931734085083, 'test/num_examples': 10000, 'score': 10959.36401629448, 'total_duration': 11866.034049749374, 'accumulated_submission_time': 10959.36401629448, 'accumulated_eval_time': 904.6943933963776, 'accumulated_logging_time': 0.7084939479827881}
I0131 16:11:40.562411 139702527031040 logging_writer.py:48] [24032] accumulated_eval_time=904.694393, accumulated_logging_time=0.708494, accumulated_submission_time=10959.364016, global_step=24032, preemption_count=0, score=10959.364016, test/accuracy=0.458000, test/loss=2.681932, test/num_examples=10000, total_duration=11866.034050, train/accuracy=0.618867, train/loss=1.865123, validation/accuracy=0.574480, validation/loss=2.063966, validation/num_examples=50000
I0131 16:12:08.154183 139702543816448 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.9698129892349243, loss=3.9951059818267822
I0131 16:12:52.968164 139702527031040 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.8432939648628235, loss=5.043840408325195
I0131 16:13:39.121090 139702543816448 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.8969264626502991, loss=4.132306098937988
I0131 16:14:25.468367 139702527031040 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7542328834533691, loss=5.39321756362915
I0131 16:15:11.419570 139702543816448 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8845947980880737, loss=3.946092128753662
I0131 16:15:57.313395 139702527031040 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.8204697966575623, loss=4.329314231872559
I0131 16:16:43.383376 139702543816448 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.7799566388130188, loss=4.949124336242676
I0131 16:17:29.117916 139702527031040 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9414092898368835, loss=3.8776378631591797
I0131 16:18:15.034411 139702543816448 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.7168788313865662, loss=5.662775039672852
I0131 16:18:40.667340 139863983413056 spec.py:321] Evaluating on the training split.
I0131 16:18:51.347018 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 16:19:14.919844 139863983413056 spec.py:349] Evaluating on the test split.
I0131 16:19:16.568747 139863983413056 submission_runner.py:408] Time since start: 12322.06s, 	Step: 24957, 	{'train/accuracy': 0.6286327838897705, 'train/loss': 1.810321569442749, 'validation/accuracy': 0.5790799856185913, 'validation/loss': 2.0259885787963867, 'validation/num_examples': 50000, 'test/accuracy': 0.46330001950263977, 'test/loss': 2.642620325088501, 'test/num_examples': 10000, 'score': 11379.409708976746, 'total_duration': 12322.063168525696, 'accumulated_submission_time': 11379.409708976746, 'accumulated_eval_time': 940.5957970619202, 'accumulated_logging_time': 0.7409365177154541}
I0131 16:19:16.586803 139702527031040 logging_writer.py:48] [24957] accumulated_eval_time=940.595797, accumulated_logging_time=0.740937, accumulated_submission_time=11379.409709, global_step=24957, preemption_count=0, score=11379.409709, test/accuracy=0.463300, test/loss=2.642620, test/num_examples=10000, total_duration=12322.063169, train/accuracy=0.628633, train/loss=1.810322, validation/accuracy=0.579080, validation/loss=2.025989, validation/num_examples=50000
I0131 16:19:34.173630 139702543816448 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8320698142051697, loss=3.923002004623413
I0131 16:20:17.865629 139702527031040 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8258431553840637, loss=4.1861114501953125
I0131 16:21:03.712216 139702543816448 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.8956149816513062, loss=3.8239309787750244
I0131 16:21:49.780507 139702527031040 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.031599521636963, loss=3.8870108127593994
I0131 16:22:35.570241 139702543816448 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.7797278761863708, loss=5.311532974243164
I0131 16:23:21.314037 139702527031040 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8314296007156372, loss=4.904684543609619
I0131 16:24:07.324253 139702543816448 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.7886216044425964, loss=4.6869587898254395
I0131 16:24:52.735311 139702527031040 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8669397830963135, loss=4.549607276916504
I0131 16:25:38.498488 139702543816448 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.8671640157699585, loss=3.943154811859131
I0131 16:26:16.659238 139863983413056 spec.py:321] Evaluating on the training split.
I0131 16:26:27.118931 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 16:26:48.762543 139863983413056 spec.py:349] Evaluating on the test split.
I0131 16:26:50.405999 139863983413056 submission_runner.py:408] Time since start: 12775.90s, 	Step: 25885, 	{'train/accuracy': 0.6464648246765137, 'train/loss': 1.7180976867675781, 'validation/accuracy': 0.5830000042915344, 'validation/loss': 1.9904863834381104, 'validation/num_examples': 50000, 'test/accuracy': 0.46400001645088196, 'test/loss': 2.601236343383789, 'test/num_examples': 10000, 'score': 11799.423764944077, 'total_duration': 12775.900420188904, 'accumulated_submission_time': 11799.423764944077, 'accumulated_eval_time': 974.3425529003143, 'accumulated_logging_time': 0.7686212062835693}
I0131 16:26:50.427583 139702527031040 logging_writer.py:48] [25885] accumulated_eval_time=974.342553, accumulated_logging_time=0.768621, accumulated_submission_time=11799.423765, global_step=25885, preemption_count=0, score=11799.423765, test/accuracy=0.464000, test/loss=2.601236, test/num_examples=10000, total_duration=12775.900420, train/accuracy=0.646465, train/loss=1.718098, validation/accuracy=0.583000, validation/loss=1.990486, validation/num_examples=50000
I0131 16:26:56.813459 139702543816448 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.8035725951194763, loss=4.8674163818359375
I0131 16:27:38.489512 139702527031040 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8775690793991089, loss=4.478112697601318
I0131 16:28:24.434842 139702543816448 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.8838991522789001, loss=3.933804512023926
I0131 16:29:10.395025 139702527031040 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.9940375089645386, loss=3.876894950866699
I0131 16:29:56.626593 139702543816448 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.0010639429092407, loss=3.8083059787750244
I0131 16:30:42.615652 139702527031040 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.0193150043487549, loss=3.991305351257324
I0131 16:31:28.703759 139702543816448 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7522621750831604, loss=5.702442169189453
I0131 16:32:14.754166 139702527031040 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.9514977335929871, loss=3.885812282562256
I0131 16:33:00.403608 139702543816448 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.0099823474884033, loss=4.082502365112305
I0131 16:33:46.245153 139702527031040 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.9589921236038208, loss=3.791670083999634
I0131 16:33:50.546518 139863983413056 spec.py:321] Evaluating on the training split.
I0131 16:34:00.959407 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 16:34:23.210649 139863983413056 spec.py:349] Evaluating on the test split.
I0131 16:34:24.849484 139863983413056 submission_runner.py:408] Time since start: 13230.34s, 	Step: 26811, 	{'train/accuracy': 0.6296288967132568, 'train/loss': 1.7748783826828003, 'validation/accuracy': 0.5895999670028687, 'validation/loss': 1.974582314491272, 'validation/num_examples': 50000, 'test/accuracy': 0.4707000255584717, 'test/loss': 2.5924384593963623, 'test/num_examples': 10000, 'score': 12219.484512329102, 'total_duration': 13230.34390258789, 'accumulated_submission_time': 12219.484512329102, 'accumulated_eval_time': 1008.6455118656158, 'accumulated_logging_time': 0.7995619773864746}
I0131 16:34:24.869718 139702543816448 logging_writer.py:48] [26811] accumulated_eval_time=1008.645512, accumulated_logging_time=0.799562, accumulated_submission_time=12219.484512, global_step=26811, preemption_count=0, score=12219.484512, test/accuracy=0.470700, test/loss=2.592438, test/num_examples=10000, total_duration=13230.343903, train/accuracy=0.629629, train/loss=1.774878, validation/accuracy=0.589600, validation/loss=1.974582, validation/num_examples=50000
I0131 16:35:01.183220 139702527031040 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.853181779384613, loss=4.1414408683776855
I0131 16:35:46.943279 139702543816448 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.1355751752853394, loss=3.73343563079834
I0131 16:36:32.634232 139702527031040 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.9464291334152222, loss=3.833827018737793
I0131 16:37:18.738115 139702543816448 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.9404428005218506, loss=3.8939080238342285
I0131 16:38:04.355925 139702527031040 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.0053343772888184, loss=3.8051538467407227
I0131 16:38:49.893568 139702543816448 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7173629403114319, loss=5.423447132110596
I0131 16:39:35.771943 139702527031040 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8186835050582886, loss=4.469817161560059
I0131 16:40:21.874258 139702543816448 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.0083638429641724, loss=3.886592388153076
I0131 16:41:07.796389 139702527031040 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8837994933128357, loss=4.350533962249756
I0131 16:41:25.010025 139863983413056 spec.py:321] Evaluating on the training split.
I0131 16:41:35.707239 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 16:41:57.867782 139863983413056 spec.py:349] Evaluating on the test split.
I0131 16:41:59.504882 139863983413056 submission_runner.py:408] Time since start: 13685.00s, 	Step: 27739, 	{'train/accuracy': 0.6431835889816284, 'train/loss': 1.694810390472412, 'validation/accuracy': 0.6007599830627441, 'validation/loss': 1.8964165449142456, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.5261025428771973, 'test/num_examples': 10000, 'score': 12639.566604852676, 'total_duration': 13684.9992685318, 'accumulated_submission_time': 12639.566604852676, 'accumulated_eval_time': 1043.1403737068176, 'accumulated_logging_time': 0.8287265300750732}
I0131 16:41:59.531690 139702543816448 logging_writer.py:48] [27739] accumulated_eval_time=1043.140374, accumulated_logging_time=0.828727, accumulated_submission_time=12639.566605, global_step=27739, preemption_count=0, score=12639.566605, test/accuracy=0.479300, test/loss=2.526103, test/num_examples=10000, total_duration=13684.999269, train/accuracy=0.643184, train/loss=1.694810, validation/accuracy=0.600760, validation/loss=1.896417, validation/num_examples=50000
I0131 16:42:24.289614 139702527031040 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.7608667612075806, loss=4.604768753051758
I0131 16:43:08.812001 139702543816448 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.9589759111404419, loss=3.872948408126831
I0131 16:43:54.581775 139702527031040 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.9117640852928162, loss=4.067753314971924
I0131 16:44:40.494458 139702543816448 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.9264918565750122, loss=3.9423458576202393
I0131 16:45:25.973976 139702527031040 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.9536349773406982, loss=3.775418281555176
I0131 16:46:11.709831 139702543816448 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.0625355243682861, loss=3.9302804470062256
I0131 16:46:57.531362 139702527031040 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.9155414700508118, loss=4.243681907653809
I0131 16:47:43.351864 139702543816448 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7473896741867065, loss=5.576623439788818
I0131 16:48:29.137201 139702527031040 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.7886573672294617, loss=5.604011535644531
I0131 16:48:59.874635 139863983413056 spec.py:321] Evaluating on the training split.
I0131 16:49:10.201923 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 16:49:32.236135 139863983413056 spec.py:349] Evaluating on the test split.
I0131 16:49:33.893267 139863983413056 submission_runner.py:408] Time since start: 14139.39s, 	Step: 28669, 	{'train/accuracy': 0.6536718606948853, 'train/loss': 1.666849136352539, 'validation/accuracy': 0.5978400111198425, 'validation/loss': 1.9145691394805908, 'validation/num_examples': 50000, 'test/accuracy': 0.4789000153541565, 'test/loss': 2.543222665786743, 'test/num_examples': 10000, 'score': 13059.846086263657, 'total_duration': 14139.38765001297, 'accumulated_submission_time': 13059.846086263657, 'accumulated_eval_time': 1077.1589756011963, 'accumulated_logging_time': 0.8670501708984375}
I0131 16:49:33.918560 139702543816448 logging_writer.py:48] [28669] accumulated_eval_time=1077.158976, accumulated_logging_time=0.867050, accumulated_submission_time=13059.846086, global_step=28669, preemption_count=0, score=13059.846086, test/accuracy=0.478900, test/loss=2.543223, test/num_examples=10000, total_duration=14139.387650, train/accuracy=0.653672, train/loss=1.666849, validation/accuracy=0.597840, validation/loss=1.914569, validation/num_examples=50000
I0131 16:49:46.696594 139702527031040 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.9326443672180176, loss=4.026315212249756
I0131 16:50:29.826915 139702543816448 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.9541852474212646, loss=3.829796075820923
I0131 16:51:15.524051 139702527031040 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.9731746912002563, loss=3.740001916885376
I0131 16:52:01.317227 139702543816448 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8780070543289185, loss=4.112607002258301
I0131 16:52:47.242537 139702527031040 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9314516186714172, loss=4.318854808807373
I0131 16:53:33.044956 139702543816448 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.8167659044265747, loss=4.498102188110352
I0131 16:54:19.077309 139702527031040 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.9118439555168152, loss=3.9131016731262207
I0131 16:55:04.943809 139702543816448 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.9941251277923584, loss=3.801896810531616
I0131 16:55:50.459661 139702527031040 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9716388583183289, loss=3.8665432929992676
I0131 16:56:34.055191 139863983413056 spec.py:321] Evaluating on the training split.
I0131 16:56:44.442589 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 16:57:06.774194 139863983413056 spec.py:349] Evaluating on the test split.
I0131 16:57:08.428727 139863983413056 submission_runner.py:408] Time since start: 14593.92s, 	Step: 29597, 	{'train/accuracy': 0.6449218392372131, 'train/loss': 1.720602035522461, 'validation/accuracy': 0.5954799652099609, 'validation/loss': 1.935060739517212, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.5695018768310547, 'test/num_examples': 10000, 'score': 13479.923082113266, 'total_duration': 14593.923149585724, 'accumulated_submission_time': 13479.923082113266, 'accumulated_eval_time': 1111.5325186252594, 'accumulated_logging_time': 0.903029203414917}
I0131 16:57:08.449082 139702543816448 logging_writer.py:48] [29597] accumulated_eval_time=1111.532519, accumulated_logging_time=0.903029, accumulated_submission_time=13479.923082, global_step=29597, preemption_count=0, score=13479.923082, test/accuracy=0.476100, test/loss=2.569502, test/num_examples=10000, total_duration=14593.923150, train/accuracy=0.644922, train/loss=1.720602, validation/accuracy=0.595480, validation/loss=1.935061, validation/num_examples=50000
I0131 16:57:10.048118 139702527031040 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.0464129447937012, loss=3.7785046100616455
I0131 16:57:50.986929 139702543816448 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.8056531548500061, loss=4.37215518951416
I0131 16:58:36.837789 139702527031040 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.8108770251274109, loss=4.7808918952941895
I0131 16:59:22.746017 139702543816448 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.9362741708755493, loss=3.8056063652038574
I0131 17:00:08.842418 139702527031040 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9410354495048523, loss=3.819725275039673
I0131 17:00:54.747710 139702543816448 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.9800073504447937, loss=3.762394905090332
I0131 17:01:40.465693 139702527031040 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.7969245314598083, loss=5.65299129486084
I0131 17:02:26.307805 139702543816448 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.9456164240837097, loss=3.875272274017334
I0131 17:03:12.290724 139702527031040 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.7946638464927673, loss=4.770418167114258
I0131 17:03:58.101955 139702543816448 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.9994092583656311, loss=3.9996769428253174
I0131 17:04:08.834913 139863983413056 spec.py:321] Evaluating on the training split.
I0131 17:04:19.573600 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 17:04:40.389850 139863983413056 spec.py:349] Evaluating on the test split.
I0131 17:04:42.035110 139863983413056 submission_runner.py:408] Time since start: 15047.53s, 	Step: 30525, 	{'train/accuracy': 0.6502929329872131, 'train/loss': 1.707275390625, 'validation/accuracy': 0.602620005607605, 'validation/loss': 1.911301851272583, 'validation/num_examples': 50000, 'test/accuracy': 0.48420003056526184, 'test/loss': 2.5386219024658203, 'test/num_examples': 10000, 'score': 13900.250252962112, 'total_duration': 15047.529522657394, 'accumulated_submission_time': 13900.250252962112, 'accumulated_eval_time': 1144.7327094078064, 'accumulated_logging_time': 0.9328234195709229}
I0131 17:04:42.054584 139702527031040 logging_writer.py:48] [30525] accumulated_eval_time=1144.732709, accumulated_logging_time=0.932823, accumulated_submission_time=13900.250253, global_step=30525, preemption_count=0, score=13900.250253, test/accuracy=0.484200, test/loss=2.538622, test/num_examples=10000, total_duration=15047.529523, train/accuracy=0.650293, train/loss=1.707275, validation/accuracy=0.602620, validation/loss=1.911302, validation/num_examples=50000
I0131 17:05:12.408665 139702543816448 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.8311779499053955, loss=5.515349388122559
I0131 17:05:57.944309 139702527031040 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.9162575006484985, loss=3.7826671600341797
I0131 17:06:43.920222 139702543816448 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.9677174091339111, loss=3.9558355808258057
I0131 17:07:30.038170 139702527031040 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.9455686807632446, loss=4.799856662750244
I0131 17:08:15.802589 139702543816448 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8235126733779907, loss=4.606522083282471
I0131 17:09:01.650390 139702527031040 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.00003981590271, loss=4.044039249420166
I0131 17:09:47.722122 139702543816448 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9895960092544556, loss=3.76969313621521
I0131 17:10:33.817380 139702527031040 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.98299640417099, loss=3.832427978515625
I0131 17:11:19.614893 139702543816448 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.9658350944519043, loss=3.6860132217407227
I0131 17:11:42.179914 139863983413056 spec.py:321] Evaluating on the training split.
I0131 17:11:52.627457 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 17:12:15.329033 139863983413056 spec.py:349] Evaluating on the test split.
I0131 17:12:16.974306 139863983413056 submission_runner.py:408] Time since start: 15502.47s, 	Step: 31451, 	{'train/accuracy': 0.6668164134025574, 'train/loss': 1.6053072214126587, 'validation/accuracy': 0.6132000088691711, 'validation/loss': 1.8501198291778564, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.4632036685943604, 'test/num_examples': 10000, 'score': 14320.317656040192, 'total_duration': 15502.468721866608, 'accumulated_submission_time': 14320.317656040192, 'accumulated_eval_time': 1179.5270998477936, 'accumulated_logging_time': 0.9612855911254883}
I0131 17:12:16.996325 139702527031040 logging_writer.py:48] [31451] accumulated_eval_time=1179.527100, accumulated_logging_time=0.961286, accumulated_submission_time=14320.317656, global_step=31451, preemption_count=0, score=14320.317656, test/accuracy=0.494800, test/loss=2.463204, test/num_examples=10000, total_duration=15502.468722, train/accuracy=0.666816, train/loss=1.605307, validation/accuracy=0.613200, validation/loss=1.850120, validation/num_examples=50000
I0131 17:12:36.956965 139702543816448 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.0325974225997925, loss=3.7431294918060303
I0131 17:13:20.685823 139702527031040 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.9923551082611084, loss=3.7704520225524902
I0131 17:14:06.772482 139702543816448 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.9517740607261658, loss=3.961275577545166
I0131 17:14:52.803047 139702527031040 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.888277530670166, loss=4.2755231857299805
I0131 17:15:38.520361 139702543816448 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.8202613592147827, loss=5.514174938201904
I0131 17:16:24.352821 139702527031040 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.9284021854400635, loss=3.788726568222046
I0131 17:17:10.387898 139702543816448 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.8760896921157837, loss=3.680051803588867
I0131 17:17:55.722820 139702527031040 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.964290976524353, loss=3.6761631965637207
I0131 17:18:41.532490 139702543816448 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.8188046813011169, loss=4.569549560546875
I0131 17:19:16.993109 139863983413056 spec.py:321] Evaluating on the training split.
I0131 17:19:27.355232 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 17:19:50.133700 139863983413056 spec.py:349] Evaluating on the test split.
I0131 17:19:51.770489 139863983413056 submission_runner.py:408] Time since start: 15957.26s, 	Step: 32379, 	{'train/accuracy': 0.6805077791213989, 'train/loss': 1.5708024501800537, 'validation/accuracy': 0.6123799681663513, 'validation/loss': 1.8718345165252686, 'validation/num_examples': 50000, 'test/accuracy': 0.4894000291824341, 'test/loss': 2.4860403537750244, 'test/num_examples': 10000, 'score': 14740.255218982697, 'total_duration': 15957.264911651611, 'accumulated_submission_time': 14740.255218982697, 'accumulated_eval_time': 1214.3044934272766, 'accumulated_logging_time': 0.9927427768707275}
I0131 17:19:51.790013 139702527031040 logging_writer.py:48] [32379] accumulated_eval_time=1214.304493, accumulated_logging_time=0.992743, accumulated_submission_time=14740.255219, global_step=32379, preemption_count=0, score=14740.255219, test/accuracy=0.489400, test/loss=2.486040, test/num_examples=10000, total_duration=15957.264912, train/accuracy=0.680508, train/loss=1.570802, validation/accuracy=0.612380, validation/loss=1.871835, validation/num_examples=50000
I0131 17:20:00.575524 139702543816448 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.010607361793518, loss=3.748774290084839
I0131 17:20:42.373340 139702527031040 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7668713331222534, loss=5.095515251159668
I0131 17:21:28.383730 139702543816448 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.935921847820282, loss=5.546141624450684
I0131 17:22:14.030665 139702527031040 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9233207702636719, loss=3.9134232997894287
I0131 17:22:59.918528 139702543816448 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0076334476470947, loss=3.740227222442627
I0131 17:23:45.606340 139702527031040 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.9588465094566345, loss=3.6856181621551514
I0131 17:24:31.422895 139702543816448 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7923859357833862, loss=5.594827175140381
I0131 17:25:17.162544 139702527031040 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.020038366317749, loss=3.711440324783325
I0131 17:26:02.878228 139702543816448 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.9621597528457642, loss=3.693965196609497
I0131 17:26:48.851662 139702527031040 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.0189530849456787, loss=3.7026472091674805
I0131 17:26:52.210109 139863983413056 spec.py:321] Evaluating on the training split.
I0131 17:27:02.804815 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 17:27:24.224867 139863983413056 spec.py:349] Evaluating on the test split.
I0131 17:27:25.856300 139863983413056 submission_runner.py:408] Time since start: 16411.35s, 	Step: 33309, 	{'train/accuracy': 0.6620507836341858, 'train/loss': 1.6771049499511719, 'validation/accuracy': 0.6145600080490112, 'validation/loss': 1.8774863481521606, 'validation/num_examples': 50000, 'test/accuracy': 0.4953000247478485, 'test/loss': 2.4875123500823975, 'test/num_examples': 10000, 'score': 15160.616579771042, 'total_duration': 16411.350699186325, 'accumulated_submission_time': 15160.616579771042, 'accumulated_eval_time': 1247.9506666660309, 'accumulated_logging_time': 1.0217430591583252}
I0131 17:27:25.882436 139702543816448 logging_writer.py:48] [33309] accumulated_eval_time=1247.950667, accumulated_logging_time=1.021743, accumulated_submission_time=15160.616580, global_step=33309, preemption_count=0, score=15160.616580, test/accuracy=0.495300, test/loss=2.487512, test/num_examples=10000, total_duration=16411.350699, train/accuracy=0.662051, train/loss=1.677105, validation/accuracy=0.614560, validation/loss=1.877486, validation/num_examples=50000
I0131 17:28:02.795402 139702527031040 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.906141996383667, loss=5.228760719299316
I0131 17:28:48.327494 139702543816448 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.9587588310241699, loss=3.7961950302124023
I0131 17:29:34.460058 139702527031040 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.973496675491333, loss=3.7910430431365967
I0131 17:30:20.650550 139702543816448 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.801069438457489, loss=4.664267539978027
I0131 17:31:06.708948 139702527031040 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.9749359488487244, loss=3.691931962966919
I0131 17:31:52.304830 139702543816448 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.8892142176628113, loss=5.611128330230713
I0131 17:32:37.976489 139702527031040 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.8618441224098206, loss=4.392910003662109
I0131 17:33:23.542209 139702543816448 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.0800572633743286, loss=3.7548606395721436
I0131 17:34:09.463090 139702527031040 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.065165400505066, loss=3.7165915966033936
I0131 17:34:26.078730 139863983413056 spec.py:321] Evaluating on the training split.
I0131 17:34:36.709531 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 17:34:58.089993 139863983413056 spec.py:349] Evaluating on the test split.
I0131 17:34:59.725630 139863983413056 submission_runner.py:408] Time since start: 16865.22s, 	Step: 34238, 	{'train/accuracy': 0.6710546612739563, 'train/loss': 1.5784615278244019, 'validation/accuracy': 0.6174600124359131, 'validation/loss': 1.8176082372665405, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.435833215713501, 'test/num_examples': 10000, 'score': 15580.754612445831, 'total_duration': 16865.220044851303, 'accumulated_submission_time': 15580.754612445831, 'accumulated_eval_time': 1281.597553730011, 'accumulated_logging_time': 1.0574004650115967}
I0131 17:34:59.746004 139702543816448 logging_writer.py:48] [34238] accumulated_eval_time=1281.597554, accumulated_logging_time=1.057400, accumulated_submission_time=15580.754612, global_step=34238, preemption_count=0, score=15580.754612, test/accuracy=0.496800, test/loss=2.435833, test/num_examples=10000, total_duration=16865.220045, train/accuracy=0.671055, train/loss=1.578462, validation/accuracy=0.617460, validation/loss=1.817608, validation/num_examples=50000
I0131 17:35:24.903116 139702527031040 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.954903244972229, loss=3.898426055908203
I0131 17:36:09.450765 139702543816448 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.9498165249824524, loss=3.7048141956329346
I0131 17:36:55.161556 139702527031040 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.9934098124504089, loss=3.6789422035217285
I0131 17:37:41.012857 139702543816448 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.9920339584350586, loss=3.6716115474700928
I0131 17:38:26.707336 139702527031040 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.9869701266288757, loss=3.667027711868286
I0131 17:39:12.797914 139702543816448 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.810473620891571, loss=4.552070140838623
I0131 17:39:58.537677 139702527031040 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.001990795135498, loss=3.7979576587677
I0131 17:40:44.419806 139702543816448 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.0039160251617432, loss=3.775627374649048
I0131 17:41:30.267940 139702527031040 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.0788317918777466, loss=3.82906436920166
I0131 17:41:59.790363 139863983413056 spec.py:321] Evaluating on the training split.
I0131 17:42:10.364736 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 17:42:31.975980 139863983413056 spec.py:349] Evaluating on the test split.
I0131 17:42:33.616777 139863983413056 submission_runner.py:408] Time since start: 17319.11s, 	Step: 35166, 	{'train/accuracy': 0.6815429329872131, 'train/loss': 1.5850794315338135, 'validation/accuracy': 0.621239960193634, 'validation/loss': 1.8454642295837402, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.4827773571014404, 'test/num_examples': 10000, 'score': 16000.741730213165, 'total_duration': 17319.111181020737, 'accumulated_submission_time': 16000.741730213165, 'accumulated_eval_time': 1315.423936367035, 'accumulated_logging_time': 1.0867114067077637}
I0131 17:42:33.642695 139702543816448 logging_writer.py:48] [35166] accumulated_eval_time=1315.423936, accumulated_logging_time=1.086711, accumulated_submission_time=16000.741730, global_step=35166, preemption_count=0, score=16000.741730, test/accuracy=0.494100, test/loss=2.482777, test/num_examples=10000, total_duration=17319.111181, train/accuracy=0.681543, train/loss=1.585079, validation/accuracy=0.621240, validation/loss=1.845464, validation/num_examples=50000
I0131 17:42:47.626133 139702527031040 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.0809532403945923, loss=3.7541675567626953
I0131 17:43:30.324865 139702543816448 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.9330058693885803, loss=3.639026641845703
I0131 17:44:16.142032 139702527031040 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.0139672756195068, loss=4.432933807373047
I0131 17:45:02.457754 139702543816448 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8266466856002808, loss=5.181427478790283
I0131 17:45:48.554911 139702527031040 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.9116959571838379, loss=4.11572790145874
I0131 17:46:34.133748 139702543816448 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.9403917193412781, loss=4.663226127624512
I0131 17:47:20.148904 139702527031040 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1506075859069824, loss=3.700040340423584
I0131 17:48:05.950017 139702543816448 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.9499887228012085, loss=3.8510427474975586
I0131 17:48:51.661534 139702527031040 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.9909411668777466, loss=3.868621349334717
I0131 17:49:33.990073 139863983413056 spec.py:321] Evaluating on the training split.
I0131 17:49:45.782023 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 17:50:06.981476 139863983413056 spec.py:349] Evaluating on the test split.
I0131 17:50:08.631668 139863983413056 submission_runner.py:408] Time since start: 17774.13s, 	Step: 36093, 	{'train/accuracy': 0.6749609112739563, 'train/loss': 1.5821787118911743, 'validation/accuracy': 0.6269599795341492, 'validation/loss': 1.7968742847442627, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.4259908199310303, 'test/num_examples': 10000, 'score': 16421.03150343895, 'total_duration': 17774.126088142395, 'accumulated_submission_time': 16421.03150343895, 'accumulated_eval_time': 1350.0655298233032, 'accumulated_logging_time': 1.1218464374542236}
I0131 17:50:08.655211 139702543816448 logging_writer.py:48] [36093] accumulated_eval_time=1350.065530, accumulated_logging_time=1.121846, accumulated_submission_time=16421.031503, global_step=36093, preemption_count=0, score=16421.031503, test/accuracy=0.501000, test/loss=2.425991, test/num_examples=10000, total_duration=17774.126088, train/accuracy=0.674961, train/loss=1.582179, validation/accuracy=0.626960, validation/loss=1.796874, validation/num_examples=50000
I0131 17:50:11.852009 139702527031040 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.040450096130371, loss=3.688324451446533
I0131 17:50:52.987350 139702543816448 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.9046352505683899, loss=4.553443908691406
I0131 17:51:38.846883 139702527031040 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.0522515773773193, loss=3.621605634689331
I0131 17:52:24.804899 139702543816448 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.1204756498336792, loss=3.7099905014038086
I0131 17:53:10.633109 139702527031040 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8710237145423889, loss=4.191329002380371
I0131 17:53:56.435439 139702543816448 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.8412132263183594, loss=4.757948398590088
I0131 17:54:42.351361 139702527031040 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.0582051277160645, loss=3.989147901535034
I0131 17:55:28.034642 139702543816448 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.0394659042358398, loss=3.7970917224884033
I0131 17:56:13.794798 139702527031040 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.8516885042190552, loss=5.332053184509277
I0131 17:56:59.584758 139702543816448 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0153329372406006, loss=3.8099637031555176
I0131 17:57:08.984436 139863983413056 spec.py:321] Evaluating on the training split.
I0131 17:57:19.276268 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 17:57:42.548744 139863983413056 spec.py:349] Evaluating on the test split.
I0131 17:57:44.186089 139863983413056 submission_runner.py:408] Time since start: 18229.68s, 	Step: 37022, 	{'train/accuracy': 0.6784374713897705, 'train/loss': 1.5355225801467896, 'validation/accuracy': 0.6299600005149841, 'validation/loss': 1.7512410879135132, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.398932933807373, 'test/num_examples': 10000, 'score': 16841.30184864998, 'total_duration': 18229.68051123619, 'accumulated_submission_time': 16841.30184864998, 'accumulated_eval_time': 1385.2672073841095, 'accumulated_logging_time': 1.1547789573669434}
I0131 17:57:44.209653 139702527031040 logging_writer.py:48] [37022] accumulated_eval_time=1385.267207, accumulated_logging_time=1.154779, accumulated_submission_time=16841.301849, global_step=37022, preemption_count=0, score=16841.301849, test/accuracy=0.500400, test/loss=2.398933, test/num_examples=10000, total_duration=18229.680511, train/accuracy=0.678437, train/loss=1.535523, validation/accuracy=0.629960, validation/loss=1.751241, validation/num_examples=50000
I0131 17:58:15.774955 139702543816448 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.9744272828102112, loss=3.6751129627227783
I0131 17:59:01.040900 139702527031040 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.8881933093070984, loss=4.931869029998779
I0131 17:59:47.088750 139702543816448 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.9332894682884216, loss=4.269449234008789
I0131 18:00:33.477200 139702527031040 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.9227616190910339, loss=3.890552043914795
I0131 18:01:19.201798 139702543816448 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.1502313613891602, loss=3.7204744815826416
I0131 18:02:05.282942 139702527031040 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.8695950508117676, loss=5.442814826965332
I0131 18:02:51.353811 139702543816448 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.9060835242271423, loss=3.986873149871826
I0131 18:03:37.120398 139702527031040 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.9612433910369873, loss=3.89184308052063
I0131 18:04:22.911130 139702543816448 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.8324446082115173, loss=5.411445617675781
I0131 18:04:44.484458 139863983413056 spec.py:321] Evaluating on the training split.
I0131 18:04:54.748872 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 18:05:19.033318 139863983413056 spec.py:349] Evaluating on the test split.
I0131 18:05:20.679477 139863983413056 submission_runner.py:408] Time since start: 18686.17s, 	Step: 37949, 	{'train/accuracy': 0.6911327838897705, 'train/loss': 1.483366847038269, 'validation/accuracy': 0.6319000124931335, 'validation/loss': 1.7460869550704956, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.372727155685425, 'test/num_examples': 10000, 'score': 17261.51871085167, 'total_duration': 18686.173897981644, 'accumulated_submission_time': 17261.51871085167, 'accumulated_eval_time': 1421.4622313976288, 'accumulated_logging_time': 1.187713623046875}
I0131 18:05:20.700414 139702527031040 logging_writer.py:48] [37949] accumulated_eval_time=1421.462231, accumulated_logging_time=1.187714, accumulated_submission_time=17261.518711, global_step=37949, preemption_count=0, score=17261.518711, test/accuracy=0.512100, test/loss=2.372727, test/num_examples=10000, total_duration=18686.173898, train/accuracy=0.691133, train/loss=1.483367, validation/accuracy=0.631900, validation/loss=1.746087, validation/num_examples=50000
I0131 18:05:41.467352 139702543816448 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0588165521621704, loss=3.7438864707946777
I0131 18:06:25.289500 139702527031040 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.0052075386047363, loss=3.7422969341278076
I0131 18:07:11.094943 139702543816448 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.0729542970657349, loss=3.720017194747925
I0131 18:07:57.066107 139702527031040 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.954573929309845, loss=4.327614784240723
I0131 18:08:42.690474 139702543816448 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.9715394377708435, loss=3.6898510456085205
I0131 18:09:28.498180 139702527031040 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.0002766847610474, loss=3.7301220893859863
I0131 18:10:14.790230 139702543816448 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.9062208533287048, loss=4.457357406616211
I0131 18:11:00.554601 139702527031040 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.970495343208313, loss=3.7308995723724365
I0131 18:11:46.554937 139702543816448 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.9869735240936279, loss=3.6736812591552734
I0131 18:12:21.046342 139863983413056 spec.py:321] Evaluating on the training split.
I0131 18:12:31.649727 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 18:12:53.567406 139863983413056 spec.py:349] Evaluating on the test split.
I0131 18:12:55.207880 139863983413056 submission_runner.py:408] Time since start: 19140.70s, 	Step: 38877, 	{'train/accuracy': 0.6896093487739563, 'train/loss': 1.5001717805862427, 'validation/accuracy': 0.6326599717140198, 'validation/loss': 1.7544867992401123, 'validation/num_examples': 50000, 'test/accuracy': 0.5072000026702881, 'test/loss': 2.3739521503448486, 'test/num_examples': 10000, 'score': 17681.805111408234, 'total_duration': 19140.70230269432, 'accumulated_submission_time': 17681.805111408234, 'accumulated_eval_time': 1455.6237666606903, 'accumulated_logging_time': 1.2181427478790283}
I0131 18:12:55.228266 139702527031040 logging_writer.py:48] [38877] accumulated_eval_time=1455.623767, accumulated_logging_time=1.218143, accumulated_submission_time=17681.805111, global_step=38877, preemption_count=0, score=17681.805111, test/accuracy=0.507200, test/loss=2.373952, test/num_examples=10000, total_duration=19140.702303, train/accuracy=0.689609, train/loss=1.500172, validation/accuracy=0.632660, validation/loss=1.754487, validation/num_examples=50000
I0131 18:13:04.808335 139702543816448 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.0414559841156006, loss=3.646099090576172
I0131 18:13:46.590427 139702527031040 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.056347370147705, loss=3.732889175415039
I0131 18:14:32.410443 139702543816448 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.0553325414657593, loss=3.689026117324829
I0131 18:15:18.294859 139702527031040 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.9582617878913879, loss=3.6220552921295166
I0131 18:16:03.962159 139702543816448 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.9075745940208435, loss=3.9546823501586914
I0131 18:16:49.547735 139702527031040 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.887197732925415, loss=4.9231061935424805
I0131 18:17:35.395575 139702543816448 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.898711085319519, loss=3.8578155040740967
I0131 18:18:21.238967 139702527031040 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.0019872188568115, loss=4.252938747406006
I0131 18:19:06.827300 139702543816448 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.015608787536621, loss=3.6270816326141357
I0131 18:19:52.818272 139702527031040 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.9730274677276611, loss=3.672421932220459
I0131 18:19:55.254248 139863983413056 spec.py:321] Evaluating on the training split.
I0131 18:20:05.742480 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 18:20:27.066634 139863983413056 spec.py:349] Evaluating on the test split.
I0131 18:20:28.703964 139863983413056 submission_runner.py:408] Time since start: 19594.20s, 	Step: 39807, 	{'train/accuracy': 0.6860156059265137, 'train/loss': 1.5207045078277588, 'validation/accuracy': 0.6359599828720093, 'validation/loss': 1.7454924583435059, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.383885622024536, 'test/num_examples': 10000, 'score': 18101.77188515663, 'total_duration': 19594.19838809967, 'accumulated_submission_time': 18101.77188515663, 'accumulated_eval_time': 1489.073492050171, 'accumulated_logging_time': 1.2485857009887695}
I0131 18:20:28.724204 139702543816448 logging_writer.py:48] [39807] accumulated_eval_time=1489.073492, accumulated_logging_time=1.248586, accumulated_submission_time=18101.771885, global_step=39807, preemption_count=0, score=18101.771885, test/accuracy=0.512200, test/loss=2.383886, test/num_examples=10000, total_duration=19594.198388, train/accuracy=0.686016, train/loss=1.520705, validation/accuracy=0.635960, validation/loss=1.745492, validation/num_examples=50000
I0131 18:21:06.714610 139702527031040 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.9750375747680664, loss=3.944406509399414
I0131 18:21:52.438921 139702543816448 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9829463362693787, loss=3.796712875366211
I0131 18:22:38.769381 139702527031040 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.9869236350059509, loss=3.9253809452056885
I0131 18:23:25.011603 139702543816448 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.057745337486267, loss=3.6700072288513184
I0131 18:24:10.790720 139702527031040 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.956208348274231, loss=4.16164493560791
I0131 18:24:56.532672 139702543816448 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.04236900806427, loss=3.6776649951934814
I0131 18:25:42.290519 139702527031040 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.9587469100952148, loss=4.079042434692383
I0131 18:26:28.066371 139702543816448 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.8593969941139221, loss=5.1002516746521
I0131 18:27:13.808991 139702527031040 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.8578478097915649, loss=5.472543716430664
I0131 18:27:29.094762 139863983413056 spec.py:321] Evaluating on the training split.
I0131 18:27:39.504993 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 18:27:59.281239 139863983413056 spec.py:349] Evaluating on the test split.
I0131 18:28:00.924649 139863983413056 submission_runner.py:408] Time since start: 20046.42s, 	Step: 40735, 	{'train/accuracy': 0.6874804496765137, 'train/loss': 1.4901330471038818, 'validation/accuracy': 0.6361799836158752, 'validation/loss': 1.7246910333633423, 'validation/num_examples': 50000, 'test/accuracy': 0.5095000267028809, 'test/loss': 2.366427183151245, 'test/num_examples': 10000, 'score': 18522.085191726685, 'total_duration': 20046.41907286644, 'accumulated_submission_time': 18522.085191726685, 'accumulated_eval_time': 1520.9033830165863, 'accumulated_logging_time': 1.2772552967071533}
I0131 18:28:00.949242 139702543816448 logging_writer.py:48] [40735] accumulated_eval_time=1520.903383, accumulated_logging_time=1.277255, accumulated_submission_time=18522.085192, global_step=40735, preemption_count=0, score=18522.085192, test/accuracy=0.509500, test/loss=2.366427, test/num_examples=10000, total_duration=20046.419073, train/accuracy=0.687480, train/loss=1.490133, validation/accuracy=0.636180, validation/loss=1.724691, validation/num_examples=50000
I0131 18:28:27.305091 139702527031040 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.9846953749656677, loss=3.691558361053467
I0131 18:29:11.958768 139702543816448 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.9491009712219238, loss=5.215634822845459
I0131 18:29:58.119577 139702527031040 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.9889895915985107, loss=3.86812424659729
I0131 18:30:44.506202 139702543816448 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.0004774332046509, loss=3.717482089996338
I0131 18:31:30.394898 139702527031040 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.9610137939453125, loss=3.746987819671631
I0131 18:32:16.654540 139702543816448 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.908266544342041, loss=4.730422496795654
I0131 18:33:02.628859 139702527031040 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.0634442567825317, loss=3.6097917556762695
I0131 18:33:48.232117 139702543816448 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8431336283683777, loss=5.308586120605469
I0131 18:34:34.198122 139702527031040 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.8755284547805786, loss=5.456546306610107
I0131 18:35:01.357353 139863983413056 spec.py:321] Evaluating on the training split.
I0131 18:35:11.776934 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 18:35:32.624194 139863983413056 spec.py:349] Evaluating on the test split.
I0131 18:35:34.262317 139863983413056 submission_runner.py:408] Time since start: 20499.76s, 	Step: 41661, 	{'train/accuracy': 0.7146679759025574, 'train/loss': 1.4313410520553589, 'validation/accuracy': 0.6459000110626221, 'validation/loss': 1.7241450548171997, 'validation/num_examples': 50000, 'test/accuracy': 0.517300009727478, 'test/loss': 2.3592562675476074, 'test/num_examples': 10000, 'score': 18942.435485124588, 'total_duration': 20499.756719589233, 'accumulated_submission_time': 18942.435485124588, 'accumulated_eval_time': 1553.8083319664001, 'accumulated_logging_time': 1.3109490871429443}
I0131 18:35:34.286053 139702543816448 logging_writer.py:48] [41661] accumulated_eval_time=1553.808332, accumulated_logging_time=1.310949, accumulated_submission_time=18942.435485, global_step=41661, preemption_count=0, score=18942.435485, test/accuracy=0.517300, test/loss=2.359256, test/num_examples=10000, total_duration=20499.756720, train/accuracy=0.714668, train/loss=1.431341, validation/accuracy=0.645900, validation/loss=1.724145, validation/num_examples=50000
I0131 18:35:50.281207 139702527031040 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.9234253764152527, loss=5.395971775054932
I0131 18:36:33.290673 139702543816448 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.8711836934089661, loss=4.544058799743652
I0131 18:37:19.213727 139702527031040 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.8675782084465027, loss=5.366605758666992
I0131 18:38:05.279611 139702543816448 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.9382671117782593, loss=4.347317695617676
I0131 18:38:50.919714 139702527031040 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.9977381825447083, loss=3.713118314743042
I0131 18:39:36.718989 139702543816448 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.041537880897522, loss=3.6039421558380127
I0131 18:40:22.673963 139702527031040 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1188347339630127, loss=3.59450101852417
I0131 18:41:08.418516 139702543816448 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.9725706577301025, loss=3.7309634685516357
I0131 18:41:54.266437 139702527031040 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.9986400008201599, loss=3.669459342956543
I0131 18:42:34.586287 139863983413056 spec.py:321] Evaluating on the training split.
I0131 18:42:44.799874 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 18:43:08.067726 139863983413056 spec.py:349] Evaluating on the test split.
I0131 18:43:09.711481 139863983413056 submission_runner.py:408] Time since start: 20955.21s, 	Step: 42589, 	{'train/accuracy': 0.6867382526397705, 'train/loss': 1.5198066234588623, 'validation/accuracy': 0.6368399858474731, 'validation/loss': 1.7426916360855103, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.363996982574463, 'test/num_examples': 10000, 'score': 19362.675671339035, 'total_duration': 20955.205878019333, 'accumulated_submission_time': 19362.675671339035, 'accumulated_eval_time': 1588.933512687683, 'accumulated_logging_time': 1.3452599048614502}
I0131 18:43:09.740054 139702543816448 logging_writer.py:48] [42589] accumulated_eval_time=1588.933513, accumulated_logging_time=1.345260, accumulated_submission_time=19362.675671, global_step=42589, preemption_count=0, score=19362.675671, test/accuracy=0.513400, test/loss=2.363997, test/num_examples=10000, total_duration=20955.205878, train/accuracy=0.686738, train/loss=1.519807, validation/accuracy=0.636840, validation/loss=1.742692, validation/num_examples=50000
I0131 18:43:14.526975 139702527031040 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.8842365145683289, loss=4.029301166534424
I0131 18:43:55.622603 139702543816448 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.885732114315033, loss=4.386161804199219
I0131 18:44:41.302169 139702527031040 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.9797441363334656, loss=3.735379219055176
I0131 18:45:27.017286 139702543816448 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.9997124075889587, loss=5.402075290679932
I0131 18:46:12.791678 139702527031040 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.011936068534851, loss=3.5880661010742188
I0131 18:46:58.542854 139702543816448 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.9941301941871643, loss=3.5793583393096924
I0131 18:47:44.299923 139702527031040 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.8787867426872253, loss=4.704180717468262
I0131 18:48:29.940524 139702543816448 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.921748161315918, loss=5.356332778930664
I0131 18:49:15.508677 139702527031040 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.0113893747329712, loss=3.6733274459838867
I0131 18:50:01.533816 139702543816448 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9538140892982483, loss=5.335339546203613
I0131 18:50:09.961698 139863983413056 spec.py:321] Evaluating on the training split.
I0131 18:50:20.435487 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 18:50:42.745975 139863983413056 spec.py:349] Evaluating on the test split.
I0131 18:50:44.384891 139863983413056 submission_runner.py:408] Time since start: 21409.88s, 	Step: 43520, 	{'train/accuracy': 0.7005859017372131, 'train/loss': 1.4200528860092163, 'validation/accuracy': 0.6437000036239624, 'validation/loss': 1.678924322128296, 'validation/num_examples': 50000, 'test/accuracy': 0.5212000012397766, 'test/loss': 2.3035788536071777, 'test/num_examples': 10000, 'score': 19782.83354473114, 'total_duration': 21409.879311323166, 'accumulated_submission_time': 19782.83354473114, 'accumulated_eval_time': 1623.3567078113556, 'accumulated_logging_time': 1.3867592811584473}
I0131 18:50:44.409412 139702527031040 logging_writer.py:48] [43520] accumulated_eval_time=1623.356708, accumulated_logging_time=1.386759, accumulated_submission_time=19782.833545, global_step=43520, preemption_count=0, score=19782.833545, test/accuracy=0.521200, test/loss=2.303579, test/num_examples=10000, total_duration=21409.879311, train/accuracy=0.700586, train/loss=1.420053, validation/accuracy=0.643700, validation/loss=1.678924, validation/num_examples=50000
I0131 18:51:16.744626 139702543816448 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.9580472111701965, loss=3.6235666275024414
I0131 18:52:02.133441 139702527031040 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.1108235120773315, loss=3.6152215003967285
I0131 18:52:48.591736 139702543816448 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.8497071266174316, loss=5.119877338409424
I0131 18:53:34.889382 139702527031040 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.1253870725631714, loss=3.7366743087768555
I0131 18:54:20.986217 139702543816448 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.0453336238861084, loss=3.678300142288208
I0131 18:55:06.547491 139702527031040 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.0115389823913574, loss=3.8507909774780273
I0131 18:55:52.355038 139702543816448 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.9333136081695557, loss=3.9314632415771484
I0131 18:56:38.177772 139702527031040 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.9274099469184875, loss=4.033473968505859
I0131 18:57:24.066931 139702543816448 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.1001615524291992, loss=3.7278075218200684
I0131 18:57:44.722829 139863983413056 spec.py:321] Evaluating on the training split.
I0131 18:57:55.259916 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 18:58:16.308997 139863983413056 spec.py:349] Evaluating on the test split.
I0131 18:58:17.937973 139863983413056 submission_runner.py:408] Time since start: 21863.43s, 	Step: 44447, 	{'train/accuracy': 0.7023046612739563, 'train/loss': 1.501721739768982, 'validation/accuracy': 0.6417799592018127, 'validation/loss': 1.76237154006958, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.387691020965576, 'test/num_examples': 10000, 'score': 20203.088237285614, 'total_duration': 21863.43239045143, 'accumulated_submission_time': 20203.088237285614, 'accumulated_eval_time': 1656.5718541145325, 'accumulated_logging_time': 1.4205570220947266}
I0131 18:58:17.960000 139702527031040 logging_writer.py:48] [44447] accumulated_eval_time=1656.571854, accumulated_logging_time=1.420557, accumulated_submission_time=20203.088237, global_step=44447, preemption_count=0, score=20203.088237, test/accuracy=0.519600, test/loss=2.387691, test/num_examples=10000, total_duration=21863.432390, train/accuracy=0.702305, train/loss=1.501722, validation/accuracy=0.641780, validation/loss=1.762372, validation/num_examples=50000
I0131 18:58:39.527970 139702543816448 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.1549190282821655, loss=3.6625237464904785
I0131 18:59:23.366421 139702527031040 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.9818095564842224, loss=3.661237955093384
I0131 19:00:09.590789 139702543816448 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.9695625305175781, loss=5.344785690307617
I0131 19:00:55.312923 139702527031040 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.0185927152633667, loss=5.346094131469727
I0131 19:01:40.990480 139702543816448 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.8797144293785095, loss=4.7948198318481445
I0131 19:02:26.844075 139702527031040 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.9369407892227173, loss=4.9282050132751465
I0131 19:03:13.095979 139702543816448 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.9552904963493347, loss=3.8494627475738525
I0131 19:03:58.572883 139702527031040 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.0524402856826782, loss=3.546466827392578
I0131 19:04:44.419795 139702543816448 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.0124130249023438, loss=4.800368309020996
I0131 19:05:17.953526 139863983413056 spec.py:321] Evaluating on the training split.
I0131 19:05:28.417456 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 19:05:51.178048 139863983413056 spec.py:349] Evaluating on the test split.
I0131 19:05:52.818088 139863983413056 submission_runner.py:408] Time since start: 22318.31s, 	Step: 45375, 	{'train/accuracy': 0.6984961032867432, 'train/loss': 1.481454610824585, 'validation/accuracy': 0.6467799544334412, 'validation/loss': 1.7090678215026855, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.3472135066986084, 'test/num_examples': 10000, 'score': 20623.023255348206, 'total_duration': 22318.312511205673, 'accumulated_submission_time': 20623.023255348206, 'accumulated_eval_time': 1691.4364099502563, 'accumulated_logging_time': 1.4526786804199219}
I0131 19:05:52.840411 139702527031040 logging_writer.py:48] [45375] accumulated_eval_time=1691.436410, accumulated_logging_time=1.452679, accumulated_submission_time=20623.023255, global_step=45375, preemption_count=0, score=20623.023255, test/accuracy=0.519500, test/loss=2.347214, test/num_examples=10000, total_duration=22318.312511, train/accuracy=0.698496, train/loss=1.481455, validation/accuracy=0.646780, validation/loss=1.709068, validation/num_examples=50000
I0131 19:06:03.217435 139702543816448 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.243255853652954, loss=3.696200370788574
I0131 19:06:45.500673 139702527031040 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8893619179725647, loss=4.8916215896606445
I0131 19:07:31.516893 139702543816448 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9761006236076355, loss=3.992368698120117
I0131 19:08:17.568943 139702527031040 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.0304142236709595, loss=3.607586622238159
I0131 19:09:03.471494 139702543816448 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.013312578201294, loss=3.6044859886169434
I0131 19:09:49.199309 139702527031040 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.9508676528930664, loss=4.148060321807861
I0131 19:10:35.173255 139702543816448 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9856146574020386, loss=3.5364625453948975
I0131 19:11:21.106531 139702527031040 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.106143832206726, loss=3.571876287460327
I0131 19:12:06.947656 139702543816448 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.8506031632423401, loss=4.723707675933838
I0131 19:12:52.977262 139702527031040 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.9943372011184692, loss=3.905677080154419
I0131 19:12:52.992076 139863983413056 spec.py:321] Evaluating on the training split.
I0131 19:13:03.726989 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 19:13:27.884880 139863983413056 spec.py:349] Evaluating on the test split.
I0131 19:13:29.528120 139863983413056 submission_runner.py:408] Time since start: 22775.02s, 	Step: 46301, 	{'train/accuracy': 0.7051757574081421, 'train/loss': 1.4099836349487305, 'validation/accuracy': 0.6536799669265747, 'validation/loss': 1.652795433998108, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.281585216522217, 'test/num_examples': 10000, 'score': 21043.11750602722, 'total_duration': 22775.022524118423, 'accumulated_submission_time': 21043.11750602722, 'accumulated_eval_time': 1727.9724340438843, 'accumulated_logging_time': 1.4842946529388428}
I0131 19:13:29.553146 139702543816448 logging_writer.py:48] [46301] accumulated_eval_time=1727.972434, accumulated_logging_time=1.484295, accumulated_submission_time=21043.117506, global_step=46301, preemption_count=0, score=21043.117506, test/accuracy=0.525800, test/loss=2.281585, test/num_examples=10000, total_duration=22775.022524, train/accuracy=0.705176, train/loss=1.409984, validation/accuracy=0.653680, validation/loss=1.652795, validation/num_examples=50000
I0131 19:14:10.399977 139702527031040 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.9879012703895569, loss=3.9999351501464844
I0131 19:14:56.286494 139702543816448 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.9270246028900146, loss=5.454453468322754
I0131 19:15:42.242993 139702527031040 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.0138022899627686, loss=5.265374660491943
I0131 19:16:28.128434 139702543816448 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.936838686466217, loss=5.154655456542969
I0131 19:17:13.921080 139702527031040 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.0479751825332642, loss=3.5240485668182373
I0131 19:17:59.809463 139702543816448 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.097861409187317, loss=3.6052541732788086
I0131 19:18:45.671307 139702527031040 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.1088651418685913, loss=3.5479187965393066
I0131 19:19:31.364687 139702543816448 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.0294662714004517, loss=3.566300392150879
I0131 19:20:17.567241 139702527031040 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.9941748380661011, loss=3.5918970108032227
I0131 19:20:29.691977 139863983413056 spec.py:321] Evaluating on the training split.
I0131 19:20:40.104184 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 19:21:00.910914 139863983413056 spec.py:349] Evaluating on the test split.
I0131 19:21:02.554365 139863983413056 submission_runner.py:408] Time since start: 23228.05s, 	Step: 47228, 	{'train/accuracy': 0.7090820074081421, 'train/loss': 1.4518290758132935, 'validation/accuracy': 0.6502000093460083, 'validation/loss': 1.708951473236084, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.330073356628418, 'test/num_examples': 10000, 'score': 21463.19794869423, 'total_duration': 23228.048787355423, 'accumulated_submission_time': 21463.19794869423, 'accumulated_eval_time': 1760.8348679542542, 'accumulated_logging_time': 1.5186994075775146}
I0131 19:21:02.575624 139702543816448 logging_writer.py:48] [47228] accumulated_eval_time=1760.834868, accumulated_logging_time=1.518699, accumulated_submission_time=21463.197949, global_step=47228, preemption_count=0, score=21463.197949, test/accuracy=0.527200, test/loss=2.330073, test/num_examples=10000, total_duration=23228.048787, train/accuracy=0.709082, train/loss=1.451829, validation/accuracy=0.650200, validation/loss=1.708951, validation/num_examples=50000
I0131 19:21:31.720878 139702527031040 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.0661429166793823, loss=3.7218010425567627
I0131 19:22:16.930457 139702543816448 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.9937939643859863, loss=3.4507999420166016
I0131 19:23:02.973589 139702527031040 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.0739033222198486, loss=3.5124025344848633
I0131 19:23:49.379761 139702543816448 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.1365958452224731, loss=3.546765089035034
I0131 19:24:34.872832 139702527031040 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.0554893016815186, loss=3.5348219871520996
I0131 19:25:20.664705 139702543816448 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.98984694480896, loss=5.4018168449401855
I0131 19:26:06.321089 139702527031040 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.9000914692878723, loss=4.03244686126709
I0131 19:26:52.053952 139702543816448 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9176784157752991, loss=5.293221950531006
I0131 19:27:37.899582 139702527031040 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.097938060760498, loss=3.48820424079895
I0131 19:28:02.742507 139863983413056 spec.py:321] Evaluating on the training split.
I0131 19:28:13.183864 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 19:28:34.686830 139863983413056 spec.py:349] Evaluating on the test split.
I0131 19:28:36.329275 139863983413056 submission_runner.py:408] Time since start: 23681.82s, 	Step: 48156, 	{'train/accuracy': 0.7205273509025574, 'train/loss': 1.3598943948745728, 'validation/accuracy': 0.6526199579238892, 'validation/loss': 1.6453591585159302, 'validation/num_examples': 50000, 'test/accuracy': 0.5291000008583069, 'test/loss': 2.2683568000793457, 'test/num_examples': 10000, 'score': 21883.307546377182, 'total_duration': 23681.823693037033, 'accumulated_submission_time': 21883.307546377182, 'accumulated_eval_time': 1794.4216213226318, 'accumulated_logging_time': 1.5487060546875}
I0131 19:28:36.353141 139702543816448 logging_writer.py:48] [48156] accumulated_eval_time=1794.421621, accumulated_logging_time=1.548706, accumulated_submission_time=21883.307546, global_step=48156, preemption_count=0, score=21883.307546, test/accuracy=0.529100, test/loss=2.268357, test/num_examples=10000, total_duration=23681.823693, train/accuracy=0.720527, train/loss=1.359894, validation/accuracy=0.652620, validation/loss=1.645359, validation/num_examples=50000
I0131 19:28:54.316525 139702527031040 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.9345085620880127, loss=4.3032989501953125
I0131 19:29:37.669445 139702543816448 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.0337291955947876, loss=3.500952959060669
I0131 19:30:23.605442 139702527031040 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.981685996055603, loss=3.76873517036438
I0131 19:31:09.724992 139702543816448 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0422388315200806, loss=3.723156452178955
I0131 19:31:55.491186 139702527031040 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0431288480758667, loss=3.6779723167419434
I0131 19:32:41.465535 139702543816448 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.0026881694793701, loss=3.560014247894287
I0131 19:33:27.488479 139702527031040 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.9926883578300476, loss=3.9488325119018555
I0131 19:34:13.236430 139702543816448 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.0041372776031494, loss=4.26762056350708
I0131 19:34:58.879424 139702527031040 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9037391543388367, loss=4.764510154724121
I0131 19:35:36.686271 139863983413056 spec.py:321] Evaluating on the training split.
I0131 19:35:47.038819 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 19:36:08.709590 139863983413056 spec.py:349] Evaluating on the test split.
I0131 19:36:10.347370 139863983413056 submission_runner.py:408] Time since start: 24135.84s, 	Step: 49084, 	{'train/accuracy': 0.7106054425239563, 'train/loss': 1.384338617324829, 'validation/accuracy': 0.6567999720573425, 'validation/loss': 1.6216844320297241, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.238567352294922, 'test/num_examples': 10000, 'score': 22303.582375764847, 'total_duration': 24135.841794013977, 'accumulated_submission_time': 22303.582375764847, 'accumulated_eval_time': 1828.0827286243439, 'accumulated_logging_time': 1.5821101665496826}
I0131 19:36:10.373762 139702543816448 logging_writer.py:48] [49084] accumulated_eval_time=1828.082729, accumulated_logging_time=1.582110, accumulated_submission_time=22303.582376, global_step=49084, preemption_count=0, score=22303.582376, test/accuracy=0.531200, test/loss=2.238567, test/num_examples=10000, total_duration=24135.841794, train/accuracy=0.710605, train/loss=1.384339, validation/accuracy=0.656800, validation/loss=1.621684, validation/num_examples=50000
I0131 19:36:17.163913 139702527031040 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.0782002210617065, loss=5.205789089202881
I0131 19:36:58.888843 139702543816448 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.0655674934387207, loss=3.6304945945739746
I0131 19:37:44.796862 139702527031040 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.078230381011963, loss=3.6765518188476562
I0131 19:38:31.201488 139702543816448 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.033737301826477, loss=3.554863691329956
I0131 19:39:17.367001 139702527031040 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0150543451309204, loss=3.536717414855957
I0131 19:40:03.502790 139702543816448 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.031472086906433, loss=3.603611469268799
I0131 19:40:49.309227 139702527031040 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.9547092318534851, loss=5.247381210327148
I0131 19:41:35.126558 139702543816448 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.0236449241638184, loss=3.6899375915527344
I0131 19:42:20.925268 139702527031040 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.0258663892745972, loss=4.895669460296631
I0131 19:43:07.008793 139702543816448 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.0515388250350952, loss=5.363776683807373
I0131 19:43:10.390530 139863983413056 spec.py:321] Evaluating on the training split.
I0131 19:43:20.893351 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 19:43:42.372205 139863983413056 spec.py:349] Evaluating on the test split.
I0131 19:43:44.013265 139863983413056 submission_runner.py:408] Time since start: 24589.51s, 	Step: 50009, 	{'train/accuracy': 0.7145702838897705, 'train/loss': 1.409250259399414, 'validation/accuracy': 0.6565799713134766, 'validation/loss': 1.664278507232666, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.2743983268737793, 'test/num_examples': 10000, 'score': 22723.54018163681, 'total_duration': 24589.507689237595, 'accumulated_submission_time': 22723.54018163681, 'accumulated_eval_time': 1861.7054600715637, 'accumulated_logging_time': 1.618788242340088}
I0131 19:43:44.037592 139702527031040 logging_writer.py:48] [50009] accumulated_eval_time=1861.705460, accumulated_logging_time=1.618788, accumulated_submission_time=22723.540182, global_step=50009, preemption_count=0, score=22723.540182, test/accuracy=0.533300, test/loss=2.274398, test/num_examples=10000, total_duration=24589.507689, train/accuracy=0.714570, train/loss=1.409250, validation/accuracy=0.656580, validation/loss=1.664279, validation/num_examples=50000
I0131 19:44:21.345500 139702543816448 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.0182490348815918, loss=3.492457389831543
I0131 19:45:07.289719 139702527031040 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.9878717660903931, loss=4.4712629318237305
I0131 19:45:53.233257 139702543816448 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.0322552919387817, loss=3.5940961837768555
I0131 19:46:39.406743 139702527031040 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.0563251972198486, loss=5.392687797546387
I0131 19:47:25.307644 139702543816448 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.0395947694778442, loss=3.4916365146636963
I0131 19:48:11.444310 139702527031040 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.10700523853302, loss=3.5597736835479736
I0131 19:48:57.150471 139702543816448 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.0194038152694702, loss=3.7764368057250977
I0131 19:49:43.056127 139702527031040 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.0872340202331543, loss=3.5386414527893066
I0131 19:50:29.370358 139702543816448 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.9632503390312195, loss=5.308509349822998
I0131 19:50:44.228886 139863983413056 spec.py:321] Evaluating on the training split.
I0131 19:50:54.466645 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 19:51:17.569285 139863983413056 spec.py:349] Evaluating on the test split.
I0131 19:51:19.212297 139863983413056 submission_runner.py:408] Time since start: 25044.71s, 	Step: 50934, 	{'train/accuracy': 0.727734386920929, 'train/loss': 1.3187165260314941, 'validation/accuracy': 0.6599999666213989, 'validation/loss': 1.6194846630096436, 'validation/num_examples': 50000, 'test/accuracy': 0.5382000207901001, 'test/loss': 2.2213504314422607, 'test/num_examples': 10000, 'score': 23143.673278331757, 'total_duration': 25044.706683397293, 'accumulated_submission_time': 23143.673278331757, 'accumulated_eval_time': 1896.6888296604156, 'accumulated_logging_time': 1.6524369716644287}
I0131 19:51:19.238656 139702527031040 logging_writer.py:48] [50934] accumulated_eval_time=1896.688830, accumulated_logging_time=1.652437, accumulated_submission_time=23143.673278, global_step=50934, preemption_count=0, score=23143.673278, test/accuracy=0.538200, test/loss=2.221350, test/num_examples=10000, total_duration=25044.706683, train/accuracy=0.727734, train/loss=1.318717, validation/accuracy=0.660000, validation/loss=1.619485, validation/num_examples=50000
I0131 19:51:46.016115 139702543816448 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1450316905975342, loss=3.826892614364624
I0131 19:52:30.907498 139702527031040 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.0300527811050415, loss=3.7710955142974854
I0131 19:53:17.060833 139702543816448 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.0574898719787598, loss=3.5498056411743164
I0131 19:54:03.547076 139702527031040 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.0186420679092407, loss=3.98984432220459
I0131 19:54:49.172601 139702543816448 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.0824804306030273, loss=3.5262579917907715
I0131 19:55:34.782426 139702527031040 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.117363691329956, loss=3.6441471576690674
I0131 19:56:20.914170 139702543816448 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.0979083776474, loss=3.6059913635253906
I0131 19:57:06.596190 139702527031040 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.0185035467147827, loss=3.9242630004882812
I0131 19:57:52.434642 139702543816448 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.981498658657074, loss=5.300265312194824
I0131 19:58:19.250504 139863983413056 spec.py:321] Evaluating on the training split.
I0131 19:58:29.616703 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 19:58:52.633195 139863983413056 spec.py:349] Evaluating on the test split.
I0131 19:58:54.274391 139863983413056 submission_runner.py:408] Time since start: 25499.77s, 	Step: 51860, 	{'train/accuracy': 0.7147851586341858, 'train/loss': 1.3640855550765991, 'validation/accuracy': 0.6587399840354919, 'validation/loss': 1.6124135255813599, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.21683406829834, 'test/num_examples': 10000, 'score': 23563.626941919327, 'total_duration': 25499.76881289482, 'accumulated_submission_time': 23563.626941919327, 'accumulated_eval_time': 1931.7127187252045, 'accumulated_logging_time': 1.6885082721710205}
I0131 19:58:54.299543 139702527031040 logging_writer.py:48] [51860] accumulated_eval_time=1931.712719, accumulated_logging_time=1.688508, accumulated_submission_time=23563.626942, global_step=51860, preemption_count=0, score=23563.626942, test/accuracy=0.536100, test/loss=2.216834, test/num_examples=10000, total_duration=25499.768813, train/accuracy=0.714785, train/loss=1.364086, validation/accuracy=0.658740, validation/loss=1.612414, validation/num_examples=50000
I0131 19:59:10.688176 139702543816448 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.9943882822990417, loss=4.953981399536133
I0131 19:59:54.048296 139702527031040 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.0078297853469849, loss=5.341099739074707
I0131 20:00:39.796977 139702543816448 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.12684965133667, loss=3.522505044937134
I0131 20:01:25.790708 139702527031040 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.092894434928894, loss=3.5221097469329834
I0131 20:02:11.757586 139702543816448 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.1051510572433472, loss=3.489018201828003
I0131 20:02:57.297378 139702527031040 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.9466920495033264, loss=3.946547031402588
I0131 20:03:42.964557 139702543816448 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.1447354555130005, loss=3.5096561908721924
I0131 20:04:29.093161 139702527031040 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.9436216950416565, loss=4.361618995666504
I0131 20:05:14.619732 139702543816448 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.034846544265747, loss=5.273991107940674
I0131 20:05:54.638794 139863983413056 spec.py:321] Evaluating on the training split.
I0131 20:06:05.188236 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 20:06:26.970378 139863983413056 spec.py:349] Evaluating on the test split.
I0131 20:06:28.613053 139863983413056 submission_runner.py:408] Time since start: 25954.11s, 	Step: 52789, 	{'train/accuracy': 0.7134960889816284, 'train/loss': 1.4122158288955688, 'validation/accuracy': 0.6552799940109253, 'validation/loss': 1.6602015495300293, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.2857484817504883, 'test/num_examples': 10000, 'score': 23983.90749502182, 'total_duration': 25954.107449531555, 'accumulated_submission_time': 23983.90749502182, 'accumulated_eval_time': 1965.6869568824768, 'accumulated_logging_time': 1.7237651348114014}
I0131 20:06:28.643023 139702527031040 logging_writer.py:48] [52789] accumulated_eval_time=1965.686957, accumulated_logging_time=1.723765, accumulated_submission_time=23983.907495, global_step=52789, preemption_count=0, score=23983.907495, test/accuracy=0.526900, test/loss=2.285748, test/num_examples=10000, total_duration=25954.107450, train/accuracy=0.713496, train/loss=1.412216, validation/accuracy=0.655280, validation/loss=1.660202, validation/num_examples=50000
I0131 20:06:33.430925 139702543816448 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.201012372970581, loss=3.5940093994140625
I0131 20:07:14.898062 139702527031040 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.0753904581069946, loss=3.398144483566284
I0131 20:08:00.507310 139702543816448 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.03542959690094, loss=3.5405075550079346
I0131 20:08:46.278851 139702527031040 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.1059718132019043, loss=3.568028450012207
I0131 20:09:31.998192 139702543816448 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.0697543621063232, loss=3.593310832977295
I0131 20:10:18.191533 139702527031040 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.1805475950241089, loss=3.5799484252929688
I0131 20:11:03.935745 139702543816448 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.1387500762939453, loss=3.550680637359619
I0131 20:11:49.568316 139702527031040 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0826386213302612, loss=5.299790382385254
I0131 20:12:35.634304 139702543816448 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.9872415065765381, loss=5.115100860595703
I0131 20:13:21.564886 139702527031040 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.051745057106018, loss=3.5113821029663086
I0131 20:13:28.994574 139863983413056 spec.py:321] Evaluating on the training split.
I0131 20:13:39.330333 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 20:13:59.936368 139863983413056 spec.py:349] Evaluating on the test split.
I0131 20:14:01.580937 139863983413056 submission_runner.py:408] Time since start: 26407.08s, 	Step: 53718, 	{'train/accuracy': 0.7314453125, 'train/loss': 1.304121494293213, 'validation/accuracy': 0.6644399762153625, 'validation/loss': 1.601436972618103, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.231038808822632, 'test/num_examples': 10000, 'score': 24404.197466611862, 'total_duration': 26407.075337409973, 'accumulated_submission_time': 24404.197466611862, 'accumulated_eval_time': 1998.2732956409454, 'accumulated_logging_time': 1.7659268379211426}
I0131 20:14:01.604628 139702543816448 logging_writer.py:48] [53718] accumulated_eval_time=1998.273296, accumulated_logging_time=1.765927, accumulated_submission_time=24404.197467, global_step=53718, preemption_count=0, score=24404.197467, test/accuracy=0.538500, test/loss=2.231039, test/num_examples=10000, total_duration=26407.075337, train/accuracy=0.731445, train/loss=1.304121, validation/accuracy=0.664440, validation/loss=1.601437, validation/num_examples=50000
I0131 20:14:35.074162 139702527031040 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.9758872985839844, loss=3.942798137664795
I0131 20:15:20.585886 139702543816448 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.0012671947479248, loss=4.139959335327148
I0131 20:16:06.457664 139702527031040 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0635814666748047, loss=3.4702062606811523
I0131 20:16:52.192353 139702543816448 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.1972565650939941, loss=3.5190911293029785
I0131 20:17:37.711619 139702527031040 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.0970462560653687, loss=3.483344078063965
I0131 20:18:23.672795 139702543816448 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.0484079122543335, loss=5.139835357666016
I0131 20:19:09.449656 139702527031040 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.0145524740219116, loss=4.597485065460205
I0131 20:19:55.351056 139702543816448 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.9490083456039429, loss=4.970526695251465
I0131 20:20:41.154401 139702527031040 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.074885368347168, loss=3.6402382850646973
I0131 20:21:01.922464 139863983413056 spec.py:321] Evaluating on the training split.
I0131 20:21:12.435163 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 20:21:34.389339 139863983413056 spec.py:349] Evaluating on the test split.
I0131 20:21:36.024414 139863983413056 submission_runner.py:408] Time since start: 26861.52s, 	Step: 54647, 	{'train/accuracy': 0.71533203125, 'train/loss': 1.3832980394363403, 'validation/accuracy': 0.6642999649047852, 'validation/loss': 1.607182502746582, 'validation/num_examples': 50000, 'test/accuracy': 0.5401000380516052, 'test/loss': 2.210639476776123, 'test/num_examples': 10000, 'score': 24824.456993341446, 'total_duration': 26861.518835544586, 'accumulated_submission_time': 24824.456993341446, 'accumulated_eval_time': 2032.3752472400665, 'accumulated_logging_time': 1.7985684871673584}
I0131 20:21:36.048065 139702543816448 logging_writer.py:48] [54647] accumulated_eval_time=2032.375247, accumulated_logging_time=1.798568, accumulated_submission_time=24824.456993, global_step=54647, preemption_count=0, score=24824.456993, test/accuracy=0.540100, test/loss=2.210639, test/num_examples=10000, total_duration=26861.518836, train/accuracy=0.715332, train/loss=1.383298, validation/accuracy=0.664300, validation/loss=1.607183, validation/num_examples=50000
I0131 20:21:57.645460 139702527031040 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.9538337588310242, loss=4.756289482116699
I0131 20:22:41.332641 139702543816448 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.018358588218689, loss=3.803306818008423
I0131 20:23:27.294959 139702527031040 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.0293689966201782, loss=3.7543458938598633
I0131 20:24:13.508429 139702543816448 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.1689074039459229, loss=3.535639762878418
I0131 20:24:59.278950 139702527031040 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0598962306976318, loss=3.511812210083008
I0131 20:25:44.964309 139702543816448 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.050400733947754, loss=5.293612480163574
I0131 20:26:30.860328 139702527031040 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.1526527404785156, loss=3.5736875534057617
I0131 20:27:16.440694 139702543816448 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.9236196279525757, loss=4.001702308654785
I0131 20:28:02.337511 139702527031040 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.9950277805328369, loss=4.922752857208252
I0131 20:28:36.032458 139863983413056 spec.py:321] Evaluating on the training split.
I0131 20:28:46.323377 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 20:29:07.169489 139863983413056 spec.py:349] Evaluating on the test split.
I0131 20:29:08.823275 139863983413056 submission_runner.py:408] Time since start: 27314.32s, 	Step: 55575, 	{'train/accuracy': 0.7239453196525574, 'train/loss': 1.3593758344650269, 'validation/accuracy': 0.6665999889373779, 'validation/loss': 1.6094951629638672, 'validation/num_examples': 50000, 'test/accuracy': 0.5408000349998474, 'test/loss': 2.2293291091918945, 'test/num_examples': 10000, 'score': 25244.383882761, 'total_duration': 27314.31769967079, 'accumulated_submission_time': 25244.383882761, 'accumulated_eval_time': 2065.166063785553, 'accumulated_logging_time': 1.8309228420257568}
I0131 20:29:08.847612 139702543816448 logging_writer.py:48] [55575] accumulated_eval_time=2065.166064, accumulated_logging_time=1.830923, accumulated_submission_time=25244.383883, global_step=55575, preemption_count=0, score=25244.383883, test/accuracy=0.540800, test/loss=2.229329, test/num_examples=10000, total_duration=27314.317700, train/accuracy=0.723945, train/loss=1.359376, validation/accuracy=0.666600, validation/loss=1.609495, validation/num_examples=50000
I0131 20:29:19.235175 139702527031040 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.241633653640747, loss=3.639887571334839
I0131 20:30:02.261226 139702543816448 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.0989915132522583, loss=3.5347654819488525
I0131 20:30:48.067116 139702527031040 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.038692593574524, loss=3.896057367324829
I0131 20:31:33.967601 139702543816448 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.14924955368042, loss=3.55938720703125
I0131 20:32:19.509207 139702527031040 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.100866436958313, loss=3.7724251747131348
I0131 20:33:05.667648 139702543816448 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0074998140335083, loss=3.631558418273926
I0131 20:33:51.473496 139702527031040 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.0889908075332642, loss=3.4948744773864746
I0131 20:34:37.528251 139702543816448 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.0730453729629517, loss=3.43576979637146
I0131 20:35:23.456484 139702527031040 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.1249496936798096, loss=3.483361005783081
I0131 20:36:09.664199 139702543816448 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.1498199701309204, loss=3.5289127826690674
I0131 20:36:09.678774 139863983413056 spec.py:321] Evaluating on the training split.
I0131 20:36:20.021733 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 20:36:41.209305 139863983413056 spec.py:349] Evaluating on the test split.
I0131 20:36:42.850601 139863983413056 submission_runner.py:408] Time since start: 27768.35s, 	Step: 56501, 	{'train/accuracy': 0.7246484160423279, 'train/loss': 1.3352673053741455, 'validation/accuracy': 0.6661999821662903, 'validation/loss': 1.6007659435272217, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.2219364643096924, 'test/num_examples': 10000, 'score': 25665.15626358986, 'total_duration': 27768.345024824142, 'accumulated_submission_time': 25665.15626358986, 'accumulated_eval_time': 2098.33789563179, 'accumulated_logging_time': 1.864485502243042}
I0131 20:36:42.873872 139702527031040 logging_writer.py:48] [56501] accumulated_eval_time=2098.337896, accumulated_logging_time=1.864486, accumulated_submission_time=25665.156264, global_step=56501, preemption_count=0, score=25665.156264, test/accuracy=0.539300, test/loss=2.221936, test/num_examples=10000, total_duration=27768.345025, train/accuracy=0.724648, train/loss=1.335267, validation/accuracy=0.666200, validation/loss=1.600766, validation/num_examples=50000
I0131 20:37:23.351941 139702543816448 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.0377906560897827, loss=3.553323268890381
I0131 20:38:08.943351 139702527031040 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.091843843460083, loss=4.92941951751709
I0131 20:38:54.837204 139702543816448 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.1905878782272339, loss=3.584425449371338
I0131 20:39:40.817012 139702527031040 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.0308823585510254, loss=3.4505534172058105
I0131 20:40:26.806287 139702543816448 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1364301443099976, loss=3.4860379695892334
I0131 20:41:12.477958 139702527031040 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.2283133268356323, loss=3.516655206680298
I0131 20:41:58.365470 139702543816448 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.109279990196228, loss=3.419050693511963
I0131 20:42:44.010540 139702527031040 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.0543127059936523, loss=3.418104648590088
I0131 20:43:29.856842 139702543816448 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.0084635019302368, loss=3.78021240234375
I0131 20:43:43.231996 139863983413056 spec.py:321] Evaluating on the training split.
I0131 20:43:53.404744 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 20:44:15.982641 139863983413056 spec.py:349] Evaluating on the test split.
I0131 20:44:17.623392 139863983413056 submission_runner.py:408] Time since start: 28223.12s, 	Step: 57431, 	{'train/accuracy': 0.7429101467132568, 'train/loss': 1.2352076768875122, 'validation/accuracy': 0.6694999933242798, 'validation/loss': 1.5599645376205444, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.1928648948669434, 'test/num_examples': 10000, 'score': 26085.455446720123, 'total_duration': 28223.11777973175, 'accumulated_submission_time': 26085.455446720123, 'accumulated_eval_time': 2132.7292597293854, 'accumulated_logging_time': 1.8969478607177734}
I0131 20:44:17.653789 139702527031040 logging_writer.py:48] [57431] accumulated_eval_time=2132.729260, accumulated_logging_time=1.896948, accumulated_submission_time=26085.455447, global_step=57431, preemption_count=0, score=26085.455447, test/accuracy=0.542300, test/loss=2.192865, test/num_examples=10000, total_duration=28223.117780, train/accuracy=0.742910, train/loss=1.235208, validation/accuracy=0.669500, validation/loss=1.559965, validation/num_examples=50000
I0131 20:44:45.622425 139702543816448 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.9579452276229858, loss=4.563647270202637
I0131 20:45:30.634394 139702527031040 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.0653942823410034, loss=3.5170724391937256
I0131 20:46:16.518215 139702543816448 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.9985331296920776, loss=3.7482106685638428
I0131 20:47:02.508887 139702527031040 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.036540150642395, loss=3.4977121353149414
I0131 20:47:48.097671 139702543816448 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.0194001197814941, loss=3.571295976638794
I0131 20:48:34.082483 139702527031040 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1026681661605835, loss=5.257027626037598
I0131 20:49:20.052938 139702543816448 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.032771348953247, loss=5.2466020584106445
I0131 20:50:05.952808 139702527031040 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.0867329835891724, loss=3.4738376140594482
I0131 20:50:51.987275 139702543816448 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.975846529006958, loss=4.360718250274658
I0131 20:51:17.711530 139863983413056 spec.py:321] Evaluating on the training split.
I0131 20:51:28.408936 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 20:51:49.849428 139863983413056 spec.py:349] Evaluating on the test split.
I0131 20:51:51.496241 139863983413056 submission_runner.py:408] Time since start: 28676.99s, 	Step: 58358, 	{'train/accuracy': 0.7205859422683716, 'train/loss': 1.3466764688491821, 'validation/accuracy': 0.6641799807548523, 'validation/loss': 1.5929832458496094, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.2014687061309814, 'test/num_examples': 10000, 'score': 26505.45462369919, 'total_duration': 28676.990658283234, 'accumulated_submission_time': 26505.45462369919, 'accumulated_eval_time': 2166.513976097107, 'accumulated_logging_time': 1.9372212886810303}
I0131 20:51:51.522791 139702527031040 logging_writer.py:48] [58358] accumulated_eval_time=2166.513976, accumulated_logging_time=1.937221, accumulated_submission_time=26505.454624, global_step=58358, preemption_count=0, score=26505.454624, test/accuracy=0.540900, test/loss=2.201469, test/num_examples=10000, total_duration=28676.990658, train/accuracy=0.720586, train/loss=1.346676, validation/accuracy=0.664180, validation/loss=1.592983, validation/num_examples=50000
I0131 20:52:08.708239 139702543816448 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.1414294242858887, loss=3.489363431930542
I0131 20:52:51.856255 139702527031040 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.0876399278640747, loss=3.547261953353882
I0131 20:53:37.902816 139702543816448 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.0874191522598267, loss=3.6080875396728516
I0131 20:54:23.849559 139702527031040 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.0272483825683594, loss=5.256359100341797
I0131 20:55:09.942262 139702543816448 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.1357694864273071, loss=3.45408034324646
I0131 20:55:56.104849 139702527031040 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.9844383001327515, loss=4.668708801269531
I0131 20:56:41.899491 139702543816448 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9506886601448059, loss=4.5883283615112305
I0131 20:57:27.677776 139702527031040 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.0716967582702637, loss=4.897600173950195
I0131 20:58:13.470154 139702543816448 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.0789239406585693, loss=3.6433560848236084
I0131 20:58:51.592736 139863983413056 spec.py:321] Evaluating on the training split.
I0131 20:59:02.846882 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 20:59:24.154514 139863983413056 spec.py:349] Evaluating on the test split.
I0131 20:59:25.789794 139863983413056 submission_runner.py:408] Time since start: 29131.28s, 	Step: 59285, 	{'train/accuracy': 0.7235937118530273, 'train/loss': 1.3309944868087769, 'validation/accuracy': 0.6640599966049194, 'validation/loss': 1.594617247581482, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.206836462020874, 'test/num_examples': 10000, 'score': 26925.466319322586, 'total_duration': 29131.28421139717, 'accumulated_submission_time': 26925.466319322586, 'accumulated_eval_time': 2200.7110509872437, 'accumulated_logging_time': 1.9730114936828613}
I0131 20:59:25.817934 139702527031040 logging_writer.py:48] [59285] accumulated_eval_time=2200.711051, accumulated_logging_time=1.973011, accumulated_submission_time=26925.466319, global_step=59285, preemption_count=0, score=26925.466319, test/accuracy=0.542100, test/loss=2.206836, test/num_examples=10000, total_duration=29131.284211, train/accuracy=0.723594, train/loss=1.330994, validation/accuracy=0.664060, validation/loss=1.594617, validation/num_examples=50000
I0131 20:59:32.207493 139702543816448 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.1410233974456787, loss=3.524730682373047
I0131 21:00:14.098670 139702527031040 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.1206340789794922, loss=3.481776237487793
I0131 21:01:00.040174 139702543816448 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.0546997785568237, loss=4.798648834228516
I0131 21:01:46.244554 139702527031040 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.1879773139953613, loss=3.4916625022888184
I0131 21:02:32.106442 139702543816448 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.0271825790405273, loss=3.8221096992492676
I0131 21:03:18.227455 139702527031040 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.038221001625061, loss=4.889551162719727
I0131 21:04:04.102520 139702543816448 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.1091556549072266, loss=3.5049855709075928
I0131 21:04:49.759807 139702527031040 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0324972867965698, loss=3.804816722869873
I0131 21:05:35.766812 139702543816448 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.9748196005821228, loss=4.164326190948486
I0131 21:06:21.747382 139702527031040 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.0050196647644043, loss=3.9001288414001465
I0131 21:06:26.041664 139863983413056 spec.py:321] Evaluating on the training split.
I0131 21:06:36.528837 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 21:06:57.856507 139863983413056 spec.py:349] Evaluating on the test split.
I0131 21:06:59.494237 139863983413056 submission_runner.py:408] Time since start: 29584.99s, 	Step: 60211, 	{'train/accuracy': 0.7406054735183716, 'train/loss': 1.2563652992248535, 'validation/accuracy': 0.6726999878883362, 'validation/loss': 1.5577574968338013, 'validation/num_examples': 50000, 'test/accuracy': 0.547700047492981, 'test/loss': 2.189788341522217, 'test/num_examples': 10000, 'score': 27345.63130736351, 'total_duration': 29584.988654613495, 'accumulated_submission_time': 27345.63130736351, 'accumulated_eval_time': 2234.1636261940002, 'accumulated_logging_time': 2.0108447074890137}
I0131 21:06:59.518697 139702543816448 logging_writer.py:48] [60211] accumulated_eval_time=2234.163626, accumulated_logging_time=2.010845, accumulated_submission_time=27345.631307, global_step=60211, preemption_count=0, score=27345.631307, test/accuracy=0.547700, test/loss=2.189788, test/num_examples=10000, total_duration=29584.988655, train/accuracy=0.740605, train/loss=1.256365, validation/accuracy=0.672700, validation/loss=1.557757, validation/num_examples=50000
I0131 21:07:35.775056 139702527031040 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.074548363685608, loss=3.452230453491211
I0131 21:08:21.567369 139702543816448 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.024828553199768, loss=4.076855182647705
I0131 21:09:07.631701 139702527031040 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.1231615543365479, loss=3.5741994380950928
I0131 21:09:53.886640 139702543816448 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.0411211252212524, loss=4.909180641174316
I0131 21:10:39.487820 139702527031040 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.1480013132095337, loss=3.4707603454589844
I0131 21:11:25.484408 139702543816448 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.025282621383667, loss=4.556483268737793
I0131 21:12:11.271878 139702527031040 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0997852087020874, loss=3.736325263977051
I0131 21:12:57.152393 139702543816448 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.0683802366256714, loss=3.5235140323638916
I0131 21:13:43.046898 139702527031040 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.1119194030761719, loss=3.615147352218628
I0131 21:13:59.551452 139863983413056 spec.py:321] Evaluating on the training split.
I0131 21:14:10.004615 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 21:14:29.957699 139863983413056 spec.py:349] Evaluating on the test split.
I0131 21:14:31.607706 139863983413056 submission_runner.py:408] Time since start: 30037.10s, 	Step: 61138, 	{'train/accuracy': 0.729296863079071, 'train/loss': 1.288324236869812, 'validation/accuracy': 0.6774599552154541, 'validation/loss': 1.5246641635894775, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.157984495162964, 'test/num_examples': 10000, 'score': 27765.606746673584, 'total_duration': 30037.10212635994, 'accumulated_submission_time': 27765.606746673584, 'accumulated_eval_time': 2266.2198588848114, 'accumulated_logging_time': 2.044236660003662}
I0131 21:14:31.635096 139702543816448 logging_writer.py:48] [61138] accumulated_eval_time=2266.219859, accumulated_logging_time=2.044237, accumulated_submission_time=27765.606747, global_step=61138, preemption_count=0, score=27765.606747, test/accuracy=0.545700, test/loss=2.157984, test/num_examples=10000, total_duration=30037.102126, train/accuracy=0.729297, train/loss=1.288324, validation/accuracy=0.677460, validation/loss=1.524664, validation/num_examples=50000
I0131 21:14:57.174154 139702527031040 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.111838698387146, loss=3.40201473236084
I0131 21:15:42.224211 139702543816448 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.037550449371338, loss=3.794917106628418
I0131 21:16:28.475593 139702527031040 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.0524288415908813, loss=3.5944578647613525
I0131 21:17:14.541496 139702543816448 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.0394163131713867, loss=4.683044910430908
I0131 21:18:00.343590 139702527031040 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.1344636678695679, loss=3.479201555252075
I0131 21:18:46.130522 139702543816448 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.181949257850647, loss=3.509646415710449
I0131 21:19:32.181457 139702527031040 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.0421589612960815, loss=3.566216230392456
I0131 21:20:18.511693 139702543816448 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.9413338899612427, loss=4.1449503898620605
I0131 21:21:04.548671 139702527031040 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.0679620504379272, loss=3.49692440032959
I0131 21:21:31.700678 139863983413056 spec.py:321] Evaluating on the training split.
I0131 21:21:42.042996 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 21:22:06.008087 139863983413056 spec.py:349] Evaluating on the test split.
I0131 21:22:07.650569 139863983413056 submission_runner.py:408] Time since start: 30493.14s, 	Step: 62061, 	{'train/accuracy': 0.7343164086341858, 'train/loss': 1.2893426418304443, 'validation/accuracy': 0.6753199696540833, 'validation/loss': 1.5548889636993408, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 2.169198751449585, 'test/num_examples': 10000, 'score': 28185.229808330536, 'total_duration': 30493.144993782043, 'accumulated_submission_time': 28185.229808330536, 'accumulated_eval_time': 2302.1697578430176, 'accumulated_logging_time': 2.4654579162597656}
I0131 21:22:07.675176 139702543816448 logging_writer.py:48] [62061] accumulated_eval_time=2302.169758, accumulated_logging_time=2.465458, accumulated_submission_time=28185.229808, global_step=62061, preemption_count=0, score=28185.229808, test/accuracy=0.546300, test/loss=2.169199, test/num_examples=10000, total_duration=30493.144994, train/accuracy=0.734316, train/loss=1.289343, validation/accuracy=0.675320, validation/loss=1.554889, validation/num_examples=50000
I0131 21:22:23.649068 139702527031040 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.9952333569526672, loss=3.906528949737549
I0131 21:23:06.945237 139702543816448 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.0452473163604736, loss=3.515620470046997
I0131 21:23:52.726702 139702527031040 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0777647495269775, loss=3.4649720191955566
I0131 21:24:38.695288 139702543816448 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.1169179677963257, loss=3.504295825958252
I0131 21:25:24.288735 139702527031040 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.0874485969543457, loss=5.066327095031738
I0131 21:26:10.517255 139702543816448 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0108791589736938, loss=3.6592018604278564
I0131 21:26:56.134488 139702527031040 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.0029022693634033, loss=3.6713576316833496
I0131 21:27:41.564879 139702543816448 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.9660512804985046, loss=4.413730621337891
I0131 21:28:27.197717 139702527031040 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.1210474967956543, loss=3.5381381511688232
I0131 21:29:07.675019 139863983413056 spec.py:321] Evaluating on the training split.
I0131 21:29:18.099176 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 21:29:43.313574 139863983413056 spec.py:349] Evaluating on the test split.
I0131 21:29:44.956458 139863983413056 submission_runner.py:408] Time since start: 30950.45s, 	Step: 62990, 	{'train/accuracy': 0.7407812476158142, 'train/loss': 1.2475316524505615, 'validation/accuracy': 0.6754399538040161, 'validation/loss': 1.5307263135910034, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.156816005706787, 'test/num_examples': 10000, 'score': 28605.168552160263, 'total_duration': 30950.45087170601, 'accumulated_submission_time': 28605.168552160263, 'accumulated_eval_time': 2339.451201438904, 'accumulated_logging_time': 2.5024900436401367}
I0131 21:29:44.986396 139702543816448 logging_writer.py:48] [62990] accumulated_eval_time=2339.451201, accumulated_logging_time=2.502490, accumulated_submission_time=28605.168552, global_step=62990, preemption_count=0, score=28605.168552, test/accuracy=0.550600, test/loss=2.156816, test/num_examples=10000, total_duration=30950.450872, train/accuracy=0.740781, train/loss=1.247532, validation/accuracy=0.675440, validation/loss=1.530726, validation/num_examples=50000
I0131 21:29:49.385709 139702527031040 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.9900256991386414, loss=4.250436305999756
I0131 21:30:30.668967 139702543816448 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.105751633644104, loss=3.3586912155151367
I0131 21:31:16.678195 139702527031040 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.1020395755767822, loss=5.196977615356445
I0131 21:32:02.700818 139702543816448 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.1416027545928955, loss=5.143928050994873
I0131 21:32:48.415910 139702527031040 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.062213659286499, loss=3.6425795555114746
I0131 21:33:34.131957 139702543816448 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.0399056673049927, loss=3.610720634460449
I0131 21:34:20.442198 139702527031040 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.0251413583755493, loss=3.7607932090759277
I0131 21:35:06.259956 139702543816448 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.1398848295211792, loss=3.5983707904815674
I0131 21:35:52.537203 139702527031040 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.138062834739685, loss=3.4538934230804443
I0131 21:36:38.698888 139702543816448 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.0479929447174072, loss=3.862802505493164
I0131 21:36:45.296873 139863983413056 spec.py:321] Evaluating on the training split.
I0131 21:36:55.896245 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 21:37:20.751696 139863983413056 spec.py:349] Evaluating on the test split.
I0131 21:37:22.401556 139863983413056 submission_runner.py:408] Time since start: 31407.90s, 	Step: 63916, 	{'train/accuracy': 0.7317968606948853, 'train/loss': 1.2713884115219116, 'validation/accuracy': 0.6771399974822998, 'validation/loss': 1.5070552825927734, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.1333210468292236, 'test/num_examples': 10000, 'score': 29025.421385526657, 'total_duration': 31407.895943164825, 'accumulated_submission_time': 29025.421385526657, 'accumulated_eval_time': 2376.5558342933655, 'accumulated_logging_time': 2.5417940616607666}
I0131 21:37:22.431515 139702527031040 logging_writer.py:48] [63916] accumulated_eval_time=2376.555834, accumulated_logging_time=2.541794, accumulated_submission_time=29025.421386, global_step=63916, preemption_count=0, score=29025.421386, test/accuracy=0.554100, test/loss=2.133321, test/num_examples=10000, total_duration=31407.895943, train/accuracy=0.731797, train/loss=1.271388, validation/accuracy=0.677140, validation/loss=1.507055, validation/num_examples=50000
I0131 21:37:56.553663 139702543816448 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1150598526000977, loss=5.065882682800293
I0131 21:38:42.311452 139702527031040 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.050000548362732, loss=5.145323753356934
I0131 21:39:28.261225 139702543816448 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.057909607887268, loss=4.2770094871521
I0131 21:40:14.257534 139702527031040 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1628412008285522, loss=4.985459804534912
I0131 21:41:00.157355 139702543816448 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.0498405694961548, loss=3.512355327606201
I0131 21:41:45.896813 139702527031040 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.0887010097503662, loss=3.4117729663848877
I0131 21:42:31.902042 139702543816448 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.1876713037490845, loss=3.526109218597412
I0131 21:43:17.930756 139702527031040 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.0870609283447266, loss=3.4807987213134766
I0131 21:44:04.105357 139702543816448 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.3559253215789795, loss=3.5241072177886963
I0131 21:44:22.683382 139863983413056 spec.py:321] Evaluating on the training split.
I0131 21:44:33.259937 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 21:44:54.741291 139863983413056 spec.py:349] Evaluating on the test split.
I0131 21:44:56.372721 139863983413056 submission_runner.py:408] Time since start: 31861.87s, 	Step: 64842, 	{'train/accuracy': 0.7364453077316284, 'train/loss': 1.2684588432312012, 'validation/accuracy': 0.6774799823760986, 'validation/loss': 1.5218348503112793, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.1238458156585693, 'test/num_examples': 10000, 'score': 29445.614727020264, 'total_duration': 31861.867134332657, 'accumulated_submission_time': 29445.614727020264, 'accumulated_eval_time': 2410.2451598644257, 'accumulated_logging_time': 2.581367254257202}
I0131 21:44:56.403649 139702527031040 logging_writer.py:48] [64842] accumulated_eval_time=2410.245160, accumulated_logging_time=2.581367, accumulated_submission_time=29445.614727, global_step=64842, preemption_count=0, score=29445.614727, test/accuracy=0.556100, test/loss=2.123846, test/num_examples=10000, total_duration=31861.867134, train/accuracy=0.736445, train/loss=1.268459, validation/accuracy=0.677480, validation/loss=1.521835, validation/num_examples=50000
I0131 21:45:19.985483 139702543816448 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.1542125940322876, loss=5.138047218322754
I0131 21:46:04.429718 139702527031040 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.05391526222229, loss=5.169981002807617
I0131 21:46:50.772117 139702543816448 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.0476049184799194, loss=5.152338981628418
I0131 21:47:36.939420 139702527031040 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.0862622261047363, loss=5.230194568634033
I0131 21:48:22.625160 139702543816448 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1337347030639648, loss=3.4797751903533936
I0131 21:49:08.723862 139702527031040 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.190487027168274, loss=3.350044012069702
I0131 21:49:54.826179 139702543816448 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.1116751432418823, loss=3.4292502403259277
I0131 21:50:40.767086 139702527031040 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.101464867591858, loss=5.151113986968994
I0131 21:51:26.634480 139702543816448 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.1950733661651611, loss=3.4168434143066406
I0131 21:51:56.499874 139863983413056 spec.py:321] Evaluating on the training split.
I0131 21:52:07.156132 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 21:52:28.546135 139863983413056 spec.py:349] Evaluating on the test split.
I0131 21:52:30.188427 139863983413056 submission_runner.py:408] Time since start: 32315.68s, 	Step: 65767, 	{'train/accuracy': 0.7422069907188416, 'train/loss': 1.2749184370040894, 'validation/accuracy': 0.6781600117683411, 'validation/loss': 1.548775553703308, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.1705105304718018, 'test/num_examples': 10000, 'score': 29865.653613567352, 'total_duration': 32315.68285059929, 'accumulated_submission_time': 29865.653613567352, 'accumulated_eval_time': 2443.933711528778, 'accumulated_logging_time': 2.6212127208709717}
I0131 21:52:30.217059 139702527031040 logging_writer.py:48] [65767] accumulated_eval_time=2443.933712, accumulated_logging_time=2.621213, accumulated_submission_time=29865.653614, global_step=65767, preemption_count=0, score=29865.653614, test/accuracy=0.552300, test/loss=2.170511, test/num_examples=10000, total_duration=32315.682851, train/accuracy=0.742207, train/loss=1.274918, validation/accuracy=0.678160, validation/loss=1.548776, validation/num_examples=50000
I0131 21:52:43.801565 139702543816448 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.1057682037353516, loss=4.073903560638428
I0131 21:53:26.512031 139702527031040 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.0833194255828857, loss=5.132462501525879
I0131 21:54:12.385216 139702543816448 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.127318263053894, loss=3.3646280765533447
I0131 21:54:58.492228 139702527031040 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.12074875831604, loss=3.4298036098480225
I0131 21:55:44.238777 139702543816448 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.1339073181152344, loss=3.7068684101104736
I0131 21:56:30.456268 139702527031040 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.0180583000183105, loss=3.6717605590820312
I0131 21:57:16.294763 139702543816448 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0425537824630737, loss=3.9610395431518555
I0131 21:58:02.338063 139702527031040 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.108437418937683, loss=4.964719295501709
I0131 21:58:48.313405 139702543816448 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.1401093006134033, loss=3.3441081047058105
I0131 21:59:30.197740 139863983413056 spec.py:321] Evaluating on the training split.
I0131 21:59:40.472608 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 22:00:02.043910 139863983413056 spec.py:349] Evaluating on the test split.
I0131 22:00:03.682847 139863983413056 submission_runner.py:408] Time since start: 32769.18s, 	Step: 66693, 	{'train/accuracy': 0.758984386920929, 'train/loss': 1.163934350013733, 'validation/accuracy': 0.6821199655532837, 'validation/loss': 1.4920015335083008, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.1089136600494385, 'test/num_examples': 10000, 'score': 30285.574808120728, 'total_duration': 32769.177268743515, 'accumulated_submission_time': 30285.574808120728, 'accumulated_eval_time': 2477.4188113212585, 'accumulated_logging_time': 2.660482168197632}
I0131 22:00:03.707406 139702527031040 logging_writer.py:48] [66693] accumulated_eval_time=2477.418811, accumulated_logging_time=2.660482, accumulated_submission_time=30285.574808, global_step=66693, preemption_count=0, score=30285.574808, test/accuracy=0.554100, test/loss=2.108914, test/num_examples=10000, total_duration=32769.177269, train/accuracy=0.758984, train/loss=1.163934, validation/accuracy=0.682120, validation/loss=1.492002, validation/num_examples=50000
I0131 22:00:06.897470 139702543816448 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.2070986032485962, loss=3.4574592113494873
I0131 22:00:48.417953 139702527031040 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.1459581851959229, loss=3.3977956771850586
I0131 22:01:34.280696 139702543816448 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.081893801689148, loss=4.902370452880859
I0131 22:02:20.348237 139702527031040 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.1631762981414795, loss=5.162732124328613
I0131 22:03:06.159062 139702543816448 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.0859178304672241, loss=4.459726333618164
I0131 22:03:51.832585 139702527031040 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.2844901084899902, loss=5.216611862182617
I0131 22:04:37.944657 139702543816448 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1831902265548706, loss=3.4169206619262695
I0131 22:05:24.099032 139702527031040 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.0652228593826294, loss=3.79152774810791
I0131 22:06:09.856010 139702543816448 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.0541942119598389, loss=4.127913475036621
I0131 22:06:55.930030 139702527031040 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.048248529434204, loss=4.217339038848877
I0131 22:07:03.954482 139863983413056 spec.py:321] Evaluating on the training split.
I0131 22:07:14.302407 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 22:07:39.309691 139863983413056 spec.py:349] Evaluating on the test split.
I0131 22:07:40.949651 139863983413056 submission_runner.py:408] Time since start: 33226.44s, 	Step: 67619, 	{'train/accuracy': 0.7391406297683716, 'train/loss': 1.2571861743927002, 'validation/accuracy': 0.6822999715805054, 'validation/loss': 1.5116750001907349, 'validation/num_examples': 50000, 'test/accuracy': 0.551300048828125, 'test/loss': 2.1411116123199463, 'test/num_examples': 10000, 'score': 30705.76295566559, 'total_duration': 33226.444074869156, 'accumulated_submission_time': 30705.76295566559, 'accumulated_eval_time': 2514.4139833450317, 'accumulated_logging_time': 2.6944541931152344}
I0131 22:07:40.975306 139702543816448 logging_writer.py:48] [67619] accumulated_eval_time=2514.413983, accumulated_logging_time=2.694454, accumulated_submission_time=30705.762956, global_step=67619, preemption_count=0, score=30705.762956, test/accuracy=0.551300, test/loss=2.141112, test/num_examples=10000, total_duration=33226.444075, train/accuracy=0.739141, train/loss=1.257186, validation/accuracy=0.682300, validation/loss=1.511675, validation/num_examples=50000
I0131 22:08:13.810937 139702527031040 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1116641759872437, loss=4.291998863220215
I0131 22:08:59.349488 139702543816448 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.1128038167953491, loss=3.4048280715942383
I0131 22:09:45.375183 139702527031040 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.0692119598388672, loss=3.5247421264648438
I0131 22:10:31.546952 139702543816448 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.0659197568893433, loss=4.037648677825928
I0131 22:11:17.432479 139702527031040 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.1033432483673096, loss=4.99739933013916
I0131 22:12:03.407321 139702543816448 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1404327154159546, loss=3.4276347160339355
I0131 22:12:49.442160 139702527031040 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.164543628692627, loss=3.6474738121032715
I0131 22:13:35.239990 139702543816448 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.0527626276016235, loss=4.229232311248779
I0131 22:14:21.213761 139702527031040 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.0076768398284912, loss=4.9380035400390625
I0131 22:14:41.098778 139863983413056 spec.py:321] Evaluating on the training split.
I0131 22:14:51.475585 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 22:15:13.421493 139863983413056 spec.py:349] Evaluating on the test split.
I0131 22:15:15.061543 139863983413056 submission_runner.py:408] Time since start: 33680.56s, 	Step: 68545, 	{'train/accuracy': 0.74609375, 'train/loss': 1.2384456396102905, 'validation/accuracy': 0.683459997177124, 'validation/loss': 1.5109264850616455, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.133676052093506, 'test/num_examples': 10000, 'score': 31125.828361272812, 'total_duration': 33680.55595970154, 'accumulated_submission_time': 31125.828361272812, 'accumulated_eval_time': 2548.3767414093018, 'accumulated_logging_time': 2.7300000190734863}
I0131 22:15:15.086707 139702543816448 logging_writer.py:48] [68545] accumulated_eval_time=2548.376741, accumulated_logging_time=2.730000, accumulated_submission_time=31125.828361, global_step=68545, preemption_count=0, score=31125.828361, test/accuracy=0.558900, test/loss=2.133676, test/num_examples=10000, total_duration=33680.555960, train/accuracy=0.746094, train/loss=1.238446, validation/accuracy=0.683460, validation/loss=1.510926, validation/num_examples=50000
I0131 22:15:37.479518 139702527031040 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.2328901290893555, loss=3.4672484397888184
I0131 22:16:21.695014 139702543816448 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.2577310800552368, loss=5.1242170333862305
I0131 22:17:07.844297 139702527031040 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.065964698791504, loss=3.3139548301696777
I0131 22:17:54.347383 139702543816448 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.1558741331100464, loss=3.3517274856567383
I0131 22:18:39.955215 139702527031040 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1497621536254883, loss=3.375230312347412
I0131 22:19:25.865014 139702543816448 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.031872272491455, loss=3.926727056503296
I0131 22:20:12.132713 139702527031040 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.0698397159576416, loss=3.3877880573272705
I0131 22:20:57.760566 139702543816448 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.0531184673309326, loss=4.775876522064209
I0131 22:21:43.583145 139702527031040 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.1515716314315796, loss=5.138888359069824
I0131 22:22:15.290321 139863983413056 spec.py:321] Evaluating on the training split.
I0131 22:22:25.637796 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 22:22:46.834262 139863983413056 spec.py:349] Evaluating on the test split.
I0131 22:22:48.483164 139863983413056 submission_runner.py:408] Time since start: 34133.98s, 	Step: 69471, 	{'train/accuracy': 0.755175769329071, 'train/loss': 1.170236349105835, 'validation/accuracy': 0.684939980506897, 'validation/loss': 1.482764482498169, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.103188991546631, 'test/num_examples': 10000, 'score': 31545.97432255745, 'total_duration': 34133.97758722305, 'accumulated_submission_time': 31545.97432255745, 'accumulated_eval_time': 2581.5695893764496, 'accumulated_logging_time': 2.7644755840301514}
I0131 22:22:48.510728 139702543816448 logging_writer.py:48] [69471] accumulated_eval_time=2581.569589, accumulated_logging_time=2.764476, accumulated_submission_time=31545.974323, global_step=69471, preemption_count=0, score=31545.974323, test/accuracy=0.561100, test/loss=2.103189, test/num_examples=10000, total_duration=34133.977587, train/accuracy=0.755176, train/loss=1.170236, validation/accuracy=0.684940, validation/loss=1.482764, validation/num_examples=50000
I0131 22:23:00.564683 139702527031040 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.040822148323059, loss=3.3490567207336426
I0131 22:23:42.798324 139702543816448 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1000386476516724, loss=3.3474678993225098
I0131 22:24:28.612828 139702527031040 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.1394329071044922, loss=3.278398036956787
I0131 22:25:14.644448 139702543816448 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.1127573251724243, loss=4.727535247802734
I0131 22:26:00.378870 139702527031040 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.047961711883545, loss=4.850442886352539
I0131 22:26:46.365019 139702543816448 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.3295979499816895, loss=3.4917335510253906
I0131 22:27:32.805071 139702527031040 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.138269066810608, loss=3.927823066711426
I0131 22:28:18.659488 139702543816448 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.0920506715774536, loss=3.6648330688476562
I0131 22:29:04.538052 139702527031040 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1283389329910278, loss=3.349221706390381
I0131 22:29:48.678803 139863983413056 spec.py:321] Evaluating on the training split.
I0131 22:29:59.129730 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 22:30:21.888304 139863983413056 spec.py:349] Evaluating on the test split.
I0131 22:30:23.534205 139863983413056 submission_runner.py:408] Time since start: 34589.03s, 	Step: 70398, 	{'train/accuracy': 0.7429296970367432, 'train/loss': 1.2822554111480713, 'validation/accuracy': 0.6846599578857422, 'validation/loss': 1.5301166772842407, 'validation/num_examples': 50000, 'test/accuracy': 0.5548000335693359, 'test/loss': 2.153522253036499, 'test/num_examples': 10000, 'score': 31966.082848072052, 'total_duration': 34589.028621673584, 'accumulated_submission_time': 31966.082848072052, 'accumulated_eval_time': 2616.4249787330627, 'accumulated_logging_time': 2.803164482116699}
I0131 22:30:23.560811 139702543816448 logging_writer.py:48] [70398] accumulated_eval_time=2616.424979, accumulated_logging_time=2.803164, accumulated_submission_time=31966.082848, global_step=70398, preemption_count=0, score=31966.082848, test/accuracy=0.554800, test/loss=2.153522, test/num_examples=10000, total_duration=34589.028622, train/accuracy=0.742930, train/loss=1.282255, validation/accuracy=0.684660, validation/loss=1.530117, validation/num_examples=50000
I0131 22:30:24.762596 139702527031040 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.2650550603866577, loss=3.386486530303955
I0131 22:31:05.852193 139702543816448 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1314994096755981, loss=3.4240896701812744
I0131 22:31:51.746974 139702527031040 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.095592737197876, loss=3.3794991970062256
I0131 22:32:37.805375 139702543816448 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.1400607824325562, loss=3.814344882965088
I0131 22:33:23.502068 139702527031040 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.0949761867523193, loss=4.246400833129883
I0131 22:34:09.189172 139702543816448 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.1179965734481812, loss=4.552944660186768
I0131 22:34:55.123136 139702527031040 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.161544919013977, loss=4.988292694091797
I0131 22:35:41.033245 139702543816448 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.1018389463424683, loss=4.418653964996338
I0131 22:36:26.788854 139702527031040 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.1422356367111206, loss=3.4346060752868652
I0131 22:37:12.804182 139702543816448 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.075456976890564, loss=3.701840400695801
I0131 22:37:23.534703 139863983413056 spec.py:321] Evaluating on the training split.
I0131 22:37:33.708702 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 22:37:55.158469 139863983413056 spec.py:349] Evaluating on the test split.
I0131 22:37:56.804765 139863983413056 submission_runner.py:408] Time since start: 35042.30s, 	Step: 71325, 	{'train/accuracy': 0.7483007907867432, 'train/loss': 1.252873182296753, 'validation/accuracy': 0.6881600022315979, 'validation/loss': 1.5203845500946045, 'validation/num_examples': 50000, 'test/accuracy': 0.5677000284194946, 'test/loss': 2.1288256645202637, 'test/num_examples': 10000, 'score': 32385.999056339264, 'total_duration': 35042.29918694496, 'accumulated_submission_time': 32385.999056339264, 'accumulated_eval_time': 2649.695028066635, 'accumulated_logging_time': 2.8389945030212402}
I0131 22:37:56.830183 139702527031040 logging_writer.py:48] [71325] accumulated_eval_time=2649.695028, accumulated_logging_time=2.838995, accumulated_submission_time=32385.999056, global_step=71325, preemption_count=0, score=32385.999056, test/accuracy=0.567700, test/loss=2.128826, test/num_examples=10000, total_duration=35042.299187, train/accuracy=0.748301, train/loss=1.252873, validation/accuracy=0.688160, validation/loss=1.520385, validation/num_examples=50000
I0131 22:38:27.212838 139702543816448 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.062946081161499, loss=4.71521520614624
I0131 22:39:12.220064 139702527031040 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.1080256700515747, loss=3.459864377975464
I0131 22:39:58.088682 139702543816448 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.2256999015808105, loss=3.356328010559082
I0131 22:40:44.217019 139702527031040 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.12372624874115, loss=3.6995785236358643
I0131 22:41:29.984729 139702543816448 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.2624632120132446, loss=5.176851749420166
I0131 22:42:15.821525 139702527031040 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.1602365970611572, loss=3.4006502628326416
I0131 22:43:01.699405 139702543816448 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.1614747047424316, loss=4.906856060028076
I0131 22:43:47.553014 139702527031040 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.0690525770187378, loss=3.605097532272339
I0131 22:44:33.639395 139702543816448 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.1566823720932007, loss=3.609215497970581
I0131 22:44:56.882745 139863983413056 spec.py:321] Evaluating on the training split.
I0131 22:45:07.454501 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 22:45:28.935066 139863983413056 spec.py:349] Evaluating on the test split.
I0131 22:45:30.577311 139863983413056 submission_runner.py:408] Time since start: 35496.07s, 	Step: 72251, 	{'train/accuracy': 0.7540820240974426, 'train/loss': 1.2140352725982666, 'validation/accuracy': 0.6817799806594849, 'validation/loss': 1.519381046295166, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 2.117690086364746, 'test/num_examples': 10000, 'score': 32805.99316358566, 'total_duration': 35496.07172703743, 'accumulated_submission_time': 32805.99316358566, 'accumulated_eval_time': 2683.389586210251, 'accumulated_logging_time': 2.8732550144195557}
I0131 22:45:30.606022 139702527031040 logging_writer.py:48] [72251] accumulated_eval_time=2683.389586, accumulated_logging_time=2.873255, accumulated_submission_time=32805.993164, global_step=72251, preemption_count=0, score=32805.993164, test/accuracy=0.561000, test/loss=2.117690, test/num_examples=10000, total_duration=35496.071727, train/accuracy=0.754082, train/loss=1.214035, validation/accuracy=0.681780, validation/loss=1.519381, validation/num_examples=50000
I0131 22:45:50.579581 139702543816448 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.0701143741607666, loss=4.889651298522949
I0131 22:46:34.281249 139702527031040 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.2623406648635864, loss=3.4259157180786133
I0131 22:47:20.357246 139702543816448 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.026790738105774, loss=4.085282325744629
I0131 22:48:06.844341 139702527031040 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.1533013582229614, loss=5.007081985473633
I0131 22:48:52.576600 139702543816448 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.2316352128982544, loss=4.815908432006836
I0131 22:49:38.512565 139702527031040 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.18288254737854, loss=4.963883399963379
I0131 22:50:24.745030 139702543816448 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.2422871589660645, loss=3.4428670406341553
I0131 22:51:10.577683 139702527031040 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.1072938442230225, loss=4.807455062866211
I0131 22:51:56.608698 139702543816448 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.2361462116241455, loss=3.35750675201416
I0131 22:52:30.725841 139863983413056 spec.py:321] Evaluating on the training split.
I0131 22:52:41.208826 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 22:53:01.527841 139863983413056 spec.py:349] Evaluating on the test split.
I0131 22:53:03.174937 139863983413056 submission_runner.py:408] Time since start: 35948.67s, 	Step: 73176, 	{'train/accuracy': 0.7476171851158142, 'train/loss': 1.2500646114349365, 'validation/accuracy': 0.6918999552726746, 'validation/loss': 1.4931375980377197, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.119715929031372, 'test/num_examples': 10000, 'score': 33226.05568480492, 'total_duration': 35948.66935944557, 'accumulated_submission_time': 33226.05568480492, 'accumulated_eval_time': 2715.83868432045, 'accumulated_logging_time': 2.911292314529419}
I0131 22:53:03.200835 139702527031040 logging_writer.py:48] [73176] accumulated_eval_time=2715.838684, accumulated_logging_time=2.911292, accumulated_submission_time=33226.055685, global_step=73176, preemption_count=0, score=33226.055685, test/accuracy=0.564000, test/loss=2.119716, test/num_examples=10000, total_duration=35948.669359, train/accuracy=0.747617, train/loss=1.250065, validation/accuracy=0.691900, validation/loss=1.493138, validation/num_examples=50000
I0131 22:53:13.183429 139702543816448 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.197566032409668, loss=3.4247138500213623
I0131 22:53:55.848748 139702527031040 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.186874270439148, loss=3.449547290802002
I0131 22:54:41.914889 139702543816448 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.0270311832427979, loss=4.5595903396606445
I0131 22:55:27.944128 139702527031040 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.1690664291381836, loss=3.345458984375
I0131 22:56:13.955137 139702543816448 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.124942421913147, loss=4.709547996520996
I0131 22:56:59.833063 139702527031040 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.1271336078643799, loss=4.121190547943115
I0131 22:57:45.926919 139702543816448 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.091584324836731, loss=3.4636034965515137
I0131 22:58:31.828821 139702527031040 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.2230302095413208, loss=3.330111503601074
I0131 22:59:17.685674 139702543816448 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.128061294555664, loss=3.6782336235046387
I0131 23:00:03.324276 139863983413056 spec.py:321] Evaluating on the training split.
I0131 23:00:13.709932 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 23:00:35.523361 139863983413056 spec.py:349] Evaluating on the test split.
I0131 23:00:37.160850 139863983413056 submission_runner.py:408] Time since start: 36402.66s, 	Step: 74100, 	{'train/accuracy': 0.7524218559265137, 'train/loss': 1.1826977729797363, 'validation/accuracy': 0.6895999908447266, 'validation/loss': 1.460847020149231, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 2.076725721359253, 'test/num_examples': 10000, 'score': 33646.12110567093, 'total_duration': 36402.65527367592, 'accumulated_submission_time': 33646.12110567093, 'accumulated_eval_time': 2749.675267457962, 'accumulated_logging_time': 2.9467294216156006}
I0131 23:00:37.189857 139702527031040 logging_writer.py:48] [74100] accumulated_eval_time=2749.675267, accumulated_logging_time=2.946729, accumulated_submission_time=33646.121106, global_step=74100, preemption_count=0, score=33646.121106, test/accuracy=0.568500, test/loss=2.076726, test/num_examples=10000, total_duration=36402.655274, train/accuracy=0.752422, train/loss=1.182698, validation/accuracy=0.689600, validation/loss=1.460847, validation/num_examples=50000
I0131 23:00:37.588480 139702543816448 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.2444911003112793, loss=3.3540468215942383
I0131 23:01:18.554949 139702527031040 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.149682641029358, loss=3.427774429321289
I0131 23:02:04.391713 139702543816448 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.1199326515197754, loss=3.931525945663452
I0131 23:02:50.345263 139702527031040 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.1767041683197021, loss=3.3740663528442383
I0131 23:03:36.490408 139702543816448 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.2199918031692505, loss=5.136449813842773
I0131 23:04:22.208219 139702527031040 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.1787409782409668, loss=3.592477560043335
I0131 23:05:08.341408 139702543816448 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.0959583520889282, loss=5.186031341552734
I0131 23:05:54.198981 139702527031040 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.274315595626831, loss=3.3954567909240723
I0131 23:06:39.935029 139702543816448 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1230807304382324, loss=4.410946846008301
I0131 23:07:25.723713 139702527031040 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.203197717666626, loss=3.880793809890747
I0131 23:07:37.288436 139863983413056 spec.py:321] Evaluating on the training split.
I0131 23:07:47.871548 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 23:08:10.928998 139863983413056 spec.py:349] Evaluating on the test split.
I0131 23:08:12.559630 139863983413056 submission_runner.py:408] Time since start: 36858.05s, 	Step: 75027, 	{'train/accuracy': 0.7589452862739563, 'train/loss': 1.1854172945022583, 'validation/accuracy': 0.6945599913597107, 'validation/loss': 1.469211220741272, 'validation/num_examples': 50000, 'test/accuracy': 0.5649999976158142, 'test/loss': 2.0995171070098877, 'test/num_examples': 10000, 'score': 34066.15981054306, 'total_duration': 36858.05403661728, 'accumulated_submission_time': 34066.15981054306, 'accumulated_eval_time': 2784.9464781284332, 'accumulated_logging_time': 2.986032724380493}
I0131 23:08:12.589249 139702543816448 logging_writer.py:48] [75027] accumulated_eval_time=2784.946478, accumulated_logging_time=2.986033, accumulated_submission_time=34066.159811, global_step=75027, preemption_count=0, score=34066.159811, test/accuracy=0.565000, test/loss=2.099517, test/num_examples=10000, total_duration=36858.054037, train/accuracy=0.758945, train/loss=1.185417, validation/accuracy=0.694560, validation/loss=1.469211, validation/num_examples=50000
I0131 23:08:42.142401 139702527031040 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.1788817644119263, loss=3.424696922302246
I0131 23:09:27.051853 139702543816448 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1988141536712646, loss=3.3942737579345703
I0131 23:10:13.180845 139702527031040 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.162168025970459, loss=3.358757257461548
I0131 23:10:59.517820 139702543816448 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.1114972829818726, loss=5.001313209533691
I0131 23:11:45.290680 139702527031040 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.1482975482940674, loss=3.391472339630127
I0131 23:12:31.471098 139702543816448 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.2168503999710083, loss=3.618607997894287
I0131 23:13:17.466124 139702527031040 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.2030972242355347, loss=3.361236572265625
I0131 23:14:03.372472 139702543816448 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.1644660234451294, loss=3.741089105606079
I0131 23:14:49.113564 139702527031040 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.2390881776809692, loss=3.40492844581604
I0131 23:15:12.668142 139863983413056 spec.py:321] Evaluating on the training split.
I0131 23:15:23.112298 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 23:15:44.831941 139863983413056 spec.py:349] Evaluating on the test split.
I0131 23:15:46.485458 139863983413056 submission_runner.py:408] Time since start: 37311.98s, 	Step: 75953, 	{'train/accuracy': 0.7703906297683716, 'train/loss': 1.1538687944412231, 'validation/accuracy': 0.6911799907684326, 'validation/loss': 1.488873839378357, 'validation/num_examples': 50000, 'test/accuracy': 0.5619000196456909, 'test/loss': 2.1018519401550293, 'test/num_examples': 10000, 'score': 34486.180584430695, 'total_duration': 37311.97987627983, 'accumulated_submission_time': 34486.180584430695, 'accumulated_eval_time': 2818.763783454895, 'accumulated_logging_time': 3.024766683578491}
I0131 23:15:46.512267 139702543816448 logging_writer.py:48] [75953] accumulated_eval_time=2818.763783, accumulated_logging_time=3.024767, accumulated_submission_time=34486.180584, global_step=75953, preemption_count=0, score=34486.180584, test/accuracy=0.561900, test/loss=2.101852, test/num_examples=10000, total_duration=37311.979876, train/accuracy=0.770391, train/loss=1.153869, validation/accuracy=0.691180, validation/loss=1.488874, validation/num_examples=50000
I0131 23:16:05.676981 139702527031040 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.1925908327102661, loss=3.329275369644165
I0131 23:16:49.159984 139702543816448 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1204582452774048, loss=3.454349994659424
I0131 23:17:35.285550 139702527031040 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.1621077060699463, loss=4.9708709716796875
I0131 23:18:21.563362 139702543816448 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.1063405275344849, loss=3.4684388637542725
I0131 23:19:07.610413 139702527031040 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.2224723100662231, loss=3.3625149726867676
I0131 23:19:53.810393 139702543816448 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.1634284257888794, loss=3.605604410171509
I0131 23:20:39.937431 139702527031040 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.2539862394332886, loss=5.129878997802734
I0131 23:21:25.753481 139702543816448 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.2050104141235352, loss=3.411755084991455
I0131 23:22:11.751355 139702527031040 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.0453698635101318, loss=3.825174570083618
I0131 23:22:46.618580 139863983413056 spec.py:321] Evaluating on the training split.
I0131 23:22:57.238625 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 23:23:16.698008 139863983413056 spec.py:349] Evaluating on the test split.
I0131 23:23:18.350519 139863983413056 submission_runner.py:408] Time since start: 37763.84s, 	Step: 76878, 	{'train/accuracy': 0.7526757717132568, 'train/loss': 1.1900469064712524, 'validation/accuracy': 0.6944400072097778, 'validation/loss': 1.4452449083328247, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 2.0509159564971924, 'test/num_examples': 10000, 'score': 34906.22839021683, 'total_duration': 37763.84492731094, 'accumulated_submission_time': 34906.22839021683, 'accumulated_eval_time': 2850.4957184791565, 'accumulated_logging_time': 3.0609076023101807}
I0131 23:23:18.383855 139702543816448 logging_writer.py:48] [76878] accumulated_eval_time=2850.495718, accumulated_logging_time=3.060908, accumulated_submission_time=34906.228390, global_step=76878, preemption_count=0, score=34906.228390, test/accuracy=0.571500, test/loss=2.050916, test/num_examples=10000, total_duration=37763.844927, train/accuracy=0.752676, train/loss=1.190047, validation/accuracy=0.694440, validation/loss=1.445245, validation/num_examples=50000
I0131 23:23:27.574021 139702527031040 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.2414089441299438, loss=3.372990846633911
I0131 23:24:10.180341 139702543816448 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.1840846538543701, loss=3.4026479721069336
I0131 23:24:56.074322 139702527031040 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.1261565685272217, loss=3.60111927986145
I0131 23:25:42.250846 139702543816448 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.2383188009262085, loss=5.135058403015137
I0131 23:26:28.209521 139702527031040 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.0682135820388794, loss=4.265955448150635
I0131 23:27:14.146620 139702543816448 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.2183023691177368, loss=3.3881046772003174
I0131 23:27:59.749612 139702527031040 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.0780583620071411, loss=4.141063213348389
I0131 23:28:45.639509 139702543816448 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.2136207818984985, loss=3.394455671310425
I0131 23:29:31.517421 139702527031040 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.2477816343307495, loss=3.5608227252960205
I0131 23:30:17.561437 139702543816448 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.396385669708252, loss=3.371398687362671
I0131 23:30:18.630018 139863983413056 spec.py:321] Evaluating on the training split.
I0131 23:30:29.176830 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 23:30:52.425416 139863983413056 spec.py:349] Evaluating on the test split.
I0131 23:30:54.070077 139863983413056 submission_runner.py:408] Time since start: 38219.56s, 	Step: 77804, 	{'train/accuracy': 0.7564452886581421, 'train/loss': 1.186036467552185, 'validation/accuracy': 0.6922799944877625, 'validation/loss': 1.4668896198272705, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 2.069314956665039, 'test/num_examples': 10000, 'score': 35326.41598343849, 'total_duration': 38219.564494132996, 'accumulated_submission_time': 35326.41598343849, 'accumulated_eval_time': 2885.9357640743256, 'accumulated_logging_time': 3.1037163734436035}
I0131 23:30:54.100674 139702527031040 logging_writer.py:48] [77804] accumulated_eval_time=2885.935764, accumulated_logging_time=3.103716, accumulated_submission_time=35326.415983, global_step=77804, preemption_count=0, score=35326.415983, test/accuracy=0.571100, test/loss=2.069315, test/num_examples=10000, total_duration=38219.564494, train/accuracy=0.756445, train/loss=1.186036, validation/accuracy=0.692280, validation/loss=1.466890, validation/num_examples=50000
I0131 23:31:33.567185 139702543816448 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.228132724761963, loss=3.406967878341675
I0131 23:32:19.521004 139702527031040 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.2066441774368286, loss=4.3708367347717285
I0131 23:33:05.626874 139702543816448 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.1686071157455444, loss=3.4317033290863037
I0131 23:33:51.290566 139702527031040 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.2555900812149048, loss=3.38866925239563
I0131 23:34:37.081546 139702543816448 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.1130813360214233, loss=3.445669412612915
I0131 23:35:23.052521 139702527031040 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.2133742570877075, loss=4.659067153930664
I0131 23:36:08.565202 139702543816448 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.073959469795227, loss=3.3893022537231445
I0131 23:36:54.374080 139702527031040 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.243024230003357, loss=3.3246748447418213
I0131 23:37:40.056242 139702543816448 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.1815638542175293, loss=3.3031294345855713
I0131 23:37:54.487090 139863983413056 spec.py:321] Evaluating on the training split.
I0131 23:38:04.990224 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 23:38:26.736299 139863983413056 spec.py:349] Evaluating on the test split.
I0131 23:38:28.372583 139863983413056 submission_runner.py:408] Time since start: 38673.87s, 	Step: 78733, 	{'train/accuracy': 0.7732617259025574, 'train/loss': 1.1194065809249878, 'validation/accuracy': 0.6947199702262878, 'validation/loss': 1.444237470626831, 'validation/num_examples': 50000, 'test/accuracy': 0.5674000382423401, 'test/loss': 2.0553789138793945, 'test/num_examples': 10000, 'score': 35746.742753982544, 'total_duration': 38673.86698675156, 'accumulated_submission_time': 35746.742753982544, 'accumulated_eval_time': 2919.8212456703186, 'accumulated_logging_time': 3.1446046829223633}
I0131 23:38:28.402494 139702527031040 logging_writer.py:48] [78733] accumulated_eval_time=2919.821246, accumulated_logging_time=3.144605, accumulated_submission_time=35746.742754, global_step=78733, preemption_count=0, score=35746.742754, test/accuracy=0.567400, test/loss=2.055379, test/num_examples=10000, total_duration=38673.866987, train/accuracy=0.773262, train/loss=1.119407, validation/accuracy=0.694720, validation/loss=1.444237, validation/num_examples=50000
I0131 23:38:55.580893 139702543816448 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.1250406503677368, loss=3.6409895420074463
I0131 23:39:40.409434 139702527031040 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.206551194190979, loss=4.174671649932861
I0131 23:40:26.493384 139702543816448 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.224289059638977, loss=3.473106861114502
I0131 23:41:12.432981 139702527031040 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.2070226669311523, loss=3.3063809871673584
I0131 23:41:57.998462 139702543816448 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.2243926525115967, loss=4.405902862548828
I0131 23:42:44.142951 139702527031040 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.2908596992492676, loss=3.400313377380371
I0131 23:43:29.896526 139702543816448 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.1843968629837036, loss=3.277775287628174
I0131 23:44:15.674531 139702527031040 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.2656651735305786, loss=3.4126949310302734
I0131 23:45:01.802542 139702543816448 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.1654843091964722, loss=3.554752826690674
I0131 23:45:28.578851 139863983413056 spec.py:321] Evaluating on the training split.
I0131 23:45:38.866928 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 23:45:58.865094 139863983413056 spec.py:349] Evaluating on the test split.
I0131 23:46:00.498857 139863983413056 submission_runner.py:408] Time since start: 39125.99s, 	Step: 79660, 	{'train/accuracy': 0.7591992020606995, 'train/loss': 1.1473793983459473, 'validation/accuracy': 0.6970199942588806, 'validation/loss': 1.4147883653640747, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 2.0305778980255127, 'test/num_examples': 10000, 'score': 36166.86061668396, 'total_duration': 39125.99325990677, 'accumulated_submission_time': 36166.86061668396, 'accumulated_eval_time': 2951.7412304878235, 'accumulated_logging_time': 3.184016704559326}
I0131 23:46:00.534243 139702527031040 logging_writer.py:48] [79660] accumulated_eval_time=2951.741230, accumulated_logging_time=3.184017, accumulated_submission_time=36166.860617, global_step=79660, preemption_count=0, score=36166.860617, test/accuracy=0.573400, test/loss=2.030578, test/num_examples=10000, total_duration=39125.993260, train/accuracy=0.759199, train/loss=1.147379, validation/accuracy=0.697020, validation/loss=1.414788, validation/num_examples=50000
I0131 23:46:16.927621 139702543816448 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.1064890623092651, loss=3.9980978965759277
I0131 23:46:59.774655 139702527031040 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.4438421726226807, loss=4.736716270446777
I0131 23:47:46.112567 139702543816448 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.1179687976837158, loss=4.229555130004883
I0131 23:48:32.227412 139702527031040 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.1889832019805908, loss=3.609088897705078
I0131 23:49:18.265698 139702543816448 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.279477596282959, loss=3.385085105895996
I0131 23:50:04.338181 139702527031040 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.245133876800537, loss=3.314852714538574
I0131 23:50:50.570611 139702543816448 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.1723268032073975, loss=3.664595365524292
I0131 23:51:36.536466 139702527031040 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.067617654800415, loss=4.078266143798828
I0131 23:52:22.480316 139702543816448 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.407663106918335, loss=3.3156135082244873
I0131 23:53:00.533643 139863983413056 spec.py:321] Evaluating on the training split.
I0131 23:53:11.147524 139863983413056 spec.py:333] Evaluating on the validation split.
I0131 23:53:29.809893 139863983413056 spec.py:349] Evaluating on the test split.
I0131 23:53:31.455579 139863983413056 submission_runner.py:408] Time since start: 39576.95s, 	Step: 80584, 	{'train/accuracy': 0.7634375095367432, 'train/loss': 1.142561674118042, 'validation/accuracy': 0.7002399563789368, 'validation/loss': 1.4214868545532227, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 2.0416553020477295, 'test/num_examples': 10000, 'score': 36586.801466464996, 'total_duration': 39576.94997668266, 'accumulated_submission_time': 36586.801466464996, 'accumulated_eval_time': 2982.663145303726, 'accumulated_logging_time': 3.2296504974365234}
I0131 23:53:31.488321 139702527031040 logging_writer.py:48] [80584] accumulated_eval_time=2982.663145, accumulated_logging_time=3.229650, accumulated_submission_time=36586.801466, global_step=80584, preemption_count=0, score=36586.801466, test/accuracy=0.571900, test/loss=2.041655, test/num_examples=10000, total_duration=39576.949977, train/accuracy=0.763438, train/loss=1.142562, validation/accuracy=0.700240, validation/loss=1.421487, validation/num_examples=50000
I0131 23:53:38.300689 139702543816448 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.326765537261963, loss=5.058345794677734
I0131 23:54:20.579127 139702527031040 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.218743085861206, loss=3.3667004108428955
I0131 23:55:06.580349 139702543816448 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.162154197692871, loss=3.503415107727051
I0131 23:55:52.641393 139702527031040 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.159301519393921, loss=3.6186330318450928
I0131 23:56:38.723767 139702543816448 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.2486224174499512, loss=3.6843137741088867
I0131 23:57:24.685958 139702527031040 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.3646619319915771, loss=3.3561439514160156
I0131 23:58:10.675887 139702543816448 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.1815489530563354, loss=4.575342178344727
I0131 23:58:56.274488 139702527031040 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.153624415397644, loss=4.663252353668213
I0131 23:59:42.403527 139702543816448 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.4927458763122559, loss=5.075599193572998
I0201 00:00:28.624001 139702527031040 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.0386066436767578, loss=4.043753623962402
I0201 00:00:31.528419 139863983413056 spec.py:321] Evaluating on the training split.
I0201 00:00:41.943892 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 00:01:04.846333 139863983413056 spec.py:349] Evaluating on the test split.
I0201 00:01:06.491952 139863983413056 submission_runner.py:408] Time since start: 40031.99s, 	Step: 81508, 	{'train/accuracy': 0.7697460651397705, 'train/loss': 1.1436564922332764, 'validation/accuracy': 0.6994400024414062, 'validation/loss': 1.451212763786316, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 2.0733675956726074, 'test/num_examples': 10000, 'score': 37006.7823369503, 'total_duration': 40031.986365795135, 'accumulated_submission_time': 37006.7823369503, 'accumulated_eval_time': 3017.626652240753, 'accumulated_logging_time': 3.2731289863586426}
I0201 00:01:06.520094 139702543816448 logging_writer.py:48] [81508] accumulated_eval_time=3017.626652, accumulated_logging_time=3.273129, accumulated_submission_time=37006.782337, global_step=81508, preemption_count=0, score=37006.782337, test/accuracy=0.573100, test/loss=2.073368, test/num_examples=10000, total_duration=40031.986366, train/accuracy=0.769746, train/loss=1.143656, validation/accuracy=0.699440, validation/loss=1.451213, validation/num_examples=50000
I0201 00:01:43.950986 139702527031040 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.1377655267715454, loss=3.936154365539551
I0201 00:02:29.877383 139702543816448 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.1118206977844238, loss=4.285017013549805
I0201 00:03:15.909419 139702527031040 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.196227788925171, loss=3.3846800327301025
I0201 00:04:01.788762 139702543816448 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.2387936115264893, loss=3.313793182373047
I0201 00:04:47.638894 139702527031040 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.1775037050247192, loss=3.2792139053344727
I0201 00:05:33.756937 139702543816448 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.2190978527069092, loss=3.395725727081299
I0201 00:06:19.527469 139702527031040 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.1811288595199585, loss=4.089545249938965
I0201 00:07:05.390717 139702543816448 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.174727201461792, loss=3.890744686126709
I0201 00:07:51.438069 139702527031040 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.2120110988616943, loss=3.4064180850982666
I0201 00:08:06.742745 139863983413056 spec.py:321] Evaluating on the training split.
I0201 00:08:17.958984 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 00:08:40.636348 139863983413056 spec.py:349] Evaluating on the test split.
I0201 00:08:42.275954 139863983413056 submission_runner.py:408] Time since start: 40487.77s, 	Step: 82435, 	{'train/accuracy': 0.7611523270606995, 'train/loss': 1.1672053337097168, 'validation/accuracy': 0.699400007724762, 'validation/loss': 1.4308583736419678, 'validation/num_examples': 50000, 'test/accuracy': 0.5703999996185303, 'test/loss': 2.0486838817596436, 'test/num_examples': 10000, 'score': 37426.94573545456, 'total_duration': 40487.770376205444, 'accumulated_submission_time': 37426.94573545456, 'accumulated_eval_time': 3053.159845352173, 'accumulated_logging_time': 3.3112025260925293}
I0201 00:08:42.305704 139702543816448 logging_writer.py:48] [82435] accumulated_eval_time=3053.159845, accumulated_logging_time=3.311203, accumulated_submission_time=37426.945735, global_step=82435, preemption_count=0, score=37426.945735, test/accuracy=0.570400, test/loss=2.048684, test/num_examples=10000, total_duration=40487.770376, train/accuracy=0.761152, train/loss=1.167205, validation/accuracy=0.699400, validation/loss=1.430858, validation/num_examples=50000
I0201 00:09:08.708008 139702527031040 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.205042839050293, loss=3.294046640396118
I0201 00:09:53.758143 139702543816448 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.2061996459960938, loss=3.704517364501953
I0201 00:10:39.482776 139702527031040 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.119944453239441, loss=4.2435479164123535
I0201 00:11:25.462414 139702543816448 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.0867972373962402, loss=4.220186710357666
I0201 00:12:11.038673 139702527031040 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.2172192335128784, loss=4.772333145141602
I0201 00:12:57.114124 139702543816448 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.2811040878295898, loss=3.372619390487671
I0201 00:13:43.226218 139702527031040 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.2472378015518188, loss=4.929948806762695
I0201 00:14:29.135393 139702543816448 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.2646310329437256, loss=3.3862106800079346
I0201 00:15:15.107736 139702527031040 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.609015941619873, loss=3.3581552505493164
I0201 00:15:42.297860 139863983413056 spec.py:321] Evaluating on the training split.
I0201 00:15:52.607269 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 00:16:17.646749 139863983413056 spec.py:349] Evaluating on the test split.
I0201 00:16:19.284008 139863983413056 submission_runner.py:408] Time since start: 40944.78s, 	Step: 83361, 	{'train/accuracy': 0.7637695074081421, 'train/loss': 1.173699975013733, 'validation/accuracy': 0.6990399956703186, 'validation/loss': 1.4479047060012817, 'validation/num_examples': 50000, 'test/accuracy': 0.5679000020027161, 'test/loss': 2.0731194019317627, 'test/num_examples': 10000, 'score': 37846.880274534225, 'total_duration': 40944.77842760086, 'accumulated_submission_time': 37846.880274534225, 'accumulated_eval_time': 3090.146003007889, 'accumulated_logging_time': 3.34938383102417}
I0201 00:16:19.311897 139702543816448 logging_writer.py:48] [83361] accumulated_eval_time=3090.146003, accumulated_logging_time=3.349384, accumulated_submission_time=37846.880275, global_step=83361, preemption_count=0, score=37846.880275, test/accuracy=0.567900, test/loss=2.073119, test/num_examples=10000, total_duration=40944.778428, train/accuracy=0.763770, train/loss=1.173700, validation/accuracy=0.699040, validation/loss=1.447905, validation/num_examples=50000
I0201 00:16:35.412633 139702527031040 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.2218302488327026, loss=3.4492735862731934
I0201 00:17:18.782769 139702543816448 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.2520402669906616, loss=3.276207685470581
I0201 00:18:04.704877 139702527031040 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.1390899419784546, loss=3.5552897453308105
I0201 00:18:50.901060 139702543816448 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.2253559827804565, loss=3.6627039909362793
I0201 00:19:36.609519 139702527031040 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.3310329914093018, loss=3.4681754112243652
I0201 00:20:22.952184 139702543816448 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.2431260347366333, loss=4.708927154541016
I0201 00:21:08.780248 139702527031040 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.2752436399459839, loss=3.307581663131714
I0201 00:21:54.680032 139702543816448 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.1492018699645996, loss=3.300797462463379
I0201 00:22:40.499470 139702527031040 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.3516473770141602, loss=3.512800931930542
I0201 00:23:19.641601 139863983413056 spec.py:321] Evaluating on the training split.
I0201 00:23:30.235724 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 00:23:54.037827 139863983413056 spec.py:349] Evaluating on the test split.
I0201 00:23:55.681382 139863983413056 submission_runner.py:408] Time since start: 41401.18s, 	Step: 84287, 	{'train/accuracy': 0.7743945121765137, 'train/loss': 1.1085599660873413, 'validation/accuracy': 0.70305997133255, 'validation/loss': 1.4031741619110107, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 2.0119788646698, 'test/num_examples': 10000, 'score': 38267.15163445473, 'total_duration': 41401.175797224045, 'accumulated_submission_time': 38267.15163445473, 'accumulated_eval_time': 3126.1857776641846, 'accumulated_logging_time': 3.3865418434143066}
I0201 00:23:55.709344 139702543816448 logging_writer.py:48] [84287] accumulated_eval_time=3126.185778, accumulated_logging_time=3.386542, accumulated_submission_time=38267.151634, global_step=84287, preemption_count=0, score=38267.151634, test/accuracy=0.577700, test/loss=2.011979, test/num_examples=10000, total_duration=41401.175797, train/accuracy=0.774395, train/loss=1.108560, validation/accuracy=0.703060, validation/loss=1.403174, validation/num_examples=50000
I0201 00:24:01.297615 139702527031040 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.4248416423797607, loss=3.2753889560699463
I0201 00:24:43.063130 139702543816448 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.2671401500701904, loss=3.275160312652588
I0201 00:25:29.000205 139702527031040 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.2569754123687744, loss=3.2941205501556396
I0201 00:26:15.026115 139702543816448 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.3643560409545898, loss=3.3710687160491943
I0201 00:27:01.001478 139702527031040 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.205748200416565, loss=4.303630828857422
I0201 00:27:46.876737 139702543816448 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.4428409337997437, loss=3.4337189197540283
I0201 00:28:32.900165 139702527031040 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.27778959274292, loss=3.242125988006592
I0201 00:29:18.798621 139702543816448 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.2861207723617554, loss=3.373055934906006
I0201 00:30:04.737747 139702527031040 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.234121561050415, loss=3.9989237785339355
I0201 00:30:50.561024 139702543816448 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.3012421131134033, loss=3.368824005126953
I0201 00:30:55.722434 139863983413056 spec.py:321] Evaluating on the training split.
I0201 00:31:06.192620 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 00:31:27.136721 139863983413056 spec.py:349] Evaluating on the test split.
I0201 00:31:28.775164 139863983413056 submission_runner.py:408] Time since start: 41854.27s, 	Step: 85213, 	{'train/accuracy': 0.7837694883346558, 'train/loss': 1.0862317085266113, 'validation/accuracy': 0.7024999856948853, 'validation/loss': 1.429186224937439, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 2.0371885299682617, 'test/num_examples': 10000, 'score': 38687.10668325424, 'total_duration': 41854.26957678795, 'accumulated_submission_time': 38687.10668325424, 'accumulated_eval_time': 3159.238482236862, 'accumulated_logging_time': 3.423867702484131}
I0201 00:31:28.806204 139702527031040 logging_writer.py:48] [85213] accumulated_eval_time=3159.238482, accumulated_logging_time=3.423868, accumulated_submission_time=38687.106683, global_step=85213, preemption_count=0, score=38687.106683, test/accuracy=0.576300, test/loss=2.037189, test/num_examples=10000, total_duration=41854.269577, train/accuracy=0.783769, train/loss=1.086232, validation/accuracy=0.702500, validation/loss=1.429186, validation/num_examples=50000
I0201 00:32:04.142740 139702543816448 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.205941081047058, loss=4.649507999420166
I0201 00:32:49.775994 139702527031040 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.2237476110458374, loss=4.110640525817871
I0201 00:33:35.831754 139702543816448 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.2923914194107056, loss=3.5537924766540527
I0201 00:34:21.893712 139702527031040 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.1765127182006836, loss=3.679248571395874
I0201 00:35:07.600259 139702543816448 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.242931842803955, loss=3.522216320037842
I0201 00:35:53.012938 139702527031040 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.258074164390564, loss=3.4345192909240723
I0201 00:36:38.850437 139702543816448 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.4127107858657837, loss=4.9555840492248535
I0201 00:37:24.891482 139702527031040 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.3510217666625977, loss=4.986854076385498
I0201 00:38:10.707841 139702543816448 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.2041199207305908, loss=4.649993896484375
I0201 00:38:28.844198 139863983413056 spec.py:321] Evaluating on the training split.
I0201 00:38:39.693792 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 00:39:02.024177 139863983413056 spec.py:349] Evaluating on the test split.
I0201 00:39:03.660712 139863983413056 submission_runner.py:408] Time since start: 42309.16s, 	Step: 86141, 	{'train/accuracy': 0.7665038704872131, 'train/loss': 1.1527868509292603, 'validation/accuracy': 0.7004599571228027, 'validation/loss': 1.423740267753601, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 2.027710199356079, 'test/num_examples': 10000, 'score': 39107.087277412415, 'total_duration': 42309.15513443947, 'accumulated_submission_time': 39107.087277412415, 'accumulated_eval_time': 3194.0549857616425, 'accumulated_logging_time': 3.4639732837677}
I0201 00:39:03.688103 139702527031040 logging_writer.py:48] [86141] accumulated_eval_time=3194.054986, accumulated_logging_time=3.463973, accumulated_submission_time=39107.087277, global_step=86141, preemption_count=0, score=39107.087277, test/accuracy=0.575300, test/loss=2.027710, test/num_examples=10000, total_duration=42309.155134, train/accuracy=0.766504, train/loss=1.152787, validation/accuracy=0.700460, validation/loss=1.423740, validation/num_examples=50000
I0201 00:39:27.669657 139702543816448 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.2073533535003662, loss=4.6638360023498535
I0201 00:40:12.137188 139702527031040 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.2993175983428955, loss=3.3431098461151123
I0201 00:40:58.186493 139702543816448 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.197385311126709, loss=3.304152250289917
I0201 00:41:44.552965 139702527031040 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.3244965076446533, loss=3.4052577018737793
I0201 00:42:30.577708 139702543816448 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.093335509300232, loss=3.5342612266540527
I0201 00:43:16.263788 139702527031040 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.3169550895690918, loss=3.4121570587158203
I0201 00:44:02.189972 139702543816448 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.309037446975708, loss=3.435558795928955
I0201 00:44:47.872399 139702527031040 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.2719988822937012, loss=3.2760021686553955
I0201 00:45:33.506211 139702543816448 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.524784803390503, loss=5.162167549133301
I0201 00:46:03.908817 139863983413056 spec.py:321] Evaluating on the training split.
I0201 00:46:14.316128 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 00:46:36.023171 139863983413056 spec.py:349] Evaluating on the test split.
I0201 00:46:37.661295 139863983413056 submission_runner.py:408] Time since start: 42763.16s, 	Step: 87068, 	{'train/accuracy': 0.7691406011581421, 'train/loss': 1.0984034538269043, 'validation/accuracy': 0.7033999562263489, 'validation/loss': 1.3939647674560547, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 2.0015461444854736, 'test/num_examples': 10000, 'score': 39527.25010895729, 'total_duration': 42763.15571928024, 'accumulated_submission_time': 39527.25010895729, 'accumulated_eval_time': 3227.807467699051, 'accumulated_logging_time': 3.500558376312256}
I0201 00:46:37.692279 139702527031040 logging_writer.py:48] [87068] accumulated_eval_time=3227.807468, accumulated_logging_time=3.500558, accumulated_submission_time=39527.250109, global_step=87068, preemption_count=0, score=39527.250109, test/accuracy=0.581000, test/loss=2.001546, test/num_examples=10000, total_duration=42763.155719, train/accuracy=0.769141, train/loss=1.098403, validation/accuracy=0.703400, validation/loss=1.393965, validation/num_examples=50000
I0201 00:46:50.894978 139702543816448 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.1314697265625, loss=4.195196151733398
I0201 00:47:33.579397 139702527031040 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.299550175666809, loss=3.3429861068725586
I0201 00:48:19.846366 139702543816448 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.1637612581253052, loss=3.3211989402770996
I0201 00:49:06.175888 139702527031040 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.196856141090393, loss=3.2912282943725586
I0201 00:49:52.042258 139702543816448 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.1414875984191895, loss=4.481321811676025
I0201 00:50:38.323778 139702527031040 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.1459940671920776, loss=4.047951698303223
I0201 00:51:24.765690 139702543816448 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.2910642623901367, loss=3.29659104347229
I0201 00:52:10.536853 139702527031040 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.327597737312317, loss=3.275498867034912
I0201 00:52:56.747676 139702543816448 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.3575893640518188, loss=4.7341508865356445
I0201 00:53:37.704207 139863983413056 spec.py:321] Evaluating on the training split.
I0201 00:53:48.012637 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 00:54:12.417727 139863983413056 spec.py:349] Evaluating on the test split.
I0201 00:54:14.053236 139863983413056 submission_runner.py:408] Time since start: 43219.55s, 	Step: 87991, 	{'train/accuracy': 0.7862499952316284, 'train/loss': 1.0665332078933716, 'validation/accuracy': 0.7097600102424622, 'validation/loss': 1.3905929327011108, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.9906413555145264, 'test/num_examples': 10000, 'score': 39947.20313882828, 'total_duration': 43219.547652721405, 'accumulated_submission_time': 39947.20313882828, 'accumulated_eval_time': 3264.1564960479736, 'accumulated_logging_time': 3.542058229446411}
I0201 00:54:14.081893 139702527031040 logging_writer.py:48] [87991] accumulated_eval_time=3264.156496, accumulated_logging_time=3.542058, accumulated_submission_time=39947.203139, global_step=87991, preemption_count=0, score=39947.203139, test/accuracy=0.586800, test/loss=1.990641, test/num_examples=10000, total_duration=43219.547653, train/accuracy=0.786250, train/loss=1.066533, validation/accuracy=0.709760, validation/loss=1.390593, validation/num_examples=50000
I0201 00:54:18.071380 139702543816448 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.3770726919174194, loss=4.7052812576293945
I0201 00:54:59.651146 139702527031040 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.3230223655700684, loss=3.5053577423095703
I0201 00:55:45.350358 139702543816448 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.3469468355178833, loss=3.349552631378174
I0201 00:56:31.395897 139702527031040 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.21968412399292, loss=3.265015125274658
I0201 00:57:17.243921 139702543816448 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.2414181232452393, loss=3.415390729904175
I0201 00:58:03.111850 139702527031040 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.1952039003372192, loss=3.6217713356018066
I0201 00:58:48.896579 139702543816448 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.2578728199005127, loss=3.3027026653289795
I0201 00:59:34.825397 139702527031040 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.3180043697357178, loss=4.389062881469727
I0201 01:00:20.809037 139702543816448 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.3493661880493164, loss=4.903987884521484
I0201 01:01:06.865185 139702527031040 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.2509489059448242, loss=3.4983229637145996
I0201 01:01:14.400238 139863983413056 spec.py:321] Evaluating on the training split.
I0201 01:01:24.984784 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 01:01:45.906524 139863983413056 spec.py:349] Evaluating on the test split.
I0201 01:01:47.540141 139863983413056 submission_runner.py:408] Time since start: 43673.03s, 	Step: 88918, 	{'train/accuracy': 0.7696874737739563, 'train/loss': 1.142638087272644, 'validation/accuracy': 0.7088800072669983, 'validation/loss': 1.4129151105880737, 'validation/num_examples': 50000, 'test/accuracy': 0.5790000557899475, 'test/loss': 2.0272250175476074, 'test/num_examples': 10000, 'score': 40367.463129758835, 'total_duration': 43673.03454852104, 'accumulated_submission_time': 40367.463129758835, 'accumulated_eval_time': 3297.296382665634, 'accumulated_logging_time': 3.579483985900879}
I0201 01:01:47.571966 139702543816448 logging_writer.py:48] [88918] accumulated_eval_time=3297.296383, accumulated_logging_time=3.579484, accumulated_submission_time=40367.463130, global_step=88918, preemption_count=0, score=40367.463130, test/accuracy=0.579000, test/loss=2.027225, test/num_examples=10000, total_duration=43673.034549, train/accuracy=0.769687, train/loss=1.142638, validation/accuracy=0.708880, validation/loss=1.412915, validation/num_examples=50000
I0201 01:02:20.892553 139702527031040 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.2676180601119995, loss=3.9422006607055664
I0201 01:03:06.142706 139702543816448 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.1731839179992676, loss=4.119116306304932
I0201 01:03:51.992007 139702527031040 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.2494730949401855, loss=3.2897987365722656
I0201 01:04:37.953846 139702543816448 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.2889350652694702, loss=3.312260150909424
I0201 01:05:23.641491 139702527031040 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.2639764547348022, loss=3.383845090866089
I0201 01:06:09.533331 139702543816448 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.2672268152236938, loss=3.3605284690856934
I0201 01:06:55.267158 139702527031040 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.133972406387329, loss=3.604414463043213
I0201 01:07:41.078202 139702543816448 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.2637531757354736, loss=3.283111572265625
I0201 01:08:26.715156 139702527031040 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.2056772708892822, loss=3.5887844562530518
I0201 01:08:47.858272 139863983413056 spec.py:321] Evaluating on the training split.
I0201 01:08:58.231308 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 01:09:21.303544 139863983413056 spec.py:349] Evaluating on the test split.
I0201 01:09:22.942058 139863983413056 submission_runner.py:408] Time since start: 44128.44s, 	Step: 89848, 	{'train/accuracy': 0.7753515243530273, 'train/loss': 1.1038709878921509, 'validation/accuracy': 0.7093200087547302, 'validation/loss': 1.3977713584899902, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 2.0033745765686035, 'test/num_examples': 10000, 'score': 40787.6904566288, 'total_duration': 44128.43647813797, 'accumulated_submission_time': 40787.6904566288, 'accumulated_eval_time': 3332.3801724910736, 'accumulated_logging_time': 3.621098756790161}
I0201 01:09:22.970556 139702543816448 logging_writer.py:48] [89848] accumulated_eval_time=3332.380172, accumulated_logging_time=3.621099, accumulated_submission_time=40787.690457, global_step=89848, preemption_count=0, score=40787.690457, test/accuracy=0.585100, test/loss=2.003375, test/num_examples=10000, total_duration=44128.436478, train/accuracy=0.775352, train/loss=1.103871, validation/accuracy=0.709320, validation/loss=1.397771, validation/num_examples=50000
I0201 01:09:44.126083 139702527031040 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.2461297512054443, loss=3.2673490047454834
I0201 01:10:28.280930 139702543816448 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.3082153797149658, loss=3.2456305027008057
I0201 01:11:14.524897 139702527031040 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.2786531448364258, loss=3.294753074645996
I0201 01:12:00.671425 139702543816448 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.3226075172424316, loss=3.2182672023773193
I0201 01:12:46.531138 139702527031040 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.2708505392074585, loss=3.3248777389526367
I0201 01:13:32.440454 139702543816448 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.232645034790039, loss=3.5132110118865967
I0201 01:14:18.240993 139702527031040 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.297008752822876, loss=3.478654623031616
I0201 01:15:04.155153 139702543816448 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.2475734949111938, loss=3.7215535640716553
I0201 01:15:49.826491 139702527031040 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.3634753227233887, loss=3.369929313659668
I0201 01:16:22.957765 139863983413056 spec.py:321] Evaluating on the training split.
I0201 01:16:33.302702 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 01:16:54.904388 139863983413056 spec.py:349] Evaluating on the test split.
I0201 01:16:56.553474 139863983413056 submission_runner.py:408] Time since start: 44582.05s, 	Step: 90774, 	{'train/accuracy': 0.781054675579071, 'train/loss': 1.074851155281067, 'validation/accuracy': 0.7090799808502197, 'validation/loss': 1.384374976158142, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.9843614101409912, 'test/num_examples': 10000, 'score': 41207.61959028244, 'total_duration': 44582.04789733887, 'accumulated_submission_time': 41207.61959028244, 'accumulated_eval_time': 3365.9759092330933, 'accumulated_logging_time': 3.6582560539245605}
I0201 01:16:56.585353 139702543816448 logging_writer.py:48] [90774] accumulated_eval_time=3365.975909, accumulated_logging_time=3.658256, accumulated_submission_time=41207.619590, global_step=90774, preemption_count=0, score=41207.619590, test/accuracy=0.587800, test/loss=1.984361, test/num_examples=10000, total_duration=44582.047897, train/accuracy=0.781055, train/loss=1.074851, validation/accuracy=0.709080, validation/loss=1.384375, validation/num_examples=50000
I0201 01:17:07.362161 139702527031040 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.3122495412826538, loss=3.403665542602539
I0201 01:17:49.538640 139702543816448 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.405626893043518, loss=5.023667812347412
I0201 01:18:35.391690 139702527031040 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.268945574760437, loss=3.297597646713257
I0201 01:19:21.702446 139702543816448 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.3167237043380737, loss=3.355315685272217
I0201 01:20:07.722300 139702527031040 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.3005430698394775, loss=3.6629557609558105
I0201 01:20:53.711269 139702543816448 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.2102099657058716, loss=3.4193074703216553
I0201 01:21:39.746036 139702527031040 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.455915093421936, loss=3.356208324432373
I0201 01:22:25.376398 139702543816448 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.211809515953064, loss=4.2457122802734375
I0201 01:23:11.126076 139702527031040 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.322735071182251, loss=3.4753217697143555
I0201 01:23:57.109771 139702543816448 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.2156503200531006, loss=3.7373158931732178
I0201 01:23:57.122490 139863983413056 spec.py:321] Evaluating on the training split.
I0201 01:24:07.280886 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 01:24:29.155036 139863983413056 spec.py:349] Evaluating on the test split.
I0201 01:24:30.791512 139863983413056 submission_runner.py:408] Time since start: 45036.29s, 	Step: 91701, 	{'train/accuracy': 0.77406245470047, 'train/loss': 1.1004438400268555, 'validation/accuracy': 0.7123000025749207, 'validation/loss': 1.3684351444244385, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.9665168523788452, 'test/num_examples': 10000, 'score': 41628.0981926918, 'total_duration': 45036.28593277931, 'accumulated_submission_time': 41628.0981926918, 'accumulated_eval_time': 3399.644933462143, 'accumulated_logging_time': 3.6996939182281494}
I0201 01:24:30.823770 139702527031040 logging_writer.py:48] [91701] accumulated_eval_time=3399.644933, accumulated_logging_time=3.699694, accumulated_submission_time=41628.098193, global_step=91701, preemption_count=0, score=41628.098193, test/accuracy=0.590900, test/loss=1.966517, test/num_examples=10000, total_duration=45036.285933, train/accuracy=0.774062, train/loss=1.100444, validation/accuracy=0.712300, validation/loss=1.368435, validation/num_examples=50000
I0201 01:25:11.644481 139702543816448 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.5140129327774048, loss=5.054346084594727
I0201 01:25:57.152661 139702527031040 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.376838207244873, loss=4.758243560791016
I0201 01:26:43.217535 139702543816448 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.2553986310958862, loss=3.190920114517212
I0201 01:27:28.895009 139702527031040 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.5284316539764404, loss=3.3283259868621826
I0201 01:28:14.717550 139702543816448 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.3365148305892944, loss=3.5537381172180176
I0201 01:29:00.661726 139702527031040 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.3632721900939941, loss=4.827126502990723
I0201 01:29:46.460908 139702543816448 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.2935619354248047, loss=3.339533567428589
I0201 01:30:32.463292 139702527031040 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.1987853050231934, loss=3.3184001445770264
I0201 01:31:18.573082 139702543816448 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.2916748523712158, loss=3.770881414413452
I0201 01:31:31.124597 139863983413056 spec.py:321] Evaluating on the training split.
I0201 01:31:41.579758 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 01:31:58.480396 139863983413056 spec.py:349] Evaluating on the test split.
I0201 01:32:00.129297 139863983413056 submission_runner.py:408] Time since start: 45485.62s, 	Step: 92629, 	{'train/accuracy': 0.7764062285423279, 'train/loss': 1.1032389402389526, 'validation/accuracy': 0.7084800004959106, 'validation/loss': 1.3871572017669678, 'validation/num_examples': 50000, 'test/accuracy': 0.5853000283241272, 'test/loss': 1.97959566116333, 'test/num_examples': 10000, 'score': 42048.34095311165, 'total_duration': 45485.623708724976, 'accumulated_submission_time': 42048.34095311165, 'accumulated_eval_time': 3428.649636030197, 'accumulated_logging_time': 3.740588903427124}
I0201 01:32:00.163222 139702527031040 logging_writer.py:48] [92629] accumulated_eval_time=3428.649636, accumulated_logging_time=3.740589, accumulated_submission_time=42048.340953, global_step=92629, preemption_count=0, score=42048.340953, test/accuracy=0.585300, test/loss=1.979596, test/num_examples=10000, total_duration=45485.623709, train/accuracy=0.776406, train/loss=1.103239, validation/accuracy=0.708480, validation/loss=1.387157, validation/num_examples=50000
I0201 01:32:29.367808 139702543816448 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.4248733520507812, loss=3.2831358909606934
I0201 01:33:15.200993 139702527031040 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.3089627027511597, loss=3.3204421997070312
I0201 01:34:00.997003 139702543816448 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.2913562059402466, loss=4.3779377937316895
I0201 01:34:46.908600 139702527031040 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.460473895072937, loss=3.4024126529693604
I0201 01:35:32.923474 139702543816448 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.3146239519119263, loss=3.765955686569214
I0201 01:36:19.013438 139702527031040 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.436560869216919, loss=3.276155710220337
I0201 01:37:05.062283 139702543816448 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.4210760593414307, loss=4.981224060058594
I0201 01:37:50.942541 139702527031040 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.1892412900924683, loss=3.787559986114502
I0201 01:38:36.805845 139702543816448 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.3214668035507202, loss=3.3286359310150146
I0201 01:39:00.286916 139863983413056 spec.py:321] Evaluating on the training split.
I0201 01:39:10.848174 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 01:39:37.586009 139863983413056 spec.py:349] Evaluating on the test split.
I0201 01:39:39.231154 139863983413056 submission_runner.py:408] Time since start: 45944.73s, 	Step: 93553, 	{'train/accuracy': 0.7830468416213989, 'train/loss': 1.04946768283844, 'validation/accuracy': 0.7099199891090393, 'validation/loss': 1.3628854751586914, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.9714946746826172, 'test/num_examples': 10000, 'score': 42468.40495491028, 'total_duration': 45944.72555589676, 'accumulated_submission_time': 42468.40495491028, 'accumulated_eval_time': 3467.5938584804535, 'accumulated_logging_time': 3.784477949142456}
I0201 01:39:39.263966 139702527031040 logging_writer.py:48] [93553] accumulated_eval_time=3467.593858, accumulated_logging_time=3.784478, accumulated_submission_time=42468.404955, global_step=93553, preemption_count=0, score=42468.404955, test/accuracy=0.587300, test/loss=1.971495, test/num_examples=10000, total_duration=45944.725556, train/accuracy=0.783047, train/loss=1.049468, validation/accuracy=0.709920, validation/loss=1.362885, validation/num_examples=50000
I0201 01:39:58.443631 139702543816448 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.3324651718139648, loss=3.2959742546081543
I0201 01:40:42.037847 139702527031040 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.4799338579177856, loss=3.299356698989868
I0201 01:41:28.167443 139702543816448 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.2197200059890747, loss=4.228960990905762
I0201 01:42:14.382134 139702527031040 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.2442831993103027, loss=3.767174005508423
I0201 01:43:00.211201 139702543816448 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.2839710712432861, loss=3.2485246658325195
I0201 01:43:46.209265 139702527031040 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.2021878957748413, loss=3.1220755577087402
I0201 01:44:32.405529 139702543816448 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.4415571689605713, loss=4.732667446136475
I0201 01:45:18.107784 139702527031040 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.2080224752426147, loss=4.29007625579834
I0201 01:46:04.167974 139702543816448 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.329354166984558, loss=3.208220958709717
I0201 01:46:39.546726 139863983413056 spec.py:321] Evaluating on the training split.
I0201 01:46:49.736490 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 01:47:13.205880 139863983413056 spec.py:349] Evaluating on the test split.
I0201 01:47:14.855120 139863983413056 submission_runner.py:408] Time since start: 46400.35s, 	Step: 94479, 	{'train/accuracy': 0.8056640625, 'train/loss': 0.9454649686813354, 'validation/accuracy': 0.718459963798523, 'validation/loss': 1.3121576309204102, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.9169158935546875, 'test/num_examples': 10000, 'score': 42888.62809586525, 'total_duration': 46400.34952402115, 'accumulated_submission_time': 42888.62809586525, 'accumulated_eval_time': 3502.9022257328033, 'accumulated_logging_time': 3.8288791179656982}
I0201 01:47:14.891127 139702527031040 logging_writer.py:48] [94479] accumulated_eval_time=3502.902226, accumulated_logging_time=3.828879, accumulated_submission_time=42888.628096, global_step=94479, preemption_count=0, score=42888.628096, test/accuracy=0.593300, test/loss=1.916916, test/num_examples=10000, total_duration=46400.349524, train/accuracy=0.805664, train/loss=0.945465, validation/accuracy=0.718460, validation/loss=1.312158, validation/num_examples=50000
I0201 01:47:23.671537 139702543816448 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.3876246213912964, loss=3.2336957454681396
I0201 01:48:05.552723 139702527031040 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.3406620025634766, loss=4.851367473602295
I0201 01:48:51.328982 139702543816448 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.3165944814682007, loss=3.217254161834717
I0201 01:49:37.247811 139702527031040 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.3184382915496826, loss=3.9242374897003174
I0201 01:50:23.468981 139702543816448 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.3868554830551147, loss=4.782121658325195
I0201 01:51:09.204809 139702527031040 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.5527772903442383, loss=4.8241963386535645
I0201 01:51:55.412690 139702543816448 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.3526397943496704, loss=3.348182439804077
I0201 01:52:41.210475 139702527031040 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.3353785276412964, loss=3.209501266479492
I0201 01:53:27.101158 139702543816448 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.45439875125885, loss=3.3000986576080322
I0201 01:54:12.825378 139702527031040 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.2114583253860474, loss=3.8150651454925537
I0201 01:54:15.141005 139863983413056 spec.py:321] Evaluating on the training split.
I0201 01:54:25.724622 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 01:54:47.378092 139863983413056 spec.py:349] Evaluating on the test split.
I0201 01:54:49.010084 139863983413056 submission_runner.py:408] Time since start: 46854.50s, 	Step: 95407, 	{'train/accuracy': 0.7821093797683716, 'train/loss': 1.0739256143569946, 'validation/accuracy': 0.7144799828529358, 'validation/loss': 1.3560128211975098, 'validation/num_examples': 50000, 'test/accuracy': 0.5925000309944153, 'test/loss': 1.9520272016525269, 'test/num_examples': 10000, 'score': 43308.81820011139, 'total_duration': 46854.504509449005, 'accumulated_submission_time': 43308.81820011139, 'accumulated_eval_time': 3536.771305322647, 'accumulated_logging_time': 3.8754472732543945}
I0201 01:54:49.039241 139702543816448 logging_writer.py:48] [95407] accumulated_eval_time=3536.771305, accumulated_logging_time=3.875447, accumulated_submission_time=43308.818200, global_step=95407, preemption_count=0, score=43308.818200, test/accuracy=0.592500, test/loss=1.952027, test/num_examples=10000, total_duration=46854.504509, train/accuracy=0.782109, train/loss=1.073926, validation/accuracy=0.714480, validation/loss=1.356013, validation/num_examples=50000
I0201 01:55:26.782793 139702527031040 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.3667067289352417, loss=3.285115957260132
I0201 01:56:12.699779 139702543816448 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.4924653768539429, loss=3.4348506927490234
I0201 01:56:58.582741 139702527031040 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.2814223766326904, loss=3.408250093460083
I0201 01:57:44.463244 139702543816448 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.3551002740859985, loss=3.2770237922668457
I0201 01:58:30.176637 139702527031040 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.3653607368469238, loss=4.898946762084961
I0201 01:59:15.999898 139702543816448 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.350911021232605, loss=3.196732521057129
I0201 02:00:01.903875 139702527031040 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.5090619325637817, loss=4.92788028717041
I0201 02:00:47.886882 139702543816448 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.2620385885238647, loss=4.310494899749756
I0201 02:01:33.901670 139702527031040 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.3053092956542969, loss=3.264132022857666
I0201 02:01:49.299853 139863983413056 spec.py:321] Evaluating on the training split.
I0201 02:01:59.732709 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 02:02:23.321205 139863983413056 spec.py:349] Evaluating on the test split.
I0201 02:02:24.959006 139863983413056 submission_runner.py:408] Time since start: 47310.45s, 	Step: 96335, 	{'train/accuracy': 0.78955078125, 'train/loss': 1.0335056781768799, 'validation/accuracy': 0.7170000076293945, 'validation/loss': 1.3376338481903076, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.9266635179519653, 'test/num_examples': 10000, 'score': 43729.01921200752, 'total_duration': 47310.45342755318, 'accumulated_submission_time': 43729.01921200752, 'accumulated_eval_time': 3572.430454492569, 'accumulated_logging_time': 3.9143614768981934}
I0201 02:02:24.992022 139702543816448 logging_writer.py:48] [96335] accumulated_eval_time=3572.430454, accumulated_logging_time=3.914361, accumulated_submission_time=43729.019212, global_step=96335, preemption_count=0, score=43729.019212, test/accuracy=0.596600, test/loss=1.926664, test/num_examples=10000, total_duration=47310.453428, train/accuracy=0.789551, train/loss=1.033506, validation/accuracy=0.717000, validation/loss=1.337634, validation/num_examples=50000
I0201 02:02:51.349585 139702527031040 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.293853521347046, loss=3.641835927963257
I0201 02:03:36.109976 139702543816448 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.4886724948883057, loss=4.754251003265381
I0201 02:04:22.185058 139702527031040 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.3440430164337158, loss=3.257495880126953
I0201 02:05:08.250353 139702543816448 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.4620023965835571, loss=4.920783996582031
I0201 02:05:53.957667 139702527031040 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.3712563514709473, loss=3.351046085357666
I0201 02:06:39.802776 139702543816448 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.3659154176712036, loss=3.2439827919006348
I0201 02:07:25.626425 139702527031040 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.2716960906982422, loss=3.2238211631774902
I0201 02:08:11.545498 139702543816448 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.1945042610168457, loss=3.936612367630005
I0201 02:08:57.490525 139702527031040 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.3332417011260986, loss=3.3364675045013428
I0201 02:09:25.220358 139863983413056 spec.py:321] Evaluating on the training split.
I0201 02:09:35.528128 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 02:09:57.839135 139863983413056 spec.py:349] Evaluating on the test split.
I0201 02:09:59.473098 139863983413056 submission_runner.py:408] Time since start: 47764.97s, 	Step: 97262, 	{'train/accuracy': 0.7940233945846558, 'train/loss': 1.0544887781143188, 'validation/accuracy': 0.7132399678230286, 'validation/loss': 1.3924885988235474, 'validation/num_examples': 50000, 'test/accuracy': 0.5884000062942505, 'test/loss': 1.9976340532302856, 'test/num_examples': 10000, 'score': 44149.1900267601, 'total_duration': 47764.96751379967, 'accumulated_submission_time': 44149.1900267601, 'accumulated_eval_time': 3606.683182001114, 'accumulated_logging_time': 3.956416368484497}
I0201 02:09:59.501948 139702543816448 logging_writer.py:48] [97262] accumulated_eval_time=3606.683182, accumulated_logging_time=3.956416, accumulated_submission_time=44149.190027, global_step=97262, preemption_count=0, score=44149.190027, test/accuracy=0.588400, test/loss=1.997634, test/num_examples=10000, total_duration=47764.967514, train/accuracy=0.794023, train/loss=1.054489, validation/accuracy=0.713240, validation/loss=1.392489, validation/num_examples=50000
I0201 02:10:15.062185 139702527031040 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.3045240640640259, loss=3.1354916095733643
I0201 02:10:58.057005 139702543816448 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.4297995567321777, loss=3.1905770301818848
I0201 02:11:44.055354 139702527031040 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.2256059646606445, loss=3.601526975631714
I0201 02:12:30.539424 139702543816448 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.266805648803711, loss=3.2998600006103516
I0201 02:13:16.401427 139702527031040 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.2627683877944946, loss=4.284598350524902
I0201 02:14:02.137131 139702543816448 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.2528342008590698, loss=4.281927585601807
I0201 02:14:48.293497 139702527031040 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.298073649406433, loss=4.232852935791016
I0201 02:15:33.952706 139702543816448 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.5073426961898804, loss=4.862554550170898
I0201 02:16:20.114406 139702527031040 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.2937496900558472, loss=3.2488853931427
I0201 02:16:59.814993 139863983413056 spec.py:321] Evaluating on the training split.
I0201 02:17:10.336574 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 02:17:30.812699 139863983413056 spec.py:349] Evaluating on the test split.
I0201 02:17:32.454790 139863983413056 submission_runner.py:408] Time since start: 48217.95s, 	Step: 98188, 	{'train/accuracy': 0.783886730670929, 'train/loss': 1.0592858791351318, 'validation/accuracy': 0.717519998550415, 'validation/loss': 1.3481186628341675, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.9524030685424805, 'test/num_examples': 10000, 'score': 44569.443935871124, 'total_duration': 48217.949201345444, 'accumulated_submission_time': 44569.443935871124, 'accumulated_eval_time': 3639.32297372818, 'accumulated_logging_time': 3.995898723602295}
I0201 02:17:32.485762 139702543816448 logging_writer.py:48] [98188] accumulated_eval_time=3639.322974, accumulated_logging_time=3.995899, accumulated_submission_time=44569.443936, global_step=98188, preemption_count=0, score=44569.443936, test/accuracy=0.591900, test/loss=1.952403, test/num_examples=10000, total_duration=48217.949201, train/accuracy=0.783887, train/loss=1.059286, validation/accuracy=0.717520, validation/loss=1.348119, validation/num_examples=50000
I0201 02:17:37.671693 139702527031040 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.3123600482940674, loss=3.630701780319214
I0201 02:18:19.648926 139702543816448 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.2202214002609253, loss=3.803837299346924
I0201 02:19:05.344715 139702527031040 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.4297529458999634, loss=3.236840009689331
I0201 02:19:51.085030 139702543816448 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.356877088546753, loss=4.243945598602295
I0201 02:20:37.075670 139702527031040 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.3258575201034546, loss=3.180598735809326
I0201 02:21:22.785795 139702543816448 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.3658338785171509, loss=3.2861645221710205
I0201 02:22:08.599342 139702527031040 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.5328290462493896, loss=4.707247257232666
I0201 02:22:54.671470 139702543816448 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.3391395807266235, loss=3.2349658012390137
I0201 02:23:40.156921 139702527031040 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.2780975103378296, loss=3.7296483516693115
I0201 02:24:26.069237 139702543816448 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.3388457298278809, loss=3.8081955909729004
I0201 02:24:32.576148 139863983413056 spec.py:321] Evaluating on the training split.
I0201 02:24:43.040728 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 02:25:05.879505 139863983413056 spec.py:349] Evaluating on the test split.
I0201 02:25:07.515319 139863983413056 submission_runner.py:408] Time since start: 48673.01s, 	Step: 99116, 	{'train/accuracy': 0.7895702719688416, 'train/loss': 1.0405603647232056, 'validation/accuracy': 0.7199599742889404, 'validation/loss': 1.3382598161697388, 'validation/num_examples': 50000, 'test/accuracy': 0.5948000550270081, 'test/loss': 1.9383907318115234, 'test/num_examples': 10000, 'score': 44989.47699189186, 'total_duration': 48673.00974225998, 'accumulated_submission_time': 44989.47699189186, 'accumulated_eval_time': 3674.262171983719, 'accumulated_logging_time': 4.035337209701538}
I0201 02:25:07.546062 139702527031040 logging_writer.py:48] [99116] accumulated_eval_time=3674.262172, accumulated_logging_time=4.035337, accumulated_submission_time=44989.476992, global_step=99116, preemption_count=0, score=44989.476992, test/accuracy=0.594800, test/loss=1.938391, test/num_examples=10000, total_duration=48673.009742, train/accuracy=0.789570, train/loss=1.040560, validation/accuracy=0.719960, validation/loss=1.338260, validation/num_examples=50000
I0201 02:25:41.454183 139702543816448 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.2476818561553955, loss=3.9644880294799805
I0201 02:26:26.945460 139702527031040 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.260898470878601, loss=3.979793071746826
I0201 02:27:13.061663 139702543816448 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.2783185243606567, loss=4.442073345184326
I0201 02:27:59.015007 139702527031040 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.3970993757247925, loss=3.2614426612854004
I0201 02:28:44.458259 139702543816448 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.3094745874404907, loss=3.2018392086029053
I0201 02:29:30.213209 139702527031040 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.350946068763733, loss=3.948915958404541
I0201 02:30:16.276053 139702543816448 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.5813958644866943, loss=4.916287899017334
I0201 02:31:02.066615 139702527031040 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.438024640083313, loss=3.2580769062042236
I0201 02:31:47.929102 139702543816448 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.4308829307556152, loss=3.4468436241149902
I0201 02:32:07.823774 139863983413056 spec.py:321] Evaluating on the training split.
I0201 02:32:18.159054 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 02:32:39.541242 139863983413056 spec.py:349] Evaluating on the test split.
I0201 02:32:41.184364 139863983413056 submission_runner.py:408] Time since start: 49126.68s, 	Step: 100045, 	{'train/accuracy': 0.7934765219688416, 'train/loss': 1.03852379322052, 'validation/accuracy': 0.719980001449585, 'validation/loss': 1.356076717376709, 'validation/num_examples': 50000, 'test/accuracy': 0.5946000218391418, 'test/loss': 1.9406942129135132, 'test/num_examples': 10000, 'score': 45409.69598340988, 'total_duration': 49126.67878437042, 'accumulated_submission_time': 45409.69598340988, 'accumulated_eval_time': 3707.622751235962, 'accumulated_logging_time': 4.07489013671875}
I0201 02:32:41.214447 139702527031040 logging_writer.py:48] [100045] accumulated_eval_time=3707.622751, accumulated_logging_time=4.074890, accumulated_submission_time=45409.695983, global_step=100045, preemption_count=0, score=45409.695983, test/accuracy=0.594600, test/loss=1.940694, test/num_examples=10000, total_duration=49126.678784, train/accuracy=0.793477, train/loss=1.038524, validation/accuracy=0.719980, validation/loss=1.356077, validation/num_examples=50000
I0201 02:33:03.564558 139702543816448 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.337761402130127, loss=3.232912063598633
I0201 02:33:47.333634 139702527031040 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.3750896453857422, loss=4.261446952819824
I0201 02:34:33.201294 139702543816448 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.4843398332595825, loss=4.709331512451172
I0201 02:35:19.350559 139702527031040 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.394845962524414, loss=3.361741065979004
I0201 02:36:05.208204 139702543816448 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.2684381008148193, loss=3.292369842529297
I0201 02:36:50.986171 139702527031040 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.428698182106018, loss=4.56229305267334
I0201 02:37:36.934827 139702543816448 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.2724552154541016, loss=3.4385430812835693
I0201 02:38:22.605003 139702527031040 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.318197250366211, loss=3.275261163711548
I0201 02:39:08.932759 139702543816448 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.3077865839004517, loss=3.2165467739105225
I0201 02:39:41.429600 139863983413056 spec.py:321] Evaluating on the training split.
I0201 02:39:52.366343 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 02:40:14.643673 139863983413056 spec.py:349] Evaluating on the test split.
I0201 02:40:16.273688 139863983413056 submission_runner.py:408] Time since start: 49581.77s, 	Step: 100973, 	{'train/accuracy': 0.7863867282867432, 'train/loss': 1.0272761583328247, 'validation/accuracy': 0.7179799675941467, 'validation/loss': 1.3234541416168213, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.9154317378997803, 'test/num_examples': 10000, 'score': 45829.853684186935, 'total_duration': 49581.768104314804, 'accumulated_submission_time': 45829.853684186935, 'accumulated_eval_time': 3742.466834306717, 'accumulated_logging_time': 4.11404824256897}
I0201 02:40:16.306343 139702527031040 logging_writer.py:48] [100973] accumulated_eval_time=3742.466834, accumulated_logging_time=4.114048, accumulated_submission_time=45829.853684, global_step=100973, preemption_count=0, score=45829.853684, test/accuracy=0.591800, test/loss=1.915432, test/num_examples=10000, total_duration=49581.768104, train/accuracy=0.786387, train/loss=1.027276, validation/accuracy=0.717980, validation/loss=1.323454, validation/num_examples=50000
I0201 02:40:27.473126 139702543816448 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.5851032733917236, loss=4.89009952545166
I0201 02:41:10.213624 139702527031040 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.4398545026779175, loss=3.866236448287964
I0201 02:41:55.787403 139702543816448 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.412314772605896, loss=3.2400624752044678
I0201 02:42:41.922516 139702527031040 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.259544849395752, loss=3.3930482864379883
I0201 02:43:27.960650 139702543816448 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.4215201139450073, loss=3.253009796142578
I0201 02:44:13.640135 139702527031040 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.2899140119552612, loss=3.408308267593384
I0201 02:44:59.502390 139702543816448 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.3853689432144165, loss=3.2590160369873047
I0201 02:45:45.527679 139702527031040 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.570931077003479, loss=3.199510097503662
I0201 02:46:31.047662 139702543816448 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.3608876466751099, loss=4.43269157409668
I0201 02:47:17.073414 139702527031040 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.3428645133972168, loss=3.234499454498291
I0201 02:47:17.087722 139863983413056 spec.py:321] Evaluating on the training split.
I0201 02:47:27.879009 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 02:47:48.217315 139863983413056 spec.py:349] Evaluating on the test split.
I0201 02:47:49.858051 139863983413056 submission_runner.py:408] Time since start: 50035.35s, 	Step: 101901, 	{'train/accuracy': 0.7957617044448853, 'train/loss': 1.0064743757247925, 'validation/accuracy': 0.7209399938583374, 'validation/loss': 1.316093921661377, 'validation/num_examples': 50000, 'test/accuracy': 0.598300039768219, 'test/loss': 1.912156343460083, 'test/num_examples': 10000, 'score': 46250.57767724991, 'total_duration': 50035.35246872902, 'accumulated_submission_time': 46250.57767724991, 'accumulated_eval_time': 3775.237138032913, 'accumulated_logging_time': 4.155972242355347}
I0201 02:47:49.890547 139702543816448 logging_writer.py:48] [101901] accumulated_eval_time=3775.237138, accumulated_logging_time=4.155972, accumulated_submission_time=46250.577677, global_step=101901, preemption_count=0, score=46250.577677, test/accuracy=0.598300, test/loss=1.912156, test/num_examples=10000, total_duration=50035.352469, train/accuracy=0.795762, train/loss=1.006474, validation/accuracy=0.720940, validation/loss=1.316094, validation/num_examples=50000
I0201 02:48:30.575913 139702527031040 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.375095248222351, loss=4.817529678344727
I0201 02:49:16.317845 139702543816448 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.7506120204925537, loss=4.842958927154541
I0201 02:50:02.461056 139702527031040 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.4207757711410522, loss=3.218968391418457
I0201 02:50:48.344861 139702543816448 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.392716646194458, loss=3.335533618927002
I0201 02:51:34.158974 139702527031040 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.6350350379943848, loss=4.873836040496826
I0201 02:52:20.191684 139702543816448 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.5271916389465332, loss=4.619390964508057
I0201 02:53:06.321123 139702527031040 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.3658473491668701, loss=3.2390286922454834
I0201 02:53:52.276570 139702543816448 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.52895188331604, loss=4.61009407043457
I0201 02:54:38.155588 139702527031040 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.5252007246017456, loss=3.208611249923706
I0201 02:54:50.270437 139863983413056 spec.py:321] Evaluating on the training split.
I0201 02:55:00.738276 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 02:55:24.067903 139863983413056 spec.py:349] Evaluating on the test split.
I0201 02:55:25.696636 139863983413056 submission_runner.py:408] Time since start: 50491.19s, 	Step: 102828, 	{'train/accuracy': 0.7928906083106995, 'train/loss': 1.0309524536132812, 'validation/accuracy': 0.7185800075531006, 'validation/loss': 1.3496639728546143, 'validation/num_examples': 50000, 'test/accuracy': 0.5942000150680542, 'test/loss': 1.9573079347610474, 'test/num_examples': 10000, 'score': 46670.89849615097, 'total_duration': 50491.19104528427, 'accumulated_submission_time': 46670.89849615097, 'accumulated_eval_time': 3810.6633427143097, 'accumulated_logging_time': 4.199147939682007}
I0201 02:55:25.729235 139702543816448 logging_writer.py:48] [102828] accumulated_eval_time=3810.663343, accumulated_logging_time=4.199148, accumulated_submission_time=46670.898496, global_step=102828, preemption_count=0, score=46670.898496, test/accuracy=0.594200, test/loss=1.957308, test/num_examples=10000, total_duration=50491.191045, train/accuracy=0.792891, train/loss=1.030952, validation/accuracy=0.718580, validation/loss=1.349664, validation/num_examples=50000
I0201 02:55:54.897513 139702527031040 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.444226861000061, loss=4.581157684326172
I0201 02:56:40.358672 139702543816448 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.3769925832748413, loss=3.298649311065674
I0201 02:57:26.402412 139702527031040 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.2824684381484985, loss=3.395336627960205
I0201 02:58:12.657628 139702543816448 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.4899178743362427, loss=4.867713928222656
I0201 02:58:58.264325 139702527031040 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.2937251329421997, loss=3.7506704330444336
I0201 02:59:44.028841 139702543816448 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.4260441064834595, loss=3.2070586681365967
I0201 03:00:30.022242 139702527031040 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.4532594680786133, loss=3.297171115875244
I0201 03:01:16.029380 139702543816448 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.2494709491729736, loss=3.8662338256835938
I0201 03:02:02.008608 139702527031040 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.3687386512756348, loss=3.8679418563842773
I0201 03:02:25.953502 139863983413056 spec.py:321] Evaluating on the training split.
I0201 03:02:36.264436 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 03:02:57.215122 139863983413056 spec.py:349] Evaluating on the test split.
I0201 03:02:58.862836 139863983413056 submission_runner.py:408] Time since start: 50944.36s, 	Step: 103754, 	{'train/accuracy': 0.8130663633346558, 'train/loss': 0.960721492767334, 'validation/accuracy': 0.7226600050926208, 'validation/loss': 1.3447332382202148, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.9341633319854736, 'test/num_examples': 10000, 'score': 47091.06334114075, 'total_duration': 50944.35725951195, 'accumulated_submission_time': 47091.06334114075, 'accumulated_eval_time': 3843.5726778507233, 'accumulated_logging_time': 4.241317510604858}
I0201 03:02:58.894859 139702543816448 logging_writer.py:48] [103754] accumulated_eval_time=3843.572678, accumulated_logging_time=4.241318, accumulated_submission_time=47091.063341, global_step=103754, preemption_count=0, score=47091.063341, test/accuracy=0.599700, test/loss=1.934163, test/num_examples=10000, total_duration=50944.357260, train/accuracy=0.813066, train/loss=0.960721, validation/accuracy=0.722660, validation/loss=1.344733, validation/num_examples=50000
I0201 03:03:17.660234 139702527031040 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.337938904762268, loss=3.1950995922088623
I0201 03:04:01.277051 139702543816448 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.472203016281128, loss=3.248976469039917
I0201 03:04:47.075032 139702527031040 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.4538254737854004, loss=3.2103426456451416
I0201 03:05:33.167330 139702543816448 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.555978536605835, loss=4.835299968719482
I0201 03:06:18.907222 139702527031040 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.4905028343200684, loss=3.1299853324890137
I0201 03:07:04.803019 139702543816448 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.41865873336792, loss=3.254833936691284
I0201 03:07:50.525999 139702527031040 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.231290578842163, loss=3.601151704788208
I0201 03:08:36.356141 139702543816448 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.3468718528747559, loss=3.7166435718536377
I0201 03:09:22.493515 139702527031040 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.325035572052002, loss=3.1884074211120605
I0201 03:09:58.986296 139863983413056 spec.py:321] Evaluating on the training split.
I0201 03:10:09.446959 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 03:10:32.562032 139863983413056 spec.py:349] Evaluating on the test split.
I0201 03:10:34.193648 139863983413056 submission_runner.py:408] Time since start: 51399.69s, 	Step: 104681, 	{'train/accuracy': 0.79408198595047, 'train/loss': 1.0491050481796265, 'validation/accuracy': 0.7261199951171875, 'validation/loss': 1.340632438659668, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.9325157403945923, 'test/num_examples': 10000, 'score': 47511.09459543228, 'total_duration': 51399.688069581985, 'accumulated_submission_time': 47511.09459543228, 'accumulated_eval_time': 3878.7800257205963, 'accumulated_logging_time': 4.284324884414673}
I0201 03:10:34.223904 139702543816448 logging_writer.py:48] [104681] accumulated_eval_time=3878.780026, accumulated_logging_time=4.284325, accumulated_submission_time=47511.094595, global_step=104681, preemption_count=0, score=47511.094595, test/accuracy=0.602800, test/loss=1.932516, test/num_examples=10000, total_duration=51399.688070, train/accuracy=0.794082, train/loss=1.049105, validation/accuracy=0.726120, validation/loss=1.340632, validation/num_examples=50000
I0201 03:10:42.592291 139702527031040 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.3911261558532715, loss=3.1907715797424316
I0201 03:11:24.714903 139702543816448 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.4219281673431396, loss=4.3535614013671875
I0201 03:12:11.296000 139702527031040 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.4360530376434326, loss=3.2227067947387695
I0201 03:12:57.544281 139702543816448 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.4687715768814087, loss=3.219202995300293
I0201 03:13:43.891306 139702527031040 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.4453914165496826, loss=4.046621322631836
I0201 03:14:29.844360 139702543816448 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.2946009635925293, loss=3.8184797763824463
I0201 03:15:15.846698 139702527031040 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.3872848749160767, loss=3.2518489360809326
I0201 03:16:01.660455 139702543816448 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.385624885559082, loss=3.3356378078460693
I0201 03:16:47.553386 139702527031040 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.4899204969406128, loss=3.1615524291992188
I0201 03:17:33.671043 139702543816448 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.5479837656021118, loss=4.601451873779297
I0201 03:17:34.568610 139863983413056 spec.py:321] Evaluating on the training split.
I0201 03:17:45.679394 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 03:18:07.012287 139863983413056 spec.py:349] Evaluating on the test split.
I0201 03:18:08.653371 139863983413056 submission_runner.py:408] Time since start: 51854.15s, 	Step: 105604, 	{'train/accuracy': 0.7981054782867432, 'train/loss': 1.0043699741363525, 'validation/accuracy': 0.7231199741363525, 'validation/loss': 1.320081353187561, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.9280401468276978, 'test/num_examples': 10000, 'score': 47930.99238157272, 'total_duration': 51854.14779257774, 'accumulated_submission_time': 47930.99238157272, 'accumulated_eval_time': 3912.8647713661194, 'accumulated_logging_time': 4.712871551513672}
I0201 03:18:08.687398 139702527031040 logging_writer.py:48] [105604] accumulated_eval_time=3912.864771, accumulated_logging_time=4.712872, accumulated_submission_time=47930.992382, global_step=105604, preemption_count=0, score=47930.992382, test/accuracy=0.601600, test/loss=1.928040, test/num_examples=10000, total_duration=51854.147793, train/accuracy=0.798105, train/loss=1.004370, validation/accuracy=0.723120, validation/loss=1.320081, validation/num_examples=50000
I0201 03:18:47.974706 139702543816448 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.518415927886963, loss=4.742228031158447
I0201 03:19:33.703580 139702527031040 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.302924633026123, loss=3.799643039703369
I0201 03:20:20.025943 139702543816448 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.3428407907485962, loss=3.929126501083374
I0201 03:21:05.989480 139702527031040 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.4341554641723633, loss=3.217195987701416
I0201 03:21:51.960182 139702543816448 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.3614411354064941, loss=3.7577390670776367
I0201 03:22:37.813816 139702527031040 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.3835784196853638, loss=3.159956693649292
I0201 03:23:23.784328 139702543816448 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.5017683506011963, loss=3.1805710792541504
I0201 03:24:10.221098 139702527031040 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.4737355709075928, loss=3.219762086868286
I0201 03:24:56.267192 139702543816448 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.474761724472046, loss=3.2442352771759033
I0201 03:25:08.791818 139863983413056 spec.py:321] Evaluating on the training split.
I0201 03:25:19.483885 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 03:25:42.423174 139863983413056 spec.py:349] Evaluating on the test split.
I0201 03:25:44.056813 139863983413056 submission_runner.py:408] Time since start: 52309.55s, 	Step: 106529, 	{'train/accuracy': 0.8086132407188416, 'train/loss': 0.9574166536331177, 'validation/accuracy': 0.7298600077629089, 'validation/loss': 1.2995718717575073, 'validation/num_examples': 50000, 'test/accuracy': 0.603600025177002, 'test/loss': 1.904357671737671, 'test/num_examples': 10000, 'score': 48351.039007902145, 'total_duration': 52309.55121731758, 'accumulated_submission_time': 48351.039007902145, 'accumulated_eval_time': 3948.1297523975372, 'accumulated_logging_time': 4.755612373352051}
I0201 03:25:44.094969 139702527031040 logging_writer.py:48] [106529] accumulated_eval_time=3948.129752, accumulated_logging_time=4.755612, accumulated_submission_time=48351.039008, global_step=106529, preemption_count=0, score=48351.039008, test/accuracy=0.603600, test/loss=1.904358, test/num_examples=10000, total_duration=52309.551217, train/accuracy=0.808613, train/loss=0.957417, validation/accuracy=0.729860, validation/loss=1.299572, validation/num_examples=50000
I0201 03:26:12.842031 139702543816448 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.3382630348205566, loss=3.08605694770813
I0201 03:26:57.857168 139702527031040 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.7517133951187134, loss=4.618216514587402
I0201 03:27:43.626442 139702543816448 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.42508065700531, loss=3.327568531036377
I0201 03:28:29.735614 139702527031040 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.3546369075775146, loss=3.1622190475463867
I0201 03:29:15.716380 139702543816448 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.5482983589172363, loss=3.2480907440185547
I0201 03:30:01.540572 139702527031040 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.5217140913009644, loss=3.210547924041748
I0201 03:30:47.265302 139702543816448 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.5934529304504395, loss=4.6053571701049805
I0201 03:31:33.225029 139702527031040 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.3930867910385132, loss=3.2764220237731934
I0201 03:32:19.014776 139702543816448 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.3442904949188232, loss=4.0104594230651855
I0201 03:32:44.460930 139863983413056 spec.py:321] Evaluating on the training split.
I0201 03:32:54.932307 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 03:33:19.391078 139863983413056 spec.py:349] Evaluating on the test split.
I0201 03:33:21.028602 139863983413056 submission_runner.py:408] Time since start: 52766.52s, 	Step: 107457, 	{'train/accuracy': 0.796582043170929, 'train/loss': 1.0255680084228516, 'validation/accuracy': 0.7306199669837952, 'validation/loss': 1.3091156482696533, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.917056679725647, 'test/num_examples': 10000, 'score': 48771.34631705284, 'total_duration': 52766.52301621437, 'accumulated_submission_time': 48771.34631705284, 'accumulated_eval_time': 3984.6974267959595, 'accumulated_logging_time': 4.803227424621582}
I0201 03:33:21.062679 139702527031040 logging_writer.py:48] [107457] accumulated_eval_time=3984.697427, accumulated_logging_time=4.803227, accumulated_submission_time=48771.346317, global_step=107457, preemption_count=0, score=48771.346317, test/accuracy=0.599900, test/loss=1.917057, test/num_examples=10000, total_duration=52766.523016, train/accuracy=0.796582, train/loss=1.025568, validation/accuracy=0.730620, validation/loss=1.309116, validation/num_examples=50000
I0201 03:33:38.627628 139702543816448 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.8565369844436646, loss=4.888000011444092
I0201 03:34:22.636402 139702527031040 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.3626346588134766, loss=3.961423397064209
I0201 03:35:08.684160 139702543816448 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.3807882070541382, loss=3.558987617492676
I0201 03:35:54.771861 139702527031040 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.384103775024414, loss=3.09580135345459
I0201 03:36:40.758265 139702543816448 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.3945049047470093, loss=3.1860218048095703
I0201 03:37:26.621552 139702527031040 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.3651784658432007, loss=3.3279495239257812
I0201 03:38:12.760947 139702543816448 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.3575557470321655, loss=3.1278393268585205
I0201 03:38:58.550486 139702527031040 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.3879027366638184, loss=3.976924419403076
I0201 03:39:44.146871 139702543816448 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.3388580083847046, loss=3.2972397804260254
I0201 03:40:21.193806 139863983413056 spec.py:321] Evaluating on the training split.
I0201 03:40:31.602626 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 03:40:53.827632 139863983413056 spec.py:349] Evaluating on the test split.
I0201 03:40:55.456764 139863983413056 submission_runner.py:408] Time since start: 53220.95s, 	Step: 108382, 	{'train/accuracy': 0.8032421469688416, 'train/loss': 0.9735230803489685, 'validation/accuracy': 0.7277199625968933, 'validation/loss': 1.296148419380188, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.8809870481491089, 'test/num_examples': 10000, 'score': 49191.41876125336, 'total_duration': 53220.951176166534, 'accumulated_submission_time': 49191.41876125336, 'accumulated_eval_time': 4018.9604184627533, 'accumulated_logging_time': 4.847645044326782}
I0201 03:40:55.487345 139702527031040 logging_writer.py:48] [108382] accumulated_eval_time=4018.960418, accumulated_logging_time=4.847645, accumulated_submission_time=49191.418761, global_step=108382, preemption_count=0, score=49191.418761, test/accuracy=0.607000, test/loss=1.880987, test/num_examples=10000, total_duration=53220.951176, train/accuracy=0.803242, train/loss=0.973523, validation/accuracy=0.727720, validation/loss=1.296148, validation/num_examples=50000
I0201 03:41:03.072481 139702543816448 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.8168222904205322, loss=4.804697036743164
I0201 03:41:45.171165 139702527031040 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.6458622217178345, loss=4.827343940734863
I0201 03:42:31.045647 139702543816448 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.6170114278793335, loss=4.4531569480896
I0201 03:43:16.972068 139702527031040 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.540295124053955, loss=4.696020603179932
I0201 03:44:02.899945 139702543816448 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.6340078115463257, loss=4.450343132019043
I0201 03:44:49.168867 139702527031040 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.451770544052124, loss=4.235057830810547
I0201 03:45:34.999066 139702543816448 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.3801552057266235, loss=4.294734954833984
I0201 03:46:20.997954 139702527031040 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.520504117012024, loss=3.210390329360962
I0201 03:47:06.874413 139702543816448 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.4234106540679932, loss=3.1506459712982178
I0201 03:47:52.664529 139702527031040 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.3652210235595703, loss=3.063243865966797
I0201 03:47:55.601525 139863983413056 spec.py:321] Evaluating on the training split.
I0201 03:48:06.248441 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 03:48:27.367587 139863983413056 spec.py:349] Evaluating on the test split.
I0201 03:48:29.012745 139863983413056 submission_runner.py:408] Time since start: 53674.51s, 	Step: 109308, 	{'train/accuracy': 0.80712890625, 'train/loss': 0.9809885025024414, 'validation/accuracy': 0.7274799942970276, 'validation/loss': 1.309841513633728, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.9155027866363525, 'test/num_examples': 10000, 'score': 49611.47404384613, 'total_duration': 53674.50714588165, 'accumulated_submission_time': 49611.47404384613, 'accumulated_eval_time': 4052.371610879898, 'accumulated_logging_time': 4.887386798858643}
I0201 03:48:29.053127 139702543816448 logging_writer.py:48] [109308] accumulated_eval_time=4052.371611, accumulated_logging_time=4.887387, accumulated_submission_time=49611.474044, global_step=109308, preemption_count=0, score=49611.474044, test/accuracy=0.602500, test/loss=1.915503, test/num_examples=10000, total_duration=53674.507146, train/accuracy=0.807129, train/loss=0.980989, validation/accuracy=0.727480, validation/loss=1.309842, validation/num_examples=50000
I0201 03:49:06.621329 139702527031040 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.3971359729766846, loss=3.591179370880127
I0201 03:49:52.715694 139702543816448 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.4943114519119263, loss=3.216061592102051
I0201 03:50:38.560844 139702527031040 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.7148784399032593, loss=4.721033573150635
I0201 03:51:24.834915 139702543816448 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.5675995349884033, loss=4.746029853820801
I0201 03:52:10.635929 139702527031040 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.544981837272644, loss=4.7491559982299805
I0201 03:52:56.700121 139702543816448 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.3945872783660889, loss=3.9790377616882324
I0201 03:53:42.433480 139702527031040 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.694476842880249, loss=4.747143268585205
I0201 03:54:28.635631 139702543816448 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.4591070413589478, loss=3.347027540206909
I0201 03:55:15.017394 139702527031040 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.4887874126434326, loss=3.186556100845337
I0201 03:55:29.395817 139863983413056 spec.py:321] Evaluating on the training split.
I0201 03:55:39.989578 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 03:55:59.850452 139863983413056 spec.py:349] Evaluating on the test split.
I0201 03:56:01.488435 139863983413056 submission_runner.py:408] Time since start: 54126.98s, 	Step: 110233, 	{'train/accuracy': 0.8057421445846558, 'train/loss': 0.956943690776825, 'validation/accuracy': 0.7321999669075012, 'validation/loss': 1.2614250183105469, 'validation/num_examples': 50000, 'test/accuracy': 0.6146000027656555, 'test/loss': 1.8612117767333984, 'test/num_examples': 10000, 'score': 50031.75916481018, 'total_duration': 54126.982813835144, 'accumulated_submission_time': 50031.75916481018, 'accumulated_eval_time': 4084.4641699790955, 'accumulated_logging_time': 4.937520742416382}
I0201 03:56:01.536482 139702543816448 logging_writer.py:48] [110233] accumulated_eval_time=4084.464170, accumulated_logging_time=4.937521, accumulated_submission_time=50031.759165, global_step=110233, preemption_count=0, score=50031.759165, test/accuracy=0.614600, test/loss=1.861212, test/num_examples=10000, total_duration=54126.982814, train/accuracy=0.805742, train/loss=0.956944, validation/accuracy=0.732200, validation/loss=1.261425, validation/num_examples=50000
I0201 03:56:28.697938 139702527031040 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.5169634819030762, loss=4.419201374053955
I0201 03:57:13.514496 139702543816448 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.4470734596252441, loss=3.8091464042663574
I0201 03:57:59.491912 139702527031040 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.5384639501571655, loss=3.205721616744995
I0201 03:58:45.766417 139702543816448 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.386846661567688, loss=3.412745237350464
I0201 03:59:31.695538 139702527031040 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.4153170585632324, loss=3.486908197402954
I0201 04:00:17.902735 139702543816448 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.5293772220611572, loss=3.2310500144958496
I0201 04:01:03.938317 139702527031040 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.4663374423980713, loss=3.1811630725860596
I0201 04:01:49.756161 139702543816448 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.6314983367919922, loss=3.175783157348633
I0201 04:02:35.680434 139702527031040 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.4411717653274536, loss=3.23049259185791
I0201 04:03:01.630980 139863983413056 spec.py:321] Evaluating on the training split.
I0201 04:03:12.222560 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 04:03:34.610011 139863983413056 spec.py:349] Evaluating on the test split.
I0201 04:03:36.251287 139863983413056 submission_runner.py:408] Time since start: 54581.75s, 	Step: 111158, 	{'train/accuracy': 0.8017968535423279, 'train/loss': 1.0286946296691895, 'validation/accuracy': 0.7317799925804138, 'validation/loss': 1.3278456926345825, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.9332977533340454, 'test/num_examples': 10000, 'score': 50451.79173922539, 'total_duration': 54581.74570274353, 'accumulated_submission_time': 50451.79173922539, 'accumulated_eval_time': 4119.084473133087, 'accumulated_logging_time': 4.998009443283081}
I0201 04:03:36.282687 139702543816448 logging_writer.py:48] [111158] accumulated_eval_time=4119.084473, accumulated_logging_time=4.998009, accumulated_submission_time=50451.791739, global_step=111158, preemption_count=0, score=50451.791739, test/accuracy=0.604300, test/loss=1.933298, test/num_examples=10000, total_duration=54581.745703, train/accuracy=0.801797, train/loss=1.028695, validation/accuracy=0.731780, validation/loss=1.327846, validation/num_examples=50000
I0201 04:03:53.447946 139702527031040 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.4860328435897827, loss=3.3734612464904785
I0201 04:04:36.555843 139702543816448 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.7824496030807495, loss=4.81652307510376
I0201 04:05:22.926039 139702527031040 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.4800407886505127, loss=3.153975486755371
I0201 04:06:09.226620 139702543816448 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.5601942539215088, loss=3.227821111679077
I0201 04:06:55.181443 139702527031040 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.510047435760498, loss=4.118830680847168
I0201 04:07:41.441542 139702543816448 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.6788029670715332, loss=4.29509162902832
I0201 04:08:27.236540 139702527031040 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.3862698078155518, loss=4.146112442016602
I0201 04:09:13.313876 139702543816448 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.4659123420715332, loss=3.962924003601074
I0201 04:09:59.504472 139702527031040 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.5816229581832886, loss=3.351370096206665
I0201 04:10:36.292874 139863983413056 spec.py:321] Evaluating on the training split.
I0201 04:10:46.734471 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 04:11:08.944210 139863983413056 spec.py:349] Evaluating on the test split.
I0201 04:11:10.574578 139863983413056 submission_runner.py:408] Time since start: 55036.07s, 	Step: 112081, 	{'train/accuracy': 0.8128515481948853, 'train/loss': 0.9389355778694153, 'validation/accuracy': 0.7346799969673157, 'validation/loss': 1.2732287645339966, 'validation/num_examples': 50000, 'test/accuracy': 0.6106000542640686, 'test/loss': 1.8744934797286987, 'test/num_examples': 10000, 'score': 50871.74392461777, 'total_duration': 55036.06898331642, 'accumulated_submission_time': 50871.74392461777, 'accumulated_eval_time': 4153.366170406342, 'accumulated_logging_time': 5.039664268493652}
I0201 04:11:10.609747 139702543816448 logging_writer.py:48] [112081] accumulated_eval_time=4153.366170, accumulated_logging_time=5.039664, accumulated_submission_time=50871.743925, global_step=112081, preemption_count=0, score=50871.743925, test/accuracy=0.610600, test/loss=1.874493, test/num_examples=10000, total_duration=55036.068983, train/accuracy=0.812852, train/loss=0.938936, validation/accuracy=0.734680, validation/loss=1.273229, validation/num_examples=50000
I0201 04:11:18.590368 139702527031040 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.4882878065109253, loss=3.1380345821380615
I0201 04:12:00.614063 139702543816448 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.5795395374298096, loss=4.282776832580566
I0201 04:12:46.720701 139702527031040 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.4569542407989502, loss=3.116831064224243
I0201 04:13:33.054780 139702543816448 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.5124670267105103, loss=3.4260854721069336
I0201 04:14:19.195827 139702527031040 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.4334181547164917, loss=3.7301180362701416
I0201 04:15:05.458108 139702543816448 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.5486642122268677, loss=3.195634365081787
I0201 04:15:51.629344 139702527031040 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.3064360618591309, loss=3.6820473670959473
I0201 04:16:37.415018 139702543816448 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.489397644996643, loss=3.1301448345184326
I0201 04:17:23.527358 139702527031040 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.4385886192321777, loss=3.4947874546051025
I0201 04:18:09.767920 139702543816448 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.7193723917007446, loss=4.575552940368652
I0201 04:18:10.753527 139863983413056 spec.py:321] Evaluating on the training split.
I0201 04:18:21.183451 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 04:18:44.108459 139863983413056 spec.py:349] Evaluating on the test split.
I0201 04:18:45.754026 139863983413056 submission_runner.py:408] Time since start: 55491.25s, 	Step: 113004, 	{'train/accuracy': 0.8268554210662842, 'train/loss': 0.8962365388870239, 'validation/accuracy': 0.7337200045585632, 'validation/loss': 1.2884944677352905, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.887868046760559, 'test/num_examples': 10000, 'score': 51291.82757949829, 'total_duration': 55491.248410224915, 'accumulated_submission_time': 51291.82757949829, 'accumulated_eval_time': 4188.3666253089905, 'accumulated_logging_time': 5.085594415664673}
I0201 04:18:45.794850 139702527031040 logging_writer.py:48] [113004] accumulated_eval_time=4188.366625, accumulated_logging_time=5.085594, accumulated_submission_time=51291.827579, global_step=113004, preemption_count=0, score=51291.827579, test/accuracy=0.607700, test/loss=1.887868, test/num_examples=10000, total_duration=55491.248410, train/accuracy=0.826855, train/loss=0.896237, validation/accuracy=0.733720, validation/loss=1.288494, validation/num_examples=50000
I0201 04:19:25.517703 139702543816448 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.482454538345337, loss=3.185563087463379
I0201 04:20:11.505340 139702527031040 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.4304425716400146, loss=3.342764139175415
I0201 04:20:57.484094 139702543816448 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.5110536813735962, loss=3.1422879695892334
I0201 04:21:43.533128 139702527031040 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.5522983074188232, loss=3.2501072883605957
I0201 04:22:29.222109 139702543816448 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.588221788406372, loss=3.125303030014038
I0201 04:23:15.328498 139702527031040 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.5814064741134644, loss=3.1385951042175293
I0201 04:24:01.225224 139702543816448 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.5715594291687012, loss=3.3132243156433105
I0201 04:24:47.044946 139702527031040 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.4267404079437256, loss=3.308377742767334
I0201 04:25:33.041539 139702543816448 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.498026728630066, loss=3.2087740898132324
I0201 04:25:45.986386 139863983413056 spec.py:321] Evaluating on the training split.
I0201 04:25:56.617328 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 04:26:18.573976 139863983413056 spec.py:349] Evaluating on the test split.
I0201 04:26:20.217516 139863983413056 submission_runner.py:408] Time since start: 55945.71s, 	Step: 113930, 	{'train/accuracy': 0.8086913824081421, 'train/loss': 0.9540855288505554, 'validation/accuracy': 0.7350999712944031, 'validation/loss': 1.2699496746063232, 'validation/num_examples': 50000, 'test/accuracy': 0.6146000027656555, 'test/loss': 1.8564660549163818, 'test/num_examples': 10000, 'score': 51711.961062669754, 'total_duration': 55945.711918354034, 'accumulated_submission_time': 51711.961062669754, 'accumulated_eval_time': 4222.597744464874, 'accumulated_logging_time': 5.1359288692474365}
I0201 04:26:20.253142 139702527031040 logging_writer.py:48] [113930] accumulated_eval_time=4222.597744, accumulated_logging_time=5.135929, accumulated_submission_time=51711.961063, global_step=113930, preemption_count=0, score=51711.961063, test/accuracy=0.614600, test/loss=1.856466, test/num_examples=10000, total_duration=55945.711918, train/accuracy=0.808691, train/loss=0.954086, validation/accuracy=0.735100, validation/loss=1.269950, validation/num_examples=50000
I0201 04:26:48.590618 139702543816448 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.5936709642410278, loss=4.5556488037109375
I0201 04:27:33.748131 139702527031040 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.5525568723678589, loss=3.1436548233032227
I0201 04:28:19.383409 139702543816448 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.5791826248168945, loss=4.281374931335449
I0201 04:29:05.427732 139702527031040 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.4865328073501587, loss=3.2193968296051025
I0201 04:29:51.151758 139702543816448 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.0634195804595947, loss=4.687139511108398
I0201 04:30:37.126726 139702527031040 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.3309036493301392, loss=3.6056325435638428
I0201 04:31:22.889702 139702543816448 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.5381590127944946, loss=3.207087278366089
I0201 04:32:08.775392 139702527031040 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.4345766305923462, loss=3.6900272369384766
I0201 04:32:54.607093 139702543816448 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.671797752380371, loss=4.351899147033691
I0201 04:33:20.226273 139863983413056 spec.py:321] Evaluating on the training split.
I0201 04:33:30.296348 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 04:33:52.517055 139863983413056 spec.py:349] Evaluating on the test split.
I0201 04:33:54.161140 139863983413056 submission_runner.py:408] Time since start: 56399.66s, 	Step: 114857, 	{'train/accuracy': 0.8112109303474426, 'train/loss': 0.9619636535644531, 'validation/accuracy': 0.7340999841690063, 'validation/loss': 1.2924312353134155, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.887677788734436, 'test/num_examples': 10000, 'score': 52131.87544989586, 'total_duration': 56399.65555882454, 'accumulated_submission_time': 52131.87544989586, 'accumulated_eval_time': 4256.532593727112, 'accumulated_logging_time': 5.181121587753296}
I0201 04:33:54.195153 139702527031040 logging_writer.py:48] [114857] accumulated_eval_time=4256.532594, accumulated_logging_time=5.181122, accumulated_submission_time=52131.875450, global_step=114857, preemption_count=0, score=52131.875450, test/accuracy=0.612100, test/loss=1.887678, test/num_examples=10000, total_duration=56399.655559, train/accuracy=0.811211, train/loss=0.961964, validation/accuracy=0.734100, validation/loss=1.292431, validation/num_examples=50000
I0201 04:34:11.788757 139702543816448 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.4355038404464722, loss=3.7691476345062256
I0201 04:34:55.212041 139702527031040 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.5197621583938599, loss=3.273409128189087
I0201 04:35:41.306820 139702543816448 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.5620783567428589, loss=3.058385133743286
I0201 04:36:27.432684 139702527031040 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.6936120986938477, loss=3.149019241333008
I0201 04:37:13.723209 139702543816448 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.5278942584991455, loss=3.186032295227051
I0201 04:37:59.579908 139702527031040 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.5078125, loss=3.2090697288513184
I0201 04:38:45.762592 139702543816448 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.4865336418151855, loss=3.391378402709961
I0201 04:39:31.650336 139702527031040 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.5583393573760986, loss=3.1508798599243164
I0201 04:40:17.520110 139702543816448 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.4316942691802979, loss=3.1054797172546387
I0201 04:40:54.338438 139863983413056 spec.py:321] Evaluating on the training split.
I0201 04:41:04.924268 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 04:41:28.320138 139863983413056 spec.py:349] Evaluating on the test split.
I0201 04:41:29.965469 139863983413056 submission_runner.py:408] Time since start: 56855.46s, 	Step: 115782, 	{'train/accuracy': 0.8207421898841858, 'train/loss': 0.9196900129318237, 'validation/accuracy': 0.7369199991226196, 'validation/loss': 1.2814921140670776, 'validation/num_examples': 50000, 'test/accuracy': 0.6084000468254089, 'test/loss': 1.8864821195602417, 'test/num_examples': 10000, 'score': 52551.95956778526, 'total_duration': 56855.459886312485, 'accumulated_submission_time': 52551.95956778526, 'accumulated_eval_time': 4292.1596002578735, 'accumulated_logging_time': 5.225587368011475}
I0201 04:41:29.998573 139702527031040 logging_writer.py:48] [115782] accumulated_eval_time=4292.159600, accumulated_logging_time=5.225587, accumulated_submission_time=52551.959568, global_step=115782, preemption_count=0, score=52551.959568, test/accuracy=0.608400, test/loss=1.886482, test/num_examples=10000, total_duration=56855.459886, train/accuracy=0.820742, train/loss=0.919690, validation/accuracy=0.736920, validation/loss=1.281492, validation/num_examples=50000
I0201 04:41:37.585693 139702543816448 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.6542683839797974, loss=4.322469711303711
I0201 04:42:19.478917 139702527031040 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.5641371011734009, loss=3.097679376602173
I0201 04:43:05.515351 139702543816448 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.5699329376220703, loss=3.855064868927002
I0201 04:43:51.694875 139702527031040 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.5564888715744019, loss=3.181687831878662
I0201 04:44:37.705874 139702543816448 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.5287413597106934, loss=3.96587872505188
I0201 04:45:23.588443 139702527031040 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.6120083332061768, loss=3.1895031929016113
I0201 04:46:09.861039 139702543816448 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.527700424194336, loss=3.5319056510925293
I0201 04:46:55.856157 139702527031040 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.5079096555709839, loss=3.128560781478882
I0201 04:47:41.583731 139702543816448 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.4266828298568726, loss=3.229686975479126
I0201 04:48:27.562472 139702527031040 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.4523009061813354, loss=3.255688428878784
I0201 04:48:30.381340 139863983413056 spec.py:321] Evaluating on the training split.
I0201 04:48:40.746201 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 04:49:00.171101 139863983413056 spec.py:349] Evaluating on the test split.
I0201 04:49:01.817119 139863983413056 submission_runner.py:408] Time since start: 57307.31s, 	Step: 116708, 	{'train/accuracy': 0.8111132383346558, 'train/loss': 0.9671093821525574, 'validation/accuracy': 0.7394199967384338, 'validation/loss': 1.2774097919464111, 'validation/num_examples': 50000, 'test/accuracy': 0.6146000027656555, 'test/loss': 1.8671879768371582, 'test/num_examples': 10000, 'score': 52972.2819852829, 'total_duration': 57307.31153726578, 'accumulated_submission_time': 52972.2819852829, 'accumulated_eval_time': 4323.5953731536865, 'accumulated_logging_time': 5.270857572555542}
I0201 04:49:01.848879 139702543816448 logging_writer.py:48] [116708] accumulated_eval_time=4323.595373, accumulated_logging_time=5.270858, accumulated_submission_time=52972.281985, global_step=116708, preemption_count=0, score=52972.281985, test/accuracy=0.614600, test/loss=1.867188, test/num_examples=10000, total_duration=57307.311537, train/accuracy=0.811113, train/loss=0.967109, validation/accuracy=0.739420, validation/loss=1.277410, validation/num_examples=50000
I0201 04:49:39.524388 139702527031040 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.5204355716705322, loss=4.1293721199035645
I0201 04:50:25.402858 139702543816448 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.4675301313400269, loss=3.2481131553649902
I0201 04:51:11.357081 139702527031040 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.5826679468154907, loss=3.1303837299346924
I0201 04:51:57.372590 139702543816448 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.5705277919769287, loss=4.206606864929199
I0201 04:52:43.328344 139702527031040 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.3969793319702148, loss=3.1667392253875732
I0201 04:53:29.141327 139702543816448 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.4711586236953735, loss=3.2478981018066406
I0201 04:54:15.057477 139702527031040 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.656421422958374, loss=3.108513593673706
I0201 04:55:00.644766 139702543816448 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.4887524843215942, loss=3.0929300785064697
I0201 04:55:46.669859 139702527031040 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.5143498182296753, loss=3.534773588180542
I0201 04:56:01.995134 139863983413056 spec.py:321] Evaluating on the training split.
I0201 04:56:12.519072 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 04:56:32.616780 139863983413056 spec.py:349] Evaluating on the test split.
I0201 04:56:34.263343 139863983413056 submission_runner.py:408] Time since start: 57759.76s, 	Step: 117635, 	{'train/accuracy': 0.8186327815055847, 'train/loss': 0.9464924931526184, 'validation/accuracy': 0.73881995677948, 'validation/loss': 1.275384545326233, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8721600770950317, 'test/num_examples': 10000, 'score': 53392.368614435196, 'total_duration': 57759.757767915726, 'accumulated_submission_time': 53392.368614435196, 'accumulated_eval_time': 4355.863595724106, 'accumulated_logging_time': 5.312883377075195}
I0201 04:56:34.298295 139702543816448 logging_writer.py:48] [117635] accumulated_eval_time=4355.863596, accumulated_logging_time=5.312883, accumulated_submission_time=53392.368614, global_step=117635, preemption_count=0, score=53392.368614, test/accuracy=0.613800, test/loss=1.872160, test/num_examples=10000, total_duration=57759.757768, train/accuracy=0.818633, train/loss=0.946492, validation/accuracy=0.738820, validation/loss=1.275385, validation/num_examples=50000
I0201 04:57:00.658326 139702527031040 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.6526979207992554, loss=3.086448907852173
I0201 04:57:45.674727 139702543816448 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.5259168148040771, loss=3.102128028869629
I0201 04:58:32.139740 139702527031040 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.722796082496643, loss=3.080066204071045
I0201 04:59:18.734375 139702543816448 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.5105340480804443, loss=3.085454225540161
I0201 05:00:05.199577 139702527031040 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.6306082010269165, loss=3.1756067276000977
I0201 05:00:51.211384 139702543816448 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.6830112934112549, loss=3.1056299209594727
I0201 05:01:37.073510 139702527031040 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.7314151525497437, loss=3.147183656692505
I0201 05:02:23.140233 139702543816448 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.002525806427002, loss=4.695505619049072
I0201 05:03:09.140126 139702527031040 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.5746828317642212, loss=4.102023124694824
I0201 05:03:34.656370 139863983413056 spec.py:321] Evaluating on the training split.
I0201 05:03:44.996902 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 05:04:09.023066 139863983413056 spec.py:349] Evaluating on the test split.
I0201 05:04:10.661705 139863983413056 submission_runner.py:408] Time since start: 58216.16s, 	Step: 118557, 	{'train/accuracy': 0.8215234279632568, 'train/loss': 0.9040040969848633, 'validation/accuracy': 0.7385199666023254, 'validation/loss': 1.2626641988754272, 'validation/num_examples': 50000, 'test/accuracy': 0.6128000020980835, 'test/loss': 1.859431505203247, 'test/num_examples': 10000, 'score': 53812.669481277466, 'total_duration': 58216.1560792923, 'accumulated_submission_time': 53812.669481277466, 'accumulated_eval_time': 4391.868913650513, 'accumulated_logging_time': 5.357113838195801}
I0201 05:04:10.694338 139702543816448 logging_writer.py:48] [118557] accumulated_eval_time=4391.868914, accumulated_logging_time=5.357114, accumulated_submission_time=53812.669481, global_step=118557, preemption_count=0, score=53812.669481, test/accuracy=0.612800, test/loss=1.859432, test/num_examples=10000, total_duration=58216.156079, train/accuracy=0.821523, train/loss=0.904004, validation/accuracy=0.738520, validation/loss=1.262664, validation/num_examples=50000
I0201 05:04:28.246967 139702527031040 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.5966280698776245, loss=3.134643793106079
I0201 05:05:11.776579 139702543816448 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.6350133419036865, loss=4.320109844207764
I0201 05:05:57.668139 139702527031040 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.6275033950805664, loss=3.1268928050994873
I0201 05:06:44.130347 139702543816448 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.5404036045074463, loss=3.5490357875823975
I0201 05:07:30.078754 139702527031040 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.5478036403656006, loss=3.0816171169281006
I0201 05:08:15.886824 139702543816448 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.6738677024841309, loss=3.151655435562134
I0201 05:09:01.882236 139702527031040 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.63722562789917, loss=3.0922446250915527
I0201 05:09:47.780905 139702543816448 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.614258050918579, loss=3.090712070465088
I0201 05:10:33.910353 139702527031040 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.519681453704834, loss=3.328958749771118
I0201 05:11:10.704744 139863983413056 spec.py:321] Evaluating on the training split.
I0201 05:11:21.124265 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 05:11:45.458831 139863983413056 spec.py:349] Evaluating on the test split.
I0201 05:11:47.096516 139863983413056 submission_runner.py:408] Time since start: 58672.59s, 	Step: 119482, 	{'train/accuracy': 0.8153515458106995, 'train/loss': 0.9577239751815796, 'validation/accuracy': 0.7410399913787842, 'validation/loss': 1.269242763519287, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.868729591369629, 'test/num_examples': 10000, 'score': 54232.62202167511, 'total_duration': 58672.590933561325, 'accumulated_submission_time': 54232.62202167511, 'accumulated_eval_time': 4428.260704755783, 'accumulated_logging_time': 5.399370908737183}
I0201 05:11:47.132573 139702543816448 logging_writer.py:48] [119482] accumulated_eval_time=4428.260705, accumulated_logging_time=5.399371, accumulated_submission_time=54232.622022, global_step=119482, preemption_count=0, score=54232.622022, test/accuracy=0.616700, test/loss=1.868730, test/num_examples=10000, total_duration=58672.590934, train/accuracy=0.815352, train/loss=0.957724, validation/accuracy=0.741040, validation/loss=1.269243, validation/num_examples=50000
I0201 05:11:54.709383 139702527031040 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.4908459186553955, loss=3.396409034729004
I0201 05:12:36.869662 139702543816448 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.6705917119979858, loss=3.419617176055908
I0201 05:13:22.600829 139702527031040 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.6280269622802734, loss=3.1819872856140137
I0201 05:14:08.637665 139702543816448 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.4870381355285645, loss=3.9459404945373535
I0201 05:14:54.339598 139702527031040 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.6447452306747437, loss=4.529930591583252
I0201 05:15:40.319641 139702543816448 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.686037302017212, loss=3.047520399093628
I0201 05:16:26.673309 139702527031040 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.5080809593200684, loss=3.97623610496521
I0201 05:17:12.628611 139702543816448 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.6332231760025024, loss=4.290866851806641
I0201 05:17:58.375541 139702527031040 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.5550957918167114, loss=4.068595886230469
I0201 05:18:44.586050 139702543816448 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.5784989595413208, loss=3.112557888031006
I0201 05:18:47.143623 139863983413056 spec.py:321] Evaluating on the training split.
I0201 05:18:57.305088 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 05:19:17.944102 139863983413056 spec.py:349] Evaluating on the test split.
I0201 05:19:19.582650 139863983413056 submission_runner.py:408] Time since start: 59125.08s, 	Step: 120407, 	{'train/accuracy': 0.8184374570846558, 'train/loss': 0.9347028136253357, 'validation/accuracy': 0.7399199604988098, 'validation/loss': 1.2617909908294678, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.8522409200668335, 'test/num_examples': 10000, 'score': 54652.57331991196, 'total_duration': 59125.07707071304, 'accumulated_submission_time': 54652.57331991196, 'accumulated_eval_time': 4460.699743509293, 'accumulated_logging_time': 5.445420742034912}
I0201 05:19:19.614715 139702527031040 logging_writer.py:48] [120407] accumulated_eval_time=4460.699744, accumulated_logging_time=5.445421, accumulated_submission_time=54652.573320, global_step=120407, preemption_count=0, score=54652.573320, test/accuracy=0.617800, test/loss=1.852241, test/num_examples=10000, total_duration=59125.077071, train/accuracy=0.818437, train/loss=0.934703, validation/accuracy=0.739920, validation/loss=1.261791, validation/num_examples=50000
I0201 05:19:58.066684 139702543816448 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.5541048049926758, loss=3.0701751708984375
I0201 05:20:43.920358 139702527031040 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.505480408668518, loss=3.908874034881592
I0201 05:21:29.773175 139702543816448 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.7055946588516235, loss=3.1435024738311768
I0201 05:22:16.020170 139702527031040 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.9779380559921265, loss=3.0555977821350098
I0201 05:23:01.584309 139702543816448 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.6993470191955566, loss=3.1586194038391113
I0201 05:23:47.428104 139702527031040 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.6818405389785767, loss=3.0419533252716064
I0201 05:24:33.273924 139702543816448 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.538284420967102, loss=3.108198881149292
I0201 05:25:19.350487 139702527031040 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.575972080230713, loss=2.971628427505493
I0201 05:26:05.288809 139702543816448 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.5954689979553223, loss=4.039940357208252
I0201 05:26:19.640221 139863983413056 spec.py:321] Evaluating on the training split.
I0201 05:26:29.916109 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 05:26:51.737485 139863983413056 spec.py:349] Evaluating on the test split.
I0201 05:26:53.383579 139863983413056 submission_runner.py:408] Time since start: 59578.88s, 	Step: 121333, 	{'train/accuracy': 0.8251171708106995, 'train/loss': 0.8875871300697327, 'validation/accuracy': 0.743619978427887, 'validation/loss': 1.2404817342758179, 'validation/num_examples': 50000, 'test/accuracy': 0.6199000477790833, 'test/loss': 1.8362445831298828, 'test/num_examples': 10000, 'score': 55072.54042840004, 'total_duration': 59578.87798953056, 'accumulated_submission_time': 55072.54042840004, 'accumulated_eval_time': 4494.443091392517, 'accumulated_logging_time': 5.487497329711914}
I0201 05:26:53.418418 139702527031040 logging_writer.py:48] [121333] accumulated_eval_time=4494.443091, accumulated_logging_time=5.487497, accumulated_submission_time=55072.540428, global_step=121333, preemption_count=0, score=55072.540428, test/accuracy=0.619900, test/loss=1.836245, test/num_examples=10000, total_duration=59578.877990, train/accuracy=0.825117, train/loss=0.887587, validation/accuracy=0.743620, validation/loss=1.240482, validation/num_examples=50000
I0201 05:27:20.711584 139702543816448 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.6489721536636353, loss=4.164723873138428
I0201 05:28:05.325724 139702527031040 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.629325270652771, loss=3.1866369247436523
I0201 05:28:51.139130 139702543816448 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.8328264951705933, loss=4.18314790725708
I0201 05:29:37.471418 139702527031040 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.621943473815918, loss=3.1966350078582764
I0201 05:30:23.409307 139702543816448 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.5141476392745972, loss=3.0289418697357178
I0201 05:31:09.178903 139702527031040 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.6722562313079834, loss=4.186712741851807
I0201 05:31:54.868687 139702543816448 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.515424132347107, loss=3.007803201675415
I0201 05:32:40.658296 139702527031040 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.645113468170166, loss=3.1168737411499023
I0201 05:33:26.523309 139702543816448 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.3426482677459717, loss=4.699883460998535
I0201 05:33:53.645313 139863983413056 spec.py:321] Evaluating on the training split.
I0201 05:34:04.183753 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 05:34:26.251152 139863983413056 spec.py:349] Evaluating on the test split.
I0201 05:34:27.883685 139863983413056 submission_runner.py:408] Time since start: 60033.38s, 	Step: 122261, 	{'train/accuracy': 0.8381249904632568, 'train/loss': 0.8315138220787048, 'validation/accuracy': 0.7446199655532837, 'validation/loss': 1.2221808433532715, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.8278734683990479, 'test/num_examples': 10000, 'score': 55492.709612846375, 'total_duration': 60033.37808465958, 'accumulated_submission_time': 55492.709612846375, 'accumulated_eval_time': 4528.681435108185, 'accumulated_logging_time': 5.531659364700317}
I0201 05:34:27.921007 139702527031040 logging_writer.py:48] [122261] accumulated_eval_time=4528.681435, accumulated_logging_time=5.531659, accumulated_submission_time=55492.709613, global_step=122261, preemption_count=0, score=55492.709613, test/accuracy=0.621200, test/loss=1.827873, test/num_examples=10000, total_duration=60033.378085, train/accuracy=0.838125, train/loss=0.831514, validation/accuracy=0.744620, validation/loss=1.222181, validation/num_examples=50000
I0201 05:34:43.907746 139702543816448 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.614121675491333, loss=3.489137887954712
I0201 05:35:27.317516 139702527031040 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.166947603225708, loss=4.649016857147217
I0201 05:36:13.047237 139702543816448 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.5898793935775757, loss=3.0340473651885986
I0201 05:36:59.251668 139702527031040 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.0541913509368896, loss=4.713959217071533
I0201 05:37:45.094255 139702543816448 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.6197073459625244, loss=3.6628689765930176
I0201 05:38:31.010545 139702527031040 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.8819234371185303, loss=4.711420059204102
I0201 05:39:16.935757 139702543816448 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.4912317991256714, loss=3.268671751022339
I0201 05:40:02.968672 139702527031040 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.7400178909301758, loss=3.059535503387451
I0201 05:40:48.929315 139702543816448 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.6922039985656738, loss=3.056682825088501
I0201 05:41:28.072029 139863983413056 spec.py:321] Evaluating on the training split.
I0201 05:41:38.499201 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 05:41:58.692501 139863983413056 spec.py:349] Evaluating on the test split.
I0201 05:42:00.334555 139863983413056 submission_runner.py:408] Time since start: 60485.83s, 	Step: 123187, 	{'train/accuracy': 0.8201562166213989, 'train/loss': 0.9035536050796509, 'validation/accuracy': 0.7415199875831604, 'validation/loss': 1.2345235347747803, 'validation/num_examples': 50000, 'test/accuracy': 0.6188000440597534, 'test/loss': 1.8316484689712524, 'test/num_examples': 10000, 'score': 55912.80021524429, 'total_duration': 60485.82897758484, 'accumulated_submission_time': 55912.80021524429, 'accumulated_eval_time': 4560.943968057632, 'accumulated_logging_time': 5.5809125900268555}
I0201 05:42:00.369681 139702527031040 logging_writer.py:48] [123187] accumulated_eval_time=4560.943968, accumulated_logging_time=5.580913, accumulated_submission_time=55912.800215, global_step=123187, preemption_count=0, score=55912.800215, test/accuracy=0.618800, test/loss=1.831648, test/num_examples=10000, total_duration=60485.828978, train/accuracy=0.820156, train/loss=0.903554, validation/accuracy=0.741520, validation/loss=1.234524, validation/num_examples=50000
I0201 05:42:05.982624 139702543816448 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.9136004447937012, loss=4.605714321136475
I0201 05:42:47.758651 139702527031040 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.7028467655181885, loss=3.1352314949035645
I0201 05:43:33.738438 139702543816448 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.675753116607666, loss=3.735776424407959
I0201 05:44:20.022818 139702527031040 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.64028799533844, loss=4.099982738494873
I0201 05:45:05.877531 139702543816448 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.6153709888458252, loss=3.0939884185791016
I0201 05:45:51.861144 139702527031040 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.5976015329360962, loss=3.4496660232543945
I0201 05:46:38.162770 139702543816448 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.6398661136627197, loss=4.680495738983154
I0201 05:47:24.381882 139702527031040 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.6803605556488037, loss=3.1783409118652344
I0201 05:48:10.137646 139702543816448 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.77839195728302, loss=4.350225925445557
I0201 05:48:56.223608 139702527031040 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.697837471961975, loss=4.273515701293945
I0201 05:49:00.516596 139863983413056 spec.py:321] Evaluating on the training split.
I0201 05:49:10.906018 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 05:49:32.901877 139863983413056 spec.py:349] Evaluating on the test split.
I0201 05:49:34.545205 139863983413056 submission_runner.py:408] Time since start: 60940.04s, 	Step: 124111, 	{'train/accuracy': 0.8274609446525574, 'train/loss': 0.8888863325119019, 'validation/accuracy': 0.7472400069236755, 'validation/loss': 1.228013038635254, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.8307791948318481, 'test/num_examples': 10000, 'score': 56332.88788199425, 'total_duration': 60940.03962993622, 'accumulated_submission_time': 56332.88788199425, 'accumulated_eval_time': 4594.97258067131, 'accumulated_logging_time': 5.626370191574097}
I0201 05:49:34.583594 139702543816448 logging_writer.py:48] [124111] accumulated_eval_time=4594.972581, accumulated_logging_time=5.626370, accumulated_submission_time=56332.887882, global_step=124111, preemption_count=0, score=56332.887882, test/accuracy=0.626000, test/loss=1.830779, test/num_examples=10000, total_duration=60940.039630, train/accuracy=0.827461, train/loss=0.888886, validation/accuracy=0.747240, validation/loss=1.228013, validation/num_examples=50000
I0201 05:50:10.976004 139702527031040 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.7470756769180298, loss=3.2540292739868164
I0201 05:50:56.622192 139702543816448 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.927300214767456, loss=4.632977485656738
I0201 05:51:43.024283 139702527031040 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.6921231746673584, loss=3.1240897178649902
I0201 05:52:28.936276 139702543816448 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.546493649482727, loss=3.0379583835601807
I0201 05:53:14.711145 139702527031040 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.56271231174469, loss=3.562391519546509
I0201 05:54:00.591936 139702543816448 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.6625893115997314, loss=3.0926296710968018
I0201 05:54:46.636395 139702527031040 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.7962054014205933, loss=3.0672521591186523
I0201 05:55:32.531939 139702543816448 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.089388608932495, loss=3.071519613265991
I0201 05:56:18.370521 139702527031040 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.6184831857681274, loss=3.666614532470703
I0201 05:56:34.560341 139863983413056 spec.py:321] Evaluating on the training split.
I0201 05:56:44.736705 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 05:57:07.323009 139863983413056 spec.py:349] Evaluating on the test split.
I0201 05:57:08.976392 139863983413056 submission_runner.py:408] Time since start: 61394.47s, 	Step: 125037, 	{'train/accuracy': 0.8400976657867432, 'train/loss': 0.8476256132125854, 'validation/accuracy': 0.7465400099754333, 'validation/loss': 1.231023907661438, 'validation/num_examples': 50000, 'test/accuracy': 0.6228000521659851, 'test/loss': 1.8111176490783691, 'test/num_examples': 10000, 'score': 56752.807072639465, 'total_duration': 61394.470771074295, 'accumulated_submission_time': 56752.807072639465, 'accumulated_eval_time': 4629.3885724544525, 'accumulated_logging_time': 5.673748731613159}
I0201 05:57:09.013813 139702543816448 logging_writer.py:48] [125037] accumulated_eval_time=4629.388572, accumulated_logging_time=5.673749, accumulated_submission_time=56752.807073, global_step=125037, preemption_count=0, score=56752.807073, test/accuracy=0.622800, test/loss=1.811118, test/num_examples=10000, total_duration=61394.470771, train/accuracy=0.840098, train/loss=0.847626, validation/accuracy=0.746540, validation/loss=1.231024, validation/num_examples=50000
I0201 05:57:34.597745 139702527031040 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.7328031063079834, loss=3.0466628074645996
I0201 05:58:19.322277 139702543816448 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.7156095504760742, loss=3.074223518371582
I0201 05:59:05.189519 139702527031040 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.7711694240570068, loss=3.0270981788635254
I0201 05:59:51.725920 139702543816448 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.672574758529663, loss=3.3658649921417236
I0201 06:00:37.686216 139702527031040 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.0843100547790527, loss=4.42093563079834
I0201 06:01:23.573853 139702543816448 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.6088380813598633, loss=2.979884386062622
I0201 06:02:09.518418 139702527031040 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.801742672920227, loss=3.956766128540039
I0201 06:02:55.321962 139702543816448 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.6843684911727905, loss=3.1062939167022705
I0201 06:03:41.418480 139702527031040 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.6773676872253418, loss=3.657156467437744
I0201 06:04:09.186424 139863983413056 spec.py:321] Evaluating on the training split.
I0201 06:04:19.762642 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 06:04:41.732261 139863983413056 spec.py:349] Evaluating on the test split.
I0201 06:04:43.372565 139863983413056 submission_runner.py:408] Time since start: 61848.87s, 	Step: 125962, 	{'train/accuracy': 0.8267577886581421, 'train/loss': 0.9042437076568604, 'validation/accuracy': 0.744219958782196, 'validation/loss': 1.2379591464996338, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8236942291259766, 'test/num_examples': 10000, 'score': 57172.921754837036, 'total_duration': 61848.86698770523, 'accumulated_submission_time': 57172.921754837036, 'accumulated_eval_time': 4663.57472038269, 'accumulated_logging_time': 5.72042441368103}
I0201 06:04:43.406051 139702543816448 logging_writer.py:48] [125962] accumulated_eval_time=4663.574720, accumulated_logging_time=5.720424, accumulated_submission_time=57172.921755, global_step=125962, preemption_count=0, score=57172.921755, test/accuracy=0.627000, test/loss=1.823694, test/num_examples=10000, total_duration=61848.866988, train/accuracy=0.826758, train/loss=0.904244, validation/accuracy=0.744220, validation/loss=1.237959, validation/num_examples=50000
I0201 06:04:58.991629 139702527031040 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.7018086910247803, loss=4.080203056335449
I0201 06:05:41.945806 139702543816448 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.6583869457244873, loss=3.1033647060394287
I0201 06:06:28.024943 139702527031040 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.6466902494430542, loss=3.0781409740448
I0201 06:07:14.476616 139702543816448 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.8318272829055786, loss=3.0977044105529785
I0201 06:08:00.227059 139702527031040 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.8405569791793823, loss=4.364112854003906
I0201 06:08:46.073531 139702543816448 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.6438770294189453, loss=3.457327127456665
I0201 06:09:32.142131 139702527031040 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.7549539804458618, loss=3.0056312084198
I0201 06:10:18.045362 139702543816448 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.5888886451721191, loss=3.7296881675720215
I0201 06:11:03.826399 139702527031040 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.7360177040100098, loss=3.0890674591064453
I0201 06:11:43.842897 139863983413056 spec.py:321] Evaluating on the training split.
I0201 06:11:54.218697 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 06:12:15.827520 139863983413056 spec.py:349] Evaluating on the test split.
I0201 06:12:17.467369 139863983413056 submission_runner.py:408] Time since start: 62302.96s, 	Step: 126889, 	{'train/accuracy': 0.829394519329071, 'train/loss': 0.8763020634651184, 'validation/accuracy': 0.7468599677085876, 'validation/loss': 1.2259188890457153, 'validation/num_examples': 50000, 'test/accuracy': 0.6248000264167786, 'test/loss': 1.8171780109405518, 'test/num_examples': 10000, 'score': 57593.299280166626, 'total_duration': 62302.96178174019, 'accumulated_submission_time': 57593.299280166626, 'accumulated_eval_time': 4697.19917845726, 'accumulated_logging_time': 5.7643516063690186}
I0201 06:12:17.503391 139702543816448 logging_writer.py:48] [126889] accumulated_eval_time=4697.199178, accumulated_logging_time=5.764352, accumulated_submission_time=57593.299280, global_step=126889, preemption_count=0, score=57593.299280, test/accuracy=0.624800, test/loss=1.817178, test/num_examples=10000, total_duration=62302.961782, train/accuracy=0.829395, train/loss=0.876302, validation/accuracy=0.746860, validation/loss=1.225919, validation/num_examples=50000
I0201 06:12:22.304202 139702527031040 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.7687705755233765, loss=3.103294849395752
I0201 06:13:04.002469 139702543816448 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.7589112520217896, loss=3.0800418853759766
I0201 06:13:49.585989 139702527031040 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.7350753545761108, loss=3.232398271560669
I0201 06:14:35.391595 139702543816448 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.6760904788970947, loss=3.0369958877563477
I0201 06:15:21.340786 139702527031040 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.6072628498077393, loss=3.924070358276367
I0201 06:16:07.522680 139702543816448 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.6662143468856812, loss=3.045231819152832
I0201 06:16:53.543961 139702527031040 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.6416949033737183, loss=3.3477401733398438
I0201 06:17:39.714988 139702543816448 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.6357154846191406, loss=3.1975433826446533
I0201 06:18:25.453916 139702527031040 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.9885268211364746, loss=3.0618276596069336
I0201 06:19:11.458279 139702543816448 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.7121342420578003, loss=4.1543731689453125
I0201 06:19:17.496510 139863983413056 spec.py:321] Evaluating on the training split.
I0201 06:19:27.995890 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 06:19:49.223907 139863983413056 spec.py:349] Evaluating on the test split.
I0201 06:19:50.858416 139863983413056 submission_runner.py:408] Time since start: 62756.35s, 	Step: 127815, 	{'train/accuracy': 0.8350195288658142, 'train/loss': 0.8415217399597168, 'validation/accuracy': 0.7514199614524841, 'validation/loss': 1.19737708568573, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.7829385995864868, 'test/num_examples': 10000, 'score': 58013.23331975937, 'total_duration': 62756.352815151215, 'accumulated_submission_time': 58013.23331975937, 'accumulated_eval_time': 4730.561057806015, 'accumulated_logging_time': 5.811323881149292}
I0201 06:19:50.897071 139702527031040 logging_writer.py:48] [127815] accumulated_eval_time=4730.561058, accumulated_logging_time=5.811324, accumulated_submission_time=58013.233320, global_step=127815, preemption_count=0, score=58013.233320, test/accuracy=0.627600, test/loss=1.782939, test/num_examples=10000, total_duration=62756.352815, train/accuracy=0.835020, train/loss=0.841522, validation/accuracy=0.751420, validation/loss=1.197377, validation/num_examples=50000
I0201 06:20:25.306036 139702543816448 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.7095587253570557, loss=3.0281689167022705
I0201 06:21:11.011894 139702527031040 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.7727075815200806, loss=3.148529529571533
I0201 06:21:57.134652 139702543816448 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.65984308719635, loss=3.153749704360962
I0201 06:22:43.231646 139702527031040 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.7547955513000488, loss=3.23095965385437
I0201 06:23:29.089847 139702543816448 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.8135018348693848, loss=3.0909619331359863
I0201 06:24:14.845301 139702527031040 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.7548342943191528, loss=2.9536142349243164
I0201 06:25:00.588748 139702543816448 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.7960457801818848, loss=3.2242953777313232
I0201 06:25:46.631572 139702527031040 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.6776514053344727, loss=3.091679096221924
I0201 06:26:32.731115 139702543816448 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.7078914642333984, loss=3.2302350997924805
I0201 06:26:51.058322 139863983413056 spec.py:321] Evaluating on the training split.
I0201 06:27:02.356208 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 06:27:24.831285 139863983413056 spec.py:349] Evaluating on the test split.
I0201 06:27:26.470030 139863983413056 submission_runner.py:408] Time since start: 63211.96s, 	Step: 128742, 	{'train/accuracy': 0.8329101204872131, 'train/loss': 0.8737267255783081, 'validation/accuracy': 0.7518799901008606, 'validation/loss': 1.2153342962265015, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.8096715211868286, 'test/num_examples': 10000, 'score': 58433.33541345596, 'total_duration': 63211.96445417404, 'accumulated_submission_time': 58433.33541345596, 'accumulated_eval_time': 4765.972766160965, 'accumulated_logging_time': 5.859126567840576}
I0201 06:27:26.505087 139702527031040 logging_writer.py:48] [128742] accumulated_eval_time=4765.972766, accumulated_logging_time=5.859127, accumulated_submission_time=58433.335413, global_step=128742, preemption_count=0, score=58433.335413, test/accuracy=0.629300, test/loss=1.809672, test/num_examples=10000, total_duration=63211.964454, train/accuracy=0.832910, train/loss=0.873727, validation/accuracy=0.751880, validation/loss=1.215334, validation/num_examples=50000
I0201 06:27:50.055226 139702543816448 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.7588386535644531, loss=3.0280516147613525
I0201 06:28:34.506856 139702527031040 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.8633581399917603, loss=3.029358386993408
I0201 06:29:20.498434 139702543816448 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.20377516746521, loss=4.619895935058594
I0201 06:30:06.752546 139702527031040 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.7376630306243896, loss=3.650498867034912
I0201 06:30:52.413028 139702543816448 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.9506118297576904, loss=4.286116600036621
I0201 06:31:38.175757 139702527031040 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.6201438903808594, loss=3.2871909141540527
I0201 06:32:24.298214 139702543816448 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.7454578876495361, loss=3.5873022079467773
I0201 06:33:10.182893 139702527031040 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.9314085245132446, loss=3.0217020511627197
I0201 06:33:56.015666 139702543816448 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.7304497957229614, loss=2.9981980323791504
I0201 06:34:27.048766 139863983413056 spec.py:321] Evaluating on the training split.
I0201 06:34:37.560685 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 06:34:54.079795 139863983413056 spec.py:349] Evaluating on the test split.
I0201 06:34:55.739805 139863983413056 submission_runner.py:408] Time since start: 63661.23s, 	Step: 129669, 	{'train/accuracy': 0.8311132788658142, 'train/loss': 0.8590825200080872, 'validation/accuracy': 0.7507799863815308, 'validation/loss': 1.2077339887619019, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.7966262102127075, 'test/num_examples': 10000, 'score': 58853.82092785835, 'total_duration': 63661.234209775925, 'accumulated_submission_time': 58853.82092785835, 'accumulated_eval_time': 4794.663794994354, 'accumulated_logging_time': 5.904484272003174}
I0201 06:34:55.787995 139702527031040 logging_writer.py:48] [129669] accumulated_eval_time=4794.663795, accumulated_logging_time=5.904484, accumulated_submission_time=58853.820928, global_step=129669, preemption_count=0, score=58853.820928, test/accuracy=0.628600, test/loss=1.796626, test/num_examples=10000, total_duration=63661.234210, train/accuracy=0.831113, train/loss=0.859083, validation/accuracy=0.750780, validation/loss=1.207734, validation/num_examples=50000
I0201 06:35:08.622218 139702543816448 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.9032400846481323, loss=3.270549774169922
I0201 06:35:52.251681 139702527031040 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.0116055011749268, loss=4.462699890136719
I0201 06:36:38.102358 139702543816448 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.057666063308716, loss=4.389566421508789
I0201 06:37:24.261953 139702527031040 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.7613893747329712, loss=4.037803649902344
I0201 06:38:10.088347 139702543816448 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.1154916286468506, loss=4.514238357543945
I0201 06:38:56.026963 139702527031040 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.6929640769958496, loss=3.074056386947632
I0201 06:39:41.918910 139702543816448 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.6769230365753174, loss=3.256507396697998
I0201 06:40:27.887166 139702527031040 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.614980697631836, loss=3.4265501499176025
I0201 06:41:13.717222 139702543816448 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.7652028799057007, loss=3.041922092437744
I0201 06:41:56.219325 139863983413056 spec.py:321] Evaluating on the training split.
I0201 06:42:06.565327 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 06:42:28.488970 139863983413056 spec.py:349] Evaluating on the test split.
I0201 06:42:30.119139 139863983413056 submission_runner.py:408] Time since start: 64115.61s, 	Step: 130594, 	{'train/accuracy': 0.84033203125, 'train/loss': 0.8434707522392273, 'validation/accuracy': 0.7518799901008606, 'validation/loss': 1.2105284929275513, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.803253412246704, 'test/num_examples': 10000, 'score': 59274.19008851051, 'total_duration': 64115.61355304718, 'accumulated_submission_time': 59274.19008851051, 'accumulated_eval_time': 4828.563596725464, 'accumulated_logging_time': 5.966588497161865}
I0201 06:42:30.153412 139702527031040 logging_writer.py:48] [130594] accumulated_eval_time=4828.563597, accumulated_logging_time=5.966588, accumulated_submission_time=59274.190089, global_step=130594, preemption_count=0, score=59274.190089, test/accuracy=0.628000, test/loss=1.803253, test/num_examples=10000, total_duration=64115.613553, train/accuracy=0.840332, train/loss=0.843471, validation/accuracy=0.751880, validation/loss=1.210528, validation/num_examples=50000
I0201 06:42:32.944830 139702543816448 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.7344448566436768, loss=3.648972749710083
I0201 06:43:14.053967 139702527031040 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.7427610158920288, loss=3.028855800628662
I0201 06:43:59.524167 139702543816448 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.7248276472091675, loss=3.5210506916046143
I0201 06:44:45.470007 139702527031040 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.7602386474609375, loss=3.0121009349823
I0201 06:45:31.268317 139702543816448 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.830619215965271, loss=3.882373571395874
I0201 06:46:17.164410 139702527031040 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.7298847436904907, loss=3.008490562438965
I0201 06:47:03.108879 139702543816448 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.755711555480957, loss=3.3625547885894775
I0201 06:47:48.659870 139702527031040 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.6027315855026245, loss=3.704397678375244
I0201 06:48:34.714791 139702543816448 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.8539707660675049, loss=3.0443227291107178
I0201 06:49:20.840871 139702527031040 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.8818942308425903, loss=3.220935344696045
I0201 06:49:30.780332 139863983413056 spec.py:321] Evaluating on the training split.
I0201 06:49:41.131248 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 06:50:01.445906 139863983413056 spec.py:349] Evaluating on the test split.
I0201 06:50:03.091628 139863983413056 submission_runner.py:408] Time since start: 64568.59s, 	Step: 131523, 	{'train/accuracy': 0.8484765291213989, 'train/loss': 0.8026385307312012, 'validation/accuracy': 0.7519800066947937, 'validation/loss': 1.2015353441238403, 'validation/num_examples': 50000, 'test/accuracy': 0.6252000331878662, 'test/loss': 1.8083374500274658, 'test/num_examples': 10000, 'score': 59694.75778841972, 'total_duration': 64568.58602309227, 'accumulated_submission_time': 59694.75778841972, 'accumulated_eval_time': 4860.874860286713, 'accumulated_logging_time': 6.009950637817383}
I0201 06:50:03.125953 139702543816448 logging_writer.py:48] [131523] accumulated_eval_time=4860.874860, accumulated_logging_time=6.009951, accumulated_submission_time=59694.757788, global_step=131523, preemption_count=0, score=59694.757788, test/accuracy=0.625200, test/loss=1.808337, test/num_examples=10000, total_duration=64568.586023, train/accuracy=0.848477, train/loss=0.802639, validation/accuracy=0.751980, validation/loss=1.201535, validation/num_examples=50000
I0201 06:50:34.300935 139702527031040 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.774332046508789, loss=3.2589025497436523
I0201 06:51:19.736549 139702543816448 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.7990773916244507, loss=3.0233027935028076
I0201 06:52:05.846798 139702527031040 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.8605613708496094, loss=3.041576623916626
I0201 06:52:51.812485 139702543816448 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.7924690246582031, loss=3.8678572177886963
I0201 06:53:37.665853 139702527031040 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.7584476470947266, loss=2.943037509918213
I0201 06:54:24.030665 139702543816448 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.87211275100708, loss=2.867568254470825
I0201 06:55:09.769085 139702527031040 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.7456978559494019, loss=3.2929885387420654
I0201 06:55:55.729610 139702543816448 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.8221945762634277, loss=3.8296399116516113
I0201 06:56:42.474602 139702527031040 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.7966653108596802, loss=3.004054546356201
I0201 06:57:03.423259 139863983413056 spec.py:321] Evaluating on the training split.
I0201 06:57:13.921508 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 06:57:37.563289 139863983413056 spec.py:349] Evaluating on the test split.
I0201 06:57:39.201574 139863983413056 submission_runner.py:408] Time since start: 65024.70s, 	Step: 132448, 	{'train/accuracy': 0.8370116949081421, 'train/loss': 0.834997832775116, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 1.1796581745147705, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.7780799865722656, 'test/num_examples': 10000, 'score': 60114.998239040375, 'total_duration': 65024.695997714996, 'accumulated_submission_time': 60114.998239040375, 'accumulated_eval_time': 4896.653185606003, 'accumulated_logging_time': 6.053093671798706}
I0201 06:57:39.240352 139702543816448 logging_writer.py:48] [132448] accumulated_eval_time=4896.653186, accumulated_logging_time=6.053094, accumulated_submission_time=60114.998239, global_step=132448, preemption_count=0, score=60114.998239, test/accuracy=0.630500, test/loss=1.778080, test/num_examples=10000, total_duration=65024.695998, train/accuracy=0.837012, train/loss=0.834998, validation/accuracy=0.754520, validation/loss=1.179658, validation/num_examples=50000
I0201 06:58:00.391030 139702527031040 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.8051058053970337, loss=3.148124933242798
I0201 06:58:44.327400 139702543816448 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.738884449005127, loss=2.979668378829956
I0201 06:59:30.832659 139702527031040 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.9734909534454346, loss=4.566073894500732
I0201 07:00:17.025906 139702543816448 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.010816812515259, loss=3.0251848697662354
I0201 07:01:02.729556 139702527031040 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.036822557449341, loss=2.9903147220611572
I0201 07:01:48.601223 139702543816448 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.7726634740829468, loss=3.0127854347229004
I0201 07:02:34.603740 139702527031040 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.7036008834838867, loss=3.5222928524017334
I0201 07:03:20.397685 139702543816448 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.883264422416687, loss=3.64306378364563
I0201 07:04:05.905396 139702527031040 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.04955792427063, loss=4.5290751457214355
I0201 07:04:39.524101 139863983413056 spec.py:321] Evaluating on the training split.
I0201 07:04:50.041176 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 07:05:10.277818 139863983413056 spec.py:349] Evaluating on the test split.
I0201 07:05:11.919368 139863983413056 submission_runner.py:408] Time since start: 65477.41s, 	Step: 133375, 	{'train/accuracy': 0.8409179449081421, 'train/loss': 0.8413550853729248, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.2021714448928833, 'validation/num_examples': 50000, 'test/accuracy': 0.6372000575065613, 'test/loss': 1.7991794347763062, 'test/num_examples': 10000, 'score': 60535.22475409508, 'total_duration': 65477.413791656494, 'accumulated_submission_time': 60535.22475409508, 'accumulated_eval_time': 4929.048456430435, 'accumulated_logging_time': 6.100675344467163}
I0201 07:05:11.953819 139702543816448 logging_writer.py:48] [133375] accumulated_eval_time=4929.048456, accumulated_logging_time=6.100675, accumulated_submission_time=60535.224754, global_step=133375, preemption_count=0, score=60535.224754, test/accuracy=0.637200, test/loss=1.799179, test/num_examples=10000, total_duration=65477.413792, train/accuracy=0.840918, train/loss=0.841355, validation/accuracy=0.755580, validation/loss=1.202171, validation/num_examples=50000
I0201 07:05:22.330217 139702527031040 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.019742250442505, loss=2.9905688762664795
I0201 07:06:04.936161 139702543816448 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.9635812044143677, loss=2.98384165763855
I0201 07:06:50.836207 139702527031040 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.9401633739471436, loss=2.9751131534576416
I0201 07:07:36.984436 139702543816448 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.047898769378662, loss=3.4978208541870117
I0201 07:08:22.964427 139702527031040 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.8131670951843262, loss=2.994959592819214
I0201 07:09:09.034278 139702543816448 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.815535545349121, loss=3.3513009548187256
I0201 07:09:55.266774 139702527031040 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.8906357288360596, loss=3.0546326637268066
I0201 07:10:41.198254 139702543816448 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.758639931678772, loss=3.2790157794952393
I0201 07:11:27.452618 139702527031040 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.8660184144973755, loss=3.4704461097717285
I0201 07:12:12.378844 139863983413056 spec.py:321] Evaluating on the training split.
I0201 07:12:23.092503 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 07:12:46.260958 139863983413056 spec.py:349] Evaluating on the test split.
I0201 07:12:47.908615 139863983413056 submission_runner.py:408] Time since start: 65933.40s, 	Step: 134299, 	{'train/accuracy': 0.8481249809265137, 'train/loss': 0.8018088936805725, 'validation/accuracy': 0.7539399862289429, 'validation/loss': 1.1947689056396484, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.78182053565979, 'test/num_examples': 10000, 'score': 60955.59069728851, 'total_duration': 65933.40303897858, 'accumulated_submission_time': 60955.59069728851, 'accumulated_eval_time': 4964.578231573105, 'accumulated_logging_time': 6.146247386932373}
I0201 07:12:47.943793 139702543816448 logging_writer.py:48] [134299] accumulated_eval_time=4964.578232, accumulated_logging_time=6.146247, accumulated_submission_time=60955.590697, global_step=134299, preemption_count=0, score=60955.590697, test/accuracy=0.631200, test/loss=1.781821, test/num_examples=10000, total_duration=65933.403039, train/accuracy=0.848125, train/loss=0.801809, validation/accuracy=0.753940, validation/loss=1.194769, validation/num_examples=50000
I0201 07:12:48.744449 139702527031040 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.7856568098068237, loss=2.9325942993164062
I0201 07:13:29.729022 139702543816448 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.0565803050994873, loss=4.411559104919434
I0201 07:14:15.355998 139702527031040 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.7747527360916138, loss=3.2548294067382812
I0201 07:15:01.236833 139702543816448 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.9044229984283447, loss=3.0667970180511475
I0201 07:15:47.056249 139702527031040 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.8974882364273071, loss=3.7300496101379395
I0201 07:16:32.912282 139702543816448 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.77418851852417, loss=3.1142120361328125
I0201 07:17:18.987290 139702527031040 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.7928311824798584, loss=2.9519896507263184
I0201 07:18:04.810712 139702543816448 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.6832773685455322, loss=3.823331117630005
I0201 07:18:50.567922 139702527031040 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.9012761116027832, loss=3.07914137840271
I0201 07:19:36.687501 139702543816448 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.0072336196899414, loss=4.3035969734191895
I0201 07:19:47.962542 139863983413056 spec.py:321] Evaluating on the training split.
I0201 07:19:58.488720 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 07:20:22.091506 139863983413056 spec.py:349] Evaluating on the test split.
I0201 07:20:23.731463 139863983413056 submission_runner.py:408] Time since start: 66389.23s, 	Step: 135226, 	{'train/accuracy': 0.8419921398162842, 'train/loss': 0.8387060165405273, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.199419617652893, 'validation/num_examples': 50000, 'test/accuracy': 0.638200044631958, 'test/loss': 1.7809736728668213, 'test/num_examples': 10000, 'score': 61375.55031371117, 'total_duration': 66389.22588658333, 'accumulated_submission_time': 61375.55031371117, 'accumulated_eval_time': 5000.347151756287, 'accumulated_logging_time': 6.192385196685791}
I0201 07:20:23.770181 139702527031040 logging_writer.py:48] [135226] accumulated_eval_time=5000.347152, accumulated_logging_time=6.192385, accumulated_submission_time=61375.550314, global_step=135226, preemption_count=0, score=61375.550314, test/accuracy=0.638200, test/loss=1.780974, test/num_examples=10000, total_duration=66389.225887, train/accuracy=0.841992, train/loss=0.838706, validation/accuracy=0.754920, validation/loss=1.199420, validation/num_examples=50000
I0201 07:20:53.714578 139702543816448 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.023144245147705, loss=3.0486459732055664
I0201 07:21:39.123361 139702527031040 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.856471061706543, loss=3.8614182472229004
I0201 07:22:25.388331 139702543816448 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.1777896881103516, loss=4.400937557220459
I0201 07:23:11.662463 139702527031040 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.1960158348083496, loss=4.3959269523620605
I0201 07:23:57.307186 139702543816448 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.8593189716339111, loss=2.947692632675171
I0201 07:24:43.369029 139702527031040 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.8706450462341309, loss=2.993770122528076
I0201 07:25:29.217646 139702543816448 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.8184906244277954, loss=2.9100990295410156
I0201 07:26:15.192154 139702527031040 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.7940951585769653, loss=3.096471071243286
I0201 07:27:01.099649 139702543816448 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.8634299039840698, loss=3.068233013153076
I0201 07:27:23.934056 139863983413056 spec.py:321] Evaluating on the training split.
I0201 07:27:34.376818 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 07:27:57.011215 139863983413056 spec.py:349] Evaluating on the test split.
I0201 07:27:58.648199 139863983413056 submission_runner.py:408] Time since start: 66844.14s, 	Step: 136151, 	{'train/accuracy': 0.8457421660423279, 'train/loss': 0.8141063451766968, 'validation/accuracy': 0.7572399973869324, 'validation/loss': 1.1856240034103394, 'validation/num_examples': 50000, 'test/accuracy': 0.6385000348091125, 'test/loss': 1.7745895385742188, 'test/num_examples': 10000, 'score': 61795.65664720535, 'total_duration': 66844.14261889458, 'accumulated_submission_time': 61795.65664720535, 'accumulated_eval_time': 5035.061300992966, 'accumulated_logging_time': 6.240199089050293}
I0201 07:27:58.685182 139702527031040 logging_writer.py:48] [136151] accumulated_eval_time=5035.061301, accumulated_logging_time=6.240199, accumulated_submission_time=61795.656647, global_step=136151, preemption_count=0, score=61795.656647, test/accuracy=0.638500, test/loss=1.774590, test/num_examples=10000, total_duration=66844.142619, train/accuracy=0.845742, train/loss=0.814106, validation/accuracy=0.757240, validation/loss=1.185624, validation/num_examples=50000
I0201 07:28:18.640952 139702543816448 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.8386716842651367, loss=3.0020699501037598
I0201 07:29:02.509919 139702527031040 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.8046728372573853, loss=3.351677894592285
I0201 07:29:48.728530 139702543816448 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.998537540435791, loss=3.432631731033325
I0201 07:30:34.915012 139702527031040 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.015644073486328, loss=3.024043083190918
I0201 07:31:20.764770 139702543816448 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.7320650815963745, loss=2.8865251541137695
I0201 07:32:06.743060 139702527031040 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.0811431407928467, loss=3.0148491859436035
I0201 07:32:52.650702 139702543816448 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.8189029693603516, loss=3.363957643508911
I0201 07:33:38.671150 139702527031040 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.901811957359314, loss=2.9595935344696045
I0201 07:34:24.597516 139702543816448 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.9233390092849731, loss=2.946286201477051
I0201 07:34:58.792547 139863983413056 spec.py:321] Evaluating on the training split.
I0201 07:35:09.251355 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 07:35:34.609699 139863983413056 spec.py:349] Evaluating on the test split.
I0201 07:35:36.251132 139863983413056 submission_runner.py:408] Time since start: 67301.75s, 	Step: 137076, 	{'train/accuracy': 0.8503710627555847, 'train/loss': 0.803084671497345, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.1784809827804565, 'validation/num_examples': 50000, 'test/accuracy': 0.6363000273704529, 'test/loss': 1.7618496417999268, 'test/num_examples': 10000, 'score': 62215.70627474785, 'total_duration': 67301.74554777145, 'accumulated_submission_time': 62215.70627474785, 'accumulated_eval_time': 5072.519873142242, 'accumulated_logging_time': 6.286186456680298}
I0201 07:35:36.286315 139702527031040 logging_writer.py:48] [137076] accumulated_eval_time=5072.519873, accumulated_logging_time=6.286186, accumulated_submission_time=62215.706275, global_step=137076, preemption_count=0, score=62215.706275, test/accuracy=0.636300, test/loss=1.761850, test/num_examples=10000, total_duration=67301.745548, train/accuracy=0.850371, train/loss=0.803085, validation/accuracy=0.757280, validation/loss=1.178481, validation/num_examples=50000
I0201 07:35:46.260963 139702543816448 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.8278318643569946, loss=3.2266769409179688
I0201 07:36:28.614421 139702527031040 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.85164475440979, loss=3.0630111694335938
I0201 07:37:14.480183 139702543816448 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.284059762954712, loss=4.476593971252441
I0201 07:38:00.452143 139702527031040 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.8472867012023926, loss=3.949113130569458
I0201 07:38:46.270443 139702543816448 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.8564183712005615, loss=2.8985538482666016
I0201 07:39:32.195474 139702527031040 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.8240654468536377, loss=3.186619520187378
I0201 07:40:18.485439 139702543816448 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.9213201999664307, loss=3.834773302078247
I0201 07:41:03.861439 139702527031040 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.8193202018737793, loss=3.247565746307373
I0201 07:41:49.704202 139702543816448 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.0613479614257812, loss=3.19020938873291
I0201 07:42:35.410266 139702527031040 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.9465465545654297, loss=3.072808265686035
I0201 07:42:36.483826 139863983413056 spec.py:321] Evaluating on the training split.
I0201 07:42:46.849966 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 07:43:08.280432 139863983413056 spec.py:349] Evaluating on the test split.
I0201 07:43:09.921689 139863983413056 submission_runner.py:408] Time since start: 67755.42s, 	Step: 138004, 	{'train/accuracy': 0.8441210985183716, 'train/loss': 0.8067802786827087, 'validation/accuracy': 0.7592200040817261, 'validation/loss': 1.1712822914123535, 'validation/num_examples': 50000, 'test/accuracy': 0.6401000022888184, 'test/loss': 1.7613399028778076, 'test/num_examples': 10000, 'score': 62635.84506726265, 'total_duration': 67755.41611027718, 'accumulated_submission_time': 62635.84506726265, 'accumulated_eval_time': 5105.9577214717865, 'accumulated_logging_time': 6.330903053283691}
I0201 07:43:09.956538 139702543816448 logging_writer.py:48] [138004] accumulated_eval_time=5105.957721, accumulated_logging_time=6.330903, accumulated_submission_time=62635.845067, global_step=138004, preemption_count=0, score=62635.845067, test/accuracy=0.640100, test/loss=1.761340, test/num_examples=10000, total_duration=67755.416110, train/accuracy=0.844121, train/loss=0.806780, validation/accuracy=0.759220, validation/loss=1.171282, validation/num_examples=50000
I0201 07:43:49.527381 139702527031040 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.8096528053283691, loss=3.612881898880005
I0201 07:44:35.202353 139702543816448 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.9567798376083374, loss=2.9736177921295166
I0201 07:45:21.410133 139702527031040 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.9846817255020142, loss=3.0071022510528564
I0201 07:46:07.475240 139702543816448 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.7138464450836182, loss=3.344367742538452
I0201 07:46:53.278562 139702527031040 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.9773311614990234, loss=3.601667881011963
I0201 07:47:39.293303 139702543816448 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.9222978353500366, loss=2.898231029510498
I0201 07:48:25.151783 139702527031040 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.2799508571624756, loss=4.351121425628662
I0201 07:49:11.260645 139702543816448 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.9090944528579712, loss=2.9732608795166016
I0201 07:49:57.515885 139702527031040 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.7773637771606445, loss=4.569207668304443
I0201 07:50:10.162872 139863983413056 spec.py:321] Evaluating on the training split.
I0201 07:50:20.630231 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 07:50:43.687598 139863983413056 spec.py:349] Evaluating on the test split.
I0201 07:50:45.331932 139863983413056 submission_runner.py:408] Time since start: 68210.83s, 	Step: 138929, 	{'train/accuracy': 0.8473241925239563, 'train/loss': 0.8092468976974487, 'validation/accuracy': 0.7607199549674988, 'validation/loss': 1.178433895111084, 'validation/num_examples': 50000, 'test/accuracy': 0.6415000557899475, 'test/loss': 1.7597088813781738, 'test/num_examples': 10000, 'score': 63055.99439263344, 'total_duration': 68210.82635331154, 'accumulated_submission_time': 63055.99439263344, 'accumulated_eval_time': 5141.126784801483, 'accumulated_logging_time': 6.374598264694214}
I0201 07:50:45.370616 139702543816448 logging_writer.py:48] [138929] accumulated_eval_time=5141.126785, accumulated_logging_time=6.374598, accumulated_submission_time=63055.994393, global_step=138929, preemption_count=0, score=63055.994393, test/accuracy=0.641500, test/loss=1.759709, test/num_examples=10000, total_duration=68210.826353, train/accuracy=0.847324, train/loss=0.809247, validation/accuracy=0.760720, validation/loss=1.178434, validation/num_examples=50000
I0201 07:51:14.112553 139702527031040 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.409505605697632, loss=4.535200595855713
I0201 07:51:59.203462 139702543816448 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.0326738357543945, loss=2.942073106765747
I0201 07:52:45.292588 139702527031040 logging_writer.py:48] [139200] global_step=139200, grad_norm=1.8485163450241089, loss=2.8529701232910156
I0201 07:53:31.306941 139702543816448 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.9186469316482544, loss=2.9302093982696533
I0201 07:54:16.885924 139702527031040 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.0019137859344482, loss=3.014606475830078
I0201 07:55:03.055385 139702543816448 logging_writer.py:48] [139500] global_step=139500, grad_norm=1.803104043006897, loss=2.87459135055542
I0201 07:55:49.108263 139702527031040 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.933929681777954, loss=2.945934295654297
I0201 07:56:34.944565 139702543816448 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.9911922216415405, loss=2.9608469009399414
I0201 07:57:21.003384 139702527031040 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.182670831680298, loss=4.290167808532715
I0201 07:57:45.587307 139863983413056 spec.py:321] Evaluating on the training split.
I0201 07:57:55.864562 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 07:58:15.292134 139863983413056 spec.py:349] Evaluating on the test split.
I0201 07:58:16.943805 139863983413056 submission_runner.py:408] Time since start: 68662.44s, 	Step: 139855, 	{'train/accuracy': 0.8530859351158142, 'train/loss': 0.7908298969268799, 'validation/accuracy': 0.7603999972343445, 'validation/loss': 1.179398775100708, 'validation/num_examples': 50000, 'test/accuracy': 0.6410000324249268, 'test/loss': 1.7686790227890015, 'test/num_examples': 10000, 'score': 63476.151854515076, 'total_duration': 68662.43821144104, 'accumulated_submission_time': 63476.151854515076, 'accumulated_eval_time': 5172.483269929886, 'accumulated_logging_time': 6.423179388046265}
I0201 07:58:16.988694 139702543816448 logging_writer.py:48] [139855] accumulated_eval_time=5172.483270, accumulated_logging_time=6.423179, accumulated_submission_time=63476.151855, global_step=139855, preemption_count=0, score=63476.151855, test/accuracy=0.641000, test/loss=1.768679, test/num_examples=10000, total_duration=68662.438211, train/accuracy=0.853086, train/loss=0.790830, validation/accuracy=0.760400, validation/loss=1.179399, validation/num_examples=50000
I0201 07:58:35.381329 139702527031040 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.525621175765991, loss=3.859348773956299
I0201 07:59:19.388363 139702543816448 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.029398202896118, loss=2.94262957572937
I0201 08:00:05.480594 139702527031040 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.9679372310638428, loss=3.9160523414611816
I0201 08:00:51.868476 139702543816448 logging_writer.py:48] [140200] global_step=140200, grad_norm=1.802057147026062, loss=3.0151352882385254
I0201 08:01:37.812774 139702527031040 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.8717718124389648, loss=3.028074026107788
I0201 08:02:23.624919 139702543816448 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.4458601474761963, loss=4.437424182891846
I0201 08:03:09.682729 139702527031040 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.039250135421753, loss=3.096879482269287
I0201 08:03:55.517709 139702543816448 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.1749942302703857, loss=3.94039249420166
I0201 08:04:41.509462 139702527031040 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.512232780456543, loss=4.451140880584717
I0201 08:05:16.966537 139863983413056 spec.py:321] Evaluating on the training split.
I0201 08:05:27.368633 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 08:05:51.435970 139863983413056 spec.py:349] Evaluating on the test split.
I0201 08:05:53.071094 139863983413056 submission_runner.py:408] Time since start: 69118.57s, 	Step: 140779, 	{'train/accuracy': 0.8641406297683716, 'train/loss': 0.7573795318603516, 'validation/accuracy': 0.7615399956703186, 'validation/loss': 1.1744194030761719, 'validation/num_examples': 50000, 'test/accuracy': 0.6406000256538391, 'test/loss': 1.7598754167556763, 'test/num_examples': 10000, 'score': 63896.06881427765, 'total_duration': 69118.56551671028, 'accumulated_submission_time': 63896.06881427765, 'accumulated_eval_time': 5208.587812423706, 'accumulated_logging_time': 6.4807868003845215}
I0201 08:05:53.106713 139702543816448 logging_writer.py:48] [140779] accumulated_eval_time=5208.587812, accumulated_logging_time=6.480787, accumulated_submission_time=63896.068814, global_step=140779, preemption_count=0, score=63896.068814, test/accuracy=0.640600, test/loss=1.759875, test/num_examples=10000, total_duration=69118.565517, train/accuracy=0.864141, train/loss=0.757380, validation/accuracy=0.761540, validation/loss=1.174419, validation/num_examples=50000
I0201 08:06:01.892022 139702527031040 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.1091647148132324, loss=3.062458038330078
I0201 08:06:43.970098 139702543816448 logging_writer.py:48] [140900] global_step=140900, grad_norm=1.9575960636138916, loss=2.9911394119262695
I0201 08:07:29.885133 139702527031040 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.044194221496582, loss=2.9543774127960205
I0201 08:08:16.113050 139702543816448 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.8056327104568481, loss=2.977335214614868
I0201 08:09:02.032076 139702527031040 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.954620122909546, loss=3.3036320209503174
I0201 08:09:48.083297 139702543816448 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.1395621299743652, loss=2.9303927421569824
I0201 08:10:34.794308 139702527031040 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.4213972091674805, loss=4.475756645202637
I0201 08:11:21.083532 139702543816448 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.124986410140991, loss=2.9621548652648926
I0201 08:12:07.214485 139702527031040 logging_writer.py:48] [141600] global_step=141600, grad_norm=1.9377038478851318, loss=2.891871690750122
I0201 08:12:53.234837 139702543816448 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.043458938598633, loss=3.010521411895752
I0201 08:12:53.246602 139863983413056 spec.py:321] Evaluating on the training split.
I0201 08:13:03.744548 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 08:13:26.277530 139863983413056 spec.py:349] Evaluating on the test split.
I0201 08:13:27.927752 139863983413056 submission_runner.py:408] Time since start: 69573.42s, 	Step: 141701, 	{'train/accuracy': 0.8532226085662842, 'train/loss': 0.7790336608886719, 'validation/accuracy': 0.7630999684333801, 'validation/loss': 1.1503957509994507, 'validation/num_examples': 50000, 'test/accuracy': 0.6450000405311584, 'test/loss': 1.7349143028259277, 'test/num_examples': 10000, 'score': 64316.15193653107, 'total_duration': 69573.42215561867, 'accumulated_submission_time': 64316.15193653107, 'accumulated_eval_time': 5243.268939495087, 'accumulated_logging_time': 6.525313138961792}
I0201 08:13:27.971983 139702527031040 logging_writer.py:48] [141701] accumulated_eval_time=5243.268939, accumulated_logging_time=6.525313, accumulated_submission_time=64316.151937, global_step=141701, preemption_count=0, score=64316.151937, test/accuracy=0.645000, test/loss=1.734914, test/num_examples=10000, total_duration=69573.422156, train/accuracy=0.853223, train/loss=0.779034, validation/accuracy=0.763100, validation/loss=1.150396, validation/num_examples=50000
I0201 08:14:08.602516 139702543816448 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.9619165658950806, loss=2.978102684020996
I0201 08:14:54.364939 139702527031040 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.1348094940185547, loss=2.9273810386657715
I0201 08:15:40.359154 139702543816448 logging_writer.py:48] [142000] global_step=142000, grad_norm=1.9859145879745483, loss=2.9682748317718506
I0201 08:16:26.387278 139702527031040 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.9229333400726318, loss=3.653205394744873
I0201 08:17:12.255255 139702543816448 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.6227009296417236, loss=4.507602691650391
I0201 08:17:58.154556 139702527031040 logging_writer.py:48] [142300] global_step=142300, grad_norm=1.971961259841919, loss=3.168146848678589
I0201 08:18:43.804305 139702543816448 logging_writer.py:48] [142400] global_step=142400, grad_norm=1.9825210571289062, loss=2.905759334564209
I0201 08:19:29.802190 139702527031040 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.156782627105713, loss=4.161334991455078
I0201 08:20:15.927354 139702543816448 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.3644683361053467, loss=4.285591125488281
I0201 08:20:28.320739 139863983413056 spec.py:321] Evaluating on the training split.
I0201 08:20:38.811042 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 08:21:00.225127 139863983413056 spec.py:349] Evaluating on the test split.
I0201 08:21:01.863907 139863983413056 submission_runner.py:408] Time since start: 70027.36s, 	Step: 142628, 	{'train/accuracy': 0.8541601300239563, 'train/loss': 0.8001324534416199, 'validation/accuracy': 0.7625600099563599, 'validation/loss': 1.1798006296157837, 'validation/num_examples': 50000, 'test/accuracy': 0.6477000117301941, 'test/loss': 1.7499827146530151, 'test/num_examples': 10000, 'score': 64736.441365003586, 'total_duration': 70027.35831856728, 'accumulated_submission_time': 64736.441365003586, 'accumulated_eval_time': 5276.812092781067, 'accumulated_logging_time': 6.580393314361572}
I0201 08:21:01.919781 139702527031040 logging_writer.py:48] [142628] accumulated_eval_time=5276.812093, accumulated_logging_time=6.580393, accumulated_submission_time=64736.441365, global_step=142628, preemption_count=0, score=64736.441365, test/accuracy=0.647700, test/loss=1.749983, test/num_examples=10000, total_duration=70027.358319, train/accuracy=0.854160, train/loss=0.800132, validation/accuracy=0.762560, validation/loss=1.179801, validation/num_examples=50000
I0201 08:21:31.115716 139702543816448 logging_writer.py:48] [142700] global_step=142700, grad_norm=1.8604098558425903, loss=3.2406885623931885
I0201 08:22:16.051084 139702527031040 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.5068094730377197, loss=4.435894012451172
I0201 08:23:02.380425 139702543816448 logging_writer.py:48] [142900] global_step=142900, grad_norm=1.92306387424469, loss=3.4255003929138184
I0201 08:23:48.461645 139702527031040 logging_writer.py:48] [143000] global_step=143000, grad_norm=1.8938984870910645, loss=3.230027198791504
I0201 08:24:34.333186 139702543816448 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.2611351013183594, loss=4.363378047943115
I0201 08:25:20.112944 139702527031040 logging_writer.py:48] [143200] global_step=143200, grad_norm=1.9115650653839111, loss=3.6501545906066895
I0201 08:26:05.999937 139702543816448 logging_writer.py:48] [143300] global_step=143300, grad_norm=2.2684667110443115, loss=4.104866027832031
I0201 08:26:51.963572 139702527031040 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.9454132318496704, loss=3.0224416255950928
I0201 08:27:37.580494 139702543816448 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.035957098007202, loss=2.9085657596588135
I0201 08:28:01.918337 139863983413056 spec.py:321] Evaluating on the training split.
I0201 08:28:12.211050 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 08:28:34.312098 139863983413056 spec.py:349] Evaluating on the test split.
I0201 08:28:35.957750 139863983413056 submission_runner.py:408] Time since start: 70481.45s, 	Step: 143555, 	{'train/accuracy': 0.8614062070846558, 'train/loss': 0.7344046831130981, 'validation/accuracy': 0.7638199925422668, 'validation/loss': 1.1369210481643677, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.7233561277389526, 'test/num_examples': 10000, 'score': 65156.379346847534, 'total_duration': 70481.45216488838, 'accumulated_submission_time': 65156.379346847534, 'accumulated_eval_time': 5310.8514902591705, 'accumulated_logging_time': 6.647460222244263}
I0201 08:28:35.997869 139702527031040 logging_writer.py:48] [143555] accumulated_eval_time=5310.851490, accumulated_logging_time=6.647460, accumulated_submission_time=65156.379347, global_step=143555, preemption_count=0, score=65156.379347, test/accuracy=0.640700, test/loss=1.723356, test/num_examples=10000, total_duration=70481.452165, train/accuracy=0.861406, train/loss=0.734405, validation/accuracy=0.763820, validation/loss=1.136921, validation/num_examples=50000
I0201 08:28:54.376499 139702543816448 logging_writer.py:48] [143600] global_step=143600, grad_norm=1.9056627750396729, loss=2.9315905570983887
I0201 08:29:38.305662 139702527031040 logging_writer.py:48] [143700] global_step=143700, grad_norm=1.932560682296753, loss=2.9255218505859375
I0201 08:30:24.387647 139702543816448 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.537367582321167, loss=4.423847675323486
I0201 08:31:10.729039 139702527031040 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.2527220249176025, loss=3.763387441635132
I0201 08:31:56.673566 139702543816448 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.503769636154175, loss=4.456688404083252
I0201 08:32:42.462105 139702527031040 logging_writer.py:48] [144100] global_step=144100, grad_norm=1.7990233898162842, loss=2.8938040733337402
I0201 08:33:28.422350 139702543816448 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.119218587875366, loss=3.5885202884674072
I0201 08:34:14.162286 139702527031040 logging_writer.py:48] [144300] global_step=144300, grad_norm=2.3850483894348145, loss=4.485504627227783
I0201 08:35:00.086638 139702543816448 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.187642812728882, loss=3.3763999938964844
I0201 08:35:36.064386 139863983413056 spec.py:321] Evaluating on the training split.
I0201 08:35:46.288169 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 08:36:10.058533 139863983413056 spec.py:349] Evaluating on the test split.
I0201 08:36:11.704411 139863983413056 submission_runner.py:408] Time since start: 70937.20s, 	Step: 144480, 	{'train/accuracy': 0.852832019329071, 'train/loss': 0.7703531980514526, 'validation/accuracy': 0.7658999562263489, 'validation/loss': 1.1399189233779907, 'validation/num_examples': 50000, 'test/accuracy': 0.6437000036239624, 'test/loss': 1.7280446290969849, 'test/num_examples': 10000, 'score': 65576.38734292984, 'total_duration': 70937.19880628586, 'accumulated_submission_time': 65576.38734292984, 'accumulated_eval_time': 5346.491506576538, 'accumulated_logging_time': 6.697022199630737}
I0201 08:36:11.747356 139702527031040 logging_writer.py:48] [144480] accumulated_eval_time=5346.491507, accumulated_logging_time=6.697022, accumulated_submission_time=65576.387343, global_step=144480, preemption_count=0, score=65576.387343, test/accuracy=0.643700, test/loss=1.728045, test/num_examples=10000, total_duration=70937.198806, train/accuracy=0.852832, train/loss=0.770353, validation/accuracy=0.765900, validation/loss=1.139919, validation/num_examples=50000
I0201 08:36:20.132126 139702543816448 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.2413299083709717, loss=3.931809902191162
I0201 08:37:02.111030 139702527031040 logging_writer.py:48] [144600] global_step=144600, grad_norm=1.9352282285690308, loss=3.361353874206543
I0201 08:37:47.951859 139702543816448 logging_writer.py:48] [144700] global_step=144700, grad_norm=1.9796005487442017, loss=3.5625572204589844
I0201 08:38:34.068941 139702527031040 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.0876991748809814, loss=3.2055306434631348
I0201 08:39:19.852729 139702543816448 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.018627166748047, loss=2.9994800090789795
I0201 08:40:05.914855 139702527031040 logging_writer.py:48] [145000] global_step=145000, grad_norm=1.859738826751709, loss=3.37593150138855
I0201 08:40:52.056936 139702543816448 logging_writer.py:48] [145100] global_step=145100, grad_norm=2.1964831352233887, loss=3.131906509399414
I0201 08:41:38.388364 139702527031040 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.2158424854278564, loss=2.947545051574707
I0201 08:42:24.231998 139702543816448 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.8150827884674072, loss=4.41746711730957
I0201 08:43:10.138484 139702527031040 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.1279802322387695, loss=2.9200568199157715
I0201 08:43:12.086940 139863983413056 spec.py:321] Evaluating on the training split.
I0201 08:43:22.382108 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 08:43:47.401043 139863983413056 spec.py:349] Evaluating on the test split.
I0201 08:43:49.045298 139863983413056 submission_runner.py:408] Time since start: 71394.54s, 	Step: 145406, 	{'train/accuracy': 0.8582421541213989, 'train/loss': 0.7630525827407837, 'validation/accuracy': 0.7664799690246582, 'validation/loss': 1.1516687870025635, 'validation/num_examples': 50000, 'test/accuracy': 0.6492000222206116, 'test/loss': 1.7340401411056519, 'test/num_examples': 10000, 'score': 65996.66863155365, 'total_duration': 71394.53972387314, 'accumulated_submission_time': 65996.66863155365, 'accumulated_eval_time': 5383.4498608112335, 'accumulated_logging_time': 6.750396251678467}
I0201 08:43:49.081260 139702543816448 logging_writer.py:48] [145406] accumulated_eval_time=5383.449861, accumulated_logging_time=6.750396, accumulated_submission_time=65996.668632, global_step=145406, preemption_count=0, score=65996.668632, test/accuracy=0.649200, test/loss=1.734040, test/num_examples=10000, total_duration=71394.539724, train/accuracy=0.858242, train/loss=0.763053, validation/accuracy=0.766480, validation/loss=1.151669, validation/num_examples=50000
I0201 08:44:27.567914 139702527031040 logging_writer.py:48] [145500] global_step=145500, grad_norm=2.0009653568267822, loss=3.2140109539031982
I0201 08:45:13.276656 139702543816448 logging_writer.py:48] [145600] global_step=145600, grad_norm=1.8562874794006348, loss=3.5644450187683105
I0201 08:45:59.257304 139702527031040 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.0368967056274414, loss=2.8972582817077637
I0201 08:46:45.278000 139702543816448 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.069423198699951, loss=3.0094354152679443
I0201 08:47:30.992751 139702527031040 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.508983850479126, loss=4.460750579833984
I0201 08:48:16.773601 139702543816448 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.1071085929870605, loss=2.909723997116089
I0201 08:49:02.719562 139702527031040 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.017423391342163, loss=2.9084553718566895
I0201 08:49:49.049362 139702543816448 logging_writer.py:48] [146200] global_step=146200, grad_norm=1.9830492734909058, loss=2.8695857524871826
I0201 08:50:35.129215 139702527031040 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.8571547269821167, loss=2.8409523963928223
I0201 08:50:49.402518 139863983413056 spec.py:321] Evaluating on the training split.
I0201 08:50:59.756285 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 08:51:24.215215 139863983413056 spec.py:349] Evaluating on the test split.
I0201 08:51:25.868029 139863983413056 submission_runner.py:408] Time since start: 71851.36s, 	Step: 146333, 	{'train/accuracy': 0.8623632788658142, 'train/loss': 0.7401703000068665, 'validation/accuracy': 0.7676799893379211, 'validation/loss': 1.1452099084854126, 'validation/num_examples': 50000, 'test/accuracy': 0.6490000486373901, 'test/loss': 1.7267446517944336, 'test/num_examples': 10000, 'score': 66416.9311683178, 'total_duration': 71851.3624317646, 'accumulated_submission_time': 66416.9311683178, 'accumulated_eval_time': 5419.915347337723, 'accumulated_logging_time': 6.796186208724976}
I0201 08:51:25.910381 139702543816448 logging_writer.py:48] [146333] accumulated_eval_time=5419.915347, accumulated_logging_time=6.796186, accumulated_submission_time=66416.931168, global_step=146333, preemption_count=0, score=66416.931168, test/accuracy=0.649000, test/loss=1.726745, test/num_examples=10000, total_duration=71851.362432, train/accuracy=0.862363, train/loss=0.740170, validation/accuracy=0.767680, validation/loss=1.145210, validation/num_examples=50000
I0201 08:51:53.054867 139702527031040 logging_writer.py:48] [146400] global_step=146400, grad_norm=2.0489046573638916, loss=3.5389952659606934
I0201 08:52:38.104649 139702543816448 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.5958940982818604, loss=4.446780204772949
I0201 08:53:24.225598 139702527031040 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.077681303024292, loss=2.9237914085388184
I0201 08:54:10.577036 139702543816448 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.129312038421631, loss=2.9421277046203613
I0201 08:54:56.169049 139702527031040 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.157674551010132, loss=4.428258895874023
I0201 08:55:42.091325 139702543816448 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.057021141052246, loss=4.146238327026367
I0201 08:56:27.985368 139702527031040 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.5374629497528076, loss=3.4832258224487305
I0201 08:57:13.712817 139702543816448 logging_writer.py:48] [147100] global_step=147100, grad_norm=1.9420533180236816, loss=3.47223162651062
I0201 08:57:59.815164 139702527031040 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.096437931060791, loss=2.914008617401123
I0201 08:58:26.170952 139863983413056 spec.py:321] Evaluating on the training split.
I0201 08:58:36.416644 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 08:58:58.911670 139863983413056 spec.py:349] Evaluating on the test split.
I0201 08:59:00.558280 139863983413056 submission_runner.py:408] Time since start: 72306.05s, 	Step: 147259, 	{'train/accuracy': 0.8596093654632568, 'train/loss': 0.7663670778274536, 'validation/accuracy': 0.7688199877738953, 'validation/loss': 1.1503161191940308, 'validation/num_examples': 50000, 'test/accuracy': 0.6520000100135803, 'test/loss': 1.7251871824264526, 'test/num_examples': 10000, 'score': 66837.13448381424, 'total_duration': 72306.05270385742, 'accumulated_submission_time': 66837.13448381424, 'accumulated_eval_time': 5454.302681922913, 'accumulated_logging_time': 6.847928524017334}
I0201 08:59:00.595932 139702543816448 logging_writer.py:48] [147259] accumulated_eval_time=5454.302682, accumulated_logging_time=6.847929, accumulated_submission_time=66837.134484, global_step=147259, preemption_count=0, score=66837.134484, test/accuracy=0.652000, test/loss=1.725187, test/num_examples=10000, total_duration=72306.052704, train/accuracy=0.859609, train/loss=0.766367, validation/accuracy=0.768820, validation/loss=1.150316, validation/num_examples=50000
I0201 08:59:17.358839 139702527031040 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.30165958404541, loss=4.087636470794678
I0201 09:00:00.626008 139702543816448 logging_writer.py:48] [147400] global_step=147400, grad_norm=1.9544007778167725, loss=3.158496379852295
I0201 09:00:46.662190 139702527031040 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.2891674041748047, loss=4.158200740814209
I0201 09:01:32.663432 139702543816448 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.3534204959869385, loss=4.255423545837402
I0201 09:02:18.771071 139702527031040 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.0292513370513916, loss=3.953737735748291
I0201 09:03:04.617459 139702543816448 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.6526758670806885, loss=4.33137321472168
I0201 09:03:50.260955 139702527031040 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.0678694248199463, loss=2.9124581813812256
I0201 09:04:37.297498 139702543816448 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.189363718032837, loss=2.9444355964660645
I0201 09:05:23.534631 139702527031040 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.1957805156707764, loss=4.290129661560059
I0201 09:06:00.726732 139863983413056 spec.py:321] Evaluating on the training split.
I0201 09:06:11.530248 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 09:06:37.189883 139863983413056 spec.py:349] Evaluating on the test split.
I0201 09:06:38.823137 139863983413056 submission_runner.py:408] Time since start: 72764.32s, 	Step: 148183, 	{'train/accuracy': 0.8612304329872131, 'train/loss': 0.7369097471237183, 'validation/accuracy': 0.7663999795913696, 'validation/loss': 1.1357945203781128, 'validation/num_examples': 50000, 'test/accuracy': 0.6517000198364258, 'test/loss': 1.709851622581482, 'test/num_examples': 10000, 'score': 67257.20768213272, 'total_duration': 72764.31754755974, 'accumulated_submission_time': 67257.20768213272, 'accumulated_eval_time': 5492.399060964584, 'accumulated_logging_time': 6.8949620723724365}
I0201 09:06:38.863423 139702543816448 logging_writer.py:48] [148183] accumulated_eval_time=5492.399061, accumulated_logging_time=6.894962, accumulated_submission_time=67257.207682, global_step=148183, preemption_count=0, score=67257.207682, test/accuracy=0.651700, test/loss=1.709852, test/num_examples=10000, total_duration=72764.317548, train/accuracy=0.861230, train/loss=0.736910, validation/accuracy=0.766400, validation/loss=1.135795, validation/num_examples=50000
I0201 09:06:46.054580 139702527031040 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.1922779083251953, loss=2.9400174617767334
I0201 09:07:28.241114 139702543816448 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.0906660556793213, loss=2.8632397651672363
I0201 09:08:14.191919 139702527031040 logging_writer.py:48] [148400] global_step=148400, grad_norm=1.9878772497177124, loss=2.9349632263183594
I0201 09:09:00.012131 139702543816448 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.110724449157715, loss=2.83284330368042
I0201 09:09:46.029930 139702527031040 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.373122215270996, loss=4.042704105377197
I0201 09:10:32.102162 139702543816448 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.3249433040618896, loss=3.9233388900756836
I0201 09:11:18.052614 139702527031040 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.213564157485962, loss=3.737675428390503
I0201 09:12:04.193369 139702543816448 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.114234685897827, loss=3.641584873199463
I0201 09:12:50.097299 139702527031040 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.546147108078003, loss=4.211536884307861
I0201 09:13:35.894212 139702543816448 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.0537636280059814, loss=2.939418315887451
I0201 09:13:39.181296 139863983413056 spec.py:321] Evaluating on the training split.
I0201 09:13:49.778609 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 09:14:12.799844 139863983413056 spec.py:349] Evaluating on the test split.
I0201 09:14:14.440914 139863983413056 submission_runner.py:408] Time since start: 73219.94s, 	Step: 149109, 	{'train/accuracy': 0.8688085675239563, 'train/loss': 0.7321873903274536, 'validation/accuracy': 0.7706199884414673, 'validation/loss': 1.136370062828064, 'validation/num_examples': 50000, 'test/accuracy': 0.6541000604629517, 'test/loss': 1.7144485712051392, 'test/num_examples': 10000, 'score': 67677.46754312515, 'total_duration': 73219.93533587456, 'accumulated_submission_time': 67677.46754312515, 'accumulated_eval_time': 5527.658671617508, 'accumulated_logging_time': 6.944510221481323}
I0201 09:14:14.477959 139702527031040 logging_writer.py:48] [149109] accumulated_eval_time=5527.658672, accumulated_logging_time=6.944510, accumulated_submission_time=67677.467543, global_step=149109, preemption_count=0, score=67677.467543, test/accuracy=0.654100, test/loss=1.714449, test/num_examples=10000, total_duration=73219.935336, train/accuracy=0.868809, train/loss=0.732187, validation/accuracy=0.770620, validation/loss=1.136370, validation/num_examples=50000
I0201 09:14:51.706078 139702543816448 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.18276047706604, loss=3.240974187850952
I0201 09:15:37.585211 139702527031040 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.1281914710998535, loss=2.8555052280426025
I0201 09:16:23.735188 139702543816448 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.226313829421997, loss=2.9862289428710938
I0201 09:17:09.846907 139702527031040 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.1282289028167725, loss=3.1785900592803955
I0201 09:17:55.444718 139702543816448 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.091637134552002, loss=2.8468475341796875
I0201 09:18:41.505249 139702527031040 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.2027010917663574, loss=2.944470167160034
I0201 09:19:27.360926 139702543816448 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.337709903717041, loss=2.868577718734741
I0201 09:20:13.542271 139702527031040 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.294259548187256, loss=2.9135961532592773
I0201 09:20:59.289744 139702543816448 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.4397966861724854, loss=4.2416157722473145
I0201 09:21:14.614776 139863983413056 spec.py:321] Evaluating on the training split.
I0201 09:21:25.213823 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 09:21:48.361365 139863983413056 spec.py:349] Evaluating on the test split.
I0201 09:21:49.999695 139863983413056 submission_runner.py:408] Time since start: 73675.49s, 	Step: 150035, 	{'train/accuracy': 0.8769726157188416, 'train/loss': 0.7250710725784302, 'validation/accuracy': 0.7684800028800964, 'validation/loss': 1.159796953201294, 'validation/num_examples': 50000, 'test/accuracy': 0.653700053691864, 'test/loss': 1.7364274263381958, 'test/num_examples': 10000, 'score': 68097.54691076279, 'total_duration': 73675.49411344528, 'accumulated_submission_time': 68097.54691076279, 'accumulated_eval_time': 5563.043598890305, 'accumulated_logging_time': 6.990314960479736}
I0201 09:21:50.041119 139702527031040 logging_writer.py:48] [150035] accumulated_eval_time=5563.043599, accumulated_logging_time=6.990315, accumulated_submission_time=68097.546911, global_step=150035, preemption_count=0, score=68097.546911, test/accuracy=0.653700, test/loss=1.736427, test/num_examples=10000, total_duration=73675.494113, train/accuracy=0.876973, train/loss=0.725071, validation/accuracy=0.768480, validation/loss=1.159797, validation/num_examples=50000
I0201 09:22:16.421304 139702543816448 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.120720624923706, loss=2.8905837535858154
I0201 09:23:01.238477 139702527031040 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.5895190238952637, loss=4.104746341705322
I0201 09:23:47.219457 139702543816448 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.6179537773132324, loss=4.334230899810791
I0201 09:24:33.778438 139702527031040 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.171429395675659, loss=3.649733543395996
I0201 09:25:19.736154 139702543816448 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.145683765411377, loss=2.847839593887329
I0201 09:26:05.671534 139702527031040 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.0833773612976074, loss=3.3518004417419434
I0201 09:26:51.421611 139702543816448 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.023618698120117, loss=3.095139503479004
I0201 09:27:37.182988 139702527031040 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.151341676712036, loss=2.8886141777038574
I0201 09:28:23.323018 139702543816448 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.2858598232269287, loss=2.95104718208313
I0201 09:28:50.489499 139863983413056 spec.py:321] Evaluating on the training split.
I0201 09:29:01.015926 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 09:29:27.598651 139863983413056 spec.py:349] Evaluating on the test split.
I0201 09:29:29.239664 139863983413056 submission_runner.py:408] Time since start: 74134.73s, 	Step: 150961, 	{'train/accuracy': 0.8651366829872131, 'train/loss': 0.7272911667823792, 'validation/accuracy': 0.7701199650764465, 'validation/loss': 1.1263177394866943, 'validation/num_examples': 50000, 'test/accuracy': 0.6527000069618225, 'test/loss': 1.708383321762085, 'test/num_examples': 10000, 'score': 68517.9379067421, 'total_duration': 74134.7340862751, 'accumulated_submission_time': 68517.9379067421, 'accumulated_eval_time': 5601.793773651123, 'accumulated_logging_time': 7.040832042694092}
I0201 09:29:29.276754 139702527031040 logging_writer.py:48] [150961] accumulated_eval_time=5601.793774, accumulated_logging_time=7.040832, accumulated_submission_time=68517.937907, global_step=150961, preemption_count=0, score=68517.937907, test/accuracy=0.652700, test/loss=1.708383, test/num_examples=10000, total_duration=74134.734086, train/accuracy=0.865137, train/loss=0.727291, validation/accuracy=0.770120, validation/loss=1.126318, validation/num_examples=50000
I0201 09:29:45.258362 139702543816448 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.1351072788238525, loss=3.3879964351654053
I0201 09:30:28.395498 139702527031040 logging_writer.py:48] [151100] global_step=151100, grad_norm=1.9808552265167236, loss=3.268319606781006
I0201 09:31:14.143065 139702543816448 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.0856411457061768, loss=2.8419888019561768
I0201 09:32:00.217731 139702527031040 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.2307188510894775, loss=2.8221027851104736
I0201 09:32:46.333744 139702543816448 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.1760406494140625, loss=2.9745934009552
I0201 09:33:32.100512 139702527031040 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.9750053882598877, loss=4.208202362060547
I0201 09:34:17.910543 139702543816448 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.020948886871338, loss=2.86527681350708
I0201 09:35:03.553819 139702527031040 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.164384603500366, loss=3.0217247009277344
I0201 09:35:49.449180 139702543816448 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.2538466453552246, loss=3.8552234172821045
I0201 09:36:29.473364 139863983413056 spec.py:321] Evaluating on the training split.
I0201 09:36:40.657350 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 09:36:59.932194 139863983413056 spec.py:349] Evaluating on the test split.
I0201 09:37:01.569847 139863983413056 submission_runner.py:408] Time since start: 74587.06s, 	Step: 151889, 	{'train/accuracy': 0.8691796660423279, 'train/loss': 0.7026453018188477, 'validation/accuracy': 0.770039975643158, 'validation/loss': 1.108427882194519, 'validation/num_examples': 50000, 'test/accuracy': 0.6552000045776367, 'test/loss': 1.6844028234481812, 'test/num_examples': 10000, 'score': 68938.07565045357, 'total_duration': 74587.0642721653, 'accumulated_submission_time': 68938.07565045357, 'accumulated_eval_time': 5633.890254974365, 'accumulated_logging_time': 7.088500022888184}
I0201 09:37:01.607372 139702527031040 logging_writer.py:48] [151889] accumulated_eval_time=5633.890255, accumulated_logging_time=7.088500, accumulated_submission_time=68938.075650, global_step=151889, preemption_count=0, score=68938.075650, test/accuracy=0.655200, test/loss=1.684403, test/num_examples=10000, total_duration=74587.064272, train/accuracy=0.869180, train/loss=0.702645, validation/accuracy=0.770040, validation/loss=1.108428, validation/num_examples=50000
I0201 09:37:06.403844 139702543816448 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.7942848205566406, loss=4.254463195800781
I0201 09:37:47.781172 139702527031040 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.148157835006714, loss=3.5161476135253906
I0201 09:38:33.708405 139702543816448 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.2049942016601562, loss=2.863076686859131
I0201 09:39:19.896365 139702527031040 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.078117609024048, loss=3.558241367340088
I0201 09:40:06.059392 139702543816448 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.9974753856658936, loss=4.187319278717041
I0201 09:40:51.934932 139702527031040 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.703864097595215, loss=4.184176921844482
I0201 09:41:37.967688 139702543816448 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.365745782852173, loss=2.8900671005249023
I0201 09:42:23.927146 139702527031040 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.21398663520813, loss=2.9786272048950195
I0201 09:43:09.979742 139702543816448 logging_writer.py:48] [152700] global_step=152700, grad_norm=1.9964439868927002, loss=3.008287191390991
I0201 09:43:55.879181 139702527031040 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.1183717250823975, loss=3.131911516189575
I0201 09:44:01.687874 139863983413056 spec.py:321] Evaluating on the training split.
I0201 09:44:12.414324 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 09:44:33.981708 139863983413056 spec.py:349] Evaluating on the test split.
I0201 09:44:35.623070 139863983413056 submission_runner.py:408] Time since start: 75041.12s, 	Step: 152814, 	{'train/accuracy': 0.8727929592132568, 'train/loss': 0.7038527727127075, 'validation/accuracy': 0.7734599709510803, 'validation/loss': 1.1162408590316772, 'validation/num_examples': 50000, 'test/accuracy': 0.6562000513076782, 'test/loss': 1.7009633779525757, 'test/num_examples': 10000, 'score': 69358.09606146812, 'total_duration': 75041.11749482155, 'accumulated_submission_time': 69358.09606146812, 'accumulated_eval_time': 5667.825475215912, 'accumulated_logging_time': 7.13709831237793}
I0201 09:44:35.664586 139702543816448 logging_writer.py:48] [152814] accumulated_eval_time=5667.825475, accumulated_logging_time=7.137098, accumulated_submission_time=69358.096061, global_step=152814, preemption_count=0, score=69358.096061, test/accuracy=0.656200, test/loss=1.700963, test/num_examples=10000, total_duration=75041.117495, train/accuracy=0.872793, train/loss=0.703853, validation/accuracy=0.773460, validation/loss=1.116241, validation/num_examples=50000
I0201 09:45:10.624102 139702527031040 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.130423069000244, loss=2.9249205589294434
I0201 09:45:56.195000 139702543816448 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.261626958847046, loss=2.847449779510498
I0201 09:46:42.018551 139702527031040 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.1571757793426514, loss=3.5034828186035156
I0201 09:47:28.120563 139702543816448 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.6130263805389404, loss=4.060330867767334
I0201 09:48:13.671630 139702527031040 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.290400743484497, loss=3.4226043224334717
I0201 09:48:59.509269 139702543816448 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.1661932468414307, loss=3.6073973178863525
I0201 09:49:45.469295 139702527031040 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.448317050933838, loss=3.5426602363586426
I0201 09:50:31.253299 139702543816448 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.142045497894287, loss=2.884714126586914
I0201 09:51:17.241326 139702527031040 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.142937183380127, loss=2.8077445030212402
I0201 09:51:35.815881 139863983413056 spec.py:321] Evaluating on the training split.
I0201 09:51:46.472961 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 09:52:07.961637 139863983413056 spec.py:349] Evaluating on the test split.
I0201 09:52:09.589055 139863983413056 submission_runner.py:408] Time since start: 75495.08s, 	Step: 153742, 	{'train/accuracy': 0.8701562285423279, 'train/loss': 0.7306351065635681, 'validation/accuracy': 0.772379994392395, 'validation/loss': 1.131606936454773, 'validation/num_examples': 50000, 'test/accuracy': 0.6551000475883484, 'test/loss': 1.710476279258728, 'test/num_examples': 10000, 'score': 69778.18906927109, 'total_duration': 75495.08348155022, 'accumulated_submission_time': 69778.18906927109, 'accumulated_eval_time': 5701.598674058914, 'accumulated_logging_time': 7.188451766967773}
I0201 09:52:09.629337 139702543816448 logging_writer.py:48] [153742] accumulated_eval_time=5701.598674, accumulated_logging_time=7.188452, accumulated_submission_time=69778.189069, global_step=153742, preemption_count=0, score=69778.189069, test/accuracy=0.655100, test/loss=1.710476, test/num_examples=10000, total_duration=75495.083482, train/accuracy=0.870156, train/loss=0.730635, validation/accuracy=0.772380, validation/loss=1.131607, validation/num_examples=50000
I0201 09:52:33.190147 139702527031040 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.1439459323883057, loss=2.845522165298462
I0201 09:53:17.508508 139702543816448 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.0732920169830322, loss=3.0790648460388184
I0201 09:54:03.617733 139702527031040 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.1823127269744873, loss=2.8508598804473877
I0201 09:54:50.013221 139702543816448 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.2969601154327393, loss=2.8912250995635986
I0201 09:55:35.720190 139702527031040 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.4287028312683105, loss=2.9312987327575684
I0201 09:56:21.417812 139702543816448 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.1884355545043945, loss=2.9345486164093018
I0201 09:57:07.778853 139702527031040 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.269541025161743, loss=3.2234508991241455
I0201 09:57:53.630217 139702543816448 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.87835955619812, loss=4.194626808166504
I0201 09:58:39.719890 139702527031040 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.2594640254974365, loss=2.9298582077026367
I0201 09:59:09.613279 139863983413056 spec.py:321] Evaluating on the training split.
I0201 09:59:19.976755 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 09:59:43.854737 139863983413056 spec.py:349] Evaluating on the test split.
I0201 09:59:45.499618 139863983413056 submission_runner.py:408] Time since start: 75950.99s, 	Step: 154667, 	{'train/accuracy': 0.8733202815055847, 'train/loss': 0.6871562004089355, 'validation/accuracy': 0.7735199928283691, 'validation/loss': 1.1001402139663696, 'validation/num_examples': 50000, 'test/accuracy': 0.6605000495910645, 'test/loss': 1.6771475076675415, 'test/num_examples': 10000, 'score': 70198.11680269241, 'total_duration': 75950.9940161705, 'accumulated_submission_time': 70198.11680269241, 'accumulated_eval_time': 5737.48500084877, 'accumulated_logging_time': 7.237675905227661}
I0201 09:59:45.548614 139702543816448 logging_writer.py:48] [154667] accumulated_eval_time=5737.485001, accumulated_logging_time=7.237676, accumulated_submission_time=70198.116803, global_step=154667, preemption_count=0, score=70198.116803, test/accuracy=0.660500, test/loss=1.677148, test/num_examples=10000, total_duration=75950.994016, train/accuracy=0.873320, train/loss=0.687156, validation/accuracy=0.773520, validation/loss=1.100140, validation/num_examples=50000
I0201 09:59:59.125187 139702527031040 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.176935911178589, loss=2.80057430267334
I0201 10:00:42.302474 139702543816448 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.2366268634796143, loss=3.219620704650879
I0201 10:01:28.293333 139702527031040 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.2880499362945557, loss=3.125690460205078
I0201 10:02:14.635595 139702543816448 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.240541696548462, loss=4.282034873962402
I0201 10:03:00.581130 139702527031040 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.324537515640259, loss=2.834669589996338
I0201 10:03:47.241688 139702543816448 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.1421706676483154, loss=3.05885910987854
I0201 10:04:33.665862 139702527031040 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.1831226348876953, loss=2.8676233291625977
I0201 10:05:19.680430 139702543816448 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.2143208980560303, loss=3.587888240814209
I0201 10:06:05.985066 139702527031040 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.2266345024108887, loss=2.8453469276428223
I0201 10:06:45.916284 139863983413056 spec.py:321] Evaluating on the training split.
I0201 10:06:56.376577 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 10:07:18.372640 139863983413056 spec.py:349] Evaluating on the test split.
I0201 10:07:20.013582 139863983413056 submission_runner.py:408] Time since start: 76405.51s, 	Step: 155588, 	{'train/accuracy': 0.874804675579071, 'train/loss': 0.6974979043006897, 'validation/accuracy': 0.7736999988555908, 'validation/loss': 1.112828254699707, 'validation/num_examples': 50000, 'test/accuracy': 0.6607000231742859, 'test/loss': 1.696984887123108, 'test/num_examples': 10000, 'score': 70618.42568159103, 'total_duration': 76405.50797510147, 'accumulated_submission_time': 70618.42568159103, 'accumulated_eval_time': 5771.582266569138, 'accumulated_logging_time': 7.296308755874634}
I0201 10:07:20.057375 139702543816448 logging_writer.py:48] [155588] accumulated_eval_time=5771.582267, accumulated_logging_time=7.296309, accumulated_submission_time=70618.425682, global_step=155588, preemption_count=0, score=70618.425682, test/accuracy=0.660700, test/loss=1.696985, test/num_examples=10000, total_duration=76405.507975, train/accuracy=0.874805, train/loss=0.697498, validation/accuracy=0.773700, validation/loss=1.112828, validation/num_examples=50000
I0201 10:07:25.245433 139702527031040 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.4528300762176514, loss=3.617727756500244
I0201 10:08:06.997904 139702543816448 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.318828821182251, loss=2.858438730239868
I0201 10:08:52.720319 139702527031040 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.5780224800109863, loss=3.933393955230713
I0201 10:09:38.874754 139702543816448 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.297527313232422, loss=3.5174543857574463
I0201 10:10:25.146118 139702527031040 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.1155059337615967, loss=2.899665117263794
I0201 10:11:10.995821 139702543816448 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.367675542831421, loss=3.949603319168091
I0201 10:11:56.825348 139702527031040 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.379289150238037, loss=3.605123519897461
I0201 10:12:42.724565 139702543816448 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.3215115070343018, loss=3.7022430896759033
I0201 10:13:29.010995 139702527031040 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.3334994316101074, loss=3.6623287200927734
I0201 10:14:15.438096 139702543816448 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.3055808544158936, loss=2.958890676498413
I0201 10:14:20.158986 139863983413056 spec.py:321] Evaluating on the training split.
I0201 10:14:30.962908 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 10:14:52.898663 139863983413056 spec.py:349] Evaluating on the test split.
I0201 10:14:54.534663 139863983413056 submission_runner.py:408] Time since start: 76860.03s, 	Step: 156512, 	{'train/accuracy': 0.8740820288658142, 'train/loss': 0.7038549780845642, 'validation/accuracy': 0.7731800079345703, 'validation/loss': 1.112505555152893, 'validation/num_examples': 50000, 'test/accuracy': 0.6598000526428223, 'test/loss': 1.6827988624572754, 'test/num_examples': 10000, 'score': 71038.46853804588, 'total_duration': 76860.0290813446, 'accumulated_submission_time': 71038.46853804588, 'accumulated_eval_time': 5805.957935094833, 'accumulated_logging_time': 7.350852727890015}
I0201 10:14:54.576423 139702527031040 logging_writer.py:48] [156512] accumulated_eval_time=5805.957935, accumulated_logging_time=7.350853, accumulated_submission_time=71038.468538, global_step=156512, preemption_count=0, score=71038.468538, test/accuracy=0.659800, test/loss=1.682799, test/num_examples=10000, total_duration=76860.029081, train/accuracy=0.874082, train/loss=0.703855, validation/accuracy=0.773180, validation/loss=1.112506, validation/num_examples=50000
I0201 10:15:30.627077 139702543816448 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.3528153896331787, loss=2.8831071853637695
I0201 10:16:16.532086 139702527031040 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.4394872188568115, loss=3.007004737854004
I0201 10:17:02.818317 139702543816448 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.362395763397217, loss=2.8928301334381104
I0201 10:17:49.105000 139702527031040 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.4985530376434326, loss=4.139931678771973
I0201 10:18:34.898467 139702543816448 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.171380043029785, loss=3.3968379497528076
I0201 10:19:20.793509 139702527031040 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.328505754470825, loss=2.8888609409332275
I0201 10:20:06.888311 139702543816448 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.115873098373413, loss=2.9076008796691895
I0201 10:20:53.025588 139702527031040 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.353926658630371, loss=2.8499019145965576
I0201 10:21:39.328819 139702543816448 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.0827062129974365, loss=3.1014697551727295
I0201 10:21:54.871633 139863983413056 spec.py:321] Evaluating on the training split.
I0201 10:22:05.452526 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 10:22:28.711474 139863983413056 spec.py:349] Evaluating on the test split.
I0201 10:22:30.337955 139863983413056 submission_runner.py:408] Time since start: 77315.83s, 	Step: 157436, 	{'train/accuracy': 0.87367182970047, 'train/loss': 0.6964795589447021, 'validation/accuracy': 0.7759000062942505, 'validation/loss': 1.108446717262268, 'validation/num_examples': 50000, 'test/accuracy': 0.6623000502586365, 'test/loss': 1.684056282043457, 'test/num_examples': 10000, 'score': 71458.70658421516, 'total_duration': 77315.83238148689, 'accumulated_submission_time': 71458.70658421516, 'accumulated_eval_time': 5841.424285888672, 'accumulated_logging_time': 7.40127420425415}
I0201 10:22:30.376039 139702527031040 logging_writer.py:48] [157436] accumulated_eval_time=5841.424286, accumulated_logging_time=7.401274, accumulated_submission_time=71458.706584, global_step=157436, preemption_count=0, score=71458.706584, test/accuracy=0.662300, test/loss=1.684056, test/num_examples=10000, total_duration=77315.832381, train/accuracy=0.873672, train/loss=0.696480, validation/accuracy=0.775900, validation/loss=1.108447, validation/num_examples=50000
I0201 10:22:56.334524 139702543816448 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.2545135021209717, loss=2.781357526779175
I0201 10:23:40.861585 139702527031040 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.173738718032837, loss=3.114525318145752
I0201 10:24:27.234521 139702543816448 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.3222997188568115, loss=2.8107099533081055
I0201 10:25:13.350014 139702527031040 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.1419215202331543, loss=2.853797197341919
I0201 10:25:59.063137 139702543816448 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.293006420135498, loss=2.9929966926574707
I0201 10:26:45.192768 139702527031040 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.399658441543579, loss=2.955888032913208
I0201 10:27:31.088747 139702543816448 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.277498483657837, loss=2.9109456539154053
I0201 10:28:16.892675 139702527031040 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.2249014377593994, loss=2.9731392860412598
I0201 10:29:03.092670 139702543816448 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.262204885482788, loss=3.0269594192504883
I0201 10:29:30.448437 139863983413056 spec.py:321] Evaluating on the training split.
I0201 10:29:40.867008 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 10:30:02.991839 139863983413056 spec.py:349] Evaluating on the test split.
I0201 10:30:04.632166 139863983413056 submission_runner.py:408] Time since start: 77770.13s, 	Step: 158361, 	{'train/accuracy': 0.8794335722923279, 'train/loss': 0.675494372844696, 'validation/accuracy': 0.7756199836730957, 'validation/loss': 1.0992703437805176, 'validation/num_examples': 50000, 'test/accuracy': 0.659000039100647, 'test/loss': 1.6712520122528076, 'test/num_examples': 10000, 'score': 71878.72174358368, 'total_duration': 77770.12656998634, 'accumulated_submission_time': 71878.72174358368, 'accumulated_eval_time': 5875.608007192612, 'accumulated_logging_time': 7.447989463806152}
I0201 10:30:04.677672 139702527031040 logging_writer.py:48] [158361] accumulated_eval_time=5875.608007, accumulated_logging_time=7.447989, accumulated_submission_time=71878.721744, global_step=158361, preemption_count=0, score=71878.721744, test/accuracy=0.659000, test/loss=1.671252, test/num_examples=10000, total_duration=77770.126570, train/accuracy=0.879434, train/loss=0.675494, validation/accuracy=0.775620, validation/loss=1.099270, validation/num_examples=50000
I0201 10:30:20.650729 139702543816448 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.2857441902160645, loss=3.095017433166504
I0201 10:31:03.942083 139702527031040 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.2784810066223145, loss=2.800233840942383
I0201 10:31:50.274959 139702543816448 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.3086018562316895, loss=2.86655855178833
I0201 10:32:36.540816 139702527031040 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.150794744491577, loss=3.1818313598632812
I0201 10:33:22.586623 139702543816448 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.3602824211120605, loss=3.3586297035217285
I0201 10:34:08.961360 139702527031040 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.2653725147247314, loss=2.7945709228515625
I0201 10:34:54.918087 139702543816448 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.362304449081421, loss=3.682563543319702
I0201 10:35:40.844818 139702527031040 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.3497049808502197, loss=3.8366146087646484
I0201 10:36:26.737400 139702543816448 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.4667322635650635, loss=3.8849713802337646
I0201 10:37:04.675707 139863983413056 spec.py:321] Evaluating on the training split.
I0201 10:37:14.949703 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 10:37:40.743743 139863983413056 spec.py:349] Evaluating on the test split.
I0201 10:37:42.380769 139863983413056 submission_runner.py:408] Time since start: 78227.88s, 	Step: 159284, 	{'train/accuracy': 0.8847265243530273, 'train/loss': 0.6685106754302979, 'validation/accuracy': 0.7770799994468689, 'validation/loss': 1.1113394498825073, 'validation/num_examples': 50000, 'test/accuracy': 0.6622000336647034, 'test/loss': 1.6857061386108398, 'test/num_examples': 10000, 'score': 72298.6612625122, 'total_duration': 78227.87518262863, 'accumulated_submission_time': 72298.6612625122, 'accumulated_eval_time': 5913.313067674637, 'accumulated_logging_time': 7.503507614135742}
I0201 10:37:42.420789 139702527031040 logging_writer.py:48] [159284] accumulated_eval_time=5913.313068, accumulated_logging_time=7.503508, accumulated_submission_time=72298.661263, global_step=159284, preemption_count=0, score=72298.661263, test/accuracy=0.662200, test/loss=1.685706, test/num_examples=10000, total_duration=78227.875183, train/accuracy=0.884727, train/loss=0.668511, validation/accuracy=0.777080, validation/loss=1.111339, validation/num_examples=50000
I0201 10:37:49.204433 139702543816448 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.2664034366607666, loss=2.7994842529296875
I0201 10:38:31.483379 139702527031040 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.4647061824798584, loss=3.9610164165496826
I0201 10:39:17.292030 139702543816448 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.21606707572937, loss=3.147948741912842
I0201 10:40:03.317807 139702527031040 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.2435665130615234, loss=2.750028610229492
I0201 10:40:49.290115 139702543816448 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.357573986053467, loss=2.9765374660491943
I0201 10:41:35.032231 139702527031040 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.250910520553589, loss=2.794532537460327
I0201 10:42:20.927003 139702543816448 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.309502601623535, loss=2.863323450088501
I0201 10:43:06.666974 139702527031040 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.5084469318389893, loss=2.943634033203125
I0201 10:43:52.303354 139702543816448 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.3521156311035156, loss=2.8146111965179443
I0201 10:44:38.525917 139702527031040 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.3001606464385986, loss=3.089125633239746
I0201 10:44:42.714551 139863983413056 spec.py:321] Evaluating on the training split.
I0201 10:44:53.253881 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 10:45:18.676332 139863983413056 spec.py:349] Evaluating on the test split.
I0201 10:45:20.321131 139863983413056 submission_runner.py:408] Time since start: 78685.82s, 	Step: 160211, 	{'train/accuracy': 0.8788671493530273, 'train/loss': 0.681140124797821, 'validation/accuracy': 0.7768399715423584, 'validation/loss': 1.102988362312317, 'validation/num_examples': 50000, 'test/accuracy': 0.6640000343322754, 'test/loss': 1.670935869216919, 'test/num_examples': 10000, 'score': 72718.89306354523, 'total_duration': 78685.8155503273, 'accumulated_submission_time': 72718.89306354523, 'accumulated_eval_time': 5950.919641017914, 'accumulated_logging_time': 7.556835651397705}
I0201 10:45:20.361169 139702543816448 logging_writer.py:48] [160211] accumulated_eval_time=5950.919641, accumulated_logging_time=7.556836, accumulated_submission_time=72718.893064, global_step=160211, preemption_count=0, score=72718.893064, test/accuracy=0.664000, test/loss=1.670936, test/num_examples=10000, total_duration=78685.815550, train/accuracy=0.878867, train/loss=0.681140, validation/accuracy=0.776840, validation/loss=1.102988, validation/num_examples=50000
I0201 10:45:56.730808 139702527031040 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.3496334552764893, loss=2.8460941314697266
I0201 10:46:42.425070 139702543816448 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.715942621231079, loss=2.8607654571533203
I0201 10:47:28.394066 139702527031040 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.534052610397339, loss=3.8423383235931396
I0201 10:48:14.436070 139702543816448 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.275360107421875, loss=3.8426263332366943
I0201 10:49:00.317140 139702527031040 logging_writer.py:48] [160700] global_step=160700, grad_norm=3.726921319961548, loss=3.6039645671844482
I0201 10:49:46.469081 139702543816448 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.7756829261779785, loss=4.030409336090088
I0201 10:50:32.471254 139702527031040 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.8118364810943604, loss=4.086354732513428
I0201 10:51:18.206172 139702543816448 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.429008722305298, loss=3.086437225341797
I0201 10:52:04.126110 139702527031040 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.339022159576416, loss=2.8332526683807373
I0201 10:52:20.368524 139863983413056 spec.py:321] Evaluating on the training split.
I0201 10:52:30.896205 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 10:52:54.047093 139863983413056 spec.py:349] Evaluating on the test split.
I0201 10:52:55.685041 139863983413056 submission_runner.py:408] Time since start: 79141.18s, 	Step: 161137, 	{'train/accuracy': 0.8810937404632568, 'train/loss': 0.6635440587997437, 'validation/accuracy': 0.7783399820327759, 'validation/loss': 1.0918289422988892, 'validation/num_examples': 50000, 'test/accuracy': 0.663800060749054, 'test/loss': 1.6655369997024536, 'test/num_examples': 10000, 'score': 73138.84251356125, 'total_duration': 79141.17946362495, 'accumulated_submission_time': 73138.84251356125, 'accumulated_eval_time': 5986.23615694046, 'accumulated_logging_time': 7.605699300765991}
I0201 10:52:55.728704 139702543816448 logging_writer.py:48] [161137] accumulated_eval_time=5986.236157, accumulated_logging_time=7.605699, accumulated_submission_time=73138.842514, global_step=161137, preemption_count=0, score=73138.842514, test/accuracy=0.663800, test/loss=1.665537, test/num_examples=10000, total_duration=79141.179464, train/accuracy=0.881094, train/loss=0.663544, validation/accuracy=0.778340, validation/loss=1.091829, validation/num_examples=50000
I0201 10:53:21.288403 139702527031040 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.424767017364502, loss=3.6501095294952393
I0201 10:54:05.932803 139702543816448 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.349792242050171, loss=2.8615007400512695
I0201 10:54:52.364301 139702527031040 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.3191957473754883, loss=2.8553295135498047
I0201 10:55:38.457520 139702543816448 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.3119592666625977, loss=3.0775809288024902
I0201 10:56:24.277362 139702527031040 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.399960994720459, loss=2.7924611568450928
I0201 10:57:10.245178 139702543816448 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.2608518600463867, loss=3.1739487648010254
I0201 10:57:56.103317 139702527031040 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.430387020111084, loss=4.301016330718994
I0201 10:58:42.052625 139702543816448 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.4341065883636475, loss=2.9896702766418457
I0201 10:59:28.186309 139702527031040 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.4156277179718018, loss=3.312479257583618
I0201 10:59:55.732705 139863983413056 spec.py:321] Evaluating on the training split.
I0201 11:00:05.955775 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 11:00:31.623477 139863983413056 spec.py:349] Evaluating on the test split.
I0201 11:00:33.269254 139863983413056 submission_runner.py:408] Time since start: 79598.76s, 	Step: 162061, 	{'train/accuracy': 0.885546863079071, 'train/loss': 0.6447293162345886, 'validation/accuracy': 0.7795199751853943, 'validation/loss': 1.0828590393066406, 'validation/num_examples': 50000, 'test/accuracy': 0.6646000146865845, 'test/loss': 1.662644624710083, 'test/num_examples': 10000, 'score': 73558.78927731514, 'total_duration': 79598.76367640495, 'accumulated_submission_time': 73558.78927731514, 'accumulated_eval_time': 6023.772705554962, 'accumulated_logging_time': 7.6580424308776855}
I0201 11:00:33.308453 139702543816448 logging_writer.py:48] [162061] accumulated_eval_time=6023.772706, accumulated_logging_time=7.658042, accumulated_submission_time=73558.789277, global_step=162061, preemption_count=0, score=73558.789277, test/accuracy=0.664600, test/loss=1.662645, test/num_examples=10000, total_duration=79598.763676, train/accuracy=0.885547, train/loss=0.644729, validation/accuracy=0.779520, validation/loss=1.082859, validation/num_examples=50000
I0201 11:00:49.291779 139702527031040 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.5259780883789062, loss=3.904362916946411
I0201 11:01:32.802432 139702543816448 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.3504204750061035, loss=2.8964786529541016
I0201 11:02:18.537947 139702527031040 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.3956501483917236, loss=2.8532989025115967
I0201 11:03:04.629888 139702543816448 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.3925962448120117, loss=2.7635583877563477
I0201 11:03:50.562658 139702527031040 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.329366445541382, loss=2.796450614929199
I0201 11:04:36.276516 139702543816448 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.8922479152679443, loss=3.533203125
I0201 11:05:22.580690 139702527031040 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.4311161041259766, loss=2.965761661529541
I0201 11:06:08.441058 139702543816448 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.2886483669281006, loss=2.7980337142944336
I0201 11:06:54.264321 139702527031040 logging_writer.py:48] [162900] global_step=162900, grad_norm=3.336540699005127, loss=4.266414642333984
I0201 11:07:33.406393 139863983413056 spec.py:321] Evaluating on the training split.
I0201 11:07:44.079143 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 11:08:10.470341 139863983413056 spec.py:349] Evaluating on the test split.
I0201 11:08:12.110695 139863983413056 submission_runner.py:408] Time since start: 80057.61s, 	Step: 162987, 	{'train/accuracy': 0.8820117115974426, 'train/loss': 0.656156063079834, 'validation/accuracy': 0.7808600068092346, 'validation/loss': 1.076492428779602, 'validation/num_examples': 50000, 'test/accuracy': 0.666700005531311, 'test/loss': 1.6550078392028809, 'test/num_examples': 10000, 'score': 73978.82945775986, 'total_duration': 80057.60511755943, 'accumulated_submission_time': 73978.82945775986, 'accumulated_eval_time': 6062.477011442184, 'accumulated_logging_time': 7.70618200302124}
I0201 11:08:12.153776 139702543816448 logging_writer.py:48] [162987] accumulated_eval_time=6062.477011, accumulated_logging_time=7.706182, accumulated_submission_time=73978.829458, global_step=162987, preemption_count=0, score=73978.829458, test/accuracy=0.666700, test/loss=1.655008, test/num_examples=10000, total_duration=80057.605118, train/accuracy=0.882012, train/loss=0.656156, validation/accuracy=0.780860, validation/loss=1.076492, validation/num_examples=50000
I0201 11:08:17.736666 139702527031040 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.6083858013153076, loss=3.197124481201172
I0201 11:08:59.403922 139702543816448 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.2745120525360107, loss=3.584869861602783
I0201 11:09:45.339598 139702527031040 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.5546071529388428, loss=2.8830456733703613
I0201 11:10:31.512801 139702543816448 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.4828879833221436, loss=2.8106343746185303
I0201 11:11:17.311017 139702527031040 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.5388824939727783, loss=2.8357644081115723
I0201 11:12:03.066539 139702543816448 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.311481475830078, loss=2.770948886871338
I0201 11:12:49.028697 139702527031040 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.281952381134033, loss=2.759706974029541
I0201 11:13:34.626610 139702543816448 logging_writer.py:48] [163700] global_step=163700, grad_norm=3.0977895259857178, loss=4.164477348327637
I0201 11:14:20.533010 139702527031040 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.5192832946777344, loss=3.4484872817993164
I0201 11:15:06.821230 139702543816448 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.3411078453063965, loss=2.8327410221099854
I0201 11:15:12.481553 139863983413056 spec.py:321] Evaluating on the training split.
I0201 11:15:22.773761 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 11:15:48.344238 139863983413056 spec.py:349] Evaluating on the test split.
I0201 11:15:49.977176 139863983413056 submission_runner.py:408] Time since start: 80515.47s, 	Step: 163914, 	{'train/accuracy': 0.8850781321525574, 'train/loss': 0.6661824584007263, 'validation/accuracy': 0.780519962310791, 'validation/loss': 1.0905977487564087, 'validation/num_examples': 50000, 'test/accuracy': 0.6621000170707703, 'test/loss': 1.6676348447799683, 'test/num_examples': 10000, 'score': 74399.09590768814, 'total_duration': 80515.47159552574, 'accumulated_submission_time': 74399.09590768814, 'accumulated_eval_time': 6099.97262597084, 'accumulated_logging_time': 7.761868000030518}
I0201 11:15:50.016982 139702527031040 logging_writer.py:48] [163914] accumulated_eval_time=6099.972626, accumulated_logging_time=7.761868, accumulated_submission_time=74399.095908, global_step=163914, preemption_count=0, score=74399.095908, test/accuracy=0.662100, test/loss=1.667635, test/num_examples=10000, total_duration=80515.471596, train/accuracy=0.885078, train/loss=0.666182, validation/accuracy=0.780520, validation/loss=1.090598, validation/num_examples=50000
I0201 11:16:25.119234 139702543816448 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.311260461807251, loss=2.7199878692626953
I0201 11:17:10.735273 139702527031040 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.279101848602295, loss=2.855255603790283
I0201 11:17:56.846353 139702543816448 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.444855213165283, loss=2.8880159854888916
I0201 11:18:43.186828 139702527031040 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.3056490421295166, loss=2.7967352867126465
I0201 11:19:28.761838 139702543816448 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.9127535820007324, loss=4.141715049743652
I0201 11:20:14.950527 139702527031040 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.424400568008423, loss=2.8663108348846436
I0201 11:21:00.883036 139702543816448 logging_writer.py:48] [164600] global_step=164600, grad_norm=3.0495827198028564, loss=4.187742710113525
I0201 11:21:46.641992 139702527031040 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.256467580795288, loss=2.8177521228790283
I0201 11:22:32.684852 139702543816448 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.884408712387085, loss=3.8490262031555176
I0201 11:22:50.235421 139863983413056 spec.py:321] Evaluating on the training split.
I0201 11:23:00.615305 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 11:23:25.892088 139863983413056 spec.py:349] Evaluating on the test split.
I0201 11:23:27.522825 139863983413056 submission_runner.py:408] Time since start: 80973.02s, 	Step: 164840, 	{'train/accuracy': 0.8875976204872131, 'train/loss': 0.6562318205833435, 'validation/accuracy': 0.7809399962425232, 'validation/loss': 1.0894443988800049, 'validation/num_examples': 50000, 'test/accuracy': 0.6657000184059143, 'test/loss': 1.671615719795227, 'test/num_examples': 10000, 'score': 74819.25520539284, 'total_duration': 80973.01725029945, 'accumulated_submission_time': 74819.25520539284, 'accumulated_eval_time': 6137.260036468506, 'accumulated_logging_time': 7.812313795089722}
I0201 11:23:27.564168 139702527031040 logging_writer.py:48] [164840] accumulated_eval_time=6137.260036, accumulated_logging_time=7.812314, accumulated_submission_time=74819.255205, global_step=164840, preemption_count=0, score=74819.255205, test/accuracy=0.665700, test/loss=1.671616, test/num_examples=10000, total_duration=80973.017250, train/accuracy=0.887598, train/loss=0.656232, validation/accuracy=0.780940, validation/loss=1.089444, validation/num_examples=50000
I0201 11:23:52.050013 139702543816448 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.369499921798706, loss=2.829602003097534
I0201 11:24:36.372430 139702527031040 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.691973924636841, loss=3.6554925441741943
I0201 11:25:22.362856 139702543816448 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.4180612564086914, loss=2.833974838256836
I0201 11:26:08.777089 139702527031040 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.3836708068847656, loss=2.8025553226470947
I0201 11:26:54.410836 139702543816448 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.4995920658111572, loss=3.8406949043273926
I0201 11:27:40.241465 139702527031040 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.5171751976013184, loss=3.363718271255493
I0201 11:28:26.353863 139702543816448 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.4436402320861816, loss=3.022718906402588
I0201 11:29:11.939625 139702527031040 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.8164076805114746, loss=3.1120665073394775
I0201 11:29:58.059009 139702543816448 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.081932544708252, loss=4.238884925842285
I0201 11:30:27.587954 139863983413056 spec.py:321] Evaluating on the training split.
I0201 11:30:38.358492 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 11:31:01.414518 139863983413056 spec.py:349] Evaluating on the test split.
I0201 11:31:03.056225 139863983413056 submission_runner.py:408] Time since start: 81428.55s, 	Step: 165766, 	{'train/accuracy': 0.8836132884025574, 'train/loss': 0.6717751026153564, 'validation/accuracy': 0.7789999842643738, 'validation/loss': 1.0890315771102905, 'validation/num_examples': 50000, 'test/accuracy': 0.663800060749054, 'test/loss': 1.661457896232605, 'test/num_examples': 10000, 'score': 75239.2183611393, 'total_duration': 81428.55065131187, 'accumulated_submission_time': 75239.2183611393, 'accumulated_eval_time': 6172.728312015533, 'accumulated_logging_time': 7.865967750549316}
I0201 11:31:03.098036 139702527031040 logging_writer.py:48] [165766] accumulated_eval_time=6172.728312, accumulated_logging_time=7.865968, accumulated_submission_time=75239.218361, global_step=165766, preemption_count=0, score=75239.218361, test/accuracy=0.663800, test/loss=1.661458, test/num_examples=10000, total_duration=81428.550651, train/accuracy=0.883613, train/loss=0.671775, validation/accuracy=0.779000, validation/loss=1.089032, validation/num_examples=50000
I0201 11:31:17.071753 139702543816448 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.390432357788086, loss=3.4376747608184814
I0201 11:32:00.000773 139702527031040 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.442798376083374, loss=2.8462584018707275
I0201 11:32:45.868111 139702543816448 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.8777921199798584, loss=4.0823588371276855
I0201 11:33:31.833368 139702527031040 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.2808008193969727, loss=2.9415531158447266
I0201 11:34:17.780216 139702543816448 logging_writer.py:48] [166200] global_step=166200, grad_norm=3.1271917819976807, loss=3.8543860912323
I0201 11:35:03.694787 139702527031040 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.507941246032715, loss=2.8127777576446533
I0201 11:35:49.904576 139702543816448 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.5956733226776123, loss=3.575319766998291
I0201 11:36:35.668962 139702527031040 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.6770730018615723, loss=3.868194818496704
I0201 11:37:21.564594 139702543816448 logging_writer.py:48] [166600] global_step=166600, grad_norm=3.0536274909973145, loss=3.993508815765381
I0201 11:38:03.221798 139863983413056 spec.py:321] Evaluating on the training split.
I0201 11:38:13.650902 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 11:38:36.494911 139863983413056 spec.py:349] Evaluating on the test split.
I0201 11:38:38.129499 139863983413056 submission_runner.py:408] Time since start: 81883.62s, 	Step: 166692, 	{'train/accuracy': 0.8857030868530273, 'train/loss': 0.6525242328643799, 'validation/accuracy': 0.7821799516677856, 'validation/loss': 1.0815180540084839, 'validation/num_examples': 50000, 'test/accuracy': 0.6668000221252441, 'test/loss': 1.6559531688690186, 'test/num_examples': 10000, 'score': 75659.28298974037, 'total_duration': 81883.62391614914, 'accumulated_submission_time': 75659.28298974037, 'accumulated_eval_time': 6207.636021375656, 'accumulated_logging_time': 7.91825795173645}
I0201 11:38:38.172937 139702527031040 logging_writer.py:48] [166692] accumulated_eval_time=6207.636021, accumulated_logging_time=7.918258, accumulated_submission_time=75659.282990, global_step=166692, preemption_count=0, score=75659.282990, test/accuracy=0.666800, test/loss=1.655953, test/num_examples=10000, total_duration=81883.623916, train/accuracy=0.885703, train/loss=0.652524, validation/accuracy=0.782180, validation/loss=1.081518, validation/num_examples=50000
I0201 11:38:41.763498 139702543816448 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.308636426925659, loss=2.8119454383850098
I0201 11:39:23.071198 139702527031040 logging_writer.py:48] [166800] global_step=166800, grad_norm=3.1804749965667725, loss=4.279930591583252
I0201 11:40:09.022352 139702543816448 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.4855387210845947, loss=2.8439152240753174
I0201 11:40:55.159900 139702527031040 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.4503087997436523, loss=2.744006633758545
I0201 11:41:41.219460 139702543816448 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.611386299133301, loss=3.698181390762329
I0201 11:42:26.886580 139702527031040 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.3835835456848145, loss=2.754638671875
I0201 11:43:12.797940 139702543816448 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.55452036857605, loss=2.8266940116882324
I0201 11:43:58.496764 139702527031040 logging_writer.py:48] [167400] global_step=167400, grad_norm=2.53493070602417, loss=2.7381021976470947
I0201 11:44:44.308527 139702543816448 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.5966711044311523, loss=3.1105825901031494
I0201 11:45:30.240123 139702527031040 logging_writer.py:48] [167600] global_step=167600, grad_norm=2.4061837196350098, loss=2.8848938941955566
I0201 11:45:38.251968 139863983413056 spec.py:321] Evaluating on the training split.
I0201 11:45:48.808758 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 11:46:11.690846 139863983413056 spec.py:349] Evaluating on the test split.
I0201 11:46:13.329767 139863983413056 submission_runner.py:408] Time since start: 82338.82s, 	Step: 167619, 	{'train/accuracy': 0.8876562118530273, 'train/loss': 0.6507769823074341, 'validation/accuracy': 0.7819199562072754, 'validation/loss': 1.0878478288650513, 'validation/num_examples': 50000, 'test/accuracy': 0.6665000319480896, 'test/loss': 1.6662715673446655, 'test/num_examples': 10000, 'score': 76079.30256128311, 'total_duration': 82338.82419419289, 'accumulated_submission_time': 76079.30256128311, 'accumulated_eval_time': 6242.713820695877, 'accumulated_logging_time': 7.97198224067688}
I0201 11:46:13.373397 139702543816448 logging_writer.py:48] [167619] accumulated_eval_time=6242.713821, accumulated_logging_time=7.971982, accumulated_submission_time=76079.302561, global_step=167619, preemption_count=0, score=76079.302561, test/accuracy=0.666500, test/loss=1.666272, test/num_examples=10000, total_duration=82338.824194, train/accuracy=0.887656, train/loss=0.650777, validation/accuracy=0.781920, validation/loss=1.087848, validation/num_examples=50000
I0201 11:46:46.095926 139702527031040 logging_writer.py:48] [167700] global_step=167700, grad_norm=3.3069446086883545, loss=4.164505958557129
I0201 11:47:31.769343 139702543816448 logging_writer.py:48] [167800] global_step=167800, grad_norm=2.6334691047668457, loss=2.8023500442504883
I0201 11:48:17.565088 139702527031040 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.4844906330108643, loss=3.442528009414673
I0201 11:49:03.744289 139702543816448 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.5953404903411865, loss=2.784777879714966
I0201 11:49:49.478472 139702527031040 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.3907103538513184, loss=2.801253318786621
I0201 11:50:35.763724 139702543816448 logging_writer.py:48] [168200] global_step=168200, grad_norm=2.418565511703491, loss=2.7965447902679443
I0201 11:51:21.750353 139702527031040 logging_writer.py:48] [168300] global_step=168300, grad_norm=2.3514373302459717, loss=2.989956855773926
I0201 11:52:07.520457 139702543816448 logging_writer.py:48] [168400] global_step=168400, grad_norm=2.7113654613494873, loss=3.1207427978515625
I0201 11:52:53.109884 139702527031040 logging_writer.py:48] [168500] global_step=168500, grad_norm=2.198413848876953, loss=2.7565512657165527
I0201 11:53:13.410383 139863983413056 spec.py:321] Evaluating on the training split.
I0201 11:53:23.668069 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 11:53:45.557279 139863983413056 spec.py:349] Evaluating on the test split.
I0201 11:53:47.202892 139863983413056 submission_runner.py:408] Time since start: 82792.70s, 	Step: 168545, 	{'train/accuracy': 0.8912890553474426, 'train/loss': 0.6282624006271362, 'validation/accuracy': 0.7827799916267395, 'validation/loss': 1.078118920326233, 'validation/num_examples': 50000, 'test/accuracy': 0.6678000092506409, 'test/loss': 1.6539418697357178, 'test/num_examples': 10000, 'score': 76499.28124332428, 'total_duration': 82792.69729566574, 'accumulated_submission_time': 76499.28124332428, 'accumulated_eval_time': 6276.506313562393, 'accumulated_logging_time': 8.024755239486694}
I0201 11:53:47.251218 139702543816448 logging_writer.py:48] [168545] accumulated_eval_time=6276.506314, accumulated_logging_time=8.024755, accumulated_submission_time=76499.281243, global_step=168545, preemption_count=0, score=76499.281243, test/accuracy=0.667800, test/loss=1.653942, test/num_examples=10000, total_duration=82792.697296, train/accuracy=0.891289, train/loss=0.628262, validation/accuracy=0.782780, validation/loss=1.078119, validation/num_examples=50000
I0201 11:54:09.606763 139702527031040 logging_writer.py:48] [168600] global_step=168600, grad_norm=2.433966636657715, loss=2.954350709915161
I0201 11:54:53.923200 139702543816448 logging_writer.py:48] [168700] global_step=168700, grad_norm=2.3851287364959717, loss=2.745424270629883
I0201 11:55:39.744673 139702527031040 logging_writer.py:48] [168800] global_step=168800, grad_norm=2.8283629417419434, loss=3.6532299518585205
I0201 11:56:26.285629 139702543816448 logging_writer.py:48] [168900] global_step=168900, grad_norm=3.5638012886047363, loss=4.2109174728393555
I0201 11:57:11.815238 139702527031040 logging_writer.py:48] [169000] global_step=169000, grad_norm=2.6427059173583984, loss=3.7940940856933594
I0201 11:57:57.616410 139702543816448 logging_writer.py:48] [169100] global_step=169100, grad_norm=2.601487398147583, loss=2.7491090297698975
I0201 11:58:43.483525 139702527031040 logging_writer.py:48] [169200] global_step=169200, grad_norm=2.378232002258301, loss=2.774601459503174
I0201 11:59:29.162234 139702543816448 logging_writer.py:48] [169300] global_step=169300, grad_norm=2.566668748855591, loss=3.422933340072632
I0201 12:00:15.521904 139702527031040 logging_writer.py:48] [169400] global_step=169400, grad_norm=2.4511914253234863, loss=2.7264437675476074
I0201 12:00:47.663999 139863983413056 spec.py:321] Evaluating on the training split.
I0201 12:00:58.188581 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 12:01:20.333371 139863983413056 spec.py:349] Evaluating on the test split.
I0201 12:01:21.976538 139863983413056 submission_runner.py:408] Time since start: 83247.47s, 	Step: 169472, 	{'train/accuracy': 0.8900976181030273, 'train/loss': 0.6365549564361572, 'validation/accuracy': 0.7829799652099609, 'validation/loss': 1.0776472091674805, 'validation/num_examples': 50000, 'test/accuracy': 0.6668000221252441, 'test/loss': 1.6524428129196167, 'test/num_examples': 10000, 'score': 76919.63348913193, 'total_duration': 83247.47095322609, 'accumulated_submission_time': 76919.63348913193, 'accumulated_eval_time': 6310.818851947784, 'accumulated_logging_time': 8.084598779678345}
I0201 12:01:22.016587 139702543816448 logging_writer.py:48] [169472] accumulated_eval_time=6310.818852, accumulated_logging_time=8.084599, accumulated_submission_time=76919.633489, global_step=169472, preemption_count=0, score=76919.633489, test/accuracy=0.666800, test/loss=1.652443, test/num_examples=10000, total_duration=83247.470953, train/accuracy=0.890098, train/loss=0.636555, validation/accuracy=0.782980, validation/loss=1.077647, validation/num_examples=50000
I0201 12:01:33.596966 139702527031040 logging_writer.py:48] [169500] global_step=169500, grad_norm=2.680239677429199, loss=3.822582960128784
I0201 12:02:16.182318 139702543816448 logging_writer.py:48] [169600] global_step=169600, grad_norm=2.312835454940796, loss=3.0922207832336426
I0201 12:03:01.788402 139702527031040 logging_writer.py:48] [169700] global_step=169700, grad_norm=2.341886281967163, loss=2.9405980110168457
I0201 12:03:47.722948 139702543816448 logging_writer.py:48] [169800] global_step=169800, grad_norm=2.443171739578247, loss=2.8503427505493164
I0201 12:04:33.509351 139702527031040 logging_writer.py:48] [169900] global_step=169900, grad_norm=2.553262233734131, loss=2.7310028076171875
I0201 12:05:19.159187 139702543816448 logging_writer.py:48] [170000] global_step=170000, grad_norm=2.5540835857391357, loss=3.056915760040283
I0201 12:06:04.878913 139702527031040 logging_writer.py:48] [170100] global_step=170100, grad_norm=2.6491262912750244, loss=2.7975878715515137
I0201 12:06:51.288215 139702543816448 logging_writer.py:48] [170200] global_step=170200, grad_norm=2.4909262657165527, loss=2.7323431968688965
I0201 12:07:37.066669 139702527031040 logging_writer.py:48] [170300] global_step=170300, grad_norm=2.3509438037872314, loss=2.7666475772857666
I0201 12:08:22.270931 139863983413056 spec.py:321] Evaluating on the training split.
I0201 12:08:32.890993 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 12:08:58.258108 139863983413056 spec.py:349] Evaluating on the test split.
I0201 12:08:59.898188 139863983413056 submission_runner.py:408] Time since start: 83705.39s, 	Step: 170400, 	{'train/accuracy': 0.8862695097923279, 'train/loss': 0.6478437781333923, 'validation/accuracy': 0.7828199863433838, 'validation/loss': 1.084557294845581, 'validation/num_examples': 50000, 'test/accuracy': 0.6687000393867493, 'test/loss': 1.6623047590255737, 'test/num_examples': 10000, 'score': 77339.8290321827, 'total_duration': 83705.39258909225, 'accumulated_submission_time': 77339.8290321827, 'accumulated_eval_time': 6348.446104764938, 'accumulated_logging_time': 8.135056495666504}
I0201 12:08:59.944162 139702543816448 logging_writer.py:48] [170400] accumulated_eval_time=6348.446105, accumulated_logging_time=8.135056, accumulated_submission_time=77339.829032, global_step=170400, preemption_count=0, score=77339.829032, test/accuracy=0.668700, test/loss=1.662305, test/num_examples=10000, total_duration=83705.392589, train/accuracy=0.886270, train/loss=0.647844, validation/accuracy=0.782820, validation/loss=1.084557, validation/num_examples=50000
I0201 12:09:00.348642 139702527031040 logging_writer.py:48] [170400] global_step=170400, grad_norm=2.450406074523926, loss=2.748474359512329
I0201 12:09:41.510273 139702543816448 logging_writer.py:48] [170500] global_step=170500, grad_norm=2.9578194618225098, loss=4.096675395965576
I0201 12:10:27.548775 139702527031040 logging_writer.py:48] [170600] global_step=170600, grad_norm=2.5858938694000244, loss=2.7869060039520264
I0201 12:11:13.438102 139702543816448 logging_writer.py:48] [170700] global_step=170700, grad_norm=2.5022778511047363, loss=2.7338249683380127
I0201 12:11:59.421303 139702527031040 logging_writer.py:48] [170800] global_step=170800, grad_norm=2.5794880390167236, loss=3.074413537979126
I0201 12:12:00.537867 139702543816448 logging_writer.py:48] [170804] global_step=170804, preemption_count=0, score=77520.331876
I0201 12:12:01.188685 139863983413056 checkpoints.py:490] Saving checkpoint at step: 170804
I0201 12:12:02.423615 139863983413056 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_2/checkpoint_170804
I0201 12:12:02.438701 139863983413056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_2/checkpoint_170804.
I0201 12:12:03.310033 139863983413056 submission_runner.py:583] Tuning trial 2/5
I0201 12:12:03.310265 139863983413056 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0201 12:12:03.334279 139863983413056 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008593749953433871, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 35.015655517578125, 'total_duration': 62.8664071559906, 'accumulated_submission_time': 35.015655517578125, 'accumulated_eval_time': 27.850645065307617, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (875, {'train/accuracy': 0.01748046837747097, 'train/loss': 6.352764129638672, 'validation/accuracy': 0.016339998692274094, 'validation/loss': 6.365495681762695, 'validation/num_examples': 50000, 'test/accuracy': 0.013000000268220901, 'test/loss': 6.417356014251709, 'test/num_examples': 10000, 'score': 455.200243473053, 'total_duration': 517.6062908172607, 'accumulated_submission_time': 455.200243473053, 'accumulated_eval_time': 62.341947078704834, 'accumulated_logging_time': 0.01773810386657715, 'global_step': 875, 'preemption_count': 0}), (1799, {'train/accuracy': 0.04648437350988388, 'train/loss': 5.830441951751709, 'validation/accuracy': 0.04227999970316887, 'validation/loss': 5.864605903625488, 'validation/num_examples': 50000, 'test/accuracy': 0.03400000184774399, 'test/loss': 5.986781597137451, 'test/num_examples': 10000, 'score': 875.5398058891296, 'total_duration': 973.7104048728943, 'accumulated_submission_time': 875.5398058891296, 'accumulated_eval_time': 98.02724885940552, 'accumulated_logging_time': 0.04830026626586914, 'global_step': 1799, 'preemption_count': 0}), (2691, {'train/accuracy': 0.07083984464406967, 'train/loss': 5.448312282562256, 'validation/accuracy': 0.06827999651432037, 'validation/loss': 5.485576152801514, 'validation/num_examples': 50000, 'test/accuracy': 0.05140000209212303, 'test/loss': 5.651369571685791, 'test/num_examples': 10000, 'score': 1295.7417635917664, 'total_duration': 1429.3486967086792, 'accumulated_submission_time': 1295.7417635917664, 'accumulated_eval_time': 133.39315724372864, 'accumulated_logging_time': 0.0725252628326416, 'global_step': 2691, 'preemption_count': 0}), (3620, {'train/accuracy': 0.1144726499915123, 'train/loss': 5.076627731323242, 'validation/accuracy': 0.1036200001835823, 'validation/loss': 5.128720760345459, 'validation/num_examples': 50000, 'test/accuracy': 0.08130000531673431, 'test/loss': 5.356257915496826, 'test/num_examples': 10000, 'score': 1715.829880952835, 'total_duration': 1883.7408828735352, 'accumulated_submission_time': 1715.829880952835, 'accumulated_eval_time': 167.62544560432434, 'accumulated_logging_time': 0.09573221206665039, 'global_step': 3620, 'preemption_count': 0}), (4546, {'train/accuracy': 0.16103515028953552, 'train/loss': 4.597641944885254, 'validation/accuracy': 0.14226000010967255, 'validation/loss': 4.700864315032959, 'validation/num_examples': 50000, 'test/accuracy': 0.10830000787973404, 'test/loss': 5.006032943725586, 'test/num_examples': 10000, 'score': 2136.0431559085846, 'total_duration': 2338.310334444046, 'accumulated_submission_time': 2136.0431559085846, 'accumulated_eval_time': 201.90626621246338, 'accumulated_logging_time': 0.12220025062561035, 'global_step': 4546, 'preemption_count': 0}), (5473, {'train/accuracy': 0.2023632824420929, 'train/loss': 4.2548089027404785, 'validation/accuracy': 0.18807999789714813, 'validation/loss': 4.334715366363525, 'validation/num_examples': 50000, 'test/accuracy': 0.14079999923706055, 'test/loss': 4.678713321685791, 'test/num_examples': 10000, 'score': 2556.31773352623, 'total_duration': 2792.7371475696564, 'accumulated_submission_time': 2556.31773352623, 'accumulated_eval_time': 235.98561549186707, 'accumulated_logging_time': 0.14621448516845703, 'global_step': 5473, 'preemption_count': 0}), (6402, {'train/accuracy': 0.25146484375, 'train/loss': 3.9174740314483643, 'validation/accuracy': 0.2351599931716919, 'validation/loss': 4.009504795074463, 'validation/num_examples': 50000, 'test/accuracy': 0.18300001323223114, 'test/loss': 4.3917012214660645, 'test/num_examples': 10000, 'score': 2976.2778012752533, 'total_duration': 3245.2699744701385, 'accumulated_submission_time': 2976.2778012752533, 'accumulated_eval_time': 268.4839344024658, 'accumulated_logging_time': 0.17171645164489746, 'global_step': 6402, 'preemption_count': 0}), (7330, {'train/accuracy': 0.2987304627895355, 'train/loss': 3.6502153873443604, 'validation/accuracy': 0.2711600065231323, 'validation/loss': 3.7866060733795166, 'validation/num_examples': 50000, 'test/accuracy': 0.20920000970363617, 'test/loss': 4.218708038330078, 'test/num_examples': 10000, 'score': 3396.4188299179077, 'total_duration': 3698.38631939888, 'accumulated_submission_time': 3396.4188299179077, 'accumulated_eval_time': 301.3875472545624, 'accumulated_logging_time': 0.1950387954711914, 'global_step': 7330, 'preemption_count': 0}), (8257, {'train/accuracy': 0.32369139790534973, 'train/loss': 3.4396770000457764, 'validation/accuracy': 0.3033199906349182, 'validation/loss': 3.5456032752990723, 'validation/num_examples': 50000, 'test/accuracy': 0.2370000183582306, 'test/loss': 4.004726886749268, 'test/num_examples': 10000, 'score': 3816.6637468338013, 'total_duration': 4151.96754193306, 'accumulated_submission_time': 3816.6637468338013, 'accumulated_eval_time': 334.6520891189575, 'accumulated_logging_time': 0.21846985816955566, 'global_step': 8257, 'preemption_count': 0}), (9184, {'train/accuracy': 0.3685546815395355, 'train/loss': 3.1528046131134033, 'validation/accuracy': 0.33935999870300293, 'validation/loss': 3.2868564128875732, 'validation/num_examples': 50000, 'test/accuracy': 0.25930002331733704, 'test/loss': 3.76635479927063, 'test/num_examples': 10000, 'score': 4236.656970024109, 'total_duration': 4605.663713693619, 'accumulated_submission_time': 4236.656970024109, 'accumulated_eval_time': 368.2774066925049, 'accumulated_logging_time': 0.2466275691986084, 'global_step': 9184, 'preemption_count': 0}), (10112, {'train/accuracy': 0.4025000035762787, 'train/loss': 3.0011579990386963, 'validation/accuracy': 0.36711999773979187, 'validation/loss': 3.1657626628875732, 'validation/num_examples': 50000, 'test/accuracy': 0.2835000157356262, 'test/loss': 3.687035322189331, 'test/num_examples': 10000, 'score': 4656.829707622528, 'total_duration': 5060.144756317139, 'accumulated_submission_time': 4656.829707622528, 'accumulated_eval_time': 402.5110983848572, 'accumulated_logging_time': 0.27249979972839355, 'global_step': 10112, 'preemption_count': 0}), (11041, {'train/accuracy': 0.4302734136581421, 'train/loss': 2.806786060333252, 'validation/accuracy': 0.3969399929046631, 'validation/loss': 2.9504761695861816, 'validation/num_examples': 50000, 'test/accuracy': 0.3059000074863434, 'test/loss': 3.4966979026794434, 'test/num_examples': 10000, 'score': 5076.838560819626, 'total_duration': 5513.161741495132, 'accumulated_submission_time': 5076.838560819626, 'accumulated_eval_time': 435.44481587409973, 'accumulated_logging_time': 0.2978024482727051, 'global_step': 11041, 'preemption_count': 0}), (11969, {'train/accuracy': 0.4562304615974426, 'train/loss': 2.67525053024292, 'validation/accuracy': 0.4208199977874756, 'validation/loss': 2.8304824829101562, 'validation/num_examples': 50000, 'test/accuracy': 0.3264000117778778, 'test/loss': 3.385175943374634, 'test/num_examples': 10000, 'score': 5497.176267147064, 'total_duration': 5965.148057460785, 'accumulated_submission_time': 5497.176267147064, 'accumulated_eval_time': 467.0167169570923, 'accumulated_logging_time': 0.32605648040771484, 'global_step': 11969, 'preemption_count': 0}), (12898, {'train/accuracy': 0.4791015386581421, 'train/loss': 2.5726089477539062, 'validation/accuracy': 0.43879997730255127, 'validation/loss': 2.7511792182922363, 'validation/num_examples': 50000, 'test/accuracy': 0.33970001339912415, 'test/loss': 3.322092294692993, 'test/num_examples': 10000, 'score': 5917.206485748291, 'total_duration': 6418.862198352814, 'accumulated_submission_time': 5917.206485748291, 'accumulated_eval_time': 500.62565183639526, 'accumulated_logging_time': 0.3518826961517334, 'global_step': 12898, 'preemption_count': 0}), (13826, {'train/accuracy': 0.5181445479393005, 'train/loss': 2.3554434776306152, 'validation/accuracy': 0.4603399932384491, 'validation/loss': 2.6169192790985107, 'validation/num_examples': 50000, 'test/accuracy': 0.35690000653266907, 'test/loss': 3.201007843017578, 'test/num_examples': 10000, 'score': 6337.3051698207855, 'total_duration': 6872.681324005127, 'accumulated_submission_time': 6337.3051698207855, 'accumulated_eval_time': 534.2692155838013, 'accumulated_logging_time': 0.37982916831970215, 'global_step': 13826, 'preemption_count': 0}), (14756, {'train/accuracy': 0.5152734518051147, 'train/loss': 2.3585195541381836, 'validation/accuracy': 0.47669997811317444, 'validation/loss': 2.536402463912964, 'validation/num_examples': 50000, 'test/accuracy': 0.3725000321865082, 'test/loss': 3.11665415763855, 'test/num_examples': 10000, 'score': 6757.488060712814, 'total_duration': 7321.107630968094, 'accumulated_submission_time': 6757.488060712814, 'accumulated_eval_time': 562.43803191185, 'accumulated_logging_time': 0.4050462245941162, 'global_step': 14756, 'preemption_count': 0}), (15681, {'train/accuracy': 0.5367968678474426, 'train/loss': 2.26556658744812, 'validation/accuracy': 0.49793997406959534, 'validation/loss': 2.443150281906128, 'validation/num_examples': 50000, 'test/accuracy': 0.38520002365112305, 'test/loss': 3.0521280765533447, 'test/num_examples': 10000, 'score': 7177.518812179565, 'total_duration': 7774.275252819061, 'accumulated_submission_time': 7177.518812179565, 'accumulated_eval_time': 595.4816019535065, 'accumulated_logging_time': 0.44942355155944824, 'global_step': 15681, 'preemption_count': 0}), (16611, {'train/accuracy': 0.5549218654632568, 'train/loss': 2.1838366985321045, 'validation/accuracy': 0.501039981842041, 'validation/loss': 2.4183454513549805, 'validation/num_examples': 50000, 'test/accuracy': 0.39080002903938293, 'test/loss': 3.024139881134033, 'test/num_examples': 10000, 'score': 7597.600474834442, 'total_duration': 8225.735122203827, 'accumulated_submission_time': 7597.600474834442, 'accumulated_eval_time': 626.7855026721954, 'accumulated_logging_time': 0.4745340347290039, 'global_step': 16611, 'preemption_count': 0}), (17542, {'train/accuracy': 0.5504687428474426, 'train/loss': 2.1938395500183105, 'validation/accuracy': 0.5118199586868286, 'validation/loss': 2.3658089637756348, 'validation/num_examples': 50000, 'test/accuracy': 0.40570002794265747, 'test/loss': 2.9616432189941406, 'test/num_examples': 10000, 'score': 8017.97752737999, 'total_duration': 8678.402475357056, 'accumulated_submission_time': 8017.97752737999, 'accumulated_eval_time': 658.9955780506134, 'accumulated_logging_time': 0.5048494338989258, 'global_step': 17542, 'preemption_count': 0}), (18464, {'train/accuracy': 0.5669335722923279, 'train/loss': 2.0964860916137695, 'validation/accuracy': 0.5268599987030029, 'validation/loss': 2.2790541648864746, 'validation/num_examples': 50000, 'test/accuracy': 0.4108000099658966, 'test/loss': 2.897355794906616, 'test/num_examples': 10000, 'score': 8437.904272794724, 'total_duration': 9132.661324977875, 'accumulated_submission_time': 8437.904272794724, 'accumulated_eval_time': 693.2478134632111, 'accumulated_logging_time': 0.5354297161102295, 'global_step': 18464, 'preemption_count': 0}), (19391, {'train/accuracy': 0.5886914134025574, 'train/loss': 1.9833548069000244, 'validation/accuracy': 0.5397199988365173, 'validation/loss': 2.214489698410034, 'validation/num_examples': 50000, 'test/accuracy': 0.4244000315666199, 'test/loss': 2.8181262016296387, 'test/num_examples': 10000, 'score': 8858.220192193985, 'total_duration': 9588.208375692368, 'accumulated_submission_time': 8858.220192193985, 'accumulated_eval_time': 728.4040546417236, 'accumulated_logging_time': 0.5617325305938721, 'global_step': 19391, 'preemption_count': 0}), (20321, {'train/accuracy': 0.5872656106948853, 'train/loss': 1.982076644897461, 'validation/accuracy': 0.5415999889373779, 'validation/loss': 2.1796276569366455, 'validation/num_examples': 50000, 'test/accuracy': 0.43080002069473267, 'test/loss': 2.7886321544647217, 'test/num_examples': 10000, 'score': 9278.46674156189, 'total_duration': 10043.739041805267, 'accumulated_submission_time': 9278.46674156189, 'accumulated_eval_time': 763.6083111763, 'accumulated_logging_time': 0.5928773880004883, 'global_step': 20321, 'preemption_count': 0}), (21248, {'train/accuracy': 0.5874413847923279, 'train/loss': 1.9971106052398682, 'validation/accuracy': 0.5496799945831299, 'validation/loss': 2.1746737957000732, 'validation/num_examples': 50000, 'test/accuracy': 0.4305000305175781, 'test/loss': 2.7999267578125, 'test/num_examples': 10000, 'score': 9698.78861618042, 'total_duration': 10500.002382278442, 'accumulated_submission_time': 9698.78861618042, 'accumulated_eval_time': 799.4674112796783, 'accumulated_logging_time': 0.6267457008361816, 'global_step': 21248, 'preemption_count': 0}), (22177, {'train/accuracy': 0.6080663800239563, 'train/loss': 1.9223328828811646, 'validation/accuracy': 0.5569199919700623, 'validation/loss': 2.1479055881500244, 'validation/num_examples': 50000, 'test/accuracy': 0.44140002131462097, 'test/loss': 2.7625842094421387, 'test/num_examples': 10000, 'score': 10118.962321043015, 'total_duration': 10955.953563451767, 'accumulated_submission_time': 10118.962321043015, 'accumulated_eval_time': 835.1677443981171, 'accumulated_logging_time': 0.6544830799102783, 'global_step': 22177, 'preemption_count': 0}), (23105, {'train/accuracy': 0.6371484398841858, 'train/loss': 1.7699031829833984, 'validation/accuracy': 0.5706999897956848, 'validation/loss': 2.06937313079834, 'validation/num_examples': 50000, 'test/accuracy': 0.4481000304222107, 'test/loss': 2.6906943321228027, 'test/num_examples': 10000, 'score': 10539.390924930573, 'total_duration': 11411.274099826813, 'accumulated_submission_time': 10539.390924930573, 'accumulated_eval_time': 869.9834413528442, 'accumulated_logging_time': 0.6817433834075928, 'global_step': 23105, 'preemption_count': 0}), (24032, {'train/accuracy': 0.6188671588897705, 'train/loss': 1.8651227951049805, 'validation/accuracy': 0.5744799971580505, 'validation/loss': 2.0639655590057373, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.681931734085083, 'test/num_examples': 10000, 'score': 10959.36401629448, 'total_duration': 11866.034049749374, 'accumulated_submission_time': 10959.36401629448, 'accumulated_eval_time': 904.6943933963776, 'accumulated_logging_time': 0.7084939479827881, 'global_step': 24032, 'preemption_count': 0}), (24957, {'train/accuracy': 0.6286327838897705, 'train/loss': 1.810321569442749, 'validation/accuracy': 0.5790799856185913, 'validation/loss': 2.0259885787963867, 'validation/num_examples': 50000, 'test/accuracy': 0.46330001950263977, 'test/loss': 2.642620325088501, 'test/num_examples': 10000, 'score': 11379.409708976746, 'total_duration': 12322.063168525696, 'accumulated_submission_time': 11379.409708976746, 'accumulated_eval_time': 940.5957970619202, 'accumulated_logging_time': 0.7409365177154541, 'global_step': 24957, 'preemption_count': 0}), (25885, {'train/accuracy': 0.6464648246765137, 'train/loss': 1.7180976867675781, 'validation/accuracy': 0.5830000042915344, 'validation/loss': 1.9904863834381104, 'validation/num_examples': 50000, 'test/accuracy': 0.46400001645088196, 'test/loss': 2.601236343383789, 'test/num_examples': 10000, 'score': 11799.423764944077, 'total_duration': 12775.900420188904, 'accumulated_submission_time': 11799.423764944077, 'accumulated_eval_time': 974.3425529003143, 'accumulated_logging_time': 0.7686212062835693, 'global_step': 25885, 'preemption_count': 0}), (26811, {'train/accuracy': 0.6296288967132568, 'train/loss': 1.7748783826828003, 'validation/accuracy': 0.5895999670028687, 'validation/loss': 1.974582314491272, 'validation/num_examples': 50000, 'test/accuracy': 0.4707000255584717, 'test/loss': 2.5924384593963623, 'test/num_examples': 10000, 'score': 12219.484512329102, 'total_duration': 13230.34390258789, 'accumulated_submission_time': 12219.484512329102, 'accumulated_eval_time': 1008.6455118656158, 'accumulated_logging_time': 0.7995619773864746, 'global_step': 26811, 'preemption_count': 0}), (27739, {'train/accuracy': 0.6431835889816284, 'train/loss': 1.694810390472412, 'validation/accuracy': 0.6007599830627441, 'validation/loss': 1.8964165449142456, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.5261025428771973, 'test/num_examples': 10000, 'score': 12639.566604852676, 'total_duration': 13684.9992685318, 'accumulated_submission_time': 12639.566604852676, 'accumulated_eval_time': 1043.1403737068176, 'accumulated_logging_time': 0.8287265300750732, 'global_step': 27739, 'preemption_count': 0}), (28669, {'train/accuracy': 0.6536718606948853, 'train/loss': 1.666849136352539, 'validation/accuracy': 0.5978400111198425, 'validation/loss': 1.9145691394805908, 'validation/num_examples': 50000, 'test/accuracy': 0.4789000153541565, 'test/loss': 2.543222665786743, 'test/num_examples': 10000, 'score': 13059.846086263657, 'total_duration': 14139.38765001297, 'accumulated_submission_time': 13059.846086263657, 'accumulated_eval_time': 1077.1589756011963, 'accumulated_logging_time': 0.8670501708984375, 'global_step': 28669, 'preemption_count': 0}), (29597, {'train/accuracy': 0.6449218392372131, 'train/loss': 1.720602035522461, 'validation/accuracy': 0.5954799652099609, 'validation/loss': 1.935060739517212, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.5695018768310547, 'test/num_examples': 10000, 'score': 13479.923082113266, 'total_duration': 14593.923149585724, 'accumulated_submission_time': 13479.923082113266, 'accumulated_eval_time': 1111.5325186252594, 'accumulated_logging_time': 0.903029203414917, 'global_step': 29597, 'preemption_count': 0}), (30525, {'train/accuracy': 0.6502929329872131, 'train/loss': 1.707275390625, 'validation/accuracy': 0.602620005607605, 'validation/loss': 1.911301851272583, 'validation/num_examples': 50000, 'test/accuracy': 0.48420003056526184, 'test/loss': 2.5386219024658203, 'test/num_examples': 10000, 'score': 13900.250252962112, 'total_duration': 15047.529522657394, 'accumulated_submission_time': 13900.250252962112, 'accumulated_eval_time': 1144.7327094078064, 'accumulated_logging_time': 0.9328234195709229, 'global_step': 30525, 'preemption_count': 0}), (31451, {'train/accuracy': 0.6668164134025574, 'train/loss': 1.6053072214126587, 'validation/accuracy': 0.6132000088691711, 'validation/loss': 1.8501198291778564, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.4632036685943604, 'test/num_examples': 10000, 'score': 14320.317656040192, 'total_duration': 15502.468721866608, 'accumulated_submission_time': 14320.317656040192, 'accumulated_eval_time': 1179.5270998477936, 'accumulated_logging_time': 0.9612855911254883, 'global_step': 31451, 'preemption_count': 0}), (32379, {'train/accuracy': 0.6805077791213989, 'train/loss': 1.5708024501800537, 'validation/accuracy': 0.6123799681663513, 'validation/loss': 1.8718345165252686, 'validation/num_examples': 50000, 'test/accuracy': 0.4894000291824341, 'test/loss': 2.4860403537750244, 'test/num_examples': 10000, 'score': 14740.255218982697, 'total_duration': 15957.264911651611, 'accumulated_submission_time': 14740.255218982697, 'accumulated_eval_time': 1214.3044934272766, 'accumulated_logging_time': 0.9927427768707275, 'global_step': 32379, 'preemption_count': 0}), (33309, {'train/accuracy': 0.6620507836341858, 'train/loss': 1.6771049499511719, 'validation/accuracy': 0.6145600080490112, 'validation/loss': 1.8774863481521606, 'validation/num_examples': 50000, 'test/accuracy': 0.4953000247478485, 'test/loss': 2.4875123500823975, 'test/num_examples': 10000, 'score': 15160.616579771042, 'total_duration': 16411.350699186325, 'accumulated_submission_time': 15160.616579771042, 'accumulated_eval_time': 1247.9506666660309, 'accumulated_logging_time': 1.0217430591583252, 'global_step': 33309, 'preemption_count': 0}), (34238, {'train/accuracy': 0.6710546612739563, 'train/loss': 1.5784615278244019, 'validation/accuracy': 0.6174600124359131, 'validation/loss': 1.8176082372665405, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.435833215713501, 'test/num_examples': 10000, 'score': 15580.754612445831, 'total_duration': 16865.220044851303, 'accumulated_submission_time': 15580.754612445831, 'accumulated_eval_time': 1281.597553730011, 'accumulated_logging_time': 1.0574004650115967, 'global_step': 34238, 'preemption_count': 0}), (35166, {'train/accuracy': 0.6815429329872131, 'train/loss': 1.5850794315338135, 'validation/accuracy': 0.621239960193634, 'validation/loss': 1.8454642295837402, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.4827773571014404, 'test/num_examples': 10000, 'score': 16000.741730213165, 'total_duration': 17319.111181020737, 'accumulated_submission_time': 16000.741730213165, 'accumulated_eval_time': 1315.423936367035, 'accumulated_logging_time': 1.0867114067077637, 'global_step': 35166, 'preemption_count': 0}), (36093, {'train/accuracy': 0.6749609112739563, 'train/loss': 1.5821787118911743, 'validation/accuracy': 0.6269599795341492, 'validation/loss': 1.7968742847442627, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.4259908199310303, 'test/num_examples': 10000, 'score': 16421.03150343895, 'total_duration': 17774.126088142395, 'accumulated_submission_time': 16421.03150343895, 'accumulated_eval_time': 1350.0655298233032, 'accumulated_logging_time': 1.1218464374542236, 'global_step': 36093, 'preemption_count': 0}), (37022, {'train/accuracy': 0.6784374713897705, 'train/loss': 1.5355225801467896, 'validation/accuracy': 0.6299600005149841, 'validation/loss': 1.7512410879135132, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.398932933807373, 'test/num_examples': 10000, 'score': 16841.30184864998, 'total_duration': 18229.68051123619, 'accumulated_submission_time': 16841.30184864998, 'accumulated_eval_time': 1385.2672073841095, 'accumulated_logging_time': 1.1547789573669434, 'global_step': 37022, 'preemption_count': 0}), (37949, {'train/accuracy': 0.6911327838897705, 'train/loss': 1.483366847038269, 'validation/accuracy': 0.6319000124931335, 'validation/loss': 1.7460869550704956, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.372727155685425, 'test/num_examples': 10000, 'score': 17261.51871085167, 'total_duration': 18686.173897981644, 'accumulated_submission_time': 17261.51871085167, 'accumulated_eval_time': 1421.4622313976288, 'accumulated_logging_time': 1.187713623046875, 'global_step': 37949, 'preemption_count': 0}), (38877, {'train/accuracy': 0.6896093487739563, 'train/loss': 1.5001717805862427, 'validation/accuracy': 0.6326599717140198, 'validation/loss': 1.7544867992401123, 'validation/num_examples': 50000, 'test/accuracy': 0.5072000026702881, 'test/loss': 2.3739521503448486, 'test/num_examples': 10000, 'score': 17681.805111408234, 'total_duration': 19140.70230269432, 'accumulated_submission_time': 17681.805111408234, 'accumulated_eval_time': 1455.6237666606903, 'accumulated_logging_time': 1.2181427478790283, 'global_step': 38877, 'preemption_count': 0}), (39807, {'train/accuracy': 0.6860156059265137, 'train/loss': 1.5207045078277588, 'validation/accuracy': 0.6359599828720093, 'validation/loss': 1.7454924583435059, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.383885622024536, 'test/num_examples': 10000, 'score': 18101.77188515663, 'total_duration': 19594.19838809967, 'accumulated_submission_time': 18101.77188515663, 'accumulated_eval_time': 1489.073492050171, 'accumulated_logging_time': 1.2485857009887695, 'global_step': 39807, 'preemption_count': 0}), (40735, {'train/accuracy': 0.6874804496765137, 'train/loss': 1.4901330471038818, 'validation/accuracy': 0.6361799836158752, 'validation/loss': 1.7246910333633423, 'validation/num_examples': 50000, 'test/accuracy': 0.5095000267028809, 'test/loss': 2.366427183151245, 'test/num_examples': 10000, 'score': 18522.085191726685, 'total_duration': 20046.41907286644, 'accumulated_submission_time': 18522.085191726685, 'accumulated_eval_time': 1520.9033830165863, 'accumulated_logging_time': 1.2772552967071533, 'global_step': 40735, 'preemption_count': 0}), (41661, {'train/accuracy': 0.7146679759025574, 'train/loss': 1.4313410520553589, 'validation/accuracy': 0.6459000110626221, 'validation/loss': 1.7241450548171997, 'validation/num_examples': 50000, 'test/accuracy': 0.517300009727478, 'test/loss': 2.3592562675476074, 'test/num_examples': 10000, 'score': 18942.435485124588, 'total_duration': 20499.756719589233, 'accumulated_submission_time': 18942.435485124588, 'accumulated_eval_time': 1553.8083319664001, 'accumulated_logging_time': 1.3109490871429443, 'global_step': 41661, 'preemption_count': 0}), (42589, {'train/accuracy': 0.6867382526397705, 'train/loss': 1.5198066234588623, 'validation/accuracy': 0.6368399858474731, 'validation/loss': 1.7426916360855103, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.363996982574463, 'test/num_examples': 10000, 'score': 19362.675671339035, 'total_duration': 20955.205878019333, 'accumulated_submission_time': 19362.675671339035, 'accumulated_eval_time': 1588.933512687683, 'accumulated_logging_time': 1.3452599048614502, 'global_step': 42589, 'preemption_count': 0}), (43520, {'train/accuracy': 0.7005859017372131, 'train/loss': 1.4200528860092163, 'validation/accuracy': 0.6437000036239624, 'validation/loss': 1.678924322128296, 'validation/num_examples': 50000, 'test/accuracy': 0.5212000012397766, 'test/loss': 2.3035788536071777, 'test/num_examples': 10000, 'score': 19782.83354473114, 'total_duration': 21409.879311323166, 'accumulated_submission_time': 19782.83354473114, 'accumulated_eval_time': 1623.3567078113556, 'accumulated_logging_time': 1.3867592811584473, 'global_step': 43520, 'preemption_count': 0}), (44447, {'train/accuracy': 0.7023046612739563, 'train/loss': 1.501721739768982, 'validation/accuracy': 0.6417799592018127, 'validation/loss': 1.76237154006958, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.387691020965576, 'test/num_examples': 10000, 'score': 20203.088237285614, 'total_duration': 21863.43239045143, 'accumulated_submission_time': 20203.088237285614, 'accumulated_eval_time': 1656.5718541145325, 'accumulated_logging_time': 1.4205570220947266, 'global_step': 44447, 'preemption_count': 0}), (45375, {'train/accuracy': 0.6984961032867432, 'train/loss': 1.481454610824585, 'validation/accuracy': 0.6467799544334412, 'validation/loss': 1.7090678215026855, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.3472135066986084, 'test/num_examples': 10000, 'score': 20623.023255348206, 'total_duration': 22318.312511205673, 'accumulated_submission_time': 20623.023255348206, 'accumulated_eval_time': 1691.4364099502563, 'accumulated_logging_time': 1.4526786804199219, 'global_step': 45375, 'preemption_count': 0}), (46301, {'train/accuracy': 0.7051757574081421, 'train/loss': 1.4099836349487305, 'validation/accuracy': 0.6536799669265747, 'validation/loss': 1.652795433998108, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.281585216522217, 'test/num_examples': 10000, 'score': 21043.11750602722, 'total_duration': 22775.022524118423, 'accumulated_submission_time': 21043.11750602722, 'accumulated_eval_time': 1727.9724340438843, 'accumulated_logging_time': 1.4842946529388428, 'global_step': 46301, 'preemption_count': 0}), (47228, {'train/accuracy': 0.7090820074081421, 'train/loss': 1.4518290758132935, 'validation/accuracy': 0.6502000093460083, 'validation/loss': 1.708951473236084, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.330073356628418, 'test/num_examples': 10000, 'score': 21463.19794869423, 'total_duration': 23228.048787355423, 'accumulated_submission_time': 21463.19794869423, 'accumulated_eval_time': 1760.8348679542542, 'accumulated_logging_time': 1.5186994075775146, 'global_step': 47228, 'preemption_count': 0}), (48156, {'train/accuracy': 0.7205273509025574, 'train/loss': 1.3598943948745728, 'validation/accuracy': 0.6526199579238892, 'validation/loss': 1.6453591585159302, 'validation/num_examples': 50000, 'test/accuracy': 0.5291000008583069, 'test/loss': 2.2683568000793457, 'test/num_examples': 10000, 'score': 21883.307546377182, 'total_duration': 23681.823693037033, 'accumulated_submission_time': 21883.307546377182, 'accumulated_eval_time': 1794.4216213226318, 'accumulated_logging_time': 1.5487060546875, 'global_step': 48156, 'preemption_count': 0}), (49084, {'train/accuracy': 0.7106054425239563, 'train/loss': 1.384338617324829, 'validation/accuracy': 0.6567999720573425, 'validation/loss': 1.6216844320297241, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.238567352294922, 'test/num_examples': 10000, 'score': 22303.582375764847, 'total_duration': 24135.841794013977, 'accumulated_submission_time': 22303.582375764847, 'accumulated_eval_time': 1828.0827286243439, 'accumulated_logging_time': 1.5821101665496826, 'global_step': 49084, 'preemption_count': 0}), (50009, {'train/accuracy': 0.7145702838897705, 'train/loss': 1.409250259399414, 'validation/accuracy': 0.6565799713134766, 'validation/loss': 1.664278507232666, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.2743983268737793, 'test/num_examples': 10000, 'score': 22723.54018163681, 'total_duration': 24589.507689237595, 'accumulated_submission_time': 22723.54018163681, 'accumulated_eval_time': 1861.7054600715637, 'accumulated_logging_time': 1.618788242340088, 'global_step': 50009, 'preemption_count': 0}), (50934, {'train/accuracy': 0.727734386920929, 'train/loss': 1.3187165260314941, 'validation/accuracy': 0.6599999666213989, 'validation/loss': 1.6194846630096436, 'validation/num_examples': 50000, 'test/accuracy': 0.5382000207901001, 'test/loss': 2.2213504314422607, 'test/num_examples': 10000, 'score': 23143.673278331757, 'total_duration': 25044.706683397293, 'accumulated_submission_time': 23143.673278331757, 'accumulated_eval_time': 1896.6888296604156, 'accumulated_logging_time': 1.6524369716644287, 'global_step': 50934, 'preemption_count': 0}), (51860, {'train/accuracy': 0.7147851586341858, 'train/loss': 1.3640855550765991, 'validation/accuracy': 0.6587399840354919, 'validation/loss': 1.6124135255813599, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.21683406829834, 'test/num_examples': 10000, 'score': 23563.626941919327, 'total_duration': 25499.76881289482, 'accumulated_submission_time': 23563.626941919327, 'accumulated_eval_time': 1931.7127187252045, 'accumulated_logging_time': 1.6885082721710205, 'global_step': 51860, 'preemption_count': 0}), (52789, {'train/accuracy': 0.7134960889816284, 'train/loss': 1.4122158288955688, 'validation/accuracy': 0.6552799940109253, 'validation/loss': 1.6602015495300293, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.2857484817504883, 'test/num_examples': 10000, 'score': 23983.90749502182, 'total_duration': 25954.107449531555, 'accumulated_submission_time': 23983.90749502182, 'accumulated_eval_time': 1965.6869568824768, 'accumulated_logging_time': 1.7237651348114014, 'global_step': 52789, 'preemption_count': 0}), (53718, {'train/accuracy': 0.7314453125, 'train/loss': 1.304121494293213, 'validation/accuracy': 0.6644399762153625, 'validation/loss': 1.601436972618103, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.231038808822632, 'test/num_examples': 10000, 'score': 24404.197466611862, 'total_duration': 26407.075337409973, 'accumulated_submission_time': 24404.197466611862, 'accumulated_eval_time': 1998.2732956409454, 'accumulated_logging_time': 1.7659268379211426, 'global_step': 53718, 'preemption_count': 0}), (54647, {'train/accuracy': 0.71533203125, 'train/loss': 1.3832980394363403, 'validation/accuracy': 0.6642999649047852, 'validation/loss': 1.607182502746582, 'validation/num_examples': 50000, 'test/accuracy': 0.5401000380516052, 'test/loss': 2.210639476776123, 'test/num_examples': 10000, 'score': 24824.456993341446, 'total_duration': 26861.518835544586, 'accumulated_submission_time': 24824.456993341446, 'accumulated_eval_time': 2032.3752472400665, 'accumulated_logging_time': 1.7985684871673584, 'global_step': 54647, 'preemption_count': 0}), (55575, {'train/accuracy': 0.7239453196525574, 'train/loss': 1.3593758344650269, 'validation/accuracy': 0.6665999889373779, 'validation/loss': 1.6094951629638672, 'validation/num_examples': 50000, 'test/accuracy': 0.5408000349998474, 'test/loss': 2.2293291091918945, 'test/num_examples': 10000, 'score': 25244.383882761, 'total_duration': 27314.31769967079, 'accumulated_submission_time': 25244.383882761, 'accumulated_eval_time': 2065.166063785553, 'accumulated_logging_time': 1.8309228420257568, 'global_step': 55575, 'preemption_count': 0}), (56501, {'train/accuracy': 0.7246484160423279, 'train/loss': 1.3352673053741455, 'validation/accuracy': 0.6661999821662903, 'validation/loss': 1.6007659435272217, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.2219364643096924, 'test/num_examples': 10000, 'score': 25665.15626358986, 'total_duration': 27768.345024824142, 'accumulated_submission_time': 25665.15626358986, 'accumulated_eval_time': 2098.33789563179, 'accumulated_logging_time': 1.864485502243042, 'global_step': 56501, 'preemption_count': 0}), (57431, {'train/accuracy': 0.7429101467132568, 'train/loss': 1.2352076768875122, 'validation/accuracy': 0.6694999933242798, 'validation/loss': 1.5599645376205444, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.1928648948669434, 'test/num_examples': 10000, 'score': 26085.455446720123, 'total_duration': 28223.11777973175, 'accumulated_submission_time': 26085.455446720123, 'accumulated_eval_time': 2132.7292597293854, 'accumulated_logging_time': 1.8969478607177734, 'global_step': 57431, 'preemption_count': 0}), (58358, {'train/accuracy': 0.7205859422683716, 'train/loss': 1.3466764688491821, 'validation/accuracy': 0.6641799807548523, 'validation/loss': 1.5929832458496094, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.2014687061309814, 'test/num_examples': 10000, 'score': 26505.45462369919, 'total_duration': 28676.990658283234, 'accumulated_submission_time': 26505.45462369919, 'accumulated_eval_time': 2166.513976097107, 'accumulated_logging_time': 1.9372212886810303, 'global_step': 58358, 'preemption_count': 0}), (59285, {'train/accuracy': 0.7235937118530273, 'train/loss': 1.3309944868087769, 'validation/accuracy': 0.6640599966049194, 'validation/loss': 1.594617247581482, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.206836462020874, 'test/num_examples': 10000, 'score': 26925.466319322586, 'total_duration': 29131.28421139717, 'accumulated_submission_time': 26925.466319322586, 'accumulated_eval_time': 2200.7110509872437, 'accumulated_logging_time': 1.9730114936828613, 'global_step': 59285, 'preemption_count': 0}), (60211, {'train/accuracy': 0.7406054735183716, 'train/loss': 1.2563652992248535, 'validation/accuracy': 0.6726999878883362, 'validation/loss': 1.5577574968338013, 'validation/num_examples': 50000, 'test/accuracy': 0.547700047492981, 'test/loss': 2.189788341522217, 'test/num_examples': 10000, 'score': 27345.63130736351, 'total_duration': 29584.988654613495, 'accumulated_submission_time': 27345.63130736351, 'accumulated_eval_time': 2234.1636261940002, 'accumulated_logging_time': 2.0108447074890137, 'global_step': 60211, 'preemption_count': 0}), (61138, {'train/accuracy': 0.729296863079071, 'train/loss': 1.288324236869812, 'validation/accuracy': 0.6774599552154541, 'validation/loss': 1.5246641635894775, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.157984495162964, 'test/num_examples': 10000, 'score': 27765.606746673584, 'total_duration': 30037.10212635994, 'accumulated_submission_time': 27765.606746673584, 'accumulated_eval_time': 2266.2198588848114, 'accumulated_logging_time': 2.044236660003662, 'global_step': 61138, 'preemption_count': 0}), (62061, {'train/accuracy': 0.7343164086341858, 'train/loss': 1.2893426418304443, 'validation/accuracy': 0.6753199696540833, 'validation/loss': 1.5548889636993408, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 2.169198751449585, 'test/num_examples': 10000, 'score': 28185.229808330536, 'total_duration': 30493.144993782043, 'accumulated_submission_time': 28185.229808330536, 'accumulated_eval_time': 2302.1697578430176, 'accumulated_logging_time': 2.4654579162597656, 'global_step': 62061, 'preemption_count': 0}), (62990, {'train/accuracy': 0.7407812476158142, 'train/loss': 1.2475316524505615, 'validation/accuracy': 0.6754399538040161, 'validation/loss': 1.5307263135910034, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.156816005706787, 'test/num_examples': 10000, 'score': 28605.168552160263, 'total_duration': 30950.45087170601, 'accumulated_submission_time': 28605.168552160263, 'accumulated_eval_time': 2339.451201438904, 'accumulated_logging_time': 2.5024900436401367, 'global_step': 62990, 'preemption_count': 0}), (63916, {'train/accuracy': 0.7317968606948853, 'train/loss': 1.2713884115219116, 'validation/accuracy': 0.6771399974822998, 'validation/loss': 1.5070552825927734, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.1333210468292236, 'test/num_examples': 10000, 'score': 29025.421385526657, 'total_duration': 31407.895943164825, 'accumulated_submission_time': 29025.421385526657, 'accumulated_eval_time': 2376.5558342933655, 'accumulated_logging_time': 2.5417940616607666, 'global_step': 63916, 'preemption_count': 0}), (64842, {'train/accuracy': 0.7364453077316284, 'train/loss': 1.2684588432312012, 'validation/accuracy': 0.6774799823760986, 'validation/loss': 1.5218348503112793, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.1238458156585693, 'test/num_examples': 10000, 'score': 29445.614727020264, 'total_duration': 31861.867134332657, 'accumulated_submission_time': 29445.614727020264, 'accumulated_eval_time': 2410.2451598644257, 'accumulated_logging_time': 2.581367254257202, 'global_step': 64842, 'preemption_count': 0}), (65767, {'train/accuracy': 0.7422069907188416, 'train/loss': 1.2749184370040894, 'validation/accuracy': 0.6781600117683411, 'validation/loss': 1.548775553703308, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.1705105304718018, 'test/num_examples': 10000, 'score': 29865.653613567352, 'total_duration': 32315.68285059929, 'accumulated_submission_time': 29865.653613567352, 'accumulated_eval_time': 2443.933711528778, 'accumulated_logging_time': 2.6212127208709717, 'global_step': 65767, 'preemption_count': 0}), (66693, {'train/accuracy': 0.758984386920929, 'train/loss': 1.163934350013733, 'validation/accuracy': 0.6821199655532837, 'validation/loss': 1.4920015335083008, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.1089136600494385, 'test/num_examples': 10000, 'score': 30285.574808120728, 'total_duration': 32769.177268743515, 'accumulated_submission_time': 30285.574808120728, 'accumulated_eval_time': 2477.4188113212585, 'accumulated_logging_time': 2.660482168197632, 'global_step': 66693, 'preemption_count': 0}), (67619, {'train/accuracy': 0.7391406297683716, 'train/loss': 1.2571861743927002, 'validation/accuracy': 0.6822999715805054, 'validation/loss': 1.5116750001907349, 'validation/num_examples': 50000, 'test/accuracy': 0.551300048828125, 'test/loss': 2.1411116123199463, 'test/num_examples': 10000, 'score': 30705.76295566559, 'total_duration': 33226.444074869156, 'accumulated_submission_time': 30705.76295566559, 'accumulated_eval_time': 2514.4139833450317, 'accumulated_logging_time': 2.6944541931152344, 'global_step': 67619, 'preemption_count': 0}), (68545, {'train/accuracy': 0.74609375, 'train/loss': 1.2384456396102905, 'validation/accuracy': 0.683459997177124, 'validation/loss': 1.5109264850616455, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.133676052093506, 'test/num_examples': 10000, 'score': 31125.828361272812, 'total_duration': 33680.55595970154, 'accumulated_submission_time': 31125.828361272812, 'accumulated_eval_time': 2548.3767414093018, 'accumulated_logging_time': 2.7300000190734863, 'global_step': 68545, 'preemption_count': 0}), (69471, {'train/accuracy': 0.755175769329071, 'train/loss': 1.170236349105835, 'validation/accuracy': 0.684939980506897, 'validation/loss': 1.482764482498169, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.103188991546631, 'test/num_examples': 10000, 'score': 31545.97432255745, 'total_duration': 34133.97758722305, 'accumulated_submission_time': 31545.97432255745, 'accumulated_eval_time': 2581.5695893764496, 'accumulated_logging_time': 2.7644755840301514, 'global_step': 69471, 'preemption_count': 0}), (70398, {'train/accuracy': 0.7429296970367432, 'train/loss': 1.2822554111480713, 'validation/accuracy': 0.6846599578857422, 'validation/loss': 1.5301166772842407, 'validation/num_examples': 50000, 'test/accuracy': 0.5548000335693359, 'test/loss': 2.153522253036499, 'test/num_examples': 10000, 'score': 31966.082848072052, 'total_duration': 34589.028621673584, 'accumulated_submission_time': 31966.082848072052, 'accumulated_eval_time': 2616.4249787330627, 'accumulated_logging_time': 2.803164482116699, 'global_step': 70398, 'preemption_count': 0}), (71325, {'train/accuracy': 0.7483007907867432, 'train/loss': 1.252873182296753, 'validation/accuracy': 0.6881600022315979, 'validation/loss': 1.5203845500946045, 'validation/num_examples': 50000, 'test/accuracy': 0.5677000284194946, 'test/loss': 2.1288256645202637, 'test/num_examples': 10000, 'score': 32385.999056339264, 'total_duration': 35042.29918694496, 'accumulated_submission_time': 32385.999056339264, 'accumulated_eval_time': 2649.695028066635, 'accumulated_logging_time': 2.8389945030212402, 'global_step': 71325, 'preemption_count': 0}), (72251, {'train/accuracy': 0.7540820240974426, 'train/loss': 1.2140352725982666, 'validation/accuracy': 0.6817799806594849, 'validation/loss': 1.519381046295166, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 2.117690086364746, 'test/num_examples': 10000, 'score': 32805.99316358566, 'total_duration': 35496.07172703743, 'accumulated_submission_time': 32805.99316358566, 'accumulated_eval_time': 2683.389586210251, 'accumulated_logging_time': 2.8732550144195557, 'global_step': 72251, 'preemption_count': 0}), (73176, {'train/accuracy': 0.7476171851158142, 'train/loss': 1.2500646114349365, 'validation/accuracy': 0.6918999552726746, 'validation/loss': 1.4931375980377197, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.119715929031372, 'test/num_examples': 10000, 'score': 33226.05568480492, 'total_duration': 35948.66935944557, 'accumulated_submission_time': 33226.05568480492, 'accumulated_eval_time': 2715.83868432045, 'accumulated_logging_time': 2.911292314529419, 'global_step': 73176, 'preemption_count': 0}), (74100, {'train/accuracy': 0.7524218559265137, 'train/loss': 1.1826977729797363, 'validation/accuracy': 0.6895999908447266, 'validation/loss': 1.460847020149231, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 2.076725721359253, 'test/num_examples': 10000, 'score': 33646.12110567093, 'total_duration': 36402.65527367592, 'accumulated_submission_time': 33646.12110567093, 'accumulated_eval_time': 2749.675267457962, 'accumulated_logging_time': 2.9467294216156006, 'global_step': 74100, 'preemption_count': 0}), (75027, {'train/accuracy': 0.7589452862739563, 'train/loss': 1.1854172945022583, 'validation/accuracy': 0.6945599913597107, 'validation/loss': 1.469211220741272, 'validation/num_examples': 50000, 'test/accuracy': 0.5649999976158142, 'test/loss': 2.0995171070098877, 'test/num_examples': 10000, 'score': 34066.15981054306, 'total_duration': 36858.05403661728, 'accumulated_submission_time': 34066.15981054306, 'accumulated_eval_time': 2784.9464781284332, 'accumulated_logging_time': 2.986032724380493, 'global_step': 75027, 'preemption_count': 0}), (75953, {'train/accuracy': 0.7703906297683716, 'train/loss': 1.1538687944412231, 'validation/accuracy': 0.6911799907684326, 'validation/loss': 1.488873839378357, 'validation/num_examples': 50000, 'test/accuracy': 0.5619000196456909, 'test/loss': 2.1018519401550293, 'test/num_examples': 10000, 'score': 34486.180584430695, 'total_duration': 37311.97987627983, 'accumulated_submission_time': 34486.180584430695, 'accumulated_eval_time': 2818.763783454895, 'accumulated_logging_time': 3.024766683578491, 'global_step': 75953, 'preemption_count': 0}), (76878, {'train/accuracy': 0.7526757717132568, 'train/loss': 1.1900469064712524, 'validation/accuracy': 0.6944400072097778, 'validation/loss': 1.4452449083328247, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 2.0509159564971924, 'test/num_examples': 10000, 'score': 34906.22839021683, 'total_duration': 37763.84492731094, 'accumulated_submission_time': 34906.22839021683, 'accumulated_eval_time': 2850.4957184791565, 'accumulated_logging_time': 3.0609076023101807, 'global_step': 76878, 'preemption_count': 0}), (77804, {'train/accuracy': 0.7564452886581421, 'train/loss': 1.186036467552185, 'validation/accuracy': 0.6922799944877625, 'validation/loss': 1.4668896198272705, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 2.069314956665039, 'test/num_examples': 10000, 'score': 35326.41598343849, 'total_duration': 38219.564494132996, 'accumulated_submission_time': 35326.41598343849, 'accumulated_eval_time': 2885.9357640743256, 'accumulated_logging_time': 3.1037163734436035, 'global_step': 77804, 'preemption_count': 0}), (78733, {'train/accuracy': 0.7732617259025574, 'train/loss': 1.1194065809249878, 'validation/accuracy': 0.6947199702262878, 'validation/loss': 1.444237470626831, 'validation/num_examples': 50000, 'test/accuracy': 0.5674000382423401, 'test/loss': 2.0553789138793945, 'test/num_examples': 10000, 'score': 35746.742753982544, 'total_duration': 38673.86698675156, 'accumulated_submission_time': 35746.742753982544, 'accumulated_eval_time': 2919.8212456703186, 'accumulated_logging_time': 3.1446046829223633, 'global_step': 78733, 'preemption_count': 0}), (79660, {'train/accuracy': 0.7591992020606995, 'train/loss': 1.1473793983459473, 'validation/accuracy': 0.6970199942588806, 'validation/loss': 1.4147883653640747, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 2.0305778980255127, 'test/num_examples': 10000, 'score': 36166.86061668396, 'total_duration': 39125.99325990677, 'accumulated_submission_time': 36166.86061668396, 'accumulated_eval_time': 2951.7412304878235, 'accumulated_logging_time': 3.184016704559326, 'global_step': 79660, 'preemption_count': 0}), (80584, {'train/accuracy': 0.7634375095367432, 'train/loss': 1.142561674118042, 'validation/accuracy': 0.7002399563789368, 'validation/loss': 1.4214868545532227, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 2.0416553020477295, 'test/num_examples': 10000, 'score': 36586.801466464996, 'total_duration': 39576.94997668266, 'accumulated_submission_time': 36586.801466464996, 'accumulated_eval_time': 2982.663145303726, 'accumulated_logging_time': 3.2296504974365234, 'global_step': 80584, 'preemption_count': 0}), (81508, {'train/accuracy': 0.7697460651397705, 'train/loss': 1.1436564922332764, 'validation/accuracy': 0.6994400024414062, 'validation/loss': 1.451212763786316, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 2.0733675956726074, 'test/num_examples': 10000, 'score': 37006.7823369503, 'total_duration': 40031.986365795135, 'accumulated_submission_time': 37006.7823369503, 'accumulated_eval_time': 3017.626652240753, 'accumulated_logging_time': 3.2731289863586426, 'global_step': 81508, 'preemption_count': 0}), (82435, {'train/accuracy': 0.7611523270606995, 'train/loss': 1.1672053337097168, 'validation/accuracy': 0.699400007724762, 'validation/loss': 1.4308583736419678, 'validation/num_examples': 50000, 'test/accuracy': 0.5703999996185303, 'test/loss': 2.0486838817596436, 'test/num_examples': 10000, 'score': 37426.94573545456, 'total_duration': 40487.770376205444, 'accumulated_submission_time': 37426.94573545456, 'accumulated_eval_time': 3053.159845352173, 'accumulated_logging_time': 3.3112025260925293, 'global_step': 82435, 'preemption_count': 0}), (83361, {'train/accuracy': 0.7637695074081421, 'train/loss': 1.173699975013733, 'validation/accuracy': 0.6990399956703186, 'validation/loss': 1.4479047060012817, 'validation/num_examples': 50000, 'test/accuracy': 0.5679000020027161, 'test/loss': 2.0731194019317627, 'test/num_examples': 10000, 'score': 37846.880274534225, 'total_duration': 40944.77842760086, 'accumulated_submission_time': 37846.880274534225, 'accumulated_eval_time': 3090.146003007889, 'accumulated_logging_time': 3.34938383102417, 'global_step': 83361, 'preemption_count': 0}), (84287, {'train/accuracy': 0.7743945121765137, 'train/loss': 1.1085599660873413, 'validation/accuracy': 0.70305997133255, 'validation/loss': 1.4031741619110107, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 2.0119788646698, 'test/num_examples': 10000, 'score': 38267.15163445473, 'total_duration': 41401.175797224045, 'accumulated_submission_time': 38267.15163445473, 'accumulated_eval_time': 3126.1857776641846, 'accumulated_logging_time': 3.3865418434143066, 'global_step': 84287, 'preemption_count': 0}), (85213, {'train/accuracy': 0.7837694883346558, 'train/loss': 1.0862317085266113, 'validation/accuracy': 0.7024999856948853, 'validation/loss': 1.429186224937439, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 2.0371885299682617, 'test/num_examples': 10000, 'score': 38687.10668325424, 'total_duration': 41854.26957678795, 'accumulated_submission_time': 38687.10668325424, 'accumulated_eval_time': 3159.238482236862, 'accumulated_logging_time': 3.423867702484131, 'global_step': 85213, 'preemption_count': 0}), (86141, {'train/accuracy': 0.7665038704872131, 'train/loss': 1.1527868509292603, 'validation/accuracy': 0.7004599571228027, 'validation/loss': 1.423740267753601, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 2.027710199356079, 'test/num_examples': 10000, 'score': 39107.087277412415, 'total_duration': 42309.15513443947, 'accumulated_submission_time': 39107.087277412415, 'accumulated_eval_time': 3194.0549857616425, 'accumulated_logging_time': 3.4639732837677, 'global_step': 86141, 'preemption_count': 0}), (87068, {'train/accuracy': 0.7691406011581421, 'train/loss': 1.0984034538269043, 'validation/accuracy': 0.7033999562263489, 'validation/loss': 1.3939647674560547, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 2.0015461444854736, 'test/num_examples': 10000, 'score': 39527.25010895729, 'total_duration': 42763.15571928024, 'accumulated_submission_time': 39527.25010895729, 'accumulated_eval_time': 3227.807467699051, 'accumulated_logging_time': 3.500558376312256, 'global_step': 87068, 'preemption_count': 0}), (87991, {'train/accuracy': 0.7862499952316284, 'train/loss': 1.0665332078933716, 'validation/accuracy': 0.7097600102424622, 'validation/loss': 1.3905929327011108, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.9906413555145264, 'test/num_examples': 10000, 'score': 39947.20313882828, 'total_duration': 43219.547652721405, 'accumulated_submission_time': 39947.20313882828, 'accumulated_eval_time': 3264.1564960479736, 'accumulated_logging_time': 3.542058229446411, 'global_step': 87991, 'preemption_count': 0}), (88918, {'train/accuracy': 0.7696874737739563, 'train/loss': 1.142638087272644, 'validation/accuracy': 0.7088800072669983, 'validation/loss': 1.4129151105880737, 'validation/num_examples': 50000, 'test/accuracy': 0.5790000557899475, 'test/loss': 2.0272250175476074, 'test/num_examples': 10000, 'score': 40367.463129758835, 'total_duration': 43673.03454852104, 'accumulated_submission_time': 40367.463129758835, 'accumulated_eval_time': 3297.296382665634, 'accumulated_logging_time': 3.579483985900879, 'global_step': 88918, 'preemption_count': 0}), (89848, {'train/accuracy': 0.7753515243530273, 'train/loss': 1.1038709878921509, 'validation/accuracy': 0.7093200087547302, 'validation/loss': 1.3977713584899902, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 2.0033745765686035, 'test/num_examples': 10000, 'score': 40787.6904566288, 'total_duration': 44128.43647813797, 'accumulated_submission_time': 40787.6904566288, 'accumulated_eval_time': 3332.3801724910736, 'accumulated_logging_time': 3.621098756790161, 'global_step': 89848, 'preemption_count': 0}), (90774, {'train/accuracy': 0.781054675579071, 'train/loss': 1.074851155281067, 'validation/accuracy': 0.7090799808502197, 'validation/loss': 1.384374976158142, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.9843614101409912, 'test/num_examples': 10000, 'score': 41207.61959028244, 'total_duration': 44582.04789733887, 'accumulated_submission_time': 41207.61959028244, 'accumulated_eval_time': 3365.9759092330933, 'accumulated_logging_time': 3.6582560539245605, 'global_step': 90774, 'preemption_count': 0}), (91701, {'train/accuracy': 0.77406245470047, 'train/loss': 1.1004438400268555, 'validation/accuracy': 0.7123000025749207, 'validation/loss': 1.3684351444244385, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.9665168523788452, 'test/num_examples': 10000, 'score': 41628.0981926918, 'total_duration': 45036.28593277931, 'accumulated_submission_time': 41628.0981926918, 'accumulated_eval_time': 3399.644933462143, 'accumulated_logging_time': 3.6996939182281494, 'global_step': 91701, 'preemption_count': 0}), (92629, {'train/accuracy': 0.7764062285423279, 'train/loss': 1.1032389402389526, 'validation/accuracy': 0.7084800004959106, 'validation/loss': 1.3871572017669678, 'validation/num_examples': 50000, 'test/accuracy': 0.5853000283241272, 'test/loss': 1.97959566116333, 'test/num_examples': 10000, 'score': 42048.34095311165, 'total_duration': 45485.623708724976, 'accumulated_submission_time': 42048.34095311165, 'accumulated_eval_time': 3428.649636030197, 'accumulated_logging_time': 3.740588903427124, 'global_step': 92629, 'preemption_count': 0}), (93553, {'train/accuracy': 0.7830468416213989, 'train/loss': 1.04946768283844, 'validation/accuracy': 0.7099199891090393, 'validation/loss': 1.3628854751586914, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.9714946746826172, 'test/num_examples': 10000, 'score': 42468.40495491028, 'total_duration': 45944.72555589676, 'accumulated_submission_time': 42468.40495491028, 'accumulated_eval_time': 3467.5938584804535, 'accumulated_logging_time': 3.784477949142456, 'global_step': 93553, 'preemption_count': 0}), (94479, {'train/accuracy': 0.8056640625, 'train/loss': 0.9454649686813354, 'validation/accuracy': 0.718459963798523, 'validation/loss': 1.3121576309204102, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.9169158935546875, 'test/num_examples': 10000, 'score': 42888.62809586525, 'total_duration': 46400.34952402115, 'accumulated_submission_time': 42888.62809586525, 'accumulated_eval_time': 3502.9022257328033, 'accumulated_logging_time': 3.8288791179656982, 'global_step': 94479, 'preemption_count': 0}), (95407, {'train/accuracy': 0.7821093797683716, 'train/loss': 1.0739256143569946, 'validation/accuracy': 0.7144799828529358, 'validation/loss': 1.3560128211975098, 'validation/num_examples': 50000, 'test/accuracy': 0.5925000309944153, 'test/loss': 1.9520272016525269, 'test/num_examples': 10000, 'score': 43308.81820011139, 'total_duration': 46854.504509449005, 'accumulated_submission_time': 43308.81820011139, 'accumulated_eval_time': 3536.771305322647, 'accumulated_logging_time': 3.8754472732543945, 'global_step': 95407, 'preemption_count': 0}), (96335, {'train/accuracy': 0.78955078125, 'train/loss': 1.0335056781768799, 'validation/accuracy': 0.7170000076293945, 'validation/loss': 1.3376338481903076, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.9266635179519653, 'test/num_examples': 10000, 'score': 43729.01921200752, 'total_duration': 47310.45342755318, 'accumulated_submission_time': 43729.01921200752, 'accumulated_eval_time': 3572.430454492569, 'accumulated_logging_time': 3.9143614768981934, 'global_step': 96335, 'preemption_count': 0}), (97262, {'train/accuracy': 0.7940233945846558, 'train/loss': 1.0544887781143188, 'validation/accuracy': 0.7132399678230286, 'validation/loss': 1.3924885988235474, 'validation/num_examples': 50000, 'test/accuracy': 0.5884000062942505, 'test/loss': 1.9976340532302856, 'test/num_examples': 10000, 'score': 44149.1900267601, 'total_duration': 47764.96751379967, 'accumulated_submission_time': 44149.1900267601, 'accumulated_eval_time': 3606.683182001114, 'accumulated_logging_time': 3.956416368484497, 'global_step': 97262, 'preemption_count': 0}), (98188, {'train/accuracy': 0.783886730670929, 'train/loss': 1.0592858791351318, 'validation/accuracy': 0.717519998550415, 'validation/loss': 1.3481186628341675, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.9524030685424805, 'test/num_examples': 10000, 'score': 44569.443935871124, 'total_duration': 48217.949201345444, 'accumulated_submission_time': 44569.443935871124, 'accumulated_eval_time': 3639.32297372818, 'accumulated_logging_time': 3.995898723602295, 'global_step': 98188, 'preemption_count': 0}), (99116, {'train/accuracy': 0.7895702719688416, 'train/loss': 1.0405603647232056, 'validation/accuracy': 0.7199599742889404, 'validation/loss': 1.3382598161697388, 'validation/num_examples': 50000, 'test/accuracy': 0.5948000550270081, 'test/loss': 1.9383907318115234, 'test/num_examples': 10000, 'score': 44989.47699189186, 'total_duration': 48673.00974225998, 'accumulated_submission_time': 44989.47699189186, 'accumulated_eval_time': 3674.262171983719, 'accumulated_logging_time': 4.035337209701538, 'global_step': 99116, 'preemption_count': 0}), (100045, {'train/accuracy': 0.7934765219688416, 'train/loss': 1.03852379322052, 'validation/accuracy': 0.719980001449585, 'validation/loss': 1.356076717376709, 'validation/num_examples': 50000, 'test/accuracy': 0.5946000218391418, 'test/loss': 1.9406942129135132, 'test/num_examples': 10000, 'score': 45409.69598340988, 'total_duration': 49126.67878437042, 'accumulated_submission_time': 45409.69598340988, 'accumulated_eval_time': 3707.622751235962, 'accumulated_logging_time': 4.07489013671875, 'global_step': 100045, 'preemption_count': 0}), (100973, {'train/accuracy': 0.7863867282867432, 'train/loss': 1.0272761583328247, 'validation/accuracy': 0.7179799675941467, 'validation/loss': 1.3234541416168213, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.9154317378997803, 'test/num_examples': 10000, 'score': 45829.853684186935, 'total_duration': 49581.768104314804, 'accumulated_submission_time': 45829.853684186935, 'accumulated_eval_time': 3742.466834306717, 'accumulated_logging_time': 4.11404824256897, 'global_step': 100973, 'preemption_count': 0}), (101901, {'train/accuracy': 0.7957617044448853, 'train/loss': 1.0064743757247925, 'validation/accuracy': 0.7209399938583374, 'validation/loss': 1.316093921661377, 'validation/num_examples': 50000, 'test/accuracy': 0.598300039768219, 'test/loss': 1.912156343460083, 'test/num_examples': 10000, 'score': 46250.57767724991, 'total_duration': 50035.35246872902, 'accumulated_submission_time': 46250.57767724991, 'accumulated_eval_time': 3775.237138032913, 'accumulated_logging_time': 4.155972242355347, 'global_step': 101901, 'preemption_count': 0}), (102828, {'train/accuracy': 0.7928906083106995, 'train/loss': 1.0309524536132812, 'validation/accuracy': 0.7185800075531006, 'validation/loss': 1.3496639728546143, 'validation/num_examples': 50000, 'test/accuracy': 0.5942000150680542, 'test/loss': 1.9573079347610474, 'test/num_examples': 10000, 'score': 46670.89849615097, 'total_duration': 50491.19104528427, 'accumulated_submission_time': 46670.89849615097, 'accumulated_eval_time': 3810.6633427143097, 'accumulated_logging_time': 4.199147939682007, 'global_step': 102828, 'preemption_count': 0}), (103754, {'train/accuracy': 0.8130663633346558, 'train/loss': 0.960721492767334, 'validation/accuracy': 0.7226600050926208, 'validation/loss': 1.3447332382202148, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.9341633319854736, 'test/num_examples': 10000, 'score': 47091.06334114075, 'total_duration': 50944.35725951195, 'accumulated_submission_time': 47091.06334114075, 'accumulated_eval_time': 3843.5726778507233, 'accumulated_logging_time': 4.241317510604858, 'global_step': 103754, 'preemption_count': 0}), (104681, {'train/accuracy': 0.79408198595047, 'train/loss': 1.0491050481796265, 'validation/accuracy': 0.7261199951171875, 'validation/loss': 1.340632438659668, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.9325157403945923, 'test/num_examples': 10000, 'score': 47511.09459543228, 'total_duration': 51399.688069581985, 'accumulated_submission_time': 47511.09459543228, 'accumulated_eval_time': 3878.7800257205963, 'accumulated_logging_time': 4.284324884414673, 'global_step': 104681, 'preemption_count': 0}), (105604, {'train/accuracy': 0.7981054782867432, 'train/loss': 1.0043699741363525, 'validation/accuracy': 0.7231199741363525, 'validation/loss': 1.320081353187561, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.9280401468276978, 'test/num_examples': 10000, 'score': 47930.99238157272, 'total_duration': 51854.14779257774, 'accumulated_submission_time': 47930.99238157272, 'accumulated_eval_time': 3912.8647713661194, 'accumulated_logging_time': 4.712871551513672, 'global_step': 105604, 'preemption_count': 0}), (106529, {'train/accuracy': 0.8086132407188416, 'train/loss': 0.9574166536331177, 'validation/accuracy': 0.7298600077629089, 'validation/loss': 1.2995718717575073, 'validation/num_examples': 50000, 'test/accuracy': 0.603600025177002, 'test/loss': 1.904357671737671, 'test/num_examples': 10000, 'score': 48351.039007902145, 'total_duration': 52309.55121731758, 'accumulated_submission_time': 48351.039007902145, 'accumulated_eval_time': 3948.1297523975372, 'accumulated_logging_time': 4.755612373352051, 'global_step': 106529, 'preemption_count': 0}), (107457, {'train/accuracy': 0.796582043170929, 'train/loss': 1.0255680084228516, 'validation/accuracy': 0.7306199669837952, 'validation/loss': 1.3091156482696533, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.917056679725647, 'test/num_examples': 10000, 'score': 48771.34631705284, 'total_duration': 52766.52301621437, 'accumulated_submission_time': 48771.34631705284, 'accumulated_eval_time': 3984.6974267959595, 'accumulated_logging_time': 4.803227424621582, 'global_step': 107457, 'preemption_count': 0}), (108382, {'train/accuracy': 0.8032421469688416, 'train/loss': 0.9735230803489685, 'validation/accuracy': 0.7277199625968933, 'validation/loss': 1.296148419380188, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.8809870481491089, 'test/num_examples': 10000, 'score': 49191.41876125336, 'total_duration': 53220.951176166534, 'accumulated_submission_time': 49191.41876125336, 'accumulated_eval_time': 4018.9604184627533, 'accumulated_logging_time': 4.847645044326782, 'global_step': 108382, 'preemption_count': 0}), (109308, {'train/accuracy': 0.80712890625, 'train/loss': 0.9809885025024414, 'validation/accuracy': 0.7274799942970276, 'validation/loss': 1.309841513633728, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.9155027866363525, 'test/num_examples': 10000, 'score': 49611.47404384613, 'total_duration': 53674.50714588165, 'accumulated_submission_time': 49611.47404384613, 'accumulated_eval_time': 4052.371610879898, 'accumulated_logging_time': 4.887386798858643, 'global_step': 109308, 'preemption_count': 0}), (110233, {'train/accuracy': 0.8057421445846558, 'train/loss': 0.956943690776825, 'validation/accuracy': 0.7321999669075012, 'validation/loss': 1.2614250183105469, 'validation/num_examples': 50000, 'test/accuracy': 0.6146000027656555, 'test/loss': 1.8612117767333984, 'test/num_examples': 10000, 'score': 50031.75916481018, 'total_duration': 54126.982813835144, 'accumulated_submission_time': 50031.75916481018, 'accumulated_eval_time': 4084.4641699790955, 'accumulated_logging_time': 4.937520742416382, 'global_step': 110233, 'preemption_count': 0}), (111158, {'train/accuracy': 0.8017968535423279, 'train/loss': 1.0286946296691895, 'validation/accuracy': 0.7317799925804138, 'validation/loss': 1.3278456926345825, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.9332977533340454, 'test/num_examples': 10000, 'score': 50451.79173922539, 'total_duration': 54581.74570274353, 'accumulated_submission_time': 50451.79173922539, 'accumulated_eval_time': 4119.084473133087, 'accumulated_logging_time': 4.998009443283081, 'global_step': 111158, 'preemption_count': 0}), (112081, {'train/accuracy': 0.8128515481948853, 'train/loss': 0.9389355778694153, 'validation/accuracy': 0.7346799969673157, 'validation/loss': 1.2732287645339966, 'validation/num_examples': 50000, 'test/accuracy': 0.6106000542640686, 'test/loss': 1.8744934797286987, 'test/num_examples': 10000, 'score': 50871.74392461777, 'total_duration': 55036.06898331642, 'accumulated_submission_time': 50871.74392461777, 'accumulated_eval_time': 4153.366170406342, 'accumulated_logging_time': 5.039664268493652, 'global_step': 112081, 'preemption_count': 0}), (113004, {'train/accuracy': 0.8268554210662842, 'train/loss': 0.8962365388870239, 'validation/accuracy': 0.7337200045585632, 'validation/loss': 1.2884944677352905, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.887868046760559, 'test/num_examples': 10000, 'score': 51291.82757949829, 'total_duration': 55491.248410224915, 'accumulated_submission_time': 51291.82757949829, 'accumulated_eval_time': 4188.3666253089905, 'accumulated_logging_time': 5.085594415664673, 'global_step': 113004, 'preemption_count': 0}), (113930, {'train/accuracy': 0.8086913824081421, 'train/loss': 0.9540855288505554, 'validation/accuracy': 0.7350999712944031, 'validation/loss': 1.2699496746063232, 'validation/num_examples': 50000, 'test/accuracy': 0.6146000027656555, 'test/loss': 1.8564660549163818, 'test/num_examples': 10000, 'score': 51711.961062669754, 'total_duration': 55945.711918354034, 'accumulated_submission_time': 51711.961062669754, 'accumulated_eval_time': 4222.597744464874, 'accumulated_logging_time': 5.1359288692474365, 'global_step': 113930, 'preemption_count': 0}), (114857, {'train/accuracy': 0.8112109303474426, 'train/loss': 0.9619636535644531, 'validation/accuracy': 0.7340999841690063, 'validation/loss': 1.2924312353134155, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.887677788734436, 'test/num_examples': 10000, 'score': 52131.87544989586, 'total_duration': 56399.65555882454, 'accumulated_submission_time': 52131.87544989586, 'accumulated_eval_time': 4256.532593727112, 'accumulated_logging_time': 5.181121587753296, 'global_step': 114857, 'preemption_count': 0}), (115782, {'train/accuracy': 0.8207421898841858, 'train/loss': 0.9196900129318237, 'validation/accuracy': 0.7369199991226196, 'validation/loss': 1.2814921140670776, 'validation/num_examples': 50000, 'test/accuracy': 0.6084000468254089, 'test/loss': 1.8864821195602417, 'test/num_examples': 10000, 'score': 52551.95956778526, 'total_duration': 56855.459886312485, 'accumulated_submission_time': 52551.95956778526, 'accumulated_eval_time': 4292.1596002578735, 'accumulated_logging_time': 5.225587368011475, 'global_step': 115782, 'preemption_count': 0}), (116708, {'train/accuracy': 0.8111132383346558, 'train/loss': 0.9671093821525574, 'validation/accuracy': 0.7394199967384338, 'validation/loss': 1.2774097919464111, 'validation/num_examples': 50000, 'test/accuracy': 0.6146000027656555, 'test/loss': 1.8671879768371582, 'test/num_examples': 10000, 'score': 52972.2819852829, 'total_duration': 57307.31153726578, 'accumulated_submission_time': 52972.2819852829, 'accumulated_eval_time': 4323.5953731536865, 'accumulated_logging_time': 5.270857572555542, 'global_step': 116708, 'preemption_count': 0}), (117635, {'train/accuracy': 0.8186327815055847, 'train/loss': 0.9464924931526184, 'validation/accuracy': 0.73881995677948, 'validation/loss': 1.275384545326233, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8721600770950317, 'test/num_examples': 10000, 'score': 53392.368614435196, 'total_duration': 57759.757767915726, 'accumulated_submission_time': 53392.368614435196, 'accumulated_eval_time': 4355.863595724106, 'accumulated_logging_time': 5.312883377075195, 'global_step': 117635, 'preemption_count': 0}), (118557, {'train/accuracy': 0.8215234279632568, 'train/loss': 0.9040040969848633, 'validation/accuracy': 0.7385199666023254, 'validation/loss': 1.2626641988754272, 'validation/num_examples': 50000, 'test/accuracy': 0.6128000020980835, 'test/loss': 1.859431505203247, 'test/num_examples': 10000, 'score': 53812.669481277466, 'total_duration': 58216.1560792923, 'accumulated_submission_time': 53812.669481277466, 'accumulated_eval_time': 4391.868913650513, 'accumulated_logging_time': 5.357113838195801, 'global_step': 118557, 'preemption_count': 0}), (119482, {'train/accuracy': 0.8153515458106995, 'train/loss': 0.9577239751815796, 'validation/accuracy': 0.7410399913787842, 'validation/loss': 1.269242763519287, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.868729591369629, 'test/num_examples': 10000, 'score': 54232.62202167511, 'total_duration': 58672.590933561325, 'accumulated_submission_time': 54232.62202167511, 'accumulated_eval_time': 4428.260704755783, 'accumulated_logging_time': 5.399370908737183, 'global_step': 119482, 'preemption_count': 0}), (120407, {'train/accuracy': 0.8184374570846558, 'train/loss': 0.9347028136253357, 'validation/accuracy': 0.7399199604988098, 'validation/loss': 1.2617909908294678, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.8522409200668335, 'test/num_examples': 10000, 'score': 54652.57331991196, 'total_duration': 59125.07707071304, 'accumulated_submission_time': 54652.57331991196, 'accumulated_eval_time': 4460.699743509293, 'accumulated_logging_time': 5.445420742034912, 'global_step': 120407, 'preemption_count': 0}), (121333, {'train/accuracy': 0.8251171708106995, 'train/loss': 0.8875871300697327, 'validation/accuracy': 0.743619978427887, 'validation/loss': 1.2404817342758179, 'validation/num_examples': 50000, 'test/accuracy': 0.6199000477790833, 'test/loss': 1.8362445831298828, 'test/num_examples': 10000, 'score': 55072.54042840004, 'total_duration': 59578.87798953056, 'accumulated_submission_time': 55072.54042840004, 'accumulated_eval_time': 4494.443091392517, 'accumulated_logging_time': 5.487497329711914, 'global_step': 121333, 'preemption_count': 0}), (122261, {'train/accuracy': 0.8381249904632568, 'train/loss': 0.8315138220787048, 'validation/accuracy': 0.7446199655532837, 'validation/loss': 1.2221808433532715, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.8278734683990479, 'test/num_examples': 10000, 'score': 55492.709612846375, 'total_duration': 60033.37808465958, 'accumulated_submission_time': 55492.709612846375, 'accumulated_eval_time': 4528.681435108185, 'accumulated_logging_time': 5.531659364700317, 'global_step': 122261, 'preemption_count': 0}), (123187, {'train/accuracy': 0.8201562166213989, 'train/loss': 0.9035536050796509, 'validation/accuracy': 0.7415199875831604, 'validation/loss': 1.2345235347747803, 'validation/num_examples': 50000, 'test/accuracy': 0.6188000440597534, 'test/loss': 1.8316484689712524, 'test/num_examples': 10000, 'score': 55912.80021524429, 'total_duration': 60485.82897758484, 'accumulated_submission_time': 55912.80021524429, 'accumulated_eval_time': 4560.943968057632, 'accumulated_logging_time': 5.5809125900268555, 'global_step': 123187, 'preemption_count': 0}), (124111, {'train/accuracy': 0.8274609446525574, 'train/loss': 0.8888863325119019, 'validation/accuracy': 0.7472400069236755, 'validation/loss': 1.228013038635254, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.8307791948318481, 'test/num_examples': 10000, 'score': 56332.88788199425, 'total_duration': 60940.03962993622, 'accumulated_submission_time': 56332.88788199425, 'accumulated_eval_time': 4594.97258067131, 'accumulated_logging_time': 5.626370191574097, 'global_step': 124111, 'preemption_count': 0}), (125037, {'train/accuracy': 0.8400976657867432, 'train/loss': 0.8476256132125854, 'validation/accuracy': 0.7465400099754333, 'validation/loss': 1.231023907661438, 'validation/num_examples': 50000, 'test/accuracy': 0.6228000521659851, 'test/loss': 1.8111176490783691, 'test/num_examples': 10000, 'score': 56752.807072639465, 'total_duration': 61394.470771074295, 'accumulated_submission_time': 56752.807072639465, 'accumulated_eval_time': 4629.3885724544525, 'accumulated_logging_time': 5.673748731613159, 'global_step': 125037, 'preemption_count': 0}), (125962, {'train/accuracy': 0.8267577886581421, 'train/loss': 0.9042437076568604, 'validation/accuracy': 0.744219958782196, 'validation/loss': 1.2379591464996338, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8236942291259766, 'test/num_examples': 10000, 'score': 57172.921754837036, 'total_duration': 61848.86698770523, 'accumulated_submission_time': 57172.921754837036, 'accumulated_eval_time': 4663.57472038269, 'accumulated_logging_time': 5.72042441368103, 'global_step': 125962, 'preemption_count': 0}), (126889, {'train/accuracy': 0.829394519329071, 'train/loss': 0.8763020634651184, 'validation/accuracy': 0.7468599677085876, 'validation/loss': 1.2259188890457153, 'validation/num_examples': 50000, 'test/accuracy': 0.6248000264167786, 'test/loss': 1.8171780109405518, 'test/num_examples': 10000, 'score': 57593.299280166626, 'total_duration': 62302.96178174019, 'accumulated_submission_time': 57593.299280166626, 'accumulated_eval_time': 4697.19917845726, 'accumulated_logging_time': 5.7643516063690186, 'global_step': 126889, 'preemption_count': 0}), (127815, {'train/accuracy': 0.8350195288658142, 'train/loss': 0.8415217399597168, 'validation/accuracy': 0.7514199614524841, 'validation/loss': 1.19737708568573, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.7829385995864868, 'test/num_examples': 10000, 'score': 58013.23331975937, 'total_duration': 62756.352815151215, 'accumulated_submission_time': 58013.23331975937, 'accumulated_eval_time': 4730.561057806015, 'accumulated_logging_time': 5.811323881149292, 'global_step': 127815, 'preemption_count': 0}), (128742, {'train/accuracy': 0.8329101204872131, 'train/loss': 0.8737267255783081, 'validation/accuracy': 0.7518799901008606, 'validation/loss': 1.2153342962265015, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.8096715211868286, 'test/num_examples': 10000, 'score': 58433.33541345596, 'total_duration': 63211.96445417404, 'accumulated_submission_time': 58433.33541345596, 'accumulated_eval_time': 4765.972766160965, 'accumulated_logging_time': 5.859126567840576, 'global_step': 128742, 'preemption_count': 0}), (129669, {'train/accuracy': 0.8311132788658142, 'train/loss': 0.8590825200080872, 'validation/accuracy': 0.7507799863815308, 'validation/loss': 1.2077339887619019, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.7966262102127075, 'test/num_examples': 10000, 'score': 58853.82092785835, 'total_duration': 63661.234209775925, 'accumulated_submission_time': 58853.82092785835, 'accumulated_eval_time': 4794.663794994354, 'accumulated_logging_time': 5.904484272003174, 'global_step': 129669, 'preemption_count': 0}), (130594, {'train/accuracy': 0.84033203125, 'train/loss': 0.8434707522392273, 'validation/accuracy': 0.7518799901008606, 'validation/loss': 1.2105284929275513, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.803253412246704, 'test/num_examples': 10000, 'score': 59274.19008851051, 'total_duration': 64115.61355304718, 'accumulated_submission_time': 59274.19008851051, 'accumulated_eval_time': 4828.563596725464, 'accumulated_logging_time': 5.966588497161865, 'global_step': 130594, 'preemption_count': 0}), (131523, {'train/accuracy': 0.8484765291213989, 'train/loss': 0.8026385307312012, 'validation/accuracy': 0.7519800066947937, 'validation/loss': 1.2015353441238403, 'validation/num_examples': 50000, 'test/accuracy': 0.6252000331878662, 'test/loss': 1.8083374500274658, 'test/num_examples': 10000, 'score': 59694.75778841972, 'total_duration': 64568.58602309227, 'accumulated_submission_time': 59694.75778841972, 'accumulated_eval_time': 4860.874860286713, 'accumulated_logging_time': 6.009950637817383, 'global_step': 131523, 'preemption_count': 0}), (132448, {'train/accuracy': 0.8370116949081421, 'train/loss': 0.834997832775116, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 1.1796581745147705, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.7780799865722656, 'test/num_examples': 10000, 'score': 60114.998239040375, 'total_duration': 65024.695997714996, 'accumulated_submission_time': 60114.998239040375, 'accumulated_eval_time': 4896.653185606003, 'accumulated_logging_time': 6.053093671798706, 'global_step': 132448, 'preemption_count': 0}), (133375, {'train/accuracy': 0.8409179449081421, 'train/loss': 0.8413550853729248, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.2021714448928833, 'validation/num_examples': 50000, 'test/accuracy': 0.6372000575065613, 'test/loss': 1.7991794347763062, 'test/num_examples': 10000, 'score': 60535.22475409508, 'total_duration': 65477.413791656494, 'accumulated_submission_time': 60535.22475409508, 'accumulated_eval_time': 4929.048456430435, 'accumulated_logging_time': 6.100675344467163, 'global_step': 133375, 'preemption_count': 0}), (134299, {'train/accuracy': 0.8481249809265137, 'train/loss': 0.8018088936805725, 'validation/accuracy': 0.7539399862289429, 'validation/loss': 1.1947689056396484, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.78182053565979, 'test/num_examples': 10000, 'score': 60955.59069728851, 'total_duration': 65933.40303897858, 'accumulated_submission_time': 60955.59069728851, 'accumulated_eval_time': 4964.578231573105, 'accumulated_logging_time': 6.146247386932373, 'global_step': 134299, 'preemption_count': 0}), (135226, {'train/accuracy': 0.8419921398162842, 'train/loss': 0.8387060165405273, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.199419617652893, 'validation/num_examples': 50000, 'test/accuracy': 0.638200044631958, 'test/loss': 1.7809736728668213, 'test/num_examples': 10000, 'score': 61375.55031371117, 'total_duration': 66389.22588658333, 'accumulated_submission_time': 61375.55031371117, 'accumulated_eval_time': 5000.347151756287, 'accumulated_logging_time': 6.192385196685791, 'global_step': 135226, 'preemption_count': 0}), (136151, {'train/accuracy': 0.8457421660423279, 'train/loss': 0.8141063451766968, 'validation/accuracy': 0.7572399973869324, 'validation/loss': 1.1856240034103394, 'validation/num_examples': 50000, 'test/accuracy': 0.6385000348091125, 'test/loss': 1.7745895385742188, 'test/num_examples': 10000, 'score': 61795.65664720535, 'total_duration': 66844.14261889458, 'accumulated_submission_time': 61795.65664720535, 'accumulated_eval_time': 5035.061300992966, 'accumulated_logging_time': 6.240199089050293, 'global_step': 136151, 'preemption_count': 0}), (137076, {'train/accuracy': 0.8503710627555847, 'train/loss': 0.803084671497345, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.1784809827804565, 'validation/num_examples': 50000, 'test/accuracy': 0.6363000273704529, 'test/loss': 1.7618496417999268, 'test/num_examples': 10000, 'score': 62215.70627474785, 'total_duration': 67301.74554777145, 'accumulated_submission_time': 62215.70627474785, 'accumulated_eval_time': 5072.519873142242, 'accumulated_logging_time': 6.286186456680298, 'global_step': 137076, 'preemption_count': 0}), (138004, {'train/accuracy': 0.8441210985183716, 'train/loss': 0.8067802786827087, 'validation/accuracy': 0.7592200040817261, 'validation/loss': 1.1712822914123535, 'validation/num_examples': 50000, 'test/accuracy': 0.6401000022888184, 'test/loss': 1.7613399028778076, 'test/num_examples': 10000, 'score': 62635.84506726265, 'total_duration': 67755.41611027718, 'accumulated_submission_time': 62635.84506726265, 'accumulated_eval_time': 5105.9577214717865, 'accumulated_logging_time': 6.330903053283691, 'global_step': 138004, 'preemption_count': 0}), (138929, {'train/accuracy': 0.8473241925239563, 'train/loss': 0.8092468976974487, 'validation/accuracy': 0.7607199549674988, 'validation/loss': 1.178433895111084, 'validation/num_examples': 50000, 'test/accuracy': 0.6415000557899475, 'test/loss': 1.7597088813781738, 'test/num_examples': 10000, 'score': 63055.99439263344, 'total_duration': 68210.82635331154, 'accumulated_submission_time': 63055.99439263344, 'accumulated_eval_time': 5141.126784801483, 'accumulated_logging_time': 6.374598264694214, 'global_step': 138929, 'preemption_count': 0}), (139855, {'train/accuracy': 0.8530859351158142, 'train/loss': 0.7908298969268799, 'validation/accuracy': 0.7603999972343445, 'validation/loss': 1.179398775100708, 'validation/num_examples': 50000, 'test/accuracy': 0.6410000324249268, 'test/loss': 1.7686790227890015, 'test/num_examples': 10000, 'score': 63476.151854515076, 'total_duration': 68662.43821144104, 'accumulated_submission_time': 63476.151854515076, 'accumulated_eval_time': 5172.483269929886, 'accumulated_logging_time': 6.423179388046265, 'global_step': 139855, 'preemption_count': 0}), (140779, {'train/accuracy': 0.8641406297683716, 'train/loss': 0.7573795318603516, 'validation/accuracy': 0.7615399956703186, 'validation/loss': 1.1744194030761719, 'validation/num_examples': 50000, 'test/accuracy': 0.6406000256538391, 'test/loss': 1.7598754167556763, 'test/num_examples': 10000, 'score': 63896.06881427765, 'total_duration': 69118.56551671028, 'accumulated_submission_time': 63896.06881427765, 'accumulated_eval_time': 5208.587812423706, 'accumulated_logging_time': 6.4807868003845215, 'global_step': 140779, 'preemption_count': 0}), (141701, {'train/accuracy': 0.8532226085662842, 'train/loss': 0.7790336608886719, 'validation/accuracy': 0.7630999684333801, 'validation/loss': 1.1503957509994507, 'validation/num_examples': 50000, 'test/accuracy': 0.6450000405311584, 'test/loss': 1.7349143028259277, 'test/num_examples': 10000, 'score': 64316.15193653107, 'total_duration': 69573.42215561867, 'accumulated_submission_time': 64316.15193653107, 'accumulated_eval_time': 5243.268939495087, 'accumulated_logging_time': 6.525313138961792, 'global_step': 141701, 'preemption_count': 0}), (142628, {'train/accuracy': 0.8541601300239563, 'train/loss': 0.8001324534416199, 'validation/accuracy': 0.7625600099563599, 'validation/loss': 1.1798006296157837, 'validation/num_examples': 50000, 'test/accuracy': 0.6477000117301941, 'test/loss': 1.7499827146530151, 'test/num_examples': 10000, 'score': 64736.441365003586, 'total_duration': 70027.35831856728, 'accumulated_submission_time': 64736.441365003586, 'accumulated_eval_time': 5276.812092781067, 'accumulated_logging_time': 6.580393314361572, 'global_step': 142628, 'preemption_count': 0}), (143555, {'train/accuracy': 0.8614062070846558, 'train/loss': 0.7344046831130981, 'validation/accuracy': 0.7638199925422668, 'validation/loss': 1.1369210481643677, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.7233561277389526, 'test/num_examples': 10000, 'score': 65156.379346847534, 'total_duration': 70481.45216488838, 'accumulated_submission_time': 65156.379346847534, 'accumulated_eval_time': 5310.8514902591705, 'accumulated_logging_time': 6.647460222244263, 'global_step': 143555, 'preemption_count': 0}), (144480, {'train/accuracy': 0.852832019329071, 'train/loss': 0.7703531980514526, 'validation/accuracy': 0.7658999562263489, 'validation/loss': 1.1399189233779907, 'validation/num_examples': 50000, 'test/accuracy': 0.6437000036239624, 'test/loss': 1.7280446290969849, 'test/num_examples': 10000, 'score': 65576.38734292984, 'total_duration': 70937.19880628586, 'accumulated_submission_time': 65576.38734292984, 'accumulated_eval_time': 5346.491506576538, 'accumulated_logging_time': 6.697022199630737, 'global_step': 144480, 'preemption_count': 0}), (145406, {'train/accuracy': 0.8582421541213989, 'train/loss': 0.7630525827407837, 'validation/accuracy': 0.7664799690246582, 'validation/loss': 1.1516687870025635, 'validation/num_examples': 50000, 'test/accuracy': 0.6492000222206116, 'test/loss': 1.7340401411056519, 'test/num_examples': 10000, 'score': 65996.66863155365, 'total_duration': 71394.53972387314, 'accumulated_submission_time': 65996.66863155365, 'accumulated_eval_time': 5383.4498608112335, 'accumulated_logging_time': 6.750396251678467, 'global_step': 145406, 'preemption_count': 0}), (146333, {'train/accuracy': 0.8623632788658142, 'train/loss': 0.7401703000068665, 'validation/accuracy': 0.7676799893379211, 'validation/loss': 1.1452099084854126, 'validation/num_examples': 50000, 'test/accuracy': 0.6490000486373901, 'test/loss': 1.7267446517944336, 'test/num_examples': 10000, 'score': 66416.9311683178, 'total_duration': 71851.3624317646, 'accumulated_submission_time': 66416.9311683178, 'accumulated_eval_time': 5419.915347337723, 'accumulated_logging_time': 6.796186208724976, 'global_step': 146333, 'preemption_count': 0}), (147259, {'train/accuracy': 0.8596093654632568, 'train/loss': 0.7663670778274536, 'validation/accuracy': 0.7688199877738953, 'validation/loss': 1.1503161191940308, 'validation/num_examples': 50000, 'test/accuracy': 0.6520000100135803, 'test/loss': 1.7251871824264526, 'test/num_examples': 10000, 'score': 66837.13448381424, 'total_duration': 72306.05270385742, 'accumulated_submission_time': 66837.13448381424, 'accumulated_eval_time': 5454.302681922913, 'accumulated_logging_time': 6.847928524017334, 'global_step': 147259, 'preemption_count': 0}), (148183, {'train/accuracy': 0.8612304329872131, 'train/loss': 0.7369097471237183, 'validation/accuracy': 0.7663999795913696, 'validation/loss': 1.1357945203781128, 'validation/num_examples': 50000, 'test/accuracy': 0.6517000198364258, 'test/loss': 1.709851622581482, 'test/num_examples': 10000, 'score': 67257.20768213272, 'total_duration': 72764.31754755974, 'accumulated_submission_time': 67257.20768213272, 'accumulated_eval_time': 5492.399060964584, 'accumulated_logging_time': 6.8949620723724365, 'global_step': 148183, 'preemption_count': 0}), (149109, {'train/accuracy': 0.8688085675239563, 'train/loss': 0.7321873903274536, 'validation/accuracy': 0.7706199884414673, 'validation/loss': 1.136370062828064, 'validation/num_examples': 50000, 'test/accuracy': 0.6541000604629517, 'test/loss': 1.7144485712051392, 'test/num_examples': 10000, 'score': 67677.46754312515, 'total_duration': 73219.93533587456, 'accumulated_submission_time': 67677.46754312515, 'accumulated_eval_time': 5527.658671617508, 'accumulated_logging_time': 6.944510221481323, 'global_step': 149109, 'preemption_count': 0}), (150035, {'train/accuracy': 0.8769726157188416, 'train/loss': 0.7250710725784302, 'validation/accuracy': 0.7684800028800964, 'validation/loss': 1.159796953201294, 'validation/num_examples': 50000, 'test/accuracy': 0.653700053691864, 'test/loss': 1.7364274263381958, 'test/num_examples': 10000, 'score': 68097.54691076279, 'total_duration': 73675.49411344528, 'accumulated_submission_time': 68097.54691076279, 'accumulated_eval_time': 5563.043598890305, 'accumulated_logging_time': 6.990314960479736, 'global_step': 150035, 'preemption_count': 0}), (150961, {'train/accuracy': 0.8651366829872131, 'train/loss': 0.7272911667823792, 'validation/accuracy': 0.7701199650764465, 'validation/loss': 1.1263177394866943, 'validation/num_examples': 50000, 'test/accuracy': 0.6527000069618225, 'test/loss': 1.708383321762085, 'test/num_examples': 10000, 'score': 68517.9379067421, 'total_duration': 74134.7340862751, 'accumulated_submission_time': 68517.9379067421, 'accumulated_eval_time': 5601.793773651123, 'accumulated_logging_time': 7.040832042694092, 'global_step': 150961, 'preemption_count': 0}), (151889, {'train/accuracy': 0.8691796660423279, 'train/loss': 0.7026453018188477, 'validation/accuracy': 0.770039975643158, 'validation/loss': 1.108427882194519, 'validation/num_examples': 50000, 'test/accuracy': 0.6552000045776367, 'test/loss': 1.6844028234481812, 'test/num_examples': 10000, 'score': 68938.07565045357, 'total_duration': 74587.0642721653, 'accumulated_submission_time': 68938.07565045357, 'accumulated_eval_time': 5633.890254974365, 'accumulated_logging_time': 7.088500022888184, 'global_step': 151889, 'preemption_count': 0}), (152814, {'train/accuracy': 0.8727929592132568, 'train/loss': 0.7038527727127075, 'validation/accuracy': 0.7734599709510803, 'validation/loss': 1.1162408590316772, 'validation/num_examples': 50000, 'test/accuracy': 0.6562000513076782, 'test/loss': 1.7009633779525757, 'test/num_examples': 10000, 'score': 69358.09606146812, 'total_duration': 75041.11749482155, 'accumulated_submission_time': 69358.09606146812, 'accumulated_eval_time': 5667.825475215912, 'accumulated_logging_time': 7.13709831237793, 'global_step': 152814, 'preemption_count': 0}), (153742, {'train/accuracy': 0.8701562285423279, 'train/loss': 0.7306351065635681, 'validation/accuracy': 0.772379994392395, 'validation/loss': 1.131606936454773, 'validation/num_examples': 50000, 'test/accuracy': 0.6551000475883484, 'test/loss': 1.710476279258728, 'test/num_examples': 10000, 'score': 69778.18906927109, 'total_duration': 75495.08348155022, 'accumulated_submission_time': 69778.18906927109, 'accumulated_eval_time': 5701.598674058914, 'accumulated_logging_time': 7.188451766967773, 'global_step': 153742, 'preemption_count': 0}), (154667, {'train/accuracy': 0.8733202815055847, 'train/loss': 0.6871562004089355, 'validation/accuracy': 0.7735199928283691, 'validation/loss': 1.1001402139663696, 'validation/num_examples': 50000, 'test/accuracy': 0.6605000495910645, 'test/loss': 1.6771475076675415, 'test/num_examples': 10000, 'score': 70198.11680269241, 'total_duration': 75950.9940161705, 'accumulated_submission_time': 70198.11680269241, 'accumulated_eval_time': 5737.48500084877, 'accumulated_logging_time': 7.237675905227661, 'global_step': 154667, 'preemption_count': 0}), (155588, {'train/accuracy': 0.874804675579071, 'train/loss': 0.6974979043006897, 'validation/accuracy': 0.7736999988555908, 'validation/loss': 1.112828254699707, 'validation/num_examples': 50000, 'test/accuracy': 0.6607000231742859, 'test/loss': 1.696984887123108, 'test/num_examples': 10000, 'score': 70618.42568159103, 'total_duration': 76405.50797510147, 'accumulated_submission_time': 70618.42568159103, 'accumulated_eval_time': 5771.582266569138, 'accumulated_logging_time': 7.296308755874634, 'global_step': 155588, 'preemption_count': 0}), (156512, {'train/accuracy': 0.8740820288658142, 'train/loss': 0.7038549780845642, 'validation/accuracy': 0.7731800079345703, 'validation/loss': 1.112505555152893, 'validation/num_examples': 50000, 'test/accuracy': 0.6598000526428223, 'test/loss': 1.6827988624572754, 'test/num_examples': 10000, 'score': 71038.46853804588, 'total_duration': 76860.0290813446, 'accumulated_submission_time': 71038.46853804588, 'accumulated_eval_time': 5805.957935094833, 'accumulated_logging_time': 7.350852727890015, 'global_step': 156512, 'preemption_count': 0}), (157436, {'train/accuracy': 0.87367182970047, 'train/loss': 0.6964795589447021, 'validation/accuracy': 0.7759000062942505, 'validation/loss': 1.108446717262268, 'validation/num_examples': 50000, 'test/accuracy': 0.6623000502586365, 'test/loss': 1.684056282043457, 'test/num_examples': 10000, 'score': 71458.70658421516, 'total_duration': 77315.83238148689, 'accumulated_submission_time': 71458.70658421516, 'accumulated_eval_time': 5841.424285888672, 'accumulated_logging_time': 7.40127420425415, 'global_step': 157436, 'preemption_count': 0}), (158361, {'train/accuracy': 0.8794335722923279, 'train/loss': 0.675494372844696, 'validation/accuracy': 0.7756199836730957, 'validation/loss': 1.0992703437805176, 'validation/num_examples': 50000, 'test/accuracy': 0.659000039100647, 'test/loss': 1.6712520122528076, 'test/num_examples': 10000, 'score': 71878.72174358368, 'total_duration': 77770.12656998634, 'accumulated_submission_time': 71878.72174358368, 'accumulated_eval_time': 5875.608007192612, 'accumulated_logging_time': 7.447989463806152, 'global_step': 158361, 'preemption_count': 0}), (159284, {'train/accuracy': 0.8847265243530273, 'train/loss': 0.6685106754302979, 'validation/accuracy': 0.7770799994468689, 'validation/loss': 1.1113394498825073, 'validation/num_examples': 50000, 'test/accuracy': 0.6622000336647034, 'test/loss': 1.6857061386108398, 'test/num_examples': 10000, 'score': 72298.6612625122, 'total_duration': 78227.87518262863, 'accumulated_submission_time': 72298.6612625122, 'accumulated_eval_time': 5913.313067674637, 'accumulated_logging_time': 7.503507614135742, 'global_step': 159284, 'preemption_count': 0}), (160211, {'train/accuracy': 0.8788671493530273, 'train/loss': 0.681140124797821, 'validation/accuracy': 0.7768399715423584, 'validation/loss': 1.102988362312317, 'validation/num_examples': 50000, 'test/accuracy': 0.6640000343322754, 'test/loss': 1.670935869216919, 'test/num_examples': 10000, 'score': 72718.89306354523, 'total_duration': 78685.8155503273, 'accumulated_submission_time': 72718.89306354523, 'accumulated_eval_time': 5950.919641017914, 'accumulated_logging_time': 7.556835651397705, 'global_step': 160211, 'preemption_count': 0}), (161137, {'train/accuracy': 0.8810937404632568, 'train/loss': 0.6635440587997437, 'validation/accuracy': 0.7783399820327759, 'validation/loss': 1.0918289422988892, 'validation/num_examples': 50000, 'test/accuracy': 0.663800060749054, 'test/loss': 1.6655369997024536, 'test/num_examples': 10000, 'score': 73138.84251356125, 'total_duration': 79141.17946362495, 'accumulated_submission_time': 73138.84251356125, 'accumulated_eval_time': 5986.23615694046, 'accumulated_logging_time': 7.605699300765991, 'global_step': 161137, 'preemption_count': 0}), (162061, {'train/accuracy': 0.885546863079071, 'train/loss': 0.6447293162345886, 'validation/accuracy': 0.7795199751853943, 'validation/loss': 1.0828590393066406, 'validation/num_examples': 50000, 'test/accuracy': 0.6646000146865845, 'test/loss': 1.662644624710083, 'test/num_examples': 10000, 'score': 73558.78927731514, 'total_duration': 79598.76367640495, 'accumulated_submission_time': 73558.78927731514, 'accumulated_eval_time': 6023.772705554962, 'accumulated_logging_time': 7.6580424308776855, 'global_step': 162061, 'preemption_count': 0}), (162987, {'train/accuracy': 0.8820117115974426, 'train/loss': 0.656156063079834, 'validation/accuracy': 0.7808600068092346, 'validation/loss': 1.076492428779602, 'validation/num_examples': 50000, 'test/accuracy': 0.666700005531311, 'test/loss': 1.6550078392028809, 'test/num_examples': 10000, 'score': 73978.82945775986, 'total_duration': 80057.60511755943, 'accumulated_submission_time': 73978.82945775986, 'accumulated_eval_time': 6062.477011442184, 'accumulated_logging_time': 7.70618200302124, 'global_step': 162987, 'preemption_count': 0}), (163914, {'train/accuracy': 0.8850781321525574, 'train/loss': 0.6661824584007263, 'validation/accuracy': 0.780519962310791, 'validation/loss': 1.0905977487564087, 'validation/num_examples': 50000, 'test/accuracy': 0.6621000170707703, 'test/loss': 1.6676348447799683, 'test/num_examples': 10000, 'score': 74399.09590768814, 'total_duration': 80515.47159552574, 'accumulated_submission_time': 74399.09590768814, 'accumulated_eval_time': 6099.97262597084, 'accumulated_logging_time': 7.761868000030518, 'global_step': 163914, 'preemption_count': 0}), (164840, {'train/accuracy': 0.8875976204872131, 'train/loss': 0.6562318205833435, 'validation/accuracy': 0.7809399962425232, 'validation/loss': 1.0894443988800049, 'validation/num_examples': 50000, 'test/accuracy': 0.6657000184059143, 'test/loss': 1.671615719795227, 'test/num_examples': 10000, 'score': 74819.25520539284, 'total_duration': 80973.01725029945, 'accumulated_submission_time': 74819.25520539284, 'accumulated_eval_time': 6137.260036468506, 'accumulated_logging_time': 7.812313795089722, 'global_step': 164840, 'preemption_count': 0}), (165766, {'train/accuracy': 0.8836132884025574, 'train/loss': 0.6717751026153564, 'validation/accuracy': 0.7789999842643738, 'validation/loss': 1.0890315771102905, 'validation/num_examples': 50000, 'test/accuracy': 0.663800060749054, 'test/loss': 1.661457896232605, 'test/num_examples': 10000, 'score': 75239.2183611393, 'total_duration': 81428.55065131187, 'accumulated_submission_time': 75239.2183611393, 'accumulated_eval_time': 6172.728312015533, 'accumulated_logging_time': 7.865967750549316, 'global_step': 165766, 'preemption_count': 0}), (166692, {'train/accuracy': 0.8857030868530273, 'train/loss': 0.6525242328643799, 'validation/accuracy': 0.7821799516677856, 'validation/loss': 1.0815180540084839, 'validation/num_examples': 50000, 'test/accuracy': 0.6668000221252441, 'test/loss': 1.6559531688690186, 'test/num_examples': 10000, 'score': 75659.28298974037, 'total_duration': 81883.62391614914, 'accumulated_submission_time': 75659.28298974037, 'accumulated_eval_time': 6207.636021375656, 'accumulated_logging_time': 7.91825795173645, 'global_step': 166692, 'preemption_count': 0}), (167619, {'train/accuracy': 0.8876562118530273, 'train/loss': 0.6507769823074341, 'validation/accuracy': 0.7819199562072754, 'validation/loss': 1.0878478288650513, 'validation/num_examples': 50000, 'test/accuracy': 0.6665000319480896, 'test/loss': 1.6662715673446655, 'test/num_examples': 10000, 'score': 76079.30256128311, 'total_duration': 82338.82419419289, 'accumulated_submission_time': 76079.30256128311, 'accumulated_eval_time': 6242.713820695877, 'accumulated_logging_time': 7.97198224067688, 'global_step': 167619, 'preemption_count': 0}), (168545, {'train/accuracy': 0.8912890553474426, 'train/loss': 0.6282624006271362, 'validation/accuracy': 0.7827799916267395, 'validation/loss': 1.078118920326233, 'validation/num_examples': 50000, 'test/accuracy': 0.6678000092506409, 'test/loss': 1.6539418697357178, 'test/num_examples': 10000, 'score': 76499.28124332428, 'total_duration': 82792.69729566574, 'accumulated_submission_time': 76499.28124332428, 'accumulated_eval_time': 6276.506313562393, 'accumulated_logging_time': 8.024755239486694, 'global_step': 168545, 'preemption_count': 0}), (169472, {'train/accuracy': 0.8900976181030273, 'train/loss': 0.6365549564361572, 'validation/accuracy': 0.7829799652099609, 'validation/loss': 1.0776472091674805, 'validation/num_examples': 50000, 'test/accuracy': 0.6668000221252441, 'test/loss': 1.6524428129196167, 'test/num_examples': 10000, 'score': 76919.63348913193, 'total_duration': 83247.47095322609, 'accumulated_submission_time': 76919.63348913193, 'accumulated_eval_time': 6310.818851947784, 'accumulated_logging_time': 8.084598779678345, 'global_step': 169472, 'preemption_count': 0}), (170400, {'train/accuracy': 0.8862695097923279, 'train/loss': 0.6478437781333923, 'validation/accuracy': 0.7828199863433838, 'validation/loss': 1.084557294845581, 'validation/num_examples': 50000, 'test/accuracy': 0.6687000393867493, 'test/loss': 1.6623047590255737, 'test/num_examples': 10000, 'score': 77339.8290321827, 'total_duration': 83705.39258909225, 'accumulated_submission_time': 77339.8290321827, 'accumulated_eval_time': 6348.446104764938, 'accumulated_logging_time': 8.135056495666504, 'global_step': 170400, 'preemption_count': 0})], 'global_step': 170804}
I0201 12:12:03.335315 139863983413056 submission_runner.py:586] Timing: 77520.33187580109
I0201 12:12:03.335412 139863983413056 submission_runner.py:588] Total number of evals: 185
I0201 12:12:03.335458 139863983413056 submission_runner.py:589] ====================
I0201 12:12:03.335504 139863983413056 submission_runner.py:542] Using RNG seed 3682051175
I0201 12:12:03.337100 139863983413056 submission_runner.py:551] --- Tuning run 3/5 ---
I0201 12:12:03.337235 139863983413056 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_3.
I0201 12:12:03.338539 139863983413056 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_3/hparams.json.
I0201 12:12:03.339330 139863983413056 submission_runner.py:206] Initializing dataset.
I0201 12:12:03.348497 139863983413056 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0201 12:12:03.364678 139863983413056 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0201 12:12:03.558191 139863983413056 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0201 12:12:07.755394 139863983413056 submission_runner.py:213] Initializing model.
I0201 12:12:14.027011 139863983413056 submission_runner.py:255] Initializing optimizer.
I0201 12:12:14.558250 139863983413056 submission_runner.py:262] Initializing metrics bundle.
I0201 12:12:14.558411 139863983413056 submission_runner.py:280] Initializing checkpoint and logger.
I0201 12:12:14.572464 139863983413056 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_3 with prefix checkpoint_
I0201 12:12:14.572587 139863983413056 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0201 12:12:30.012506 139863983413056 logger_utils.py:220] Unable to record git information. Continuing without it.
I0201 12:12:45.207664 139863983413056 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_3/flags_0.json.
I0201 12:12:45.212418 139863983413056 submission_runner.py:314] Starting training loop.
I0201 12:13:21.398175 139702501852928 logging_writer.py:48] [0] global_step=0, grad_norm=0.3730001747608185, loss=6.907756328582764
I0201 12:13:21.410736 139863983413056 spec.py:321] Evaluating on the training split.
I0201 12:13:29.672888 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 12:13:47.105151 139863983413056 spec.py:349] Evaluating on the test split.
I0201 12:13:48.741208 139863983413056 submission_runner.py:408] Time since start: 63.53s, 	Step: 1, 	{'train/accuracy': 0.0011328124674037099, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.198220014572144, 'total_duration': 63.52872085571289, 'accumulated_submission_time': 36.198220014572144, 'accumulated_eval_time': 27.33039879798889, 'accumulated_logging_time': 0}
I0201 12:13:48.751358 139702510245632 logging_writer.py:48] [1] accumulated_eval_time=27.330399, accumulated_logging_time=0, accumulated_submission_time=36.198220, global_step=1, preemption_count=0, score=36.198220, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=63.528721, train/accuracy=0.001133, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0201 12:14:57.275819 139702543816448 logging_writer.py:48] [100] global_step=100, grad_norm=0.42597833275794983, loss=6.905826568603516
I0201 12:15:42.874268 139702527031040 logging_writer.py:48] [200] global_step=200, grad_norm=0.48652443289756775, loss=6.8911590576171875
I0201 12:16:29.467450 139702543816448 logging_writer.py:48] [300] global_step=300, grad_norm=0.5867069363594055, loss=6.847630500793457
I0201 12:17:16.278041 139702527031040 logging_writer.py:48] [400] global_step=400, grad_norm=0.7113400101661682, loss=6.806668281555176
I0201 12:18:02.500794 139702543816448 logging_writer.py:48] [500] global_step=500, grad_norm=0.7483394742012024, loss=6.841069221496582
I0201 12:18:48.920276 139702527031040 logging_writer.py:48] [600] global_step=600, grad_norm=0.8830324411392212, loss=6.720629692077637
I0201 12:19:35.120118 139702543816448 logging_writer.py:48] [700] global_step=700, grad_norm=1.27978515625, loss=6.653428077697754
I0201 12:20:21.627761 139702527031040 logging_writer.py:48] [800] global_step=800, grad_norm=1.8315693140029907, loss=6.628334999084473
I0201 12:20:49.109711 139863983413056 spec.py:321] Evaluating on the training split.
I0201 12:21:00.034823 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 12:21:24.383507 139863983413056 spec.py:349] Evaluating on the test split.
I0201 12:21:26.021166 139863983413056 submission_runner.py:408] Time since start: 520.81s, 	Step: 861, 	{'train/accuracy': 0.0126953125, 'train/loss': 6.42455530166626, 'validation/accuracy': 0.014179999940097332, 'validation/loss': 6.434906482696533, 'validation/num_examples': 50000, 'test/accuracy': 0.011200000531971455, 'test/loss': 6.476788520812988, 'test/num_examples': 10000, 'score': 456.5025894641876, 'total_duration': 520.8086996078491, 'accumulated_submission_time': 456.5025894641876, 'accumulated_eval_time': 64.24186062812805, 'accumulated_logging_time': 0.019777774810791016}
I0201 12:21:26.042237 139702543816448 logging_writer.py:48] [861] accumulated_eval_time=64.241861, accumulated_logging_time=0.019778, accumulated_submission_time=456.502589, global_step=861, preemption_count=0, score=456.502589, test/accuracy=0.011200, test/loss=6.476789, test/num_examples=10000, total_duration=520.808700, train/accuracy=0.012695, train/loss=6.424555, validation/accuracy=0.014180, validation/loss=6.434906, validation/num_examples=50000
I0201 12:21:42.002774 139702527031040 logging_writer.py:48] [900] global_step=900, grad_norm=1.143245816230774, loss=6.505919933319092
I0201 12:22:25.432240 139702543816448 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.326413631439209, loss=6.477494716644287
I0201 12:23:11.617026 139702527031040 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4018160104751587, loss=6.572812557220459
I0201 12:23:57.644507 139702543816448 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.7003480195999146, loss=6.433330535888672
I0201 12:24:44.211474 139702527031040 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.518916606903076, loss=6.356621742248535
I0201 12:25:30.202753 139702543816448 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.075230598449707, loss=6.666839122772217
I0201 12:26:16.598407 139702527031040 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.5719352960586548, loss=6.614465713500977
I0201 12:27:02.669557 139702543816448 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.7858338356018066, loss=6.119983673095703
I0201 12:27:48.535653 139702527031040 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.5524312257766724, loss=6.339648246765137
I0201 12:28:26.379451 139863983413056 spec.py:321] Evaluating on the training split.
I0201 12:28:37.180806 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 12:28:59.623890 139863983413056 spec.py:349] Evaluating on the test split.
I0201 12:29:01.263132 139863983413056 submission_runner.py:408] Time since start: 976.05s, 	Step: 1784, 	{'train/accuracy': 0.03832031041383743, 'train/loss': 5.833907127380371, 'validation/accuracy': 0.035019997507333755, 'validation/loss': 5.863661289215088, 'validation/num_examples': 50000, 'test/accuracy': 0.027400001883506775, 'test/loss': 5.9866838455200195, 'test/num_examples': 10000, 'score': 876.7815659046173, 'total_duration': 976.0506365299225, 'accumulated_submission_time': 876.7815659046173, 'accumulated_eval_time': 99.12552428245544, 'accumulated_logging_time': 0.050582170486450195}
I0201 12:29:01.279635 139702543816448 logging_writer.py:48] [1784] accumulated_eval_time=99.125524, accumulated_logging_time=0.050582, accumulated_submission_time=876.781566, global_step=1784, preemption_count=0, score=876.781566, test/accuracy=0.027400, test/loss=5.986684, test/num_examples=10000, total_duration=976.050637, train/accuracy=0.038320, train/loss=5.833907, validation/accuracy=0.035020, validation/loss=5.863661, validation/num_examples=50000
I0201 12:29:08.058372 139702527031040 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.3561606407165527, loss=6.478886604309082
I0201 12:29:50.280089 139702543816448 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.921940565109253, loss=6.04310941696167
I0201 12:30:36.181859 139702527031040 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.639317274093628, loss=6.024866580963135
I0201 12:31:22.346041 139702543816448 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.9970800876617432, loss=6.0568671226501465
I0201 12:32:08.206425 139702527031040 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.4138344526290894, loss=6.662714958190918
I0201 12:32:54.429635 139702543816448 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.9507396221160889, loss=5.932197093963623
I0201 12:33:40.593730 139702527031040 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.436392068862915, loss=5.868715286254883
I0201 12:34:26.716069 139702543816448 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.7913703918457031, loss=6.318092346191406
I0201 12:35:12.618584 139702527031040 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.9546445608139038, loss=5.866639137268066
I0201 12:35:58.599373 139702543816448 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.988426685333252, loss=6.478633880615234
I0201 12:36:01.495713 139863983413056 spec.py:321] Evaluating on the training split.
I0201 12:36:12.008663 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 12:36:31.089670 139863983413056 spec.py:349] Evaluating on the test split.
I0201 12:36:32.742595 139863983413056 submission_runner.py:408] Time since start: 1427.53s, 	Step: 2708, 	{'train/accuracy': 0.06650390475988388, 'train/loss': 5.404107093811035, 'validation/accuracy': 0.06127999722957611, 'validation/loss': 5.447628498077393, 'validation/num_examples': 50000, 'test/accuracy': 0.04920000210404396, 'test/loss': 5.628585338592529, 'test/num_examples': 10000, 'score': 1296.9359059333801, 'total_duration': 1427.5301036834717, 'accumulated_submission_time': 1296.9359059333801, 'accumulated_eval_time': 130.37237691879272, 'accumulated_logging_time': 0.08036112785339355}
I0201 12:36:32.758907 139702527031040 logging_writer.py:48] [2708] accumulated_eval_time=130.372377, accumulated_logging_time=0.080361, accumulated_submission_time=1296.935906, global_step=2708, preemption_count=0, score=1296.935906, test/accuracy=0.049200, test/loss=5.628585, test/num_examples=10000, total_duration=1427.530104, train/accuracy=0.066504, train/loss=5.404107, validation/accuracy=0.061280, validation/loss=5.447628, validation/num_examples=50000
I0201 12:37:11.226004 139702543816448 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.9222412109375, loss=6.200257301330566
I0201 12:37:57.218020 139702527031040 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.3847506046295166, loss=5.756564617156982
I0201 12:38:43.077311 139702543816448 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.7999231815338135, loss=5.736381530761719
I0201 12:39:29.164942 139702527031040 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.6850018501281738, loss=5.786929607391357
I0201 12:40:15.527855 139702543816448 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.664171814918518, loss=6.315087795257568
I0201 12:41:01.665889 139702527031040 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.008206844329834, loss=5.602178573608398
I0201 12:41:47.676230 139702543816448 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.9390980005264282, loss=5.7002716064453125
I0201 12:42:33.906192 139702527031040 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.3232309818267822, loss=5.821995258331299
I0201 12:43:20.313091 139702543816448 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.997231364250183, loss=5.562823295593262
I0201 12:43:32.773499 139863983413056 spec.py:321] Evaluating on the training split.
I0201 12:43:44.292724 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 12:44:08.725295 139863983413056 spec.py:349] Evaluating on the test split.
I0201 12:44:10.369740 139863983413056 submission_runner.py:408] Time since start: 1885.16s, 	Step: 3629, 	{'train/accuracy': 0.09648437052965164, 'train/loss': 5.060458660125732, 'validation/accuracy': 0.089819997549057, 'validation/loss': 5.1000800132751465, 'validation/num_examples': 50000, 'test/accuracy': 0.06850000470876694, 'test/loss': 5.34055233001709, 'test/num_examples': 10000, 'score': 1716.8910706043243, 'total_duration': 1885.1572682857513, 'accumulated_submission_time': 1716.8910706043243, 'accumulated_eval_time': 167.9686098098755, 'accumulated_logging_time': 0.10755348205566406}
I0201 12:44:10.384684 139702527031040 logging_writer.py:48] [3629] accumulated_eval_time=167.968610, accumulated_logging_time=0.107553, accumulated_submission_time=1716.891071, global_step=3629, preemption_count=0, score=1716.891071, test/accuracy=0.068500, test/loss=5.340552, test/num_examples=10000, total_duration=1885.157268, train/accuracy=0.096484, train/loss=5.060459, validation/accuracy=0.089820, validation/loss=5.100080, validation/num_examples=50000
I0201 12:44:39.118895 139702543816448 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.8179844617843628, loss=5.553959369659424
I0201 12:45:24.849503 139702527031040 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.801101565361023, loss=5.504227638244629
I0201 12:46:10.961727 139702543816448 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.8146698474884033, loss=5.512336730957031
I0201 12:46:57.186437 139702527031040 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.0604209899902344, loss=6.04647970199585
I0201 12:47:43.480435 139702543816448 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.9223084449768066, loss=5.917953968048096
I0201 12:48:29.602609 139702527031040 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.2829738855361938, loss=6.08131217956543
I0201 12:49:15.633679 139702543816448 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.7545685768127441, loss=5.736080646514893
I0201 12:50:01.827495 139702527031040 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.714334487915039, loss=6.459114074707031
I0201 12:50:47.863537 139702543816448 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.7890945672988892, loss=5.214183807373047
I0201 12:51:10.449569 139863983413056 spec.py:321] Evaluating on the training split.
I0201 12:51:20.939379 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 12:51:44.934593 139863983413056 spec.py:349] Evaluating on the test split.
I0201 12:51:46.581920 139863983413056 submission_runner.py:408] Time since start: 2341.37s, 	Step: 4551, 	{'train/accuracy': 0.13283203542232513, 'train/loss': 4.688483715057373, 'validation/accuracy': 0.12459999322891235, 'validation/loss': 4.738650321960449, 'validation/num_examples': 50000, 'test/accuracy': 0.09270000457763672, 'test/loss': 5.04966402053833, 'test/num_examples': 10000, 'score': 2136.898129463196, 'total_duration': 2341.369452238083, 'accumulated_submission_time': 2136.898129463196, 'accumulated_eval_time': 204.10096502304077, 'accumulated_logging_time': 0.13211870193481445}
I0201 12:51:46.596318 139702527031040 logging_writer.py:48] [4551] accumulated_eval_time=204.100965, accumulated_logging_time=0.132119, accumulated_submission_time=2136.898129, global_step=4551, preemption_count=0, score=2136.898129, test/accuracy=0.092700, test/loss=5.049664, test/num_examples=10000, total_duration=2341.369452, train/accuracy=0.132832, train/loss=4.688484, validation/accuracy=0.124600, validation/loss=4.738650, validation/num_examples=50000
I0201 12:52:06.561575 139702543816448 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.550036907196045, loss=6.307428359985352
I0201 12:52:50.691871 139702527031040 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.4556094408035278, loss=6.161683082580566
I0201 12:53:36.972226 139702543816448 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.8018219470977783, loss=5.380245208740234
I0201 12:54:23.177258 139702527031040 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.346120834350586, loss=5.212430953979492
I0201 12:55:09.410525 139702543816448 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.7890658378601074, loss=5.059898853302002
I0201 12:55:56.232541 139702527031040 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.0211236476898193, loss=4.953685283660889
I0201 12:56:42.478494 139702543816448 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.9557839632034302, loss=4.970442295074463
I0201 12:57:28.717232 139702527031040 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.9382652044296265, loss=5.239869594573975
I0201 12:58:14.997807 139702543816448 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.6802046298980713, loss=4.953936576843262
I0201 12:58:46.993006 139863983413056 spec.py:321] Evaluating on the training split.
I0201 12:58:57.638694 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 12:59:22.369327 139863983413056 spec.py:349] Evaluating on the test split.
I0201 12:59:24.023484 139863983413056 submission_runner.py:408] Time since start: 2798.81s, 	Step: 5471, 	{'train/accuracy': 0.17716796696186066, 'train/loss': 4.302088737487793, 'validation/accuracy': 0.16154000163078308, 'validation/loss': 4.381013870239258, 'validation/num_examples': 50000, 'test/accuracy': 0.12320000678300858, 'test/loss': 4.732566833496094, 'test/num_examples': 10000, 'score': 2557.2378079891205, 'total_duration': 2798.8110077381134, 'accumulated_submission_time': 2557.2378079891205, 'accumulated_eval_time': 241.13143801689148, 'accumulated_logging_time': 0.15525579452514648}
I0201 12:59:24.038727 139702527031040 logging_writer.py:48] [5471] accumulated_eval_time=241.131438, accumulated_logging_time=0.155256, accumulated_submission_time=2557.237808, global_step=5471, preemption_count=0, score=2557.237808, test/accuracy=0.123200, test/loss=4.732567, test/num_examples=10000, total_duration=2798.811008, train/accuracy=0.177168, train/loss=4.302089, validation/accuracy=0.161540, validation/loss=4.381014, validation/num_examples=50000
I0201 12:59:36.012898 139702543816448 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.7888177633285522, loss=5.094569206237793
I0201 13:00:19.431916 139702527031040 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.8045130968093872, loss=4.988819599151611
I0201 13:01:05.239691 139702543816448 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.6044209003448486, loss=6.055097579956055
I0201 13:01:51.378002 139702527031040 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.8823440074920654, loss=4.8557233810424805
I0201 13:02:37.461737 139702543816448 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.7885384559631348, loss=5.391362190246582
I0201 13:03:23.486924 139702527031040 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.831038475036621, loss=4.882784843444824
I0201 13:04:09.612653 139702543816448 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.8634506464004517, loss=4.7302680015563965
I0201 13:04:55.604908 139702527031040 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.5932129621505737, loss=6.215350151062012
I0201 13:05:41.937321 139702543816448 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.056370973587036, loss=4.684145927429199
I0201 13:06:24.325759 139863983413056 spec.py:321] Evaluating on the training split.
I0201 13:06:34.691575 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 13:07:01.726515 139863983413056 spec.py:349] Evaluating on the test split.
I0201 13:07:03.365163 139863983413056 submission_runner.py:408] Time since start: 3258.15s, 	Step: 6394, 	{'train/accuracy': 0.22121092677116394, 'train/loss': 3.9720683097839355, 'validation/accuracy': 0.19923999905586243, 'validation/loss': 4.102482318878174, 'validation/num_examples': 50000, 'test/accuracy': 0.14920000731945038, 'test/loss': 4.503682613372803, 'test/num_examples': 10000, 'score': 2977.4658873081207, 'total_duration': 3258.1526939868927, 'accumulated_submission_time': 2977.4658873081207, 'accumulated_eval_time': 280.170841217041, 'accumulated_logging_time': 0.18082594871520996}
I0201 13:07:03.380011 139702527031040 logging_writer.py:48] [6394] accumulated_eval_time=280.170841, accumulated_logging_time=0.180826, accumulated_submission_time=2977.465887, global_step=6394, preemption_count=0, score=2977.465887, test/accuracy=0.149200, test/loss=4.503683, test/num_examples=10000, total_duration=3258.152694, train/accuracy=0.221211, train/loss=3.972068, validation/accuracy=0.199240, validation/loss=4.102482, validation/num_examples=50000
I0201 13:07:06.189264 139702543816448 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.3033196926116943, loss=4.641814231872559
I0201 13:07:47.829284 139702527031040 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.6357593536376953, loss=4.986795425415039
I0201 13:08:33.884955 139702543816448 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.898472785949707, loss=5.070291996002197
I0201 13:09:20.140967 139702527031040 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.726277470588684, loss=6.11394739151001
I0201 13:10:06.425742 139702543816448 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.7782893180847168, loss=4.5090532302856445
I0201 13:10:52.377798 139702527031040 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.8531540632247925, loss=4.791421890258789
I0201 13:11:38.538039 139702543816448 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.7937051057815552, loss=4.4754438400268555
I0201 13:12:24.571382 139702527031040 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.9081416130065918, loss=6.149421691894531
I0201 13:13:10.637219 139702543816448 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.5849790573120117, loss=5.4391303062438965
I0201 13:13:56.593644 139702527031040 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.1540653705596924, loss=6.191502094268799
I0201 13:14:03.713577 139863983413056 spec.py:321] Evaluating on the training split.
I0201 13:14:14.486397 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 13:14:38.152283 139863983413056 spec.py:349] Evaluating on the test split.
I0201 13:14:39.792417 139863983413056 submission_runner.py:408] Time since start: 3714.58s, 	Step: 7317, 	{'train/accuracy': 0.2605859339237213, 'train/loss': 3.638662576675415, 'validation/accuracy': 0.24587999284267426, 'validation/loss': 3.7360968589782715, 'validation/num_examples': 50000, 'test/accuracy': 0.18530000746250153, 'test/loss': 4.179815769195557, 'test/num_examples': 10000, 'score': 3397.741530895233, 'total_duration': 3714.579946756363, 'accumulated_submission_time': 3397.741530895233, 'accumulated_eval_time': 316.2496666908264, 'accumulated_logging_time': 0.205230712890625}
I0201 13:14:39.807892 139702543816448 logging_writer.py:48] [7317] accumulated_eval_time=316.249667, accumulated_logging_time=0.205231, accumulated_submission_time=3397.741531, global_step=7317, preemption_count=0, score=3397.741531, test/accuracy=0.185300, test/loss=4.179816, test/num_examples=10000, total_duration=3714.579947, train/accuracy=0.260586, train/loss=3.638663, validation/accuracy=0.245880, validation/loss=3.736097, validation/num_examples=50000
I0201 13:15:13.462138 139702527031040 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.4600131511688232, loss=6.002787113189697
I0201 13:15:59.546286 139702543816448 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.6733479499816895, loss=5.916168212890625
I0201 13:16:45.674733 139702527031040 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.7499688863754272, loss=4.648464679718018
I0201 13:17:31.833499 139702543816448 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.7727378606796265, loss=4.646594047546387
I0201 13:18:17.841205 139702527031040 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.45480215549469, loss=6.19253396987915
I0201 13:19:03.914532 139702543816448 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.8030996322631836, loss=4.804637908935547
I0201 13:19:50.082521 139702527031040 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.626402497291565, loss=4.33705997467041
I0201 13:20:36.315150 139702543816448 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.930254578590393, loss=4.344307899475098
I0201 13:21:22.614167 139702527031040 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.9549801349639893, loss=4.437388896942139
I0201 13:21:40.012985 139863983413056 spec.py:321] Evaluating on the training split.
I0201 13:21:50.534803 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 13:22:14.958491 139863983413056 spec.py:349] Evaluating on the test split.
I0201 13:22:16.591385 139863983413056 submission_runner.py:408] Time since start: 4171.38s, 	Step: 8240, 	{'train/accuracy': 0.2956250011920929, 'train/loss': 3.4536590576171875, 'validation/accuracy': 0.27459999918937683, 'validation/loss': 3.5595672130584717, 'validation/num_examples': 50000, 'test/accuracy': 0.21070000529289246, 'test/loss': 4.018807411193848, 'test/num_examples': 10000, 'score': 3817.889068841934, 'total_duration': 4171.378915309906, 'accumulated_submission_time': 3817.889068841934, 'accumulated_eval_time': 352.8280653953552, 'accumulated_logging_time': 0.2301778793334961}
I0201 13:22:16.608362 139702543816448 logging_writer.py:48] [8240] accumulated_eval_time=352.828065, accumulated_logging_time=0.230178, accumulated_submission_time=3817.889069, global_step=8240, preemption_count=0, score=3817.889069, test/accuracy=0.210700, test/loss=4.018807, test/num_examples=10000, total_duration=4171.378915, train/accuracy=0.295625, train/loss=3.453659, validation/accuracy=0.274600, validation/loss=3.559567, validation/num_examples=50000
I0201 13:22:40.986809 139702527031040 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.736444354057312, loss=5.525643825531006
I0201 13:23:25.826455 139702543816448 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.098008394241333, loss=4.355989456176758
I0201 13:24:11.737195 139702527031040 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.8909012079238892, loss=4.660821437835693
I0201 13:24:57.910579 139702543816448 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.9261348247528076, loss=4.107975959777832
I0201 13:25:43.730769 139702527031040 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.0196378231048584, loss=4.245428085327148
I0201 13:26:29.929514 139702543816448 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.7549662590026855, loss=4.158097743988037
I0201 13:27:15.925233 139702527031040 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.868177890777588, loss=4.040077209472656
I0201 13:28:01.994524 139702543816448 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.8968499898910522, loss=4.1056952476501465
I0201 13:28:47.945070 139702527031040 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.6331666707992554, loss=4.016891956329346
I0201 13:29:16.971984 139863983413056 spec.py:321] Evaluating on the training split.
I0201 13:29:27.579696 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 13:29:53.084095 139863983413056 spec.py:349] Evaluating on the test split.
I0201 13:29:54.720162 139863983413056 submission_runner.py:408] Time since start: 4629.51s, 	Step: 9165, 	{'train/accuracy': 0.3382031321525574, 'train/loss': 3.15322208404541, 'validation/accuracy': 0.3057200014591217, 'validation/loss': 3.3277082443237305, 'validation/num_examples': 50000, 'test/accuracy': 0.2387000173330307, 'test/loss': 3.8212127685546875, 'test/num_examples': 10000, 'score': 4238.194683074951, 'total_duration': 4629.507674217224, 'accumulated_submission_time': 4238.194683074951, 'accumulated_eval_time': 390.5762298107147, 'accumulated_logging_time': 0.2563011646270752}
I0201 13:29:54.738181 139702543816448 logging_writer.py:48] [9165] accumulated_eval_time=390.576230, accumulated_logging_time=0.256301, accumulated_submission_time=4238.194683, global_step=9165, preemption_count=0, score=4238.194683, test/accuracy=0.238700, test/loss=3.821213, test/num_examples=10000, total_duration=4629.507674, train/accuracy=0.338203, train/loss=3.153222, validation/accuracy=0.305720, validation/loss=3.327708, validation/num_examples=50000
I0201 13:30:09.119280 139702527031040 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.6459957361221313, loss=4.102603912353516
I0201 13:30:52.508955 139702543816448 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.0934994220733643, loss=4.011582374572754
I0201 13:31:38.580488 139702527031040 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.021324634552002, loss=4.001642227172852
I0201 13:32:24.615701 139702543816448 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.6250215768814087, loss=4.834798336029053
I0201 13:33:10.520532 139702527031040 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.445777416229248, loss=5.069528579711914
I0201 13:33:56.268682 139702543816448 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.8549728393554688, loss=4.188676357269287
I0201 13:34:42.362333 139702527031040 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.4225621223449707, loss=5.445147514343262
I0201 13:35:28.159895 139702543816448 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.9178866147994995, loss=3.9132235050201416
I0201 13:36:14.595895 139702527031040 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.3511090278625488, loss=6.012540340423584
I0201 13:36:55.002183 139863983413056 spec.py:321] Evaluating on the training split.
I0201 13:37:05.700032 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 13:37:28.675303 139863983413056 spec.py:349] Evaluating on the test split.
I0201 13:37:30.315736 139863983413056 submission_runner.py:408] Time since start: 5085.10s, 	Step: 10090, 	{'train/accuracy': 0.3600195348262787, 'train/loss': 3.029609203338623, 'validation/accuracy': 0.3360999822616577, 'validation/loss': 3.154364824295044, 'validation/num_examples': 50000, 'test/accuracy': 0.2547000050544739, 'test/loss': 3.6767055988311768, 'test/num_examples': 10000, 'score': 4658.40030002594, 'total_duration': 5085.103245258331, 'accumulated_submission_time': 4658.40030002594, 'accumulated_eval_time': 425.8897521495819, 'accumulated_logging_time': 0.2844047546386719}
I0201 13:37:30.339969 139702543816448 logging_writer.py:48] [10090] accumulated_eval_time=425.889752, accumulated_logging_time=0.284405, accumulated_submission_time=4658.400300, global_step=10090, preemption_count=0, score=4658.400300, test/accuracy=0.254700, test/loss=3.676706, test/num_examples=10000, total_duration=5085.103245, train/accuracy=0.360020, train/loss=3.029609, validation/accuracy=0.336100, validation/loss=3.154365, validation/num_examples=50000
I0201 13:37:34.739027 139702527031040 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.2737653255462646, loss=5.454110622406006
I0201 13:38:16.478441 139702543816448 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.7679502964019775, loss=4.187911033630371
I0201 13:39:02.448149 139702527031040 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.2473125457763672, loss=5.397934913635254
I0201 13:39:48.973932 139702543816448 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.3790884017944336, loss=4.79221248626709
I0201 13:40:35.244329 139702527031040 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.7248992919921875, loss=4.089899063110352
I0201 13:41:21.185530 139702543816448 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.8133392333984375, loss=3.866657257080078
I0201 13:42:07.268559 139702527031040 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.8492883443832397, loss=3.776972532272339
I0201 13:42:53.416550 139702543816448 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.4210618734359741, loss=5.852882385253906
I0201 13:43:39.466993 139702527031040 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.5805113315582275, loss=3.7789664268493652
I0201 13:44:25.666313 139702543816448 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.9450583457946777, loss=3.8324012756347656
I0201 13:44:30.356630 139863983413056 spec.py:321] Evaluating on the training split.
I0201 13:44:40.927931 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 13:45:00.702447 139863983413056 spec.py:349] Evaluating on the test split.
I0201 13:45:02.355202 139863983413056 submission_runner.py:408] Time since start: 5537.14s, 	Step: 11012, 	{'train/accuracy': 0.38132810592651367, 'train/loss': 2.868154764175415, 'validation/accuracy': 0.35593998432159424, 'validation/loss': 3.013925552368164, 'validation/num_examples': 50000, 'test/accuracy': 0.27800002694129944, 'test/loss': 3.559499740600586, 'test/num_examples': 10000, 'score': 5078.3580095767975, 'total_duration': 5537.1427166461945, 'accumulated_submission_time': 5078.3580095767975, 'accumulated_eval_time': 457.88829278945923, 'accumulated_logging_time': 0.3195030689239502}
I0201 13:45:02.372738 139702527031040 logging_writer.py:48] [11012] accumulated_eval_time=457.888293, accumulated_logging_time=0.319503, accumulated_submission_time=5078.358010, global_step=11012, preemption_count=0, score=5078.358010, test/accuracy=0.278000, test/loss=3.559500, test/num_examples=10000, total_duration=5537.142717, train/accuracy=0.381328, train/loss=2.868155, validation/accuracy=0.355940, validation/loss=3.013926, validation/num_examples=50000
I0201 13:45:39.072658 139702543816448 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.5880663394927979, loss=5.067218780517578
I0201 13:46:24.836947 139702527031040 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.912438988685608, loss=3.67344331741333
I0201 13:47:11.385924 139702543816448 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.4635009765625, loss=5.462715148925781
I0201 13:47:57.625877 139702527031040 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.753800630569458, loss=3.9291508197784424
I0201 13:48:43.587110 139702543816448 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.1797586679458618, loss=5.532253742218018
I0201 13:49:29.877438 139702527031040 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.6807448863983154, loss=4.591633319854736
I0201 13:50:16.150668 139702543816448 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.5401215553283691, loss=4.324398040771484
I0201 13:51:01.986627 139702527031040 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.806317687034607, loss=3.6727523803710938
I0201 13:51:48.197080 139702543816448 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.7486870288848877, loss=3.572131395339966
I0201 13:52:02.596665 139863983413056 spec.py:321] Evaluating on the training split.
I0201 13:52:13.025471 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 13:52:38.463799 139863983413056 spec.py:349] Evaluating on the test split.
I0201 13:52:40.109801 139863983413056 submission_runner.py:408] Time since start: 5994.90s, 	Step: 11933, 	{'train/accuracy': 0.40974608063697815, 'train/loss': 2.723881721496582, 'validation/accuracy': 0.37139999866485596, 'validation/loss': 2.902348041534424, 'validation/num_examples': 50000, 'test/accuracy': 0.2905000150203705, 'test/loss': 3.4545247554779053, 'test/num_examples': 10000, 'score': 5498.060791969299, 'total_duration': 5994.897334814072, 'accumulated_submission_time': 5498.060791969299, 'accumulated_eval_time': 495.40143156051636, 'accumulated_logging_time': 0.8100032806396484}
I0201 13:52:40.125931 139702527031040 logging_writer.py:48] [11933] accumulated_eval_time=495.401432, accumulated_logging_time=0.810003, accumulated_submission_time=5498.060792, global_step=11933, preemption_count=0, score=5498.060792, test/accuracy=0.290500, test/loss=3.454525, test/num_examples=10000, total_duration=5994.897335, train/accuracy=0.409746, train/loss=2.723882, validation/accuracy=0.371400, validation/loss=2.902348, validation/num_examples=50000
I0201 13:53:07.281492 139702543816448 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4882482290267944, loss=4.357831001281738
I0201 13:53:52.803959 139702527031040 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.9399594068527222, loss=3.5737407207489014
I0201 13:54:38.956908 139702543816448 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.610260248184204, loss=3.877889633178711
I0201 13:55:25.378391 139702527031040 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.9849244356155396, loss=3.695521831512451
I0201 13:56:11.349527 139702543816448 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.7724941968917847, loss=3.799849271774292
I0201 13:56:57.650052 139702527031040 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.5651346445083618, loss=4.086397647857666
I0201 13:57:43.616310 139702543816448 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.8412004709243774, loss=3.6058051586151123
I0201 13:58:29.466032 139702527031040 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.9544391632080078, loss=3.6113593578338623
I0201 13:59:15.734912 139702543816448 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.044354200363159, loss=3.5639548301696777
I0201 13:59:40.251470 139863983413056 spec.py:321] Evaluating on the training split.
I0201 13:59:50.983039 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 14:00:16.660597 139863983413056 spec.py:349] Evaluating on the test split.
I0201 14:00:18.302071 139863983413056 submission_runner.py:408] Time since start: 6453.09s, 	Step: 12855, 	{'train/accuracy': 0.41914060711860657, 'train/loss': 2.710547924041748, 'validation/accuracy': 0.3916800022125244, 'validation/loss': 2.8580996990203857, 'validation/num_examples': 50000, 'test/accuracy': 0.30080002546310425, 'test/loss': 3.418243408203125, 'test/num_examples': 10000, 'score': 5918.129729747772, 'total_duration': 6453.089605808258, 'accumulated_submission_time': 5918.129729747772, 'accumulated_eval_time': 533.4520351886749, 'accumulated_logging_time': 0.8349728584289551}
I0201 14:00:18.318635 139702527031040 logging_writer.py:48] [12855] accumulated_eval_time=533.452035, accumulated_logging_time=0.834973, accumulated_submission_time=5918.129730, global_step=12855, preemption_count=0, score=5918.129730, test/accuracy=0.300800, test/loss=3.418243, test/num_examples=10000, total_duration=6453.089606, train/accuracy=0.419141, train/loss=2.710548, validation/accuracy=0.391680, validation/loss=2.858100, validation/num_examples=50000
I0201 14:00:36.683627 139702543816448 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.750937819480896, loss=3.673644542694092
I0201 14:01:20.657240 139702527031040 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.8469598293304443, loss=4.111449718475342
I0201 14:02:06.600886 139702543816448 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.5370184183120728, loss=3.9912168979644775
I0201 14:02:52.771816 139702527031040 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.6356151103973389, loss=3.503678321838379
I0201 14:03:38.772462 139702543816448 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.7451176643371582, loss=3.7054836750030518
I0201 14:04:24.875626 139702527031040 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.2050917148590088, loss=5.236294746398926
I0201 14:05:10.829839 139702543816448 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3602917194366455, loss=5.158098220825195
I0201 14:05:56.605343 139702527031040 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.8532439470291138, loss=3.617290496826172
I0201 14:06:42.520333 139702543816448 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.9070017337799072, loss=3.6452412605285645
I0201 14:07:18.618823 139863983413056 spec.py:321] Evaluating on the training split.
I0201 14:07:29.125547 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 14:07:54.186786 139863983413056 spec.py:349] Evaluating on the test split.
I0201 14:07:55.818387 139863983413056 submission_runner.py:408] Time since start: 6910.61s, 	Step: 13780, 	{'train/accuracy': 0.4387499988079071, 'train/loss': 2.571505308151245, 'validation/accuracy': 0.4053399860858917, 'validation/loss': 2.7366902828216553, 'validation/num_examples': 50000, 'test/accuracy': 0.31040000915527344, 'test/loss': 3.3097615242004395, 'test/num_examples': 10000, 'score': 6338.371341228485, 'total_duration': 6910.605922698975, 'accumulated_submission_time': 6338.371341228485, 'accumulated_eval_time': 570.6516087055206, 'accumulated_logging_time': 0.8618414402008057}
I0201 14:07:55.834221 139702527031040 logging_writer.py:48] [13780] accumulated_eval_time=570.651609, accumulated_logging_time=0.861841, accumulated_submission_time=6338.371341, global_step=13780, preemption_count=0, score=6338.371341, test/accuracy=0.310400, test/loss=3.309762, test/num_examples=10000, total_duration=6910.605923, train/accuracy=0.438750, train/loss=2.571505, validation/accuracy=0.405340, validation/loss=2.736690, validation/num_examples=50000
I0201 14:08:04.209544 139702543816448 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.058156967163086, loss=5.758580207824707
I0201 14:08:46.363591 139702527031040 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.2992404699325562, loss=4.464515686035156
I0201 14:09:32.349267 139702543816448 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.635587215423584, loss=3.6520891189575195
I0201 14:10:18.556517 139702527031040 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.8251475095748901, loss=3.4291789531707764
I0201 14:11:04.422038 139702543816448 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.6334974765777588, loss=3.9936137199401855
I0201 14:11:50.414581 139702527031040 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.7437678575515747, loss=3.5373001098632812
I0201 14:12:36.358750 139702543816448 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.6586443185806274, loss=4.508067607879639
I0201 14:13:22.231199 139702527031040 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.656090259552002, loss=3.512636423110962
I0201 14:14:08.107375 139702543816448 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.7032756805419922, loss=4.4864068031311035
I0201 14:14:54.158499 139702527031040 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.117546796798706, loss=5.849221229553223
I0201 14:14:56.157331 139863983413056 spec.py:321] Evaluating on the training split.
I0201 14:15:06.673370 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 14:15:29.200814 139863983413056 spec.py:349] Evaluating on the test split.
I0201 14:15:30.838602 139863983413056 submission_runner.py:408] Time since start: 7365.63s, 	Step: 14706, 	{'train/accuracy': 0.4510742127895355, 'train/loss': 2.485666036605835, 'validation/accuracy': 0.41613999009132385, 'validation/loss': 2.670121192932129, 'validation/num_examples': 50000, 'test/accuracy': 0.3216000199317932, 'test/loss': 3.267530918121338, 'test/num_examples': 10000, 'score': 6758.6344385147095, 'total_duration': 7365.6261332035065, 'accumulated_submission_time': 6758.6344385147095, 'accumulated_eval_time': 605.3328831195831, 'accumulated_logging_time': 0.8873722553253174}
I0201 14:15:30.855154 139702543816448 logging_writer.py:48] [14706] accumulated_eval_time=605.332883, accumulated_logging_time=0.887372, accumulated_submission_time=6758.634439, global_step=14706, preemption_count=0, score=6758.634439, test/accuracy=0.321600, test/loss=3.267531, test/num_examples=10000, total_duration=7365.626133, train/accuracy=0.451074, train/loss=2.485666, validation/accuracy=0.416140, validation/loss=2.670121, validation/num_examples=50000
I0201 14:16:09.786747 139702527031040 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.5607185363769531, loss=3.434941053390503
I0201 14:16:55.476916 139702543816448 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.7718534469604492, loss=3.8572607040405273
I0201 14:17:41.854139 139702527031040 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.0701704025268555, loss=5.751544952392578
I0201 14:18:27.927711 139702543816448 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.6974045038223267, loss=3.5949583053588867
I0201 14:19:13.921576 139702527031040 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.747836709022522, loss=3.397228240966797
I0201 14:20:00.136842 139702543816448 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.737292766571045, loss=3.435455322265625
I0201 14:20:46.223099 139702527031040 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.8539259433746338, loss=3.5328750610351562
I0201 14:21:32.290771 139702543816448 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.1503448486328125, loss=5.5463104248046875
I0201 14:22:18.419488 139702527031040 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.6262781620025635, loss=3.9342992305755615
I0201 14:22:30.948770 139863983413056 spec.py:321] Evaluating on the training split.
I0201 14:22:40.902004 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 14:23:05.027305 139863983413056 spec.py:349] Evaluating on the test split.
I0201 14:23:06.670658 139863983413056 submission_runner.py:408] Time since start: 7821.46s, 	Step: 15629, 	{'train/accuracy': 0.47089841961860657, 'train/loss': 2.4013569355010986, 'validation/accuracy': 0.42739999294281006, 'validation/loss': 2.6038594245910645, 'validation/num_examples': 50000, 'test/accuracy': 0.33170002698898315, 'test/loss': 3.190614938735962, 'test/num_examples': 10000, 'score': 7178.670622348785, 'total_duration': 7821.458172559738, 'accumulated_submission_time': 7178.670622348785, 'accumulated_eval_time': 641.0547397136688, 'accumulated_logging_time': 0.9128148555755615}
I0201 14:23:06.689177 139702543816448 logging_writer.py:48] [15629] accumulated_eval_time=641.054740, accumulated_logging_time=0.912815, accumulated_submission_time=7178.670622, global_step=15629, preemption_count=0, score=7178.670622, test/accuracy=0.331700, test/loss=3.190615, test/num_examples=10000, total_duration=7821.458173, train/accuracy=0.470898, train/loss=2.401357, validation/accuracy=0.427400, validation/loss=2.603859, validation/num_examples=50000
I0201 14:23:35.458521 139702527031040 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.666157603263855, loss=3.4970126152038574
I0201 14:24:20.722834 139702543816448 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.824938416481018, loss=3.4343433380126953
I0201 14:25:06.819104 139702527031040 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.3368934392929077, loss=4.037542343139648
I0201 14:25:52.854794 139702543816448 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.5740063190460205, loss=3.88093900680542
I0201 14:26:38.711794 139702527031040 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.3790415525436401, loss=4.045161724090576
I0201 14:27:24.841256 139702543816448 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.411308765411377, loss=4.815791606903076
I0201 14:28:11.020528 139702527031040 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.7253252267837524, loss=3.3391506671905518
I0201 14:28:56.670146 139702543816448 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.2220345735549927, loss=5.727639675140381
I0201 14:29:42.602402 139702527031040 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.5827581882476807, loss=3.3700456619262695
I0201 14:30:06.795732 139863983413056 spec.py:321] Evaluating on the training split.
I0201 14:30:17.352802 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 14:30:43.499955 139863983413056 spec.py:349] Evaluating on the test split.
I0201 14:30:45.134399 139863983413056 submission_runner.py:408] Time since start: 8279.92s, 	Step: 16554, 	{'train/accuracy': 0.47603514790534973, 'train/loss': 2.324101448059082, 'validation/accuracy': 0.4444599747657776, 'validation/loss': 2.4943294525146484, 'validation/num_examples': 50000, 'test/accuracy': 0.34140002727508545, 'test/loss': 3.1120223999023438, 'test/num_examples': 10000, 'score': 7598.7170877456665, 'total_duration': 8279.92192864418, 'accumulated_submission_time': 7598.7170877456665, 'accumulated_eval_time': 679.3933956623077, 'accumulated_logging_time': 0.9427704811096191}
I0201 14:30:45.151426 139702543816448 logging_writer.py:48] [16554] accumulated_eval_time=679.393396, accumulated_logging_time=0.942770, accumulated_submission_time=7598.717088, global_step=16554, preemption_count=0, score=7598.717088, test/accuracy=0.341400, test/loss=3.112022, test/num_examples=10000, total_duration=8279.921929, train/accuracy=0.476035, train/loss=2.324101, validation/accuracy=0.444460, validation/loss=2.494329, validation/num_examples=50000
I0201 14:31:03.894465 139702527031040 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.2253633737564087, loss=5.558523178100586
I0201 14:31:47.703138 139702543816448 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.1666864156723022, loss=5.45991325378418
I0201 14:32:33.750134 139702527031040 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.4167883396148682, loss=3.7418758869171143
I0201 14:33:19.805602 139702543816448 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.108874797821045, loss=5.633167266845703
I0201 14:34:05.478548 139702527031040 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.623288869857788, loss=3.3493096828460693
I0201 14:34:51.508193 139702543816448 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.4469479322433472, loss=3.2782790660858154
I0201 14:35:37.561556 139702527031040 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.5125696659088135, loss=3.406109571456909
I0201 14:36:23.294570 139702543816448 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.5634572505950928, loss=3.4598660469055176
I0201 14:37:09.197279 139702527031040 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.4864754676818848, loss=3.1960344314575195
I0201 14:37:45.499006 139863983413056 spec.py:321] Evaluating on the training split.
I0201 14:37:56.324262 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 14:38:21.298224 139863983413056 spec.py:349] Evaluating on the test split.
I0201 14:38:22.938054 139863983413056 submission_runner.py:408] Time since start: 8737.73s, 	Step: 17481, 	{'train/accuracy': 0.48291015625, 'train/loss': 2.2987401485443115, 'validation/accuracy': 0.4461599886417389, 'validation/loss': 2.4863901138305664, 'validation/num_examples': 50000, 'test/accuracy': 0.34610000252723694, 'test/loss': 3.0988028049468994, 'test/num_examples': 10000, 'score': 8019.005577802658, 'total_duration': 8737.72558760643, 'accumulated_submission_time': 8019.005577802658, 'accumulated_eval_time': 716.8324489593506, 'accumulated_logging_time': 0.9697284698486328}
I0201 14:38:22.958202 139702543816448 logging_writer.py:48] [17481] accumulated_eval_time=716.832449, accumulated_logging_time=0.969728, accumulated_submission_time=8019.005578, global_step=17481, preemption_count=0, score=8019.005578, test/accuracy=0.346100, test/loss=3.098803, test/num_examples=10000, total_duration=8737.725588, train/accuracy=0.482910, train/loss=2.298740, validation/accuracy=0.446160, validation/loss=2.486390, validation/num_examples=50000
I0201 14:38:30.938181 139702527031040 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.6736595630645752, loss=3.3203554153442383
I0201 14:39:12.924328 139702543816448 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.6618156433105469, loss=3.9218668937683105
I0201 14:39:59.137306 139702527031040 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.4123419523239136, loss=3.6757993698120117
I0201 14:40:45.465745 139702543816448 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.1181600093841553, loss=3.2874295711517334
I0201 14:41:31.207378 139702527031040 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.5834728479385376, loss=3.452209711074829
I0201 14:42:17.280622 139702543816448 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.3812642097473145, loss=3.5707080364227295
I0201 14:43:03.381195 139702527031040 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.8236918449401855, loss=3.2161026000976562
I0201 14:43:49.230470 139702543816448 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.1036027669906616, loss=4.5381879806518555
I0201 14:44:35.466354 139702527031040 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.5874360799789429, loss=3.379260301589966
I0201 14:45:21.584168 139702543816448 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.6373701095581055, loss=3.6604223251342773
I0201 14:45:23.082627 139863983413056 spec.py:321] Evaluating on the training split.
I0201 14:45:33.721428 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 14:45:59.107748 139863983413056 spec.py:349] Evaluating on the test split.
I0201 14:46:00.749508 139863983413056 submission_runner.py:408] Time since start: 9195.54s, 	Step: 18405, 	{'train/accuracy': 0.5003905892372131, 'train/loss': 2.1953961849212646, 'validation/accuracy': 0.44909998774528503, 'validation/loss': 2.455754041671753, 'validation/num_examples': 50000, 'test/accuracy': 0.35190001130104065, 'test/loss': 3.06425142288208, 'test/num_examples': 10000, 'score': 8439.072633743286, 'total_duration': 9195.537045240402, 'accumulated_submission_time': 8439.072633743286, 'accumulated_eval_time': 754.4993402957916, 'accumulated_logging_time': 0.9993364810943604}
I0201 14:46:00.766433 139702527031040 logging_writer.py:48] [18405] accumulated_eval_time=754.499340, accumulated_logging_time=0.999336, accumulated_submission_time=8439.072634, global_step=18405, preemption_count=0, score=8439.072634, test/accuracy=0.351900, test/loss=3.064251, test/num_examples=10000, total_duration=9195.537045, train/accuracy=0.500391, train/loss=2.195396, validation/accuracy=0.449100, validation/loss=2.455754, validation/num_examples=50000
I0201 14:46:40.174830 139702543816448 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.6952805519104004, loss=3.339550733566284
I0201 14:47:26.406364 139702527031040 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.39863920211792, loss=3.415987730026245
I0201 14:48:12.792159 139702543816448 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.7411807775497437, loss=3.4004621505737305
I0201 14:48:59.433163 139702527031040 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.3078218698501587, loss=3.9337525367736816
I0201 14:49:45.146348 139702543816448 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.098618268966675, loss=3.2437174320220947
I0201 14:50:31.524159 139702527031040 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.122883915901184, loss=5.596717357635498
I0201 14:51:17.586505 139702543816448 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.6515486240386963, loss=3.200265407562256
I0201 14:52:03.469772 139702527031040 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.6679966449737549, loss=3.283512592315674
I0201 14:52:49.719522 139702543816448 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.425204873085022, loss=4.031250953674316
I0201 14:53:00.888926 139863983413056 spec.py:321] Evaluating on the training split.
I0201 14:53:11.222682 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 14:53:34.854161 139863983413056 spec.py:349] Evaluating on the test split.
I0201 14:53:36.502415 139863983413056 submission_runner.py:408] Time since start: 9651.29s, 	Step: 19326, 	{'train/accuracy': 0.49574217200279236, 'train/loss': 2.2166545391082764, 'validation/accuracy': 0.46535998582839966, 'validation/loss': 2.381878137588501, 'validation/num_examples': 50000, 'test/accuracy': 0.3622000217437744, 'test/loss': 2.999075174331665, 'test/num_examples': 10000, 'score': 8859.13781619072, 'total_duration': 9651.289949893951, 'accumulated_submission_time': 8859.13781619072, 'accumulated_eval_time': 790.1128647327423, 'accumulated_logging_time': 1.0250554084777832}
I0201 14:53:36.519409 139702527031040 logging_writer.py:48] [19326] accumulated_eval_time=790.112865, accumulated_logging_time=1.025055, accumulated_submission_time=8859.137816, global_step=19326, preemption_count=0, score=8859.137816, test/accuracy=0.362200, test/loss=2.999075, test/num_examples=10000, total_duration=9651.289950, train/accuracy=0.495742, train/loss=2.216655, validation/accuracy=0.465360, validation/loss=2.381878, validation/num_examples=50000
I0201 14:54:06.497445 139702543816448 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.5512142181396484, loss=3.241673707962036
I0201 14:54:51.749847 139702527031040 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.3895753622055054, loss=4.9520368576049805
I0201 14:55:37.694945 139702543816448 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.2813475131988525, loss=4.941860198974609
I0201 14:56:23.805638 139702527031040 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.5515682697296143, loss=3.335280418395996
I0201 14:57:09.646700 139702543816448 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.6451184749603271, loss=3.171968936920166
I0201 14:57:55.703065 139702527031040 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.1034746170043945, loss=3.3177719116210938
I0201 14:58:41.657906 139702543816448 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.5842105150222778, loss=3.1587107181549072
I0201 14:59:27.910984 139702527031040 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.510206937789917, loss=3.226999282836914
I0201 15:00:13.873468 139702543816448 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.2108798027038574, loss=4.619785785675049
I0201 15:00:36.660493 139863983413056 spec.py:321] Evaluating on the training split.
I0201 15:00:47.098820 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 15:01:11.364323 139863983413056 spec.py:349] Evaluating on the test split.
I0201 15:01:13.006578 139863983413056 submission_runner.py:408] Time since start: 10107.79s, 	Step: 20251, 	{'train/accuracy': 0.49443358182907104, 'train/loss': 2.2306392192840576, 'validation/accuracy': 0.46230000257492065, 'validation/loss': 2.414182186126709, 'validation/num_examples': 50000, 'test/accuracy': 0.3603000044822693, 'test/loss': 3.0382838249206543, 'test/num_examples': 10000, 'score': 9279.222533941269, 'total_duration': 10107.794107198715, 'accumulated_submission_time': 9279.222533941269, 'accumulated_eval_time': 826.458952665329, 'accumulated_logging_time': 1.0508620738983154}
I0201 15:01:13.024028 139702527031040 logging_writer.py:48] [20251] accumulated_eval_time=826.458953, accumulated_logging_time=1.050862, accumulated_submission_time=9279.222534, global_step=20251, preemption_count=0, score=9279.222534, test/accuracy=0.360300, test/loss=3.038284, test/num_examples=10000, total_duration=10107.794107, train/accuracy=0.494434, train/loss=2.230639, validation/accuracy=0.462300, validation/loss=2.414182, validation/num_examples=50000
I0201 15:01:32.988477 139702543816448 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.124692440032959, loss=4.465823173522949
I0201 15:02:16.856815 139702527031040 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.4320342540740967, loss=3.0857443809509277
I0201 15:03:02.949946 139702543816448 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.6290082931518555, loss=3.7645606994628906
I0201 15:03:49.024804 139702527031040 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.514090657234192, loss=3.378901958465576
I0201 15:04:34.494503 139702543816448 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.5600271224975586, loss=3.1240713596343994
I0201 15:05:20.292585 139702527031040 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.3111343383789062, loss=3.931290626525879
I0201 15:06:06.396698 139702543816448 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.6189281940460205, loss=3.1319005489349365
I0201 15:06:52.022961 139702527031040 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.1020641326904297, loss=5.090263366699219
I0201 15:07:38.088778 139702543816448 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.723008394241333, loss=3.2245161533355713
I0201 15:08:13.119049 139863983413056 spec.py:321] Evaluating on the training split.
I0201 15:08:23.548224 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 15:08:50.207256 139863983413056 spec.py:349] Evaluating on the test split.
I0201 15:08:51.845195 139863983413056 submission_runner.py:408] Time since start: 10566.63s, 	Step: 21178, 	{'train/accuracy': 0.5224609375, 'train/loss': 2.1027472019195557, 'validation/accuracy': 0.4746599793434143, 'validation/loss': 2.3242554664611816, 'validation/num_examples': 50000, 'test/accuracy': 0.37050002813339233, 'test/loss': 2.947197437286377, 'test/num_examples': 10000, 'score': 9699.259632349014, 'total_duration': 10566.632721424103, 'accumulated_submission_time': 9699.259632349014, 'accumulated_eval_time': 865.185097694397, 'accumulated_logging_time': 1.0772264003753662}
I0201 15:08:51.862802 139702527031040 logging_writer.py:48] [21178] accumulated_eval_time=865.185098, accumulated_logging_time=1.077226, accumulated_submission_time=9699.259632, global_step=21178, preemption_count=0, score=9699.259632, test/accuracy=0.370500, test/loss=2.947197, test/num_examples=10000, total_duration=10566.632721, train/accuracy=0.522461, train/loss=2.102747, validation/accuracy=0.474660, validation/loss=2.324255, validation/num_examples=50000
I0201 15:09:01.059395 139702543816448 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.3083469867706299, loss=5.4624433517456055
I0201 15:09:43.657153 139702527031040 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.5062495470046997, loss=3.160951614379883
I0201 15:10:29.948360 139702543816448 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.5610935688018799, loss=3.1981208324432373
I0201 15:11:16.187695 139702527031040 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.4312604665756226, loss=3.6534671783447266
I0201 15:12:02.092561 139702543816448 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.5089612007141113, loss=3.894882917404175
I0201 15:12:48.184116 139702527031040 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.9926527142524719, loss=5.559102535247803
I0201 15:13:34.349900 139702543816448 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.9435191750526428, loss=5.681524276733398
I0201 15:14:20.503999 139702527031040 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.6657966375350952, loss=3.212637186050415
I0201 15:15:06.620533 139702543816448 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.56844961643219, loss=3.254053831100464
I0201 15:15:52.453069 139702527031040 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.5758461952209473, loss=3.095637559890747
I0201 15:15:52.467663 139863983413056 spec.py:321] Evaluating on the training split.
I0201 15:16:03.070759 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 15:16:27.558103 139863983413056 spec.py:349] Evaluating on the test split.
I0201 15:16:29.199285 139863983413056 submission_runner.py:408] Time since start: 11023.99s, 	Step: 22101, 	{'train/accuracy': 0.5175585746765137, 'train/loss': 2.128702402114868, 'validation/accuracy': 0.4822399914264679, 'validation/loss': 2.302961826324463, 'validation/num_examples': 50000, 'test/accuracy': 0.3824000060558319, 'test/loss': 2.909083604812622, 'test/num_examples': 10000, 'score': 10119.803411722183, 'total_duration': 11023.986797571182, 'accumulated_submission_time': 10119.803411722183, 'accumulated_eval_time': 901.9166934490204, 'accumulated_logging_time': 1.106586217880249}
I0201 15:16:29.220202 139702543816448 logging_writer.py:48] [22101] accumulated_eval_time=901.916693, accumulated_logging_time=1.106586, accumulated_submission_time=10119.803412, global_step=22101, preemption_count=0, score=10119.803412, test/accuracy=0.382400, test/loss=2.909084, test/num_examples=10000, total_duration=11023.986798, train/accuracy=0.517559, train/loss=2.128702, validation/accuracy=0.482240, validation/loss=2.302962, validation/num_examples=50000
I0201 15:17:10.388251 139702527031040 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.0832388401031494, loss=5.2610979080200195
I0201 15:17:56.534130 139702543816448 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.5389814376831055, loss=3.160875082015991
I0201 15:18:42.734018 139702527031040 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.632754921913147, loss=3.127139091491699
I0201 15:19:28.681310 139702543816448 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.140819787979126, loss=5.226834297180176
I0201 15:20:15.542007 139702527031040 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.5485069751739502, loss=3.1546339988708496
I0201 15:21:01.630570 139702543816448 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.384231686592102, loss=3.936959743499756
I0201 15:21:47.466526 139702527031040 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.5960347652435303, loss=3.2732937335968018
I0201 15:22:33.369221 139702543816448 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.6611641645431519, loss=3.035768508911133
I0201 15:23:19.448011 139702527031040 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.4791067838668823, loss=3.1256186962127686
I0201 15:23:29.226510 139863983413056 spec.py:321] Evaluating on the training split.
I0201 15:23:39.562108 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 15:24:01.013682 139863983413056 spec.py:349] Evaluating on the test split.
I0201 15:24:02.649682 139863983413056 submission_runner.py:408] Time since start: 11477.44s, 	Step: 23023, 	{'train/accuracy': 0.5186718702316284, 'train/loss': 2.1031439304351807, 'validation/accuracy': 0.4840799868106842, 'validation/loss': 2.2893989086151123, 'validation/num_examples': 50000, 'test/accuracy': 0.3818000257015228, 'test/loss': 2.9118733406066895, 'test/num_examples': 10000, 'score': 10539.751255273819, 'total_duration': 11477.437187194824, 'accumulated_submission_time': 10539.751255273819, 'accumulated_eval_time': 935.33984375, 'accumulated_logging_time': 1.1371748447418213}
I0201 15:24:02.668051 139702543816448 logging_writer.py:48] [23023] accumulated_eval_time=935.339844, accumulated_logging_time=1.137175, accumulated_submission_time=10539.751255, global_step=23023, preemption_count=0, score=10539.751255, test/accuracy=0.381800, test/loss=2.911873, test/num_examples=10000, total_duration=11477.437187, train/accuracy=0.518672, train/loss=2.103144, validation/accuracy=0.484080, validation/loss=2.289399, validation/num_examples=50000
I0201 15:24:33.993927 139702527031040 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.5635062456130981, loss=3.183011770248413
I0201 15:25:19.512667 139702543816448 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.436118721961975, loss=3.240835428237915
I0201 15:26:05.612494 139702527031040 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.5760105848312378, loss=3.129162549972534
I0201 15:26:51.860182 139702543816448 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.5737683773040771, loss=3.0798299312591553
I0201 15:27:37.741731 139702527031040 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.534827709197998, loss=3.1781795024871826
I0201 15:28:23.787021 139702543816448 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.3360466957092285, loss=4.417983531951904
I0201 15:29:09.772525 139702527031040 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.6081805229187012, loss=3.1379289627075195
I0201 15:29:56.106586 139702543816448 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.306795597076416, loss=5.458395957946777
I0201 15:30:42.024800 139702527031040 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.4916599988937378, loss=3.323716163635254
I0201 15:31:02.942931 139863983413056 spec.py:321] Evaluating on the training split.
I0201 15:31:13.780158 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 15:31:38.387714 139863983413056 spec.py:349] Evaluating on the test split.
I0201 15:31:40.032256 139863983413056 submission_runner.py:408] Time since start: 11934.82s, 	Step: 23947, 	{'train/accuracy': 0.5406249761581421, 'train/loss': 2.0112240314483643, 'validation/accuracy': 0.49449998140335083, 'validation/loss': 2.2325828075408936, 'validation/num_examples': 50000, 'test/accuracy': 0.38760000467300415, 'test/loss': 2.855060577392578, 'test/num_examples': 10000, 'score': 10959.968374729156, 'total_duration': 11934.81978225708, 'accumulated_submission_time': 10959.968374729156, 'accumulated_eval_time': 972.4291639328003, 'accumulated_logging_time': 1.1649210453033447}
I0201 15:31:40.050898 139702543816448 logging_writer.py:48] [23947] accumulated_eval_time=972.429164, accumulated_logging_time=1.164921, accumulated_submission_time=10959.968375, global_step=23947, preemption_count=0, score=10959.968375, test/accuracy=0.387600, test/loss=2.855061, test/num_examples=10000, total_duration=11934.819782, train/accuracy=0.540625, train/loss=2.011224, validation/accuracy=0.494500, validation/loss=2.232583, validation/num_examples=50000
I0201 15:32:01.626007 139702527031040 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.3760645389556885, loss=3.6009716987609863
I0201 15:32:45.701232 139702543816448 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.7777565717697144, loss=3.1578054428100586
I0201 15:33:31.570551 139702527031040 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.383809208869934, loss=4.766563415527344
I0201 15:34:17.749714 139702543816448 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.6015175580978394, loss=3.374945640563965
I0201 15:35:03.458383 139702527031040 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.1493695974349976, loss=5.179837703704834
I0201 15:35:49.259585 139702543816448 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.6055325269699097, loss=3.065099000930786
I0201 15:36:35.315482 139702527031040 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.2580132484436035, loss=3.6851603984832764
I0201 15:37:21.131433 139702543816448 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.152395248413086, loss=4.600312232971191
I0201 15:38:07.068559 139702527031040 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.6022053956985474, loss=3.0549285411834717
I0201 15:38:40.410726 139863983413056 spec.py:321] Evaluating on the training split.
I0201 15:38:50.964290 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 15:39:13.713671 139863983413056 spec.py:349] Evaluating on the test split.
I0201 15:39:15.351101 139863983413056 submission_runner.py:408] Time since start: 12390.14s, 	Step: 24875, 	{'train/accuracy': 0.5497460961341858, 'train/loss': 1.9716825485229492, 'validation/accuracy': 0.5027799606323242, 'validation/loss': 2.185875654220581, 'validation/num_examples': 50000, 'test/accuracy': 0.39560002088546753, 'test/loss': 2.823624849319458, 'test/num_examples': 10000, 'score': 11380.271517753601, 'total_duration': 12390.1386282444, 'accumulated_submission_time': 11380.271517753601, 'accumulated_eval_time': 1007.3695442676544, 'accumulated_logging_time': 1.192338228225708}
I0201 15:39:15.376667 139702543816448 logging_writer.py:48] [24875] accumulated_eval_time=1007.369544, accumulated_logging_time=1.192338, accumulated_submission_time=11380.271518, global_step=24875, preemption_count=0, score=11380.271518, test/accuracy=0.395600, test/loss=2.823625, test/num_examples=10000, total_duration=12390.138628, train/accuracy=0.549746, train/loss=1.971683, validation/accuracy=0.502780, validation/loss=2.185876, validation/num_examples=50000
I0201 15:39:25.761711 139702527031040 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0162607431411743, loss=5.471030235290527
I0201 15:40:08.472937 139702543816448 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.491262435913086, loss=3.096259593963623
I0201 15:40:54.908396 139702527031040 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.3963572978973389, loss=3.4695518016815186
I0201 15:41:41.087089 139702543816448 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.4804103374481201, loss=2.9448366165161133
I0201 15:42:27.120446 139702527031040 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.6618014574050903, loss=2.9080424308776855
I0201 15:43:13.488618 139702543816448 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.2572109699249268, loss=5.144245624542236
I0201 15:43:59.496949 139702527031040 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.1032321453094482, loss=4.5255608558654785
I0201 15:44:45.274719 139702543816448 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.2811235189437866, loss=4.248979091644287
I0201 15:45:31.426397 139702527031040 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.2580194473266602, loss=4.051130294799805
I0201 15:46:15.689378 139863983413056 spec.py:321] Evaluating on the training split.
I0201 15:46:26.040021 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 15:46:52.221309 139863983413056 spec.py:349] Evaluating on the test split.
I0201 15:46:53.874143 139863983413056 submission_runner.py:408] Time since start: 12848.66s, 	Step: 25798, 	{'train/accuracy': 0.5396679639816284, 'train/loss': 2.003579616546631, 'validation/accuracy': 0.5015400052070618, 'validation/loss': 2.187553644180298, 'validation/num_examples': 50000, 'test/accuracy': 0.3968000113964081, 'test/loss': 2.8089516162872314, 'test/num_examples': 10000, 'score': 11800.526648044586, 'total_duration': 12848.661672592163, 'accumulated_submission_time': 11800.526648044586, 'accumulated_eval_time': 1045.554309129715, 'accumulated_logging_time': 1.227823257446289}
I0201 15:46:53.894396 139702543816448 logging_writer.py:48] [25798] accumulated_eval_time=1045.554309, accumulated_logging_time=1.227823, accumulated_submission_time=11800.526648, global_step=25798, preemption_count=0, score=11800.526648, test/accuracy=0.396800, test/loss=2.808952, test/num_examples=10000, total_duration=12848.661673, train/accuracy=0.539668, train/loss=2.003580, validation/accuracy=0.501540, validation/loss=2.187554, validation/num_examples=50000
I0201 15:46:55.095662 139702527031040 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.6090914011001587, loss=3.0958170890808105
I0201 15:47:36.149439 139702543816448 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.3044321537017822, loss=4.490025997161865
I0201 15:48:22.120337 139702527031040 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.3400957584381104, loss=3.9016060829162598
I0201 15:49:08.532287 139702543816448 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.6067306995391846, loss=3.0997564792633057
I0201 15:49:54.650028 139702527031040 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.5012702941894531, loss=2.993149995803833
I0201 15:50:41.188481 139702543816448 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.4530071020126343, loss=2.8960375785827637
I0201 15:51:27.860713 139702527031040 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.740822196006775, loss=3.2289392948150635
I0201 15:52:13.914955 139702543816448 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.1838748455047607, loss=5.590712547302246
I0201 15:52:59.787816 139702527031040 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.6018427610397339, loss=3.0133743286132812
I0201 15:53:45.801915 139702543816448 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.6067649126052856, loss=3.416365623474121
I0201 15:53:54.249976 139863983413056 spec.py:321] Evaluating on the training split.
I0201 15:54:05.339110 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 15:54:28.817028 139863983413056 spec.py:349] Evaluating on the test split.
I0201 15:54:30.448575 139863983413056 submission_runner.py:408] Time since start: 13305.24s, 	Step: 26720, 	{'train/accuracy': 0.5524218678474426, 'train/loss': 1.9628337621688843, 'validation/accuracy': 0.5076599717140198, 'validation/loss': 2.161849021911621, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.789376735687256, 'test/num_examples': 10000, 'score': 12220.824908733368, 'total_duration': 13305.236095428467, 'accumulated_submission_time': 12220.824908733368, 'accumulated_eval_time': 1081.7528929710388, 'accumulated_logging_time': 1.2576560974121094}
I0201 15:54:30.469273 139702527031040 logging_writer.py:48] [26720] accumulated_eval_time=1081.752893, accumulated_logging_time=1.257656, accumulated_submission_time=12220.824909, global_step=26720, preemption_count=0, score=12220.824909, test/accuracy=0.398900, test/loss=2.789377, test/num_examples=10000, total_duration=13305.236095, train/accuracy=0.552422, train/loss=1.962834, validation/accuracy=0.507660, validation/loss=2.161849, validation/num_examples=50000
I0201 15:55:02.855576 139702543816448 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.6737267971038818, loss=2.8533639907836914
I0201 15:55:48.581296 139702527031040 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.4788570404052734, loss=3.4767274856567383
I0201 15:56:34.622874 139702543816448 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.6278233528137207, loss=2.8178532123565674
I0201 15:57:20.589206 139702527031040 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.6053476333618164, loss=2.930454730987549
I0201 15:58:06.497704 139702543816448 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.6850528717041016, loss=3.023172378540039
I0201 15:58:52.516619 139702527031040 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.6419682502746582, loss=2.8912360668182373
I0201 15:59:38.549745 139702543816448 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.1721709966659546, loss=5.2874250411987305
I0201 16:00:24.719959 139702527031040 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.432084321975708, loss=3.8502209186553955
I0201 16:01:11.188115 139702543816448 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.6666467189788818, loss=2.9059834480285645
I0201 16:01:30.766067 139863983413056 spec.py:321] Evaluating on the training split.
I0201 16:01:41.145029 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 16:02:02.144668 139863983413056 spec.py:349] Evaluating on the test split.
I0201 16:02:03.800028 139863983413056 submission_runner.py:408] Time since start: 13758.59s, 	Step: 27644, 	{'train/accuracy': 0.5774218440055847, 'train/loss': 1.7861711978912354, 'validation/accuracy': 0.5199399590492249, 'validation/loss': 2.085148334503174, 'validation/num_examples': 50000, 'test/accuracy': 0.4028000235557556, 'test/loss': 2.7451486587524414, 'test/num_examples': 10000, 'score': 12641.06434583664, 'total_duration': 13758.587542057037, 'accumulated_submission_time': 12641.06434583664, 'accumulated_eval_time': 1114.786839723587, 'accumulated_logging_time': 1.2878186702728271}
I0201 16:02:03.823917 139702527031040 logging_writer.py:48] [27644] accumulated_eval_time=1114.786840, accumulated_logging_time=1.287819, accumulated_submission_time=12641.064346, global_step=27644, preemption_count=0, score=12641.064346, test/accuracy=0.402800, test/loss=2.745149, test/num_examples=10000, total_duration=13758.587542, train/accuracy=0.577422, train/loss=1.786171, validation/accuracy=0.519940, validation/loss=2.085148, validation/num_examples=50000
I0201 16:02:26.592478 139702543816448 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.6023941040039062, loss=3.739034652709961
I0201 16:03:11.142921 139702527031040 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.3451861143112183, loss=4.151669979095459
I0201 16:03:57.247350 139702543816448 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.7830040454864502, loss=3.012341022491455
I0201 16:04:43.676892 139702527031040 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.627511739730835, loss=3.3407163619995117
I0201 16:05:29.661868 139702543816448 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.47883939743042, loss=3.1596360206604004
I0201 16:06:15.666113 139702527031040 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.5817633867263794, loss=2.930359363555908
I0201 16:07:01.552961 139702543816448 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.6720423698425293, loss=2.9991307258605957
I0201 16:07:47.383868 139702527031040 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.3883349895477295, loss=3.5295839309692383
I0201 16:08:33.567241 139702543816448 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.1218782663345337, loss=5.440316200256348
I0201 16:09:04.170471 139863983413056 spec.py:321] Evaluating on the training split.
I0201 16:09:14.512168 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 16:09:37.200072 139863983413056 spec.py:349] Evaluating on the test split.
I0201 16:09:38.846343 139863983413056 submission_runner.py:408] Time since start: 14213.63s, 	Step: 28568, 	{'train/accuracy': 0.5564843416213989, 'train/loss': 1.902564525604248, 'validation/accuracy': 0.5218999981880188, 'validation/loss': 2.0633559226989746, 'validation/num_examples': 50000, 'test/accuracy': 0.41270002722740173, 'test/loss': 2.69579815864563, 'test/num_examples': 10000, 'score': 13061.353293895721, 'total_duration': 14213.63385272026, 'accumulated_submission_time': 13061.353293895721, 'accumulated_eval_time': 1149.4626867771149, 'accumulated_logging_time': 1.321131706237793}
I0201 16:09:38.866499 139702527031040 logging_writer.py:48] [28568] accumulated_eval_time=1149.462687, accumulated_logging_time=1.321132, accumulated_submission_time=13061.353294, global_step=28568, preemption_count=0, score=13061.353294, test/accuracy=0.412700, test/loss=2.695798, test/num_examples=10000, total_duration=14213.633853, train/accuracy=0.556484, train/loss=1.902565, validation/accuracy=0.521900, validation/loss=2.063356, validation/num_examples=50000
I0201 16:09:52.043279 139702543816448 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.1644248962402344, loss=5.4405131340026855
I0201 16:10:35.025552 139702527031040 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.3831778764724731, loss=3.303192377090454
I0201 16:11:21.239318 139702543816448 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.616838812828064, loss=2.96435809135437
I0201 16:12:07.565092 139702527031040 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.6519237756729126, loss=2.911160945892334
I0201 16:12:53.527252 139702543816448 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.364166021347046, loss=3.381165027618408
I0201 16:13:39.525001 139702527031040 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.3857638835906982, loss=3.700404405593872
I0201 16:14:25.681587 139702543816448 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.3526599407196045, loss=4.057705402374268
I0201 16:15:11.714726 139702527031040 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.5280288457870483, loss=3.213444232940674
I0201 16:15:57.359545 139702543816448 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.5549838542938232, loss=2.9464643001556396
I0201 16:16:39.294518 139863983413056 spec.py:321] Evaluating on the training split.
I0201 16:16:49.765316 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 16:17:13.379388 139863983413056 spec.py:349] Evaluating on the test split.
I0201 16:17:15.019617 139863983413056 submission_runner.py:408] Time since start: 14669.81s, 	Step: 29493, 	{'train/accuracy': 0.5679101347923279, 'train/loss': 1.855337381362915, 'validation/accuracy': 0.5210599899291992, 'validation/loss': 2.056286096572876, 'validation/num_examples': 50000, 'test/accuracy': 0.40730002522468567, 'test/loss': 2.698023557662964, 'test/num_examples': 10000, 'score': 13481.723677873611, 'total_duration': 14669.807126045227, 'accumulated_submission_time': 13481.723677873611, 'accumulated_eval_time': 1185.187756061554, 'accumulated_logging_time': 1.3507835865020752}
I0201 16:17:15.039933 139702527031040 logging_writer.py:48] [29493] accumulated_eval_time=1185.187756, accumulated_logging_time=1.350784, accumulated_submission_time=13481.723678, global_step=29493, preemption_count=0, score=13481.723678, test/accuracy=0.407300, test/loss=2.698024, test/num_examples=10000, total_duration=14669.807126, train/accuracy=0.567910, train/loss=1.855337, validation/accuracy=0.521060, validation/loss=2.056286, validation/num_examples=50000
I0201 16:17:18.229827 139702543816448 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.585460901260376, loss=3.0468616485595703
I0201 16:17:59.618316 139702527031040 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.6988492012023926, loss=2.864581346511841
I0201 16:18:45.518552 139702543816448 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.3574148416519165, loss=3.814429521560669
I0201 16:19:31.517985 139702527031040 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.1807374954223633, loss=4.330883979797363
I0201 16:20:17.802891 139702543816448 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.5590962171554565, loss=2.930382013320923
I0201 16:21:03.989827 139702527031040 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.6781799793243408, loss=2.882108688354492
I0201 16:21:50.072807 139702543816448 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.7035092115402222, loss=2.8007662296295166
I0201 16:22:36.038475 139702527031040 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.266183614730835, loss=5.452932357788086
I0201 16:23:22.153051 139702543816448 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.630466341972351, loss=3.0449979305267334
I0201 16:24:08.005534 139702527031040 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.2974605560302734, loss=4.375948429107666
I0201 16:24:15.452103 139863983413056 spec.py:321] Evaluating on the training split.
I0201 16:24:25.954911 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 16:24:48.136350 139863983413056 spec.py:349] Evaluating on the test split.
I0201 16:24:49.776176 139863983413056 submission_runner.py:408] Time since start: 15124.56s, 	Step: 30418, 	{'train/accuracy': 0.5766796469688416, 'train/loss': 1.8442224264144897, 'validation/accuracy': 0.5261200070381165, 'validation/loss': 2.0872247219085693, 'validation/num_examples': 50000, 'test/accuracy': 0.40950003266334534, 'test/loss': 2.7296950817108154, 'test/num_examples': 10000, 'score': 13902.074913978577, 'total_duration': 15124.563690185547, 'accumulated_submission_time': 13902.074913978577, 'accumulated_eval_time': 1219.5118567943573, 'accumulated_logging_time': 1.3829002380371094}
I0201 16:24:49.797790 139702543816448 logging_writer.py:48] [30418] accumulated_eval_time=1219.511857, accumulated_logging_time=1.382900, accumulated_submission_time=13902.074914, global_step=30418, preemption_count=0, score=13902.074914, test/accuracy=0.409500, test/loss=2.729695, test/num_examples=10000, total_duration=15124.563690, train/accuracy=0.576680, train/loss=1.844222, validation/accuracy=0.526120, validation/loss=2.087225, validation/num_examples=50000
I0201 16:25:23.030404 139702527031040 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.3636162281036377, loss=3.234517812728882
I0201 16:26:08.687908 139702543816448 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.3405038118362427, loss=5.34672212600708
I0201 16:26:54.548872 139702527031040 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.6821609735488892, loss=2.892910957336426
I0201 16:27:40.598612 139702543816448 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.713736891746521, loss=3.2119863033294678
I0201 16:28:26.619714 139702527031040 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.3505009412765503, loss=4.46168851852417
I0201 16:29:12.448103 139702543816448 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.4548453092575073, loss=4.169770240783691
I0201 16:29:58.488312 139702527031040 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.6436669826507568, loss=3.276203155517578
I0201 16:30:44.788359 139702543816448 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.7483175992965698, loss=2.979356527328491
I0201 16:31:31.293262 139702527031040 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.564151406288147, loss=2.9736268520355225
I0201 16:31:49.823430 139863983413056 spec.py:321] Evaluating on the training split.
I0201 16:32:00.303971 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 16:32:27.938232 139863983413056 spec.py:349] Evaluating on the test split.
I0201 16:32:29.576893 139863983413056 submission_runner.py:408] Time since start: 15584.36s, 	Step: 31342, 	{'train/accuracy': 0.5602929592132568, 'train/loss': 1.9060845375061035, 'validation/accuracy': 0.5257999897003174, 'validation/loss': 2.082095146179199, 'validation/num_examples': 50000, 'test/accuracy': 0.4130000174045563, 'test/loss': 2.724041700363159, 'test/num_examples': 10000, 'score': 14322.042706489563, 'total_duration': 15584.364404439926, 'accumulated_submission_time': 14322.042706489563, 'accumulated_eval_time': 1259.265320301056, 'accumulated_logging_time': 1.4137554168701172}
I0201 16:32:29.599367 139702543816448 logging_writer.py:48] [31342] accumulated_eval_time=1259.265320, accumulated_logging_time=1.413755, accumulated_submission_time=14322.042706, global_step=31342, preemption_count=0, score=14322.042706, test/accuracy=0.413000, test/loss=2.724042, test/num_examples=10000, total_duration=15584.364404, train/accuracy=0.560293, train/loss=1.906085, validation/accuracy=0.525800, validation/loss=2.082095, validation/num_examples=50000
I0201 16:32:53.171122 139702527031040 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.7815004587173462, loss=2.789790391921997
I0201 16:33:37.606007 139702543816448 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.671010971069336, loss=2.7873520851135254
I0201 16:34:23.605132 139702527031040 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.5658042430877686, loss=2.9071435928344727
I0201 16:35:10.076002 139702543816448 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.584893822669983, loss=3.163760185241699
I0201 16:35:55.845513 139702527031040 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.254641056060791, loss=3.7153360843658447
I0201 16:36:41.910499 139702543816448 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.1569042205810547, loss=5.384088516235352
I0201 16:37:27.649827 139702527031040 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.6660971641540527, loss=2.9295291900634766
I0201 16:38:13.611651 139702543816448 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.574219822883606, loss=2.7773244380950928
I0201 16:38:59.679140 139702527031040 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.6157466173171997, loss=2.774172782897949
I0201 16:39:29.618130 139863983413056 spec.py:321] Evaluating on the training split.
I0201 16:39:40.048581 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 16:40:00.270571 139863983413056 spec.py:349] Evaluating on the test split.
I0201 16:40:01.923359 139863983413056 submission_runner.py:408] Time since start: 16036.71s, 	Step: 32267, 	{'train/accuracy': 0.5718163847923279, 'train/loss': 1.8387211561203003, 'validation/accuracy': 0.5347399711608887, 'validation/loss': 2.0247344970703125, 'validation/num_examples': 50000, 'test/accuracy': 0.42010003328323364, 'test/loss': 2.6634790897369385, 'test/num_examples': 10000, 'score': 14742.001463413239, 'total_duration': 16036.710843086243, 'accumulated_submission_time': 14742.001463413239, 'accumulated_eval_time': 1291.5704834461212, 'accumulated_logging_time': 1.4477450847625732}
I0201 16:40:01.950416 139702543816448 logging_writer.py:48] [32267] accumulated_eval_time=1291.570483, accumulated_logging_time=1.447745, accumulated_submission_time=14742.001463, global_step=32267, preemption_count=0, score=14742.001463, test/accuracy=0.420100, test/loss=2.663479, test/num_examples=10000, total_duration=16036.710843, train/accuracy=0.571816, train/loss=1.838721, validation/accuracy=0.534740, validation/loss=2.024734, validation/num_examples=50000
I0201 16:40:15.543717 139702527031040 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.1862491369247437, loss=4.099643707275391
I0201 16:40:58.356153 139702543816448 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.6990504264831543, loss=2.9036948680877686
I0201 16:41:44.425005 139702527031040 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.3419924974441528, loss=4.827057838439941
I0201 16:42:30.990522 139702543816448 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.450073003768921, loss=5.424258232116699
I0201 16:43:16.886417 139702527031040 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.6380488872528076, loss=3.1065900325775146
I0201 16:44:02.899280 139702543816448 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.6988571882247925, loss=2.8026912212371826
I0201 16:44:48.977819 139702527031040 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.4743272066116333, loss=2.785588264465332
I0201 16:45:34.772788 139702543816448 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.1361021995544434, loss=5.5026326179504395
I0201 16:46:20.538904 139702527031040 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.7280282974243164, loss=2.764420986175537
I0201 16:47:02.104100 139863983413056 spec.py:321] Evaluating on the training split.
I0201 16:47:12.514407 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 16:47:35.383003 139863983413056 spec.py:349] Evaluating on the test split.
I0201 16:47:37.029237 139863983413056 submission_runner.py:408] Time since start: 16491.82s, 	Step: 33192, 	{'train/accuracy': 0.5762890577316284, 'train/loss': 1.8025901317596436, 'validation/accuracy': 0.5324400067329407, 'validation/loss': 2.0144195556640625, 'validation/num_examples': 50000, 'test/accuracy': 0.42170003056526184, 'test/loss': 2.6554317474365234, 'test/num_examples': 10000, 'score': 15162.095947265625, 'total_duration': 16491.816769123077, 'accumulated_submission_time': 15162.095947265625, 'accumulated_eval_time': 1326.4956283569336, 'accumulated_logging_time': 1.4855966567993164}
I0201 16:47:37.049362 139702543816448 logging_writer.py:48] [33192] accumulated_eval_time=1326.495628, accumulated_logging_time=1.485597, accumulated_submission_time=15162.095947, global_step=33192, preemption_count=0, score=15162.095947, test/accuracy=0.421700, test/loss=2.655432, test/num_examples=10000, total_duration=16491.816769, train/accuracy=0.576289, train/loss=1.802590, validation/accuracy=0.532440, validation/loss=2.014420, validation/num_examples=50000
I0201 16:47:40.641900 139702527031040 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.8164573907852173, loss=2.864797830581665
I0201 16:48:22.271183 139702543816448 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.6595231294631958, loss=2.8001277446746826
I0201 16:49:08.025239 139702527031040 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.3892372846603394, loss=5.060999393463135
I0201 16:49:54.213971 139702543816448 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.654237985610962, loss=2.998720407485962
I0201 16:50:40.150351 139702527031040 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.603075623512268, loss=2.9578044414520264
I0201 16:51:26.300225 139702543816448 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.2852015495300293, loss=4.311630725860596
I0201 16:52:12.778818 139702527031040 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.657828450202942, loss=2.765782356262207
I0201 16:52:58.580499 139702543816448 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.3231608867645264, loss=5.486917495727539
I0201 16:53:45.011438 139702527031040 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.3203821182250977, loss=3.8563311100006104
I0201 16:54:30.940337 139702543816448 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.692943811416626, loss=2.870142936706543
I0201 16:54:37.172542 139863983413056 spec.py:321] Evaluating on the training split.
I0201 16:54:47.596311 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 16:55:11.135508 139863983413056 spec.py:349] Evaluating on the test split.
I0201 16:55:12.775339 139863983413056 submission_runner.py:408] Time since start: 16947.56s, 	Step: 34115, 	{'train/accuracy': 0.5796874761581421, 'train/loss': 1.8108338117599487, 'validation/accuracy': 0.533840000629425, 'validation/loss': 2.0201752185821533, 'validation/num_examples': 50000, 'test/accuracy': 0.42420002818107605, 'test/loss': 2.6502912044525146, 'test/num_examples': 10000, 'score': 15582.160798072815, 'total_duration': 16947.562863588333, 'accumulated_submission_time': 15582.160798072815, 'accumulated_eval_time': 1362.09840965271, 'accumulated_logging_time': 1.5149762630462646}
I0201 16:55:12.794526 139702527031040 logging_writer.py:48] [34115] accumulated_eval_time=1362.098410, accumulated_logging_time=1.514976, accumulated_submission_time=15582.160798, global_step=34115, preemption_count=0, score=15582.160798, test/accuracy=0.424200, test/loss=2.650291, test/num_examples=10000, total_duration=16947.562864, train/accuracy=0.579687, train/loss=1.810834, validation/accuracy=0.533840, validation/loss=2.020175, validation/num_examples=50000
I0201 16:55:47.398719 139702543816448 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.6625174283981323, loss=2.9347424507141113
I0201 16:56:33.198529 139702527031040 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.7419711351394653, loss=3.107088565826416
I0201 16:57:19.455287 139702543816448 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.6050385236740112, loss=2.751775026321411
I0201 16:58:05.594733 139702527031040 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.8286694288253784, loss=2.867029905319214
I0201 16:58:51.432749 139702543816448 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.6118353605270386, loss=2.798652172088623
I0201 16:59:37.581441 139702527031040 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.7007696628570557, loss=2.7792985439300537
I0201 17:00:23.934917 139702543816448 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.2990095615386963, loss=4.1044135093688965
I0201 17:01:09.979362 139702527031040 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.7893266677856445, loss=2.9449000358581543
I0201 17:01:55.930793 139702543816448 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.7185394763946533, loss=2.9239754676818848
I0201 17:02:12.976518 139863983413056 spec.py:321] Evaluating on the training split.
I0201 17:02:23.619477 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 17:02:47.588771 139863983413056 spec.py:349] Evaluating on the test split.
I0201 17:02:49.238577 139863983413056 submission_runner.py:408] Time since start: 17404.03s, 	Step: 35038, 	{'train/accuracy': 0.5776171684265137, 'train/loss': 1.8064240217208862, 'validation/accuracy': 0.5378199815750122, 'validation/loss': 1.9878385066986084, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.645028591156006, 'test/num_examples': 10000, 'score': 16002.27912735939, 'total_duration': 17404.02609181404, 'accumulated_submission_time': 16002.27912735939, 'accumulated_eval_time': 1398.3604464530945, 'accumulated_logging_time': 1.5435802936553955}
I0201 17:02:49.261356 139702527031040 logging_writer.py:48] [35038] accumulated_eval_time=1398.360446, accumulated_logging_time=1.543580, accumulated_submission_time=16002.279127, global_step=35038, preemption_count=0, score=16002.279127, test/accuracy=0.423400, test/loss=2.645029, test/num_examples=10000, total_duration=17404.026092, train/accuracy=0.577617, train/loss=1.806424, validation/accuracy=0.537820, validation/loss=1.987839, validation/num_examples=50000
I0201 17:03:14.425891 139702543816448 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.677510380744934, loss=3.0146288871765137
I0201 17:03:59.090792 139702527031040 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.7342504262924194, loss=2.846447229385376
I0201 17:04:45.206275 139702543816448 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.7162131071090698, loss=2.752960443496704
I0201 17:05:31.561000 139702527031040 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.5041804313659668, loss=3.9766898155212402
I0201 17:06:17.623317 139702543816448 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.1755595207214355, loss=4.993884563446045
I0201 17:07:03.650283 139702527031040 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.5091228485107422, loss=3.4033381938934326
I0201 17:07:49.411665 139702543816448 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.296249270439148, loss=4.210530757904053
I0201 17:08:35.327321 139702527031040 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.8465116024017334, loss=2.8243231773376465
I0201 17:09:21.287651 139702543816448 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.5072592496871948, loss=2.9503555297851562
I0201 17:09:49.598625 139863983413056 spec.py:321] Evaluating on the training split.
I0201 17:09:59.966795 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 17:10:26.646345 139863983413056 spec.py:349] Evaluating on the test split.
I0201 17:10:28.294580 139863983413056 submission_runner.py:408] Time since start: 17863.08s, 	Step: 35963, 	{'train/accuracy': 0.5839062333106995, 'train/loss': 1.7839018106460571, 'validation/accuracy': 0.5367000102996826, 'validation/loss': 1.999006986618042, 'validation/num_examples': 50000, 'test/accuracy': 0.41920003294944763, 'test/loss': 2.6471946239471436, 'test/num_examples': 10000, 'score': 16422.558165311813, 'total_duration': 17863.082113027573, 'accumulated_submission_time': 16422.558165311813, 'accumulated_eval_time': 1437.056425333023, 'accumulated_logging_time': 1.5760712623596191}
I0201 17:10:28.314584 139702527031040 logging_writer.py:48] [35963] accumulated_eval_time=1437.056425, accumulated_logging_time=1.576071, accumulated_submission_time=16422.558165, global_step=35963, preemption_count=0, score=16422.558165, test/accuracy=0.419200, test/loss=2.647195, test/num_examples=10000, total_duration=17863.082113, train/accuracy=0.583906, train/loss=1.783902, validation/accuracy=0.536700, validation/loss=1.999007, validation/num_examples=50000
I0201 17:10:43.478107 139702543816448 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.4766530990600586, loss=2.936034679412842
I0201 17:11:26.954667 139702527031040 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.6420458555221558, loss=2.8128223419189453
I0201 17:12:12.993943 139702543816448 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.6943916082382202, loss=4.16817045211792
I0201 17:12:59.330237 139702527031040 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.8740718364715576, loss=2.6990437507629395
I0201 17:13:45.172951 139702543816448 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.8511366844177246, loss=2.836768627166748
I0201 17:14:30.847366 139702527031040 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.5093114376068115, loss=3.6277055740356445
I0201 17:15:16.715947 139702543816448 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.3161163330078125, loss=4.34694766998291
I0201 17:16:02.387273 139702527031040 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.6273095607757568, loss=3.1529059410095215
I0201 17:16:48.417596 139702543816448 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.819495677947998, loss=2.992583751678467
I0201 17:17:28.639416 139863983413056 spec.py:321] Evaluating on the training split.
I0201 17:17:38.988337 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 17:18:04.221836 139863983413056 spec.py:349] Evaluating on the test split.
I0201 17:18:05.864717 139863983413056 submission_runner.py:408] Time since start: 18320.65s, 	Step: 36889, 	{'train/accuracy': 0.6082812547683716, 'train/loss': 1.6362735033035278, 'validation/accuracy': 0.5448200106620789, 'validation/loss': 1.9368997812271118, 'validation/num_examples': 50000, 'test/accuracy': 0.42970001697540283, 'test/loss': 2.5912978649139404, 'test/num_examples': 10000, 'score': 16842.82310938835, 'total_duration': 18320.652250528336, 'accumulated_submission_time': 16842.82310938835, 'accumulated_eval_time': 1474.281730890274, 'accumulated_logging_time': 1.6072852611541748}
I0201 17:18:05.885191 139702527031040 logging_writer.py:48] [36889] accumulated_eval_time=1474.281731, accumulated_logging_time=1.607285, accumulated_submission_time=16842.823109, global_step=36889, preemption_count=0, score=16842.823109, test/accuracy=0.429700, test/loss=2.591298, test/num_examples=10000, total_duration=18320.652251, train/accuracy=0.608281, train/loss=1.636274, validation/accuracy=0.544820, validation/loss=1.936900, validation/num_examples=50000
I0201 17:18:10.677019 139702543816448 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.3000291585922241, loss=5.213603973388672
I0201 17:18:52.355496 139702527031040 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.6043881177902222, loss=2.9531562328338623
I0201 17:19:38.246882 139702543816448 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.667974829673767, loss=2.8034143447875977
I0201 17:20:24.477515 139702527031040 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.4611108303070068, loss=4.660815715789795
I0201 17:21:10.444694 139702543816448 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.527417778968811, loss=3.705125331878662
I0201 17:21:56.333675 139702527031040 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.5346107482910156, loss=3.166123867034912
I0201 17:22:42.409458 139702543816448 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.609688401222229, loss=2.852464199066162
I0201 17:23:28.483910 139702527031040 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.5611882209777832, loss=5.33671236038208
I0201 17:24:14.529579 139702543816448 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.5383920669555664, loss=3.2581827640533447
I0201 17:25:00.402064 139702527031040 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.6700471639633179, loss=3.039170980453491
I0201 17:25:06.047790 139863983413056 spec.py:321] Evaluating on the training split.
I0201 17:25:16.276226 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 17:25:42.189448 139863983413056 spec.py:349] Evaluating on the test split.
I0201 17:25:43.829827 139863983413056 submission_runner.py:408] Time since start: 18778.62s, 	Step: 37814, 	{'train/accuracy': 0.5752734541893005, 'train/loss': 1.8207134008407593, 'validation/accuracy': 0.5349000096321106, 'validation/loss': 2.0128426551818848, 'validation/num_examples': 50000, 'test/accuracy': 0.42080003023147583, 'test/loss': 2.656367540359497, 'test/num_examples': 10000, 'score': 17262.928169488907, 'total_duration': 18778.617341041565, 'accumulated_submission_time': 17262.928169488907, 'accumulated_eval_time': 1512.0637485980988, 'accumulated_logging_time': 1.636359453201294}
I0201 17:25:43.852534 139702543816448 logging_writer.py:48] [37814] accumulated_eval_time=1512.063749, accumulated_logging_time=1.636359, accumulated_submission_time=17262.928169, global_step=37814, preemption_count=0, score=17262.928169, test/accuracy=0.420800, test/loss=2.656368, test/num_examples=10000, total_duration=18778.617341, train/accuracy=0.575273, train/loss=1.820713, validation/accuracy=0.534900, validation/loss=2.012843, validation/num_examples=50000
I0201 17:26:19.122498 139702527031040 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.2280855178833008, loss=5.3374104499816895
I0201 17:27:04.967256 139702543816448 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.70343816280365, loss=2.853222608566284
I0201 17:27:50.876212 139702527031040 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.7731765508651733, loss=2.9761905670166016
I0201 17:28:37.055384 139702543816448 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.7328243255615234, loss=2.8420708179473877
I0201 17:29:23.107180 139702527031040 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.4241459369659424, loss=3.800827980041504
I0201 17:30:09.173634 139702543816448 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.8922381401062012, loss=2.8766348361968994
I0201 17:30:55.143886 139702527031040 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.7461286783218384, loss=2.8563883304595947
I0201 17:31:41.073752 139702543816448 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.5116045475006104, loss=3.913630485534668
I0201 17:32:26.995906 139702527031040 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.6371530294418335, loss=2.857977867126465
I0201 17:32:44.112913 139863983413056 spec.py:321] Evaluating on the training split.
I0201 17:32:54.476696 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 17:33:18.805775 139863983413056 spec.py:349] Evaluating on the test split.
I0201 17:33:20.440188 139863983413056 submission_runner.py:408] Time since start: 19235.23s, 	Step: 38739, 	{'train/accuracy': 0.5896679759025574, 'train/loss': 1.7389613389968872, 'validation/accuracy': 0.5483999848365784, 'validation/loss': 1.9300124645233154, 'validation/num_examples': 50000, 'test/accuracy': 0.43140003085136414, 'test/loss': 2.607285737991333, 'test/num_examples': 10000, 'score': 17683.129102230072, 'total_duration': 19235.227719545364, 'accumulated_submission_time': 17683.129102230072, 'accumulated_eval_time': 1548.39102768898, 'accumulated_logging_time': 1.6686515808105469}
I0201 17:33:20.459935 139702543816448 logging_writer.py:48] [38739] accumulated_eval_time=1548.391028, accumulated_logging_time=1.668652, accumulated_submission_time=17683.129102, global_step=38739, preemption_count=0, score=17683.129102, test/accuracy=0.431400, test/loss=2.607286, test/num_examples=10000, total_duration=19235.227720, train/accuracy=0.589668, train/loss=1.738961, validation/accuracy=0.548400, validation/loss=1.930012, validation/num_examples=50000
I0201 17:33:45.217469 139702527031040 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.769731044769287, loss=2.797271490097046
I0201 17:34:29.978992 139702543816448 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.6927491426467896, loss=2.7894704341888428
I0201 17:35:16.116050 139702527031040 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.742251992225647, loss=2.766599178314209
I0201 17:36:02.237446 139702543816448 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.7668966054916382, loss=2.78524112701416
I0201 17:36:47.976559 139702527031040 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.8503375053405762, loss=2.761230707168579
I0201 17:37:34.113520 139702543816448 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.589982509613037, loss=3.3263096809387207
I0201 17:38:19.911157 139702527031040 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.357757568359375, loss=4.682044506072998
I0201 17:39:05.655661 139702543816448 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.5894073247909546, loss=3.074455976486206
I0201 17:39:51.654131 139702527031040 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.6481194496154785, loss=3.647498607635498
I0201 17:40:20.685349 139863983413056 spec.py:321] Evaluating on the training split.
I0201 17:40:31.318186 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 17:40:56.280217 139863983413056 spec.py:349] Evaluating on the test split.
I0201 17:40:57.931674 139863983413056 submission_runner.py:408] Time since start: 19692.72s, 	Step: 39665, 	{'train/accuracy': 0.6051952838897705, 'train/loss': 1.6787258386611938, 'validation/accuracy': 0.5502399802207947, 'validation/loss': 1.9336005449295044, 'validation/num_examples': 50000, 'test/accuracy': 0.4336000084877014, 'test/loss': 2.5970046520233154, 'test/num_examples': 10000, 'score': 18103.296102762222, 'total_duration': 19692.71918320656, 'accumulated_submission_time': 18103.296102762222, 'accumulated_eval_time': 1585.6373362541199, 'accumulated_logging_time': 1.6978001594543457}
I0201 17:40:57.956394 139702543816448 logging_writer.py:48] [39665] accumulated_eval_time=1585.637336, accumulated_logging_time=1.697800, accumulated_submission_time=18103.296103, global_step=39665, preemption_count=0, score=18103.296103, test/accuracy=0.433600, test/loss=2.597005, test/num_examples=10000, total_duration=19692.719183, train/accuracy=0.605195, train/loss=1.678726, validation/accuracy=0.550240, validation/loss=1.933601, validation/num_examples=50000
I0201 17:41:12.342133 139702527031040 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.5611910820007324, loss=2.6616480350494385
I0201 17:41:55.196302 139702543816448 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.5463931560516357, loss=2.7153027057647705
I0201 17:42:41.005208 139702527031040 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.6932119131088257, loss=3.199228525161743
I0201 17:43:27.092742 139702543816448 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.6980314254760742, loss=2.9635701179504395
I0201 17:44:13.197391 139702527031040 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.604846477508545, loss=3.179863452911377
I0201 17:44:59.271778 139702543816448 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.6439554691314697, loss=2.682018518447876
I0201 17:45:45.041416 139702527031040 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.578657865524292, loss=3.5122852325439453
I0201 17:46:30.918377 139702543816448 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.8307442665100098, loss=2.798053026199341
I0201 17:47:16.866827 139702527031040 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.8086036443710327, loss=3.3510589599609375
I0201 17:47:57.935602 139863983413056 spec.py:321] Evaluating on the training split.
I0201 17:48:08.507261 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 17:48:32.524574 139863983413056 spec.py:349] Evaluating on the test split.
I0201 17:48:34.168784 139863983413056 submission_runner.py:408] Time since start: 20148.96s, 	Step: 40591, 	{'train/accuracy': 0.5888671875, 'train/loss': 1.7536296844482422, 'validation/accuracy': 0.5473399758338928, 'validation/loss': 1.943955898284912, 'validation/num_examples': 50000, 'test/accuracy': 0.43070003390312195, 'test/loss': 2.6018872261047363, 'test/num_examples': 10000, 'score': 18523.215060949326, 'total_duration': 20148.956298351288, 'accumulated_submission_time': 18523.215060949326, 'accumulated_eval_time': 1621.870540380478, 'accumulated_logging_time': 1.7322437763214111}
I0201 17:48:34.198511 139702543816448 logging_writer.py:48] [40591] accumulated_eval_time=1621.870540, accumulated_logging_time=1.732244, accumulated_submission_time=18523.215061, global_step=40591, preemption_count=0, score=18523.215061, test/accuracy=0.430700, test/loss=2.601887, test/num_examples=10000, total_duration=20148.956298, train/accuracy=0.588867, train/loss=1.753630, validation/accuracy=0.547340, validation/loss=1.943956, validation/num_examples=50000
I0201 17:48:38.178608 139702527031040 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.2885411977767944, loss=4.9364728927612305
I0201 17:49:19.678065 139702543816448 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.2003343105316162, loss=5.440291404724121
I0201 17:50:05.856590 139702527031040 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7014769315719604, loss=2.80346417427063
I0201 17:50:52.032729 139702543816448 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.1781977415084839, loss=5.074483871459961
I0201 17:51:38.030379 139702527031040 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.6513111591339111, loss=3.094217300415039
I0201 17:52:24.159394 139702543816448 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.6461151838302612, loss=2.841918468475342
I0201 17:53:10.107308 139702527031040 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.7024134397506714, loss=2.913637399673462
I0201 17:53:56.366786 139702543816448 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.2750540971755981, loss=4.455202102661133
I0201 17:54:42.420355 139702527031040 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.6992088556289673, loss=2.6742048263549805
I0201 17:55:28.382041 139702543816448 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.18950355052948, loss=5.232975959777832
I0201 17:55:34.524274 139863983413056 spec.py:321] Evaluating on the training split.
I0201 17:55:44.837916 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 17:56:10.052150 139863983413056 spec.py:349] Evaluating on the test split.
I0201 17:56:11.685951 139863983413056 submission_runner.py:408] Time since start: 20606.47s, 	Step: 41515, 	{'train/accuracy': 0.5938476324081421, 'train/loss': 1.7254453897476196, 'validation/accuracy': 0.5546000003814697, 'validation/loss': 1.9125330448150635, 'validation/num_examples': 50000, 'test/accuracy': 0.4418000280857086, 'test/loss': 2.5483005046844482, 'test/num_examples': 10000, 'score': 18943.480389356613, 'total_duration': 20606.473484039307, 'accumulated_submission_time': 18943.480389356613, 'accumulated_eval_time': 1659.0322132110596, 'accumulated_logging_time': 1.7727165222167969}
I0201 17:56:11.706641 139702527031040 logging_writer.py:48] [41515] accumulated_eval_time=1659.032213, accumulated_logging_time=1.772717, accumulated_submission_time=18943.480389, global_step=41515, preemption_count=0, score=18943.480389, test/accuracy=0.441800, test/loss=2.548301, test/num_examples=10000, total_duration=20606.473484, train/accuracy=0.593848, train/loss=1.725445, validation/accuracy=0.554600, validation/loss=1.912533, validation/num_examples=50000
I0201 17:56:46.302209 139702543816448 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.1892271041870117, loss=5.373259544372559
I0201 17:57:32.143494 139702527031040 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.3206963539123535, loss=5.31214714050293
I0201 17:58:18.209633 139702543816448 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.3112214803695679, loss=4.179912090301514
I0201 17:59:04.283127 139702527031040 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.2062619924545288, loss=5.230225563049316
I0201 17:59:50.163978 139702543816448 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.3748782873153687, loss=3.8599276542663574
I0201 18:00:36.049880 139702527031040 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.7057569026947021, loss=2.849609851837158
I0201 18:01:21.979370 139702543816448 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.7588516473770142, loss=2.676640510559082
I0201 18:02:07.960090 139702527031040 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.89676833152771, loss=2.6890385150909424
I0201 18:02:53.979954 139702543816448 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.541000485420227, loss=2.9341111183166504
I0201 18:03:11.691678 139863983413056 spec.py:321] Evaluating on the training split.
I0201 18:03:22.032706 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 18:03:45.718454 139863983413056 spec.py:349] Evaluating on the test split.
I0201 18:03:47.364089 139863983413056 submission_runner.py:408] Time since start: 21062.15s, 	Step: 42440, 	{'train/accuracy': 0.5986718535423279, 'train/loss': 1.7187060117721558, 'validation/accuracy': 0.5518999695777893, 'validation/loss': 1.947245478630066, 'validation/num_examples': 50000, 'test/accuracy': 0.4280000329017639, 'test/loss': 2.6177022457122803, 'test/num_examples': 10000, 'score': 19363.407828569412, 'total_duration': 21062.151589155197, 'accumulated_submission_time': 19363.407828569412, 'accumulated_eval_time': 1694.7046456336975, 'accumulated_logging_time': 1.80218505859375}
I0201 18:03:47.387857 139702527031040 logging_writer.py:48] [42440] accumulated_eval_time=1694.704646, accumulated_logging_time=1.802185, accumulated_submission_time=19363.407829, global_step=42440, preemption_count=0, score=19363.407829, test/accuracy=0.428000, test/loss=2.617702, test/num_examples=10000, total_duration=21062.151589, train/accuracy=0.598672, train/loss=1.718706, validation/accuracy=0.551900, validation/loss=1.947245, validation/num_examples=50000
I0201 18:04:11.737028 139702543816448 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.655867338180542, loss=2.7959141731262207
I0201 18:04:56.527167 139702527031040 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.4447518587112427, loss=3.3372013568878174
I0201 18:05:42.524409 139702543816448 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.455090880393982, loss=3.83720326423645
I0201 18:06:28.732535 139702527031040 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.6812649965286255, loss=2.8915014266967773
I0201 18:07:14.461511 139702543816448 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.3323346376419067, loss=5.331291675567627
I0201 18:08:00.522800 139702527031040 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.8133357763290405, loss=2.7125823497772217
I0201 18:08:46.455415 139702543816448 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.7024717330932617, loss=2.69399356842041
I0201 18:09:32.353508 139702527031040 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.4435839653015137, loss=4.357646942138672
I0201 18:10:18.383855 139702543816448 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.453046202659607, loss=5.303346157073975
I0201 18:10:47.751361 139863983413056 spec.py:321] Evaluating on the training split.
I0201 18:10:58.118544 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 18:11:23.097628 139863983413056 spec.py:349] Evaluating on the test split.
I0201 18:11:24.748266 139863983413056 submission_runner.py:408] Time since start: 21519.54s, 	Step: 43366, 	{'train/accuracy': 0.6058202981948853, 'train/loss': 1.6716299057006836, 'validation/accuracy': 0.5638200044631958, 'validation/loss': 1.8678628206253052, 'validation/num_examples': 50000, 'test/accuracy': 0.44520002603530884, 'test/loss': 2.5290257930755615, 'test/num_examples': 10000, 'score': 19783.71187520027, 'total_duration': 21519.535794973373, 'accumulated_submission_time': 19783.71187520027, 'accumulated_eval_time': 1731.7015480995178, 'accumulated_logging_time': 1.8368933200836182}
I0201 18:11:24.769401 139702527031040 logging_writer.py:48] [43366] accumulated_eval_time=1731.701548, accumulated_logging_time=1.836893, accumulated_submission_time=19783.711875, global_step=43366, preemption_count=0, score=19783.711875, test/accuracy=0.445200, test/loss=2.529026, test/num_examples=10000, total_duration=21519.535795, train/accuracy=0.605820, train/loss=1.671630, validation/accuracy=0.563820, validation/loss=1.867863, validation/num_examples=50000
I0201 18:11:38.742007 139702543816448 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.7013353109359741, loss=2.7859625816345215
I0201 18:12:21.781371 139702527031040 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.3288811445236206, loss=5.1625871658325195
I0201 18:13:07.689492 139702543816448 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.7133684158325195, loss=2.7125213146209717
I0201 18:13:53.642573 139702527031040 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.8900753259658813, loss=2.791152000427246
I0201 18:14:39.739430 139702543816448 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.1535801887512207, loss=4.980685710906982
I0201 18:15:25.645505 139702527031040 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.6461554765701294, loss=2.8825995922088623
I0201 18:16:12.185780 139702543816448 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.6258726119995117, loss=2.7987658977508545
I0201 18:16:57.620516 139702527031040 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.6237691640853882, loss=3.096278667449951
I0201 18:17:43.570995 139702543816448 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.6429461240768433, loss=3.195812225341797
I0201 18:18:25.193686 139863983413056 spec.py:321] Evaluating on the training split.
I0201 18:18:35.478627 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 18:18:57.896949 139863983413056 spec.py:349] Evaluating on the test split.
I0201 18:18:59.534379 139863983413056 submission_runner.py:408] Time since start: 21974.32s, 	Step: 44292, 	{'train/accuracy': 0.6031640768051147, 'train/loss': 1.6963953971862793, 'validation/accuracy': 0.562279999256134, 'validation/loss': 1.8898544311523438, 'validation/num_examples': 50000, 'test/accuracy': 0.44270002841949463, 'test/loss': 2.5435431003570557, 'test/num_examples': 10000, 'score': 20204.07502055168, 'total_duration': 21974.32190656662, 'accumulated_submission_time': 20204.07502055168, 'accumulated_eval_time': 1766.042251586914, 'accumulated_logging_time': 1.8706142902374268}
I0201 18:18:59.555492 139702527031040 logging_writer.py:48] [44292] accumulated_eval_time=1766.042252, accumulated_logging_time=1.870614, accumulated_submission_time=20204.075021, global_step=44292, preemption_count=0, score=20204.075021, test/accuracy=0.442700, test/loss=2.543543, test/num_examples=10000, total_duration=21974.321907, train/accuracy=0.603164, train/loss=1.696395, validation/accuracy=0.562280, validation/loss=1.889854, validation/num_examples=50000
I0201 18:19:03.164566 139702543816448 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.585263729095459, loss=3.3370361328125
I0201 18:19:44.811632 139702527031040 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.827693223953247, loss=2.920722246170044
I0201 18:20:30.985923 139702543816448 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.668023705482483, loss=2.7889888286590576
I0201 18:21:17.119040 139702527031040 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.8905245065689087, loss=2.8145639896392822
I0201 18:22:03.194663 139702543816448 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.5235058069229126, loss=5.250491142272949
I0201 18:22:48.953663 139702527031040 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.3404512405395508, loss=5.303478240966797
I0201 18:23:34.986588 139702543816448 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.277500033378601, loss=4.486217498779297
I0201 18:24:20.863257 139702527031040 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.4380204677581787, loss=4.7432637214660645
I0201 18:25:07.239509 139702543816448 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.5971119403839111, loss=3.144680976867676
I0201 18:25:52.989989 139702527031040 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.6971787214279175, loss=2.659862995147705
I0201 18:25:59.555267 139863983413056 spec.py:321] Evaluating on the training split.
I0201 18:26:10.100177 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 18:26:34.849419 139863983413056 spec.py:349] Evaluating on the test split.
I0201 18:26:36.498388 139863983413056 submission_runner.py:408] Time since start: 22431.29s, 	Step: 45216, 	{'train/accuracy': 0.6105663776397705, 'train/loss': 1.6492289304733276, 'validation/accuracy': 0.564579963684082, 'validation/loss': 1.8600382804870605, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.5134053230285645, 'test/num_examples': 10000, 'score': 20624.01596546173, 'total_duration': 22431.28592157364, 'accumulated_submission_time': 20624.01596546173, 'accumulated_eval_time': 1802.985370874405, 'accumulated_logging_time': 1.9021704196929932}
I0201 18:26:36.519339 139702543816448 logging_writer.py:48] [45216] accumulated_eval_time=1802.985371, accumulated_logging_time=1.902170, accumulated_submission_time=20624.015965, global_step=45216, preemption_count=0, score=20624.015965, test/accuracy=0.447300, test/loss=2.513405, test/num_examples=10000, total_duration=22431.285922, train/accuracy=0.610566, train/loss=1.649229, validation/accuracy=0.564580, validation/loss=1.860038, validation/num_examples=50000
I0201 18:27:10.723133 139702527031040 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.3562676906585693, loss=4.51500940322876
I0201 18:27:56.314201 139702543816448 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.9046504497528076, loss=2.7842257022857666
I0201 18:28:42.317600 139702527031040 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.3579246997833252, loss=4.6183648109436035
I0201 18:29:28.450756 139702543816448 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.5482012033462524, loss=3.289415121078491
I0201 18:30:14.744194 139702527031040 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.5639957189559937, loss=2.6399118900299072
I0201 18:31:00.952304 139702543816448 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.6533679962158203, loss=2.751190185546875
I0201 18:31:46.875034 139702527031040 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.593170404434204, loss=3.5478804111480713
I0201 18:32:32.831292 139702543816448 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.0382800102233887, loss=2.6761538982391357
I0201 18:33:18.912616 139702527031040 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.8633981943130493, loss=2.674048662185669
I0201 18:33:36.947272 139863983413056 spec.py:321] Evaluating on the training split.
I0201 18:33:47.283741 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 18:34:13.691718 139863983413056 spec.py:349] Evaluating on the test split.
I0201 18:34:15.337669 139863983413056 submission_runner.py:408] Time since start: 22890.13s, 	Step: 46141, 	{'train/accuracy': 0.6305859088897705, 'train/loss': 1.5546183586120605, 'validation/accuracy': 0.5636999607086182, 'validation/loss': 1.8670766353607178, 'validation/num_examples': 50000, 'test/accuracy': 0.4474000334739685, 'test/loss': 2.5046355724334717, 'test/num_examples': 10000, 'score': 21044.385004997253, 'total_duration': 22890.125202655792, 'accumulated_submission_time': 21044.385004997253, 'accumulated_eval_time': 1841.3757576942444, 'accumulated_logging_time': 1.9337167739868164}
I0201 18:34:15.362503 139702543816448 logging_writer.py:48] [46141] accumulated_eval_time=1841.375758, accumulated_logging_time=1.933717, accumulated_submission_time=21044.385005, global_step=46141, preemption_count=0, score=21044.385005, test/accuracy=0.447400, test/loss=2.504636, test/num_examples=10000, total_duration=22890.125203, train/accuracy=0.630586, train/loss=1.554618, validation/accuracy=0.563700, validation/loss=1.867077, validation/num_examples=50000
I0201 18:34:39.299317 139702527031040 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.3947538137435913, loss=4.4647345542907715
I0201 18:35:24.200393 139702543816448 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.6337617635726929, loss=3.209984064102173
I0201 18:36:10.156602 139702527031040 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.4994750022888184, loss=3.3546841144561768
I0201 18:36:56.200045 139702543816448 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.4344488382339478, loss=5.418328762054443
I0201 18:37:41.807554 139702527031040 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.671164631843567, loss=5.196037769317627
I0201 18:38:27.518892 139702543816448 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.334314227104187, loss=5.03629207611084
I0201 18:39:13.666929 139702527031040 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.635233759880066, loss=2.676255702972412
I0201 18:39:59.840475 139702543816448 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.6637763977050781, loss=2.656109094619751
I0201 18:40:46.082030 139702527031040 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.8150739669799805, loss=2.6721699237823486
I0201 18:41:15.495244 139863983413056 spec.py:321] Evaluating on the training split.
I0201 18:41:25.791176 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 18:41:49.369992 139863983413056 spec.py:349] Evaluating on the test split.
I0201 18:41:51.017742 139863983413056 submission_runner.py:408] Time since start: 23345.81s, 	Step: 47066, 	{'train/accuracy': 0.60595703125, 'train/loss': 1.6388249397277832, 'validation/accuracy': 0.5649600028991699, 'validation/loss': 1.8310381174087524, 'validation/num_examples': 50000, 'test/accuracy': 0.4488000273704529, 'test/loss': 2.477980613708496, 'test/num_examples': 10000, 'score': 21464.45929455757, 'total_duration': 23345.80527472496, 'accumulated_submission_time': 21464.45929455757, 'accumulated_eval_time': 1876.8982713222504, 'accumulated_logging_time': 1.9686899185180664}
I0201 18:41:51.039272 139702543816448 logging_writer.py:48] [47066] accumulated_eval_time=1876.898271, accumulated_logging_time=1.968690, accumulated_submission_time=21464.459295, global_step=47066, preemption_count=0, score=21464.459295, test/accuracy=0.448800, test/loss=2.477981, test/num_examples=10000, total_duration=23345.805275, train/accuracy=0.605957, train/loss=1.638825, validation/accuracy=0.564960, validation/loss=1.831038, validation/num_examples=50000
I0201 18:42:05.002840 139702527031040 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.7010157108306885, loss=2.7003533840179443
I0201 18:42:47.787059 139702543816448 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.6557379961013794, loss=2.6954641342163086
I0201 18:43:34.113419 139702527031040 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8688009977340698, loss=2.875875234603882
I0201 18:44:20.591780 139702543816448 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.714949607849121, loss=2.5044312477111816
I0201 18:45:06.447166 139702527031040 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.7428985834121704, loss=2.52968692779541
I0201 18:45:52.754498 139702543816448 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.7441638708114624, loss=2.566833257675171
I0201 18:46:38.746705 139702527031040 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.808719277381897, loss=2.6049976348876953
I0201 18:47:24.745032 139702543816448 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.207887887954712, loss=5.352043628692627
I0201 18:48:10.548550 139702527031040 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.3542859554290771, loss=3.4088783264160156
I0201 18:48:51.101707 139863983413056 spec.py:321] Evaluating on the training split.
I0201 18:49:01.322404 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 18:49:24.681803 139863983413056 spec.py:349] Evaluating on the test split.
I0201 18:49:26.318711 139863983413056 submission_runner.py:408] Time since start: 23801.11s, 	Step: 47990, 	{'train/accuracy': 0.6087695360183716, 'train/loss': 1.669023036956787, 'validation/accuracy': 0.5663599967956543, 'validation/loss': 1.8735897541046143, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.5234055519104004, 'test/num_examples': 10000, 'score': 21884.46242260933, 'total_duration': 23801.106241226196, 'accumulated_submission_time': 21884.46242260933, 'accumulated_eval_time': 1912.115308046341, 'accumulated_logging_time': 2.0009567737579346}
I0201 18:49:26.340149 139702543816448 logging_writer.py:48] [47990] accumulated_eval_time=1912.115308, accumulated_logging_time=2.000957, accumulated_submission_time=21884.462423, global_step=47990, preemption_count=0, score=21884.462423, test/accuracy=0.450200, test/loss=2.523406, test/num_examples=10000, total_duration=23801.106241, train/accuracy=0.608770, train/loss=1.669023, validation/accuracy=0.566360, validation/loss=1.873590, validation/num_examples=50000
I0201 18:49:30.730672 139702527031040 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.4071574211120605, loss=5.2312726974487305
I0201 18:50:12.310406 139702543816448 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.8399971723556519, loss=2.633652925491333
I0201 18:50:58.036826 139702527031040 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.446565866470337, loss=3.742231607437134
I0201 18:51:44.434426 139702543816448 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.8134918212890625, loss=2.6363306045532227
I0201 18:52:30.506310 139702527031040 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.592989206314087, loss=2.9708335399627686
I0201 18:53:16.495074 139702543816448 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.6423102617263794, loss=2.9097812175750732
I0201 18:54:02.846982 139702527031040 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.7771681547164917, loss=2.855480670928955
I0201 18:54:48.765210 139702543816448 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.6752935647964478, loss=2.637176036834717
I0201 18:55:35.123053 139702527031040 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.483399510383606, loss=3.1767396926879883
I0201 18:56:21.192387 139702543816448 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.4238981008529663, loss=3.74212908744812
I0201 18:56:26.454868 139863983413056 spec.py:321] Evaluating on the training split.
I0201 18:56:36.741055 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 18:56:59.373061 139863983413056 spec.py:349] Evaluating on the test split.
I0201 18:57:01.012583 139863983413056 submission_runner.py:408] Time since start: 24255.80s, 	Step: 48913, 	{'train/accuracy': 0.6250976324081421, 'train/loss': 1.5893003940582275, 'validation/accuracy': 0.5663999915122986, 'validation/loss': 1.850501537322998, 'validation/num_examples': 50000, 'test/accuracy': 0.4472000300884247, 'test/loss': 2.5053343772888184, 'test/num_examples': 10000, 'score': 22304.51851439476, 'total_duration': 24255.80009675026, 'accumulated_submission_time': 22304.51851439476, 'accumulated_eval_time': 1946.6730072498322, 'accumulated_logging_time': 2.0325334072113037}
I0201 18:57:01.035745 139702527031040 logging_writer.py:48] [48913] accumulated_eval_time=1946.673007, accumulated_logging_time=2.032533, accumulated_submission_time=22304.518514, global_step=48913, preemption_count=0, score=22304.518514, test/accuracy=0.447200, test/loss=2.505334, test/num_examples=10000, total_duration=24255.800097, train/accuracy=0.625098, train/loss=1.589300, validation/accuracy=0.566400, validation/loss=1.850502, validation/num_examples=50000
I0201 18:57:36.468407 139702543816448 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.3991693258285522, loss=4.468645095825195
I0201 18:58:22.389075 139702527031040 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.511089563369751, loss=5.160040378570557
I0201 18:59:08.714237 139702543816448 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.842138648033142, loss=2.851973056793213
I0201 18:59:55.107342 139702527031040 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.8436347246170044, loss=2.8776376247406006
I0201 19:00:40.875995 139702543816448 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.753624439239502, loss=2.676302433013916
I0201 19:01:26.831823 139702527031040 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.8453713655471802, loss=2.6640915870666504
I0201 19:02:12.634498 139702543816448 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.7941831350326538, loss=2.8726296424865723
I0201 19:02:58.258978 139702527031040 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.2343716621398926, loss=5.154962539672852
I0201 19:03:44.270967 139702543816448 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.7918567657470703, loss=2.863431930541992
I0201 19:04:01.352036 139863983413056 spec.py:321] Evaluating on the training split.
I0201 19:04:12.623669 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 19:04:39.168661 139863983413056 spec.py:349] Evaluating on the test split.
I0201 19:04:40.804820 139863983413056 submission_runner.py:408] Time since start: 24715.59s, 	Step: 49839, 	{'train/accuracy': 0.6113671660423279, 'train/loss': 1.6454654932022095, 'validation/accuracy': 0.5694599747657776, 'validation/loss': 1.8334907293319702, 'validation/num_examples': 50000, 'test/accuracy': 0.45280003547668457, 'test/loss': 2.4803237915039062, 'test/num_examples': 10000, 'score': 22724.775852918625, 'total_duration': 24715.59235072136, 'accumulated_submission_time': 22724.775852918625, 'accumulated_eval_time': 1986.1257948875427, 'accumulated_logging_time': 2.0658884048461914}
I0201 19:04:40.827362 139702527031040 logging_writer.py:48] [49839] accumulated_eval_time=1986.125795, accumulated_logging_time=2.065888, accumulated_submission_time=22724.775853, global_step=49839, preemption_count=0, score=22724.775853, test/accuracy=0.452800, test/loss=2.480324, test/num_examples=10000, total_duration=24715.592351, train/accuracy=0.611367, train/loss=1.645465, validation/accuracy=0.569460, validation/loss=1.833491, validation/num_examples=50000
I0201 19:05:05.549053 139702543816448 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.5331292152404785, loss=4.668319225311279
I0201 19:05:50.167744 139702527031040 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.5168471336364746, loss=5.308995723724365
I0201 19:06:36.407697 139702543816448 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.8623257875442505, loss=2.572589635848999
I0201 19:07:22.626931 139702527031040 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.5386892557144165, loss=4.040910720825195
I0201 19:08:08.243940 139702543816448 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.6426342725753784, loss=2.7170262336730957
I0201 19:08:54.248121 139702527031040 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.5401793718338013, loss=5.324451923370361
I0201 19:09:40.024957 139702543816448 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.9517945051193237, loss=2.615222454071045
I0201 19:10:26.049678 139702527031040 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.6243131160736084, loss=2.6959311962127686
I0201 19:11:12.010791 139702543816448 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.5559016466140747, loss=3.0390663146972656
I0201 19:11:41.080304 139863983413056 spec.py:321] Evaluating on the training split.
I0201 19:11:51.854377 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 19:12:13.644845 139863983413056 spec.py:349] Evaluating on the test split.
I0201 19:12:15.294088 139863983413056 submission_runner.py:408] Time since start: 25170.08s, 	Step: 50765, 	{'train/accuracy': 0.6122851371765137, 'train/loss': 1.6380223035812378, 'validation/accuracy': 0.5708400011062622, 'validation/loss': 1.8340604305267334, 'validation/num_examples': 50000, 'test/accuracy': 0.4515000283718109, 'test/loss': 2.4958276748657227, 'test/num_examples': 10000, 'score': 23144.970401525497, 'total_duration': 25170.08160185814, 'accumulated_submission_time': 23144.970401525497, 'accumulated_eval_time': 2020.3395702838898, 'accumulated_logging_time': 2.098045825958252}
I0201 19:12:15.323156 139702527031040 logging_writer.py:48] [50765] accumulated_eval_time=2020.339570, accumulated_logging_time=2.098046, accumulated_submission_time=23144.970402, global_step=50765, preemption_count=0, score=23144.970402, test/accuracy=0.451500, test/loss=2.495828, test/num_examples=10000, total_duration=25170.081602, train/accuracy=0.612285, train/loss=1.638022, validation/accuracy=0.570840, validation/loss=1.834060, validation/num_examples=50000
I0201 19:12:29.689346 139702543816448 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.7086135149002075, loss=2.6114749908447266
I0201 19:13:12.845937 139702527031040 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.2573162317276, loss=5.242277145385742
I0201 19:13:58.985291 139702543816448 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.6117174625396729, loss=3.003655195236206
I0201 19:14:45.402352 139702527031040 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.6563929319381714, loss=2.9304141998291016
I0201 19:15:31.445814 139702543816448 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7317266464233398, loss=2.6619839668273926
I0201 19:16:17.707864 139702527031040 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.6708869934082031, loss=3.2756285667419434
I0201 19:17:03.972147 139702543816448 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.6308194398880005, loss=2.652578830718994
I0201 19:17:49.535240 139702527031040 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.8254824876785278, loss=2.8076999187469482
I0201 19:18:35.618694 139702543816448 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.8311436176300049, loss=2.764017105102539
I0201 19:19:15.358994 139863983413056 spec.py:321] Evaluating on the training split.
I0201 19:19:25.467499 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 19:19:50.404903 139863983413056 spec.py:349] Evaluating on the test split.
I0201 19:19:52.052858 139863983413056 submission_runner.py:408] Time since start: 25626.84s, 	Step: 51688, 	{'train/accuracy': 0.6258788704872131, 'train/loss': 1.5507782697677612, 'validation/accuracy': 0.5779600143432617, 'validation/loss': 1.7888613939285278, 'validation/num_examples': 50000, 'test/accuracy': 0.4626000225543976, 'test/loss': 2.4470291137695312, 'test/num_examples': 10000, 'score': 23564.94721007347, 'total_duration': 25626.840389966965, 'accumulated_submission_time': 23564.94721007347, 'accumulated_eval_time': 2057.0334384441376, 'accumulated_logging_time': 2.137653350830078}
I0201 19:19:52.074907 139702527031040 logging_writer.py:48] [51688] accumulated_eval_time=2057.033438, accumulated_logging_time=2.137653, accumulated_submission_time=23564.947210, global_step=51688, preemption_count=0, score=23564.947210, test/accuracy=0.462600, test/loss=2.447029, test/num_examples=10000, total_duration=25626.840390, train/accuracy=0.625879, train/loss=1.550778, validation/accuracy=0.577960, validation/loss=1.788861, validation/num_examples=50000
I0201 19:19:57.261079 139702543816448 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.4757165908813477, loss=3.198899269104004
I0201 19:20:39.006777 139702527031040 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.3755474090576172, loss=5.207523822784424
I0201 19:21:24.942646 139702543816448 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.3933578729629517, loss=4.718279838562012
I0201 19:22:11.101719 139702527031040 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.5034968852996826, loss=5.33446741104126
I0201 19:22:56.795213 139702543816448 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.8326117992401123, loss=2.6410017013549805
I0201 19:23:42.697312 139702527031040 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.8293728828430176, loss=2.6385250091552734
I0201 19:24:29.420635 139702543816448 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.8450121879577637, loss=2.6198861598968506
I0201 19:25:15.075281 139702527031040 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.5466734170913696, loss=3.358236312866211
I0201 19:26:00.980186 139702543816448 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.7726542949676514, loss=2.559218406677246
I0201 19:26:47.293967 139702527031040 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.3822681903839111, loss=3.8665781021118164
I0201 19:26:52.487267 139863983413056 spec.py:321] Evaluating on the training split.
I0201 19:27:02.845444 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 19:27:28.673539 139863983413056 spec.py:349] Evaluating on the test split.
I0201 19:27:30.313932 139863983413056 submission_runner.py:408] Time since start: 26085.10s, 	Step: 52613, 	{'train/accuracy': 0.6236132383346558, 'train/loss': 1.5819681882858276, 'validation/accuracy': 0.5786799788475037, 'validation/loss': 1.7916535139083862, 'validation/num_examples': 50000, 'test/accuracy': 0.46310001611709595, 'test/loss': 2.449575424194336, 'test/num_examples': 10000, 'score': 23985.300182819366, 'total_duration': 26085.101460695267, 'accumulated_submission_time': 23985.300182819366, 'accumulated_eval_time': 2094.8600878715515, 'accumulated_logging_time': 2.1691770553588867}
I0201 19:27:30.338733 139702543816448 logging_writer.py:48] [52613] accumulated_eval_time=2094.860088, accumulated_logging_time=2.169177, accumulated_submission_time=23985.300183, global_step=52613, preemption_count=0, score=23985.300183, test/accuracy=0.463100, test/loss=2.449575, test/num_examples=10000, total_duration=26085.101461, train/accuracy=0.623613, train/loss=1.581968, validation/accuracy=0.578680, validation/loss=1.791654, validation/num_examples=50000
I0201 19:28:05.803482 139702527031040 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.4087507724761963, loss=5.208797454833984
I0201 19:28:51.567621 139702543816448 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8973841667175293, loss=2.6955816745758057
I0201 19:29:37.757843 139702527031040 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.7552438974380493, loss=2.43528151512146
I0201 19:30:23.981696 139702543816448 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.9058326482772827, loss=2.652106285095215
I0201 19:31:09.966407 139702527031040 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.7847925424575806, loss=2.7027742862701416
I0201 19:31:56.044113 139702543816448 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.7311532497406006, loss=2.7329862117767334
I0201 19:32:42.128457 139702527031040 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.6206867694854736, loss=2.703195571899414
I0201 19:33:27.994552 139702543816448 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.8553543090820312, loss=2.634798049926758
I0201 19:34:13.845467 139702527031040 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.5248749256134033, loss=5.2731733322143555
I0201 19:34:30.611176 139863983413056 spec.py:321] Evaluating on the training split.
I0201 19:34:40.897886 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 19:35:01.336818 139863983413056 spec.py:349] Evaluating on the test split.
I0201 19:35:02.975365 139863983413056 submission_runner.py:408] Time since start: 26537.76s, 	Step: 53538, 	{'train/accuracy': 0.6176171898841858, 'train/loss': 1.5891666412353516, 'validation/accuracy': 0.5787599682807922, 'validation/loss': 1.7716038227081299, 'validation/num_examples': 50000, 'test/accuracy': 0.46000000834465027, 'test/loss': 2.4227559566497803, 'test/num_examples': 10000, 'score': 24405.514179944992, 'total_duration': 26537.762898921967, 'accumulated_submission_time': 24405.514179944992, 'accumulated_eval_time': 2127.224277496338, 'accumulated_logging_time': 2.20285701751709}
I0201 19:35:02.999541 139702543816448 logging_writer.py:48] [53538] accumulated_eval_time=2127.224277, accumulated_logging_time=2.202857, accumulated_submission_time=24405.514180, global_step=53538, preemption_count=0, score=24405.514180, test/accuracy=0.460000, test/loss=2.422756, test/num_examples=10000, total_duration=26537.762899, train/accuracy=0.617617, train/loss=1.589167, validation/accuracy=0.578760, validation/loss=1.771604, validation/num_examples=50000
I0201 19:35:28.153073 139702527031040 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.3061516284942627, loss=5.022042274475098
I0201 19:36:12.463590 139702543816448 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.8343939781188965, loss=2.682645559310913
I0201 19:36:58.936506 139702527031040 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.5839300155639648, loss=3.2867226600646973
I0201 19:37:45.174463 139702543816448 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.5840564966201782, loss=3.5995676517486572
I0201 19:38:30.914720 139702527031040 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.8290674686431885, loss=2.6853160858154297
I0201 19:39:16.705564 139702543816448 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.8350398540496826, loss=2.6480109691619873
I0201 19:40:02.606866 139702527031040 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.7444958686828613, loss=2.534696578979492
I0201 19:40:48.553920 139702543816448 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.3049900531768799, loss=5.075965881347656
I0201 19:41:34.437949 139702527031040 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.7750129699707031, loss=4.319265365600586
I0201 19:42:03.161390 139863983413056 spec.py:321] Evaluating on the training split.
I0201 19:42:13.756110 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 19:42:37.761474 139863983413056 spec.py:349] Evaluating on the test split.
I0201 19:42:39.395015 139863983413056 submission_runner.py:408] Time since start: 26994.18s, 	Step: 54464, 	{'train/accuracy': 0.6196874976158142, 'train/loss': 1.6350294351577759, 'validation/accuracy': 0.5768199563026428, 'validation/loss': 1.8407946825027466, 'validation/num_examples': 50000, 'test/accuracy': 0.45350003242492676, 'test/loss': 2.493711471557617, 'test/num_examples': 10000, 'score': 24825.61731696129, 'total_duration': 26994.1825401783, 'accumulated_submission_time': 24825.61731696129, 'accumulated_eval_time': 2163.457891225815, 'accumulated_logging_time': 2.23595929145813}
I0201 19:42:39.419231 139702543816448 logging_writer.py:48] [54464] accumulated_eval_time=2163.457891, accumulated_logging_time=2.235959, accumulated_submission_time=24825.617317, global_step=54464, preemption_count=0, score=24825.617317, test/accuracy=0.453500, test/loss=2.493711, test/num_examples=10000, total_duration=26994.182540, train/accuracy=0.619687, train/loss=1.635029, validation/accuracy=0.576820, validation/loss=1.840795, validation/num_examples=50000
I0201 19:42:54.562713 139702527031040 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.3346176147460938, loss=4.849820137023926
I0201 19:43:37.447945 139702543816448 logging_writer.py:48] [54600] global_step=54600, grad_norm=2.156564235687256, loss=2.8523879051208496
I0201 19:44:24.287325 139702527031040 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.3402149677276611, loss=4.484653949737549
I0201 19:45:10.913575 139702543816448 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.7885910272598267, loss=2.974961519241333
I0201 19:45:56.998578 139702527031040 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.6210922002792358, loss=2.9241483211517334
I0201 19:46:43.043867 139702543816448 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.836044430732727, loss=2.565666437149048
I0201 19:47:29.644220 139702527031040 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.9468334913253784, loss=2.697401762008667
I0201 19:48:15.639254 139702543816448 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.4271812438964844, loss=5.265207767486572
I0201 19:49:01.769479 139702527031040 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.8714847564697266, loss=2.674750804901123
I0201 19:49:39.586442 139863983413056 spec.py:321] Evaluating on the training split.
I0201 19:49:50.230543 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 19:50:13.476987 139863983413056 spec.py:349] Evaluating on the test split.
I0201 19:50:15.119229 139863983413056 submission_runner.py:408] Time since start: 27449.91s, 	Step: 55384, 	{'train/accuracy': 0.6449999809265137, 'train/loss': 1.5044242143630981, 'validation/accuracy': 0.5765599608421326, 'validation/loss': 1.8060835599899292, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.4601993560791016, 'test/num_examples': 10000, 'score': 25245.340619325638, 'total_duration': 27449.90674352646, 'accumulated_submission_time': 25245.340619325638, 'accumulated_eval_time': 2198.990670442581, 'accumulated_logging_time': 2.656230926513672}
I0201 19:50:15.149862 139702543816448 logging_writer.py:48] [55384] accumulated_eval_time=2198.990670, accumulated_logging_time=2.656231, accumulated_submission_time=25245.340619, global_step=55384, preemption_count=0, score=25245.340619, test/accuracy=0.460400, test/loss=2.460199, test/num_examples=10000, total_duration=27449.906744, train/accuracy=0.645000, train/loss=1.504424, validation/accuracy=0.576560, validation/loss=1.806084, validation/num_examples=50000
I0201 19:50:21.936625 139702527031040 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.7010079622268677, loss=3.416116237640381
I0201 19:51:03.793879 139702543816448 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.4560128450393677, loss=4.763505935668945
I0201 19:51:49.727281 139702527031040 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.9195746183395386, loss=2.899414539337158
I0201 19:52:36.040152 139702543816448 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.9588834047317505, loss=2.6719412803649902
I0201 19:53:22.059421 139702527031040 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.5824036598205566, loss=3.231482982635498
I0201 19:54:07.841380 139702543816448 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.9401936531066895, loss=2.719938278198242
I0201 19:54:53.933049 139702527031040 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.8762081861495972, loss=2.889280080795288
I0201 19:55:40.198663 139702543816448 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.7814360857009888, loss=2.8634634017944336
I0201 19:56:25.851133 139702527031040 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.5936540365219116, loss=2.597130298614502
I0201 19:57:12.130511 139702543816448 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.7482984066009521, loss=2.5193135738372803
I0201 19:57:15.524398 139863983413056 spec.py:321] Evaluating on the training split.
I0201 19:57:25.910386 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 19:57:54.786213 139863983413056 spec.py:349] Evaluating on the test split.
I0201 19:57:56.429317 139863983413056 submission_runner.py:408] Time since start: 27911.22s, 	Step: 56309, 	{'train/accuracy': 0.621777355670929, 'train/loss': 1.5656689405441284, 'validation/accuracy': 0.5817599892616272, 'validation/loss': 1.7517997026443481, 'validation/num_examples': 50000, 'test/accuracy': 0.4602000117301941, 'test/loss': 2.4136600494384766, 'test/num_examples': 10000, 'score': 25665.65577197075, 'total_duration': 27911.216849565506, 'accumulated_submission_time': 25665.65577197075, 'accumulated_eval_time': 2239.8955862522125, 'accumulated_logging_time': 2.6976802349090576}
I0201 19:57:56.453519 139702527031040 logging_writer.py:48] [56309] accumulated_eval_time=2239.895586, accumulated_logging_time=2.697680, accumulated_submission_time=25665.655772, global_step=56309, preemption_count=0, score=25665.655772, test/accuracy=0.460200, test/loss=2.413660, test/num_examples=10000, total_duration=27911.216850, train/accuracy=0.621777, train/loss=1.565669, validation/accuracy=0.581760, validation/loss=1.751800, validation/num_examples=50000
I0201 19:58:34.156022 139702543816448 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.8573276996612549, loss=2.513836622238159
I0201 19:59:19.888669 139702527031040 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.8124929666519165, loss=2.5905399322509766
I0201 20:00:06.141690 139702543816448 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.8841242790222168, loss=2.610811710357666
I0201 20:00:52.098478 139702527031040 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.4152250289916992, loss=4.716179370880127
I0201 20:01:38.250450 139702543816448 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.9485031366348267, loss=2.63287091255188
I0201 20:02:24.233126 139702527031040 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.9770433902740479, loss=2.6206791400909424
I0201 20:03:10.229753 139702543816448 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.7703555822372437, loss=2.6449365615844727
I0201 20:03:56.135980 139702527031040 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.8620182275772095, loss=2.6304588317871094
I0201 20:04:42.199837 139702543816448 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.8429142236709595, loss=2.4842991828918457
I0201 20:04:56.929421 139863983413056 spec.py:321] Evaluating on the training split.
I0201 20:05:07.395498 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 20:05:32.782184 139863983413056 spec.py:349] Evaluating on the test split.
I0201 20:05:34.435919 139863983413056 submission_runner.py:408] Time since start: 28369.22s, 	Step: 57234, 	{'train/accuracy': 0.6290820240974426, 'train/loss': 1.5716264247894287, 'validation/accuracy': 0.5814799666404724, 'validation/loss': 1.7926048040390015, 'validation/num_examples': 50000, 'test/accuracy': 0.4628000259399414, 'test/loss': 2.429422378540039, 'test/num_examples': 10000, 'score': 26086.074779748917, 'total_duration': 28369.223430871964, 'accumulated_submission_time': 26086.074779748917, 'accumulated_eval_time': 2277.402089357376, 'accumulated_logging_time': 2.7310519218444824}
I0201 20:05:34.463541 139702527031040 logging_writer.py:48] [57234] accumulated_eval_time=2277.402089, accumulated_logging_time=2.731052, accumulated_submission_time=26086.074780, global_step=57234, preemption_count=0, score=26086.074780, test/accuracy=0.462800, test/loss=2.429422, test/num_examples=10000, total_duration=28369.223431, train/accuracy=0.629082, train/loss=1.571626, validation/accuracy=0.581480, validation/loss=1.792605, validation/num_examples=50000
I0201 20:06:01.189805 139702543816448 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.7164455652236938, loss=2.426474094390869
I0201 20:06:45.987698 139702527031040 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.6119270324707031, loss=3.0264852046966553
I0201 20:07:31.972229 139702543816448 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.4980942010879517, loss=4.232061862945557
I0201 20:08:18.765921 139702527031040 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.7724403142929077, loss=2.617171049118042
I0201 20:09:04.487608 139702543816448 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.6687633991241455, loss=2.9873690605163574
I0201 20:09:50.509070 139702527031040 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8712788820266724, loss=2.5286169052124023
I0201 20:10:36.339718 139702543816448 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.6588034629821777, loss=2.7374367713928223
I0201 20:11:22.146276 139702527031040 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.6527677774429321, loss=5.166220664978027
I0201 20:12:08.276294 139702543816448 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.411767840385437, loss=5.29878568649292
I0201 20:12:34.591317 139863983413056 spec.py:321] Evaluating on the training split.
I0201 20:12:44.865488 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 20:13:10.069503 139863983413056 spec.py:349] Evaluating on the test split.
I0201 20:13:11.711163 139863983413056 submission_runner.py:408] Time since start: 28826.50s, 	Step: 58159, 	{'train/accuracy': 0.6390234231948853, 'train/loss': 1.5097546577453613, 'validation/accuracy': 0.5824999809265137, 'validation/loss': 1.7665115594863892, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.406198501586914, 'test/num_examples': 10000, 'score': 26506.14146876335, 'total_duration': 28826.498693466187, 'accumulated_submission_time': 26506.14146876335, 'accumulated_eval_time': 2314.5219326019287, 'accumulated_logging_time': 2.77089786529541}
I0201 20:13:11.737904 139702527031040 logging_writer.py:48] [58159] accumulated_eval_time=2314.521933, accumulated_logging_time=2.770898, accumulated_submission_time=26506.141469, global_step=58159, preemption_count=0, score=26506.141469, test/accuracy=0.461700, test/loss=2.406199, test/num_examples=10000, total_duration=28826.498693, train/accuracy=0.639023, train/loss=1.509755, validation/accuracy=0.582500, validation/loss=1.766512, validation/num_examples=50000
I0201 20:13:28.493952 139702543816448 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.8006988763809204, loss=2.5623676776885986
I0201 20:14:11.903102 139702527031040 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.4220240116119385, loss=3.9178626537323
I0201 20:14:57.755757 139702543816448 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9987410306930542, loss=2.608748197555542
I0201 20:15:44.233690 139702527031040 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.0569262504577637, loss=2.712885618209839
I0201 20:16:30.054232 139702543816448 logging_writer.py:48] [58600] global_step=58600, grad_norm=2.15297532081604, loss=2.817805767059326
I0201 20:17:16.045125 139702527031040 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.3200434446334839, loss=5.193811416625977
I0201 20:18:02.409764 139702543816448 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.8935248851776123, loss=2.5492935180664062
I0201 20:18:48.352654 139702527031040 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.4456883668899536, loss=4.449765205383301
I0201 20:19:34.344916 139702543816448 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.4579604864120483, loss=4.2565999031066895
I0201 20:20:11.930849 139863983413056 spec.py:321] Evaluating on the training split.
I0201 20:20:22.345844 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 20:20:43.842656 139863983413056 spec.py:349] Evaluating on the test split.
I0201 20:20:45.487049 139863983413056 submission_runner.py:408] Time since start: 29280.27s, 	Step: 59083, 	{'train/accuracy': 0.6219140291213989, 'train/loss': 1.5699963569641113, 'validation/accuracy': 0.5807999968528748, 'validation/loss': 1.7605829238891602, 'validation/num_examples': 50000, 'test/accuracy': 0.46970000863075256, 'test/loss': 2.399991273880005, 'test/num_examples': 10000, 'score': 26926.27724289894, 'total_duration': 29280.27455854416, 'accumulated_submission_time': 26926.27724289894, 'accumulated_eval_time': 2348.0781197547913, 'accumulated_logging_time': 2.8062548637390137}
I0201 20:20:45.515462 139702527031040 logging_writer.py:48] [59083] accumulated_eval_time=2348.078120, accumulated_logging_time=2.806255, accumulated_submission_time=26926.277243, global_step=59083, preemption_count=0, score=26926.277243, test/accuracy=0.469700, test/loss=2.399991, test/num_examples=10000, total_duration=29280.274559, train/accuracy=0.621914, train/loss=1.569996, validation/accuracy=0.580800, validation/loss=1.760583, validation/num_examples=50000
I0201 20:20:52.712000 139702543816448 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.675558090209961, loss=4.707001686096191
I0201 20:21:34.901180 139702527031040 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.7319806814193726, loss=2.7892556190490723
I0201 20:22:20.871032 139702543816448 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8212685585021973, loss=2.630117654800415
I0201 20:23:07.060305 139702527031040 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.8236877918243408, loss=2.6074514389038086
I0201 20:23:52.912069 139702543816448 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.5675631761550903, loss=4.6229705810546875
I0201 20:24:38.928492 139702527031040 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.9373764991760254, loss=2.613921642303467
I0201 20:25:25.014329 139702543816448 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.682399034500122, loss=3.0603444576263428
I0201 20:26:11.136555 139702527031040 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.4739210605621338, loss=4.729747295379639
I0201 20:26:57.196875 139702543816448 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.9228664636611938, loss=2.5402145385742188
I0201 20:27:43.130275 139702527031040 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.72378671169281, loss=3.0709121227264404
I0201 20:27:45.562771 139863983413056 spec.py:321] Evaluating on the training split.
I0201 20:27:55.885638 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 20:28:19.800882 139863983413056 spec.py:349] Evaluating on the test split.
I0201 20:28:21.437229 139863983413056 submission_runner.py:408] Time since start: 29736.22s, 	Step: 60007, 	{'train/accuracy': 0.6282616853713989, 'train/loss': 1.5407322645187378, 'validation/accuracy': 0.5867399573326111, 'validation/loss': 1.7369784116744995, 'validation/num_examples': 50000, 'test/accuracy': 0.4677000343799591, 'test/loss': 2.3942172527313232, 'test/num_examples': 10000, 'score': 27346.266345262527, 'total_duration': 29736.22474217415, 'accumulated_submission_time': 27346.266345262527, 'accumulated_eval_time': 2383.9525430202484, 'accumulated_logging_time': 2.843522310256958}
I0201 20:28:21.466034 139702543816448 logging_writer.py:48] [60007] accumulated_eval_time=2383.952543, accumulated_logging_time=2.843522, accumulated_submission_time=27346.266345, global_step=60007, preemption_count=0, score=27346.266345, test/accuracy=0.467700, test/loss=2.394217, test/num_examples=10000, total_duration=29736.224742, train/accuracy=0.628262, train/loss=1.540732, validation/accuracy=0.586740, validation/loss=1.736978, validation/num_examples=50000
I0201 20:28:59.830747 139702527031040 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.6695901155471802, loss=3.5817384719848633
I0201 20:29:45.777104 139702543816448 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.653272271156311, loss=3.322538375854492
I0201 20:30:32.105941 139702527031040 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.7154202461242676, loss=2.5150177478790283
I0201 20:31:18.230402 139702543816448 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.7381582260131836, loss=3.5749528408050537
I0201 20:32:04.212208 139702527031040 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.700226902961731, loss=2.6893398761749268
I0201 20:32:50.058145 139702543816448 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.5011500120162964, loss=4.779260635375977
I0201 20:33:35.623644 139702527031040 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.9555885791778564, loss=2.650996685028076
I0201 20:34:21.459987 139702543816448 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.547560453414917, loss=4.225151062011719
I0201 20:35:07.407705 139702527031040 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.7022285461425781, loss=2.93583083152771
I0201 20:35:21.735363 139863983413056 spec.py:321] Evaluating on the training split.
I0201 20:35:32.019151 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 20:35:59.061450 139863983413056 spec.py:349] Evaluating on the test split.
I0201 20:36:00.705667 139863983413056 submission_runner.py:408] Time since start: 30195.49s, 	Step: 60933, 	{'train/accuracy': 0.6407226324081421, 'train/loss': 1.5198386907577515, 'validation/accuracy': 0.5875200033187866, 'validation/loss': 1.75584077835083, 'validation/num_examples': 50000, 'test/accuracy': 0.4727000296115875, 'test/loss': 2.403726816177368, 'test/num_examples': 10000, 'score': 27766.477121591568, 'total_duration': 30195.49317908287, 'accumulated_submission_time': 27766.477121591568, 'accumulated_eval_time': 2422.9228308200836, 'accumulated_logging_time': 2.881842613220215}
I0201 20:36:00.737964 139702543816448 logging_writer.py:48] [60933] accumulated_eval_time=2422.922831, accumulated_logging_time=2.881843, accumulated_submission_time=27766.477122, global_step=60933, preemption_count=0, score=27766.477122, test/accuracy=0.472700, test/loss=2.403727, test/num_examples=10000, total_duration=30195.493179, train/accuracy=0.640723, train/loss=1.519839, validation/accuracy=0.587520, validation/loss=1.755841, validation/num_examples=50000
I0201 20:36:27.862555 139702527031040 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.8383454084396362, loss=2.6734914779663086
I0201 20:37:12.668672 139702543816448 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.9147409200668335, loss=2.747396945953369
I0201 20:37:58.617583 139702527031040 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.8241685628890991, loss=2.5300357341766357
I0201 20:38:45.110062 139702543816448 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.834557294845581, loss=3.051450252532959
I0201 20:39:31.070502 139702527031040 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.6164205074310303, loss=2.747150421142578
I0201 20:40:17.142382 139702543816448 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.6312334537506104, loss=4.3875603675842285
I0201 20:41:03.466908 139702527031040 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.021710157394409, loss=2.586555004119873
I0201 20:41:49.169667 139702543816448 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.9692131280899048, loss=2.539685010910034
I0201 20:42:35.188134 139702527031040 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.7896277904510498, loss=2.788816452026367
I0201 20:43:01.135310 139863983413056 spec.py:321] Evaluating on the training split.
I0201 20:43:11.657198 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 20:43:34.309208 139863983413056 spec.py:349] Evaluating on the test split.
I0201 20:43:35.948011 139863983413056 submission_runner.py:408] Time since start: 30650.74s, 	Step: 61858, 	{'train/accuracy': 0.6363281011581421, 'train/loss': 1.5116547346115112, 'validation/accuracy': 0.5927199721336365, 'validation/loss': 1.7103793621063232, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.3759047985076904, 'test/num_examples': 10000, 'score': 28186.816585302353, 'total_duration': 30650.73554301262, 'accumulated_submission_time': 28186.816585302353, 'accumulated_eval_time': 2457.7355239391327, 'accumulated_logging_time': 2.923898696899414}
I0201 20:43:35.972421 139702543816448 logging_writer.py:48] [61858] accumulated_eval_time=2457.735524, accumulated_logging_time=2.923899, accumulated_submission_time=28186.816585, global_step=61858, preemption_count=0, score=28186.816585, test/accuracy=0.471600, test/loss=2.375905, test/num_examples=10000, total_duration=30650.735543, train/accuracy=0.636328, train/loss=1.511655, validation/accuracy=0.592720, validation/loss=1.710379, validation/num_examples=50000
I0201 20:43:53.152372 139702527031040 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.4379795789718628, loss=3.64766788482666
I0201 20:44:36.627711 139702543816448 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.9921027421951294, loss=2.626394033432007
I0201 20:45:22.815337 139702527031040 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.5188630819320679, loss=3.2220780849456787
I0201 20:46:09.089838 139702543816448 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.9414092302322388, loss=2.6203012466430664
I0201 20:46:54.906536 139702527031040 logging_writer.py:48] [62300] global_step=62300, grad_norm=2.0281083583831787, loss=2.5301105976104736
I0201 20:47:40.848843 139702543816448 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.9405025243759155, loss=2.6174509525299072
I0201 20:48:26.637568 139702527031040 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.6467210054397583, loss=5.038339614868164
I0201 20:49:13.079664 139702543816448 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.642220377922058, loss=2.9486141204833984
I0201 20:49:59.424838 139702527031040 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.686583399772644, loss=2.8317251205444336
I0201 20:50:36.276500 139863983413056 spec.py:321] Evaluating on the training split.
I0201 20:50:46.740818 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 20:51:09.020445 139863983413056 spec.py:349] Evaluating on the test split.
I0201 20:51:10.668071 139863983413056 submission_runner.py:408] Time since start: 31105.46s, 	Step: 62782, 	{'train/accuracy': 0.6325976252555847, 'train/loss': 1.5511209964752197, 'validation/accuracy': 0.5876799821853638, 'validation/loss': 1.7578150033950806, 'validation/num_examples': 50000, 'test/accuracy': 0.4659000337123871, 'test/loss': 2.4069180488586426, 'test/num_examples': 10000, 'score': 28607.062667131424, 'total_duration': 31105.45560526848, 'accumulated_submission_time': 28607.062667131424, 'accumulated_eval_time': 2492.127090215683, 'accumulated_logging_time': 2.957786798477173}
I0201 20:51:10.704260 139702543816448 logging_writer.py:48] [62782] accumulated_eval_time=2492.127090, accumulated_logging_time=2.957787, accumulated_submission_time=28607.062667, global_step=62782, preemption_count=0, score=28607.062667, test/accuracy=0.465900, test/loss=2.406918, test/num_examples=10000, total_duration=31105.455605, train/accuracy=0.632598, train/loss=1.551121, validation/accuracy=0.587680, validation/loss=1.757815, validation/num_examples=50000
I0201 20:51:18.292903 139702527031040 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.3495597839355469, loss=3.9968011379241943
I0201 20:52:00.321928 139702543816448 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.021810531616211, loss=2.7107813358306885
I0201 20:52:46.174556 139702527031040 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.8972618579864502, loss=3.7901360988616943
I0201 20:53:32.313185 139702543816448 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.836888313293457, loss=2.350850820541382
I0201 20:54:18.215763 139702527031040 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.5698223114013672, loss=5.253699779510498
I0201 20:55:04.137846 139702543816448 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.5340800285339355, loss=5.056475639343262
I0201 20:55:49.811167 139702527031040 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.6692215204238892, loss=2.9885025024414062
I0201 20:56:35.627774 139702543816448 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.7394073009490967, loss=2.7315566539764404
I0201 20:57:21.845010 139702527031040 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.8234786987304688, loss=3.051236152648926
I0201 20:58:07.737479 139702543816448 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.7745800018310547, loss=2.7614893913269043
I0201 20:58:11.018385 139863983413056 spec.py:321] Evaluating on the training split.
I0201 20:58:21.561752 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 20:58:47.575367 139863983413056 spec.py:349] Evaluating on the test split.
I0201 20:58:49.217278 139863983413056 submission_runner.py:408] Time since start: 31564.00s, 	Step: 63709, 	{'train/accuracy': 0.6412500143051147, 'train/loss': 1.4811878204345703, 'validation/accuracy': 0.594819962978363, 'validation/loss': 1.6979410648345947, 'validation/num_examples': 50000, 'test/accuracy': 0.47520002722740173, 'test/loss': 2.3501551151275635, 'test/num_examples': 10000, 'score': 29027.318502426147, 'total_duration': 31564.00478863716, 'accumulated_submission_time': 29027.318502426147, 'accumulated_eval_time': 2530.325961828232, 'accumulated_logging_time': 3.003383159637451}
I0201 20:58:49.249563 139702527031040 logging_writer.py:48] [63709] accumulated_eval_time=2530.325962, accumulated_logging_time=3.003383, accumulated_submission_time=29027.318502, global_step=63709, preemption_count=0, score=29027.318502, test/accuracy=0.475200, test/loss=2.350155, test/num_examples=10000, total_duration=31564.004789, train/accuracy=0.641250, train/loss=1.481188, validation/accuracy=0.594820, validation/loss=1.697941, validation/num_examples=50000
I0201 20:59:26.856159 139702543816448 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.7086286544799805, loss=2.5438621044158936
I0201 21:00:12.888315 139702527031040 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.6004782915115356, loss=3.1642775535583496
I0201 21:00:59.019047 139702543816448 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.435469388961792, loss=4.944472312927246
I0201 21:01:45.034843 139702527031040 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.4205965995788574, loss=5.18248987197876
I0201 21:02:31.117718 139702543816448 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.501539707183838, loss=3.813425064086914
I0201 21:03:17.123508 139702527031040 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.671386957168579, loss=4.887869834899902
I0201 21:04:03.030910 139702543816448 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.899159550666809, loss=2.601875066757202
I0201 21:04:48.843977 139702527031040 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.109485387802124, loss=2.534743547439575
I0201 21:05:34.847162 139702543816448 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.8539164066314697, loss=2.733582019805908
I0201 21:05:49.251266 139863983413056 spec.py:321] Evaluating on the training split.
I0201 21:05:59.585975 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 21:06:25.349506 139863983413056 spec.py:349] Evaluating on the test split.
I0201 21:06:26.995501 139863983413056 submission_runner.py:408] Time since start: 32021.78s, 	Step: 64633, 	{'train/accuracy': 0.6591210961341858, 'train/loss': 1.411446452140808, 'validation/accuracy': 0.5887599587440491, 'validation/loss': 1.7354134321212769, 'validation/num_examples': 50000, 'test/accuracy': 0.4707000255584717, 'test/loss': 2.3777599334716797, 'test/num_examples': 10000, 'score': 29447.261779785156, 'total_duration': 32021.783009290695, 'accumulated_submission_time': 29447.261779785156, 'accumulated_eval_time': 2568.0701701641083, 'accumulated_logging_time': 3.045332431793213}
I0201 21:06:27.025482 139702527031040 logging_writer.py:48] [64633] accumulated_eval_time=2568.070170, accumulated_logging_time=3.045332, accumulated_submission_time=29447.261780, global_step=64633, preemption_count=0, score=29447.261780, test/accuracy=0.470700, test/loss=2.377760, test/num_examples=10000, total_duration=32021.783009, train/accuracy=0.659121, train/loss=1.411446, validation/accuracy=0.588760, validation/loss=1.735413, validation/num_examples=50000
I0201 21:06:54.177772 139702543816448 logging_writer.py:48] [64700] global_step=64700, grad_norm=2.1116182804107666, loss=2.5429539680480957
I0201 21:07:39.221752 139702527031040 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.7766927480697632, loss=2.5996763706207275
I0201 21:08:25.268289 139702543816448 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.6509648561477661, loss=5.121983528137207
I0201 21:09:11.307686 139702527031040 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.6147387027740479, loss=5.169768810272217
I0201 21:09:57.562674 139702543816448 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.6018539667129517, loss=5.098976135253906
I0201 21:10:43.438381 139702527031040 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.717197299003601, loss=5.2375898361206055
I0201 21:11:29.090389 139702543816448 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.9977807998657227, loss=2.6767239570617676
I0201 21:12:14.792829 139702527031040 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.7448114156723022, loss=2.4377975463867188
I0201 21:13:00.535046 139702543816448 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.8444533348083496, loss=2.4915688037872314
I0201 21:13:27.337534 139863983413056 spec.py:321] Evaluating on the training split.
I0201 21:13:37.521594 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 21:14:02.427107 139863983413056 spec.py:349] Evaluating on the test split.
I0201 21:14:04.054738 139863983413056 submission_runner.py:408] Time since start: 32478.84s, 	Step: 65560, 	{'train/accuracy': 0.6402148008346558, 'train/loss': 1.5128191709518433, 'validation/accuracy': 0.5958399772644043, 'validation/loss': 1.723629355430603, 'validation/num_examples': 50000, 'test/accuracy': 0.47600001096725464, 'test/loss': 2.3619251251220703, 'test/num_examples': 10000, 'score': 29867.51460146904, 'total_duration': 32478.842272996902, 'accumulated_submission_time': 29867.51460146904, 'accumulated_eval_time': 2604.7873711586, 'accumulated_logging_time': 3.084872245788574}
I0201 21:14:04.079422 139702527031040 logging_writer.py:48] [65560] accumulated_eval_time=2604.787371, accumulated_logging_time=3.084872, accumulated_submission_time=29867.514601, global_step=65560, preemption_count=0, score=29867.514601, test/accuracy=0.476000, test/loss=2.361925, test/num_examples=10000, total_duration=32478.842273, train/accuracy=0.640215, train/loss=1.512819, validation/accuracy=0.595840, validation/loss=1.723629, validation/num_examples=50000
I0201 21:14:20.431434 139702543816448 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.5061094760894775, loss=5.143259525299072
I0201 21:15:03.839579 139702527031040 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.7866008281707764, loss=2.403999090194702
I0201 21:15:49.559149 139702543816448 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.744346261024475, loss=3.524502992630005
I0201 21:16:35.503716 139702527031040 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.4165598154067993, loss=5.115861415863037
I0201 21:17:21.323910 139702543816448 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.8802201747894287, loss=2.4448893070220947
I0201 21:18:07.164541 139702527031040 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.0837924480438232, loss=2.538367748260498
I0201 21:18:53.160162 139702543816448 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.781333565711975, loss=3.013692855834961
I0201 21:19:39.388177 139702527031040 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.694027304649353, loss=2.9512596130371094
I0201 21:20:25.681484 139702543816448 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.7791398763656616, loss=3.310058355331421
I0201 21:21:04.412238 139863983413056 spec.py:321] Evaluating on the training split.
I0201 21:21:14.762563 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 21:21:38.470658 139863983413056 spec.py:349] Evaluating on the test split.
I0201 21:21:40.120922 139863983413056 submission_runner.py:408] Time since start: 32934.91s, 	Step: 66486, 	{'train/accuracy': 0.6462304592132568, 'train/loss': 1.4637900590896606, 'validation/accuracy': 0.6012399792671204, 'validation/loss': 1.669540286064148, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.334697723388672, 'test/num_examples': 10000, 'score': 30287.78778719902, 'total_duration': 32934.90845179558, 'accumulated_submission_time': 30287.78778719902, 'accumulated_eval_time': 2640.4960482120514, 'accumulated_logging_time': 3.11910343170166}
I0201 21:21:40.146633 139702527031040 logging_writer.py:48] [66486] accumulated_eval_time=2640.496048, accumulated_logging_time=3.119103, accumulated_submission_time=30287.787787, global_step=66486, preemption_count=0, score=30287.787787, test/accuracy=0.479300, test/loss=2.334698, test/num_examples=10000, total_duration=32934.908452, train/accuracy=0.646230, train/loss=1.463790, validation/accuracy=0.601240, validation/loss=1.669540, validation/num_examples=50000
I0201 21:21:46.127148 139702543816448 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.4732611179351807, loss=4.8871588706970215
I0201 21:22:27.718285 139702527031040 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.8958719968795776, loss=2.4930009841918945
I0201 21:23:13.810586 139702543816448 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.977502465248108, loss=2.5523576736450195
I0201 21:24:00.062263 139702527031040 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.8720498085021973, loss=2.46621036529541
I0201 21:24:45.663166 139702543816448 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.4312797784805298, loss=4.754188537597656
I0201 21:25:31.634381 139702527031040 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.7387229204177856, loss=5.046111106872559
I0201 21:26:17.494971 139702543816448 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.3554116487503052, loss=4.043510437011719
I0201 21:27:03.329233 139702527031040 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.8627378940582275, loss=5.287075996398926
I0201 21:27:49.324833 139702543816448 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.9403272867202759, loss=2.5739848613739014
I0201 21:28:35.311054 139702527031040 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.8091802597045898, loss=3.0804171562194824
I0201 21:28:40.494931 139863983413056 spec.py:321] Evaluating on the training split.
I0201 21:28:51.079025 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 21:29:15.209033 139863983413056 spec.py:349] Evaluating on the test split.
I0201 21:29:16.855042 139863983413056 submission_runner.py:408] Time since start: 33391.64s, 	Step: 67413, 	{'train/accuracy': 0.6551952958106995, 'train/loss': 1.444200873374939, 'validation/accuracy': 0.6001799702644348, 'validation/loss': 1.7015769481658936, 'validation/num_examples': 50000, 'test/accuracy': 0.4799000322818756, 'test/loss': 2.3611066341400146, 'test/num_examples': 10000, 'score': 30708.076306819916, 'total_duration': 33391.64254951477, 'accumulated_submission_time': 30708.076306819916, 'accumulated_eval_time': 2676.85613656044, 'accumulated_logging_time': 3.1553802490234375}
I0201 21:29:16.884753 139702543816448 logging_writer.py:48] [67413] accumulated_eval_time=2676.856137, accumulated_logging_time=3.155380, accumulated_submission_time=30708.076307, global_step=67413, preemption_count=0, score=30708.076307, test/accuracy=0.479900, test/loss=2.361107, test/num_examples=10000, total_duration=33391.642550, train/accuracy=0.655195, train/loss=1.444201, validation/accuracy=0.600180, validation/loss=1.701577, validation/num_examples=50000
I0201 21:29:52.350049 139702527031040 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.6033943891525269, loss=3.588463306427002
I0201 21:30:38.342038 139702543816448 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.4999030828475952, loss=3.71755051612854
I0201 21:31:24.642243 139702527031040 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.5120478868484497, loss=3.824575424194336
I0201 21:32:10.603803 139702543816448 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.8949148654937744, loss=2.438938617706299
I0201 21:32:56.752641 139702527031040 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.9951878786087036, loss=2.6587579250335693
I0201 21:33:42.929308 139702543816448 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.709153652191162, loss=3.45859432220459
I0201 21:34:29.052860 139702527031040 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.5203020572662354, loss=4.949182987213135
I0201 21:35:15.318791 139702543816448 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.8400993347167969, loss=2.4455552101135254
I0201 21:36:01.603266 139702527031040 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.6747528314590454, loss=2.8022313117980957
I0201 21:36:17.012402 139863983413056 spec.py:321] Evaluating on the training split.
I0201 21:36:27.380685 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 21:36:52.222751 139863983413056 spec.py:349] Evaluating on the test split.
I0201 21:36:53.859467 139863983413056 submission_runner.py:408] Time since start: 33848.65s, 	Step: 68335, 	{'train/accuracy': 0.6418554782867432, 'train/loss': 1.4940626621246338, 'validation/accuracy': 0.6005799770355225, 'validation/loss': 1.6846686601638794, 'validation/num_examples': 50000, 'test/accuracy': 0.48170003294944763, 'test/loss': 2.3237926959991455, 'test/num_examples': 10000, 'score': 31128.145755052567, 'total_duration': 33848.646995306015, 'accumulated_submission_time': 31128.145755052567, 'accumulated_eval_time': 2713.703197956085, 'accumulated_logging_time': 3.194664239883423}
I0201 21:36:53.886938 139702543816448 logging_writer.py:48] [68335] accumulated_eval_time=2713.703198, accumulated_logging_time=3.194664, accumulated_submission_time=31128.145755, global_step=68335, preemption_count=0, score=31128.145755, test/accuracy=0.481700, test/loss=2.323793, test/num_examples=10000, total_duration=33848.646995, train/accuracy=0.641855, train/loss=1.494063, validation/accuracy=0.600580, validation/loss=1.684669, validation/num_examples=50000
I0201 21:37:20.216618 139702527031040 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.5476502180099487, loss=3.6803154945373535
I0201 21:38:05.009352 139702543816448 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.366884469985962, loss=4.820668697357178
I0201 21:38:51.125163 139702527031040 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.024341106414795, loss=2.571824073791504
I0201 21:39:37.385989 139702543816448 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.5484811067581177, loss=5.0173749923706055
I0201 21:40:23.724107 139702527031040 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.1564366817474365, loss=2.440725088119507
I0201 21:41:10.023118 139702543816448 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.8757964372634888, loss=2.4286787509918213
I0201 21:41:55.781099 139702527031040 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.8500643968582153, loss=2.482966899871826
I0201 21:42:41.800048 139702543816448 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.536723256111145, loss=3.2969970703125
I0201 21:43:28.022575 139702527031040 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.7918649911880493, loss=2.4768447875976562
I0201 21:43:54.240799 139863983413056 spec.py:321] Evaluating on the training split.
I0201 21:44:04.306479 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 21:44:28.676085 139863983413056 spec.py:349] Evaluating on the test split.
I0201 21:44:30.314184 139863983413056 submission_runner.py:408] Time since start: 34305.10s, 	Step: 69259, 	{'train/accuracy': 0.6425195336341858, 'train/loss': 1.519197940826416, 'validation/accuracy': 0.5944199562072754, 'validation/loss': 1.7298539876937866, 'validation/num_examples': 50000, 'test/accuracy': 0.47780001163482666, 'test/loss': 2.381270170211792, 'test/num_examples': 10000, 'score': 31548.4422082901, 'total_duration': 34305.10170960426, 'accumulated_submission_time': 31548.4422082901, 'accumulated_eval_time': 2749.776581287384, 'accumulated_logging_time': 3.231003999710083}
I0201 21:44:30.343402 139702543816448 logging_writer.py:48] [69259] accumulated_eval_time=2749.776581, accumulated_logging_time=3.231004, accumulated_submission_time=31548.442208, global_step=69259, preemption_count=0, score=31548.442208, test/accuracy=0.477800, test/loss=2.381270, test/num_examples=10000, total_duration=34305.101710, train/accuracy=0.642520, train/loss=1.519198, validation/accuracy=0.594420, validation/loss=1.729854, validation/num_examples=50000
I0201 21:44:47.097187 139702527031040 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.5142462253570557, loss=4.591647148132324
I0201 21:45:30.322753 139702543816448 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.5050513744354248, loss=5.078254699707031
I0201 21:46:16.258414 139702527031040 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.7616996765136719, loss=2.4677574634552
I0201 21:47:02.451042 139702543816448 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.2658090591430664, loss=2.3931374549865723
I0201 21:47:48.245133 139702527031040 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.7544423341751099, loss=2.345944881439209
I0201 21:48:34.183341 139702543816448 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.4357879161834717, loss=4.454511642456055
I0201 21:49:20.391157 139702527031040 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3603298664093018, loss=4.732402801513672
I0201 21:50:06.482537 139702543816448 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.99822199344635, loss=2.5404160022735596
I0201 21:50:52.718950 139702527031040 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.5953103303909302, loss=3.287997245788574
I0201 21:51:30.539135 139863983413056 spec.py:321] Evaluating on the training split.
I0201 21:51:40.734893 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 21:52:02.429609 139863983413056 spec.py:349] Evaluating on the test split.
I0201 21:52:04.067835 139863983413056 submission_runner.py:408] Time since start: 34758.86s, 	Step: 70184, 	{'train/accuracy': 0.6552343368530273, 'train/loss': 1.4339993000030518, 'validation/accuracy': 0.6019399762153625, 'validation/loss': 1.6860952377319336, 'validation/num_examples': 50000, 'test/accuracy': 0.48260003328323364, 'test/loss': 2.3297083377838135, 'test/num_examples': 10000, 'score': 31968.580335378647, 'total_duration': 34758.855362176895, 'accumulated_submission_time': 31968.580335378647, 'accumulated_eval_time': 2783.305285215378, 'accumulated_logging_time': 3.269001007080078}
I0201 21:52:04.093162 139702543816448 logging_writer.py:48] [70184] accumulated_eval_time=2783.305285, accumulated_logging_time=3.269001, accumulated_submission_time=31968.580335, global_step=70184, preemption_count=0, score=31968.580335, test/accuracy=0.482600, test/loss=2.329708, test/num_examples=10000, total_duration=34758.855362, train/accuracy=0.655234, train/loss=1.433999, validation/accuracy=0.601940, validation/loss=1.686095, validation/num_examples=50000
I0201 21:52:10.889599 139702527031040 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.7164376974105835, loss=2.9285738468170166
I0201 21:52:52.885236 139702543816448 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.8153328895568848, loss=2.39909029006958
I0201 21:53:38.801265 139702527031040 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.8817484378814697, loss=2.5085928440093994
I0201 21:54:25.104646 139702543816448 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.8183850049972534, loss=2.4457879066467285
I0201 21:55:10.995476 139702527031040 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.8895119428634644, loss=2.527132987976074
I0201 21:55:56.877474 139702543816448 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.772513747215271, loss=3.110560417175293
I0201 21:56:42.891762 139702527031040 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.467972755432129, loss=3.6792445182800293
I0201 21:57:28.649913 139702543816448 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.4330662488937378, loss=4.274659156799316
I0201 21:58:14.398715 139702527031040 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.6458740234375, loss=4.902593612670898
I0201 21:59:00.209091 139702543816448 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.4733725786209106, loss=4.068573951721191
I0201 21:59:04.485449 139863983413056 spec.py:321] Evaluating on the training split.
I0201 21:59:14.846399 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 21:59:37.850345 139863983413056 spec.py:349] Evaluating on the test split.
I0201 21:59:39.481137 139863983413056 submission_runner.py:408] Time since start: 35214.27s, 	Step: 71111, 	{'train/accuracy': 0.6490820050239563, 'train/loss': 1.4559168815612793, 'validation/accuracy': 0.6016600131988525, 'validation/loss': 1.6674160957336426, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.3340394496917725, 'test/num_examples': 10000, 'score': 32388.90991282463, 'total_duration': 35214.268662929535, 'accumulated_submission_time': 32388.90991282463, 'accumulated_eval_time': 2818.3009536266327, 'accumulated_logging_time': 3.307988405227661}
I0201 21:59:39.509799 139702527031040 logging_writer.py:48] [71111] accumulated_eval_time=2818.300954, accumulated_logging_time=3.307988, accumulated_submission_time=32388.909913, global_step=71111, preemption_count=0, score=32388.909913, test/accuracy=0.485000, test/loss=2.334039, test/num_examples=10000, total_duration=35214.268663, train/accuracy=0.649082, train/loss=1.455917, validation/accuracy=0.601660, validation/loss=1.667416, validation/num_examples=50000
I0201 22:00:15.922880 139702543816448 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.0401437282562256, loss=2.5351319313049316
I0201 22:01:01.900060 139702527031040 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.7134250402450562, loss=2.9185585975646973
I0201 22:01:48.145549 139702543816448 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.5220402479171753, loss=4.477433204650879
I0201 22:02:34.225411 139702527031040 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.7463675737380981, loss=2.6170520782470703
I0201 22:03:20.171866 139702543816448 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.1447625160217285, loss=2.428208827972412
I0201 22:04:06.002014 139702527031040 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.708310604095459, loss=2.9721179008483887
I0201 22:04:52.119179 139702543816448 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.7961851358413696, loss=5.110629081726074
I0201 22:05:37.832311 139702527031040 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.8533235788345337, loss=2.4153478145599365
I0201 22:06:23.697093 139702543816448 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.5219353437423706, loss=4.817187786102295
I0201 22:06:39.876715 139863983413056 spec.py:321] Evaluating on the training split.
I0201 22:06:50.466462 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 22:07:13.080114 139863983413056 spec.py:349] Evaluating on the test split.
I0201 22:07:14.720691 139863983413056 submission_runner.py:408] Time since start: 35669.51s, 	Step: 72037, 	{'train/accuracy': 0.6522070169448853, 'train/loss': 1.439338207244873, 'validation/accuracy': 0.6070799827575684, 'validation/loss': 1.6486047506332397, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.3152353763580322, 'test/num_examples': 10000, 'score': 32809.21870470047, 'total_duration': 35669.50822305679, 'accumulated_submission_time': 32809.21870470047, 'accumulated_eval_time': 2853.14493060112, 'accumulated_logging_time': 3.345659017562866}
I0201 22:07:14.748720 139702527031040 logging_writer.py:48] [72037] accumulated_eval_time=2853.144931, accumulated_logging_time=3.345659, accumulated_submission_time=32809.218705, global_step=72037, preemption_count=0, score=32809.218705, test/accuracy=0.486900, test/loss=2.315235, test/num_examples=10000, total_duration=35669.508223, train/accuracy=0.652207, train/loss=1.439338, validation/accuracy=0.607080, validation/loss=1.648605, validation/num_examples=50000
I0201 22:07:40.299134 139702543816448 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.7336125373840332, loss=2.782648801803589
I0201 22:08:24.832029 139702527031040 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.945736289024353, loss=2.802065134048462
I0201 22:09:10.787845 139702543816448 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.4345623254776, loss=4.865477561950684
I0201 22:09:57.155210 139702527031040 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.9173558950424194, loss=2.5091984272003174
I0201 22:10:43.083162 139702543816448 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.1704957485198975, loss=3.550307035446167
I0201 22:11:29.606542 139702527031040 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.5955859422683716, loss=4.959519863128662
I0201 22:12:15.798131 139702543816448 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.7387220859527588, loss=4.677105903625488
I0201 22:13:01.799445 139702527031040 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.8592493534088135, loss=4.885230541229248
I0201 22:13:47.750666 139702543816448 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.198240280151367, loss=2.5421652793884277
I0201 22:14:15.121022 139863983413056 spec.py:321] Evaluating on the training split.
I0201 22:14:26.099769 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 22:14:50.847448 139863983413056 spec.py:349] Evaluating on the test split.
I0201 22:14:52.486068 139863983413056 submission_runner.py:408] Time since start: 36127.27s, 	Step: 72961, 	{'train/accuracy': 0.6541991829872131, 'train/loss': 1.4503023624420166, 'validation/accuracy': 0.6034799814224243, 'validation/loss': 1.6737196445465088, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.3369715213775635, 'test/num_examples': 10000, 'score': 33229.53257513046, 'total_duration': 36127.273602962494, 'accumulated_submission_time': 33229.53257513046, 'accumulated_eval_time': 2890.5099818706512, 'accumulated_logging_time': 3.3830513954162598}
I0201 22:14:52.514960 139702527031040 logging_writer.py:48] [72961] accumulated_eval_time=2890.509982, accumulated_logging_time=3.383051, accumulated_submission_time=33229.532575, global_step=72961, preemption_count=0, score=33229.532575, test/accuracy=0.483400, test/loss=2.336972, test/num_examples=10000, total_duration=36127.273603, train/accuracy=0.654199, train/loss=1.450302, validation/accuracy=0.603480, validation/loss=1.673720, validation/num_examples=50000
I0201 22:15:08.493573 139702543816448 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.417912244796753, loss=4.654791831970215
I0201 22:15:51.724637 139702527031040 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.917574405670166, loss=2.4200170040130615
I0201 22:16:37.560338 139702543816448 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.8490965366363525, loss=2.5270867347717285
I0201 22:17:23.622246 139702527031040 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.9427776336669922, loss=2.517690658569336
I0201 22:18:09.346028 139702543816448 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.4772804975509644, loss=4.2664079666137695
I0201 22:18:55.144323 139702527031040 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.9828460216522217, loss=2.4601118564605713
I0201 22:19:41.460730 139702543816448 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.6674553155899048, loss=4.483460903167725
I0201 22:20:27.150520 139702527031040 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.5884015560150146, loss=3.624856472015381
I0201 22:21:13.381990 139702543816448 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.7741553783416748, loss=2.556358814239502
I0201 22:21:52.644900 139863983413056 spec.py:321] Evaluating on the training split.
I0201 22:22:03.142292 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 22:22:26.565244 139863983413056 spec.py:349] Evaluating on the test split.
I0201 22:22:28.210576 139863983413056 submission_runner.py:408] Time since start: 36583.00s, 	Step: 73887, 	{'train/accuracy': 0.6709960699081421, 'train/loss': 1.3740272521972656, 'validation/accuracy': 0.6000399589538574, 'validation/loss': 1.691603183746338, 'validation/num_examples': 50000, 'test/accuracy': 0.4837000370025635, 'test/loss': 2.336214303970337, 'test/num_examples': 10000, 'score': 33649.60353899002, 'total_duration': 36582.99809360504, 'accumulated_submission_time': 33649.60353899002, 'accumulated_eval_time': 2926.0756623744965, 'accumulated_logging_time': 3.4222118854522705}
I0201 22:22:28.240008 139702527031040 logging_writer.py:48] [73887] accumulated_eval_time=2926.075662, accumulated_logging_time=3.422212, accumulated_submission_time=33649.603539, global_step=73887, preemption_count=0, score=33649.603539, test/accuracy=0.483700, test/loss=2.336214, test/num_examples=10000, total_duration=36582.998094, train/accuracy=0.670996, train/loss=1.374027, validation/accuracy=0.600040, validation/loss=1.691603, validation/num_examples=50000
I0201 22:22:33.813249 139702543816448 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.8399909734725952, loss=2.3217430114746094
I0201 22:23:15.459383 139702527031040 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.7261284589767456, loss=2.930330753326416
I0201 22:24:01.495742 139702543816448 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.0021822452545166, loss=2.435401439666748
I0201 22:24:47.858495 139702527031040 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.8389685153961182, loss=2.4963572025299072
I0201 22:25:33.825337 139702543816448 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.646725058555603, loss=3.332718849182129
I0201 22:26:19.905735 139702527031040 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.9440772533416748, loss=2.498814344406128
I0201 22:27:06.041631 139702543816448 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.6959354877471924, loss=5.13661527633667
I0201 22:27:51.948937 139702527031040 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.806239366531372, loss=2.8553061485290527
I0201 22:28:37.856932 139702543816448 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.5853639841079712, loss=5.1563496589660645
I0201 22:29:23.928906 139702527031040 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.0083627700805664, loss=2.5226521492004395
I0201 22:29:28.229890 139863983413056 spec.py:321] Evaluating on the training split.
I0201 22:29:38.477252 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 22:30:01.012331 139863983413056 spec.py:349] Evaluating on the test split.
I0201 22:30:02.657865 139863983413056 submission_runner.py:408] Time since start: 37037.45s, 	Step: 74811, 	{'train/accuracy': 0.6478906273841858, 'train/loss': 1.4585990905761719, 'validation/accuracy': 0.6050599813461304, 'validation/loss': 1.6543669700622559, 'validation/num_examples': 50000, 'test/accuracy': 0.48850002884864807, 'test/loss': 2.2894341945648193, 'test/num_examples': 10000, 'score': 34069.536211013794, 'total_duration': 37037.44539427757, 'accumulated_submission_time': 34069.536211013794, 'accumulated_eval_time': 2960.50363445282, 'accumulated_logging_time': 3.460890054702759}
I0201 22:30:02.685173 139702543816448 logging_writer.py:48] [74811] accumulated_eval_time=2960.503634, accumulated_logging_time=3.460890, accumulated_submission_time=34069.536211, global_step=74811, preemption_count=0, score=34069.536211, test/accuracy=0.488500, test/loss=2.289434, test/num_examples=10000, total_duration=37037.445394, train/accuracy=0.647891, train/loss=1.458599, validation/accuracy=0.605060, validation/loss=1.654367, validation/num_examples=50000
I0201 22:30:39.265791 139702527031040 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.5783720016479492, loss=3.995708703994751
I0201 22:31:24.863386 139702543816448 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.7560429573059082, loss=3.251025676727295
I0201 22:32:11.430557 139702527031040 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.9048537015914917, loss=2.4953246116638184
I0201 22:32:57.664910 139702543816448 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.0072293281555176, loss=2.4564132690429688
I0201 22:33:43.477989 139702527031040 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.8219574689865112, loss=2.408298969268799
I0201 22:34:29.978807 139702543816448 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.4011437892913818, loss=4.882656097412109
I0201 22:35:15.850979 139702527031040 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.0194952487945557, loss=2.4801857471466064
I0201 22:36:01.812070 139702543816448 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.890311360359192, loss=2.934156894683838
I0201 22:36:47.961631 139702527031040 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.9468097686767578, loss=2.454643487930298
I0201 22:37:02.787266 139863983413056 spec.py:321] Evaluating on the training split.
I0201 22:37:13.044237 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 22:37:37.930915 139863983413056 spec.py:349] Evaluating on the test split.
I0201 22:37:39.558718 139863983413056 submission_runner.py:408] Time since start: 37494.35s, 	Step: 75734, 	{'train/accuracy': 0.6602148413658142, 'train/loss': 1.4000751972198486, 'validation/accuracy': 0.6098600029945374, 'validation/loss': 1.6153631210327148, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.2635269165039062, 'test/num_examples': 10000, 'score': 34489.581547021866, 'total_duration': 37494.34624528885, 'accumulated_submission_time': 34489.581547021866, 'accumulated_eval_time': 2997.275089740753, 'accumulated_logging_time': 3.4971187114715576}
I0201 22:37:39.584772 139702543816448 logging_writer.py:48] [75734] accumulated_eval_time=2997.275090, accumulated_logging_time=3.497119, accumulated_submission_time=34489.581547, global_step=75734, preemption_count=0, score=34489.581547, test/accuracy=0.495100, test/loss=2.263527, test/num_examples=10000, total_duration=37494.346245, train/accuracy=0.660215, train/loss=1.400075, validation/accuracy=0.609860, validation/loss=1.615363, validation/num_examples=50000
I0201 22:38:06.348692 139702527031040 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.7166309356689453, loss=2.9973196983337402
I0201 22:38:51.302801 139702543816448 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.8302710056304932, loss=2.4305691719055176
I0201 22:39:37.232713 139702527031040 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.9336432218551636, loss=2.387544870376587
I0201 22:40:23.793483 139702543816448 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.9091181755065918, loss=2.650557279586792
I0201 22:41:09.514076 139702527031040 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.516860842704773, loss=4.874723434448242
I0201 22:41:55.348570 139702543816448 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.8335965871810913, loss=2.5809996128082275
I0201 22:42:41.692589 139702527031040 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.0350558757781982, loss=2.4758028984069824
I0201 22:43:27.471987 139702543816448 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.946516990661621, loss=2.893648147583008
I0201 22:44:13.475353 139702527031040 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.8208874464035034, loss=5.0847930908203125
I0201 22:44:39.690556 139863983413056 spec.py:321] Evaluating on the training split.
I0201 22:44:49.911869 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 22:45:16.281451 139863983413056 spec.py:349] Evaluating on the test split.
I0201 22:45:17.923905 139863983413056 submission_runner.py:408] Time since start: 37952.71s, 	Step: 76659, 	{'train/accuracy': 0.6640429496765137, 'train/loss': 1.389413833618164, 'validation/accuracy': 0.6104399561882019, 'validation/loss': 1.645629644393921, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.289534568786621, 'test/num_examples': 10000, 'score': 34909.62964272499, 'total_duration': 37952.71143436432, 'accumulated_submission_time': 34909.62964272499, 'accumulated_eval_time': 3035.50843667984, 'accumulated_logging_time': 3.5318963527679443}
I0201 22:45:17.952420 139702543816448 logging_writer.py:48] [76659] accumulated_eval_time=3035.508437, accumulated_logging_time=3.531896, accumulated_submission_time=34909.629643, global_step=76659, preemption_count=0, score=34909.629643, test/accuracy=0.491500, test/loss=2.289535, test/num_examples=10000, total_duration=37952.711434, train/accuracy=0.664043, train/loss=1.389414, validation/accuracy=0.610440, validation/loss=1.645630, validation/num_examples=50000
I0201 22:45:34.740059 139702527031040 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.9372342824935913, loss=2.519752025604248
I0201 22:46:18.144204 139702543816448 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.7433604001998901, loss=3.123722791671753
I0201 22:47:04.027982 139702527031040 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.1109890937805176, loss=2.455199956893921
I0201 22:47:50.084278 139702543816448 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.9002704620361328, loss=2.4344522953033447
I0201 22:48:35.950450 139702527031040 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.890620470046997, loss=2.8310978412628174
I0201 22:49:21.731544 139702543816448 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.8374725580215454, loss=5.170588970184326
I0201 22:50:07.790302 139702527031040 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.7115567922592163, loss=3.7333462238311768
I0201 22:50:53.495486 139702543816448 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.2481276988983154, loss=2.520252227783203
I0201 22:51:39.387755 139702527031040 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.6743873357772827, loss=3.6372151374816895
I0201 22:52:18.277128 139863983413056 spec.py:321] Evaluating on the training split.
I0201 22:52:28.436525 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 22:52:52.596657 139863983413056 spec.py:349] Evaluating on the test split.
I0201 22:52:54.231381 139863983413056 submission_runner.py:408] Time since start: 38409.02s, 	Step: 77586, 	{'train/accuracy': 0.6577734351158142, 'train/loss': 1.4027554988861084, 'validation/accuracy': 0.6133599877357483, 'validation/loss': 1.6025428771972656, 'validation/num_examples': 50000, 'test/accuracy': 0.4926000237464905, 'test/loss': 2.264949083328247, 'test/num_examples': 10000, 'score': 35329.89449548721, 'total_duration': 38409.01891493797, 'accumulated_submission_time': 35329.89449548721, 'accumulated_eval_time': 3071.4626858234406, 'accumulated_logging_time': 3.5703811645507812}
I0201 22:52:54.260011 139702543816448 logging_writer.py:48] [77586] accumulated_eval_time=3071.462686, accumulated_logging_time=3.570381, accumulated_submission_time=35329.894495, global_step=77586, preemption_count=0, score=35329.894495, test/accuracy=0.492600, test/loss=2.264949, test/num_examples=10000, total_duration=38409.018915, train/accuracy=0.657773, train/loss=1.402755, validation/accuracy=0.613360, validation/loss=1.602543, validation/num_examples=50000
I0201 22:53:00.250614 139702527031040 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.864017367362976, loss=2.4411728382110596
I0201 22:53:41.872176 139702543816448 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.0025227069854736, loss=2.6408159732818604
I0201 22:54:27.797888 139702527031040 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.0944788455963135, loss=2.422717809677124
I0201 22:55:14.117151 139702543816448 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.0283396244049072, loss=2.4271974563598633
I0201 22:56:00.385519 139702527031040 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.829429030418396, loss=4.057861328125
I0201 22:56:46.165370 139702543816448 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.082490921020508, loss=2.520460367202759
I0201 22:57:32.061788 139702527031040 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.0191879272460938, loss=2.380354404449463
I0201 22:58:17.955456 139702543816448 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.7879624366760254, loss=2.5444114208221436
I0201 22:59:04.066442 139702527031040 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.6977425813674927, loss=4.485599517822266
I0201 22:59:49.975366 139702543816448 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.9176874160766602, loss=2.550971269607544
I0201 22:59:54.271820 139863983413056 spec.py:321] Evaluating on the training split.
I0201 23:00:04.790713 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 23:00:27.611512 139863983413056 spec.py:349] Evaluating on the test split.
I0201 23:00:29.246446 139863983413056 submission_runner.py:408] Time since start: 38864.03s, 	Step: 78511, 	{'train/accuracy': 0.6576171517372131, 'train/loss': 1.4034545421600342, 'validation/accuracy': 0.6142399907112122, 'validation/loss': 1.60440194606781, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.2740235328674316, 'test/num_examples': 10000, 'score': 35749.847000837326, 'total_duration': 38864.033963918686, 'accumulated_submission_time': 35749.847000837326, 'accumulated_eval_time': 3106.437283039093, 'accumulated_logging_time': 3.6099631786346436}
I0201 23:00:29.272681 139702527031040 logging_writer.py:48] [78511] accumulated_eval_time=3106.437283, accumulated_logging_time=3.609963, accumulated_submission_time=35749.847001, global_step=78511, preemption_count=0, score=35749.847001, test/accuracy=0.491900, test/loss=2.274024, test/num_examples=10000, total_duration=38864.033964, train/accuracy=0.657617, train/loss=1.403455, validation/accuracy=0.614240, validation/loss=1.604402, validation/num_examples=50000
I0201 23:01:05.562368 139702543816448 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.954119086265564, loss=2.418712615966797
I0201 23:01:51.645867 139702527031040 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.1135902404785156, loss=2.3924989700317383
I0201 23:02:37.672396 139702543816448 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.746825933456421, loss=2.8722028732299805
I0201 23:03:23.828678 139702527031040 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.5390453338623047, loss=3.7352914810180664
I0201 23:04:09.694474 139702543816448 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.044935464859009, loss=2.6119544506073
I0201 23:04:55.740233 139702527031040 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.033038854598999, loss=2.364145278930664
I0201 23:05:41.842089 139702543816448 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.730468511581421, loss=4.008373737335205
I0201 23:06:27.781907 139702527031040 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.8837394714355469, loss=2.456789255142212
I0201 23:07:13.720886 139702543816448 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.9222055673599243, loss=2.343952178955078
I0201 23:07:29.527328 139863983413056 spec.py:321] Evaluating on the training split.
I0201 23:07:39.798879 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 23:08:00.679604 139863983413056 spec.py:349] Evaluating on the test split.
I0201 23:08:02.323637 139863983413056 submission_runner.py:408] Time since start: 39317.11s, 	Step: 79436, 	{'train/accuracy': 0.6655468344688416, 'train/loss': 1.3878076076507568, 'validation/accuracy': 0.6120399832725525, 'validation/loss': 1.6240185499191284, 'validation/num_examples': 50000, 'test/accuracy': 0.4887000322341919, 'test/loss': 2.2839529514312744, 'test/num_examples': 10000, 'score': 36170.04280281067, 'total_duration': 39317.1111471653, 'accumulated_submission_time': 36170.04280281067, 'accumulated_eval_time': 3139.2335624694824, 'accumulated_logging_time': 3.6466240882873535}
I0201 23:08:02.353680 139702527031040 logging_writer.py:48] [79436] accumulated_eval_time=3139.233562, accumulated_logging_time=3.646624, accumulated_submission_time=36170.042803, global_step=79436, preemption_count=0, score=36170.042803, test/accuracy=0.488700, test/loss=2.283953, test/num_examples=10000, total_duration=39317.111147, train/accuracy=0.665547, train/loss=1.387808, validation/accuracy=0.612040, validation/loss=1.624019, validation/num_examples=50000
I0201 23:08:28.466334 139702543816448 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.7562793493270874, loss=2.4071266651153564
I0201 23:09:13.395815 139702527031040 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.8631303310394287, loss=2.680725574493408
I0201 23:09:59.669524 139702543816448 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.6427068710327148, loss=3.3876922130584717
I0201 23:10:46.086568 139702527031040 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.5328890085220337, loss=4.53659200668335
I0201 23:11:31.880035 139702543816448 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.6471877098083496, loss=3.7774288654327393
I0201 23:12:17.771783 139702527031040 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.8292672634124756, loss=2.8256633281707764
I0201 23:13:03.769514 139702543816448 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.9966988563537598, loss=2.42887544631958
I0201 23:13:49.711591 139702527031040 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.0129201412200928, loss=2.3136510848999023
I0201 23:14:35.605880 139702543816448 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.8137515783309937, loss=2.9599356651306152
I0201 23:15:02.455897 139863983413056 spec.py:321] Evaluating on the training split.
I0201 23:15:12.744594 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 23:15:37.188717 139863983413056 spec.py:349] Evaluating on the test split.
I0201 23:15:38.830150 139863983413056 submission_runner.py:408] Time since start: 39773.62s, 	Step: 80360, 	{'train/accuracy': 0.6602929830551147, 'train/loss': 1.4149186611175537, 'validation/accuracy': 0.6128799915313721, 'validation/loss': 1.6237813234329224, 'validation/num_examples': 50000, 'test/accuracy': 0.49820002913475037, 'test/loss': 2.274289131164551, 'test/num_examples': 10000, 'score': 36590.088024139404, 'total_duration': 39773.61768245697, 'accumulated_submission_time': 36590.088024139404, 'accumulated_eval_time': 3175.6078023910522, 'accumulated_logging_time': 3.685581922531128}
I0201 23:15:38.856909 139702527031040 logging_writer.py:48] [80360] accumulated_eval_time=3175.607802, accumulated_logging_time=3.685582, accumulated_submission_time=36590.088024, global_step=80360, preemption_count=0, score=36590.088024, test/accuracy=0.498200, test/loss=2.274289, test/num_examples=10000, total_duration=39773.617682, train/accuracy=0.660293, train/loss=1.414919, validation/accuracy=0.612880, validation/loss=1.623781, validation/num_examples=50000
I0201 23:15:55.215240 139702543816448 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.6874010562896729, loss=3.5204336643218994
I0201 23:16:38.477038 139702527031040 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.035569190979004, loss=2.350552558898926
I0201 23:17:24.529175 139702543816448 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.6914517879486084, loss=5.029158115386963
I0201 23:18:10.865880 139702527031040 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.9007697105407715, loss=2.4399142265319824
I0201 23:18:56.602786 139702543816448 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.8203085660934448, loss=2.6330137252807617
I0201 23:19:42.700035 139702527031040 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.8536081314086914, loss=2.8258321285247803
I0201 23:20:29.198390 139702543816448 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.6971166133880615, loss=2.878359794616699
I0201 23:21:15.077873 139702527031040 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.21327805519104, loss=2.3221046924591064
I0201 23:22:01.044005 139702543816448 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.6508300304412842, loss=4.315886974334717
I0201 23:22:38.902799 139863983413056 spec.py:321] Evaluating on the training split.
I0201 23:22:49.306139 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 23:23:14.402493 139863983413056 spec.py:349] Evaluating on the test split.
I0201 23:23:16.045935 139863983413056 submission_runner.py:408] Time since start: 40230.83s, 	Step: 81284, 	{'train/accuracy': 0.6623827815055847, 'train/loss': 1.4213883876800537, 'validation/accuracy': 0.6172999739646912, 'validation/loss': 1.631671667098999, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.2799715995788574, 'test/num_examples': 10000, 'score': 37010.074617385864, 'total_duration': 40230.83346366882, 'accumulated_submission_time': 37010.074617385864, 'accumulated_eval_time': 3212.7509384155273, 'accumulated_logging_time': 3.7231364250183105}
I0201 23:23:16.073621 139702527031040 logging_writer.py:48] [81284] accumulated_eval_time=3212.750938, accumulated_logging_time=3.723136, accumulated_submission_time=37010.074617, global_step=81284, preemption_count=0, score=37010.074617, test/accuracy=0.491400, test/loss=2.279972, test/num_examples=10000, total_duration=40230.833464, train/accuracy=0.662383, train/loss=1.421388, validation/accuracy=0.617300, validation/loss=1.631672, validation/num_examples=50000
I0201 23:23:22.862989 139702543816448 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.5127533674240112, loss=4.502079963684082
I0201 23:24:04.801038 139702527031040 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.7192091941833496, loss=4.978181838989258
I0201 23:24:50.693645 139702543816448 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.655794382095337, loss=3.4983603954315186
I0201 23:25:37.017693 139702527031040 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.5858882665634155, loss=3.298015594482422
I0201 23:26:23.003676 139702543816448 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.790907859802246, loss=3.990938186645508
I0201 23:27:08.953660 139702527031040 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.0208194255828857, loss=2.5399246215820312
I0201 23:27:55.099069 139702543816448 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.171882390975952, loss=2.373774528503418
I0201 23:28:40.931862 139702527031040 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.9677631855010986, loss=2.386673927307129
I0201 23:29:27.085304 139702543816448 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.946465253829956, loss=2.536677360534668
I0201 23:30:13.339008 139702527031040 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.6186797618865967, loss=3.6169190406799316
I0201 23:30:16.248043 139863983413056 spec.py:321] Evaluating on the training split.
I0201 23:30:26.627673 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 23:30:50.545064 139863983413056 spec.py:349] Evaluating on the test split.
I0201 23:30:52.180990 139863983413056 submission_runner.py:408] Time since start: 40686.97s, 	Step: 82208, 	{'train/accuracy': 0.6692187190055847, 'train/loss': 1.3704071044921875, 'validation/accuracy': 0.6172400116920471, 'validation/loss': 1.5985249280929565, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.2693800926208496, 'test/num_examples': 10000, 'score': 37430.19033193588, 'total_duration': 40686.96852493286, 'accumulated_submission_time': 37430.19033193588, 'accumulated_eval_time': 3248.6838982105255, 'accumulated_logging_time': 3.760983467102051}
I0201 23:30:52.207732 139702543816448 logging_writer.py:48] [82208] accumulated_eval_time=3248.683898, accumulated_logging_time=3.760983, accumulated_submission_time=37430.190332, global_step=82208, preemption_count=0, score=37430.190332, test/accuracy=0.494500, test/loss=2.269380, test/num_examples=10000, total_duration=40686.968525, train/accuracy=0.669219, train/loss=1.370407, validation/accuracy=0.617240, validation/loss=1.598525, validation/num_examples=50000
I0201 23:31:29.906924 139702527031040 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.9115113019943237, loss=3.2561378479003906
I0201 23:32:15.995467 139702543816448 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.093792200088501, loss=2.491114377975464
I0201 23:33:02.128734 139702527031040 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.0224461555480957, loss=2.3746838569641113
I0201 23:33:48.309630 139702543816448 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.9204480648040771, loss=2.968130111694336
I0201 23:34:34.225799 139702527031040 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.568731665611267, loss=3.752507448196411
I0201 23:35:20.201902 139702543816448 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.597007155418396, loss=3.750500440597534
I0201 23:36:05.973473 139702527031040 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.543147325515747, loss=4.6161274909973145
I0201 23:36:51.839940 139702543816448 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.0789954662323, loss=2.4177591800689697
I0201 23:37:38.028758 139702527031040 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.8071272373199463, loss=4.8805155754089355
I0201 23:37:52.310359 139863983413056 spec.py:321] Evaluating on the training split.
I0201 23:38:02.690443 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 23:38:28.077584 139863983413056 spec.py:349] Evaluating on the test split.
I0201 23:38:29.713324 139863983413056 submission_runner.py:408] Time since start: 41144.50s, 	Step: 83133, 	{'train/accuracy': 0.6871874928474426, 'train/loss': 1.3113399744033813, 'validation/accuracy': 0.6131600141525269, 'validation/loss': 1.6342535018920898, 'validation/num_examples': 50000, 'test/accuracy': 0.4952000379562378, 'test/loss': 2.2838222980499268, 'test/num_examples': 10000, 'score': 37850.235020160675, 'total_duration': 41144.50082588196, 'accumulated_submission_time': 37850.235020160675, 'accumulated_eval_time': 3286.08683013916, 'accumulated_logging_time': 3.796276569366455}
I0201 23:38:29.747169 139702543816448 logging_writer.py:48] [83133] accumulated_eval_time=3286.086830, accumulated_logging_time=3.796277, accumulated_submission_time=37850.235020, global_step=83133, preemption_count=0, score=37850.235020, test/accuracy=0.495200, test/loss=2.283822, test/num_examples=10000, total_duration=41144.500826, train/accuracy=0.687187, train/loss=1.311340, validation/accuracy=0.613160, validation/loss=1.634254, validation/num_examples=50000
I0201 23:38:56.903765 139702527031040 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.104719877243042, loss=2.427119255065918
I0201 23:39:41.875614 139702543816448 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.2073044776916504, loss=2.4410905838012695
I0201 23:40:28.242205 139702527031040 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.891505241394043, loss=2.5136303901672363
I0201 23:41:14.536497 139702543816448 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.1705987453460693, loss=2.3764443397521973
I0201 23:42:00.167912 139702527031040 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.9796947240829468, loss=2.7774930000305176
I0201 23:42:46.516529 139702543816448 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.8223775625228882, loss=2.8870348930358887
I0201 23:43:32.278844 139702527031040 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.8736704587936401, loss=2.4821527004241943
I0201 23:44:18.557443 139702543816448 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.7341138124465942, loss=4.498500823974609
I0201 23:45:04.595006 139702527031040 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.0051543712615967, loss=2.3810126781463623
I0201 23:45:30.132467 139863983413056 spec.py:321] Evaluating on the training split.
I0201 23:45:40.528893 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 23:46:04.561195 139863983413056 spec.py:349] Evaluating on the test split.
I0201 23:46:06.196376 139863983413056 submission_runner.py:408] Time since start: 41600.98s, 	Step: 84057, 	{'train/accuracy': 0.6665624976158142, 'train/loss': 1.3889672756195068, 'validation/accuracy': 0.6204999685287476, 'validation/loss': 1.5967681407928467, 'validation/num_examples': 50000, 'test/accuracy': 0.5052000284194946, 'test/loss': 2.2316551208496094, 'test/num_examples': 10000, 'score': 38270.56211447716, 'total_duration': 41600.98387527466, 'accumulated_submission_time': 38270.56211447716, 'accumulated_eval_time': 3322.1507127285004, 'accumulated_logging_time': 3.8398702144622803}
I0201 23:46:06.223308 139702543816448 logging_writer.py:48] [84057] accumulated_eval_time=3322.150713, accumulated_logging_time=3.839870, accumulated_submission_time=38270.562114, global_step=84057, preemption_count=0, score=38270.562114, test/accuracy=0.505200, test/loss=2.231655, test/num_examples=10000, total_duration=41600.983875, train/accuracy=0.666562, train/loss=1.388967, validation/accuracy=0.620500, validation/loss=1.596768, validation/num_examples=50000
I0201 23:46:24.003888 139702527031040 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.1304609775543213, loss=2.309981107711792
I0201 23:47:07.482172 139702543816448 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.081399440765381, loss=2.6425130367279053
I0201 23:47:53.500569 139702527031040 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.914947271347046, loss=2.276350975036621
I0201 23:48:39.748089 139702543816448 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.114096164703369, loss=2.423872470855713
I0201 23:49:25.424901 139702527031040 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.0992417335510254, loss=2.3177895545959473
I0201 23:50:11.554018 139702543816448 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.082446813583374, loss=2.5092623233795166
I0201 23:50:57.471254 139702527031040 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.6417659521102905, loss=3.907756805419922
I0201 23:51:43.298516 139702543816448 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.617884397506714, loss=2.5195846557617188
I0201 23:52:29.499945 139702527031040 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.982927918434143, loss=2.2670938968658447
I0201 23:53:06.305104 139863983413056 spec.py:321] Evaluating on the training split.
I0201 23:53:17.016090 139863983413056 spec.py:333] Evaluating on the validation split.
I0201 23:53:40.439515 139863983413056 spec.py:349] Evaluating on the test split.
I0201 23:53:42.077318 139863983413056 submission_runner.py:408] Time since start: 42056.86s, 	Step: 84982, 	{'train/accuracy': 0.6727148294448853, 'train/loss': 1.3522733449935913, 'validation/accuracy': 0.6233800053596497, 'validation/loss': 1.5781606435775757, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.2197320461273193, 'test/num_examples': 10000, 'score': 38690.585122823715, 'total_duration': 42056.864844083786, 'accumulated_submission_time': 38690.585122823715, 'accumulated_eval_time': 3357.9229278564453, 'accumulated_logging_time': 3.876704692840576}
I0201 23:53:42.107258 139702543816448 logging_writer.py:48] [84982] accumulated_eval_time=3357.922928, accumulated_logging_time=3.876705, accumulated_submission_time=38690.585123, global_step=84982, preemption_count=0, score=38690.585123, test/accuracy=0.504300, test/loss=2.219732, test/num_examples=10000, total_duration=42056.864844, train/accuracy=0.672715, train/loss=1.352273, validation/accuracy=0.623380, validation/loss=1.578161, validation/num_examples=50000
I0201 23:53:49.688842 139702527031040 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.2694573402404785, loss=2.502605438232422
I0201 23:54:32.042383 139702543816448 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.8657035827636719, loss=3.4031615257263184
I0201 23:55:18.026086 139702527031040 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.016573905944824, loss=2.343867063522339
I0201 23:56:04.118419 139702543816448 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.6481142044067383, loss=4.387799263000488
I0201 23:56:49.911769 139702527031040 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.6944100856781006, loss=3.584439277648926
I0201 23:57:35.925078 139702543816448 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.0691657066345215, loss=2.7075881958007812
I0201 23:58:21.888955 139702527031040 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.9183146953582764, loss=2.904052972793579
I0201 23:59:07.695589 139702543816448 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.982630968093872, loss=2.5971572399139404
I0201 23:59:53.711848 139702527031040 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.8728801012039185, loss=2.525871753692627
I0202 00:00:39.846290 139702543816448 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.8629578351974487, loss=4.965602874755859
I0202 00:00:42.156700 139863983413056 spec.py:321] Evaluating on the training split.
I0202 00:00:52.625565 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 00:01:16.564562 139863983413056 spec.py:349] Evaluating on the test split.
I0202 00:01:18.210425 139863983413056 submission_runner.py:408] Time since start: 42513.00s, 	Step: 85907, 	{'train/accuracy': 0.6804882884025574, 'train/loss': 1.3314138650894165, 'validation/accuracy': 0.6174600124359131, 'validation/loss': 1.6178598403930664, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.280888557434082, 'test/num_examples': 10000, 'score': 39110.57365632057, 'total_duration': 42512.99793553352, 'accumulated_submission_time': 39110.57365632057, 'accumulated_eval_time': 3393.9766433238983, 'accumulated_logging_time': 3.9189043045043945}
I0202 00:01:18.243992 139702527031040 logging_writer.py:48] [85907] accumulated_eval_time=3393.976643, accumulated_logging_time=3.918904, accumulated_submission_time=39110.573656, global_step=85907, preemption_count=0, score=39110.573656, test/accuracy=0.493900, test/loss=2.280889, test/num_examples=10000, total_duration=42512.997936, train/accuracy=0.680488, train/loss=1.331414, validation/accuracy=0.617460, validation/loss=1.617860, validation/num_examples=50000
I0202 00:01:56.460882 139702543816448 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.5819038152694702, loss=4.964382171630859
I0202 00:02:42.089960 139702527031040 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.8934630155563354, loss=4.47036075592041
I0202 00:03:28.116057 139702543816448 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.800271987915039, loss=4.418087482452393
I0202 00:04:14.038068 139702527031040 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.9942680597305298, loss=2.3869075775146484
I0202 00:05:00.263415 139702543816448 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.902134656906128, loss=2.393094778060913
I0202 00:05:46.322074 139702527031040 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.041292905807495, loss=2.533799648284912
I0202 00:06:32.640769 139702543816448 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.7609050273895264, loss=2.613250732421875
I0202 00:07:18.801659 139702527031040 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.040985107421875, loss=2.434185028076172
I0202 00:08:04.873022 139702543816448 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.0565145015716553, loss=2.5868308544158936
I0202 00:08:18.626925 139863983413056 spec.py:321] Evaluating on the training split.
I0202 00:08:28.805467 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 00:08:53.343838 139863983413056 spec.py:349] Evaluating on the test split.
I0202 00:08:54.991674 139863983413056 submission_runner.py:408] Time since start: 42969.78s, 	Step: 86832, 	{'train/accuracy': 0.669238269329071, 'train/loss': 1.371837854385376, 'validation/accuracy': 0.6244199872016907, 'validation/loss': 1.578890323638916, 'validation/num_examples': 50000, 'test/accuracy': 0.5002000331878662, 'test/loss': 2.2396240234375, 'test/num_examples': 10000, 'score': 39530.89902305603, 'total_duration': 42969.77918601036, 'accumulated_submission_time': 39530.89902305603, 'accumulated_eval_time': 3430.3413774967194, 'accumulated_logging_time': 3.9617397785186768}
I0202 00:08:55.024473 139702527031040 logging_writer.py:48] [86832] accumulated_eval_time=3430.341377, accumulated_logging_time=3.961740, accumulated_submission_time=39530.899023, global_step=86832, preemption_count=0, score=39530.899023, test/accuracy=0.500200, test/loss=2.239624, test/num_examples=10000, total_duration=42969.779186, train/accuracy=0.669238, train/loss=1.371838, validation/accuracy=0.624420, validation/loss=1.578890, validation/num_examples=50000
I0202 00:09:22.577726 139702543816448 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.0931053161621094, loss=2.3088035583496094
I0202 00:10:07.915470 139702527031040 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.640602707862854, loss=5.1522417068481445
I0202 00:10:54.181721 139702543816448 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.8579864501953125, loss=3.795396327972412
I0202 00:11:40.606845 139702527031040 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.0067503452301025, loss=2.3958587646484375
I0202 00:12:26.645476 139702543816448 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.9496585130691528, loss=2.2733664512634277
I0202 00:13:13.168459 139702527031040 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.9810866117477417, loss=2.2844488620758057
I0202 00:13:59.101746 139702543816448 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.5591957569122314, loss=4.163820743560791
I0202 00:14:45.745705 139702527031040 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.7290593385696411, loss=3.50764536857605
I0202 00:15:32.308955 139702543816448 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.2070488929748535, loss=2.3685457706451416
I0202 00:15:55.373561 139863983413056 spec.py:321] Evaluating on the training split.
I0202 00:16:05.961762 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 00:16:29.190078 139863983413056 spec.py:349] Evaluating on the test split.
I0202 00:16:30.821791 139863983413056 submission_runner.py:408] Time since start: 43425.61s, 	Step: 87751, 	{'train/accuracy': 0.674023449420929, 'train/loss': 1.3504189252853394, 'validation/accuracy': 0.6277799606323242, 'validation/loss': 1.5659699440002441, 'validation/num_examples': 50000, 'test/accuracy': 0.5024000406265259, 'test/loss': 2.2063982486724854, 'test/num_examples': 10000, 'score': 39951.18798828125, 'total_duration': 43425.609325408936, 'accumulated_submission_time': 39951.18798828125, 'accumulated_eval_time': 3465.7896132469177, 'accumulated_logging_time': 4.005863666534424}
I0202 00:16:30.848996 139702527031040 logging_writer.py:48] [87751] accumulated_eval_time=3465.789613, accumulated_logging_time=4.005864, accumulated_submission_time=39951.187988, global_step=87751, preemption_count=0, score=39951.187988, test/accuracy=0.502400, test/loss=2.206398, test/num_examples=10000, total_duration=43425.609325, train/accuracy=0.674023, train/loss=1.350419, validation/accuracy=0.627780, validation/loss=1.565970, validation/num_examples=50000
I0202 00:16:50.800925 139702543816448 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.9500477313995361, loss=2.27813458442688
I0202 00:17:34.503914 139702527031040 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.8312081098556519, loss=4.598902702331543
I0202 00:18:20.604274 139702543816448 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.790536880493164, loss=4.515995979309082
I0202 00:19:07.072590 139702527031040 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.1615118980407715, loss=2.6609370708465576
I0202 00:19:53.018657 139702543816448 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.0020604133605957, loss=2.2761473655700684
I0202 00:20:38.998937 139702527031040 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.248394727706909, loss=2.330315113067627
I0202 00:21:25.096433 139702543816448 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.9877156019210815, loss=2.4230127334594727
I0202 00:22:11.086498 139702527031040 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.8957741260528564, loss=2.7575931549072266
I0202 00:22:57.094843 139702543816448 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.033222198486328, loss=2.2906370162963867
I0202 00:23:30.852453 139863983413056 spec.py:321] Evaluating on the training split.
I0202 00:23:41.236476 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 00:24:03.967798 139863983413056 spec.py:349] Evaluating on the test split.
I0202 00:24:05.612587 139863983413056 submission_runner.py:408] Time since start: 43880.40s, 	Step: 88675, 	{'train/accuracy': 0.6822851300239563, 'train/loss': 1.2980283498764038, 'validation/accuracy': 0.6274799704551697, 'validation/loss': 1.5562463998794556, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.1928582191467285, 'test/num_examples': 10000, 'score': 40371.13100576401, 'total_duration': 43880.40012168884, 'accumulated_submission_time': 40371.13100576401, 'accumulated_eval_time': 3500.5497431755066, 'accumulated_logging_time': 4.045783042907715}
I0202 00:24:05.641169 139702527031040 logging_writer.py:48] [88675] accumulated_eval_time=3500.549743, accumulated_logging_time=4.045783, accumulated_submission_time=40371.131006, global_step=88675, preemption_count=0, score=40371.131006, test/accuracy=0.508500, test/loss=2.192858, test/num_examples=10000, total_duration=43880.400122, train/accuracy=0.682285, train/loss=1.298028, validation/accuracy=0.627480, validation/loss=1.556246, validation/num_examples=50000
I0202 00:24:16.020269 139702543816448 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.7198935747146606, loss=4.027759075164795
I0202 00:24:58.603165 139702527031040 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.7376337051391602, loss=4.814870357513428
I0202 00:25:44.813314 139702543816448 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.7478476762771606, loss=2.572200059890747
I0202 00:26:31.011269 139702527031040 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.8064312934875488, loss=3.395528793334961
I0202 00:27:16.884685 139702543816448 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.7436431646347046, loss=3.594494104385376
I0202 00:28:02.736720 139702527031040 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.253380060195923, loss=2.3656818866729736
I0202 00:28:48.884432 139702543816448 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.1032392978668213, loss=2.359438180923462
I0202 00:29:34.890679 139702527031040 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.2239809036254883, loss=2.431304693222046
I0202 00:30:21.079254 139702543816448 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.1692562103271484, loss=2.3475799560546875
I0202 00:31:05.972285 139863983413056 spec.py:321] Evaluating on the training split.
I0202 00:31:16.307706 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 00:31:39.480784 139863983413056 spec.py:349] Evaluating on the test split.
I0202 00:31:41.121717 139863983413056 submission_runner.py:408] Time since start: 44335.91s, 	Step: 89599, 	{'train/accuracy': 0.6733984351158142, 'train/loss': 1.3592572212219238, 'validation/accuracy': 0.6269800066947937, 'validation/loss': 1.5658754110336304, 'validation/num_examples': 50000, 'test/accuracy': 0.5076000094413757, 'test/loss': 2.21124529838562, 'test/num_examples': 10000, 'score': 40791.40249633789, 'total_duration': 44335.90924882889, 'accumulated_submission_time': 40791.40249633789, 'accumulated_eval_time': 3535.699191570282, 'accumulated_logging_time': 4.085117340087891}
I0202 00:31:41.149932 139702527031040 logging_writer.py:48] [89599] accumulated_eval_time=3535.699192, accumulated_logging_time=4.085117, accumulated_submission_time=40791.402496, global_step=89599, preemption_count=0, score=40791.402496, test/accuracy=0.507600, test/loss=2.211245, test/num_examples=10000, total_duration=44335.909249, train/accuracy=0.673398, train/loss=1.359257, validation/accuracy=0.626980, validation/loss=1.565875, validation/num_examples=50000
I0202 00:31:41.951538 139702543816448 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.0850071907043457, loss=2.825571060180664
I0202 00:32:23.023887 139702527031040 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.02363657951355, loss=2.31304669380188
I0202 00:33:08.755285 139702543816448 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.8888829946517944, loss=2.7243754863739014
I0202 00:33:54.891638 139702527031040 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.028942346572876, loss=2.3085718154907227
I0202 00:34:40.872172 139702543816448 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.206946849822998, loss=2.239375114440918
I0202 00:35:26.938695 139702527031040 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.0424578189849854, loss=2.3099050521850586
I0202 00:36:13.356412 139702543816448 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.0310049057006836, loss=2.245166301727295
I0202 00:36:59.220302 139702527031040 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.3117856979370117, loss=2.3667962551116943
I0202 00:37:45.269221 139702543816448 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.9377822875976562, loss=2.690915822982788
I0202 00:38:31.166751 139702527031040 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.037282705307007, loss=2.6064178943634033
I0202 00:38:41.364288 139863983413056 spec.py:321] Evaluating on the training split.
I0202 00:38:51.778146 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 00:39:13.711736 139863983413056 spec.py:349] Evaluating on the test split.
I0202 00:39:15.351354 139863983413056 submission_runner.py:408] Time since start: 44790.14s, 	Step: 90524, 	{'train/accuracy': 0.6755468845367432, 'train/loss': 1.3368297815322876, 'validation/accuracy': 0.6282599568367004, 'validation/loss': 1.5485241413116455, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.1989150047302246, 'test/num_examples': 10000, 'score': 41211.55885767937, 'total_duration': 44790.13888645172, 'accumulated_submission_time': 41211.55885767937, 'accumulated_eval_time': 3569.6862609386444, 'accumulated_logging_time': 4.122882843017578}
I0202 00:39:15.379591 139702543816448 logging_writer.py:48] [90524] accumulated_eval_time=3569.686261, accumulated_logging_time=4.122883, accumulated_submission_time=41211.558858, global_step=90524, preemption_count=0, score=41211.558858, test/accuracy=0.505600, test/loss=2.198915, test/num_examples=10000, total_duration=44790.138886, train/accuracy=0.675547, train/loss=1.336830, validation/accuracy=0.628260, validation/loss=1.548524, validation/num_examples=50000
I0202 00:39:46.089369 139702527031040 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.8570290803909302, loss=2.9286813735961914
I0202 00:40:31.532347 139702543816448 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.146343469619751, loss=2.4844729900360107
I0202 00:41:17.815092 139702527031040 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.1309409141540527, loss=2.537367343902588
I0202 00:42:04.120123 139702543816448 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.7120219469070435, loss=5.009117126464844
I0202 00:42:50.243181 139702527031040 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.9788037538528442, loss=2.325007438659668
I0202 00:43:36.486324 139702543816448 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.052619218826294, loss=2.355001926422119
I0202 00:44:22.223843 139702527031040 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.8876996040344238, loss=2.8696677684783936
I0202 00:45:08.233205 139702543816448 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.014439105987549, loss=2.4320149421691895
I0202 00:45:54.455667 139702527031040 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.0695714950561523, loss=2.354252815246582
I0202 00:46:15.771416 139863983413056 spec.py:321] Evaluating on the training split.
I0202 00:46:26.240590 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 00:46:51.430698 139863983413056 spec.py:349] Evaluating on the test split.
I0202 00:46:53.071188 139863983413056 submission_runner.py:408] Time since start: 45247.86s, 	Step: 91448, 	{'train/accuracy': 0.6873828172683716, 'train/loss': 1.2694848775863647, 'validation/accuracy': 0.6334599852561951, 'validation/loss': 1.521888256072998, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.1726202964782715, 'test/num_examples': 10000, 'score': 41631.89220118523, 'total_duration': 45247.85872173309, 'accumulated_submission_time': 41631.89220118523, 'accumulated_eval_time': 3606.986034631729, 'accumulated_logging_time': 4.161207437515259}
I0202 00:46:53.100919 139702543816448 logging_writer.py:48] [91448] accumulated_eval_time=3606.986035, accumulated_logging_time=4.161207, accumulated_submission_time=41631.892201, global_step=91448, preemption_count=0, score=41631.892201, test/accuracy=0.513700, test/loss=2.172620, test/num_examples=10000, total_duration=45247.858722, train/accuracy=0.687383, train/loss=1.269485, validation/accuracy=0.633460, validation/loss=1.521888, validation/num_examples=50000
I0202 00:47:14.242299 139702527031040 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.7663614749908447, loss=3.8408496379852295
I0202 00:47:58.455633 139702543816448 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.1031198501586914, loss=2.602621078491211
I0202 00:48:44.577735 139702527031040 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.196042060852051, loss=3.0254056453704834
I0202 00:49:30.887120 139702543816448 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.9565753936767578, loss=4.959254264831543
I0202 00:50:16.877906 139702527031040 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.814703345298767, loss=4.626157283782959
I0202 00:51:02.850948 139702543816448 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.9953757524490356, loss=2.146195888519287
I0202 00:51:48.548386 139702527031040 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.2130255699157715, loss=2.4076685905456543
I0202 00:52:34.801729 139702543816448 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.092667579650879, loss=2.6579267978668213
I0202 00:53:20.780909 139702527031040 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.0385031700134277, loss=4.748847007751465
I0202 00:53:53.413298 139863983413056 spec.py:321] Evaluating on the training split.
I0202 00:54:04.104388 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 00:54:29.856901 139863983413056 spec.py:349] Evaluating on the test split.
I0202 00:54:31.499015 139863983413056 submission_runner.py:408] Time since start: 45706.29s, 	Step: 92373, 	{'train/accuracy': 0.699414074420929, 'train/loss': 1.262794017791748, 'validation/accuracy': 0.6269800066947937, 'validation/loss': 1.5701531171798706, 'validation/num_examples': 50000, 'test/accuracy': 0.5078999996185303, 'test/loss': 2.2144458293914795, 'test/num_examples': 10000, 'score': 42052.14500403404, 'total_duration': 45706.286526441574, 'accumulated_submission_time': 42052.14500403404, 'accumulated_eval_time': 3645.071723461151, 'accumulated_logging_time': 4.2016355991363525}
I0202 00:54:31.538843 139702543816448 logging_writer.py:48] [92373] accumulated_eval_time=3645.071723, accumulated_logging_time=4.201636, accumulated_submission_time=42052.145004, global_step=92373, preemption_count=0, score=42052.145004, test/accuracy=0.507900, test/loss=2.214446, test/num_examples=10000, total_duration=45706.286526, train/accuracy=0.699414, train/loss=1.262794, validation/accuracy=0.626980, validation/loss=1.570153, validation/num_examples=50000
I0202 00:54:42.710332 139702527031040 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.071110725402832, loss=2.408271312713623
I0202 00:55:25.162464 139702543816448 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.1452903747558594, loss=2.363696336746216
I0202 00:56:11.174139 139702527031040 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.811637043952942, loss=3.0446159839630127
I0202 00:56:57.501532 139702543816448 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.09053373336792, loss=2.2834787368774414
I0202 00:57:43.529051 139702527031040 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.064610719680786, loss=2.3520686626434326
I0202 00:58:29.501240 139702543816448 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.759921908378601, loss=3.9563677310943604
I0202 00:59:15.530706 139702527031040 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.1960155963897705, loss=2.5511093139648438
I0202 01:00:01.539260 139702543816448 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.8550268411636353, loss=3.071484327316284
I0202 01:00:47.403904 139702527031040 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.348233699798584, loss=2.396177053451538
I0202 01:01:31.690927 139863983413056 spec.py:321] Evaluating on the training split.
I0202 01:01:42.272522 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 01:02:02.925563 139863983413056 spec.py:349] Evaluating on the test split.
I0202 01:02:04.564283 139863983413056 submission_runner.py:408] Time since start: 46159.35s, 	Step: 93298, 	{'train/accuracy': 0.6812304258346558, 'train/loss': 1.2977181673049927, 'validation/accuracy': 0.63646000623703, 'validation/loss': 1.5049082040786743, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.159391164779663, 'test/num_examples': 10000, 'score': 42472.23790502548, 'total_duration': 46159.35181570053, 'accumulated_submission_time': 42472.23790502548, 'accumulated_eval_time': 3677.9450783729553, 'accumulated_logging_time': 4.251505374908447}
I0202 01:02:04.592543 139702543816448 logging_writer.py:48] [93298] accumulated_eval_time=3677.945078, accumulated_logging_time=4.251505, accumulated_submission_time=42472.237905, global_step=93298, preemption_count=0, score=42472.237905, test/accuracy=0.516100, test/loss=2.159391, test/num_examples=10000, total_duration=46159.351816, train/accuracy=0.681230, train/loss=1.297718, validation/accuracy=0.636460, validation/loss=1.504908, validation/num_examples=50000
I0202 01:02:05.790119 139702527031040 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.7996643781661987, loss=4.968138217926025
I0202 01:02:46.938276 139702543816448 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.8127049207687378, loss=3.156831741333008
I0202 01:03:32.697711 139702527031040 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.424572706222534, loss=2.370476007461548
I0202 01:04:18.932408 139702543816448 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.2813947200775146, loss=2.314314126968384
I0202 01:05:04.909813 139702527031040 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.2811522483825684, loss=2.435016393661499
I0202 01:05:50.891508 139702543816448 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.6798535585403442, loss=3.7941770553588867
I0202 01:06:37.620957 139702527031040 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.860464334487915, loss=3.0919692516326904
I0202 01:07:23.537986 139702543816448 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.1821799278259277, loss=2.215707302093506
I0202 01:08:09.784512 139702527031040 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.020838975906372, loss=2.101597785949707
I0202 01:08:56.006030 139702543816448 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.953965187072754, loss=4.571847438812256
I0202 01:09:04.959011 139863983413056 spec.py:321] Evaluating on the training split.
I0202 01:09:15.436996 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 01:09:41.984030 139863983413056 spec.py:349] Evaluating on the test split.
I0202 01:09:43.623925 139863983413056 submission_runner.py:408] Time since start: 46618.41s, 	Step: 94221, 	{'train/accuracy': 0.6886913776397705, 'train/loss': 1.2740516662597656, 'validation/accuracy': 0.634880006313324, 'validation/loss': 1.5176643133163452, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.162588119506836, 'test/num_examples': 10000, 'score': 42892.54639649391, 'total_duration': 46618.41143655777, 'accumulated_submission_time': 42892.54639649391, 'accumulated_eval_time': 3716.6099610328674, 'accumulated_logging_time': 4.289197206497192}
I0202 01:09:43.656589 139702527031040 logging_writer.py:48] [94221] accumulated_eval_time=3716.609961, accumulated_logging_time=4.289197, accumulated_submission_time=42892.546396, global_step=94221, preemption_count=0, score=42892.546396, test/accuracy=0.513400, test/loss=2.162588, test/num_examples=10000, total_duration=46618.411437, train/accuracy=0.688691, train/loss=1.274052, validation/accuracy=0.634880, validation/loss=1.517664, validation/num_examples=50000
I0202 01:10:15.690762 139702543816448 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.7084859609603882, loss=3.863974094390869
I0202 01:11:01.457999 139702527031040 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.036536931991577, loss=2.2260055541992188
I0202 01:11:47.382709 139702543816448 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.264348268508911, loss=2.268052577972412
I0202 01:12:33.450912 139702527031040 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.7893601655960083, loss=4.747200012207031
I0202 01:13:19.497284 139702543816448 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.1962809562683105, loss=2.264698028564453
I0202 01:14:05.611266 139702527031040 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.9175758361816406, loss=3.252261161804199
I0202 01:14:51.744794 139702543816448 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.8915989398956299, loss=4.6057963371276855
I0202 01:15:37.557488 139702527031040 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.2651262283325195, loss=4.70236873626709
I0202 01:16:23.676452 139702543816448 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.251077651977539, loss=2.4445269107818604
I0202 01:16:44.011342 139863983413056 spec.py:321] Evaluating on the training split.
I0202 01:16:54.301319 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 01:17:20.735272 139863983413056 spec.py:349] Evaluating on the test split.
I0202 01:17:22.375219 139863983413056 submission_runner.py:408] Time since start: 47077.16s, 	Step: 95146, 	{'train/accuracy': 0.7041796445846558, 'train/loss': 1.1996102333068848, 'validation/accuracy': 0.6363599896430969, 'validation/loss': 1.4963476657867432, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.128796100616455, 'test/num_examples': 10000, 'score': 43312.84077787399, 'total_duration': 47077.16275238991, 'accumulated_submission_time': 43312.84077787399, 'accumulated_eval_time': 3754.973846912384, 'accumulated_logging_time': 4.333668947219849}
I0202 01:17:22.409909 139702527031040 logging_writer.py:48] [95146] accumulated_eval_time=3754.973847, accumulated_logging_time=4.333669, accumulated_submission_time=43312.840778, global_step=95146, preemption_count=0, score=43312.840778, test/accuracy=0.516300, test/loss=2.128796, test/num_examples=10000, total_duration=47077.162752, train/accuracy=0.704180, train/loss=1.199610, validation/accuracy=0.636360, validation/loss=1.496348, validation/num_examples=50000
I0202 01:17:44.380945 139702543816448 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.094306468963623, loss=2.129319190979004
I0202 01:18:28.752459 139702527031040 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.2162487506866455, loss=2.2563083171844482
I0202 01:19:14.813487 139702543816448 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.9462159872055054, loss=3.1383731365203857
I0202 01:20:01.099359 139702527031040 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.039226531982422, loss=2.329580783843994
I0202 01:20:46.939530 139702543816448 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.3793673515319824, loss=2.5384011268615723
I0202 01:21:33.107753 139702527031040 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.1056149005889893, loss=2.546461582183838
I0202 01:22:18.995389 139702543816448 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.4009175300598145, loss=2.3077545166015625
I0202 01:23:04.749454 139702527031040 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.8892253637313843, loss=4.864750862121582
I0202 01:23:50.906008 139702543816448 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.0960912704467773, loss=2.2249271869659424
I0202 01:24:22.686973 139863983413056 spec.py:321] Evaluating on the training split.
I0202 01:24:33.990405 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 01:24:58.924896 139863983413056 spec.py:349] Evaluating on the test split.
I0202 01:25:00.564970 139863983413056 submission_runner.py:408] Time since start: 47535.35s, 	Step: 96071, 	{'train/accuracy': 0.6895898580551147, 'train/loss': 1.2599599361419678, 'validation/accuracy': 0.6400399804115295, 'validation/loss': 1.4780635833740234, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.110060930252075, 'test/num_examples': 10000, 'score': 43733.06075167656, 'total_duration': 47535.352481126785, 'accumulated_submission_time': 43733.06075167656, 'accumulated_eval_time': 3792.851813316345, 'accumulated_logging_time': 4.377025365829468}
I0202 01:25:00.599669 139702527031040 logging_writer.py:48] [96071] accumulated_eval_time=3792.851813, accumulated_logging_time=4.377025, accumulated_submission_time=43733.060752, global_step=96071, preemption_count=0, score=43733.060752, test/accuracy=0.517900, test/loss=2.110061, test/num_examples=10000, total_duration=47535.352481, train/accuracy=0.689590, train/loss=1.259960, validation/accuracy=0.640040, validation/loss=1.478064, validation/num_examples=50000
I0202 01:25:12.572829 139702543816448 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.7089693546295166, loss=4.840362548828125
I0202 01:25:54.923036 139702527031040 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.7785769701004028, loss=3.89054012298584
I0202 01:26:40.900323 139702543816448 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.2132821083068848, loss=2.3070173263549805
I0202 01:27:27.299509 139702527031040 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.013615131378174, loss=2.865917444229126
I0202 01:28:13.166742 139702543816448 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.001356840133667, loss=4.597013473510742
I0202 01:28:59.361448 139702527031040 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.094001531600952, loss=2.2314035892486572
I0202 01:29:45.537375 139702543816448 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.8185476064682007, loss=4.852652072906494
I0202 01:30:31.765824 139702527031040 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.3104617595672607, loss=2.3918228149414062
I0202 01:31:17.724953 139702543816448 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.203632354736328, loss=2.2133541107177734
I0202 01:32:00.878121 139863983413056 spec.py:321] Evaluating on the training split.
I0202 01:32:11.156532 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 01:32:36.556222 139863983413056 spec.py:349] Evaluating on the test split.
I0202 01:32:38.194694 139863983413056 submission_runner.py:408] Time since start: 47992.98s, 	Step: 96996, 	{'train/accuracy': 0.690234363079071, 'train/loss': 1.2644189596176147, 'validation/accuracy': 0.642300009727478, 'validation/loss': 1.4981861114501953, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.145113468170166, 'test/num_examples': 10000, 'score': 44153.27914762497, 'total_duration': 47992.982228040695, 'accumulated_submission_time': 44153.27914762497, 'accumulated_eval_time': 3830.168391227722, 'accumulated_logging_time': 4.4223480224609375}
I0202 01:32:38.230766 139702527031040 logging_writer.py:48] [96996] accumulated_eval_time=3830.168391, accumulated_logging_time=4.422348, accumulated_submission_time=44153.279148, global_step=96996, preemption_count=0, score=44153.279148, test/accuracy=0.516500, test/loss=2.145113, test/num_examples=10000, total_duration=47992.982228, train/accuracy=0.690234, train/loss=1.264419, validation/accuracy=0.642300, validation/loss=1.498186, validation/num_examples=50000
I0202 01:32:40.222024 139702543816448 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.092388391494751, loss=2.1856348514556885
I0202 01:33:21.762930 139702527031040 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.6818993091583252, loss=3.3838696479797363
I0202 01:34:07.636574 139702543816448 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.1714396476745605, loss=2.3740155696868896
I0202 01:34:53.635268 139702527031040 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.1706326007843018, loss=2.086902141571045
I0202 01:35:40.161560 139702543816448 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.113435983657837, loss=2.130317449569702
I0202 01:36:25.908812 139702527031040 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.169109582901001, loss=2.8166184425354004
I0202 01:37:12.173252 139702543816448 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.167584180831909, loss=2.3220794200897217
I0202 01:37:58.279324 139702527031040 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.765228033065796, loss=3.9293949604034424
I0202 01:38:44.391791 139702543816448 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.10591459274292, loss=3.9214396476745605
I0202 01:39:30.739318 139702527031040 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.8124500513076782, loss=3.8198442459106445
I0202 01:39:38.472016 139863983413056 spec.py:321] Evaluating on the training split.
I0202 01:39:49.398554 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 01:40:12.869449 139863983413056 spec.py:349] Evaluating on the test split.
I0202 01:40:14.505768 139863983413056 submission_runner.py:408] Time since start: 48449.29s, 	Step: 97914, 	{'train/accuracy': 0.6972265243530273, 'train/loss': 1.2512993812561035, 'validation/accuracy': 0.6369799971580505, 'validation/loss': 1.5186965465545654, 'validation/num_examples': 50000, 'test/accuracy': 0.5138000249862671, 'test/loss': 2.157207727432251, 'test/num_examples': 10000, 'score': 44573.461153030396, 'total_duration': 48449.29330062866, 'accumulated_submission_time': 44573.461153030396, 'accumulated_eval_time': 3866.2021346092224, 'accumulated_logging_time': 4.46885085105896}
I0202 01:40:14.537303 139702543816448 logging_writer.py:48] [97914] accumulated_eval_time=3866.202135, accumulated_logging_time=4.468851, accumulated_submission_time=44573.461153, global_step=97914, preemption_count=0, score=44573.461153, test/accuracy=0.513800, test/loss=2.157208, test/num_examples=10000, total_duration=48449.293301, train/accuracy=0.697227, train/loss=1.251299, validation/accuracy=0.636980, validation/loss=1.518697, validation/num_examples=50000
I0202 01:40:49.377607 139702527031040 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.4967041015625, loss=4.851774215698242
I0202 01:41:35.694154 139702543816448 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.1498754024505615, loss=2.300816059112549
I0202 01:42:21.765657 139702527031040 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.038094997406006, loss=2.8552420139312744
I0202 01:43:08.051171 139702543816448 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.8602216243743896, loss=3.191542148590088
I0202 01:43:54.118731 139702527031040 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.1215851306915283, loss=2.2051472663879395
I0202 01:44:40.392868 139702543816448 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.8699536323547363, loss=3.7822229862213135
I0202 01:45:26.509911 139702527031040 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.0277085304260254, loss=2.0842247009277344
I0202 01:46:12.499393 139702543816448 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.230909585952759, loss=2.352736711502075
I0202 01:46:58.256885 139702527031040 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.816537857055664, loss=4.565706729888916
I0202 01:47:14.639701 139863983413056 spec.py:321] Evaluating on the training split.
I0202 01:47:25.007868 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 01:47:51.513948 139863983413056 spec.py:349] Evaluating on the test split.
I0202 01:47:53.161712 139863983413056 submission_runner.py:408] Time since start: 48907.95s, 	Step: 98837, 	{'train/accuracy': 0.6911327838897705, 'train/loss': 1.2608174085617065, 'validation/accuracy': 0.6458399891853333, 'validation/loss': 1.466307282447815, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.1083343029022217, 'test/num_examples': 10000, 'score': 44993.504257678986, 'total_duration': 48907.949244976044, 'accumulated_submission_time': 44993.504257678986, 'accumulated_eval_time': 3904.7241473197937, 'accumulated_logging_time': 4.510102987289429}
I0202 01:47:53.193829 139702543816448 logging_writer.py:48] [98837] accumulated_eval_time=3904.724147, accumulated_logging_time=4.510103, accumulated_submission_time=44993.504258, global_step=98837, preemption_count=0, score=44993.504258, test/accuracy=0.519900, test/loss=2.108334, test/num_examples=10000, total_duration=48907.949245, train/accuracy=0.691133, train/loss=1.260817, validation/accuracy=0.645840, validation/loss=1.466307, validation/num_examples=50000
I0202 01:48:18.759934 139702527031040 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.321049690246582, loss=2.290266990661621
I0202 01:49:03.639330 139702543816448 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.9428735971450806, loss=3.012622833251953
I0202 01:49:49.931266 139702527031040 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.9932364225387573, loss=3.173959732055664
I0202 01:50:36.312688 139702543816448 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.875468134880066, loss=3.293714761734009
I0202 01:51:21.969940 139702527031040 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.8180102109909058, loss=3.3912339210510254
I0202 01:52:08.158875 139702543816448 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.025113821029663, loss=4.245540618896484
I0202 01:52:53.887030 139702527031040 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.041198253631592, loss=2.37784481048584
I0202 01:53:40.120277 139702543816448 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.197676658630371, loss=2.2830491065979004
I0202 01:54:26.648274 139702527031040 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.8878867626190186, loss=3.300664186477661
I0202 01:54:53.649595 139863983413056 spec.py:321] Evaluating on the training split.
I0202 01:55:04.092742 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 01:55:31.673537 139863983413056 spec.py:349] Evaluating on the test split.
I0202 01:55:33.312080 139863983413056 submission_runner.py:408] Time since start: 49368.10s, 	Step: 99760, 	{'train/accuracy': 0.6932421922683716, 'train/loss': 1.244040608406067, 'validation/accuracy': 0.6432799696922302, 'validation/loss': 1.4662216901779175, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.098806858062744, 'test/num_examples': 10000, 'score': 45413.90217757225, 'total_duration': 49368.09958767891, 'accumulated_submission_time': 45413.90217757225, 'accumulated_eval_time': 3944.386614084244, 'accumulated_logging_time': 4.552144289016724}
I0202 01:55:33.350425 139702543816448 logging_writer.py:48] [99760] accumulated_eval_time=3944.386614, accumulated_logging_time=4.552144, accumulated_submission_time=45413.902178, global_step=99760, preemption_count=0, score=45413.902178, test/accuracy=0.519900, test/loss=2.098807, test/num_examples=10000, total_duration=49368.099588, train/accuracy=0.693242, train/loss=1.244041, validation/accuracy=0.643280, validation/loss=1.466222, validation/num_examples=50000
I0202 01:55:49.725562 139702527031040 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.8417679071426392, loss=4.824582099914551
I0202 01:56:33.604403 139702543816448 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.0447466373443604, loss=2.3132574558258057
I0202 01:57:19.738257 139702527031040 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.304393768310547, loss=2.5080552101135254
I0202 01:58:06.344776 139702543816448 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.4123575687408447, loss=2.244957685470581
I0202 01:58:52.275797 139702527031040 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.0611588954925537, loss=3.856672763824463
I0202 01:59:38.467000 139702543816448 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.9995092153549194, loss=4.5325469970703125
I0202 02:00:24.966448 139702527031040 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.160078525543213, loss=2.413196563720703
I0202 02:01:11.099604 139702543816448 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.1984879970550537, loss=2.353929042816162
I0202 02:01:57.215816 139702527031040 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.7965008020401, loss=4.233755111694336
I0202 02:02:33.510467 139863983413056 spec.py:321] Evaluating on the training split.
I0202 02:02:43.787051 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 02:03:10.239225 139863983413056 spec.py:349] Evaluating on the test split.
I0202 02:03:11.886359 139863983413056 submission_runner.py:408] Time since start: 49826.67s, 	Step: 100680, 	{'train/accuracy': 0.7011132836341858, 'train/loss': 1.2188524007797241, 'validation/accuracy': 0.6454199552536011, 'validation/loss': 1.4721068143844604, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.113632917404175, 'test/num_examples': 10000, 'score': 45834.003454208374, 'total_duration': 49826.67389035225, 'accumulated_submission_time': 45834.003454208374, 'accumulated_eval_time': 3982.7625353336334, 'accumulated_logging_time': 4.600781679153442}
I0202 02:03:11.922747 139702543816448 logging_writer.py:48] [100680] accumulated_eval_time=3982.762535, accumulated_logging_time=4.600782, accumulated_submission_time=45834.003454, global_step=100680, preemption_count=0, score=45834.003454, test/accuracy=0.520000, test/loss=2.113633, test/num_examples=10000, total_duration=49826.673890, train/accuracy=0.701113, train/loss=1.218852, validation/accuracy=0.645420, validation/loss=1.472107, validation/num_examples=50000
I0202 02:03:20.305047 139702527031040 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.2428033351898193, loss=2.5700578689575195
I0202 02:04:02.642867 139702543816448 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.168743371963501, loss=2.3113667964935303
I0202 02:04:48.959701 139702527031040 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.5313217639923096, loss=2.193356513977051
I0202 02:05:35.278382 139702543816448 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.5582828521728516, loss=4.912113666534424
I0202 02:06:21.290894 139702527031040 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.1623642444610596, loss=3.3354477882385254
I0202 02:07:07.322812 139702543816448 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.2520079612731934, loss=2.2760491371154785
I0202 02:07:53.750232 139702527031040 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.0936801433563232, loss=2.5342493057250977
I0202 02:08:40.196889 139702543816448 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.2094614505767822, loss=2.2869348526000977
I0202 02:09:26.211454 139702527031040 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.336398124694824, loss=2.528566360473633
I0202 02:10:11.973972 139863983413056 spec.py:321] Evaluating on the training split.
I0202 02:10:22.521592 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 02:10:49.504887 139863983413056 spec.py:349] Evaluating on the test split.
I0202 02:10:51.139080 139863983413056 submission_runner.py:408] Time since start: 50285.93s, 	Step: 101600, 	{'train/accuracy': 0.7016406059265137, 'train/loss': 1.2149200439453125, 'validation/accuracy': 0.6446999907493591, 'validation/loss': 1.46499764919281, 'validation/num_examples': 50000, 'test/accuracy': 0.522100031375885, 'test/loss': 2.124752998352051, 'test/num_examples': 10000, 'score': 46253.99683356285, 'total_duration': 50285.926607847214, 'accumulated_submission_time': 46253.99683356285, 'accumulated_eval_time': 4021.927656650543, 'accumulated_logging_time': 4.6470115184783936}
I0202 02:10:51.174613 139702543816448 logging_writer.py:48] [101600] accumulated_eval_time=4021.927657, accumulated_logging_time=4.647012, accumulated_submission_time=46253.996834, global_step=101600, preemption_count=0, score=46253.996834, test/accuracy=0.522100, test/loss=2.124753, test/num_examples=10000, total_duration=50285.926608, train/accuracy=0.701641, train/loss=1.214920, validation/accuracy=0.644700, validation/loss=1.464998, validation/num_examples=50000
I0202 02:10:51.579235 139702527031040 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.2441813945770264, loss=2.248067855834961
I0202 02:11:32.877489 139702543816448 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.487996816635132, loss=2.196500539779663
I0202 02:12:18.690441 139702527031040 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.94541335105896, loss=4.136138916015625
I0202 02:13:04.919977 139702543816448 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.310131549835205, loss=2.283134698867798
I0202 02:13:50.943599 139702527031040 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.8717817068099976, loss=4.680813312530518
I0202 02:14:36.834326 139702543816448 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.732723355293274, loss=4.792640686035156
I0202 02:15:23.272679 139702527031040 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.3502399921417236, loss=2.301412582397461
I0202 02:16:09.200208 139702543816448 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.1350672245025635, loss=2.3225252628326416
I0202 02:16:55.228624 139702527031040 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.3714218139648438, loss=4.8301777839660645
I0202 02:17:41.643363 139702543816448 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.953097939491272, loss=4.371132850646973
I0202 02:17:51.579378 139863983413056 spec.py:321] Evaluating on the training split.
I0202 02:18:02.047114 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 02:18:28.960850 139863983413056 spec.py:349] Evaluating on the test split.
I0202 02:18:30.605362 139863983413056 submission_runner.py:408] Time since start: 50745.39s, 	Step: 102523, 	{'train/accuracy': 0.6954296827316284, 'train/loss': 1.231560230255127, 'validation/accuracy': 0.6503599882125854, 'validation/loss': 1.4419031143188477, 'validation/num_examples': 50000, 'test/accuracy': 0.5236999988555908, 'test/loss': 2.1005005836486816, 'test/num_examples': 10000, 'score': 46674.343448877335, 'total_duration': 50745.39287304878, 'accumulated_submission_time': 46674.343448877335, 'accumulated_eval_time': 4060.9536135196686, 'accumulated_logging_time': 4.69239354133606}
I0202 02:18:30.642498 139702527031040 logging_writer.py:48] [102523] accumulated_eval_time=4060.953614, accumulated_logging_time=4.692394, accumulated_submission_time=46674.343449, global_step=102523, preemption_count=0, score=46674.343449, test/accuracy=0.523700, test/loss=2.100501, test/num_examples=10000, total_duration=50745.392873, train/accuracy=0.695430, train/loss=1.231560, validation/accuracy=0.650360, validation/loss=1.441903, validation/num_examples=50000
I0202 02:19:01.943122 139702543816448 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.146458148956299, loss=2.1623239517211914
I0202 02:19:48.058349 139702527031040 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.8386191129684448, loss=4.390252113342285
I0202 02:20:34.516010 139702543816448 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.254666566848755, loss=2.250797748565674
I0202 02:21:20.776887 139702527031040 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.89020574092865, loss=4.345271587371826
I0202 02:22:06.892824 139702543816448 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.127713918685913, loss=2.3018999099731445
I0202 02:22:52.976320 139702527031040 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.100965976715088, loss=2.466372013092041
I0202 02:23:39.142548 139702543816448 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.074620008468628, loss=4.761805534362793
I0202 02:24:25.193318 139702527031040 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.1308844089508057, loss=3.0446717739105225
I0202 02:25:11.296818 139702543816448 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.1788206100463867, loss=2.1672301292419434
I0202 02:25:30.845750 139863983413056 spec.py:321] Evaluating on the training split.
I0202 02:25:41.449582 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 02:26:05.046490 139863983413056 spec.py:349] Evaluating on the test split.
I0202 02:26:06.680691 139863983413056 submission_runner.py:408] Time since start: 51201.47s, 	Step: 103444, 	{'train/accuracy': 0.7022265195846558, 'train/loss': 1.2212992906570435, 'validation/accuracy': 0.6457799673080444, 'validation/loss': 1.4728409051895142, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.1184866428375244, 'test/num_examples': 10000, 'score': 47094.48765182495, 'total_duration': 51201.4682199955, 'accumulated_submission_time': 47094.48765182495, 'accumulated_eval_time': 4096.788547039032, 'accumulated_logging_time': 4.73948335647583}
I0202 02:26:06.710850 139702527031040 logging_writer.py:48] [103444] accumulated_eval_time=4096.788547, accumulated_logging_time=4.739483, accumulated_submission_time=47094.487652, global_step=103444, preemption_count=0, score=47094.487652, test/accuracy=0.524700, test/loss=2.118487, test/num_examples=10000, total_duration=51201.468220, train/accuracy=0.702227, train/loss=1.221299, validation/accuracy=0.645780, validation/loss=1.472841, validation/num_examples=50000
I0202 02:26:29.453805 139702543816448 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.3094329833984375, loss=2.4132444858551025
I0202 02:27:14.115549 139702527031040 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.9117931127548218, loss=3.258737087249756
I0202 02:28:00.315980 139702543816448 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.111511707305908, loss=3.228879451751709
I0202 02:28:46.873426 139702527031040 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.289005994796753, loss=2.2076706886291504
I0202 02:29:33.181854 139702543816448 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.3909213542938232, loss=2.2621870040893555
I0202 02:30:19.510675 139702527031040 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.5436980724334717, loss=2.218935251235962
I0202 02:31:05.747397 139702543816448 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.009140968322754, loss=4.747067928314209
I0202 02:31:51.548715 139702527031040 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.2382402420043945, loss=2.12068510055542
I0202 02:32:37.882828 139702543816448 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.3289568424224854, loss=2.214686870574951
I0202 02:33:06.858625 139863983413056 spec.py:321] Evaluating on the training split.
I0202 02:33:17.134793 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 02:33:42.540497 139863983413056 spec.py:349] Evaluating on the test split.
I0202 02:33:44.176837 139863983413056 submission_runner.py:408] Time since start: 51658.96s, 	Step: 104365, 	{'train/accuracy': 0.7180859446525574, 'train/loss': 1.164534091949463, 'validation/accuracy': 0.6500999927520752, 'validation/loss': 1.4714568853378296, 'validation/num_examples': 50000, 'test/accuracy': 0.528700053691864, 'test/loss': 2.0928614139556885, 'test/num_examples': 10000, 'score': 47514.577988386154, 'total_duration': 51658.96434521675, 'accumulated_submission_time': 47514.577988386154, 'accumulated_eval_time': 4134.106735706329, 'accumulated_logging_time': 4.77846884727478}
I0202 02:33:44.212852 139702527031040 logging_writer.py:48] [104365] accumulated_eval_time=4134.106736, accumulated_logging_time=4.778469, accumulated_submission_time=47514.577988, global_step=104365, preemption_count=0, score=47514.577988, test/accuracy=0.528700, test/loss=2.092861, test/num_examples=10000, total_duration=51658.964345, train/accuracy=0.718086, train/loss=1.164534, validation/accuracy=0.650100, validation/loss=1.471457, validation/num_examples=50000
I0202 02:33:58.591890 139702543816448 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.100614547729492, loss=2.8069324493408203
I0202 02:34:41.810272 139702527031040 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.538784980773926, loss=3.010892868041992
I0202 02:35:27.820078 139702543816448 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.427459239959717, loss=2.165689706802368
I0202 02:36:13.980796 139702527031040 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.330624580383301, loss=2.201648712158203
I0202 02:36:59.879707 139702543816448 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.8306719064712524, loss=4.018527984619141
I0202 02:37:45.993403 139702527031040 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.286634922027588, loss=2.2227275371551514
I0202 02:38:32.270480 139702543816448 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.276632785797119, loss=2.1437339782714844
I0202 02:39:18.713461 139702527031040 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.2768728733062744, loss=3.5006349086761475
I0202 02:40:05.012837 139702543816448 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.923924207687378, loss=3.2772679328918457
I0202 02:40:44.609642 139863983413056 spec.py:321] Evaluating on the training split.
I0202 02:40:55.369415 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 02:41:21.567411 139863983413056 spec.py:349] Evaluating on the test split.
I0202 02:41:23.211396 139863983413056 submission_runner.py:408] Time since start: 52118.00s, 	Step: 105288, 	{'train/accuracy': 0.6989648342132568, 'train/loss': 1.2240859270095825, 'validation/accuracy': 0.6522799730300903, 'validation/loss': 1.4324557781219482, 'validation/num_examples': 50000, 'test/accuracy': 0.5332000255584717, 'test/loss': 2.0801806449890137, 'test/num_examples': 10000, 'score': 47934.9159283638, 'total_duration': 52117.99890470505, 'accumulated_submission_time': 47934.9159283638, 'accumulated_eval_time': 4172.708475351334, 'accumulated_logging_time': 4.825288534164429}
I0202 02:41:23.247281 139702527031040 logging_writer.py:48] [105288] accumulated_eval_time=4172.708475, accumulated_logging_time=4.825289, accumulated_submission_time=47934.915928, global_step=105288, preemption_count=0, score=47934.915928, test/accuracy=0.533200, test/loss=2.080181, test/num_examples=10000, total_duration=52117.998905, train/accuracy=0.698965, train/loss=1.224086, validation/accuracy=0.652280, validation/loss=1.432456, validation/num_examples=50000
I0202 02:41:28.437147 139702543816448 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.3457515239715576, loss=2.221085786819458
I0202 02:42:10.300864 139702527031040 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.3761096000671387, loss=2.4638266563415527
I0202 02:42:56.338342 139702543816448 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.3239216804504395, loss=2.0339913368225098
I0202 02:43:42.951185 139702527031040 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.2185425758361816, loss=4.436830997467041
I0202 02:44:29.049059 139702543816448 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.924453616142273, loss=4.611205101013184
I0202 02:45:14.910555 139702527031040 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.9737075567245483, loss=3.176595449447632
I0202 02:46:01.208348 139702543816448 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.0029590129852295, loss=3.3580291271209717
I0202 02:46:47.242146 139702527031040 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.6173269748687744, loss=2.3203630447387695
I0202 02:47:33.664129 139702543816448 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.990335464477539, loss=3.1243581771850586
I0202 02:48:19.959228 139702527031040 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.2055060863494873, loss=2.1515469551086426
I0202 02:48:23.383429 139863983413056 spec.py:321] Evaluating on the training split.
I0202 02:48:33.838527 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 02:49:03.435360 139863983413056 spec.py:349] Evaluating on the test split.
I0202 02:49:05.086718 139863983413056 submission_runner.py:408] Time since start: 52579.87s, 	Step: 106209, 	{'train/accuracy': 0.7043554782867432, 'train/loss': 1.194347858428955, 'validation/accuracy': 0.650879979133606, 'validation/loss': 1.4379023313522339, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.0719387531280518, 'test/num_examples': 10000, 'score': 48354.99125123024, 'total_duration': 52579.87425208092, 'accumulated_submission_time': 48354.99125123024, 'accumulated_eval_time': 4214.411760091782, 'accumulated_logging_time': 4.873553991317749}
I0202 02:49:05.117646 139702543816448 logging_writer.py:48] [106209] accumulated_eval_time=4214.411760, accumulated_logging_time=4.873554, accumulated_submission_time=48354.991251, global_step=106209, preemption_count=0, score=48354.991251, test/accuracy=0.531400, test/loss=2.071939, test/num_examples=10000, total_duration=52579.874252, train/accuracy=0.704355, train/loss=1.194348, validation/accuracy=0.650880, validation/loss=1.437902, validation/num_examples=50000
I0202 02:49:42.688041 139702527031040 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.266979694366455, loss=2.057626247406006
I0202 02:50:29.033404 139702543816448 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.6172189712524414, loss=2.2127628326416016
I0202 02:51:15.293231 139702527031040 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.2372210025787354, loss=2.282135248184204
I0202 02:52:01.670533 139702543816448 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.3896589279174805, loss=2.0339250564575195
I0202 02:52:47.722989 139702527031040 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.0845413208007812, loss=4.494280815124512
I0202 02:53:33.864899 139702543816448 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.2187654972076416, loss=2.398715019226074
I0202 02:54:19.705703 139702527031040 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.5590403079986572, loss=2.061168909072876
I0202 02:55:05.824507 139702543816448 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.5051352977752686, loss=2.247978687286377
I0202 02:55:51.884859 139702527031040 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.4889073371887207, loss=2.2924630641937256
I0202 02:56:05.207237 139863983413056 spec.py:321] Evaluating on the training split.
I0202 02:56:15.571939 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 02:56:44.689666 139863983413056 spec.py:349] Evaluating on the test split.
I0202 02:56:46.329301 139863983413056 submission_runner.py:408] Time since start: 53041.12s, 	Step: 107131, 	{'train/accuracy': 0.717578113079071, 'train/loss': 1.1480196714401245, 'validation/accuracy': 0.6521399617195129, 'validation/loss': 1.4363003969192505, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.086843490600586, 'test/num_examples': 10000, 'score': 48775.022804260254, 'total_duration': 53041.11679935455, 'accumulated_submission_time': 48775.022804260254, 'accumulated_eval_time': 4255.533785581589, 'accumulated_logging_time': 4.913725852966309}
I0202 02:56:46.372512 139702543816448 logging_writer.py:48] [107131] accumulated_eval_time=4255.533786, accumulated_logging_time=4.913726, accumulated_submission_time=48775.022804, global_step=107131, preemption_count=0, score=48775.022804, test/accuracy=0.534000, test/loss=2.086843, test/num_examples=10000, total_duration=53041.116799, train/accuracy=0.717578, train/loss=1.148020, validation/accuracy=0.652140, validation/loss=1.436300, validation/num_examples=50000
I0202 02:57:14.296828 139702527031040 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.035428285598755, loss=4.402929782867432
I0202 02:57:59.763446 139702543816448 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.2369208335876465, loss=2.268092155456543
I0202 02:58:45.867141 139702527031040 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.0227415561676025, loss=3.4145023822784424
I0202 02:59:32.165846 139702543816448 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.1209492683410645, loss=4.738518238067627
I0202 03:00:18.594689 139702527031040 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.032953977584839, loss=3.351677179336548
I0202 03:01:05.033250 139702543816448 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.1254849433898926, loss=2.76916766166687
I0202 03:01:51.357988 139702527031040 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.2871949672698975, loss=2.0712969303131104
I0202 03:02:37.902611 139702543816448 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.2865874767303467, loss=2.258518695831299
I0202 03:03:24.117207 139702527031040 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.24595046043396, loss=2.3841776847839355
I0202 03:03:46.388170 139863983413056 spec.py:321] Evaluating on the training split.
I0202 03:03:56.664147 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 03:04:26.547911 139863983413056 spec.py:349] Evaluating on the test split.
I0202 03:04:28.184582 139863983413056 submission_runner.py:408] Time since start: 53502.97s, 	Step: 108050, 	{'train/accuracy': 0.7084179520606995, 'train/loss': 1.183990240097046, 'validation/accuracy': 0.661359965801239, 'validation/loss': 1.3978753089904785, 'validation/num_examples': 50000, 'test/accuracy': 0.5369000434875488, 'test/loss': 2.0427968502044678, 'test/num_examples': 10000, 'score': 49194.9777302742, 'total_duration': 53502.97209262848, 'accumulated_submission_time': 49194.9777302742, 'accumulated_eval_time': 4297.330185413361, 'accumulated_logging_time': 4.968867778778076}
I0202 03:04:28.221958 139702543816448 logging_writer.py:48] [108050] accumulated_eval_time=4297.330185, accumulated_logging_time=4.968868, accumulated_submission_time=49194.977730, global_step=108050, preemption_count=0, score=49194.977730, test/accuracy=0.536900, test/loss=2.042797, test/num_examples=10000, total_duration=53502.972093, train/accuracy=0.708418, train/loss=1.183990, validation/accuracy=0.661360, validation/loss=1.397875, validation/num_examples=50000
I0202 03:04:48.551022 139702527031040 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.588120698928833, loss=2.131873846054077
I0202 03:05:32.998032 139702543816448 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.0343148708343506, loss=3.4488525390625
I0202 03:06:19.031471 139702527031040 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.2674241065979004, loss=2.3716633319854736
I0202 03:07:05.508051 139702543816448 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.318044900894165, loss=4.701435089111328
I0202 03:07:51.474535 139702527031040 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.05749773979187, loss=4.771082878112793
I0202 03:08:37.505449 139702543816448 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.1152055263519287, loss=4.10467004776001
I0202 03:09:23.832718 139702527031040 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.9687819480895996, loss=4.526607513427734
I0202 03:10:10.279082 139702543816448 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.0706334114074707, loss=4.196476459503174
I0202 03:10:56.892470 139702527031040 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.1126699447631836, loss=3.870622158050537
I0202 03:11:28.288601 139863983413056 spec.py:321] Evaluating on the training split.
I0202 03:11:38.758620 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 03:12:04.112088 139863983413056 spec.py:349] Evaluating on the test split.
I0202 03:12:05.748035 139863983413056 submission_runner.py:408] Time since start: 53960.54s, 	Step: 108970, 	{'train/accuracy': 0.7154492139816284, 'train/loss': 1.1541640758514404, 'validation/accuracy': 0.6593199968338013, 'validation/loss': 1.3946455717086792, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.0466535091400146, 'test/num_examples': 10000, 'score': 49614.986085653305, 'total_duration': 53960.53556919098, 'accumulated_submission_time': 49614.986085653305, 'accumulated_eval_time': 4334.789614200592, 'accumulated_logging_time': 5.015544414520264}
I0202 03:12:05.781845 139702543816448 logging_writer.py:48] [108970] accumulated_eval_time=4334.789614, accumulated_logging_time=5.015544, accumulated_submission_time=49614.986086, global_step=108970, preemption_count=0, score=49614.986086, test/accuracy=0.534300, test/loss=2.046654, test/num_examples=10000, total_duration=53960.535569, train/accuracy=0.715449, train/loss=1.154164, validation/accuracy=0.659320, validation/loss=1.394646, validation/num_examples=50000
I0202 03:12:18.152177 139702527031040 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.8581278324127197, loss=3.9327337741851807
I0202 03:13:01.141777 139702543816448 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.389315366744995, loss=2.1747958660125732
I0202 03:13:47.086503 139702527031040 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.4610798358917236, loss=2.121983051300049
I0202 03:14:33.180957 139702543816448 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.416752338409424, loss=2.007389545440674
I0202 03:15:19.412444 139702527031040 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.13566255569458, loss=2.8460745811462402
I0202 03:16:05.622013 139702543816448 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.410475015640259, loss=2.2309446334838867
I0202 03:16:51.911113 139702527031040 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.460757255554199, loss=4.5934576988220215
I0202 03:17:38.280821 139702543816448 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.1543962955474854, loss=4.704883575439453
I0202 03:18:24.375911 139702527031040 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.0714056491851807, loss=4.630452632904053
I0202 03:19:05.999127 139863983413056 spec.py:321] Evaluating on the training split.
I0202 03:19:16.186919 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 03:19:41.508544 139863983413056 spec.py:349] Evaluating on the test split.
I0202 03:19:43.146046 139863983413056 submission_runner.py:408] Time since start: 54417.93s, 	Step: 109892, 	{'train/accuracy': 0.7190625071525574, 'train/loss': 1.153494119644165, 'validation/accuracy': 0.6587399840354919, 'validation/loss': 1.4109584093093872, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.0479791164398193, 'test/num_examples': 10000, 'score': 50035.14333152771, 'total_duration': 54417.93356990814, 'accumulated_submission_time': 50035.14333152771, 'accumulated_eval_time': 4371.936519861221, 'accumulated_logging_time': 5.060314655303955}
I0202 03:19:43.179666 139702543816448 logging_writer.py:48] [109892] accumulated_eval_time=4371.936520, accumulated_logging_time=5.060315, accumulated_submission_time=50035.143332, global_step=109892, preemption_count=0, score=50035.143332, test/accuracy=0.537700, test/loss=2.047979, test/num_examples=10000, total_duration=54417.933570, train/accuracy=0.719063, train/loss=1.153494, validation/accuracy=0.658740, validation/loss=1.410958, validation/num_examples=50000
I0202 03:19:46.802585 139702527031040 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.9383600950241089, loss=3.4466488361358643
I0202 03:20:28.262739 139702543816448 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.221527099609375, loss=4.634578227996826
I0202 03:21:14.446197 139702527031040 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.6305131912231445, loss=2.4921469688415527
I0202 03:22:00.923225 139702543816448 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.3232414722442627, loss=2.130474090576172
I0202 03:22:47.027842 139702527031040 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.9103654623031616, loss=4.129997730255127
I0202 03:23:32.891043 139702543816448 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.011519432067871, loss=3.058866500854492
I0202 03:24:19.355967 139702527031040 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.4055492877960205, loss=2.2287869453430176
I0202 03:25:05.554587 139702543816448 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.1612753868103027, loss=2.5206289291381836
I0202 03:25:51.671171 139702527031040 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.1777215003967285, loss=2.6435964107513428
I0202 03:26:37.692376 139702543816448 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.2972044944763184, loss=2.2421951293945312
I0202 03:26:43.435747 139863983413056 spec.py:321] Evaluating on the training split.
I0202 03:26:53.933719 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 03:27:20.870509 139863983413056 spec.py:349] Evaluating on the test split.
I0202 03:27:22.515616 139863983413056 submission_runner.py:408] Time since start: 54877.30s, 	Step: 110814, 	{'train/accuracy': 0.707324206829071, 'train/loss': 1.1942484378814697, 'validation/accuracy': 0.6566799879074097, 'validation/loss': 1.4237462282180786, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.042734384536743, 'test/num_examples': 10000, 'score': 50455.339581251144, 'total_duration': 54877.30311059952, 'accumulated_submission_time': 50455.339581251144, 'accumulated_eval_time': 4411.016355514526, 'accumulated_logging_time': 5.104965448379517}
I0202 03:27:22.551806 139702527031040 logging_writer.py:48] [110814] accumulated_eval_time=4411.016356, accumulated_logging_time=5.104965, accumulated_submission_time=50455.339581, global_step=110814, preemption_count=0, score=50455.339581, test/accuracy=0.537200, test/loss=2.042734, test/num_examples=10000, total_duration=54877.303111, train/accuracy=0.707324, train/loss=1.194248, validation/accuracy=0.656680, validation/loss=1.423746, validation/num_examples=50000
I0202 03:27:57.716919 139702543816448 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.544407844543457, loss=2.2486884593963623
I0202 03:28:43.478372 139702527031040 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.408722400665283, loss=2.1608142852783203
I0202 03:29:29.529926 139702543816448 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.568873643875122, loss=2.1711130142211914
I0202 03:30:16.175200 139702527031040 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.6552281379699707, loss=2.555283546447754
I0202 03:31:02.499307 139702543816448 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.1496808528900146, loss=4.7425079345703125
I0202 03:31:48.959992 139702527031040 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.3709986209869385, loss=2.085078477859497
I0202 03:32:34.873348 139702543816448 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.3906238079071045, loss=2.231152296066284
I0202 03:33:21.364418 139702527031040 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.9914683103561401, loss=3.572138786315918
I0202 03:34:07.608273 139702543816448 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.101191520690918, loss=3.9552175998687744
I0202 03:34:22.955164 139863983413056 spec.py:321] Evaluating on the training split.
I0202 03:34:33.448936 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 03:35:00.105585 139863983413056 spec.py:349] Evaluating on the test split.
I0202 03:35:01.764977 139863983413056 submission_runner.py:408] Time since start: 55336.55s, 	Step: 111735, 	{'train/accuracy': 0.7171288728713989, 'train/loss': 1.144237995147705, 'validation/accuracy': 0.6643199920654297, 'validation/loss': 1.3885068893432617, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.033914566040039, 'test/num_examples': 10000, 'score': 50875.68436551094, 'total_duration': 55336.55248832703, 'accumulated_submission_time': 50875.68436551094, 'accumulated_eval_time': 4449.826142311096, 'accumulated_logging_time': 5.151665925979614}
I0202 03:35:01.801414 139702527031040 logging_writer.py:48] [111735] accumulated_eval_time=4449.826142, accumulated_logging_time=5.151666, accumulated_submission_time=50875.684366, global_step=111735, preemption_count=0, score=50875.684366, test/accuracy=0.539200, test/loss=2.033915, test/num_examples=10000, total_duration=55336.552488, train/accuracy=0.717129, train/loss=1.144238, validation/accuracy=0.664320, validation/loss=1.388507, validation/num_examples=50000
I0202 03:35:28.164039 139702543816448 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.0849950313568115, loss=3.631861686706543
I0202 03:36:13.037425 139702527031040 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.1083762645721436, loss=3.2832064628601074
I0202 03:36:59.071632 139702543816448 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.3409993648529053, loss=2.4044406414031982
I0202 03:37:45.622729 139702527031040 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.2688162326812744, loss=2.0952930450439453
I0202 03:38:31.423763 139702543816448 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.165788412094116, loss=3.92921781539917
I0202 03:39:17.659046 139702527031040 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.6910696029663086, loss=2.142610549926758
I0202 03:40:03.889871 139702543816448 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.3933260440826416, loss=2.5754342079162598
I0202 03:40:49.997608 139702527031040 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.9794130325317383, loss=3.078282356262207
I0202 03:41:36.588761 139702543816448 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.437251091003418, loss=2.079401969909668
I0202 03:42:01.852456 139863983413056 spec.py:321] Evaluating on the training split.
I0202 03:42:12.438485 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 03:42:39.837352 139863983413056 spec.py:349] Evaluating on the test split.
I0202 03:42:41.474547 139863983413056 submission_runner.py:408] Time since start: 55796.26s, 	Step: 112656, 	{'train/accuracy': 0.7238867282867432, 'train/loss': 1.1074589490890503, 'validation/accuracy': 0.6677199602127075, 'validation/loss': 1.3568204641342163, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.0181593894958496, 'test/num_examples': 10000, 'score': 51295.67676925659, 'total_duration': 55796.26206231117, 'accumulated_submission_time': 51295.67676925659, 'accumulated_eval_time': 4489.4482209682465, 'accumulated_logging_time': 5.197967767715454}
I0202 03:42:41.509382 139702527031040 logging_writer.py:48] [112656] accumulated_eval_time=4489.448221, accumulated_logging_time=5.197968, accumulated_submission_time=51295.676769, global_step=112656, preemption_count=0, score=51295.676769, test/accuracy=0.538500, test/loss=2.018159, test/num_examples=10000, total_duration=55796.262062, train/accuracy=0.723887, train/loss=1.107459, validation/accuracy=0.667720, validation/loss=1.356820, validation/num_examples=50000
I0202 03:42:59.451251 139702543816448 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.073599338531494, loss=2.945570945739746
I0202 03:43:43.322724 139702527031040 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.37198805809021, loss=2.0854692459106445
I0202 03:44:29.260886 139702543816448 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.3284080028533936, loss=2.608185052871704
I0202 03:45:15.641253 139702527031040 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.1197502613067627, loss=4.305840492248535
I0202 03:46:01.854695 139702543816448 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.5174946784973145, loss=2.183689594268799
I0202 03:46:48.034378 139702527031040 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.674569845199585, loss=2.49849796295166
I0202 03:47:34.205249 139702543816448 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.5064566135406494, loss=2.0759289264678955
I0202 03:48:20.281073 139702527031040 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.592103958129883, loss=2.2976651191711426
I0202 03:49:06.426746 139702543816448 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.6423840522766113, loss=2.0761966705322266
I0202 03:49:41.501248 139863983413056 spec.py:321] Evaluating on the training split.
I0202 03:49:52.311995 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 03:50:16.768635 139863983413056 spec.py:349] Evaluating on the test split.
I0202 03:50:18.407486 139863983413056 submission_runner.py:408] Time since start: 56253.20s, 	Step: 113578, 	{'train/accuracy': 0.7433788776397705, 'train/loss': 1.0337588787078857, 'validation/accuracy': 0.6680999994277954, 'validation/loss': 1.3554725646972656, 'validation/num_examples': 50000, 'test/accuracy': 0.5476000308990479, 'test/loss': 1.9904879331588745, 'test/num_examples': 10000, 'score': 51715.61089348793, 'total_duration': 56253.195014476776, 'accumulated_submission_time': 51715.61089348793, 'accumulated_eval_time': 4526.354462623596, 'accumulated_logging_time': 5.241725444793701}
I0202 03:50:18.442919 139702527031040 logging_writer.py:48] [113578] accumulated_eval_time=4526.354463, accumulated_logging_time=5.241725, accumulated_submission_time=51715.610893, global_step=113578, preemption_count=0, score=51715.610893, test/accuracy=0.547600, test/loss=1.990488, test/num_examples=10000, total_duration=56253.195014, train/accuracy=0.743379, train/loss=1.033759, validation/accuracy=0.668100, validation/loss=1.355473, validation/num_examples=50000
I0202 03:50:27.611036 139702543816448 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.7619216442108154, loss=2.0905394554138184
I0202 03:51:10.054534 139702527031040 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.4101004600524902, loss=2.3248419761657715
I0202 03:51:55.886476 139702543816448 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.3268604278564453, loss=2.4019250869750977
I0202 03:52:42.285357 139702527031040 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.5815086364746094, loss=2.183751344680786
I0202 03:53:27.932859 139702543816448 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.283106803894043, loss=4.322056770324707
I0202 03:54:14.084116 139702527031040 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.441786766052246, loss=2.07169246673584
I0202 03:55:00.174407 139702543816448 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.3277852535247803, loss=3.880866050720215
I0202 03:55:46.179037 139702527031040 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.375779390335083, loss=2.1400046348571777
I0202 03:56:32.294526 139702543816448 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.4251558780670166, loss=4.496890068054199
I0202 03:57:18.479397 139702527031040 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.1153976917266846, loss=2.840712547302246
I0202 03:57:18.491968 139863983413056 spec.py:321] Evaluating on the training split.
I0202 03:57:28.929460 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 03:57:56.345532 139863983413056 spec.py:349] Evaluating on the test split.
I0202 03:57:57.990354 139863983413056 submission_runner.py:408] Time since start: 56712.78s, 	Step: 114501, 	{'train/accuracy': 0.7174609303474426, 'train/loss': 1.1560657024383545, 'validation/accuracy': 0.6663399934768677, 'validation/loss': 1.3868917226791382, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.03826904296875, 'test/num_examples': 10000, 'score': 52135.60233712196, 'total_duration': 56712.77788186073, 'accumulated_submission_time': 52135.60233712196, 'accumulated_eval_time': 4565.85283613205, 'accumulated_logging_time': 5.286535739898682}
I0202 03:57:58.022228 139702543816448 logging_writer.py:48] [114501] accumulated_eval_time=4565.852836, accumulated_logging_time=5.286536, accumulated_submission_time=52135.602337, global_step=114501, preemption_count=0, score=52135.602337, test/accuracy=0.545700, test/loss=2.038269, test/num_examples=10000, total_duration=56712.777882, train/accuracy=0.717461, train/loss=1.156066, validation/accuracy=0.666340, validation/loss=1.386892, validation/num_examples=50000
I0202 03:58:38.986178 139702527031040 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.4774532318115234, loss=2.139313220977783
I0202 03:59:24.831024 139702543816448 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.19193434715271, loss=2.934180974960327
I0202 04:00:11.341468 139702527031040 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.1512529850006104, loss=4.000176429748535
I0202 04:00:57.341457 139702543816448 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.186737060546875, loss=3.102268934249878
I0202 04:01:43.290909 139702527031040 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.5133259296417236, loss=2.2892508506774902
I0202 04:02:30.123367 139702543816448 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.6425986289978027, loss=1.9523948431015015
I0202 04:03:16.364350 139702527031040 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.436709403991699, loss=2.0524117946624756
I0202 04:04:02.307973 139702543816448 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.44860577583313, loss=2.2285056114196777
I0202 04:04:48.422344 139702527031040 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.4873552322387695, loss=2.140272855758667
I0202 04:04:58.316147 139863983413056 spec.py:321] Evaluating on the training split.
I0202 04:05:08.914500 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 04:05:35.714993 139863983413056 spec.py:349] Evaluating on the test split.
I0202 04:05:37.354977 139863983413056 submission_runner.py:408] Time since start: 57172.14s, 	Step: 115423, 	{'train/accuracy': 0.7275195121765137, 'train/loss': 1.0962249040603638, 'validation/accuracy': 0.6660999655723572, 'validation/loss': 1.3563467264175415, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 1.9989176988601685, 'test/num_examples': 10000, 'score': 52555.83881497383, 'total_duration': 57172.142484903336, 'accumulated_submission_time': 52555.83881497383, 'accumulated_eval_time': 4604.891656398773, 'accumulated_logging_time': 5.327480792999268}
I0202 04:05:37.394500 139702543816448 logging_writer.py:48] [115423] accumulated_eval_time=4604.891656, accumulated_logging_time=5.327481, accumulated_submission_time=52555.838815, global_step=115423, preemption_count=0, score=52555.838815, test/accuracy=0.538400, test/loss=1.998918, test/num_examples=10000, total_duration=57172.142485, train/accuracy=0.727520, train/loss=1.096225, validation/accuracy=0.666100, validation/loss=1.356347, validation/num_examples=50000
I0202 04:06:08.734768 139702527031040 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.4503695964813232, loss=2.5160486698150635
I0202 04:06:54.553805 139702543816448 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.674910545349121, loss=2.174626350402832
I0202 04:07:40.853530 139702527031040 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.334808111190796, loss=2.0058462619781494
I0202 04:08:27.207373 139702543816448 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.3065898418426514, loss=4.021100997924805
I0202 04:09:13.362338 139702527031040 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.580946445465088, loss=2.007594108581543
I0202 04:09:59.798854 139702543816448 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.2073817253112793, loss=3.2260947227478027
I0202 04:10:45.917161 139702527031040 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.5694565773010254, loss=2.1422016620635986
I0202 04:11:32.174959 139702543816448 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.3240506649017334, loss=3.4040234088897705
I0202 04:12:18.235166 139702527031040 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.5474205017089844, loss=2.1376402378082275
I0202 04:12:37.499225 139863983413056 spec.py:321] Evaluating on the training split.
I0202 04:12:47.912963 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 04:13:12.195446 139863983413056 spec.py:349] Evaluating on the test split.
I0202 04:13:13.848697 139863983413056 submission_runner.py:408] Time since start: 57628.64s, 	Step: 116343, 	{'train/accuracy': 0.7395312190055847, 'train/loss': 1.0397402048110962, 'validation/accuracy': 0.6725199818611145, 'validation/loss': 1.3434137105941772, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 1.994463562965393, 'test/num_examples': 10000, 'score': 52975.88430976868, 'total_duration': 57628.63620185852, 'accumulated_submission_time': 52975.88430976868, 'accumulated_eval_time': 4641.2410888671875, 'accumulated_logging_time': 5.377520561218262}
I0202 04:13:13.887537 139702543816448 logging_writer.py:48] [116343] accumulated_eval_time=4641.241089, accumulated_logging_time=5.377521, accumulated_submission_time=52975.884310, global_step=116343, preemption_count=0, score=52975.884310, test/accuracy=0.546800, test/loss=1.994464, test/num_examples=10000, total_duration=57628.636202, train/accuracy=0.739531, train/loss=1.039740, validation/accuracy=0.672520, validation/loss=1.343414, validation/num_examples=50000
I0202 04:13:37.029868 139702527031040 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.251943826675415, loss=2.655924081802368
I0202 04:14:21.496114 139702543816448 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.4183003902435303, loss=2.025160074234009
I0202 04:15:07.662346 139702527031040 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.337584972381592, loss=2.23344087600708
I0202 04:15:54.120432 139702543816448 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.543452024459839, loss=2.3473918437957764
I0202 04:16:39.987402 139702527031040 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.275322675704956, loss=3.703162908554077
I0202 04:17:25.930155 139702543816448 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.3500125408172607, loss=2.295288324356079
I0202 04:18:12.092207 139702527031040 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.710052967071533, loss=2.012749195098877
I0202 04:18:57.991458 139702543816448 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.2666945457458496, loss=3.804241180419922
I0202 04:19:43.917754 139702527031040 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.4696903228759766, loss=2.0861470699310303
I0202 04:20:14.088339 139863983413056 spec.py:321] Evaluating on the training split.
I0202 04:20:24.483400 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 04:20:52.091008 139863983413056 spec.py:349] Evaluating on the test split.
I0202 04:20:53.723107 139863983413056 submission_runner.py:408] Time since start: 58088.51s, 	Step: 117267, 	{'train/accuracy': 0.7267968654632568, 'train/loss': 1.0954101085662842, 'validation/accuracy': 0.6725599765777588, 'validation/loss': 1.3405753374099731, 'validation/num_examples': 50000, 'test/accuracy': 0.5462000370025635, 'test/loss': 1.9877381324768066, 'test/num_examples': 10000, 'score': 53396.02493786812, 'total_duration': 58088.51064157486, 'accumulated_submission_time': 53396.02493786812, 'accumulated_eval_time': 4680.875846385956, 'accumulated_logging_time': 5.427154302597046}
I0202 04:20:53.755714 139702543816448 logging_writer.py:48] [117267] accumulated_eval_time=4680.875846, accumulated_logging_time=5.427154, accumulated_submission_time=53396.024938, global_step=117267, preemption_count=0, score=53396.024938, test/accuracy=0.546200, test/loss=1.987738, test/num_examples=10000, total_duration=58088.510642, train/accuracy=0.726797, train/loss=1.095410, validation/accuracy=0.672560, validation/loss=1.340575, validation/num_examples=50000
I0202 04:21:07.336756 139702527031040 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.4959001541137695, loss=2.2844481468200684
I0202 04:21:50.649757 139702543816448 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.3930604457855225, loss=2.020800828933716
I0202 04:22:36.843814 139702527031040 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.6312954425811768, loss=2.079710006713867
I0202 04:23:23.630167 139702543816448 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.3341503143310547, loss=2.718465805053711
I0202 04:24:09.673744 139702527031040 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.369431972503662, loss=1.963038682937622
I0202 04:24:55.525345 139702543816448 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.5618276596069336, loss=2.049356698989868
I0202 04:25:41.776662 139702527031040 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.405932664871216, loss=1.9975123405456543
I0202 04:26:27.833856 139702543816448 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.589719295501709, loss=2.0405476093292236
I0202 04:27:13.900328 139702527031040 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.624056816101074, loss=2.0955846309661865
I0202 04:27:53.932799 139863983413056 spec.py:321] Evaluating on the training split.
I0202 04:28:04.622117 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 04:28:32.080939 139863983413056 spec.py:349] Evaluating on the test split.
I0202 04:28:33.719766 139863983413056 submission_runner.py:408] Time since start: 58548.51s, 	Step: 118189, 	{'train/accuracy': 0.7324609160423279, 'train/loss': 1.0725865364074707, 'validation/accuracy': 0.6753399968147278, 'validation/loss': 1.3302712440490723, 'validation/num_examples': 50000, 'test/accuracy': 0.551300048828125, 'test/loss': 1.9637449979782104, 'test/num_examples': 10000, 'score': 53816.141922950745, 'total_duration': 58548.5072760582, 'accumulated_submission_time': 53816.141922950745, 'accumulated_eval_time': 4720.6627950668335, 'accumulated_logging_time': 5.471050262451172}
I0202 04:28:33.755396 139702543816448 logging_writer.py:48] [118189] accumulated_eval_time=4720.662795, accumulated_logging_time=5.471050, accumulated_submission_time=53816.141923, global_step=118189, preemption_count=0, score=53816.141923, test/accuracy=0.551300, test/loss=1.963745, test/num_examples=10000, total_duration=58548.507276, train/accuracy=0.732461, train/loss=1.072587, validation/accuracy=0.675340, validation/loss=1.330271, validation/num_examples=50000
I0202 04:28:38.541200 139702527031040 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.5360240936279297, loss=2.0234687328338623
I0202 04:29:20.624908 139702543816448 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.8056583404541016, loss=2.0886266231536865
I0202 04:30:06.350388 139702527031040 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.453295946121216, loss=4.604455947875977
I0202 04:30:52.584418 139702543816448 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.204538583755493, loss=3.632999897003174
I0202 04:31:38.610039 139702527031040 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.998877763748169, loss=2.117253065109253
I0202 04:32:24.778030 139702543816448 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.490741729736328, loss=3.9099133014678955
I0202 04:33:10.947193 139702527031040 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.8597288131713867, loss=2.0869126319885254
I0202 04:33:57.268949 139702543816448 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.4653635025024414, loss=2.733257532119751
I0202 04:34:43.178599 139702527031040 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.930037021636963, loss=1.975919485092163
I0202 04:35:29.299795 139702543816448 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.5682613849639893, loss=2.129549264907837
I0202 04:35:34.032171 139863983413056 spec.py:321] Evaluating on the training split.
I0202 04:35:45.210122 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 04:36:08.846887 139863983413056 spec.py:349] Evaluating on the test split.
I0202 04:36:10.484740 139863983413056 submission_runner.py:408] Time since start: 59005.27s, 	Step: 119112, 	{'train/accuracy': 0.7394335865974426, 'train/loss': 1.0667121410369873, 'validation/accuracy': 0.6752600073814392, 'validation/loss': 1.3475139141082764, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 1.9856129884719849, 'test/num_examples': 10000, 'score': 54236.35823750496, 'total_duration': 59005.27227306366, 'accumulated_submission_time': 54236.35823750496, 'accumulated_eval_time': 4757.115355014801, 'accumulated_logging_time': 5.5186426639556885}
I0202 04:36:10.521184 139702527031040 logging_writer.py:48] [119112] accumulated_eval_time=4757.115355, accumulated_logging_time=5.518643, accumulated_submission_time=54236.358238, global_step=119112, preemption_count=0, score=54236.358238, test/accuracy=0.552700, test/loss=1.985613, test/num_examples=10000, total_duration=59005.272273, train/accuracy=0.739434, train/loss=1.066712, validation/accuracy=0.675260, validation/loss=1.347514, validation/num_examples=50000
I0202 04:36:46.426946 139702543816448 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.5278706550598145, loss=2.0279340744018555
I0202 04:37:32.379433 139702527031040 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.828838586807251, loss=2.0549917221069336
I0202 04:38:18.809102 139702543816448 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.4585416316986084, loss=2.4228103160858154
I0202 04:39:05.051153 139702527031040 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.2364323139190674, loss=2.456393241882324
I0202 04:39:51.195220 139702543816448 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.5573649406433105, loss=2.5204601287841797
I0202 04:40:37.740179 139702527031040 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.4730117321014404, loss=2.1485018730163574
I0202 04:41:23.742417 139702543816448 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.439236879348755, loss=3.400599956512451
I0202 04:42:09.939190 139702527031040 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.5684216022491455, loss=4.303878307342529
I0202 04:42:56.251869 139702543816448 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.649468183517456, loss=2.012068033218384
I0202 04:43:10.670954 139863983413056 spec.py:321] Evaluating on the training split.
I0202 04:43:21.120161 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 04:43:44.287750 139863983413056 spec.py:349] Evaluating on the test split.
I0202 04:43:45.926361 139863983413056 submission_runner.py:408] Time since start: 59460.71s, 	Step: 120033, 	{'train/accuracy': 0.729199230670929, 'train/loss': 1.0772337913513184, 'validation/accuracy': 0.675279974937439, 'validation/loss': 1.3255867958068848, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 1.9687131643295288, 'test/num_examples': 10000, 'score': 54656.450139045715, 'total_duration': 59460.713866472244, 'accumulated_submission_time': 54656.450139045715, 'accumulated_eval_time': 4792.37073636055, 'accumulated_logging_time': 5.564018726348877}
I0202 04:43:45.965361 139702527031040 logging_writer.py:48] [120033] accumulated_eval_time=4792.370736, accumulated_logging_time=5.564019, accumulated_submission_time=54656.450139, global_step=120033, preemption_count=0, score=54656.450139, test/accuracy=0.549100, test/loss=1.968713, test/num_examples=10000, total_duration=59460.713866, train/accuracy=0.729199, train/loss=1.077234, validation/accuracy=0.675280, validation/loss=1.325587, validation/num_examples=50000
I0202 04:44:13.128240 139702543816448 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.2357518672943115, loss=3.4756863117218018
I0202 04:44:58.304948 139702527031040 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.623279094696045, loss=3.9771077632904053
I0202 04:45:44.634194 139702543816448 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.295727252960205, loss=3.5337629318237305
I0202 04:46:30.972501 139702527031040 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.569287061691284, loss=2.1294350624084473
I0202 04:47:17.066416 139702543816448 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.4135477542877197, loss=2.033608913421631
I0202 04:48:03.008787 139702527031040 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.256791114807129, loss=3.3320558071136475
I0202 04:48:49.050575 139702543816448 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.4634478092193604, loss=2.1091392040252686
I0202 04:49:35.208657 139702527031040 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.504866123199463, loss=1.983285903930664
I0202 04:50:21.493695 139702543816448 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.863877296447754, loss=2.0995514392852783
I0202 04:50:46.078944 139863983413056 spec.py:321] Evaluating on the training split.
I0202 04:50:56.485089 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 04:51:22.634721 139863983413056 spec.py:349] Evaluating on the test split.
I0202 04:51:24.279680 139863983413056 submission_runner.py:408] Time since start: 59919.07s, 	Step: 120955, 	{'train/accuracy': 0.7299999594688416, 'train/loss': 1.0698095560073853, 'validation/accuracy': 0.6780999898910522, 'validation/loss': 1.307468056678772, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 1.95231294631958, 'test/num_examples': 10000, 'score': 55076.50438141823, 'total_duration': 59919.06721377373, 'accumulated_submission_time': 55076.50438141823, 'accumulated_eval_time': 4830.57146859169, 'accumulated_logging_time': 5.6144208908081055}
I0202 04:51:24.313153 139702527031040 logging_writer.py:48] [120955] accumulated_eval_time=4830.571469, accumulated_logging_time=5.614421, accumulated_submission_time=55076.504381, global_step=120955, preemption_count=0, score=55076.504381, test/accuracy=0.552900, test/loss=1.952313, test/num_examples=10000, total_duration=59919.067214, train/accuracy=0.730000, train/loss=1.069810, validation/accuracy=0.678100, validation/loss=1.307468, validation/num_examples=50000
I0202 04:51:42.691119 139702543816448 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.761486768722534, loss=1.9293875694274902
I0202 04:52:26.470404 139702527031040 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.47050142288208, loss=2.008112907409668
I0202 04:53:12.794780 139702543816448 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.732037305831909, loss=1.9370265007019043
I0202 04:53:58.935363 139702527031040 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.4589176177978516, loss=3.5018246173858643
I0202 04:54:45.635901 139702543816448 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.3877391815185547, loss=3.7768239974975586
I0202 04:55:31.767749 139702527031040 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.8146393299102783, loss=2.1968953609466553
I0202 04:56:18.013896 139702543816448 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.3097214698791504, loss=3.797229051589966
I0202 04:57:04.255669 139702527031040 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.4977200031280518, loss=2.225123882293701
I0202 04:57:50.579738 139702543816448 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.9395911693573, loss=1.9526705741882324
I0202 04:58:24.448506 139863983413056 spec.py:321] Evaluating on the training split.
I0202 04:58:34.818890 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 04:59:00.048401 139863983413056 spec.py:349] Evaluating on the test split.
I0202 04:59:01.706793 139863983413056 submission_runner.py:408] Time since start: 60376.49s, 	Step: 121875, 	{'train/accuracy': 0.7435156106948853, 'train/loss': 1.0205103158950806, 'validation/accuracy': 0.6786999702453613, 'validation/loss': 1.2984049320220947, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.928056001663208, 'test/num_examples': 10000, 'score': 55496.58257865906, 'total_duration': 60376.49431824684, 'accumulated_submission_time': 55496.58257865906, 'accumulated_eval_time': 4867.829748630524, 'accumulated_logging_time': 5.6567864418029785}
I0202 04:59:01.741518 139702527031040 logging_writer.py:48] [121875] accumulated_eval_time=4867.829749, accumulated_logging_time=5.656786, accumulated_submission_time=55496.582579, global_step=121875, preemption_count=0, score=55496.582579, test/accuracy=0.555000, test/loss=1.928056, test/num_examples=10000, total_duration=60376.494318, train/accuracy=0.743516, train/loss=1.020510, validation/accuracy=0.678700, validation/loss=1.298405, validation/num_examples=50000
I0202 04:59:12.127064 139702543816448 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.3300514221191406, loss=3.741462469100952
I0202 04:59:54.876263 139702527031040 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.517012357711792, loss=1.8821464776992798
I0202 05:00:41.129879 139702543816448 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.6079044342041016, loss=2.044005870819092
I0202 05:01:27.504754 139702527031040 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.5430848598480225, loss=4.503270626068115
I0202 05:02:13.501877 139702543816448 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.0240650177001953, loss=2.5867936611175537
I0202 05:02:59.906970 139702527031040 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.760114908218384, loss=4.535965442657471
I0202 05:03:46.057818 139702543816448 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.8350517749786377, loss=1.948119878768921
I0202 05:04:32.331038 139702527031040 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.6333723068237305, loss=4.488833904266357
I0202 05:05:19.085390 139702543816448 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.4337258338928223, loss=2.9468445777893066
I0202 05:06:01.859889 139863983413056 spec.py:321] Evaluating on the training split.
I0202 05:06:12.307169 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 05:06:38.691537 139863983413056 spec.py:349] Evaluating on the test split.
I0202 05:06:40.332149 139863983413056 submission_runner.py:408] Time since start: 60835.12s, 	Step: 122794, 	{'train/accuracy': 0.7494726181030273, 'train/loss': 1.0063884258270264, 'validation/accuracy': 0.6796199679374695, 'validation/loss': 1.3093905448913574, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 1.9321649074554443, 'test/num_examples': 10000, 'score': 55916.64206838608, 'total_duration': 60835.119685173035, 'accumulated_submission_time': 55916.64206838608, 'accumulated_eval_time': 4906.302020072937, 'accumulated_logging_time': 5.70208215713501}
I0202 05:06:40.368284 139702527031040 logging_writer.py:48] [122794] accumulated_eval_time=4906.302020, accumulated_logging_time=5.702082, accumulated_submission_time=55916.642068, global_step=122794, preemption_count=0, score=55916.642068, test/accuracy=0.560100, test/loss=1.932165, test/num_examples=10000, total_duration=60835.119685, train/accuracy=0.749473, train/loss=1.006388, validation/accuracy=0.679620, validation/loss=1.309391, validation/num_examples=50000
I0202 05:06:43.160856 139702543816448 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.588728666305542, loss=4.556952476501465
I0202 05:07:24.839628 139702527031040 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.613434314727783, loss=2.3738155364990234
I0202 05:08:10.911052 139702543816448 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.8639848232269287, loss=1.9404897689819336
I0202 05:08:57.422508 139702527031040 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.7626028060913086, loss=1.9677140712738037
I0202 05:09:43.597864 139702543816448 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.996619701385498, loss=4.373678207397461
I0202 05:10:29.687097 139702527031040 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.6462721824645996, loss=2.1603307723999023
I0202 05:11:15.927906 139702543816448 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.3386082649230957, loss=3.0614383220672607
I0202 05:12:02.253929 139702527031040 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.3353323936462402, loss=3.596726894378662
I0202 05:12:48.362066 139702543816448 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.9063758850097656, loss=1.991523265838623
I0202 05:13:34.621199 139702527031040 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.689286231994629, loss=2.6453957557678223
I0202 05:13:40.701164 139863983413056 spec.py:321] Evaluating on the training split.
I0202 05:13:51.232582 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 05:14:15.547245 139863983413056 spec.py:349] Evaluating on the test split.
I0202 05:14:17.189910 139863983413056 submission_runner.py:408] Time since start: 61291.98s, 	Step: 123715, 	{'train/accuracy': 0.7424414157867432, 'train/loss': 1.035762071609497, 'validation/accuracy': 0.6828199625015259, 'validation/loss': 1.2859761714935303, 'validation/num_examples': 50000, 'test/accuracy': 0.5570999979972839, 'test/loss': 1.9202402830123901, 'test/num_examples': 10000, 'score': 56336.915759801865, 'total_duration': 61291.97744345665, 'accumulated_submission_time': 56336.915759801865, 'accumulated_eval_time': 4942.790773868561, 'accumulated_logging_time': 5.748456001281738}
I0202 05:14:17.227428 139702543816448 logging_writer.py:48] [123715] accumulated_eval_time=4942.790774, accumulated_logging_time=5.748456, accumulated_submission_time=56336.915760, global_step=123715, preemption_count=0, score=56336.915760, test/accuracy=0.557100, test/loss=1.920240, test/num_examples=10000, total_duration=61291.977443, train/accuracy=0.742441, train/loss=1.035762, validation/accuracy=0.682820, validation/loss=1.285976, validation/num_examples=50000
I0202 05:14:51.970780 139702527031040 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.8583693504333496, loss=4.535520076751709
I0202 05:15:38.096875 139702543816448 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.9004876613616943, loss=2.071035861968994
I0202 05:16:24.398369 139702527031040 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.7108418941497803, loss=3.996857166290283
I0202 05:17:10.851385 139702543816448 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.4262566566467285, loss=3.8902177810668945
I0202 05:17:57.191168 139702527031040 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.6753132343292236, loss=2.2808570861816406
I0202 05:18:43.452842 139702543816448 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.8288979530334473, loss=4.474186420440674
I0202 05:19:29.492374 139702527031040 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.867680788040161, loss=2.0182437896728516
I0202 05:20:15.603864 139702543816448 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.8428285121917725, loss=1.9282474517822266
I0202 05:21:01.727974 139702527031040 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.5199620723724365, loss=2.7095890045166016
I0202 05:21:17.479495 139863983413056 spec.py:321] Evaluating on the training split.
I0202 05:21:27.857261 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 05:21:56.245117 139863983413056 spec.py:349] Evaluating on the test split.
I0202 05:21:57.878194 139863983413056 submission_runner.py:408] Time since start: 61752.67s, 	Step: 124636, 	{'train/accuracy': 0.7409374713897705, 'train/loss': 1.054355263710022, 'validation/accuracy': 0.6823399662971497, 'validation/loss': 1.299355149269104, 'validation/num_examples': 50000, 'test/accuracy': 0.5631000399589539, 'test/loss': 1.9321929216384888, 'test/num_examples': 10000, 'score': 56757.108996629715, 'total_duration': 61752.66572880745, 'accumulated_submission_time': 56757.108996629715, 'accumulated_eval_time': 4983.189473390579, 'accumulated_logging_time': 5.796940326690674}
I0202 05:21:57.911578 139702543816448 logging_writer.py:48] [124636] accumulated_eval_time=4983.189473, accumulated_logging_time=5.796940, accumulated_submission_time=56757.108997, global_step=124636, preemption_count=0, score=56757.108997, test/accuracy=0.563100, test/loss=1.932193, test/num_examples=10000, total_duration=61752.665729, train/accuracy=0.740937, train/loss=1.054355, validation/accuracy=0.682340, validation/loss=1.299355, validation/num_examples=50000
I0202 05:22:23.859463 139702527031040 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.727205514907837, loss=2.0547211170196533
I0202 05:23:09.400189 139702543816448 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.8905866146087646, loss=1.9662480354309082
I0202 05:23:55.437085 139702527031040 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.7673747539520264, loss=1.9483952522277832
I0202 05:24:41.508047 139702543816448 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.6282358169555664, loss=2.9766621589660645
I0202 05:25:27.797774 139702527031040 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.7419474124908447, loss=1.9418649673461914
I0202 05:26:14.065218 139702543816448 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.852822780609131, loss=2.0091731548309326
I0202 05:27:00.078964 139702527031040 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.002660036087036, loss=1.9085168838500977
I0202 05:27:46.199731 139702543816448 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.577474594116211, loss=2.4343483448028564
I0202 05:28:32.675285 139702527031040 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.6800665855407715, loss=4.114108085632324
I0202 05:28:57.956459 139863983413056 spec.py:321] Evaluating on the training split.
I0202 05:29:08.412724 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 05:29:37.722724 139863983413056 spec.py:349] Evaluating on the test split.
I0202 05:29:39.370091 139863983413056 submission_runner.py:408] Time since start: 62214.16s, 	Step: 125557, 	{'train/accuracy': 0.7582616806030273, 'train/loss': 0.9785751104354858, 'validation/accuracy': 0.6868399977684021, 'validation/loss': 1.288220763206482, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 1.9148192405700684, 'test/num_examples': 10000, 'score': 57177.093199014664, 'total_duration': 62214.15758442879, 'accumulated_submission_time': 57177.093199014664, 'accumulated_eval_time': 5024.603069782257, 'accumulated_logging_time': 5.8421630859375}
I0202 05:29:39.410937 139702543816448 logging_writer.py:48] [125557] accumulated_eval_time=5024.603070, accumulated_logging_time=5.842163, accumulated_submission_time=57177.093199, global_step=125557, preemption_count=0, score=57177.093199, test/accuracy=0.563200, test/loss=1.914819, test/num_examples=10000, total_duration=62214.157584, train/accuracy=0.758262, train/loss=0.978575, validation/accuracy=0.686840, validation/loss=1.288221, validation/num_examples=50000
I0202 05:29:56.966451 139702527031040 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.62557053565979, loss=1.884308099746704
I0202 05:30:40.611545 139702543816448 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.751539945602417, loss=3.4887678623199463
I0202 05:31:26.705191 139702527031040 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.018057346343994, loss=2.113710403442383
I0202 05:32:13.106749 139702543816448 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.55120849609375, loss=2.9490511417388916
I0202 05:32:58.958416 139702527031040 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.64199161529541, loss=3.575731039047241
I0202 05:33:44.978631 139702543816448 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.3869524002075195, loss=2.064521551132202
I0202 05:34:31.056409 139702527031040 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.7895045280456543, loss=1.9562877416610718
I0202 05:35:17.184333 139702543816448 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.9569408893585205, loss=2.059328556060791
I0202 05:36:03.340728 139702527031040 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.5979740619659424, loss=4.044309616088867
I0202 05:36:39.752877 139863983413056 spec.py:321] Evaluating on the training split.
I0202 05:36:50.119595 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 05:37:19.172673 139863983413056 spec.py:349] Evaluating on the test split.
I0202 05:37:20.806799 139863983413056 submission_runner.py:408] Time since start: 62675.59s, 	Step: 126481, 	{'train/accuracy': 0.744921863079071, 'train/loss': 1.0215235948562622, 'validation/accuracy': 0.6889199614524841, 'validation/loss': 1.2752596139907837, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9243862628936768, 'test/num_examples': 10000, 'score': 57597.376497745514, 'total_duration': 62675.59431099892, 'accumulated_submission_time': 57597.376497745514, 'accumulated_eval_time': 5065.656978368759, 'accumulated_logging_time': 5.892143726348877}
I0202 05:37:20.847314 139702543816448 logging_writer.py:48] [126481] accumulated_eval_time=5065.656978, accumulated_logging_time=5.892144, accumulated_submission_time=57597.376498, global_step=126481, preemption_count=0, score=57597.376498, test/accuracy=0.561300, test/loss=1.924386, test/num_examples=10000, total_duration=62675.594311, train/accuracy=0.744922, train/loss=1.021524, validation/accuracy=0.688920, validation/loss=1.275260, validation/num_examples=50000
I0202 05:37:28.826263 139702527031040 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.6971476078033447, loss=2.5254857540130615
I0202 05:38:11.116030 139702543816448 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.029171943664551, loss=1.8771944046020508
I0202 05:38:57.251314 139702527031040 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.4474236965179443, loss=3.020894765853882
I0202 05:39:43.326962 139702543816448 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.849623203277588, loss=1.951656460762024
I0202 05:40:29.588452 139702527031040 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.9003801345825195, loss=2.0155580043792725
I0202 05:41:15.459988 139702543816448 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.893935441970825, loss=2.0179736614227295
I0202 05:42:01.435837 139702527031040 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.1076812744140625, loss=2.2504210472106934
I0202 05:42:47.429661 139702543816448 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.1233325004577637, loss=1.996168851852417
I0202 05:43:33.491899 139702527031040 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.686309576034546, loss=3.346766710281372
I0202 05:44:19.684207 139702543816448 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.7225403785705566, loss=1.9006706476211548
I0202 05:44:20.850382 139863983413056 spec.py:321] Evaluating on the training split.
I0202 05:44:31.033541 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 05:44:58.066045 139863983413056 spec.py:349] Evaluating on the test split.
I0202 05:44:59.700929 139863983413056 submission_runner.py:408] Time since start: 63134.49s, 	Step: 127404, 	{'train/accuracy': 0.7497265338897705, 'train/loss': 1.0092304944992065, 'validation/accuracy': 0.6896399855613708, 'validation/loss': 1.2700551748275757, 'validation/num_examples': 50000, 'test/accuracy': 0.572100043296814, 'test/loss': 1.8867379426956177, 'test/num_examples': 10000, 'score': 58017.32027029991, 'total_duration': 63134.48843693733, 'accumulated_submission_time': 58017.32027029991, 'accumulated_eval_time': 5104.507493257523, 'accumulated_logging_time': 5.9442808628082275}
I0202 05:44:59.739392 139702527031040 logging_writer.py:48] [127404] accumulated_eval_time=5104.507493, accumulated_logging_time=5.944281, accumulated_submission_time=58017.320270, global_step=127404, preemption_count=0, score=58017.320270, test/accuracy=0.572100, test/loss=1.886738, test/num_examples=10000, total_duration=63134.488437, train/accuracy=0.749727, train/loss=1.009230, validation/accuracy=0.689640, validation/loss=1.270055, validation/num_examples=50000
I0202 05:45:39.332068 139702543816448 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.4494431018829346, loss=2.4855995178222656
I0202 05:46:25.321017 139702527031040 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.081644058227539, loss=2.0864417552948
I0202 05:47:11.698278 139702543816448 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.926447629928589, loss=1.9879934787750244
I0202 05:47:57.569430 139702527031040 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.8255937099456787, loss=3.722991466522217
I0202 05:48:43.742056 139702543816448 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.8809778690338135, loss=1.9000458717346191
I0202 05:49:30.052111 139702527031040 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.783569574356079, loss=2.0804286003112793
I0202 05:50:16.278861 139702543816448 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.9542551040649414, loss=2.1400070190429688
I0202 05:51:02.242315 139702527031040 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.7372794151306152, loss=2.319730758666992
I0202 05:51:48.567323 139702543816448 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.0633411407470703, loss=1.9888949394226074
I0202 05:51:59.795342 139863983413056 spec.py:321] Evaluating on the training split.
I0202 05:52:10.188724 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 05:52:35.245633 139863983413056 spec.py:349] Evaluating on the test split.
I0202 05:52:36.889610 139863983413056 submission_runner.py:408] Time since start: 63591.68s, 	Step: 128326, 	{'train/accuracy': 0.7568749785423279, 'train/loss': 0.9682868123054504, 'validation/accuracy': 0.6923800110816956, 'validation/loss': 1.2558635473251343, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 1.8772536516189575, 'test/num_examples': 10000, 'score': 58437.31800484657, 'total_duration': 63591.67713737488, 'accumulated_submission_time': 58437.31800484657, 'accumulated_eval_time': 5141.601754665375, 'accumulated_logging_time': 5.991713047027588}
I0202 05:52:36.924218 139702527031040 logging_writer.py:48] [128326] accumulated_eval_time=5141.601755, accumulated_logging_time=5.991713, accumulated_submission_time=58437.318005, global_step=128326, preemption_count=0, score=58437.318005, test/accuracy=0.569800, test/loss=1.877254, test/num_examples=10000, total_duration=63591.677137, train/accuracy=0.756875, train/loss=0.968287, validation/accuracy=0.692380, validation/loss=1.255864, validation/num_examples=50000
I0202 05:53:06.857863 139702543816448 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.9403693675994873, loss=1.8272130489349365
I0202 05:53:52.280850 139702527031040 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.777388334274292, loss=2.162433385848999
I0202 05:54:38.483843 139702543816448 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.1561076641082764, loss=2.046884059906006
I0202 05:55:25.097005 139702527031040 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.8159639835357666, loss=2.1814072132110596
I0202 05:56:11.236335 139702543816448 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.1591084003448486, loss=1.901116967201233
I0202 05:56:57.606894 139702527031040 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.9793782234191895, loss=1.9441564083099365
I0202 05:57:43.922055 139702543816448 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.8679325580596924, loss=4.505151748657227
I0202 05:58:30.189034 139702527031040 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.582632303237915, loss=2.885608196258545
I0202 05:59:16.447629 139702543816448 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.7879788875579834, loss=4.0243072509765625
I0202 05:59:37.323727 139863983413056 spec.py:321] Evaluating on the training split.
I0202 05:59:47.812790 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 06:00:11.433793 139863983413056 spec.py:349] Evaluating on the test split.
I0202 06:00:13.079513 139863983413056 submission_runner.py:408] Time since start: 64047.87s, 	Step: 129247, 	{'train/accuracy': 0.7479491829872131, 'train/loss': 0.9964887499809265, 'validation/accuracy': 0.6901599764823914, 'validation/loss': 1.2488561868667603, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 1.8783817291259766, 'test/num_examples': 10000, 'score': 58857.65893149376, 'total_duration': 64047.86702609062, 'accumulated_submission_time': 58857.65893149376, 'accumulated_eval_time': 5177.357530832291, 'accumulated_logging_time': 6.036379098892212}
I0202 06:00:13.119514 139702527031040 logging_writer.py:48] [129247] accumulated_eval_time=5177.357531, accumulated_logging_time=6.036379, accumulated_submission_time=58857.658931, global_step=129247, preemption_count=0, score=58857.658931, test/accuracy=0.569200, test/loss=1.878382, test/num_examples=10000, total_duration=64047.867026, train/accuracy=0.747949, train/loss=0.996489, validation/accuracy=0.690160, validation/loss=1.248856, validation/num_examples=50000
I0202 06:00:34.677043 139702543816448 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.4502112865448, loss=2.330322742462158
I0202 06:01:18.778335 139702527031040 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.4983415603637695, loss=2.7416133880615234
I0202 06:02:05.123073 139702543816448 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.946573495864868, loss=1.9345811605453491
I0202 06:02:51.303650 139702527031040 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.2272205352783203, loss=1.8804810047149658
I0202 06:03:37.389300 139702543816448 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.9127252101898193, loss=2.2889347076416016
I0202 06:04:23.431885 139702527031040 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.248922109603882, loss=4.253242492675781
I0202 06:05:09.790282 139702543816448 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.04963755607605, loss=4.169863700866699
I0202 06:05:55.709351 139702527031040 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.6134023666381836, loss=3.4683001041412354
I0202 06:06:41.901852 139702543816448 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.7051682472229004, loss=4.23723030090332
I0202 06:07:13.319293 139863983413056 spec.py:321] Evaluating on the training split.
I0202 06:07:23.531444 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 06:07:52.640213 139863983413056 spec.py:349] Evaluating on the test split.
I0202 06:07:54.287701 139863983413056 submission_runner.py:408] Time since start: 64509.08s, 	Step: 130169, 	{'train/accuracy': 0.75501948595047, 'train/loss': 0.979481041431427, 'validation/accuracy': 0.6937199831008911, 'validation/loss': 1.2402454614639282, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 1.873192310333252, 'test/num_examples': 10000, 'score': 59277.79971027374, 'total_duration': 64509.075212717056, 'accumulated_submission_time': 59277.79971027374, 'accumulated_eval_time': 5218.325902700424, 'accumulated_logging_time': 6.086957216262817}
I0202 06:07:54.328361 139702527031040 logging_writer.py:48] [130169] accumulated_eval_time=5218.325903, accumulated_logging_time=6.086957, accumulated_submission_time=59277.799710, global_step=130169, preemption_count=0, score=59277.799710, test/accuracy=0.570500, test/loss=1.873192, test/num_examples=10000, total_duration=64509.075213, train/accuracy=0.755019, train/loss=0.979481, validation/accuracy=0.693720, validation/loss=1.240245, validation/num_examples=50000
I0202 06:08:07.089538 139702543816448 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.9494283199310303, loss=2.003627061843872
I0202 06:08:50.061252 139702527031040 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.8616316318511963, loss=2.3551719188690186
I0202 06:09:36.272721 139702543816448 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.078238010406494, loss=2.5153279304504395
I0202 06:10:23.023240 139702527031040 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.1146440505981445, loss=1.9873273372650146
I0202 06:11:09.117935 139702543816448 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.4647421836853027, loss=2.9337406158447266
I0202 06:11:55.263232 139702527031040 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.0738725662231445, loss=1.9634970426559448
I0202 06:12:41.607790 139702543816448 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.0933456420898438, loss=2.6806323528289795
I0202 06:13:27.904760 139702527031040 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.460221290588379, loss=1.8648898601531982
I0202 06:14:14.150356 139702543816448 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.1171445846557617, loss=3.2917749881744385
I0202 06:14:54.638152 139863983413056 spec.py:321] Evaluating on the training split.
I0202 06:15:04.997365 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 06:15:34.636935 139863983413056 spec.py:349] Evaluating on the test split.
I0202 06:15:36.279224 139863983413056 submission_runner.py:408] Time since start: 64971.07s, 	Step: 131090, 	{'train/accuracy': 0.7644726634025574, 'train/loss': 0.9445186257362366, 'validation/accuracy': 0.6960799694061279, 'validation/loss': 1.2283663749694824, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 1.867382287979126, 'test/num_examples': 10000, 'score': 59698.05014848709, 'total_duration': 64971.06675004959, 'accumulated_submission_time': 59698.05014848709, 'accumulated_eval_time': 5259.966984272003, 'accumulated_logging_time': 6.138098955154419}
I0202 06:15:36.317109 139702527031040 logging_writer.py:48] [131090] accumulated_eval_time=5259.966984, accumulated_logging_time=6.138099, accumulated_submission_time=59698.050148, global_step=131090, preemption_count=0, score=59698.050148, test/accuracy=0.572200, test/loss=1.867382, test/num_examples=10000, total_duration=64971.066750, train/accuracy=0.764473, train/loss=0.944519, validation/accuracy=0.696080, validation/loss=1.228366, validation/num_examples=50000
I0202 06:15:40.713950 139702543816448 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.2234883308410645, loss=1.9132839441299438
I0202 06:16:22.556749 139702527031040 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.8562862873077393, loss=2.4275360107421875
I0202 06:17:08.382607 139702543816448 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.739469289779663, loss=3.0503499507904053
I0202 06:17:55.260565 139702527031040 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.0164437294006348, loss=1.9340143203735352
I0202 06:18:41.444946 139702543816448 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.786943197250366, loss=2.1907899379730225
I0202 06:19:27.445969 139702527031040 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.979137897491455, loss=2.2199482917785645
I0202 06:20:13.633590 139702543816448 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.8609843254089355, loss=1.849555253982544
I0202 06:20:59.683841 139702527031040 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.087063789367676, loss=1.9172673225402832
I0202 06:21:45.555400 139702543816448 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.931635618209839, loss=3.2270588874816895
I0202 06:22:31.565782 139702527031040 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.9563848972320557, loss=1.785999059677124
I0202 06:22:36.290842 139863983413056 spec.py:321] Evaluating on the training split.
I0202 06:22:46.652252 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 06:23:13.356680 139863983413056 spec.py:349] Evaluating on the test split.
I0202 06:23:14.990226 139863983413056 submission_runner.py:408] Time since start: 65429.78s, 	Step: 132012, 	{'train/accuracy': 0.763867199420929, 'train/loss': 0.9349223375320435, 'validation/accuracy': 0.6986199617385864, 'validation/loss': 1.2158799171447754, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.8444945812225342, 'test/num_examples': 10000, 'score': 60117.96479392052, 'total_duration': 65429.777752399445, 'accumulated_submission_time': 60117.96479392052, 'accumulated_eval_time': 5298.666358947754, 'accumulated_logging_time': 6.187035799026489}
I0202 06:23:15.028759 139702543816448 logging_writer.py:48] [132012] accumulated_eval_time=5298.666359, accumulated_logging_time=6.187036, accumulated_submission_time=60117.964794, global_step=132012, preemption_count=0, score=60117.964794, test/accuracy=0.577300, test/loss=1.844495, test/num_examples=10000, total_duration=65429.777752, train/accuracy=0.763867, train/loss=0.934922, validation/accuracy=0.698620, validation/loss=1.215880, validation/num_examples=50000
I0202 06:23:50.992717 139702527031040 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.1191606521606445, loss=1.719590187072754
I0202 06:24:36.617740 139702543816448 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.9463160037994385, loss=2.314713478088379
I0202 06:25:22.817061 139702527031040 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.9455316066741943, loss=3.216301441192627
I0202 06:26:09.275716 139702543816448 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.28293776512146, loss=1.8783910274505615
I0202 06:26:54.868242 139702527031040 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.872040271759033, loss=2.093620538711548
I0202 06:27:41.316628 139702543816448 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.131230115890503, loss=1.9098447561264038
I0202 06:28:27.701912 139702527031040 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.1805381774902344, loss=4.419543743133545
I0202 06:29:13.639827 139702543816448 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.944542169570923, loss=1.9026480913162231
I0202 06:29:59.887424 139702527031040 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.973351001739502, loss=1.899274230003357
I0202 06:30:15.339662 139863983413056 spec.py:321] Evaluating on the training split.
I0202 06:30:25.576694 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 06:30:51.449778 139863983413056 spec.py:349] Evaluating on the test split.
I0202 06:30:53.086595 139863983413056 submission_runner.py:408] Time since start: 65887.87s, 	Step: 132935, 	{'train/accuracy': 0.7568749785423279, 'train/loss': 0.9747950434684753, 'validation/accuracy': 0.6997199654579163, 'validation/loss': 1.2330219745635986, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 1.8723223209381104, 'test/num_examples': 10000, 'score': 60538.21879982948, 'total_duration': 65887.87410736084, 'accumulated_submission_time': 60538.21879982948, 'accumulated_eval_time': 5336.413270950317, 'accumulated_logging_time': 6.234623432159424}
I0202 06:30:53.126673 139702543816448 logging_writer.py:48] [132935] accumulated_eval_time=5336.413271, accumulated_logging_time=6.234623, accumulated_submission_time=60538.218800, global_step=132935, preemption_count=0, score=60538.218800, test/accuracy=0.571500, test/loss=1.872322, test/num_examples=10000, total_duration=65887.874107, train/accuracy=0.756875, train/loss=0.974795, validation/accuracy=0.699720, validation/loss=1.233022, validation/num_examples=50000
I0202 06:31:19.446976 139702527031040 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.1445634365081787, loss=1.9001790285110474
I0202 06:32:04.373620 139702543816448 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.114081859588623, loss=2.713270902633667
I0202 06:32:50.409645 139702527031040 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.8914589881896973, loss=2.8727564811706543
I0202 06:33:36.429402 139702543816448 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.0524020195007324, loss=4.238920211791992
I0202 06:34:22.388796 139702527031040 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.107884168624878, loss=1.8158029317855835
I0202 06:35:08.596484 139702543816448 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.3275749683380127, loss=1.8632022142410278
I0202 06:35:54.332081 139702527031040 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.208378553390503, loss=1.8115686178207397
I0202 06:36:40.479252 139702543816448 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.923737049102783, loss=2.6581175327301025
I0202 06:37:26.512938 139702527031040 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.823678970336914, loss=1.8558777570724487
I0202 06:37:53.337738 139863983413056 spec.py:321] Evaluating on the training split.
I0202 06:38:03.762953 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 06:38:27.648565 139863983413056 spec.py:349] Evaluating on the test split.
I0202 06:38:29.283428 139863983413056 submission_runner.py:408] Time since start: 66344.07s, 	Step: 133860, 	{'train/accuracy': 0.7635351419448853, 'train/loss': 0.9517484307289124, 'validation/accuracy': 0.7021600008010864, 'validation/loss': 1.2254695892333984, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 1.8516438007354736, 'test/num_examples': 10000, 'score': 60958.368864774704, 'total_duration': 66344.07096099854, 'accumulated_submission_time': 60958.368864774704, 'accumulated_eval_time': 5372.358961343765, 'accumulated_logging_time': 6.28567361831665}
I0202 06:38:29.318426 139702543816448 logging_writer.py:48] [133860] accumulated_eval_time=5372.358961, accumulated_logging_time=6.285674, accumulated_submission_time=60958.368865, global_step=133860, preemption_count=0, score=60958.368865, test/accuracy=0.576300, test/loss=1.851644, test/num_examples=10000, total_duration=66344.070961, train/accuracy=0.763535, train/loss=0.951748, validation/accuracy=0.702160, validation/loss=1.225470, validation/num_examples=50000
I0202 06:38:45.669255 139702527031040 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.0510826110839844, loss=2.404649257659912
I0202 06:39:28.953527 139702543816448 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.351062536239624, loss=1.963006854057312
I0202 06:40:15.049768 139702527031040 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.742703676223755, loss=2.3421075344085693
I0202 06:41:01.362253 139702543816448 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.224433422088623, loss=2.6362011432647705
I0202 06:41:47.131207 139702527031040 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.9091756343841553, loss=1.7196136713027954
I0202 06:42:33.459460 139702543816448 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.1845219135284424, loss=4.181146621704102
I0202 06:43:19.548493 139702527031040 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.8617465496063232, loss=2.2385191917419434
I0202 06:44:05.788741 139702543816448 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.1993958950042725, loss=1.9743976593017578
I0202 06:44:51.946351 139702527031040 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.8705484867095947, loss=2.9631059169769287
I0202 06:45:29.459149 139863983413056 spec.py:321] Evaluating on the training split.
I0202 06:45:39.892307 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 06:46:03.333014 139863983413056 spec.py:349] Evaluating on the test split.
I0202 06:46:04.967764 139863983413056 submission_runner.py:408] Time since start: 66799.76s, 	Step: 134783, 	{'train/accuracy': 0.7740820050239563, 'train/loss': 0.883020281791687, 'validation/accuracy': 0.6988799571990967, 'validation/loss': 1.2179479598999023, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.8422235250473022, 'test/num_examples': 10000, 'score': 61378.45201802254, 'total_duration': 66799.75529813766, 'accumulated_submission_time': 61378.45201802254, 'accumulated_eval_time': 5407.8675990104675, 'accumulated_logging_time': 6.32945704460144}
I0202 06:46:05.007920 139702543816448 logging_writer.py:48] [134783] accumulated_eval_time=5407.867599, accumulated_logging_time=6.329457, accumulated_submission_time=61378.452018, global_step=134783, preemption_count=0, score=61378.452018, test/accuracy=0.575000, test/loss=1.842224, test/num_examples=10000, total_duration=66799.755298, train/accuracy=0.774082, train/loss=0.883020, validation/accuracy=0.698880, validation/loss=1.217948, validation/num_examples=50000
I0202 06:46:12.184995 139702527031040 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.171825408935547, loss=2.0612590312957764
I0202 06:46:54.369133 139702543816448 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.1830508708953857, loss=1.784496545791626
I0202 06:47:40.031753 139702527031040 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.095090389251709, loss=3.16875958442688
I0202 06:48:26.306061 139702543816448 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.404724597930908, loss=1.9513609409332275
I0202 06:49:12.555078 139702527031040 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.9273500442504883, loss=3.963458299636841
I0202 06:49:58.648270 139702543816448 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.3421287536621094, loss=1.8945728540420532
I0202 06:50:44.713726 139702527031040 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.715972900390625, loss=3.193669319152832
I0202 06:51:30.593602 139702543816448 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.0234646797180176, loss=4.122120380401611
I0202 06:52:16.584304 139702527031040 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.11214280128479, loss=4.101853847503662
I0202 06:53:02.551829 139702543816448 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.4862523078918457, loss=1.8431665897369385
I0202 06:53:04.970862 139863983413056 spec.py:321] Evaluating on the training split.
I0202 06:53:15.343906 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 06:53:40.106411 139863983413056 spec.py:349] Evaluating on the test split.
I0202 06:53:41.746419 139863983413056 submission_runner.py:408] Time since start: 67256.53s, 	Step: 135707, 	{'train/accuracy': 0.761914074420929, 'train/loss': 0.9550774097442627, 'validation/accuracy': 0.6998999714851379, 'validation/loss': 1.2158119678497314, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.8288977146148682, 'test/num_examples': 10000, 'score': 61798.35517120361, 'total_duration': 67256.5339550972, 'accumulated_submission_time': 61798.35517120361, 'accumulated_eval_time': 5444.643156290054, 'accumulated_logging_time': 6.38027548789978}
I0202 06:53:41.781563 139702527031040 logging_writer.py:48] [135707] accumulated_eval_time=5444.643156, accumulated_logging_time=6.380275, accumulated_submission_time=61798.355171, global_step=135707, preemption_count=0, score=61798.355171, test/accuracy=0.583300, test/loss=1.828898, test/num_examples=10000, total_duration=67256.533955, train/accuracy=0.761914, train/loss=0.955077, validation/accuracy=0.699900, validation/loss=1.215812, validation/num_examples=50000
I0202 06:54:20.049770 139702543816448 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.273893356323242, loss=1.7768080234527588
I0202 06:55:05.876279 139702527031040 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.1568472385406494, loss=1.8235398530960083
I0202 06:55:51.946202 139702543816448 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.2040393352508545, loss=2.0728793144226074
I0202 06:56:37.810272 139702527031040 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.5709636211395264, loss=1.9237309694290161
I0202 06:57:23.808174 139702543816448 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.122177839279175, loss=1.8770289421081543
I0202 06:58:09.837106 139702527031040 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.103895425796509, loss=2.3701210021972656
I0202 06:58:56.073952 139702543816448 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.9374589920043945, loss=2.5561819076538086
I0202 06:59:41.928440 139702527031040 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.5026674270629883, loss=1.8849713802337646
I0202 07:00:28.188331 139702543816448 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.2822680473327637, loss=1.7520445585250854
I0202 07:00:42.038850 139863983413056 spec.py:321] Evaluating on the training split.
I0202 07:00:52.318872 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 07:01:16.554883 139863983413056 spec.py:349] Evaluating on the test split.
I0202 07:01:18.193946 139863983413056 submission_runner.py:408] Time since start: 67712.98s, 	Step: 136632, 	{'train/accuracy': 0.7692968845367432, 'train/loss': 0.9049091935157776, 'validation/accuracy': 0.7047799825668335, 'validation/loss': 1.1906603574752808, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.79646897315979, 'test/num_examples': 10000, 'score': 62218.55541443825, 'total_duration': 67712.98147702217, 'accumulated_submission_time': 62218.55541443825, 'accumulated_eval_time': 5480.798250198364, 'accumulated_logging_time': 6.4242448806762695}
I0202 07:01:18.230086 139702527031040 logging_writer.py:48] [136632] accumulated_eval_time=5480.798250, accumulated_logging_time=6.424245, accumulated_submission_time=62218.555414, global_step=136632, preemption_count=0, score=62218.555414, test/accuracy=0.581900, test/loss=1.796469, test/num_examples=10000, total_duration=67712.981477, train/accuracy=0.769297, train/loss=0.904909, validation/accuracy=0.704780, validation/loss=1.190660, validation/num_examples=50000
I0202 07:01:45.770421 139702543816448 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.509348154067993, loss=1.8815340995788574
I0202 07:02:30.702444 139702527031040 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.1046245098114014, loss=2.422593593597412
I0202 07:03:17.004656 139702543816448 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.216081380844116, loss=1.8156397342681885
I0202 07:04:03.221151 139702527031040 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.307202100753784, loss=1.7698307037353516
I0202 07:04:49.016193 139702543816448 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.3047964572906494, loss=2.1875791549682617
I0202 07:05:34.908808 139702527031040 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.5359880924224854, loss=1.9845623970031738
I0202 07:06:20.813750 139702543816448 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.211024761199951, loss=4.22999382019043
I0202 07:07:06.940434 139702527031040 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.1444218158721924, loss=3.3577606678009033
I0202 07:07:52.755268 139702543816448 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.2092108726501465, loss=1.7478264570236206
I0202 07:08:18.622580 139863983413056 spec.py:321] Evaluating on the training split.
I0202 07:08:29.038829 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 07:09:00.481502 139863983413056 spec.py:349] Evaluating on the test split.
I0202 07:09:02.118916 139863983413056 submission_runner.py:408] Time since start: 68176.91s, 	Step: 137558, 	{'train/accuracy': 0.7782226204872131, 'train/loss': 0.8680867552757263, 'validation/accuracy': 0.7066400051116943, 'validation/loss': 1.1834466457366943, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 1.7963061332702637, 'test/num_examples': 10000, 'score': 62638.88868141174, 'total_duration': 68176.90643644333, 'accumulated_submission_time': 62638.88868141174, 'accumulated_eval_time': 5524.29457783699, 'accumulated_logging_time': 6.470351934432983}
I0202 07:09:02.175381 139702527031040 logging_writer.py:48] [137558] accumulated_eval_time=5524.294578, accumulated_logging_time=6.470352, accumulated_submission_time=62638.888681, global_step=137558, preemption_count=0, score=62638.888681, test/accuracy=0.580000, test/loss=1.796306, test/num_examples=10000, total_duration=68176.906436, train/accuracy=0.778223, train/loss=0.868087, validation/accuracy=0.706640, validation/loss=1.183447, validation/num_examples=50000
I0202 07:09:19.325179 139702543816448 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.31685209274292, loss=2.105980396270752
I0202 07:10:03.262544 139702527031040 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.2518110275268555, loss=3.2077455520629883
I0202 07:10:49.053926 139702543816448 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.1659271717071533, loss=2.2884809970855713
I0202 07:11:35.217250 139702527031040 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.2814269065856934, loss=2.2681984901428223
I0202 07:12:21.232523 139702543816448 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.1319825649261475, loss=1.9683910608291626
I0202 07:13:07.228039 139702527031040 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.0911574363708496, loss=2.864074468612671
I0202 07:13:53.212306 139702543816448 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.4053285121917725, loss=1.7607133388519287
I0202 07:14:39.159888 139702527031040 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.4040215015411377, loss=1.8526670932769775
I0202 07:15:25.209656 139702543816448 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.0321784019470215, loss=2.3809077739715576
I0202 07:16:02.332325 139863983413056 spec.py:321] Evaluating on the training split.
I0202 07:16:12.588625 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 07:16:36.585021 139863983413056 spec.py:349] Evaluating on the test split.
I0202 07:16:38.226236 139863983413056 submission_runner.py:408] Time since start: 68633.01s, 	Step: 138483, 	{'train/accuracy': 0.768750011920929, 'train/loss': 0.9112576842308044, 'validation/accuracy': 0.7040199637413025, 'validation/loss': 1.1906100511550903, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.7935667037963867, 'test/num_examples': 10000, 'score': 63058.986698150635, 'total_duration': 68633.01377010345, 'accumulated_submission_time': 63058.986698150635, 'accumulated_eval_time': 5560.188486814499, 'accumulated_logging_time': 6.537533760070801}
I0202 07:16:38.264834 139702527031040 logging_writer.py:48] [138483] accumulated_eval_time=5560.188487, accumulated_logging_time=6.537534, accumulated_submission_time=63058.986698, global_step=138483, preemption_count=0, score=63058.986698, test/accuracy=0.584800, test/loss=1.793567, test/num_examples=10000, total_duration=68633.013770, train/accuracy=0.768750, train/loss=0.911258, validation/accuracy=0.704020, validation/loss=1.190610, validation/num_examples=50000
I0202 07:16:45.459811 139702543816448 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.410994052886963, loss=2.840761423110962
I0202 07:17:27.504187 139702527031040 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.68605637550354, loss=1.7153706550598145
I0202 07:18:13.504970 139702543816448 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.704127073287964, loss=4.0737714767456055
I0202 07:18:59.780666 139702527031040 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.5668461322784424, loss=1.7964357137680054
I0202 07:19:45.812056 139702543816448 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.33876371383667, loss=4.329762935638428
I0202 07:20:31.918238 139702527031040 logging_writer.py:48] [139000] global_step=139000, grad_norm=4.26707124710083, loss=4.324765682220459
I0202 07:21:18.196750 139702543816448 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.508714437484741, loss=1.7691185474395752
I0202 07:22:04.184921 139702527031040 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.1181037425994873, loss=1.6322617530822754
I0202 07:22:50.347341 139702543816448 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.5114598274230957, loss=1.8437635898590088
I0202 07:23:36.561725 139702527031040 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.652937412261963, loss=1.8436928987503052
I0202 07:23:38.232544 139863983413056 spec.py:321] Evaluating on the training split.
I0202 07:23:49.120282 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 07:24:20.983933 139863983413056 spec.py:349] Evaluating on the test split.
I0202 07:24:22.620738 139863983413056 submission_runner.py:408] Time since start: 69097.41s, 	Step: 139405, 	{'train/accuracy': 0.775195300579071, 'train/loss': 0.8927517533302307, 'validation/accuracy': 0.7079600095748901, 'validation/loss': 1.1707615852355957, 'validation/num_examples': 50000, 'test/accuracy': 0.585800051689148, 'test/loss': 1.7905820608139038, 'test/num_examples': 10000, 'score': 63478.894491672516, 'total_duration': 69097.40825605392, 'accumulated_submission_time': 63478.894491672516, 'accumulated_eval_time': 5604.57666349411, 'accumulated_logging_time': 6.587302923202515}
I0202 07:24:22.665974 139702543816448 logging_writer.py:48] [139405] accumulated_eval_time=5604.576663, accumulated_logging_time=6.587303, accumulated_submission_time=63478.894492, global_step=139405, preemption_count=0, score=63478.894492, test/accuracy=0.585800, test/loss=1.790582, test/num_examples=10000, total_duration=69097.408256, train/accuracy=0.775195, train/loss=0.892752, validation/accuracy=0.707960, validation/loss=1.170762, validation/num_examples=50000
I0202 07:25:01.932343 139702527031040 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.3728432655334473, loss=1.628797173500061
I0202 07:25:47.749206 139702543816448 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.5363681316375732, loss=1.7684115171432495
I0202 07:26:33.800970 139702527031040 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.826122522354126, loss=1.7941316366195679
I0202 07:27:20.047217 139702543816448 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.4745266437530518, loss=3.9756104946136475
I0202 07:28:06.116911 139702527031040 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.027754783630371, loss=3.2392125129699707
I0202 07:28:52.072543 139702543816448 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.608659267425537, loss=1.8366061449050903
I0202 07:29:38.011718 139702527031040 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.097677230834961, loss=3.270139694213867
I0202 07:30:24.565944 139702543816448 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.5451009273529053, loss=1.9545056819915771
I0202 07:31:10.546116 139702527031040 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.494643449783325, loss=1.9408214092254639
I0202 07:31:22.630008 139863983413056 spec.py:321] Evaluating on the training split.
I0202 07:31:32.952447 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 07:31:59.158567 139863983413056 spec.py:349] Evaluating on the test split.
I0202 07:32:00.811211 139863983413056 submission_runner.py:408] Time since start: 69555.60s, 	Step: 140328, 	{'train/accuracy': 0.7830859422683716, 'train/loss': 0.8475367426872253, 'validation/accuracy': 0.7130999565124512, 'validation/loss': 1.1560105085372925, 'validation/num_examples': 50000, 'test/accuracy': 0.5877000093460083, 'test/loss': 1.7814221382141113, 'test/num_examples': 10000, 'score': 63898.79946422577, 'total_duration': 69555.59872603416, 'accumulated_submission_time': 63898.79946422577, 'accumulated_eval_time': 5642.757848501205, 'accumulated_logging_time': 6.641981601715088}
I0202 07:32:00.855041 139702543816448 logging_writer.py:48] [140328] accumulated_eval_time=5642.757849, accumulated_logging_time=6.641982, accumulated_submission_time=63898.799464, global_step=140328, preemption_count=0, score=63898.799464, test/accuracy=0.587700, test/loss=1.781422, test/num_examples=10000, total_duration=69555.598726, train/accuracy=0.783086, train/loss=0.847537, validation/accuracy=0.713100, validation/loss=1.156011, validation/num_examples=50000
I0202 07:32:30.016264 139702527031040 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.297806739807129, loss=4.220694065093994
I0202 07:33:15.501541 139702543816448 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.8885788917541504, loss=2.0398216247558594
I0202 07:34:01.670768 139702527031040 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.2378523349761963, loss=3.4082422256469727
I0202 07:34:48.018330 139702543816448 logging_writer.py:48] [140700] global_step=140700, grad_norm=4.3042192459106445, loss=4.2325239181518555
I0202 07:35:34.150189 139702527031040 logging_writer.py:48] [140800] global_step=140800, grad_norm=4.171080112457275, loss=1.9686404466629028
I0202 07:36:20.380054 139702543816448 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.422002077102661, loss=1.8449831008911133
I0202 07:37:06.298938 139702527031040 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.6244516372680664, loss=1.834012508392334
I0202 07:37:53.285307 139702543816448 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.4226326942443848, loss=1.8502106666564941
I0202 07:38:39.582189 139702527031040 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.2646918296813965, loss=2.401069164276123
I0202 07:39:00.836833 139863983413056 spec.py:321] Evaluating on the training split.
I0202 07:39:11.272409 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 07:39:44.174294 139863983413056 spec.py:349] Evaluating on the test split.
I0202 07:39:45.822294 139863983413056 submission_runner.py:408] Time since start: 70020.61s, 	Step: 141248, 	{'train/accuracy': 0.7801562547683716, 'train/loss': 0.8689774870872498, 'validation/accuracy': 0.7124599814414978, 'validation/loss': 1.151098370552063, 'validation/num_examples': 50000, 'test/accuracy': 0.5913000106811523, 'test/loss': 1.7659611701965332, 'test/num_examples': 10000, 'score': 64318.720710515976, 'total_duration': 70020.60980701447, 'accumulated_submission_time': 64318.720710515976, 'accumulated_eval_time': 5687.743291378021, 'accumulated_logging_time': 6.696745872497559}
I0202 07:39:45.864755 139702543816448 logging_writer.py:48] [141248] accumulated_eval_time=5687.743291, accumulated_logging_time=6.696746, accumulated_submission_time=64318.720711, global_step=141248, preemption_count=0, score=64318.720711, test/accuracy=0.591300, test/loss=1.765961, test/num_examples=10000, total_duration=70020.609807, train/accuracy=0.780156, train/loss=0.868977, validation/accuracy=0.712460, validation/loss=1.151098, validation/num_examples=50000
I0202 07:40:07.012792 139702527031040 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.3724241256713867, loss=1.6942028999328613
I0202 07:40:51.700163 139702543816448 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.554809331893921, loss=4.262327194213867
I0202 07:41:38.211000 139702527031040 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.174100399017334, loss=1.7522273063659668
I0202 07:42:24.605091 139702543816448 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.686892509460449, loss=1.6787850856781006
I0202 07:43:10.703226 139702527031040 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.8783445358276367, loss=1.917080044746399
I0202 07:43:56.554934 139702543816448 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.4344944953918457, loss=1.7419397830963135
I0202 07:44:42.594807 139702527031040 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.6992685794830322, loss=1.8104169368743896
I0202 07:45:28.525203 139702543816448 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.489899158477783, loss=1.7672007083892822
I0202 07:46:14.538987 139702527031040 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.1901934146881104, loss=2.8893675804138184
I0202 07:46:45.847342 139863983413056 spec.py:321] Evaluating on the training split.
I0202 07:46:56.957013 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 07:47:22.404069 139863983413056 spec.py:349] Evaluating on the test split.
I0202 07:47:24.044865 139863983413056 submission_runner.py:408] Time since start: 70478.83s, 	Step: 142170, 	{'train/accuracy': 0.7764452695846558, 'train/loss': 0.8885748982429504, 'validation/accuracy': 0.7087399959564209, 'validation/loss': 1.1719297170639038, 'validation/num_examples': 50000, 'test/accuracy': 0.58760005235672, 'test/loss': 1.8026561737060547, 'test/num_examples': 10000, 'score': 64738.64544630051, 'total_duration': 70478.83239912987, 'accumulated_submission_time': 64738.64544630051, 'accumulated_eval_time': 5725.94082069397, 'accumulated_logging_time': 6.748511791229248}
I0202 07:47:24.081384 139702543816448 logging_writer.py:48] [142170] accumulated_eval_time=5725.940821, accumulated_logging_time=6.748512, accumulated_submission_time=64738.645446, global_step=142170, preemption_count=0, score=64738.645446, test/accuracy=0.587600, test/loss=1.802656, test/num_examples=10000, total_duration=70478.832399, train/accuracy=0.776445, train/loss=0.888575, validation/accuracy=0.708740, validation/loss=1.171930, validation/num_examples=50000
I0202 07:47:36.436599 139702527031040 logging_writer.py:48] [142200] global_step=142200, grad_norm=4.242071151733398, loss=4.338135719299316
I0202 07:48:19.136315 139702543816448 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.437426805496216, loss=2.1759650707244873
I0202 07:49:05.391673 139702527031040 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.5400118827819824, loss=1.6935447454452515
I0202 07:49:51.559704 139702543816448 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.394620180130005, loss=3.725672960281372
I0202 07:50:37.673961 139702527031040 logging_writer.py:48] [142600] global_step=142600, grad_norm=4.572680473327637, loss=4.032169818878174
I0202 07:51:24.097698 139702543816448 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.5361201763153076, loss=2.2856318950653076
I0202 07:52:10.283018 139702527031040 logging_writer.py:48] [142800] global_step=142800, grad_norm=4.212101936340332, loss=4.215014457702637
I0202 07:52:56.238409 139702543816448 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.5925803184509277, loss=2.530478000640869
I0202 07:53:42.180236 139702527031040 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.6898741722106934, loss=2.219130754470825
I0202 07:54:24.371327 139863983413056 spec.py:321] Evaluating on the training split.
I0202 07:54:34.842100 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 07:55:01.088969 139863983413056 spec.py:349] Evaluating on the test split.
I0202 07:55:02.733751 139863983413056 submission_runner.py:408] Time since start: 70937.52s, 	Step: 143093, 	{'train/accuracy': 0.7829882502555847, 'train/loss': 0.8484798073768616, 'validation/accuracy': 0.7154799699783325, 'validation/loss': 1.1398857831954956, 'validation/num_examples': 50000, 'test/accuracy': 0.5895000100135803, 'test/loss': 1.7539395093917847, 'test/num_examples': 10000, 'score': 65158.876828193665, 'total_duration': 70937.52128458023, 'accumulated_submission_time': 65158.876828193665, 'accumulated_eval_time': 5764.303265571594, 'accumulated_logging_time': 6.794252634048462}
I0202 07:55:02.769901 139702543816448 logging_writer.py:48] [143093] accumulated_eval_time=5764.303266, accumulated_logging_time=6.794253, accumulated_submission_time=65158.876828, global_step=143093, preemption_count=0, score=65158.876828, test/accuracy=0.589500, test/loss=1.753940, test/num_examples=10000, total_duration=70937.521285, train/accuracy=0.782988, train/loss=0.848480, validation/accuracy=0.715480, validation/loss=1.139886, validation/num_examples=50000
I0202 07:55:05.957475 139702527031040 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.7953150272369385, loss=4.059189796447754
I0202 07:55:47.800206 139702543816448 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.3043267726898193, loss=2.8798768520355225
I0202 07:56:33.710506 139702527031040 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.7492473125457764, loss=3.52669095993042
I0202 07:57:19.782170 139702543816448 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.518167734146118, loss=1.8603787422180176
I0202 07:58:05.617248 139702527031040 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.7537364959716797, loss=1.712648630142212
I0202 07:58:51.473066 139702543816448 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.9066319465637207, loss=1.745349645614624
I0202 07:59:37.807581 139702527031040 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.697108268737793, loss=1.7333472967147827
I0202 08:00:24.259412 139702543816448 logging_writer.py:48] [143800] global_step=143800, grad_norm=4.170485496520996, loss=4.165689468383789
I0202 08:01:10.303536 139702527031040 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.318804979324341, loss=3.0340347290039062
I0202 08:01:56.479650 139702543816448 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.918074369430542, loss=4.151546478271484
I0202 08:02:03.127763 139863983413056 spec.py:321] Evaluating on the training split.
I0202 08:02:13.337301 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 08:02:45.682880 139863983413056 spec.py:349] Evaluating on the test split.
I0202 08:02:47.317245 139863983413056 submission_runner.py:408] Time since start: 71402.10s, 	Step: 144016, 	{'train/accuracy': 0.7986913919448853, 'train/loss': 0.7790917158126831, 'validation/accuracy': 0.7196399569511414, 'validation/loss': 1.120820164680481, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.7356946468353271, 'test/num_examples': 10000, 'score': 65579.17691516876, 'total_duration': 71402.10477161407, 'accumulated_submission_time': 65579.17691516876, 'accumulated_eval_time': 5808.492747783661, 'accumulated_logging_time': 6.839449644088745}
I0202 08:02:47.353693 139702527031040 logging_writer.py:48] [144016] accumulated_eval_time=5808.492748, accumulated_logging_time=6.839450, accumulated_submission_time=65579.176915, global_step=144016, preemption_count=0, score=65579.176915, test/accuracy=0.596200, test/loss=1.735695, test/num_examples=10000, total_duration=71402.104772, train/accuracy=0.798691, train/loss=0.779092, validation/accuracy=0.719640, validation/loss=1.120820, validation/num_examples=50000
I0202 08:03:21.819855 139702543816448 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.608886957168579, loss=1.7167961597442627
I0202 08:04:07.613234 139702527031040 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.5880441665649414, loss=2.762800693511963
I0202 08:04:53.689774 139702543816448 logging_writer.py:48] [144300] global_step=144300, grad_norm=4.265956878662109, loss=4.2115278244018555
I0202 08:05:39.857742 139702527031040 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.5174813270568848, loss=2.378782272338867
I0202 08:06:25.839139 139702543816448 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.5324361324310303, loss=3.321963310241699
I0202 08:07:11.997472 139702527031040 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.2014825344085693, loss=2.4372127056121826
I0202 08:07:57.857783 139702543816448 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.622612953186035, loss=2.7736222743988037
I0202 08:08:43.843925 139702527031040 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.497084379196167, loss=2.1803712844848633
I0202 08:09:29.878783 139702543816448 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.9247992038726807, loss=1.8272428512573242
I0202 08:09:47.525783 139863983413056 spec.py:321] Evaluating on the training split.
I0202 08:09:58.067897 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 08:10:23.408775 139863983413056 spec.py:349] Evaluating on the test split.
I0202 08:10:25.054316 139863983413056 submission_runner.py:408] Time since start: 71859.84s, 	Step: 144940, 	{'train/accuracy': 0.7861132621765137, 'train/loss': 0.835200846195221, 'validation/accuracy': 0.718999981880188, 'validation/loss': 1.122183084487915, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.7483426332473755, 'test/num_examples': 10000, 'score': 65999.29121685028, 'total_duration': 71859.84184217453, 'accumulated_submission_time': 65999.29121685028, 'accumulated_eval_time': 5846.021278142929, 'accumulated_logging_time': 6.884845018386841}
I0202 08:10:25.092246 139702527031040 logging_writer.py:48] [144940] accumulated_eval_time=5846.021278, accumulated_logging_time=6.884845, accumulated_submission_time=65999.291217, global_step=144940, preemption_count=0, score=65999.291217, test/accuracy=0.592900, test/loss=1.748343, test/num_examples=10000, total_duration=71859.841842, train/accuracy=0.786113, train/loss=0.835201, validation/accuracy=0.719000, validation/loss=1.122183, validation/num_examples=50000
I0202 08:10:49.443725 139702543816448 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.5595638751983643, loss=2.551265239715576
I0202 08:11:34.002124 139702527031040 logging_writer.py:48] [145100] global_step=145100, grad_norm=4.15470552444458, loss=1.999196171760559
I0202 08:12:20.262254 139702543816448 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.7791757583618164, loss=1.6891117095947266
I0202 08:13:06.571905 139702527031040 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.8049826622009277, loss=4.10719108581543
I0202 08:13:52.448839 139702543816448 logging_writer.py:48] [145400] global_step=145400, grad_norm=4.035030841827393, loss=1.7561640739440918
I0202 08:14:38.319678 139702527031040 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.5949859619140625, loss=2.139930248260498
I0202 08:15:24.227916 139702543816448 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.453496217727661, loss=2.782290458679199
I0202 08:16:10.210131 139702527031040 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.632211208343506, loss=1.6030088663101196
I0202 08:16:56.246060 139702543816448 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.883066415786743, loss=1.8804337978363037
I0202 08:17:25.285988 139863983413056 spec.py:321] Evaluating on the training split.
I0202 08:17:35.566611 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 08:18:00.671350 139863983413056 spec.py:349] Evaluating on the test split.
I0202 08:18:02.323214 139863983413056 submission_runner.py:408] Time since start: 72317.11s, 	Step: 145865, 	{'train/accuracy': 0.789746105670929, 'train/loss': 0.8230050206184387, 'validation/accuracy': 0.7236599922180176, 'validation/loss': 1.11073899269104, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.7311910390853882, 'test/num_examples': 10000, 'score': 66419.42792582512, 'total_duration': 72317.11073088646, 'accumulated_submission_time': 66419.42792582512, 'accumulated_eval_time': 5883.058493375778, 'accumulated_logging_time': 6.93181300163269}
I0202 08:18:02.360098 139702527031040 logging_writer.py:48] [145865] accumulated_eval_time=5883.058493, accumulated_logging_time=6.931813, accumulated_submission_time=66419.427926, global_step=145865, preemption_count=0, score=66419.427926, test/accuracy=0.600600, test/loss=1.731191, test/num_examples=10000, total_duration=72317.110731, train/accuracy=0.789746, train/loss=0.823005, validation/accuracy=0.723660, validation/loss=1.110739, validation/num_examples=50000
I0202 08:18:16.723413 139702543816448 logging_writer.py:48] [145900] global_step=145900, grad_norm=4.029061794281006, loss=4.165863037109375
I0202 08:18:59.853426 139702527031040 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.7942636013031006, loss=1.7263762950897217
I0202 08:19:45.682268 139702543816448 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.8649935722351074, loss=1.7251912355422974
I0202 08:20:32.216920 139702527031040 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.664647340774536, loss=1.7038209438323975
I0202 08:21:17.929821 139702543816448 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.793771743774414, loss=1.6449954509735107
I0202 08:22:04.101732 139702527031040 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.6329166889190674, loss=2.7538111209869385
I0202 08:22:50.010322 139702543816448 logging_writer.py:48] [146500] global_step=146500, grad_norm=4.603946208953857, loss=4.184206008911133
I0202 08:23:35.825133 139702527031040 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.966430902481079, loss=1.7410120964050293
I0202 08:24:21.848073 139702543816448 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.570432186126709, loss=1.7662628889083862
I0202 08:25:02.748920 139863983413056 spec.py:321] Evaluating on the training split.
I0202 08:25:13.007231 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 08:25:40.131690 139863983413056 spec.py:349] Evaluating on the test split.
I0202 08:25:41.770214 139863983413056 submission_runner.py:408] Time since start: 72776.56s, 	Step: 146791, 	{'train/accuracy': 0.7936913967132568, 'train/loss': 0.7987895011901855, 'validation/accuracy': 0.7238999605178833, 'validation/loss': 1.1063193082809448, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.7203985452651978, 'test/num_examples': 10000, 'score': 66839.75595474243, 'total_duration': 72776.55774569511, 'accumulated_submission_time': 66839.75595474243, 'accumulated_eval_time': 5922.079788208008, 'accumulated_logging_time': 6.981026649475098}
I0202 08:25:41.810979 139702527031040 logging_writer.py:48] [146791] accumulated_eval_time=5922.079788, accumulated_logging_time=6.981027, accumulated_submission_time=66839.755955, global_step=146791, preemption_count=0, score=66839.755955, test/accuracy=0.598600, test/loss=1.720399, test/num_examples=10000, total_duration=72776.557746, train/accuracy=0.793691, train/loss=0.798790, validation/accuracy=0.723900, validation/loss=1.106319, validation/num_examples=50000
I0202 08:25:45.794913 139702543816448 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.438999652862549, loss=4.135356903076172
I0202 08:26:27.344664 139702527031040 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.2810020446777344, loss=3.5972652435302734
I0202 08:27:13.180051 139702543816448 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.3604514598846436, loss=2.6550087928771973
I0202 08:27:59.396129 139702527031040 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.5211644172668457, loss=2.6262803077697754
I0202 08:28:45.289200 139702543816448 logging_writer.py:48] [147200] global_step=147200, grad_norm=4.149035453796387, loss=1.661368489265442
I0202 08:29:31.241216 139702527031040 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.7280445098876953, loss=3.6189565658569336
I0202 08:30:17.477647 139702543816448 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.7799551486968994, loss=2.1550042629241943
I0202 08:31:03.450673 139702527031040 logging_writer.py:48] [147500] global_step=147500, grad_norm=4.556593894958496, loss=3.6860172748565674
I0202 08:31:49.480009 139702543816448 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.8012373447418213, loss=3.8555538654327393
I0202 08:32:35.904526 139702527031040 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.3948588371276855, loss=3.3621976375579834
I0202 08:32:42.001725 139863983413056 spec.py:321] Evaluating on the training split.
I0202 08:32:52.422636 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 08:33:17.685505 139863983413056 spec.py:349] Evaluating on the test split.
I0202 08:33:19.323624 139863983413056 submission_runner.py:408] Time since start: 73234.11s, 	Step: 147715, 	{'train/accuracy': 0.7894726395606995, 'train/loss': 0.8196678757667542, 'validation/accuracy': 0.7218999862670898, 'validation/loss': 1.1062513589859009, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.724432110786438, 'test/num_examples': 10000, 'score': 67259.88791394234, 'total_duration': 73234.11113333702, 'accumulated_submission_time': 67259.88791394234, 'accumulated_eval_time': 5959.401660680771, 'accumulated_logging_time': 7.031947135925293}
I0202 08:33:19.364490 139702543816448 logging_writer.py:48] [147715] accumulated_eval_time=5959.401661, accumulated_logging_time=7.031947, accumulated_submission_time=67259.887914, global_step=147715, preemption_count=0, score=67259.887914, test/accuracy=0.600800, test/loss=1.724432, test/num_examples=10000, total_duration=73234.111133, train/accuracy=0.789473, train/loss=0.819668, validation/accuracy=0.721900, validation/loss=1.106251, validation/num_examples=50000
I0202 08:33:53.978555 139702527031040 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.27979850769043, loss=3.991901397705078
I0202 08:34:39.755545 139702543816448 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.046796798706055, loss=1.7025192975997925
I0202 08:35:26.123074 139702527031040 logging_writer.py:48] [148000] global_step=148000, grad_norm=4.06230354309082, loss=1.7168725728988647
I0202 08:36:12.428636 139702543816448 logging_writer.py:48] [148100] global_step=148100, grad_norm=4.205604076385498, loss=3.841926097869873
I0202 08:36:58.239043 139702527031040 logging_writer.py:48] [148200] global_step=148200, grad_norm=4.206076145172119, loss=1.7565035820007324
I0202 08:37:44.214758 139702543816448 logging_writer.py:48] [148300] global_step=148300, grad_norm=4.0951080322265625, loss=1.6255158185958862
I0202 08:38:30.196511 139702527031040 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.8249993324279785, loss=1.7588247060775757
I0202 08:39:16.145795 139702543816448 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.8540353775024414, loss=1.5674219131469727
I0202 08:40:02.357391 139702527031040 logging_writer.py:48] [148600] global_step=148600, grad_norm=4.078295707702637, loss=3.5167596340179443
I0202 08:40:19.545091 139863983413056 spec.py:321] Evaluating on the training split.
I0202 08:40:30.014779 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 08:40:54.777227 139863983413056 spec.py:349] Evaluating on the test split.
I0202 08:40:56.428886 139863983413056 submission_runner.py:408] Time since start: 73691.22s, 	Step: 148639, 	{'train/accuracy': 0.7953320145606995, 'train/loss': 0.7963201999664307, 'validation/accuracy': 0.724079966545105, 'validation/loss': 1.092710256576538, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.7002700567245483, 'test/num_examples': 10000, 'score': 67680.01170182228, 'total_duration': 73691.21638917923, 'accumulated_submission_time': 67680.01170182228, 'accumulated_eval_time': 5996.285425662994, 'accumulated_logging_time': 7.0817482471466064}
I0202 08:40:56.475291 139702543816448 logging_writer.py:48] [148639] accumulated_eval_time=5996.285426, accumulated_logging_time=7.081748, accumulated_submission_time=67680.011702, global_step=148639, preemption_count=0, score=67680.011702, test/accuracy=0.601600, test/loss=1.700270, test/num_examples=10000, total_duration=73691.216389, train/accuracy=0.795332, train/loss=0.796320, validation/accuracy=0.724080, validation/loss=1.092710, validation/num_examples=50000
I0202 08:41:21.226028 139702527031040 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.5611302852630615, loss=3.3002166748046875
I0202 08:42:05.908615 139702543816448 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.88946270942688, loss=3.1398351192474365
I0202 08:42:51.996091 139702527031040 logging_writer.py:48] [148900] global_step=148900, grad_norm=4.012310028076172, loss=2.829893112182617
I0202 08:43:38.089708 139702543816448 logging_writer.py:48] [149000] global_step=149000, grad_norm=4.027632713317871, loss=3.8014068603515625
I0202 08:44:23.957535 139702527031040 logging_writer.py:48] [149100] global_step=149100, grad_norm=4.359620571136475, loss=1.7480554580688477
I0202 08:45:10.589690 139702543816448 logging_writer.py:48] [149200] global_step=149200, grad_norm=4.379641056060791, loss=2.2314858436584473
I0202 08:45:56.377230 139702527031040 logging_writer.py:48] [149300] global_step=149300, grad_norm=4.185597896575928, loss=1.6593021154403687
I0202 08:46:42.366522 139702543816448 logging_writer.py:48] [149400] global_step=149400, grad_norm=4.055706024169922, loss=1.7940093278884888
I0202 08:47:28.303659 139702527031040 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.989337682723999, loss=2.2033636569976807
I0202 08:47:56.468036 139863983413056 spec.py:321] Evaluating on the training split.
I0202 08:48:06.795098 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 08:48:34.302278 139863983413056 spec.py:349] Evaluating on the test split.
I0202 08:48:35.941966 139863983413056 submission_runner.py:408] Time since start: 74150.73s, 	Step: 149563, 	{'train/accuracy': 0.7990038990974426, 'train/loss': 0.7832265496253967, 'validation/accuracy': 0.7269600033760071, 'validation/loss': 1.0977861881256104, 'validation/num_examples': 50000, 'test/accuracy': 0.6046000123023987, 'test/loss': 1.7144430875778198, 'test/num_examples': 10000, 'score': 68099.94502019882, 'total_duration': 74150.72949790955, 'accumulated_submission_time': 68099.94502019882, 'accumulated_eval_time': 6035.759362697601, 'accumulated_logging_time': 7.138274669647217}
I0202 08:48:35.983534 139702543816448 logging_writer.py:48] [149563] accumulated_eval_time=6035.759363, accumulated_logging_time=7.138275, accumulated_submission_time=68099.945020, global_step=149563, preemption_count=0, score=68099.945020, test/accuracy=0.604600, test/loss=1.714443, test/num_examples=10000, total_duration=74150.729498, train/accuracy=0.799004, train/loss=0.783227, validation/accuracy=0.726960, validation/loss=1.097786, validation/num_examples=50000
I0202 08:48:51.136512 139702527031040 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.9866902828216553, loss=1.556275725364685
I0202 08:49:34.495178 139702543816448 logging_writer.py:48] [149700] global_step=149700, grad_norm=4.213221073150635, loss=1.7479289770126343
I0202 08:50:20.497033 139702527031040 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.293403625488281, loss=1.6248228549957275
I0202 08:51:06.843994 139702543816448 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.166405200958252, loss=1.7031217813491821
I0202 08:51:52.400607 139702527031040 logging_writer.py:48] [150000] global_step=150000, grad_norm=4.318554878234863, loss=3.8121750354766846
I0202 08:52:38.109983 139702543816448 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.974452018737793, loss=1.6385246515274048
I0202 08:53:24.309965 139702527031040 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.8984954357147217, loss=3.6684868335723877
I0202 08:54:10.119075 139702543816448 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.614601135253906, loss=3.998401165008545
I0202 08:54:56.284623 139702527031040 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.7226901054382324, loss=2.8598148822784424
I0202 08:55:36.086678 139863983413056 spec.py:321] Evaluating on the training split.
I0202 08:55:46.336802 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 08:56:12.659587 139863983413056 spec.py:349] Evaluating on the test split.
I0202 08:56:14.303699 139863983413056 submission_runner.py:408] Time since start: 74609.09s, 	Step: 150488, 	{'train/accuracy': 0.7956249713897705, 'train/loss': 0.7945720553398132, 'validation/accuracy': 0.7290799617767334, 'validation/loss': 1.082510232925415, 'validation/num_examples': 50000, 'test/accuracy': 0.6063000559806824, 'test/loss': 1.6938530206680298, 'test/num_examples': 10000, 'score': 68519.99004364014, 'total_duration': 74609.09123158455, 'accumulated_submission_time': 68519.99004364014, 'accumulated_eval_time': 6073.976391792297, 'accumulated_logging_time': 7.188690185546875}
I0202 08:56:14.342190 139702543816448 logging_writer.py:48] [150488] accumulated_eval_time=6073.976392, accumulated_logging_time=7.188690, accumulated_submission_time=68519.990044, global_step=150488, preemption_count=0, score=68519.990044, test/accuracy=0.606300, test/loss=1.693853, test/num_examples=10000, total_duration=74609.091232, train/accuracy=0.795625, train/loss=0.794572, validation/accuracy=0.729080, validation/loss=1.082510, validation/num_examples=50000
I0202 08:56:19.525544 139702527031040 logging_writer.py:48] [150500] global_step=150500, grad_norm=4.18177604675293, loss=1.6265121698379517
I0202 08:57:01.177128 139702543816448 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.8894457817077637, loss=2.4319353103637695
I0202 08:57:47.197555 139702527031040 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.8374712467193604, loss=1.9637929201126099
I0202 08:58:33.592561 139702543816448 logging_writer.py:48] [150800] global_step=150800, grad_norm=4.217276573181152, loss=1.620305061340332
I0202 08:59:19.761290 139702527031040 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.295711040496826, loss=1.7634018659591675
I0202 09:00:05.960243 139702543816448 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.1015849113464355, loss=2.4614453315734863
I0202 09:00:51.745407 139702527031040 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.7625045776367188, loss=2.3054661750793457
I0202 09:01:37.801674 139702543816448 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.202942848205566, loss=1.5344904661178589
I0202 09:02:23.984271 139702527031040 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.593806743621826, loss=1.5876812934875488
I0202 09:03:10.117032 139702543816448 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.100658416748047, loss=1.7904022932052612
I0202 09:03:14.426321 139863983413056 spec.py:321] Evaluating on the training split.
I0202 09:03:24.949200 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 09:03:50.639187 139863983413056 spec.py:349] Evaluating on the test split.
I0202 09:03:52.274753 139863983413056 submission_runner.py:408] Time since start: 75067.06s, 	Step: 151411, 	{'train/accuracy': 0.7971875071525574, 'train/loss': 0.7923197746276855, 'validation/accuracy': 0.7300999760627747, 'validation/loss': 1.0846587419509888, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.6871274709701538, 'test/num_examples': 10000, 'score': 68940.01628899574, 'total_duration': 75067.06227970123, 'accumulated_submission_time': 68940.01628899574, 'accumulated_eval_time': 6111.824824333191, 'accumulated_logging_time': 7.236317157745361}
I0202 09:03:52.312884 139702527031040 logging_writer.py:48] [151411] accumulated_eval_time=6111.824824, accumulated_logging_time=7.236317, accumulated_submission_time=68940.016289, global_step=151411, preemption_count=0, score=68940.016289, test/accuracy=0.610200, test/loss=1.687127, test/num_examples=10000, total_duration=75067.062280, train/accuracy=0.797188, train/loss=0.792320, validation/accuracy=0.730100, validation/loss=1.084659, validation/num_examples=50000
I0202 09:04:28.920185 139702543816448 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.6937408447265625, loss=3.724818229675293
I0202 09:05:14.670960 139702527031040 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.154228687286377, loss=1.638405680656433
I0202 09:06:00.481709 139702543816448 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.274308204650879, loss=1.858495831489563
I0202 09:06:46.699088 139702527031040 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.167100429534912, loss=3.19307279586792
I0202 09:07:32.582318 139702543816448 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.788556098937988, loss=3.8348255157470703
I0202 09:08:18.704065 139702527031040 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.1605658531188965, loss=2.66939377784729
I0202 09:09:04.607450 139702543816448 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.547794818878174, loss=1.6258726119995117
I0202 09:09:50.688453 139702527031040 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.9243345260620117, loss=2.8052282333374023
I0202 09:10:36.721808 139702543816448 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.5419135093688965, loss=3.748091459274292
I0202 09:10:52.569814 139863983413056 spec.py:321] Evaluating on the training split.
I0202 09:11:03.132283 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 09:11:32.926524 139863983413056 spec.py:349] Evaluating on the test split.
I0202 09:11:34.569179 139863983413056 submission_runner.py:408] Time since start: 75529.36s, 	Step: 152336, 	{'train/accuracy': 0.8050390481948853, 'train/loss': 0.7471283078193665, 'validation/accuracy': 0.7305999994277954, 'validation/loss': 1.0720800161361694, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.6767473220825195, 'test/num_examples': 10000, 'score': 69360.21492862701, 'total_duration': 75529.35670304298, 'accumulated_submission_time': 69360.21492862701, 'accumulated_eval_time': 6153.8241646289825, 'accumulated_logging_time': 7.284022569656372}
I0202 09:11:34.606599 139702527031040 logging_writer.py:48] [152336] accumulated_eval_time=6153.824165, accumulated_logging_time=7.284023, accumulated_submission_time=69360.214929, global_step=152336, preemption_count=0, score=69360.214929, test/accuracy=0.612100, test/loss=1.676747, test/num_examples=10000, total_duration=75529.356703, train/accuracy=0.805039, train/loss=0.747128, validation/accuracy=0.730600, validation/loss=1.072080, validation/num_examples=50000
I0202 09:12:00.547140 139702543816448 logging_writer.py:48] [152400] global_step=152400, grad_norm=5.113221645355225, loss=3.8023147583007812
I0202 09:12:45.373587 139702527031040 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.871628761291504, loss=1.6450419425964355
I0202 09:13:31.224447 139702543816448 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.31131649017334, loss=1.7577667236328125
I0202 09:14:34.976142 139702527031040 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.621730804443359, loss=1.898493766784668
I0202 09:15:21.809819 139702543816448 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.080721378326416, loss=2.0415937900543213
I0202 09:16:07.853744 139702527031040 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.672557830810547, loss=1.7230870723724365
I0202 09:16:53.839146 139702543816448 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.278436183929443, loss=1.602142333984375
I0202 09:17:39.734774 139702527031040 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.108706474304199, loss=2.7033886909484863
I0202 09:18:25.744162 139702543816448 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.871597766876221, loss=3.5134525299072266
I0202 09:18:34.588941 139863983413056 spec.py:321] Evaluating on the training split.
I0202 09:18:45.151446 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 09:19:08.226327 139863983413056 spec.py:349] Evaluating on the test split.
I0202 09:19:09.872958 139863983413056 submission_runner.py:408] Time since start: 75984.66s, 	Step: 153221, 	{'train/accuracy': 0.8126757740974426, 'train/loss': 0.722992479801178, 'validation/accuracy': 0.7340399622917175, 'validation/loss': 1.0540411472320557, 'validation/num_examples': 50000, 'test/accuracy': 0.6136000156402588, 'test/loss': 1.6623154878616333, 'test/num_examples': 10000, 'score': 69780.14233207703, 'total_duration': 75984.6604912281, 'accumulated_submission_time': 69780.14233207703, 'accumulated_eval_time': 6189.108179092407, 'accumulated_logging_time': 7.329882383346558}
I0202 09:19:09.910738 139702527031040 logging_writer.py:48] [153221] accumulated_eval_time=6189.108179, accumulated_logging_time=7.329882, accumulated_submission_time=69780.142332, global_step=153221, preemption_count=0, score=69780.142332, test/accuracy=0.613600, test/loss=1.662315, test/num_examples=10000, total_duration=75984.660491, train/accuracy=0.812676, train/loss=0.722992, validation/accuracy=0.734040, validation/loss=1.054041, validation/num_examples=50000
I0202 09:19:41.859578 139702543816448 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.240606307983398, loss=2.5132594108581543
I0202 09:20:27.755048 139702527031040 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.9255404472351074, loss=2.834498405456543
I0202 09:21:14.067961 139702543816448 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.142918109893799, loss=2.648895025253296
I0202 09:21:59.987140 139702527031040 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.340967178344727, loss=1.6523981094360352
I0202 09:22:45.721597 139702543816448 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.28324556350708, loss=1.5809000730514526
I0202 09:23:31.670572 139702527031040 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.519532203674316, loss=1.5603947639465332
I0202 09:24:17.788579 139702543816448 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.950024366378784, loss=1.9745289087295532
I0202 09:25:03.858168 139702527031040 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.458071708679199, loss=1.6005885601043701
I0202 09:25:49.763619 139702543816448 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.421088218688965, loss=1.6253713369369507
I0202 09:26:10.157152 139863983413056 spec.py:321] Evaluating on the training split.
I0202 09:26:20.528559 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 09:26:43.189208 139863983413056 spec.py:349] Evaluating on the test split.
I0202 09:26:44.822446 139863983413056 submission_runner.py:408] Time since start: 76439.61s, 	Step: 154146, 	{'train/accuracy': 0.8069726228713989, 'train/loss': 0.7545697093009949, 'validation/accuracy': 0.7346999645233154, 'validation/loss': 1.059226393699646, 'validation/num_examples': 50000, 'test/accuracy': 0.6131000518798828, 'test/loss': 1.6655516624450684, 'test/num_examples': 10000, 'score': 70200.330078125, 'total_duration': 76439.60998177528, 'accumulated_submission_time': 70200.330078125, 'accumulated_eval_time': 6223.7734811306, 'accumulated_logging_time': 7.37775182723999}
I0202 09:26:44.864280 139702527031040 logging_writer.py:48] [154146] accumulated_eval_time=6223.773481, accumulated_logging_time=7.377752, accumulated_submission_time=70200.330078, global_step=154146, preemption_count=0, score=70200.330078, test/accuracy=0.613100, test/loss=1.665552, test/num_examples=10000, total_duration=76439.609982, train/accuracy=0.806973, train/loss=0.754570, validation/accuracy=0.734700, validation/loss=1.059226, validation/num_examples=50000
I0202 09:27:06.821022 139702543816448 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.7941107749938965, loss=1.6836949586868286
I0202 09:27:50.830072 139702527031040 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.465673446655273, loss=1.7891759872436523
I0202 09:28:37.143739 139702543816448 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.489135265350342, loss=2.1611738204956055
I0202 09:29:23.649367 139702527031040 logging_writer.py:48] [154500] global_step=154500, grad_norm=5.013509750366211, loss=3.7820775508880615
I0202 09:30:09.595688 139702543816448 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.446626663208008, loss=1.6835286617279053
I0202 09:30:55.456382 139702527031040 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.453321933746338, loss=1.5233811140060425
I0202 09:31:41.303451 139702543816448 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.3552021980285645, loss=2.23046612739563
I0202 09:32:27.165543 139702527031040 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.559505939483643, loss=2.014737606048584
I0202 09:33:13.388376 139702543816448 logging_writer.py:48] [155000] global_step=155000, grad_norm=5.367422580718994, loss=3.8965137004852295
I0202 09:33:45.140756 139863983413056 spec.py:321] Evaluating on the training split.
I0202 09:33:55.621934 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 09:34:21.358096 139863983413056 spec.py:349] Evaluating on the test split.
I0202 09:34:22.999925 139863983413056 submission_runner.py:408] Time since start: 76897.79s, 	Step: 155071, 	{'train/accuracy': 0.8126562237739563, 'train/loss': 0.7402761578559875, 'validation/accuracy': 0.7357199788093567, 'validation/loss': 1.0593024492263794, 'validation/num_examples': 50000, 'test/accuracy': 0.6117000579833984, 'test/loss': 1.6622600555419922, 'test/num_examples': 10000, 'score': 70620.54906439781, 'total_duration': 76897.78743886948, 'accumulated_submission_time': 70620.54906439781, 'accumulated_eval_time': 6261.632649898529, 'accumulated_logging_time': 7.428417921066284}
I0202 09:34:23.049762 139702527031040 logging_writer.py:48] [155071] accumulated_eval_time=6261.632650, accumulated_logging_time=7.428418, accumulated_submission_time=70620.549064, global_step=155071, preemption_count=0, score=70620.549064, test/accuracy=0.611700, test/loss=1.662260, test/num_examples=10000, total_duration=76897.787439, train/accuracy=0.812656, train/loss=0.740276, validation/accuracy=0.735720, validation/loss=1.059302, validation/num_examples=50000
I0202 09:34:35.020854 139702543816448 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.249114036560059, loss=1.5055903196334839
I0202 09:35:25.617085 139702527031040 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.149027347564697, loss=1.977491855621338
I0202 09:36:14.162834 139702543816448 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.457746505737305, loss=1.6131844520568848
I0202 09:37:00.207704 139702527031040 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.398375988006592, loss=2.773466110229492
I0202 09:37:46.303281 139702543816448 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.804681777954102, loss=1.5767799615859985
I0202 09:38:32.322227 139702527031040 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.285262584686279, loss=2.863908290863037
I0202 09:39:18.386480 139702543816448 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.564514636993408, loss=1.6383785009384155
I0202 09:40:04.407101 139702527031040 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.632608890533447, loss=3.298764705657959
I0202 09:40:50.274714 139702543816448 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.310107707977295, loss=2.627739906311035
I0202 09:41:23.010069 139863983413056 spec.py:321] Evaluating on the training split.
I0202 09:41:33.242543 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 09:41:57.712259 139863983413056 spec.py:349] Evaluating on the test split.
I0202 09:41:59.342875 139863983413056 submission_runner.py:408] Time since start: 77354.13s, 	Step: 155973, 	{'train/accuracy': 0.81556636095047, 'train/loss': 0.7248516082763672, 'validation/accuracy': 0.7346000075340271, 'validation/loss': 1.0620793104171753, 'validation/num_examples': 50000, 'test/accuracy': 0.6123000383377075, 'test/loss': 1.667306900024414, 'test/num_examples': 10000, 'score': 71040.45113134384, 'total_duration': 77354.13040804863, 'accumulated_submission_time': 71040.45113134384, 'accumulated_eval_time': 6297.965461015701, 'accumulated_logging_time': 7.48820424079895}
I0202 09:41:59.384129 139702527031040 logging_writer.py:48] [155973] accumulated_eval_time=6297.965461, accumulated_logging_time=7.488204, accumulated_submission_time=71040.451131, global_step=155973, preemption_count=0, score=71040.451131, test/accuracy=0.612300, test/loss=1.667307, test/num_examples=10000, total_duration=77354.130408, train/accuracy=0.815566, train/loss=0.724852, validation/accuracy=0.734600, validation/loss=1.062079, validation/num_examples=50000
I0202 09:42:10.547796 139702543816448 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.394628524780273, loss=1.6599819660186768
I0202 09:42:53.146125 139702527031040 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.49028205871582, loss=3.331422805786133
I0202 09:43:39.378874 139702543816448 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.566746234893799, loss=2.830953359603882
I0202 09:44:25.365692 139702527031040 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.414945125579834, loss=2.9946160316467285
I0202 09:45:11.576867 139702543816448 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.451058864593506, loss=2.9002981185913086
I0202 09:45:57.777161 139702527031040 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.6912360191345215, loss=1.7779093980789185
I0202 09:46:43.698456 139702543816448 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.851466655731201, loss=1.6290011405944824
I0202 09:47:29.435646 139702527031040 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.619847297668457, loss=1.790147066116333
I0202 09:48:15.446439 139702543816448 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.707745552062988, loss=1.6458243131637573
I0202 09:48:59.830446 139863983413056 spec.py:321] Evaluating on the training split.
I0202 09:49:10.540268 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 09:49:35.821635 139863983413056 spec.py:349] Evaluating on the test split.
I0202 09:49:37.447489 139863983413056 submission_runner.py:408] Time since start: 77812.24s, 	Step: 156898, 	{'train/accuracy': 0.8093163967132568, 'train/loss': 0.7406788468360901, 'validation/accuracy': 0.7397199869155884, 'validation/loss': 1.0419944524765015, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.6425464153289795, 'test/num_examples': 10000, 'score': 71460.83744478226, 'total_duration': 77812.2350218296, 'accumulated_submission_time': 71460.83744478226, 'accumulated_eval_time': 6335.5825090408325, 'accumulated_logging_time': 7.540948152542114}
I0202 09:49:37.489044 139702527031040 logging_writer.py:48] [156898] accumulated_eval_time=6335.582509, accumulated_logging_time=7.540948, accumulated_submission_time=71460.837445, global_step=156898, preemption_count=0, score=71460.837445, test/accuracy=0.617000, test/loss=1.642546, test/num_examples=10000, total_duration=77812.235022, train/accuracy=0.809316, train/loss=0.740679, validation/accuracy=0.739720, validation/loss=1.041994, validation/num_examples=50000
I0202 09:49:38.686760 139702543816448 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.941903114318848, loss=3.5634915828704834
I0202 09:50:19.834907 139702527031040 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.7885026931762695, loss=2.5048060417175293
I0202 09:51:05.576222 139702543816448 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.808347702026367, loss=1.5779752731323242
I0202 09:51:51.680303 139702527031040 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.278740882873535, loss=1.7196851968765259
I0202 09:52:37.907225 139702543816448 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.923038005828857, loss=1.5647014379501343
I0202 09:53:24.090435 139702527031040 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.341804027557373, loss=1.9938533306121826
I0202 09:54:09.730855 139702543816448 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.826234817504883, loss=1.5215325355529785
I0202 09:54:55.368922 139702527031040 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.712658405303955, loss=1.94111967086792
I0202 09:55:41.531529 139702543816448 logging_writer.py:48] [157700] global_step=157700, grad_norm=5.20238733291626, loss=1.5267311334609985
I0202 09:56:27.376908 139702527031040 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.828542232513428, loss=1.6155643463134766
I0202 09:56:37.609025 139863983413056 spec.py:321] Evaluating on the training split.
I0202 09:56:48.120366 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 09:57:13.301912 139863983413056 spec.py:349] Evaluating on the test split.
I0202 09:57:14.945920 139863983413056 submission_runner.py:408] Time since start: 78269.73s, 	Step: 157824, 	{'train/accuracy': 0.8129101395606995, 'train/loss': 0.7125070095062256, 'validation/accuracy': 0.7402399778366089, 'validation/loss': 1.0308599472045898, 'validation/num_examples': 50000, 'test/accuracy': 0.6149000525474548, 'test/loss': 1.6336321830749512, 'test/num_examples': 10000, 'score': 71880.89972639084, 'total_duration': 78269.73344492912, 'accumulated_submission_time': 71880.89972639084, 'accumulated_eval_time': 6372.919394493103, 'accumulated_logging_time': 7.591905832290649}
I0202 09:57:14.988507 139702543816448 logging_writer.py:48] [157824] accumulated_eval_time=6372.919394, accumulated_logging_time=7.591906, accumulated_submission_time=71880.899726, global_step=157824, preemption_count=0, score=71880.899726, test/accuracy=0.614900, test/loss=1.633632, test/num_examples=10000, total_duration=78269.733445, train/accuracy=0.812910, train/loss=0.712507, validation/accuracy=0.740240, validation/loss=1.030860, validation/num_examples=50000
I0202 09:57:45.705377 139702527031040 logging_writer.py:48] [157900] global_step=157900, grad_norm=5.059665679931641, loss=1.8379329442977905
I0202 09:58:31.354986 139702543816448 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.776202201843262, loss=1.7739675045013428
I0202 09:59:17.221659 139702527031040 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.909168243408203, loss=1.627936601638794
I0202 10:00:03.626582 139702543816448 logging_writer.py:48] [158200] global_step=158200, grad_norm=5.232338905334473, loss=1.8089641332626343
I0202 10:00:49.569844 139702527031040 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.769645690917969, loss=1.8015767335891724
I0202 10:01:35.633439 139702543816448 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.343324661254883, loss=1.9675449132919312
I0202 10:02:21.583549 139702527031040 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.643301486968994, loss=1.5389773845672607
I0202 10:03:07.771654 139702543816448 logging_writer.py:48] [158600] global_step=158600, grad_norm=5.129131317138672, loss=1.6138041019439697
I0202 10:03:53.897053 139702527031040 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.671298503875732, loss=2.184751272201538
I0202 10:04:15.209818 139863983413056 spec.py:321] Evaluating on the training split.
I0202 10:04:25.694681 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 10:04:51.684114 139863983413056 spec.py:349] Evaluating on the test split.
I0202 10:04:53.326234 139863983413056 submission_runner.py:408] Time since start: 78728.11s, 	Step: 158748, 	{'train/accuracy': 0.8221093416213989, 'train/loss': 0.6898041367530823, 'validation/accuracy': 0.7427399754524231, 'validation/loss': 1.029032826423645, 'validation/num_examples': 50000, 'test/accuracy': 0.6207000017166138, 'test/loss': 1.6169078350067139, 'test/num_examples': 10000, 'score': 72301.06321072578, 'total_duration': 78728.11376452446, 'accumulated_submission_time': 72301.06321072578, 'accumulated_eval_time': 6411.03583574295, 'accumulated_logging_time': 7.643307447433472}
I0202 10:04:53.365678 139702543816448 logging_writer.py:48] [158748] accumulated_eval_time=6411.035836, accumulated_logging_time=7.643307, accumulated_submission_time=72301.063211, global_step=158748, preemption_count=0, score=72301.063211, test/accuracy=0.620700, test/loss=1.616908, test/num_examples=10000, total_duration=78728.113765, train/accuracy=0.822109, train/loss=0.689804, validation/accuracy=0.742740, validation/loss=1.029033, validation/num_examples=50000
I0202 10:05:14.505384 139702527031040 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.469939708709717, loss=2.3500216007232666
I0202 10:05:59.110926 139702543816448 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.7409138679504395, loss=1.4531915187835693
I0202 10:06:45.306137 139702527031040 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.864309310913086, loss=2.9247498512268066
I0202 10:07:31.734468 139702543816448 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.438911437988281, loss=3.1692779064178467
I0202 10:08:17.675173 139702527031040 logging_writer.py:48] [159200] global_step=159200, grad_norm=5.252131462097168, loss=3.202413558959961
I0202 10:09:03.749236 139702543816448 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.763032913208008, loss=1.5261805057525635
I0202 10:09:49.866593 139702527031040 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.728455543518066, loss=3.3667335510253906
I0202 10:10:35.811124 139702543816448 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.727487087249756, loss=2.118129014968872
I0202 10:11:21.853793 139702527031040 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.834348678588867, loss=1.462942123413086
I0202 10:11:53.658904 139863983413056 spec.py:321] Evaluating on the training split.
I0202 10:12:04.072985 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 10:12:32.630421 139863983413056 spec.py:349] Evaluating on the test split.
I0202 10:12:34.270607 139863983413056 submission_runner.py:408] Time since start: 79189.06s, 	Step: 159671, 	{'train/accuracy': 0.81800776720047, 'train/loss': 0.7054724097251892, 'validation/accuracy': 0.7444599866867065, 'validation/loss': 1.0100157260894775, 'validation/num_examples': 50000, 'test/accuracy': 0.6194000244140625, 'test/loss': 1.619384765625, 'test/num_examples': 10000, 'score': 72721.30015707016, 'total_duration': 79189.0581202507, 'accumulated_submission_time': 72721.30015707016, 'accumulated_eval_time': 6451.647526979446, 'accumulated_logging_time': 7.691318511962891}
I0202 10:12:34.320328 139702543816448 logging_writer.py:48] [159671] accumulated_eval_time=6451.647527, accumulated_logging_time=7.691319, accumulated_submission_time=72721.300157, global_step=159671, preemption_count=0, score=72721.300157, test/accuracy=0.619400, test/loss=1.619385, test/num_examples=10000, total_duration=79189.058120, train/accuracy=0.818008, train/loss=0.705472, validation/accuracy=0.744460, validation/loss=1.010016, validation/num_examples=50000
I0202 10:12:46.275378 139702527031040 logging_writer.py:48] [159700] global_step=159700, grad_norm=5.065016269683838, loss=1.787543535232544
I0202 10:13:29.191744 139702543816448 logging_writer.py:48] [159800] global_step=159800, grad_norm=5.408440113067627, loss=1.5027203559875488
I0202 10:14:15.218400 139702527031040 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.737207889556885, loss=1.551954984664917
I0202 10:15:01.514066 139702543816448 logging_writer.py:48] [160000] global_step=160000, grad_norm=5.349136829376221, loss=1.7754926681518555
I0202 10:15:47.466375 139702527031040 logging_writer.py:48] [160100] global_step=160100, grad_norm=5.172685146331787, loss=1.5145924091339111
I0202 10:16:33.813616 139702543816448 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.5663371086120605, loss=1.9341511726379395
I0202 10:17:19.897905 139702527031040 logging_writer.py:48] [160300] global_step=160300, grad_norm=5.015321254730225, loss=1.5827829837799072
I0202 10:18:06.033428 139702543816448 logging_writer.py:48] [160400] global_step=160400, grad_norm=5.028287887573242, loss=1.6517084836959839
I0202 10:18:52.238172 139702527031040 logging_writer.py:48] [160500] global_step=160500, grad_norm=5.094338893890381, loss=3.191926956176758
I0202 10:19:34.552838 139863983413056 spec.py:321] Evaluating on the training split.
I0202 10:19:45.241153 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 10:20:09.122070 139863983413056 spec.py:349] Evaluating on the test split.
I0202 10:20:10.757097 139863983413056 submission_runner.py:408] Time since start: 79645.54s, 	Step: 160594, 	{'train/accuracy': 0.8192187547683716, 'train/loss': 0.6979327201843262, 'validation/accuracy': 0.7441799640655518, 'validation/loss': 1.0171470642089844, 'validation/num_examples': 50000, 'test/accuracy': 0.6256000399589539, 'test/loss': 1.6140958070755005, 'test/num_examples': 10000, 'score': 73141.47015810013, 'total_duration': 79645.54460167885, 'accumulated_submission_time': 73141.47015810013, 'accumulated_eval_time': 6487.851769685745, 'accumulated_logging_time': 7.754884719848633}
I0202 10:20:10.799726 139702543816448 logging_writer.py:48] [160594] accumulated_eval_time=6487.851770, accumulated_logging_time=7.754885, accumulated_submission_time=73141.470158, global_step=160594, preemption_count=0, score=73141.470158, test/accuracy=0.625600, test/loss=1.614096, test/num_examples=10000, total_duration=79645.544602, train/accuracy=0.819219, train/loss=0.697933, validation/accuracy=0.744180, validation/loss=1.017147, validation/num_examples=50000
I0202 10:20:13.592076 139702527031040 logging_writer.py:48] [160600] global_step=160600, grad_norm=5.259407043457031, loss=3.2088825702667236
I0202 10:20:54.907917 139702543816448 logging_writer.py:48] [160700] global_step=160700, grad_norm=5.129915714263916, loss=2.80017352104187
I0202 10:21:40.662693 139702527031040 logging_writer.py:48] [160800] global_step=160800, grad_norm=6.297491550445557, loss=3.4651224613189697
I0202 10:22:26.880908 139702543816448 logging_writer.py:48] [160900] global_step=160900, grad_norm=5.226619720458984, loss=3.496610641479492
I0202 10:23:12.791633 139702527031040 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.661121845245361, loss=1.9104418754577637
I0202 10:23:58.842601 139702543816448 logging_writer.py:48] [161100] global_step=161100, grad_norm=5.3137922286987305, loss=1.471397042274475
I0202 10:24:45.021497 139702527031040 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.75844669342041, loss=2.8617987632751465
I0202 10:25:30.850918 139702543816448 logging_writer.py:48] [161300] global_step=161300, grad_norm=5.43066930770874, loss=1.5511133670806885
I0202 10:26:17.238917 139702527031040 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.793386459350586, loss=1.536847710609436
I0202 10:27:03.311302 139702543816448 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.9637556076049805, loss=2.00675630569458
I0202 10:27:11.190936 139863983413056 spec.py:321] Evaluating on the training split.
I0202 10:27:21.731489 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 10:27:45.581011 139863983413056 spec.py:349] Evaluating on the test split.
I0202 10:27:47.215589 139863983413056 submission_runner.py:408] Time since start: 80102.00s, 	Step: 161519, 	{'train/accuracy': 0.8247851133346558, 'train/loss': 0.6756976842880249, 'validation/accuracy': 0.7467600107192993, 'validation/loss': 1.0093703269958496, 'validation/num_examples': 50000, 'test/accuracy': 0.626300036907196, 'test/loss': 1.5992350578308105, 'test/num_examples': 10000, 'score': 73561.80247449875, 'total_duration': 80102.00310349464, 'accumulated_submission_time': 73561.80247449875, 'accumulated_eval_time': 6523.876390695572, 'accumulated_logging_time': 7.80646276473999}
I0202 10:27:47.255310 139702527031040 logging_writer.py:48] [161519] accumulated_eval_time=6523.876391, accumulated_logging_time=7.806463, accumulated_submission_time=73561.802474, global_step=161519, preemption_count=0, score=73561.802474, test/accuracy=0.626300, test/loss=1.599235, test/num_examples=10000, total_duration=80102.003103, train/accuracy=0.824785, train/loss=0.675698, validation/accuracy=0.746760, validation/loss=1.009370, validation/num_examples=50000
I0202 10:28:19.996484 139702543816448 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.984381198883057, loss=1.521349310874939
I0202 10:29:05.635615 139702527031040 logging_writer.py:48] [161700] global_step=161700, grad_norm=5.029701232910156, loss=2.1139674186706543
I0202 10:29:51.703498 139702543816448 logging_writer.py:48] [161800] global_step=161800, grad_norm=6.044429302215576, loss=3.8418052196502686
I0202 10:30:37.803384 139702527031040 logging_writer.py:48] [161900] global_step=161900, grad_norm=5.113649845123291, loss=1.7631916999816895
I0202 10:31:23.599611 139702543816448 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.4505615234375, loss=2.2759265899658203
I0202 10:32:09.539060 139702527031040 logging_writer.py:48] [162100] global_step=162100, grad_norm=5.787649154663086, loss=3.3063597679138184
I0202 10:32:55.389720 139702543816448 logging_writer.py:48] [162200] global_step=162200, grad_norm=5.502329349517822, loss=1.5777900218963623
I0202 10:33:41.013005 139702527031040 logging_writer.py:48] [162300] global_step=162300, grad_norm=5.624847412109375, loss=1.5318663120269775
I0202 10:34:27.196968 139702543816448 logging_writer.py:48] [162400] global_step=162400, grad_norm=5.048059463500977, loss=1.3887232542037964
I0202 10:34:47.623056 139863983413056 spec.py:321] Evaluating on the training split.
I0202 10:34:58.010480 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 10:35:27.094825 139863983413056 spec.py:349] Evaluating on the test split.
I0202 10:35:28.744799 139863983413056 submission_runner.py:408] Time since start: 80563.53s, 	Step: 162446, 	{'train/accuracy': 0.8268163800239563, 'train/loss': 0.6841356754302979, 'validation/accuracy': 0.7471999526023865, 'validation/loss': 1.016317367553711, 'validation/num_examples': 50000, 'test/accuracy': 0.6248000264167786, 'test/loss': 1.616760015487671, 'test/num_examples': 10000, 'score': 73982.11289167404, 'total_duration': 80563.53231620789, 'accumulated_submission_time': 73982.11289167404, 'accumulated_eval_time': 6564.9981191158295, 'accumulated_logging_time': 7.855017900466919}
I0202 10:35:28.793286 139702527031040 logging_writer.py:48] [162446] accumulated_eval_time=6564.998119, accumulated_logging_time=7.855018, accumulated_submission_time=73982.112892, global_step=162446, preemption_count=0, score=73982.112892, test/accuracy=0.624800, test/loss=1.616760, test/num_examples=10000, total_duration=80563.532316, train/accuracy=0.826816, train/loss=0.684136, validation/accuracy=0.747200, validation/loss=1.016317, validation/num_examples=50000
I0202 10:35:50.730410 139702543816448 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.90260124206543, loss=1.4626600742340088
I0202 10:36:35.015972 139702527031040 logging_writer.py:48] [162600] global_step=162600, grad_norm=5.203921318054199, loss=2.643495798110962
I0202 10:37:21.233333 139702543816448 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.969643592834473, loss=1.779151439666748
I0202 10:38:07.437629 139702527031040 logging_writer.py:48] [162800] global_step=162800, grad_norm=5.1644744873046875, loss=1.5276304483413696
I0202 10:38:53.280489 139702543816448 logging_writer.py:48] [162900] global_step=162900, grad_norm=6.5722198486328125, loss=3.9308793544769287
I0202 10:39:39.246414 139702527031040 logging_writer.py:48] [163000] global_step=163000, grad_norm=5.2916436195373535, loss=2.126866579055786
I0202 10:40:25.299248 139702543816448 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.885157108306885, loss=2.761107921600342
I0202 10:41:11.228565 139702527031040 logging_writer.py:48] [163200] global_step=163200, grad_norm=5.954459190368652, loss=1.6101515293121338
I0202 10:41:57.198600 139702543816448 logging_writer.py:48] [163300] global_step=163300, grad_norm=5.424367904663086, loss=1.4967058897018433
I0202 10:42:28.929853 139863983413056 spec.py:321] Evaluating on the training split.
I0202 10:42:39.258340 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 10:43:04.614130 139863983413056 spec.py:349] Evaluating on the test split.
I0202 10:43:06.253726 139863983413056 submission_runner.py:408] Time since start: 81021.04s, 	Step: 163371, 	{'train/accuracy': 0.8298437595367432, 'train/loss': 0.6641115546226501, 'validation/accuracy': 0.7482799887657166, 'validation/loss': 0.9978412389755249, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.6025879383087158, 'test/num_examples': 10000, 'score': 74402.1906940937, 'total_duration': 81021.04125189781, 'accumulated_submission_time': 74402.1906940937, 'accumulated_eval_time': 6602.321990013123, 'accumulated_logging_time': 7.912995100021362}
I0202 10:43:06.294174 139702527031040 logging_writer.py:48] [163371] accumulated_eval_time=6602.321990, accumulated_logging_time=7.912995, accumulated_submission_time=74402.190694, global_step=163371, preemption_count=0, score=74402.190694, test/accuracy=0.627000, test/loss=1.602588, test/num_examples=10000, total_duration=81021.041252, train/accuracy=0.829844, train/loss=0.664112, validation/accuracy=0.748280, validation/loss=0.997841, validation/num_examples=50000
I0202 10:43:18.257371 139702543816448 logging_writer.py:48] [163400] global_step=163400, grad_norm=5.526743412017822, loss=1.4818099737167358
I0202 10:44:00.824691 139702527031040 logging_writer.py:48] [163500] global_step=163500, grad_norm=5.49716854095459, loss=1.5016076564788818
I0202 10:44:46.822257 139702543816448 logging_writer.py:48] [163600] global_step=163600, grad_norm=5.343366622924805, loss=1.3952313661575317
I0202 10:45:32.855884 139702527031040 logging_writer.py:48] [163700] global_step=163700, grad_norm=5.872386932373047, loss=3.7018842697143555
I0202 10:46:18.610990 139702543816448 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.9726996421813965, loss=2.483323812484741
I0202 10:47:04.762961 139702527031040 logging_writer.py:48] [163900] global_step=163900, grad_norm=5.233633995056152, loss=1.4710843563079834
I0202 10:47:51.021013 139702543816448 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.937679767608643, loss=1.361998438835144
I0202 10:48:36.896595 139702527031040 logging_writer.py:48] [164100] global_step=164100, grad_norm=5.2603254318237305, loss=1.5762202739715576
I0202 10:49:22.917731 139702543816448 logging_writer.py:48] [164200] global_step=164200, grad_norm=5.537458419799805, loss=1.5783897638320923
I0202 10:50:06.433445 139863983413056 spec.py:321] Evaluating on the training split.
I0202 10:50:16.976629 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 10:50:40.027904 139863983413056 spec.py:349] Evaluating on the test split.
I0202 10:50:41.672901 139863983413056 submission_runner.py:408] Time since start: 81476.46s, 	Step: 164296, 	{'train/accuracy': 0.8291991949081421, 'train/loss': 0.6516792178153992, 'validation/accuracy': 0.7512399554252625, 'validation/loss': 0.9861694574356079, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.5875972509384155, 'test/num_examples': 10000, 'score': 74822.27236771584, 'total_duration': 81476.4604113102, 'accumulated_submission_time': 74822.27236771584, 'accumulated_eval_time': 6637.561418771744, 'accumulated_logging_time': 7.962857246398926}
I0202 10:50:41.723503 139702527031040 logging_writer.py:48] [164296] accumulated_eval_time=6637.561419, accumulated_logging_time=7.962857, accumulated_submission_time=74822.272368, global_step=164296, preemption_count=0, score=74822.272368, test/accuracy=0.629600, test/loss=1.587597, test/num_examples=10000, total_duration=81476.460411, train/accuracy=0.829199, train/loss=0.651679, validation/accuracy=0.751240, validation/loss=0.986169, validation/num_examples=50000
I0202 10:50:43.719619 139702543816448 logging_writer.py:48] [164300] global_step=164300, grad_norm=5.308638572692871, loss=1.5130337476730347
I0202 10:51:24.806316 139702527031040 logging_writer.py:48] [164400] global_step=164400, grad_norm=6.045804023742676, loss=3.6009225845336914
I0202 10:52:10.816172 139702543816448 logging_writer.py:48] [164500] global_step=164500, grad_norm=5.591948986053467, loss=1.5462493896484375
I0202 10:52:56.932026 139702527031040 logging_writer.py:48] [164600] global_step=164600, grad_norm=6.148308277130127, loss=3.6499814987182617
I0202 10:53:42.812875 139702543816448 logging_writer.py:48] [164700] global_step=164700, grad_norm=5.637672424316406, loss=1.4964326620101929
I0202 10:54:28.689805 139702527031040 logging_writer.py:48] [164800] global_step=164800, grad_norm=6.078380107879639, loss=3.19596529006958
I0202 10:55:14.724392 139702543816448 logging_writer.py:48] [164900] global_step=164900, grad_norm=5.589259624481201, loss=1.5023198127746582
I0202 10:56:00.555708 139702527031040 logging_writer.py:48] [165000] global_step=165000, grad_norm=5.086392402648926, loss=2.8439016342163086
I0202 10:56:46.438816 139702543816448 logging_writer.py:48] [165100] global_step=165100, grad_norm=5.421557426452637, loss=1.5247052907943726
I0202 10:57:32.651460 139702527031040 logging_writer.py:48] [165200] global_step=165200, grad_norm=5.511481761932373, loss=1.474234938621521
I0202 10:57:41.982608 139863983413056 spec.py:321] Evaluating on the training split.
I0202 10:57:53.257283 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 10:58:18.572654 139863983413056 spec.py:349] Evaluating on the test split.
I0202 10:58:20.223836 139863983413056 submission_runner.py:408] Time since start: 81935.01s, 	Step: 165222, 	{'train/accuracy': 0.8324413895606995, 'train/loss': 0.6385616660118103, 'validation/accuracy': 0.7501399517059326, 'validation/loss': 0.9882864356040955, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.5926567316055298, 'test/num_examples': 10000, 'score': 75242.47238945961, 'total_duration': 81935.01135158539, 'accumulated_submission_time': 75242.47238945961, 'accumulated_eval_time': 6675.802627325058, 'accumulated_logging_time': 8.023729801177979}
I0202 10:58:20.271399 139702543816448 logging_writer.py:48] [165222] accumulated_eval_time=6675.802627, accumulated_logging_time=8.023730, accumulated_submission_time=75242.472389, global_step=165222, preemption_count=0, score=75242.472389, test/accuracy=0.628200, test/loss=1.592657, test/num_examples=10000, total_duration=81935.011352, train/accuracy=0.832441, train/loss=0.638562, validation/accuracy=0.750140, validation/loss=0.988286, validation/num_examples=50000
I0202 10:58:51.799981 139702527031040 logging_writer.py:48] [165300] global_step=165300, grad_norm=5.3496785163879395, loss=3.1277682781219482
I0202 10:59:37.201429 139702543816448 logging_writer.py:48] [165400] global_step=165400, grad_norm=5.639420509338379, loss=2.408052921295166
I0202 11:00:23.366726 139702527031040 logging_writer.py:48] [165500] global_step=165500, grad_norm=5.735984802246094, loss=1.8023314476013184
I0202 11:01:09.602729 139702543816448 logging_writer.py:48] [165600] global_step=165600, grad_norm=5.792112350463867, loss=2.048784017562866
I0202 11:01:55.393715 139702527031040 logging_writer.py:48] [165700] global_step=165700, grad_norm=6.300249099731445, loss=3.8039443492889404
I0202 11:02:41.404380 139702543816448 logging_writer.py:48] [165800] global_step=165800, grad_norm=5.30291223526001, loss=2.550612211227417
I0202 11:03:27.384414 139702527031040 logging_writer.py:48] [165900] global_step=165900, grad_norm=5.791014671325684, loss=1.50763738155365
I0202 11:04:13.431160 139702543816448 logging_writer.py:48] [166000] global_step=166000, grad_norm=5.971958637237549, loss=3.4916634559631348
I0202 11:04:59.433962 139702527031040 logging_writer.py:48] [166100] global_step=166100, grad_norm=5.201334476470947, loss=1.7528249025344849
I0202 11:05:20.697090 139863983413056 spec.py:321] Evaluating on the training split.
I0202 11:05:31.205037 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 11:05:57.184678 139863983413056 spec.py:349] Evaluating on the test split.
I0202 11:05:58.818340 139863983413056 submission_runner.py:408] Time since start: 82393.61s, 	Step: 166148, 	{'train/accuracy': 0.8308203220367432, 'train/loss': 0.660889208316803, 'validation/accuracy': 0.7534799575805664, 'validation/loss': 0.9846858382225037, 'validation/num_examples': 50000, 'test/accuracy': 0.6331000328063965, 'test/loss': 1.5844221115112305, 'test/num_examples': 10000, 'score': 75662.8382794857, 'total_duration': 82393.60587334633, 'accumulated_submission_time': 75662.8382794857, 'accumulated_eval_time': 6713.923867702484, 'accumulated_logging_time': 8.082376480102539}
I0202 11:05:58.862833 139702543816448 logging_writer.py:48] [166148] accumulated_eval_time=6713.923868, accumulated_logging_time=8.082376, accumulated_submission_time=75662.838279, global_step=166148, preemption_count=0, score=75662.838279, test/accuracy=0.633100, test/loss=1.584422, test/num_examples=10000, total_duration=82393.605873, train/accuracy=0.830820, train/loss=0.660889, validation/accuracy=0.753480, validation/loss=0.984686, validation/num_examples=50000
I0202 11:06:20.016667 139702527031040 logging_writer.py:48] [166200] global_step=166200, grad_norm=5.9934163093566895, loss=3.125270366668701
I0202 11:07:04.152576 139702543816448 logging_writer.py:48] [166300] global_step=166300, grad_norm=5.477733135223389, loss=1.4961330890655518
I0202 11:07:50.353283 139702527031040 logging_writer.py:48] [166400] global_step=166400, grad_norm=5.897802829742432, loss=2.672626495361328
I0202 11:08:36.477100 139702543816448 logging_writer.py:48] [166500] global_step=166500, grad_norm=5.633114814758301, loss=3.150876045227051
I0202 11:09:22.228338 139702527031040 logging_writer.py:48] [166600] global_step=166600, grad_norm=6.1202473640441895, loss=3.4449350833892822
I0202 11:10:08.740930 139702543816448 logging_writer.py:48] [166700] global_step=166700, grad_norm=5.407628536224365, loss=1.5500861406326294
I0202 11:10:54.560384 139702527031040 logging_writer.py:48] [166800] global_step=166800, grad_norm=6.025914192199707, loss=3.856743335723877
I0202 11:11:40.503650 139702543816448 logging_writer.py:48] [166900] global_step=166900, grad_norm=5.78902006149292, loss=1.4690501689910889
I0202 11:12:26.728613 139702527031040 logging_writer.py:48] [167000] global_step=167000, grad_norm=5.528299808502197, loss=1.4400272369384766
I0202 11:12:59.026636 139863983413056 spec.py:321] Evaluating on the training split.
I0202 11:13:09.504988 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 11:13:36.491363 139863983413056 spec.py:349] Evaluating on the test split.
I0202 11:13:38.131114 139863983413056 submission_runner.py:408] Time since start: 82852.92s, 	Step: 167072, 	{'train/accuracy': 0.8302929401397705, 'train/loss': 0.645725667476654, 'validation/accuracy': 0.7534199953079224, 'validation/loss': 0.9803726077079773, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.5846593379974365, 'test/num_examples': 10000, 'score': 76082.94535136223, 'total_duration': 82852.91863751411, 'accumulated_submission_time': 76082.94535136223, 'accumulated_eval_time': 6753.028325080872, 'accumulated_logging_time': 8.135641813278198}
I0202 11:13:38.173850 139702543816448 logging_writer.py:48] [167072] accumulated_eval_time=6753.028325, accumulated_logging_time=8.135642, accumulated_submission_time=76082.945351, global_step=167072, preemption_count=0, score=76082.945351, test/accuracy=0.631300, test/loss=1.584659, test/num_examples=10000, total_duration=82852.918638, train/accuracy=0.830293, train/loss=0.645726, validation/accuracy=0.753420, validation/loss=0.980373, validation/num_examples=50000
I0202 11:13:49.743369 139702527031040 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.874711036682129, loss=2.9459333419799805
I0202 11:14:32.532758 139702543816448 logging_writer.py:48] [167200] global_step=167200, grad_norm=6.0496110916137695, loss=1.426517367362976
I0202 11:15:18.390635 139702527031040 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.982079982757568, loss=1.4982802867889404
I0202 11:16:04.493654 139702543816448 logging_writer.py:48] [167400] global_step=167400, grad_norm=5.659810543060303, loss=1.3843669891357422
I0202 11:16:50.096634 139702527031040 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.415899753570557, loss=1.906925082206726
I0202 11:17:36.395069 139702543816448 logging_writer.py:48] [167600] global_step=167600, grad_norm=5.621063232421875, loss=1.5502809286117554
I0202 11:18:22.532360 139702527031040 logging_writer.py:48] [167700] global_step=167700, grad_norm=7.959578990936279, loss=3.7179267406463623
I0202 11:19:08.532105 139702543816448 logging_writer.py:48] [167800] global_step=167800, grad_norm=6.1263580322265625, loss=1.5337140560150146
I0202 11:19:54.497598 139702527031040 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.90056848526001, loss=2.496551036834717
I0202 11:20:38.235827 139863983413056 spec.py:321] Evaluating on the training split.
I0202 11:20:48.685495 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 11:21:14.239962 139863983413056 spec.py:349] Evaluating on the test split.
I0202 11:21:15.878285 139863983413056 submission_runner.py:408] Time since start: 83310.67s, 	Step: 167997, 	{'train/accuracy': 0.8384179472923279, 'train/loss': 0.6208813190460205, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 0.9750881791114807, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.5767680406570435, 'test/num_examples': 10000, 'score': 76502.94843864441, 'total_duration': 83310.66582012177, 'accumulated_submission_time': 76502.94843864441, 'accumulated_eval_time': 6790.670788764954, 'accumulated_logging_time': 8.188108682632446}
I0202 11:21:15.919080 139702543816448 logging_writer.py:48] [167997] accumulated_eval_time=6790.670789, accumulated_logging_time=8.188109, accumulated_submission_time=76502.948439, global_step=167997, preemption_count=0, score=76502.948439, test/accuracy=0.632200, test/loss=1.576768, test/num_examples=10000, total_duration=83310.665820, train/accuracy=0.838418, train/loss=0.620881, validation/accuracy=0.755440, validation/loss=0.975088, validation/num_examples=50000
I0202 11:21:17.515543 139702527031040 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.505122661590576, loss=1.4127979278564453
I0202 11:21:58.616765 139702543816448 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.895132541656494, loss=1.440619707107544
I0202 11:22:44.465287 139702527031040 logging_writer.py:48] [168200] global_step=168200, grad_norm=5.595158100128174, loss=1.515265941619873
I0202 11:23:30.656532 139702543816448 logging_writer.py:48] [168300] global_step=168300, grad_norm=5.867397785186768, loss=1.7653993368148804
I0202 11:24:16.625442 139702527031040 logging_writer.py:48] [168400] global_step=168400, grad_norm=6.482553005218506, loss=2.016190528869629
I0202 11:25:02.434160 139702543816448 logging_writer.py:48] [168500] global_step=168500, grad_norm=5.422512054443359, loss=1.4650745391845703
I0202 11:25:48.544585 139702527031040 logging_writer.py:48] [168600] global_step=168600, grad_norm=5.8706536293029785, loss=1.7141920328140259
I0202 11:26:34.544687 139702543816448 logging_writer.py:48] [168700] global_step=168700, grad_norm=5.96247673034668, loss=1.3894646167755127
I0202 11:27:20.410790 139702527031040 logging_writer.py:48] [168800] global_step=168800, grad_norm=5.418644428253174, loss=2.8098268508911133
I0202 11:28:06.702692 139702543816448 logging_writer.py:48] [168900] global_step=168900, grad_norm=6.184303283691406, loss=3.674172878265381
I0202 11:28:16.154172 139863983413056 spec.py:321] Evaluating on the training split.
I0202 11:28:26.704152 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 11:28:49.466288 139863983413056 spec.py:349] Evaluating on the test split.
I0202 11:28:51.102762 139863983413056 submission_runner.py:408] Time since start: 83765.89s, 	Step: 168922, 	{'train/accuracy': 0.8319921493530273, 'train/loss': 0.6606417894363403, 'validation/accuracy': 0.756060004234314, 'validation/loss': 0.9795495271682739, 'validation/num_examples': 50000, 'test/accuracy': 0.6335000395774841, 'test/loss': 1.5818995237350464, 'test/num_examples': 10000, 'score': 76923.12551164627, 'total_duration': 83765.89028906822, 'accumulated_submission_time': 76923.12551164627, 'accumulated_eval_time': 6825.619375705719, 'accumulated_logging_time': 8.23816442489624}
I0202 11:28:51.144209 139702527031040 logging_writer.py:48] [168922] accumulated_eval_time=6825.619376, accumulated_logging_time=8.238164, accumulated_submission_time=76923.125512, global_step=168922, preemption_count=0, score=76923.125512, test/accuracy=0.633500, test/loss=1.581900, test/num_examples=10000, total_duration=83765.890289, train/accuracy=0.831992, train/loss=0.660642, validation/accuracy=0.756060, validation/loss=0.979550, validation/num_examples=50000
I0202 11:29:22.893083 139702543816448 logging_writer.py:48] [169000] global_step=169000, grad_norm=5.830074787139893, loss=3.1399343013763428
I0202 11:30:08.650073 139702527031040 logging_writer.py:48] [169100] global_step=169100, grad_norm=5.836517333984375, loss=1.343837022781372
I0202 11:30:54.618668 139702543816448 logging_writer.py:48] [169200] global_step=169200, grad_norm=5.558228969573975, loss=1.374422550201416
I0202 11:31:40.694171 139702527031040 logging_writer.py:48] [169300] global_step=169300, grad_norm=5.7786664962768555, loss=2.4061615467071533
I0202 11:32:26.527394 139702543816448 logging_writer.py:48] [169400] global_step=169400, grad_norm=6.064565658569336, loss=1.3805387020111084
I0202 11:33:12.440611 139702527031040 logging_writer.py:48] [169500] global_step=169500, grad_norm=5.831180572509766, loss=3.060375452041626
I0202 11:33:58.100496 139702543816448 logging_writer.py:48] [169600] global_step=169600, grad_norm=5.806901454925537, loss=1.9780348539352417
I0202 11:34:43.823599 139702527031040 logging_writer.py:48] [169700] global_step=169700, grad_norm=5.503386974334717, loss=1.7333036661148071
I0202 11:35:29.824498 139702543816448 logging_writer.py:48] [169800] global_step=169800, grad_norm=6.374658107757568, loss=1.5111548900604248
I0202 11:35:51.482198 139863983413056 spec.py:321] Evaluating on the training split.
I0202 11:36:01.738080 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 11:36:25.159529 139863983413056 spec.py:349] Evaluating on the test split.
I0202 11:36:26.789153 139863983413056 submission_runner.py:408] Time since start: 84221.58s, 	Step: 169849, 	{'train/accuracy': 0.8330858945846558, 'train/loss': 0.6320017576217651, 'validation/accuracy': 0.7576000094413757, 'validation/loss': 0.9574471712112427, 'validation/num_examples': 50000, 'test/accuracy': 0.6391000151634216, 'test/loss': 1.561057209968567, 'test/num_examples': 10000, 'score': 77343.40371894836, 'total_duration': 84221.57668566704, 'accumulated_submission_time': 77343.40371894836, 'accumulated_eval_time': 6860.926365375519, 'accumulated_logging_time': 8.290139198303223}
I0202 11:36:26.833820 139702527031040 logging_writer.py:48] [169849] accumulated_eval_time=6860.926365, accumulated_logging_time=8.290139, accumulated_submission_time=77343.403719, global_step=169849, preemption_count=0, score=77343.403719, test/accuracy=0.639100, test/loss=1.561057, test/num_examples=10000, total_duration=84221.576686, train/accuracy=0.833086, train/loss=0.632002, validation/accuracy=0.757600, validation/loss=0.957447, validation/num_examples=50000
I0202 11:36:47.590078 139702543816448 logging_writer.py:48] [169900] global_step=169900, grad_norm=6.300940036773682, loss=1.3667868375778198
I0202 11:37:31.336252 139702527031040 logging_writer.py:48] [170000] global_step=170000, grad_norm=5.4286627769470215, loss=1.7610169649124146
I0202 11:38:17.076514 139702543816448 logging_writer.py:48] [170100] global_step=170100, grad_norm=6.147312164306641, loss=1.4606871604919434
I0202 11:39:03.579863 139702527031040 logging_writer.py:48] [170200] global_step=170200, grad_norm=5.954315662384033, loss=1.4155104160308838
I0202 11:39:23.511351 139702543816448 logging_writer.py:48] [170245] global_step=170245, preemption_count=0, score=77520.005915
I0202 11:39:24.124559 139863983413056 checkpoints.py:490] Saving checkpoint at step: 170245
I0202 11:39:25.356730 139863983413056 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_3/checkpoint_170245
I0202 11:39:25.375413 139863983413056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_3/checkpoint_170245.
I0202 11:39:26.296084 139863983413056 submission_runner.py:583] Tuning trial 3/5
I0202 11:39:26.296330 139863983413056 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0202 11:39:26.305280 139863983413056 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011328124674037099, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.198220014572144, 'total_duration': 63.52872085571289, 'accumulated_submission_time': 36.198220014572144, 'accumulated_eval_time': 27.33039879798889, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (861, {'train/accuracy': 0.0126953125, 'train/loss': 6.42455530166626, 'validation/accuracy': 0.014179999940097332, 'validation/loss': 6.434906482696533, 'validation/num_examples': 50000, 'test/accuracy': 0.011200000531971455, 'test/loss': 6.476788520812988, 'test/num_examples': 10000, 'score': 456.5025894641876, 'total_duration': 520.8086996078491, 'accumulated_submission_time': 456.5025894641876, 'accumulated_eval_time': 64.24186062812805, 'accumulated_logging_time': 0.019777774810791016, 'global_step': 861, 'preemption_count': 0}), (1784, {'train/accuracy': 0.03832031041383743, 'train/loss': 5.833907127380371, 'validation/accuracy': 0.035019997507333755, 'validation/loss': 5.863661289215088, 'validation/num_examples': 50000, 'test/accuracy': 0.027400001883506775, 'test/loss': 5.9866838455200195, 'test/num_examples': 10000, 'score': 876.7815659046173, 'total_duration': 976.0506365299225, 'accumulated_submission_time': 876.7815659046173, 'accumulated_eval_time': 99.12552428245544, 'accumulated_logging_time': 0.050582170486450195, 'global_step': 1784, 'preemption_count': 0}), (2708, {'train/accuracy': 0.06650390475988388, 'train/loss': 5.404107093811035, 'validation/accuracy': 0.06127999722957611, 'validation/loss': 5.447628498077393, 'validation/num_examples': 50000, 'test/accuracy': 0.04920000210404396, 'test/loss': 5.628585338592529, 'test/num_examples': 10000, 'score': 1296.9359059333801, 'total_duration': 1427.5301036834717, 'accumulated_submission_time': 1296.9359059333801, 'accumulated_eval_time': 130.37237691879272, 'accumulated_logging_time': 0.08036112785339355, 'global_step': 2708, 'preemption_count': 0}), (3629, {'train/accuracy': 0.09648437052965164, 'train/loss': 5.060458660125732, 'validation/accuracy': 0.089819997549057, 'validation/loss': 5.1000800132751465, 'validation/num_examples': 50000, 'test/accuracy': 0.06850000470876694, 'test/loss': 5.34055233001709, 'test/num_examples': 10000, 'score': 1716.8910706043243, 'total_duration': 1885.1572682857513, 'accumulated_submission_time': 1716.8910706043243, 'accumulated_eval_time': 167.9686098098755, 'accumulated_logging_time': 0.10755348205566406, 'global_step': 3629, 'preemption_count': 0}), (4551, {'train/accuracy': 0.13283203542232513, 'train/loss': 4.688483715057373, 'validation/accuracy': 0.12459999322891235, 'validation/loss': 4.738650321960449, 'validation/num_examples': 50000, 'test/accuracy': 0.09270000457763672, 'test/loss': 5.04966402053833, 'test/num_examples': 10000, 'score': 2136.898129463196, 'total_duration': 2341.369452238083, 'accumulated_submission_time': 2136.898129463196, 'accumulated_eval_time': 204.10096502304077, 'accumulated_logging_time': 0.13211870193481445, 'global_step': 4551, 'preemption_count': 0}), (5471, {'train/accuracy': 0.17716796696186066, 'train/loss': 4.302088737487793, 'validation/accuracy': 0.16154000163078308, 'validation/loss': 4.381013870239258, 'validation/num_examples': 50000, 'test/accuracy': 0.12320000678300858, 'test/loss': 4.732566833496094, 'test/num_examples': 10000, 'score': 2557.2378079891205, 'total_duration': 2798.8110077381134, 'accumulated_submission_time': 2557.2378079891205, 'accumulated_eval_time': 241.13143801689148, 'accumulated_logging_time': 0.15525579452514648, 'global_step': 5471, 'preemption_count': 0}), (6394, {'train/accuracy': 0.22121092677116394, 'train/loss': 3.9720683097839355, 'validation/accuracy': 0.19923999905586243, 'validation/loss': 4.102482318878174, 'validation/num_examples': 50000, 'test/accuracy': 0.14920000731945038, 'test/loss': 4.503682613372803, 'test/num_examples': 10000, 'score': 2977.4658873081207, 'total_duration': 3258.1526939868927, 'accumulated_submission_time': 2977.4658873081207, 'accumulated_eval_time': 280.170841217041, 'accumulated_logging_time': 0.18082594871520996, 'global_step': 6394, 'preemption_count': 0}), (7317, {'train/accuracy': 0.2605859339237213, 'train/loss': 3.638662576675415, 'validation/accuracy': 0.24587999284267426, 'validation/loss': 3.7360968589782715, 'validation/num_examples': 50000, 'test/accuracy': 0.18530000746250153, 'test/loss': 4.179815769195557, 'test/num_examples': 10000, 'score': 3397.741530895233, 'total_duration': 3714.579946756363, 'accumulated_submission_time': 3397.741530895233, 'accumulated_eval_time': 316.2496666908264, 'accumulated_logging_time': 0.205230712890625, 'global_step': 7317, 'preemption_count': 0}), (8240, {'train/accuracy': 0.2956250011920929, 'train/loss': 3.4536590576171875, 'validation/accuracy': 0.27459999918937683, 'validation/loss': 3.5595672130584717, 'validation/num_examples': 50000, 'test/accuracy': 0.21070000529289246, 'test/loss': 4.018807411193848, 'test/num_examples': 10000, 'score': 3817.889068841934, 'total_duration': 4171.378915309906, 'accumulated_submission_time': 3817.889068841934, 'accumulated_eval_time': 352.8280653953552, 'accumulated_logging_time': 0.2301778793334961, 'global_step': 8240, 'preemption_count': 0}), (9165, {'train/accuracy': 0.3382031321525574, 'train/loss': 3.15322208404541, 'validation/accuracy': 0.3057200014591217, 'validation/loss': 3.3277082443237305, 'validation/num_examples': 50000, 'test/accuracy': 0.2387000173330307, 'test/loss': 3.8212127685546875, 'test/num_examples': 10000, 'score': 4238.194683074951, 'total_duration': 4629.507674217224, 'accumulated_submission_time': 4238.194683074951, 'accumulated_eval_time': 390.5762298107147, 'accumulated_logging_time': 0.2563011646270752, 'global_step': 9165, 'preemption_count': 0}), (10090, {'train/accuracy': 0.3600195348262787, 'train/loss': 3.029609203338623, 'validation/accuracy': 0.3360999822616577, 'validation/loss': 3.154364824295044, 'validation/num_examples': 50000, 'test/accuracy': 0.2547000050544739, 'test/loss': 3.6767055988311768, 'test/num_examples': 10000, 'score': 4658.40030002594, 'total_duration': 5085.103245258331, 'accumulated_submission_time': 4658.40030002594, 'accumulated_eval_time': 425.8897521495819, 'accumulated_logging_time': 0.2844047546386719, 'global_step': 10090, 'preemption_count': 0}), (11012, {'train/accuracy': 0.38132810592651367, 'train/loss': 2.868154764175415, 'validation/accuracy': 0.35593998432159424, 'validation/loss': 3.013925552368164, 'validation/num_examples': 50000, 'test/accuracy': 0.27800002694129944, 'test/loss': 3.559499740600586, 'test/num_examples': 10000, 'score': 5078.3580095767975, 'total_duration': 5537.1427166461945, 'accumulated_submission_time': 5078.3580095767975, 'accumulated_eval_time': 457.88829278945923, 'accumulated_logging_time': 0.3195030689239502, 'global_step': 11012, 'preemption_count': 0}), (11933, {'train/accuracy': 0.40974608063697815, 'train/loss': 2.723881721496582, 'validation/accuracy': 0.37139999866485596, 'validation/loss': 2.902348041534424, 'validation/num_examples': 50000, 'test/accuracy': 0.2905000150203705, 'test/loss': 3.4545247554779053, 'test/num_examples': 10000, 'score': 5498.060791969299, 'total_duration': 5994.897334814072, 'accumulated_submission_time': 5498.060791969299, 'accumulated_eval_time': 495.40143156051636, 'accumulated_logging_time': 0.8100032806396484, 'global_step': 11933, 'preemption_count': 0}), (12855, {'train/accuracy': 0.41914060711860657, 'train/loss': 2.710547924041748, 'validation/accuracy': 0.3916800022125244, 'validation/loss': 2.8580996990203857, 'validation/num_examples': 50000, 'test/accuracy': 0.30080002546310425, 'test/loss': 3.418243408203125, 'test/num_examples': 10000, 'score': 5918.129729747772, 'total_duration': 6453.089605808258, 'accumulated_submission_time': 5918.129729747772, 'accumulated_eval_time': 533.4520351886749, 'accumulated_logging_time': 0.8349728584289551, 'global_step': 12855, 'preemption_count': 0}), (13780, {'train/accuracy': 0.4387499988079071, 'train/loss': 2.571505308151245, 'validation/accuracy': 0.4053399860858917, 'validation/loss': 2.7366902828216553, 'validation/num_examples': 50000, 'test/accuracy': 0.31040000915527344, 'test/loss': 3.3097615242004395, 'test/num_examples': 10000, 'score': 6338.371341228485, 'total_duration': 6910.605922698975, 'accumulated_submission_time': 6338.371341228485, 'accumulated_eval_time': 570.6516087055206, 'accumulated_logging_time': 0.8618414402008057, 'global_step': 13780, 'preemption_count': 0}), (14706, {'train/accuracy': 0.4510742127895355, 'train/loss': 2.485666036605835, 'validation/accuracy': 0.41613999009132385, 'validation/loss': 2.670121192932129, 'validation/num_examples': 50000, 'test/accuracy': 0.3216000199317932, 'test/loss': 3.267530918121338, 'test/num_examples': 10000, 'score': 6758.6344385147095, 'total_duration': 7365.6261332035065, 'accumulated_submission_time': 6758.6344385147095, 'accumulated_eval_time': 605.3328831195831, 'accumulated_logging_time': 0.8873722553253174, 'global_step': 14706, 'preemption_count': 0}), (15629, {'train/accuracy': 0.47089841961860657, 'train/loss': 2.4013569355010986, 'validation/accuracy': 0.42739999294281006, 'validation/loss': 2.6038594245910645, 'validation/num_examples': 50000, 'test/accuracy': 0.33170002698898315, 'test/loss': 3.190614938735962, 'test/num_examples': 10000, 'score': 7178.670622348785, 'total_duration': 7821.458172559738, 'accumulated_submission_time': 7178.670622348785, 'accumulated_eval_time': 641.0547397136688, 'accumulated_logging_time': 0.9128148555755615, 'global_step': 15629, 'preemption_count': 0}), (16554, {'train/accuracy': 0.47603514790534973, 'train/loss': 2.324101448059082, 'validation/accuracy': 0.4444599747657776, 'validation/loss': 2.4943294525146484, 'validation/num_examples': 50000, 'test/accuracy': 0.34140002727508545, 'test/loss': 3.1120223999023438, 'test/num_examples': 10000, 'score': 7598.7170877456665, 'total_duration': 8279.92192864418, 'accumulated_submission_time': 7598.7170877456665, 'accumulated_eval_time': 679.3933956623077, 'accumulated_logging_time': 0.9427704811096191, 'global_step': 16554, 'preemption_count': 0}), (17481, {'train/accuracy': 0.48291015625, 'train/loss': 2.2987401485443115, 'validation/accuracy': 0.4461599886417389, 'validation/loss': 2.4863901138305664, 'validation/num_examples': 50000, 'test/accuracy': 0.34610000252723694, 'test/loss': 3.0988028049468994, 'test/num_examples': 10000, 'score': 8019.005577802658, 'total_duration': 8737.72558760643, 'accumulated_submission_time': 8019.005577802658, 'accumulated_eval_time': 716.8324489593506, 'accumulated_logging_time': 0.9697284698486328, 'global_step': 17481, 'preemption_count': 0}), (18405, {'train/accuracy': 0.5003905892372131, 'train/loss': 2.1953961849212646, 'validation/accuracy': 0.44909998774528503, 'validation/loss': 2.455754041671753, 'validation/num_examples': 50000, 'test/accuracy': 0.35190001130104065, 'test/loss': 3.06425142288208, 'test/num_examples': 10000, 'score': 8439.072633743286, 'total_duration': 9195.537045240402, 'accumulated_submission_time': 8439.072633743286, 'accumulated_eval_time': 754.4993402957916, 'accumulated_logging_time': 0.9993364810943604, 'global_step': 18405, 'preemption_count': 0}), (19326, {'train/accuracy': 0.49574217200279236, 'train/loss': 2.2166545391082764, 'validation/accuracy': 0.46535998582839966, 'validation/loss': 2.381878137588501, 'validation/num_examples': 50000, 'test/accuracy': 0.3622000217437744, 'test/loss': 2.999075174331665, 'test/num_examples': 10000, 'score': 8859.13781619072, 'total_duration': 9651.289949893951, 'accumulated_submission_time': 8859.13781619072, 'accumulated_eval_time': 790.1128647327423, 'accumulated_logging_time': 1.0250554084777832, 'global_step': 19326, 'preemption_count': 0}), (20251, {'train/accuracy': 0.49443358182907104, 'train/loss': 2.2306392192840576, 'validation/accuracy': 0.46230000257492065, 'validation/loss': 2.414182186126709, 'validation/num_examples': 50000, 'test/accuracy': 0.3603000044822693, 'test/loss': 3.0382838249206543, 'test/num_examples': 10000, 'score': 9279.222533941269, 'total_duration': 10107.794107198715, 'accumulated_submission_time': 9279.222533941269, 'accumulated_eval_time': 826.458952665329, 'accumulated_logging_time': 1.0508620738983154, 'global_step': 20251, 'preemption_count': 0}), (21178, {'train/accuracy': 0.5224609375, 'train/loss': 2.1027472019195557, 'validation/accuracy': 0.4746599793434143, 'validation/loss': 2.3242554664611816, 'validation/num_examples': 50000, 'test/accuracy': 0.37050002813339233, 'test/loss': 2.947197437286377, 'test/num_examples': 10000, 'score': 9699.259632349014, 'total_duration': 10566.632721424103, 'accumulated_submission_time': 9699.259632349014, 'accumulated_eval_time': 865.185097694397, 'accumulated_logging_time': 1.0772264003753662, 'global_step': 21178, 'preemption_count': 0}), (22101, {'train/accuracy': 0.5175585746765137, 'train/loss': 2.128702402114868, 'validation/accuracy': 0.4822399914264679, 'validation/loss': 2.302961826324463, 'validation/num_examples': 50000, 'test/accuracy': 0.3824000060558319, 'test/loss': 2.909083604812622, 'test/num_examples': 10000, 'score': 10119.803411722183, 'total_duration': 11023.986797571182, 'accumulated_submission_time': 10119.803411722183, 'accumulated_eval_time': 901.9166934490204, 'accumulated_logging_time': 1.106586217880249, 'global_step': 22101, 'preemption_count': 0}), (23023, {'train/accuracy': 0.5186718702316284, 'train/loss': 2.1031439304351807, 'validation/accuracy': 0.4840799868106842, 'validation/loss': 2.2893989086151123, 'validation/num_examples': 50000, 'test/accuracy': 0.3818000257015228, 'test/loss': 2.9118733406066895, 'test/num_examples': 10000, 'score': 10539.751255273819, 'total_duration': 11477.437187194824, 'accumulated_submission_time': 10539.751255273819, 'accumulated_eval_time': 935.33984375, 'accumulated_logging_time': 1.1371748447418213, 'global_step': 23023, 'preemption_count': 0}), (23947, {'train/accuracy': 0.5406249761581421, 'train/loss': 2.0112240314483643, 'validation/accuracy': 0.49449998140335083, 'validation/loss': 2.2325828075408936, 'validation/num_examples': 50000, 'test/accuracy': 0.38760000467300415, 'test/loss': 2.855060577392578, 'test/num_examples': 10000, 'score': 10959.968374729156, 'total_duration': 11934.81978225708, 'accumulated_submission_time': 10959.968374729156, 'accumulated_eval_time': 972.4291639328003, 'accumulated_logging_time': 1.1649210453033447, 'global_step': 23947, 'preemption_count': 0}), (24875, {'train/accuracy': 0.5497460961341858, 'train/loss': 1.9716825485229492, 'validation/accuracy': 0.5027799606323242, 'validation/loss': 2.185875654220581, 'validation/num_examples': 50000, 'test/accuracy': 0.39560002088546753, 'test/loss': 2.823624849319458, 'test/num_examples': 10000, 'score': 11380.271517753601, 'total_duration': 12390.1386282444, 'accumulated_submission_time': 11380.271517753601, 'accumulated_eval_time': 1007.3695442676544, 'accumulated_logging_time': 1.192338228225708, 'global_step': 24875, 'preemption_count': 0}), (25798, {'train/accuracy': 0.5396679639816284, 'train/loss': 2.003579616546631, 'validation/accuracy': 0.5015400052070618, 'validation/loss': 2.187553644180298, 'validation/num_examples': 50000, 'test/accuracy': 0.3968000113964081, 'test/loss': 2.8089516162872314, 'test/num_examples': 10000, 'score': 11800.526648044586, 'total_duration': 12848.661672592163, 'accumulated_submission_time': 11800.526648044586, 'accumulated_eval_time': 1045.554309129715, 'accumulated_logging_time': 1.227823257446289, 'global_step': 25798, 'preemption_count': 0}), (26720, {'train/accuracy': 0.5524218678474426, 'train/loss': 1.9628337621688843, 'validation/accuracy': 0.5076599717140198, 'validation/loss': 2.161849021911621, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.789376735687256, 'test/num_examples': 10000, 'score': 12220.824908733368, 'total_duration': 13305.236095428467, 'accumulated_submission_time': 12220.824908733368, 'accumulated_eval_time': 1081.7528929710388, 'accumulated_logging_time': 1.2576560974121094, 'global_step': 26720, 'preemption_count': 0}), (27644, {'train/accuracy': 0.5774218440055847, 'train/loss': 1.7861711978912354, 'validation/accuracy': 0.5199399590492249, 'validation/loss': 2.085148334503174, 'validation/num_examples': 50000, 'test/accuracy': 0.4028000235557556, 'test/loss': 2.7451486587524414, 'test/num_examples': 10000, 'score': 12641.06434583664, 'total_duration': 13758.587542057037, 'accumulated_submission_time': 12641.06434583664, 'accumulated_eval_time': 1114.786839723587, 'accumulated_logging_time': 1.2878186702728271, 'global_step': 27644, 'preemption_count': 0}), (28568, {'train/accuracy': 0.5564843416213989, 'train/loss': 1.902564525604248, 'validation/accuracy': 0.5218999981880188, 'validation/loss': 2.0633559226989746, 'validation/num_examples': 50000, 'test/accuracy': 0.41270002722740173, 'test/loss': 2.69579815864563, 'test/num_examples': 10000, 'score': 13061.353293895721, 'total_duration': 14213.63385272026, 'accumulated_submission_time': 13061.353293895721, 'accumulated_eval_time': 1149.4626867771149, 'accumulated_logging_time': 1.321131706237793, 'global_step': 28568, 'preemption_count': 0}), (29493, {'train/accuracy': 0.5679101347923279, 'train/loss': 1.855337381362915, 'validation/accuracy': 0.5210599899291992, 'validation/loss': 2.056286096572876, 'validation/num_examples': 50000, 'test/accuracy': 0.40730002522468567, 'test/loss': 2.698023557662964, 'test/num_examples': 10000, 'score': 13481.723677873611, 'total_duration': 14669.807126045227, 'accumulated_submission_time': 13481.723677873611, 'accumulated_eval_time': 1185.187756061554, 'accumulated_logging_time': 1.3507835865020752, 'global_step': 29493, 'preemption_count': 0}), (30418, {'train/accuracy': 0.5766796469688416, 'train/loss': 1.8442224264144897, 'validation/accuracy': 0.5261200070381165, 'validation/loss': 2.0872247219085693, 'validation/num_examples': 50000, 'test/accuracy': 0.40950003266334534, 'test/loss': 2.7296950817108154, 'test/num_examples': 10000, 'score': 13902.074913978577, 'total_duration': 15124.563690185547, 'accumulated_submission_time': 13902.074913978577, 'accumulated_eval_time': 1219.5118567943573, 'accumulated_logging_time': 1.3829002380371094, 'global_step': 30418, 'preemption_count': 0}), (31342, {'train/accuracy': 0.5602929592132568, 'train/loss': 1.9060845375061035, 'validation/accuracy': 0.5257999897003174, 'validation/loss': 2.082095146179199, 'validation/num_examples': 50000, 'test/accuracy': 0.4130000174045563, 'test/loss': 2.724041700363159, 'test/num_examples': 10000, 'score': 14322.042706489563, 'total_duration': 15584.364404439926, 'accumulated_submission_time': 14322.042706489563, 'accumulated_eval_time': 1259.265320301056, 'accumulated_logging_time': 1.4137554168701172, 'global_step': 31342, 'preemption_count': 0}), (32267, {'train/accuracy': 0.5718163847923279, 'train/loss': 1.8387211561203003, 'validation/accuracy': 0.5347399711608887, 'validation/loss': 2.0247344970703125, 'validation/num_examples': 50000, 'test/accuracy': 0.42010003328323364, 'test/loss': 2.6634790897369385, 'test/num_examples': 10000, 'score': 14742.001463413239, 'total_duration': 16036.710843086243, 'accumulated_submission_time': 14742.001463413239, 'accumulated_eval_time': 1291.5704834461212, 'accumulated_logging_time': 1.4477450847625732, 'global_step': 32267, 'preemption_count': 0}), (33192, {'train/accuracy': 0.5762890577316284, 'train/loss': 1.8025901317596436, 'validation/accuracy': 0.5324400067329407, 'validation/loss': 2.0144195556640625, 'validation/num_examples': 50000, 'test/accuracy': 0.42170003056526184, 'test/loss': 2.6554317474365234, 'test/num_examples': 10000, 'score': 15162.095947265625, 'total_duration': 16491.816769123077, 'accumulated_submission_time': 15162.095947265625, 'accumulated_eval_time': 1326.4956283569336, 'accumulated_logging_time': 1.4855966567993164, 'global_step': 33192, 'preemption_count': 0}), (34115, {'train/accuracy': 0.5796874761581421, 'train/loss': 1.8108338117599487, 'validation/accuracy': 0.533840000629425, 'validation/loss': 2.0201752185821533, 'validation/num_examples': 50000, 'test/accuracy': 0.42420002818107605, 'test/loss': 2.6502912044525146, 'test/num_examples': 10000, 'score': 15582.160798072815, 'total_duration': 16947.562863588333, 'accumulated_submission_time': 15582.160798072815, 'accumulated_eval_time': 1362.09840965271, 'accumulated_logging_time': 1.5149762630462646, 'global_step': 34115, 'preemption_count': 0}), (35038, {'train/accuracy': 0.5776171684265137, 'train/loss': 1.8064240217208862, 'validation/accuracy': 0.5378199815750122, 'validation/loss': 1.9878385066986084, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.645028591156006, 'test/num_examples': 10000, 'score': 16002.27912735939, 'total_duration': 17404.02609181404, 'accumulated_submission_time': 16002.27912735939, 'accumulated_eval_time': 1398.3604464530945, 'accumulated_logging_time': 1.5435802936553955, 'global_step': 35038, 'preemption_count': 0}), (35963, {'train/accuracy': 0.5839062333106995, 'train/loss': 1.7839018106460571, 'validation/accuracy': 0.5367000102996826, 'validation/loss': 1.999006986618042, 'validation/num_examples': 50000, 'test/accuracy': 0.41920003294944763, 'test/loss': 2.6471946239471436, 'test/num_examples': 10000, 'score': 16422.558165311813, 'total_duration': 17863.082113027573, 'accumulated_submission_time': 16422.558165311813, 'accumulated_eval_time': 1437.056425333023, 'accumulated_logging_time': 1.5760712623596191, 'global_step': 35963, 'preemption_count': 0}), (36889, {'train/accuracy': 0.6082812547683716, 'train/loss': 1.6362735033035278, 'validation/accuracy': 0.5448200106620789, 'validation/loss': 1.9368997812271118, 'validation/num_examples': 50000, 'test/accuracy': 0.42970001697540283, 'test/loss': 2.5912978649139404, 'test/num_examples': 10000, 'score': 16842.82310938835, 'total_duration': 18320.652250528336, 'accumulated_submission_time': 16842.82310938835, 'accumulated_eval_time': 1474.281730890274, 'accumulated_logging_time': 1.6072852611541748, 'global_step': 36889, 'preemption_count': 0}), (37814, {'train/accuracy': 0.5752734541893005, 'train/loss': 1.8207134008407593, 'validation/accuracy': 0.5349000096321106, 'validation/loss': 2.0128426551818848, 'validation/num_examples': 50000, 'test/accuracy': 0.42080003023147583, 'test/loss': 2.656367540359497, 'test/num_examples': 10000, 'score': 17262.928169488907, 'total_duration': 18778.617341041565, 'accumulated_submission_time': 17262.928169488907, 'accumulated_eval_time': 1512.0637485980988, 'accumulated_logging_time': 1.636359453201294, 'global_step': 37814, 'preemption_count': 0}), (38739, {'train/accuracy': 0.5896679759025574, 'train/loss': 1.7389613389968872, 'validation/accuracy': 0.5483999848365784, 'validation/loss': 1.9300124645233154, 'validation/num_examples': 50000, 'test/accuracy': 0.43140003085136414, 'test/loss': 2.607285737991333, 'test/num_examples': 10000, 'score': 17683.129102230072, 'total_duration': 19235.227719545364, 'accumulated_submission_time': 17683.129102230072, 'accumulated_eval_time': 1548.39102768898, 'accumulated_logging_time': 1.6686515808105469, 'global_step': 38739, 'preemption_count': 0}), (39665, {'train/accuracy': 0.6051952838897705, 'train/loss': 1.6787258386611938, 'validation/accuracy': 0.5502399802207947, 'validation/loss': 1.9336005449295044, 'validation/num_examples': 50000, 'test/accuracy': 0.4336000084877014, 'test/loss': 2.5970046520233154, 'test/num_examples': 10000, 'score': 18103.296102762222, 'total_duration': 19692.71918320656, 'accumulated_submission_time': 18103.296102762222, 'accumulated_eval_time': 1585.6373362541199, 'accumulated_logging_time': 1.6978001594543457, 'global_step': 39665, 'preemption_count': 0}), (40591, {'train/accuracy': 0.5888671875, 'train/loss': 1.7536296844482422, 'validation/accuracy': 0.5473399758338928, 'validation/loss': 1.943955898284912, 'validation/num_examples': 50000, 'test/accuracy': 0.43070003390312195, 'test/loss': 2.6018872261047363, 'test/num_examples': 10000, 'score': 18523.215060949326, 'total_duration': 20148.956298351288, 'accumulated_submission_time': 18523.215060949326, 'accumulated_eval_time': 1621.870540380478, 'accumulated_logging_time': 1.7322437763214111, 'global_step': 40591, 'preemption_count': 0}), (41515, {'train/accuracy': 0.5938476324081421, 'train/loss': 1.7254453897476196, 'validation/accuracy': 0.5546000003814697, 'validation/loss': 1.9125330448150635, 'validation/num_examples': 50000, 'test/accuracy': 0.4418000280857086, 'test/loss': 2.5483005046844482, 'test/num_examples': 10000, 'score': 18943.480389356613, 'total_duration': 20606.473484039307, 'accumulated_submission_time': 18943.480389356613, 'accumulated_eval_time': 1659.0322132110596, 'accumulated_logging_time': 1.7727165222167969, 'global_step': 41515, 'preemption_count': 0}), (42440, {'train/accuracy': 0.5986718535423279, 'train/loss': 1.7187060117721558, 'validation/accuracy': 0.5518999695777893, 'validation/loss': 1.947245478630066, 'validation/num_examples': 50000, 'test/accuracy': 0.4280000329017639, 'test/loss': 2.6177022457122803, 'test/num_examples': 10000, 'score': 19363.407828569412, 'total_duration': 21062.151589155197, 'accumulated_submission_time': 19363.407828569412, 'accumulated_eval_time': 1694.7046456336975, 'accumulated_logging_time': 1.80218505859375, 'global_step': 42440, 'preemption_count': 0}), (43366, {'train/accuracy': 0.6058202981948853, 'train/loss': 1.6716299057006836, 'validation/accuracy': 0.5638200044631958, 'validation/loss': 1.8678628206253052, 'validation/num_examples': 50000, 'test/accuracy': 0.44520002603530884, 'test/loss': 2.5290257930755615, 'test/num_examples': 10000, 'score': 19783.71187520027, 'total_duration': 21519.535794973373, 'accumulated_submission_time': 19783.71187520027, 'accumulated_eval_time': 1731.7015480995178, 'accumulated_logging_time': 1.8368933200836182, 'global_step': 43366, 'preemption_count': 0}), (44292, {'train/accuracy': 0.6031640768051147, 'train/loss': 1.6963953971862793, 'validation/accuracy': 0.562279999256134, 'validation/loss': 1.8898544311523438, 'validation/num_examples': 50000, 'test/accuracy': 0.44270002841949463, 'test/loss': 2.5435431003570557, 'test/num_examples': 10000, 'score': 20204.07502055168, 'total_duration': 21974.32190656662, 'accumulated_submission_time': 20204.07502055168, 'accumulated_eval_time': 1766.042251586914, 'accumulated_logging_time': 1.8706142902374268, 'global_step': 44292, 'preemption_count': 0}), (45216, {'train/accuracy': 0.6105663776397705, 'train/loss': 1.6492289304733276, 'validation/accuracy': 0.564579963684082, 'validation/loss': 1.8600382804870605, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.5134053230285645, 'test/num_examples': 10000, 'score': 20624.01596546173, 'total_duration': 22431.28592157364, 'accumulated_submission_time': 20624.01596546173, 'accumulated_eval_time': 1802.985370874405, 'accumulated_logging_time': 1.9021704196929932, 'global_step': 45216, 'preemption_count': 0}), (46141, {'train/accuracy': 0.6305859088897705, 'train/loss': 1.5546183586120605, 'validation/accuracy': 0.5636999607086182, 'validation/loss': 1.8670766353607178, 'validation/num_examples': 50000, 'test/accuracy': 0.4474000334739685, 'test/loss': 2.5046355724334717, 'test/num_examples': 10000, 'score': 21044.385004997253, 'total_duration': 22890.125202655792, 'accumulated_submission_time': 21044.385004997253, 'accumulated_eval_time': 1841.3757576942444, 'accumulated_logging_time': 1.9337167739868164, 'global_step': 46141, 'preemption_count': 0}), (47066, {'train/accuracy': 0.60595703125, 'train/loss': 1.6388249397277832, 'validation/accuracy': 0.5649600028991699, 'validation/loss': 1.8310381174087524, 'validation/num_examples': 50000, 'test/accuracy': 0.4488000273704529, 'test/loss': 2.477980613708496, 'test/num_examples': 10000, 'score': 21464.45929455757, 'total_duration': 23345.80527472496, 'accumulated_submission_time': 21464.45929455757, 'accumulated_eval_time': 1876.8982713222504, 'accumulated_logging_time': 1.9686899185180664, 'global_step': 47066, 'preemption_count': 0}), (47990, {'train/accuracy': 0.6087695360183716, 'train/loss': 1.669023036956787, 'validation/accuracy': 0.5663599967956543, 'validation/loss': 1.8735897541046143, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.5234055519104004, 'test/num_examples': 10000, 'score': 21884.46242260933, 'total_duration': 23801.106241226196, 'accumulated_submission_time': 21884.46242260933, 'accumulated_eval_time': 1912.115308046341, 'accumulated_logging_time': 2.0009567737579346, 'global_step': 47990, 'preemption_count': 0}), (48913, {'train/accuracy': 0.6250976324081421, 'train/loss': 1.5893003940582275, 'validation/accuracy': 0.5663999915122986, 'validation/loss': 1.850501537322998, 'validation/num_examples': 50000, 'test/accuracy': 0.4472000300884247, 'test/loss': 2.5053343772888184, 'test/num_examples': 10000, 'score': 22304.51851439476, 'total_duration': 24255.80009675026, 'accumulated_submission_time': 22304.51851439476, 'accumulated_eval_time': 1946.6730072498322, 'accumulated_logging_time': 2.0325334072113037, 'global_step': 48913, 'preemption_count': 0}), (49839, {'train/accuracy': 0.6113671660423279, 'train/loss': 1.6454654932022095, 'validation/accuracy': 0.5694599747657776, 'validation/loss': 1.8334907293319702, 'validation/num_examples': 50000, 'test/accuracy': 0.45280003547668457, 'test/loss': 2.4803237915039062, 'test/num_examples': 10000, 'score': 22724.775852918625, 'total_duration': 24715.59235072136, 'accumulated_submission_time': 22724.775852918625, 'accumulated_eval_time': 1986.1257948875427, 'accumulated_logging_time': 2.0658884048461914, 'global_step': 49839, 'preemption_count': 0}), (50765, {'train/accuracy': 0.6122851371765137, 'train/loss': 1.6380223035812378, 'validation/accuracy': 0.5708400011062622, 'validation/loss': 1.8340604305267334, 'validation/num_examples': 50000, 'test/accuracy': 0.4515000283718109, 'test/loss': 2.4958276748657227, 'test/num_examples': 10000, 'score': 23144.970401525497, 'total_duration': 25170.08160185814, 'accumulated_submission_time': 23144.970401525497, 'accumulated_eval_time': 2020.3395702838898, 'accumulated_logging_time': 2.098045825958252, 'global_step': 50765, 'preemption_count': 0}), (51688, {'train/accuracy': 0.6258788704872131, 'train/loss': 1.5507782697677612, 'validation/accuracy': 0.5779600143432617, 'validation/loss': 1.7888613939285278, 'validation/num_examples': 50000, 'test/accuracy': 0.4626000225543976, 'test/loss': 2.4470291137695312, 'test/num_examples': 10000, 'score': 23564.94721007347, 'total_duration': 25626.840389966965, 'accumulated_submission_time': 23564.94721007347, 'accumulated_eval_time': 2057.0334384441376, 'accumulated_logging_time': 2.137653350830078, 'global_step': 51688, 'preemption_count': 0}), (52613, {'train/accuracy': 0.6236132383346558, 'train/loss': 1.5819681882858276, 'validation/accuracy': 0.5786799788475037, 'validation/loss': 1.7916535139083862, 'validation/num_examples': 50000, 'test/accuracy': 0.46310001611709595, 'test/loss': 2.449575424194336, 'test/num_examples': 10000, 'score': 23985.300182819366, 'total_duration': 26085.101460695267, 'accumulated_submission_time': 23985.300182819366, 'accumulated_eval_time': 2094.8600878715515, 'accumulated_logging_time': 2.1691770553588867, 'global_step': 52613, 'preemption_count': 0}), (53538, {'train/accuracy': 0.6176171898841858, 'train/loss': 1.5891666412353516, 'validation/accuracy': 0.5787599682807922, 'validation/loss': 1.7716038227081299, 'validation/num_examples': 50000, 'test/accuracy': 0.46000000834465027, 'test/loss': 2.4227559566497803, 'test/num_examples': 10000, 'score': 24405.514179944992, 'total_duration': 26537.762898921967, 'accumulated_submission_time': 24405.514179944992, 'accumulated_eval_time': 2127.224277496338, 'accumulated_logging_time': 2.20285701751709, 'global_step': 53538, 'preemption_count': 0}), (54464, {'train/accuracy': 0.6196874976158142, 'train/loss': 1.6350294351577759, 'validation/accuracy': 0.5768199563026428, 'validation/loss': 1.8407946825027466, 'validation/num_examples': 50000, 'test/accuracy': 0.45350003242492676, 'test/loss': 2.493711471557617, 'test/num_examples': 10000, 'score': 24825.61731696129, 'total_duration': 26994.1825401783, 'accumulated_submission_time': 24825.61731696129, 'accumulated_eval_time': 2163.457891225815, 'accumulated_logging_time': 2.23595929145813, 'global_step': 54464, 'preemption_count': 0}), (55384, {'train/accuracy': 0.6449999809265137, 'train/loss': 1.5044242143630981, 'validation/accuracy': 0.5765599608421326, 'validation/loss': 1.8060835599899292, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.4601993560791016, 'test/num_examples': 10000, 'score': 25245.340619325638, 'total_duration': 27449.90674352646, 'accumulated_submission_time': 25245.340619325638, 'accumulated_eval_time': 2198.990670442581, 'accumulated_logging_time': 2.656230926513672, 'global_step': 55384, 'preemption_count': 0}), (56309, {'train/accuracy': 0.621777355670929, 'train/loss': 1.5656689405441284, 'validation/accuracy': 0.5817599892616272, 'validation/loss': 1.7517997026443481, 'validation/num_examples': 50000, 'test/accuracy': 0.4602000117301941, 'test/loss': 2.4136600494384766, 'test/num_examples': 10000, 'score': 25665.65577197075, 'total_duration': 27911.216849565506, 'accumulated_submission_time': 25665.65577197075, 'accumulated_eval_time': 2239.8955862522125, 'accumulated_logging_time': 2.6976802349090576, 'global_step': 56309, 'preemption_count': 0}), (57234, {'train/accuracy': 0.6290820240974426, 'train/loss': 1.5716264247894287, 'validation/accuracy': 0.5814799666404724, 'validation/loss': 1.7926048040390015, 'validation/num_examples': 50000, 'test/accuracy': 0.4628000259399414, 'test/loss': 2.429422378540039, 'test/num_examples': 10000, 'score': 26086.074779748917, 'total_duration': 28369.223430871964, 'accumulated_submission_time': 26086.074779748917, 'accumulated_eval_time': 2277.402089357376, 'accumulated_logging_time': 2.7310519218444824, 'global_step': 57234, 'preemption_count': 0}), (58159, {'train/accuracy': 0.6390234231948853, 'train/loss': 1.5097546577453613, 'validation/accuracy': 0.5824999809265137, 'validation/loss': 1.7665115594863892, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.406198501586914, 'test/num_examples': 10000, 'score': 26506.14146876335, 'total_duration': 28826.498693466187, 'accumulated_submission_time': 26506.14146876335, 'accumulated_eval_time': 2314.5219326019287, 'accumulated_logging_time': 2.77089786529541, 'global_step': 58159, 'preemption_count': 0}), (59083, {'train/accuracy': 0.6219140291213989, 'train/loss': 1.5699963569641113, 'validation/accuracy': 0.5807999968528748, 'validation/loss': 1.7605829238891602, 'validation/num_examples': 50000, 'test/accuracy': 0.46970000863075256, 'test/loss': 2.399991273880005, 'test/num_examples': 10000, 'score': 26926.27724289894, 'total_duration': 29280.27455854416, 'accumulated_submission_time': 26926.27724289894, 'accumulated_eval_time': 2348.0781197547913, 'accumulated_logging_time': 2.8062548637390137, 'global_step': 59083, 'preemption_count': 0}), (60007, {'train/accuracy': 0.6282616853713989, 'train/loss': 1.5407322645187378, 'validation/accuracy': 0.5867399573326111, 'validation/loss': 1.7369784116744995, 'validation/num_examples': 50000, 'test/accuracy': 0.4677000343799591, 'test/loss': 2.3942172527313232, 'test/num_examples': 10000, 'score': 27346.266345262527, 'total_duration': 29736.22474217415, 'accumulated_submission_time': 27346.266345262527, 'accumulated_eval_time': 2383.9525430202484, 'accumulated_logging_time': 2.843522310256958, 'global_step': 60007, 'preemption_count': 0}), (60933, {'train/accuracy': 0.6407226324081421, 'train/loss': 1.5198386907577515, 'validation/accuracy': 0.5875200033187866, 'validation/loss': 1.75584077835083, 'validation/num_examples': 50000, 'test/accuracy': 0.4727000296115875, 'test/loss': 2.403726816177368, 'test/num_examples': 10000, 'score': 27766.477121591568, 'total_duration': 30195.49317908287, 'accumulated_submission_time': 27766.477121591568, 'accumulated_eval_time': 2422.9228308200836, 'accumulated_logging_time': 2.881842613220215, 'global_step': 60933, 'preemption_count': 0}), (61858, {'train/accuracy': 0.6363281011581421, 'train/loss': 1.5116547346115112, 'validation/accuracy': 0.5927199721336365, 'validation/loss': 1.7103793621063232, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.3759047985076904, 'test/num_examples': 10000, 'score': 28186.816585302353, 'total_duration': 30650.73554301262, 'accumulated_submission_time': 28186.816585302353, 'accumulated_eval_time': 2457.7355239391327, 'accumulated_logging_time': 2.923898696899414, 'global_step': 61858, 'preemption_count': 0}), (62782, {'train/accuracy': 0.6325976252555847, 'train/loss': 1.5511209964752197, 'validation/accuracy': 0.5876799821853638, 'validation/loss': 1.7578150033950806, 'validation/num_examples': 50000, 'test/accuracy': 0.4659000337123871, 'test/loss': 2.4069180488586426, 'test/num_examples': 10000, 'score': 28607.062667131424, 'total_duration': 31105.45560526848, 'accumulated_submission_time': 28607.062667131424, 'accumulated_eval_time': 2492.127090215683, 'accumulated_logging_time': 2.957786798477173, 'global_step': 62782, 'preemption_count': 0}), (63709, {'train/accuracy': 0.6412500143051147, 'train/loss': 1.4811878204345703, 'validation/accuracy': 0.594819962978363, 'validation/loss': 1.6979410648345947, 'validation/num_examples': 50000, 'test/accuracy': 0.47520002722740173, 'test/loss': 2.3501551151275635, 'test/num_examples': 10000, 'score': 29027.318502426147, 'total_duration': 31564.00478863716, 'accumulated_submission_time': 29027.318502426147, 'accumulated_eval_time': 2530.325961828232, 'accumulated_logging_time': 3.003383159637451, 'global_step': 63709, 'preemption_count': 0}), (64633, {'train/accuracy': 0.6591210961341858, 'train/loss': 1.411446452140808, 'validation/accuracy': 0.5887599587440491, 'validation/loss': 1.7354134321212769, 'validation/num_examples': 50000, 'test/accuracy': 0.4707000255584717, 'test/loss': 2.3777599334716797, 'test/num_examples': 10000, 'score': 29447.261779785156, 'total_duration': 32021.783009290695, 'accumulated_submission_time': 29447.261779785156, 'accumulated_eval_time': 2568.0701701641083, 'accumulated_logging_time': 3.045332431793213, 'global_step': 64633, 'preemption_count': 0}), (65560, {'train/accuracy': 0.6402148008346558, 'train/loss': 1.5128191709518433, 'validation/accuracy': 0.5958399772644043, 'validation/loss': 1.723629355430603, 'validation/num_examples': 50000, 'test/accuracy': 0.47600001096725464, 'test/loss': 2.3619251251220703, 'test/num_examples': 10000, 'score': 29867.51460146904, 'total_duration': 32478.842272996902, 'accumulated_submission_time': 29867.51460146904, 'accumulated_eval_time': 2604.7873711586, 'accumulated_logging_time': 3.084872245788574, 'global_step': 65560, 'preemption_count': 0}), (66486, {'train/accuracy': 0.6462304592132568, 'train/loss': 1.4637900590896606, 'validation/accuracy': 0.6012399792671204, 'validation/loss': 1.669540286064148, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.334697723388672, 'test/num_examples': 10000, 'score': 30287.78778719902, 'total_duration': 32934.90845179558, 'accumulated_submission_time': 30287.78778719902, 'accumulated_eval_time': 2640.4960482120514, 'accumulated_logging_time': 3.11910343170166, 'global_step': 66486, 'preemption_count': 0}), (67413, {'train/accuracy': 0.6551952958106995, 'train/loss': 1.444200873374939, 'validation/accuracy': 0.6001799702644348, 'validation/loss': 1.7015769481658936, 'validation/num_examples': 50000, 'test/accuracy': 0.4799000322818756, 'test/loss': 2.3611066341400146, 'test/num_examples': 10000, 'score': 30708.076306819916, 'total_duration': 33391.64254951477, 'accumulated_submission_time': 30708.076306819916, 'accumulated_eval_time': 2676.85613656044, 'accumulated_logging_time': 3.1553802490234375, 'global_step': 67413, 'preemption_count': 0}), (68335, {'train/accuracy': 0.6418554782867432, 'train/loss': 1.4940626621246338, 'validation/accuracy': 0.6005799770355225, 'validation/loss': 1.6846686601638794, 'validation/num_examples': 50000, 'test/accuracy': 0.48170003294944763, 'test/loss': 2.3237926959991455, 'test/num_examples': 10000, 'score': 31128.145755052567, 'total_duration': 33848.646995306015, 'accumulated_submission_time': 31128.145755052567, 'accumulated_eval_time': 2713.703197956085, 'accumulated_logging_time': 3.194664239883423, 'global_step': 68335, 'preemption_count': 0}), (69259, {'train/accuracy': 0.6425195336341858, 'train/loss': 1.519197940826416, 'validation/accuracy': 0.5944199562072754, 'validation/loss': 1.7298539876937866, 'validation/num_examples': 50000, 'test/accuracy': 0.47780001163482666, 'test/loss': 2.381270170211792, 'test/num_examples': 10000, 'score': 31548.4422082901, 'total_duration': 34305.10170960426, 'accumulated_submission_time': 31548.4422082901, 'accumulated_eval_time': 2749.776581287384, 'accumulated_logging_time': 3.231003999710083, 'global_step': 69259, 'preemption_count': 0}), (70184, {'train/accuracy': 0.6552343368530273, 'train/loss': 1.4339993000030518, 'validation/accuracy': 0.6019399762153625, 'validation/loss': 1.6860952377319336, 'validation/num_examples': 50000, 'test/accuracy': 0.48260003328323364, 'test/loss': 2.3297083377838135, 'test/num_examples': 10000, 'score': 31968.580335378647, 'total_duration': 34758.855362176895, 'accumulated_submission_time': 31968.580335378647, 'accumulated_eval_time': 2783.305285215378, 'accumulated_logging_time': 3.269001007080078, 'global_step': 70184, 'preemption_count': 0}), (71111, {'train/accuracy': 0.6490820050239563, 'train/loss': 1.4559168815612793, 'validation/accuracy': 0.6016600131988525, 'validation/loss': 1.6674160957336426, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.3340394496917725, 'test/num_examples': 10000, 'score': 32388.90991282463, 'total_duration': 35214.268662929535, 'accumulated_submission_time': 32388.90991282463, 'accumulated_eval_time': 2818.3009536266327, 'accumulated_logging_time': 3.307988405227661, 'global_step': 71111, 'preemption_count': 0}), (72037, {'train/accuracy': 0.6522070169448853, 'train/loss': 1.439338207244873, 'validation/accuracy': 0.6070799827575684, 'validation/loss': 1.6486047506332397, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.3152353763580322, 'test/num_examples': 10000, 'score': 32809.21870470047, 'total_duration': 35669.50822305679, 'accumulated_submission_time': 32809.21870470047, 'accumulated_eval_time': 2853.14493060112, 'accumulated_logging_time': 3.345659017562866, 'global_step': 72037, 'preemption_count': 0}), (72961, {'train/accuracy': 0.6541991829872131, 'train/loss': 1.4503023624420166, 'validation/accuracy': 0.6034799814224243, 'validation/loss': 1.6737196445465088, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.3369715213775635, 'test/num_examples': 10000, 'score': 33229.53257513046, 'total_duration': 36127.273602962494, 'accumulated_submission_time': 33229.53257513046, 'accumulated_eval_time': 2890.5099818706512, 'accumulated_logging_time': 3.3830513954162598, 'global_step': 72961, 'preemption_count': 0}), (73887, {'train/accuracy': 0.6709960699081421, 'train/loss': 1.3740272521972656, 'validation/accuracy': 0.6000399589538574, 'validation/loss': 1.691603183746338, 'validation/num_examples': 50000, 'test/accuracy': 0.4837000370025635, 'test/loss': 2.336214303970337, 'test/num_examples': 10000, 'score': 33649.60353899002, 'total_duration': 36582.99809360504, 'accumulated_submission_time': 33649.60353899002, 'accumulated_eval_time': 2926.0756623744965, 'accumulated_logging_time': 3.4222118854522705, 'global_step': 73887, 'preemption_count': 0}), (74811, {'train/accuracy': 0.6478906273841858, 'train/loss': 1.4585990905761719, 'validation/accuracy': 0.6050599813461304, 'validation/loss': 1.6543669700622559, 'validation/num_examples': 50000, 'test/accuracy': 0.48850002884864807, 'test/loss': 2.2894341945648193, 'test/num_examples': 10000, 'score': 34069.536211013794, 'total_duration': 37037.44539427757, 'accumulated_submission_time': 34069.536211013794, 'accumulated_eval_time': 2960.50363445282, 'accumulated_logging_time': 3.460890054702759, 'global_step': 74811, 'preemption_count': 0}), (75734, {'train/accuracy': 0.6602148413658142, 'train/loss': 1.4000751972198486, 'validation/accuracy': 0.6098600029945374, 'validation/loss': 1.6153631210327148, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.2635269165039062, 'test/num_examples': 10000, 'score': 34489.581547021866, 'total_duration': 37494.34624528885, 'accumulated_submission_time': 34489.581547021866, 'accumulated_eval_time': 2997.275089740753, 'accumulated_logging_time': 3.4971187114715576, 'global_step': 75734, 'preemption_count': 0}), (76659, {'train/accuracy': 0.6640429496765137, 'train/loss': 1.389413833618164, 'validation/accuracy': 0.6104399561882019, 'validation/loss': 1.645629644393921, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.289534568786621, 'test/num_examples': 10000, 'score': 34909.62964272499, 'total_duration': 37952.71143436432, 'accumulated_submission_time': 34909.62964272499, 'accumulated_eval_time': 3035.50843667984, 'accumulated_logging_time': 3.5318963527679443, 'global_step': 76659, 'preemption_count': 0}), (77586, {'train/accuracy': 0.6577734351158142, 'train/loss': 1.4027554988861084, 'validation/accuracy': 0.6133599877357483, 'validation/loss': 1.6025428771972656, 'validation/num_examples': 50000, 'test/accuracy': 0.4926000237464905, 'test/loss': 2.264949083328247, 'test/num_examples': 10000, 'score': 35329.89449548721, 'total_duration': 38409.01891493797, 'accumulated_submission_time': 35329.89449548721, 'accumulated_eval_time': 3071.4626858234406, 'accumulated_logging_time': 3.5703811645507812, 'global_step': 77586, 'preemption_count': 0}), (78511, {'train/accuracy': 0.6576171517372131, 'train/loss': 1.4034545421600342, 'validation/accuracy': 0.6142399907112122, 'validation/loss': 1.60440194606781, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.2740235328674316, 'test/num_examples': 10000, 'score': 35749.847000837326, 'total_duration': 38864.033963918686, 'accumulated_submission_time': 35749.847000837326, 'accumulated_eval_time': 3106.437283039093, 'accumulated_logging_time': 3.6099631786346436, 'global_step': 78511, 'preemption_count': 0}), (79436, {'train/accuracy': 0.6655468344688416, 'train/loss': 1.3878076076507568, 'validation/accuracy': 0.6120399832725525, 'validation/loss': 1.6240185499191284, 'validation/num_examples': 50000, 'test/accuracy': 0.4887000322341919, 'test/loss': 2.2839529514312744, 'test/num_examples': 10000, 'score': 36170.04280281067, 'total_duration': 39317.1111471653, 'accumulated_submission_time': 36170.04280281067, 'accumulated_eval_time': 3139.2335624694824, 'accumulated_logging_time': 3.6466240882873535, 'global_step': 79436, 'preemption_count': 0}), (80360, {'train/accuracy': 0.6602929830551147, 'train/loss': 1.4149186611175537, 'validation/accuracy': 0.6128799915313721, 'validation/loss': 1.6237813234329224, 'validation/num_examples': 50000, 'test/accuracy': 0.49820002913475037, 'test/loss': 2.274289131164551, 'test/num_examples': 10000, 'score': 36590.088024139404, 'total_duration': 39773.61768245697, 'accumulated_submission_time': 36590.088024139404, 'accumulated_eval_time': 3175.6078023910522, 'accumulated_logging_time': 3.685581922531128, 'global_step': 80360, 'preemption_count': 0}), (81284, {'train/accuracy': 0.6623827815055847, 'train/loss': 1.4213883876800537, 'validation/accuracy': 0.6172999739646912, 'validation/loss': 1.631671667098999, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.2799715995788574, 'test/num_examples': 10000, 'score': 37010.074617385864, 'total_duration': 40230.83346366882, 'accumulated_submission_time': 37010.074617385864, 'accumulated_eval_time': 3212.7509384155273, 'accumulated_logging_time': 3.7231364250183105, 'global_step': 81284, 'preemption_count': 0}), (82208, {'train/accuracy': 0.6692187190055847, 'train/loss': 1.3704071044921875, 'validation/accuracy': 0.6172400116920471, 'validation/loss': 1.5985249280929565, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.2693800926208496, 'test/num_examples': 10000, 'score': 37430.19033193588, 'total_duration': 40686.96852493286, 'accumulated_submission_time': 37430.19033193588, 'accumulated_eval_time': 3248.6838982105255, 'accumulated_logging_time': 3.760983467102051, 'global_step': 82208, 'preemption_count': 0}), (83133, {'train/accuracy': 0.6871874928474426, 'train/loss': 1.3113399744033813, 'validation/accuracy': 0.6131600141525269, 'validation/loss': 1.6342535018920898, 'validation/num_examples': 50000, 'test/accuracy': 0.4952000379562378, 'test/loss': 2.2838222980499268, 'test/num_examples': 10000, 'score': 37850.235020160675, 'total_duration': 41144.50082588196, 'accumulated_submission_time': 37850.235020160675, 'accumulated_eval_time': 3286.08683013916, 'accumulated_logging_time': 3.796276569366455, 'global_step': 83133, 'preemption_count': 0}), (84057, {'train/accuracy': 0.6665624976158142, 'train/loss': 1.3889672756195068, 'validation/accuracy': 0.6204999685287476, 'validation/loss': 1.5967681407928467, 'validation/num_examples': 50000, 'test/accuracy': 0.5052000284194946, 'test/loss': 2.2316551208496094, 'test/num_examples': 10000, 'score': 38270.56211447716, 'total_duration': 41600.98387527466, 'accumulated_submission_time': 38270.56211447716, 'accumulated_eval_time': 3322.1507127285004, 'accumulated_logging_time': 3.8398702144622803, 'global_step': 84057, 'preemption_count': 0}), (84982, {'train/accuracy': 0.6727148294448853, 'train/loss': 1.3522733449935913, 'validation/accuracy': 0.6233800053596497, 'validation/loss': 1.5781606435775757, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.2197320461273193, 'test/num_examples': 10000, 'score': 38690.585122823715, 'total_duration': 42056.864844083786, 'accumulated_submission_time': 38690.585122823715, 'accumulated_eval_time': 3357.9229278564453, 'accumulated_logging_time': 3.876704692840576, 'global_step': 84982, 'preemption_count': 0}), (85907, {'train/accuracy': 0.6804882884025574, 'train/loss': 1.3314138650894165, 'validation/accuracy': 0.6174600124359131, 'validation/loss': 1.6178598403930664, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.280888557434082, 'test/num_examples': 10000, 'score': 39110.57365632057, 'total_duration': 42512.99793553352, 'accumulated_submission_time': 39110.57365632057, 'accumulated_eval_time': 3393.9766433238983, 'accumulated_logging_time': 3.9189043045043945, 'global_step': 85907, 'preemption_count': 0}), (86832, {'train/accuracy': 0.669238269329071, 'train/loss': 1.371837854385376, 'validation/accuracy': 0.6244199872016907, 'validation/loss': 1.578890323638916, 'validation/num_examples': 50000, 'test/accuracy': 0.5002000331878662, 'test/loss': 2.2396240234375, 'test/num_examples': 10000, 'score': 39530.89902305603, 'total_duration': 42969.77918601036, 'accumulated_submission_time': 39530.89902305603, 'accumulated_eval_time': 3430.3413774967194, 'accumulated_logging_time': 3.9617397785186768, 'global_step': 86832, 'preemption_count': 0}), (87751, {'train/accuracy': 0.674023449420929, 'train/loss': 1.3504189252853394, 'validation/accuracy': 0.6277799606323242, 'validation/loss': 1.5659699440002441, 'validation/num_examples': 50000, 'test/accuracy': 0.5024000406265259, 'test/loss': 2.2063982486724854, 'test/num_examples': 10000, 'score': 39951.18798828125, 'total_duration': 43425.609325408936, 'accumulated_submission_time': 39951.18798828125, 'accumulated_eval_time': 3465.7896132469177, 'accumulated_logging_time': 4.005863666534424, 'global_step': 87751, 'preemption_count': 0}), (88675, {'train/accuracy': 0.6822851300239563, 'train/loss': 1.2980283498764038, 'validation/accuracy': 0.6274799704551697, 'validation/loss': 1.5562463998794556, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.1928582191467285, 'test/num_examples': 10000, 'score': 40371.13100576401, 'total_duration': 43880.40012168884, 'accumulated_submission_time': 40371.13100576401, 'accumulated_eval_time': 3500.5497431755066, 'accumulated_logging_time': 4.045783042907715, 'global_step': 88675, 'preemption_count': 0}), (89599, {'train/accuracy': 0.6733984351158142, 'train/loss': 1.3592572212219238, 'validation/accuracy': 0.6269800066947937, 'validation/loss': 1.5658754110336304, 'validation/num_examples': 50000, 'test/accuracy': 0.5076000094413757, 'test/loss': 2.21124529838562, 'test/num_examples': 10000, 'score': 40791.40249633789, 'total_duration': 44335.90924882889, 'accumulated_submission_time': 40791.40249633789, 'accumulated_eval_time': 3535.699191570282, 'accumulated_logging_time': 4.085117340087891, 'global_step': 89599, 'preemption_count': 0}), (90524, {'train/accuracy': 0.6755468845367432, 'train/loss': 1.3368297815322876, 'validation/accuracy': 0.6282599568367004, 'validation/loss': 1.5485241413116455, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.1989150047302246, 'test/num_examples': 10000, 'score': 41211.55885767937, 'total_duration': 44790.13888645172, 'accumulated_submission_time': 41211.55885767937, 'accumulated_eval_time': 3569.6862609386444, 'accumulated_logging_time': 4.122882843017578, 'global_step': 90524, 'preemption_count': 0}), (91448, {'train/accuracy': 0.6873828172683716, 'train/loss': 1.2694848775863647, 'validation/accuracy': 0.6334599852561951, 'validation/loss': 1.521888256072998, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.1726202964782715, 'test/num_examples': 10000, 'score': 41631.89220118523, 'total_duration': 45247.85872173309, 'accumulated_submission_time': 41631.89220118523, 'accumulated_eval_time': 3606.986034631729, 'accumulated_logging_time': 4.161207437515259, 'global_step': 91448, 'preemption_count': 0}), (92373, {'train/accuracy': 0.699414074420929, 'train/loss': 1.262794017791748, 'validation/accuracy': 0.6269800066947937, 'validation/loss': 1.5701531171798706, 'validation/num_examples': 50000, 'test/accuracy': 0.5078999996185303, 'test/loss': 2.2144458293914795, 'test/num_examples': 10000, 'score': 42052.14500403404, 'total_duration': 45706.286526441574, 'accumulated_submission_time': 42052.14500403404, 'accumulated_eval_time': 3645.071723461151, 'accumulated_logging_time': 4.2016355991363525, 'global_step': 92373, 'preemption_count': 0}), (93298, {'train/accuracy': 0.6812304258346558, 'train/loss': 1.2977181673049927, 'validation/accuracy': 0.63646000623703, 'validation/loss': 1.5049082040786743, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.159391164779663, 'test/num_examples': 10000, 'score': 42472.23790502548, 'total_duration': 46159.35181570053, 'accumulated_submission_time': 42472.23790502548, 'accumulated_eval_time': 3677.9450783729553, 'accumulated_logging_time': 4.251505374908447, 'global_step': 93298, 'preemption_count': 0}), (94221, {'train/accuracy': 0.6886913776397705, 'train/loss': 1.2740516662597656, 'validation/accuracy': 0.634880006313324, 'validation/loss': 1.5176643133163452, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.162588119506836, 'test/num_examples': 10000, 'score': 42892.54639649391, 'total_duration': 46618.41143655777, 'accumulated_submission_time': 42892.54639649391, 'accumulated_eval_time': 3716.6099610328674, 'accumulated_logging_time': 4.289197206497192, 'global_step': 94221, 'preemption_count': 0}), (95146, {'train/accuracy': 0.7041796445846558, 'train/loss': 1.1996102333068848, 'validation/accuracy': 0.6363599896430969, 'validation/loss': 1.4963476657867432, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.128796100616455, 'test/num_examples': 10000, 'score': 43312.84077787399, 'total_duration': 47077.16275238991, 'accumulated_submission_time': 43312.84077787399, 'accumulated_eval_time': 3754.973846912384, 'accumulated_logging_time': 4.333668947219849, 'global_step': 95146, 'preemption_count': 0}), (96071, {'train/accuracy': 0.6895898580551147, 'train/loss': 1.2599599361419678, 'validation/accuracy': 0.6400399804115295, 'validation/loss': 1.4780635833740234, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.110060930252075, 'test/num_examples': 10000, 'score': 43733.06075167656, 'total_duration': 47535.352481126785, 'accumulated_submission_time': 43733.06075167656, 'accumulated_eval_time': 3792.851813316345, 'accumulated_logging_time': 4.377025365829468, 'global_step': 96071, 'preemption_count': 0}), (96996, {'train/accuracy': 0.690234363079071, 'train/loss': 1.2644189596176147, 'validation/accuracy': 0.642300009727478, 'validation/loss': 1.4981861114501953, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.145113468170166, 'test/num_examples': 10000, 'score': 44153.27914762497, 'total_duration': 47992.982228040695, 'accumulated_submission_time': 44153.27914762497, 'accumulated_eval_time': 3830.168391227722, 'accumulated_logging_time': 4.4223480224609375, 'global_step': 96996, 'preemption_count': 0}), (97914, {'train/accuracy': 0.6972265243530273, 'train/loss': 1.2512993812561035, 'validation/accuracy': 0.6369799971580505, 'validation/loss': 1.5186965465545654, 'validation/num_examples': 50000, 'test/accuracy': 0.5138000249862671, 'test/loss': 2.157207727432251, 'test/num_examples': 10000, 'score': 44573.461153030396, 'total_duration': 48449.29330062866, 'accumulated_submission_time': 44573.461153030396, 'accumulated_eval_time': 3866.2021346092224, 'accumulated_logging_time': 4.46885085105896, 'global_step': 97914, 'preemption_count': 0}), (98837, {'train/accuracy': 0.6911327838897705, 'train/loss': 1.2608174085617065, 'validation/accuracy': 0.6458399891853333, 'validation/loss': 1.466307282447815, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.1083343029022217, 'test/num_examples': 10000, 'score': 44993.504257678986, 'total_duration': 48907.949244976044, 'accumulated_submission_time': 44993.504257678986, 'accumulated_eval_time': 3904.7241473197937, 'accumulated_logging_time': 4.510102987289429, 'global_step': 98837, 'preemption_count': 0}), (99760, {'train/accuracy': 0.6932421922683716, 'train/loss': 1.244040608406067, 'validation/accuracy': 0.6432799696922302, 'validation/loss': 1.4662216901779175, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.098806858062744, 'test/num_examples': 10000, 'score': 45413.90217757225, 'total_duration': 49368.09958767891, 'accumulated_submission_time': 45413.90217757225, 'accumulated_eval_time': 3944.386614084244, 'accumulated_logging_time': 4.552144289016724, 'global_step': 99760, 'preemption_count': 0}), (100680, {'train/accuracy': 0.7011132836341858, 'train/loss': 1.2188524007797241, 'validation/accuracy': 0.6454199552536011, 'validation/loss': 1.4721068143844604, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.113632917404175, 'test/num_examples': 10000, 'score': 45834.003454208374, 'total_duration': 49826.67389035225, 'accumulated_submission_time': 45834.003454208374, 'accumulated_eval_time': 3982.7625353336334, 'accumulated_logging_time': 4.600781679153442, 'global_step': 100680, 'preemption_count': 0}), (101600, {'train/accuracy': 0.7016406059265137, 'train/loss': 1.2149200439453125, 'validation/accuracy': 0.6446999907493591, 'validation/loss': 1.46499764919281, 'validation/num_examples': 50000, 'test/accuracy': 0.522100031375885, 'test/loss': 2.124752998352051, 'test/num_examples': 10000, 'score': 46253.99683356285, 'total_duration': 50285.926607847214, 'accumulated_submission_time': 46253.99683356285, 'accumulated_eval_time': 4021.927656650543, 'accumulated_logging_time': 4.6470115184783936, 'global_step': 101600, 'preemption_count': 0}), (102523, {'train/accuracy': 0.6954296827316284, 'train/loss': 1.231560230255127, 'validation/accuracy': 0.6503599882125854, 'validation/loss': 1.4419031143188477, 'validation/num_examples': 50000, 'test/accuracy': 0.5236999988555908, 'test/loss': 2.1005005836486816, 'test/num_examples': 10000, 'score': 46674.343448877335, 'total_duration': 50745.39287304878, 'accumulated_submission_time': 46674.343448877335, 'accumulated_eval_time': 4060.9536135196686, 'accumulated_logging_time': 4.69239354133606, 'global_step': 102523, 'preemption_count': 0}), (103444, {'train/accuracy': 0.7022265195846558, 'train/loss': 1.2212992906570435, 'validation/accuracy': 0.6457799673080444, 'validation/loss': 1.4728409051895142, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.1184866428375244, 'test/num_examples': 10000, 'score': 47094.48765182495, 'total_duration': 51201.4682199955, 'accumulated_submission_time': 47094.48765182495, 'accumulated_eval_time': 4096.788547039032, 'accumulated_logging_time': 4.73948335647583, 'global_step': 103444, 'preemption_count': 0}), (104365, {'train/accuracy': 0.7180859446525574, 'train/loss': 1.164534091949463, 'validation/accuracy': 0.6500999927520752, 'validation/loss': 1.4714568853378296, 'validation/num_examples': 50000, 'test/accuracy': 0.528700053691864, 'test/loss': 2.0928614139556885, 'test/num_examples': 10000, 'score': 47514.577988386154, 'total_duration': 51658.96434521675, 'accumulated_submission_time': 47514.577988386154, 'accumulated_eval_time': 4134.106735706329, 'accumulated_logging_time': 4.77846884727478, 'global_step': 104365, 'preemption_count': 0}), (105288, {'train/accuracy': 0.6989648342132568, 'train/loss': 1.2240859270095825, 'validation/accuracy': 0.6522799730300903, 'validation/loss': 1.4324557781219482, 'validation/num_examples': 50000, 'test/accuracy': 0.5332000255584717, 'test/loss': 2.0801806449890137, 'test/num_examples': 10000, 'score': 47934.9159283638, 'total_duration': 52117.99890470505, 'accumulated_submission_time': 47934.9159283638, 'accumulated_eval_time': 4172.708475351334, 'accumulated_logging_time': 4.825288534164429, 'global_step': 105288, 'preemption_count': 0}), (106209, {'train/accuracy': 0.7043554782867432, 'train/loss': 1.194347858428955, 'validation/accuracy': 0.650879979133606, 'validation/loss': 1.4379023313522339, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.0719387531280518, 'test/num_examples': 10000, 'score': 48354.99125123024, 'total_duration': 52579.87425208092, 'accumulated_submission_time': 48354.99125123024, 'accumulated_eval_time': 4214.411760091782, 'accumulated_logging_time': 4.873553991317749, 'global_step': 106209, 'preemption_count': 0}), (107131, {'train/accuracy': 0.717578113079071, 'train/loss': 1.1480196714401245, 'validation/accuracy': 0.6521399617195129, 'validation/loss': 1.4363003969192505, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.086843490600586, 'test/num_examples': 10000, 'score': 48775.022804260254, 'total_duration': 53041.11679935455, 'accumulated_submission_time': 48775.022804260254, 'accumulated_eval_time': 4255.533785581589, 'accumulated_logging_time': 4.913725852966309, 'global_step': 107131, 'preemption_count': 0}), (108050, {'train/accuracy': 0.7084179520606995, 'train/loss': 1.183990240097046, 'validation/accuracy': 0.661359965801239, 'validation/loss': 1.3978753089904785, 'validation/num_examples': 50000, 'test/accuracy': 0.5369000434875488, 'test/loss': 2.0427968502044678, 'test/num_examples': 10000, 'score': 49194.9777302742, 'total_duration': 53502.97209262848, 'accumulated_submission_time': 49194.9777302742, 'accumulated_eval_time': 4297.330185413361, 'accumulated_logging_time': 4.968867778778076, 'global_step': 108050, 'preemption_count': 0}), (108970, {'train/accuracy': 0.7154492139816284, 'train/loss': 1.1541640758514404, 'validation/accuracy': 0.6593199968338013, 'validation/loss': 1.3946455717086792, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.0466535091400146, 'test/num_examples': 10000, 'score': 49614.986085653305, 'total_duration': 53960.53556919098, 'accumulated_submission_time': 49614.986085653305, 'accumulated_eval_time': 4334.789614200592, 'accumulated_logging_time': 5.015544414520264, 'global_step': 108970, 'preemption_count': 0}), (109892, {'train/accuracy': 0.7190625071525574, 'train/loss': 1.153494119644165, 'validation/accuracy': 0.6587399840354919, 'validation/loss': 1.4109584093093872, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.0479791164398193, 'test/num_examples': 10000, 'score': 50035.14333152771, 'total_duration': 54417.93356990814, 'accumulated_submission_time': 50035.14333152771, 'accumulated_eval_time': 4371.936519861221, 'accumulated_logging_time': 5.060314655303955, 'global_step': 109892, 'preemption_count': 0}), (110814, {'train/accuracy': 0.707324206829071, 'train/loss': 1.1942484378814697, 'validation/accuracy': 0.6566799879074097, 'validation/loss': 1.4237462282180786, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.042734384536743, 'test/num_examples': 10000, 'score': 50455.339581251144, 'total_duration': 54877.30311059952, 'accumulated_submission_time': 50455.339581251144, 'accumulated_eval_time': 4411.016355514526, 'accumulated_logging_time': 5.104965448379517, 'global_step': 110814, 'preemption_count': 0}), (111735, {'train/accuracy': 0.7171288728713989, 'train/loss': 1.144237995147705, 'validation/accuracy': 0.6643199920654297, 'validation/loss': 1.3885068893432617, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.033914566040039, 'test/num_examples': 10000, 'score': 50875.68436551094, 'total_duration': 55336.55248832703, 'accumulated_submission_time': 50875.68436551094, 'accumulated_eval_time': 4449.826142311096, 'accumulated_logging_time': 5.151665925979614, 'global_step': 111735, 'preemption_count': 0}), (112656, {'train/accuracy': 0.7238867282867432, 'train/loss': 1.1074589490890503, 'validation/accuracy': 0.6677199602127075, 'validation/loss': 1.3568204641342163, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.0181593894958496, 'test/num_examples': 10000, 'score': 51295.67676925659, 'total_duration': 55796.26206231117, 'accumulated_submission_time': 51295.67676925659, 'accumulated_eval_time': 4489.4482209682465, 'accumulated_logging_time': 5.197967767715454, 'global_step': 112656, 'preemption_count': 0}), (113578, {'train/accuracy': 0.7433788776397705, 'train/loss': 1.0337588787078857, 'validation/accuracy': 0.6680999994277954, 'validation/loss': 1.3554725646972656, 'validation/num_examples': 50000, 'test/accuracy': 0.5476000308990479, 'test/loss': 1.9904879331588745, 'test/num_examples': 10000, 'score': 51715.61089348793, 'total_duration': 56253.195014476776, 'accumulated_submission_time': 51715.61089348793, 'accumulated_eval_time': 4526.354462623596, 'accumulated_logging_time': 5.241725444793701, 'global_step': 113578, 'preemption_count': 0}), (114501, {'train/accuracy': 0.7174609303474426, 'train/loss': 1.1560657024383545, 'validation/accuracy': 0.6663399934768677, 'validation/loss': 1.3868917226791382, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.03826904296875, 'test/num_examples': 10000, 'score': 52135.60233712196, 'total_duration': 56712.77788186073, 'accumulated_submission_time': 52135.60233712196, 'accumulated_eval_time': 4565.85283613205, 'accumulated_logging_time': 5.286535739898682, 'global_step': 114501, 'preemption_count': 0}), (115423, {'train/accuracy': 0.7275195121765137, 'train/loss': 1.0962249040603638, 'validation/accuracy': 0.6660999655723572, 'validation/loss': 1.3563467264175415, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 1.9989176988601685, 'test/num_examples': 10000, 'score': 52555.83881497383, 'total_duration': 57172.142484903336, 'accumulated_submission_time': 52555.83881497383, 'accumulated_eval_time': 4604.891656398773, 'accumulated_logging_time': 5.327480792999268, 'global_step': 115423, 'preemption_count': 0}), (116343, {'train/accuracy': 0.7395312190055847, 'train/loss': 1.0397402048110962, 'validation/accuracy': 0.6725199818611145, 'validation/loss': 1.3434137105941772, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 1.994463562965393, 'test/num_examples': 10000, 'score': 52975.88430976868, 'total_duration': 57628.63620185852, 'accumulated_submission_time': 52975.88430976868, 'accumulated_eval_time': 4641.2410888671875, 'accumulated_logging_time': 5.377520561218262, 'global_step': 116343, 'preemption_count': 0}), (117267, {'train/accuracy': 0.7267968654632568, 'train/loss': 1.0954101085662842, 'validation/accuracy': 0.6725599765777588, 'validation/loss': 1.3405753374099731, 'validation/num_examples': 50000, 'test/accuracy': 0.5462000370025635, 'test/loss': 1.9877381324768066, 'test/num_examples': 10000, 'score': 53396.02493786812, 'total_duration': 58088.51064157486, 'accumulated_submission_time': 53396.02493786812, 'accumulated_eval_time': 4680.875846385956, 'accumulated_logging_time': 5.427154302597046, 'global_step': 117267, 'preemption_count': 0}), (118189, {'train/accuracy': 0.7324609160423279, 'train/loss': 1.0725865364074707, 'validation/accuracy': 0.6753399968147278, 'validation/loss': 1.3302712440490723, 'validation/num_examples': 50000, 'test/accuracy': 0.551300048828125, 'test/loss': 1.9637449979782104, 'test/num_examples': 10000, 'score': 53816.141922950745, 'total_duration': 58548.5072760582, 'accumulated_submission_time': 53816.141922950745, 'accumulated_eval_time': 4720.6627950668335, 'accumulated_logging_time': 5.471050262451172, 'global_step': 118189, 'preemption_count': 0}), (119112, {'train/accuracy': 0.7394335865974426, 'train/loss': 1.0667121410369873, 'validation/accuracy': 0.6752600073814392, 'validation/loss': 1.3475139141082764, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 1.9856129884719849, 'test/num_examples': 10000, 'score': 54236.35823750496, 'total_duration': 59005.27227306366, 'accumulated_submission_time': 54236.35823750496, 'accumulated_eval_time': 4757.115355014801, 'accumulated_logging_time': 5.5186426639556885, 'global_step': 119112, 'preemption_count': 0}), (120033, {'train/accuracy': 0.729199230670929, 'train/loss': 1.0772337913513184, 'validation/accuracy': 0.675279974937439, 'validation/loss': 1.3255867958068848, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 1.9687131643295288, 'test/num_examples': 10000, 'score': 54656.450139045715, 'total_duration': 59460.713866472244, 'accumulated_submission_time': 54656.450139045715, 'accumulated_eval_time': 4792.37073636055, 'accumulated_logging_time': 5.564018726348877, 'global_step': 120033, 'preemption_count': 0}), (120955, {'train/accuracy': 0.7299999594688416, 'train/loss': 1.0698095560073853, 'validation/accuracy': 0.6780999898910522, 'validation/loss': 1.307468056678772, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 1.95231294631958, 'test/num_examples': 10000, 'score': 55076.50438141823, 'total_duration': 59919.06721377373, 'accumulated_submission_time': 55076.50438141823, 'accumulated_eval_time': 4830.57146859169, 'accumulated_logging_time': 5.6144208908081055, 'global_step': 120955, 'preemption_count': 0}), (121875, {'train/accuracy': 0.7435156106948853, 'train/loss': 1.0205103158950806, 'validation/accuracy': 0.6786999702453613, 'validation/loss': 1.2984049320220947, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.928056001663208, 'test/num_examples': 10000, 'score': 55496.58257865906, 'total_duration': 60376.49431824684, 'accumulated_submission_time': 55496.58257865906, 'accumulated_eval_time': 4867.829748630524, 'accumulated_logging_time': 5.6567864418029785, 'global_step': 121875, 'preemption_count': 0}), (122794, {'train/accuracy': 0.7494726181030273, 'train/loss': 1.0063884258270264, 'validation/accuracy': 0.6796199679374695, 'validation/loss': 1.3093905448913574, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 1.9321649074554443, 'test/num_examples': 10000, 'score': 55916.64206838608, 'total_duration': 60835.119685173035, 'accumulated_submission_time': 55916.64206838608, 'accumulated_eval_time': 4906.302020072937, 'accumulated_logging_time': 5.70208215713501, 'global_step': 122794, 'preemption_count': 0}), (123715, {'train/accuracy': 0.7424414157867432, 'train/loss': 1.035762071609497, 'validation/accuracy': 0.6828199625015259, 'validation/loss': 1.2859761714935303, 'validation/num_examples': 50000, 'test/accuracy': 0.5570999979972839, 'test/loss': 1.9202402830123901, 'test/num_examples': 10000, 'score': 56336.915759801865, 'total_duration': 61291.97744345665, 'accumulated_submission_time': 56336.915759801865, 'accumulated_eval_time': 4942.790773868561, 'accumulated_logging_time': 5.748456001281738, 'global_step': 123715, 'preemption_count': 0}), (124636, {'train/accuracy': 0.7409374713897705, 'train/loss': 1.054355263710022, 'validation/accuracy': 0.6823399662971497, 'validation/loss': 1.299355149269104, 'validation/num_examples': 50000, 'test/accuracy': 0.5631000399589539, 'test/loss': 1.9321929216384888, 'test/num_examples': 10000, 'score': 56757.108996629715, 'total_duration': 61752.66572880745, 'accumulated_submission_time': 56757.108996629715, 'accumulated_eval_time': 4983.189473390579, 'accumulated_logging_time': 5.796940326690674, 'global_step': 124636, 'preemption_count': 0}), (125557, {'train/accuracy': 0.7582616806030273, 'train/loss': 0.9785751104354858, 'validation/accuracy': 0.6868399977684021, 'validation/loss': 1.288220763206482, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 1.9148192405700684, 'test/num_examples': 10000, 'score': 57177.093199014664, 'total_duration': 62214.15758442879, 'accumulated_submission_time': 57177.093199014664, 'accumulated_eval_time': 5024.603069782257, 'accumulated_logging_time': 5.8421630859375, 'global_step': 125557, 'preemption_count': 0}), (126481, {'train/accuracy': 0.744921863079071, 'train/loss': 1.0215235948562622, 'validation/accuracy': 0.6889199614524841, 'validation/loss': 1.2752596139907837, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9243862628936768, 'test/num_examples': 10000, 'score': 57597.376497745514, 'total_duration': 62675.59431099892, 'accumulated_submission_time': 57597.376497745514, 'accumulated_eval_time': 5065.656978368759, 'accumulated_logging_time': 5.892143726348877, 'global_step': 126481, 'preemption_count': 0}), (127404, {'train/accuracy': 0.7497265338897705, 'train/loss': 1.0092304944992065, 'validation/accuracy': 0.6896399855613708, 'validation/loss': 1.2700551748275757, 'validation/num_examples': 50000, 'test/accuracy': 0.572100043296814, 'test/loss': 1.8867379426956177, 'test/num_examples': 10000, 'score': 58017.32027029991, 'total_duration': 63134.48843693733, 'accumulated_submission_time': 58017.32027029991, 'accumulated_eval_time': 5104.507493257523, 'accumulated_logging_time': 5.9442808628082275, 'global_step': 127404, 'preemption_count': 0}), (128326, {'train/accuracy': 0.7568749785423279, 'train/loss': 0.9682868123054504, 'validation/accuracy': 0.6923800110816956, 'validation/loss': 1.2558635473251343, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 1.8772536516189575, 'test/num_examples': 10000, 'score': 58437.31800484657, 'total_duration': 63591.67713737488, 'accumulated_submission_time': 58437.31800484657, 'accumulated_eval_time': 5141.601754665375, 'accumulated_logging_time': 5.991713047027588, 'global_step': 128326, 'preemption_count': 0}), (129247, {'train/accuracy': 0.7479491829872131, 'train/loss': 0.9964887499809265, 'validation/accuracy': 0.6901599764823914, 'validation/loss': 1.2488561868667603, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 1.8783817291259766, 'test/num_examples': 10000, 'score': 58857.65893149376, 'total_duration': 64047.86702609062, 'accumulated_submission_time': 58857.65893149376, 'accumulated_eval_time': 5177.357530832291, 'accumulated_logging_time': 6.036379098892212, 'global_step': 129247, 'preemption_count': 0}), (130169, {'train/accuracy': 0.75501948595047, 'train/loss': 0.979481041431427, 'validation/accuracy': 0.6937199831008911, 'validation/loss': 1.2402454614639282, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 1.873192310333252, 'test/num_examples': 10000, 'score': 59277.79971027374, 'total_duration': 64509.075212717056, 'accumulated_submission_time': 59277.79971027374, 'accumulated_eval_time': 5218.325902700424, 'accumulated_logging_time': 6.086957216262817, 'global_step': 130169, 'preemption_count': 0}), (131090, {'train/accuracy': 0.7644726634025574, 'train/loss': 0.9445186257362366, 'validation/accuracy': 0.6960799694061279, 'validation/loss': 1.2283663749694824, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 1.867382287979126, 'test/num_examples': 10000, 'score': 59698.05014848709, 'total_duration': 64971.06675004959, 'accumulated_submission_time': 59698.05014848709, 'accumulated_eval_time': 5259.966984272003, 'accumulated_logging_time': 6.138098955154419, 'global_step': 131090, 'preemption_count': 0}), (132012, {'train/accuracy': 0.763867199420929, 'train/loss': 0.9349223375320435, 'validation/accuracy': 0.6986199617385864, 'validation/loss': 1.2158799171447754, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.8444945812225342, 'test/num_examples': 10000, 'score': 60117.96479392052, 'total_duration': 65429.777752399445, 'accumulated_submission_time': 60117.96479392052, 'accumulated_eval_time': 5298.666358947754, 'accumulated_logging_time': 6.187035799026489, 'global_step': 132012, 'preemption_count': 0}), (132935, {'train/accuracy': 0.7568749785423279, 'train/loss': 0.9747950434684753, 'validation/accuracy': 0.6997199654579163, 'validation/loss': 1.2330219745635986, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 1.8723223209381104, 'test/num_examples': 10000, 'score': 60538.21879982948, 'total_duration': 65887.87410736084, 'accumulated_submission_time': 60538.21879982948, 'accumulated_eval_time': 5336.413270950317, 'accumulated_logging_time': 6.234623432159424, 'global_step': 132935, 'preemption_count': 0}), (133860, {'train/accuracy': 0.7635351419448853, 'train/loss': 0.9517484307289124, 'validation/accuracy': 0.7021600008010864, 'validation/loss': 1.2254695892333984, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 1.8516438007354736, 'test/num_examples': 10000, 'score': 60958.368864774704, 'total_duration': 66344.07096099854, 'accumulated_submission_time': 60958.368864774704, 'accumulated_eval_time': 5372.358961343765, 'accumulated_logging_time': 6.28567361831665, 'global_step': 133860, 'preemption_count': 0}), (134783, {'train/accuracy': 0.7740820050239563, 'train/loss': 0.883020281791687, 'validation/accuracy': 0.6988799571990967, 'validation/loss': 1.2179479598999023, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.8422235250473022, 'test/num_examples': 10000, 'score': 61378.45201802254, 'total_duration': 66799.75529813766, 'accumulated_submission_time': 61378.45201802254, 'accumulated_eval_time': 5407.8675990104675, 'accumulated_logging_time': 6.32945704460144, 'global_step': 134783, 'preemption_count': 0}), (135707, {'train/accuracy': 0.761914074420929, 'train/loss': 0.9550774097442627, 'validation/accuracy': 0.6998999714851379, 'validation/loss': 1.2158119678497314, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.8288977146148682, 'test/num_examples': 10000, 'score': 61798.35517120361, 'total_duration': 67256.5339550972, 'accumulated_submission_time': 61798.35517120361, 'accumulated_eval_time': 5444.643156290054, 'accumulated_logging_time': 6.38027548789978, 'global_step': 135707, 'preemption_count': 0}), (136632, {'train/accuracy': 0.7692968845367432, 'train/loss': 0.9049091935157776, 'validation/accuracy': 0.7047799825668335, 'validation/loss': 1.1906603574752808, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.79646897315979, 'test/num_examples': 10000, 'score': 62218.55541443825, 'total_duration': 67712.98147702217, 'accumulated_submission_time': 62218.55541443825, 'accumulated_eval_time': 5480.798250198364, 'accumulated_logging_time': 6.4242448806762695, 'global_step': 136632, 'preemption_count': 0}), (137558, {'train/accuracy': 0.7782226204872131, 'train/loss': 0.8680867552757263, 'validation/accuracy': 0.7066400051116943, 'validation/loss': 1.1834466457366943, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 1.7963061332702637, 'test/num_examples': 10000, 'score': 62638.88868141174, 'total_duration': 68176.90643644333, 'accumulated_submission_time': 62638.88868141174, 'accumulated_eval_time': 5524.29457783699, 'accumulated_logging_time': 6.470351934432983, 'global_step': 137558, 'preemption_count': 0}), (138483, {'train/accuracy': 0.768750011920929, 'train/loss': 0.9112576842308044, 'validation/accuracy': 0.7040199637413025, 'validation/loss': 1.1906100511550903, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.7935667037963867, 'test/num_examples': 10000, 'score': 63058.986698150635, 'total_duration': 68633.01377010345, 'accumulated_submission_time': 63058.986698150635, 'accumulated_eval_time': 5560.188486814499, 'accumulated_logging_time': 6.537533760070801, 'global_step': 138483, 'preemption_count': 0}), (139405, {'train/accuracy': 0.775195300579071, 'train/loss': 0.8927517533302307, 'validation/accuracy': 0.7079600095748901, 'validation/loss': 1.1707615852355957, 'validation/num_examples': 50000, 'test/accuracy': 0.585800051689148, 'test/loss': 1.7905820608139038, 'test/num_examples': 10000, 'score': 63478.894491672516, 'total_duration': 69097.40825605392, 'accumulated_submission_time': 63478.894491672516, 'accumulated_eval_time': 5604.57666349411, 'accumulated_logging_time': 6.587302923202515, 'global_step': 139405, 'preemption_count': 0}), (140328, {'train/accuracy': 0.7830859422683716, 'train/loss': 0.8475367426872253, 'validation/accuracy': 0.7130999565124512, 'validation/loss': 1.1560105085372925, 'validation/num_examples': 50000, 'test/accuracy': 0.5877000093460083, 'test/loss': 1.7814221382141113, 'test/num_examples': 10000, 'score': 63898.79946422577, 'total_duration': 69555.59872603416, 'accumulated_submission_time': 63898.79946422577, 'accumulated_eval_time': 5642.757848501205, 'accumulated_logging_time': 6.641981601715088, 'global_step': 140328, 'preemption_count': 0}), (141248, {'train/accuracy': 0.7801562547683716, 'train/loss': 0.8689774870872498, 'validation/accuracy': 0.7124599814414978, 'validation/loss': 1.151098370552063, 'validation/num_examples': 50000, 'test/accuracy': 0.5913000106811523, 'test/loss': 1.7659611701965332, 'test/num_examples': 10000, 'score': 64318.720710515976, 'total_duration': 70020.60980701447, 'accumulated_submission_time': 64318.720710515976, 'accumulated_eval_time': 5687.743291378021, 'accumulated_logging_time': 6.696745872497559, 'global_step': 141248, 'preemption_count': 0}), (142170, {'train/accuracy': 0.7764452695846558, 'train/loss': 0.8885748982429504, 'validation/accuracy': 0.7087399959564209, 'validation/loss': 1.1719297170639038, 'validation/num_examples': 50000, 'test/accuracy': 0.58760005235672, 'test/loss': 1.8026561737060547, 'test/num_examples': 10000, 'score': 64738.64544630051, 'total_duration': 70478.83239912987, 'accumulated_submission_time': 64738.64544630051, 'accumulated_eval_time': 5725.94082069397, 'accumulated_logging_time': 6.748511791229248, 'global_step': 142170, 'preemption_count': 0}), (143093, {'train/accuracy': 0.7829882502555847, 'train/loss': 0.8484798073768616, 'validation/accuracy': 0.7154799699783325, 'validation/loss': 1.1398857831954956, 'validation/num_examples': 50000, 'test/accuracy': 0.5895000100135803, 'test/loss': 1.7539395093917847, 'test/num_examples': 10000, 'score': 65158.876828193665, 'total_duration': 70937.52128458023, 'accumulated_submission_time': 65158.876828193665, 'accumulated_eval_time': 5764.303265571594, 'accumulated_logging_time': 6.794252634048462, 'global_step': 143093, 'preemption_count': 0}), (144016, {'train/accuracy': 0.7986913919448853, 'train/loss': 0.7790917158126831, 'validation/accuracy': 0.7196399569511414, 'validation/loss': 1.120820164680481, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.7356946468353271, 'test/num_examples': 10000, 'score': 65579.17691516876, 'total_duration': 71402.10477161407, 'accumulated_submission_time': 65579.17691516876, 'accumulated_eval_time': 5808.492747783661, 'accumulated_logging_time': 6.839449644088745, 'global_step': 144016, 'preemption_count': 0}), (144940, {'train/accuracy': 0.7861132621765137, 'train/loss': 0.835200846195221, 'validation/accuracy': 0.718999981880188, 'validation/loss': 1.122183084487915, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.7483426332473755, 'test/num_examples': 10000, 'score': 65999.29121685028, 'total_duration': 71859.84184217453, 'accumulated_submission_time': 65999.29121685028, 'accumulated_eval_time': 5846.021278142929, 'accumulated_logging_time': 6.884845018386841, 'global_step': 144940, 'preemption_count': 0}), (145865, {'train/accuracy': 0.789746105670929, 'train/loss': 0.8230050206184387, 'validation/accuracy': 0.7236599922180176, 'validation/loss': 1.11073899269104, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.7311910390853882, 'test/num_examples': 10000, 'score': 66419.42792582512, 'total_duration': 72317.11073088646, 'accumulated_submission_time': 66419.42792582512, 'accumulated_eval_time': 5883.058493375778, 'accumulated_logging_time': 6.93181300163269, 'global_step': 145865, 'preemption_count': 0}), (146791, {'train/accuracy': 0.7936913967132568, 'train/loss': 0.7987895011901855, 'validation/accuracy': 0.7238999605178833, 'validation/loss': 1.1063193082809448, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.7203985452651978, 'test/num_examples': 10000, 'score': 66839.75595474243, 'total_duration': 72776.55774569511, 'accumulated_submission_time': 66839.75595474243, 'accumulated_eval_time': 5922.079788208008, 'accumulated_logging_time': 6.981026649475098, 'global_step': 146791, 'preemption_count': 0}), (147715, {'train/accuracy': 0.7894726395606995, 'train/loss': 0.8196678757667542, 'validation/accuracy': 0.7218999862670898, 'validation/loss': 1.1062513589859009, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.724432110786438, 'test/num_examples': 10000, 'score': 67259.88791394234, 'total_duration': 73234.11113333702, 'accumulated_submission_time': 67259.88791394234, 'accumulated_eval_time': 5959.401660680771, 'accumulated_logging_time': 7.031947135925293, 'global_step': 147715, 'preemption_count': 0}), (148639, {'train/accuracy': 0.7953320145606995, 'train/loss': 0.7963201999664307, 'validation/accuracy': 0.724079966545105, 'validation/loss': 1.092710256576538, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.7002700567245483, 'test/num_examples': 10000, 'score': 67680.01170182228, 'total_duration': 73691.21638917923, 'accumulated_submission_time': 67680.01170182228, 'accumulated_eval_time': 5996.285425662994, 'accumulated_logging_time': 7.0817482471466064, 'global_step': 148639, 'preemption_count': 0}), (149563, {'train/accuracy': 0.7990038990974426, 'train/loss': 0.7832265496253967, 'validation/accuracy': 0.7269600033760071, 'validation/loss': 1.0977861881256104, 'validation/num_examples': 50000, 'test/accuracy': 0.6046000123023987, 'test/loss': 1.7144430875778198, 'test/num_examples': 10000, 'score': 68099.94502019882, 'total_duration': 74150.72949790955, 'accumulated_submission_time': 68099.94502019882, 'accumulated_eval_time': 6035.759362697601, 'accumulated_logging_time': 7.138274669647217, 'global_step': 149563, 'preemption_count': 0}), (150488, {'train/accuracy': 0.7956249713897705, 'train/loss': 0.7945720553398132, 'validation/accuracy': 0.7290799617767334, 'validation/loss': 1.082510232925415, 'validation/num_examples': 50000, 'test/accuracy': 0.6063000559806824, 'test/loss': 1.6938530206680298, 'test/num_examples': 10000, 'score': 68519.99004364014, 'total_duration': 74609.09123158455, 'accumulated_submission_time': 68519.99004364014, 'accumulated_eval_time': 6073.976391792297, 'accumulated_logging_time': 7.188690185546875, 'global_step': 150488, 'preemption_count': 0}), (151411, {'train/accuracy': 0.7971875071525574, 'train/loss': 0.7923197746276855, 'validation/accuracy': 0.7300999760627747, 'validation/loss': 1.0846587419509888, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.6871274709701538, 'test/num_examples': 10000, 'score': 68940.01628899574, 'total_duration': 75067.06227970123, 'accumulated_submission_time': 68940.01628899574, 'accumulated_eval_time': 6111.824824333191, 'accumulated_logging_time': 7.236317157745361, 'global_step': 151411, 'preemption_count': 0}), (152336, {'train/accuracy': 0.8050390481948853, 'train/loss': 0.7471283078193665, 'validation/accuracy': 0.7305999994277954, 'validation/loss': 1.0720800161361694, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.6767473220825195, 'test/num_examples': 10000, 'score': 69360.21492862701, 'total_duration': 75529.35670304298, 'accumulated_submission_time': 69360.21492862701, 'accumulated_eval_time': 6153.8241646289825, 'accumulated_logging_time': 7.284022569656372, 'global_step': 152336, 'preemption_count': 0}), (153221, {'train/accuracy': 0.8126757740974426, 'train/loss': 0.722992479801178, 'validation/accuracy': 0.7340399622917175, 'validation/loss': 1.0540411472320557, 'validation/num_examples': 50000, 'test/accuracy': 0.6136000156402588, 'test/loss': 1.6623154878616333, 'test/num_examples': 10000, 'score': 69780.14233207703, 'total_duration': 75984.6604912281, 'accumulated_submission_time': 69780.14233207703, 'accumulated_eval_time': 6189.108179092407, 'accumulated_logging_time': 7.329882383346558, 'global_step': 153221, 'preemption_count': 0}), (154146, {'train/accuracy': 0.8069726228713989, 'train/loss': 0.7545697093009949, 'validation/accuracy': 0.7346999645233154, 'validation/loss': 1.059226393699646, 'validation/num_examples': 50000, 'test/accuracy': 0.6131000518798828, 'test/loss': 1.6655516624450684, 'test/num_examples': 10000, 'score': 70200.330078125, 'total_duration': 76439.60998177528, 'accumulated_submission_time': 70200.330078125, 'accumulated_eval_time': 6223.7734811306, 'accumulated_logging_time': 7.37775182723999, 'global_step': 154146, 'preemption_count': 0}), (155071, {'train/accuracy': 0.8126562237739563, 'train/loss': 0.7402761578559875, 'validation/accuracy': 0.7357199788093567, 'validation/loss': 1.0593024492263794, 'validation/num_examples': 50000, 'test/accuracy': 0.6117000579833984, 'test/loss': 1.6622600555419922, 'test/num_examples': 10000, 'score': 70620.54906439781, 'total_duration': 76897.78743886948, 'accumulated_submission_time': 70620.54906439781, 'accumulated_eval_time': 6261.632649898529, 'accumulated_logging_time': 7.428417921066284, 'global_step': 155071, 'preemption_count': 0}), (155973, {'train/accuracy': 0.81556636095047, 'train/loss': 0.7248516082763672, 'validation/accuracy': 0.7346000075340271, 'validation/loss': 1.0620793104171753, 'validation/num_examples': 50000, 'test/accuracy': 0.6123000383377075, 'test/loss': 1.667306900024414, 'test/num_examples': 10000, 'score': 71040.45113134384, 'total_duration': 77354.13040804863, 'accumulated_submission_time': 71040.45113134384, 'accumulated_eval_time': 6297.965461015701, 'accumulated_logging_time': 7.48820424079895, 'global_step': 155973, 'preemption_count': 0}), (156898, {'train/accuracy': 0.8093163967132568, 'train/loss': 0.7406788468360901, 'validation/accuracy': 0.7397199869155884, 'validation/loss': 1.0419944524765015, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.6425464153289795, 'test/num_examples': 10000, 'score': 71460.83744478226, 'total_duration': 77812.2350218296, 'accumulated_submission_time': 71460.83744478226, 'accumulated_eval_time': 6335.5825090408325, 'accumulated_logging_time': 7.540948152542114, 'global_step': 156898, 'preemption_count': 0}), (157824, {'train/accuracy': 0.8129101395606995, 'train/loss': 0.7125070095062256, 'validation/accuracy': 0.7402399778366089, 'validation/loss': 1.0308599472045898, 'validation/num_examples': 50000, 'test/accuracy': 0.6149000525474548, 'test/loss': 1.6336321830749512, 'test/num_examples': 10000, 'score': 71880.89972639084, 'total_duration': 78269.73344492912, 'accumulated_submission_time': 71880.89972639084, 'accumulated_eval_time': 6372.919394493103, 'accumulated_logging_time': 7.591905832290649, 'global_step': 157824, 'preemption_count': 0}), (158748, {'train/accuracy': 0.8221093416213989, 'train/loss': 0.6898041367530823, 'validation/accuracy': 0.7427399754524231, 'validation/loss': 1.029032826423645, 'validation/num_examples': 50000, 'test/accuracy': 0.6207000017166138, 'test/loss': 1.6169078350067139, 'test/num_examples': 10000, 'score': 72301.06321072578, 'total_duration': 78728.11376452446, 'accumulated_submission_time': 72301.06321072578, 'accumulated_eval_time': 6411.03583574295, 'accumulated_logging_time': 7.643307447433472, 'global_step': 158748, 'preemption_count': 0}), (159671, {'train/accuracy': 0.81800776720047, 'train/loss': 0.7054724097251892, 'validation/accuracy': 0.7444599866867065, 'validation/loss': 1.0100157260894775, 'validation/num_examples': 50000, 'test/accuracy': 0.6194000244140625, 'test/loss': 1.619384765625, 'test/num_examples': 10000, 'score': 72721.30015707016, 'total_duration': 79189.0581202507, 'accumulated_submission_time': 72721.30015707016, 'accumulated_eval_time': 6451.647526979446, 'accumulated_logging_time': 7.691318511962891, 'global_step': 159671, 'preemption_count': 0}), (160594, {'train/accuracy': 0.8192187547683716, 'train/loss': 0.6979327201843262, 'validation/accuracy': 0.7441799640655518, 'validation/loss': 1.0171470642089844, 'validation/num_examples': 50000, 'test/accuracy': 0.6256000399589539, 'test/loss': 1.6140958070755005, 'test/num_examples': 10000, 'score': 73141.47015810013, 'total_duration': 79645.54460167885, 'accumulated_submission_time': 73141.47015810013, 'accumulated_eval_time': 6487.851769685745, 'accumulated_logging_time': 7.754884719848633, 'global_step': 160594, 'preemption_count': 0}), (161519, {'train/accuracy': 0.8247851133346558, 'train/loss': 0.6756976842880249, 'validation/accuracy': 0.7467600107192993, 'validation/loss': 1.0093703269958496, 'validation/num_examples': 50000, 'test/accuracy': 0.626300036907196, 'test/loss': 1.5992350578308105, 'test/num_examples': 10000, 'score': 73561.80247449875, 'total_duration': 80102.00310349464, 'accumulated_submission_time': 73561.80247449875, 'accumulated_eval_time': 6523.876390695572, 'accumulated_logging_time': 7.80646276473999, 'global_step': 161519, 'preemption_count': 0}), (162446, {'train/accuracy': 0.8268163800239563, 'train/loss': 0.6841356754302979, 'validation/accuracy': 0.7471999526023865, 'validation/loss': 1.016317367553711, 'validation/num_examples': 50000, 'test/accuracy': 0.6248000264167786, 'test/loss': 1.616760015487671, 'test/num_examples': 10000, 'score': 73982.11289167404, 'total_duration': 80563.53231620789, 'accumulated_submission_time': 73982.11289167404, 'accumulated_eval_time': 6564.9981191158295, 'accumulated_logging_time': 7.855017900466919, 'global_step': 162446, 'preemption_count': 0}), (163371, {'train/accuracy': 0.8298437595367432, 'train/loss': 0.6641115546226501, 'validation/accuracy': 0.7482799887657166, 'validation/loss': 0.9978412389755249, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.6025879383087158, 'test/num_examples': 10000, 'score': 74402.1906940937, 'total_duration': 81021.04125189781, 'accumulated_submission_time': 74402.1906940937, 'accumulated_eval_time': 6602.321990013123, 'accumulated_logging_time': 7.912995100021362, 'global_step': 163371, 'preemption_count': 0}), (164296, {'train/accuracy': 0.8291991949081421, 'train/loss': 0.6516792178153992, 'validation/accuracy': 0.7512399554252625, 'validation/loss': 0.9861694574356079, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.5875972509384155, 'test/num_examples': 10000, 'score': 74822.27236771584, 'total_duration': 81476.4604113102, 'accumulated_submission_time': 74822.27236771584, 'accumulated_eval_time': 6637.561418771744, 'accumulated_logging_time': 7.962857246398926, 'global_step': 164296, 'preemption_count': 0}), (165222, {'train/accuracy': 0.8324413895606995, 'train/loss': 0.6385616660118103, 'validation/accuracy': 0.7501399517059326, 'validation/loss': 0.9882864356040955, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.5926567316055298, 'test/num_examples': 10000, 'score': 75242.47238945961, 'total_duration': 81935.01135158539, 'accumulated_submission_time': 75242.47238945961, 'accumulated_eval_time': 6675.802627325058, 'accumulated_logging_time': 8.023729801177979, 'global_step': 165222, 'preemption_count': 0}), (166148, {'train/accuracy': 0.8308203220367432, 'train/loss': 0.660889208316803, 'validation/accuracy': 0.7534799575805664, 'validation/loss': 0.9846858382225037, 'validation/num_examples': 50000, 'test/accuracy': 0.6331000328063965, 'test/loss': 1.5844221115112305, 'test/num_examples': 10000, 'score': 75662.8382794857, 'total_duration': 82393.60587334633, 'accumulated_submission_time': 75662.8382794857, 'accumulated_eval_time': 6713.923867702484, 'accumulated_logging_time': 8.082376480102539, 'global_step': 166148, 'preemption_count': 0}), (167072, {'train/accuracy': 0.8302929401397705, 'train/loss': 0.645725667476654, 'validation/accuracy': 0.7534199953079224, 'validation/loss': 0.9803726077079773, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.5846593379974365, 'test/num_examples': 10000, 'score': 76082.94535136223, 'total_duration': 82852.91863751411, 'accumulated_submission_time': 76082.94535136223, 'accumulated_eval_time': 6753.028325080872, 'accumulated_logging_time': 8.135641813278198, 'global_step': 167072, 'preemption_count': 0}), (167997, {'train/accuracy': 0.8384179472923279, 'train/loss': 0.6208813190460205, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 0.9750881791114807, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.5767680406570435, 'test/num_examples': 10000, 'score': 76502.94843864441, 'total_duration': 83310.66582012177, 'accumulated_submission_time': 76502.94843864441, 'accumulated_eval_time': 6790.670788764954, 'accumulated_logging_time': 8.188108682632446, 'global_step': 167997, 'preemption_count': 0}), (168922, {'train/accuracy': 0.8319921493530273, 'train/loss': 0.6606417894363403, 'validation/accuracy': 0.756060004234314, 'validation/loss': 0.9795495271682739, 'validation/num_examples': 50000, 'test/accuracy': 0.6335000395774841, 'test/loss': 1.5818995237350464, 'test/num_examples': 10000, 'score': 76923.12551164627, 'total_duration': 83765.89028906822, 'accumulated_submission_time': 76923.12551164627, 'accumulated_eval_time': 6825.619375705719, 'accumulated_logging_time': 8.23816442489624, 'global_step': 168922, 'preemption_count': 0}), (169849, {'train/accuracy': 0.8330858945846558, 'train/loss': 0.6320017576217651, 'validation/accuracy': 0.7576000094413757, 'validation/loss': 0.9574471712112427, 'validation/num_examples': 50000, 'test/accuracy': 0.6391000151634216, 'test/loss': 1.561057209968567, 'test/num_examples': 10000, 'score': 77343.40371894836, 'total_duration': 84221.57668566704, 'accumulated_submission_time': 77343.40371894836, 'accumulated_eval_time': 6860.926365375519, 'accumulated_logging_time': 8.290139198303223, 'global_step': 169849, 'preemption_count': 0})], 'global_step': 170245}
I0202 11:39:26.306112 139863983413056 submission_runner.py:586] Timing: 77520.00591540337
I0202 11:39:26.306189 139863983413056 submission_runner.py:588] Total number of evals: 185
I0202 11:39:26.306233 139863983413056 submission_runner.py:589] ====================
I0202 11:39:26.306277 139863983413056 submission_runner.py:542] Using RNG seed 3682051175
I0202 11:39:26.307795 139863983413056 submission_runner.py:551] --- Tuning run 4/5 ---
I0202 11:39:26.307899 139863983413056 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_4.
I0202 11:39:26.310932 139863983413056 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_4/hparams.json.
I0202 11:39:26.311744 139863983413056 submission_runner.py:206] Initializing dataset.
I0202 11:39:26.320793 139863983413056 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0202 11:39:26.335236 139863983413056 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0202 11:39:26.532088 139863983413056 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0202 11:39:31.359341 139863983413056 submission_runner.py:213] Initializing model.
I0202 11:39:36.952148 139863983413056 submission_runner.py:255] Initializing optimizer.
I0202 11:39:37.436363 139863983413056 submission_runner.py:262] Initializing metrics bundle.
I0202 11:39:37.436522 139863983413056 submission_runner.py:280] Initializing checkpoint and logger.
I0202 11:39:37.451902 139863983413056 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_4 with prefix checkpoint_
I0202 11:39:37.452016 139863983413056 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0202 11:39:53.272085 139863983413056 logger_utils.py:220] Unable to record git information. Continuing without it.
I0202 11:40:08.797753 139863983413056 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_4/flags_0.json.
I0202 11:40:08.802842 139863983413056 submission_runner.py:314] Starting training loop.
I0202 11:40:43.656046 139702501852928 logging_writer.py:48] [0] global_step=0, grad_norm=0.3733336329460144, loss=6.907756328582764
I0202 11:40:43.667615 139863983413056 spec.py:321] Evaluating on the training split.
I0202 11:40:51.930566 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 11:41:10.255942 139863983413056 spec.py:349] Evaluating on the test split.
I0202 11:41:11.883826 139863983413056 submission_runner.py:408] Time since start: 63.08s, 	Step: 1, 	{'train/accuracy': 0.0010351561941206455, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 34.864447355270386, 'total_duration': 63.08093595504761, 'accumulated_submission_time': 34.864447355270386, 'accumulated_eval_time': 28.21615719795227, 'accumulated_logging_time': 0}
I0202 11:41:11.892237 139702510245632 logging_writer.py:48] [1] accumulated_eval_time=28.216157, accumulated_logging_time=0, accumulated_submission_time=34.864447, global_step=1, preemption_count=0, score=34.864447, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=63.080936, train/accuracy=0.001035, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0202 11:42:20.908120 139702543816448 logging_writer.py:48] [100] global_step=100, grad_norm=0.5345216989517212, loss=6.830348968505859
I0202 11:43:06.305614 139702527031040 logging_writer.py:48] [200] global_step=200, grad_norm=0.9364715814590454, loss=6.705363750457764
I0202 11:43:52.432207 139702543816448 logging_writer.py:48] [300] global_step=300, grad_norm=0.9885214567184448, loss=6.528993606567383
I0202 11:44:39.004023 139702527031040 logging_writer.py:48] [400] global_step=400, grad_norm=0.8801767230033875, loss=6.527554512023926
I0202 11:45:25.870903 139702543816448 logging_writer.py:48] [500] global_step=500, grad_norm=0.8617330193519592, loss=6.761857509613037
I0202 11:46:12.258539 139702527031040 logging_writer.py:48] [600] global_step=600, grad_norm=0.8609159588813782, loss=6.379199028015137
I0202 11:46:58.430346 139702543816448 logging_writer.py:48] [700] global_step=700, grad_norm=0.9657757878303528, loss=6.2344651222229
I0202 11:47:44.932190 139702527031040 logging_writer.py:48] [800] global_step=800, grad_norm=1.1662923097610474, loss=6.430248260498047
I0202 11:48:12.115916 139863983413056 spec.py:321] Evaluating on the training split.
I0202 11:48:22.986713 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 11:48:49.504203 139863983413056 spec.py:349] Evaluating on the test split.
I0202 11:48:51.140716 139863983413056 submission_runner.py:408] Time since start: 522.34s, 	Step: 860, 	{'train/accuracy': 0.033867187798023224, 'train/loss': 5.902928829193115, 'validation/accuracy': 0.030639998614788055, 'validation/loss': 5.9316911697387695, 'validation/num_examples': 50000, 'test/accuracy': 0.026000000536441803, 'test/loss': 6.065794467926025, 'test/num_examples': 10000, 'score': 455.0325014591217, 'total_duration': 522.3378174304962, 'accumulated_submission_time': 455.0325014591217, 'accumulated_eval_time': 67.2409439086914, 'accumulated_logging_time': 0.017840147018432617}
I0202 11:48:51.157833 139702543816448 logging_writer.py:48] [860] accumulated_eval_time=67.240944, accumulated_logging_time=0.017840, accumulated_submission_time=455.032501, global_step=860, preemption_count=0, score=455.032501, test/accuracy=0.026000, test/loss=6.065794, test/num_examples=10000, total_duration=522.337817, train/accuracy=0.033867, train/loss=5.902929, validation/accuracy=0.030640, validation/loss=5.931691, validation/num_examples=50000
I0202 11:49:07.494398 139702527031040 logging_writer.py:48] [900] global_step=900, grad_norm=0.7381635904312134, loss=6.107237815856934
I0202 11:49:51.442827 139702543816448 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7238590121269226, loss=6.1719889640808105
I0202 11:50:37.886319 139702527031040 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6781456470489502, loss=6.419310569763184
I0202 11:51:23.717969 139702543816448 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5571498274803162, loss=6.070754528045654
I0202 11:52:10.300337 139702527031040 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6338270306587219, loss=6.056466579437256
I0202 11:52:56.834366 139702543816448 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5477051138877869, loss=6.660135746002197
I0202 11:53:43.336718 139702527031040 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6671792268753052, loss=6.620871543884277
I0202 11:54:29.654534 139702543816448 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6723983287811279, loss=5.844419479370117
I0202 11:55:15.979523 139702527031040 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5401071310043335, loss=6.196493148803711
I0202 11:55:51.252706 139863983413056 spec.py:321] Evaluating on the training split.
I0202 11:56:01.744255 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 11:56:34.876626 139863983413056 spec.py:349] Evaluating on the test split.
I0202 11:56:36.518925 139863983413056 submission_runner.py:408] Time since start: 987.72s, 	Step: 1778, 	{'train/accuracy': 0.07476562261581421, 'train/loss': 5.325953960418701, 'validation/accuracy': 0.06904000043869019, 'validation/loss': 5.398590087890625, 'validation/num_examples': 50000, 'test/accuracy': 0.05450000241398811, 'test/loss': 5.616189956665039, 'test/num_examples': 10000, 'score': 875.0685901641846, 'total_duration': 987.7160251140594, 'accumulated_submission_time': 875.0685901641846, 'accumulated_eval_time': 112.50715756416321, 'accumulated_logging_time': 0.04444408416748047}
I0202 11:56:36.534709 139702543816448 logging_writer.py:48] [1778] accumulated_eval_time=112.507158, accumulated_logging_time=0.044444, accumulated_submission_time=875.068590, global_step=1778, preemption_count=0, score=875.068590, test/accuracy=0.054500, test/loss=5.616190, test/num_examples=10000, total_duration=987.716025, train/accuracy=0.074766, train/loss=5.325954, validation/accuracy=0.069040, validation/loss=5.398590, validation/num_examples=50000
I0202 11:56:45.729036 139702527031040 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5627044439315796, loss=6.429224967956543
I0202 11:57:28.395537 139702543816448 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6312671899795532, loss=5.83145809173584
I0202 11:58:14.561457 139702527031040 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5685180425643921, loss=5.820254802703857
I0202 11:59:00.612331 139702543816448 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.5455794334411621, loss=5.819797039031982
I0202 11:59:46.767936 139702527031040 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.4190826117992401, loss=6.65287446975708
I0202 12:00:33.261681 139702543816448 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.474109411239624, loss=5.7534379959106445
I0202 12:01:19.360445 139702527031040 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5341278910636902, loss=5.584723472595215
I0202 12:02:05.947165 139702543816448 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5426924824714661, loss=6.230339527130127
I0202 12:02:52.460342 139702527031040 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.5800073146820068, loss=5.72927713394165
I0202 12:03:36.784870 139863983413056 spec.py:321] Evaluating on the training split.
I0202 12:03:47.235511 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 12:04:13.625613 139863983413056 spec.py:349] Evaluating on the test split.
I0202 12:04:15.258113 139863983413056 submission_runner.py:408] Time since start: 1446.46s, 	Step: 2697, 	{'train/accuracy': 0.09357421845197678, 'train/loss': 5.147493362426758, 'validation/accuracy': 0.08664000034332275, 'validation/loss': 5.2080559730529785, 'validation/num_examples': 50000, 'test/accuracy': 0.06680000573396683, 'test/loss': 5.455690860748291, 'test/num_examples': 10000, 'score': 1295.2597908973694, 'total_duration': 1446.4552066326141, 'accumulated_submission_time': 1295.2597908973694, 'accumulated_eval_time': 150.98039412498474, 'accumulated_logging_time': 0.07066822052001953}
I0202 12:04:15.275272 139702543816448 logging_writer.py:48] [2697] accumulated_eval_time=150.980394, accumulated_logging_time=0.070668, accumulated_submission_time=1295.259791, global_step=2697, preemption_count=0, score=1295.259791, test/accuracy=0.066800, test/loss=5.455691, test/num_examples=10000, total_duration=1446.455207, train/accuracy=0.093574, train/loss=5.147493, validation/accuracy=0.086640, validation/loss=5.208056, validation/num_examples=50000
I0202 12:04:16.868414 139702527031040 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5016589164733887, loss=6.410989284515381
I0202 12:04:58.566145 139702543816448 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.4623541235923767, loss=6.036454200744629
I0202 12:05:44.816710 139702527031040 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5771682858467102, loss=5.361193656921387
I0202 12:06:30.816299 139702543816448 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5537741184234619, loss=5.495504856109619
I0202 12:07:17.071606 139702527031040 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5665851831436157, loss=5.5125651359558105
I0202 12:08:03.299041 139702543816448 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.44448748230934143, loss=6.210102081298828
I0202 12:08:49.288304 139702527031040 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5414896607398987, loss=5.328244209289551
I0202 12:09:35.414880 139702543816448 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5846937894821167, loss=5.376117706298828
I0202 12:10:21.706099 139702527031040 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.4932369291782379, loss=5.550894737243652
I0202 12:11:08.019676 139702543816448 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.491920530796051, loss=5.14748477935791
I0202 12:11:15.422362 139863983413056 spec.py:321] Evaluating on the training split.
I0202 12:11:26.032865 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 12:11:57.108800 139863983413056 spec.py:349] Evaluating on the test split.
I0202 12:11:58.744038 139863983413056 submission_runner.py:408] Time since start: 1909.94s, 	Step: 3618, 	{'train/accuracy': 0.13871093094348907, 'train/loss': 4.64686393737793, 'validation/accuracy': 0.12814000248908997, 'validation/loss': 4.720630645751953, 'validation/num_examples': 50000, 'test/accuracy': 0.09710000455379486, 'test/loss': 5.038281440734863, 'test/num_examples': 10000, 'score': 1715.3475415706635, 'total_duration': 1909.9411296844482, 'accumulated_submission_time': 1715.3475415706635, 'accumulated_eval_time': 194.3020474910736, 'accumulated_logging_time': 0.09827184677124023}
I0202 12:11:58.759926 139702527031040 logging_writer.py:48] [3618] accumulated_eval_time=194.302047, accumulated_logging_time=0.098272, accumulated_submission_time=1715.347542, global_step=3618, preemption_count=0, score=1715.347542, test/accuracy=0.097100, test/loss=5.038281, test/num_examples=10000, total_duration=1909.941130, train/accuracy=0.138711, train/loss=4.646864, validation/accuracy=0.128140, validation/loss=4.720631, validation/num_examples=50000
I0202 12:12:32.653907 139702543816448 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.79903244972229, loss=5.43383264541626
I0202 12:13:18.784380 139702527031040 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5050894618034363, loss=5.221935272216797
I0202 12:14:04.414970 139702543816448 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.938707709312439, loss=5.454494476318359
I0202 12:14:50.569674 139702527031040 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6403164267539978, loss=5.906692028045654
I0202 12:15:36.632837 139702543816448 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.46122968196868896, loss=5.771821022033691
I0202 12:16:22.758738 139702527031040 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5156849026679993, loss=5.922493934631348
I0202 12:17:08.816857 139702543816448 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.469963401556015, loss=5.553432464599609
I0202 12:17:55.016875 139702527031040 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7023059725761414, loss=6.426706790924072
I0202 12:18:41.054625 139702543816448 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7102047204971313, loss=4.936614990234375
I0202 12:18:59.165464 139863983413056 spec.py:321] Evaluating on the training split.
I0202 12:19:10.217594 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 12:19:40.052039 139863983413056 spec.py:349] Evaluating on the test split.
I0202 12:19:41.686826 139863983413056 submission_runner.py:408] Time since start: 2372.88s, 	Step: 4541, 	{'train/accuracy': 0.16164061427116394, 'train/loss': 4.394140720367432, 'validation/accuracy': 0.15173999965190887, 'validation/loss': 4.4949469566345215, 'validation/num_examples': 50000, 'test/accuracy': 0.11710000783205032, 'test/loss': 4.889176845550537, 'test/num_examples': 10000, 'score': 2135.6945128440857, 'total_duration': 2372.8839206695557, 'accumulated_submission_time': 2135.6945128440857, 'accumulated_eval_time': 236.82340264320374, 'accumulated_logging_time': 0.1237492561340332}
I0202 12:19:41.703645 139702527031040 logging_writer.py:48] [4541] accumulated_eval_time=236.823403, accumulated_logging_time=0.123749, accumulated_submission_time=2135.694513, global_step=4541, preemption_count=0, score=2135.694513, test/accuracy=0.117100, test/loss=4.889177, test/num_examples=10000, total_duration=2372.883921, train/accuracy=0.161641, train/loss=4.394141, validation/accuracy=0.151740, validation/loss=4.494947, validation/num_examples=50000
I0202 12:20:05.649801 139702543816448 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7220094203948975, loss=6.293412208557129
I0202 12:20:50.769187 139702527031040 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9701777696609497, loss=6.19979190826416
I0202 12:21:39.166593 139702543816448 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6287320256233215, loss=5.071437835693359
I0202 12:22:25.470546 139702527031040 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8174305558204651, loss=5.047633647918701
I0202 12:23:11.823525 139702543816448 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6670150756835938, loss=4.849181652069092
I0202 12:23:57.942475 139702527031040 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8336706161499023, loss=4.9192047119140625
I0202 12:24:44.152889 139702543816448 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7229728102684021, loss=4.83998966217041
I0202 12:25:30.338380 139702527031040 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8372674584388733, loss=5.153849124908447
I0202 12:26:16.545351 139702543816448 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8453390002250671, loss=4.8716607093811035
I0202 12:26:42.110090 139863983413056 spec.py:321] Evaluating on the training split.
I0202 12:26:52.886840 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 12:27:21.170794 139863983413056 spec.py:349] Evaluating on the test split.
I0202 12:27:22.818245 139863983413056 submission_runner.py:408] Time since start: 2834.02s, 	Step: 5457, 	{'train/accuracy': 0.19478514790534973, 'train/loss': 4.121174335479736, 'validation/accuracy': 0.18053999543190002, 'validation/loss': 4.2075347900390625, 'validation/num_examples': 50000, 'test/accuracy': 0.13819999992847443, 'test/loss': 4.630496978759766, 'test/num_examples': 10000, 'score': 2556.0419986248016, 'total_duration': 2834.0153284072876, 'accumulated_submission_time': 2556.0419986248016, 'accumulated_eval_time': 277.5315291881561, 'accumulated_logging_time': 0.15093755722045898}
I0202 12:27:22.836801 139702527031040 logging_writer.py:48] [5457] accumulated_eval_time=277.531529, accumulated_logging_time=0.150938, accumulated_submission_time=2556.041999, global_step=5457, preemption_count=0, score=2556.041999, test/accuracy=0.138200, test/loss=4.630497, test/num_examples=10000, total_duration=2834.015328, train/accuracy=0.194785, train/loss=4.121174, validation/accuracy=0.180540, validation/loss=4.207535, validation/num_examples=50000
I0202 12:27:40.421748 139702543816448 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8130438923835754, loss=5.106575012207031
I0202 12:28:24.597923 139702527031040 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6923523545265198, loss=4.989982604980469
I0202 12:29:10.837646 139702543816448 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7521790862083435, loss=6.021505355834961
I0202 12:29:57.108753 139702527031040 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8681135177612305, loss=4.872824668884277
I0202 12:30:43.271786 139702543816448 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.79527348279953, loss=5.381453037261963
I0202 12:31:29.432021 139702527031040 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6507522463798523, loss=4.912208557128906
I0202 12:32:15.489289 139702543816448 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.868328869342804, loss=4.7050461769104
I0202 12:33:01.497335 139702527031040 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.776151716709137, loss=6.2213215827941895
I0202 12:33:47.577725 139702543816448 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6938967108726501, loss=4.670645713806152
I0202 12:34:23.020408 139863983413056 spec.py:321] Evaluating on the training split.
I0202 12:34:33.645586 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 12:35:01.128959 139863983413056 spec.py:349] Evaluating on the test split.
I0202 12:35:02.774278 139863983413056 submission_runner.py:408] Time since start: 3293.97s, 	Step: 6379, 	{'train/accuracy': 0.22484374046325684, 'train/loss': 3.9965386390686035, 'validation/accuracy': 0.20787999033927917, 'validation/loss': 4.081354141235352, 'validation/num_examples': 50000, 'test/accuracy': 0.15680000185966492, 'test/loss': 4.4948883056640625, 'test/num_examples': 10000, 'score': 2976.1662895679474, 'total_duration': 3293.9713699817657, 'accumulated_submission_time': 2976.1662895679474, 'accumulated_eval_time': 317.28538703918457, 'accumulated_logging_time': 0.17990326881408691}
I0202 12:35:02.795433 139702527031040 logging_writer.py:48] [6379] accumulated_eval_time=317.285387, accumulated_logging_time=0.179903, accumulated_submission_time=2976.166290, global_step=6379, preemption_count=0, score=2976.166290, test/accuracy=0.156800, test/loss=4.494888, test/num_examples=10000, total_duration=3293.971370, train/accuracy=0.224844, train/loss=3.996539, validation/accuracy=0.207880, validation/loss=4.081354, validation/num_examples=50000
I0202 12:35:11.593689 139702543816448 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.764985203742981, loss=4.637851238250732
I0202 12:35:54.334532 139702527031040 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7488972544670105, loss=4.976012706756592
I0202 12:36:40.894944 139702543816448 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6443669199943542, loss=5.005857944488525
I0202 12:37:27.189418 139702527031040 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6457234621047974, loss=6.115891933441162
I0202 12:38:13.465492 139702543816448 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7828369736671448, loss=4.6076531410217285
I0202 12:38:59.510631 139702527031040 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7717591524124146, loss=4.821822166442871
I0202 12:39:45.730634 139702543816448 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7787801027297974, loss=4.553265571594238
I0202 12:40:31.933879 139702527031040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.810637354850769, loss=6.210451126098633
I0202 12:41:18.117650 139702543816448 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7205615043640137, loss=5.562838077545166
I0202 12:42:03.127820 139863983413056 spec.py:321] Evaluating on the training split.
I0202 12:42:13.674487 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 12:42:41.717506 139863983413056 spec.py:349] Evaluating on the test split.
I0202 12:42:43.360674 139863983413056 submission_runner.py:408] Time since start: 3754.56s, 	Step: 7299, 	{'train/accuracy': 0.23093749582767487, 'train/loss': 3.953554630279541, 'validation/accuracy': 0.21237999200820923, 'validation/loss': 4.063230991363525, 'validation/num_examples': 50000, 'test/accuracy': 0.16200000047683716, 'test/loss': 4.486764430999756, 'test/num_examples': 10000, 'score': 3396.437614440918, 'total_duration': 3754.5577280521393, 'accumulated_submission_time': 3396.437614440918, 'accumulated_eval_time': 357.51818895339966, 'accumulated_logging_time': 0.21305513381958008}
I0202 12:42:43.383940 139702527031040 logging_writer.py:48] [7299] accumulated_eval_time=357.518189, accumulated_logging_time=0.213055, accumulated_submission_time=3396.437614, global_step=7299, preemption_count=0, score=3396.437614, test/accuracy=0.162000, test/loss=4.486764, test/num_examples=10000, total_duration=3754.557728, train/accuracy=0.230937, train/loss=3.953555, validation/accuracy=0.212380, validation/loss=4.063231, validation/num_examples=50000
I0202 12:42:44.185019 139702543816448 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9402281045913696, loss=6.352651596069336
I0202 12:43:25.923258 139702527031040 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.672074556350708, loss=6.036479473114014
I0202 12:44:12.600877 139702543816448 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6631049513816833, loss=5.935543060302734
I0202 12:44:58.925916 139702527031040 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8095515966415405, loss=4.807714462280273
I0202 12:45:45.270445 139702543816448 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8499273657798767, loss=4.845102787017822
I0202 12:46:31.320327 139702527031040 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8959536552429199, loss=6.316991329193115
I0202 12:47:17.406001 139702543816448 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8642561435699463, loss=4.95554780960083
I0202 12:48:03.682374 139702527031040 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7494382262229919, loss=4.569912910461426
I0202 12:48:49.697546 139702543816448 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8374708294868469, loss=4.583261013031006
I0202 12:49:36.016543 139702527031040 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.785817563533783, loss=4.586575031280518
I0202 12:49:43.486848 139863983413056 spec.py:321] Evaluating on the training split.
I0202 12:49:54.220292 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 12:50:21.033461 139863983413056 spec.py:349] Evaluating on the test split.
I0202 12:50:22.670375 139863983413056 submission_runner.py:408] Time since start: 4213.87s, 	Step: 8218, 	{'train/accuracy': 0.2625390589237213, 'train/loss': 3.6284167766571045, 'validation/accuracy': 0.22551999986171722, 'validation/loss': 3.8713552951812744, 'validation/num_examples': 50000, 'test/accuracy': 0.17330001294612885, 'test/loss': 4.334794521331787, 'test/num_examples': 10000, 'score': 3816.4799420833588, 'total_duration': 4213.867478132248, 'accumulated_submission_time': 3816.4799420833588, 'accumulated_eval_time': 396.70170760154724, 'accumulated_logging_time': 0.24888205528259277}
I0202 12:50:22.688195 139702543816448 logging_writer.py:48] [8218] accumulated_eval_time=396.701708, accumulated_logging_time=0.248882, accumulated_submission_time=3816.479942, global_step=8218, preemption_count=0, score=3816.479942, test/accuracy=0.173300, test/loss=4.334795, test/num_examples=10000, total_duration=4213.867478, train/accuracy=0.262539, train/loss=3.628417, validation/accuracy=0.225520, validation/loss=3.871355, validation/num_examples=50000
I0202 12:50:56.296296 139702527031040 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.8955356478691101, loss=5.6644158363342285
I0202 12:51:42.199652 139702543816448 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.965713381767273, loss=4.596444606781006
I0202 12:52:28.452968 139702527031040 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7581695914268494, loss=4.853734970092773
I0202 12:53:14.718468 139702543816448 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.835753858089447, loss=4.503652572631836
I0202 12:54:00.568439 139702527031040 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.1039187908172607, loss=4.645851135253906
I0202 12:54:46.663253 139702543816448 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.0359487533569336, loss=4.609296798706055
I0202 12:55:32.396393 139702527031040 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8594492673873901, loss=4.335393905639648
I0202 12:56:18.411897 139702543816448 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.782848060131073, loss=4.50020170211792
I0202 12:57:04.490963 139702527031040 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7520169019699097, loss=4.472762107849121
I0202 12:57:22.971872 139863983413056 spec.py:321] Evaluating on the training split.
I0202 12:57:33.330657 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 12:58:04.440772 139863983413056 spec.py:349] Evaluating on the test split.
I0202 12:58:06.088298 139863983413056 submission_runner.py:408] Time since start: 4677.29s, 	Step: 9142, 	{'train/accuracy': 0.27253904938697815, 'train/loss': 3.545860767364502, 'validation/accuracy': 0.2532599866390228, 'validation/loss': 3.675501585006714, 'validation/num_examples': 50000, 'test/accuracy': 0.1899000108242035, 'test/loss': 4.1776041984558105, 'test/num_examples': 10000, 'score': 4236.704192399979, 'total_duration': 4677.285391569138, 'accumulated_submission_time': 4236.704192399979, 'accumulated_eval_time': 439.81812286376953, 'accumulated_logging_time': 0.2767214775085449}
I0202 12:58:06.106216 139702543816448 logging_writer.py:48] [9142] accumulated_eval_time=439.818123, accumulated_logging_time=0.276721, accumulated_submission_time=4236.704192, global_step=9142, preemption_count=0, score=4236.704192, test/accuracy=0.189900, test/loss=4.177604, test/num_examples=10000, total_duration=4677.285392, train/accuracy=0.272539, train/loss=3.545861, validation/accuracy=0.253260, validation/loss=3.675502, validation/num_examples=50000
I0202 12:58:29.690269 139702527031040 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9473923444747925, loss=4.51999568939209
I0202 12:59:14.698789 139702543816448 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.0630180835723877, loss=4.434875965118408
I0202 13:00:00.861474 139702527031040 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0938321352005005, loss=4.556095123291016
I0202 13:00:47.000755 139702543816448 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8684910535812378, loss=5.128691673278809
I0202 13:01:32.912664 139702527031040 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7108232378959656, loss=5.316350936889648
I0202 13:02:18.857653 139702543816448 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.9474972486495972, loss=4.667666435241699
I0202 13:03:04.953475 139702527031040 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.911955714225769, loss=5.740231513977051
I0202 13:03:50.995535 139702543816448 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8370245099067688, loss=4.4339094161987305
I0202 13:04:37.265575 139702527031040 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7940009236335754, loss=6.247061729431152
I0202 13:05:06.123418 139863983413056 spec.py:321] Evaluating on the training split.
I0202 13:05:16.647154 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 13:05:42.203962 139863983413056 spec.py:349] Evaluating on the test split.
I0202 13:05:43.845041 139863983413056 submission_runner.py:408] Time since start: 5135.04s, 	Step: 10064, 	{'train/accuracy': 0.25773435831069946, 'train/loss': 3.733825206756592, 'validation/accuracy': 0.2406199872493744, 'validation/loss': 3.855492115020752, 'validation/num_examples': 50000, 'test/accuracy': 0.1834000051021576, 'test/loss': 4.324698448181152, 'test/num_examples': 10000, 'score': 4656.660678386688, 'total_duration': 5135.04213309288, 'accumulated_submission_time': 4656.660678386688, 'accumulated_eval_time': 477.5397388935089, 'accumulated_logging_time': 0.3062405586242676}
I0202 13:05:43.861963 139702543816448 logging_writer.py:48] [10064] accumulated_eval_time=477.539739, accumulated_logging_time=0.306241, accumulated_submission_time=4656.660678, global_step=10064, preemption_count=0, score=4656.660678, test/accuracy=0.183400, test/loss=4.324698, test/num_examples=10000, total_duration=5135.042133, train/accuracy=0.257734, train/loss=3.733825, validation/accuracy=0.240620, validation/loss=3.855492, validation/num_examples=50000
I0202 13:05:58.626754 139702527031040 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.8595941066741943, loss=5.763238906860352
I0202 13:06:41.943488 139702543816448 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8305898904800415, loss=4.7467546463012695
I0202 13:07:28.144387 139702527031040 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7892189025878906, loss=5.663271427154541
I0202 13:08:14.319498 139702543816448 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7124142050743103, loss=5.131750106811523
I0202 13:09:00.378959 139702527031040 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8139036297798157, loss=4.54396915435791
I0202 13:09:46.388347 139702543816448 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.905139684677124, loss=4.485304832458496
I0202 13:10:32.549854 139702527031040 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.0894763469696045, loss=4.310498237609863
I0202 13:11:18.489074 139702543816448 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8542043566703796, loss=6.0481719970703125
I0202 13:12:04.400049 139702527031040 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9581426978111267, loss=4.392798900604248
I0202 13:12:43.966228 139863983413056 spec.py:321] Evaluating on the training split.
I0202 13:12:54.537237 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 13:13:21.855750 139863983413056 spec.py:349] Evaluating on the test split.
I0202 13:13:23.506264 139863983413056 submission_runner.py:408] Time since start: 5594.70s, 	Step: 10987, 	{'train/accuracy': 0.27595701813697815, 'train/loss': 3.5516586303710938, 'validation/accuracy': 0.24587999284267426, 'validation/loss': 3.746058225631714, 'validation/num_examples': 50000, 'test/accuracy': 0.19140000641345978, 'test/loss': 4.227960586547852, 'test/num_examples': 10000, 'score': 5076.703187704086, 'total_duration': 5594.703353404999, 'accumulated_submission_time': 5076.703187704086, 'accumulated_eval_time': 517.0797474384308, 'accumulated_logging_time': 0.33579182624816895}
I0202 13:13:23.523874 139702543816448 logging_writer.py:48] [10987] accumulated_eval_time=517.079747, accumulated_logging_time=0.335792, accumulated_submission_time=5076.703188, global_step=10987, preemption_count=0, score=5076.703188, test/accuracy=0.191400, test/loss=4.227961, test/num_examples=10000, total_duration=5594.703353, train/accuracy=0.275957, train/loss=3.551659, validation/accuracy=0.245880, validation/loss=3.746058, validation/num_examples=50000
I0202 13:13:29.107913 139702527031040 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8771808743476868, loss=4.470302581787109
I0202 13:14:11.500374 139702543816448 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.0196194648742676, loss=5.496303558349609
I0202 13:14:57.608461 139702527031040 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9724316000938416, loss=4.352787017822266
I0202 13:15:44.021006 139702543816448 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5866622924804688, loss=5.753744602203369
I0202 13:16:30.021432 139702527031040 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0181857347488403, loss=4.574311256408691
I0202 13:17:16.182183 139702543816448 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6971254348754883, loss=5.879260540008545
I0202 13:18:02.161823 139702527031040 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7601763606071472, loss=5.061145782470703
I0202 13:18:48.198490 139702543816448 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.9356080293655396, loss=4.755903720855713
I0202 13:19:34.373685 139702527031040 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9186021685600281, loss=4.273209095001221
I0202 13:20:20.689938 139702543816448 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7908354997634888, loss=4.158857822418213
I0202 13:20:23.597845 139863983413056 spec.py:321] Evaluating on the training split.
I0202 13:20:34.377444 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 13:21:00.307825 139863983413056 spec.py:349] Evaluating on the test split.
I0202 13:21:01.953761 139863983413056 submission_runner.py:408] Time since start: 6053.15s, 	Step: 11908, 	{'train/accuracy': 0.28855466842651367, 'train/loss': 3.454838752746582, 'validation/accuracy': 0.2699199914932251, 'validation/loss': 3.5662355422973633, 'validation/num_examples': 50000, 'test/accuracy': 0.20250001549720764, 'test/loss': 4.11765718460083, 'test/num_examples': 10000, 'score': 5496.717879772186, 'total_duration': 6053.150849819183, 'accumulated_submission_time': 5496.717879772186, 'accumulated_eval_time': 555.4356439113617, 'accumulated_logging_time': 0.3640127182006836}
I0202 13:21:01.974775 139702527031040 logging_writer.py:48] [11908] accumulated_eval_time=555.435644, accumulated_logging_time=0.364013, accumulated_submission_time=5496.717880, global_step=11908, preemption_count=0, score=5496.717880, test/accuracy=0.202500, test/loss=4.117657, test/num_examples=10000, total_duration=6053.150850, train/accuracy=0.288555, train/loss=3.454839, validation/accuracy=0.269920, validation/loss=3.566236, validation/num_examples=50000
I0202 13:21:40.202301 139702543816448 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8410575985908508, loss=4.857517719268799
I0202 13:22:26.056259 139702527031040 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.8294625878334045, loss=4.148814678192139
I0202 13:23:12.447835 139702543816448 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8269412517547607, loss=4.468865394592285
I0202 13:23:58.573346 139702527031040 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.0308820009231567, loss=4.318445682525635
I0202 13:24:44.746640 139702543816448 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.1441874504089355, loss=4.410031318664551
I0202 13:25:31.243277 139702527031040 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.719523549079895, loss=4.594873905181885
I0202 13:26:17.450324 139702543816448 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.8435115814208984, loss=4.321447372436523
I0202 13:27:03.702023 139702527031040 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.008525013923645, loss=4.272442817687988
I0202 13:27:49.906275 139702543816448 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.8457686901092529, loss=4.225842475891113
I0202 13:28:02.115159 139863983413056 spec.py:321] Evaluating on the training split.
I0202 13:28:12.504945 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 13:28:37.763302 139863983413056 spec.py:349] Evaluating on the test split.
I0202 13:28:39.398370 139863983413056 submission_runner.py:408] Time since start: 6510.60s, 	Step: 12828, 	{'train/accuracy': 0.3011523485183716, 'train/loss': 3.359318256378174, 'validation/accuracy': 0.2775000035762787, 'validation/loss': 3.4931159019470215, 'validation/num_examples': 50000, 'test/accuracy': 0.21570001542568207, 'test/loss': 4.02402925491333, 'test/num_examples': 10000, 'score': 5916.797674417496, 'total_duration': 6510.595469713211, 'accumulated_submission_time': 5916.797674417496, 'accumulated_eval_time': 592.7188277244568, 'accumulated_logging_time': 0.3962678909301758}
I0202 13:28:39.416181 139702527031040 logging_writer.py:48] [12828] accumulated_eval_time=592.718828, accumulated_logging_time=0.396268, accumulated_submission_time=5916.797674, global_step=12828, preemption_count=0, score=5916.797674, test/accuracy=0.215700, test/loss=4.024029, test/num_examples=10000, total_duration=6510.595470, train/accuracy=0.301152, train/loss=3.359318, validation/accuracy=0.277500, validation/loss=3.493116, validation/num_examples=50000
I0202 13:29:08.567939 139702543816448 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9495214223861694, loss=4.323539733886719
I0202 13:29:54.309374 139702527031040 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7059480547904968, loss=4.634171962738037
I0202 13:30:40.465248 139702543816448 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.0048367977142334, loss=4.618791103363037
I0202 13:31:26.624959 139702527031040 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.873649001121521, loss=4.143475532531738
I0202 13:32:12.565575 139702543816448 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9573612213134766, loss=4.375027656555176
I0202 13:32:58.676991 139702527031040 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0946893692016602, loss=5.692624092102051
I0202 13:33:44.997414 139702543816448 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.911392331123352, loss=5.521785736083984
I0202 13:34:31.086720 139702527031040 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8388561606407166, loss=4.337480545043945
I0202 13:35:17.388070 139702543816448 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.8019886612892151, loss=4.238554954528809
I0202 13:35:39.468572 139863983413056 spec.py:321] Evaluating on the training split.
I0202 13:35:50.054321 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 13:36:18.697203 139863983413056 spec.py:349] Evaluating on the test split.
I0202 13:36:20.334691 139863983413056 submission_runner.py:408] Time since start: 6971.53s, 	Step: 13749, 	{'train/accuracy': 0.3214453160762787, 'train/loss': 3.2423040866851807, 'validation/accuracy': 0.28751999139785767, 'validation/loss': 3.422166109085083, 'validation/num_examples': 50000, 'test/accuracy': 0.22340001165866852, 'test/loss': 3.946427583694458, 'test/num_examples': 10000, 'score': 6336.788841247559, 'total_duration': 6971.531793117523, 'accumulated_submission_time': 6336.788841247559, 'accumulated_eval_time': 633.5849332809448, 'accumulated_logging_time': 0.42644309997558594}
I0202 13:36:20.354780 139702527031040 logging_writer.py:48] [13749] accumulated_eval_time=633.584933, accumulated_logging_time=0.426443, accumulated_submission_time=6336.788841, global_step=13749, preemption_count=0, score=6336.788841, test/accuracy=0.223400, test/loss=3.946428, test/num_examples=10000, total_duration=6971.531793, train/accuracy=0.321445, train/loss=3.242304, validation/accuracy=0.287520, validation/loss=3.422166, validation/num_examples=50000
I0202 13:36:41.141084 139702543816448 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5854787230491638, loss=6.067930221557617
I0202 13:37:25.237903 139702527031040 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.9342197179794312, loss=4.947025775909424
I0202 13:38:10.932171 139702543816448 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.8269967436790466, loss=4.361880302429199
I0202 13:38:56.997427 139702527031040 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.124159574508667, loss=4.223669528961182
I0202 13:39:43.084487 139702543816448 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.8229771852493286, loss=4.545106887817383
I0202 13:40:29.528450 139702527031040 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.1271770000457764, loss=4.306148052215576
I0202 13:41:15.208827 139702543816448 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7551562786102295, loss=4.980468273162842
I0202 13:42:01.006953 139702527031040 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9487071633338928, loss=4.214694976806641
I0202 13:42:46.930224 139702543816448 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0866789817810059, loss=5.161400318145752
I0202 13:43:20.387455 139863983413056 spec.py:321] Evaluating on the training split.
I0202 13:43:30.719215 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 13:43:55.694248 139863983413056 spec.py:349] Evaluating on the test split.
I0202 13:43:57.329109 139863983413056 submission_runner.py:408] Time since start: 7428.53s, 	Step: 14674, 	{'train/accuracy': 0.3084179759025574, 'train/loss': 3.319817066192627, 'validation/accuracy': 0.28895998001098633, 'validation/loss': 3.429847478866577, 'validation/num_examples': 50000, 'test/accuracy': 0.2208000123500824, 'test/loss': 3.9683961868286133, 'test/num_examples': 10000, 'score': 6756.758174657822, 'total_duration': 7428.526203870773, 'accumulated_submission_time': 6756.758174657822, 'accumulated_eval_time': 670.5265691280365, 'accumulated_logging_time': 0.46064305305480957}
I0202 13:43:57.348007 139702527031040 logging_writer.py:48] [14674] accumulated_eval_time=670.526569, accumulated_logging_time=0.460643, accumulated_submission_time=6756.758175, global_step=14674, preemption_count=0, score=6756.758175, test/accuracy=0.220800, test/loss=3.968396, test/num_examples=10000, total_duration=7428.526204, train/accuracy=0.308418, train/loss=3.319817, validation/accuracy=0.288960, validation/loss=3.429847, validation/num_examples=50000
I0202 13:44:08.124097 139702543816448 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9265987873077393, loss=6.211450576782227
I0202 13:44:50.993233 139702527031040 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.844987154006958, loss=4.150547981262207
I0202 13:45:36.882382 139702543816448 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.9920761585235596, loss=4.568625450134277
I0202 13:46:23.033385 139702527031040 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7288442850112915, loss=6.096277236938477
I0202 13:47:08.804396 139702543816448 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.921166718006134, loss=4.398562431335449
I0202 13:47:54.614218 139702527031040 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.2224328517913818, loss=4.225708484649658
I0202 13:48:40.760987 139702543816448 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9720394611358643, loss=4.222898960113525
I0202 13:49:26.966095 139702527031040 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.9732195138931274, loss=4.21353816986084
I0202 13:50:13.619403 139702543816448 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.805618941783905, loss=5.947927474975586
I0202 13:50:57.549341 139863983413056 spec.py:321] Evaluating on the training split.
I0202 13:51:08.016021 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 13:51:38.053533 139863983413056 spec.py:349] Evaluating on the test split.
I0202 13:51:39.686644 139863983413056 submission_runner.py:408] Time since start: 7890.88s, 	Step: 15597, 	{'train/accuracy': 0.31492185592651367, 'train/loss': 3.302459239959717, 'validation/accuracy': 0.2924000024795532, 'validation/loss': 3.4336822032928467, 'validation/num_examples': 50000, 'test/accuracy': 0.22180001437664032, 'test/loss': 3.960664987564087, 'test/num_examples': 10000, 'score': 7176.900419473648, 'total_duration': 7890.88374376297, 'accumulated_submission_time': 7176.900419473648, 'accumulated_eval_time': 712.6638605594635, 'accumulated_logging_time': 0.490314245223999}
I0202 13:51:39.704452 139702527031040 logging_writer.py:48] [15597] accumulated_eval_time=712.663861, accumulated_logging_time=0.490314, accumulated_submission_time=7176.900419, global_step=15597, preemption_count=0, score=7176.900419, test/accuracy=0.221800, test/loss=3.960665, test/num_examples=10000, total_duration=7890.883744, train/accuracy=0.314922, train/loss=3.302459, validation/accuracy=0.292400, validation/loss=3.433682, validation/num_examples=50000
I0202 13:51:41.309278 139702543816448 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.159125566482544, loss=4.67448616027832
I0202 13:52:22.899625 139702527031040 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9667463898658752, loss=4.297616004943848
I0202 13:53:09.078905 139702543816448 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.8667739033699036, loss=4.260015964508057
I0202 13:53:55.067754 139702527031040 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.7916175723075867, loss=4.671741008758545
I0202 13:54:41.383741 139702543816448 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7968276739120483, loss=4.5202507972717285
I0202 13:55:27.742832 139702527031040 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7467046976089478, loss=4.576982498168945
I0202 13:56:14.046230 139702543816448 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.8080392479896545, loss=5.301507472991943
I0202 13:57:00.325450 139702527031040 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9252340197563171, loss=4.201272010803223
I0202 13:57:46.486011 139702543816448 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.805381178855896, loss=6.135560035705566
I0202 13:58:32.500187 139702527031040 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9210094809532166, loss=4.235541343688965
I0202 13:58:40.121339 139863983413056 spec.py:321] Evaluating on the training split.
I0202 13:58:50.541071 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 13:59:15.720568 139863983413056 spec.py:349] Evaluating on the test split.
I0202 13:59:17.360056 139863983413056 submission_runner.py:408] Time since start: 8348.56s, 	Step: 16518, 	{'train/accuracy': 0.3135937452316284, 'train/loss': 3.3022851943969727, 'validation/accuracy': 0.28995999693870544, 'validation/loss': 3.4567174911499023, 'validation/num_examples': 50000, 'test/accuracy': 0.21640001237392426, 'test/loss': 4.005506992340088, 'test/num_examples': 10000, 'score': 7597.258638143539, 'total_duration': 8348.557153463364, 'accumulated_submission_time': 7597.258638143539, 'accumulated_eval_time': 749.902556180954, 'accumulated_logging_time': 0.5184402465820312}
I0202 13:59:17.378360 139702543816448 logging_writer.py:48] [16518] accumulated_eval_time=749.902556, accumulated_logging_time=0.518440, accumulated_submission_time=7597.258638, global_step=16518, preemption_count=0, score=7597.258638, test/accuracy=0.216400, test/loss=4.005507, test/num_examples=10000, total_duration=8348.557153, train/accuracy=0.313594, train/loss=3.302285, validation/accuracy=0.289960, validation/loss=3.456717, validation/num_examples=50000
I0202 13:59:51.094633 139702527031040 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.7864179611206055, loss=5.960054397583008
I0202 14:00:37.174272 139702543816448 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.7931481003761292, loss=5.866457939147949
I0202 14:01:23.393654 139702527031040 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.9514481425285339, loss=4.484777450561523
I0202 14:02:09.295476 139702543816448 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.9207645654678345, loss=6.028582572937012
I0202 14:02:55.190910 139702527031040 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.813728928565979, loss=4.160599708557129
I0202 14:03:41.222306 139702543816448 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.8942364454269409, loss=4.1791253089904785
I0202 14:04:27.102300 139702527031040 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.8680948615074158, loss=4.229104995727539
I0202 14:05:13.001808 139702543816448 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0974801778793335, loss=4.273540019989014
I0202 14:05:59.182836 139702527031040 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.8211893439292908, loss=4.03405237197876
I0202 14:06:17.609874 139863983413056 spec.py:321] Evaluating on the training split.
I0202 14:06:29.112382 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 14:06:56.815767 139863983413056 spec.py:349] Evaluating on the test split.
I0202 14:06:58.462723 139863983413056 submission_runner.py:408] Time since start: 8809.66s, 	Step: 17442, 	{'train/accuracy': 0.34935545921325684, 'train/loss': 3.045586585998535, 'validation/accuracy': 0.311519980430603, 'validation/loss': 3.2577993869781494, 'validation/num_examples': 50000, 'test/accuracy': 0.24160000681877136, 'test/loss': 3.8327126502990723, 'test/num_examples': 10000, 'score': 8017.4315502643585, 'total_duration': 8809.659813642502, 'accumulated_submission_time': 8017.4315502643585, 'accumulated_eval_time': 790.755383014679, 'accumulated_logging_time': 0.5460658073425293}
I0202 14:06:58.481108 139702543816448 logging_writer.py:48] [17442] accumulated_eval_time=790.755383, accumulated_logging_time=0.546066, accumulated_submission_time=8017.431550, global_step=17442, preemption_count=0, score=8017.431550, test/accuracy=0.241600, test/loss=3.832713, test/num_examples=10000, total_duration=8809.659814, train/accuracy=0.349355, train/loss=3.045587, validation/accuracy=0.311520, validation/loss=3.257799, validation/num_examples=50000
I0202 14:07:22.050408 139702527031040 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.0053290128707886, loss=4.205556869506836
I0202 14:08:07.000667 139702543816448 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.9670931696891785, loss=4.557013034820557
I0202 14:08:52.577251 139702527031040 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9283017516136169, loss=4.416618347167969
I0202 14:09:38.828826 139702543816448 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.9467746615409851, loss=4.203639984130859
I0202 14:10:25.077436 139702527031040 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.9623034000396729, loss=4.168286323547363
I0202 14:11:10.984158 139702543816448 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.8622223734855652, loss=4.260438919067383
I0202 14:11:56.978283 139702527031040 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9062119126319885, loss=3.990424871444702
I0202 14:12:43.069116 139702543816448 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8283354640007019, loss=5.085024833679199
I0202 14:13:29.490135 139702527031040 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.9255688190460205, loss=4.130565643310547
I0202 14:13:58.621120 139863983413056 spec.py:321] Evaluating on the training split.
I0202 14:14:09.430846 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 14:14:37.121141 139863983413056 spec.py:349] Evaluating on the test split.
I0202 14:14:38.761387 139863983413056 submission_runner.py:408] Time since start: 9269.96s, 	Step: 18365, 	{'train/accuracy': 0.33476561307907104, 'train/loss': 3.173603057861328, 'validation/accuracy': 0.3100999891757965, 'validation/loss': 3.299381732940674, 'validation/num_examples': 50000, 'test/accuracy': 0.2379000186920166, 'test/loss': 3.853330612182617, 'test/num_examples': 10000, 'score': 8437.51029419899, 'total_duration': 9269.958486557007, 'accumulated_submission_time': 8437.51029419899, 'accumulated_eval_time': 830.8956499099731, 'accumulated_logging_time': 0.5777482986450195}
I0202 14:14:38.779992 139702543816448 logging_writer.py:48] [18365] accumulated_eval_time=830.895650, accumulated_logging_time=0.577748, accumulated_submission_time=8437.510294, global_step=18365, preemption_count=0, score=8437.510294, test/accuracy=0.237900, test/loss=3.853331, test/num_examples=10000, total_duration=9269.958487, train/accuracy=0.334766, train/loss=3.173603, validation/accuracy=0.310100, validation/loss=3.299382, validation/num_examples=50000
I0202 14:14:53.158125 139702527031040 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.9865732789039612, loss=4.405195236206055
I0202 14:15:36.726531 139702543816448 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.8120871186256409, loss=4.050954341888428
I0202 14:16:23.070553 139702527031040 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8563953042030334, loss=4.238134860992432
I0202 14:17:08.954670 139702543816448 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.8327959775924683, loss=4.182830810546875
I0202 14:17:55.415495 139702527031040 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.8790367841720581, loss=4.600828170776367
I0202 14:18:41.383275 139702543816448 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.0515174865722656, loss=4.010654926300049
I0202 14:19:27.527787 139702527031040 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7927196621894836, loss=5.988415718078613
I0202 14:20:14.034072 139702543816448 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.9412006735801697, loss=3.9469003677368164
I0202 14:20:59.974576 139702527031040 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9848600625991821, loss=4.0766801834106445
I0202 14:21:38.799536 139863983413056 spec.py:321] Evaluating on the training split.
I0202 14:21:49.287997 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 14:22:13.816356 139863983413056 spec.py:349] Evaluating on the test split.
I0202 14:22:15.462859 139863983413056 submission_runner.py:408] Time since start: 9726.66s, 	Step: 19286, 	{'train/accuracy': 0.328125, 'train/loss': 3.206876516342163, 'validation/accuracy': 0.30417999625205994, 'validation/loss': 3.352670431137085, 'validation/num_examples': 50000, 'test/accuracy': 0.234700009226799, 'test/loss': 3.8799962997436523, 'test/num_examples': 10000, 'score': 8857.471648454666, 'total_duration': 9726.6599547863, 'accumulated_submission_time': 8857.471648454666, 'accumulated_eval_time': 867.5589497089386, 'accumulated_logging_time': 0.6066219806671143}
I0202 14:22:15.481509 139702543816448 logging_writer.py:48] [19286] accumulated_eval_time=867.558950, accumulated_logging_time=0.606622, accumulated_submission_time=8857.471648, global_step=19286, preemption_count=0, score=8857.471648, test/accuracy=0.234700, test/loss=3.879996, test/num_examples=10000, total_duration=9726.659955, train/accuracy=0.328125, train/loss=3.206877, validation/accuracy=0.304180, validation/loss=3.352670, validation/num_examples=50000
I0202 14:22:21.470900 139702527031040 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7625678181648254, loss=4.6840901374816895
I0202 14:23:03.707125 139702543816448 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9837219715118408, loss=4.136518478393555
I0202 14:23:49.554755 139702527031040 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.883416473865509, loss=5.373471736907959
I0202 14:24:35.550060 139702543816448 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.0220695734024048, loss=5.516304016113281
I0202 14:25:21.658575 139702527031040 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.8592907786369324, loss=4.119453430175781
I0202 14:26:07.755668 139702543816448 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8954810500144958, loss=4.019302845001221
I0202 14:26:53.913552 139702527031040 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.8233185410499573, loss=4.089097499847412
I0202 14:27:40.041244 139702543816448 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9461005330085754, loss=4.116623878479004
I0202 14:28:26.358310 139702527031040 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.9503924250602722, loss=4.099891185760498
I0202 14:29:12.380073 139702543816448 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8989408016204834, loss=5.2086873054504395
I0202 14:29:15.698442 139863983413056 spec.py:321] Evaluating on the training split.
I0202 14:29:26.102655 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 14:29:57.305465 139863983413056 spec.py:349] Evaluating on the test split.
I0202 14:29:58.946895 139863983413056 submission_runner.py:408] Time since start: 10190.14s, 	Step: 20209, 	{'train/accuracy': 0.3524218797683716, 'train/loss': 3.0684006214141846, 'validation/accuracy': 0.311379998922348, 'validation/loss': 3.3076298236846924, 'validation/num_examples': 50000, 'test/accuracy': 0.2412000149488449, 'test/loss': 3.833801746368408, 'test/num_examples': 10000, 'score': 9277.629351854324, 'total_duration': 10190.143986225128, 'accumulated_submission_time': 9277.629351854324, 'accumulated_eval_time': 910.8073680400848, 'accumulated_logging_time': 0.6355025768280029}
I0202 14:29:58.966396 139702527031040 logging_writer.py:48] [20209] accumulated_eval_time=910.807368, accumulated_logging_time=0.635503, accumulated_submission_time=9277.629352, global_step=20209, preemption_count=0, score=9277.629352, test/accuracy=0.241200, test/loss=3.833802, test/num_examples=10000, total_duration=10190.143986, train/accuracy=0.352422, train/loss=3.068401, validation/accuracy=0.311380, validation/loss=3.307630, validation/num_examples=50000
I0202 14:30:36.946759 139702543816448 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8192636370658875, loss=5.094793796539307
I0202 14:31:22.701035 139702527031040 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7977713942527771, loss=3.872389316558838
I0202 14:32:08.779495 139702543816448 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7987619638442993, loss=4.486496448516846
I0202 14:32:54.524300 139702527031040 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7518908381462097, loss=4.13431453704834
I0202 14:33:40.662396 139702543816448 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.0603357553482056, loss=4.0178117752075195
I0202 14:34:26.874581 139702527031040 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.9539233446121216, loss=4.708803176879883
I0202 14:35:12.978512 139702543816448 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7929478287696838, loss=4.043827533721924
I0202 14:35:59.131034 139702527031040 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6902105212211609, loss=5.600911617279053
I0202 14:36:45.450821 139702543816448 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.9973850250244141, loss=4.175238132476807
I0202 14:36:59.032371 139863983413056 spec.py:321] Evaluating on the training split.
I0202 14:37:09.660833 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 14:37:42.327377 139863983413056 spec.py:349] Evaluating on the test split.
I0202 14:37:43.970182 139863983413056 submission_runner.py:408] Time since start: 10655.17s, 	Step: 21131, 	{'train/accuracy': 0.3587304651737213, 'train/loss': 3.0196619033813477, 'validation/accuracy': 0.33785998821258545, 'validation/loss': 3.1473286151885986, 'validation/num_examples': 50000, 'test/accuracy': 0.25600001215934753, 'test/loss': 3.7276759147644043, 'test/num_examples': 10000, 'score': 9697.635033369064, 'total_duration': 10655.167280435562, 'accumulated_submission_time': 9697.635033369064, 'accumulated_eval_time': 955.7451527118683, 'accumulated_logging_time': 0.6662991046905518}
I0202 14:37:43.994103 139702527031040 logging_writer.py:48] [21131] accumulated_eval_time=955.745153, accumulated_logging_time=0.666299, accumulated_submission_time=9697.635033, global_step=21131, preemption_count=0, score=9697.635033, test/accuracy=0.256000, test/loss=3.727676, test/num_examples=10000, total_duration=10655.167280, train/accuracy=0.358730, train/loss=3.019662, validation/accuracy=0.337860, validation/loss=3.147329, validation/num_examples=50000
I0202 14:38:11.964637 139702543816448 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6821924448013306, loss=5.851603984832764
I0202 14:38:57.532773 139702527031040 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.9449408650398254, loss=4.027190685272217
I0202 14:39:43.343945 139702543816448 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8249884843826294, loss=4.244356155395508
I0202 14:40:29.911112 139702527031040 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8369321823120117, loss=4.383904933929443
I0202 14:41:16.182903 139702543816448 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7783768773078918, loss=4.531444072723389
I0202 14:42:02.359423 139702527031040 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6522330641746521, loss=6.04488468170166
I0202 14:42:48.438743 139702543816448 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6784818172454834, loss=6.080042839050293
I0202 14:43:34.607501 139702527031040 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.8285995721817017, loss=4.0380120277404785
I0202 14:44:20.764631 139702543816448 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7861215472221375, loss=4.068595886230469
I0202 14:44:44.230754 139863983413056 spec.py:321] Evaluating on the training split.
I0202 14:44:54.653496 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 14:45:21.935832 139863983413056 spec.py:349] Evaluating on the test split.
I0202 14:45:23.574946 139863983413056 submission_runner.py:408] Time since start: 11114.77s, 	Step: 22053, 	{'train/accuracy': 0.34773436188697815, 'train/loss': 3.0713601112365723, 'validation/accuracy': 0.3216799795627594, 'validation/loss': 3.2210161685943604, 'validation/num_examples': 50000, 'test/accuracy': 0.2412000149488449, 'test/loss': 3.8103978633880615, 'test/num_examples': 10000, 'score': 10117.811880588531, 'total_duration': 11114.772018909454, 'accumulated_submission_time': 10117.811880588531, 'accumulated_eval_time': 995.0893120765686, 'accumulated_logging_time': 0.700777530670166}
I0202 14:45:23.600142 139702527031040 logging_writer.py:48] [22053] accumulated_eval_time=995.089312, accumulated_logging_time=0.700778, accumulated_submission_time=10117.811881, global_step=22053, preemption_count=0, score=10117.811881, test/accuracy=0.241200, test/loss=3.810398, test/num_examples=10000, total_duration=11114.772019, train/accuracy=0.347734, train/loss=3.071360, validation/accuracy=0.321680, validation/loss=3.221016, validation/num_examples=50000
I0202 14:45:42.783010 139702543816448 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.986849308013916, loss=4.0250678062438965
I0202 14:46:27.162122 139702527031040 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7167060375213623, loss=5.798017978668213
I0202 14:47:13.316921 139702543816448 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.8837645053863525, loss=4.107908725738525
I0202 14:47:59.250592 139702527031040 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.8997026085853577, loss=3.953582286834717
I0202 14:48:45.283910 139702543816448 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8809871077537537, loss=5.759507179260254
I0202 14:49:31.467193 139702527031040 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.8585108518600464, loss=4.03073263168335
I0202 14:50:17.302886 139702543816448 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9193844795227051, loss=4.715344429016113
I0202 14:51:03.462082 139702527031040 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0225772857666016, loss=4.200569152832031
I0202 14:51:49.616420 139702543816448 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.0498298406600952, loss=3.9357755184173584
I0202 14:52:23.940297 139863983413056 spec.py:321] Evaluating on the training split.
I0202 14:52:34.630418 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 14:53:03.202239 139863983413056 spec.py:349] Evaluating on the test split.
I0202 14:53:04.843092 139863983413056 submission_runner.py:408] Time since start: 11576.04s, 	Step: 22976, 	{'train/accuracy': 0.3622460961341858, 'train/loss': 2.9715335369110107, 'validation/accuracy': 0.3338399827480316, 'validation/loss': 3.14302396774292, 'validation/num_examples': 50000, 'test/accuracy': 0.25540000200271606, 'test/loss': 3.7081446647644043, 'test/num_examples': 10000, 'score': 10538.091529846191, 'total_duration': 11576.040191173553, 'accumulated_submission_time': 10538.091529846191, 'accumulated_eval_time': 1035.9921073913574, 'accumulated_logging_time': 0.7372596263885498}
I0202 14:53:04.865750 139702527031040 logging_writer.py:48] [22976] accumulated_eval_time=1035.992107, accumulated_logging_time=0.737260, accumulated_submission_time=10538.091530, global_step=22976, preemption_count=0, score=10538.091530, test/accuracy=0.255400, test/loss=3.708145, test/num_examples=10000, total_duration=11576.040191, train/accuracy=0.362246, train/loss=2.971534, validation/accuracy=0.333840, validation/loss=3.143024, validation/num_examples=50000
I0202 14:53:14.852651 139702543816448 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7916836738586426, loss=4.0509538650512695
I0202 14:53:57.651238 139702527031040 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0379369258880615, loss=4.0291972160339355
I0202 14:54:43.655442 139702543816448 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.9063907861709595, loss=4.099341869354248
I0202 14:55:29.635158 139702527031040 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.069555640220642, loss=4.088879585266113
I0202 14:56:15.630108 139702543816448 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.0253239870071411, loss=4.100215911865234
I0202 14:57:01.516131 139702527031040 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8111092448234558, loss=4.059859275817871
I0202 14:57:47.596827 139702543816448 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.0522133111953735, loss=5.0386505126953125
I0202 14:58:33.694052 139702527031040 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.9270645380020142, loss=4.01908540725708
I0202 14:59:20.173886 139702543816448 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.9634166955947876, loss=6.018855094909668
I0202 15:00:05.120213 139863983413056 spec.py:321] Evaluating on the training split.
I0202 15:00:15.489405 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 15:00:43.277447 139863983413056 spec.py:349] Evaluating on the test split.
I0202 15:00:44.923552 139863983413056 submission_runner.py:408] Time since start: 12036.12s, 	Step: 23900, 	{'train/accuracy': 0.34919920563697815, 'train/loss': 3.0883233547210693, 'validation/accuracy': 0.3263799846172333, 'validation/loss': 3.207261085510254, 'validation/num_examples': 50000, 'test/accuracy': 0.25140002369880676, 'test/loss': 3.753178596496582, 'test/num_examples': 10000, 'score': 10958.286712884903, 'total_duration': 12036.120604991913, 'accumulated_submission_time': 10958.286712884903, 'accumulated_eval_time': 1075.7954070568085, 'accumulated_logging_time': 0.7705366611480713}
I0202 15:00:44.945429 139702527031040 logging_writer.py:48] [23900] accumulated_eval_time=1075.795407, accumulated_logging_time=0.770537, accumulated_submission_time=10958.286713, global_step=23900, preemption_count=0, score=10958.286713, test/accuracy=0.251400, test/loss=3.753179, test/num_examples=10000, total_duration=12036.120605, train/accuracy=0.349199, train/loss=3.088323, validation/accuracy=0.326380, validation/loss=3.207261, validation/num_examples=50000
I0202 15:00:45.350611 139702543816448 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8846330642700195, loss=4.163837432861328
I0202 15:01:26.751216 139702527031040 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9614909887313843, loss=4.497766971588135
I0202 15:02:12.728553 139702543816448 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.9810447096824646, loss=4.156996726989746
I0202 15:02:58.889673 139702527031040 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.8607525825500488, loss=5.305513381958008
I0202 15:03:44.855056 139702543816448 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.8368075489997864, loss=4.224048614501953
I0202 15:04:30.911014 139702527031040 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7695536613464355, loss=5.710073947906494
I0202 15:05:16.695316 139702543816448 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.9170611500740051, loss=3.956301689147949
I0202 15:06:02.608504 139702527031040 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.0406744480133057, loss=4.519926071166992
I0202 15:06:48.566165 139702543816448 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.8033859133720398, loss=5.231401443481445
I0202 15:07:34.511210 139702527031040 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9548317193984985, loss=3.985670566558838
I0202 15:07:45.268277 139863983413056 spec.py:321] Evaluating on the training split.
I0202 15:07:55.541087 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 15:08:25.940819 139863983413056 spec.py:349] Evaluating on the test split.
I0202 15:08:27.580415 139863983413056 submission_runner.py:408] Time since start: 12498.78s, 	Step: 24825, 	{'train/accuracy': 0.3492382764816284, 'train/loss': 3.0505244731903076, 'validation/accuracy': 0.32909998297691345, 'validation/loss': 3.18064284324646, 'validation/num_examples': 50000, 'test/accuracy': 0.24990001320838928, 'test/loss': 3.7398154735565186, 'test/num_examples': 10000, 'score': 11378.547968149185, 'total_duration': 12498.776052236557, 'accumulated_submission_time': 11378.547968149185, 'accumulated_eval_time': 1118.106077671051, 'accumulated_logging_time': 0.8033137321472168}
I0202 15:08:27.600209 139702543816448 logging_writer.py:48] [24825] accumulated_eval_time=1118.106078, accumulated_logging_time=0.803314, accumulated_submission_time=11378.547968, global_step=24825, preemption_count=0, score=11378.547968, test/accuracy=0.249900, test/loss=3.739815, test/num_examples=10000, total_duration=12498.776052, train/accuracy=0.349238, train/loss=3.050524, validation/accuracy=0.329100, validation/loss=3.180643, validation/num_examples=50000
I0202 15:08:57.996934 139702527031040 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6395668387413025, loss=5.988489151000977
I0202 15:09:43.913812 139702543816448 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.9782079458236694, loss=3.999507427215576
I0202 15:10:30.569805 139702527031040 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.9238240122795105, loss=4.341541290283203
I0202 15:11:16.651217 139702543816448 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.964221179485321, loss=4.0083112716674805
I0202 15:12:02.895237 139702527031040 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.9190958142280579, loss=3.8811120986938477
I0202 15:12:49.132241 139702543816448 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.8362177610397339, loss=5.645938873291016
I0202 15:13:34.997443 139702527031040 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.7427529692649841, loss=5.176154136657715
I0202 15:14:21.075613 139702543816448 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.783673882484436, loss=4.935534954071045
I0202 15:15:06.947194 139702527031040 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8206584453582764, loss=4.795205116271973
I0202 15:15:27.602994 139863983413056 spec.py:321] Evaluating on the training split.
I0202 15:15:37.962861 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 15:16:02.507135 139863983413056 spec.py:349] Evaluating on the test split.
I0202 15:16:04.154454 139863983413056 submission_runner.py:408] Time since start: 12955.35s, 	Step: 25746, 	{'train/accuracy': 0.36884763836860657, 'train/loss': 2.933948516845703, 'validation/accuracy': 0.33541998267173767, 'validation/loss': 3.1056737899780273, 'validation/num_examples': 50000, 'test/accuracy': 0.255700021982193, 'test/loss': 3.705024480819702, 'test/num_examples': 10000, 'score': 11798.491114139557, 'total_duration': 12955.351569652557, 'accumulated_submission_time': 11798.491114139557, 'accumulated_eval_time': 1154.6575469970703, 'accumulated_logging_time': 0.8338415622711182}
I0202 15:16:04.179187 139702543816448 logging_writer.py:48] [25746] accumulated_eval_time=1154.657547, accumulated_logging_time=0.833842, accumulated_submission_time=11798.491114, global_step=25746, preemption_count=0, score=11798.491114, test/accuracy=0.255700, test/loss=3.705024, test/num_examples=10000, total_duration=12955.351570, train/accuracy=0.368848, train/loss=2.933949, validation/accuracy=0.335420, validation/loss=3.105674, validation/num_examples=50000
I0202 15:16:26.160499 139702527031040 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.9414855241775513, loss=4.057427883148193
I0202 15:17:10.348014 139702543816448 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.8532906770706177, loss=5.168822288513184
I0202 15:17:56.399354 139702527031040 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.9955148696899414, loss=4.714320182800293
I0202 15:18:42.745641 139702543816448 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9915348291397095, loss=4.044113636016846
I0202 15:19:28.789039 139702527031040 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.9229810833930969, loss=3.925548791885376
I0202 15:20:15.404669 139702543816448 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.065225601196289, loss=3.972158908843994
I0202 15:21:01.474481 139702527031040 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.9877189993858337, loss=4.106895923614502
I0202 15:21:47.611090 139702543816448 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7667962312698364, loss=6.075879096984863
I0202 15:22:33.885302 139702527031040 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.9422827959060669, loss=3.9021120071411133
I0202 15:23:04.659480 139863983413056 spec.py:321] Evaluating on the training split.
I0202 15:23:15.121842 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 15:23:41.150996 139863983413056 spec.py:349] Evaluating on the test split.
I0202 15:23:42.790650 139863983413056 submission_runner.py:408] Time since start: 13413.99s, 	Step: 26668, 	{'train/accuracy': 0.37541013956069946, 'train/loss': 2.905060052871704, 'validation/accuracy': 0.3468399941921234, 'validation/loss': 3.0606956481933594, 'validation/num_examples': 50000, 'test/accuracy': 0.27080002427101135, 'test/loss': 3.6579036712646484, 'test/num_examples': 10000, 'score': 12218.910951375961, 'total_duration': 13413.987750291824, 'accumulated_submission_time': 12218.910951375961, 'accumulated_eval_time': 1192.788717508316, 'accumulated_logging_time': 0.8701872825622559}
I0202 15:23:42.811732 139702543816448 logging_writer.py:48] [26668] accumulated_eval_time=1192.788718, accumulated_logging_time=0.870187, accumulated_submission_time=12218.910951, global_step=26668, preemption_count=0, score=12218.910951, test/accuracy=0.270800, test/loss=3.657904, test/num_examples=10000, total_duration=13413.987750, train/accuracy=0.375410, train/loss=2.905060, validation/accuracy=0.346840, validation/loss=3.060696, validation/num_examples=50000
I0202 15:23:55.997223 139702527031040 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.7881877422332764, loss=4.199071407318115
I0202 15:24:39.092312 139702543816448 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.9733687043190002, loss=3.889125347137451
I0202 15:25:25.271779 139702527031040 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.897928774356842, loss=4.244045257568359
I0202 15:26:11.493984 139702543816448 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.9542774558067322, loss=3.8084003925323486
I0202 15:26:57.471390 139702527031040 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.3595551252365112, loss=4.065139293670654
I0202 15:27:43.583904 139702543816448 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.9192895293235779, loss=3.980703115463257
I0202 15:28:29.632707 139702527031040 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.9160926938056946, loss=3.9242241382598877
I0202 15:29:15.683544 139702543816448 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7854816317558289, loss=5.859409332275391
I0202 15:30:01.956518 139702527031040 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7563912272453308, loss=4.661805152893066
I0202 15:30:42.881700 139863983413056 spec.py:321] Evaluating on the training split.
I0202 15:30:53.388455 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 15:31:22.382620 139863983413056 spec.py:349] Evaluating on the test split.
I0202 15:31:24.027244 139863983413056 submission_runner.py:408] Time since start: 13875.22s, 	Step: 27590, 	{'train/accuracy': 0.37593749165534973, 'train/loss': 2.8957693576812744, 'validation/accuracy': 0.3487599790096283, 'validation/loss': 3.039360284805298, 'validation/num_examples': 50000, 'test/accuracy': 0.2639000117778778, 'test/loss': 3.661264181137085, 'test/num_examples': 10000, 'score': 12638.921879768372, 'total_duration': 13875.224354982376, 'accumulated_submission_time': 12638.921879768372, 'accumulated_eval_time': 1233.934255361557, 'accumulated_logging_time': 0.9021317958831787}
I0202 15:31:24.048207 139702543816448 logging_writer.py:48] [27590] accumulated_eval_time=1233.934255, accumulated_logging_time=0.902132, accumulated_submission_time=12638.921880, global_step=27590, preemption_count=0, score=12638.921880, test/accuracy=0.263900, test/loss=3.661264, test/num_examples=10000, total_duration=13875.224355, train/accuracy=0.375937, train/loss=2.895769, validation/accuracy=0.348760, validation/loss=3.039360, validation/num_examples=50000
I0202 15:31:28.444458 139702527031040 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.012508511543274, loss=3.9261856079101562
I0202 15:32:10.419040 139702543816448 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.9636455774307251, loss=4.566967964172363
I0202 15:32:56.051107 139702527031040 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.853705883026123, loss=4.897056579589844
I0202 15:33:42.253130 139702543816448 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.0828197002410889, loss=3.955428123474121
I0202 15:34:28.174900 139702527031040 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.0871115922927856, loss=4.253906726837158
I0202 15:35:14.371362 139702543816448 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.8467363119125366, loss=4.019529342651367
I0202 15:36:00.261724 139702527031040 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.0963867902755737, loss=3.9379067420959473
I0202 15:36:46.344452 139702543816448 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.1053273677825928, loss=3.998168706893921
I0202 15:37:32.561478 139702527031040 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.869009792804718, loss=4.357147693634033
I0202 15:38:18.668689 139702543816448 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8014229536056519, loss=6.024172782897949
I0202 15:38:24.248228 139863983413056 spec.py:321] Evaluating on the training split.
I0202 15:38:34.912198 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 15:38:59.346705 139863983413056 spec.py:349] Evaluating on the test split.
I0202 15:39:00.994330 139863983413056 submission_runner.py:408] Time since start: 14332.19s, 	Step: 28514, 	{'train/accuracy': 0.3650195300579071, 'train/loss': 2.984952926635742, 'validation/accuracy': 0.3395199775695801, 'validation/loss': 3.1396758556365967, 'validation/num_examples': 50000, 'test/accuracy': 0.25620001554489136, 'test/loss': 3.7381186485290527, 'test/num_examples': 10000, 'score': 13059.063136339188, 'total_duration': 14332.19144487381, 'accumulated_submission_time': 13059.063136339188, 'accumulated_eval_time': 1270.6803524494171, 'accumulated_logging_time': 0.9330759048461914}
I0202 15:39:01.013153 139702527031040 logging_writer.py:48] [28514] accumulated_eval_time=1270.680352, accumulated_logging_time=0.933076, accumulated_submission_time=13059.063136, global_step=28514, preemption_count=0, score=13059.063136, test/accuracy=0.256200, test/loss=3.738119, test/num_examples=10000, total_duration=14332.191445, train/accuracy=0.365020, train/loss=2.984953, validation/accuracy=0.339520, validation/loss=3.139676, validation/num_examples=50000
I0202 15:39:36.304881 139702543816448 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.5981813669204712, loss=5.96960973739624
I0202 15:40:22.261732 139702527031040 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.2983593940734863, loss=4.331139087677002
I0202 15:41:08.815078 139702543816448 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.9492724537849426, loss=3.884652614593506
I0202 15:41:54.344692 139702527031040 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.9853845238685608, loss=3.8224074840545654
I0202 15:42:40.478265 139702543816448 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.9180676937103271, loss=4.220930576324463
I0202 15:43:26.446020 139702527031040 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.7043994665145874, loss=4.4721856117248535
I0202 15:44:12.539776 139702543816448 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.7864968180656433, loss=4.795750141143799
I0202 15:44:58.762495 139702527031040 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.1699421405792236, loss=4.131901741027832
I0202 15:45:44.795077 139702543816448 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.9191145896911621, loss=3.9572105407714844
I0202 15:46:01.136955 139863983413056 spec.py:321] Evaluating on the training split.
I0202 15:46:11.432651 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 15:46:40.372421 139863983413056 spec.py:349] Evaluating on the test split.
I0202 15:46:42.018883 139863983413056 submission_runner.py:408] Time since start: 14793.22s, 	Step: 29437, 	{'train/accuracy': 0.39070311188697815, 'train/loss': 2.8178062438964844, 'validation/accuracy': 0.3384000062942505, 'validation/loss': 3.126830577850342, 'validation/num_examples': 50000, 'test/accuracy': 0.2629000246524811, 'test/loss': 3.685798168182373, 'test/num_examples': 10000, 'score': 13479.126105070114, 'total_duration': 14793.215996265411, 'accumulated_submission_time': 13479.126105070114, 'accumulated_eval_time': 1311.5622823238373, 'accumulated_logging_time': 0.9640073776245117}
I0202 15:46:42.040693 139702527031040 logging_writer.py:48] [29437] accumulated_eval_time=1311.562282, accumulated_logging_time=0.964007, accumulated_submission_time=13479.126105, global_step=29437, preemption_count=0, score=13479.126105, test/accuracy=0.262900, test/loss=3.685798, test/num_examples=10000, total_duration=14793.215996, train/accuracy=0.390703, train/loss=2.817806, validation/accuracy=0.338400, validation/loss=3.126831, validation/num_examples=50000
I0202 15:47:07.635201 139702543816448 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9480278491973877, loss=3.9752025604248047
I0202 15:47:52.579122 139702527031040 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.0161892175674438, loss=3.794236660003662
I0202 15:48:38.902909 139702543816448 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.7578343152999878, loss=4.5926384925842285
I0202 15:49:24.911621 139702527031040 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.7468435168266296, loss=5.066203594207764
I0202 15:50:11.457215 139702543816448 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.8633622527122498, loss=3.8505308628082275
I0202 15:50:57.502568 139702527031040 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9675185680389404, loss=3.85205078125
I0202 15:51:44.151992 139702543816448 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.9831188321113586, loss=3.960285186767578
I0202 15:52:30.524404 139702527031040 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.8223737478256226, loss=6.0703253746032715
I0202 15:53:17.071779 139702543816448 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.1755268573760986, loss=4.087425708770752
I0202 15:53:42.058387 139863983413056 spec.py:321] Evaluating on the training split.
I0202 15:53:52.471005 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 15:54:18.913516 139863983413056 spec.py:349] Evaluating on the test split.
I0202 15:54:20.554398 139863983413056 submission_runner.py:408] Time since start: 15251.75s, 	Step: 30356, 	{'train/accuracy': 0.37205076217651367, 'train/loss': 2.9231514930725098, 'validation/accuracy': 0.346699982881546, 'validation/loss': 3.082463026046753, 'validation/num_examples': 50000, 'test/accuracy': 0.2647000253200531, 'test/loss': 3.649627447128296, 'test/num_examples': 10000, 'score': 13899.084905862808, 'total_duration': 15251.751507043839, 'accumulated_submission_time': 13899.084905862808, 'accumulated_eval_time': 1350.0582914352417, 'accumulated_logging_time': 0.9966273307800293}
I0202 15:54:20.578847 139702527031040 logging_writer.py:48] [30356] accumulated_eval_time=1350.058291, accumulated_logging_time=0.996627, accumulated_submission_time=13899.084906, global_step=30356, preemption_count=0, score=13899.084906, test/accuracy=0.264700, test/loss=3.649627, test/num_examples=10000, total_duration=15251.751507, train/accuracy=0.372051, train/loss=2.923151, validation/accuracy=0.346700, validation/loss=3.082463, validation/num_examples=50000
I0202 15:54:38.556422 139702543816448 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.7883768677711487, loss=5.0385589599609375
I0202 15:55:22.278604 139702527031040 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.9365114569664001, loss=4.151862621307373
I0202 15:56:08.635962 139702543816448 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.0596040487289429, loss=6.038608074188232
I0202 15:56:54.716539 139702527031040 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.9972075819969177, loss=3.903610944747925
I0202 15:57:40.793552 139702543816448 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.0120973587036133, loss=4.216578483581543
I0202 15:58:26.963350 139702527031040 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.835411548614502, loss=5.10565185546875
I0202 15:59:13.146933 139702543816448 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8080617189407349, loss=4.953505992889404
I0202 15:59:59.138195 139702527031040 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.1168657541275024, loss=4.284275531768799
I0202 16:00:45.588221 139702543816448 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9860507249832153, loss=4.081297874450684
I0202 16:01:20.925125 139863983413056 spec.py:321] Evaluating on the training split.
I0202 16:01:31.347429 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 16:01:58.649747 139863983413056 spec.py:349] Evaluating on the test split.
I0202 16:02:00.289864 139863983413056 submission_runner.py:408] Time since start: 15711.49s, 	Step: 31278, 	{'train/accuracy': 0.3739062547683716, 'train/loss': 2.9314498901367188, 'validation/accuracy': 0.34731999039649963, 'validation/loss': 3.0704736709594727, 'validation/num_examples': 50000, 'test/accuracy': 0.2718999981880188, 'test/loss': 3.6703150272369385, 'test/num_examples': 10000, 'score': 14319.371235609055, 'total_duration': 15711.486956119537, 'accumulated_submission_time': 14319.371235609055, 'accumulated_eval_time': 1389.423010110855, 'accumulated_logging_time': 1.0317187309265137}
I0202 16:02:00.316351 139702527031040 logging_writer.py:48] [31278] accumulated_eval_time=1389.423010, accumulated_logging_time=1.031719, accumulated_submission_time=14319.371236, global_step=31278, preemption_count=0, score=14319.371236, test/accuracy=0.271900, test/loss=3.670315, test/num_examples=10000, total_duration=15711.486956, train/accuracy=0.373906, train/loss=2.931450, validation/accuracy=0.347320, validation/loss=3.070474, validation/num_examples=50000
I0202 16:02:09.488918 139702543816448 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.8897432684898376, loss=3.936446189880371
I0202 16:02:52.101829 139702527031040 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.0294636487960815, loss=3.8456618785858154
I0202 16:03:37.967255 139702543816448 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.1722029447555542, loss=3.7901058197021484
I0202 16:04:24.187999 139702527031040 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.0212881565093994, loss=3.978037118911743
I0202 16:05:10.257361 139702543816448 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.9540274143218994, loss=4.000466823577881
I0202 16:05:56.153477 139702527031040 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.876075804233551, loss=4.473258972167969
I0202 16:06:42.179459 139702543816448 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.6924268007278442, loss=5.959731578826904
I0202 16:07:28.189525 139702527031040 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.9886060357093811, loss=3.8562910556793213
I0202 16:08:14.145364 139702543816448 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.2245817184448242, loss=3.8459386825561523
I0202 16:09:00.157197 139702527031040 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.0891770124435425, loss=3.836282253265381
I0202 16:09:00.306659 139863983413056 spec.py:321] Evaluating on the training split.
I0202 16:09:11.170519 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 16:09:40.037414 139863983413056 spec.py:349] Evaluating on the test split.
I0202 16:09:41.675370 139863983413056 submission_runner.py:408] Time since start: 16172.87s, 	Step: 32202, 	{'train/accuracy': 0.40431639552116394, 'train/loss': 2.71505069732666, 'validation/accuracy': 0.3649599850177765, 'validation/loss': 2.9368364810943604, 'validation/num_examples': 50000, 'test/accuracy': 0.2857000231742859, 'test/loss': 3.5437114238739014, 'test/num_examples': 10000, 'score': 14739.30179309845, 'total_duration': 16172.872477769852, 'accumulated_submission_time': 14739.30179309845, 'accumulated_eval_time': 1430.7917094230652, 'accumulated_logging_time': 1.0686018466949463}
I0202 16:09:41.695542 139702543816448 logging_writer.py:48] [32202] accumulated_eval_time=1430.791709, accumulated_logging_time=1.068602, accumulated_submission_time=14739.301793, global_step=32202, preemption_count=0, score=14739.301793, test/accuracy=0.285700, test/loss=3.543711, test/num_examples=10000, total_duration=16172.872478, train/accuracy=0.404316, train/loss=2.715051, validation/accuracy=0.364960, validation/loss=2.936836, validation/num_examples=50000
I0202 16:10:22.574275 139702527031040 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.7924705743789673, loss=4.86083459854126
I0202 16:11:08.217642 139702543816448 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.1519038677215576, loss=3.9558701515197754
I0202 16:11:54.589399 139702527031040 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7350173592567444, loss=5.426372528076172
I0202 16:12:40.466027 139702543816448 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.7476224303245544, loss=6.002590179443359
I0202 16:13:26.530578 139702527031040 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9705690145492554, loss=4.039506912231445
I0202 16:14:12.479331 139702543816448 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0331369638442993, loss=3.739577293395996
I0202 16:14:58.473595 139702527031040 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.98345947265625, loss=3.8279733657836914
I0202 16:15:44.668959 139702543816448 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.685326337814331, loss=6.001791000366211
I0202 16:16:30.776610 139702527031040 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.9731255769729614, loss=3.7714104652404785
I0202 16:16:41.838996 139863983413056 spec.py:321] Evaluating on the training split.
I0202 16:16:52.200747 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 16:17:19.538097 139863983413056 spec.py:349] Evaluating on the test split.
I0202 16:17:21.165175 139863983413056 submission_runner.py:408] Time since start: 16632.36s, 	Step: 33126, 	{'train/accuracy': 0.39134764671325684, 'train/loss': 2.812678575515747, 'validation/accuracy': 0.36403998732566833, 'validation/loss': 2.9665307998657227, 'validation/num_examples': 50000, 'test/accuracy': 0.2803000211715698, 'test/loss': 3.540454387664795, 'test/num_examples': 10000, 'score': 15159.38614320755, 'total_duration': 16632.36226463318, 'accumulated_submission_time': 15159.38614320755, 'accumulated_eval_time': 1470.1178567409515, 'accumulated_logging_time': 1.098562479019165}
I0202 16:17:21.185469 139702543816448 logging_writer.py:48] [33126] accumulated_eval_time=1470.117857, accumulated_logging_time=1.098562, accumulated_submission_time=15159.386143, global_step=33126, preemption_count=0, score=15159.386143, test/accuracy=0.280300, test/loss=3.540454, test/num_examples=10000, total_duration=16632.362265, train/accuracy=0.391348, train/loss=2.812679, validation/accuracy=0.364040, validation/loss=2.966531, validation/num_examples=50000
I0202 16:17:51.167637 139702527031040 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.9525113701820374, loss=3.8161749839782715
I0202 16:18:36.865684 139702543816448 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.181748867034912, loss=3.7151827812194824
I0202 16:19:23.124870 139702527031040 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.868009090423584, loss=5.688826084136963
I0202 16:20:09.167121 139702543816448 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7766522169113159, loss=3.948949098587036
I0202 16:20:55.253035 139702527031040 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.8833573460578918, loss=3.8901045322418213
I0202 16:21:41.313900 139702543816448 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.7829539179801941, loss=4.9917707443237305
I0202 16:22:27.694474 139702527031040 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.0164624452590942, loss=3.696451425552368
I0202 16:23:13.868608 139702543816448 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.7043675780296326, loss=6.07036018371582
I0202 16:23:59.805015 139702527031040 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.9731448292732239, loss=4.650125980377197
I0202 16:24:21.264300 139863983413056 spec.py:321] Evaluating on the training split.
I0202 16:24:31.863250 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 16:24:58.574625 139863983413056 spec.py:349] Evaluating on the test split.
I0202 16:25:00.213968 139863983413056 submission_runner.py:408] Time since start: 17091.41s, 	Step: 34048, 	{'train/accuracy': 0.382148414850235, 'train/loss': 2.845215082168579, 'validation/accuracy': 0.3581399917602539, 'validation/loss': 2.999690532684326, 'validation/num_examples': 50000, 'test/accuracy': 0.2736000120639801, 'test/loss': 3.5886569023132324, 'test/num_examples': 10000, 'score': 15579.406750202179, 'total_duration': 17091.41108250618, 'accumulated_submission_time': 15579.406750202179, 'accumulated_eval_time': 1509.0675301551819, 'accumulated_logging_time': 1.1289377212524414}
I0202 16:25:00.238089 139702543816448 logging_writer.py:48] [34048] accumulated_eval_time=1509.067530, accumulated_logging_time=1.128938, accumulated_submission_time=15579.406750, global_step=34048, preemption_count=0, score=15579.406750, test/accuracy=0.273600, test/loss=3.588657, test/num_examples=10000, total_duration=17091.411083, train/accuracy=0.382148, train/loss=2.845215, validation/accuracy=0.358140, validation/loss=2.999691, validation/num_examples=50000
I0202 16:25:21.430527 139702527031040 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.9233489632606506, loss=3.824052095413208
I0202 16:26:05.688303 139702543816448 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.0627847909927368, loss=3.937814235687256
I0202 16:26:51.706821 139702527031040 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.9651649594306946, loss=3.946256160736084
I0202 16:27:37.485114 139702543816448 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.006591796875, loss=3.7674968242645264
I0202 16:28:23.600296 139702527031040 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.9338606595993042, loss=3.7809369564056396
I0202 16:29:09.356886 139702543816448 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.0435681343078613, loss=3.7521111965179443
I0202 16:29:55.576361 139702527031040 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.2482569217681885, loss=3.7570674419403076
I0202 16:30:42.002589 139702543816448 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.895993709564209, loss=4.865699291229248
I0202 16:31:27.736046 139702527031040 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.3199806213378906, loss=3.8719489574432373
I0202 16:32:00.611016 139863983413056 spec.py:321] Evaluating on the training split.
I0202 16:32:11.127292 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 16:32:40.782538 139863983413056 spec.py:349] Evaluating on the test split.
I0202 16:32:42.435968 139863983413056 submission_runner.py:408] Time since start: 17553.63s, 	Step: 34973, 	{'train/accuracy': 0.39232420921325684, 'train/loss': 2.8107075691223145, 'validation/accuracy': 0.3644599914550781, 'validation/loss': 2.975851058959961, 'validation/num_examples': 50000, 'test/accuracy': 0.2762000262737274, 'test/loss': 3.5893852710723877, 'test/num_examples': 10000, 'score': 15999.715313196182, 'total_duration': 17553.633061647415, 'accumulated_submission_time': 15999.715313196182, 'accumulated_eval_time': 1550.8924548625946, 'accumulated_logging_time': 1.1684050559997559}
I0202 16:32:42.459424 139702543816448 logging_writer.py:48] [34973] accumulated_eval_time=1550.892455, accumulated_logging_time=1.168405, accumulated_submission_time=15999.715313, global_step=34973, preemption_count=0, score=15999.715313, test/accuracy=0.276200, test/loss=3.589385, test/num_examples=10000, total_duration=17553.633062, train/accuracy=0.392324, train/loss=2.810708, validation/accuracy=0.364460, validation/loss=2.975851, validation/num_examples=50000
I0202 16:32:53.626750 139702527031040 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.2130051851272583, loss=3.947608232498169
I0202 16:33:36.635609 139702543816448 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.0791878700256348, loss=3.939434051513672
I0202 16:34:22.361161 139702527031040 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.0051277875900269, loss=3.8512158393859863
I0202 16:35:08.514789 139702543816448 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.0476834774017334, loss=3.7510123252868652
I0202 16:35:54.186042 139702527031040 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.9964620471000671, loss=4.614562511444092
I0202 16:36:40.204974 139702543816448 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7801310420036316, loss=5.634303569793701
I0202 16:37:26.266246 139702527031040 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.9351165890693665, loss=4.223998069763184
I0202 16:38:12.287016 139702543816448 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.9313392043113708, loss=4.96051025390625
I0202 16:38:58.163743 139702527031040 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.0893090963363647, loss=3.9184951782226562
I0202 16:39:42.759037 139863983413056 spec.py:321] Evaluating on the training split.
I0202 16:39:53.481279 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 16:40:25.377005 139863983413056 spec.py:349] Evaluating on the test split.
I0202 16:40:27.018755 139863983413056 submission_runner.py:408] Time since start: 18018.22s, 	Step: 35899, 	{'train/accuracy': 0.399726539850235, 'train/loss': 2.7130661010742188, 'validation/accuracy': 0.3743399977684021, 'validation/loss': 2.8684468269348145, 'validation/num_examples': 50000, 'test/accuracy': 0.2859000265598297, 'test/loss': 3.4763166904449463, 'test/num_examples': 10000, 'score': 16419.95436644554, 'total_duration': 18018.215858459473, 'accumulated_submission_time': 16419.95436644554, 'accumulated_eval_time': 1595.1521625518799, 'accumulated_logging_time': 1.2027950286865234}
I0202 16:40:27.040726 139702543816448 logging_writer.py:48] [35899] accumulated_eval_time=1595.152163, accumulated_logging_time=1.202795, accumulated_submission_time=16419.954366, global_step=35899, preemption_count=0, score=16419.954366, test/accuracy=0.285900, test/loss=3.476317, test/num_examples=10000, total_duration=18018.215858, train/accuracy=0.399727, train/loss=2.713066, validation/accuracy=0.374340, validation/loss=2.868447, validation/num_examples=50000
I0202 16:40:27.843589 139702527031040 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.0718637704849243, loss=3.8748111724853516
I0202 16:41:09.213135 139702543816448 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.031771183013916, loss=3.930736541748047
I0202 16:41:55.237131 139702527031040 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.2944658994674683, loss=3.863126516342163
I0202 16:42:41.521067 139702543816448 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.9584056735038757, loss=4.885262489318848
I0202 16:43:27.730505 139702527031040 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.9394606351852417, loss=3.741321563720703
I0202 16:44:13.830058 139702543816448 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.9070622324943542, loss=3.7374179363250732
I0202 16:44:59.799049 139702527031040 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1819615364074707, loss=4.389090538024902
I0202 16:45:45.794513 139702543816448 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.8731609582901001, loss=5.0954718589782715
I0202 16:46:31.875245 139702527031040 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.0326457023620605, loss=4.0435051918029785
I0202 16:47:17.935452 139702543816448 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.0490567684173584, loss=4.019504547119141
I0202 16:47:27.301380 139863983413056 spec.py:321] Evaluating on the training split.
I0202 16:47:37.792258 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 16:48:05.821911 139863983413056 spec.py:349] Evaluating on the test split.
I0202 16:48:07.457654 139863983413056 submission_runner.py:408] Time since start: 18478.65s, 	Step: 36822, 	{'train/accuracy': 0.4059179723262787, 'train/loss': 2.7009963989257812, 'validation/accuracy': 0.37731999158859253, 'validation/loss': 2.8525683879852295, 'validation/num_examples': 50000, 'test/accuracy': 0.28930002450942993, 'test/loss': 3.473834991455078, 'test/num_examples': 10000, 'score': 16840.155430793762, 'total_duration': 18478.654767751694, 'accumulated_submission_time': 16840.155430793762, 'accumulated_eval_time': 1635.3084263801575, 'accumulated_logging_time': 1.2354793548583984}
I0202 16:48:07.480213 139702527031040 logging_writer.py:48] [36822] accumulated_eval_time=1635.308426, accumulated_logging_time=1.235479, accumulated_submission_time=16840.155431, global_step=36822, preemption_count=0, score=16840.155431, test/accuracy=0.289300, test/loss=3.473835, test/num_examples=10000, total_duration=18478.654768, train/accuracy=0.405918, train/loss=2.700996, validation/accuracy=0.377320, validation/loss=2.852568, validation/num_examples=50000
I0202 16:48:39.161961 139702543816448 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.8460747599601746, loss=5.820315361022949
I0202 16:49:25.228450 139702527031040 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.969002902507782, loss=4.015997409820557
I0202 16:50:11.866078 139702543816448 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.9962703585624695, loss=3.7704968452453613
I0202 16:50:57.847823 139702527031040 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.8963906168937683, loss=5.361469268798828
I0202 16:51:44.041432 139702543816448 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.002854824066162, loss=4.468870639801025
I0202 16:52:30.342672 139702527031040 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.9702272415161133, loss=4.032850742340088
I0202 16:53:16.612934 139702543816448 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.9926098585128784, loss=3.8441929817199707
I0202 16:54:02.983228 139702527031040 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.9029313325881958, loss=5.959307670593262
I0202 16:54:49.139905 139702543816448 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.8114886283874512, loss=3.9876160621643066
I0202 16:55:07.816352 139863983413056 spec.py:321] Evaluating on the training split.
I0202 16:55:18.407615 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 16:55:48.336634 139863983413056 spec.py:349] Evaluating on the test split.
I0202 16:55:49.973264 139863983413056 submission_runner.py:408] Time since start: 18941.17s, 	Step: 37742, 	{'train/accuracy': 0.41218748688697815, 'train/loss': 2.6718852519989014, 'validation/accuracy': 0.38161998987197876, 'validation/loss': 2.845273971557617, 'validation/num_examples': 50000, 'test/accuracy': 0.30150002241134644, 'test/loss': 3.418804407119751, 'test/num_examples': 10000, 'score': 17260.433208703995, 'total_duration': 18941.170355796814, 'accumulated_submission_time': 17260.433208703995, 'accumulated_eval_time': 1677.465342760086, 'accumulated_logging_time': 1.268315076828003}
I0202 16:55:49.997640 139702527031040 logging_writer.py:48] [37742] accumulated_eval_time=1677.465343, accumulated_logging_time=1.268315, accumulated_submission_time=17260.433209, global_step=37742, preemption_count=0, score=17260.433209, test/accuracy=0.301500, test/loss=3.418804, test/num_examples=10000, total_duration=18941.170356, train/accuracy=0.412187, train/loss=2.671885, validation/accuracy=0.381620, validation/loss=2.845274, validation/num_examples=50000
I0202 16:56:13.574043 139702543816448 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.9545343518257141, loss=3.885133743286133
I0202 16:56:58.536170 139702527031040 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.7378444075584412, loss=5.901092529296875
I0202 16:57:44.648258 139702543816448 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.2204890251159668, loss=3.9542908668518066
I0202 16:58:30.815735 139702527031040 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.0224854946136475, loss=3.9343438148498535
I0202 16:59:16.904136 139702543816448 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.0586591958999634, loss=3.792848825454712
I0202 17:00:03.233083 139702527031040 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.7973225116729736, loss=4.5383405685424805
I0202 17:00:49.364939 139702543816448 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.9847093820571899, loss=3.8340060710906982
I0202 17:01:35.567639 139702527031040 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.021336317062378, loss=3.8631365299224854
I0202 17:02:22.059203 139702543816448 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.9174717664718628, loss=4.711472511291504
I0202 17:02:50.282922 139863983413056 spec.py:321] Evaluating on the training split.
I0202 17:03:00.777204 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 17:03:27.072596 139863983413056 spec.py:349] Evaluating on the test split.
I0202 17:03:28.716132 139863983413056 submission_runner.py:408] Time since start: 19399.91s, 	Step: 38663, 	{'train/accuracy': 0.4493359327316284, 'train/loss': 2.501319169998169, 'validation/accuracy': 0.38349997997283936, 'validation/loss': 2.8455910682678223, 'validation/num_examples': 50000, 'test/accuracy': 0.2939000129699707, 'test/loss': 3.432493209838867, 'test/num_examples': 10000, 'score': 17680.66015148163, 'total_duration': 19399.91322350502, 'accumulated_submission_time': 17680.66015148163, 'accumulated_eval_time': 1715.8985350131989, 'accumulated_logging_time': 1.3021540641784668}
I0202 17:03:28.741999 139702527031040 logging_writer.py:48] [38663] accumulated_eval_time=1715.898535, accumulated_logging_time=1.302154, accumulated_submission_time=17680.660151, global_step=38663, preemption_count=0, score=17680.660151, test/accuracy=0.293900, test/loss=3.432493, test/num_examples=10000, total_duration=19399.913224, train/accuracy=0.449336, train/loss=2.501319, validation/accuracy=0.383500, validation/loss=2.845591, validation/num_examples=50000
I0202 17:03:43.928191 139702543816448 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.9549312591552734, loss=3.767580032348633
I0202 17:04:27.727652 139702527031040 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.0331391096115112, loss=3.8981900215148926
I0202 17:05:13.460188 139702543816448 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.2082546949386597, loss=3.87092924118042
I0202 17:05:59.532537 139702527031040 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.0380274057388306, loss=3.7392020225524902
I0202 17:06:45.472987 139702543816448 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.0629019737243652, loss=3.8507096767425537
I0202 17:07:31.369019 139702527031040 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.9068427085876465, loss=3.6792569160461426
I0202 17:08:17.437981 139702543816448 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.9379364848136902, loss=4.178288459777832
I0202 17:09:03.528365 139702527031040 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.8373828530311584, loss=5.315378665924072
I0202 17:09:49.527391 139702543816448 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.9878533482551575, loss=3.917335271835327
I0202 17:10:28.874357 139863983413056 spec.py:321] Evaluating on the training split.
I0202 17:10:39.378387 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 17:11:07.531916 139863983413056 spec.py:349] Evaluating on the test split.
I0202 17:11:09.177504 139863983413056 submission_runner.py:408] Time since start: 19860.37s, 	Step: 39587, 	{'train/accuracy': 0.3864062428474426, 'train/loss': 2.9066293239593506, 'validation/accuracy': 0.3610599935054779, 'validation/loss': 3.0364768505096436, 'validation/num_examples': 50000, 'test/accuracy': 0.26750001311302185, 'test/loss': 3.6234524250030518, 'test/num_examples': 10000, 'score': 18100.73324918747, 'total_duration': 19860.374609947205, 'accumulated_submission_time': 18100.73324918747, 'accumulated_eval_time': 1756.2016875743866, 'accumulated_logging_time': 1.3390777111053467}
I0202 17:11:09.200630 139702527031040 logging_writer.py:48] [39587] accumulated_eval_time=1756.201688, accumulated_logging_time=1.339078, accumulated_submission_time=18100.733249, global_step=39587, preemption_count=0, score=18100.733249, test/accuracy=0.267500, test/loss=3.623452, test/num_examples=10000, total_duration=19860.374610, train/accuracy=0.386406, train/loss=2.906629, validation/accuracy=0.361060, validation/loss=3.036477, validation/num_examples=50000
I0202 17:11:14.796592 139702543816448 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.9118703603744507, loss=4.4203715324401855
I0202 17:11:56.920677 139702527031040 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.9520094394683838, loss=3.7079412937164307
I0202 17:12:42.968868 139702543816448 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.2077102661132812, loss=3.6951797008514404
I0202 17:13:29.317874 139702527031040 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.0638608932495117, loss=4.096092224121094
I0202 17:14:15.358725 139702543816448 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9457967877388, loss=3.792462110519409
I0202 17:15:01.581533 139702527031040 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.0945490598678589, loss=4.059951305389404
I0202 17:15:47.454863 139702543816448 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.1457372903823853, loss=3.6576058864593506
I0202 17:16:33.750819 139702527031040 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.9694944024085999, loss=4.279360771179199
I0202 17:17:19.840676 139702543816448 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.1775494813919067, loss=3.713123321533203
I0202 17:18:05.842426 139702527031040 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.9044180512428284, loss=4.098767280578613
I0202 17:18:09.637581 139863983413056 spec.py:321] Evaluating on the training split.
I0202 17:18:20.511788 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 17:18:45.847144 139863983413056 spec.py:349] Evaluating on the test split.
I0202 17:18:47.483080 139863983413056 submission_runner.py:408] Time since start: 20318.68s, 	Step: 40510, 	{'train/accuracy': 0.40166014432907104, 'train/loss': 2.776913642883301, 'validation/accuracy': 0.3719799816608429, 'validation/loss': 2.9364328384399414, 'validation/num_examples': 50000, 'test/accuracy': 0.289900004863739, 'test/loss': 3.5260701179504395, 'test/num_examples': 10000, 'score': 18521.109936714172, 'total_duration': 20318.680195093155, 'accumulated_submission_time': 18521.109936714172, 'accumulated_eval_time': 1794.0471782684326, 'accumulated_logging_time': 1.3741655349731445}
I0202 17:18:47.503714 139702543816448 logging_writer.py:48] [40510] accumulated_eval_time=1794.047178, accumulated_logging_time=1.374166, accumulated_submission_time=18521.109937, global_step=40510, preemption_count=0, score=18521.109937, test/accuracy=0.289900, test/loss=3.526070, test/num_examples=10000, total_duration=20318.680195, train/accuracy=0.401660, train/loss=2.776914, validation/accuracy=0.371980, validation/loss=2.936433, validation/num_examples=50000
I0202 17:19:24.688530 139702527031040 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.7533107399940491, loss=5.566181182861328
I0202 17:20:10.664179 139702543816448 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.8890808820724487, loss=5.973081588745117
I0202 17:20:57.180592 139702527031040 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.0019432306289673, loss=3.728104591369629
I0202 17:21:42.753718 139702543816448 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.8873242735862732, loss=5.720485210418701
I0202 17:22:28.899182 139702527031040 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.0168384313583374, loss=3.958073139190674
I0202 17:23:14.923659 139702543816448 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.8830352425575256, loss=3.735670328140259
I0202 17:24:00.762895 139702527031040 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.4427589178085327, loss=3.9905872344970703
I0202 17:24:47.264731 139702543816448 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.9214503765106201, loss=5.146849155426025
I0202 17:25:33.109322 139702527031040 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.9896025061607361, loss=3.6666367053985596
I0202 17:25:47.809362 139863983413056 spec.py:321] Evaluating on the training split.
I0202 17:25:58.292192 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 17:26:24.372031 139863983413056 spec.py:349] Evaluating on the test split.
I0202 17:26:26.015263 139863983413056 submission_runner.py:408] Time since start: 20777.21s, 	Step: 41434, 	{'train/accuracy': 0.423164039850235, 'train/loss': 2.5967025756835938, 'validation/accuracy': 0.3805999755859375, 'validation/loss': 2.8329660892486572, 'validation/num_examples': 50000, 'test/accuracy': 0.2957000136375427, 'test/loss': 3.4568240642547607, 'test/num_examples': 10000, 'score': 18941.358196496964, 'total_duration': 20777.212375164032, 'accumulated_submission_time': 18941.358196496964, 'accumulated_eval_time': 1832.2530777454376, 'accumulated_logging_time': 1.404280662536621}
I0202 17:26:26.041251 139702543816448 logging_writer.py:48] [41434] accumulated_eval_time=1832.253078, accumulated_logging_time=1.404281, accumulated_submission_time=18941.358196, global_step=41434, preemption_count=0, score=18941.358196, test/accuracy=0.295700, test/loss=3.456824, test/num_examples=10000, total_duration=20777.212375, train/accuracy=0.423164, train/loss=2.596703, validation/accuracy=0.380600, validation/loss=2.832966, validation/num_examples=50000
I0202 17:26:52.791218 139702527031040 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7345792651176453, loss=5.829810619354248
I0202 17:27:37.850183 139702543816448 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6631972789764404, loss=5.94636869430542
I0202 17:28:24.157660 139702527031040 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7495197057723999, loss=5.866023063659668
I0202 17:29:09.952150 139702543816448 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.8459196090698242, loss=4.928896903991699
I0202 17:29:56.208728 139702527031040 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6833702325820923, loss=5.8157572746276855
I0202 17:30:42.257857 139702543816448 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.8832631707191467, loss=4.60942268371582
I0202 17:31:28.144027 139702527031040 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.880943775177002, loss=3.742680788040161
I0202 17:32:14.121800 139702543816448 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1057672500610352, loss=3.575129508972168
I0202 17:32:59.922363 139702527031040 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1825838088989258, loss=3.7046165466308594
I0202 17:33:26.096145 139863983413056 spec.py:321] Evaluating on the training split.
I0202 17:33:36.304763 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 17:34:03.163809 139863983413056 spec.py:349] Evaluating on the test split.
I0202 17:34:04.806502 139863983413056 submission_runner.py:408] Time since start: 21236.00s, 	Step: 42358, 	{'train/accuracy': 0.4092773199081421, 'train/loss': 2.6755664348602295, 'validation/accuracy': 0.38871997594833374, 'validation/loss': 2.8146812915802, 'validation/num_examples': 50000, 'test/accuracy': 0.29920002818107605, 'test/loss': 3.452272891998291, 'test/num_examples': 10000, 'score': 19361.352199077606, 'total_duration': 21236.003611803055, 'accumulated_submission_time': 19361.352199077606, 'accumulated_eval_time': 1870.96342420578, 'accumulated_logging_time': 1.4418244361877441}
I0202 17:34:04.829071 139702543816448 logging_writer.py:48] [42358] accumulated_eval_time=1870.963424, accumulated_logging_time=1.441824, accumulated_submission_time=19361.352199, global_step=42358, preemption_count=0, score=19361.352199, test/accuracy=0.299200, test/loss=3.452273, test/num_examples=10000, total_duration=21236.003612, train/accuracy=0.409277, train/loss=2.675566, validation/accuracy=0.388720, validation/loss=2.814681, validation/num_examples=50000
I0202 17:34:21.989460 139702527031040 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.9285172820091248, loss=3.853363513946533
I0202 17:35:05.666869 139702543816448 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.123671293258667, loss=3.768125295639038
I0202 17:35:52.083809 139702527031040 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.8637319803237915, loss=4.115941047668457
I0202 17:36:38.510356 139702543816448 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.8710488080978394, loss=4.643572807312012
I0202 17:37:24.571696 139702527031040 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.0177981853485107, loss=3.8302178382873535
I0202 17:38:10.560600 139702543816448 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.86148601770401, loss=5.919769763946533
I0202 17:38:56.604415 139702527031040 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9923799633979797, loss=3.65018367767334
I0202 17:39:42.825634 139702543816448 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.9357534050941467, loss=3.626648426055908
I0202 17:40:28.953741 139702527031040 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.8628410696983337, loss=5.077508926391602
I0202 17:41:05.131484 139863983413056 spec.py:321] Evaluating on the training split.
I0202 17:41:15.472764 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 17:41:41.123493 139863983413056 spec.py:349] Evaluating on the test split.
I0202 17:41:42.761581 139863983413056 submission_runner.py:408] Time since start: 21693.96s, 	Step: 43280, 	{'train/accuracy': 0.41355466842651367, 'train/loss': 2.665776252746582, 'validation/accuracy': 0.38312000036239624, 'validation/loss': 2.8275225162506104, 'validation/num_examples': 50000, 'test/accuracy': 0.2969000041484833, 'test/loss': 3.4342548847198486, 'test/num_examples': 10000, 'score': 19781.59530377388, 'total_duration': 21693.95869255066, 'accumulated_submission_time': 19781.59530377388, 'accumulated_eval_time': 1908.5935270786285, 'accumulated_logging_time': 1.474895715713501}
I0202 17:41:42.784517 139702543816448 logging_writer.py:48] [43280] accumulated_eval_time=1908.593527, accumulated_logging_time=1.474896, accumulated_submission_time=19781.595304, global_step=43280, preemption_count=0, score=19781.595304, test/accuracy=0.296900, test/loss=3.434255, test/num_examples=10000, total_duration=21693.958693, train/accuracy=0.413555, train/loss=2.665776, validation/accuracy=0.383120, validation/loss=2.827523, validation/num_examples=50000
I0202 17:41:51.163754 139702527031040 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7402071952819824, loss=5.888539791107178
I0202 17:42:33.506159 139702543816448 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.0293809175491333, loss=3.6324820518493652
I0202 17:43:19.480204 139702527031040 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.8148705363273621, loss=5.842733860015869
I0202 17:44:05.582252 139702543816448 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.0724455118179321, loss=3.7023191452026367
I0202 17:44:51.431793 139702527031040 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.1894218921661377, loss=3.689539909362793
I0202 17:45:37.784234 139702543816448 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.7986119985580444, loss=5.681650161743164
I0202 17:46:23.681747 139702527031040 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.0280259847640991, loss=3.8367908000946045
I0202 17:47:10.126509 139702543816448 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.012284755706787, loss=3.678724527359009
I0202 17:47:55.831480 139702527031040 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.0675461292266846, loss=3.936363935470581
I0202 17:48:41.956232 139702543816448 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.9422741532325745, loss=4.052981376647949
I0202 17:48:42.984533 139863983413056 spec.py:321] Evaluating on the training split.
I0202 17:48:53.260850 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 17:49:20.509513 139863983413056 spec.py:349] Evaluating on the test split.
I0202 17:49:22.143236 139863983413056 submission_runner.py:408] Time since start: 22153.34s, 	Step: 44204, 	{'train/accuracy': 0.4298437535762787, 'train/loss': 2.6165740489959717, 'validation/accuracy': 0.3955000042915344, 'validation/loss': 2.792757034301758, 'validation/num_examples': 50000, 'test/accuracy': 0.305400013923645, 'test/loss': 3.4004056453704834, 'test/num_examples': 10000, 'score': 20201.73545074463, 'total_duration': 22153.340349674225, 'accumulated_submission_time': 20201.73545074463, 'accumulated_eval_time': 1947.7522237300873, 'accumulated_logging_time': 1.50919771194458}
I0202 17:49:22.164973 139702527031040 logging_writer.py:48] [44204] accumulated_eval_time=1947.752224, accumulated_logging_time=1.509198, accumulated_submission_time=20201.735451, global_step=44204, preemption_count=0, score=20201.735451, test/accuracy=0.305400, test/loss=3.400406, test/num_examples=10000, total_duration=22153.340350, train/accuracy=0.429844, train/loss=2.616574, validation/accuracy=0.395500, validation/loss=2.792757, validation/num_examples=50000
I0202 17:50:01.888850 139702543816448 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.0002013444900513, loss=4.145143508911133
I0202 17:50:47.799103 139702527031040 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.1726915836334229, loss=3.810683250427246
I0202 17:51:33.773901 139702543816448 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.9427109956741333, loss=3.6295745372772217
I0202 17:52:19.625585 139702527031040 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.9377907514572144, loss=3.714383125305176
I0202 17:53:05.901491 139702543816448 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.7636593580245972, loss=5.821929931640625
I0202 17:53:51.591416 139702527031040 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.8054341077804565, loss=5.861145973205566
I0202 17:54:37.231977 139702543816448 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6754346489906311, loss=5.180569171905518
I0202 17:55:23.196947 139702527031040 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.855622410774231, loss=5.392502784729004
I0202 17:56:09.475832 139702543816448 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.9740063548088074, loss=3.918501138687134
I0202 17:56:22.443236 139863983413056 spec.py:321] Evaluating on the training split.
I0202 17:56:32.853386 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 17:57:00.905400 139863983413056 spec.py:349] Evaluating on the test split.
I0202 17:57:02.561496 139863983413056 submission_runner.py:408] Time since start: 22613.76s, 	Step: 45130, 	{'train/accuracy': 0.41789060831069946, 'train/loss': 2.693882465362549, 'validation/accuracy': 0.3900199830532074, 'validation/loss': 2.8285815715789795, 'validation/num_examples': 50000, 'test/accuracy': 0.3004000186920166, 'test/loss': 3.4367313385009766, 'test/num_examples': 10000, 'score': 20621.955298423767, 'total_duration': 22613.75858616829, 'accumulated_submission_time': 20621.955298423767, 'accumulated_eval_time': 1987.8704631328583, 'accumulated_logging_time': 1.5406432151794434}
I0202 17:57:02.586884 139702527031040 logging_writer.py:48] [45130] accumulated_eval_time=1987.870463, accumulated_logging_time=1.540643, accumulated_submission_time=20621.955298, global_step=45130, preemption_count=0, score=20621.955298, test/accuracy=0.300400, test/loss=3.436731, test/num_examples=10000, total_duration=22613.758586, train/accuracy=0.417891, train/loss=2.693882, validation/accuracy=0.390020, validation/loss=2.828582, validation/num_examples=50000
I0202 17:57:30.940598 139702543816448 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.8491596579551697, loss=3.5680365562438965
I0202 17:58:16.516006 139702527031040 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.8185924887657166, loss=5.144651412963867
I0202 17:59:02.656159 139702543816448 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.075449824333191, loss=3.823249340057373
I0202 17:59:48.773746 139702527031040 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.7388292551040649, loss=5.3116960525512695
I0202 18:00:34.798471 139702543816448 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9507821202278137, loss=4.127741813659668
I0202 18:01:20.910958 139702527031040 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.1106891632080078, loss=3.682555675506592
I0202 18:02:06.950742 139702543816448 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.2377227544784546, loss=3.712899684906006
I0202 18:02:53.081206 139702527031040 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.9270323514938354, loss=4.286577224731445
I0202 18:03:38.865308 139702543816448 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.2868133783340454, loss=3.7459537982940674
I0202 18:04:02.630713 139863983413056 spec.py:321] Evaluating on the training split.
I0202 18:04:13.101671 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 18:04:42.097056 139863983413056 spec.py:349] Evaluating on the test split.
I0202 18:04:43.728273 139863983413056 submission_runner.py:408] Time since start: 23074.93s, 	Step: 46053, 	{'train/accuracy': 0.42689451575279236, 'train/loss': 2.6220920085906982, 'validation/accuracy': 0.39969998598098755, 'validation/loss': 2.773547410964966, 'validation/num_examples': 50000, 'test/accuracy': 0.3076000213623047, 'test/loss': 3.3951525688171387, 'test/num_examples': 10000, 'score': 21041.939405441284, 'total_duration': 23074.925387620926, 'accumulated_submission_time': 21041.939405441284, 'accumulated_eval_time': 2028.9680247306824, 'accumulated_logging_time': 1.5772721767425537}
I0202 18:04:43.753779 139702527031040 logging_writer.py:48] [46053] accumulated_eval_time=2028.968025, accumulated_logging_time=1.577272, accumulated_submission_time=21041.939405, global_step=46053, preemption_count=0, score=21041.939405, test/accuracy=0.307600, test/loss=3.395153, test/num_examples=10000, total_duration=23074.925388, train/accuracy=0.426895, train/loss=2.622092, validation/accuracy=0.399700, validation/loss=2.773547, validation/num_examples=50000
I0202 18:05:02.901806 139702543816448 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.9923983216285706, loss=3.6273210048675537
I0202 18:05:47.056967 139702527031040 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.746509850025177, loss=5.099117755889893
I0202 18:06:33.469640 139702543816448 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.9591794610023499, loss=3.9710209369659424
I0202 18:07:19.646937 139702527031040 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.9900485277175903, loss=4.123591899871826
I0202 18:08:05.769735 139702543816448 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.9281976819038391, loss=6.0008111000061035
I0202 18:08:51.625526 139702527031040 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.0081554651260376, loss=5.780300140380859
I0202 18:09:37.658867 139702543816448 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7300354838371277, loss=5.616311073303223
I0202 18:10:23.835463 139702527031040 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.9169520139694214, loss=3.634960651397705
I0202 18:11:09.929246 139702543816448 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.9611978530883789, loss=3.5051701068878174
I0202 18:11:44.064841 139863983413056 spec.py:321] Evaluating on the training split.
I0202 18:11:54.466702 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 18:12:22.438305 139863983413056 spec.py:349] Evaluating on the test split.
I0202 18:12:24.080728 139863983413056 submission_runner.py:408] Time since start: 23535.28s, 	Step: 46976, 	{'train/accuracy': 0.4323046803474426, 'train/loss': 2.5363075733184814, 'validation/accuracy': 0.405239999294281, 'validation/loss': 2.6976065635681152, 'validation/num_examples': 50000, 'test/accuracy': 0.31460002064704895, 'test/loss': 3.3456552028656006, 'test/num_examples': 10000, 'score': 21462.19115138054, 'total_duration': 23535.27784347534, 'accumulated_submission_time': 21462.19115138054, 'accumulated_eval_time': 2068.98393702507, 'accumulated_logging_time': 1.6132659912109375}
I0202 18:12:24.104308 139702527031040 logging_writer.py:48] [46976] accumulated_eval_time=2068.983937, accumulated_logging_time=1.613266, accumulated_submission_time=21462.191151, global_step=46976, preemption_count=0, score=21462.191151, test/accuracy=0.314600, test/loss=3.345655, test/num_examples=10000, total_duration=23535.277843, train/accuracy=0.432305, train/loss=2.536308, validation/accuracy=0.405240, validation/loss=2.697607, validation/num_examples=50000
I0202 18:12:34.074079 139702543816448 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.0621857643127441, loss=3.5783653259277344
I0202 18:13:16.861466 139702527031040 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.8883353471755981, loss=3.5639114379882812
I0202 18:14:03.072302 139702543816448 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.0959018468856812, loss=3.6212141513824463
I0202 18:14:49.128767 139702527031040 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.0596956014633179, loss=3.761035442352295
I0202 18:15:35.068627 139702543816448 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.9920053482055664, loss=3.427546262741089
I0202 18:16:20.932467 139702527031040 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.0585154294967651, loss=3.529731035232544
I0202 18:17:06.999640 139702543816448 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.1032638549804688, loss=3.5934088230133057
I0202 18:17:52.682996 139702527031040 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2581664323806763, loss=3.670581817626953
I0202 18:18:38.750704 139702543816448 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.7665671706199646, loss=5.906576156616211
I0202 18:19:24.649883 139702527031040 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.7352020740509033, loss=4.148533821105957
I0202 18:19:24.663942 139863983413056 spec.py:321] Evaluating on the training split.
I0202 18:19:35.157567 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 18:20:02.778857 139863983413056 spec.py:349] Evaluating on the test split.
I0202 18:20:04.411497 139863983413056 submission_runner.py:408] Time since start: 23995.61s, 	Step: 47901, 	{'train/accuracy': 0.45130857825279236, 'train/loss': 2.50014066696167, 'validation/accuracy': 0.3981199860572815, 'validation/loss': 2.7976644039154053, 'validation/num_examples': 50000, 'test/accuracy': 0.2980000078678131, 'test/loss': 3.4122321605682373, 'test/num_examples': 10000, 'score': 21882.690141916275, 'total_duration': 23995.60861515999, 'accumulated_submission_time': 21882.690141916275, 'accumulated_eval_time': 2108.731509923935, 'accumulated_logging_time': 1.6486258506774902}
I0202 18:20:04.433633 139702543816448 logging_writer.py:48] [47901] accumulated_eval_time=2108.731510, accumulated_logging_time=1.648626, accumulated_submission_time=21882.690142, global_step=47901, preemption_count=0, score=21882.690142, test/accuracy=0.298000, test/loss=3.412232, test/num_examples=10000, total_duration=23995.608615, train/accuracy=0.451309, train/loss=2.500141, validation/accuracy=0.398120, validation/loss=2.797664, validation/num_examples=50000
I0202 18:20:46.037424 139702527031040 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9857484698295593, loss=5.850600242614746
I0202 18:21:31.754877 139702543816448 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.9884652495384216, loss=3.5049386024475098
I0202 18:22:17.943610 139702527031040 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.01315438747406, loss=4.494713306427002
I0202 18:23:03.727221 139702543816448 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.1782454252243042, loss=3.6601099967956543
I0202 18:23:49.686673 139702527031040 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.1455049514770508, loss=3.883037567138672
I0202 18:24:35.674914 139702543816448 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0161174535751343, loss=3.799464225769043
I0202 18:25:21.656393 139702527031040 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0149015188217163, loss=3.6945581436157227
I0202 18:26:07.575618 139702543816448 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.109739899635315, loss=3.6022024154663086
I0202 18:26:53.675239 139702527031040 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.0025938749313354, loss=4.023744583129883
I0202 18:27:04.437514 139863983413056 spec.py:321] Evaluating on the training split.
I0202 18:27:14.919124 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 18:27:45.319546 139863983413056 spec.py:349] Evaluating on the test split.
I0202 18:27:46.966220 139863983413056 submission_runner.py:408] Time since start: 24458.16s, 	Step: 48825, 	{'train/accuracy': 0.4263085722923279, 'train/loss': 2.605025053024292, 'validation/accuracy': 0.4018400013446808, 'validation/loss': 2.7520787715911865, 'validation/num_examples': 50000, 'test/accuracy': 0.3126000165939331, 'test/loss': 3.3366458415985107, 'test/num_examples': 10000, 'score': 22302.164969682693, 'total_duration': 24458.163280963898, 'accumulated_submission_time': 22302.164969682693, 'accumulated_eval_time': 2151.260172843933, 'accumulated_logging_time': 2.1510133743286133}
I0202 18:27:46.999083 139702543816448 logging_writer.py:48] [48825] accumulated_eval_time=2151.260173, accumulated_logging_time=2.151013, accumulated_submission_time=22302.164970, global_step=48825, preemption_count=0, score=22302.164970, test/accuracy=0.312600, test/loss=3.336646, test/num_examples=10000, total_duration=24458.163281, train/accuracy=0.426309, train/loss=2.605025, validation/accuracy=0.401840, validation/loss=2.752079, validation/num_examples=50000
I0202 18:28:17.534617 139702527031040 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.8889537453651428, loss=4.441836357116699
I0202 18:29:03.311388 139702543816448 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.7551244497299194, loss=5.13344144821167
I0202 18:29:49.595505 139702527031040 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.8382907509803772, loss=5.75011682510376
I0202 18:30:35.634092 139702543816448 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.0877351760864258, loss=3.776754856109619
I0202 18:31:21.468137 139702527031040 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.0058820247650146, loss=3.8442978858947754
I0202 18:32:07.572635 139702543816448 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.0359585285186768, loss=3.7129716873168945
I0202 18:32:53.719781 139702527031040 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.9880335330963135, loss=3.5132246017456055
I0202 18:33:39.702053 139702543816448 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.9983301758766174, loss=3.755251169204712
I0202 18:34:25.757490 139702527031040 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.9579301476478577, loss=5.862265586853027
I0202 18:34:46.974058 139863983413056 spec.py:321] Evaluating on the training split.
I0202 18:34:57.247867 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 18:35:29.319930 139863983413056 spec.py:349] Evaluating on the test split.
I0202 18:35:30.956561 139863983413056 submission_runner.py:408] Time since start: 24922.15s, 	Step: 49748, 	{'train/accuracy': 0.4362890422344208, 'train/loss': 2.5398664474487305, 'validation/accuracy': 0.40761998295783997, 'validation/loss': 2.714684247970581, 'validation/num_examples': 50000, 'test/accuracy': 0.31700000166893005, 'test/loss': 3.326667070388794, 'test/num_examples': 10000, 'score': 22722.07783985138, 'total_duration': 24922.15366792679, 'accumulated_submission_time': 22722.07783985138, 'accumulated_eval_time': 2195.242655277252, 'accumulated_logging_time': 2.196490526199341}
I0202 18:35:30.980693 139702543816448 logging_writer.py:48] [49748] accumulated_eval_time=2195.242655, accumulated_logging_time=2.196491, accumulated_submission_time=22722.077840, global_step=49748, preemption_count=0, score=22722.077840, test/accuracy=0.317000, test/loss=3.326667, test/num_examples=10000, total_duration=24922.153668, train/accuracy=0.436289, train/loss=2.539866, validation/accuracy=0.407620, validation/loss=2.714684, validation/num_examples=50000
I0202 18:35:52.135674 139702527031040 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.030905842781067, loss=3.711787462234497
I0202 18:36:36.694833 139702543816448 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.8985425233840942, loss=5.329866409301758
I0202 18:37:23.100550 139702527031040 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.0667985677719116, loss=6.019549369812012
I0202 18:38:09.498861 139702543816448 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9958614706993103, loss=3.5647459030151367
I0202 18:38:55.305452 139702527031040 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.9185057878494263, loss=4.706110954284668
I0202 18:39:41.412882 139702543816448 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.0829108953475952, loss=3.515272855758667
I0202 18:40:27.612100 139702527031040 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.9627583622932434, loss=5.926880359649658
I0202 18:41:13.417119 139702543816448 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.082901120185852, loss=3.6076011657714844
I0202 18:41:59.509845 139702527031040 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.1033811569213867, loss=3.65765643119812
I0202 18:42:31.029207 139863983413056 spec.py:321] Evaluating on the training split.
I0202 18:42:41.367842 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 18:43:06.996416 139863983413056 spec.py:349] Evaluating on the test split.
I0202 18:43:08.634522 139863983413056 submission_runner.py:408] Time since start: 25379.83s, 	Step: 50670, 	{'train/accuracy': 0.4491601586341858, 'train/loss': 2.490548610687256, 'validation/accuracy': 0.402319997549057, 'validation/loss': 2.752784013748169, 'validation/num_examples': 50000, 'test/accuracy': 0.3070000112056732, 'test/loss': 3.3801825046539307, 'test/num_examples': 10000, 'score': 23142.06842494011, 'total_duration': 25379.83161520958, 'accumulated_submission_time': 23142.06842494011, 'accumulated_eval_time': 2232.8479537963867, 'accumulated_logging_time': 2.2305619716644287}
I0202 18:43:08.666935 139702543816448 logging_writer.py:48] [50670] accumulated_eval_time=2232.847954, accumulated_logging_time=2.230562, accumulated_submission_time=23142.068425, global_step=50670, preemption_count=0, score=23142.068425, test/accuracy=0.307000, test/loss=3.380183, test/num_examples=10000, total_duration=25379.831615, train/accuracy=0.449160, train/loss=2.490549, validation/accuracy=0.402320, validation/loss=2.752784, validation/num_examples=50000
I0202 18:43:21.050572 139702527031040 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.03476881980896, loss=3.853527545928955
I0202 18:44:04.280671 139702543816448 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.4309310913085938, loss=3.6359832286834717
I0202 18:44:50.249544 139702527031040 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.039838433265686, loss=5.888555526733398
I0202 18:45:36.220467 139702543816448 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.9657847881317139, loss=3.8384928703308105
I0202 18:46:22.395133 139702527031040 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.9177467226982117, loss=3.7108373641967773
I0202 18:47:08.596474 139702543816448 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.9020128846168518, loss=3.477417230606079
I0202 18:47:54.543004 139702527031040 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.88545823097229, loss=4.034921646118164
I0202 18:48:40.546323 139702543816448 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.0363750457763672, loss=3.5817673206329346
I0202 18:49:26.659886 139702527031040 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.1794830560684204, loss=3.734589099884033
I0202 18:50:08.819976 139863983413056 spec.py:321] Evaluating on the training split.
I0202 18:50:19.213779 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 18:50:51.048548 139863983413056 spec.py:349] Evaluating on the test split.
I0202 18:50:52.684750 139863983413056 submission_runner.py:408] Time since start: 25843.88s, 	Step: 51593, 	{'train/accuracy': 0.4297265410423279, 'train/loss': 2.5958352088928223, 'validation/accuracy': 0.4063799977302551, 'validation/loss': 2.737758159637451, 'validation/num_examples': 50000, 'test/accuracy': 0.31530001759529114, 'test/loss': 3.344174385070801, 'test/num_examples': 10000, 'score': 23562.160203695297, 'total_duration': 25843.881860017776, 'accumulated_submission_time': 23562.160203695297, 'accumulated_eval_time': 2276.712729215622, 'accumulated_logging_time': 2.2754602432250977}
I0202 18:50:52.709371 139702543816448 logging_writer.py:48] [51593] accumulated_eval_time=2276.712729, accumulated_logging_time=2.275460, accumulated_submission_time=23562.160204, global_step=51593, preemption_count=0, score=23562.160204, test/accuracy=0.315300, test/loss=3.344174, test/num_examples=10000, total_duration=25843.881860, train/accuracy=0.429727, train/loss=2.595835, validation/accuracy=0.406380, validation/loss=2.737758, validation/num_examples=50000
I0202 18:50:55.900563 139702527031040 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.9339087605476379, loss=3.6268110275268555
I0202 18:51:37.749037 139702543816448 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.9096066951751709, loss=3.960033416748047
I0202 18:52:23.733103 139702527031040 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.7403327226638794, loss=5.762659549713135
I0202 18:53:09.808363 139702543816448 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.927046000957489, loss=5.401924133300781
I0202 18:53:55.711190 139702527031040 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.0909066200256348, loss=5.977150917053223
I0202 18:54:41.748514 139702543816448 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.3127270936965942, loss=3.6989545822143555
I0202 18:55:27.641908 139702527031040 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.383137822151184, loss=3.5708436965942383
I0202 18:56:14.210211 139702543816448 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.079207420349121, loss=3.589653968811035
I0202 18:56:59.840681 139702527031040 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.8176489472389221, loss=3.9840898513793945
I0202 18:57:45.644671 139702543816448 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.1774576902389526, loss=3.556901216506958
I0202 18:57:52.776223 139863983413056 spec.py:321] Evaluating on the training split.
I0202 18:58:03.173508 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 18:58:33.501643 139863983413056 spec.py:349] Evaluating on the test split.
I0202 18:58:35.153356 139863983413056 submission_runner.py:408] Time since start: 26306.35s, 	Step: 52517, 	{'train/accuracy': 0.43990233540534973, 'train/loss': 2.5211288928985596, 'validation/accuracy': 0.4089199900627136, 'validation/loss': 2.6849489212036133, 'validation/num_examples': 50000, 'test/accuracy': 0.32100000977516174, 'test/loss': 3.283968210220337, 'test/num_examples': 10000, 'score': 23982.16872549057, 'total_duration': 26306.350472450256, 'accumulated_submission_time': 23982.16872549057, 'accumulated_eval_time': 2319.089858531952, 'accumulated_logging_time': 2.310560941696167}
I0202 18:58:35.179764 139702527031040 logging_writer.py:48] [52517] accumulated_eval_time=2319.089859, accumulated_logging_time=2.310561, accumulated_submission_time=23982.168725, global_step=52517, preemption_count=0, score=23982.168725, test/accuracy=0.321000, test/loss=3.283968, test/num_examples=10000, total_duration=26306.350472, train/accuracy=0.439902, train/loss=2.521129, validation/accuracy=0.408920, validation/loss=2.684949, validation/num_examples=50000
I0202 18:59:09.291652 139702543816448 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.8205424547195435, loss=4.63107967376709
I0202 18:59:55.147250 139702527031040 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.8972247242927551, loss=5.829412937164307
I0202 19:00:41.488867 139702543816448 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.9862146377563477, loss=3.659360647201538
I0202 19:01:27.695960 139702527031040 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.0728553533554077, loss=3.3688173294067383
I0202 19:02:13.715054 139702543816448 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.0567960739135742, loss=3.6242945194244385
I0202 19:02:59.749741 139702527031040 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.0485087633132935, loss=3.595559597015381
I0202 19:03:45.784976 139702543816448 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.9286081194877625, loss=3.6091506481170654
I0202 19:04:31.930458 139702527031040 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.0754659175872803, loss=3.6992645263671875
I0202 19:05:17.931991 139702543816448 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.0184684991836548, loss=3.471204996109009
I0202 19:05:35.477206 139863983413056 spec.py:321] Evaluating on the training split.
I0202 19:05:45.877524 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 19:06:13.632355 139863983413056 spec.py:349] Evaluating on the test split.
I0202 19:06:15.274822 139863983413056 submission_runner.py:408] Time since start: 26766.47s, 	Step: 53440, 	{'train/accuracy': 0.4496484398841858, 'train/loss': 2.463557720184326, 'validation/accuracy': 0.41543999314308167, 'validation/loss': 2.6775717735290527, 'validation/num_examples': 50000, 'test/accuracy': 0.32760000228881836, 'test/loss': 3.283280611038208, 'test/num_examples': 10000, 'score': 24402.401746749878, 'total_duration': 26766.471939086914, 'accumulated_submission_time': 24402.401746749878, 'accumulated_eval_time': 2358.8874881267548, 'accumulated_logging_time': 2.348323106765747}
I0202 19:06:15.299491 139702527031040 logging_writer.py:48] [53440] accumulated_eval_time=2358.887488, accumulated_logging_time=2.348323, accumulated_submission_time=24402.401747, global_step=53440, preemption_count=0, score=24402.401747, test/accuracy=0.327600, test/loss=3.283281, test/num_examples=10000, total_duration=26766.471939, train/accuracy=0.449648, train/loss=2.463558, validation/accuracy=0.415440, validation/loss=2.677572, validation/num_examples=50000
I0202 19:06:39.680803 139702543816448 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.9777777194976807, loss=5.829723358154297
I0202 19:07:24.786318 139702527031040 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.7422196865081787, loss=5.6373724937438965
I0202 19:08:10.792652 139702543816448 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.0570472478866577, loss=3.545884370803833
I0202 19:08:57.246398 139702527031040 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.9238943457603455, loss=4.033581733703613
I0202 19:09:43.034099 139702543816448 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.8960961103439331, loss=4.309752464294434
I0202 19:10:29.201497 139702527031040 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0083438158035278, loss=3.5245134830474854
I0202 19:11:15.235167 139702543816448 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.3950592279434204, loss=3.6587955951690674
I0202 19:12:01.400249 139702527031040 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.0751498937606812, loss=3.530043840408325
I0202 19:12:47.308588 139702543816448 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9153428673744202, loss=5.793733596801758
I0202 19:13:15.606109 139863983413056 spec.py:321] Evaluating on the training split.
I0202 19:13:25.929680 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 19:13:53.336061 139863983413056 spec.py:349] Evaluating on the test split.
I0202 19:13:54.975921 139863983413056 submission_runner.py:408] Time since start: 27226.17s, 	Step: 54363, 	{'train/accuracy': 0.43671873211860657, 'train/loss': 2.5562655925750732, 'validation/accuracy': 0.4078799784183502, 'validation/loss': 2.7128429412841797, 'validation/num_examples': 50000, 'test/accuracy': 0.3160000145435333, 'test/loss': 3.333500862121582, 'test/num_examples': 10000, 'score': 24822.649163007736, 'total_duration': 27226.17301273346, 'accumulated_submission_time': 24822.649163007736, 'accumulated_eval_time': 2398.257269382477, 'accumulated_logging_time': 2.3831875324249268}
I0202 19:13:55.007301 139702527031040 logging_writer.py:48] [54363] accumulated_eval_time=2398.257269, accumulated_logging_time=2.383188, accumulated_submission_time=24822.649163, global_step=54363, preemption_count=0, score=24822.649163, test/accuracy=0.316000, test/loss=3.333501, test/num_examples=10000, total_duration=27226.173013, train/accuracy=0.436719, train/loss=2.556266, validation/accuracy=0.407880, validation/loss=2.712843, validation/num_examples=50000
I0202 19:14:10.182967 139702543816448 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.9849992990493774, loss=5.0213518142700195
I0202 19:14:53.917269 139702527031040 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.7604299187660217, loss=5.547453880310059
I0202 19:15:39.723617 139702543816448 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.0193809270858765, loss=3.6981663703918457
I0202 19:16:25.843924 139702527031040 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.734176754951477, loss=5.195296287536621
I0202 19:17:11.717465 139702543816448 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.8985187411308289, loss=3.7715187072753906
I0202 19:17:57.599984 139702527031040 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.9223560690879822, loss=3.8696835041046143
I0202 19:18:43.399779 139702543816448 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.982053279876709, loss=3.569305419921875
I0202 19:19:29.625846 139702527031040 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0035207271575928, loss=3.5880746841430664
I0202 19:20:15.770610 139702543816448 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.8686074018478394, loss=5.887260913848877
I0202 19:20:55.114320 139863983413056 spec.py:321] Evaluating on the training split.
I0202 19:21:05.277631 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 19:21:36.135577 139863983413056 spec.py:349] Evaluating on the test split.
I0202 19:21:37.778131 139863983413056 submission_runner.py:408] Time since start: 27688.98s, 	Step: 55287, 	{'train/accuracy': 0.44783201813697815, 'train/loss': 2.4865450859069824, 'validation/accuracy': 0.42361998558044434, 'validation/loss': 2.6358258724212646, 'validation/num_examples': 50000, 'test/accuracy': 0.3257000148296356, 'test/loss': 3.252009153366089, 'test/num_examples': 10000, 'score': 25242.692935943604, 'total_duration': 27688.975242853165, 'accumulated_submission_time': 25242.692935943604, 'accumulated_eval_time': 2440.921109676361, 'accumulated_logging_time': 2.425750970840454}
I0202 19:21:37.804942 139702527031040 logging_writer.py:48] [55287] accumulated_eval_time=2440.921110, accumulated_logging_time=2.425751, accumulated_submission_time=25242.692936, global_step=55287, preemption_count=0, score=25242.692936, test/accuracy=0.325700, test/loss=3.252009, test/num_examples=10000, total_duration=27688.975243, train/accuracy=0.447832, train/loss=2.486545, validation/accuracy=0.423620, validation/loss=2.635826, validation/num_examples=50000
I0202 19:21:43.388055 139702543816448 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.0421109199523926, loss=3.5549681186676025
I0202 19:22:25.280540 139702527031040 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.8955511450767517, loss=4.092161655426025
I0202 19:23:11.605755 139702543816448 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.9570260047912598, loss=5.4395952224731445
I0202 19:23:57.501551 139702527031040 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.0145212411880493, loss=3.752333164215088
I0202 19:24:43.621856 139702543816448 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.1045284271240234, loss=3.7187728881835938
I0202 19:25:29.846643 139702527031040 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0691163539886475, loss=4.040620803833008
I0202 19:26:15.800693 139702543816448 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.2548308372497559, loss=3.6849896907806396
I0202 19:27:01.790380 139702527031040 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.9563911557197571, loss=3.741438865661621
I0202 19:27:48.219357 139702543816448 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0741029977798462, loss=3.738687515258789
I0202 19:28:34.260532 139702527031040 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9062051773071289, loss=3.4778387546539307
I0202 19:28:37.996676 139863983413056 spec.py:321] Evaluating on the training split.
I0202 19:28:48.445113 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 19:29:16.607090 139863983413056 spec.py:349] Evaluating on the test split.
I0202 19:29:18.242625 139863983413056 submission_runner.py:408] Time since start: 28149.44s, 	Step: 56210, 	{'train/accuracy': 0.4586718678474426, 'train/loss': 2.4052109718322754, 'validation/accuracy': 0.42155998945236206, 'validation/loss': 2.599883794784546, 'validation/num_examples': 50000, 'test/accuracy': 0.326200008392334, 'test/loss': 3.243481397628784, 'test/num_examples': 10000, 'score': 25662.820051670074, 'total_duration': 28149.439738035202, 'accumulated_submission_time': 25662.820051670074, 'accumulated_eval_time': 2481.1670627593994, 'accumulated_logging_time': 2.4647953510284424}
I0202 19:29:18.267228 139702543816448 logging_writer.py:48] [56210] accumulated_eval_time=2481.167063, accumulated_logging_time=2.464795, accumulated_submission_time=25662.820052, global_step=56210, preemption_count=0, score=25662.820052, test/accuracy=0.326200, test/loss=3.243481, test/num_examples=10000, total_duration=28149.439738, train/accuracy=0.458672, train/loss=2.405211, validation/accuracy=0.421560, validation/loss=2.599884, validation/num_examples=50000
I0202 19:29:55.832717 139702527031040 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.0277142524719238, loss=3.4348344802856445
I0202 19:30:41.927398 139702543816448 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.032701849937439, loss=3.468522310256958
I0202 19:31:28.486823 139702527031040 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.0678995847702026, loss=3.620805263519287
I0202 19:32:14.608325 139702543816448 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.123082160949707, loss=3.6526010036468506
I0202 19:33:00.829789 139702527031040 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.8405835628509521, loss=5.368208885192871
I0202 19:33:46.903964 139702543816448 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.0145245790481567, loss=3.5157294273376465
I0202 19:34:32.974929 139702527031040 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.1670253276824951, loss=3.6356310844421387
I0202 19:35:19.176876 139702543816448 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.9639970064163208, loss=3.4198575019836426
I0202 19:36:05.223515 139702527031040 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.1459386348724365, loss=3.4818129539489746
I0202 19:36:18.656524 139863983413056 spec.py:321] Evaluating on the training split.
I0202 19:36:28.975092 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 19:36:58.312919 139863983413056 spec.py:349] Evaluating on the test split.
I0202 19:36:59.944465 139863983413056 submission_runner.py:408] Time since start: 28611.14s, 	Step: 57131, 	{'train/accuracy': 0.4611132740974426, 'train/loss': 2.4022233486175537, 'validation/accuracy': 0.4213999807834625, 'validation/loss': 2.619320869445801, 'validation/num_examples': 50000, 'test/accuracy': 0.329800009727478, 'test/loss': 3.234283447265625, 'test/num_examples': 10000, 'score': 26083.149917840958, 'total_duration': 28611.141579151154, 'accumulated_submission_time': 26083.149917840958, 'accumulated_eval_time': 2522.4550380706787, 'accumulated_logging_time': 2.499359130859375}
I0202 19:36:59.968698 139702543816448 logging_writer.py:48] [57131] accumulated_eval_time=2522.455038, accumulated_logging_time=2.499359, accumulated_submission_time=26083.149918, global_step=57131, preemption_count=0, score=26083.149918, test/accuracy=0.329800, test/loss=3.234283, test/num_examples=10000, total_duration=28611.141579, train/accuracy=0.461113, train/loss=2.402223, validation/accuracy=0.421400, validation/loss=2.619321, validation/num_examples=50000
I0202 19:37:27.947444 139702527031040 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9665701389312744, loss=3.487971305847168
I0202 19:38:13.462323 139702543816448 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.1081976890563965, loss=3.4280753135681152
I0202 19:38:59.529983 139702527031040 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.9852379560470581, loss=3.8375725746154785
I0202 19:39:45.475496 139702543816448 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.7535532116889954, loss=4.926065921783447
I0202 19:40:32.032088 139702527031040 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.0385897159576416, loss=3.4718823432922363
I0202 19:41:17.880548 139702543816448 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.9892487525939941, loss=3.738535165786743
I0202 19:42:03.867505 139702527031040 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.1065449714660645, loss=3.4933903217315674
I0202 19:42:49.923597 139702543816448 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.1342014074325562, loss=3.6619186401367188
I0202 19:43:35.750577 139702527031040 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.0078452825546265, loss=5.824626922607422
I0202 19:44:00.251876 139863983413056 spec.py:321] Evaluating on the training split.
I0202 19:44:10.754557 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 19:44:41.447359 139863983413056 spec.py:349] Evaluating on the test split.
I0202 19:44:43.083953 139863983413056 submission_runner.py:408] Time since start: 29074.28s, 	Step: 58055, 	{'train/accuracy': 0.44603514671325684, 'train/loss': 2.520827054977417, 'validation/accuracy': 0.4186599850654602, 'validation/loss': 2.6796395778656006, 'validation/num_examples': 50000, 'test/accuracy': 0.3215000033378601, 'test/loss': 3.2995548248291016, 'test/num_examples': 10000, 'score': 26503.37421274185, 'total_duration': 29074.28104519844, 'accumulated_submission_time': 26503.37421274185, 'accumulated_eval_time': 2565.287081718445, 'accumulated_logging_time': 2.5334596633911133}
I0202 19:44:43.113484 139702543816448 logging_writer.py:48] [58055] accumulated_eval_time=2565.287082, accumulated_logging_time=2.533460, accumulated_submission_time=26503.374213, global_step=58055, preemption_count=0, score=26503.374213, test/accuracy=0.321500, test/loss=3.299555, test/num_examples=10000, total_duration=29074.281045, train/accuracy=0.446035, train/loss=2.520827, validation/accuracy=0.418660, validation/loss=2.679640, validation/num_examples=50000
I0202 19:45:01.500044 139702527031040 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.8330503106117249, loss=5.878714561462402
I0202 19:45:45.359877 139702543816448 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.0429332256317139, loss=3.5217206478118896
I0202 19:46:31.399545 139702527031040 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.880632758140564, loss=4.627368927001953
I0202 19:47:17.384490 139702543816448 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.2151851654052734, loss=3.6023600101470947
I0202 19:48:03.456167 139702527031040 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1443681716918945, loss=3.7295918464660645
I0202 19:48:49.616963 139702543816448 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.1348801851272583, loss=3.589231491088867
I0202 19:49:35.588587 139702527031040 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.8163235187530518, loss=5.825718402862549
I0202 19:50:22.260496 139702543816448 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.1747100353240967, loss=3.519967555999756
I0202 19:51:07.823304 139702527031040 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.7613197565078735, loss=5.145464897155762
I0202 19:51:43.377373 139863983413056 spec.py:321] Evaluating on the training split.
I0202 19:51:53.740973 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 19:52:22.339676 139863983413056 spec.py:349] Evaluating on the test split.
I0202 19:52:23.989967 139863983413056 submission_runner.py:408] Time since start: 29535.19s, 	Step: 58979, 	{'train/accuracy': 0.46378904581069946, 'train/loss': 2.4174797534942627, 'validation/accuracy': 0.4327999949455261, 'validation/loss': 2.582634925842285, 'validation/num_examples': 50000, 'test/accuracy': 0.3387000262737274, 'test/loss': 3.2149226665496826, 'test/num_examples': 10000, 'score': 26923.577298164368, 'total_duration': 29535.1870803833, 'accumulated_submission_time': 26923.577298164368, 'accumulated_eval_time': 2605.8996703624725, 'accumulated_logging_time': 2.574249744415283}
I0202 19:52:24.018530 139702543816448 logging_writer.py:48] [58979] accumulated_eval_time=2605.899670, accumulated_logging_time=2.574250, accumulated_submission_time=26923.577298, global_step=58979, preemption_count=0, score=26923.577298, test/accuracy=0.338700, test/loss=3.214923, test/num_examples=10000, total_duration=29535.187080, train/accuracy=0.463789, train/loss=2.417480, validation/accuracy=0.432800, validation/loss=2.582635, validation/num_examples=50000
I0202 19:52:32.806567 139702527031040 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.8188512921333313, loss=5.024693489074707
I0202 19:53:15.498941 139702543816448 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.044838547706604, loss=5.405387878417969
I0202 19:54:01.345877 139702527031040 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.9467259645462036, loss=3.6369073390960693
I0202 19:54:47.680735 139702543816448 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.138278603553772, loss=3.551628351211548
I0202 19:55:33.727217 139702527031040 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.1291555166244507, loss=3.4776761531829834
I0202 19:56:19.906996 139702543816448 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.9225597381591797, loss=5.303938388824463
I0202 19:57:05.648113 139702527031040 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.0553380250930786, loss=3.481062412261963
I0202 19:57:51.670571 139702543816448 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.9564333558082581, loss=3.8407561779022217
I0202 19:58:37.757931 139702527031040 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.9838350415229797, loss=5.412966251373291
I0202 19:59:23.691308 139702543816448 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.9943710565567017, loss=3.4091410636901855
I0202 19:59:24.266257 139863983413056 spec.py:321] Evaluating on the training split.
I0202 19:59:34.811417 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 20:00:03.253499 139863983413056 spec.py:349] Evaluating on the test split.
I0202 20:00:04.900288 139863983413056 submission_runner.py:408] Time since start: 29996.10s, 	Step: 59903, 	{'train/accuracy': 0.47880858182907104, 'train/loss': 2.342374563217163, 'validation/accuracy': 0.42433997988700867, 'validation/loss': 2.625638961791992, 'validation/num_examples': 50000, 'test/accuracy': 0.3347000181674957, 'test/loss': 3.231863021850586, 'test/num_examples': 10000, 'score': 27343.765065908432, 'total_duration': 29996.09737586975, 'accumulated_submission_time': 27343.765065908432, 'accumulated_eval_time': 2646.5336713790894, 'accumulated_logging_time': 2.614187479019165}
I0202 20:00:04.933327 139702527031040 logging_writer.py:48] [59903] accumulated_eval_time=2646.533671, accumulated_logging_time=2.614187, accumulated_submission_time=27343.765066, global_step=59903, preemption_count=0, score=27343.765066, test/accuracy=0.334700, test/loss=3.231863, test/num_examples=10000, total_duration=29996.097376, train/accuracy=0.478809, train/loss=2.342375, validation/accuracy=0.424340, validation/loss=2.625639, validation/num_examples=50000
I0202 20:00:45.573458 139702543816448 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0258607864379883, loss=3.8891403675079346
I0202 20:01:31.768693 139702527031040 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.9499396085739136, loss=4.370032787322998
I0202 20:02:18.139242 139702543816448 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.9840912222862244, loss=4.049741744995117
I0202 20:03:04.278446 139702527031040 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.0918481349945068, loss=3.299574851989746
I0202 20:03:50.407782 139702543816448 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.992702066898346, loss=4.246805191040039
I0202 20:04:36.427990 139702527031040 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.1550484895706177, loss=3.5937273502349854
I0202 20:05:22.614812 139702543816448 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.8396662473678589, loss=5.426980972290039
I0202 20:06:08.655997 139702527031040 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.0890616178512573, loss=3.5803446769714355
I0202 20:06:54.682478 139702543816448 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.7687485814094543, loss=4.867619514465332
I0202 20:07:05.032432 139863983413056 spec.py:321] Evaluating on the training split.
I0202 20:07:15.533502 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 20:07:45.224097 139863983413056 spec.py:349] Evaluating on the test split.
I0202 20:07:46.879292 139863983413056 submission_runner.py:408] Time since start: 30458.08s, 	Step: 60824, 	{'train/accuracy': 0.4606054723262787, 'train/loss': 2.44146728515625, 'validation/accuracy': 0.42865997552871704, 'validation/loss': 2.598587989807129, 'validation/num_examples': 50000, 'test/accuracy': 0.3337000012397766, 'test/loss': 3.2204599380493164, 'test/num_examples': 10000, 'score': 27763.803510665894, 'total_duration': 30458.07638692856, 'accumulated_submission_time': 27763.803510665894, 'accumulated_eval_time': 2688.3805034160614, 'accumulated_logging_time': 2.658954620361328}
I0202 20:07:46.906943 139702527031040 logging_writer.py:48] [60824] accumulated_eval_time=2688.380503, accumulated_logging_time=2.658955, accumulated_submission_time=27763.803511, global_step=60824, preemption_count=0, score=27763.803511, test/accuracy=0.333700, test/loss=3.220460, test/num_examples=10000, total_duration=30458.076387, train/accuracy=0.460605, train/loss=2.441467, validation/accuracy=0.428660, validation/loss=2.598588, validation/num_examples=50000
I0202 20:08:17.655826 139702543816448 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.2585031986236572, loss=3.8663032054901123
I0202 20:09:03.200971 139702527031040 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.1531407833099365, loss=3.590519666671753
I0202 20:09:49.551394 139702543816448 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.0012199878692627, loss=3.570622682571411
I0202 20:10:35.475066 139702527031040 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.326640009880066, loss=3.4712259769439697
I0202 20:11:21.845165 139702543816448 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.9598431587219238, loss=3.8890862464904785
I0202 20:12:07.767275 139702527031040 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.9107776284217834, loss=3.5332934856414795
I0202 20:12:53.542525 139702543816448 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.9428238272666931, loss=5.127219200134277
I0202 20:13:39.555456 139702527031040 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.1953437328338623, loss=3.529088020324707
I0202 20:14:25.592657 139702543816448 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.2358949184417725, loss=3.4054596424102783
I0202 20:14:46.964749 139863983413056 spec.py:321] Evaluating on the training split.
I0202 20:14:57.209275 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 20:15:24.514246 139863983413056 spec.py:349] Evaluating on the test split.
I0202 20:15:26.158880 139863983413056 submission_runner.py:408] Time since start: 30917.36s, 	Step: 61748, 	{'train/accuracy': 0.4458398222923279, 'train/loss': 2.5256237983703613, 'validation/accuracy': 0.41613999009132385, 'validation/loss': 2.6907958984375, 'validation/num_examples': 50000, 'test/accuracy': 0.3223000168800354, 'test/loss': 3.279371738433838, 'test/num_examples': 10000, 'score': 28183.8014895916, 'total_duration': 30917.355994701385, 'accumulated_submission_time': 28183.8014895916, 'accumulated_eval_time': 2727.574634075165, 'accumulated_logging_time': 2.6977574825286865}
I0202 20:15:26.183344 139702527031040 logging_writer.py:48] [61748] accumulated_eval_time=2727.574634, accumulated_logging_time=2.697757, accumulated_submission_time=28183.801490, global_step=61748, preemption_count=0, score=28183.801490, test/accuracy=0.322300, test/loss=3.279372, test/num_examples=10000, total_duration=30917.355995, train/accuracy=0.445840, train/loss=2.525624, validation/accuracy=0.416140, validation/loss=2.690796, validation/num_examples=50000
I0202 20:15:47.350103 139702543816448 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.030767798423767, loss=3.6635942459106445
I0202 20:16:31.839463 139702527031040 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.9408921599388123, loss=4.3180389404296875
I0202 20:17:18.138269 139702543816448 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.1796730756759644, loss=3.564366340637207
I0202 20:18:04.015214 139702527031040 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.9140763878822327, loss=3.9663054943084717
I0202 20:18:50.155259 139702543816448 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.0639537572860718, loss=3.4915103912353516
I0202 20:19:36.356200 139702527031040 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0555490255355835, loss=3.4227042198181152
I0202 20:20:22.491890 139702543816448 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.3659238815307617, loss=3.5423898696899414
I0202 20:21:08.550058 139702527031040 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.7619490027427673, loss=5.634424686431885
I0202 20:21:54.924110 139702543816448 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.021453619003296, loss=3.7135772705078125
I0202 20:22:26.417360 139863983413056 spec.py:321] Evaluating on the training split.
I0202 20:22:37.113006 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 20:23:06.182051 139863983413056 spec.py:349] Evaluating on the test split.
I0202 20:23:07.824583 139863983413056 submission_runner.py:408] Time since start: 31379.02s, 	Step: 62670, 	{'train/accuracy': 0.47507810592651367, 'train/loss': 2.3306970596313477, 'validation/accuracy': 0.43865999579429626, 'validation/loss': 2.5384957790374756, 'validation/num_examples': 50000, 'test/accuracy': 0.3391000032424927, 'test/loss': 3.1987009048461914, 'test/num_examples': 10000, 'score': 28603.97628927231, 'total_duration': 31379.021688699722, 'accumulated_submission_time': 28603.97628927231, 'accumulated_eval_time': 2768.981840610504, 'accumulated_logging_time': 2.7327027320861816}
I0202 20:23:07.851376 139702527031040 logging_writer.py:48] [62670] accumulated_eval_time=2768.981841, accumulated_logging_time=2.732703, accumulated_submission_time=28603.976289, global_step=62670, preemption_count=0, score=28603.976289, test/accuracy=0.339100, test/loss=3.198701, test/num_examples=10000, total_duration=31379.021689, train/accuracy=0.475078, train/loss=2.330697, validation/accuracy=0.438660, validation/loss=2.538496, validation/num_examples=50000
I0202 20:23:20.232435 139702543816448 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.0337404012680054, loss=3.6923091411590576
I0202 20:24:03.357852 139702527031040 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.9007460474967957, loss=4.742393970489502
I0202 20:24:49.481907 139702543816448 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.017690896987915, loss=3.4656147956848145
I0202 20:25:35.674368 139702527031040 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.8838088512420654, loss=4.407896995544434
I0202 20:26:21.580355 139702543816448 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.0368326902389526, loss=3.2272891998291016
I0202 20:27:07.910846 139702527031040 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.9062853455543518, loss=5.866466999053955
I0202 20:27:53.967065 139702543816448 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.8163814544677734, loss=5.655496120452881
I0202 20:28:40.002442 139702527031040 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.137834072113037, loss=3.753068685531616
I0202 20:29:26.147829 139702543816448 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.9918745160102844, loss=3.6123859882354736
I0202 20:30:07.857863 139863983413056 spec.py:321] Evaluating on the training split.
I0202 20:30:19.332802 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 20:30:44.638339 139863983413056 spec.py:349] Evaluating on the test split.
I0202 20:30:46.277132 139863983413056 submission_runner.py:408] Time since start: 31837.47s, 	Step: 63592, 	{'train/accuracy': 0.4749414026737213, 'train/loss': 2.3414981365203857, 'validation/accuracy': 0.44200000166893005, 'validation/loss': 2.4970359802246094, 'validation/num_examples': 50000, 'test/accuracy': 0.34150001406669617, 'test/loss': 3.151350259780884, 'test/num_examples': 10000, 'score': 29023.923259735107, 'total_duration': 31837.474231004715, 'accumulated_submission_time': 29023.923259735107, 'accumulated_eval_time': 2807.4011034965515, 'accumulated_logging_time': 2.7702078819274902}
I0202 20:30:46.304115 139702527031040 logging_writer.py:48] [63592] accumulated_eval_time=2807.401103, accumulated_logging_time=2.770208, accumulated_submission_time=29023.923260, global_step=63592, preemption_count=0, score=29023.923260, test/accuracy=0.341500, test/loss=3.151350, test/num_examples=10000, total_duration=31837.474231, train/accuracy=0.474941, train/loss=2.341498, validation/accuracy=0.442000, validation/loss=2.497036, validation/num_examples=50000
I0202 20:30:49.888777 139702543816448 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.3174911737442017, loss=3.9687113761901855
I0202 20:31:31.544452 139702527031040 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.1623953580856323, loss=3.6518430709838867
I0202 20:32:17.966506 139702543816448 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.9819685816764832, loss=3.344381093978882
I0202 20:33:04.463809 139702527031040 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.8994617462158203, loss=3.8575615882873535
I0202 20:33:50.498660 139702543816448 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.7948546409606934, loss=5.542868614196777
I0202 20:34:36.448721 139702527031040 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.8727762699127197, loss=5.801632881164551
I0202 20:35:22.672479 139702543816448 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.8237398266792297, loss=4.55113410949707
I0202 20:36:08.420219 139702527031040 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.9211637377738953, loss=5.5581769943237305
I0202 20:36:54.432778 139702543816448 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.121848702430725, loss=3.5028350353240967
I0202 20:37:40.688264 139702527031040 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.1381860971450806, loss=3.3743250370025635
I0202 20:37:46.343998 139863983413056 spec.py:321] Evaluating on the training split.
I0202 20:37:56.779319 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 20:38:24.705950 139863983413056 spec.py:349] Evaluating on the test split.
I0202 20:38:26.354083 139863983413056 submission_runner.py:408] Time since start: 32297.55s, 	Step: 64514, 	{'train/accuracy': 0.4628320336341858, 'train/loss': 2.441080331802368, 'validation/accuracy': 0.43181997537612915, 'validation/loss': 2.605339527130127, 'validation/num_examples': 50000, 'test/accuracy': 0.3314000070095062, 'test/loss': 3.231776237487793, 'test/num_examples': 10000, 'score': 29443.90414404869, 'total_duration': 32297.551177978516, 'accumulated_submission_time': 29443.90414404869, 'accumulated_eval_time': 2847.411164045334, 'accumulated_logging_time': 2.8072707653045654}
I0202 20:38:26.383129 139702543816448 logging_writer.py:48] [64514] accumulated_eval_time=2847.411164, accumulated_logging_time=2.807271, accumulated_submission_time=29443.904144, global_step=64514, preemption_count=0, score=29443.904144, test/accuracy=0.331400, test/loss=3.231776, test/num_examples=10000, total_duration=32297.551178, train/accuracy=0.462832, train/loss=2.441080, validation/accuracy=0.431820, validation/loss=2.605340, validation/num_examples=50000
I0202 20:39:01.699509 139702527031040 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.9979321956634521, loss=3.564255714416504
I0202 20:39:47.640820 139702543816448 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.2852002382278442, loss=3.5114951133728027
I0202 20:40:33.957741 139702527031040 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.083517074584961, loss=3.4388959407806396
I0202 20:41:20.154360 139702543816448 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.9298334717750549, loss=5.71783447265625
I0202 20:42:06.236222 139702527031040 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.9214598536491394, loss=5.757779121398926
I0202 20:42:52.678138 139702543816448 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.8642600774765015, loss=5.744990348815918
I0202 20:43:38.487852 139702527031040 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.8080078959465027, loss=5.7712178230285645
I0202 20:44:24.401005 139702543816448 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.0003381967544556, loss=3.4745213985443115
I0202 20:45:10.353994 139702527031040 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.051985740661621, loss=3.332233190536499
I0202 20:45:26.661332 139863983413056 spec.py:321] Evaluating on the training split.
I0202 20:45:36.864792 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 20:46:04.993911 139863983413056 spec.py:349] Evaluating on the test split.
I0202 20:46:06.634669 139863983413056 submission_runner.py:408] Time since start: 32757.83s, 	Step: 65437, 	{'train/accuracy': 0.45894530415534973, 'train/loss': 2.488585948944092, 'validation/accuracy': 0.42405998706817627, 'validation/loss': 2.6650540828704834, 'validation/num_examples': 50000, 'test/accuracy': 0.3297000229358673, 'test/loss': 3.2900028228759766, 'test/num_examples': 10000, 'score': 29864.123959302902, 'total_duration': 32757.831778764725, 'accumulated_submission_time': 29864.123959302902, 'accumulated_eval_time': 2887.384506225586, 'accumulated_logging_time': 2.8465631008148193}
I0202 20:46:06.662288 139702543816448 logging_writer.py:48] [65437] accumulated_eval_time=2887.384506, accumulated_logging_time=2.846563, accumulated_submission_time=29864.123959, global_step=65437, preemption_count=0, score=29864.123959, test/accuracy=0.329700, test/loss=3.290003, test/num_examples=10000, total_duration=32757.831779, train/accuracy=0.458945, train/loss=2.488586, validation/accuracy=0.424060, validation/loss=2.665054, validation/num_examples=50000
I0202 20:46:32.240099 139702527031040 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.1402618885040283, loss=3.528526782989502
I0202 20:47:17.210051 139702543816448 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.9110787510871887, loss=5.791683197021484
I0202 20:48:03.622210 139702527031040 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.0929800271987915, loss=3.4024229049682617
I0202 20:48:49.489478 139702543816448 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.0142680406570435, loss=4.248885631561279
I0202 20:49:35.409315 139702527031040 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.6652792692184448, loss=5.754370212554932
I0202 20:50:21.526284 139702543816448 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.0174521207809448, loss=3.4203083515167236
I0202 20:51:07.803740 139702527031040 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.1383050680160522, loss=3.5293450355529785
I0202 20:51:53.814087 139702543816448 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0342625379562378, loss=3.7708845138549805
I0202 20:52:40.094293 139702527031040 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.089145541191101, loss=3.755852222442627
I0202 20:53:06.985383 139863983413056 spec.py:321] Evaluating on the training split.
I0202 20:53:17.512086 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 20:53:47.723925 139863983413056 spec.py:349] Evaluating on the test split.
I0202 20:53:49.375818 139863983413056 submission_runner.py:408] Time since start: 33220.57s, 	Step: 66360, 	{'train/accuracy': 0.46708983182907104, 'train/loss': 2.3807220458984375, 'validation/accuracy': 0.4341999888420105, 'validation/loss': 2.5594401359558105, 'validation/num_examples': 50000, 'test/accuracy': 0.3379000127315521, 'test/loss': 3.1919198036193848, 'test/num_examples': 10000, 'score': 30284.387431383133, 'total_duration': 33220.57291150093, 'accumulated_submission_time': 30284.387431383133, 'accumulated_eval_time': 2929.774935245514, 'accumulated_logging_time': 2.8840224742889404}
I0202 20:53:49.406688 139702543816448 logging_writer.py:48] [66360] accumulated_eval_time=2929.774935, accumulated_logging_time=2.884022, accumulated_submission_time=30284.387431, global_step=66360, preemption_count=0, score=30284.387431, test/accuracy=0.337900, test/loss=3.191920, test/num_examples=10000, total_duration=33220.572912, train/accuracy=0.467090, train/loss=2.380722, validation/accuracy=0.434200, validation/loss=2.559440, validation/num_examples=50000
I0202 20:54:05.785596 139702527031040 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.9618301391601562, loss=4.0481719970703125
I0202 20:54:49.511358 139702543816448 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.0022510290145874, loss=5.562978744506836
I0202 20:55:35.639239 139702527031040 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.228167176246643, loss=3.333770513534546
I0202 20:56:21.756088 139702543816448 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.4769586324691772, loss=3.5339972972869873
I0202 20:57:07.802749 139702527031040 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.0891506671905518, loss=3.3136544227600098
I0202 20:57:53.714899 139702543816448 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.8909833431243896, loss=5.45024299621582
I0202 20:58:39.775831 139702527031040 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.8359878659248352, loss=5.715145111083984
I0202 20:59:25.745259 139702543816448 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.8605083227157593, loss=4.716804027557373
I0202 21:00:11.916359 139702527031040 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.966597855091095, loss=5.8453850746154785
I0202 21:00:49.686768 139863983413056 spec.py:321] Evaluating on the training split.
I0202 21:01:00.347073 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 21:01:30.629451 139863983413056 spec.py:349] Evaluating on the test split.
I0202 21:01:32.281239 139863983413056 submission_runner.py:408] Time since start: 33683.48s, 	Step: 67284, 	{'train/accuracy': 0.46605467796325684, 'train/loss': 2.3730216026306152, 'validation/accuracy': 0.4387199878692627, 'validation/loss': 2.5333032608032227, 'validation/num_examples': 50000, 'test/accuracy': 0.3418000042438507, 'test/loss': 3.1739964485168457, 'test/num_examples': 10000, 'score': 30704.6074051857, 'total_duration': 33683.47833299637, 'accumulated_submission_time': 30704.6074051857, 'accumulated_eval_time': 2972.3694083690643, 'accumulated_logging_time': 2.92598557472229}
I0202 21:01:32.315288 139702543816448 logging_writer.py:48] [67284] accumulated_eval_time=2972.369408, accumulated_logging_time=2.925986, accumulated_submission_time=30704.607405, global_step=67284, preemption_count=0, score=30704.607405, test/accuracy=0.341800, test/loss=3.173996, test/num_examples=10000, total_duration=33683.478333, train/accuracy=0.466055, train/loss=2.373022, validation/accuracy=0.438720, validation/loss=2.533303, validation/num_examples=50000
I0202 21:01:39.108167 139702527031040 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.3667587041854858, loss=3.469050645828247
I0202 21:02:21.460492 139702543816448 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.1462210416793823, loss=3.814627170562744
I0202 21:03:07.572586 139702527031040 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.9174681305885315, loss=4.347039222717285
I0202 21:03:53.854279 139702543816448 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.845375657081604, loss=4.364270210266113
I0202 21:04:39.643087 139702527031040 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.8772972822189331, loss=4.505681991577148
I0202 21:05:25.653408 139702543816448 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.1663930416107178, loss=3.425496816635132
I0202 21:06:11.955306 139702527031040 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.9791271090507507, loss=3.3952887058258057
I0202 21:06:57.867533 139702543816448 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.0388776063919067, loss=4.171043395996094
I0202 21:07:43.908071 139702527031040 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.0936518907546997, loss=5.706701755523682
I0202 21:08:30.028894 139702543816448 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1015197038650513, loss=3.3451108932495117
I0202 21:08:32.454480 139863983413056 spec.py:321] Evaluating on the training split.
I0202 21:08:42.857772 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 21:09:07.600730 139863983413056 spec.py:349] Evaluating on the test split.
I0202 21:09:09.244947 139863983413056 submission_runner.py:408] Time since start: 34140.44s, 	Step: 68207, 	{'train/accuracy': 0.4681445062160492, 'train/loss': 2.3622944355010986, 'validation/accuracy': 0.43553999066352844, 'validation/loss': 2.5348927974700928, 'validation/num_examples': 50000, 'test/accuracy': 0.33980002999305725, 'test/loss': 3.1727473735809326, 'test/num_examples': 10000, 'score': 31124.684980630875, 'total_duration': 34140.442061424255, 'accumulated_submission_time': 31124.684980630875, 'accumulated_eval_time': 3009.159858226776, 'accumulated_logging_time': 2.972452402114868}
I0202 21:09:09.270584 139702527031040 logging_writer.py:48] [68207] accumulated_eval_time=3009.159858, accumulated_logging_time=2.972452, accumulated_submission_time=31124.684981, global_step=68207, preemption_count=0, score=31124.684981, test/accuracy=0.339800, test/loss=3.172747, test/num_examples=10000, total_duration=34140.442061, train/accuracy=0.468145, train/loss=2.362294, validation/accuracy=0.435540, validation/loss=2.534893, validation/num_examples=50000
I0202 21:09:47.973122 139702543816448 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.093072772026062, loss=3.5905089378356934
I0202 21:10:34.148843 139702527031040 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.948348879814148, loss=4.403061866760254
I0202 21:11:20.307115 139702543816448 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.847415030002594, loss=5.4503912925720215
I0202 21:12:06.244843 139702527031040 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.07053804397583, loss=3.408005952835083
I0202 21:12:52.247832 139702543816448 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.9337559938430786, loss=5.758352279663086
I0202 21:13:38.561374 139702527031040 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.0716168880462646, loss=3.33284068107605
I0202 21:14:24.689171 139702543816448 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.236412525177002, loss=3.3994550704956055
I0202 21:15:10.545578 139702527031040 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.142698049545288, loss=3.299347162246704
I0202 21:15:56.593601 139702543816448 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.8353824019432068, loss=4.0251312255859375
I0202 21:16:10.021956 139863983413056 spec.py:321] Evaluating on the training split.
I0202 21:16:20.635125 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 21:16:47.418730 139863983413056 spec.py:349] Evaluating on the test split.
I0202 21:16:49.068784 139863983413056 submission_runner.py:408] Time since start: 34600.27s, 	Step: 69130, 	{'train/accuracy': 0.4894726574420929, 'train/loss': 2.2886223793029785, 'validation/accuracy': 0.43111997842788696, 'validation/loss': 2.5940520763397217, 'validation/num_examples': 50000, 'test/accuracy': 0.33150002360343933, 'test/loss': 3.2263267040252686, 'test/num_examples': 10000, 'score': 31545.37599992752, 'total_duration': 34600.265894174576, 'accumulated_submission_time': 31545.37599992752, 'accumulated_eval_time': 3048.2066645622253, 'accumulated_logging_time': 3.0094172954559326}
I0202 21:16:49.095753 139702527031040 logging_writer.py:48] [69130] accumulated_eval_time=3048.206665, accumulated_logging_time=3.009417, accumulated_submission_time=31545.376000, global_step=69130, preemption_count=0, score=31545.376000, test/accuracy=0.331500, test/loss=3.226327, test/num_examples=10000, total_duration=34600.265894, train/accuracy=0.489473, train/loss=2.288622, validation/accuracy=0.431120, validation/loss=2.594052, validation/num_examples=50000
I0202 21:17:17.474843 139702543816448 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.0350383520126343, loss=3.3464949131011963
I0202 21:18:03.035839 139702527031040 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.7790049314498901, loss=5.2445783615112305
I0202 21:18:49.160589 139702543816448 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.9367516040802002, loss=5.763915061950684
I0202 21:19:35.191322 139702527031040 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.0245386362075806, loss=3.343080997467041
I0202 21:20:21.565452 139702543816448 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.093086838722229, loss=3.3614346981048584
I0202 21:21:07.371305 139702527031040 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.1639525890350342, loss=3.2485694885253906
I0202 21:21:53.322893 139702543816448 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.8537641167640686, loss=5.156530857086182
I0202 21:22:39.372516 139702527031040 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.9409628510475159, loss=5.383636474609375
I0202 21:23:25.495002 139702543816448 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.173436164855957, loss=3.5497756004333496
I0202 21:23:49.365307 139863983413056 spec.py:321] Evaluating on the training split.
I0202 21:23:59.786748 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 21:24:29.558980 139863983413056 spec.py:349] Evaluating on the test split.
I0202 21:24:31.202992 139863983413056 submission_runner.py:408] Time since start: 35062.40s, 	Step: 70053, 	{'train/accuracy': 0.48564451932907104, 'train/loss': 2.2598440647125244, 'validation/accuracy': 0.45795997977256775, 'validation/loss': 2.3936898708343506, 'validation/num_examples': 50000, 'test/accuracy': 0.35690000653266907, 'test/loss': 3.05293607711792, 'test/num_examples': 10000, 'score': 31965.58531689644, 'total_duration': 35062.400102853775, 'accumulated_submission_time': 31965.58531689644, 'accumulated_eval_time': 3090.0443358421326, 'accumulated_logging_time': 3.047203779220581}
I0202 21:24:31.233319 139702527031040 logging_writer.py:48] [70053] accumulated_eval_time=3090.044336, accumulated_logging_time=3.047204, accumulated_submission_time=31965.585317, global_step=70053, preemption_count=0, score=31965.585317, test/accuracy=0.356900, test/loss=3.052936, test/num_examples=10000, total_duration=35062.400103, train/accuracy=0.485645, train/loss=2.259844, validation/accuracy=0.457960, validation/loss=2.393690, validation/num_examples=50000
I0202 21:24:50.402441 139702543816448 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.934461772441864, loss=4.078068733215332
I0202 21:25:34.541250 139702527031040 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.2072477340698242, loss=3.7409205436706543
I0202 21:26:20.488548 139702543816448 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.2745802402496338, loss=3.283951997756958
I0202 21:27:06.646853 139702527031040 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.0088844299316406, loss=3.298495054244995
I0202 21:27:52.444310 139702543816448 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.0487922430038452, loss=3.2074151039123535
I0202 21:28:38.375923 139702527031040 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.0082226991653442, loss=3.3367116451263428
I0202 21:29:24.406572 139702543816448 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.028383731842041, loss=3.8303182125091553
I0202 21:30:10.523199 139702527031040 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.8944506645202637, loss=4.409360885620117
I0202 21:30:56.470759 139702543816448 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.7839944958686829, loss=4.890108585357666
I0202 21:31:31.310246 139863983413056 spec.py:321] Evaluating on the training split.
I0202 21:31:41.680233 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 21:32:07.468517 139863983413056 spec.py:349] Evaluating on the test split.
I0202 21:32:09.131391 139863983413056 submission_runner.py:408] Time since start: 35520.33s, 	Step: 70977, 	{'train/accuracy': 0.482421875, 'train/loss': 2.2963407039642334, 'validation/accuracy': 0.44609999656677246, 'validation/loss': 2.483259916305542, 'validation/num_examples': 50000, 'test/accuracy': 0.349700003862381, 'test/loss': 3.116337776184082, 'test/num_examples': 10000, 'score': 32385.602464437485, 'total_duration': 35520.32847523689, 'accumulated_submission_time': 32385.602464437485, 'accumulated_eval_time': 3127.865446805954, 'accumulated_logging_time': 3.0879249572753906}
I0202 21:32:09.163146 139702527031040 logging_writer.py:48] [70977] accumulated_eval_time=3127.865447, accumulated_logging_time=3.087925, accumulated_submission_time=32385.602464, global_step=70977, preemption_count=0, score=32385.602464, test/accuracy=0.349700, test/loss=3.116338, test/num_examples=10000, total_duration=35520.328475, train/accuracy=0.482422, train/loss=2.296341, validation/accuracy=0.446100, validation/loss=2.483260, validation/num_examples=50000
I0202 21:32:18.738243 139702543816448 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.7861624360084534, loss=5.507390975952148
I0202 21:33:01.567513 139702527031040 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.9086530208587646, loss=4.779860973358154
I0202 21:33:47.894882 139702543816448 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.0126516819000244, loss=3.3490517139434814
I0202 21:34:34.435700 139702527031040 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.0810881853103638, loss=3.6968557834625244
I0202 21:35:20.753782 139702543816448 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.8771387934684753, loss=5.175991535186768
I0202 21:36:06.661176 139702527031040 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.0805435180664062, loss=3.513582468032837
I0202 21:36:52.481374 139702543816448 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.2808338403701782, loss=3.4012043476104736
I0202 21:37:38.645848 139702527031040 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1217557191848755, loss=3.798942804336548
I0202 21:38:24.621615 139702543816448 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.8790467977523804, loss=5.760334491729736
I0202 21:39:09.328826 139863983413056 spec.py:321] Evaluating on the training split.
I0202 21:39:19.691461 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 21:39:50.062949 139863983413056 spec.py:349] Evaluating on the test split.
I0202 21:39:51.703947 139863983413056 submission_runner.py:408] Time since start: 35982.90s, 	Step: 71899, 	{'train/accuracy': 0.49373045563697815, 'train/loss': 2.219071626663208, 'validation/accuracy': 0.4499399960041046, 'validation/loss': 2.4469850063323975, 'validation/num_examples': 50000, 'test/accuracy': 0.34880000352859497, 'test/loss': 3.096506118774414, 'test/num_examples': 10000, 'score': 32805.70732951164, 'total_duration': 35982.90106034279, 'accumulated_submission_time': 32805.70732951164, 'accumulated_eval_time': 3170.2405710220337, 'accumulated_logging_time': 3.131728172302246}
I0202 21:39:51.731142 139702527031040 logging_writer.py:48] [71899] accumulated_eval_time=3170.240571, accumulated_logging_time=3.131728, accumulated_submission_time=32805.707330, global_step=71899, preemption_count=0, score=32805.707330, test/accuracy=0.348800, test/loss=3.096506, test/num_examples=10000, total_duration=35982.901060, train/accuracy=0.493730, train/loss=2.219072, validation/accuracy=0.449940, validation/loss=2.446985, validation/num_examples=50000
I0202 21:39:52.540821 139702543816448 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.1282933950424194, loss=3.3566086292266846
I0202 21:40:34.139539 139702527031040 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.8805298805236816, loss=5.494643688201904
I0202 21:41:19.840734 139702543816448 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.0432406663894653, loss=3.498915433883667
I0202 21:42:05.993381 139702527031040 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.1257792711257935, loss=3.6429519653320312
I0202 21:42:51.843236 139702543816448 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.9035068154335022, loss=5.516566753387451
I0202 21:43:37.865632 139702527031040 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.1089962720870972, loss=3.3059170246124268
I0202 21:44:24.120697 139702543816448 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.0061571598052979, loss=4.256336688995361
I0202 21:45:10.508182 139702527031040 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.9041531085968018, loss=5.5561418533325195
I0202 21:45:56.346258 139702543816448 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.9111257791519165, loss=5.321310520172119
I0202 21:46:42.389666 139702527031040 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.916454017162323, loss=5.5233869552612305
I0202 21:46:51.859237 139863983413056 spec.py:321] Evaluating on the training split.
I0202 21:47:02.196409 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 21:47:32.512100 139863983413056 spec.py:349] Evaluating on the test split.
I0202 21:47:34.150609 139863983413056 submission_runner.py:408] Time since start: 36445.35s, 	Step: 72822, 	{'train/accuracy': 0.4931640625, 'train/loss': 2.2360095977783203, 'validation/accuracy': 0.46393999457359314, 'validation/loss': 2.3808131217956543, 'validation/num_examples': 50000, 'test/accuracy': 0.3605000078678131, 'test/loss': 3.04154372215271, 'test/num_examples': 10000, 'score': 33225.77615022659, 'total_duration': 36445.34771442413, 'accumulated_submission_time': 33225.77615022659, 'accumulated_eval_time': 3212.5319378376007, 'accumulated_logging_time': 3.1703007221221924}
I0202 21:47:34.177332 139702543816448 logging_writer.py:48] [72822] accumulated_eval_time=3212.531938, accumulated_logging_time=3.170301, accumulated_submission_time=33225.776150, global_step=72822, preemption_count=0, score=33225.776150, test/accuracy=0.360500, test/loss=3.041544, test/num_examples=10000, total_duration=36445.347714, train/accuracy=0.493164, train/loss=2.236010, validation/accuracy=0.463940, validation/loss=2.380813, validation/num_examples=50000
I0202 21:48:05.882370 139702527031040 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.0789064168930054, loss=3.3987743854522705
I0202 21:48:51.757506 139702543816448 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.7094487547874451, loss=5.275576591491699
I0202 21:49:37.981617 139702527031040 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.2695943117141724, loss=3.3101718425750732
I0202 21:50:24.308788 139702543816448 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.3273574113845825, loss=3.4713785648345947
I0202 21:51:10.372201 139702527031040 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.4082006216049194, loss=3.4001412391662598
I0202 21:51:56.408063 139702543816448 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.8368726968765259, loss=4.9630842208862305
I0202 21:52:42.633298 139702527031040 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.107967734336853, loss=3.3066415786743164
I0202 21:53:28.981810 139702543816448 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.9223207235336304, loss=5.187182903289795
I0202 21:54:15.021222 139702527031040 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.9241051077842712, loss=4.27193021774292
I0202 21:54:34.608276 139863983413056 spec.py:321] Evaluating on the training split.
I0202 21:54:45.259836 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 21:55:09.277513 139863983413056 spec.py:349] Evaluating on the test split.
I0202 21:55:10.927408 139863983413056 submission_runner.py:408] Time since start: 36902.12s, 	Step: 73744, 	{'train/accuracy': 0.48167967796325684, 'train/loss': 2.3041117191314697, 'validation/accuracy': 0.45201998949050903, 'validation/loss': 2.4658422470092773, 'validation/num_examples': 50000, 'test/accuracy': 0.3540000021457672, 'test/loss': 3.107847213745117, 'test/num_examples': 10000, 'score': 33646.14836072922, 'total_duration': 36902.12452173233, 'accumulated_submission_time': 33646.14836072922, 'accumulated_eval_time': 3248.851069688797, 'accumulated_logging_time': 3.207165241241455}
I0202 21:55:10.954184 139702543816448 logging_writer.py:48] [73744] accumulated_eval_time=3248.851070, accumulated_logging_time=3.207165, accumulated_submission_time=33646.148361, global_step=73744, preemption_count=0, score=33646.148361, test/accuracy=0.354000, test/loss=3.107847, test/num_examples=10000, total_duration=36902.124522, train/accuracy=0.481680, train/loss=2.304112, validation/accuracy=0.452020, validation/loss=2.465842, validation/num_examples=50000
I0202 21:55:33.740381 139702527031040 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.1253671646118164, loss=3.3997514247894287
I0202 21:56:18.262329 139702543816448 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.2735846042633057, loss=3.3008873462677
I0202 21:57:04.160060 139702527031040 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.9890273213386536, loss=3.6833810806274414
I0202 21:57:50.281775 139702543816448 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.0287790298461914, loss=3.3430280685424805
I0202 21:58:36.139417 139702527031040 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.6388368606567383, loss=3.3149471282958984
I0202 21:59:21.929213 139702543816448 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.183548927307129, loss=3.9997072219848633
I0202 22:00:08.134027 139702527031040 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.1719125509262085, loss=3.350158214569092
I0202 22:00:54.307474 139702543816448 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.993615984916687, loss=5.775046348571777
I0202 22:01:40.620580 139702527031040 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.013616919517517, loss=3.564539909362793
I0202 22:02:11.157221 139863983413056 spec.py:321] Evaluating on the training split.
I0202 22:02:21.518360 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 22:02:49.530006 139863983413056 spec.py:349] Evaluating on the test split.
I0202 22:02:51.166384 139863983413056 submission_runner.py:408] Time since start: 37362.36s, 	Step: 74668, 	{'train/accuracy': 0.5001562237739563, 'train/loss': 2.175226926803589, 'validation/accuracy': 0.46567997336387634, 'validation/loss': 2.3801429271698, 'validation/num_examples': 50000, 'test/accuracy': 0.36100003123283386, 'test/loss': 3.0509543418884277, 'test/num_examples': 10000, 'score': 34066.290695905685, 'total_duration': 37362.36349225044, 'accumulated_submission_time': 34066.290695905685, 'accumulated_eval_time': 3288.8602344989777, 'accumulated_logging_time': 3.2456398010253906}
I0202 22:02:51.199145 139702543816448 logging_writer.py:48] [74668] accumulated_eval_time=3288.860234, accumulated_logging_time=3.245640, accumulated_submission_time=34066.290696, global_step=74668, preemption_count=0, score=34066.290696, test/accuracy=0.361000, test/loss=3.050954, test/num_examples=10000, total_duration=37362.363492, train/accuracy=0.500156, train/loss=2.175227, validation/accuracy=0.465680, validation/loss=2.380143, validation/num_examples=50000
I0202 22:03:04.381249 139702527031040 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.8457880616188049, loss=5.770761966705322
I0202 22:03:47.671186 139702543816448 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.1822624206542969, loss=3.446197509765625
I0202 22:04:33.696503 139702527031040 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.8888866901397705, loss=4.676535606384277
I0202 22:05:19.853103 139702543816448 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.1076388359069824, loss=3.9907445907592773
I0202 22:06:06.442607 139702527031040 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.1097893714904785, loss=3.345625877380371
I0202 22:06:52.564400 139702543816448 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.0120465755462646, loss=3.3288350105285645
I0202 22:07:38.532022 139702527031040 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.0568292140960693, loss=3.257490634918213
I0202 22:08:24.714480 139702543816448 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.8955976366996765, loss=5.601794242858887
I0202 22:09:10.699836 139702527031040 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.0811903476715088, loss=3.344496011734009
I0202 22:09:51.641176 139863983413056 spec.py:321] Evaluating on the training split.
I0202 22:10:01.913602 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 22:10:33.101682 139863983413056 spec.py:349] Evaluating on the test split.
I0202 22:10:34.736557 139863983413056 submission_runner.py:408] Time since start: 37825.93s, 	Step: 75590, 	{'train/accuracy': 0.49730467796325684, 'train/loss': 2.2028791904449463, 'validation/accuracy': 0.46421998739242554, 'validation/loss': 2.3795573711395264, 'validation/num_examples': 50000, 'test/accuracy': 0.36330002546310425, 'test/loss': 3.0194509029388428, 'test/num_examples': 10000, 'score': 34486.66932654381, 'total_duration': 37825.93365931511, 'accumulated_submission_time': 34486.66932654381, 'accumulated_eval_time': 3331.9556045532227, 'accumulated_logging_time': 3.2923521995544434}
I0202 22:10:34.764247 139702543816448 logging_writer.py:48] [75590] accumulated_eval_time=3331.955605, accumulated_logging_time=3.292352, accumulated_submission_time=34486.669327, global_step=75590, preemption_count=0, score=34486.669327, test/accuracy=0.363300, test/loss=3.019451, test/num_examples=10000, total_duration=37825.933659, train/accuracy=0.497305, train/loss=2.202879, validation/accuracy=0.464220, validation/loss=2.379557, validation/num_examples=50000
I0202 22:10:39.162867 139702527031040 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.0581738948822021, loss=3.6719799041748047
I0202 22:11:21.144695 139702543816448 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.0539449453353882, loss=3.2756495475769043
I0202 22:12:07.218590 139702527031040 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.0847735404968262, loss=3.7621145248413086
I0202 22:12:53.432963 139702543816448 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.3406661748886108, loss=3.272533893585205
I0202 22:13:39.382259 139702527031040 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.2012170553207397, loss=3.201446771621704
I0202 22:14:25.687143 139702543816448 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.3463704586029053, loss=3.4495058059692383
I0202 22:15:11.712807 139702527031040 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.8033982515335083, loss=5.505494117736816
I0202 22:15:57.631511 139702543816448 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.024646282196045, loss=3.3840904235839844
I0202 22:16:44.549830 139702527031040 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.1481178998947144, loss=3.2597923278808594
I0202 22:17:30.600525 139702543816448 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.9127926230430603, loss=3.6276276111602783
I0202 22:17:34.991427 139863983413056 spec.py:321] Evaluating on the training split.
I0202 22:17:45.318293 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 22:18:11.266227 139863983413056 spec.py:349] Evaluating on the test split.
I0202 22:18:12.912873 139863983413056 submission_runner.py:408] Time since start: 38284.11s, 	Step: 76511, 	{'train/accuracy': 0.49726560711860657, 'train/loss': 2.211764097213745, 'validation/accuracy': 0.46473997831344604, 'validation/loss': 2.3848724365234375, 'validation/num_examples': 50000, 'test/accuracy': 0.3605000078678131, 'test/loss': 3.0538387298583984, 'test/num_examples': 10000, 'score': 34906.83700180054, 'total_duration': 38284.109989881516, 'accumulated_submission_time': 34906.83700180054, 'accumulated_eval_time': 3369.877052307129, 'accumulated_logging_time': 3.3313422203063965}
I0202 22:18:12.943829 139702527031040 logging_writer.py:48] [76511] accumulated_eval_time=3369.877052, accumulated_logging_time=3.331342, accumulated_submission_time=34906.837002, global_step=76511, preemption_count=0, score=34906.837002, test/accuracy=0.360500, test/loss=3.053839, test/num_examples=10000, total_duration=38284.109990, train/accuracy=0.497266, train/loss=2.211764, validation/accuracy=0.464740, validation/loss=2.384872, validation/num_examples=50000
I0202 22:18:49.822376 139702543816448 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.9676514267921448, loss=5.697113990783691
I0202 22:19:35.610550 139702527031040 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.1840652227401733, loss=3.403329610824585
I0202 22:20:22.264335 139702543816448 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.8966729044914246, loss=3.8475704193115234
I0202 22:21:08.430120 139702527031040 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.0818524360656738, loss=3.2807681560516357
I0202 22:21:54.519836 139702543816448 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.050917148590088, loss=3.2692456245422363
I0202 22:22:40.609109 139702527031040 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.2475208044052124, loss=3.6127593517303467
I0202 22:23:26.720955 139702543816448 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.9004393219947815, loss=5.81986141204834
I0202 22:24:12.720219 139702527031040 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.9796190857887268, loss=4.441413402557373
I0202 22:24:58.897521 139702543816448 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.045120120048523, loss=3.2871015071868896
I0202 22:25:12.970285 139863983413056 spec.py:321] Evaluating on the training split.
I0202 22:25:23.288876 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 22:25:53.751537 139863983413056 spec.py:349] Evaluating on the test split.
I0202 22:25:55.388415 139863983413056 submission_runner.py:408] Time since start: 38746.59s, 	Step: 77432, 	{'train/accuracy': 0.5082421898841858, 'train/loss': 2.163210153579712, 'validation/accuracy': 0.46818000078201294, 'validation/loss': 2.3664684295654297, 'validation/num_examples': 50000, 'test/accuracy': 0.36820000410079956, 'test/loss': 3.0080747604370117, 'test/num_examples': 10000, 'score': 35326.80542087555, 'total_duration': 38746.585496902466, 'accumulated_submission_time': 35326.80542087555, 'accumulated_eval_time': 3412.295135498047, 'accumulated_logging_time': 3.3718671798706055}
I0202 22:25:55.419136 139702527031040 logging_writer.py:48] [77432] accumulated_eval_time=3412.295135, accumulated_logging_time=3.371867, accumulated_submission_time=35326.805421, global_step=77432, preemption_count=0, score=35326.805421, test/accuracy=0.368200, test/loss=3.008075, test/num_examples=10000, total_duration=38746.585497, train/accuracy=0.508242, train/loss=2.163210, validation/accuracy=0.468180, validation/loss=2.366468, validation/num_examples=50000
I0202 22:26:23.195201 139702543816448 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.9170125126838684, loss=4.260622978210449
I0202 22:27:08.497996 139702527031040 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.1257257461547852, loss=3.3699750900268555
I0202 22:27:54.272958 139702543816448 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.0659351348876953, loss=3.573174476623535
I0202 22:28:40.286303 139702527031040 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.0872567892074585, loss=3.3477373123168945
I0202 22:29:26.502293 139702543816448 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.1894965171813965, loss=3.2664105892181396
I0202 22:30:12.621880 139702527031040 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.91029953956604, loss=4.699921607971191
I0202 22:30:58.656061 139702543816448 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.0118781328201294, loss=3.3108725547790527
I0202 22:31:44.702197 139702527031040 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.2135918140411377, loss=3.2916128635406494
I0202 22:32:30.612084 139702543816448 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.9749295711517334, loss=3.2557878494262695
I0202 22:32:55.550976 139863983413056 spec.py:321] Evaluating on the training split.
I0202 22:33:05.871200 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 22:33:37.640299 139863983413056 spec.py:349] Evaluating on the test split.
I0202 22:33:39.269152 139863983413056 submission_runner.py:408] Time since start: 39210.47s, 	Step: 78356, 	{'train/accuracy': 0.5389648079872131, 'train/loss': 2.0390491485595703, 'validation/accuracy': 0.47265997529029846, 'validation/loss': 2.385406494140625, 'validation/num_examples': 50000, 'test/accuracy': 0.37230002880096436, 'test/loss': 3.007336378097534, 'test/num_examples': 10000, 'score': 35746.876959085464, 'total_duration': 39210.46626496315, 'accumulated_submission_time': 35746.876959085464, 'accumulated_eval_time': 3456.013298511505, 'accumulated_logging_time': 3.413897752761841}
I0202 22:33:39.299756 139702527031040 logging_writer.py:48] [78356] accumulated_eval_time=3456.013299, accumulated_logging_time=3.413898, accumulated_submission_time=35746.876959, global_step=78356, preemption_count=0, score=35746.876959, test/accuracy=0.372300, test/loss=3.007336, test/num_examples=10000, total_duration=39210.466265, train/accuracy=0.538965, train/loss=2.039049, validation/accuracy=0.472660, validation/loss=2.385406, validation/num_examples=50000
I0202 22:33:57.259072 139702543816448 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.9836002588272095, loss=5.133550643920898
I0202 22:34:41.196550 139702527031040 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.0757447481155396, loss=3.4140758514404297
I0202 22:35:27.400728 139702543816448 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.1065032482147217, loss=3.2436187267303467
I0202 22:36:13.570284 139702527031040 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.202906608581543, loss=3.2637205123901367
I0202 22:36:59.534488 139702543816448 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.0324983596801758, loss=3.505113124847412
I0202 22:37:45.520960 139702527031040 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.8555226922035217, loss=4.3465800285339355
I0202 22:38:31.387632 139702543816448 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.052417278289795, loss=3.463599681854248
I0202 22:39:17.462934 139702527031040 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.1327064037322998, loss=3.2980058193206787
I0202 22:40:03.525496 139702543816448 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.8622994422912598, loss=4.770925998687744
I0202 22:40:39.480422 139863983413056 spec.py:321] Evaluating on the training split.
I0202 22:40:49.934824 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 22:41:19.631564 139863983413056 spec.py:349] Evaluating on the test split.
I0202 22:41:21.273680 139863983413056 submission_runner.py:408] Time since start: 39672.47s, 	Step: 79280, 	{'train/accuracy': 0.5073632597923279, 'train/loss': 2.175447940826416, 'validation/accuracy': 0.4732399880886078, 'validation/loss': 2.356532573699951, 'validation/num_examples': 50000, 'test/accuracy': 0.3680000305175781, 'test/loss': 2.9997689723968506, 'test/num_examples': 10000, 'score': 36166.997678518295, 'total_duration': 39672.47076559067, 'accumulated_submission_time': 36166.997678518295, 'accumulated_eval_time': 3497.8065342903137, 'accumulated_logging_time': 3.4558606147766113}
I0202 22:41:21.306743 139702527031040 logging_writer.py:48] [79280] accumulated_eval_time=3497.806534, accumulated_logging_time=3.455861, accumulated_submission_time=36166.997679, global_step=79280, preemption_count=0, score=36166.997679, test/accuracy=0.368000, test/loss=2.999769, test/num_examples=10000, total_duration=39672.470766, train/accuracy=0.507363, train/loss=2.175448, validation/accuracy=0.473240, validation/loss=2.356533, validation/num_examples=50000
I0202 22:41:29.693828 139702543816448 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.2644208669662476, loss=3.338550329208374
I0202 22:42:12.198009 139702527031040 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.052464485168457, loss=3.0581891536712646
I0202 22:42:58.006248 139702543816448 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.215592384338379, loss=3.3149633407592773
I0202 22:43:44.147420 139702527031040 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.9599079489707947, loss=3.3928470611572266
I0202 22:44:30.021543 139702543816448 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.9270567297935486, loss=4.070410251617432
I0202 22:45:16.085046 139702527031040 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.8661671280860901, loss=5.1390275955200195
I0202 22:46:02.182554 139702543816448 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.8017650246620178, loss=4.442225456237793
I0202 22:46:48.030182 139702527031040 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.0110688209533691, loss=3.4674770832061768
I0202 22:47:34.392810 139702543816448 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.138301968574524, loss=3.3196136951446533
I0202 22:48:20.402529 139702527031040 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.3058892488479614, loss=3.197336196899414
I0202 22:48:21.432138 139863983413056 spec.py:321] Evaluating on the training split.
I0202 22:48:31.824790 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 22:48:59.072810 139863983413056 spec.py:349] Evaluating on the test split.
I0202 22:49:00.716304 139863983413056 submission_runner.py:408] Time since start: 40131.91s, 	Step: 80204, 	{'train/accuracy': 0.5040234327316284, 'train/loss': 2.234501361846924, 'validation/accuracy': 0.469760000705719, 'validation/loss': 2.405181884765625, 'validation/num_examples': 50000, 'test/accuracy': 0.3631000220775604, 'test/loss': 3.042214870452881, 'test/num_examples': 10000, 'score': 36587.06204032898, 'total_duration': 40131.91340637207, 'accumulated_submission_time': 36587.06204032898, 'accumulated_eval_time': 3537.0906777381897, 'accumulated_logging_time': 3.5011050701141357}
I0202 22:49:00.745859 139702543816448 logging_writer.py:48] [80204] accumulated_eval_time=3537.090678, accumulated_logging_time=3.501105, accumulated_submission_time=36587.062040, global_step=80204, preemption_count=0, score=36587.062040, test/accuracy=0.363100, test/loss=3.042215, test/num_examples=10000, total_duration=40131.913406, train/accuracy=0.504023, train/loss=2.234501, validation/accuracy=0.469760, validation/loss=2.405182, validation/num_examples=50000
I0202 22:49:40.708204 139702527031040 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.9766733646392822, loss=3.6375296115875244
I0202 22:50:26.846385 139702543816448 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.9142706990242004, loss=4.189971923828125
I0202 22:51:13.055477 139702527031040 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.139794111251831, loss=3.259021520614624
I0202 22:51:58.886807 139702543816448 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.0539779663085938, loss=5.693992614746094
I0202 22:52:45.010658 139702527031040 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.0695432424545288, loss=3.235443353652954
I0202 22:53:31.095340 139702543816448 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.97056645154953, loss=3.312769889831543
I0202 22:54:17.094892 139702527031040 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.0794960260391235, loss=3.539437770843506
I0202 22:55:03.148488 139702543816448 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.0002890825271606, loss=3.709033966064453
I0202 22:55:49.496333 139702527031040 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.0493412017822266, loss=3.173346996307373
I0202 22:56:01.108449 139863983413056 spec.py:321] Evaluating on the training split.
I0202 22:56:11.669565 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 22:56:40.746391 139863983413056 spec.py:349] Evaluating on the test split.
I0202 22:56:42.385302 139863983413056 submission_runner.py:408] Time since start: 40593.58s, 	Step: 81127, 	{'train/accuracy': 0.5310351252555847, 'train/loss': 2.0206193923950195, 'validation/accuracy': 0.48405998945236206, 'validation/loss': 2.27791166305542, 'validation/num_examples': 50000, 'test/accuracy': 0.3806000053882599, 'test/loss': 2.9327199459075928, 'test/num_examples': 10000, 'score': 37007.36421537399, 'total_duration': 40593.58241772652, 'accumulated_submission_time': 37007.36421537399, 'accumulated_eval_time': 3578.3675305843353, 'accumulated_logging_time': 3.5425891876220703}
I0202 22:56:42.413786 139702543816448 logging_writer.py:48] [81127] accumulated_eval_time=3578.367531, accumulated_logging_time=3.542589, accumulated_submission_time=37007.364215, global_step=81127, preemption_count=0, score=37007.364215, test/accuracy=0.380600, test/loss=2.932720, test/num_examples=10000, total_duration=40593.582418, train/accuracy=0.531035, train/loss=2.020619, validation/accuracy=0.484060, validation/loss=2.277912, validation/num_examples=50000
I0202 22:57:12.016644 139702527031040 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.915173351764679, loss=4.961716651916504
I0202 22:57:57.992307 139702543816448 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.9832630157470703, loss=5.178177833557129
I0202 22:58:44.616331 139702527031040 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.9632402658462524, loss=5.684421539306641
I0202 22:59:30.870931 139702543816448 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.8840748071670532, loss=4.172224998474121
I0202 23:00:17.267120 139702527031040 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.0145392417907715, loss=3.9904885292053223
I0202 23:01:03.347184 139702543816448 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.9676835536956787, loss=4.563869476318359
I0202 23:01:49.651908 139702527031040 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.2745260000228882, loss=3.3837289810180664
I0202 23:02:36.087454 139702543816448 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.9932679533958435, loss=3.1874818801879883
I0202 23:03:22.234180 139702527031040 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.041122555732727, loss=3.1060802936553955
I0202 23:03:42.699831 139863983413056 spec.py:321] Evaluating on the training split.
I0202 23:03:53.304399 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 23:04:22.555853 139863983413056 spec.py:349] Evaluating on the test split.
I0202 23:04:24.198309 139863983413056 submission_runner.py:408] Time since start: 41055.40s, 	Step: 82046, 	{'train/accuracy': 0.5068749785423279, 'train/loss': 2.1524007320404053, 'validation/accuracy': 0.4747999906539917, 'validation/loss': 2.3249146938323975, 'validation/num_examples': 50000, 'test/accuracy': 0.3708000183105469, 'test/loss': 2.9745867252349854, 'test/num_examples': 10000, 'score': 37427.59137535095, 'total_duration': 41055.39542388916, 'accumulated_submission_time': 37427.59137535095, 'accumulated_eval_time': 3619.8660113811493, 'accumulated_logging_time': 3.581484079360962}
I0202 23:04:24.233389 139702543816448 logging_writer.py:48] [82046] accumulated_eval_time=3619.866011, accumulated_logging_time=3.581484, accumulated_submission_time=37427.591375, global_step=82046, preemption_count=0, score=37427.591375, test/accuracy=0.370800, test/loss=2.974587, test/num_examples=10000, total_duration=41055.395424, train/accuracy=0.506875, train/loss=2.152401, validation/accuracy=0.474800, validation/loss=2.324915, validation/num_examples=50000
I0202 23:04:46.215249 139702527031040 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.3161789178848267, loss=3.3779571056365967
I0202 23:05:30.905465 139702543816448 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.9419671893119812, loss=4.280270576477051
I0202 23:06:17.285733 139702527031040 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.0154483318328857, loss=3.9080936908721924
I0202 23:07:03.667833 139702543816448 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.1707497835159302, loss=3.277090072631836
I0202 23:07:50.136186 139702527031040 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.0782313346862793, loss=3.2027504444122314
I0202 23:08:36.672543 139702543816448 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.0246334075927734, loss=3.6886565685272217
I0202 23:09:22.499689 139702527031040 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.8663761019706726, loss=4.390079021453857
I0202 23:10:08.490478 139702543816448 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.8703514933586121, loss=4.372952461242676
I0202 23:10:54.765499 139702527031040 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.7795344591140747, loss=5.196924209594727
I0202 23:11:24.229692 139863983413056 spec.py:321] Evaluating on the training split.
I0202 23:11:34.898488 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 23:12:05.682668 139863983413056 spec.py:349] Evaluating on the test split.
I0202 23:12:07.318155 139863983413056 submission_runner.py:408] Time since start: 41518.52s, 	Step: 82966, 	{'train/accuracy': 0.5151953101158142, 'train/loss': 2.1322968006134033, 'validation/accuracy': 0.47915998101234436, 'validation/loss': 2.317836284637451, 'validation/num_examples': 50000, 'test/accuracy': 0.3758000135421753, 'test/loss': 2.9638731479644775, 'test/num_examples': 10000, 'score': 37847.527940034866, 'total_duration': 41518.515270233154, 'accumulated_submission_time': 37847.527940034866, 'accumulated_eval_time': 3662.954482316971, 'accumulated_logging_time': 3.6278164386749268}
I0202 23:12:07.354270 139702543816448 logging_writer.py:48] [82966] accumulated_eval_time=3662.954482, accumulated_logging_time=3.627816, accumulated_submission_time=37847.527940, global_step=82966, preemption_count=0, score=37847.527940, test/accuracy=0.375800, test/loss=2.963873, test/num_examples=10000, total_duration=41518.515270, train/accuracy=0.515195, train/loss=2.132297, validation/accuracy=0.479160, validation/loss=2.317836, validation/num_examples=50000
I0202 23:12:21.337630 139702527031040 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.214561104774475, loss=3.2875146865844727
I0202 23:13:04.783300 139702543816448 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.9869560599327087, loss=5.509089469909668
I0202 23:13:50.933250 139702527031040 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.1663836240768433, loss=3.2283856868743896
I0202 23:14:37.061023 139702543816448 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.114912748336792, loss=3.2870893478393555
I0202 23:15:23.123131 139702527031040 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.0522245168685913, loss=3.2736549377441406
I0202 23:16:09.178798 139702543816448 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.2675801515579224, loss=3.1749954223632812
I0202 23:16:55.199661 139702527031040 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.2788234949111938, loss=3.4561307430267334
I0202 23:17:41.575333 139702543816448 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.1071052551269531, loss=3.5688185691833496
I0202 23:18:27.612873 139702527031040 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.07093346118927, loss=3.247265577316284
I0202 23:19:07.607861 139863983413056 spec.py:321] Evaluating on the training split.
I0202 23:19:17.861464 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 23:19:46.231695 139863983413056 spec.py:349] Evaluating on the test split.
I0202 23:19:47.867029 139863983413056 submission_runner.py:408] Time since start: 41979.06s, 	Step: 83888, 	{'train/accuracy': 0.5267773270606995, 'train/loss': 2.0652716159820557, 'validation/accuracy': 0.4811999797821045, 'validation/loss': 2.2873899936676025, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 2.935068368911743, 'test/num_examples': 10000, 'score': 38267.72216629982, 'total_duration': 41979.064101696014, 'accumulated_submission_time': 38267.72216629982, 'accumulated_eval_time': 3703.2136142253876, 'accumulated_logging_time': 3.6741631031036377}
I0202 23:19:47.900302 139702543816448 logging_writer.py:48] [83888] accumulated_eval_time=3703.213614, accumulated_logging_time=3.674163, accumulated_submission_time=38267.722166, global_step=83888, preemption_count=0, score=38267.722166, test/accuracy=0.377500, test/loss=2.935068, test/num_examples=10000, total_duration=41979.064102, train/accuracy=0.526777, train/loss=2.065272, validation/accuracy=0.481200, validation/loss=2.287390, validation/num_examples=50000
I0202 23:19:53.088831 139702527031040 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.844801664352417, loss=5.185192108154297
I0202 23:20:35.213119 139702543816448 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.2240160703659058, loss=3.207024574279785
I0202 23:21:21.222911 139702527031040 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.121026873588562, loss=3.135563611984253
I0202 23:22:07.511374 139702543816448 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.218932032585144, loss=3.453582763671875
I0202 23:22:53.483193 139702527031040 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.0588877201080322, loss=3.0734384059906006
I0202 23:23:39.779945 139702543816448 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.0420303344726562, loss=3.1467251777648926
I0202 23:24:25.692502 139702527031040 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.0508991479873657, loss=3.0171730518341064
I0202 23:25:11.613969 139702543816448 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.4050102233886719, loss=3.4335341453552246
I0202 23:25:57.585902 139702527031040 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.8815210461616516, loss=4.463702201843262
I0202 23:26:43.808385 139702543816448 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.1890137195587158, loss=3.3617897033691406
I0202 23:26:48.124429 139863983413056 spec.py:321] Evaluating on the training split.
I0202 23:26:58.569777 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 23:27:27.071946 139863983413056 spec.py:349] Evaluating on the test split.
I0202 23:27:28.709254 139863983413056 submission_runner.py:408] Time since start: 42439.91s, 	Step: 84811, 	{'train/accuracy': 0.52587890625, 'train/loss': 2.056821823120117, 'validation/accuracy': 0.49289998412132263, 'validation/loss': 2.2359001636505127, 'validation/num_examples': 50000, 'test/accuracy': 0.3895000219345093, 'test/loss': 2.8796260356903076, 'test/num_examples': 10000, 'score': 38687.88592624664, 'total_duration': 42439.906369924545, 'accumulated_submission_time': 38687.88592624664, 'accumulated_eval_time': 3743.7984251976013, 'accumulated_logging_time': 3.718817710876465}
I0202 23:27:28.737524 139702527031040 logging_writer.py:48] [84811] accumulated_eval_time=3743.798425, accumulated_logging_time=3.718818, accumulated_submission_time=38687.885926, global_step=84811, preemption_count=0, score=38687.885926, test/accuracy=0.389500, test/loss=2.879626, test/num_examples=10000, total_duration=42439.906370, train/accuracy=0.525879, train/loss=2.056822, validation/accuracy=0.492900, validation/loss=2.235900, validation/num_examples=50000
I0202 23:28:05.630343 139702543816448 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.2507305145263672, loss=3.0642080307006836
I0202 23:28:51.891105 139702527031040 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.1825437545776367, loss=3.339223623275757
I0202 23:29:38.534856 139702543816448 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.911803662776947, loss=4.050478935241699
I0202 23:30:24.479252 139702527031040 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.1612775325775146, loss=3.2293288707733154
I0202 23:31:10.769589 139702543816448 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.8891634941101074, loss=5.0345964431762695
I0202 23:31:56.691540 139702527031040 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.065212607383728, loss=4.342044353485107
I0202 23:32:42.603439 139702543816448 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.1982804536819458, loss=3.4902567863464355
I0202 23:33:28.701636 139702527031040 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.097476840019226, loss=3.64005184173584
I0202 23:34:14.734150 139702543816448 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.9950402975082397, loss=3.3861429691314697
I0202 23:34:29.078124 139863983413056 spec.py:321] Evaluating on the training split.
I0202 23:34:39.578048 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 23:35:08.574644 139863983413056 spec.py:349] Evaluating on the test split.
I0202 23:35:10.218782 139863983413056 submission_runner.py:408] Time since start: 42901.42s, 	Step: 85733, 	{'train/accuracy': 0.517382800579071, 'train/loss': 2.116588830947876, 'validation/accuracy': 0.48499998450279236, 'validation/loss': 2.2845561504364014, 'validation/num_examples': 50000, 'test/accuracy': 0.3757000267505646, 'test/loss': 2.9468016624450684, 'test/num_examples': 10000, 'score': 39108.16635656357, 'total_duration': 42901.415877103806, 'accumulated_submission_time': 39108.16635656357, 'accumulated_eval_time': 3784.9390771389008, 'accumulated_logging_time': 3.758352756500244}
I0202 23:35:10.252653 139702527031040 logging_writer.py:48] [85733] accumulated_eval_time=3784.939077, accumulated_logging_time=3.758353, accumulated_submission_time=39108.166357, global_step=85733, preemption_count=0, score=39108.166357, test/accuracy=0.375700, test/loss=2.946802, test/num_examples=10000, total_duration=42901.415877, train/accuracy=0.517383, train/loss=2.116589, validation/accuracy=0.485000, validation/loss=2.284556, validation/num_examples=50000
I0202 23:35:37.632529 139702543816448 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.2173973321914673, loss=3.288855791091919
I0202 23:36:22.769829 139702527031040 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.005275011062622, loss=5.630261421203613
I0202 23:37:08.849673 139702543816448 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.7577563524246216, loss=5.572264194488525
I0202 23:37:55.133066 139702527031040 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.9708748459815979, loss=5.098194122314453
I0202 23:38:41.074774 139702543816448 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.1059104204177856, loss=5.148398399353027
I0202 23:39:27.121709 139702527031040 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.0872349739074707, loss=3.128290891647339
I0202 23:40:13.510983 139702543816448 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.1970700025558472, loss=3.187234401702881
I0202 23:40:59.355511 139702527031040 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.08000648021698, loss=3.2810873985290527
I0202 23:41:45.672508 139702543816448 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.0486844778060913, loss=3.3968236446380615
I0202 23:42:10.275555 139863983413056 spec.py:321] Evaluating on the training split.
I0202 23:42:21.639056 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 23:42:49.260086 139863983413056 spec.py:349] Evaluating on the test split.
I0202 23:42:50.903979 139863983413056 submission_runner.py:408] Time since start: 43362.10s, 	Step: 86655, 	{'train/accuracy': 0.5392773151397705, 'train/loss': 2.0084869861602783, 'validation/accuracy': 0.5009399652481079, 'validation/loss': 2.2077176570892334, 'validation/num_examples': 50000, 'test/accuracy': 0.3890000283718109, 'test/loss': 2.89555287361145, 'test/num_examples': 10000, 'score': 39528.129534721375, 'total_duration': 43362.101089954376, 'accumulated_submission_time': 39528.129534721375, 'accumulated_eval_time': 3825.567506790161, 'accumulated_logging_time': 3.802644729614258}
I0202 23:42:50.933623 139702527031040 logging_writer.py:48] [86655] accumulated_eval_time=3825.567507, accumulated_logging_time=3.802645, accumulated_submission_time=39528.129535, global_step=86655, preemption_count=0, score=39528.129535, test/accuracy=0.389000, test/loss=2.895553, test/num_examples=10000, total_duration=43362.101090, train/accuracy=0.539277, train/loss=2.008487, validation/accuracy=0.500940, validation/loss=2.207718, validation/num_examples=50000
I0202 23:43:09.310355 139702543816448 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.246902346611023, loss=3.2963056564331055
I0202 23:43:53.092782 139702527031040 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.1310213804244995, loss=3.3820114135742188
I0202 23:44:39.547557 139702543816448 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.0413380861282349, loss=2.9904980659484863
I0202 23:45:25.810826 139702527031040 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.9123965501785278, loss=5.791103363037109
I0202 23:46:12.142869 139702543816448 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.032987117767334, loss=4.431375026702881
I0202 23:46:58.229424 139702527031040 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.0923759937286377, loss=3.237560272216797
I0202 23:47:44.159979 139702543816448 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.1529916524887085, loss=3.08561635017395
I0202 23:48:30.418280 139702527031040 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.1293220520019531, loss=3.063689708709717
I0202 23:49:16.271151 139702543816448 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.8670650720596313, loss=4.8185343742370605
I0202 23:49:50.984934 139863983413056 spec.py:321] Evaluating on the training split.
I0202 23:50:01.549345 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 23:50:31.604917 139863983413056 spec.py:349] Evaluating on the test split.
I0202 23:50:33.257280 139863983413056 submission_runner.py:408] Time since start: 43824.45s, 	Step: 87576, 	{'train/accuracy': 0.5444140434265137, 'train/loss': 1.9728312492370605, 'validation/accuracy': 0.492499977350235, 'validation/loss': 2.241783618927002, 'validation/num_examples': 50000, 'test/accuracy': 0.38040003180503845, 'test/loss': 2.9136619567871094, 'test/num_examples': 10000, 'score': 39948.1226670742, 'total_duration': 43824.45436620712, 'accumulated_submission_time': 39948.1226670742, 'accumulated_eval_time': 3867.839832305908, 'accumulated_logging_time': 3.842219352722168}
I0202 23:50:33.292152 139702527031040 logging_writer.py:48] [87576] accumulated_eval_time=3867.839832, accumulated_logging_time=3.842219, accumulated_submission_time=39948.122667, global_step=87576, preemption_count=0, score=39948.122667, test/accuracy=0.380400, test/loss=2.913662, test/num_examples=10000, total_duration=43824.454366, train/accuracy=0.544414, train/loss=1.972831, validation/accuracy=0.492500, validation/loss=2.241784, validation/num_examples=50000
I0202 23:50:43.276704 139702543816448 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.0547113418579102, loss=4.186553955078125
I0202 23:51:26.137605 139702527031040 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.2334481477737427, loss=3.1257004737854004
I0202 23:52:12.120142 139702543816448 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.0865827798843384, loss=3.1333343982696533
I0202 23:52:58.122011 139702527031040 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.9395187497138977, loss=5.236999988555908
I0202 23:53:43.844130 139702543816448 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.0019992589950562, loss=5.218179225921631
I0202 23:54:30.441114 139702527031040 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.3980509042739868, loss=3.3611721992492676
I0202 23:55:16.507037 139702543816448 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.2400856018066406, loss=3.0582947731018066
I0202 23:56:02.925631 139702527031040 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.0900509357452393, loss=3.07338285446167
I0202 23:56:48.869011 139702543816448 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.0498924255371094, loss=3.172462224960327
I0202 23:57:33.685023 139863983413056 spec.py:321] Evaluating on the training split.
I0202 23:57:44.147146 139863983413056 spec.py:333] Evaluating on the validation split.
I0202 23:58:09.296825 139863983413056 spec.py:349] Evaluating on the test split.
I0202 23:58:10.941196 139863983413056 submission_runner.py:408] Time since start: 44282.14s, 	Step: 88499, 	{'train/accuracy': 0.5226757526397705, 'train/loss': 2.081761598587036, 'validation/accuracy': 0.491599977016449, 'validation/loss': 2.257352590560913, 'validation/num_examples': 50000, 'test/accuracy': 0.3840000033378601, 'test/loss': 2.8978774547576904, 'test/num_examples': 10000, 'score': 40368.45597243309, 'total_duration': 44282.1383099556, 'accumulated_submission_time': 40368.45597243309, 'accumulated_eval_time': 3905.0960161685944, 'accumulated_logging_time': 3.8879823684692383}
I0202 23:58:10.971338 139702527031040 logging_writer.py:48] [88499] accumulated_eval_time=3905.096016, accumulated_logging_time=3.887982, accumulated_submission_time=40368.455972, global_step=88499, preemption_count=0, score=40368.455972, test/accuracy=0.384000, test/loss=2.897877, test/num_examples=10000, total_duration=44282.138310, train/accuracy=0.522676, train/loss=2.081762, validation/accuracy=0.491600, validation/loss=2.257353, validation/num_examples=50000
I0202 23:58:11.780384 139702543816448 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.0518757104873657, loss=3.441816806793213
I0202 23:58:53.181200 139702527031040 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.0752511024475098, loss=3.09743332862854
I0202 23:59:39.284915 139702543816448 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.9771949648857117, loss=4.693027019500732
I0203 00:00:25.972740 139702527031040 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.9265561699867249, loss=5.4202046394348145
I0203 00:01:12.318852 139702543816448 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.0420305728912354, loss=3.2731595039367676
I0203 00:01:58.345387 139702527031040 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.9839283227920532, loss=4.039104461669922
I0203 00:02:44.535892 139702543816448 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.9714381098747253, loss=4.285013198852539
I0203 00:03:30.502537 139702527031040 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.3258877992630005, loss=3.209947109222412
I0203 00:04:16.709970 139702543816448 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.1392236948013306, loss=3.122204065322876
I0203 00:05:02.656551 139702527031040 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.1677923202514648, loss=3.0824947357177734
I0203 00:05:11.064491 139863983413056 spec.py:321] Evaluating on the training split.
I0203 00:05:21.358758 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 00:05:52.914653 139863983413056 spec.py:349] Evaluating on the test split.
I0203 00:05:54.547816 139863983413056 submission_runner.py:408] Time since start: 44745.74s, 	Step: 89420, 	{'train/accuracy': 0.5298827886581421, 'train/loss': 2.077981472015381, 'validation/accuracy': 0.4909399747848511, 'validation/loss': 2.27143931388855, 'validation/num_examples': 50000, 'test/accuracy': 0.38270002603530884, 'test/loss': 2.9215173721313477, 'test/num_examples': 10000, 'score': 40788.488649606705, 'total_duration': 44745.744921684265, 'accumulated_submission_time': 40788.488649606705, 'accumulated_eval_time': 3948.5793375968933, 'accumulated_logging_time': 3.9286277294158936}
I0203 00:05:54.578668 139702543816448 logging_writer.py:48] [89420] accumulated_eval_time=3948.579338, accumulated_logging_time=3.928628, accumulated_submission_time=40788.488650, global_step=89420, preemption_count=0, score=40788.488650, test/accuracy=0.382700, test/loss=2.921517, test/num_examples=10000, total_duration=44745.744922, train/accuracy=0.529883, train/loss=2.077981, validation/accuracy=0.490940, validation/loss=2.271439, validation/num_examples=50000
I0203 00:06:27.263613 139702527031040 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.1225006580352783, loss=3.1830663681030273
I0203 00:07:13.215317 139702543816448 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.0379709005355835, loss=3.498728036880493
I0203 00:07:59.685648 139702527031040 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.2670291662216187, loss=3.059685230255127
I0203 00:08:45.482346 139702543816448 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.0096668004989624, loss=3.437206506729126
I0203 00:09:31.762509 139702527031040 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.1107553243637085, loss=3.0446386337280273
I0203 00:10:17.824450 139702543816448 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.2531468868255615, loss=3.045444965362549
I0203 00:11:04.062152 139702527031040 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.1571568250656128, loss=3.05625057220459
I0203 00:11:49.990314 139702543816448 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.2905791997909546, loss=3.063746452331543
I0203 00:12:35.920142 139702527031040 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.0552337169647217, loss=3.103757858276367
I0203 00:12:54.987904 139863983413056 spec.py:321] Evaluating on the training split.
I0203 00:13:05.362899 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 00:13:38.634030 139863983413056 spec.py:349] Evaluating on the test split.
I0203 00:13:40.281550 139863983413056 submission_runner.py:408] Time since start: 45211.48s, 	Step: 90343, 	{'train/accuracy': 0.5615038871765137, 'train/loss': 1.8954758644104004, 'validation/accuracy': 0.5057199597358704, 'validation/loss': 2.180562734603882, 'validation/num_examples': 50000, 'test/accuracy': 0.3936000168323517, 'test/loss': 2.8484578132629395, 'test/num_examples': 10000, 'score': 41208.838418483734, 'total_duration': 45211.47863817215, 'accumulated_submission_time': 41208.838418483734, 'accumulated_eval_time': 3993.8729910850525, 'accumulated_logging_time': 3.9703309535980225}
I0203 00:13:40.316086 139702543816448 logging_writer.py:48] [90343] accumulated_eval_time=3993.872991, accumulated_logging_time=3.970331, accumulated_submission_time=41208.838418, global_step=90343, preemption_count=0, score=41208.838418, test/accuracy=0.393600, test/loss=2.848458, test/num_examples=10000, total_duration=45211.478638, train/accuracy=0.561504, train/loss=1.895476, validation/accuracy=0.505720, validation/loss=2.180563, validation/num_examples=50000
I0203 00:14:03.495642 139702527031040 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.0547090768814087, loss=3.476951837539673
I0203 00:14:48.088928 139702543816448 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.072009563446045, loss=3.305058240890503
I0203 00:15:34.169767 139702527031040 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.9522360563278198, loss=3.5414395332336426
I0203 00:16:20.093593 139702543816448 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.1081064939498901, loss=3.179051637649536
I0203 00:17:06.307971 139702527031040 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.0436235666275024, loss=3.2012643814086914
I0203 00:17:52.097192 139702543816448 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.9561624526977539, loss=5.647824287414551
I0203 00:18:38.265008 139702527031040 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.400530457496643, loss=3.106947660446167
I0203 00:19:24.205121 139702543816448 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.067138671875, loss=3.0765647888183594
I0203 00:20:10.449468 139702527031040 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.0237427949905396, loss=3.507279634475708
I0203 00:20:40.599773 139863983413056 spec.py:321] Evaluating on the training split.
I0203 00:20:51.306936 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 00:21:22.666218 139863983413056 spec.py:349] Evaluating on the test split.
I0203 00:21:24.301835 139863983413056 submission_runner.py:408] Time since start: 45675.50s, 	Step: 91267, 	{'train/accuracy': 0.5329882502555847, 'train/loss': 2.0743818283081055, 'validation/accuracy': 0.5001400113105774, 'validation/loss': 2.2381608486175537, 'validation/num_examples': 50000, 'test/accuracy': 0.38920003175735474, 'test/loss': 2.885896921157837, 'test/num_examples': 10000, 'score': 41629.06167125702, 'total_duration': 45675.498945236206, 'accumulated_submission_time': 41629.06167125702, 'accumulated_eval_time': 4037.57505941391, 'accumulated_logging_time': 4.016332626342773}
I0203 00:21:24.336331 139702543816448 logging_writer.py:48] [91267] accumulated_eval_time=4037.575059, accumulated_logging_time=4.016333, accumulated_submission_time=41629.061671, global_step=91267, preemption_count=0, score=41629.061671, test/accuracy=0.389200, test/loss=2.885897, test/num_examples=10000, total_duration=45675.498945, train/accuracy=0.532988, train/loss=2.074382, validation/accuracy=0.500140, validation/loss=2.238161, validation/num_examples=50000
I0203 00:21:38.380923 139702527031040 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.123980164527893, loss=3.2367587089538574
I0203 00:22:21.719102 139702543816448 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.0632541179656982, loss=3.120588541030884
I0203 00:23:07.614861 139702527031040 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.9776433706283569, loss=4.428287029266357
I0203 00:23:53.920399 139702543816448 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.1887534856796265, loss=3.3708748817443848
I0203 00:24:40.065824 139702527031040 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.1254323720932007, loss=3.765542507171631
I0203 00:25:26.356468 139702543816448 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.1115200519561768, loss=5.656772613525391
I0203 00:26:12.558608 139702527031040 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.1583796739578247, loss=5.344355583190918
I0203 00:26:58.669650 139702543816448 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.2871755361557007, loss=2.9495437145233154
I0203 00:27:44.677625 139702527031040 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.2393198013305664, loss=3.257781505584717
I0203 00:28:24.750417 139863983413056 spec.py:321] Evaluating on the training split.
I0203 00:28:35.175963 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 00:29:03.812860 139863983413056 spec.py:349] Evaluating on the test split.
I0203 00:29:05.454925 139863983413056 submission_runner.py:408] Time since start: 46136.65s, 	Step: 92188, 	{'train/accuracy': 0.5424413681030273, 'train/loss': 1.9888652563095093, 'validation/accuracy': 0.5024399757385254, 'validation/loss': 2.1791367530822754, 'validation/num_examples': 50000, 'test/accuracy': 0.3930000066757202, 'test/loss': 2.8366096019744873, 'test/num_examples': 10000, 'score': 42048.947972774506, 'total_duration': 46136.65203642845, 'accumulated_submission_time': 42048.947972774506, 'accumulated_eval_time': 4078.279564142227, 'accumulated_logging_time': 4.529749393463135}
I0203 00:29:05.488740 139702543816448 logging_writer.py:48] [92188] accumulated_eval_time=4078.279564, accumulated_logging_time=4.529749, accumulated_submission_time=42048.947973, global_step=92188, preemption_count=0, score=42048.947973, test/accuracy=0.393000, test/loss=2.836610, test/num_examples=10000, total_duration=46136.652036, train/accuracy=0.542441, train/loss=1.988865, validation/accuracy=0.502440, validation/loss=2.179137, validation/num_examples=50000
I0203 00:29:10.673107 139702527031040 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.1539278030395508, loss=3.371126174926758
I0203 00:29:53.054797 139702543816448 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.8599293828010559, loss=5.40801477432251
I0203 00:30:39.120484 139702527031040 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.1023393869400024, loss=3.1700754165649414
I0203 00:31:25.484441 139702543816448 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.1640044450759888, loss=3.174227476119995
I0203 00:32:11.862274 139702527031040 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.0413458347320557, loss=3.688286781311035
I0203 00:32:57.679707 139702543816448 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.1408694982528687, loss=3.0196502208709717
I0203 00:33:43.775207 139702527031040 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.2706987857818604, loss=3.0838394165039062
I0203 00:34:29.713085 139702543816448 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.1491566896438599, loss=4.6778669357299805
I0203 00:35:15.906027 139702527031040 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.274425983428955, loss=3.3593294620513916
I0203 00:36:01.940432 139702543816448 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.9374581575393677, loss=3.758115291595459
I0203 00:36:05.778772 139863983413056 spec.py:321] Evaluating on the training split.
I0203 00:36:16.225546 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 00:36:46.815569 139863983413056 spec.py:349] Evaluating on the test split.
I0203 00:36:48.456969 139863983413056 submission_runner.py:408] Time since start: 46599.65s, 	Step: 93110, 	{'train/accuracy': 0.5608007907867432, 'train/loss': 1.8958534002304077, 'validation/accuracy': 0.5133000016212463, 'validation/loss': 2.126741886138916, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.773594617843628, 'test/num_examples': 10000, 'score': 42469.17872309685, 'total_duration': 46599.65407395363, 'accumulated_submission_time': 42469.17872309685, 'accumulated_eval_time': 4120.957757472992, 'accumulated_logging_time': 4.573975324630737}
I0203 00:36:48.487986 139702527031040 logging_writer.py:48] [93110] accumulated_eval_time=4120.957757, accumulated_logging_time=4.573975, accumulated_submission_time=42469.178723, global_step=93110, preemption_count=0, score=42469.178723, test/accuracy=0.407500, test/loss=2.773595, test/num_examples=10000, total_duration=46599.654074, train/accuracy=0.560801, train/loss=1.895853, validation/accuracy=0.513300, validation/loss=2.126742, validation/num_examples=50000
I0203 00:37:25.789677 139702543816448 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.1788723468780518, loss=3.037613868713379
I0203 00:38:11.575797 139702527031040 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.8186725974082947, loss=5.552651882171631
I0203 00:38:57.525002 139702543816448 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.0281215906143188, loss=3.8064558506011963
I0203 00:39:43.601197 139702527031040 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.2012338638305664, loss=3.0511488914489746
I0203 00:40:30.003029 139702543816448 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.3107554912567139, loss=3.1200690269470215
I0203 00:41:15.983097 139702527031040 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.7772276401519775, loss=3.1907236576080322
I0203 00:42:01.936802 139702543816448 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.9659176468849182, loss=4.452960968017578
I0203 00:42:48.676845 139702527031040 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.0283430814743042, loss=3.788022041320801
I0203 00:43:34.774606 139702543816448 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.078311562538147, loss=2.9334559440612793
I0203 00:43:48.730710 139863983413056 spec.py:321] Evaluating on the training split.
I0203 00:43:59.178698 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 00:44:26.596410 139863983413056 spec.py:349] Evaluating on the test split.
I0203 00:44:28.234997 139863983413056 submission_runner.py:408] Time since start: 47059.43s, 	Step: 94031, 	{'train/accuracy': 0.5450390577316284, 'train/loss': 1.9546549320220947, 'validation/accuracy': 0.5126599669456482, 'validation/loss': 2.1251654624938965, 'validation/num_examples': 50000, 'test/accuracy': 0.4058000147342682, 'test/loss': 2.794235944747925, 'test/num_examples': 10000, 'score': 42889.36215043068, 'total_duration': 47059.432107925415, 'accumulated_submission_time': 42889.36215043068, 'accumulated_eval_time': 4160.462041378021, 'accumulated_logging_time': 4.615373611450195}
I0203 00:44:28.268527 139702527031040 logging_writer.py:48] [94031] accumulated_eval_time=4160.462041, accumulated_logging_time=4.615374, accumulated_submission_time=42889.362150, global_step=94031, preemption_count=0, score=42889.362150, test/accuracy=0.405800, test/loss=2.794236, test/num_examples=10000, total_duration=47059.432108, train/accuracy=0.545039, train/loss=1.954655, validation/accuracy=0.512660, validation/loss=2.125165, validation/num_examples=50000
I0203 00:44:56.553785 139702543816448 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.1862865686416626, loss=2.930467367172241
I0203 00:45:42.279423 139702527031040 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.29851496219635, loss=5.418929100036621
I0203 00:46:28.389271 139702543816448 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.9063961505889893, loss=4.472188949584961
I0203 00:47:14.558453 139702527031040 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.2198588848114014, loss=2.9958901405334473
I0203 00:48:00.525033 139702543816448 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.4406445026397705, loss=3.0475220680236816
I0203 00:48:46.364866 139702527031040 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.8849032521247864, loss=5.427791118621826
I0203 00:49:32.521030 139702543816448 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.1394858360290527, loss=2.998307228088379
I0203 00:50:18.617248 139702527031040 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.9451423287391663, loss=3.9109296798706055
I0203 00:51:04.790720 139702543816448 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.0166922807693481, loss=5.300215244293213
I0203 00:51:28.480639 139863983413056 spec.py:321] Evaluating on the training split.
I0203 00:51:39.056322 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 00:52:09.195298 139863983413056 spec.py:349] Evaluating on the test split.
I0203 00:52:10.832066 139863983413056 submission_runner.py:408] Time since start: 47522.03s, 	Step: 94953, 	{'train/accuracy': 0.5402538776397705, 'train/loss': 2.001154661178589, 'validation/accuracy': 0.5038999915122986, 'validation/loss': 2.1799263954162598, 'validation/num_examples': 50000, 'test/accuracy': 0.40060001611709595, 'test/loss': 2.8371763229370117, 'test/num_examples': 10000, 'score': 43309.5159702301, 'total_duration': 47522.02918076515, 'accumulated_submission_time': 43309.5159702301, 'accumulated_eval_time': 4202.813487291336, 'accumulated_logging_time': 4.658671140670776}
I0203 00:52:10.863908 139702527031040 logging_writer.py:48] [94953] accumulated_eval_time=4202.813487, accumulated_logging_time=4.658671, accumulated_submission_time=43309.515970, global_step=94953, preemption_count=0, score=43309.515970, test/accuracy=0.400600, test/loss=2.837176, test/num_examples=10000, total_duration=47522.029181, train/accuracy=0.540254, train/loss=2.001155, validation/accuracy=0.503900, validation/loss=2.179926, validation/num_examples=50000
I0203 00:52:30.045610 139702543816448 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.0845354795455933, loss=5.311893463134766
I0203 00:53:14.750468 139702527031040 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.0890506505966187, loss=3.1346418857574463
I0203 00:54:00.788163 139702543816448 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.1755681037902832, loss=2.949276924133301
I0203 00:54:47.079521 139702527031040 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.1668636798858643, loss=3.049635648727417
I0203 00:55:32.969141 139702543816448 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.0767723321914673, loss=3.887256145477295
I0203 00:56:19.311945 139702527031040 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.1124300956726074, loss=3.078669786453247
I0203 00:57:05.415037 139702543816448 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.1866923570632935, loss=3.2916247844696045
I0203 00:57:51.551139 139702527031040 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.3616963624954224, loss=3.1954214572906494
I0203 00:58:37.405288 139702543816448 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.2651007175445557, loss=3.0424017906188965
I0203 00:59:11.262867 139863983413056 spec.py:321] Evaluating on the training split.
I0203 00:59:21.974123 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 00:59:49.760975 139863983413056 spec.py:349] Evaluating on the test split.
I0203 00:59:51.399933 139863983413056 submission_runner.py:408] Time since start: 47982.60s, 	Step: 95875, 	{'train/accuracy': 0.5454687476158142, 'train/loss': 2.008453845977783, 'validation/accuracy': 0.5034399628639221, 'validation/loss': 2.21916127204895, 'validation/num_examples': 50000, 'test/accuracy': 0.3922000229358673, 'test/loss': 2.864680528640747, 'test/num_examples': 10000, 'score': 43729.85532140732, 'total_duration': 47982.59704566002, 'accumulated_submission_time': 43729.85532140732, 'accumulated_eval_time': 4242.95054268837, 'accumulated_logging_time': 4.701233148574829}
I0203 00:59:51.431147 139702527031040 logging_writer.py:48] [95875] accumulated_eval_time=4242.950543, accumulated_logging_time=4.701233, accumulated_submission_time=43729.855321, global_step=95875, preemption_count=0, score=43729.855321, test/accuracy=0.392200, test/loss=2.864681, test/num_examples=10000, total_duration=47982.597046, train/accuracy=0.545469, train/loss=2.008454, validation/accuracy=0.503440, validation/loss=2.219161, validation/num_examples=50000
I0203 01:00:01.817569 139702543816448 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.9051679968833923, loss=5.492555141448975
I0203 01:00:45.000504 139702527031040 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.1858148574829102, loss=2.97468638420105
I0203 01:01:31.301805 139702543816448 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.884756863117218, loss=5.488603115081787
I0203 01:02:17.757633 139702527031040 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.9684520959854126, loss=4.484951496124268
I0203 01:03:03.628866 139702543816448 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.2774893045425415, loss=3.0549254417419434
I0203 01:03:49.823156 139702527031040 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.1031259298324585, loss=3.528909921646118
I0203 01:04:35.969650 139702543816448 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.984978973865509, loss=5.23301362991333
I0203 01:05:22.203528 139702527031040 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.1385281085968018, loss=3.001420497894287
I0203 01:06:08.376963 139702543816448 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.8597626686096191, loss=5.468570709228516
I0203 01:06:51.554202 139863983413056 spec.py:321] Evaluating on the training split.
I0203 01:07:01.994931 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 01:07:30.855798 139863983413056 spec.py:349] Evaluating on the test split.
I0203 01:07:32.488697 139863983413056 submission_runner.py:408] Time since start: 48443.69s, 	Step: 96795, 	{'train/accuracy': 0.5572656393051147, 'train/loss': 1.9084703922271729, 'validation/accuracy': 0.5183199644088745, 'validation/loss': 2.112875461578369, 'validation/num_examples': 50000, 'test/accuracy': 0.40960001945495605, 'test/loss': 2.749626398086548, 'test/num_examples': 10000, 'score': 44149.91594219208, 'total_duration': 48443.68581080437, 'accumulated_submission_time': 44149.91594219208, 'accumulated_eval_time': 4283.885046005249, 'accumulated_logging_time': 4.746007680892944}
I0203 01:07:32.522255 139702527031040 logging_writer.py:48] [96795] accumulated_eval_time=4283.885046, accumulated_logging_time=4.746008, accumulated_submission_time=44149.915942, global_step=96795, preemption_count=0, score=44149.915942, test/accuracy=0.409600, test/loss=2.749626, test/num_examples=10000, total_duration=48443.685811, train/accuracy=0.557266, train/loss=1.908470, validation/accuracy=0.518320, validation/loss=2.112875, validation/num_examples=50000
I0203 01:07:34.914555 139702543816448 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.2312616109848022, loss=3.1798923015594482
I0203 01:08:16.701294 139702527031040 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.3562132120132446, loss=2.968554735183716
I0203 01:09:02.598053 139702543816448 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.2086890935897827, loss=2.8391542434692383
I0203 01:09:48.925953 139702527031040 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.0792309045791626, loss=4.093456268310547
I0203 01:10:35.037265 139702543816448 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.3019273281097412, loss=3.212425947189331
I0203 01:11:20.938777 139702527031040 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.2065749168395996, loss=2.8800909519195557
I0203 01:12:07.357364 139702543816448 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.254470944404602, loss=2.898491621017456
I0203 01:12:53.305830 139702527031040 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.0084011554718018, loss=3.454866886138916
I0203 01:13:39.611146 139702543816448 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.1906613111495972, loss=3.0721776485443115
I0203 01:14:25.711911 139702527031040 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.9276261329650879, loss=4.514675140380859
I0203 01:14:32.782872 139863983413056 spec.py:321] Evaluating on the training split.
I0203 01:14:43.179447 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 01:15:10.489399 139863983413056 spec.py:349] Evaluating on the test split.
I0203 01:15:12.123867 139863983413056 submission_runner.py:408] Time since start: 48903.32s, 	Step: 97717, 	{'train/accuracy': 0.5448241829872131, 'train/loss': 1.973633050918579, 'validation/accuracy': 0.5102199912071228, 'validation/loss': 2.158968210220337, 'validation/num_examples': 50000, 'test/accuracy': 0.4025000333786011, 'test/loss': 2.8153152465820312, 'test/num_examples': 10000, 'score': 44570.11754012108, 'total_duration': 48903.320981264114, 'accumulated_submission_time': 44570.11754012108, 'accumulated_eval_time': 4323.22603559494, 'accumulated_logging_time': 4.789544582366943}
I0203 01:15:12.153977 139702543816448 logging_writer.py:48] [97717] accumulated_eval_time=4323.226036, accumulated_logging_time=4.789545, accumulated_submission_time=44570.117540, global_step=97717, preemption_count=0, score=44570.117540, test/accuracy=0.402500, test/loss=2.815315, test/num_examples=10000, total_duration=48903.320981, train/accuracy=0.544824, train/loss=1.973633, validation/accuracy=0.510220, validation/loss=2.158968, validation/num_examples=50000
I0203 01:15:46.135361 139702527031040 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.0358432531356812, loss=4.506734848022461
I0203 01:16:32.096456 139702543816448 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.974536120891571, loss=4.48012638092041
I0203 01:17:18.178844 139702527031040 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.1332640647888184, loss=5.54324197769165
I0203 01:18:04.402308 139702543816448 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.1913652420043945, loss=3.052649974822998
I0203 01:18:50.683295 139702527031040 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.0108131170272827, loss=3.5023248195648193
I0203 01:19:36.721673 139702543816448 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.9576819539070129, loss=3.8366587162017822
I0203 01:20:23.017782 139702527031040 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.239125370979309, loss=2.9240565299987793
I0203 01:21:08.905373 139702543816448 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.0393109321594238, loss=4.4038987159729
I0203 01:21:55.040391 139702527031040 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.207411527633667, loss=2.9029242992401123
I0203 01:22:12.286901 139863983413056 spec.py:321] Evaluating on the training split.
I0203 01:22:22.840022 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 01:22:50.889597 139863983413056 spec.py:349] Evaluating on the test split.
I0203 01:22:52.526007 139863983413056 submission_runner.py:408] Time since start: 49363.72s, 	Step: 98639, 	{'train/accuracy': 0.5551952719688416, 'train/loss': 1.9290554523468018, 'validation/accuracy': 0.5148800015449524, 'validation/loss': 2.135251760482788, 'validation/num_examples': 50000, 'test/accuracy': 0.40620002150535583, 'test/loss': 2.7946832180023193, 'test/num_examples': 10000, 'score': 44990.19146609306, 'total_duration': 49363.72312116623, 'accumulated_submission_time': 44990.19146609306, 'accumulated_eval_time': 4363.465132236481, 'accumulated_logging_time': 4.829688310623169}
I0203 01:22:52.556416 139702543816448 logging_writer.py:48] [98639] accumulated_eval_time=4363.465132, accumulated_logging_time=4.829688, accumulated_submission_time=44990.191466, global_step=98639, preemption_count=0, score=44990.191466, test/accuracy=0.406200, test/loss=2.794683, test/num_examples=10000, total_duration=49363.723121, train/accuracy=0.555195, train/loss=1.929055, validation/accuracy=0.514880, validation/loss=2.135252, validation/num_examples=50000
I0203 01:23:17.338759 139702527031040 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.1494100093841553, loss=3.0577147006988525
I0203 01:24:02.257816 139702543816448 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.8526191711425781, loss=5.15342903137207
I0203 01:24:48.851775 139702527031040 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.317214846611023, loss=3.0199527740478516
I0203 01:25:34.909058 139702543816448 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.0186259746551514, loss=3.6790127754211426
I0203 01:26:21.326317 139702527031040 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.0364787578582764, loss=3.7809083461761475
I0203 01:27:07.479241 139702543816448 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.8839068412780762, loss=3.895810604095459
I0203 01:27:53.486720 139702527031040 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.0441689491271973, loss=4.016456127166748
I0203 01:28:39.708697 139702543816448 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.0015658140182495, loss=4.934134483337402
I0203 01:29:25.712962 139702527031040 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.0811671018600464, loss=3.007056474685669
I0203 01:29:52.763049 139863983413056 spec.py:321] Evaluating on the training split.
I0203 01:30:03.244228 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 01:30:33.360605 139863983413056 spec.py:349] Evaluating on the test split.
I0203 01:30:35.006627 139863983413056 submission_runner.py:408] Time since start: 49826.20s, 	Step: 99560, 	{'train/accuracy': 0.5842382907867432, 'train/loss': 1.8113583326339722, 'validation/accuracy': 0.5146600008010864, 'validation/loss': 2.1451916694641113, 'validation/num_examples': 50000, 'test/accuracy': 0.40700000524520874, 'test/loss': 2.797633647918701, 'test/num_examples': 10000, 'score': 45410.33908557892, 'total_duration': 49826.20372629166, 'accumulated_submission_time': 45410.33908557892, 'accumulated_eval_time': 4405.708698511124, 'accumulated_logging_time': 4.869691371917725}
I0203 01:30:35.038788 139702543816448 logging_writer.py:48] [99560] accumulated_eval_time=4405.708699, accumulated_logging_time=4.869691, accumulated_submission_time=45410.339086, global_step=99560, preemption_count=0, score=45410.339086, test/accuracy=0.407000, test/loss=2.797634, test/num_examples=10000, total_duration=49826.203726, train/accuracy=0.584238, train/loss=1.811358, validation/accuracy=0.514660, validation/loss=2.145192, validation/num_examples=50000
I0203 01:30:51.434162 139702527031040 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.3960380554199219, loss=3.055601119995117
I0203 01:31:35.365283 139702543816448 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.9087220430374146, loss=3.9070322513580322
I0203 01:32:21.570948 139702527031040 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.9214943051338196, loss=5.451423645019531
I0203 01:33:07.840947 139702543816448 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.1886265277862549, loss=2.9839963912963867
I0203 01:33:53.841624 139702527031040 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.1184065341949463, loss=3.2307372093200684
I0203 01:34:40.421838 139702543816448 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.3128589391708374, loss=3.016803503036499
I0203 01:35:26.196825 139702527031040 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.137163519859314, loss=4.510786056518555
I0203 01:36:12.315185 139702543816448 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.0182350873947144, loss=5.2411112785339355
I0203 01:36:58.306348 139702527031040 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.3349597454071045, loss=3.169248104095459
I0203 01:37:35.249438 139863983413056 spec.py:321] Evaluating on the training split.
I0203 01:37:45.601597 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 01:38:12.171038 139863983413056 spec.py:349] Evaluating on the test split.
I0203 01:38:13.804750 139863983413056 submission_runner.py:408] Time since start: 50285.00s, 	Step: 100482, 	{'train/accuracy': 0.5579687356948853, 'train/loss': 1.9476128816604614, 'validation/accuracy': 0.5199399590492249, 'validation/loss': 2.1243672370910645, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.762568473815918, 'test/num_examples': 10000, 'score': 45830.48991537094, 'total_duration': 50285.001859903336, 'accumulated_submission_time': 45830.48991537094, 'accumulated_eval_time': 4444.26401591301, 'accumulated_logging_time': 4.912209749221802}
I0203 01:38:13.841717 139702543816448 logging_writer.py:48] [100482] accumulated_eval_time=4444.264016, accumulated_logging_time=4.912210, accumulated_submission_time=45830.489915, global_step=100482, preemption_count=0, score=45830.489915, test/accuracy=0.408600, test/loss=2.762568, test/num_examples=10000, total_duration=50285.001860, train/accuracy=0.557969, train/loss=1.947613, validation/accuracy=0.519940, validation/loss=2.124367, validation/num_examples=50000
I0203 01:38:21.429605 139702527031040 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.1454761028289795, loss=3.0962185859680176
I0203 01:39:04.029588 139702543816448 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.9751875996589661, loss=4.930509567260742
I0203 01:39:50.046244 139702527031040 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.1067721843719482, loss=3.231367349624634
I0203 01:40:36.132396 139702543816448 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.1332517862319946, loss=3.0082578659057617
I0203 01:41:22.254307 139702527031040 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.1758977174758911, loss=2.9943065643310547
I0203 01:42:08.380586 139702543816448 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.1362546682357788, loss=5.549412727355957
I0203 01:42:54.412977 139702527031040 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.0683715343475342, loss=3.827495574951172
I0203 01:43:40.392499 139702543816448 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.3591164350509644, loss=2.9558820724487305
I0203 01:44:26.616234 139702527031040 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.0985382795333862, loss=3.1308796405792236
I0203 01:45:12.859254 139702543816448 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.125925898551941, loss=2.9538016319274902
I0203 01:45:13.904264 139863983413056 spec.py:321] Evaluating on the training split.
I0203 01:45:24.048673 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 01:45:48.705265 139863983413056 spec.py:349] Evaluating on the test split.
I0203 01:45:50.348656 139863983413056 submission_runner.py:408] Time since start: 50741.55s, 	Step: 101404, 	{'train/accuracy': 0.5579296946525574, 'train/loss': 1.942072868347168, 'validation/accuracy': 0.5202599763870239, 'validation/loss': 2.1204233169555664, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.7876458168029785, 'test/num_examples': 10000, 'score': 46250.49274253845, 'total_duration': 50741.545766592026, 'accumulated_submission_time': 46250.49274253845, 'accumulated_eval_time': 4480.708422660828, 'accumulated_logging_time': 4.960192918777466}
I0203 01:45:50.380536 139702527031040 logging_writer.py:48] [101404] accumulated_eval_time=4480.708423, accumulated_logging_time=4.960193, accumulated_submission_time=46250.492743, global_step=101404, preemption_count=0, score=46250.492743, test/accuracy=0.408600, test/loss=2.787646, test/num_examples=10000, total_duration=50741.545767, train/accuracy=0.557930, train/loss=1.942073, validation/accuracy=0.520260, validation/loss=2.120423, validation/num_examples=50000
I0203 01:46:30.098502 139702543816448 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.1532237529754639, loss=3.2284743785858154
I0203 01:47:15.860096 139702527031040 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.220265507698059, loss=3.005117893218994
I0203 01:48:02.379111 139702543816448 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.2608755826950073, loss=2.880038261413574
I0203 01:48:48.306295 139702527031040 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.0726889371871948, loss=4.794015407562256
I0203 01:49:34.589167 139702543816448 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.2386749982833862, loss=2.967594861984253
I0203 01:50:20.906078 139702527031040 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.8474275469779968, loss=5.371723651885986
I0203 01:51:06.957004 139702543816448 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.0926847457885742, loss=5.4237518310546875
I0203 01:51:53.288491 139702527031040 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.3050134181976318, loss=2.9882144927978516
I0203 01:52:39.513963 139702543816448 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.3551911115646362, loss=3.0750160217285156
I0203 01:52:50.615645 139863983413056 spec.py:321] Evaluating on the training split.
I0203 01:53:01.043223 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 01:53:30.253924 139863983413056 spec.py:349] Evaluating on the test split.
I0203 01:53:31.897774 139863983413056 submission_runner.py:408] Time since start: 51203.09s, 	Step: 102326, 	{'train/accuracy': 0.5746093392372131, 'train/loss': 1.825226068496704, 'validation/accuracy': 0.5278399586677551, 'validation/loss': 2.0796656608581543, 'validation/num_examples': 50000, 'test/accuracy': 0.4100000262260437, 'test/loss': 2.769395112991333, 'test/num_examples': 10000, 'score': 46670.66937637329, 'total_duration': 51203.094888448715, 'accumulated_submission_time': 46670.66937637329, 'accumulated_eval_time': 4521.99057674408, 'accumulated_logging_time': 5.001747369766235}
I0203 01:53:31.928296 139702527031040 logging_writer.py:48] [102326] accumulated_eval_time=4521.990577, accumulated_logging_time=5.001747, accumulated_submission_time=46670.669376, global_step=102326, preemption_count=0, score=46670.669376, test/accuracy=0.410000, test/loss=2.769395, test/num_examples=10000, total_duration=51203.094888, train/accuracy=0.574609, train/loss=1.825226, validation/accuracy=0.527840, validation/loss=2.079666, validation/num_examples=50000
I0203 01:54:02.109227 139702543816448 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.160352349281311, loss=5.4818902015686035
I0203 01:54:47.960185 139702527031040 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.007563591003418, loss=5.0208330154418945
I0203 01:55:34.851479 139702543816448 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.2561132907867432, loss=2.9538493156433105
I0203 01:56:20.992864 139702527031040 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.9341093897819519, loss=5.046046257019043
I0203 01:57:07.108844 139702543816448 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.2831329107284546, loss=2.8955445289611816
I0203 01:57:53.162339 139702527031040 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.9208586812019348, loss=5.021422386169434
I0203 01:58:39.313879 139702543816448 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.417211651802063, loss=3.0724375247955322
I0203 01:59:25.182576 139702527031040 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.2033122777938843, loss=3.1906542778015137
I0203 02:00:11.330265 139702543816448 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.9531710147857666, loss=5.414791584014893
I0203 02:00:32.157982 139863983413056 spec.py:321] Evaluating on the training split.
I0203 02:00:42.419305 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 02:01:09.117318 139863983413056 spec.py:349] Evaluating on the test split.
I0203 02:01:10.756350 139863983413056 submission_runner.py:408] Time since start: 51661.95s, 	Step: 103247, 	{'train/accuracy': 0.5560937523841858, 'train/loss': 1.9529153108596802, 'validation/accuracy': 0.5209599733352661, 'validation/loss': 2.1211466789245605, 'validation/num_examples': 50000, 'test/accuracy': 0.4067000150680542, 'test/loss': 2.7682337760925293, 'test/num_examples': 10000, 'score': 47090.8395152092, 'total_duration': 51661.95341157913, 'accumulated_submission_time': 47090.8395152092, 'accumulated_eval_time': 4560.5888912677765, 'accumulated_logging_time': 5.0430192947387695}
I0203 02:01:10.795295 139702527031040 logging_writer.py:48] [103247] accumulated_eval_time=4560.588891, accumulated_logging_time=5.043019, accumulated_submission_time=47090.839515, global_step=103247, preemption_count=0, score=47090.839515, test/accuracy=0.406700, test/loss=2.768234, test/num_examples=10000, total_duration=51661.953412, train/accuracy=0.556094, train/loss=1.952915, validation/accuracy=0.520960, validation/loss=2.121147, validation/num_examples=50000
I0203 02:01:32.373975 139702543816448 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.108954668045044, loss=3.6394033432006836
I0203 02:02:16.803016 139702527031040 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.2040528059005737, loss=2.8879458904266357
I0203 02:03:02.818981 139702543816448 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.1362638473510742, loss=3.070230484008789
I0203 02:03:48.807893 139702527031040 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.0129649639129639, loss=3.880232572555542
I0203 02:04:34.701713 139702543816448 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.0609914064407349, loss=3.864518404006958
I0203 02:05:20.726083 139702527031040 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.2590739727020264, loss=2.872185468673706
I0203 02:06:06.817686 139702543816448 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.212450623512268, loss=2.954202175140381
I0203 02:06:52.676340 139702527031040 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.3020141124725342, loss=2.9875292778015137
I0203 02:07:38.750209 139702543816448 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.9683976173400879, loss=5.437675952911377
I0203 02:08:10.968124 139863983413056 spec.py:321] Evaluating on the training split.
I0203 02:08:21.502629 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 02:08:52.595919 139863983413056 spec.py:349] Evaluating on the test split.
I0203 02:08:54.231683 139863983413056 submission_runner.py:408] Time since start: 52125.43s, 	Step: 104172, 	{'train/accuracy': 0.5689452886581421, 'train/loss': 1.8281947374343872, 'validation/accuracy': 0.5344399809837341, 'validation/loss': 2.0128085613250732, 'validation/num_examples': 50000, 'test/accuracy': 0.4191000163555145, 'test/loss': 2.6920504570007324, 'test/num_examples': 10000, 'score': 47510.9509665966, 'total_duration': 52125.428775310516, 'accumulated_submission_time': 47510.9509665966, 'accumulated_eval_time': 4603.852442026138, 'accumulated_logging_time': 5.094490051269531}
I0203 02:08:54.276411 139702527031040 logging_writer.py:48] [104172] accumulated_eval_time=4603.852442, accumulated_logging_time=5.094490, accumulated_submission_time=47510.950967, global_step=104172, preemption_count=0, score=47510.950967, test/accuracy=0.419100, test/loss=2.692050, test/num_examples=10000, total_duration=52125.428775, train/accuracy=0.568945, train/loss=1.828195, validation/accuracy=0.534440, validation/loss=2.012809, validation/num_examples=50000
I0203 02:09:05.851375 139702543816448 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.2177540063858032, loss=2.7939186096191406
I0203 02:09:48.986807 139702527031040 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.2756892442703247, loss=2.9926815032958984
I0203 02:10:35.144069 139702543816448 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.1031123399734497, loss=3.511204719543457
I0203 02:11:21.076756 139702527031040 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.2731058597564697, loss=3.676041603088379
I0203 02:12:07.130993 139702543816448 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.2425782680511475, loss=2.884401798248291
I0203 02:12:53.126960 139702527031040 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.1782581806182861, loss=2.879338502883911
I0203 02:13:39.207023 139702543816448 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.07314133644104, loss=4.631776332855225
I0203 02:14:25.481616 139702527031040 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.379620909690857, loss=2.937466859817505
I0203 02:15:11.590034 139702543816448 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.3080036640167236, loss=2.8581912517547607
I0203 02:15:54.289765 139863983413056 spec.py:321] Evaluating on the training split.
I0203 02:16:04.729681 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 02:16:32.577495 139863983413056 spec.py:349] Evaluating on the test split.
I0203 02:16:34.219820 139863983413056 submission_runner.py:408] Time since start: 52585.42s, 	Step: 105094, 	{'train/accuracy': 0.5889062285423279, 'train/loss': 1.7638208866119385, 'validation/accuracy': 0.5421199798583984, 'validation/loss': 1.9910471439361572, 'validation/num_examples': 50000, 'test/accuracy': 0.4271000325679779, 'test/loss': 2.6605894565582275, 'test/num_examples': 10000, 'score': 47930.90433549881, 'total_duration': 52585.41693139076, 'accumulated_submission_time': 47930.90433549881, 'accumulated_eval_time': 4643.782491207123, 'accumulated_logging_time': 5.150495529174805}
I0203 02:16:34.259643 139702527031040 logging_writer.py:48] [105094] accumulated_eval_time=4643.782491, accumulated_logging_time=5.150496, accumulated_submission_time=47930.904335, global_step=105094, preemption_count=0, score=47930.904335, test/accuracy=0.427100, test/loss=2.660589, test/num_examples=10000, total_duration=52585.416931, train/accuracy=0.588906, train/loss=1.763821, validation/accuracy=0.542120, validation/loss=1.991047, validation/num_examples=50000
I0203 02:16:37.060556 139702543816448 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.3854800462722778, loss=4.138514518737793
I0203 02:17:18.945932 139702527031040 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.0326164960861206, loss=3.79201602935791
I0203 02:18:04.641489 139702543816448 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.3240554332733154, loss=2.967534065246582
I0203 02:18:50.768622 139702527031040 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.3654325008392334, loss=3.1817586421966553
I0203 02:19:36.891150 139702543816448 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.2496647834777832, loss=2.7449469566345215
I0203 02:20:23.535935 139702527031040 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.995964527130127, loss=5.132374286651611
I0203 02:21:09.615280 139702543816448 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.8942642211914062, loss=5.165821552276611
I0203 02:21:55.712837 139702527031040 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.0569815635681152, loss=3.7590930461883545
I0203 02:22:42.014549 139702543816448 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.089833378791809, loss=4.014749526977539
I0203 02:23:28.282722 139702527031040 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.291778564453125, loss=3.0468430519104004
I0203 02:23:34.285323 139863983413056 spec.py:321] Evaluating on the training split.
I0203 02:23:44.544780 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 02:24:10.734296 139863983413056 spec.py:349] Evaluating on the test split.
I0203 02:24:12.384495 139863983413056 submission_runner.py:408] Time since start: 53043.58s, 	Step: 106015, 	{'train/accuracy': 0.5689062476158142, 'train/loss': 1.8828868865966797, 'validation/accuracy': 0.5313199758529663, 'validation/loss': 2.0591819286346436, 'validation/num_examples': 50000, 'test/accuracy': 0.4189000129699707, 'test/loss': 2.7256243228912354, 'test/num_examples': 10000, 'score': 48350.86739444733, 'total_duration': 53043.58160710335, 'accumulated_submission_time': 48350.86739444733, 'accumulated_eval_time': 4681.881660699844, 'accumulated_logging_time': 5.203859329223633}
I0203 02:24:12.418376 139702543816448 logging_writer.py:48] [106015] accumulated_eval_time=4681.881661, accumulated_logging_time=5.203859, accumulated_submission_time=48350.867394, global_step=106015, preemption_count=0, score=48350.867394, test/accuracy=0.418900, test/loss=2.725624, test/num_examples=10000, total_duration=53043.581607, train/accuracy=0.568906, train/loss=1.882887, validation/accuracy=0.531320, validation/loss=2.059182, validation/num_examples=50000
I0203 02:24:47.458835 139702527031040 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.0842558145523071, loss=3.7976186275482178
I0203 02:25:33.540726 139702543816448 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.2088316679000854, loss=2.749007225036621
I0203 02:26:19.815027 139702527031040 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.308700442314148, loss=2.7396950721740723
I0203 02:27:05.995944 139702543816448 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.1963691711425781, loss=2.827451467514038
I0203 02:27:51.840692 139702527031040 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.2444249391555786, loss=2.9248435497283936
I0203 02:28:37.962663 139702543816448 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.1848865747451782, loss=2.7161543369293213
I0203 02:29:23.896697 139702527031040 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.020114541053772, loss=5.1114959716796875
I0203 02:30:10.274441 139702543816448 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.2750673294067383, loss=3.028134346008301
I0203 02:30:56.355733 139702527031040 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.2539458274841309, loss=2.7686100006103516
I0203 02:31:12.506999 139863983413056 spec.py:321] Evaluating on the training split.
I0203 02:31:22.852676 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 02:31:49.590933 139863983413056 spec.py:349] Evaluating on the test split.
I0203 02:31:51.225861 139863983413056 submission_runner.py:408] Time since start: 53502.42s, 	Step: 106937, 	{'train/accuracy': 0.5787695050239563, 'train/loss': 1.8417892456054688, 'validation/accuracy': 0.5412399768829346, 'validation/loss': 2.022822380065918, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.653942584991455, 'test/num_examples': 10000, 'score': 48770.8959608078, 'total_duration': 53502.422973394394, 'accumulated_submission_time': 48770.8959608078, 'accumulated_eval_time': 4720.600524902344, 'accumulated_logging_time': 5.249224662780762}
I0203 02:31:51.262091 139702543816448 logging_writer.py:48] [106937] accumulated_eval_time=4720.600525, accumulated_logging_time=5.249225, accumulated_submission_time=48770.895961, global_step=106937, preemption_count=0, score=48770.895961, test/accuracy=0.431300, test/loss=2.653943, test/num_examples=10000, total_duration=53502.422973, train/accuracy=0.578770, train/loss=1.841789, validation/accuracy=0.541240, validation/loss=2.022822, validation/num_examples=50000
I0203 02:32:16.842233 139702527031040 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.2471873760223389, loss=2.928882598876953
I0203 02:33:01.750935 139702543816448 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.3374607563018799, loss=2.9165427684783936
I0203 02:33:48.122551 139702527031040 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.9721519351005554, loss=5.00850248336792
I0203 02:34:34.091397 139702543816448 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.211365818977356, loss=2.966628313064575
I0203 02:35:20.215301 139702527031040 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.0107940435409546, loss=3.943509578704834
I0203 02:36:06.229689 139702543816448 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.0386806726455688, loss=5.441751956939697
I0203 02:36:52.778931 139702527031040 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.0648211240768433, loss=3.920011520385742
I0203 02:37:38.725316 139702543816448 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.1463444232940674, loss=3.4332761764526367
I0203 02:38:24.704900 139702527031040 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.35153067111969, loss=2.7966113090515137
I0203 02:38:51.662352 139863983413056 spec.py:321] Evaluating on the training split.
I0203 02:39:01.973151 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 02:39:30.430102 139863983413056 spec.py:349] Evaluating on the test split.
I0203 02:39:32.079401 139863983413056 submission_runner.py:408] Time since start: 53963.28s, 	Step: 107860, 	{'train/accuracy': 0.5873827934265137, 'train/loss': 1.7708899974822998, 'validation/accuracy': 0.5433200001716614, 'validation/loss': 1.9900877475738525, 'validation/num_examples': 50000, 'test/accuracy': 0.43250003457069397, 'test/loss': 2.6258180141448975, 'test/num_examples': 10000, 'score': 49191.236491680145, 'total_duration': 53963.276510477066, 'accumulated_submission_time': 49191.236491680145, 'accumulated_eval_time': 4761.017573356628, 'accumulated_logging_time': 5.296191930770874}
I0203 02:39:32.112627 139702543816448 logging_writer.py:48] [107860] accumulated_eval_time=4761.017573, accumulated_logging_time=5.296192, accumulated_submission_time=49191.236492, global_step=107860, preemption_count=0, score=49191.236492, test/accuracy=0.432500, test/loss=2.625818, test/num_examples=10000, total_duration=53963.276510, train/accuracy=0.587383, train/loss=1.770890, validation/accuracy=0.543320, validation/loss=1.990088, validation/num_examples=50000
I0203 02:39:48.493771 139702527031040 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.3245759010314941, loss=2.9538426399230957
I0203 02:40:32.278560 139702543816448 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.1206177473068237, loss=3.0450782775878906
I0203 02:41:18.324601 139702527031040 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.2785943746566772, loss=2.850054979324341
I0203 02:42:04.489184 139702543816448 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.0677717924118042, loss=4.007631301879883
I0203 02:42:50.224880 139702527031040 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.2457977533340454, loss=2.9804067611694336
I0203 02:43:36.173047 139702543816448 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.1087404489517212, loss=5.380923748016357
I0203 02:44:22.240621 139702527031040 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.0019408464431763, loss=5.403628349304199
I0203 02:45:08.255494 139702543816448 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.2347602844238281, loss=4.8203325271606445
I0203 02:45:54.330060 139702527031040 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.9177917242050171, loss=5.142744541168213
I0203 02:46:32.391207 139863983413056 spec.py:321] Evaluating on the training split.
I0203 02:46:42.902700 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 02:47:08.023242 139863983413056 spec.py:349] Evaluating on the test split.
I0203 02:47:09.665092 139863983413056 submission_runner.py:408] Time since start: 54420.86s, 	Step: 108784, 	{'train/accuracy': 0.6213085651397705, 'train/loss': 1.629205584526062, 'validation/accuracy': 0.5485599637031555, 'validation/loss': 1.9674816131591797, 'validation/num_examples': 50000, 'test/accuracy': 0.43700000643730164, 'test/loss': 2.632002353668213, 'test/num_examples': 10000, 'score': 49611.455362319946, 'total_duration': 54420.862203359604, 'accumulated_submission_time': 49611.455362319946, 'accumulated_eval_time': 4798.291470050812, 'accumulated_logging_time': 5.3400962352752686}
I0203 02:47:09.701345 139702543816448 logging_writer.py:48] [108784] accumulated_eval_time=4798.291470, accumulated_logging_time=5.340096, accumulated_submission_time=49611.455362, global_step=108784, preemption_count=0, score=49611.455362, test/accuracy=0.437000, test/loss=2.632002, test/num_examples=10000, total_duration=54420.862203, train/accuracy=0.621309, train/loss=1.629206, validation/accuracy=0.548560, validation/loss=1.967482, validation/num_examples=50000
I0203 02:47:16.492767 139702527031040 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.0169934034347534, loss=4.822702884674072
I0203 02:47:58.981770 139702543816448 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.1450014114379883, loss=4.456629753112793
I0203 02:48:44.743353 139702527031040 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.0879271030426025, loss=4.510076999664307
I0203 02:49:31.141843 139702543816448 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.1746653318405151, loss=2.8526053428649902
I0203 02:50:17.576063 139702527031040 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.1983087062835693, loss=2.871746063232422
I0203 02:51:03.813512 139702543816448 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.3294850587844849, loss=2.638833522796631
I0203 02:51:49.927228 139702527031040 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.1783214807510376, loss=3.3883962631225586
I0203 02:52:36.061652 139702543816448 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.249093770980835, loss=2.87585711479187
I0203 02:53:22.081058 139702527031040 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.3075296878814697, loss=5.267569541931152
I0203 02:54:08.411803 139702543816448 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.9934069514274597, loss=5.266785621643066
I0203 02:54:09.965009 139863983413056 spec.py:321] Evaluating on the training split.
I0203 02:54:21.736049 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 02:54:50.232327 139863983413056 spec.py:349] Evaluating on the test split.
I0203 02:54:51.869187 139863983413056 submission_runner.py:408] Time since start: 54883.07s, 	Step: 109705, 	{'train/accuracy': 0.588671863079071, 'train/loss': 1.7554982900619507, 'validation/accuracy': 0.5467999577522278, 'validation/loss': 1.9544671773910522, 'validation/num_examples': 50000, 'test/accuracy': 0.4294000267982483, 'test/loss': 2.6271207332611084, 'test/num_examples': 10000, 'score': 50031.65995430946, 'total_duration': 54883.06630086899, 'accumulated_submission_time': 50031.65995430946, 'accumulated_eval_time': 4840.195640087128, 'accumulated_logging_time': 5.386731386184692}
I0203 02:54:51.901420 139702527031040 logging_writer.py:48] [109705] accumulated_eval_time=4840.195640, accumulated_logging_time=5.386731, accumulated_submission_time=50031.659954, global_step=109705, preemption_count=0, score=50031.659954, test/accuracy=0.429400, test/loss=2.627121, test/num_examples=10000, total_duration=54883.066301, train/accuracy=0.588672, train/loss=1.755498, validation/accuracy=0.546800, validation/loss=1.954467, validation/num_examples=50000
I0203 02:55:31.332910 139702543816448 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.0596346855163574, loss=5.272332191467285
I0203 02:56:17.343589 139702527031040 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.067748785018921, loss=4.056578636169434
I0203 02:57:03.414164 139702543816448 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.9092621207237244, loss=5.293654441833496
I0203 02:57:49.840743 139702527031040 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.251763105392456, loss=3.1063804626464844
I0203 02:58:35.667687 139702543816448 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.2624834775924683, loss=2.7947330474853516
I0203 02:59:21.579193 139702527031040 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.0180251598358154, loss=4.720139503479004
I0203 03:00:07.793523 139702543816448 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.1198797225952148, loss=3.7465691566467285
I0203 03:00:53.905104 139702527031040 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.3138128519058228, loss=2.8835957050323486
I0203 03:01:40.091716 139702543816448 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.3808820247650146, loss=3.1871695518493652
I0203 03:01:52.198166 139863983413056 spec.py:321] Evaluating on the training split.
I0203 03:02:02.566197 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 03:02:30.245768 139863983413056 spec.py:349] Evaluating on the test split.
I0203 03:02:31.881749 139863983413056 submission_runner.py:408] Time since start: 55343.08s, 	Step: 110628, 	{'train/accuracy': 0.595898449420929, 'train/loss': 1.7019425630569458, 'validation/accuracy': 0.5554800033569336, 'validation/loss': 1.911659836769104, 'validation/num_examples': 50000, 'test/accuracy': 0.4384000301361084, 'test/loss': 2.5746302604675293, 'test/num_examples': 10000, 'score': 50451.89798164368, 'total_duration': 55343.07885932922, 'accumulated_submission_time': 50451.89798164368, 'accumulated_eval_time': 4879.87920832634, 'accumulated_logging_time': 5.428632497787476}
I0203 03:02:31.913636 139702527031040 logging_writer.py:48] [110628] accumulated_eval_time=4879.879208, accumulated_logging_time=5.428632, accumulated_submission_time=50451.897982, global_step=110628, preemption_count=0, score=50451.897982, test/accuracy=0.438400, test/loss=2.574630, test/num_examples=10000, total_duration=55343.078859, train/accuracy=0.595898, train/loss=1.701943, validation/accuracy=0.555480, validation/loss=1.911660, validation/num_examples=50000
I0203 03:03:01.032073 139702543816448 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.2829238176345825, loss=3.2469077110290527
I0203 03:03:46.902987 139702527031040 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.2319260835647583, loss=2.9037532806396484
I0203 03:04:33.062524 139702543816448 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.240417242050171, loss=2.8504393100738525
I0203 03:05:18.914056 139702527031040 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.2490179538726807, loss=2.7999629974365234
I0203 03:06:05.085079 139702543816448 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.2769030332565308, loss=2.8556478023529053
I0203 03:06:51.187305 139702527031040 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.2306352853775024, loss=3.142137050628662
I0203 03:07:37.281470 139702543816448 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.9380550980567932, loss=5.403353214263916
I0203 03:08:23.590972 139702527031040 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.2496778964996338, loss=2.773022413253784
I0203 03:09:09.481428 139702543816448 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.3621200323104858, loss=2.985776424407959
I0203 03:09:32.049953 139863983413056 spec.py:321] Evaluating on the training split.
I0203 03:09:42.718870 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 03:10:09.046158 139863983413056 spec.py:349] Evaluating on the test split.
I0203 03:10:10.688320 139863983413056 submission_runner.py:408] Time since start: 55801.89s, 	Step: 111551, 	{'train/accuracy': 0.6108593344688416, 'train/loss': 1.6443220376968384, 'validation/accuracy': 0.5592799782752991, 'validation/loss': 1.905354380607605, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.577054500579834, 'test/num_examples': 10000, 'score': 50871.974254608154, 'total_duration': 55801.88543534279, 'accumulated_submission_time': 50871.974254608154, 'accumulated_eval_time': 4918.517581939697, 'accumulated_logging_time': 5.472025394439697}
I0203 03:10:10.725335 139702527031040 logging_writer.py:48] [111551] accumulated_eval_time=4918.517582, accumulated_logging_time=5.472025, accumulated_submission_time=50871.974255, global_step=111551, preemption_count=0, score=50871.974255, test/accuracy=0.439400, test/loss=2.577055, test/num_examples=10000, total_duration=55801.885435, train/accuracy=0.610859, train/loss=1.644322, validation/accuracy=0.559280, validation/loss=1.905354, validation/num_examples=50000
I0203 03:10:30.678464 139702543816448 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.0025146007537842, loss=4.205665111541748
I0203 03:11:15.175680 139702527031040 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.0644587278366089, loss=4.493480205535889
I0203 03:12:01.086099 139702543816448 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.143035650253296, loss=4.264676094055176
I0203 03:12:47.353708 139702527031040 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.167602777481079, loss=3.8581011295318604
I0203 03:13:33.384016 139702543816448 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.1540557146072388, loss=2.985988140106201
I0203 03:14:19.615123 139702527031040 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.2853920459747314, loss=2.765592098236084
I0203 03:15:05.455314 139702543816448 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.1239283084869385, loss=4.545541286468506
I0203 03:15:51.486410 139702527031040 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.451518177986145, loss=2.831303834915161
I0203 03:16:37.545739 139702543816448 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.3061882257461548, loss=3.127549648284912
I0203 03:17:11.035159 139863983413056 spec.py:321] Evaluating on the training split.
I0203 03:17:21.462937 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 03:17:49.644647 139863983413056 spec.py:349] Evaluating on the test split.
I0203 03:17:51.280721 139863983413056 submission_runner.py:408] Time since start: 56262.48s, 	Step: 112474, 	{'train/accuracy': 0.5939062237739563, 'train/loss': 1.7227932214736938, 'validation/accuracy': 0.5597599744796753, 'validation/loss': 1.904951810836792, 'validation/num_examples': 50000, 'test/accuracy': 0.44140002131462097, 'test/loss': 2.582552194595337, 'test/num_examples': 10000, 'score': 51292.22484111786, 'total_duration': 56262.47783136368, 'accumulated_submission_time': 51292.22484111786, 'accumulated_eval_time': 4958.763142347336, 'accumulated_logging_time': 5.5198681354522705}
I0203 03:17:51.317445 139702527031040 logging_writer.py:48] [112474] accumulated_eval_time=4958.763142, accumulated_logging_time=5.519868, accumulated_submission_time=51292.224841, global_step=112474, preemption_count=0, score=51292.224841, test/accuracy=0.441400, test/loss=2.582552, test/num_examples=10000, total_duration=56262.477831, train/accuracy=0.593906, train/loss=1.722793, validation/accuracy=0.559760, validation/loss=1.904952, validation/num_examples=50000
I0203 03:18:02.093185 139702543816448 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.06179678440094, loss=3.6100170612335205
I0203 03:18:45.156610 139702527031040 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.3876034021377563, loss=2.8224375247955322
I0203 03:19:31.119731 139702543816448 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.1493040323257446, loss=3.53024959564209
I0203 03:20:17.815670 139702527031040 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.3719712495803833, loss=2.749453067779541
I0203 03:21:03.710741 139702543816448 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.3013051748275757, loss=3.2691826820373535
I0203 03:21:49.927972 139702527031040 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.0193395614624023, loss=4.9118757247924805
I0203 03:22:36.271172 139702543816448 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.433204174041748, loss=2.8857192993164062
I0203 03:23:22.491640 139702527031040 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.55659019947052, loss=3.1203346252441406
I0203 03:24:08.616170 139702543816448 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.3026190996170044, loss=2.768693447113037
I0203 03:24:51.667136 139863983413056 spec.py:321] Evaluating on the training split.
I0203 03:25:02.214217 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 03:25:32.084449 139863983413056 spec.py:349] Evaluating on the test split.
I0203 03:25:33.725927 139863983413056 submission_runner.py:408] Time since start: 56724.92s, 	Step: 113395, 	{'train/accuracy': 0.6010937094688416, 'train/loss': 1.6848231554031372, 'validation/accuracy': 0.5621399879455566, 'validation/loss': 1.8820499181747437, 'validation/num_examples': 50000, 'test/accuracy': 0.43890002369880676, 'test/loss': 2.5632612705230713, 'test/num_examples': 10000, 'score': 51712.5141146183, 'total_duration': 56724.923040151596, 'accumulated_submission_time': 51712.5141146183, 'accumulated_eval_time': 5000.821955919266, 'accumulated_logging_time': 5.5690460205078125}
I0203 03:25:33.763186 139702527031040 logging_writer.py:48] [113395] accumulated_eval_time=5000.821956, accumulated_logging_time=5.569046, accumulated_submission_time=51712.514115, global_step=113395, preemption_count=0, score=51712.514115, test/accuracy=0.438900, test/loss=2.563261, test/num_examples=10000, total_duration=56724.923040, train/accuracy=0.601094, train/loss=1.684823, validation/accuracy=0.562140, validation/loss=1.882050, validation/num_examples=50000
I0203 03:25:36.166230 139702543816448 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.3318703174591064, loss=2.900329828262329
I0203 03:26:18.015906 139702527031040 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.287400484085083, loss=2.722151279449463
I0203 03:27:04.132634 139702543816448 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.2707290649414062, loss=2.6990957260131836
I0203 03:27:50.257149 139702527031040 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.332086443901062, loss=2.9897806644439697
I0203 03:28:36.111284 139702543816448 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.2201919555664062, loss=2.9838674068450928
I0203 03:29:22.547648 139702527031040 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.6859370470046997, loss=2.8540141582489014
I0203 03:30:08.852446 139702543816448 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.0360835790634155, loss=4.9067463874816895
I0203 03:30:55.229916 139702527031040 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.5239906311035156, loss=2.642058849334717
I0203 03:31:41.296620 139702543816448 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.0334514379501343, loss=4.484942436218262
I0203 03:32:27.421577 139702527031040 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.2827637195587158, loss=2.812601089477539
I0203 03:32:34.013566 139863983413056 spec.py:321] Evaluating on the training split.
I0203 03:32:44.406659 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 03:33:06.520319 139863983413056 spec.py:349] Evaluating on the test split.
I0203 03:33:08.158208 139863983413056 submission_runner.py:408] Time since start: 57179.36s, 	Step: 114316, 	{'train/accuracy': 0.6037890315055847, 'train/loss': 1.70114004611969, 'validation/accuracy': 0.55485999584198, 'validation/loss': 1.9423660039901733, 'validation/num_examples': 50000, 'test/accuracy': 0.442300021648407, 'test/loss': 2.572660207748413, 'test/num_examples': 10000, 'score': 52132.70584130287, 'total_duration': 57179.35532331467, 'accumulated_submission_time': 52132.70584130287, 'accumulated_eval_time': 5034.966580152512, 'accumulated_logging_time': 5.616888523101807}
I0203 03:33:08.191017 139702543816448 logging_writer.py:48] [114316] accumulated_eval_time=5034.966580, accumulated_logging_time=5.616889, accumulated_submission_time=52132.705841, global_step=114316, preemption_count=0, score=52132.705841, test/accuracy=0.442300, test/loss=2.572660, test/num_examples=10000, total_duration=57179.355323, train/accuracy=0.603789, train/loss=1.701140, validation/accuracy=0.554860, validation/loss=1.942366, validation/num_examples=50000
I0203 03:33:42.536484 139702527031040 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.1139613389968872, loss=5.1446003913879395
I0203 03:34:28.552244 139702543816448 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.1159933805465698, loss=3.2870523929595947
I0203 03:35:14.772086 139702527031040 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.3479750156402588, loss=2.721843719482422
I0203 03:36:00.800856 139702543816448 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.1773313283920288, loss=3.4700517654418945
I0203 03:36:47.251574 139702527031040 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.0235685110092163, loss=4.5788774490356445
I0203 03:37:33.432054 139702543816448 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.057526707649231, loss=3.6609747409820557
I0203 03:38:19.678434 139702527031040 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.2910617589950562, loss=2.9877989292144775
I0203 03:39:06.166247 139702543816448 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.2499127388000488, loss=2.646265983581543
I0203 03:39:52.030578 139702527031040 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.3830434083938599, loss=2.7032933235168457
I0203 03:40:08.307808 139863983413056 spec.py:321] Evaluating on the training split.
I0203 03:40:18.687281 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 03:40:47.901673 139863983413056 spec.py:349] Evaluating on the test split.
I0203 03:40:49.546223 139863983413056 submission_runner.py:408] Time since start: 57640.74s, 	Step: 115237, 	{'train/accuracy': 0.6056835651397705, 'train/loss': 1.67186439037323, 'validation/accuracy': 0.5682199597358704, 'validation/loss': 1.8541991710662842, 'validation/num_examples': 50000, 'test/accuracy': 0.44860002398490906, 'test/loss': 2.519437313079834, 'test/num_examples': 10000, 'score': 52552.76272273064, 'total_duration': 57640.74333691597, 'accumulated_submission_time': 52552.76272273064, 'accumulated_eval_time': 5076.205003976822, 'accumulated_logging_time': 5.660736322402954}
I0203 03:40:49.580031 139702543816448 logging_writer.py:48] [115237] accumulated_eval_time=5076.205004, accumulated_logging_time=5.660736, accumulated_submission_time=52552.762723, global_step=115237, preemption_count=0, score=52552.762723, test/accuracy=0.448600, test/loss=2.519437, test/num_examples=10000, total_duration=57640.743337, train/accuracy=0.605684, train/loss=1.671864, validation/accuracy=0.568220, validation/loss=1.854199, validation/num_examples=50000
I0203 03:41:15.129811 139702527031040 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.3227512836456299, loss=2.759317398071289
I0203 03:42:00.324072 139702543816448 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.2726585865020752, loss=2.7873733043670654
I0203 03:42:46.437314 139702527031040 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.1657838821411133, loss=3.0503177642822266
I0203 03:43:32.581579 139702543816448 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.2635458707809448, loss=2.7708001136779785
I0203 03:44:18.612683 139702527031040 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.5829060077667236, loss=2.712297201156616
I0203 03:45:04.693731 139702543816448 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.074570894241333, loss=4.600997447967529
I0203 03:45:50.717694 139702527031040 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.3804291486740112, loss=2.6402013301849365
I0203 03:46:36.761674 139702543816448 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.0939855575561523, loss=3.795377016067505
I0203 03:47:22.886287 139702527031040 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.3635438680648804, loss=2.8095531463623047
I0203 03:47:49.858659 139863983413056 spec.py:321] Evaluating on the training split.
I0203 03:48:00.395859 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 03:48:29.400830 139863983413056 spec.py:349] Evaluating on the test split.
I0203 03:48:31.043454 139863983413056 submission_runner.py:408] Time since start: 58102.24s, 	Step: 116160, 	{'train/accuracy': 0.6073437333106995, 'train/loss': 1.675774097442627, 'validation/accuracy': 0.5725799798965454, 'validation/loss': 1.846383810043335, 'validation/num_examples': 50000, 'test/accuracy': 0.45350003242492676, 'test/loss': 2.516984462738037, 'test/num_examples': 10000, 'score': 52972.98017334938, 'total_duration': 58102.24056863785, 'accumulated_submission_time': 52972.98017334938, 'accumulated_eval_time': 5117.389803171158, 'accumulated_logging_time': 5.706765413284302}
I0203 03:48:31.077159 139702543816448 logging_writer.py:48] [116160] accumulated_eval_time=5117.389803, accumulated_logging_time=5.706765, accumulated_submission_time=52972.980173, global_step=116160, preemption_count=0, score=52972.980173, test/accuracy=0.453500, test/loss=2.516984, test/num_examples=10000, total_duration=58102.240569, train/accuracy=0.607344, train/loss=1.675774, validation/accuracy=0.572580, validation/loss=1.846384, validation/num_examples=50000
I0203 03:48:47.438932 139702527031040 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.1228926181793213, loss=3.969768524169922
I0203 03:49:30.959416 139702543816448 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.337530493736267, loss=2.7553317546844482
I0203 03:50:17.789513 139702527031040 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.2038525342941284, loss=3.2408149242401123
I0203 03:51:03.789684 139702543816448 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.3978614807128906, loss=2.6299023628234863
I0203 03:51:49.836842 139702527031040 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.248158574104309, loss=2.8532967567443848
I0203 03:52:35.797530 139702543816448 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.2637392282485962, loss=2.8985559940338135
I0203 03:53:21.949501 139702527031040 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.0533721446990967, loss=4.234681129455566
I0203 03:54:08.025540 139702543816448 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.3700331449508667, loss=2.8153629302978516
I0203 03:54:54.028359 139702527031040 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.4816687107086182, loss=2.729261636734009
I0203 03:55:31.327089 139863983413056 spec.py:321] Evaluating on the training split.
I0203 03:55:41.884700 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 03:56:10.596506 139863983413056 spec.py:349] Evaluating on the test split.
I0203 03:56:12.244714 139863983413056 submission_runner.py:408] Time since start: 58563.44s, 	Step: 117083, 	{'train/accuracy': 0.6232812404632568, 'train/loss': 1.5855211019515991, 'validation/accuracy': 0.5778599977493286, 'validation/loss': 1.8170597553253174, 'validation/num_examples': 50000, 'test/accuracy': 0.46070003509521484, 'test/loss': 2.4718594551086426, 'test/num_examples': 10000, 'score': 53393.17074465752, 'total_duration': 58563.441803216934, 'accumulated_submission_time': 53393.17074465752, 'accumulated_eval_time': 5158.30740070343, 'accumulated_logging_time': 5.750935316085815}
I0203 03:56:12.289818 139702543816448 logging_writer.py:48] [117083] accumulated_eval_time=5158.307401, accumulated_logging_time=5.750935, accumulated_submission_time=53393.170745, global_step=117083, preemption_count=0, score=53393.170745, test/accuracy=0.460700, test/loss=2.471859, test/num_examples=10000, total_duration=58563.441803, train/accuracy=0.623281, train/loss=1.585521, validation/accuracy=0.577860, validation/loss=1.817060, validation/num_examples=50000
I0203 03:56:19.479766 139702527031040 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.2063676118850708, loss=4.441048622131348
I0203 03:57:01.900493 139702543816448 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.2660105228424072, loss=2.6850290298461914
I0203 03:57:47.913786 139702527031040 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.2530653476715088, loss=2.866997241973877
I0203 03:58:33.889075 139702543816448 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.4126006364822388, loss=2.724761962890625
I0203 03:59:19.979748 139702527031040 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.4215844869613647, loss=2.6984703540802
I0203 04:00:06.329501 139702543816448 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.2643731832504272, loss=3.2508699893951416
I0203 04:00:52.324543 139702527031040 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.4075125455856323, loss=2.6089026927948
I0203 04:01:38.340590 139702543816448 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.3625056743621826, loss=2.6113853454589844
I0203 04:02:24.500056 139702527031040 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.3378353118896484, loss=2.7168092727661133
I0203 04:03:10.780760 139702543816448 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.3398977518081665, loss=2.711470365524292
I0203 04:03:12.301949 139863983413056 spec.py:321] Evaluating on the training split.
I0203 04:03:22.649785 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 04:03:50.569194 139863983413056 spec.py:349] Evaluating on the test split.
I0203 04:03:52.213107 139863983413056 submission_runner.py:408] Time since start: 59023.41s, 	Step: 118005, 	{'train/accuracy': 0.6318945288658142, 'train/loss': 1.5542774200439453, 'validation/accuracy': 0.5718399882316589, 'validation/loss': 1.8370327949523926, 'validation/num_examples': 50000, 'test/accuracy': 0.46230003237724304, 'test/loss': 2.502239465713501, 'test/num_examples': 10000, 'score': 53813.120841264725, 'total_duration': 59023.410198926926, 'accumulated_submission_time': 53813.120841264725, 'accumulated_eval_time': 5198.218531131744, 'accumulated_logging_time': 5.809661865234375}
I0203 04:03:52.247716 139702527031040 logging_writer.py:48] [118005] accumulated_eval_time=5198.218531, accumulated_logging_time=5.809662, accumulated_submission_time=53813.120841, global_step=118005, preemption_count=0, score=53813.120841, test/accuracy=0.462300, test/loss=2.502239, test/num_examples=10000, total_duration=59023.410199, train/accuracy=0.631895, train/loss=1.554277, validation/accuracy=0.571840, validation/loss=1.837033, validation/num_examples=50000
I0203 04:04:31.852572 139702543816448 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.2685272693634033, loss=2.68865966796875
I0203 04:05:17.693526 139702527031040 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.329789161682129, loss=2.727956771850586
I0203 04:06:03.695723 139702543816448 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.4505614042282104, loss=2.790900230407715
I0203 04:06:49.445480 139702527031040 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.1289093494415283, loss=5.201138019561768
I0203 04:07:35.482347 139702543816448 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.0973180532455444, loss=4.153406143188477
I0203 04:08:21.541524 139702527031040 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.3793652057647705, loss=2.6771416664123535
I0203 04:09:07.595326 139702543816448 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.1794360876083374, loss=4.4665608406066895
I0203 04:09:53.564231 139702527031040 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.4572887420654297, loss=2.727128028869629
I0203 04:10:39.844865 139702543816448 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.1604259014129639, loss=3.2559025287628174
I0203 04:10:52.527061 139863983413056 spec.py:321] Evaluating on the training split.
I0203 04:11:02.851519 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 04:11:33.191972 139863983413056 spec.py:349] Evaluating on the test split.
I0203 04:11:34.833534 139863983413056 submission_runner.py:408] Time since start: 59486.03s, 	Step: 118929, 	{'train/accuracy': 0.6213085651397705, 'train/loss': 1.6146199703216553, 'validation/accuracy': 0.5793200135231018, 'validation/loss': 1.8063288927078247, 'validation/num_examples': 50000, 'test/accuracy': 0.4620000123977661, 'test/loss': 2.4387903213500977, 'test/num_examples': 10000, 'score': 54233.34093928337, 'total_duration': 59486.030648469925, 'accumulated_submission_time': 54233.34093928337, 'accumulated_eval_time': 5240.525000333786, 'accumulated_logging_time': 5.854615688323975}
I0203 04:11:34.867194 139702527031040 logging_writer.py:48] [118929] accumulated_eval_time=5240.525000, accumulated_logging_time=5.854616, accumulated_submission_time=54233.340939, global_step=118929, preemption_count=0, score=54233.340939, test/accuracy=0.462000, test/loss=2.438790, test/num_examples=10000, total_duration=59486.030648, train/accuracy=0.621309, train/loss=1.614620, validation/accuracy=0.579320, validation/loss=1.806329, validation/num_examples=50000
I0203 04:12:03.648658 139702543816448 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.3346688747406006, loss=2.618708610534668
I0203 04:12:49.268472 139702527031040 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.5050749778747559, loss=2.7336583137512207
I0203 04:13:35.398278 139702543816448 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.3172987699508667, loss=2.597822427749634
I0203 04:14:21.486743 139702527031040 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.3152981996536255, loss=2.7513022422790527
I0203 04:15:07.760230 139702543816448 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.4771769046783447, loss=3.0846354961395264
I0203 04:15:53.642090 139702527031040 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.3459655046463013, loss=3.031255006790161
I0203 04:16:40.164988 139702543816448 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.2704116106033325, loss=3.049020767211914
I0203 04:17:26.073265 139702527031040 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.3473858833312988, loss=2.7701072692871094
I0203 04:18:12.093836 139702543816448 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.179819941520691, loss=3.933361291885376
I0203 04:18:35.278197 139863983413056 spec.py:321] Evaluating on the training split.
I0203 04:18:45.580641 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 04:19:09.029430 139863983413056 spec.py:349] Evaluating on the test split.
I0203 04:19:10.676441 139863983413056 submission_runner.py:408] Time since start: 59941.87s, 	Step: 119852, 	{'train/accuracy': 0.6187695264816284, 'train/loss': 1.6314858198165894, 'validation/accuracy': 0.5747199654579163, 'validation/loss': 1.8536180257797241, 'validation/num_examples': 50000, 'test/accuracy': 0.45830002427101135, 'test/loss': 2.520430088043213, 'test/num_examples': 10000, 'score': 54653.69129776955, 'total_duration': 59941.873532533646, 'accumulated_submission_time': 54653.69129776955, 'accumulated_eval_time': 5275.923225164413, 'accumulated_logging_time': 5.900138854980469}
I0203 04:19:10.713802 139702527031040 logging_writer.py:48] [119852] accumulated_eval_time=5275.923225, accumulated_logging_time=5.900139, accumulated_submission_time=54653.691298, global_step=119852, preemption_count=0, score=54653.691298, test/accuracy=0.458300, test/loss=2.520430, test/num_examples=10000, total_duration=59941.873533, train/accuracy=0.618770, train/loss=1.631486, validation/accuracy=0.574720, validation/loss=1.853618, validation/num_examples=50000
I0203 04:19:30.279395 139702543816448 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.2856003046035767, loss=4.942613124847412
I0203 04:20:14.671892 139702527031040 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.3950761556625366, loss=2.6015126705169678
I0203 04:21:01.194250 139702543816448 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.1221424341201782, loss=3.975174903869629
I0203 04:21:47.137620 139702527031040 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.3391510248184204, loss=4.505730152130127
I0203 04:22:33.545566 139702543816448 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.221971035003662, loss=4.087132930755615
I0203 04:23:19.776054 139702527031040 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.4070836305618286, loss=2.7361066341400146
I0203 04:24:05.797025 139702543816448 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.44280207157135, loss=2.6590163707733154
I0203 04:24:51.786393 139702527031040 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.303304672241211, loss=3.9285314083099365
I0203 04:25:37.828367 139702543816448 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.428194522857666, loss=2.738076686859131
I0203 04:26:10.809979 139863983413056 spec.py:321] Evaluating on the training split.
I0203 04:26:21.031821 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 04:26:49.306941 139863983413056 spec.py:349] Evaluating on the test split.
I0203 04:26:50.944302 139863983413056 submission_runner.py:408] Time since start: 60402.14s, 	Step: 120773, 	{'train/accuracy': 0.6486914157867432, 'train/loss': 1.4689342975616455, 'validation/accuracy': 0.5861999988555908, 'validation/loss': 1.7645444869995117, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.431938886642456, 'test/num_examples': 10000, 'score': 55073.72949528694, 'total_duration': 60402.14140725136, 'accumulated_submission_time': 55073.72949528694, 'accumulated_eval_time': 5316.057541370392, 'accumulated_logging_time': 5.947498083114624}
I0203 04:26:50.983336 139702527031040 logging_writer.py:48] [120773] accumulated_eval_time=5316.057541, accumulated_logging_time=5.947498, accumulated_submission_time=55073.729495, global_step=120773, preemption_count=0, score=55073.729495, test/accuracy=0.468700, test/loss=2.431939, test/num_examples=10000, total_duration=60402.141407, train/accuracy=0.648691, train/loss=1.468934, validation/accuracy=0.586200, validation/loss=1.764544, validation/num_examples=50000
I0203 04:27:02.153315 139702543816448 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.285518765449524, loss=2.5763654708862305
I0203 04:27:44.951129 139702527031040 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.2868220806121826, loss=2.686837673187256
I0203 04:28:31.123991 139702543816448 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.3226532936096191, loss=2.562757968902588
I0203 04:29:17.379970 139702527031040 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.3757574558258057, loss=2.6249823570251465
I0203 04:30:03.917707 139702543816448 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.3128845691680908, loss=2.490152597427368
I0203 04:30:49.988485 139702527031040 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.1081345081329346, loss=4.02055025100708
I0203 04:31:36.988717 139702543816448 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.1443772315979004, loss=4.347476005554199
I0203 04:32:22.981121 139702527031040 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.587357997894287, loss=2.8252689838409424
I0203 04:33:09.250039 139702543816448 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.0139179229736328, loss=4.360526084899902
I0203 04:33:51.101163 139863983413056 spec.py:321] Evaluating on the training split.
I0203 04:34:01.456792 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 04:34:31.258669 139863983413056 spec.py:349] Evaluating on the test split.
I0203 04:34:32.902361 139863983413056 submission_runner.py:408] Time since start: 60864.10s, 	Step: 121692, 	{'train/accuracy': 0.6215234398841858, 'train/loss': 1.6078003644943237, 'validation/accuracy': 0.5834999680519104, 'validation/loss': 1.7998595237731934, 'validation/num_examples': 50000, 'test/accuracy': 0.4652000367641449, 'test/loss': 2.4484989643096924, 'test/num_examples': 10000, 'score': 55493.78798747063, 'total_duration': 60864.09947562218, 'accumulated_submission_time': 55493.78798747063, 'accumulated_eval_time': 5357.858735084534, 'accumulated_logging_time': 5.998109817504883}
I0203 04:34:32.937446 139702527031040 logging_writer.py:48] [121692] accumulated_eval_time=5357.858735, accumulated_logging_time=5.998110, accumulated_submission_time=55493.787987, global_step=121692, preemption_count=0, score=55493.787987, test/accuracy=0.465200, test/loss=2.448499, test/num_examples=10000, total_duration=60864.099476, train/accuracy=0.621523, train/loss=1.607800, validation/accuracy=0.583500, validation/loss=1.799860, validation/num_examples=50000
I0203 04:34:36.535848 139702543816448 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.4002773761749268, loss=2.781528949737549
I0203 04:35:18.649281 139702527031040 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.4710731506347656, loss=2.57460355758667
I0203 04:36:04.430579 139702543816448 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.0559927225112915, loss=4.314036846160889
I0203 04:36:50.457275 139702527031040 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.377034068107605, loss=2.5240638256073
I0203 04:37:36.293086 139702543816448 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.4039725065231323, loss=2.6314873695373535
I0203 04:38:22.360083 139702527031040 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.1944395303726196, loss=5.14718770980835
I0203 04:39:08.451320 139702543816448 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.3462120294570923, loss=3.1402266025543213
I0203 04:39:54.931097 139702527031040 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.1061419248580933, loss=5.0823187828063965
I0203 04:40:40.826143 139702543816448 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.4657196998596191, loss=2.5577971935272217
I0203 04:41:27.089204 139702527031040 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.2266733646392822, loss=5.173183441162109
I0203 04:41:33.279208 139863983413056 spec.py:321] Evaluating on the training split.
I0203 04:41:43.585808 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 04:42:11.577195 139863983413056 spec.py:349] Evaluating on the test split.
I0203 04:42:13.208578 139863983413056 submission_runner.py:408] Time since start: 61324.41s, 	Step: 122615, 	{'train/accuracy': 0.6367577910423279, 'train/loss': 1.5228012800216675, 'validation/accuracy': 0.5913800001144409, 'validation/loss': 1.7486042976379395, 'validation/num_examples': 50000, 'test/accuracy': 0.4740000367164612, 'test/loss': 2.3974080085754395, 'test/num_examples': 10000, 'score': 55914.06940293312, 'total_duration': 61324.40569233894, 'accumulated_submission_time': 55914.06940293312, 'accumulated_eval_time': 5397.7881071567535, 'accumulated_logging_time': 6.045310974121094}
I0203 04:42:13.244624 139702543816448 logging_writer.py:48] [122615] accumulated_eval_time=5397.788107, accumulated_logging_time=6.045311, accumulated_submission_time=55914.069403, global_step=122615, preemption_count=0, score=55914.069403, test/accuracy=0.474000, test/loss=2.397408, test/num_examples=10000, total_duration=61324.405692, train/accuracy=0.636758, train/loss=1.522801, validation/accuracy=0.591380, validation/loss=1.748604, validation/num_examples=50000
I0203 04:42:48.134991 139702527031040 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.1788479089736938, loss=3.426847457885742
I0203 04:43:33.824642 139702543816448 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.1751519441604614, loss=5.265839099884033
I0203 04:44:19.973529 139702527031040 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.324504017829895, loss=2.9418106079101562
I0203 04:45:06.038016 139702543816448 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.4083716869354248, loss=2.5650668144226074
I0203 04:45:52.001651 139702527031040 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.5430653095245361, loss=2.6257119178771973
I0203 04:46:37.790605 139702543816448 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.2343716621398926, loss=5.0197014808654785
I0203 04:47:23.536557 139702527031040 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.5694279670715332, loss=2.750560760498047
I0203 04:48:09.640753 139702543816448 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.2400175333023071, loss=3.5580978393554688
I0203 04:48:55.696420 139702527031040 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.1120007038116455, loss=4.0491228103637695
I0203 04:49:13.383282 139863983413056 spec.py:321] Evaluating on the training split.
I0203 04:49:23.978747 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 04:49:54.165219 139863983413056 spec.py:349] Evaluating on the test split.
I0203 04:49:55.811231 139863983413056 submission_runner.py:408] Time since start: 61787.01s, 	Step: 123540, 	{'train/accuracy': 0.644335925579071, 'train/loss': 1.5328458547592163, 'validation/accuracy': 0.5850600004196167, 'validation/loss': 1.7931092977523804, 'validation/num_examples': 50000, 'test/accuracy': 0.46820002794265747, 'test/loss': 2.4418222904205322, 'test/num_examples': 10000, 'score': 56334.1478741169, 'total_duration': 61787.00834584236, 'accumulated_submission_time': 56334.1478741169, 'accumulated_eval_time': 5440.216062545776, 'accumulated_logging_time': 6.09236216545105}
I0203 04:49:55.848677 139702543816448 logging_writer.py:48] [123540] accumulated_eval_time=5440.216063, accumulated_logging_time=6.092362, accumulated_submission_time=56334.147874, global_step=123540, preemption_count=0, score=56334.147874, test/accuracy=0.468200, test/loss=2.441822, test/num_examples=10000, total_duration=61787.008346, train/accuracy=0.644336, train/loss=1.532846, validation/accuracy=0.585060, validation/loss=1.793109, validation/num_examples=50000
I0203 04:50:20.204680 139702527031040 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.4440559148788452, loss=2.644500970840454
I0203 04:51:05.198243 139702543816448 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.3292899131774902, loss=3.150085926055908
I0203 04:51:51.440668 139702527031040 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.1289408206939697, loss=5.077130317687988
I0203 04:52:37.898501 139702543816448 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.4149161577224731, loss=2.709818124771118
I0203 04:53:23.775724 139702527031040 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.2446331977844238, loss=4.590534687042236
I0203 04:54:09.799348 139702543816448 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.0753871202468872, loss=4.400728702545166
I0203 04:54:55.979303 139702527031040 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.413283109664917, loss=2.8102664947509766
I0203 04:55:42.153490 139702543816448 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.230437994003296, loss=5.086867332458496
I0203 04:56:28.062984 139702527031040 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.4360301494598389, loss=2.6166746616363525
I0203 04:56:55.832478 139863983413056 spec.py:321] Evaluating on the training split.
I0203 04:57:06.228791 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 04:57:34.397973 139863983413056 spec.py:349] Evaluating on the test split.
I0203 04:57:36.042896 139863983413056 submission_runner.py:408] Time since start: 62247.24s, 	Step: 124462, 	{'train/accuracy': 0.6310741901397705, 'train/loss': 1.5595864057540894, 'validation/accuracy': 0.5909199714660645, 'validation/loss': 1.7579379081726074, 'validation/num_examples': 50000, 'test/accuracy': 0.4686000347137451, 'test/loss': 2.41546893119812, 'test/num_examples': 10000, 'score': 56754.07245540619, 'total_duration': 62247.24000930786, 'accumulated_submission_time': 56754.07245540619, 'accumulated_eval_time': 5480.426479578018, 'accumulated_logging_time': 6.140239953994751}
I0203 04:57:36.082674 139702543816448 logging_writer.py:48] [124462] accumulated_eval_time=5480.426480, accumulated_logging_time=6.140240, accumulated_submission_time=56754.072455, global_step=124462, preemption_count=0, score=56754.072455, test/accuracy=0.468600, test/loss=2.415469, test/num_examples=10000, total_duration=62247.240009, train/accuracy=0.631074, train/loss=1.559586, validation/accuracy=0.590920, validation/loss=1.757938, validation/num_examples=50000
I0203 04:57:51.672193 139702527031040 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.4335614442825317, loss=2.538069248199463
I0203 04:58:35.338858 139702543816448 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.3417118787765503, loss=3.324002504348755
I0203 04:59:21.564713 139702527031040 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.3494054079055786, loss=2.6868391036987305
I0203 05:00:07.852853 139702543816448 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.4477277994155884, loss=2.533388614654541
I0203 05:00:53.874255 139702527031040 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.4947413206100464, loss=2.536931276321411
I0203 05:01:39.729443 139702543816448 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.2572135925292969, loss=3.4996774196624756
I0203 05:02:26.449782 139702527031040 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.4932503700256348, loss=2.5571649074554443
I0203 05:03:12.619133 139702543816448 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.4561561346054077, loss=2.4948232173919678
I0203 05:03:58.711086 139702527031040 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.5740991830825806, loss=2.5258913040161133
I0203 05:04:36.157064 139863983413056 spec.py:321] Evaluating on the training split.
I0203 05:04:47.050921 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 05:05:17.322588 139863983413056 spec.py:349] Evaluating on the test split.
I0203 05:05:18.965103 139863983413056 submission_runner.py:408] Time since start: 62710.16s, 	Step: 125382, 	{'train/accuracy': 0.6387109160423279, 'train/loss': 1.5228734016418457, 'validation/accuracy': 0.5959399938583374, 'validation/loss': 1.7375160455703735, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.390855073928833, 'test/num_examples': 10000, 'score': 57174.0851354599, 'total_duration': 62710.162212610245, 'accumulated_submission_time': 57174.0851354599, 'accumulated_eval_time': 5523.234518289566, 'accumulated_logging_time': 6.193271636962891}
I0203 05:05:19.002504 139702543816448 logging_writer.py:48] [125382] accumulated_eval_time=5523.234518, accumulated_logging_time=6.193272, accumulated_submission_time=57174.085135, global_step=125382, preemption_count=0, score=57174.085135, test/accuracy=0.475500, test/loss=2.390855, test/num_examples=10000, total_duration=62710.162213, train/accuracy=0.638711, train/loss=1.522873, validation/accuracy=0.595940, validation/loss=1.737516, validation/num_examples=50000
I0203 05:05:26.581604 139702527031040 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.4642397165298462, loss=3.0039548873901367
I0203 05:06:09.133981 139702543816448 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.1429047584533691, loss=4.71636438369751
I0203 05:06:55.012261 139702527031040 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.5272853374481201, loss=2.4192605018615723
I0203 05:07:41.196019 139702543816448 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.2233282327651978, loss=3.9191830158233643
I0203 05:08:27.288456 139702527031040 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.500656247138977, loss=2.7267091274261475
I0203 05:09:13.472668 139702543816448 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.2267733812332153, loss=3.424846887588501
I0203 05:09:59.780467 139702527031040 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.1945604085922241, loss=4.178192138671875
I0203 05:10:45.843232 139702543816448 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.4223048686981201, loss=2.652268171310425
I0203 05:11:32.120203 139702527031040 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.5205187797546387, loss=2.594022750854492
I0203 05:12:18.151712 139702543816448 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.4588905572891235, loss=2.590628147125244
I0203 05:12:19.194990 139863983413056 spec.py:321] Evaluating on the training split.
I0203 05:12:29.544675 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 05:12:57.966524 139863983413056 spec.py:349] Evaluating on the test split.
I0203 05:12:59.607218 139863983413056 submission_runner.py:408] Time since start: 63170.80s, 	Step: 126304, 	{'train/accuracy': 0.6492577791213989, 'train/loss': 1.4756730794906616, 'validation/accuracy': 0.5982199907302856, 'validation/loss': 1.706802487373352, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.3549671173095703, 'test/num_examples': 10000, 'score': 57594.218314647675, 'total_duration': 63170.8043320179, 'accumulated_submission_time': 57594.218314647675, 'accumulated_eval_time': 5563.646774530411, 'accumulated_logging_time': 6.2415242195129395}
I0203 05:12:59.642423 139702527031040 logging_writer.py:48] [126304] accumulated_eval_time=5563.646775, accumulated_logging_time=6.241524, accumulated_submission_time=57594.218315, global_step=126304, preemption_count=0, score=57594.218315, test/accuracy=0.480900, test/loss=2.354967, test/num_examples=10000, total_duration=63170.804332, train/accuracy=0.649258, train/loss=1.475673, validation/accuracy=0.598220, validation/loss=1.706802, validation/num_examples=50000
I0203 05:13:39.615847 139702543816448 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.1782869100570679, loss=4.611234188079834
I0203 05:14:25.325781 139702527031040 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.2884453535079956, loss=3.0403902530670166
I0203 05:15:11.462347 139702543816448 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.452651023864746, loss=2.471651077270508
I0203 05:15:57.310823 139702527031040 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.2585606575012207, loss=3.503166913986206
I0203 05:16:43.280171 139702543816448 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.4797985553741455, loss=2.490877628326416
I0203 05:17:29.486135 139702527031040 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.4726663827896118, loss=2.5094423294067383
I0203 05:18:15.619089 139702543816448 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.4035730361938477, loss=2.489229440689087
I0203 05:19:01.695133 139702527031040 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.3499128818511963, loss=2.7468795776367188
I0203 05:19:47.821084 139702543816448 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.5763458013534546, loss=2.5575356483459473
I0203 05:20:00.018393 139863983413056 spec.py:321] Evaluating on the training split.
I0203 05:20:10.679167 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 05:20:42.518045 139863983413056 spec.py:349] Evaluating on the test split.
I0203 05:20:44.151793 139863983413056 submission_runner.py:408] Time since start: 63635.35s, 	Step: 127228, 	{'train/accuracy': 0.6450781226158142, 'train/loss': 1.5104620456695557, 'validation/accuracy': 0.5978999733924866, 'validation/loss': 1.7296024560928345, 'validation/num_examples': 50000, 'test/accuracy': 0.47210001945495605, 'test/loss': 2.3971104621887207, 'test/num_examples': 10000, 'score': 58014.53499889374, 'total_duration': 63635.348907232285, 'accumulated_submission_time': 58014.53499889374, 'accumulated_eval_time': 5607.780184268951, 'accumulated_logging_time': 6.286353588104248}
I0203 05:20:44.189927 139702527031040 logging_writer.py:48] [127228] accumulated_eval_time=5607.780184, accumulated_logging_time=6.286354, accumulated_submission_time=58014.534999, global_step=127228, preemption_count=0, score=58014.534999, test/accuracy=0.472100, test/loss=2.397110, test/num_examples=10000, total_duration=63635.348907, train/accuracy=0.645078, train/loss=1.510462, validation/accuracy=0.597900, validation/loss=1.729602, validation/num_examples=50000
I0203 05:21:13.328008 139702543816448 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.1828652620315552, loss=3.8437979221343994
I0203 05:21:58.939157 139702527031040 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.474380612373352, loss=2.4467170238494873
I0203 05:22:45.201297 139702543816448 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.2712647914886475, loss=2.942326784133911
I0203 05:23:31.493721 139702527031040 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.6913783550262451, loss=2.659285545349121
I0203 05:24:17.482192 139702543816448 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.554935336112976, loss=2.6139111518859863
I0203 05:25:03.521268 139702527031040 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.2272255420684814, loss=4.2601776123046875
I0203 05:25:49.807334 139702543816448 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.6900473833084106, loss=2.5183136463165283
I0203 05:26:35.884598 139702527031040 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.3640918731689453, loss=2.6314597129821777
I0203 05:27:22.070084 139702543816448 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.415432333946228, loss=2.6855459213256836
I0203 05:27:44.333221 139863983413056 spec.py:321] Evaluating on the training split.
I0203 05:27:54.774462 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 05:28:24.048987 139863983413056 spec.py:349] Evaluating on the test split.
I0203 05:28:25.677957 139863983413056 submission_runner.py:408] Time since start: 64096.88s, 	Step: 128150, 	{'train/accuracy': 0.6434765458106995, 'train/loss': 1.4898433685302734, 'validation/accuracy': 0.6008999943733215, 'validation/loss': 1.686245083808899, 'validation/num_examples': 50000, 'test/accuracy': 0.48260003328323364, 'test/loss': 2.3594110012054443, 'test/num_examples': 10000, 'score': 58434.6179728508, 'total_duration': 64096.87507081032, 'accumulated_submission_time': 58434.6179728508, 'accumulated_eval_time': 5649.124925851822, 'accumulated_logging_time': 6.335825204849243}
I0203 05:28:25.717119 139702527031040 logging_writer.py:48] [128150] accumulated_eval_time=5649.124926, accumulated_logging_time=6.335825, accumulated_submission_time=58434.617973, global_step=128150, preemption_count=0, score=58434.617973, test/accuracy=0.482600, test/loss=2.359411, test/num_examples=10000, total_duration=64096.875071, train/accuracy=0.643477, train/loss=1.489843, validation/accuracy=0.600900, validation/loss=1.686245, validation/num_examples=50000
I0203 05:28:46.065843 139702543816448 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.3083423376083374, loss=2.801561117172241
I0203 05:29:30.453124 139702527031040 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.741973638534546, loss=2.6457066535949707
I0203 05:30:16.649625 139702543816448 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.3896294832229614, loss=2.3295857906341553
I0203 05:31:02.710028 139702527031040 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.4751263856887817, loss=2.7496466636657715
I0203 05:31:48.690368 139702543816448 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.5764744281768799, loss=2.5996077060699463
I0203 05:32:34.884265 139702527031040 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.5613843202590942, loss=2.749990463256836
I0203 05:33:20.878980 139702543816448 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.4497344493865967, loss=2.453700304031372
I0203 05:34:07.246289 139702527031040 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.5472217798233032, loss=2.615595817565918
I0203 05:34:53.054419 139702543816448 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.2221397161483765, loss=5.025081634521484
I0203 05:35:25.997230 139863983413056 spec.py:321] Evaluating on the training split.
I0203 05:35:36.503446 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 05:36:05.113639 139863983413056 spec.py:349] Evaluating on the test split.
I0203 05:36:06.755622 139863983413056 submission_runner.py:408] Time since start: 64557.95s, 	Step: 129073, 	{'train/accuracy': 0.6571874618530273, 'train/loss': 1.4405206441879272, 'validation/accuracy': 0.6071999669075012, 'validation/loss': 1.6774921417236328, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.321425199508667, 'test/num_examples': 10000, 'score': 58854.83988237381, 'total_duration': 64557.95269560814, 'accumulated_submission_time': 58854.83988237381, 'accumulated_eval_time': 5689.88328742981, 'accumulated_logging_time': 6.38477087020874}
I0203 05:36:06.799208 139702527031040 logging_writer.py:48] [129073] accumulated_eval_time=5689.883287, accumulated_logging_time=6.384771, accumulated_submission_time=58854.839882, global_step=129073, preemption_count=0, score=58854.839882, test/accuracy=0.490700, test/loss=2.321425, test/num_examples=10000, total_duration=64557.952696, train/accuracy=0.657187, train/loss=1.440521, validation/accuracy=0.607200, validation/loss=1.677492, validation/num_examples=50000
I0203 05:36:17.978491 139702543816448 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.4106945991516113, loss=3.40891170501709
I0203 05:37:00.970663 139702527031040 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.2831051349639893, loss=4.614445209503174
I0203 05:37:46.898590 139702543816448 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.4211307764053345, loss=2.8582448959350586
I0203 05:38:33.115216 139702527031040 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.3928099870681763, loss=3.235236644744873
I0203 05:39:19.004607 139702543816448 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.3834223747253418, loss=2.497082471847534
I0203 05:40:05.638343 139702527031040 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.713370442390442, loss=2.480870485305786
I0203 05:40:51.228235 139702543816448 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.3129349946975708, loss=2.774076461791992
I0203 05:41:37.189066 139702527031040 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.388533115386963, loss=4.906219482421875
I0203 05:42:23.406810 139702543816448 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.1914557218551636, loss=4.752327919006348
I0203 05:43:06.757444 139863983413056 spec.py:321] Evaluating on the training split.
I0203 05:43:17.146583 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 05:43:49.229928 139863983413056 spec.py:349] Evaluating on the test split.
I0203 05:43:50.865256 139863983413056 submission_runner.py:408] Time since start: 65022.06s, 	Step: 129996, 	{'train/accuracy': 0.6743749976158142, 'train/loss': 1.3620802164077759, 'validation/accuracy': 0.6096000075340271, 'validation/loss': 1.6756395101547241, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.335404634475708, 'test/num_examples': 10000, 'score': 59274.73733615875, 'total_duration': 65022.062368392944, 'accumulated_submission_time': 59274.73733615875, 'accumulated_eval_time': 5733.991091012955, 'accumulated_logging_time': 6.439935207366943}
I0203 05:43:50.912289 139702527031040 logging_writer.py:48] [129996] accumulated_eval_time=5733.991091, accumulated_logging_time=6.439935, accumulated_submission_time=59274.737336, global_step=129996, preemption_count=0, score=59274.737336, test/accuracy=0.486900, test/loss=2.335405, test/num_examples=10000, total_duration=65022.062368, train/accuracy=0.674375, train/loss=1.362080, validation/accuracy=0.609600, validation/loss=1.675640, validation/num_examples=50000
I0203 05:43:52.906746 139702543816448 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.2742137908935547, loss=4.028842926025391
I0203 05:44:34.454548 139702527031040 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.293503761291504, loss=4.866732120513916
I0203 05:46:11.370595 139702543816448 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.5659104585647583, loss=2.587254762649536
I0203 05:46:58.843769 139702527031040 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.3201789855957031, loss=2.803861618041992
I0203 05:47:45.365195 139702543816448 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.4423540830612183, loss=3.0682449340820312
I0203 05:48:31.741792 139702527031040 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.5543181896209717, loss=2.625288724899292
I0203 05:49:18.178524 139702543816448 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.27921462059021, loss=3.378584861755371
I0203 05:50:04.691299 139702527031040 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.6226478815078735, loss=2.525202989578247
I0203 05:50:51.166238 139702543816448 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.3410989046096802, loss=3.086322069168091
I0203 05:50:51.180123 139863983413056 spec.py:321] Evaluating on the training split.
I0203 05:51:01.714430 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 05:51:28.259931 139863983413056 spec.py:349] Evaluating on the test split.
I0203 05:51:29.903359 139863983413056 submission_runner.py:408] Time since start: 65481.10s, 	Step: 130801, 	{'train/accuracy': 0.6576952934265137, 'train/loss': 1.4295475482940674, 'validation/accuracy': 0.6107199788093567, 'validation/loss': 1.6498571634292603, 'validation/num_examples': 50000, 'test/accuracy': 0.4888000190258026, 'test/loss': 2.307673454284668, 'test/num_examples': 10000, 'score': 59694.95303297043, 'total_duration': 65481.10044121742, 'accumulated_submission_time': 59694.95303297043, 'accumulated_eval_time': 5772.714293003082, 'accumulated_logging_time': 6.497478723526001}
I0203 05:51:29.949060 139702527031040 logging_writer.py:48] [130801] accumulated_eval_time=5772.714293, accumulated_logging_time=6.497479, accumulated_submission_time=59694.953033, global_step=130801, preemption_count=0, score=59694.953033, test/accuracy=0.488800, test/loss=2.307673, test/num_examples=10000, total_duration=65481.100441, train/accuracy=0.657695, train/loss=1.429548, validation/accuracy=0.610720, validation/loss=1.649857, validation/num_examples=50000
I0203 05:52:11.328269 139702543816448 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.7573723793029785, loss=2.4470608234405518
I0203 05:52:57.075922 139702527031040 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.3546814918518066, loss=3.813734292984009
I0203 05:53:43.199852 139702543816448 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.5380080938339233, loss=2.407290458679199
I0203 05:54:29.098276 139702527031040 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.3628290891647339, loss=2.909681558609009
I0203 05:55:15.029673 139702543816448 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.322659969329834, loss=3.4585535526275635
I0203 05:56:01.223098 139702527031040 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.3728171586990356, loss=2.4661824703216553
I0203 05:56:47.239955 139702543816448 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.5561500787734985, loss=2.7335610389709473
I0203 05:57:33.381629 139702527031040 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.5472323894500732, loss=2.6791090965270996
I0203 05:58:19.435575 139702543816448 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.570225477218628, loss=2.4653961658477783
I0203 05:58:30.260004 139863983413056 spec.py:321] Evaluating on the training split.
I0203 05:58:41.072358 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 05:59:07.517827 139863983413056 spec.py:349] Evaluating on the test split.
I0203 05:59:09.153443 139863983413056 submission_runner.py:408] Time since start: 65940.35s, 	Step: 131725, 	{'train/accuracy': 0.6663476228713989, 'train/loss': 1.3924933671951294, 'validation/accuracy': 0.6150000095367432, 'validation/loss': 1.636078119277954, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.286726474761963, 'test/num_examples': 10000, 'score': 60115.202788591385, 'total_duration': 65940.35055208206, 'accumulated_submission_time': 60115.202788591385, 'accumulated_eval_time': 5811.607728004456, 'accumulated_logging_time': 6.554529428482056}
I0203 05:59:09.192038 139702527031040 logging_writer.py:48] [131725] accumulated_eval_time=5811.607728, accumulated_logging_time=6.554529, accumulated_submission_time=60115.202789, global_step=131725, preemption_count=0, score=60115.202789, test/accuracy=0.494100, test/loss=2.286726, test/num_examples=10000, total_duration=65940.350552, train/accuracy=0.666348, train/loss=1.392493, validation/accuracy=0.615000, validation/loss=1.636078, validation/num_examples=50000
I0203 05:59:39.645069 139702543816448 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.5713962316513062, loss=2.4811654090881348
I0203 06:00:25.729965 139702527031040 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.3049561977386475, loss=3.737983226776123
I0203 06:01:12.074017 139702543816448 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.4910252094268799, loss=2.3516178131103516
I0203 06:01:58.102866 139702527031040 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.4830454587936401, loss=2.27651309967041
I0203 06:02:44.068386 139702543816448 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.5310397148132324, loss=2.81333065032959
I0203 06:03:30.138840 139702527031040 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.3834036588668823, loss=3.792623281478882
I0203 06:04:16.663810 139702543816448 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.6632564067840576, loss=2.3865180015563965
I0203 06:05:02.350679 139702527031040 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.6525392532348633, loss=2.5659143924713135
I0203 06:05:48.385389 139702543816448 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.5748618841171265, loss=2.4831907749176025
I0203 06:06:09.240251 139863983413056 spec.py:321] Evaluating on the training split.
I0203 06:06:20.794503 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 06:06:47.077404 139863983413056 spec.py:349] Evaluating on the test split.
I0203 06:06:48.706912 139863983413056 submission_runner.py:408] Time since start: 66399.90s, 	Step: 132646, 	{'train/accuracy': 0.6916796565055847, 'train/loss': 1.280269742012024, 'validation/accuracy': 0.6193599700927734, 'validation/loss': 1.6119118928909302, 'validation/num_examples': 50000, 'test/accuracy': 0.4978000223636627, 'test/loss': 2.2578117847442627, 'test/num_examples': 10000, 'score': 60535.19282770157, 'total_duration': 66399.90402460098, 'accumulated_submission_time': 60535.19282770157, 'accumulated_eval_time': 5851.074378013611, 'accumulated_logging_time': 6.603094100952148}
I0203 06:06:48.742460 139702527031040 logging_writer.py:48] [132646] accumulated_eval_time=5851.074378, accumulated_logging_time=6.603094, accumulated_submission_time=60535.192828, global_step=132646, preemption_count=0, score=60535.192828, test/accuracy=0.497800, test/loss=2.257812, test/num_examples=10000, total_duration=66399.904025, train/accuracy=0.691680, train/loss=1.280270, validation/accuracy=0.619360, validation/loss=1.611912, validation/num_examples=50000
I0203 06:07:10.704108 139702543816448 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.3567875623703003, loss=4.9572834968566895
I0203 06:07:54.952425 139702527031040 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.5360968112945557, loss=2.4534308910369873
I0203 06:08:40.864567 139702543816448 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.4831829071044922, loss=2.3729498386383057
I0203 06:09:26.985516 139702527031040 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.5720045566558838, loss=2.3961451053619385
I0203 06:10:13.006278 139702543816448 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.4324315786361694, loss=3.1561999320983887
I0203 06:10:58.866369 139702527031040 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.3869247436523438, loss=3.3033432960510254
I0203 06:11:44.986514 139702543816448 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.2513666152954102, loss=4.8854570388793945
I0203 06:12:31.181805 139702527031040 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.5188853740692139, loss=2.428272008895874
I0203 06:13:17.359011 139702543816448 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.643646001815796, loss=2.435784339904785
I0203 06:13:48.847810 139863983413056 spec.py:321] Evaluating on the training split.
I0203 06:13:59.294746 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 06:14:27.695338 139863983413056 spec.py:349] Evaluating on the test split.
I0203 06:14:29.331799 139863983413056 submission_runner.py:408] Time since start: 66860.53s, 	Step: 133570, 	{'train/accuracy': 0.6681445240974426, 'train/loss': 1.4086965322494507, 'validation/accuracy': 0.624239981174469, 'validation/loss': 1.609699010848999, 'validation/num_examples': 50000, 'test/accuracy': 0.4993000328540802, 'test/loss': 2.2752673625946045, 'test/num_examples': 10000, 'score': 60955.238864421844, 'total_duration': 66860.528911829, 'accumulated_submission_time': 60955.238864421844, 'accumulated_eval_time': 5891.558381795883, 'accumulated_logging_time': 6.649211645126343}
I0203 06:14:29.369007 139702527031040 logging_writer.py:48] [133570] accumulated_eval_time=5891.558382, accumulated_logging_time=6.649212, accumulated_submission_time=60955.238864, global_step=133570, preemption_count=0, score=60955.238864, test/accuracy=0.499300, test/loss=2.275267, test/num_examples=10000, total_duration=66860.528912, train/accuracy=0.668145, train/loss=1.408697, validation/accuracy=0.624240, validation/loss=1.609699, validation/num_examples=50000
I0203 06:14:41.740136 139702543816448 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.7406874895095825, loss=2.3844525814056396
I0203 06:15:25.023751 139702527031040 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.5546724796295166, loss=3.1219027042388916
I0203 06:16:11.229362 139702543816448 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.7443286180496216, loss=2.4060657024383545
I0203 06:16:57.581961 139702527031040 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.5337153673171997, loss=2.8730316162109375
I0203 06:17:43.382900 139702543816448 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.5672787427902222, loss=2.471510648727417
I0203 06:18:29.481261 139702527031040 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.6365922689437866, loss=2.769958972930908
I0203 06:19:15.353638 139702543816448 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.4729065895080566, loss=3.099719524383545
I0203 06:20:01.613297 139702527031040 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.6094095706939697, loss=2.299480438232422
I0203 06:20:47.655724 139702543816448 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.2953904867172241, loss=4.6807379722595215
I0203 06:21:29.541997 139863983413056 spec.py:321] Evaluating on the training split.
I0203 06:21:39.889795 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 06:22:09.656535 139863983413056 spec.py:349] Evaluating on the test split.
I0203 06:22:11.298888 139863983413056 submission_runner.py:408] Time since start: 67322.50s, 	Step: 134493, 	{'train/accuracy': 0.6716992259025574, 'train/loss': 1.369166374206543, 'validation/accuracy': 0.6208400130271912, 'validation/loss': 1.6039738655090332, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.2548952102661133, 'test/num_examples': 10000, 'score': 61375.3524119854, 'total_duration': 67322.49597454071, 'accumulated_submission_time': 61375.3524119854, 'accumulated_eval_time': 5933.31524014473, 'accumulated_logging_time': 6.696808576583862}
I0203 06:22:11.344135 139702527031040 logging_writer.py:48] [134493] accumulated_eval_time=5933.315240, accumulated_logging_time=6.696809, accumulated_submission_time=61375.352412, global_step=134493, preemption_count=0, score=61375.352412, test/accuracy=0.503400, test/loss=2.254895, test/num_examples=10000, total_duration=67322.495975, train/accuracy=0.671699, train/loss=1.369166, validation/accuracy=0.620840, validation/loss=1.603974, validation/num_examples=50000
I0203 06:22:15.041243 139702543816448 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.5275447368621826, loss=2.666076183319092
I0203 06:22:56.859484 139702527031040 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.662837028503418, loss=2.4401211738586426
I0203 06:23:42.829993 139702543816448 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.3456966876983643, loss=3.404034376144409
I0203 06:24:29.078786 139702527031040 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.5983898639678955, loss=2.5492286682128906
I0203 06:25:15.136621 139702543816448 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.6981511116027832, loss=2.329463005065918
I0203 06:26:01.213784 139702527031040 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.3907811641693115, loss=3.6984071731567383
I0203 06:26:47.296632 139702543816448 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.6427522897720337, loss=2.4409894943237305
I0203 06:27:33.952762 139702527031040 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.3323109149932861, loss=4.5540947914123535
I0203 06:28:19.957816 139702543816448 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.6330955028533936, loss=2.377305746078491
I0203 06:29:06.212468 139702527031040 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.3411539793014526, loss=3.563149929046631
I0203 06:29:11.411245 139863983413056 spec.py:321] Evaluating on the training split.
I0203 06:29:21.955089 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 06:29:47.759966 139863983413056 spec.py:349] Evaluating on the test split.
I0203 06:29:49.412043 139863983413056 submission_runner.py:408] Time since start: 67780.61s, 	Step: 135413, 	{'train/accuracy': 0.6820898056030273, 'train/loss': 1.3533567190170288, 'validation/accuracy': 0.6220999956130981, 'validation/loss': 1.6352964639663696, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.288580894470215, 'test/num_examples': 10000, 'score': 61794.85926914215, 'total_duration': 67780.60915780067, 'accumulated_submission_time': 61794.85926914215, 'accumulated_eval_time': 5971.316042661667, 'accumulated_logging_time': 7.2534918785095215}
I0203 06:29:49.452398 139702543816448 logging_writer.py:48] [135413] accumulated_eval_time=5971.316043, accumulated_logging_time=7.253492, accumulated_submission_time=61794.859269, global_step=135413, preemption_count=0, score=61794.859269, test/accuracy=0.495900, test/loss=2.288581, test/num_examples=10000, total_duration=67780.609158, train/accuracy=0.682090, train/loss=1.353357, validation/accuracy=0.622100, validation/loss=1.635296, validation/num_examples=50000
I0203 06:30:25.216950 139702527031040 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.3469829559326172, loss=4.753387451171875
I0203 06:31:11.176967 139702543816448 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.383213758468628, loss=4.692895412445068
I0203 06:31:57.599244 139702527031040 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.6288676261901855, loss=2.368846893310547
I0203 06:32:43.759057 139702543816448 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.6683735847473145, loss=2.38686203956604
I0203 06:33:29.791352 139702527031040 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.692216396331787, loss=2.2820847034454346
I0203 06:34:15.983256 139702543816448 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.65618097782135, loss=2.589266777038574
I0203 06:35:02.109676 139702527031040 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.6926063299179077, loss=2.438305377960205
I0203 06:35:48.379229 139702543816448 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.5310415029525757, loss=2.4060218334198
I0203 06:36:34.365929 139702527031040 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.4844971895217896, loss=2.862288475036621
I0203 06:36:49.768144 139863983413056 spec.py:321] Evaluating on the training split.
I0203 06:37:00.431948 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 06:37:29.905374 139863983413056 spec.py:349] Evaluating on the test split.
I0203 06:37:31.547997 139863983413056 submission_runner.py:408] Time since start: 68242.75s, 	Step: 136335, 	{'train/accuracy': 0.67431640625, 'train/loss': 1.3767963647842407, 'validation/accuracy': 0.6293999552726746, 'validation/loss': 1.5818134546279907, 'validation/num_examples': 50000, 'test/accuracy': 0.5031000375747681, 'test/loss': 2.237124443054199, 'test/num_examples': 10000, 'score': 62215.11662912369, 'total_duration': 68242.74508333206, 'accumulated_submission_time': 62215.11662912369, 'accumulated_eval_time': 6013.095870256424, 'accumulated_logging_time': 7.303630352020264}
I0203 06:37:31.593116 139702543816448 logging_writer.py:48] [136335] accumulated_eval_time=6013.095870, accumulated_logging_time=7.303630, accumulated_submission_time=62215.116629, global_step=136335, preemption_count=0, score=62215.116629, test/accuracy=0.503100, test/loss=2.237124, test/num_examples=10000, total_duration=68242.745083, train/accuracy=0.674316, train/loss=1.376796, validation/accuracy=0.629400, validation/loss=1.581813, validation/num_examples=50000
I0203 06:37:57.977126 139702527031040 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.345173716545105, loss=2.9839491844177246
I0203 06:38:43.082512 139702543816448 logging_writer.py:48] [136500] global_step=136500, grad_norm=1.6451613903045654, loss=2.3602137565612793
I0203 06:39:29.309388 139702527031040 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.704638123512268, loss=2.204071521759033
I0203 06:40:15.749894 139702543816448 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.6113265752792358, loss=2.377821207046509
I0203 06:41:01.884824 139702527031040 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.649950385093689, loss=2.906993865966797
I0203 06:41:47.911838 139702543816448 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.6651026010513306, loss=2.3379812240600586
I0203 06:42:34.149342 139702527031040 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.8905079364776611, loss=2.294901132583618
I0203 06:43:20.325570 139702543816448 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.5540847778320312, loss=2.635953903198242
I0203 06:44:06.439275 139702527031040 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.669625997543335, loss=2.462831735610962
I0203 06:44:31.794569 139863983413056 spec.py:321] Evaluating on the training split.
I0203 06:44:42.076221 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 06:45:08.368871 139863983413056 spec.py:349] Evaluating on the test split.
I0203 06:45:10.009282 139863983413056 submission_runner.py:408] Time since start: 68701.21s, 	Step: 137257, 	{'train/accuracy': 0.6786132454872131, 'train/loss': 1.3735076189041138, 'validation/accuracy': 0.6330199837684631, 'validation/loss': 1.5894250869750977, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.2336137294769287, 'test/num_examples': 10000, 'score': 62635.25749588013, 'total_duration': 68701.20639562607, 'accumulated_submission_time': 62635.25749588013, 'accumulated_eval_time': 6051.310604095459, 'accumulated_logging_time': 7.360164165496826}
I0203 06:45:10.045277 139702543816448 logging_writer.py:48] [137257] accumulated_eval_time=6051.310604, accumulated_logging_time=7.360164, accumulated_submission_time=62635.257496, global_step=137257, preemption_count=0, score=62635.257496, test/accuracy=0.509600, test/loss=2.233614, test/num_examples=10000, total_duration=68701.206396, train/accuracy=0.678613, train/loss=1.373508, validation/accuracy=0.633020, validation/loss=1.589425, validation/num_examples=50000
I0203 06:45:27.626980 139702527031040 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.4618104696273804, loss=4.792290210723877
I0203 06:46:11.557996 139702543816448 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.375975251197815, loss=3.7932214736938477
I0203 06:46:57.831987 139702527031040 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.796449899673462, loss=2.2764241695404053
I0203 06:47:43.950223 139702543816448 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.6126548051834106, loss=2.586609125137329
I0203 06:48:30.792395 139702527031040 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.4396711587905884, loss=3.6672468185424805
I0203 06:49:16.966082 139702543816448 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.4970855712890625, loss=2.691538095474243
I0203 06:50:03.732392 139702527031040 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.5993216037750244, loss=2.6997601985931396
I0203 06:50:50.157661 139702543816448 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.7785942554473877, loss=2.4774837493896484
I0203 06:51:36.179258 139702527031040 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.5473989248275757, loss=3.332719326019287
I0203 06:52:10.103014 139863983413056 spec.py:321] Evaluating on the training split.
I0203 06:52:20.431352 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 06:52:53.060180 139863983413056 spec.py:349] Evaluating on the test split.
I0203 06:52:54.695477 139863983413056 submission_runner.py:408] Time since start: 69165.89s, 	Step: 138175, 	{'train/accuracy': 0.6922070384025574, 'train/loss': 1.2771286964416504, 'validation/accuracy': 0.6366400122642517, 'validation/loss': 1.529810905456543, 'validation/num_examples': 50000, 'test/accuracy': 0.5169000029563904, 'test/loss': 2.163562297821045, 'test/num_examples': 10000, 'score': 63055.25727057457, 'total_duration': 69165.89258766174, 'accumulated_submission_time': 63055.25727057457, 'accumulated_eval_time': 6095.903052806854, 'accumulated_logging_time': 7.405869483947754}
I0203 06:52:54.735129 139702543816448 logging_writer.py:48] [138175] accumulated_eval_time=6095.903053, accumulated_logging_time=7.405869, accumulated_submission_time=63055.257271, global_step=138175, preemption_count=0, score=63055.257271, test/accuracy=0.516900, test/loss=2.163562, test/num_examples=10000, total_duration=69165.892588, train/accuracy=0.692207, train/loss=1.277129, validation/accuracy=0.636640, validation/loss=1.529811, validation/num_examples=50000
I0203 06:53:05.115324 139702527031040 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.674856424331665, loss=2.2820968627929688
I0203 06:53:48.276314 139702543816448 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.6701580286026, loss=2.334129571914673
I0203 06:54:34.168362 139702527031040 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.5098673105239868, loss=2.8256454467773438
I0203 06:55:20.431509 139702543816448 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.3662283420562744, loss=3.25968074798584
I0203 06:56:06.626092 139702527031040 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.8163020610809326, loss=2.2410049438476562
I0203 06:56:52.677883 139702543816448 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.4830213785171509, loss=4.571552276611328
I0203 06:57:38.888032 139702527031040 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.7018299102783203, loss=2.3367223739624023
I0203 06:58:25.503749 139702543816448 logging_writer.py:48] [138900] global_step=138900, grad_norm=1.431429386138916, loss=4.969667434692383
I0203 06:59:11.801482 139702527031040 logging_writer.py:48] [139000] global_step=139000, grad_norm=1.505975604057312, loss=4.895138740539551
I0203 06:59:54.732891 139863983413056 spec.py:321] Evaluating on the training split.
I0203 07:00:04.906085 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 07:00:32.998141 139863983413056 spec.py:349] Evaluating on the test split.
I0203 07:00:34.644528 139863983413056 submission_runner.py:408] Time since start: 69625.84s, 	Step: 139094, 	{'train/accuracy': 0.6813281178474426, 'train/loss': 1.3332164287567139, 'validation/accuracy': 0.6382799744606018, 'validation/loss': 1.5517253875732422, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.2069404125213623, 'test/num_examples': 10000, 'score': 63475.19632101059, 'total_duration': 69625.84164237976, 'accumulated_submission_time': 63475.19632101059, 'accumulated_eval_time': 6135.814694881439, 'accumulated_logging_time': 7.4559032917022705}
I0203 07:00:34.688915 139702543816448 logging_writer.py:48] [139094] accumulated_eval_time=6135.814695, accumulated_logging_time=7.455903, accumulated_submission_time=63475.196321, global_step=139094, preemption_count=0, score=63475.196321, test/accuracy=0.510900, test/loss=2.206940, test/num_examples=10000, total_duration=69625.841642, train/accuracy=0.681328, train/loss=1.333216, validation/accuracy=0.638280, validation/loss=1.551725, validation/num_examples=50000
I0203 07:00:37.493921 139702527031040 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.7778455018997192, loss=2.2130792140960693
I0203 07:01:19.591393 139702543816448 logging_writer.py:48] [139200] global_step=139200, grad_norm=1.7332146167755127, loss=2.2243738174438477
I0203 07:02:05.535115 139702527031040 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.8270589113235474, loss=2.3111889362335205
I0203 07:02:51.705412 139702543816448 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.9100978374481201, loss=2.3068082332611084
I0203 07:03:37.842748 139702527031040 logging_writer.py:48] [139500] global_step=139500, grad_norm=1.7703044414520264, loss=2.1357853412628174
I0203 07:04:24.065608 139702543816448 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.7784874439239502, loss=2.238154649734497
I0203 07:05:10.137167 139702527031040 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.696774959564209, loss=2.263127565383911
I0203 07:05:56.171876 139702543816448 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.4141535758972168, loss=4.467403411865234
I0203 07:06:42.280467 139702527031040 logging_writer.py:48] [139900] global_step=139900, grad_norm=1.3569177389144897, loss=3.6722447872161865
I0203 07:07:28.480996 139702543816448 logging_writer.py:48] [140000] global_step=140000, grad_norm=1.7793898582458496, loss=2.3209807872772217
I0203 07:07:35.068958 139863983413056 spec.py:321] Evaluating on the training split.
I0203 07:07:45.718192 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 07:08:09.854080 139863983413056 spec.py:349] Evaluating on the test split.
I0203 07:08:11.506122 139863983413056 submission_runner.py:408] Time since start: 70082.70s, 	Step: 140016, 	{'train/accuracy': 0.6907812356948853, 'train/loss': 1.2872713804244995, 'validation/accuracy': 0.6406999826431274, 'validation/loss': 1.5119783878326416, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.150461196899414, 'test/num_examples': 10000, 'score': 63895.5168299675, 'total_duration': 70082.7032134533, 'accumulated_submission_time': 63895.5168299675, 'accumulated_eval_time': 6172.251842260361, 'accumulated_logging_time': 7.511298418045044}
I0203 07:08:11.552301 139702527031040 logging_writer.py:48] [140016] accumulated_eval_time=6172.251842, accumulated_logging_time=7.511298, accumulated_submission_time=63895.516830, global_step=140016, preemption_count=0, score=63895.516830, test/accuracy=0.518800, test/loss=2.150461, test/num_examples=10000, total_duration=70082.703213, train/accuracy=0.690781, train/loss=1.287271, validation/accuracy=0.640700, validation/loss=1.511978, validation/num_examples=50000
I0203 07:08:46.129951 139702543816448 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.4350128173828125, loss=3.7637686729431152
I0203 07:09:32.097060 139702527031040 logging_writer.py:48] [140200] global_step=140200, grad_norm=1.7383301258087158, loss=2.444916248321533
I0203 07:10:18.363418 139702543816448 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.6671783924102783, loss=2.36000394821167
I0203 07:11:04.581971 139702527031040 logging_writer.py:48] [140400] global_step=140400, grad_norm=1.4751750230789185, loss=4.7317047119140625
I0203 07:11:50.612204 139702543816448 logging_writer.py:48] [140500] global_step=140500, grad_norm=1.7780735492706299, loss=2.5125980377197266
I0203 07:12:36.708455 139702527031040 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.435036063194275, loss=3.8683922290802
I0203 07:13:22.725983 139702543816448 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.4312571287155151, loss=4.775051116943359
I0203 07:14:09.023988 139702527031040 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.6831449270248413, loss=2.4271976947784424
I0203 07:14:54.988917 139702543816448 logging_writer.py:48] [140900] global_step=140900, grad_norm=1.6642067432403564, loss=2.284000873565674
I0203 07:15:11.790796 139863983413056 spec.py:321] Evaluating on the training split.
I0203 07:15:22.102807 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 07:15:52.110640 139863983413056 spec.py:349] Evaluating on the test split.
I0203 07:15:53.744394 139863983413056 submission_runner.py:408] Time since start: 70544.94s, 	Step: 140938, 	{'train/accuracy': 0.6983007788658142, 'train/loss': 1.2475330829620361, 'validation/accuracy': 0.6419399976730347, 'validation/loss': 1.5060182809829712, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.1491525173187256, 'test/num_examples': 10000, 'score': 64315.69512438774, 'total_duration': 70544.94150233269, 'accumulated_submission_time': 64315.69512438774, 'accumulated_eval_time': 6214.205435991287, 'accumulated_logging_time': 7.5685484409332275}
I0203 07:15:53.780935 139702527031040 logging_writer.py:48] [140938] accumulated_eval_time=6214.205436, accumulated_logging_time=7.568548, accumulated_submission_time=64315.695124, global_step=140938, preemption_count=0, score=64315.695124, test/accuracy=0.518800, test/loss=2.149153, test/num_examples=10000, total_duration=70544.941502, train/accuracy=0.698301, train/loss=1.247533, validation/accuracy=0.641940, validation/loss=1.506018, validation/num_examples=50000
I0203 07:16:18.932762 139702543816448 logging_writer.py:48] [141000] global_step=141000, grad_norm=1.7132865190505981, loss=2.324606418609619
I0203 07:17:03.867340 139702527031040 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.8060263395309448, loss=2.4160921573638916
I0203 07:17:49.970888 139702543816448 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.5308938026428223, loss=2.8516783714294434
I0203 07:18:36.027886 139702527031040 logging_writer.py:48] [141300] global_step=141300, grad_norm=1.6688052415847778, loss=2.2036447525024414
I0203 07:19:22.335241 139702543816448 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.505849003791809, loss=4.821298599243164
I0203 07:20:08.377318 139702527031040 logging_writer.py:48] [141500] global_step=141500, grad_norm=1.7759145498275757, loss=2.2569284439086914
I0203 07:20:54.502962 139702543816448 logging_writer.py:48] [141600] global_step=141600, grad_norm=1.6187223196029663, loss=2.189208984375
I0203 07:21:40.518463 139702527031040 logging_writer.py:48] [141700] global_step=141700, grad_norm=1.8471477031707764, loss=2.406740427017212
I0203 07:22:26.409193 139702543816448 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.698330044746399, loss=2.253276824951172
I0203 07:22:54.034071 139863983413056 spec.py:321] Evaluating on the training split.
I0203 07:23:04.531364 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 07:23:33.702678 139863983413056 spec.py:349] Evaluating on the test split.
I0203 07:23:35.345529 139863983413056 submission_runner.py:408] Time since start: 71006.54s, 	Step: 141862, 	{'train/accuracy': 0.7147851586341858, 'train/loss': 1.200818657875061, 'validation/accuracy': 0.6396399736404419, 'validation/loss': 1.5271742343902588, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.164371967315674, 'test/num_examples': 10000, 'score': 64735.889827251434, 'total_duration': 71006.54262113571, 'accumulated_submission_time': 64735.889827251434, 'accumulated_eval_time': 6255.516871213913, 'accumulated_logging_time': 7.614865064620972}
I0203 07:23:35.389600 139702527031040 logging_writer.py:48] [141862] accumulated_eval_time=6255.516871, accumulated_logging_time=7.614865, accumulated_submission_time=64735.889827, global_step=141862, preemption_count=0, score=64735.889827, test/accuracy=0.516100, test/loss=2.164372, test/num_examples=10000, total_duration=71006.542621, train/accuracy=0.714785, train/loss=1.200819, validation/accuracy=0.639640, validation/loss=1.527174, validation/num_examples=50000
I0203 07:23:50.959090 139702543816448 logging_writer.py:48] [141900] global_step=141900, grad_norm=1.7040802240371704, loss=2.236300230026245
I0203 07:24:34.553299 139702527031040 logging_writer.py:48] [142000] global_step=142000, grad_norm=1.8424272537231445, loss=2.25300669670105
I0203 07:25:20.806195 139702543816448 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.46858549118042, loss=3.2814197540283203
I0203 07:26:06.879249 139702527031040 logging_writer.py:48] [142200] global_step=142200, grad_norm=1.6208391189575195, loss=4.822530269622803
I0203 07:26:52.850711 139702543816448 logging_writer.py:48] [142300] global_step=142300, grad_norm=1.636983036994934, loss=2.603827953338623
I0203 07:27:38.937999 139702527031040 logging_writer.py:48] [142400] global_step=142400, grad_norm=1.8341151475906372, loss=2.1777420043945312
I0203 07:28:24.958849 139702543816448 logging_writer.py:48] [142500] global_step=142500, grad_norm=1.5386220216751099, loss=4.204163074493408
I0203 07:29:10.958381 139702527031040 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.2618231773376465, loss=4.6417694091796875
I0203 07:29:57.703229 139702543816448 logging_writer.py:48] [142700] global_step=142700, grad_norm=1.7181918621063232, loss=2.6886157989501953
I0203 07:30:35.750182 139863983413056 spec.py:321] Evaluating on the training split.
I0203 07:30:46.424420 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 07:31:12.152524 139863983413056 spec.py:349] Evaluating on the test split.
I0203 07:31:13.788885 139863983413056 submission_runner.py:408] Time since start: 71464.99s, 	Step: 142784, 	{'train/accuracy': 0.6896093487739563, 'train/loss': 1.3375908136367798, 'validation/accuracy': 0.6431199908256531, 'validation/loss': 1.5491735935211182, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.190412759780884, 'test/num_examples': 10000, 'score': 65156.19044685364, 'total_duration': 71464.98599791527, 'accumulated_submission_time': 65156.19044685364, 'accumulated_eval_time': 6293.555589437485, 'accumulated_logging_time': 7.669762372970581}
I0203 07:31:13.830088 139702527031040 logging_writer.py:48] [142784] accumulated_eval_time=6293.555589, accumulated_logging_time=7.669762, accumulated_submission_time=65156.190447, global_step=142784, preemption_count=0, score=65156.190447, test/accuracy=0.519700, test/loss=2.190413, test/num_examples=10000, total_duration=71464.985998, train/accuracy=0.689609, train/loss=1.337591, validation/accuracy=0.643120, validation/loss=1.549174, validation/num_examples=50000
I0203 07:31:20.928487 139702543816448 logging_writer.py:48] [142800] global_step=142800, grad_norm=1.4600064754486084, loss=4.692450523376465
I0203 07:32:03.285172 139702527031040 logging_writer.py:48] [142900] global_step=142900, grad_norm=1.6297035217285156, loss=2.9310927391052246
I0203 07:32:49.349097 139702543816448 logging_writer.py:48] [143000] global_step=143000, grad_norm=1.730804204940796, loss=2.6612963676452637
I0203 07:33:35.696487 139702527031040 logging_writer.py:48] [143100] global_step=143100, grad_norm=1.5378196239471436, loss=4.603425979614258
I0203 07:34:21.740065 139702543816448 logging_writer.py:48] [143200] global_step=143200, grad_norm=1.6068733930587769, loss=3.3430964946746826
I0203 07:35:08.112617 139702527031040 logging_writer.py:48] [143300] global_step=143300, grad_norm=1.6712958812713623, loss=4.0183281898498535
I0203 07:35:53.960024 139702543816448 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.7207540273666382, loss=2.337319850921631
I0203 07:36:40.127754 139702527031040 logging_writer.py:48] [143500] global_step=143500, grad_norm=1.8910480737686157, loss=2.209674596786499
I0203 07:37:26.282421 139702543816448 logging_writer.py:48] [143600] global_step=143600, grad_norm=1.9694764614105225, loss=2.163940668106079
I0203 07:38:12.270274 139702527031040 logging_writer.py:48] [143700] global_step=143700, grad_norm=1.8469675779342651, loss=2.264937400817871
I0203 07:38:13.907582 139863983413056 spec.py:321] Evaluating on the training split.
I0203 07:38:24.523815 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 07:38:56.773307 139863983413056 spec.py:349] Evaluating on the test split.
I0203 07:38:58.406203 139863983413056 submission_runner.py:408] Time since start: 71929.60s, 	Step: 143705, 	{'train/accuracy': 0.7058203220367432, 'train/loss': 1.2415093183517456, 'validation/accuracy': 0.6496599912643433, 'validation/loss': 1.489941954612732, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.1392199993133545, 'test/num_examples': 10000, 'score': 65576.20358800888, 'total_duration': 71929.60329174995, 'accumulated_submission_time': 65576.20358800888, 'accumulated_eval_time': 6338.054198503494, 'accumulated_logging_time': 7.726184368133545}
I0203 07:38:58.450340 139702543816448 logging_writer.py:48] [143705] accumulated_eval_time=6338.054199, accumulated_logging_time=7.726184, accumulated_submission_time=65576.203588, global_step=143705, preemption_count=0, score=65576.203588, test/accuracy=0.525600, test/loss=2.139220, test/num_examples=10000, total_duration=71929.603292, train/accuracy=0.705820, train/loss=1.241509, validation/accuracy=0.649660, validation/loss=1.489942, validation/num_examples=50000
I0203 07:39:38.274209 139702527031040 logging_writer.py:48] [143800] global_step=143800, grad_norm=1.6304099559783936, loss=4.751976013183594
I0203 07:40:24.544912 139702543816448 logging_writer.py:48] [143900] global_step=143900, grad_norm=1.5683908462524414, loss=3.4752721786499023
I0203 07:41:10.908469 139702527031040 logging_writer.py:48] [144000] global_step=144000, grad_norm=1.7342826128005981, loss=4.740481376647949
I0203 07:41:57.213051 139702543816448 logging_writer.py:48] [144100] global_step=144100, grad_norm=1.814717173576355, loss=2.1127567291259766
I0203 07:42:43.433866 139702527031040 logging_writer.py:48] [144200] global_step=144200, grad_norm=1.6562753915786743, loss=3.183938980102539
I0203 07:43:29.823952 139702543816448 logging_writer.py:48] [144300] global_step=144300, grad_norm=1.7991080284118652, loss=4.816430568695068
I0203 07:44:16.086343 139702527031040 logging_writer.py:48] [144400] global_step=144400, grad_norm=1.7784650325775146, loss=2.804551124572754
I0203 07:45:02.161942 139702543816448 logging_writer.py:48] [144500] global_step=144500, grad_norm=1.5499873161315918, loss=3.8231050968170166
I0203 07:45:48.589766 139702527031040 logging_writer.py:48] [144600] global_step=144600, grad_norm=1.6529799699783325, loss=2.7900171279907227
I0203 07:45:58.853584 139863983413056 spec.py:321] Evaluating on the training split.
I0203 07:46:09.270317 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 07:46:40.840849 139863983413056 spec.py:349] Evaluating on the test split.
I0203 07:46:42.486881 139863983413056 submission_runner.py:408] Time since start: 72393.68s, 	Step: 144624, 	{'train/accuracy': 0.7205273509025574, 'train/loss': 1.1597256660461426, 'validation/accuracy': 0.6532999873161316, 'validation/loss': 1.449857473373413, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.090836763381958, 'test/num_examples': 10000, 'score': 65996.54626846313, 'total_duration': 72393.68397283554, 'accumulated_submission_time': 65996.54626846313, 'accumulated_eval_time': 6381.687472581863, 'accumulated_logging_time': 7.781303644180298}
I0203 07:46:42.529949 139702543816448 logging_writer.py:48] [144624] accumulated_eval_time=6381.687473, accumulated_logging_time=7.781304, accumulated_submission_time=65996.546268, global_step=144624, preemption_count=0, score=65996.546268, test/accuracy=0.531500, test/loss=2.090837, test/num_examples=10000, total_duration=72393.683973, train/accuracy=0.720527, train/loss=1.159726, validation/accuracy=0.653300, validation/loss=1.449857, validation/num_examples=50000
I0203 07:47:13.548810 139702527031040 logging_writer.py:48] [144700] global_step=144700, grad_norm=1.4933091402053833, loss=3.214747667312622
I0203 07:47:59.568409 139702543816448 logging_writer.py:48] [144800] global_step=144800, grad_norm=1.8551206588745117, loss=2.62984561920166
I0203 07:48:45.878798 139702527031040 logging_writer.py:48] [144900] global_step=144900, grad_norm=1.8965250253677368, loss=2.314713716506958
I0203 07:49:32.334066 139702543816448 logging_writer.py:48] [145000] global_step=145000, grad_norm=1.8691270351409912, loss=2.938875198364258
I0203 07:50:18.820845 139702527031040 logging_writer.py:48] [145100] global_step=145100, grad_norm=1.8188726902008057, loss=2.404041290283203
I0203 07:51:05.242879 139702543816448 logging_writer.py:48] [145200] global_step=145200, grad_norm=1.9359869956970215, loss=2.1566359996795654
I0203 07:51:51.410191 139702527031040 logging_writer.py:48] [145300] global_step=145300, grad_norm=1.584027886390686, loss=4.671821594238281
I0203 07:52:37.705551 139702543816448 logging_writer.py:48] [145400] global_step=145400, grad_norm=1.8850241899490356, loss=2.2533512115478516
I0203 07:53:23.903791 139702527031040 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.8050830364227295, loss=2.520475149154663
I0203 07:53:42.809678 139863983413056 spec.py:321] Evaluating on the training split.
I0203 07:53:53.215056 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 07:54:24.562460 139863983413056 spec.py:349] Evaluating on the test split.
I0203 07:54:26.209040 139863983413056 submission_runner.py:408] Time since start: 72857.41s, 	Step: 145543, 	{'train/accuracy': 0.7084179520606995, 'train/loss': 1.2068352699279785, 'validation/accuracy': 0.6576600074768066, 'validation/loss': 1.4416378736495972, 'validation/num_examples': 50000, 'test/accuracy': 0.5302000045776367, 'test/loss': 2.0874135494232178, 'test/num_examples': 10000, 'score': 66416.76606321335, 'total_duration': 72857.4061486721, 'accumulated_submission_time': 66416.76606321335, 'accumulated_eval_time': 6425.086839675903, 'accumulated_logging_time': 7.835384845733643}
I0203 07:54:26.247537 139702543816448 logging_writer.py:48] [145543] accumulated_eval_time=6425.086840, accumulated_logging_time=7.835385, accumulated_submission_time=66416.766063, global_step=145543, preemption_count=0, score=66416.766063, test/accuracy=0.530200, test/loss=2.087414, test/num_examples=10000, total_duration=72857.406149, train/accuracy=0.708418, train/loss=1.206835, validation/accuracy=0.657660, validation/loss=1.441638, validation/num_examples=50000
I0203 07:54:49.424844 139702527031040 logging_writer.py:48] [145600] global_step=145600, grad_norm=1.7245138883590698, loss=3.2215301990509033
I0203 07:55:34.371861 139702543816448 logging_writer.py:48] [145700] global_step=145700, grad_norm=1.8601495027542114, loss=2.1345407962799072
I0203 07:56:20.578936 139702527031040 logging_writer.py:48] [145800] global_step=145800, grad_norm=1.8716315031051636, loss=2.309985399246216
I0203 07:57:06.629186 139702543816448 logging_writer.py:48] [145900] global_step=145900, grad_norm=1.8006101846694946, loss=4.70320463180542
I0203 07:57:52.603816 139702527031040 logging_writer.py:48] [146000] global_step=146000, grad_norm=1.9374430179595947, loss=2.1430695056915283
I0203 07:58:38.584874 139702543816448 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.0620615482330322, loss=2.173116445541382
I0203 07:59:24.814249 139702527031040 logging_writer.py:48] [146200] global_step=146200, grad_norm=1.9416399002075195, loss=2.169888496398926
I0203 08:00:10.989745 139702543816448 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.9157803058624268, loss=2.0337204933166504
I0203 08:00:57.496901 139702527031040 logging_writer.py:48] [146400] global_step=146400, grad_norm=1.56281316280365, loss=3.1362392902374268
I0203 08:01:26.360377 139863983413056 spec.py:321] Evaluating on the training split.
I0203 08:01:36.798802 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 08:02:05.532385 139863983413056 spec.py:349] Evaluating on the test split.
I0203 08:02:07.167218 139863983413056 submission_runner.py:408] Time since start: 73318.36s, 	Step: 146464, 	{'train/accuracy': 0.7099999785423279, 'train/loss': 1.185759425163269, 'validation/accuracy': 0.6562199592590332, 'validation/loss': 1.433472990989685, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.0838398933410645, 'test/num_examples': 10000, 'score': 66836.82033586502, 'total_duration': 73318.36433267593, 'accumulated_submission_time': 66836.82033586502, 'accumulated_eval_time': 6465.893684387207, 'accumulated_logging_time': 7.883531093597412}
I0203 08:02:07.206305 139702543816448 logging_writer.py:48] [146464] accumulated_eval_time=6465.893684, accumulated_logging_time=7.883531, accumulated_submission_time=66836.820336, global_step=146464, preemption_count=0, score=66836.820336, test/accuracy=0.534900, test/loss=2.083840, test/num_examples=10000, total_duration=73318.364333, train/accuracy=0.710000, train/loss=1.185759, validation/accuracy=0.656220, validation/loss=1.433473, validation/num_examples=50000
I0203 08:02:21.985783 139702527031040 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.121213436126709, loss=4.777701377868652
I0203 08:03:05.720142 139702543816448 logging_writer.py:48] [146600] global_step=146600, grad_norm=1.8368000984191895, loss=2.092982530593872
I0203 08:03:51.683127 139702527031040 logging_writer.py:48] [146700] global_step=146700, grad_norm=1.9994080066680908, loss=2.215195655822754
I0203 08:04:37.735332 139702543816448 logging_writer.py:48] [146800] global_step=146800, grad_norm=1.8589320182800293, loss=4.697195529937744
I0203 08:05:23.605254 139702527031040 logging_writer.py:48] [146900] global_step=146900, grad_norm=1.7928946018218994, loss=4.111971855163574
I0203 08:06:09.694349 139702543816448 logging_writer.py:48] [147000] global_step=147000, grad_norm=1.6425610780715942, loss=2.9643375873565674
I0203 08:06:55.740677 139702527031040 logging_writer.py:48] [147100] global_step=147100, grad_norm=1.6368598937988281, loss=2.9491653442382812
I0203 08:07:41.730203 139702543816448 logging_writer.py:48] [147200] global_step=147200, grad_norm=1.8876031637191772, loss=2.1168806552886963
I0203 08:08:27.712584 139702527031040 logging_writer.py:48] [147300] global_step=147300, grad_norm=1.5939459800720215, loss=4.1131391525268555
I0203 08:09:07.670794 139863983413056 spec.py:321] Evaluating on the training split.
I0203 08:09:17.850150 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 08:09:45.510581 139863983413056 spec.py:349] Evaluating on the test split.
I0203 08:09:47.158509 139863983413056 submission_runner.py:408] Time since start: 73778.36s, 	Step: 147388, 	{'train/accuracy': 0.71875, 'train/loss': 1.1505794525146484, 'validation/accuracy': 0.6584999561309814, 'validation/loss': 1.419907569885254, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.0673604011535645, 'test/num_examples': 10000, 'score': 67257.22652721405, 'total_duration': 73778.35560202599, 'accumulated_submission_time': 67257.22652721405, 'accumulated_eval_time': 6505.38139629364, 'accumulated_logging_time': 7.932077884674072}
I0203 08:09:47.203470 139702543816448 logging_writer.py:48] [147388] accumulated_eval_time=6505.381396, accumulated_logging_time=7.932078, accumulated_submission_time=67257.226527, global_step=147388, preemption_count=0, score=67257.226527, test/accuracy=0.533300, test/loss=2.067360, test/num_examples=10000, total_duration=73778.355602, train/accuracy=0.718750, train/loss=1.150579, validation/accuracy=0.658500, validation/loss=1.419908, validation/num_examples=50000
I0203 08:09:52.392474 139702527031040 logging_writer.py:48] [147400] global_step=147400, grad_norm=1.7380205392837524, loss=2.510403871536255
I0203 08:10:34.661796 139702543816448 logging_writer.py:48] [147500] global_step=147500, grad_norm=1.7819862365722656, loss=4.152867317199707
I0203 08:11:20.521867 139702527031040 logging_writer.py:48] [147600] global_step=147600, grad_norm=1.7090909481048584, loss=4.399948596954346
I0203 08:12:07.068618 139702543816448 logging_writer.py:48] [147700] global_step=147700, grad_norm=1.682718276977539, loss=3.8755710124969482
I0203 08:12:52.906707 139702527031040 logging_writer.py:48] [147800] global_step=147800, grad_norm=1.7341665029525757, loss=4.524858474731445
I0203 08:13:39.193576 139702543816448 logging_writer.py:48] [147900] global_step=147900, grad_norm=1.9575806856155396, loss=2.0959036350250244
I0203 08:14:25.295031 139702527031040 logging_writer.py:48] [148000] global_step=148000, grad_norm=1.877292275428772, loss=2.174593448638916
I0203 08:15:11.255974 139702543816448 logging_writer.py:48] [148100] global_step=148100, grad_norm=1.7298539876937866, loss=4.335241317749023
I0203 08:15:57.173320 139702527031040 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.0423004627227783, loss=2.215099573135376
I0203 08:16:43.111848 139702543816448 logging_writer.py:48] [148300] global_step=148300, grad_norm=1.8668240308761597, loss=2.075892686843872
I0203 08:16:47.424561 139863983413056 spec.py:321] Evaluating on the training split.
I0203 08:16:57.726736 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 08:17:27.593759 139863983413056 spec.py:349] Evaluating on the test split.
I0203 08:17:29.229311 139863983413056 submission_runner.py:408] Time since start: 74240.43s, 	Step: 148311, 	{'train/accuracy': 0.7202538847923279, 'train/loss': 1.1635055541992188, 'validation/accuracy': 0.6650800108909607, 'validation/loss': 1.4090675115585327, 'validation/num_examples': 50000, 'test/accuracy': 0.5368000268936157, 'test/loss': 2.0515666007995605, 'test/num_examples': 10000, 'score': 67677.38717126846, 'total_duration': 74240.42641305923, 'accumulated_submission_time': 67677.38717126846, 'accumulated_eval_time': 6547.186131954193, 'accumulated_logging_time': 7.988610029220581}
I0203 08:17:29.268757 139702527031040 logging_writer.py:48] [148311] accumulated_eval_time=6547.186132, accumulated_logging_time=7.988610, accumulated_submission_time=67677.387171, global_step=148311, preemption_count=0, score=67677.387171, test/accuracy=0.536800, test/loss=2.051567, test/num_examples=10000, total_duration=74240.426413, train/accuracy=0.720254, train/loss=1.163506, validation/accuracy=0.665080, validation/loss=1.409068, validation/num_examples=50000
I0203 08:18:05.991949 139702543816448 logging_writer.py:48] [148400] global_step=148400, grad_norm=1.840282917022705, loss=2.1869170665740967
I0203 08:18:51.834589 139702527031040 logging_writer.py:48] [148500] global_step=148500, grad_norm=1.7906020879745483, loss=1.9578217267990112
I0203 08:19:37.982230 139702543816448 logging_writer.py:48] [148600] global_step=148600, grad_norm=1.685890793800354, loss=3.9679367542266846
I0203 08:20:24.248344 139702527031040 logging_writer.py:48] [148700] global_step=148700, grad_norm=1.7338435649871826, loss=3.790128231048584
I0203 08:21:10.465144 139702543816448 logging_writer.py:48] [148800] global_step=148800, grad_norm=1.7473398447036743, loss=3.504351854324341
I0203 08:21:56.679717 139702527031040 logging_writer.py:48] [148900] global_step=148900, grad_norm=1.826194167137146, loss=3.237642765045166
I0203 08:22:42.773098 139702543816448 logging_writer.py:48] [149000] global_step=149000, grad_norm=1.7536938190460205, loss=4.266993045806885
I0203 08:23:28.923746 139702527031040 logging_writer.py:48] [149100] global_step=149100, grad_norm=1.9888453483581543, loss=2.135429620742798
I0203 08:24:15.148740 139702543816448 logging_writer.py:48] [149200] global_step=149200, grad_norm=1.8986752033233643, loss=2.564434289932251
I0203 08:24:29.477945 139863983413056 spec.py:321] Evaluating on the training split.
I0203 08:24:40.021000 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 08:25:06.631586 139863983413056 spec.py:349] Evaluating on the test split.
I0203 08:25:08.275910 139863983413056 submission_runner.py:408] Time since start: 74699.47s, 	Step: 149233, 	{'train/accuracy': 0.7199999690055847, 'train/loss': 1.1488616466522217, 'validation/accuracy': 0.6662200093269348, 'validation/loss': 1.3948432207107544, 'validation/num_examples': 50000, 'test/accuracy': 0.5412999987602234, 'test/loss': 2.040438652038574, 'test/num_examples': 10000, 'score': 68097.53750610352, 'total_duration': 74699.47302079201, 'accumulated_submission_time': 68097.53750610352, 'accumulated_eval_time': 6585.98409485817, 'accumulated_logging_time': 8.03798794746399}
I0203 08:25:08.316990 139702527031040 logging_writer.py:48] [149233] accumulated_eval_time=6585.984095, accumulated_logging_time=8.037988, accumulated_submission_time=68097.537506, global_step=149233, preemption_count=0, score=68097.537506, test/accuracy=0.541300, test/loss=2.040439, test/num_examples=10000, total_duration=74699.473021, train/accuracy=0.720000, train/loss=1.148862, validation/accuracy=0.666220, validation/loss=1.394843, validation/num_examples=50000
I0203 08:25:35.499356 139702543816448 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.0284361839294434, loss=2.091299295425415
I0203 08:26:21.011371 139702527031040 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.1228256225585938, loss=2.265566825866699
I0203 08:27:07.397850 139702543816448 logging_writer.py:48] [149500] global_step=149500, grad_norm=1.9934237003326416, loss=2.5009522438049316
I0203 08:27:53.306679 139702527031040 logging_writer.py:48] [149600] global_step=149600, grad_norm=1.8755033016204834, loss=1.9988601207733154
I0203 08:28:39.441269 139702543816448 logging_writer.py:48] [149700] global_step=149700, grad_norm=1.9558354616165161, loss=2.1548240184783936
I0203 08:29:25.556475 139702527031040 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.150662899017334, loss=2.0649988651275635
I0203 08:30:12.053305 139702543816448 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.046769142150879, loss=2.198636531829834
I0203 08:30:58.165084 139702527031040 logging_writer.py:48] [150000] global_step=150000, grad_norm=1.8530462980270386, loss=4.335969924926758
I0203 08:31:44.301210 139702543816448 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.0904476642608643, loss=2.0831613540649414
I0203 08:32:08.300494 139863983413056 spec.py:321] Evaluating on the training split.
I0203 08:32:18.720237 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 08:32:49.181052 139863983413056 spec.py:349] Evaluating on the test split.
I0203 08:32:50.821957 139863983413056 submission_runner.py:408] Time since start: 75162.02s, 	Step: 150153, 	{'train/accuracy': 0.7326562404632568, 'train/loss': 1.1124690771102905, 'validation/accuracy': 0.6711199879646301, 'validation/loss': 1.3810205459594727, 'validation/num_examples': 50000, 'test/accuracy': 0.547700047492981, 'test/loss': 2.0186476707458496, 'test/num_examples': 10000, 'score': 68517.46277451515, 'total_duration': 75162.01907277107, 'accumulated_submission_time': 68517.46277451515, 'accumulated_eval_time': 6628.50556063652, 'accumulated_logging_time': 8.088744640350342}
I0203 08:32:50.866774 139702527031040 logging_writer.py:48] [150153] accumulated_eval_time=6628.505561, accumulated_logging_time=8.088745, accumulated_submission_time=68517.462775, global_step=150153, preemption_count=0, score=68517.462775, test/accuracy=0.547700, test/loss=2.018648, test/num_examples=10000, total_duration=75162.019073, train/accuracy=0.732656, train/loss=1.112469, validation/accuracy=0.671120, validation/loss=1.381021, validation/num_examples=50000
I0203 08:33:10.028681 139702543816448 logging_writer.py:48] [150200] global_step=150200, grad_norm=1.930201768875122, loss=4.196252822875977
I0203 08:33:54.083254 139702527031040 logging_writer.py:48] [150300] global_step=150300, grad_norm=1.8287065029144287, loss=4.424140453338623
I0203 08:34:40.327287 139702543816448 logging_writer.py:48] [150400] global_step=150400, grad_norm=1.7164393663406372, loss=3.2288856506347656
I0203 08:35:26.627183 139702527031040 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.032815933227539, loss=2.0363121032714844
I0203 08:36:12.526047 139702543816448 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.0117456912994385, loss=2.7598800659179688
I0203 08:36:58.679839 139702527031040 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.023951292037964, loss=2.379939556121826
I0203 08:37:44.668980 139702543816448 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.050131320953369, loss=2.077021598815918
I0203 08:38:30.845397 139702527031040 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.1149308681488037, loss=2.1608877182006836
I0203 08:39:17.080587 139702543816448 logging_writer.py:48] [151000] global_step=151000, grad_norm=1.797668218612671, loss=2.7832510471343994
I0203 08:39:50.901960 139863983413056 spec.py:321] Evaluating on the training split.
I0203 08:40:01.261754 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 08:40:28.139981 139863983413056 spec.py:349] Evaluating on the test split.
I0203 08:40:29.783164 139863983413056 submission_runner.py:408] Time since start: 75620.98s, 	Step: 151075, 	{'train/accuracy': 0.7326562404632568, 'train/loss': 1.1057353019714355, 'validation/accuracy': 0.673039972782135, 'validation/loss': 1.3807213306427002, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.029315233230591, 'test/num_examples': 10000, 'score': 68937.43747639656, 'total_duration': 75620.98027873039, 'accumulated_submission_time': 68937.43747639656, 'accumulated_eval_time': 6667.386779785156, 'accumulated_logging_time': 8.145340204238892}
I0203 08:40:29.823300 139702527031040 logging_writer.py:48] [151075] accumulated_eval_time=6667.386780, accumulated_logging_time=8.145340, accumulated_submission_time=68937.437476, global_step=151075, preemption_count=0, score=68937.437476, test/accuracy=0.546100, test/loss=2.029315, test/num_examples=10000, total_duration=75620.980279, train/accuracy=0.732656, train/loss=1.105735, validation/accuracy=0.673040, validation/loss=1.380721, validation/num_examples=50000
I0203 08:40:40.194466 139702543816448 logging_writer.py:48] [151100] global_step=151100, grad_norm=1.8377656936645508, loss=2.6251115798950195
I0203 08:41:23.248150 139702527031040 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.263969898223877, loss=2.0084125995635986
I0203 08:42:09.506437 139702543816448 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.139185667037964, loss=2.037250518798828
I0203 08:42:55.480742 139702527031040 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.1553232669830322, loss=2.2302489280700684
I0203 08:43:41.350687 139702543816448 logging_writer.py:48] [151500] global_step=151500, grad_norm=1.913954496383667, loss=4.226973056793213
I0203 08:44:27.288735 139702527031040 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.0902352333068848, loss=2.0913400650024414
I0203 08:45:13.263256 139702543816448 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.1829583644866943, loss=2.2101221084594727
I0203 08:45:59.207044 139702527031040 logging_writer.py:48] [151800] global_step=151800, grad_norm=1.8686773777008057, loss=3.6289634704589844
I0203 08:46:45.368766 139702543816448 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.140561819076538, loss=4.295640468597412
I0203 08:47:30.119360 139863983413056 spec.py:321] Evaluating on the training split.
I0203 08:47:40.476007 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 08:48:10.452219 139863983413056 spec.py:349] Evaluating on the test split.
I0203 08:48:12.084534 139863983413056 submission_runner.py:408] Time since start: 76083.28s, 	Step: 151999, 	{'train/accuracy': 0.7305663824081421, 'train/loss': 1.1032029390335083, 'validation/accuracy': 0.6752399802207947, 'validation/loss': 1.345995306968689, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 1.9906830787658691, 'test/num_examples': 10000, 'score': 69357.67158484459, 'total_duration': 76083.28164362907, 'accumulated_submission_time': 69357.67158484459, 'accumulated_eval_time': 6709.3519904613495, 'accumulated_logging_time': 8.198824405670166}
I0203 08:48:12.128426 139702527031040 logging_writer.py:48] [151999] accumulated_eval_time=6709.351990, accumulated_logging_time=8.198824, accumulated_submission_time=69357.671585, global_step=151999, preemption_count=0, score=69357.671585, test/accuracy=0.547100, test/loss=1.990683, test/num_examples=10000, total_duration=76083.281644, train/accuracy=0.730566, train/loss=1.103203, validation/accuracy=0.675240, validation/loss=1.345995, validation/num_examples=50000
I0203 08:48:12.938554 139702543816448 logging_writer.py:48] [152000] global_step=152000, grad_norm=1.8529008626937866, loss=3.064962387084961
I0203 08:48:54.354752 139702527031040 logging_writer.py:48] [152100] global_step=152100, grad_norm=1.9740015268325806, loss=2.0050506591796875
I0203 08:49:40.249441 139702543816448 logging_writer.py:48] [152200] global_step=152200, grad_norm=1.764684796333313, loss=3.148847818374634
I0203 08:50:26.805010 139702527031040 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.0409605503082275, loss=4.201487064361572
I0203 08:51:12.778413 139702543816448 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.142340660095215, loss=4.303493499755859
I0203 08:51:58.817239 139702527031040 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.290858030319214, loss=2.1147267818450928
I0203 08:52:45.074877 139702543816448 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.220416307449341, loss=2.1714420318603516
I0203 08:53:31.052148 139702527031040 logging_writer.py:48] [152700] global_step=152700, grad_norm=1.9115965366363525, loss=2.1789097785949707
I0203 08:54:17.016104 139702543816448 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.10269832611084, loss=2.4708335399627686
I0203 08:55:02.938196 139702527031040 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.0774359703063965, loss=2.1438372135162354
I0203 08:55:12.357640 139863983413056 spec.py:321] Evaluating on the training split.
I0203 08:55:22.862431 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 08:55:48.724572 139863983413056 spec.py:349] Evaluating on the test split.
I0203 08:55:50.363075 139863983413056 submission_runner.py:408] Time since start: 76541.56s, 	Step: 152922, 	{'train/accuracy': 0.7342382669448853, 'train/loss': 1.077256202697754, 'validation/accuracy': 0.6779199838638306, 'validation/loss': 1.3336390256881714, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.967795968055725, 'test/num_examples': 10000, 'score': 69777.8404636383, 'total_duration': 76541.56018471718, 'accumulated_submission_time': 69777.8404636383, 'accumulated_eval_time': 6747.357416629791, 'accumulated_logging_time': 8.25408148765564}
I0203 08:55:50.405778 139702543816448 logging_writer.py:48] [152922] accumulated_eval_time=6747.357417, accumulated_logging_time=8.254081, accumulated_submission_time=69777.840464, global_step=152922, preemption_count=0, score=69777.840464, test/accuracy=0.555000, test/loss=1.967796, test/num_examples=10000, total_duration=76541.560185, train/accuracy=0.734238, train/loss=1.077256, validation/accuracy=0.677920, validation/loss=1.333639, validation/num_examples=50000
I0203 08:56:22.018535 139702527031040 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.0504066944122314, loss=2.0332655906677246
I0203 08:57:07.722946 139702543816448 logging_writer.py:48] [153100] global_step=153100, grad_norm=1.9627819061279297, loss=3.0432562828063965
I0203 08:57:54.176889 139702527031040 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.164355516433716, loss=3.942746162414551
I0203 08:58:40.178785 139702543816448 logging_writer.py:48] [153300] global_step=153300, grad_norm=1.8316324949264526, loss=2.8484320640563965
I0203 08:59:26.410509 139702527031040 logging_writer.py:48] [153400] global_step=153400, grad_norm=1.864332675933838, loss=3.1836514472961426
I0203 09:00:12.593864 139702543816448 logging_writer.py:48] [153500] global_step=153500, grad_norm=1.9282559156417847, loss=3.0566699504852295
I0203 09:00:58.530422 139702527031040 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.1968002319335938, loss=2.010040044784546
I0203 09:01:44.760730 139702543816448 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.2000207901000977, loss=1.9826563596725464
I0203 09:02:30.979188 139702527031040 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.3293492794036865, loss=1.9337832927703857
I0203 09:02:50.531146 139863983413056 spec.py:321] Evaluating on the training split.
I0203 09:03:01.045024 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 09:03:29.678771 139863983413056 spec.py:349] Evaluating on the test split.
I0203 09:03:31.325036 139863983413056 submission_runner.py:408] Time since start: 77002.52s, 	Step: 153844, 	{'train/accuracy': 0.755175769329071, 'train/loss': 0.9857242107391357, 'validation/accuracy': 0.6827999949455261, 'validation/loss': 1.3085020780563354, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9330166578292847, 'test/num_examples': 10000, 'score': 70197.90728449821, 'total_duration': 77002.52214980125, 'accumulated_submission_time': 70197.90728449821, 'accumulated_eval_time': 6788.15131855011, 'accumulated_logging_time': 8.30648159980774}
I0203 09:03:31.367746 139702543816448 logging_writer.py:48] [153844] accumulated_eval_time=6788.151319, accumulated_logging_time=8.306482, accumulated_submission_time=70197.907284, global_step=153844, preemption_count=0, score=70197.907284, test/accuracy=0.561300, test/loss=1.933017, test/num_examples=10000, total_duration=77002.522150, train/accuracy=0.755176, train/loss=0.985724, validation/accuracy=0.682800, validation/loss=1.308502, validation/num_examples=50000
I0203 09:03:54.109392 139702527031040 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.0076422691345215, loss=2.2755346298217773
I0203 09:04:38.742586 139702543816448 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.054481267929077, loss=1.9732825756072998
I0203 09:05:24.838705 139702527031040 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.0000240802764893, loss=1.9888370037078857
I0203 09:06:11.022141 139702543816448 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.4561893939971924, loss=2.0683937072753906
I0203 09:06:56.917809 139702527031040 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.360926628112793, loss=2.1258537769317627
I0203 09:07:43.335289 139702543816448 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.0084357261657715, loss=2.475461721420288
I0203 09:08:29.548505 139702527031040 logging_writer.py:48] [154500] global_step=154500, grad_norm=1.9584077596664429, loss=4.225456237792969
I0203 09:09:15.625150 139702543816448 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.233241558074951, loss=2.099672555923462
I0203 09:10:01.858873 139702527031040 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.061739206314087, loss=1.8561832904815674
I0203 09:10:31.529551 139863983413056 spec.py:321] Evaluating on the training split.
I0203 09:10:42.034738 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 09:11:10.779901 139863983413056 spec.py:349] Evaluating on the test split.
I0203 09:11:12.413863 139863983413056 submission_runner.py:408] Time since start: 77463.61s, 	Step: 154766, 	{'train/accuracy': 0.7420898079872131, 'train/loss': 1.0481141805648804, 'validation/accuracy': 0.6864399909973145, 'validation/loss': 1.2994166612625122, 'validation/num_examples': 50000, 'test/accuracy': 0.5594000220298767, 'test/loss': 1.9377702474594116, 'test/num_examples': 10000, 'score': 70618.0102212429, 'total_duration': 77463.61097240448, 'accumulated_submission_time': 70618.0102212429, 'accumulated_eval_time': 6829.035629749298, 'accumulated_logging_time': 8.359469890594482}
I0203 09:11:12.457278 139702543816448 logging_writer.py:48] [154766] accumulated_eval_time=6829.035630, accumulated_logging_time=8.359470, accumulated_submission_time=70618.010221, global_step=154766, preemption_count=0, score=70618.010221, test/accuracy=0.559400, test/loss=1.937770, test/num_examples=10000, total_duration=77463.610972, train/accuracy=0.742090, train/loss=1.048114, validation/accuracy=0.686440, validation/loss=1.299417, validation/num_examples=50000
I0203 09:11:26.435001 139702527031040 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.039933443069458, loss=2.545701503753662
I0203 09:12:09.896004 139702543816448 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.1795430183410645, loss=2.414414644241333
I0203 09:12:56.107617 139702527031040 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.165814161300659, loss=4.444417953491211
I0203 09:13:42.419252 139702543816448 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.252372980117798, loss=1.8992713689804077
I0203 09:14:28.599706 139702527031040 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.071280002593994, loss=2.239966869354248
I0203 09:15:14.714720 139702543816448 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.2764036655426025, loss=1.870159387588501
I0203 09:16:00.566974 139702527031040 logging_writer.py:48] [155400] global_step=155400, grad_norm=1.912554144859314, loss=3.0757741928100586
I0203 09:16:46.903091 139702543816448 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.404733180999756, loss=1.9949473142623901
I0203 09:17:32.963541 139702527031040 logging_writer.py:48] [155600] global_step=155600, grad_norm=1.941036581993103, loss=3.1704187393188477
I0203 09:18:12.792489 139863983413056 spec.py:321] Evaluating on the training split.
I0203 09:18:24.240708 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 09:18:52.250828 139863983413056 spec.py:349] Evaluating on the test split.
I0203 09:18:53.890780 139863983413056 submission_runner.py:408] Time since start: 77925.09s, 	Step: 155688, 	{'train/accuracy': 0.7477148175239563, 'train/loss': 1.0200145244598389, 'validation/accuracy': 0.6866199970245361, 'validation/loss': 1.2940551042556763, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.915860891342163, 'test/num_examples': 10000, 'score': 71038.28627920151, 'total_duration': 77925.08789467812, 'accumulated_submission_time': 71038.28627920151, 'accumulated_eval_time': 6870.133940458298, 'accumulated_logging_time': 8.413815975189209}
I0203 09:18:53.932200 139702543816448 logging_writer.py:48] [155688] accumulated_eval_time=6870.133940, accumulated_logging_time=8.413816, accumulated_submission_time=71038.286279, global_step=155688, preemption_count=0, score=71038.286279, test/accuracy=0.563800, test/loss=1.915861, test/num_examples=10000, total_duration=77925.087895, train/accuracy=0.747715, train/loss=1.020015, validation/accuracy=0.686620, validation/loss=1.294055, validation/num_examples=50000
I0203 09:18:59.114696 139702527031040 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.457462787628174, loss=2.0471057891845703
I0203 09:19:41.145492 139702543816448 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.0327696800231934, loss=3.6993227005004883
I0203 09:20:27.439750 139702527031040 logging_writer.py:48] [155900] global_step=155900, grad_norm=1.888688325881958, loss=2.9225220680236816
I0203 09:21:13.906271 139702543816448 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.226381540298462, loss=2.0318055152893066
I0203 09:22:00.001936 139702527031040 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.1678595542907715, loss=3.7365059852600098
I0203 09:22:46.290164 139702543816448 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.0669004917144775, loss=3.181743621826172
I0203 09:23:32.445556 139702527031040 logging_writer.py:48] [156300] global_step=156300, grad_norm=1.9592673778533936, loss=3.271960735321045
I0203 09:24:18.914838 139702543816448 logging_writer.py:48] [156400] global_step=156400, grad_norm=1.9681118726730347, loss=3.27947998046875
I0203 09:25:04.964426 139702527031040 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.1955981254577637, loss=2.1818151473999023
I0203 09:25:50.977666 139702543816448 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.279200553894043, loss=1.9672648906707764
I0203 09:25:53.942311 139863983413056 spec.py:321] Evaluating on the training split.
I0203 09:26:04.506991 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 09:26:34.861449 139863983413056 spec.py:349] Evaluating on the test split.
I0203 09:26:36.499846 139863983413056 submission_runner.py:408] Time since start: 78387.70s, 	Step: 156608, 	{'train/accuracy': 0.7582226395606995, 'train/loss': 0.9849535822868347, 'validation/accuracy': 0.6900399923324585, 'validation/loss': 1.2790842056274414, 'validation/num_examples': 50000, 'test/accuracy': 0.5662000179290771, 'test/loss': 1.920639157295227, 'test/num_examples': 10000, 'score': 71458.23702192307, 'total_duration': 78387.6969614029, 'accumulated_submission_time': 71458.23702192307, 'accumulated_eval_time': 6912.691474199295, 'accumulated_logging_time': 8.466750383377075}
I0203 09:26:36.542331 139702527031040 logging_writer.py:48] [156608] accumulated_eval_time=6912.691474, accumulated_logging_time=8.466750, accumulated_submission_time=71458.237022, global_step=156608, preemption_count=0, score=71458.237022, test/accuracy=0.566200, test/loss=1.920639, test/num_examples=10000, total_duration=78387.696961, train/accuracy=0.758223, train/loss=0.984954, validation/accuracy=0.690040, validation/loss=1.279084, validation/num_examples=50000
I0203 09:27:14.894782 139702543816448 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.119922637939453, loss=2.098531723022461
I0203 09:28:00.699277 139702527031040 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.369357109069824, loss=1.9913005828857422
I0203 09:28:47.101406 139702543816448 logging_writer.py:48] [156900] global_step=156900, grad_norm=1.9328454732894897, loss=3.981200933456421
I0203 09:29:33.227943 139702527031040 logging_writer.py:48] [157000] global_step=157000, grad_norm=1.9611526727676392, loss=2.823803186416626
I0203 09:30:19.862152 139702543816448 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.43648624420166, loss=1.9731228351593018
I0203 09:31:05.988254 139702527031040 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.6197221279144287, loss=2.078263282775879
I0203 09:31:52.312188 139702543816448 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.2719764709472656, loss=2.0062575340270996
I0203 09:32:38.762765 139702527031040 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.1152493953704834, loss=2.310594320297241
I0203 09:33:24.752792 139702543816448 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.4355945587158203, loss=1.9183462858200073
I0203 09:33:36.920389 139863983413056 spec.py:321] Evaluating on the training split.
I0203 09:33:47.284259 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 09:34:18.928188 139863983413056 spec.py:349] Evaluating on the test split.
I0203 09:34:20.562848 139863983413056 submission_runner.py:408] Time since start: 78851.76s, 	Step: 157528, 	{'train/accuracy': 0.7495703101158142, 'train/loss': 1.0092881917953491, 'validation/accuracy': 0.6941199898719788, 'validation/loss': 1.263446569442749, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.915724515914917, 'test/num_examples': 10000, 'score': 71878.5546181202, 'total_duration': 78851.75995445251, 'accumulated_submission_time': 71878.5546181202, 'accumulated_eval_time': 6956.333927631378, 'accumulated_logging_time': 8.52064037322998}
I0203 09:34:20.603218 139702527031040 logging_writer.py:48] [157528] accumulated_eval_time=6956.333928, accumulated_logging_time=8.520640, accumulated_submission_time=71878.554618, global_step=157528, preemption_count=0, score=71878.554618, test/accuracy=0.569600, test/loss=1.915725, test/num_examples=10000, total_duration=78851.759954, train/accuracy=0.749570, train/loss=1.009288, validation/accuracy=0.694120, validation/loss=1.263447, validation/num_examples=50000
I0203 09:34:49.758809 139702543816448 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.1128063201904297, loss=2.304652452468872
I0203 09:35:35.948189 139702527031040 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.30364990234375, loss=1.8930168151855469
I0203 09:36:21.820554 139702543816448 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.396982431411743, loss=1.9744181632995605
I0203 09:37:07.911429 139702527031040 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.2710623741149902, loss=2.1279091835021973
I0203 09:37:53.717746 139702543816448 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.345182418823242, loss=2.1106274127960205
I0203 09:38:39.631269 139702527031040 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.4473304748535156, loss=1.8991198539733887
I0203 09:39:25.665407 139702543816448 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.306077480316162, loss=2.134694814682007
I0203 09:40:11.956511 139702527031040 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.2576537132263184, loss=2.1850593090057373
I0203 09:40:57.785681 139702543816448 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.2263100147247314, loss=2.323430061340332
I0203 09:41:20.874402 139863983413056 spec.py:321] Evaluating on the training split.
I0203 09:41:31.426552 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 09:42:03.918936 139863983413056 spec.py:349] Evaluating on the test split.
I0203 09:42:05.555682 139863983413056 submission_runner.py:408] Time since start: 79316.75s, 	Step: 158452, 	{'train/accuracy': 0.7608984112739563, 'train/loss': 0.9779459238052368, 'validation/accuracy': 0.6959199905395508, 'validation/loss': 1.251377820968628, 'validation/num_examples': 50000, 'test/accuracy': 0.5752000212669373, 'test/loss': 1.885601282119751, 'test/num_examples': 10000, 'score': 72298.76634907722, 'total_duration': 79316.75279378891, 'accumulated_submission_time': 72298.76634907722, 'accumulated_eval_time': 7001.015208482742, 'accumulated_logging_time': 8.571056127548218}
I0203 09:42:05.596994 139702527031040 logging_writer.py:48] [158452] accumulated_eval_time=7001.015208, accumulated_logging_time=8.571056, accumulated_submission_time=72298.766349, global_step=158452, preemption_count=0, score=72298.766349, test/accuracy=0.575200, test/loss=1.885601, test/num_examples=10000, total_duration=79316.752794, train/accuracy=0.760898, train/loss=0.977946, validation/accuracy=0.695920, validation/loss=1.251378, validation/num_examples=50000
I0203 09:42:25.150797 139702543816448 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.4484527111053467, loss=1.9137885570526123
I0203 09:43:09.546946 139702527031040 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.4539291858673096, loss=1.9774761199951172
I0203 09:43:55.741386 139702543816448 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.2886202335357666, loss=2.4843547344207764
I0203 09:44:41.962687 139702527031040 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.2996044158935547, loss=2.695664882659912
I0203 09:45:28.476750 139702543816448 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.391575336456299, loss=1.8581135272979736
I0203 09:46:14.519252 139702527031040 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.175459384918213, loss=3.220412254333496
I0203 09:47:00.719247 139702543816448 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.226085662841797, loss=3.5809569358825684
I0203 09:47:46.993171 139702527031040 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.3224875926971436, loss=3.632615566253662
I0203 09:48:33.348104 139702543816448 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.2967677116394043, loss=1.8611162900924683
I0203 09:49:05.922202 139863983413056 spec.py:321] Evaluating on the training split.
I0203 09:49:16.517549 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 09:49:45.610632 139863983413056 spec.py:349] Evaluating on the test split.
I0203 09:49:47.255129 139863983413056 submission_runner.py:408] Time since start: 79778.45s, 	Step: 159372, 	{'train/accuracy': 0.7640234231948853, 'train/loss': 0.9492297768592834, 'validation/accuracy': 0.6974599957466125, 'validation/loss': 1.241568922996521, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 1.8790431022644043, 'test/num_examples': 10000, 'score': 72719.03267765045, 'total_duration': 79778.45223927498, 'accumulated_submission_time': 72719.03267765045, 'accumulated_eval_time': 7042.348134994507, 'accumulated_logging_time': 8.622626304626465}
I0203 09:49:47.300965 139702527031040 logging_writer.py:48] [159372] accumulated_eval_time=7042.348135, accumulated_logging_time=8.622626, accumulated_submission_time=72719.032678, global_step=159372, preemption_count=0, score=72719.032678, test/accuracy=0.572500, test/loss=1.879043, test/num_examples=10000, total_duration=79778.452239, train/accuracy=0.764023, train/loss=0.949230, validation/accuracy=0.697460, validation/loss=1.241569, validation/num_examples=50000
I0203 09:49:58.872219 139702543816448 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.1701440811157227, loss=3.8037891387939453
I0203 09:50:42.018554 139702527031040 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.242727279663086, loss=2.4669606685638428
I0203 09:51:27.958618 139702543816448 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.5198123455047607, loss=1.830070972442627
I0203 09:52:14.087663 139702527031040 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.3746097087860107, loss=2.110301971435547
I0203 09:53:00.000772 139702543816448 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.6745100021362305, loss=1.8684813976287842
I0203 09:53:45.903735 139702527031040 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.5148062705993652, loss=1.9520066976547241
I0203 09:54:31.963215 139702543816448 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.577440023422241, loss=2.106930732727051
I0203 09:55:18.071924 139702527031040 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.521185874938965, loss=1.9209717512130737
I0203 09:56:04.434012 139702543816448 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.281677007675171, loss=2.186776638031006
I0203 09:56:47.615254 139863983413056 spec.py:321] Evaluating on the training split.
I0203 09:56:58.238698 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 09:57:26.477895 139863983413056 spec.py:349] Evaluating on the test split.
I0203 09:57:28.119042 139863983413056 submission_runner.py:408] Time since start: 80239.32s, 	Step: 160296, 	{'train/accuracy': 0.7643359303474426, 'train/loss': 0.9486043453216553, 'validation/accuracy': 0.703000009059906, 'validation/loss': 1.2157114744186401, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 1.8601552248001099, 'test/num_examples': 10000, 'score': 73139.28754115105, 'total_duration': 80239.31615614891, 'accumulated_submission_time': 73139.28754115105, 'accumulated_eval_time': 7082.851930856705, 'accumulated_logging_time': 8.679443597793579}
I0203 09:57:28.163383 139702527031040 logging_writer.py:48] [160296] accumulated_eval_time=7082.851931, accumulated_logging_time=8.679444, accumulated_submission_time=73139.287541, global_step=160296, preemption_count=0, score=73139.287541, test/accuracy=0.574800, test/loss=1.860155, test/num_examples=10000, total_duration=80239.316156, train/accuracy=0.764336, train/loss=0.948604, validation/accuracy=0.703000, validation/loss=1.215711, validation/num_examples=50000
I0203 09:57:30.159899 139702543816448 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.4697959423065186, loss=1.9297806024551392
I0203 09:58:11.932108 139702527031040 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.587965250015259, loss=1.9844508171081543
I0203 09:58:57.899703 139702543816448 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.389313220977783, loss=3.591871738433838
I0203 09:59:44.122649 139702527031040 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.2711260318756104, loss=3.522972583770752
I0203 10:00:30.252717 139702543816448 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.29563045501709, loss=3.1323230266571045
I0203 10:01:16.400660 139702527031040 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.683147430419922, loss=3.856250524520874
I0203 10:02:02.215243 139702543816448 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.2885589599609375, loss=3.91227650642395
I0203 10:02:48.294414 139702527031040 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.546135425567627, loss=2.235146999359131
I0203 10:03:34.229176 139702543816448 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.628493309020996, loss=1.8931504487991333
I0203 10:04:20.463075 139702527031040 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.379960536956787, loss=3.239351511001587
I0203 10:04:28.448805 139863983413056 spec.py:321] Evaluating on the training split.
I0203 10:04:38.729118 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 10:05:08.992922 139863983413056 spec.py:349] Evaluating on the test split.
I0203 10:05:10.646894 139863983413056 submission_runner.py:408] Time since start: 80701.84s, 	Step: 161219, 	{'train/accuracy': 0.763378918170929, 'train/loss': 0.9769259691238403, 'validation/accuracy': 0.7021999955177307, 'validation/loss': 1.2391090393066406, 'validation/num_examples': 50000, 'test/accuracy': 0.5754000544548035, 'test/loss': 1.8825575113296509, 'test/num_examples': 10000, 'score': 73559.51320672035, 'total_duration': 80701.84395289421, 'accumulated_submission_time': 73559.51320672035, 'accumulated_eval_time': 7125.049999952316, 'accumulated_logging_time': 8.735102891921997}
I0203 10:05:10.699368 139702543816448 logging_writer.py:48] [161219] accumulated_eval_time=7125.050000, accumulated_logging_time=8.735103, accumulated_submission_time=73559.513207, global_step=161219, preemption_count=0, score=73559.513207, test/accuracy=0.575400, test/loss=1.882558, test/num_examples=10000, total_duration=80701.843953, train/accuracy=0.763379, train/loss=0.976926, validation/accuracy=0.702200, validation/loss=1.239109, validation/num_examples=50000
I0203 10:05:43.809171 139702527031040 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.467726945877075, loss=1.9025126695632935
I0203 10:06:29.957089 139702543816448 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.4767513275146484, loss=1.8406355381011963
I0203 10:07:16.111433 139702527031040 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.415506362915039, loss=2.3539202213287354
I0203 10:08:01.945723 139702543816448 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.4313507080078125, loss=1.8270753622055054
I0203 10:08:47.965499 139702527031040 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.372438669204712, loss=2.4050612449645996
I0203 10:09:34.130263 139702543816448 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.4327142238616943, loss=4.298861980438232
I0203 10:10:20.475152 139702527031040 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.557551622390747, loss=2.119424343109131
I0203 10:11:06.328215 139702543816448 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.288496494293213, loss=2.6207454204559326
I0203 10:11:52.237898 139702527031040 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.3930344581604004, loss=3.6184210777282715
I0203 10:12:10.825138 139863983413056 spec.py:321] Evaluating on the training split.
I0203 10:12:21.162611 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 10:12:51.302221 139863983413056 spec.py:349] Evaluating on the test split.
I0203 10:12:52.950004 139863983413056 submission_runner.py:408] Time since start: 81164.15s, 	Step: 162142, 	{'train/accuracy': 0.7700781226158142, 'train/loss': 0.9373509287834167, 'validation/accuracy': 0.7052599787712097, 'validation/loss': 1.2130261659622192, 'validation/num_examples': 50000, 'test/accuracy': 0.579300045967102, 'test/loss': 1.8460499048233032, 'test/num_examples': 10000, 'score': 73979.57974791527, 'total_duration': 81164.14709663391, 'accumulated_submission_time': 73979.57974791527, 'accumulated_eval_time': 7167.17483496666, 'accumulated_logging_time': 8.798385381698608}
I0203 10:12:52.999951 139702543816448 logging_writer.py:48] [162142] accumulated_eval_time=7167.174835, accumulated_logging_time=8.798385, accumulated_submission_time=73979.579748, global_step=162142, preemption_count=0, score=73979.579748, test/accuracy=0.579300, test/loss=1.846050, test/num_examples=10000, total_duration=81164.147097, train/accuracy=0.770078, train/loss=0.937351, validation/accuracy=0.705260, validation/loss=1.213026, validation/num_examples=50000
I0203 10:13:16.560755 139702527031040 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.6666884422302246, loss=1.9772090911865234
I0203 10:14:01.242439 139702543816448 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.5901007652282715, loss=1.8452121019363403
I0203 10:14:47.313861 139702527031040 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.4838955402374268, loss=1.7218003273010254
I0203 10:15:33.552528 139702543816448 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.451184034347534, loss=1.7200596332550049
I0203 10:16:19.651334 139702527031040 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.3893837928771973, loss=2.960658550262451
I0203 10:17:05.842709 139702543816448 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.435037136077881, loss=2.0572779178619385
I0203 10:17:51.708352 139702527031040 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.575476884841919, loss=1.7961801290512085
I0203 10:18:37.970180 139702543816448 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.4817020893096924, loss=4.289982795715332
I0203 10:19:24.195330 139702527031040 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.324373245239258, loss=2.4241254329681396
I0203 10:19:52.979897 139863983413056 spec.py:321] Evaluating on the training split.
I0203 10:20:03.482216 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 10:20:36.477861 139863983413056 spec.py:349] Evaluating on the test split.
I0203 10:20:38.112354 139863983413056 submission_runner.py:408] Time since start: 81629.31s, 	Step: 163064, 	{'train/accuracy': 0.7858203053474426, 'train/loss': 0.8678929209709167, 'validation/accuracy': 0.7084400057792664, 'validation/loss': 1.2065235376358032, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.8323715925216675, 'test/num_examples': 10000, 'score': 74399.50149416924, 'total_duration': 81629.3094651699, 'accumulated_submission_time': 74399.50149416924, 'accumulated_eval_time': 7212.307286977768, 'accumulated_logging_time': 8.858510732650757}
I0203 10:20:38.154762 139702543816448 logging_writer.py:48] [163064] accumulated_eval_time=7212.307287, accumulated_logging_time=8.858511, accumulated_submission_time=74399.501494, global_step=163064, preemption_count=0, score=74399.501494, test/accuracy=0.583300, test/loss=1.832372, test/num_examples=10000, total_duration=81629.309465, train/accuracy=0.785820, train/loss=0.867893, validation/accuracy=0.708440, validation/loss=1.206524, validation/num_examples=50000
I0203 10:20:52.916012 139702527031040 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.339916706085205, loss=3.0712807178497314
I0203 10:21:36.589251 139702543816448 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.6508469581604004, loss=1.9771103858947754
I0203 10:22:22.529022 139702527031040 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.824171543121338, loss=1.8460899591445923
I0203 10:23:08.752830 139702543816448 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.8404555320739746, loss=1.866782784461975
I0203 10:23:54.562870 139702527031040 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.783839464187622, loss=1.790845513343811
I0203 10:24:40.606908 139702543816448 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.5920348167419434, loss=1.7472686767578125
I0203 10:25:26.740714 139702527031040 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.765622138977051, loss=4.168459892272949
I0203 10:26:12.958558 139702543816448 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.380629539489746, loss=2.787716865539551
I0203 10:26:59.191291 139702527031040 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.6718051433563232, loss=1.8152364492416382
I0203 10:27:38.337125 139863983413056 spec.py:321] Evaluating on the training split.
I0203 10:27:48.679081 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 10:28:15.430449 139863983413056 spec.py:349] Evaluating on the test split.
I0203 10:28:17.075364 139863983413056 submission_runner.py:408] Time since start: 82088.27s, 	Step: 163987, 	{'train/accuracy': 0.7753320336341858, 'train/loss': 0.9123504161834717, 'validation/accuracy': 0.7140399813652039, 'validation/loss': 1.184422492980957, 'validation/num_examples': 50000, 'test/accuracy': 0.5903000235557556, 'test/loss': 1.8158957958221436, 'test/num_examples': 10000, 'score': 74819.62465143204, 'total_duration': 82088.27244186401, 'accumulated_submission_time': 74819.62465143204, 'accumulated_eval_time': 7251.045498132706, 'accumulated_logging_time': 8.912059307098389}
I0203 10:28:17.127170 139702543816448 logging_writer.py:48] [163987] accumulated_eval_time=7251.045498, accumulated_logging_time=8.912059, accumulated_submission_time=74819.624651, global_step=163987, preemption_count=0, score=74819.624651, test/accuracy=0.590300, test/loss=1.815896, test/num_examples=10000, total_duration=82088.272442, train/accuracy=0.775332, train/loss=0.912350, validation/accuracy=0.714040, validation/loss=1.184422, validation/num_examples=50000
I0203 10:28:22.713937 139702527031040 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.75103759765625, loss=1.7157527208328247
I0203 10:29:04.691770 139702543816448 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.763321876525879, loss=1.8772549629211426
I0203 10:29:50.755983 139702527031040 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.692791700363159, loss=1.9005913734436035
I0203 10:30:37.167362 139702543816448 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.784233808517456, loss=1.8728363513946533
I0203 10:31:22.950060 139702527031040 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.6565027236938477, loss=3.9928102493286133
I0203 10:32:09.023779 139702543816448 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.8087353706359863, loss=1.8960567712783813
I0203 10:32:54.984235 139702527031040 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.9934499263763428, loss=4.049821376800537
I0203 10:33:41.187343 139702543816448 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.637166976928711, loss=1.7273797988891602
I0203 10:34:27.312393 139702527031040 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.828974485397339, loss=3.5318942070007324
I0203 10:35:13.629774 139702543816448 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.731189727783203, loss=1.7646827697753906
I0203 10:35:17.075999 139863983413056 spec.py:321] Evaluating on the training split.
I0203 10:35:27.596533 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 10:35:56.141048 139863983413056 spec.py:349] Evaluating on the test split.
I0203 10:35:57.784601 139863983413056 submission_runner.py:408] Time since start: 82548.98s, 	Step: 164909, 	{'train/accuracy': 0.7803320288658142, 'train/loss': 0.8811646103858948, 'validation/accuracy': 0.7166399955749512, 'validation/loss': 1.1691982746124268, 'validation/num_examples': 50000, 'test/accuracy': 0.5915000438690186, 'test/loss': 1.7959223985671997, 'test/num_examples': 10000, 'score': 75239.51298332214, 'total_duration': 82548.98171710968, 'accumulated_submission_time': 75239.51298332214, 'accumulated_eval_time': 7291.754102945328, 'accumulated_logging_time': 8.975491523742676}
I0203 10:35:57.825948 139702527031040 logging_writer.py:48] [164909] accumulated_eval_time=7291.754103, accumulated_logging_time=8.975492, accumulated_submission_time=75239.512983, global_step=164909, preemption_count=0, score=75239.512983, test/accuracy=0.591500, test/loss=1.795922, test/num_examples=10000, total_duration=82548.981717, train/accuracy=0.780332, train/loss=0.881165, validation/accuracy=0.716640, validation/loss=1.169198, validation/num_examples=50000
I0203 10:36:35.619706 139702543816448 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.443406820297241, loss=3.1486473083496094
I0203 10:37:21.652784 139702527031040 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.6142239570617676, loss=1.7915722131729126
I0203 10:38:08.058954 139702543816448 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.7282044887542725, loss=1.7896429300308228
I0203 10:38:54.072511 139702527031040 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.417757034301758, loss=3.484055280685425
I0203 10:39:40.034675 139702543816448 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.511249542236328, loss=2.664384603500366
I0203 10:40:26.203178 139702527031040 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.592555046081543, loss=2.0085866451263428
I0203 10:41:12.220635 139702543816448 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.793616533279419, loss=2.3371009826660156
I0203 10:41:58.250047 139702527031040 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.7280683517456055, loss=4.286463737487793
I0203 10:42:44.441147 139702543816448 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.456038236618042, loss=2.8052263259887695
I0203 10:42:57.908791 139863983413056 spec.py:321] Evaluating on the training split.
I0203 10:43:08.589730 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 10:43:40.259812 139863983413056 spec.py:349] Evaluating on the test split.
I0203 10:43:41.902174 139863983413056 submission_runner.py:408] Time since start: 83013.10s, 	Step: 165831, 	{'train/accuracy': 0.788378894329071, 'train/loss': 0.8617101907730103, 'validation/accuracy': 0.7175999879837036, 'validation/loss': 1.1666576862335205, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 1.8022934198379517, 'test/num_examples': 10000, 'score': 75659.53583550453, 'total_duration': 83013.09928798676, 'accumulated_submission_time': 75659.53583550453, 'accumulated_eval_time': 7335.747500419617, 'accumulated_logging_time': 9.027804374694824}
I0203 10:43:41.943167 139702527031040 logging_writer.py:48] [165831] accumulated_eval_time=7335.747500, accumulated_logging_time=9.027804, accumulated_submission_time=75659.535836, global_step=165831, preemption_count=0, score=75659.535836, test/accuracy=0.589600, test/loss=1.802293, test/num_examples=10000, total_duration=83013.099288, train/accuracy=0.788379, train/loss=0.861710, validation/accuracy=0.717600, validation/loss=1.166658, validation/num_examples=50000
I0203 10:44:09.902104 139702543816448 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.063157081604004, loss=1.8809785842895508
I0203 10:44:55.144568 139702527031040 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.72843861579895, loss=3.9088497161865234
I0203 10:45:41.402798 139702543816448 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.8412418365478516, loss=2.003857135772705
I0203 10:46:27.843840 139702527031040 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.6567368507385254, loss=3.4484713077545166
I0203 10:47:13.921168 139702543816448 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.67212176322937, loss=1.7586259841918945
I0203 10:48:00.303019 139702527031040 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.804694175720215, loss=2.93454909324646
I0203 10:48:46.649604 139702543816448 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.6813623905181885, loss=3.5073306560516357
I0203 10:49:32.888550 139702527031040 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.7552828788757324, loss=3.8153796195983887
I0203 10:50:19.269232 139702543816448 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.7827260494232178, loss=1.7955706119537354
I0203 10:50:41.921378 139863983413056 spec.py:321] Evaluating on the training split.
I0203 10:50:52.398594 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 10:51:20.331038 139863983413056 spec.py:349] Evaluating on the test split.
I0203 10:51:21.975406 139863983413056 submission_runner.py:408] Time since start: 83473.17s, 	Step: 166751, 	{'train/accuracy': 0.7841405868530273, 'train/loss': 0.869185209274292, 'validation/accuracy': 0.7195799946784973, 'validation/loss': 1.1540203094482422, 'validation/num_examples': 50000, 'test/accuracy': 0.5955000519752502, 'test/loss': 1.7816680669784546, 'test/num_examples': 10000, 'score': 76079.45230317116, 'total_duration': 83473.17249202728, 'accumulated_submission_time': 76079.45230317116, 'accumulated_eval_time': 7375.801503419876, 'accumulated_logging_time': 9.081603765487671}
I0203 10:51:22.026551 139702527031040 logging_writer.py:48] [166751] accumulated_eval_time=7375.801503, accumulated_logging_time=9.081604, accumulated_submission_time=76079.452303, global_step=166751, preemption_count=0, score=76079.452303, test/accuracy=0.595500, test/loss=1.781668, test/num_examples=10000, total_duration=83473.172492, train/accuracy=0.784141, train/loss=0.869185, validation/accuracy=0.719580, validation/loss=1.154020, validation/num_examples=50000
I0203 10:51:41.993280 139702543816448 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.8702845573425293, loss=4.277138710021973
I0203 10:52:26.229520 139702527031040 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.658214569091797, loss=1.796937108039856
I0203 10:53:12.311712 139702543816448 logging_writer.py:48] [167000] global_step=167000, grad_norm=3.0300166606903076, loss=1.7211517095565796
I0203 10:53:58.336838 139702527031040 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.6829261779785156, loss=3.310673952102661
I0203 10:54:44.452848 139702543816448 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.8309640884399414, loss=1.6764304637908936
I0203 10:55:30.666538 139702527031040 logging_writer.py:48] [167300] global_step=167300, grad_norm=3.2784550189971924, loss=1.8131755590438843
I0203 10:56:16.701661 139702543816448 logging_writer.py:48] [167400] global_step=167400, grad_norm=2.7016196250915527, loss=1.5950530767440796
I0203 10:57:02.756503 139702527031040 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.8571953773498535, loss=2.1989474296569824
I0203 10:57:48.748565 139702543816448 logging_writer.py:48] [167600] global_step=167600, grad_norm=3.006120204925537, loss=1.8081120252609253
I0203 10:58:22.286339 139863983413056 spec.py:321] Evaluating on the training split.
I0203 10:58:32.632755 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 10:59:01.947805 139863983413056 spec.py:349] Evaluating on the test split.
I0203 10:59:03.592417 139863983413056 submission_runner.py:408] Time since start: 83934.79s, 	Step: 167674, 	{'train/accuracy': 0.7900976538658142, 'train/loss': 0.8440901041030884, 'validation/accuracy': 0.7210599780082703, 'validation/loss': 1.1322126388549805, 'validation/num_examples': 50000, 'test/accuracy': 0.5976999998092651, 'test/loss': 1.7546520233154297, 'test/num_examples': 10000, 'score': 76499.65189146996, 'total_duration': 83934.78953242302, 'accumulated_submission_time': 76499.65189146996, 'accumulated_eval_time': 7417.107574224472, 'accumulated_logging_time': 9.143556594848633}
I0203 10:59:03.634614 139702527031040 logging_writer.py:48] [167674] accumulated_eval_time=7417.107574, accumulated_logging_time=9.143557, accumulated_submission_time=76499.651891, global_step=167674, preemption_count=0, score=76499.651891, test/accuracy=0.597700, test/loss=1.754652, test/num_examples=10000, total_duration=83934.789532, train/accuracy=0.790098, train/loss=0.844090, validation/accuracy=0.721060, validation/loss=1.132213, validation/num_examples=50000
I0203 10:59:14.403085 139702543816448 logging_writer.py:48] [167700] global_step=167700, grad_norm=3.199699640274048, loss=4.132631778717041
I0203 10:59:57.425877 139702527031040 logging_writer.py:48] [167800] global_step=167800, grad_norm=3.000460624694824, loss=1.815910816192627
I0203 11:00:43.285446 139702543816448 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.4569365978240967, loss=2.7791855335235596
I0203 11:01:29.581801 139702527031040 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.9150967597961426, loss=1.6877317428588867
I0203 11:02:15.542485 139702543816448 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.848478078842163, loss=1.6562860012054443
I0203 11:03:01.514016 139702527031040 logging_writer.py:48] [168200] global_step=168200, grad_norm=2.9382436275482178, loss=1.763007640838623
I0203 11:03:47.555879 139702543816448 logging_writer.py:48] [168300] global_step=168300, grad_norm=2.863251209259033, loss=1.9954437017440796
I0203 11:04:33.686370 139702527031040 logging_writer.py:48] [168400] global_step=168400, grad_norm=2.8460631370544434, loss=2.1790049076080322
I0203 11:05:19.808151 139702543816448 logging_writer.py:48] [168500] global_step=168500, grad_norm=2.9048051834106445, loss=1.6593455076217651
I0203 11:06:03.909474 139863983413056 spec.py:321] Evaluating on the training split.
I0203 11:06:14.448729 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 11:06:45.689906 139863983413056 spec.py:349] Evaluating on the test split.
I0203 11:06:47.339349 139863983413056 submission_runner.py:408] Time since start: 84398.54s, 	Step: 168597, 	{'train/accuracy': 0.7947070002555847, 'train/loss': 0.8163579106330872, 'validation/accuracy': 0.7242000102996826, 'validation/loss': 1.1301054954528809, 'validation/num_examples': 50000, 'test/accuracy': 0.6009000539779663, 'test/loss': 1.7406065464019775, 'test/num_examples': 10000, 'score': 76919.8663828373, 'total_duration': 84398.5364639759, 'accumulated_submission_time': 76919.8663828373, 'accumulated_eval_time': 7460.537466287613, 'accumulated_logging_time': 9.19641399383545}
I0203 11:06:47.386993 139702527031040 logging_writer.py:48] [168597] accumulated_eval_time=7460.537466, accumulated_logging_time=9.196414, accumulated_submission_time=76919.866383, global_step=168597, preemption_count=0, score=76919.866383, test/accuracy=0.600900, test/loss=1.740607, test/num_examples=10000, total_duration=84398.536464, train/accuracy=0.794707, train/loss=0.816358, validation/accuracy=0.724200, validation/loss=1.130105, validation/num_examples=50000
I0203 11:06:48.987094 139702543816448 logging_writer.py:48] [168600] global_step=168600, grad_norm=2.8333306312561035, loss=1.9848582744598389
I0203 11:07:30.674424 139702527031040 logging_writer.py:48] [168700] global_step=168700, grad_norm=3.0806190967559814, loss=1.7005336284637451
I0203 11:08:16.744753 139702543816448 logging_writer.py:48] [168800] global_step=168800, grad_norm=2.8428473472595215, loss=3.115286111831665
I0203 11:09:03.267439 139702527031040 logging_writer.py:48] [168900] global_step=168900, grad_norm=3.1141064167022705, loss=4.134634017944336
I0203 11:09:49.120625 139702543816448 logging_writer.py:48] [169000] global_step=169000, grad_norm=2.8873190879821777, loss=3.378647804260254
I0203 11:10:35.276542 139702527031040 logging_writer.py:48] [169100] global_step=169100, grad_norm=2.9178812503814697, loss=1.7180876731872559
I0203 11:11:21.361649 139702543816448 logging_writer.py:48] [169200] global_step=169200, grad_norm=2.7806334495544434, loss=1.6324716806411743
I0203 11:12:07.334252 139702527031040 logging_writer.py:48] [169300] global_step=169300, grad_norm=2.6753487586975098, loss=2.700598955154419
I0203 11:12:53.433170 139702543816448 logging_writer.py:48] [169400] global_step=169400, grad_norm=2.879916191101074, loss=1.6166155338287354
I0203 11:13:39.272851 139702527031040 logging_writer.py:48] [169500] global_step=169500, grad_norm=3.029477596282959, loss=3.3469111919403076
I0203 11:13:47.428576 139863983413056 spec.py:321] Evaluating on the training split.
I0203 11:13:57.748719 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 11:14:28.802775 139863983413056 spec.py:349] Evaluating on the test split.
I0203 11:14:30.444655 139863983413056 submission_runner.py:408] Time since start: 84861.64s, 	Step: 169519, 	{'train/accuracy': 0.7934960722923279, 'train/loss': 0.8324291706085205, 'validation/accuracy': 0.725820004940033, 'validation/loss': 1.1191824674606323, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.7409383058547974, 'test/num_examples': 10000, 'score': 77339.84844470024, 'total_duration': 84861.64176750183, 'accumulated_submission_time': 77339.84844470024, 'accumulated_eval_time': 7503.553560256958, 'accumulated_logging_time': 9.254878997802734}
I0203 11:14:30.489133 139702543816448 logging_writer.py:48] [169519] accumulated_eval_time=7503.553560, accumulated_logging_time=9.254879, accumulated_submission_time=77339.848445, global_step=169519, preemption_count=0, score=77339.848445, test/accuracy=0.602400, test/loss=1.740938, test/num_examples=10000, total_duration=84861.641768, train/accuracy=0.793496, train/loss=0.832429, validation/accuracy=0.725820, validation/loss=1.119182, validation/num_examples=50000
I0203 11:15:03.854874 139702527031040 logging_writer.py:48] [169600] global_step=169600, grad_norm=2.7647852897644043, loss=2.1878435611724854
I0203 11:15:49.473627 139702543816448 logging_writer.py:48] [169700] global_step=169700, grad_norm=2.6832921504974365, loss=1.903586506843567
I0203 11:16:35.766223 139702527031040 logging_writer.py:48] [169800] global_step=169800, grad_norm=2.9250576496124268, loss=1.7831860780715942
I0203 11:17:21.808814 139702543816448 logging_writer.py:48] [169900] global_step=169900, grad_norm=3.0676958560943604, loss=1.635420799255371
I0203 11:17:30.870835 139702527031040 logging_writer.py:48] [169921] global_step=169921, preemption_count=0, score=77520.149893
I0203 11:17:31.549602 139863983413056 checkpoints.py:490] Saving checkpoint at step: 169921
I0203 11:17:32.815978 139863983413056 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_4/checkpoint_169921
I0203 11:17:32.836366 139863983413056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_4/checkpoint_169921.
I0203 11:17:33.580497 139863983413056 submission_runner.py:583] Tuning trial 4/5
I0203 11:17:33.580713 139863983413056 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0203 11:17:33.590334 139863983413056 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010351561941206455, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 34.864447355270386, 'total_duration': 63.08093595504761, 'accumulated_submission_time': 34.864447355270386, 'accumulated_eval_time': 28.21615719795227, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (860, {'train/accuracy': 0.033867187798023224, 'train/loss': 5.902928829193115, 'validation/accuracy': 0.030639998614788055, 'validation/loss': 5.9316911697387695, 'validation/num_examples': 50000, 'test/accuracy': 0.026000000536441803, 'test/loss': 6.065794467926025, 'test/num_examples': 10000, 'score': 455.0325014591217, 'total_duration': 522.3378174304962, 'accumulated_submission_time': 455.0325014591217, 'accumulated_eval_time': 67.2409439086914, 'accumulated_logging_time': 0.017840147018432617, 'global_step': 860, 'preemption_count': 0}), (1778, {'train/accuracy': 0.07476562261581421, 'train/loss': 5.325953960418701, 'validation/accuracy': 0.06904000043869019, 'validation/loss': 5.398590087890625, 'validation/num_examples': 50000, 'test/accuracy': 0.05450000241398811, 'test/loss': 5.616189956665039, 'test/num_examples': 10000, 'score': 875.0685901641846, 'total_duration': 987.7160251140594, 'accumulated_submission_time': 875.0685901641846, 'accumulated_eval_time': 112.50715756416321, 'accumulated_logging_time': 0.04444408416748047, 'global_step': 1778, 'preemption_count': 0}), (2697, {'train/accuracy': 0.09357421845197678, 'train/loss': 5.147493362426758, 'validation/accuracy': 0.08664000034332275, 'validation/loss': 5.2080559730529785, 'validation/num_examples': 50000, 'test/accuracy': 0.06680000573396683, 'test/loss': 5.455690860748291, 'test/num_examples': 10000, 'score': 1295.2597908973694, 'total_duration': 1446.4552066326141, 'accumulated_submission_time': 1295.2597908973694, 'accumulated_eval_time': 150.98039412498474, 'accumulated_logging_time': 0.07066822052001953, 'global_step': 2697, 'preemption_count': 0}), (3618, {'train/accuracy': 0.13871093094348907, 'train/loss': 4.64686393737793, 'validation/accuracy': 0.12814000248908997, 'validation/loss': 4.720630645751953, 'validation/num_examples': 50000, 'test/accuracy': 0.09710000455379486, 'test/loss': 5.038281440734863, 'test/num_examples': 10000, 'score': 1715.3475415706635, 'total_duration': 1909.9411296844482, 'accumulated_submission_time': 1715.3475415706635, 'accumulated_eval_time': 194.3020474910736, 'accumulated_logging_time': 0.09827184677124023, 'global_step': 3618, 'preemption_count': 0}), (4541, {'train/accuracy': 0.16164061427116394, 'train/loss': 4.394140720367432, 'validation/accuracy': 0.15173999965190887, 'validation/loss': 4.4949469566345215, 'validation/num_examples': 50000, 'test/accuracy': 0.11710000783205032, 'test/loss': 4.889176845550537, 'test/num_examples': 10000, 'score': 2135.6945128440857, 'total_duration': 2372.8839206695557, 'accumulated_submission_time': 2135.6945128440857, 'accumulated_eval_time': 236.82340264320374, 'accumulated_logging_time': 0.1237492561340332, 'global_step': 4541, 'preemption_count': 0}), (5457, {'train/accuracy': 0.19478514790534973, 'train/loss': 4.121174335479736, 'validation/accuracy': 0.18053999543190002, 'validation/loss': 4.2075347900390625, 'validation/num_examples': 50000, 'test/accuracy': 0.13819999992847443, 'test/loss': 4.630496978759766, 'test/num_examples': 10000, 'score': 2556.0419986248016, 'total_duration': 2834.0153284072876, 'accumulated_submission_time': 2556.0419986248016, 'accumulated_eval_time': 277.5315291881561, 'accumulated_logging_time': 0.15093755722045898, 'global_step': 5457, 'preemption_count': 0}), (6379, {'train/accuracy': 0.22484374046325684, 'train/loss': 3.9965386390686035, 'validation/accuracy': 0.20787999033927917, 'validation/loss': 4.081354141235352, 'validation/num_examples': 50000, 'test/accuracy': 0.15680000185966492, 'test/loss': 4.4948883056640625, 'test/num_examples': 10000, 'score': 2976.1662895679474, 'total_duration': 3293.9713699817657, 'accumulated_submission_time': 2976.1662895679474, 'accumulated_eval_time': 317.28538703918457, 'accumulated_logging_time': 0.17990326881408691, 'global_step': 6379, 'preemption_count': 0}), (7299, {'train/accuracy': 0.23093749582767487, 'train/loss': 3.953554630279541, 'validation/accuracy': 0.21237999200820923, 'validation/loss': 4.063230991363525, 'validation/num_examples': 50000, 'test/accuracy': 0.16200000047683716, 'test/loss': 4.486764430999756, 'test/num_examples': 10000, 'score': 3396.437614440918, 'total_duration': 3754.5577280521393, 'accumulated_submission_time': 3396.437614440918, 'accumulated_eval_time': 357.51818895339966, 'accumulated_logging_time': 0.21305513381958008, 'global_step': 7299, 'preemption_count': 0}), (8218, {'train/accuracy': 0.2625390589237213, 'train/loss': 3.6284167766571045, 'validation/accuracy': 0.22551999986171722, 'validation/loss': 3.8713552951812744, 'validation/num_examples': 50000, 'test/accuracy': 0.17330001294612885, 'test/loss': 4.334794521331787, 'test/num_examples': 10000, 'score': 3816.4799420833588, 'total_duration': 4213.867478132248, 'accumulated_submission_time': 3816.4799420833588, 'accumulated_eval_time': 396.70170760154724, 'accumulated_logging_time': 0.24888205528259277, 'global_step': 8218, 'preemption_count': 0}), (9142, {'train/accuracy': 0.27253904938697815, 'train/loss': 3.545860767364502, 'validation/accuracy': 0.2532599866390228, 'validation/loss': 3.675501585006714, 'validation/num_examples': 50000, 'test/accuracy': 0.1899000108242035, 'test/loss': 4.1776041984558105, 'test/num_examples': 10000, 'score': 4236.704192399979, 'total_duration': 4677.285391569138, 'accumulated_submission_time': 4236.704192399979, 'accumulated_eval_time': 439.81812286376953, 'accumulated_logging_time': 0.2767214775085449, 'global_step': 9142, 'preemption_count': 0}), (10064, {'train/accuracy': 0.25773435831069946, 'train/loss': 3.733825206756592, 'validation/accuracy': 0.2406199872493744, 'validation/loss': 3.855492115020752, 'validation/num_examples': 50000, 'test/accuracy': 0.1834000051021576, 'test/loss': 4.324698448181152, 'test/num_examples': 10000, 'score': 4656.660678386688, 'total_duration': 5135.04213309288, 'accumulated_submission_time': 4656.660678386688, 'accumulated_eval_time': 477.5397388935089, 'accumulated_logging_time': 0.3062405586242676, 'global_step': 10064, 'preemption_count': 0}), (10987, {'train/accuracy': 0.27595701813697815, 'train/loss': 3.5516586303710938, 'validation/accuracy': 0.24587999284267426, 'validation/loss': 3.746058225631714, 'validation/num_examples': 50000, 'test/accuracy': 0.19140000641345978, 'test/loss': 4.227960586547852, 'test/num_examples': 10000, 'score': 5076.703187704086, 'total_duration': 5594.703353404999, 'accumulated_submission_time': 5076.703187704086, 'accumulated_eval_time': 517.0797474384308, 'accumulated_logging_time': 0.33579182624816895, 'global_step': 10987, 'preemption_count': 0}), (11908, {'train/accuracy': 0.28855466842651367, 'train/loss': 3.454838752746582, 'validation/accuracy': 0.2699199914932251, 'validation/loss': 3.5662355422973633, 'validation/num_examples': 50000, 'test/accuracy': 0.20250001549720764, 'test/loss': 4.11765718460083, 'test/num_examples': 10000, 'score': 5496.717879772186, 'total_duration': 6053.150849819183, 'accumulated_submission_time': 5496.717879772186, 'accumulated_eval_time': 555.4356439113617, 'accumulated_logging_time': 0.3640127182006836, 'global_step': 11908, 'preemption_count': 0}), (12828, {'train/accuracy': 0.3011523485183716, 'train/loss': 3.359318256378174, 'validation/accuracy': 0.2775000035762787, 'validation/loss': 3.4931159019470215, 'validation/num_examples': 50000, 'test/accuracy': 0.21570001542568207, 'test/loss': 4.02402925491333, 'test/num_examples': 10000, 'score': 5916.797674417496, 'total_duration': 6510.595469713211, 'accumulated_submission_time': 5916.797674417496, 'accumulated_eval_time': 592.7188277244568, 'accumulated_logging_time': 0.3962678909301758, 'global_step': 12828, 'preemption_count': 0}), (13749, {'train/accuracy': 0.3214453160762787, 'train/loss': 3.2423040866851807, 'validation/accuracy': 0.28751999139785767, 'validation/loss': 3.422166109085083, 'validation/num_examples': 50000, 'test/accuracy': 0.22340001165866852, 'test/loss': 3.946427583694458, 'test/num_examples': 10000, 'score': 6336.788841247559, 'total_duration': 6971.531793117523, 'accumulated_submission_time': 6336.788841247559, 'accumulated_eval_time': 633.5849332809448, 'accumulated_logging_time': 0.42644309997558594, 'global_step': 13749, 'preemption_count': 0}), (14674, {'train/accuracy': 0.3084179759025574, 'train/loss': 3.319817066192627, 'validation/accuracy': 0.28895998001098633, 'validation/loss': 3.429847478866577, 'validation/num_examples': 50000, 'test/accuracy': 0.2208000123500824, 'test/loss': 3.9683961868286133, 'test/num_examples': 10000, 'score': 6756.758174657822, 'total_duration': 7428.526203870773, 'accumulated_submission_time': 6756.758174657822, 'accumulated_eval_time': 670.5265691280365, 'accumulated_logging_time': 0.46064305305480957, 'global_step': 14674, 'preemption_count': 0}), (15597, {'train/accuracy': 0.31492185592651367, 'train/loss': 3.302459239959717, 'validation/accuracy': 0.2924000024795532, 'validation/loss': 3.4336822032928467, 'validation/num_examples': 50000, 'test/accuracy': 0.22180001437664032, 'test/loss': 3.960664987564087, 'test/num_examples': 10000, 'score': 7176.900419473648, 'total_duration': 7890.88374376297, 'accumulated_submission_time': 7176.900419473648, 'accumulated_eval_time': 712.6638605594635, 'accumulated_logging_time': 0.490314245223999, 'global_step': 15597, 'preemption_count': 0}), (16518, {'train/accuracy': 0.3135937452316284, 'train/loss': 3.3022851943969727, 'validation/accuracy': 0.28995999693870544, 'validation/loss': 3.4567174911499023, 'validation/num_examples': 50000, 'test/accuracy': 0.21640001237392426, 'test/loss': 4.005506992340088, 'test/num_examples': 10000, 'score': 7597.258638143539, 'total_duration': 8348.557153463364, 'accumulated_submission_time': 7597.258638143539, 'accumulated_eval_time': 749.902556180954, 'accumulated_logging_time': 0.5184402465820312, 'global_step': 16518, 'preemption_count': 0}), (17442, {'train/accuracy': 0.34935545921325684, 'train/loss': 3.045586585998535, 'validation/accuracy': 0.311519980430603, 'validation/loss': 3.2577993869781494, 'validation/num_examples': 50000, 'test/accuracy': 0.24160000681877136, 'test/loss': 3.8327126502990723, 'test/num_examples': 10000, 'score': 8017.4315502643585, 'total_duration': 8809.659813642502, 'accumulated_submission_time': 8017.4315502643585, 'accumulated_eval_time': 790.755383014679, 'accumulated_logging_time': 0.5460658073425293, 'global_step': 17442, 'preemption_count': 0}), (18365, {'train/accuracy': 0.33476561307907104, 'train/loss': 3.173603057861328, 'validation/accuracy': 0.3100999891757965, 'validation/loss': 3.299381732940674, 'validation/num_examples': 50000, 'test/accuracy': 0.2379000186920166, 'test/loss': 3.853330612182617, 'test/num_examples': 10000, 'score': 8437.51029419899, 'total_duration': 9269.958486557007, 'accumulated_submission_time': 8437.51029419899, 'accumulated_eval_time': 830.8956499099731, 'accumulated_logging_time': 0.5777482986450195, 'global_step': 18365, 'preemption_count': 0}), (19286, {'train/accuracy': 0.328125, 'train/loss': 3.206876516342163, 'validation/accuracy': 0.30417999625205994, 'validation/loss': 3.352670431137085, 'validation/num_examples': 50000, 'test/accuracy': 0.234700009226799, 'test/loss': 3.8799962997436523, 'test/num_examples': 10000, 'score': 8857.471648454666, 'total_duration': 9726.6599547863, 'accumulated_submission_time': 8857.471648454666, 'accumulated_eval_time': 867.5589497089386, 'accumulated_logging_time': 0.6066219806671143, 'global_step': 19286, 'preemption_count': 0}), (20209, {'train/accuracy': 0.3524218797683716, 'train/loss': 3.0684006214141846, 'validation/accuracy': 0.311379998922348, 'validation/loss': 3.3076298236846924, 'validation/num_examples': 50000, 'test/accuracy': 0.2412000149488449, 'test/loss': 3.833801746368408, 'test/num_examples': 10000, 'score': 9277.629351854324, 'total_duration': 10190.143986225128, 'accumulated_submission_time': 9277.629351854324, 'accumulated_eval_time': 910.8073680400848, 'accumulated_logging_time': 0.6355025768280029, 'global_step': 20209, 'preemption_count': 0}), (21131, {'train/accuracy': 0.3587304651737213, 'train/loss': 3.0196619033813477, 'validation/accuracy': 0.33785998821258545, 'validation/loss': 3.1473286151885986, 'validation/num_examples': 50000, 'test/accuracy': 0.25600001215934753, 'test/loss': 3.7276759147644043, 'test/num_examples': 10000, 'score': 9697.635033369064, 'total_duration': 10655.167280435562, 'accumulated_submission_time': 9697.635033369064, 'accumulated_eval_time': 955.7451527118683, 'accumulated_logging_time': 0.6662991046905518, 'global_step': 21131, 'preemption_count': 0}), (22053, {'train/accuracy': 0.34773436188697815, 'train/loss': 3.0713601112365723, 'validation/accuracy': 0.3216799795627594, 'validation/loss': 3.2210161685943604, 'validation/num_examples': 50000, 'test/accuracy': 0.2412000149488449, 'test/loss': 3.8103978633880615, 'test/num_examples': 10000, 'score': 10117.811880588531, 'total_duration': 11114.772018909454, 'accumulated_submission_time': 10117.811880588531, 'accumulated_eval_time': 995.0893120765686, 'accumulated_logging_time': 0.700777530670166, 'global_step': 22053, 'preemption_count': 0}), (22976, {'train/accuracy': 0.3622460961341858, 'train/loss': 2.9715335369110107, 'validation/accuracy': 0.3338399827480316, 'validation/loss': 3.14302396774292, 'validation/num_examples': 50000, 'test/accuracy': 0.25540000200271606, 'test/loss': 3.7081446647644043, 'test/num_examples': 10000, 'score': 10538.091529846191, 'total_duration': 11576.040191173553, 'accumulated_submission_time': 10538.091529846191, 'accumulated_eval_time': 1035.9921073913574, 'accumulated_logging_time': 0.7372596263885498, 'global_step': 22976, 'preemption_count': 0}), (23900, {'train/accuracy': 0.34919920563697815, 'train/loss': 3.0883233547210693, 'validation/accuracy': 0.3263799846172333, 'validation/loss': 3.207261085510254, 'validation/num_examples': 50000, 'test/accuracy': 0.25140002369880676, 'test/loss': 3.753178596496582, 'test/num_examples': 10000, 'score': 10958.286712884903, 'total_duration': 12036.120604991913, 'accumulated_submission_time': 10958.286712884903, 'accumulated_eval_time': 1075.7954070568085, 'accumulated_logging_time': 0.7705366611480713, 'global_step': 23900, 'preemption_count': 0}), (24825, {'train/accuracy': 0.3492382764816284, 'train/loss': 3.0505244731903076, 'validation/accuracy': 0.32909998297691345, 'validation/loss': 3.18064284324646, 'validation/num_examples': 50000, 'test/accuracy': 0.24990001320838928, 'test/loss': 3.7398154735565186, 'test/num_examples': 10000, 'score': 11378.547968149185, 'total_duration': 12498.776052236557, 'accumulated_submission_time': 11378.547968149185, 'accumulated_eval_time': 1118.106077671051, 'accumulated_logging_time': 0.8033137321472168, 'global_step': 24825, 'preemption_count': 0}), (25746, {'train/accuracy': 0.36884763836860657, 'train/loss': 2.933948516845703, 'validation/accuracy': 0.33541998267173767, 'validation/loss': 3.1056737899780273, 'validation/num_examples': 50000, 'test/accuracy': 0.255700021982193, 'test/loss': 3.705024480819702, 'test/num_examples': 10000, 'score': 11798.491114139557, 'total_duration': 12955.351569652557, 'accumulated_submission_time': 11798.491114139557, 'accumulated_eval_time': 1154.6575469970703, 'accumulated_logging_time': 0.8338415622711182, 'global_step': 25746, 'preemption_count': 0}), (26668, {'train/accuracy': 0.37541013956069946, 'train/loss': 2.905060052871704, 'validation/accuracy': 0.3468399941921234, 'validation/loss': 3.0606956481933594, 'validation/num_examples': 50000, 'test/accuracy': 0.27080002427101135, 'test/loss': 3.6579036712646484, 'test/num_examples': 10000, 'score': 12218.910951375961, 'total_duration': 13413.987750291824, 'accumulated_submission_time': 12218.910951375961, 'accumulated_eval_time': 1192.788717508316, 'accumulated_logging_time': 0.8701872825622559, 'global_step': 26668, 'preemption_count': 0}), (27590, {'train/accuracy': 0.37593749165534973, 'train/loss': 2.8957693576812744, 'validation/accuracy': 0.3487599790096283, 'validation/loss': 3.039360284805298, 'validation/num_examples': 50000, 'test/accuracy': 0.2639000117778778, 'test/loss': 3.661264181137085, 'test/num_examples': 10000, 'score': 12638.921879768372, 'total_duration': 13875.224354982376, 'accumulated_submission_time': 12638.921879768372, 'accumulated_eval_time': 1233.934255361557, 'accumulated_logging_time': 0.9021317958831787, 'global_step': 27590, 'preemption_count': 0}), (28514, {'train/accuracy': 0.3650195300579071, 'train/loss': 2.984952926635742, 'validation/accuracy': 0.3395199775695801, 'validation/loss': 3.1396758556365967, 'validation/num_examples': 50000, 'test/accuracy': 0.25620001554489136, 'test/loss': 3.7381186485290527, 'test/num_examples': 10000, 'score': 13059.063136339188, 'total_duration': 14332.19144487381, 'accumulated_submission_time': 13059.063136339188, 'accumulated_eval_time': 1270.6803524494171, 'accumulated_logging_time': 0.9330759048461914, 'global_step': 28514, 'preemption_count': 0}), (29437, {'train/accuracy': 0.39070311188697815, 'train/loss': 2.8178062438964844, 'validation/accuracy': 0.3384000062942505, 'validation/loss': 3.126830577850342, 'validation/num_examples': 50000, 'test/accuracy': 0.2629000246524811, 'test/loss': 3.685798168182373, 'test/num_examples': 10000, 'score': 13479.126105070114, 'total_duration': 14793.215996265411, 'accumulated_submission_time': 13479.126105070114, 'accumulated_eval_time': 1311.5622823238373, 'accumulated_logging_time': 0.9640073776245117, 'global_step': 29437, 'preemption_count': 0}), (30356, {'train/accuracy': 0.37205076217651367, 'train/loss': 2.9231514930725098, 'validation/accuracy': 0.346699982881546, 'validation/loss': 3.082463026046753, 'validation/num_examples': 50000, 'test/accuracy': 0.2647000253200531, 'test/loss': 3.649627447128296, 'test/num_examples': 10000, 'score': 13899.084905862808, 'total_duration': 15251.751507043839, 'accumulated_submission_time': 13899.084905862808, 'accumulated_eval_time': 1350.0582914352417, 'accumulated_logging_time': 0.9966273307800293, 'global_step': 30356, 'preemption_count': 0}), (31278, {'train/accuracy': 0.3739062547683716, 'train/loss': 2.9314498901367188, 'validation/accuracy': 0.34731999039649963, 'validation/loss': 3.0704736709594727, 'validation/num_examples': 50000, 'test/accuracy': 0.2718999981880188, 'test/loss': 3.6703150272369385, 'test/num_examples': 10000, 'score': 14319.371235609055, 'total_duration': 15711.486956119537, 'accumulated_submission_time': 14319.371235609055, 'accumulated_eval_time': 1389.423010110855, 'accumulated_logging_time': 1.0317187309265137, 'global_step': 31278, 'preemption_count': 0}), (32202, {'train/accuracy': 0.40431639552116394, 'train/loss': 2.71505069732666, 'validation/accuracy': 0.3649599850177765, 'validation/loss': 2.9368364810943604, 'validation/num_examples': 50000, 'test/accuracy': 0.2857000231742859, 'test/loss': 3.5437114238739014, 'test/num_examples': 10000, 'score': 14739.30179309845, 'total_duration': 16172.872477769852, 'accumulated_submission_time': 14739.30179309845, 'accumulated_eval_time': 1430.7917094230652, 'accumulated_logging_time': 1.0686018466949463, 'global_step': 32202, 'preemption_count': 0}), (33126, {'train/accuracy': 0.39134764671325684, 'train/loss': 2.812678575515747, 'validation/accuracy': 0.36403998732566833, 'validation/loss': 2.9665307998657227, 'validation/num_examples': 50000, 'test/accuracy': 0.2803000211715698, 'test/loss': 3.540454387664795, 'test/num_examples': 10000, 'score': 15159.38614320755, 'total_duration': 16632.36226463318, 'accumulated_submission_time': 15159.38614320755, 'accumulated_eval_time': 1470.1178567409515, 'accumulated_logging_time': 1.098562479019165, 'global_step': 33126, 'preemption_count': 0}), (34048, {'train/accuracy': 0.382148414850235, 'train/loss': 2.845215082168579, 'validation/accuracy': 0.3581399917602539, 'validation/loss': 2.999690532684326, 'validation/num_examples': 50000, 'test/accuracy': 0.2736000120639801, 'test/loss': 3.5886569023132324, 'test/num_examples': 10000, 'score': 15579.406750202179, 'total_duration': 17091.41108250618, 'accumulated_submission_time': 15579.406750202179, 'accumulated_eval_time': 1509.0675301551819, 'accumulated_logging_time': 1.1289377212524414, 'global_step': 34048, 'preemption_count': 0}), (34973, {'train/accuracy': 0.39232420921325684, 'train/loss': 2.8107075691223145, 'validation/accuracy': 0.3644599914550781, 'validation/loss': 2.975851058959961, 'validation/num_examples': 50000, 'test/accuracy': 0.2762000262737274, 'test/loss': 3.5893852710723877, 'test/num_examples': 10000, 'score': 15999.715313196182, 'total_duration': 17553.633061647415, 'accumulated_submission_time': 15999.715313196182, 'accumulated_eval_time': 1550.8924548625946, 'accumulated_logging_time': 1.1684050559997559, 'global_step': 34973, 'preemption_count': 0}), (35899, {'train/accuracy': 0.399726539850235, 'train/loss': 2.7130661010742188, 'validation/accuracy': 0.3743399977684021, 'validation/loss': 2.8684468269348145, 'validation/num_examples': 50000, 'test/accuracy': 0.2859000265598297, 'test/loss': 3.4763166904449463, 'test/num_examples': 10000, 'score': 16419.95436644554, 'total_duration': 18018.215858459473, 'accumulated_submission_time': 16419.95436644554, 'accumulated_eval_time': 1595.1521625518799, 'accumulated_logging_time': 1.2027950286865234, 'global_step': 35899, 'preemption_count': 0}), (36822, {'train/accuracy': 0.4059179723262787, 'train/loss': 2.7009963989257812, 'validation/accuracy': 0.37731999158859253, 'validation/loss': 2.8525683879852295, 'validation/num_examples': 50000, 'test/accuracy': 0.28930002450942993, 'test/loss': 3.473834991455078, 'test/num_examples': 10000, 'score': 16840.155430793762, 'total_duration': 18478.654767751694, 'accumulated_submission_time': 16840.155430793762, 'accumulated_eval_time': 1635.3084263801575, 'accumulated_logging_time': 1.2354793548583984, 'global_step': 36822, 'preemption_count': 0}), (37742, {'train/accuracy': 0.41218748688697815, 'train/loss': 2.6718852519989014, 'validation/accuracy': 0.38161998987197876, 'validation/loss': 2.845273971557617, 'validation/num_examples': 50000, 'test/accuracy': 0.30150002241134644, 'test/loss': 3.418804407119751, 'test/num_examples': 10000, 'score': 17260.433208703995, 'total_duration': 18941.170355796814, 'accumulated_submission_time': 17260.433208703995, 'accumulated_eval_time': 1677.465342760086, 'accumulated_logging_time': 1.268315076828003, 'global_step': 37742, 'preemption_count': 0}), (38663, {'train/accuracy': 0.4493359327316284, 'train/loss': 2.501319169998169, 'validation/accuracy': 0.38349997997283936, 'validation/loss': 2.8455910682678223, 'validation/num_examples': 50000, 'test/accuracy': 0.2939000129699707, 'test/loss': 3.432493209838867, 'test/num_examples': 10000, 'score': 17680.66015148163, 'total_duration': 19399.91322350502, 'accumulated_submission_time': 17680.66015148163, 'accumulated_eval_time': 1715.8985350131989, 'accumulated_logging_time': 1.3021540641784668, 'global_step': 38663, 'preemption_count': 0}), (39587, {'train/accuracy': 0.3864062428474426, 'train/loss': 2.9066293239593506, 'validation/accuracy': 0.3610599935054779, 'validation/loss': 3.0364768505096436, 'validation/num_examples': 50000, 'test/accuracy': 0.26750001311302185, 'test/loss': 3.6234524250030518, 'test/num_examples': 10000, 'score': 18100.73324918747, 'total_duration': 19860.374609947205, 'accumulated_submission_time': 18100.73324918747, 'accumulated_eval_time': 1756.2016875743866, 'accumulated_logging_time': 1.3390777111053467, 'global_step': 39587, 'preemption_count': 0}), (40510, {'train/accuracy': 0.40166014432907104, 'train/loss': 2.776913642883301, 'validation/accuracy': 0.3719799816608429, 'validation/loss': 2.9364328384399414, 'validation/num_examples': 50000, 'test/accuracy': 0.289900004863739, 'test/loss': 3.5260701179504395, 'test/num_examples': 10000, 'score': 18521.109936714172, 'total_duration': 20318.680195093155, 'accumulated_submission_time': 18521.109936714172, 'accumulated_eval_time': 1794.0471782684326, 'accumulated_logging_time': 1.3741655349731445, 'global_step': 40510, 'preemption_count': 0}), (41434, {'train/accuracy': 0.423164039850235, 'train/loss': 2.5967025756835938, 'validation/accuracy': 0.3805999755859375, 'validation/loss': 2.8329660892486572, 'validation/num_examples': 50000, 'test/accuracy': 0.2957000136375427, 'test/loss': 3.4568240642547607, 'test/num_examples': 10000, 'score': 18941.358196496964, 'total_duration': 20777.212375164032, 'accumulated_submission_time': 18941.358196496964, 'accumulated_eval_time': 1832.2530777454376, 'accumulated_logging_time': 1.404280662536621, 'global_step': 41434, 'preemption_count': 0}), (42358, {'train/accuracy': 0.4092773199081421, 'train/loss': 2.6755664348602295, 'validation/accuracy': 0.38871997594833374, 'validation/loss': 2.8146812915802, 'validation/num_examples': 50000, 'test/accuracy': 0.29920002818107605, 'test/loss': 3.452272891998291, 'test/num_examples': 10000, 'score': 19361.352199077606, 'total_duration': 21236.003611803055, 'accumulated_submission_time': 19361.352199077606, 'accumulated_eval_time': 1870.96342420578, 'accumulated_logging_time': 1.4418244361877441, 'global_step': 42358, 'preemption_count': 0}), (43280, {'train/accuracy': 0.41355466842651367, 'train/loss': 2.665776252746582, 'validation/accuracy': 0.38312000036239624, 'validation/loss': 2.8275225162506104, 'validation/num_examples': 50000, 'test/accuracy': 0.2969000041484833, 'test/loss': 3.4342548847198486, 'test/num_examples': 10000, 'score': 19781.59530377388, 'total_duration': 21693.95869255066, 'accumulated_submission_time': 19781.59530377388, 'accumulated_eval_time': 1908.5935270786285, 'accumulated_logging_time': 1.474895715713501, 'global_step': 43280, 'preemption_count': 0}), (44204, {'train/accuracy': 0.4298437535762787, 'train/loss': 2.6165740489959717, 'validation/accuracy': 0.3955000042915344, 'validation/loss': 2.792757034301758, 'validation/num_examples': 50000, 'test/accuracy': 0.305400013923645, 'test/loss': 3.4004056453704834, 'test/num_examples': 10000, 'score': 20201.73545074463, 'total_duration': 22153.340349674225, 'accumulated_submission_time': 20201.73545074463, 'accumulated_eval_time': 1947.7522237300873, 'accumulated_logging_time': 1.50919771194458, 'global_step': 44204, 'preemption_count': 0}), (45130, {'train/accuracy': 0.41789060831069946, 'train/loss': 2.693882465362549, 'validation/accuracy': 0.3900199830532074, 'validation/loss': 2.8285815715789795, 'validation/num_examples': 50000, 'test/accuracy': 0.3004000186920166, 'test/loss': 3.4367313385009766, 'test/num_examples': 10000, 'score': 20621.955298423767, 'total_duration': 22613.75858616829, 'accumulated_submission_time': 20621.955298423767, 'accumulated_eval_time': 1987.8704631328583, 'accumulated_logging_time': 1.5406432151794434, 'global_step': 45130, 'preemption_count': 0}), (46053, {'train/accuracy': 0.42689451575279236, 'train/loss': 2.6220920085906982, 'validation/accuracy': 0.39969998598098755, 'validation/loss': 2.773547410964966, 'validation/num_examples': 50000, 'test/accuracy': 0.3076000213623047, 'test/loss': 3.3951525688171387, 'test/num_examples': 10000, 'score': 21041.939405441284, 'total_duration': 23074.925387620926, 'accumulated_submission_time': 21041.939405441284, 'accumulated_eval_time': 2028.9680247306824, 'accumulated_logging_time': 1.5772721767425537, 'global_step': 46053, 'preemption_count': 0}), (46976, {'train/accuracy': 0.4323046803474426, 'train/loss': 2.5363075733184814, 'validation/accuracy': 0.405239999294281, 'validation/loss': 2.6976065635681152, 'validation/num_examples': 50000, 'test/accuracy': 0.31460002064704895, 'test/loss': 3.3456552028656006, 'test/num_examples': 10000, 'score': 21462.19115138054, 'total_duration': 23535.27784347534, 'accumulated_submission_time': 21462.19115138054, 'accumulated_eval_time': 2068.98393702507, 'accumulated_logging_time': 1.6132659912109375, 'global_step': 46976, 'preemption_count': 0}), (47901, {'train/accuracy': 0.45130857825279236, 'train/loss': 2.50014066696167, 'validation/accuracy': 0.3981199860572815, 'validation/loss': 2.7976644039154053, 'validation/num_examples': 50000, 'test/accuracy': 0.2980000078678131, 'test/loss': 3.4122321605682373, 'test/num_examples': 10000, 'score': 21882.690141916275, 'total_duration': 23995.60861515999, 'accumulated_submission_time': 21882.690141916275, 'accumulated_eval_time': 2108.731509923935, 'accumulated_logging_time': 1.6486258506774902, 'global_step': 47901, 'preemption_count': 0}), (48825, {'train/accuracy': 0.4263085722923279, 'train/loss': 2.605025053024292, 'validation/accuracy': 0.4018400013446808, 'validation/loss': 2.7520787715911865, 'validation/num_examples': 50000, 'test/accuracy': 0.3126000165939331, 'test/loss': 3.3366458415985107, 'test/num_examples': 10000, 'score': 22302.164969682693, 'total_duration': 24458.163280963898, 'accumulated_submission_time': 22302.164969682693, 'accumulated_eval_time': 2151.260172843933, 'accumulated_logging_time': 2.1510133743286133, 'global_step': 48825, 'preemption_count': 0}), (49748, {'train/accuracy': 0.4362890422344208, 'train/loss': 2.5398664474487305, 'validation/accuracy': 0.40761998295783997, 'validation/loss': 2.714684247970581, 'validation/num_examples': 50000, 'test/accuracy': 0.31700000166893005, 'test/loss': 3.326667070388794, 'test/num_examples': 10000, 'score': 22722.07783985138, 'total_duration': 24922.15366792679, 'accumulated_submission_time': 22722.07783985138, 'accumulated_eval_time': 2195.242655277252, 'accumulated_logging_time': 2.196490526199341, 'global_step': 49748, 'preemption_count': 0}), (50670, {'train/accuracy': 0.4491601586341858, 'train/loss': 2.490548610687256, 'validation/accuracy': 0.402319997549057, 'validation/loss': 2.752784013748169, 'validation/num_examples': 50000, 'test/accuracy': 0.3070000112056732, 'test/loss': 3.3801825046539307, 'test/num_examples': 10000, 'score': 23142.06842494011, 'total_duration': 25379.83161520958, 'accumulated_submission_time': 23142.06842494011, 'accumulated_eval_time': 2232.8479537963867, 'accumulated_logging_time': 2.2305619716644287, 'global_step': 50670, 'preemption_count': 0}), (51593, {'train/accuracy': 0.4297265410423279, 'train/loss': 2.5958352088928223, 'validation/accuracy': 0.4063799977302551, 'validation/loss': 2.737758159637451, 'validation/num_examples': 50000, 'test/accuracy': 0.31530001759529114, 'test/loss': 3.344174385070801, 'test/num_examples': 10000, 'score': 23562.160203695297, 'total_duration': 25843.881860017776, 'accumulated_submission_time': 23562.160203695297, 'accumulated_eval_time': 2276.712729215622, 'accumulated_logging_time': 2.2754602432250977, 'global_step': 51593, 'preemption_count': 0}), (52517, {'train/accuracy': 0.43990233540534973, 'train/loss': 2.5211288928985596, 'validation/accuracy': 0.4089199900627136, 'validation/loss': 2.6849489212036133, 'validation/num_examples': 50000, 'test/accuracy': 0.32100000977516174, 'test/loss': 3.283968210220337, 'test/num_examples': 10000, 'score': 23982.16872549057, 'total_duration': 26306.350472450256, 'accumulated_submission_time': 23982.16872549057, 'accumulated_eval_time': 2319.089858531952, 'accumulated_logging_time': 2.310560941696167, 'global_step': 52517, 'preemption_count': 0}), (53440, {'train/accuracy': 0.4496484398841858, 'train/loss': 2.463557720184326, 'validation/accuracy': 0.41543999314308167, 'validation/loss': 2.6775717735290527, 'validation/num_examples': 50000, 'test/accuracy': 0.32760000228881836, 'test/loss': 3.283280611038208, 'test/num_examples': 10000, 'score': 24402.401746749878, 'total_duration': 26766.471939086914, 'accumulated_submission_time': 24402.401746749878, 'accumulated_eval_time': 2358.8874881267548, 'accumulated_logging_time': 2.348323106765747, 'global_step': 53440, 'preemption_count': 0}), (54363, {'train/accuracy': 0.43671873211860657, 'train/loss': 2.5562655925750732, 'validation/accuracy': 0.4078799784183502, 'validation/loss': 2.7128429412841797, 'validation/num_examples': 50000, 'test/accuracy': 0.3160000145435333, 'test/loss': 3.333500862121582, 'test/num_examples': 10000, 'score': 24822.649163007736, 'total_duration': 27226.17301273346, 'accumulated_submission_time': 24822.649163007736, 'accumulated_eval_time': 2398.257269382477, 'accumulated_logging_time': 2.3831875324249268, 'global_step': 54363, 'preemption_count': 0}), (55287, {'train/accuracy': 0.44783201813697815, 'train/loss': 2.4865450859069824, 'validation/accuracy': 0.42361998558044434, 'validation/loss': 2.6358258724212646, 'validation/num_examples': 50000, 'test/accuracy': 0.3257000148296356, 'test/loss': 3.252009153366089, 'test/num_examples': 10000, 'score': 25242.692935943604, 'total_duration': 27688.975242853165, 'accumulated_submission_time': 25242.692935943604, 'accumulated_eval_time': 2440.921109676361, 'accumulated_logging_time': 2.425750970840454, 'global_step': 55287, 'preemption_count': 0}), (56210, {'train/accuracy': 0.4586718678474426, 'train/loss': 2.4052109718322754, 'validation/accuracy': 0.42155998945236206, 'validation/loss': 2.599883794784546, 'validation/num_examples': 50000, 'test/accuracy': 0.326200008392334, 'test/loss': 3.243481397628784, 'test/num_examples': 10000, 'score': 25662.820051670074, 'total_duration': 28149.439738035202, 'accumulated_submission_time': 25662.820051670074, 'accumulated_eval_time': 2481.1670627593994, 'accumulated_logging_time': 2.4647953510284424, 'global_step': 56210, 'preemption_count': 0}), (57131, {'train/accuracy': 0.4611132740974426, 'train/loss': 2.4022233486175537, 'validation/accuracy': 0.4213999807834625, 'validation/loss': 2.619320869445801, 'validation/num_examples': 50000, 'test/accuracy': 0.329800009727478, 'test/loss': 3.234283447265625, 'test/num_examples': 10000, 'score': 26083.149917840958, 'total_duration': 28611.141579151154, 'accumulated_submission_time': 26083.149917840958, 'accumulated_eval_time': 2522.4550380706787, 'accumulated_logging_time': 2.499359130859375, 'global_step': 57131, 'preemption_count': 0}), (58055, {'train/accuracy': 0.44603514671325684, 'train/loss': 2.520827054977417, 'validation/accuracy': 0.4186599850654602, 'validation/loss': 2.6796395778656006, 'validation/num_examples': 50000, 'test/accuracy': 0.3215000033378601, 'test/loss': 3.2995548248291016, 'test/num_examples': 10000, 'score': 26503.37421274185, 'total_duration': 29074.28104519844, 'accumulated_submission_time': 26503.37421274185, 'accumulated_eval_time': 2565.287081718445, 'accumulated_logging_time': 2.5334596633911133, 'global_step': 58055, 'preemption_count': 0}), (58979, {'train/accuracy': 0.46378904581069946, 'train/loss': 2.4174797534942627, 'validation/accuracy': 0.4327999949455261, 'validation/loss': 2.582634925842285, 'validation/num_examples': 50000, 'test/accuracy': 0.3387000262737274, 'test/loss': 3.2149226665496826, 'test/num_examples': 10000, 'score': 26923.577298164368, 'total_duration': 29535.1870803833, 'accumulated_submission_time': 26923.577298164368, 'accumulated_eval_time': 2605.8996703624725, 'accumulated_logging_time': 2.574249744415283, 'global_step': 58979, 'preemption_count': 0}), (59903, {'train/accuracy': 0.47880858182907104, 'train/loss': 2.342374563217163, 'validation/accuracy': 0.42433997988700867, 'validation/loss': 2.625638961791992, 'validation/num_examples': 50000, 'test/accuracy': 0.3347000181674957, 'test/loss': 3.231863021850586, 'test/num_examples': 10000, 'score': 27343.765065908432, 'total_duration': 29996.09737586975, 'accumulated_submission_time': 27343.765065908432, 'accumulated_eval_time': 2646.5336713790894, 'accumulated_logging_time': 2.614187479019165, 'global_step': 59903, 'preemption_count': 0}), (60824, {'train/accuracy': 0.4606054723262787, 'train/loss': 2.44146728515625, 'validation/accuracy': 0.42865997552871704, 'validation/loss': 2.598587989807129, 'validation/num_examples': 50000, 'test/accuracy': 0.3337000012397766, 'test/loss': 3.2204599380493164, 'test/num_examples': 10000, 'score': 27763.803510665894, 'total_duration': 30458.07638692856, 'accumulated_submission_time': 27763.803510665894, 'accumulated_eval_time': 2688.3805034160614, 'accumulated_logging_time': 2.658954620361328, 'global_step': 60824, 'preemption_count': 0}), (61748, {'train/accuracy': 0.4458398222923279, 'train/loss': 2.5256237983703613, 'validation/accuracy': 0.41613999009132385, 'validation/loss': 2.6907958984375, 'validation/num_examples': 50000, 'test/accuracy': 0.3223000168800354, 'test/loss': 3.279371738433838, 'test/num_examples': 10000, 'score': 28183.8014895916, 'total_duration': 30917.355994701385, 'accumulated_submission_time': 28183.8014895916, 'accumulated_eval_time': 2727.574634075165, 'accumulated_logging_time': 2.6977574825286865, 'global_step': 61748, 'preemption_count': 0}), (62670, {'train/accuracy': 0.47507810592651367, 'train/loss': 2.3306970596313477, 'validation/accuracy': 0.43865999579429626, 'validation/loss': 2.5384957790374756, 'validation/num_examples': 50000, 'test/accuracy': 0.3391000032424927, 'test/loss': 3.1987009048461914, 'test/num_examples': 10000, 'score': 28603.97628927231, 'total_duration': 31379.021688699722, 'accumulated_submission_time': 28603.97628927231, 'accumulated_eval_time': 2768.981840610504, 'accumulated_logging_time': 2.7327027320861816, 'global_step': 62670, 'preemption_count': 0}), (63592, {'train/accuracy': 0.4749414026737213, 'train/loss': 2.3414981365203857, 'validation/accuracy': 0.44200000166893005, 'validation/loss': 2.4970359802246094, 'validation/num_examples': 50000, 'test/accuracy': 0.34150001406669617, 'test/loss': 3.151350259780884, 'test/num_examples': 10000, 'score': 29023.923259735107, 'total_duration': 31837.474231004715, 'accumulated_submission_time': 29023.923259735107, 'accumulated_eval_time': 2807.4011034965515, 'accumulated_logging_time': 2.7702078819274902, 'global_step': 63592, 'preemption_count': 0}), (64514, {'train/accuracy': 0.4628320336341858, 'train/loss': 2.441080331802368, 'validation/accuracy': 0.43181997537612915, 'validation/loss': 2.605339527130127, 'validation/num_examples': 50000, 'test/accuracy': 0.3314000070095062, 'test/loss': 3.231776237487793, 'test/num_examples': 10000, 'score': 29443.90414404869, 'total_duration': 32297.551177978516, 'accumulated_submission_time': 29443.90414404869, 'accumulated_eval_time': 2847.411164045334, 'accumulated_logging_time': 2.8072707653045654, 'global_step': 64514, 'preemption_count': 0}), (65437, {'train/accuracy': 0.45894530415534973, 'train/loss': 2.488585948944092, 'validation/accuracy': 0.42405998706817627, 'validation/loss': 2.6650540828704834, 'validation/num_examples': 50000, 'test/accuracy': 0.3297000229358673, 'test/loss': 3.2900028228759766, 'test/num_examples': 10000, 'score': 29864.123959302902, 'total_duration': 32757.831778764725, 'accumulated_submission_time': 29864.123959302902, 'accumulated_eval_time': 2887.384506225586, 'accumulated_logging_time': 2.8465631008148193, 'global_step': 65437, 'preemption_count': 0}), (66360, {'train/accuracy': 0.46708983182907104, 'train/loss': 2.3807220458984375, 'validation/accuracy': 0.4341999888420105, 'validation/loss': 2.5594401359558105, 'validation/num_examples': 50000, 'test/accuracy': 0.3379000127315521, 'test/loss': 3.1919198036193848, 'test/num_examples': 10000, 'score': 30284.387431383133, 'total_duration': 33220.57291150093, 'accumulated_submission_time': 30284.387431383133, 'accumulated_eval_time': 2929.774935245514, 'accumulated_logging_time': 2.8840224742889404, 'global_step': 66360, 'preemption_count': 0}), (67284, {'train/accuracy': 0.46605467796325684, 'train/loss': 2.3730216026306152, 'validation/accuracy': 0.4387199878692627, 'validation/loss': 2.5333032608032227, 'validation/num_examples': 50000, 'test/accuracy': 0.3418000042438507, 'test/loss': 3.1739964485168457, 'test/num_examples': 10000, 'score': 30704.6074051857, 'total_duration': 33683.47833299637, 'accumulated_submission_time': 30704.6074051857, 'accumulated_eval_time': 2972.3694083690643, 'accumulated_logging_time': 2.92598557472229, 'global_step': 67284, 'preemption_count': 0}), (68207, {'train/accuracy': 0.4681445062160492, 'train/loss': 2.3622944355010986, 'validation/accuracy': 0.43553999066352844, 'validation/loss': 2.5348927974700928, 'validation/num_examples': 50000, 'test/accuracy': 0.33980002999305725, 'test/loss': 3.1727473735809326, 'test/num_examples': 10000, 'score': 31124.684980630875, 'total_duration': 34140.442061424255, 'accumulated_submission_time': 31124.684980630875, 'accumulated_eval_time': 3009.159858226776, 'accumulated_logging_time': 2.972452402114868, 'global_step': 68207, 'preemption_count': 0}), (69130, {'train/accuracy': 0.4894726574420929, 'train/loss': 2.2886223793029785, 'validation/accuracy': 0.43111997842788696, 'validation/loss': 2.5940520763397217, 'validation/num_examples': 50000, 'test/accuracy': 0.33150002360343933, 'test/loss': 3.2263267040252686, 'test/num_examples': 10000, 'score': 31545.37599992752, 'total_duration': 34600.265894174576, 'accumulated_submission_time': 31545.37599992752, 'accumulated_eval_time': 3048.2066645622253, 'accumulated_logging_time': 3.0094172954559326, 'global_step': 69130, 'preemption_count': 0}), (70053, {'train/accuracy': 0.48564451932907104, 'train/loss': 2.2598440647125244, 'validation/accuracy': 0.45795997977256775, 'validation/loss': 2.3936898708343506, 'validation/num_examples': 50000, 'test/accuracy': 0.35690000653266907, 'test/loss': 3.05293607711792, 'test/num_examples': 10000, 'score': 31965.58531689644, 'total_duration': 35062.400102853775, 'accumulated_submission_time': 31965.58531689644, 'accumulated_eval_time': 3090.0443358421326, 'accumulated_logging_time': 3.047203779220581, 'global_step': 70053, 'preemption_count': 0}), (70977, {'train/accuracy': 0.482421875, 'train/loss': 2.2963407039642334, 'validation/accuracy': 0.44609999656677246, 'validation/loss': 2.483259916305542, 'validation/num_examples': 50000, 'test/accuracy': 0.349700003862381, 'test/loss': 3.116337776184082, 'test/num_examples': 10000, 'score': 32385.602464437485, 'total_duration': 35520.32847523689, 'accumulated_submission_time': 32385.602464437485, 'accumulated_eval_time': 3127.865446805954, 'accumulated_logging_time': 3.0879249572753906, 'global_step': 70977, 'preemption_count': 0}), (71899, {'train/accuracy': 0.49373045563697815, 'train/loss': 2.219071626663208, 'validation/accuracy': 0.4499399960041046, 'validation/loss': 2.4469850063323975, 'validation/num_examples': 50000, 'test/accuracy': 0.34880000352859497, 'test/loss': 3.096506118774414, 'test/num_examples': 10000, 'score': 32805.70732951164, 'total_duration': 35982.90106034279, 'accumulated_submission_time': 32805.70732951164, 'accumulated_eval_time': 3170.2405710220337, 'accumulated_logging_time': 3.131728172302246, 'global_step': 71899, 'preemption_count': 0}), (72822, {'train/accuracy': 0.4931640625, 'train/loss': 2.2360095977783203, 'validation/accuracy': 0.46393999457359314, 'validation/loss': 2.3808131217956543, 'validation/num_examples': 50000, 'test/accuracy': 0.3605000078678131, 'test/loss': 3.04154372215271, 'test/num_examples': 10000, 'score': 33225.77615022659, 'total_duration': 36445.34771442413, 'accumulated_submission_time': 33225.77615022659, 'accumulated_eval_time': 3212.5319378376007, 'accumulated_logging_time': 3.1703007221221924, 'global_step': 72822, 'preemption_count': 0}), (73744, {'train/accuracy': 0.48167967796325684, 'train/loss': 2.3041117191314697, 'validation/accuracy': 0.45201998949050903, 'validation/loss': 2.4658422470092773, 'validation/num_examples': 50000, 'test/accuracy': 0.3540000021457672, 'test/loss': 3.107847213745117, 'test/num_examples': 10000, 'score': 33646.14836072922, 'total_duration': 36902.12452173233, 'accumulated_submission_time': 33646.14836072922, 'accumulated_eval_time': 3248.851069688797, 'accumulated_logging_time': 3.207165241241455, 'global_step': 73744, 'preemption_count': 0}), (74668, {'train/accuracy': 0.5001562237739563, 'train/loss': 2.175226926803589, 'validation/accuracy': 0.46567997336387634, 'validation/loss': 2.3801429271698, 'validation/num_examples': 50000, 'test/accuracy': 0.36100003123283386, 'test/loss': 3.0509543418884277, 'test/num_examples': 10000, 'score': 34066.290695905685, 'total_duration': 37362.36349225044, 'accumulated_submission_time': 34066.290695905685, 'accumulated_eval_time': 3288.8602344989777, 'accumulated_logging_time': 3.2456398010253906, 'global_step': 74668, 'preemption_count': 0}), (75590, {'train/accuracy': 0.49730467796325684, 'train/loss': 2.2028791904449463, 'validation/accuracy': 0.46421998739242554, 'validation/loss': 2.3795573711395264, 'validation/num_examples': 50000, 'test/accuracy': 0.36330002546310425, 'test/loss': 3.0194509029388428, 'test/num_examples': 10000, 'score': 34486.66932654381, 'total_duration': 37825.93365931511, 'accumulated_submission_time': 34486.66932654381, 'accumulated_eval_time': 3331.9556045532227, 'accumulated_logging_time': 3.2923521995544434, 'global_step': 75590, 'preemption_count': 0}), (76511, {'train/accuracy': 0.49726560711860657, 'train/loss': 2.211764097213745, 'validation/accuracy': 0.46473997831344604, 'validation/loss': 2.3848724365234375, 'validation/num_examples': 50000, 'test/accuracy': 0.3605000078678131, 'test/loss': 3.0538387298583984, 'test/num_examples': 10000, 'score': 34906.83700180054, 'total_duration': 38284.109989881516, 'accumulated_submission_time': 34906.83700180054, 'accumulated_eval_time': 3369.877052307129, 'accumulated_logging_time': 3.3313422203063965, 'global_step': 76511, 'preemption_count': 0}), (77432, {'train/accuracy': 0.5082421898841858, 'train/loss': 2.163210153579712, 'validation/accuracy': 0.46818000078201294, 'validation/loss': 2.3664684295654297, 'validation/num_examples': 50000, 'test/accuracy': 0.36820000410079956, 'test/loss': 3.0080747604370117, 'test/num_examples': 10000, 'score': 35326.80542087555, 'total_duration': 38746.585496902466, 'accumulated_submission_time': 35326.80542087555, 'accumulated_eval_time': 3412.295135498047, 'accumulated_logging_time': 3.3718671798706055, 'global_step': 77432, 'preemption_count': 0}), (78356, {'train/accuracy': 0.5389648079872131, 'train/loss': 2.0390491485595703, 'validation/accuracy': 0.47265997529029846, 'validation/loss': 2.385406494140625, 'validation/num_examples': 50000, 'test/accuracy': 0.37230002880096436, 'test/loss': 3.007336378097534, 'test/num_examples': 10000, 'score': 35746.876959085464, 'total_duration': 39210.46626496315, 'accumulated_submission_time': 35746.876959085464, 'accumulated_eval_time': 3456.013298511505, 'accumulated_logging_time': 3.413897752761841, 'global_step': 78356, 'preemption_count': 0}), (79280, {'train/accuracy': 0.5073632597923279, 'train/loss': 2.175447940826416, 'validation/accuracy': 0.4732399880886078, 'validation/loss': 2.356532573699951, 'validation/num_examples': 50000, 'test/accuracy': 0.3680000305175781, 'test/loss': 2.9997689723968506, 'test/num_examples': 10000, 'score': 36166.997678518295, 'total_duration': 39672.47076559067, 'accumulated_submission_time': 36166.997678518295, 'accumulated_eval_time': 3497.8065342903137, 'accumulated_logging_time': 3.4558606147766113, 'global_step': 79280, 'preemption_count': 0}), (80204, {'train/accuracy': 0.5040234327316284, 'train/loss': 2.234501361846924, 'validation/accuracy': 0.469760000705719, 'validation/loss': 2.405181884765625, 'validation/num_examples': 50000, 'test/accuracy': 0.3631000220775604, 'test/loss': 3.042214870452881, 'test/num_examples': 10000, 'score': 36587.06204032898, 'total_duration': 40131.91340637207, 'accumulated_submission_time': 36587.06204032898, 'accumulated_eval_time': 3537.0906777381897, 'accumulated_logging_time': 3.5011050701141357, 'global_step': 80204, 'preemption_count': 0}), (81127, {'train/accuracy': 0.5310351252555847, 'train/loss': 2.0206193923950195, 'validation/accuracy': 0.48405998945236206, 'validation/loss': 2.27791166305542, 'validation/num_examples': 50000, 'test/accuracy': 0.3806000053882599, 'test/loss': 2.9327199459075928, 'test/num_examples': 10000, 'score': 37007.36421537399, 'total_duration': 40593.58241772652, 'accumulated_submission_time': 37007.36421537399, 'accumulated_eval_time': 3578.3675305843353, 'accumulated_logging_time': 3.5425891876220703, 'global_step': 81127, 'preemption_count': 0}), (82046, {'train/accuracy': 0.5068749785423279, 'train/loss': 2.1524007320404053, 'validation/accuracy': 0.4747999906539917, 'validation/loss': 2.3249146938323975, 'validation/num_examples': 50000, 'test/accuracy': 0.3708000183105469, 'test/loss': 2.9745867252349854, 'test/num_examples': 10000, 'score': 37427.59137535095, 'total_duration': 41055.39542388916, 'accumulated_submission_time': 37427.59137535095, 'accumulated_eval_time': 3619.8660113811493, 'accumulated_logging_time': 3.581484079360962, 'global_step': 82046, 'preemption_count': 0}), (82966, {'train/accuracy': 0.5151953101158142, 'train/loss': 2.1322968006134033, 'validation/accuracy': 0.47915998101234436, 'validation/loss': 2.317836284637451, 'validation/num_examples': 50000, 'test/accuracy': 0.3758000135421753, 'test/loss': 2.9638731479644775, 'test/num_examples': 10000, 'score': 37847.527940034866, 'total_duration': 41518.515270233154, 'accumulated_submission_time': 37847.527940034866, 'accumulated_eval_time': 3662.954482316971, 'accumulated_logging_time': 3.6278164386749268, 'global_step': 82966, 'preemption_count': 0}), (83888, {'train/accuracy': 0.5267773270606995, 'train/loss': 2.0652716159820557, 'validation/accuracy': 0.4811999797821045, 'validation/loss': 2.2873899936676025, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 2.935068368911743, 'test/num_examples': 10000, 'score': 38267.72216629982, 'total_duration': 41979.064101696014, 'accumulated_submission_time': 38267.72216629982, 'accumulated_eval_time': 3703.2136142253876, 'accumulated_logging_time': 3.6741631031036377, 'global_step': 83888, 'preemption_count': 0}), (84811, {'train/accuracy': 0.52587890625, 'train/loss': 2.056821823120117, 'validation/accuracy': 0.49289998412132263, 'validation/loss': 2.2359001636505127, 'validation/num_examples': 50000, 'test/accuracy': 0.3895000219345093, 'test/loss': 2.8796260356903076, 'test/num_examples': 10000, 'score': 38687.88592624664, 'total_duration': 42439.906369924545, 'accumulated_submission_time': 38687.88592624664, 'accumulated_eval_time': 3743.7984251976013, 'accumulated_logging_time': 3.718817710876465, 'global_step': 84811, 'preemption_count': 0}), (85733, {'train/accuracy': 0.517382800579071, 'train/loss': 2.116588830947876, 'validation/accuracy': 0.48499998450279236, 'validation/loss': 2.2845561504364014, 'validation/num_examples': 50000, 'test/accuracy': 0.3757000267505646, 'test/loss': 2.9468016624450684, 'test/num_examples': 10000, 'score': 39108.16635656357, 'total_duration': 42901.415877103806, 'accumulated_submission_time': 39108.16635656357, 'accumulated_eval_time': 3784.9390771389008, 'accumulated_logging_time': 3.758352756500244, 'global_step': 85733, 'preemption_count': 0}), (86655, {'train/accuracy': 0.5392773151397705, 'train/loss': 2.0084869861602783, 'validation/accuracy': 0.5009399652481079, 'validation/loss': 2.2077176570892334, 'validation/num_examples': 50000, 'test/accuracy': 0.3890000283718109, 'test/loss': 2.89555287361145, 'test/num_examples': 10000, 'score': 39528.129534721375, 'total_duration': 43362.101089954376, 'accumulated_submission_time': 39528.129534721375, 'accumulated_eval_time': 3825.567506790161, 'accumulated_logging_time': 3.802644729614258, 'global_step': 86655, 'preemption_count': 0}), (87576, {'train/accuracy': 0.5444140434265137, 'train/loss': 1.9728312492370605, 'validation/accuracy': 0.492499977350235, 'validation/loss': 2.241783618927002, 'validation/num_examples': 50000, 'test/accuracy': 0.38040003180503845, 'test/loss': 2.9136619567871094, 'test/num_examples': 10000, 'score': 39948.1226670742, 'total_duration': 43824.45436620712, 'accumulated_submission_time': 39948.1226670742, 'accumulated_eval_time': 3867.839832305908, 'accumulated_logging_time': 3.842219352722168, 'global_step': 87576, 'preemption_count': 0}), (88499, {'train/accuracy': 0.5226757526397705, 'train/loss': 2.081761598587036, 'validation/accuracy': 0.491599977016449, 'validation/loss': 2.257352590560913, 'validation/num_examples': 50000, 'test/accuracy': 0.3840000033378601, 'test/loss': 2.8978774547576904, 'test/num_examples': 10000, 'score': 40368.45597243309, 'total_duration': 44282.1383099556, 'accumulated_submission_time': 40368.45597243309, 'accumulated_eval_time': 3905.0960161685944, 'accumulated_logging_time': 3.8879823684692383, 'global_step': 88499, 'preemption_count': 0}), (89420, {'train/accuracy': 0.5298827886581421, 'train/loss': 2.077981472015381, 'validation/accuracy': 0.4909399747848511, 'validation/loss': 2.27143931388855, 'validation/num_examples': 50000, 'test/accuracy': 0.38270002603530884, 'test/loss': 2.9215173721313477, 'test/num_examples': 10000, 'score': 40788.488649606705, 'total_duration': 44745.744921684265, 'accumulated_submission_time': 40788.488649606705, 'accumulated_eval_time': 3948.5793375968933, 'accumulated_logging_time': 3.9286277294158936, 'global_step': 89420, 'preemption_count': 0}), (90343, {'train/accuracy': 0.5615038871765137, 'train/loss': 1.8954758644104004, 'validation/accuracy': 0.5057199597358704, 'validation/loss': 2.180562734603882, 'validation/num_examples': 50000, 'test/accuracy': 0.3936000168323517, 'test/loss': 2.8484578132629395, 'test/num_examples': 10000, 'score': 41208.838418483734, 'total_duration': 45211.47863817215, 'accumulated_submission_time': 41208.838418483734, 'accumulated_eval_time': 3993.8729910850525, 'accumulated_logging_time': 3.9703309535980225, 'global_step': 90343, 'preemption_count': 0}), (91267, {'train/accuracy': 0.5329882502555847, 'train/loss': 2.0743818283081055, 'validation/accuracy': 0.5001400113105774, 'validation/loss': 2.2381608486175537, 'validation/num_examples': 50000, 'test/accuracy': 0.38920003175735474, 'test/loss': 2.885896921157837, 'test/num_examples': 10000, 'score': 41629.06167125702, 'total_duration': 45675.498945236206, 'accumulated_submission_time': 41629.06167125702, 'accumulated_eval_time': 4037.57505941391, 'accumulated_logging_time': 4.016332626342773, 'global_step': 91267, 'preemption_count': 0}), (92188, {'train/accuracy': 0.5424413681030273, 'train/loss': 1.9888652563095093, 'validation/accuracy': 0.5024399757385254, 'validation/loss': 2.1791367530822754, 'validation/num_examples': 50000, 'test/accuracy': 0.3930000066757202, 'test/loss': 2.8366096019744873, 'test/num_examples': 10000, 'score': 42048.947972774506, 'total_duration': 46136.65203642845, 'accumulated_submission_time': 42048.947972774506, 'accumulated_eval_time': 4078.279564142227, 'accumulated_logging_time': 4.529749393463135, 'global_step': 92188, 'preemption_count': 0}), (93110, {'train/accuracy': 0.5608007907867432, 'train/loss': 1.8958534002304077, 'validation/accuracy': 0.5133000016212463, 'validation/loss': 2.126741886138916, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.773594617843628, 'test/num_examples': 10000, 'score': 42469.17872309685, 'total_duration': 46599.65407395363, 'accumulated_submission_time': 42469.17872309685, 'accumulated_eval_time': 4120.957757472992, 'accumulated_logging_time': 4.573975324630737, 'global_step': 93110, 'preemption_count': 0}), (94031, {'train/accuracy': 0.5450390577316284, 'train/loss': 1.9546549320220947, 'validation/accuracy': 0.5126599669456482, 'validation/loss': 2.1251654624938965, 'validation/num_examples': 50000, 'test/accuracy': 0.4058000147342682, 'test/loss': 2.794235944747925, 'test/num_examples': 10000, 'score': 42889.36215043068, 'total_duration': 47059.432107925415, 'accumulated_submission_time': 42889.36215043068, 'accumulated_eval_time': 4160.462041378021, 'accumulated_logging_time': 4.615373611450195, 'global_step': 94031, 'preemption_count': 0}), (94953, {'train/accuracy': 0.5402538776397705, 'train/loss': 2.001154661178589, 'validation/accuracy': 0.5038999915122986, 'validation/loss': 2.1799263954162598, 'validation/num_examples': 50000, 'test/accuracy': 0.40060001611709595, 'test/loss': 2.8371763229370117, 'test/num_examples': 10000, 'score': 43309.5159702301, 'total_duration': 47522.02918076515, 'accumulated_submission_time': 43309.5159702301, 'accumulated_eval_time': 4202.813487291336, 'accumulated_logging_time': 4.658671140670776, 'global_step': 94953, 'preemption_count': 0}), (95875, {'train/accuracy': 0.5454687476158142, 'train/loss': 2.008453845977783, 'validation/accuracy': 0.5034399628639221, 'validation/loss': 2.21916127204895, 'validation/num_examples': 50000, 'test/accuracy': 0.3922000229358673, 'test/loss': 2.864680528640747, 'test/num_examples': 10000, 'score': 43729.85532140732, 'total_duration': 47982.59704566002, 'accumulated_submission_time': 43729.85532140732, 'accumulated_eval_time': 4242.95054268837, 'accumulated_logging_time': 4.701233148574829, 'global_step': 95875, 'preemption_count': 0}), (96795, {'train/accuracy': 0.5572656393051147, 'train/loss': 1.9084703922271729, 'validation/accuracy': 0.5183199644088745, 'validation/loss': 2.112875461578369, 'validation/num_examples': 50000, 'test/accuracy': 0.40960001945495605, 'test/loss': 2.749626398086548, 'test/num_examples': 10000, 'score': 44149.91594219208, 'total_duration': 48443.68581080437, 'accumulated_submission_time': 44149.91594219208, 'accumulated_eval_time': 4283.885046005249, 'accumulated_logging_time': 4.746007680892944, 'global_step': 96795, 'preemption_count': 0}), (97717, {'train/accuracy': 0.5448241829872131, 'train/loss': 1.973633050918579, 'validation/accuracy': 0.5102199912071228, 'validation/loss': 2.158968210220337, 'validation/num_examples': 50000, 'test/accuracy': 0.4025000333786011, 'test/loss': 2.8153152465820312, 'test/num_examples': 10000, 'score': 44570.11754012108, 'total_duration': 48903.320981264114, 'accumulated_submission_time': 44570.11754012108, 'accumulated_eval_time': 4323.22603559494, 'accumulated_logging_time': 4.789544582366943, 'global_step': 97717, 'preemption_count': 0}), (98639, {'train/accuracy': 0.5551952719688416, 'train/loss': 1.9290554523468018, 'validation/accuracy': 0.5148800015449524, 'validation/loss': 2.135251760482788, 'validation/num_examples': 50000, 'test/accuracy': 0.40620002150535583, 'test/loss': 2.7946832180023193, 'test/num_examples': 10000, 'score': 44990.19146609306, 'total_duration': 49363.72312116623, 'accumulated_submission_time': 44990.19146609306, 'accumulated_eval_time': 4363.465132236481, 'accumulated_logging_time': 4.829688310623169, 'global_step': 98639, 'preemption_count': 0}), (99560, {'train/accuracy': 0.5842382907867432, 'train/loss': 1.8113583326339722, 'validation/accuracy': 0.5146600008010864, 'validation/loss': 2.1451916694641113, 'validation/num_examples': 50000, 'test/accuracy': 0.40700000524520874, 'test/loss': 2.797633647918701, 'test/num_examples': 10000, 'score': 45410.33908557892, 'total_duration': 49826.20372629166, 'accumulated_submission_time': 45410.33908557892, 'accumulated_eval_time': 4405.708698511124, 'accumulated_logging_time': 4.869691371917725, 'global_step': 99560, 'preemption_count': 0}), (100482, {'train/accuracy': 0.5579687356948853, 'train/loss': 1.9476128816604614, 'validation/accuracy': 0.5199399590492249, 'validation/loss': 2.1243672370910645, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.762568473815918, 'test/num_examples': 10000, 'score': 45830.48991537094, 'total_duration': 50285.001859903336, 'accumulated_submission_time': 45830.48991537094, 'accumulated_eval_time': 4444.26401591301, 'accumulated_logging_time': 4.912209749221802, 'global_step': 100482, 'preemption_count': 0}), (101404, {'train/accuracy': 0.5579296946525574, 'train/loss': 1.942072868347168, 'validation/accuracy': 0.5202599763870239, 'validation/loss': 2.1204233169555664, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.7876458168029785, 'test/num_examples': 10000, 'score': 46250.49274253845, 'total_duration': 50741.545766592026, 'accumulated_submission_time': 46250.49274253845, 'accumulated_eval_time': 4480.708422660828, 'accumulated_logging_time': 4.960192918777466, 'global_step': 101404, 'preemption_count': 0}), (102326, {'train/accuracy': 0.5746093392372131, 'train/loss': 1.825226068496704, 'validation/accuracy': 0.5278399586677551, 'validation/loss': 2.0796656608581543, 'validation/num_examples': 50000, 'test/accuracy': 0.4100000262260437, 'test/loss': 2.769395112991333, 'test/num_examples': 10000, 'score': 46670.66937637329, 'total_duration': 51203.094888448715, 'accumulated_submission_time': 46670.66937637329, 'accumulated_eval_time': 4521.99057674408, 'accumulated_logging_time': 5.001747369766235, 'global_step': 102326, 'preemption_count': 0}), (103247, {'train/accuracy': 0.5560937523841858, 'train/loss': 1.9529153108596802, 'validation/accuracy': 0.5209599733352661, 'validation/loss': 2.1211466789245605, 'validation/num_examples': 50000, 'test/accuracy': 0.4067000150680542, 'test/loss': 2.7682337760925293, 'test/num_examples': 10000, 'score': 47090.8395152092, 'total_duration': 51661.95341157913, 'accumulated_submission_time': 47090.8395152092, 'accumulated_eval_time': 4560.5888912677765, 'accumulated_logging_time': 5.0430192947387695, 'global_step': 103247, 'preemption_count': 0}), (104172, {'train/accuracy': 0.5689452886581421, 'train/loss': 1.8281947374343872, 'validation/accuracy': 0.5344399809837341, 'validation/loss': 2.0128085613250732, 'validation/num_examples': 50000, 'test/accuracy': 0.4191000163555145, 'test/loss': 2.6920504570007324, 'test/num_examples': 10000, 'score': 47510.9509665966, 'total_duration': 52125.428775310516, 'accumulated_submission_time': 47510.9509665966, 'accumulated_eval_time': 4603.852442026138, 'accumulated_logging_time': 5.094490051269531, 'global_step': 104172, 'preemption_count': 0}), (105094, {'train/accuracy': 0.5889062285423279, 'train/loss': 1.7638208866119385, 'validation/accuracy': 0.5421199798583984, 'validation/loss': 1.9910471439361572, 'validation/num_examples': 50000, 'test/accuracy': 0.4271000325679779, 'test/loss': 2.6605894565582275, 'test/num_examples': 10000, 'score': 47930.90433549881, 'total_duration': 52585.41693139076, 'accumulated_submission_time': 47930.90433549881, 'accumulated_eval_time': 4643.782491207123, 'accumulated_logging_time': 5.150495529174805, 'global_step': 105094, 'preemption_count': 0}), (106015, {'train/accuracy': 0.5689062476158142, 'train/loss': 1.8828868865966797, 'validation/accuracy': 0.5313199758529663, 'validation/loss': 2.0591819286346436, 'validation/num_examples': 50000, 'test/accuracy': 0.4189000129699707, 'test/loss': 2.7256243228912354, 'test/num_examples': 10000, 'score': 48350.86739444733, 'total_duration': 53043.58160710335, 'accumulated_submission_time': 48350.86739444733, 'accumulated_eval_time': 4681.881660699844, 'accumulated_logging_time': 5.203859329223633, 'global_step': 106015, 'preemption_count': 0}), (106937, {'train/accuracy': 0.5787695050239563, 'train/loss': 1.8417892456054688, 'validation/accuracy': 0.5412399768829346, 'validation/loss': 2.022822380065918, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.653942584991455, 'test/num_examples': 10000, 'score': 48770.8959608078, 'total_duration': 53502.422973394394, 'accumulated_submission_time': 48770.8959608078, 'accumulated_eval_time': 4720.600524902344, 'accumulated_logging_time': 5.249224662780762, 'global_step': 106937, 'preemption_count': 0}), (107860, {'train/accuracy': 0.5873827934265137, 'train/loss': 1.7708899974822998, 'validation/accuracy': 0.5433200001716614, 'validation/loss': 1.9900877475738525, 'validation/num_examples': 50000, 'test/accuracy': 0.43250003457069397, 'test/loss': 2.6258180141448975, 'test/num_examples': 10000, 'score': 49191.236491680145, 'total_duration': 53963.276510477066, 'accumulated_submission_time': 49191.236491680145, 'accumulated_eval_time': 4761.017573356628, 'accumulated_logging_time': 5.296191930770874, 'global_step': 107860, 'preemption_count': 0}), (108784, {'train/accuracy': 0.6213085651397705, 'train/loss': 1.629205584526062, 'validation/accuracy': 0.5485599637031555, 'validation/loss': 1.9674816131591797, 'validation/num_examples': 50000, 'test/accuracy': 0.43700000643730164, 'test/loss': 2.632002353668213, 'test/num_examples': 10000, 'score': 49611.455362319946, 'total_duration': 54420.862203359604, 'accumulated_submission_time': 49611.455362319946, 'accumulated_eval_time': 4798.291470050812, 'accumulated_logging_time': 5.3400962352752686, 'global_step': 108784, 'preemption_count': 0}), (109705, {'train/accuracy': 0.588671863079071, 'train/loss': 1.7554982900619507, 'validation/accuracy': 0.5467999577522278, 'validation/loss': 1.9544671773910522, 'validation/num_examples': 50000, 'test/accuracy': 0.4294000267982483, 'test/loss': 2.6271207332611084, 'test/num_examples': 10000, 'score': 50031.65995430946, 'total_duration': 54883.06630086899, 'accumulated_submission_time': 50031.65995430946, 'accumulated_eval_time': 4840.195640087128, 'accumulated_logging_time': 5.386731386184692, 'global_step': 109705, 'preemption_count': 0}), (110628, {'train/accuracy': 0.595898449420929, 'train/loss': 1.7019425630569458, 'validation/accuracy': 0.5554800033569336, 'validation/loss': 1.911659836769104, 'validation/num_examples': 50000, 'test/accuracy': 0.4384000301361084, 'test/loss': 2.5746302604675293, 'test/num_examples': 10000, 'score': 50451.89798164368, 'total_duration': 55343.07885932922, 'accumulated_submission_time': 50451.89798164368, 'accumulated_eval_time': 4879.87920832634, 'accumulated_logging_time': 5.428632497787476, 'global_step': 110628, 'preemption_count': 0}), (111551, {'train/accuracy': 0.6108593344688416, 'train/loss': 1.6443220376968384, 'validation/accuracy': 0.5592799782752991, 'validation/loss': 1.905354380607605, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.577054500579834, 'test/num_examples': 10000, 'score': 50871.974254608154, 'total_duration': 55801.88543534279, 'accumulated_submission_time': 50871.974254608154, 'accumulated_eval_time': 4918.517581939697, 'accumulated_logging_time': 5.472025394439697, 'global_step': 111551, 'preemption_count': 0}), (112474, {'train/accuracy': 0.5939062237739563, 'train/loss': 1.7227932214736938, 'validation/accuracy': 0.5597599744796753, 'validation/loss': 1.904951810836792, 'validation/num_examples': 50000, 'test/accuracy': 0.44140002131462097, 'test/loss': 2.582552194595337, 'test/num_examples': 10000, 'score': 51292.22484111786, 'total_duration': 56262.47783136368, 'accumulated_submission_time': 51292.22484111786, 'accumulated_eval_time': 4958.763142347336, 'accumulated_logging_time': 5.5198681354522705, 'global_step': 112474, 'preemption_count': 0}), (113395, {'train/accuracy': 0.6010937094688416, 'train/loss': 1.6848231554031372, 'validation/accuracy': 0.5621399879455566, 'validation/loss': 1.8820499181747437, 'validation/num_examples': 50000, 'test/accuracy': 0.43890002369880676, 'test/loss': 2.5632612705230713, 'test/num_examples': 10000, 'score': 51712.5141146183, 'total_duration': 56724.923040151596, 'accumulated_submission_time': 51712.5141146183, 'accumulated_eval_time': 5000.821955919266, 'accumulated_logging_time': 5.5690460205078125, 'global_step': 113395, 'preemption_count': 0}), (114316, {'train/accuracy': 0.6037890315055847, 'train/loss': 1.70114004611969, 'validation/accuracy': 0.55485999584198, 'validation/loss': 1.9423660039901733, 'validation/num_examples': 50000, 'test/accuracy': 0.442300021648407, 'test/loss': 2.572660207748413, 'test/num_examples': 10000, 'score': 52132.70584130287, 'total_duration': 57179.35532331467, 'accumulated_submission_time': 52132.70584130287, 'accumulated_eval_time': 5034.966580152512, 'accumulated_logging_time': 5.616888523101807, 'global_step': 114316, 'preemption_count': 0}), (115237, {'train/accuracy': 0.6056835651397705, 'train/loss': 1.67186439037323, 'validation/accuracy': 0.5682199597358704, 'validation/loss': 1.8541991710662842, 'validation/num_examples': 50000, 'test/accuracy': 0.44860002398490906, 'test/loss': 2.519437313079834, 'test/num_examples': 10000, 'score': 52552.76272273064, 'total_duration': 57640.74333691597, 'accumulated_submission_time': 52552.76272273064, 'accumulated_eval_time': 5076.205003976822, 'accumulated_logging_time': 5.660736322402954, 'global_step': 115237, 'preemption_count': 0}), (116160, {'train/accuracy': 0.6073437333106995, 'train/loss': 1.675774097442627, 'validation/accuracy': 0.5725799798965454, 'validation/loss': 1.846383810043335, 'validation/num_examples': 50000, 'test/accuracy': 0.45350003242492676, 'test/loss': 2.516984462738037, 'test/num_examples': 10000, 'score': 52972.98017334938, 'total_duration': 58102.24056863785, 'accumulated_submission_time': 52972.98017334938, 'accumulated_eval_time': 5117.389803171158, 'accumulated_logging_time': 5.706765413284302, 'global_step': 116160, 'preemption_count': 0}), (117083, {'train/accuracy': 0.6232812404632568, 'train/loss': 1.5855211019515991, 'validation/accuracy': 0.5778599977493286, 'validation/loss': 1.8170597553253174, 'validation/num_examples': 50000, 'test/accuracy': 0.46070003509521484, 'test/loss': 2.4718594551086426, 'test/num_examples': 10000, 'score': 53393.17074465752, 'total_duration': 58563.441803216934, 'accumulated_submission_time': 53393.17074465752, 'accumulated_eval_time': 5158.30740070343, 'accumulated_logging_time': 5.750935316085815, 'global_step': 117083, 'preemption_count': 0}), (118005, {'train/accuracy': 0.6318945288658142, 'train/loss': 1.5542774200439453, 'validation/accuracy': 0.5718399882316589, 'validation/loss': 1.8370327949523926, 'validation/num_examples': 50000, 'test/accuracy': 0.46230003237724304, 'test/loss': 2.502239465713501, 'test/num_examples': 10000, 'score': 53813.120841264725, 'total_duration': 59023.410198926926, 'accumulated_submission_time': 53813.120841264725, 'accumulated_eval_time': 5198.218531131744, 'accumulated_logging_time': 5.809661865234375, 'global_step': 118005, 'preemption_count': 0}), (118929, {'train/accuracy': 0.6213085651397705, 'train/loss': 1.6146199703216553, 'validation/accuracy': 0.5793200135231018, 'validation/loss': 1.8063288927078247, 'validation/num_examples': 50000, 'test/accuracy': 0.4620000123977661, 'test/loss': 2.4387903213500977, 'test/num_examples': 10000, 'score': 54233.34093928337, 'total_duration': 59486.030648469925, 'accumulated_submission_time': 54233.34093928337, 'accumulated_eval_time': 5240.525000333786, 'accumulated_logging_time': 5.854615688323975, 'global_step': 118929, 'preemption_count': 0}), (119852, {'train/accuracy': 0.6187695264816284, 'train/loss': 1.6314858198165894, 'validation/accuracy': 0.5747199654579163, 'validation/loss': 1.8536180257797241, 'validation/num_examples': 50000, 'test/accuracy': 0.45830002427101135, 'test/loss': 2.520430088043213, 'test/num_examples': 10000, 'score': 54653.69129776955, 'total_duration': 59941.873532533646, 'accumulated_submission_time': 54653.69129776955, 'accumulated_eval_time': 5275.923225164413, 'accumulated_logging_time': 5.900138854980469, 'global_step': 119852, 'preemption_count': 0}), (120773, {'train/accuracy': 0.6486914157867432, 'train/loss': 1.4689342975616455, 'validation/accuracy': 0.5861999988555908, 'validation/loss': 1.7645444869995117, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.431938886642456, 'test/num_examples': 10000, 'score': 55073.72949528694, 'total_duration': 60402.14140725136, 'accumulated_submission_time': 55073.72949528694, 'accumulated_eval_time': 5316.057541370392, 'accumulated_logging_time': 5.947498083114624, 'global_step': 120773, 'preemption_count': 0}), (121692, {'train/accuracy': 0.6215234398841858, 'train/loss': 1.6078003644943237, 'validation/accuracy': 0.5834999680519104, 'validation/loss': 1.7998595237731934, 'validation/num_examples': 50000, 'test/accuracy': 0.4652000367641449, 'test/loss': 2.4484989643096924, 'test/num_examples': 10000, 'score': 55493.78798747063, 'total_duration': 60864.09947562218, 'accumulated_submission_time': 55493.78798747063, 'accumulated_eval_time': 5357.858735084534, 'accumulated_logging_time': 5.998109817504883, 'global_step': 121692, 'preemption_count': 0}), (122615, {'train/accuracy': 0.6367577910423279, 'train/loss': 1.5228012800216675, 'validation/accuracy': 0.5913800001144409, 'validation/loss': 1.7486042976379395, 'validation/num_examples': 50000, 'test/accuracy': 0.4740000367164612, 'test/loss': 2.3974080085754395, 'test/num_examples': 10000, 'score': 55914.06940293312, 'total_duration': 61324.40569233894, 'accumulated_submission_time': 55914.06940293312, 'accumulated_eval_time': 5397.7881071567535, 'accumulated_logging_time': 6.045310974121094, 'global_step': 122615, 'preemption_count': 0}), (123540, {'train/accuracy': 0.644335925579071, 'train/loss': 1.5328458547592163, 'validation/accuracy': 0.5850600004196167, 'validation/loss': 1.7931092977523804, 'validation/num_examples': 50000, 'test/accuracy': 0.46820002794265747, 'test/loss': 2.4418222904205322, 'test/num_examples': 10000, 'score': 56334.1478741169, 'total_duration': 61787.00834584236, 'accumulated_submission_time': 56334.1478741169, 'accumulated_eval_time': 5440.216062545776, 'accumulated_logging_time': 6.09236216545105, 'global_step': 123540, 'preemption_count': 0}), (124462, {'train/accuracy': 0.6310741901397705, 'train/loss': 1.5595864057540894, 'validation/accuracy': 0.5909199714660645, 'validation/loss': 1.7579379081726074, 'validation/num_examples': 50000, 'test/accuracy': 0.4686000347137451, 'test/loss': 2.41546893119812, 'test/num_examples': 10000, 'score': 56754.07245540619, 'total_duration': 62247.24000930786, 'accumulated_submission_time': 56754.07245540619, 'accumulated_eval_time': 5480.426479578018, 'accumulated_logging_time': 6.140239953994751, 'global_step': 124462, 'preemption_count': 0}), (125382, {'train/accuracy': 0.6387109160423279, 'train/loss': 1.5228734016418457, 'validation/accuracy': 0.5959399938583374, 'validation/loss': 1.7375160455703735, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.390855073928833, 'test/num_examples': 10000, 'score': 57174.0851354599, 'total_duration': 62710.162212610245, 'accumulated_submission_time': 57174.0851354599, 'accumulated_eval_time': 5523.234518289566, 'accumulated_logging_time': 6.193271636962891, 'global_step': 125382, 'preemption_count': 0}), (126304, {'train/accuracy': 0.6492577791213989, 'train/loss': 1.4756730794906616, 'validation/accuracy': 0.5982199907302856, 'validation/loss': 1.706802487373352, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.3549671173095703, 'test/num_examples': 10000, 'score': 57594.218314647675, 'total_duration': 63170.8043320179, 'accumulated_submission_time': 57594.218314647675, 'accumulated_eval_time': 5563.646774530411, 'accumulated_logging_time': 6.2415242195129395, 'global_step': 126304, 'preemption_count': 0}), (127228, {'train/accuracy': 0.6450781226158142, 'train/loss': 1.5104620456695557, 'validation/accuracy': 0.5978999733924866, 'validation/loss': 1.7296024560928345, 'validation/num_examples': 50000, 'test/accuracy': 0.47210001945495605, 'test/loss': 2.3971104621887207, 'test/num_examples': 10000, 'score': 58014.53499889374, 'total_duration': 63635.348907232285, 'accumulated_submission_time': 58014.53499889374, 'accumulated_eval_time': 5607.780184268951, 'accumulated_logging_time': 6.286353588104248, 'global_step': 127228, 'preemption_count': 0}), (128150, {'train/accuracy': 0.6434765458106995, 'train/loss': 1.4898433685302734, 'validation/accuracy': 0.6008999943733215, 'validation/loss': 1.686245083808899, 'validation/num_examples': 50000, 'test/accuracy': 0.48260003328323364, 'test/loss': 2.3594110012054443, 'test/num_examples': 10000, 'score': 58434.6179728508, 'total_duration': 64096.87507081032, 'accumulated_submission_time': 58434.6179728508, 'accumulated_eval_time': 5649.124925851822, 'accumulated_logging_time': 6.335825204849243, 'global_step': 128150, 'preemption_count': 0}), (129073, {'train/accuracy': 0.6571874618530273, 'train/loss': 1.4405206441879272, 'validation/accuracy': 0.6071999669075012, 'validation/loss': 1.6774921417236328, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.321425199508667, 'test/num_examples': 10000, 'score': 58854.83988237381, 'total_duration': 64557.95269560814, 'accumulated_submission_time': 58854.83988237381, 'accumulated_eval_time': 5689.88328742981, 'accumulated_logging_time': 6.38477087020874, 'global_step': 129073, 'preemption_count': 0}), (129996, {'train/accuracy': 0.6743749976158142, 'train/loss': 1.3620802164077759, 'validation/accuracy': 0.6096000075340271, 'validation/loss': 1.6756395101547241, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.335404634475708, 'test/num_examples': 10000, 'score': 59274.73733615875, 'total_duration': 65022.062368392944, 'accumulated_submission_time': 59274.73733615875, 'accumulated_eval_time': 5733.991091012955, 'accumulated_logging_time': 6.439935207366943, 'global_step': 129996, 'preemption_count': 0}), (130801, {'train/accuracy': 0.6576952934265137, 'train/loss': 1.4295475482940674, 'validation/accuracy': 0.6107199788093567, 'validation/loss': 1.6498571634292603, 'validation/num_examples': 50000, 'test/accuracy': 0.4888000190258026, 'test/loss': 2.307673454284668, 'test/num_examples': 10000, 'score': 59694.95303297043, 'total_duration': 65481.10044121742, 'accumulated_submission_time': 59694.95303297043, 'accumulated_eval_time': 5772.714293003082, 'accumulated_logging_time': 6.497478723526001, 'global_step': 130801, 'preemption_count': 0}), (131725, {'train/accuracy': 0.6663476228713989, 'train/loss': 1.3924933671951294, 'validation/accuracy': 0.6150000095367432, 'validation/loss': 1.636078119277954, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.286726474761963, 'test/num_examples': 10000, 'score': 60115.202788591385, 'total_duration': 65940.35055208206, 'accumulated_submission_time': 60115.202788591385, 'accumulated_eval_time': 5811.607728004456, 'accumulated_logging_time': 6.554529428482056, 'global_step': 131725, 'preemption_count': 0}), (132646, {'train/accuracy': 0.6916796565055847, 'train/loss': 1.280269742012024, 'validation/accuracy': 0.6193599700927734, 'validation/loss': 1.6119118928909302, 'validation/num_examples': 50000, 'test/accuracy': 0.4978000223636627, 'test/loss': 2.2578117847442627, 'test/num_examples': 10000, 'score': 60535.19282770157, 'total_duration': 66399.90402460098, 'accumulated_submission_time': 60535.19282770157, 'accumulated_eval_time': 5851.074378013611, 'accumulated_logging_time': 6.603094100952148, 'global_step': 132646, 'preemption_count': 0}), (133570, {'train/accuracy': 0.6681445240974426, 'train/loss': 1.4086965322494507, 'validation/accuracy': 0.624239981174469, 'validation/loss': 1.609699010848999, 'validation/num_examples': 50000, 'test/accuracy': 0.4993000328540802, 'test/loss': 2.2752673625946045, 'test/num_examples': 10000, 'score': 60955.238864421844, 'total_duration': 66860.528911829, 'accumulated_submission_time': 60955.238864421844, 'accumulated_eval_time': 5891.558381795883, 'accumulated_logging_time': 6.649211645126343, 'global_step': 133570, 'preemption_count': 0}), (134493, {'train/accuracy': 0.6716992259025574, 'train/loss': 1.369166374206543, 'validation/accuracy': 0.6208400130271912, 'validation/loss': 1.6039738655090332, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.2548952102661133, 'test/num_examples': 10000, 'score': 61375.3524119854, 'total_duration': 67322.49597454071, 'accumulated_submission_time': 61375.3524119854, 'accumulated_eval_time': 5933.31524014473, 'accumulated_logging_time': 6.696808576583862, 'global_step': 134493, 'preemption_count': 0}), (135413, {'train/accuracy': 0.6820898056030273, 'train/loss': 1.3533567190170288, 'validation/accuracy': 0.6220999956130981, 'validation/loss': 1.6352964639663696, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.288580894470215, 'test/num_examples': 10000, 'score': 61794.85926914215, 'total_duration': 67780.60915780067, 'accumulated_submission_time': 61794.85926914215, 'accumulated_eval_time': 5971.316042661667, 'accumulated_logging_time': 7.2534918785095215, 'global_step': 135413, 'preemption_count': 0}), (136335, {'train/accuracy': 0.67431640625, 'train/loss': 1.3767963647842407, 'validation/accuracy': 0.6293999552726746, 'validation/loss': 1.5818134546279907, 'validation/num_examples': 50000, 'test/accuracy': 0.5031000375747681, 'test/loss': 2.237124443054199, 'test/num_examples': 10000, 'score': 62215.11662912369, 'total_duration': 68242.74508333206, 'accumulated_submission_time': 62215.11662912369, 'accumulated_eval_time': 6013.095870256424, 'accumulated_logging_time': 7.303630352020264, 'global_step': 136335, 'preemption_count': 0}), (137257, {'train/accuracy': 0.6786132454872131, 'train/loss': 1.3735076189041138, 'validation/accuracy': 0.6330199837684631, 'validation/loss': 1.5894250869750977, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.2336137294769287, 'test/num_examples': 10000, 'score': 62635.25749588013, 'total_duration': 68701.20639562607, 'accumulated_submission_time': 62635.25749588013, 'accumulated_eval_time': 6051.310604095459, 'accumulated_logging_time': 7.360164165496826, 'global_step': 137257, 'preemption_count': 0}), (138175, {'train/accuracy': 0.6922070384025574, 'train/loss': 1.2771286964416504, 'validation/accuracy': 0.6366400122642517, 'validation/loss': 1.529810905456543, 'validation/num_examples': 50000, 'test/accuracy': 0.5169000029563904, 'test/loss': 2.163562297821045, 'test/num_examples': 10000, 'score': 63055.25727057457, 'total_duration': 69165.89258766174, 'accumulated_submission_time': 63055.25727057457, 'accumulated_eval_time': 6095.903052806854, 'accumulated_logging_time': 7.405869483947754, 'global_step': 138175, 'preemption_count': 0}), (139094, {'train/accuracy': 0.6813281178474426, 'train/loss': 1.3332164287567139, 'validation/accuracy': 0.6382799744606018, 'validation/loss': 1.5517253875732422, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.2069404125213623, 'test/num_examples': 10000, 'score': 63475.19632101059, 'total_duration': 69625.84164237976, 'accumulated_submission_time': 63475.19632101059, 'accumulated_eval_time': 6135.814694881439, 'accumulated_logging_time': 7.4559032917022705, 'global_step': 139094, 'preemption_count': 0}), (140016, {'train/accuracy': 0.6907812356948853, 'train/loss': 1.2872713804244995, 'validation/accuracy': 0.6406999826431274, 'validation/loss': 1.5119783878326416, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.150461196899414, 'test/num_examples': 10000, 'score': 63895.5168299675, 'total_duration': 70082.7032134533, 'accumulated_submission_time': 63895.5168299675, 'accumulated_eval_time': 6172.251842260361, 'accumulated_logging_time': 7.511298418045044, 'global_step': 140016, 'preemption_count': 0}), (140938, {'train/accuracy': 0.6983007788658142, 'train/loss': 1.2475330829620361, 'validation/accuracy': 0.6419399976730347, 'validation/loss': 1.5060182809829712, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.1491525173187256, 'test/num_examples': 10000, 'score': 64315.69512438774, 'total_duration': 70544.94150233269, 'accumulated_submission_time': 64315.69512438774, 'accumulated_eval_time': 6214.205435991287, 'accumulated_logging_time': 7.5685484409332275, 'global_step': 140938, 'preemption_count': 0}), (141862, {'train/accuracy': 0.7147851586341858, 'train/loss': 1.200818657875061, 'validation/accuracy': 0.6396399736404419, 'validation/loss': 1.5271742343902588, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.164371967315674, 'test/num_examples': 10000, 'score': 64735.889827251434, 'total_duration': 71006.54262113571, 'accumulated_submission_time': 64735.889827251434, 'accumulated_eval_time': 6255.516871213913, 'accumulated_logging_time': 7.614865064620972, 'global_step': 141862, 'preemption_count': 0}), (142784, {'train/accuracy': 0.6896093487739563, 'train/loss': 1.3375908136367798, 'validation/accuracy': 0.6431199908256531, 'validation/loss': 1.5491735935211182, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.190412759780884, 'test/num_examples': 10000, 'score': 65156.19044685364, 'total_duration': 71464.98599791527, 'accumulated_submission_time': 65156.19044685364, 'accumulated_eval_time': 6293.555589437485, 'accumulated_logging_time': 7.669762372970581, 'global_step': 142784, 'preemption_count': 0}), (143705, {'train/accuracy': 0.7058203220367432, 'train/loss': 1.2415093183517456, 'validation/accuracy': 0.6496599912643433, 'validation/loss': 1.489941954612732, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.1392199993133545, 'test/num_examples': 10000, 'score': 65576.20358800888, 'total_duration': 71929.60329174995, 'accumulated_submission_time': 65576.20358800888, 'accumulated_eval_time': 6338.054198503494, 'accumulated_logging_time': 7.726184368133545, 'global_step': 143705, 'preemption_count': 0}), (144624, {'train/accuracy': 0.7205273509025574, 'train/loss': 1.1597256660461426, 'validation/accuracy': 0.6532999873161316, 'validation/loss': 1.449857473373413, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.090836763381958, 'test/num_examples': 10000, 'score': 65996.54626846313, 'total_duration': 72393.68397283554, 'accumulated_submission_time': 65996.54626846313, 'accumulated_eval_time': 6381.687472581863, 'accumulated_logging_time': 7.781303644180298, 'global_step': 144624, 'preemption_count': 0}), (145543, {'train/accuracy': 0.7084179520606995, 'train/loss': 1.2068352699279785, 'validation/accuracy': 0.6576600074768066, 'validation/loss': 1.4416378736495972, 'validation/num_examples': 50000, 'test/accuracy': 0.5302000045776367, 'test/loss': 2.0874135494232178, 'test/num_examples': 10000, 'score': 66416.76606321335, 'total_duration': 72857.4061486721, 'accumulated_submission_time': 66416.76606321335, 'accumulated_eval_time': 6425.086839675903, 'accumulated_logging_time': 7.835384845733643, 'global_step': 145543, 'preemption_count': 0}), (146464, {'train/accuracy': 0.7099999785423279, 'train/loss': 1.185759425163269, 'validation/accuracy': 0.6562199592590332, 'validation/loss': 1.433472990989685, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.0838398933410645, 'test/num_examples': 10000, 'score': 66836.82033586502, 'total_duration': 73318.36433267593, 'accumulated_submission_time': 66836.82033586502, 'accumulated_eval_time': 6465.893684387207, 'accumulated_logging_time': 7.883531093597412, 'global_step': 146464, 'preemption_count': 0}), (147388, {'train/accuracy': 0.71875, 'train/loss': 1.1505794525146484, 'validation/accuracy': 0.6584999561309814, 'validation/loss': 1.419907569885254, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.0673604011535645, 'test/num_examples': 10000, 'score': 67257.22652721405, 'total_duration': 73778.35560202599, 'accumulated_submission_time': 67257.22652721405, 'accumulated_eval_time': 6505.38139629364, 'accumulated_logging_time': 7.932077884674072, 'global_step': 147388, 'preemption_count': 0}), (148311, {'train/accuracy': 0.7202538847923279, 'train/loss': 1.1635055541992188, 'validation/accuracy': 0.6650800108909607, 'validation/loss': 1.4090675115585327, 'validation/num_examples': 50000, 'test/accuracy': 0.5368000268936157, 'test/loss': 2.0515666007995605, 'test/num_examples': 10000, 'score': 67677.38717126846, 'total_duration': 74240.42641305923, 'accumulated_submission_time': 67677.38717126846, 'accumulated_eval_time': 6547.186131954193, 'accumulated_logging_time': 7.988610029220581, 'global_step': 148311, 'preemption_count': 0}), (149233, {'train/accuracy': 0.7199999690055847, 'train/loss': 1.1488616466522217, 'validation/accuracy': 0.6662200093269348, 'validation/loss': 1.3948432207107544, 'validation/num_examples': 50000, 'test/accuracy': 0.5412999987602234, 'test/loss': 2.040438652038574, 'test/num_examples': 10000, 'score': 68097.53750610352, 'total_duration': 74699.47302079201, 'accumulated_submission_time': 68097.53750610352, 'accumulated_eval_time': 6585.98409485817, 'accumulated_logging_time': 8.03798794746399, 'global_step': 149233, 'preemption_count': 0}), (150153, {'train/accuracy': 0.7326562404632568, 'train/loss': 1.1124690771102905, 'validation/accuracy': 0.6711199879646301, 'validation/loss': 1.3810205459594727, 'validation/num_examples': 50000, 'test/accuracy': 0.547700047492981, 'test/loss': 2.0186476707458496, 'test/num_examples': 10000, 'score': 68517.46277451515, 'total_duration': 75162.01907277107, 'accumulated_submission_time': 68517.46277451515, 'accumulated_eval_time': 6628.50556063652, 'accumulated_logging_time': 8.088744640350342, 'global_step': 150153, 'preemption_count': 0}), (151075, {'train/accuracy': 0.7326562404632568, 'train/loss': 1.1057353019714355, 'validation/accuracy': 0.673039972782135, 'validation/loss': 1.3807213306427002, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.029315233230591, 'test/num_examples': 10000, 'score': 68937.43747639656, 'total_duration': 75620.98027873039, 'accumulated_submission_time': 68937.43747639656, 'accumulated_eval_time': 6667.386779785156, 'accumulated_logging_time': 8.145340204238892, 'global_step': 151075, 'preemption_count': 0}), (151999, {'train/accuracy': 0.7305663824081421, 'train/loss': 1.1032029390335083, 'validation/accuracy': 0.6752399802207947, 'validation/loss': 1.345995306968689, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 1.9906830787658691, 'test/num_examples': 10000, 'score': 69357.67158484459, 'total_duration': 76083.28164362907, 'accumulated_submission_time': 69357.67158484459, 'accumulated_eval_time': 6709.3519904613495, 'accumulated_logging_time': 8.198824405670166, 'global_step': 151999, 'preemption_count': 0}), (152922, {'train/accuracy': 0.7342382669448853, 'train/loss': 1.077256202697754, 'validation/accuracy': 0.6779199838638306, 'validation/loss': 1.3336390256881714, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.967795968055725, 'test/num_examples': 10000, 'score': 69777.8404636383, 'total_duration': 76541.56018471718, 'accumulated_submission_time': 69777.8404636383, 'accumulated_eval_time': 6747.357416629791, 'accumulated_logging_time': 8.25408148765564, 'global_step': 152922, 'preemption_count': 0}), (153844, {'train/accuracy': 0.755175769329071, 'train/loss': 0.9857242107391357, 'validation/accuracy': 0.6827999949455261, 'validation/loss': 1.3085020780563354, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9330166578292847, 'test/num_examples': 10000, 'score': 70197.90728449821, 'total_duration': 77002.52214980125, 'accumulated_submission_time': 70197.90728449821, 'accumulated_eval_time': 6788.15131855011, 'accumulated_logging_time': 8.30648159980774, 'global_step': 153844, 'preemption_count': 0}), (154766, {'train/accuracy': 0.7420898079872131, 'train/loss': 1.0481141805648804, 'validation/accuracy': 0.6864399909973145, 'validation/loss': 1.2994166612625122, 'validation/num_examples': 50000, 'test/accuracy': 0.5594000220298767, 'test/loss': 1.9377702474594116, 'test/num_examples': 10000, 'score': 70618.0102212429, 'total_duration': 77463.61097240448, 'accumulated_submission_time': 70618.0102212429, 'accumulated_eval_time': 6829.035629749298, 'accumulated_logging_time': 8.359469890594482, 'global_step': 154766, 'preemption_count': 0}), (155688, {'train/accuracy': 0.7477148175239563, 'train/loss': 1.0200145244598389, 'validation/accuracy': 0.6866199970245361, 'validation/loss': 1.2940551042556763, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.915860891342163, 'test/num_examples': 10000, 'score': 71038.28627920151, 'total_duration': 77925.08789467812, 'accumulated_submission_time': 71038.28627920151, 'accumulated_eval_time': 6870.133940458298, 'accumulated_logging_time': 8.413815975189209, 'global_step': 155688, 'preemption_count': 0}), (156608, {'train/accuracy': 0.7582226395606995, 'train/loss': 0.9849535822868347, 'validation/accuracy': 0.6900399923324585, 'validation/loss': 1.2790842056274414, 'validation/num_examples': 50000, 'test/accuracy': 0.5662000179290771, 'test/loss': 1.920639157295227, 'test/num_examples': 10000, 'score': 71458.23702192307, 'total_duration': 78387.6969614029, 'accumulated_submission_time': 71458.23702192307, 'accumulated_eval_time': 6912.691474199295, 'accumulated_logging_time': 8.466750383377075, 'global_step': 156608, 'preemption_count': 0}), (157528, {'train/accuracy': 0.7495703101158142, 'train/loss': 1.0092881917953491, 'validation/accuracy': 0.6941199898719788, 'validation/loss': 1.263446569442749, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.915724515914917, 'test/num_examples': 10000, 'score': 71878.5546181202, 'total_duration': 78851.75995445251, 'accumulated_submission_time': 71878.5546181202, 'accumulated_eval_time': 6956.333927631378, 'accumulated_logging_time': 8.52064037322998, 'global_step': 157528, 'preemption_count': 0}), (158452, {'train/accuracy': 0.7608984112739563, 'train/loss': 0.9779459238052368, 'validation/accuracy': 0.6959199905395508, 'validation/loss': 1.251377820968628, 'validation/num_examples': 50000, 'test/accuracy': 0.5752000212669373, 'test/loss': 1.885601282119751, 'test/num_examples': 10000, 'score': 72298.76634907722, 'total_duration': 79316.75279378891, 'accumulated_submission_time': 72298.76634907722, 'accumulated_eval_time': 7001.015208482742, 'accumulated_logging_time': 8.571056127548218, 'global_step': 158452, 'preemption_count': 0}), (159372, {'train/accuracy': 0.7640234231948853, 'train/loss': 0.9492297768592834, 'validation/accuracy': 0.6974599957466125, 'validation/loss': 1.241568922996521, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 1.8790431022644043, 'test/num_examples': 10000, 'score': 72719.03267765045, 'total_duration': 79778.45223927498, 'accumulated_submission_time': 72719.03267765045, 'accumulated_eval_time': 7042.348134994507, 'accumulated_logging_time': 8.622626304626465, 'global_step': 159372, 'preemption_count': 0}), (160296, {'train/accuracy': 0.7643359303474426, 'train/loss': 0.9486043453216553, 'validation/accuracy': 0.703000009059906, 'validation/loss': 1.2157114744186401, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 1.8601552248001099, 'test/num_examples': 10000, 'score': 73139.28754115105, 'total_duration': 80239.31615614891, 'accumulated_submission_time': 73139.28754115105, 'accumulated_eval_time': 7082.851930856705, 'accumulated_logging_time': 8.679443597793579, 'global_step': 160296, 'preemption_count': 0}), (161219, {'train/accuracy': 0.763378918170929, 'train/loss': 0.9769259691238403, 'validation/accuracy': 0.7021999955177307, 'validation/loss': 1.2391090393066406, 'validation/num_examples': 50000, 'test/accuracy': 0.5754000544548035, 'test/loss': 1.8825575113296509, 'test/num_examples': 10000, 'score': 73559.51320672035, 'total_duration': 80701.84395289421, 'accumulated_submission_time': 73559.51320672035, 'accumulated_eval_time': 7125.049999952316, 'accumulated_logging_time': 8.735102891921997, 'global_step': 161219, 'preemption_count': 0}), (162142, {'train/accuracy': 0.7700781226158142, 'train/loss': 0.9373509287834167, 'validation/accuracy': 0.7052599787712097, 'validation/loss': 1.2130261659622192, 'validation/num_examples': 50000, 'test/accuracy': 0.579300045967102, 'test/loss': 1.8460499048233032, 'test/num_examples': 10000, 'score': 73979.57974791527, 'total_duration': 81164.14709663391, 'accumulated_submission_time': 73979.57974791527, 'accumulated_eval_time': 7167.17483496666, 'accumulated_logging_time': 8.798385381698608, 'global_step': 162142, 'preemption_count': 0}), (163064, {'train/accuracy': 0.7858203053474426, 'train/loss': 0.8678929209709167, 'validation/accuracy': 0.7084400057792664, 'validation/loss': 1.2065235376358032, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.8323715925216675, 'test/num_examples': 10000, 'score': 74399.50149416924, 'total_duration': 81629.3094651699, 'accumulated_submission_time': 74399.50149416924, 'accumulated_eval_time': 7212.307286977768, 'accumulated_logging_time': 8.858510732650757, 'global_step': 163064, 'preemption_count': 0}), (163987, {'train/accuracy': 0.7753320336341858, 'train/loss': 0.9123504161834717, 'validation/accuracy': 0.7140399813652039, 'validation/loss': 1.184422492980957, 'validation/num_examples': 50000, 'test/accuracy': 0.5903000235557556, 'test/loss': 1.8158957958221436, 'test/num_examples': 10000, 'score': 74819.62465143204, 'total_duration': 82088.27244186401, 'accumulated_submission_time': 74819.62465143204, 'accumulated_eval_time': 7251.045498132706, 'accumulated_logging_time': 8.912059307098389, 'global_step': 163987, 'preemption_count': 0}), (164909, {'train/accuracy': 0.7803320288658142, 'train/loss': 0.8811646103858948, 'validation/accuracy': 0.7166399955749512, 'validation/loss': 1.1691982746124268, 'validation/num_examples': 50000, 'test/accuracy': 0.5915000438690186, 'test/loss': 1.7959223985671997, 'test/num_examples': 10000, 'score': 75239.51298332214, 'total_duration': 82548.98171710968, 'accumulated_submission_time': 75239.51298332214, 'accumulated_eval_time': 7291.754102945328, 'accumulated_logging_time': 8.975491523742676, 'global_step': 164909, 'preemption_count': 0}), (165831, {'train/accuracy': 0.788378894329071, 'train/loss': 0.8617101907730103, 'validation/accuracy': 0.7175999879837036, 'validation/loss': 1.1666576862335205, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 1.8022934198379517, 'test/num_examples': 10000, 'score': 75659.53583550453, 'total_duration': 83013.09928798676, 'accumulated_submission_time': 75659.53583550453, 'accumulated_eval_time': 7335.747500419617, 'accumulated_logging_time': 9.027804374694824, 'global_step': 165831, 'preemption_count': 0}), (166751, {'train/accuracy': 0.7841405868530273, 'train/loss': 0.869185209274292, 'validation/accuracy': 0.7195799946784973, 'validation/loss': 1.1540203094482422, 'validation/num_examples': 50000, 'test/accuracy': 0.5955000519752502, 'test/loss': 1.7816680669784546, 'test/num_examples': 10000, 'score': 76079.45230317116, 'total_duration': 83473.17249202728, 'accumulated_submission_time': 76079.45230317116, 'accumulated_eval_time': 7375.801503419876, 'accumulated_logging_time': 9.081603765487671, 'global_step': 166751, 'preemption_count': 0}), (167674, {'train/accuracy': 0.7900976538658142, 'train/loss': 0.8440901041030884, 'validation/accuracy': 0.7210599780082703, 'validation/loss': 1.1322126388549805, 'validation/num_examples': 50000, 'test/accuracy': 0.5976999998092651, 'test/loss': 1.7546520233154297, 'test/num_examples': 10000, 'score': 76499.65189146996, 'total_duration': 83934.78953242302, 'accumulated_submission_time': 76499.65189146996, 'accumulated_eval_time': 7417.107574224472, 'accumulated_logging_time': 9.143556594848633, 'global_step': 167674, 'preemption_count': 0}), (168597, {'train/accuracy': 0.7947070002555847, 'train/loss': 0.8163579106330872, 'validation/accuracy': 0.7242000102996826, 'validation/loss': 1.1301054954528809, 'validation/num_examples': 50000, 'test/accuracy': 0.6009000539779663, 'test/loss': 1.7406065464019775, 'test/num_examples': 10000, 'score': 76919.8663828373, 'total_duration': 84398.5364639759, 'accumulated_submission_time': 76919.8663828373, 'accumulated_eval_time': 7460.537466287613, 'accumulated_logging_time': 9.19641399383545, 'global_step': 168597, 'preemption_count': 0}), (169519, {'train/accuracy': 0.7934960722923279, 'train/loss': 0.8324291706085205, 'validation/accuracy': 0.725820004940033, 'validation/loss': 1.1191824674606323, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.7409383058547974, 'test/num_examples': 10000, 'score': 77339.84844470024, 'total_duration': 84861.64176750183, 'accumulated_submission_time': 77339.84844470024, 'accumulated_eval_time': 7503.553560256958, 'accumulated_logging_time': 9.254878997802734, 'global_step': 169519, 'preemption_count': 0})], 'global_step': 169921}
I0203 11:17:33.591413 139863983413056 submission_runner.py:586] Timing: 77520.14989256859
I0203 11:17:33.591511 139863983413056 submission_runner.py:588] Total number of evals: 185
I0203 11:17:33.591576 139863983413056 submission_runner.py:589] ====================
I0203 11:17:33.591630 139863983413056 submission_runner.py:542] Using RNG seed 3682051175
I0203 11:17:33.593101 139863983413056 submission_runner.py:551] --- Tuning run 5/5 ---
I0203 11:17:33.593220 139863983413056 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_5.
I0203 11:17:33.597512 139863983413056 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_5/hparams.json.
I0203 11:17:33.598319 139863983413056 submission_runner.py:206] Initializing dataset.
I0203 11:17:33.607443 139863983413056 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0203 11:17:33.617655 139863983413056 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0203 11:17:33.808358 139863983413056 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0203 11:17:38.979925 139863983413056 submission_runner.py:213] Initializing model.
I0203 11:17:44.800771 139863983413056 submission_runner.py:255] Initializing optimizer.
I0203 11:17:45.274986 139863983413056 submission_runner.py:262] Initializing metrics bundle.
I0203 11:17:45.275155 139863983413056 submission_runner.py:280] Initializing checkpoint and logger.
I0203 11:17:45.290366 139863983413056 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_5 with prefix checkpoint_
I0203 11:17:45.290509 139863983413056 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0203 11:18:01.333823 139863983413056 logger_utils.py:220] Unable to record git information. Continuing without it.
I0203 11:18:17.117460 139863983413056 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_5/flags_0.json.
I0203 11:18:17.122595 139863983413056 submission_runner.py:314] Starting training loop.
I0203 11:18:56.786591 139702501852928 logging_writer.py:48] [0] global_step=0, grad_norm=0.35481423139572144, loss=6.907756328582764
I0203 11:18:56.799036 139863983413056 spec.py:321] Evaluating on the training split.
I0203 11:19:05.229218 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 11:19:23.975276 139863983413056 spec.py:349] Evaluating on the test split.
I0203 11:19:25.604700 139863983413056 submission_runner.py:408] Time since start: 68.48s, 	Step: 1, 	{'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 39.676350355148315, 'total_duration': 68.48205900192261, 'accumulated_submission_time': 39.676350355148315, 'accumulated_eval_time': 28.80561327934265, 'accumulated_logging_time': 0}
I0203 11:19:25.613093 139702510245632 logging_writer.py:48] [1] accumulated_eval_time=28.805613, accumulated_logging_time=0, accumulated_submission_time=39.676350, global_step=1, preemption_count=0, score=39.676350, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=68.482059, train/accuracy=0.000996, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0203 11:20:36.471830 139702543816448 logging_writer.py:48] [100] global_step=100, grad_norm=0.5150549411773682, loss=6.882768154144287
I0203 11:21:22.101589 139702527031040 logging_writer.py:48] [200] global_step=200, grad_norm=0.5564903616905212, loss=6.778781414031982
I0203 11:22:08.735095 139702543816448 logging_writer.py:48] [300] global_step=300, grad_norm=0.6884644031524658, loss=6.68750524520874
I0203 11:22:55.692524 139702527031040 logging_writer.py:48] [400] global_step=400, grad_norm=0.7286935448646545, loss=6.601668834686279
I0203 11:23:42.776854 139702543816448 logging_writer.py:48] [500] global_step=500, grad_norm=1.1129255294799805, loss=6.733525276184082
I0203 11:24:29.449819 139702527031040 logging_writer.py:48] [600] global_step=600, grad_norm=1.0685781240463257, loss=6.418367385864258
I0203 11:25:16.239360 139702543816448 logging_writer.py:48] [700] global_step=700, grad_norm=1.251718282699585, loss=6.278539180755615
I0203 11:26:02.893119 139702527031040 logging_writer.py:48] [800] global_step=800, grad_norm=1.5896072387695312, loss=6.417484760284424
I0203 11:26:25.994431 139863983413056 spec.py:321] Evaluating on the training split.
I0203 11:26:36.896895 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 11:27:04.786565 139863983413056 spec.py:349] Evaluating on the test split.
I0203 11:27:06.424330 139863983413056 submission_runner.py:408] Time since start: 529.30s, 	Step: 851, 	{'train/accuracy': 0.034257810562849045, 'train/loss': 5.893809795379639, 'validation/accuracy': 0.027739999815821648, 'validation/loss': 5.958045482635498, 'validation/num_examples': 50000, 'test/accuracy': 0.023000001907348633, 'test/loss': 6.076109886169434, 'test/num_examples': 10000, 'score': 460.0015518665314, 'total_duration': 529.3016893863678, 'accumulated_submission_time': 460.0015518665314, 'accumulated_eval_time': 69.23552632331848, 'accumulated_logging_time': 0.01894378662109375}
I0203 11:27:06.439695 139702543816448 logging_writer.py:48] [851] accumulated_eval_time=69.235526, accumulated_logging_time=0.018944, accumulated_submission_time=460.001552, global_step=851, preemption_count=0, score=460.001552, test/accuracy=0.023000, test/loss=6.076110, test/num_examples=10000, total_duration=529.301689, train/accuracy=0.034258, train/loss=5.893810, validation/accuracy=0.027740, validation/loss=5.958045, validation/num_examples=50000
I0203 11:27:27.269312 139702527031040 logging_writer.py:48] [900] global_step=900, grad_norm=1.2343697547912598, loss=6.154626369476318
I0203 11:28:11.627784 139702543816448 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.469826579093933, loss=6.126753330230713
I0203 11:28:58.264119 139702527031040 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.065638780593872, loss=6.372529029846191
I0203 11:29:44.729834 139702543816448 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.914354145526886, loss=6.077780723571777
I0203 11:30:31.763831 139702527031040 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0949915647506714, loss=6.0106658935546875
I0203 11:31:18.532213 139702543816448 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9325736165046692, loss=6.620751857757568
I0203 11:32:05.206593 139702527031040 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9543834924697876, loss=6.568089485168457
I0203 11:32:51.669203 139702543816448 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.17545747756958, loss=5.722254753112793
I0203 11:33:38.400764 139702527031040 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8718817234039307, loss=6.088677883148193
I0203 11:34:06.436859 139863983413056 spec.py:321] Evaluating on the training split.
I0203 11:34:17.341969 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 11:34:48.805239 139863983413056 spec.py:349] Evaluating on the test split.
I0203 11:34:50.448902 139863983413056 submission_runner.py:408] Time since start: 993.33s, 	Step: 1762, 	{'train/accuracy': 0.07244140654802322, 'train/loss': 5.277771472930908, 'validation/accuracy': 0.06849999725818634, 'validation/loss': 5.329521656036377, 'validation/num_examples': 50000, 'test/accuracy': 0.05340000241994858, 'test/loss': 5.5414252281188965, 'test/num_examples': 10000, 'score': 879.9417262077332, 'total_duration': 993.3262569904327, 'accumulated_submission_time': 879.9417262077332, 'accumulated_eval_time': 113.24756622314453, 'accumulated_logging_time': 0.043344736099243164}
I0203 11:34:50.465932 139702543816448 logging_writer.py:48] [1762] accumulated_eval_time=113.247566, accumulated_logging_time=0.043345, accumulated_submission_time=879.941726, global_step=1762, preemption_count=0, score=879.941726, test/accuracy=0.053400, test/loss=5.541425, test/num_examples=10000, total_duration=993.326257, train/accuracy=0.072441, train/loss=5.277771, validation/accuracy=0.068500, validation/loss=5.329522, validation/num_examples=50000
I0203 11:35:06.721559 139702527031040 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8259152770042419, loss=6.37346076965332
I0203 11:35:50.393478 139702543816448 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0979453325271606, loss=5.70723819732666
I0203 11:36:36.673915 139702527031040 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.1962759494781494, loss=5.683844089508057
I0203 11:37:23.046124 139702543816448 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8866325616836548, loss=5.748126029968262
I0203 11:38:09.027374 139702527031040 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7592352032661438, loss=6.621973991394043
I0203 11:38:55.379716 139702543816448 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9219961166381836, loss=5.596906661987305
I0203 11:39:41.672267 139702527031040 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.0318683385849, loss=5.477217674255371
I0203 11:40:28.415044 139702543816448 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8577829599380493, loss=6.175538539886475
I0203 11:41:14.911300 139702527031040 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.0317561626434326, loss=5.519587993621826
I0203 11:41:50.905032 139863983413056 spec.py:321] Evaluating on the training split.
I0203 11:42:01.594948 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 11:42:32.897384 139863983413056 spec.py:349] Evaluating on the test split.
I0203 11:42:34.538523 139863983413056 submission_runner.py:408] Time since start: 1457.42s, 	Step: 2680, 	{'train/accuracy': 0.11164062470197678, 'train/loss': 4.909753799438477, 'validation/accuracy': 0.10255999863147736, 'validation/loss': 4.9625959396362305, 'validation/num_examples': 50000, 'test/accuracy': 0.08150000125169754, 'test/loss': 5.237037658691406, 'test/num_examples': 10000, 'score': 1300.3221225738525, 'total_duration': 1457.4158778190613, 'accumulated_submission_time': 1300.3221225738525, 'accumulated_eval_time': 156.88109421730042, 'accumulated_logging_time': 0.06997036933898926}
I0203 11:42:34.554841 139702543816448 logging_writer.py:48] [2680] accumulated_eval_time=156.881094, accumulated_logging_time=0.069970, accumulated_submission_time=1300.322123, global_step=2680, preemption_count=0, score=1300.322123, test/accuracy=0.081500, test/loss=5.237038, test/num_examples=10000, total_duration=1457.415878, train/accuracy=0.111641, train/loss=4.909754, validation/accuracy=0.102560, validation/loss=4.962596, validation/num_examples=50000
I0203 11:42:43.316124 139702527031040 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8455367088317871, loss=6.361429691314697
I0203 11:43:26.425060 139702543816448 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7103568911552429, loss=5.953795909881592
I0203 11:44:12.500965 139702527031040 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.134060263633728, loss=5.227036476135254
I0203 11:44:58.643882 139702543816448 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9150376319885254, loss=5.299949645996094
I0203 11:45:44.903424 139702527031040 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8386076092720032, loss=5.281569957733154
I0203 11:46:31.152732 139702543816448 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8508657813072205, loss=6.175346374511719
I0203 11:47:17.458809 139702527031040 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.091082215309143, loss=5.163763046264648
I0203 11:48:03.713441 139702543816448 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.0141369104385376, loss=5.185232639312744
I0203 11:48:50.080325 139702527031040 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9647085666656494, loss=5.331710338592529
I0203 11:49:34.616624 139863983413056 spec.py:321] Evaluating on the training split.
I0203 11:49:45.243774 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 11:50:13.198892 139863983413056 spec.py:349] Evaluating on the test split.
I0203 11:50:14.848047 139863983413056 submission_runner.py:408] Time since start: 1917.73s, 	Step: 3597, 	{'train/accuracy': 0.17169921100139618, 'train/loss': 4.300407409667969, 'validation/accuracy': 0.15811999142169952, 'validation/loss': 4.405792236328125, 'validation/num_examples': 50000, 'test/accuracy': 0.12030000239610672, 'test/loss': 4.7625908851623535, 'test/num_examples': 10000, 'score': 1720.324553012848, 'total_duration': 1917.7254054546356, 'accumulated_submission_time': 1720.324553012848, 'accumulated_eval_time': 197.1125226020813, 'accumulated_logging_time': 0.09761691093444824}
I0203 11:50:14.871858 139702543816448 logging_writer.py:48] [3597] accumulated_eval_time=197.112523, accumulated_logging_time=0.097617, accumulated_submission_time=1720.324553, global_step=3597, preemption_count=0, score=1720.324553, test/accuracy=0.120300, test/loss=4.762591, test/num_examples=10000, total_duration=1917.725405, train/accuracy=0.171699, train/loss=4.300407, validation/accuracy=0.158120, validation/loss=4.405792, validation/num_examples=50000
I0203 11:50:16.545761 139702527031040 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9438086152076721, loss=5.040762901306152
I0203 11:50:59.182856 139702543816448 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8958747982978821, loss=5.01273775100708
I0203 11:51:45.703702 139702527031040 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8225496411323547, loss=4.995363235473633
I0203 11:52:32.193124 139702543816448 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.0672225952148438, loss=5.083460330963135
I0203 11:53:18.471525 139702527031040 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.1159508228302002, loss=5.752025127410889
I0203 11:54:04.993759 139702543816448 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8212863802909851, loss=5.617069721221924
I0203 11:54:51.302629 139702527031040 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6442615389823914, loss=5.813440799713135
I0203 11:55:37.650578 139702543816448 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7531411647796631, loss=5.417860507965088
I0203 11:56:23.791222 139702527031040 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6589046716690063, loss=6.303141117095947
I0203 11:57:10.168454 139702543816448 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8780345916748047, loss=4.598004341125488
I0203 11:57:14.916749 139863983413056 spec.py:321] Evaluating on the training split.
I0203 11:57:25.516021 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 11:57:58.211045 139863983413056 spec.py:349] Evaluating on the test split.
I0203 11:57:59.850035 139863983413056 submission_runner.py:408] Time since start: 2382.73s, 	Step: 4512, 	{'train/accuracy': 0.2356249988079071, 'train/loss': 3.8305723667144775, 'validation/accuracy': 0.2140599936246872, 'validation/loss': 3.9363467693328857, 'validation/num_examples': 50000, 'test/accuracy': 0.16520000994205475, 'test/loss': 4.379038333892822, 'test/num_examples': 10000, 'score': 2140.311147928238, 'total_duration': 2382.7273893356323, 'accumulated_submission_time': 2140.311147928238, 'accumulated_eval_time': 242.04579830169678, 'accumulated_logging_time': 0.13115668296813965}
I0203 11:57:59.866666 139702527031040 logging_writer.py:48] [4512] accumulated_eval_time=242.045798, accumulated_logging_time=0.131157, accumulated_submission_time=2140.311148, global_step=4512, preemption_count=0, score=2140.311148, test/accuracy=0.165200, test/loss=4.379038, test/num_examples=10000, total_duration=2382.727389, train/accuracy=0.235625, train/loss=3.830572, validation/accuracy=0.214060, validation/loss=3.936347, validation/num_examples=50000
I0203 11:58:37.181505 139702543816448 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7161315083503723, loss=6.1304030418396
I0203 11:59:23.280007 139702527031040 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8602617979049683, loss=5.998776912689209
I0203 12:00:10.023113 139702543816448 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7200592756271362, loss=4.77745246887207
I0203 12:00:56.188334 139702527031040 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.9234135746955872, loss=4.63364315032959
I0203 12:01:42.578620 139702543816448 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8017137050628662, loss=4.483269691467285
I0203 12:02:29.310085 139702527031040 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8602249622344971, loss=4.416117191314697
I0203 12:03:15.686254 139702543816448 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7594650387763977, loss=4.385814666748047
I0203 12:04:01.936345 139702527031040 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.782014012336731, loss=4.749907970428467
I0203 12:04:48.466869 139702543816448 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.925089418888092, loss=4.397670745849609
I0203 12:05:00.062247 139863983413056 spec.py:321] Evaluating on the training split.
I0203 12:05:10.636614 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 12:05:42.094938 139863983413056 spec.py:349] Evaluating on the test split.
I0203 12:05:43.730806 139863983413056 submission_runner.py:408] Time since start: 2846.61s, 	Step: 5427, 	{'train/accuracy': 0.26445311307907104, 'train/loss': 3.6391592025756836, 'validation/accuracy': 0.2471199929714203, 'validation/loss': 3.742400884628296, 'validation/num_examples': 50000, 'test/accuracy': 0.18620000779628754, 'test/loss': 4.206192493438721, 'test/num_examples': 10000, 'score': 2560.448935985565, 'total_duration': 2846.608163833618, 'accumulated_submission_time': 2560.448935985565, 'accumulated_eval_time': 285.71435475349426, 'accumulated_logging_time': 0.15716123580932617}
I0203 12:05:43.747018 139702527031040 logging_writer.py:48] [5427] accumulated_eval_time=285.714355, accumulated_logging_time=0.157161, accumulated_submission_time=2560.448936, global_step=5427, preemption_count=0, score=2560.448936, test/accuracy=0.186200, test/loss=4.206192, test/num_examples=10000, total_duration=2846.608164, train/accuracy=0.264453, train/loss=3.639159, validation/accuracy=0.247120, validation/loss=3.742401, validation/num_examples=50000
I0203 12:06:14.650614 139702543816448 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8648523092269897, loss=4.601029396057129
I0203 12:07:00.212670 139702527031040 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.8425251245498657, loss=4.574732780456543
I0203 12:07:46.734706 139702543816448 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6376389265060425, loss=5.813008785247803
I0203 12:08:33.065974 139702527031040 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.0765838623046875, loss=4.315268516540527
I0203 12:09:19.539764 139702543816448 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7104735374450684, loss=5.118950366973877
I0203 12:10:05.762673 139702527031040 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8331696391105652, loss=4.5020222663879395
I0203 12:10:52.074307 139702543816448 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8056639432907104, loss=4.218576908111572
I0203 12:11:38.176812 139702527031040 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7267475724220276, loss=6.009871006011963
I0203 12:12:24.524991 139702543816448 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.972801148891449, loss=4.201082229614258
I0203 12:12:44.149391 139863983413056 spec.py:321] Evaluating on the training split.
I0203 12:12:54.908745 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 12:13:28.052082 139863983413056 spec.py:349] Evaluating on the test split.
I0203 12:13:29.686294 139863983413056 submission_runner.py:408] Time since start: 3312.56s, 	Step: 6344, 	{'train/accuracy': 0.3071679472923279, 'train/loss': 3.325169086456299, 'validation/accuracy': 0.27605998516082764, 'validation/loss': 3.493992328643799, 'validation/num_examples': 50000, 'test/accuracy': 0.21490001678466797, 'test/loss': 4.034944534301758, 'test/num_examples': 10000, 'score': 2980.7929248809814, 'total_duration': 3312.563648700714, 'accumulated_submission_time': 2980.7929248809814, 'accumulated_eval_time': 331.25124192237854, 'accumulated_logging_time': 0.1834251880645752}
I0203 12:13:29.702918 139702527031040 logging_writer.py:48] [6344] accumulated_eval_time=331.251242, accumulated_logging_time=0.183425, accumulated_submission_time=2980.792925, global_step=6344, preemption_count=0, score=2980.792925, test/accuracy=0.214900, test/loss=4.034945, test/num_examples=10000, total_duration=3312.563649, train/accuracy=0.307168, train/loss=3.325169, validation/accuracy=0.276060, validation/loss=3.493992, validation/num_examples=50000
I0203 12:13:53.485737 139702543816448 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8778371810913086, loss=4.064633846282959
I0203 12:14:38.274515 139702527031040 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8881415724754333, loss=4.5201897621154785
I0203 12:15:24.549591 139702543816448 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7468044757843018, loss=4.710378170013428
I0203 12:16:10.753068 139702527031040 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6581149697303772, loss=5.941772937774658
I0203 12:16:56.964905 139702543816448 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8722521066665649, loss=4.08803129196167
I0203 12:17:42.958012 139702527031040 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7725175023078918, loss=4.382660865783691
I0203 12:18:29.220273 139702543816448 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8222447633743286, loss=4.020928382873535
I0203 12:19:15.413845 139702527031040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.771710216999054, loss=5.994627952575684
I0203 12:20:01.726749 139702543816448 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6093837022781372, loss=5.190196514129639
I0203 12:20:30.075827 139863983413056 spec.py:321] Evaluating on the training split.
I0203 12:20:40.565982 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 12:21:07.799682 139863983413056 spec.py:349] Evaluating on the test split.
I0203 12:21:09.435515 139863983413056 submission_runner.py:408] Time since start: 3772.31s, 	Step: 7263, 	{'train/accuracy': 0.34326171875, 'train/loss': 3.0648884773254395, 'validation/accuracy': 0.3183799982070923, 'validation/loss': 3.202389717102051, 'validation/num_examples': 50000, 'test/accuracy': 0.24670001864433289, 'test/loss': 3.7745606899261475, 'test/num_examples': 10000, 'score': 3401.106611251831, 'total_duration': 3772.3128740787506, 'accumulated_submission_time': 3401.106611251831, 'accumulated_eval_time': 370.6109387874603, 'accumulated_logging_time': 0.21096396446228027}
I0203 12:21:09.452208 139702527031040 logging_writer.py:48] [7263] accumulated_eval_time=370.610939, accumulated_logging_time=0.210964, accumulated_submission_time=3401.106611, global_step=7263, preemption_count=0, score=3401.106611, test/accuracy=0.246700, test/loss=3.774561, test/num_examples=10000, total_duration=3772.312874, train/accuracy=0.343262, train/loss=3.064888, validation/accuracy=0.318380, validation/loss=3.202390, validation/num_examples=50000
I0203 12:21:25.302255 139702543816448 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5321136116981506, loss=6.04217529296875
I0203 12:22:09.484958 139702527031040 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5450034737586975, loss=5.777462482452393
I0203 12:22:55.982311 139702543816448 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5934554934501648, loss=5.6995110511779785
I0203 12:23:42.181029 139702527031040 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8052859306335449, loss=4.3082075119018555
I0203 12:24:28.384390 139702543816448 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8950766324996948, loss=4.300976276397705
I0203 12:25:14.691890 139702527031040 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6735624670982361, loss=6.035606384277344
I0203 12:26:00.874934 139702543816448 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7334468960762024, loss=4.481833457946777
I0203 12:26:47.254583 139702527031040 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7766801714897156, loss=4.013165473937988
I0203 12:27:33.824653 139702543816448 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8682312965393066, loss=4.016997337341309
I0203 12:28:09.693119 139863983413056 spec.py:321] Evaluating on the training split.
I0203 12:28:20.337901 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 12:28:53.962545 139863983413056 spec.py:349] Evaluating on the test split.
I0203 12:28:55.604115 139863983413056 submission_runner.py:408] Time since start: 4238.48s, 	Step: 8179, 	{'train/accuracy': 0.3716796934604645, 'train/loss': 2.930692672729492, 'validation/accuracy': 0.3402999937534332, 'validation/loss': 3.099444627761841, 'validation/num_examples': 50000, 'test/accuracy': 0.25950002670288086, 'test/loss': 3.666433095932007, 'test/num_examples': 10000, 'score': 3821.2906200885773, 'total_duration': 4238.481454372406, 'accumulated_submission_time': 3821.2906200885773, 'accumulated_eval_time': 416.5219187736511, 'accumulated_logging_time': 0.23738980293273926}
I0203 12:28:55.623618 139702527031040 logging_writer.py:48] [8179] accumulated_eval_time=416.521919, accumulated_logging_time=0.237390, accumulated_submission_time=3821.290620, global_step=8179, preemption_count=0, score=3821.290620, test/accuracy=0.259500, test/loss=3.666433, test/num_examples=10000, total_duration=4238.481454, train/accuracy=0.371680, train/loss=2.930693, validation/accuracy=0.340300, validation/loss=3.099445, validation/num_examples=50000
I0203 12:29:04.794874 139702543816448 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.8063001036643982, loss=4.074224472045898
I0203 12:29:48.132950 139702527031040 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6418281197547913, loss=5.326871395111084
I0203 12:30:34.199333 139702543816448 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.0762689113616943, loss=3.964694023132324
I0203 12:31:20.447057 139702527031040 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.703572690486908, loss=4.4104838371276855
I0203 12:32:06.730503 139702543816448 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.829249382019043, loss=3.7657363414764404
I0203 12:32:52.636375 139702527031040 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8432143330574036, loss=3.9468905925750732
I0203 12:33:39.077437 139702543816448 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.8733055591583252, loss=3.867238759994507
I0203 12:34:25.259381 139702527031040 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.9045759439468384, loss=3.7253689765930176
I0203 12:35:11.633898 139702543816448 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.9373178482055664, loss=3.817317008972168
I0203 12:35:55.779698 139863983413056 spec.py:321] Evaluating on the training split.
I0203 12:36:06.489533 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 12:36:34.388596 139863983413056 spec.py:349] Evaluating on the test split.
I0203 12:36:36.039055 139863983413056 submission_runner.py:408] Time since start: 4698.92s, 	Step: 9097, 	{'train/accuracy': 0.3936523497104645, 'train/loss': 2.7883851528167725, 'validation/accuracy': 0.3587999939918518, 'validation/loss': 2.984739303588867, 'validation/num_examples': 50000, 'test/accuracy': 0.28550001978874207, 'test/loss': 3.554089307785034, 'test/num_examples': 10000, 'score': 4241.387323856354, 'total_duration': 4698.91641163826, 'accumulated_submission_time': 4241.387323856354, 'accumulated_eval_time': 456.7812805175781, 'accumulated_logging_time': 0.26792001724243164}
I0203 12:36:36.056760 139702527031040 logging_writer.py:48] [9097] accumulated_eval_time=456.781281, accumulated_logging_time=0.267920, accumulated_submission_time=4241.387324, global_step=9097, preemption_count=0, score=4241.387324, test/accuracy=0.285500, test/loss=3.554089, test/num_examples=10000, total_duration=4698.916412, train/accuracy=0.393652, train/loss=2.788385, validation/accuracy=0.358800, validation/loss=2.984739, validation/num_examples=50000
I0203 12:36:37.729502 139702543816448 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.772339940071106, loss=3.814423084259033
I0203 12:37:20.264305 139702527031040 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8617860078811646, loss=3.8581383228302
I0203 12:38:06.477370 139702543816448 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9143542647361755, loss=3.6837704181671143
I0203 12:38:52.879083 139702527031040 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.9919195175170898, loss=3.7997548580169678
I0203 12:39:38.837209 139702543816448 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7412509322166443, loss=4.680950164794922
I0203 12:40:25.363252 139702527031040 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7095078229904175, loss=4.894421100616455
I0203 12:41:11.364101 139702543816448 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.0590890645980835, loss=3.900954008102417
I0203 12:41:57.652472 139702527031040 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7267805337905884, loss=5.323526382446289
I0203 12:42:43.860516 139702543816448 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9015602469444275, loss=3.702305555343628
I0203 12:43:30.116691 139702527031040 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.682062029838562, loss=5.957408428192139
I0203 12:43:36.384361 139863983413056 spec.py:321] Evaluating on the training split.
I0203 12:43:47.033274 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 12:44:13.324699 139863983413056 spec.py:349] Evaluating on the test split.
I0203 12:44:14.968641 139863983413056 submission_runner.py:408] Time since start: 5157.85s, 	Step: 10015, 	{'train/accuracy': 0.4039062261581421, 'train/loss': 2.737269163131714, 'validation/accuracy': 0.3750399947166443, 'validation/loss': 2.905771493911743, 'validation/num_examples': 50000, 'test/accuracy': 0.28710001707077026, 'test/loss': 3.4907071590423584, 'test/num_examples': 10000, 'score': 4661.656677007675, 'total_duration': 5157.845973968506, 'accumulated_submission_time': 4661.656677007675, 'accumulated_eval_time': 495.365522146225, 'accumulated_logging_time': 0.29549193382263184}
I0203 12:44:14.984713 139702543816448 logging_writer.py:48] [10015] accumulated_eval_time=495.365522, accumulated_logging_time=0.295492, accumulated_submission_time=4661.656677, global_step=10015, preemption_count=0, score=4661.656677, test/accuracy=0.287100, test/loss=3.490707, test/num_examples=10000, total_duration=5157.845974, train/accuracy=0.403906, train/loss=2.737269, validation/accuracy=0.375040, validation/loss=2.905771, validation/num_examples=50000
I0203 12:44:50.868553 139702527031040 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7868652939796448, loss=5.418969631195068
I0203 12:45:36.913493 139702543816448 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8705279231071472, loss=4.053795337677002
I0203 12:46:23.331458 139702527031040 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.608879804611206, loss=5.274396896362305
I0203 12:47:09.605651 139702543816448 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7066664695739746, loss=4.647432804107666
I0203 12:47:55.773937 139702527031040 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.882070779800415, loss=3.8936767578125
I0203 12:48:41.941963 139702543816448 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8397086262702942, loss=3.6801629066467285
I0203 12:49:27.952097 139702527031040 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8977341651916504, loss=3.540579080581665
I0203 12:50:14.129275 139702543816448 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7183797955513, loss=5.751096725463867
I0203 12:51:00.388623 139702527031040 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9886684417724609, loss=3.6122236251831055
I0203 12:51:15.431581 139863983413056 spec.py:321] Evaluating on the training split.
I0203 12:51:26.014751 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 12:51:56.141987 139863983413056 spec.py:349] Evaluating on the test split.
I0203 12:51:57.777668 139863983413056 submission_runner.py:408] Time since start: 5620.66s, 	Step: 10934, 	{'train/accuracy': 0.423164039850235, 'train/loss': 2.606679677963257, 'validation/accuracy': 0.3898399770259857, 'validation/loss': 2.784226894378662, 'validation/num_examples': 50000, 'test/accuracy': 0.2964000105857849, 'test/loss': 3.4158220291137695, 'test/num_examples': 10000, 'score': 5082.045745134354, 'total_duration': 5620.655026435852, 'accumulated_submission_time': 5082.045745134354, 'accumulated_eval_time': 537.7116749286652, 'accumulated_logging_time': 0.320845365524292}
I0203 12:51:57.794339 139702543816448 logging_writer.py:48] [10934] accumulated_eval_time=537.711675, accumulated_logging_time=0.320845, accumulated_submission_time=5082.045745, global_step=10934, preemption_count=0, score=5082.045745, test/accuracy=0.296400, test/loss=3.415822, test/num_examples=10000, total_duration=5620.655026, train/accuracy=0.423164, train/loss=2.606680, validation/accuracy=0.389840, validation/loss=2.784227, validation/num_examples=50000
I0203 12:52:25.758219 139702527031040 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9111130833625793, loss=3.6677427291870117
I0203 12:53:11.096136 139702543816448 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7124606966972351, loss=4.929272651672363
I0203 12:53:57.411352 139702527031040 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.2438997030258179, loss=3.630439043045044
I0203 12:54:43.895169 139702543816448 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7112497091293335, loss=5.367686748504639
I0203 12:55:29.806429 139702527031040 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.8928307890892029, loss=3.8701891899108887
I0203 12:56:16.100620 139702543816448 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6855998039245605, loss=5.468418121337891
I0203 12:57:02.292739 139702527031040 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7927471399307251, loss=4.498906135559082
I0203 12:57:48.398297 139702543816448 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8788571357727051, loss=4.1509528160095215
I0203 12:58:34.529571 139702527031040 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.0802403688430786, loss=3.609539270401001
I0203 12:58:58.205022 139863983413056 spec.py:321] Evaluating on the training split.
I0203 12:59:08.906658 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 12:59:39.584064 139863983413056 spec.py:349] Evaluating on the test split.
I0203 12:59:41.222560 139863983413056 submission_runner.py:408] Time since start: 6084.10s, 	Step: 11853, 	{'train/accuracy': 0.4403710961341858, 'train/loss': 2.523897886276245, 'validation/accuracy': 0.4095799922943115, 'validation/loss': 2.7042064666748047, 'validation/num_examples': 50000, 'test/accuracy': 0.31450000405311584, 'test/loss': 3.3171584606170654, 'test/num_examples': 10000, 'score': 5502.399238586426, 'total_duration': 6084.099897861481, 'accumulated_submission_time': 5502.399238586426, 'accumulated_eval_time': 580.7291917800903, 'accumulated_logging_time': 0.34621691703796387}
I0203 12:59:41.243489 139702543816448 logging_writer.py:48] [11853] accumulated_eval_time=580.729192, accumulated_logging_time=0.346217, accumulated_submission_time=5502.399239, global_step=11853, preemption_count=0, score=5502.399239, test/accuracy=0.314500, test/loss=3.317158, test/num_examples=10000, total_duration=6084.099898, train/accuracy=0.440371, train/loss=2.523898, validation/accuracy=0.409580, validation/loss=2.704206, validation/num_examples=50000
I0203 13:00:01.261969 139702527031040 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.8722807765007019, loss=3.404136896133423
I0203 13:00:45.877395 139702543816448 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8100907802581787, loss=4.231626033782959
I0203 13:01:32.260974 139702527031040 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.9821921586990356, loss=3.4446427822113037
I0203 13:02:18.473145 139702543816448 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.9009311199188232, loss=3.7541046142578125
I0203 13:03:04.791428 139702527031040 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.0146230459213257, loss=3.540710926055908
I0203 13:03:50.852154 139702543816448 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.9488963484764099, loss=3.670779228210449
I0203 13:04:37.351226 139702527031040 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7770348191261292, loss=3.927192449569702
I0203 13:05:23.686367 139702543816448 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.0115116834640503, loss=3.5445384979248047
I0203 13:06:09.834028 139702527031040 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.951888382434845, loss=3.448646068572998
I0203 13:06:41.475445 139863983413056 spec.py:321] Evaluating on the training split.
I0203 13:06:52.057526 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 13:07:23.466890 139863983413056 spec.py:349] Evaluating on the test split.
I0203 13:07:25.109166 139863983413056 submission_runner.py:408] Time since start: 6547.99s, 	Step: 12770, 	{'train/accuracy': 0.47035154700279236, 'train/loss': 2.3473031520843506, 'validation/accuracy': 0.41335999965667725, 'validation/loss': 2.656049966812134, 'validation/num_examples': 50000, 'test/accuracy': 0.3165000081062317, 'test/loss': 3.3088340759277344, 'test/num_examples': 10000, 'score': 5922.57227897644, 'total_duration': 6547.986525058746, 'accumulated_submission_time': 5922.57227897644, 'accumulated_eval_time': 624.3629055023193, 'accumulated_logging_time': 0.3769981861114502}
I0203 13:07:25.126739 139702543816448 logging_writer.py:48] [12770] accumulated_eval_time=624.362906, accumulated_logging_time=0.376998, accumulated_submission_time=5922.572279, global_step=12770, preemption_count=0, score=5922.572279, test/accuracy=0.316500, test/loss=3.308834, test/num_examples=10000, total_duration=6547.986525, train/accuracy=0.470352, train/loss=2.347303, validation/accuracy=0.413360, validation/loss=2.656050, validation/num_examples=50000
I0203 13:07:38.049481 139702527031040 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.8971883654594421, loss=3.5047309398651123
I0203 13:08:21.685374 139702543816448 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.0331000089645386, loss=3.5809223651885986
I0203 13:09:07.768419 139702527031040 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8028284907341003, loss=3.9199678897857666
I0203 13:09:54.238860 139702543816448 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9138471484184265, loss=3.9164183139801025
I0203 13:10:40.297995 139702527031040 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.9558134078979492, loss=3.4103236198425293
I0203 13:11:26.433411 139702543816448 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9976141452789307, loss=3.5811409950256348
I0203 13:12:12.630028 139702527031040 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.7256345748901367, loss=5.178220272064209
I0203 13:12:58.653567 139702543816448 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.933643102645874, loss=5.057770252227783
I0203 13:13:44.829529 139702527031040 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.0203570127487183, loss=3.5382156372070312
I0203 13:14:25.186204 139863983413056 spec.py:321] Evaluating on the training split.
I0203 13:14:35.721542 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 13:15:06.272251 139863983413056 spec.py:349] Evaluating on the test split.
I0203 13:15:07.908475 139863983413056 submission_runner.py:408] Time since start: 7010.79s, 	Step: 13689, 	{'train/accuracy': 0.4599999785423279, 'train/loss': 2.3901679515838623, 'validation/accuracy': 0.42781999707221985, 'validation/loss': 2.5512709617614746, 'validation/num_examples': 50000, 'test/accuracy': 0.33740001916885376, 'test/loss': 3.1996219158172607, 'test/num_examples': 10000, 'score': 6342.573100805283, 'total_duration': 7010.78583407402, 'accumulated_submission_time': 6342.573100805283, 'accumulated_eval_time': 667.0851843357086, 'accumulated_logging_time': 0.40462446212768555}
I0203 13:15:07.930122 139702543816448 logging_writer.py:48] [13689] accumulated_eval_time=667.085184, accumulated_logging_time=0.404624, accumulated_submission_time=6342.573101, global_step=13689, preemption_count=0, score=6342.573101, test/accuracy=0.337400, test/loss=3.199622, test/num_examples=10000, total_duration=7010.785834, train/accuracy=0.460000, train/loss=2.390168, validation/accuracy=0.427820, validation/loss=2.551271, validation/num_examples=50000
I0203 13:15:12.995442 139702527031040 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9553179144859314, loss=3.522620439529419
I0203 13:15:56.099476 139702543816448 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6849808692932129, loss=5.70970344543457
I0203 13:16:42.444557 139702527031040 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.8006699681282043, loss=4.3953633308410645
I0203 13:17:29.088559 139702543816448 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.9104121923446655, loss=3.5579051971435547
I0203 13:18:14.915471 139702527031040 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.939248263835907, loss=3.2933006286621094
I0203 13:19:01.034352 139702543816448 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.0017057657241821, loss=3.8922479152679443
I0203 13:19:47.189996 139702527031040 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.3531757593154907, loss=3.3948116302490234
I0203 13:20:33.506123 139702543816448 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.863273561000824, loss=4.415146827697754
I0203 13:21:19.773370 139702527031040 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.1625529527664185, loss=3.4772326946258545
I0203 13:22:06.132765 139702543816448 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.890887439250946, loss=4.4585490226745605
I0203 13:22:08.147828 139863983413056 spec.py:321] Evaluating on the training split.
I0203 13:22:19.013688 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 13:22:51.213509 139863983413056 spec.py:349] Evaluating on the test split.
I0203 13:22:52.847300 139863983413056 submission_runner.py:408] Time since start: 7475.72s, 	Step: 14606, 	{'train/accuracy': 0.4755273461341858, 'train/loss': 2.356476306915283, 'validation/accuracy': 0.4366599917411804, 'validation/loss': 2.5436606407165527, 'validation/num_examples': 50000, 'test/accuracy': 0.3416000306606293, 'test/loss': 3.1861348152160645, 'test/num_examples': 10000, 'score': 6762.732401609421, 'total_duration': 7475.724648714066, 'accumulated_submission_time': 6762.732401609421, 'accumulated_eval_time': 711.7846443653107, 'accumulated_logging_time': 0.43683457374572754}
I0203 13:22:52.865114 139702527031040 logging_writer.py:48] [14606] accumulated_eval_time=711.784644, accumulated_logging_time=0.436835, accumulated_submission_time=6762.732402, global_step=14606, preemption_count=0, score=6762.732402, test/accuracy=0.341600, test/loss=3.186135, test/num_examples=10000, total_duration=7475.724649, train/accuracy=0.475527, train/loss=2.356476, validation/accuracy=0.436660, validation/loss=2.543661, validation/num_examples=50000
I0203 13:23:32.872379 139702543816448 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6803330779075623, loss=5.792836666107178
I0203 13:24:18.991779 139702527031040 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.0118021965026855, loss=3.3575279712677
I0203 13:25:05.342271 139702543816448 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.023691177368164, loss=3.797170639038086
I0203 13:25:51.709135 139702527031040 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8022938370704651, loss=5.746588706970215
I0203 13:26:37.640654 139702543816448 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.0097079277038574, loss=3.5222134590148926
I0203 13:27:23.893099 139702527031040 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.0226809978485107, loss=3.336364984512329
I0203 13:28:10.114631 139702543816448 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.0286710262298584, loss=3.338737726211548
I0203 13:28:56.403987 139702527031040 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.9973735213279724, loss=3.425906181335449
I0203 13:29:42.825688 139702543816448 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7870989441871643, loss=5.529419422149658
I0203 13:29:52.890927 139863983413056 spec.py:321] Evaluating on the training split.
I0203 13:30:03.481776 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 13:30:35.406596 139863983413056 spec.py:349] Evaluating on the test split.
I0203 13:30:37.060070 139863983413056 submission_runner.py:408] Time since start: 7939.94s, 	Step: 15523, 	{'train/accuracy': 0.49757811427116394, 'train/loss': 2.191378593444824, 'validation/accuracy': 0.44443997740745544, 'validation/loss': 2.4651525020599365, 'validation/num_examples': 50000, 'test/accuracy': 0.34470000863075256, 'test/loss': 3.0966248512268066, 'test/num_examples': 10000, 'score': 7182.700782775879, 'total_duration': 7939.937408447266, 'accumulated_submission_time': 7182.700782775879, 'accumulated_eval_time': 755.9537699222565, 'accumulated_logging_time': 0.4638805389404297}
I0203 13:30:37.080309 139702527031040 logging_writer.py:48] [15523] accumulated_eval_time=755.953770, accumulated_logging_time=0.463881, accumulated_submission_time=7182.700783, global_step=15523, preemption_count=0, score=7182.700783, test/accuracy=0.344700, test/loss=3.096625, test/num_examples=10000, total_duration=7939.937408, train/accuracy=0.497578, train/loss=2.191379, validation/accuracy=0.444440, validation/loss=2.465153, validation/num_examples=50000
I0203 13:31:09.640247 139702543816448 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.944422721862793, loss=3.847369909286499
I0203 13:31:55.268865 139702527031040 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.0363942384719849, loss=3.3318443298339844
I0203 13:32:41.454932 139702543816448 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0221396684646606, loss=3.424368381500244
I0203 13:33:27.524684 139702527031040 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.7800648808479309, loss=4.0272698402404785
I0203 13:34:13.854533 139702543816448 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9851267337799072, loss=3.844505786895752
I0203 13:35:00.088670 139702527031040 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.8434166312217712, loss=4.030141830444336
I0203 13:35:46.251707 139702543816448 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.901730477809906, loss=4.760365962982178
I0203 13:36:32.820526 139702527031040 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9872183203697205, loss=3.307311534881592
I0203 13:37:19.191113 139702543816448 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.8154845833778381, loss=5.703171253204346
I0203 13:37:37.290563 139863983413056 spec.py:321] Evaluating on the training split.
I0203 13:37:48.231097 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 13:38:20.504353 139863983413056 spec.py:349] Evaluating on the test split.
I0203 13:38:22.147460 139863983413056 submission_runner.py:408] Time since start: 8405.02s, 	Step: 16441, 	{'train/accuracy': 0.486152321100235, 'train/loss': 2.2722318172454834, 'validation/accuracy': 0.45513999462127686, 'validation/loss': 2.4447381496429443, 'validation/num_examples': 50000, 'test/accuracy': 0.3492000102996826, 'test/loss': 3.093480348587036, 'test/num_examples': 10000, 'score': 7602.853166103363, 'total_duration': 8405.024823188782, 'accumulated_submission_time': 7602.853166103363, 'accumulated_eval_time': 800.8106706142426, 'accumulated_logging_time': 0.4935455322265625}
I0203 13:38:22.165080 139702527031040 logging_writer.py:48] [16441] accumulated_eval_time=800.810671, accumulated_logging_time=0.493546, accumulated_submission_time=7602.853166, global_step=16441, preemption_count=0, score=7602.853166, test/accuracy=0.349200, test/loss=3.093480, test/num_examples=10000, total_duration=8405.024823, train/accuracy=0.486152, train/loss=2.272232, validation/accuracy=0.455140, validation/loss=2.444738, validation/num_examples=50000
I0203 13:38:47.225201 139702543816448 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.0503374338150024, loss=3.3532867431640625
I0203 13:39:32.034137 139702527031040 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.806981086730957, loss=5.5389580726623535
I0203 13:40:18.432672 139702543816448 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.8083229660987854, loss=5.361333847045898
I0203 13:41:04.794166 139702527031040 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.9736477136611938, loss=3.7087244987487793
I0203 13:41:51.006943 139702543816448 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7280773520469666, loss=5.532514572143555
I0203 13:42:37.282366 139702527031040 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1163955926895142, loss=3.3320114612579346
I0203 13:43:23.654180 139702543816448 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9981054067611694, loss=3.2797772884368896
I0203 13:44:09.756145 139702527031040 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9778501987457275, loss=3.3606295585632324
I0203 13:44:55.988740 139702543816448 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0737515687942505, loss=3.3890058994293213
I0203 13:45:22.599532 139863983413056 spec.py:321] Evaluating on the training split.
I0203 13:45:33.123720 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 13:46:01.141684 139863983413056 spec.py:349] Evaluating on the test split.
I0203 13:46:02.795333 139863983413056 submission_runner.py:408] Time since start: 8865.67s, 	Step: 17359, 	{'train/accuracy': 0.5013867020606995, 'train/loss': 2.1820547580718994, 'validation/accuracy': 0.4605799913406372, 'validation/loss': 2.3915188312530518, 'validation/num_examples': 50000, 'test/accuracy': 0.3612000048160553, 'test/loss': 3.0269052982330322, 'test/num_examples': 10000, 'score': 8023.229565858841, 'total_duration': 8865.672659635544, 'accumulated_submission_time': 8023.229565858841, 'accumulated_eval_time': 841.0064516067505, 'accumulated_logging_time': 0.5205333232879639}
I0203 13:46:02.821589 139702527031040 logging_writer.py:48] [17359] accumulated_eval_time=841.006452, accumulated_logging_time=0.520533, accumulated_submission_time=8023.229566, global_step=17359, preemption_count=0, score=8023.229566, test/accuracy=0.361200, test/loss=3.026905, test/num_examples=10000, total_duration=8865.672660, train/accuracy=0.501387, train/loss=2.182055, validation/accuracy=0.460580, validation/loss=2.391519, validation/num_examples=50000
I0203 13:46:20.332455 139702543816448 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.0157854557037354, loss=3.056156635284424
I0203 13:47:04.973253 139702527031040 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.095510721206665, loss=3.2894067764282227
I0203 13:47:51.456973 139702543816448 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.2057781219482422, loss=3.858332395553589
I0203 13:48:37.864945 139702527031040 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.0472725629806519, loss=3.616825819015503
I0203 13:49:24.487208 139702543816448 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.2725986242294312, loss=3.2378036975860596
I0203 13:50:10.957308 139702527031040 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.0510172843933105, loss=3.3382270336151123
I0203 13:50:57.393317 139702543816448 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.920251190662384, loss=3.4858992099761963
I0203 13:51:43.611990 139702527031040 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.0852911472320557, loss=3.1347899436950684
I0203 13:52:30.071753 139702543816448 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7603349685668945, loss=4.50544548034668
I0203 13:53:03.258646 139863983413056 spec.py:321] Evaluating on the training split.
I0203 13:53:13.613942 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 13:53:45.826794 139863983413056 spec.py:349] Evaluating on the test split.
I0203 13:53:47.479721 139863983413056 submission_runner.py:408] Time since start: 9330.36s, 	Step: 18273, 	{'train/accuracy': 0.5225195288658142, 'train/loss': 2.098334789276123, 'validation/accuracy': 0.4728599786758423, 'validation/loss': 2.336740732192993, 'validation/num_examples': 50000, 'test/accuracy': 0.36230000853538513, 'test/loss': 3.014296293258667, 'test/num_examples': 10000, 'score': 8443.607513904572, 'total_duration': 9330.35705280304, 'accumulated_submission_time': 8443.607513904572, 'accumulated_eval_time': 885.2275066375732, 'accumulated_logging_time': 0.5578651428222656}
I0203 13:53:47.504599 139702527031040 logging_writer.py:48] [18273] accumulated_eval_time=885.227507, accumulated_logging_time=0.557865, accumulated_submission_time=8443.607514, global_step=18273, preemption_count=0, score=8443.607514, test/accuracy=0.362300, test/loss=3.014296, test/num_examples=10000, total_duration=9330.357053, train/accuracy=0.522520, train/loss=2.098335, validation/accuracy=0.472860, validation/loss=2.336741, validation/num_examples=50000
I0203 13:53:59.181413 139702543816448 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.0574220418930054, loss=3.2500717639923096
I0203 13:54:42.934586 139702527031040 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.2299212217330933, loss=3.513308048248291
I0203 13:55:29.223238 139702543816448 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.9900044202804565, loss=3.2173349857330322
I0203 13:56:15.533575 139702527031040 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.9577952027320862, loss=3.353630542755127
I0203 13:57:01.527913 139702543816448 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.0136152505874634, loss=3.2324421405792236
I0203 13:57:48.098698 139702527031040 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.9224457144737244, loss=3.9209518432617188
I0203 13:58:34.312486 139702543816448 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.2150040864944458, loss=3.141749858856201
I0203 13:59:20.471189 139702527031040 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7086838483810425, loss=5.52351188659668
I0203 14:00:06.996652 139702543816448 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.175871729850769, loss=3.1625242233276367
I0203 14:00:47.721498 139863983413056 spec.py:321] Evaluating on the training split.
I0203 14:00:58.275943 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 14:01:30.119051 139863983413056 spec.py:349] Evaluating on the test split.
I0203 14:01:31.760855 139863983413056 submission_runner.py:408] Time since start: 9794.64s, 	Step: 19190, 	{'train/accuracy': 0.5135741829872131, 'train/loss': 2.104053258895874, 'validation/accuracy': 0.47693997621536255, 'validation/loss': 2.3006768226623535, 'validation/num_examples': 50000, 'test/accuracy': 0.37050002813339233, 'test/loss': 2.9391238689422607, 'test/num_examples': 10000, 'score': 8863.76538324356, 'total_duration': 9794.63821029663, 'accumulated_submission_time': 8863.76538324356, 'accumulated_eval_time': 929.2668771743774, 'accumulated_logging_time': 0.5929629802703857}
I0203 14:01:31.780448 139702527031040 logging_writer.py:48] [19190] accumulated_eval_time=929.266877, accumulated_logging_time=0.592963, accumulated_submission_time=8863.765383, global_step=19190, preemption_count=0, score=8863.765383, test/accuracy=0.370500, test/loss=2.939124, test/num_examples=10000, total_duration=9794.638210, train/accuracy=0.513574, train/loss=2.104053, validation/accuracy=0.476940, validation/loss=2.300677, validation/num_examples=50000
I0203 14:01:36.373049 139702543816448 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.0680891275405884, loss=3.221698045730591
I0203 14:02:19.147604 139702527031040 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.8725838661193848, loss=4.001114368438721
I0203 14:03:05.718904 139702543816448 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0470759868621826, loss=3.2029871940612793
I0203 14:03:52.152996 139702527031040 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8920486569404602, loss=4.837192535400391
I0203 14:04:38.313472 139702543816448 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.8157893419265747, loss=4.813333988189697
I0203 14:05:24.734633 139702527031040 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0347329378128052, loss=3.2214832305908203
I0203 14:06:11.218064 139702543816448 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.0340888500213623, loss=3.1577553749084473
I0203 14:06:57.522841 139702527031040 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.4669547080993652, loss=3.2728700637817383
I0203 14:07:43.874126 139702543816448 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.2346727848052979, loss=3.153618335723877
I0203 14:08:30.386516 139702527031040 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.9295744299888611, loss=3.1517579555511475
I0203 14:08:31.905830 139863983413056 spec.py:321] Evaluating on the training split.
I0203 14:08:42.396467 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 14:09:10.896991 139863983413056 spec.py:349] Evaluating on the test split.
I0203 14:09:12.530249 139863983413056 submission_runner.py:408] Time since start: 10255.41s, 	Step: 20105, 	{'train/accuracy': 0.5260156393051147, 'train/loss': 2.0405845642089844, 'validation/accuracy': 0.4841799736022949, 'validation/loss': 2.2629568576812744, 'validation/num_examples': 50000, 'test/accuracy': 0.3801000118255615, 'test/loss': 2.9380943775177, 'test/num_examples': 10000, 'score': 9283.831866025925, 'total_duration': 10255.40760755539, 'accumulated_submission_time': 9283.831866025925, 'accumulated_eval_time': 969.8912837505341, 'accumulated_logging_time': 0.623236894607544}
I0203 14:09:12.549288 139702543816448 logging_writer.py:48] [20105] accumulated_eval_time=969.891284, accumulated_logging_time=0.623237, accumulated_submission_time=9283.831866, global_step=20105, preemption_count=0, score=9283.831866, test/accuracy=0.380100, test/loss=2.938094, test/num_examples=10000, total_duration=10255.407608, train/accuracy=0.526016, train/loss=2.040585, validation/accuracy=0.484180, validation/loss=2.262957, validation/num_examples=50000
I0203 14:09:53.155302 139702527031040 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8132612705230713, loss=4.552373886108398
I0203 14:10:39.175652 139702543816448 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8056975603103638, loss=4.451532363891602
I0203 14:11:25.697261 139702527031040 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.0702441930770874, loss=3.08418869972229
I0203 14:12:11.871091 139702543816448 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.004684567451477, loss=3.7629475593566895
I0203 14:12:58.017154 139702527031040 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.0349012613296509, loss=3.3507907390594482
I0203 14:13:44.338179 139702543816448 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.1048704385757446, loss=3.0963869094848633
I0203 14:14:30.549654 139702527031040 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.8315117359161377, loss=3.94645357131958
I0203 14:15:16.892637 139702543816448 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.14280366897583, loss=3.0447659492492676
I0203 14:16:03.226927 139702527031040 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8220562934875488, loss=5.046961784362793
I0203 14:16:12.729476 139863983413056 spec.py:321] Evaluating on the training split.
I0203 14:16:23.364872 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 14:16:54.452718 139863983413056 spec.py:349] Evaluating on the test split.
I0203 14:16:56.094145 139863983413056 submission_runner.py:408] Time since start: 10718.97s, 	Step: 21022, 	{'train/accuracy': 0.53369140625, 'train/loss': 2.0299012660980225, 'validation/accuracy': 0.4887999892234802, 'validation/loss': 2.2722764015197754, 'validation/num_examples': 50000, 'test/accuracy': 0.3781000077724457, 'test/loss': 2.9239919185638428, 'test/num_examples': 10000, 'score': 9703.953919887543, 'total_duration': 10718.971504211426, 'accumulated_submission_time': 9703.953919887543, 'accumulated_eval_time': 1013.2559487819672, 'accumulated_logging_time': 0.6527237892150879}
I0203 14:16:56.112352 139702543816448 logging_writer.py:48] [21022] accumulated_eval_time=1013.255949, accumulated_logging_time=0.652724, accumulated_submission_time=9703.953920, global_step=21022, preemption_count=0, score=9703.953920, test/accuracy=0.378100, test/loss=2.923992, test/num_examples=10000, total_duration=10718.971504, train/accuracy=0.533691, train/loss=2.029901, validation/accuracy=0.488800, validation/loss=2.272276, validation/num_examples=50000
I0203 14:17:29.092006 139702527031040 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.040604591369629, loss=3.160367727279663
I0203 14:18:15.023976 139702543816448 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.8733299970626831, loss=5.4049530029296875
I0203 14:19:01.780991 139702527031040 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.0928280353546143, loss=3.213735342025757
I0203 14:19:48.009628 139702543816448 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.2225847244262695, loss=3.198831558227539
I0203 14:20:34.493220 139702527031040 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.9723484516143799, loss=3.5542232990264893
I0203 14:21:20.580580 139702543816448 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.967984139919281, loss=3.877960443496704
I0203 14:22:06.827668 139702527031040 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7468982934951782, loss=5.560317039489746
I0203 14:22:53.070666 139702543816448 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8076804280281067, loss=5.632023334503174
I0203 14:23:39.232995 139702527031040 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.0486764907836914, loss=3.1970181465148926
I0203 14:23:56.369327 139863983413056 spec.py:321] Evaluating on the training split.
I0203 14:24:07.472779 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 14:24:36.076077 139863983413056 spec.py:349] Evaluating on the test split.
I0203 14:24:37.711934 139863983413056 submission_runner.py:408] Time since start: 11180.59s, 	Step: 21939, 	{'train/accuracy': 0.5484570264816284, 'train/loss': 1.9116195440292358, 'validation/accuracy': 0.5009399652481079, 'validation/loss': 2.1616222858428955, 'validation/num_examples': 50000, 'test/accuracy': 0.3904000222682953, 'test/loss': 2.829202175140381, 'test/num_examples': 10000, 'score': 10124.150616884232, 'total_duration': 11180.589293718338, 'accumulated_submission_time': 10124.150616884232, 'accumulated_eval_time': 1054.5985553264618, 'accumulated_logging_time': 0.6823267936706543}
I0203 14:24:37.730353 139702543816448 logging_writer.py:48] [21939] accumulated_eval_time=1054.598555, accumulated_logging_time=0.682327, accumulated_submission_time=10124.150617, global_step=21939, preemption_count=0, score=10124.150617, test/accuracy=0.390400, test/loss=2.829202, test/num_examples=10000, total_duration=11180.589294, train/accuracy=0.548457, train/loss=1.911620, validation/accuracy=0.500940, validation/loss=2.161622, validation/num_examples=50000
I0203 14:25:03.610156 139702527031040 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.00815749168396, loss=3.22495174407959
I0203 14:25:48.547793 139702543816448 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.0653982162475586, loss=3.0831663608551025
I0203 14:26:34.901158 139702527031040 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9029058814048767, loss=5.282081127166748
I0203 14:27:21.403892 139702543816448 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.155773639678955, loss=3.09163236618042
I0203 14:28:07.586455 139702527031040 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.07893705368042, loss=3.0142202377319336
I0203 14:28:53.883512 139702543816448 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7996967434883118, loss=5.18956184387207
I0203 14:29:40.852474 139702527031040 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.1232446432113647, loss=3.102104902267456
I0203 14:30:27.573576 139702543816448 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.989974319934845, loss=3.9334545135498047
I0203 14:31:14.147869 139702527031040 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.1520973443984985, loss=3.309403419494629
I0203 14:31:37.722844 139863983413056 spec.py:321] Evaluating on the training split.
I0203 14:31:48.307367 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 14:32:19.797940 139863983413056 spec.py:349] Evaluating on the test split.
I0203 14:32:21.444153 139863983413056 submission_runner.py:408] Time since start: 11644.32s, 	Step: 22852, 	{'train/accuracy': 0.5477929711341858, 'train/loss': 1.9292596578598022, 'validation/accuracy': 0.5063599944114685, 'validation/loss': 2.1407365798950195, 'validation/num_examples': 50000, 'test/accuracy': 0.3939000070095062, 'test/loss': 2.801138162612915, 'test/num_examples': 10000, 'score': 10544.085697889328, 'total_duration': 11644.321509361267, 'accumulated_submission_time': 10544.085697889328, 'accumulated_eval_time': 1098.3198697566986, 'accumulated_logging_time': 0.7099740505218506}
I0203 14:32:21.467294 139702543816448 logging_writer.py:48] [22852] accumulated_eval_time=1098.319870, accumulated_logging_time=0.709974, accumulated_submission_time=10544.085698, global_step=22852, preemption_count=0, score=10544.085698, test/accuracy=0.393900, test/loss=2.801138, test/num_examples=10000, total_duration=11644.321509, train/accuracy=0.547793, train/loss=1.929260, validation/accuracy=0.506360, validation/loss=2.140737, validation/num_examples=50000
I0203 14:32:41.928497 139702527031040 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.1022419929504395, loss=2.98946475982666
I0203 14:33:26.595829 139702543816448 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.0835641622543335, loss=3.0347468852996826
I0203 14:34:12.998339 139702527031040 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.1012147665023804, loss=3.0956156253814697
I0203 14:34:59.213604 139702543816448 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.205515742301941, loss=3.243870258331299
I0203 14:35:45.391606 139702527031040 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.0934103727340698, loss=3.1433911323547363
I0203 14:36:31.720190 139702543816448 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.2414125204086304, loss=3.098750114440918
I0203 14:37:17.916436 139702527031040 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.0348730087280273, loss=3.0956385135650635
I0203 14:38:04.346908 139702543816448 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.0448635816574097, loss=4.4348297119140625
I0203 14:38:50.605068 139702527031040 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.095882534980774, loss=3.0793750286102295
I0203 14:39:21.758370 139863983413056 spec.py:321] Evaluating on the training split.
I0203 14:39:32.022735 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 14:40:00.780330 139863983413056 spec.py:349] Evaluating on the test split.
I0203 14:40:02.433238 139863983413056 submission_runner.py:408] Time since start: 12105.31s, 	Step: 23768, 	{'train/accuracy': 0.5549609065055847, 'train/loss': 1.9112342596054077, 'validation/accuracy': 0.5130199790000916, 'validation/loss': 2.142449378967285, 'validation/num_examples': 50000, 'test/accuracy': 0.3993000090122223, 'test/loss': 2.8055648803710938, 'test/num_examples': 10000, 'score': 10964.318322658539, 'total_duration': 12105.310588121414, 'accumulated_submission_time': 10964.318322658539, 'accumulated_eval_time': 1138.9947321414948, 'accumulated_logging_time': 0.7432305812835693}
I0203 14:40:02.453093 139702543816448 logging_writer.py:48] [23768] accumulated_eval_time=1138.994732, accumulated_logging_time=0.743231, accumulated_submission_time=10964.318323, global_step=23768, preemption_count=0, score=10964.318323, test/accuracy=0.399300, test/loss=2.805565, test/num_examples=10000, total_duration=12105.310588, train/accuracy=0.554961, train/loss=1.911234, validation/accuracy=0.513020, validation/loss=2.142449, validation/num_examples=50000
I0203 14:40:16.220577 139702527031040 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.0011727809906006, loss=5.430969715118408
I0203 14:40:59.571169 139702543816448 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.0375304222106934, loss=3.3264660835266113
I0203 14:41:45.719441 139702527031040 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.0636017322540283, loss=3.653090715408325
I0203 14:42:31.971978 139702543816448 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0890653133392334, loss=3.099571466445923
I0203 14:43:17.986571 139702527031040 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.9954302906990051, loss=4.6989850997924805
I0203 14:44:04.222410 139702543816448 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.0577301979064941, loss=3.340575695037842
I0203 14:44:50.355631 139702527031040 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8115221858024597, loss=5.151986598968506
I0203 14:45:36.516175 139702543816448 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0951814651489258, loss=3.0603091716766357
I0203 14:46:23.127631 139702527031040 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.9794190526008606, loss=3.716446876525879
I0203 14:47:02.513516 139863983413056 spec.py:321] Evaluating on the training split.
I0203 14:47:12.941547 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 14:47:43.670969 139863983413056 spec.py:349] Evaluating on the test split.
I0203 14:47:45.314331 139863983413056 submission_runner.py:408] Time since start: 12568.19s, 	Step: 24687, 	{'train/accuracy': 0.5798242092132568, 'train/loss': 1.8216063976287842, 'validation/accuracy': 0.5144199728965759, 'validation/loss': 2.1380128860473633, 'validation/num_examples': 50000, 'test/accuracy': 0.39900001883506775, 'test/loss': 2.804219961166382, 'test/num_examples': 10000, 'score': 11384.320001840591, 'total_duration': 12568.191683530807, 'accumulated_submission_time': 11384.320001840591, 'accumulated_eval_time': 1181.7955298423767, 'accumulated_logging_time': 0.773090124130249}
I0203 14:47:45.336894 139702543816448 logging_writer.py:48] [24687] accumulated_eval_time=1181.795530, accumulated_logging_time=0.773090, accumulated_submission_time=11384.320002, global_step=24687, preemption_count=0, score=11384.320002, test/accuracy=0.399000, test/loss=2.804220, test/num_examples=10000, total_duration=12568.191684, train/accuracy=0.579824, train/loss=1.821606, validation/accuracy=0.514420, validation/loss=2.138013, validation/num_examples=50000
I0203 14:47:51.174347 139702527031040 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.887526273727417, loss=4.576954364776611
I0203 14:48:34.158863 139702543816448 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.0736782550811768, loss=2.987112045288086
I0203 14:49:20.211241 139702527031040 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.7499258518218994, loss=5.47455358505249
I0203 14:50:06.803454 139702543816448 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.2410457134246826, loss=3.0787272453308105
I0203 14:50:53.068192 139702527031040 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.9628372192382812, loss=3.5189943313598633
I0203 14:51:39.131282 139702543816448 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.0905647277832031, loss=3.0008068084716797
I0203 14:52:25.298407 139702527031040 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.1135185956954956, loss=2.9381937980651855
I0203 14:53:11.445333 139702543816448 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.8238950371742249, loss=5.0566277503967285
I0203 14:53:57.504104 139702527031040 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8732973337173462, loss=4.513093948364258
I0203 14:54:43.688529 139702543816448 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.9000216722488403, loss=4.23181676864624
I0203 14:54:45.669408 139863983413056 spec.py:321] Evaluating on the training split.
I0203 14:54:56.032345 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 14:55:27.034662 139863983413056 spec.py:349] Evaluating on the test split.
I0203 14:55:28.677403 139863983413056 submission_runner.py:408] Time since start: 13031.55s, 	Step: 25606, 	{'train/accuracy': 0.5553905963897705, 'train/loss': 1.961132287979126, 'validation/accuracy': 0.5149999856948853, 'validation/loss': 2.1607208251953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4036000072956085, 'test/loss': 2.8055355548858643, 'test/num_examples': 10000, 'score': 11804.5945789814, 'total_duration': 13031.554756641388, 'accumulated_submission_time': 11804.5945789814, 'accumulated_eval_time': 1224.8035056591034, 'accumulated_logging_time': 0.8047606945037842}
I0203 14:55:28.696459 139702527031040 logging_writer.py:48] [25606] accumulated_eval_time=1224.803506, accumulated_logging_time=0.804761, accumulated_submission_time=11804.594579, global_step=25606, preemption_count=0, score=11804.594579, test/accuracy=0.403600, test/loss=2.805536, test/num_examples=10000, total_duration=13031.554757, train/accuracy=0.555391, train/loss=1.961132, validation/accuracy=0.515000, validation/loss=2.160721, validation/num_examples=50000
I0203 14:56:08.560656 139702543816448 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8652629256248474, loss=4.044030666351318
I0203 14:56:54.390288 139702527031040 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.1687660217285156, loss=3.120882034301758
I0203 14:57:41.031863 139702543816448 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.0071371793746948, loss=4.542144298553467
I0203 14:58:27.191752 139702527031040 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.0186713933944702, loss=3.873939275741577
I0203 14:59:13.481349 139702543816448 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.1485272645950317, loss=3.0721728801727295
I0203 14:59:59.696355 139702527031040 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.122072696685791, loss=3.031761884689331
I0203 15:00:46.287361 139702543816448 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.1320058107376099, loss=2.867765426635742
I0203 15:01:32.749842 139702527031040 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.0309170484542847, loss=3.151217460632324
I0203 15:02:18.765914 139702543816448 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.8700100779533386, loss=5.535142421722412
I0203 15:02:29.044406 139863983413056 spec.py:321] Evaluating on the training split.
I0203 15:02:39.510117 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 15:03:11.129589 139863983413056 spec.py:349] Evaluating on the test split.
I0203 15:03:12.766070 139863983413056 submission_runner.py:408] Time since start: 13495.64s, 	Step: 26524, 	{'train/accuracy': 0.5643359422683716, 'train/loss': 1.8898110389709473, 'validation/accuracy': 0.5171599984169006, 'validation/loss': 2.1188228130340576, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.7826554775238037, 'test/num_examples': 10000, 'score': 12224.88507938385, 'total_duration': 13495.64342713356, 'accumulated_submission_time': 12224.88507938385, 'accumulated_eval_time': 1268.5251622200012, 'accumulated_logging_time': 0.8329160213470459}
I0203 15:03:12.785238 139702527031040 logging_writer.py:48] [26524] accumulated_eval_time=1268.525162, accumulated_logging_time=0.832916, accumulated_submission_time=12224.885079, global_step=26524, preemption_count=0, score=12224.885079, test/accuracy=0.407500, test/loss=2.782655, test/num_examples=10000, total_duration=13495.643427, train/accuracy=0.564336, train/loss=1.889811, validation/accuracy=0.517160, validation/loss=2.118823, validation/num_examples=50000
I0203 15:03:44.912002 139702543816448 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.0801420211791992, loss=3.005100727081299
I0203 15:04:30.365873 139702527031040 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.0015720129013062, loss=3.3567423820495605
I0203 15:05:16.745606 139702543816448 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.1991733312606812, loss=2.9624054431915283
I0203 15:06:02.779871 139702527031040 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.0650506019592285, loss=3.444119453430176
I0203 15:06:48.911459 139702543816448 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.1202304363250732, loss=2.8429789543151855
I0203 15:07:34.853025 139702527031040 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.0175279378890991, loss=2.926558017730713
I0203 15:08:20.938273 139702543816448 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.086006999015808, loss=3.081233501434326
I0203 15:09:07.081578 139702527031040 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.1430466175079346, loss=2.9527556896209717
I0203 15:09:53.202784 139702543816448 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.9042653441429138, loss=5.233560562133789
I0203 15:10:12.957357 139863983413056 spec.py:321] Evaluating on the training split.
I0203 15:10:23.432696 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 15:10:54.636890 139863983413056 spec.py:349] Evaluating on the test split.
I0203 15:10:56.273820 139863983413056 submission_runner.py:408] Time since start: 13959.15s, 	Step: 27444, 	{'train/accuracy': 0.5781835913658142, 'train/loss': 1.8442374467849731, 'validation/accuracy': 0.5189599990844727, 'validation/loss': 2.119422197341919, 'validation/num_examples': 50000, 'test/accuracy': 0.4106000065803528, 'test/loss': 2.772930145263672, 'test/num_examples': 10000, 'score': 12644.999958276749, 'total_duration': 13959.151177167892, 'accumulated_submission_time': 12644.999958276749, 'accumulated_eval_time': 1311.8416454792023, 'accumulated_logging_time': 0.8607838153839111}
I0203 15:10:56.297733 139702527031040 logging_writer.py:48] [27444] accumulated_eval_time=1311.841645, accumulated_logging_time=0.860784, accumulated_submission_time=12644.999958, global_step=27444, preemption_count=0, score=12644.999958, test/accuracy=0.410600, test/loss=2.772930, test/num_examples=10000, total_duration=13959.151177, train/accuracy=0.578184, train/loss=1.844237, validation/accuracy=0.518960, validation/loss=2.119422, validation/num_examples=50000
I0203 15:11:20.068882 139702543816448 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.849703848361969, loss=3.8741350173950195
I0203 15:12:05.262284 139702527031040 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.068204641342163, loss=2.921393871307373
I0203 15:12:51.394750 139702543816448 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.9144884347915649, loss=3.7954211235046387
I0203 15:13:37.751824 139702527031040 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.8692968487739563, loss=4.140300750732422
I0203 15:14:24.028352 139702543816448 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.1066100597381592, loss=2.985616683959961
I0203 15:15:10.497202 139702527031040 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.1303890943527222, loss=3.3082046508789062
I0203 15:15:56.628431 139702543816448 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.0700067281723022, loss=3.1380319595336914
I0203 15:16:42.871726 139702527031040 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.0820552110671997, loss=2.9567766189575195
I0203 15:17:29.148647 139702543816448 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.2362470626831055, loss=3.0205023288726807
I0203 15:17:56.619396 139863983413056 spec.py:321] Evaluating on the training split.
I0203 15:18:07.110404 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 15:18:40.175887 139863983413056 spec.py:349] Evaluating on the test split.
I0203 15:18:41.820941 139863983413056 submission_runner.py:408] Time since start: 14424.70s, 	Step: 28361, 	{'train/accuracy': 0.5642968416213989, 'train/loss': 1.883542776107788, 'validation/accuracy': 0.5241999626159668, 'validation/loss': 2.089167356491089, 'validation/num_examples': 50000, 'test/accuracy': 0.41040003299713135, 'test/loss': 2.7521159648895264, 'test/num_examples': 10000, 'score': 13065.262185811996, 'total_duration': 14424.698271989822, 'accumulated_submission_time': 13065.262185811996, 'accumulated_eval_time': 1357.0431625843048, 'accumulated_logging_time': 0.895582914352417}
I0203 15:18:41.844841 139702527031040 logging_writer.py:48] [28361] accumulated_eval_time=1357.043163, accumulated_logging_time=0.895583, accumulated_submission_time=13065.262186, global_step=28361, preemption_count=0, score=13065.262186, test/accuracy=0.410400, test/loss=2.752116, test/num_examples=10000, total_duration=14424.698272, train/accuracy=0.564297, train/loss=1.883543, validation/accuracy=0.524200, validation/loss=2.089167, validation/num_examples=50000
I0203 15:18:58.523277 139702543816448 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.0245296955108643, loss=3.544590950012207
I0203 15:19:42.464824 139702527031040 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7656909823417664, loss=5.409623622894287
I0203 15:20:29.176500 139702543816448 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.7889432311058044, loss=5.452531337738037
I0203 15:21:15.480483 139702527031040 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.1378531455993652, loss=3.2752251625061035
I0203 15:22:01.780032 139702543816448 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.2165359258651733, loss=2.9229841232299805
I0203 15:22:47.892196 139702527031040 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.1274504661560059, loss=2.860536575317383
I0203 15:23:34.360640 139702543816448 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.9792618751525879, loss=3.3856985569000244
I0203 15:24:20.477848 139702527031040 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9769533276557922, loss=3.7307252883911133
I0203 15:25:06.972519 139702543816448 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.9912671446800232, loss=4.1327643394470215
I0203 15:25:41.944100 139863983413056 spec.py:321] Evaluating on the training split.
I0203 15:25:52.410661 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 15:26:23.713478 139863983413056 spec.py:349] Evaluating on the test split.
I0203 15:26:25.354108 139863983413056 submission_runner.py:408] Time since start: 14888.23s, 	Step: 29278, 	{'train/accuracy': 0.5771288871765137, 'train/loss': 1.784113883972168, 'validation/accuracy': 0.5302799940109253, 'validation/loss': 2.028592348098755, 'validation/num_examples': 50000, 'test/accuracy': 0.42420002818107605, 'test/loss': 2.681459426879883, 'test/num_examples': 10000, 'score': 13485.303869009018, 'total_duration': 14888.231466531754, 'accumulated_submission_time': 13485.303869009018, 'accumulated_eval_time': 1400.4531652927399, 'accumulated_logging_time': 0.9288933277130127}
I0203 15:26:25.374311 139702527031040 logging_writer.py:48] [29278] accumulated_eval_time=1400.453165, accumulated_logging_time=0.928893, accumulated_submission_time=13485.303869, global_step=29278, preemption_count=0, score=13485.303869, test/accuracy=0.424200, test/loss=2.681459, test/num_examples=10000, total_duration=14888.231467, train/accuracy=0.577129, train/loss=1.784114, validation/accuracy=0.530280, validation/loss=2.028592, validation/num_examples=50000
I0203 15:26:34.952716 139702543816448 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.126323938369751, loss=3.1497437953948975
I0203 15:27:18.385438 139702527031040 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.3901258707046509, loss=2.942300796508789
I0203 15:28:04.883470 139702543816448 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.0788216590881348, loss=3.0044448375701904
I0203 15:28:51.276695 139702527031040 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.1711922883987427, loss=2.8567209243774414
I0203 15:29:37.520278 139702543816448 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.0048532485961914, loss=3.822549343109131
I0203 15:30:23.804492 139702527031040 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.9621857404708862, loss=4.36424446105957
I0203 15:31:10.110390 139702543816448 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.0478752851486206, loss=2.889150619506836
I0203 15:31:56.486113 139702527031040 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.1324292421340942, loss=2.898672580718994
I0203 15:32:43.042333 139702543816448 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.0986984968185425, loss=2.8250207901000977
I0203 15:33:25.465836 139863983413056 spec.py:321] Evaluating on the training split.
I0203 15:33:36.194889 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 15:34:08.934785 139863983413056 spec.py:349] Evaluating on the test split.
I0203 15:34:10.587855 139863983413056 submission_runner.py:408] Time since start: 15353.47s, 	Step: 30193, 	{'train/accuracy': 0.5874413847923279, 'train/loss': 1.7344344854354858, 'validation/accuracy': 0.5366799831390381, 'validation/loss': 1.992333173751831, 'validation/num_examples': 50000, 'test/accuracy': 0.4198000133037567, 'test/loss': 2.670804500579834, 'test/num_examples': 10000, 'score': 13905.338715076447, 'total_duration': 15353.465184688568, 'accumulated_submission_time': 13905.338715076447, 'accumulated_eval_time': 1445.575161933899, 'accumulated_logging_time': 0.9585323333740234}
I0203 15:34:10.610988 139702527031040 logging_writer.py:48] [30193] accumulated_eval_time=1445.575162, accumulated_logging_time=0.958532, accumulated_submission_time=13905.338715, global_step=30193, preemption_count=0, score=13905.338715, test/accuracy=0.419800, test/loss=2.670805, test/num_examples=10000, total_duration=15353.465185, train/accuracy=0.587441, train/loss=1.734434, validation/accuracy=0.536680, validation/loss=1.992333, validation/num_examples=50000
I0203 15:34:13.943016 139702543816448 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.894932210445404, loss=5.488049030303955
I0203 15:34:56.651895 139702527031040 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.1654447317123413, loss=2.9609315395355225
I0203 15:35:42.983534 139702543816448 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.8940252065658569, loss=4.3539042472839355
I0203 15:36:29.497573 139702527031040 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0226799249649048, loss=3.2081713676452637
I0203 15:37:15.563127 139702543816448 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.0543293952941895, loss=5.374751091003418
I0203 15:38:01.835673 139702527031040 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.0887829065322876, loss=2.868462085723877
I0203 15:38:48.042772 139702543816448 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.150186538696289, loss=3.15891170501709
I0203 15:39:34.341862 139702527031040 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0509321689605713, loss=4.399901390075684
I0203 15:40:20.820267 139702543816448 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8709226846694946, loss=4.175513744354248
I0203 15:41:07.004401 139702527031040 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.1592869758605957, loss=3.329800605773926
I0203 15:41:10.873827 139863983413056 spec.py:321] Evaluating on the training split.
I0203 15:41:21.382510 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 15:41:53.670003 139863983413056 spec.py:349] Evaluating on the test split.
I0203 15:41:55.310823 139863983413056 submission_runner.py:408] Time since start: 15818.19s, 	Step: 31110, 	{'train/accuracy': 0.5776171684265137, 'train/loss': 1.8119839429855347, 'validation/accuracy': 0.5379199981689453, 'validation/loss': 2.0045900344848633, 'validation/num_examples': 50000, 'test/accuracy': 0.4220000207424164, 'test/loss': 2.683004379272461, 'test/num_examples': 10000, 'score': 14325.543489933014, 'total_duration': 15818.188180923462, 'accumulated_submission_time': 14325.543489933014, 'accumulated_eval_time': 1490.0121433734894, 'accumulated_logging_time': 0.9918410778045654}
I0203 15:41:55.331515 139702543816448 logging_writer.py:48] [31110] accumulated_eval_time=1490.012143, accumulated_logging_time=0.991841, accumulated_submission_time=14325.543490, global_step=31110, preemption_count=0, score=14325.543490, test/accuracy=0.422000, test/loss=2.683004, test/num_examples=10000, total_duration=15818.188181, train/accuracy=0.577617, train/loss=1.811984, validation/accuracy=0.537920, validation/loss=2.004590, validation/num_examples=50000
I0203 15:42:33.719788 139702527031040 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.1481229066848755, loss=2.9869678020477295
I0203 15:43:20.168936 139702543816448 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.0130218267440796, loss=2.9712677001953125
I0203 15:44:06.260305 139702527031040 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.1120598316192627, loss=2.768234968185425
I0203 15:44:52.407285 139702543816448 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.0728589296340942, loss=2.7522988319396973
I0203 15:45:38.734583 139702527031040 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.2549400329589844, loss=2.9049391746520996
I0203 15:46:25.003283 139702543816448 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.1318986415863037, loss=3.1173501014709473
I0203 15:47:11.193969 139702527031040 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.3311676979064941, loss=3.7640438079833984
I0203 15:47:57.432781 139702543816448 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.9520852565765381, loss=5.356083869934082
I0203 15:48:43.653688 139702527031040 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.0607696771621704, loss=2.9310903549194336
I0203 15:48:55.391344 139863983413056 spec.py:321] Evaluating on the training split.
I0203 15:49:05.977782 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 15:49:37.563430 139863983413056 spec.py:349] Evaluating on the test split.
I0203 15:49:39.206683 139863983413056 submission_runner.py:408] Time since start: 16282.08s, 	Step: 32027, 	{'train/accuracy': 0.5791406035423279, 'train/loss': 1.7816259860992432, 'validation/accuracy': 0.5400199890136719, 'validation/loss': 1.9844132661819458, 'validation/num_examples': 50000, 'test/accuracy': 0.4261000156402588, 'test/loss': 2.6556942462921143, 'test/num_examples': 10000, 'score': 14745.545434951782, 'total_duration': 16282.084006547928, 'accumulated_submission_time': 14745.545434951782, 'accumulated_eval_time': 1533.8274431228638, 'accumulated_logging_time': 1.0221738815307617}
I0203 15:49:39.237976 139702543816448 logging_writer.py:48] [32027] accumulated_eval_time=1533.827443, accumulated_logging_time=1.022174, accumulated_submission_time=14745.545435, global_step=32027, preemption_count=0, score=14745.545435, test/accuracy=0.426100, test/loss=2.655694, test/num_examples=10000, total_duration=16282.084007, train/accuracy=0.579141, train/loss=1.781626, validation/accuracy=0.540020, validation/loss=1.984413, validation/num_examples=50000
I0203 15:50:10.128929 139702527031040 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.126794457435608, loss=2.754011869430542
I0203 15:50:55.865664 139702543816448 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.2715668678283691, loss=2.849252223968506
I0203 15:51:42.224762 139702527031040 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.881567656993866, loss=4.085593223571777
I0203 15:52:28.459854 139702543816448 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.1482954025268555, loss=2.832167625427246
I0203 15:53:14.852103 139702527031040 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.950322151184082, loss=4.776339530944824
I0203 15:54:01.433496 139702543816448 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.9153781533241272, loss=5.386784553527832
I0203 15:54:47.753074 139702527031040 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.2118088006973267, loss=3.1490068435668945
I0203 15:55:34.215066 139702543816448 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.1783959865570068, loss=2.86472225189209
I0203 15:56:20.553278 139702527031040 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.9779636859893799, loss=2.752336025238037
I0203 15:56:39.260864 139863983413056 spec.py:321] Evaluating on the training split.
I0203 15:56:49.649441 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 15:57:22.487074 139863983413056 spec.py:349] Evaluating on the test split.
I0203 15:57:24.120482 139863983413056 submission_runner.py:408] Time since start: 16747.00s, 	Step: 32942, 	{'train/accuracy': 0.5919530987739563, 'train/loss': 1.6846200227737427, 'validation/accuracy': 0.5460000038146973, 'validation/loss': 1.936972737312317, 'validation/num_examples': 50000, 'test/accuracy': 0.428600013256073, 'test/loss': 2.6139447689056396, 'test/num_examples': 10000, 'score': 15165.510427713394, 'total_duration': 16746.997824668884, 'accumulated_submission_time': 15165.510427713394, 'accumulated_eval_time': 1578.6870546340942, 'accumulated_logging_time': 1.0637776851654053}
I0203 15:57:24.147817 139702543816448 logging_writer.py:48] [32942] accumulated_eval_time=1578.687055, accumulated_logging_time=1.063778, accumulated_submission_time=15165.510428, global_step=32942, preemption_count=0, score=15165.510428, test/accuracy=0.428600, test/loss=2.613945, test/num_examples=10000, total_duration=16746.997825, train/accuracy=0.591953, train/loss=1.684620, validation/accuracy=0.546000, validation/loss=1.936973, validation/num_examples=50000
I0203 15:57:48.750612 139702527031040 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7972854971885681, loss=5.4751434326171875
I0203 15:58:33.606374 139702543816448 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.153709888458252, loss=2.687964916229248
I0203 15:59:20.123251 139702527031040 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.1499361991882324, loss=2.862276554107666
I0203 16:00:06.557997 139702543816448 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.1056796312332153, loss=2.7306509017944336
I0203 16:00:52.952995 139702527031040 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.0424928665161133, loss=5.038327693939209
I0203 16:01:39.171175 139702543816448 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.0875576734542847, loss=3.022165298461914
I0203 16:02:25.753200 139702527031040 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.1295160055160522, loss=2.9303526878356934
I0203 16:03:12.271510 139702543816448 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.8941906690597534, loss=4.279422760009766
I0203 16:03:58.829608 139702527031040 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1443006992340088, loss=2.7252609729766846
I0203 16:04:24.448889 139863983413056 spec.py:321] Evaluating on the training split.
I0203 16:04:35.092391 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 16:05:06.477179 139863983413056 spec.py:349] Evaluating on the test split.
I0203 16:05:08.122926 139863983413056 submission_runner.py:408] Time since start: 17211.00s, 	Step: 33857, 	{'train/accuracy': 0.5914843678474426, 'train/loss': 1.794201374053955, 'validation/accuracy': 0.5385199785232544, 'validation/loss': 2.034351110458374, 'validation/num_examples': 50000, 'test/accuracy': 0.42730000615119934, 'test/loss': 2.6944146156311035, 'test/num_examples': 10000, 'score': 15585.75400686264, 'total_duration': 17211.00025987625, 'accumulated_submission_time': 15585.75400686264, 'accumulated_eval_time': 1622.3610565662384, 'accumulated_logging_time': 1.1009063720703125}
I0203 16:05:08.150813 139702543816448 logging_writer.py:48] [33857] accumulated_eval_time=1622.361057, accumulated_logging_time=1.100906, accumulated_submission_time=15585.754007, global_step=33857, preemption_count=0, score=15585.754007, test/accuracy=0.427300, test/loss=2.694415, test/num_examples=10000, total_duration=17211.000260, train/accuracy=0.591484, train/loss=1.794201, validation/accuracy=0.538520, validation/loss=2.034351, validation/num_examples=50000
I0203 16:05:26.510234 139702527031040 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.8675616979598999, loss=5.4896039962768555
I0203 16:06:10.761432 139702543816448 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.9347475171089172, loss=3.787705659866333
I0203 16:06:57.174940 139702527031040 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.2281442880630493, loss=2.829148530960083
I0203 16:07:43.435726 139702543816448 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.1128824949264526, loss=2.9069879055023193
I0203 16:08:29.559624 139702527031040 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.065237283706665, loss=3.141312599182129
I0203 16:09:15.788433 139702543816448 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.7382997274398804, loss=2.794071674346924
I0203 16:10:02.156115 139702527031040 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.0944768190383911, loss=2.8627500534057617
I0203 16:10:48.443349 139702543816448 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.1030654907226562, loss=2.742217779159546
I0203 16:11:34.768458 139702527031040 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.2660908699035645, loss=2.7554519176483154
I0203 16:12:08.322201 139863983413056 spec.py:321] Evaluating on the training split.
I0203 16:12:18.844608 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 16:12:50.831499 139863983413056 spec.py:349] Evaluating on the test split.
I0203 16:12:52.474629 139863983413056 submission_runner.py:408] Time since start: 17675.35s, 	Step: 34774, 	{'train/accuracy': 0.5895116925239563, 'train/loss': 1.7374787330627441, 'validation/accuracy': 0.5465199947357178, 'validation/loss': 1.960172414779663, 'validation/num_examples': 50000, 'test/accuracy': 0.43390002846717834, 'test/loss': 2.622077703475952, 'test/num_examples': 10000, 'score': 16005.865160703659, 'total_duration': 17675.351983308792, 'accumulated_submission_time': 16005.865160703659, 'accumulated_eval_time': 1666.5135188102722, 'accumulated_logging_time': 1.1385252475738525}
I0203 16:12:52.498725 139702543816448 logging_writer.py:48] [34774] accumulated_eval_time=1666.513519, accumulated_logging_time=1.138525, accumulated_submission_time=16005.865161, global_step=34774, preemption_count=0, score=16005.865161, test/accuracy=0.433900, test/loss=2.622078, test/num_examples=10000, total_duration=17675.351983, train/accuracy=0.589512, train/loss=1.737479, validation/accuracy=0.546520, validation/loss=1.960172, validation/num_examples=50000
I0203 16:13:03.759514 139702527031040 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.8906562328338623, loss=4.118481636047363
I0203 16:13:47.432543 139702543816448 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.149478793144226, loss=2.9622368812561035
I0203 16:14:33.799247 139702527031040 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.198700189590454, loss=2.8899197578430176
I0203 16:15:20.450963 139702543816448 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.2493629455566406, loss=2.9304356575012207
I0203 16:16:06.640230 139702527031040 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.1640472412109375, loss=2.864321231842041
I0203 16:16:52.877327 139702543816448 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.1169943809509277, loss=2.64902925491333
I0203 16:17:39.077418 139702527031040 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.0283294916152954, loss=3.961094856262207
I0203 16:18:25.485265 139702543816448 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7881335616111755, loss=4.933935165405273
I0203 16:19:11.665352 139702527031040 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.0472396612167358, loss=3.4529426097869873
I0203 16:19:52.589034 139863983413056 spec.py:321] Evaluating on the training split.
I0203 16:20:03.232047 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 16:20:36.696252 139863983413056 spec.py:349] Evaluating on the test split.
I0203 16:20:38.327500 139863983413056 submission_runner.py:408] Time since start: 18141.20s, 	Step: 35690, 	{'train/accuracy': 0.5945702791213989, 'train/loss': 1.7534534931182861, 'validation/accuracy': 0.5510799884796143, 'validation/loss': 1.9650921821594238, 'validation/num_examples': 50000, 'test/accuracy': 0.43400001525878906, 'test/loss': 2.628279209136963, 'test/num_examples': 10000, 'score': 16425.89613389969, 'total_duration': 18141.204838514328, 'accumulated_submission_time': 16425.89613389969, 'accumulated_eval_time': 1712.2519705295563, 'accumulated_logging_time': 1.1743512153625488}
I0203 16:20:38.353393 139702543816448 logging_writer.py:48] [35690] accumulated_eval_time=1712.251971, accumulated_logging_time=1.174351, accumulated_submission_time=16425.896134, global_step=35690, preemption_count=0, score=16425.896134, test/accuracy=0.434000, test/loss=2.628279, test/num_examples=10000, total_duration=18141.204839, train/accuracy=0.594570, train/loss=1.753453, validation/accuracy=0.551080, validation/loss=1.965092, validation/num_examples=50000
I0203 16:20:42.934792 139702527031040 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.899370014667511, loss=4.22405481338501
I0203 16:21:26.059193 139702543816448 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.4741876125335693, loss=2.843568801879883
I0203 16:22:11.991581 139702527031040 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.1925909519195557, loss=2.9805970191955566
I0203 16:22:58.414195 139702543816448 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.0723870992660522, loss=3.036918878555298
I0203 16:23:44.981906 139702527031040 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.1940096616744995, loss=2.8424410820007324
I0203 16:24:31.051858 139702543816448 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.0688637495040894, loss=4.179513454437256
I0203 16:25:17.802773 139702527031040 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.258679986000061, loss=2.6775500774383545
I0203 16:26:03.968973 139702543816448 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.250147819519043, loss=2.8237905502319336
I0203 16:26:50.160069 139702527031040 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.0204790830612183, loss=3.62252140045166
I0203 16:27:36.474855 139702543816448 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.9075230956077576, loss=4.33070707321167
I0203 16:27:38.635870 139863983413056 spec.py:321] Evaluating on the training split.
I0203 16:27:49.001086 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 16:28:23.167489 139863983413056 spec.py:349] Evaluating on the test split.
I0203 16:28:24.809861 139863983413056 submission_runner.py:408] Time since start: 18607.69s, 	Step: 36606, 	{'train/accuracy': 0.6235546469688416, 'train/loss': 1.5661062002182007, 'validation/accuracy': 0.553059995174408, 'validation/loss': 1.8976249694824219, 'validation/num_examples': 50000, 'test/accuracy': 0.4409000277519226, 'test/loss': 2.563768148422241, 'test/num_examples': 10000, 'score': 16846.11967420578, 'total_duration': 18607.68721485138, 'accumulated_submission_time': 16846.11967420578, 'accumulated_eval_time': 1758.4259514808655, 'accumulated_logging_time': 1.2110042572021484}
I0203 16:28:24.834585 139702527031040 logging_writer.py:48] [36606] accumulated_eval_time=1758.425951, accumulated_logging_time=1.211004, accumulated_submission_time=16846.119674, global_step=36606, preemption_count=0, score=16846.119674, test/accuracy=0.440900, test/loss=2.563768, test/num_examples=10000, total_duration=18607.687215, train/accuracy=0.623555, train/loss=1.566106, validation/accuracy=0.553060, validation/loss=1.897625, validation/num_examples=50000
I0203 16:29:05.040851 139702543816448 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.044499397277832, loss=3.164043664932251
I0203 16:29:51.090931 139702527031040 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.045157790184021, loss=2.9446258544921875
I0203 16:30:37.462114 139702543816448 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.9931995272636414, loss=5.185983657836914
I0203 16:31:23.724489 139702527031040 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0793421268463135, loss=2.9625144004821777
I0203 16:32:09.985778 139702543816448 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.1457489728927612, loss=2.739088773727417
I0203 16:32:56.359905 139702527031040 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.0284169912338257, loss=4.659341812133789
I0203 16:33:42.789240 139702543816448 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.1482980251312256, loss=3.7105655670166016
I0203 16:34:29.327371 139702527031040 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.0265647172927856, loss=3.1813764572143555
I0203 16:35:15.880172 139702543816448 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.2217415571212769, loss=2.8559741973876953
I0203 16:35:24.940232 139863983413056 spec.py:321] Evaluating on the training split.
I0203 16:35:35.323158 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 16:36:08.328406 139863983413056 spec.py:349] Evaluating on the test split.
I0203 16:36:09.960264 139863983413056 submission_runner.py:408] Time since start: 19072.84s, 	Step: 37521, 	{'train/accuracy': 0.5977148413658142, 'train/loss': 1.7096517086029053, 'validation/accuracy': 0.5522400140762329, 'validation/loss': 1.9204543828964233, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.5619406700134277, 'test/num_examples': 10000, 'score': 17266.167387723923, 'total_duration': 19072.837619304657, 'accumulated_submission_time': 17266.167387723923, 'accumulated_eval_time': 1803.4459762573242, 'accumulated_logging_time': 1.2457661628723145}
I0203 16:36:09.982471 139702527031040 logging_writer.py:48] [37521] accumulated_eval_time=1803.445976, accumulated_logging_time=1.245766, accumulated_submission_time=17266.167388, global_step=37521, preemption_count=0, score=17266.167388, test/accuracy=0.442200, test/loss=2.561941, test/num_examples=10000, total_duration=19072.837619, train/accuracy=0.597715, train/loss=1.709652, validation/accuracy=0.552240, validation/loss=1.920454, validation/num_examples=50000
I0203 16:36:43.385904 139702543816448 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.1271723508834839, loss=5.331691265106201
I0203 16:37:29.415444 139702527031040 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.0153769254684448, loss=3.231898784637451
I0203 16:38:15.903808 139702543816448 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.1531717777252197, loss=3.07878041267395
I0203 16:39:02.085209 139702527031040 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.8527017831802368, loss=5.284350872039795
I0203 16:39:48.544036 139702543816448 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.1108754873275757, loss=2.816824197769165
I0203 16:40:34.748056 139702527031040 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.2738122940063477, loss=2.9039876461029053
I0203 16:41:20.947458 139702543816448 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.1435667276382446, loss=2.8062336444854736
I0203 16:42:07.312617 139702527031040 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.9092074036598206, loss=3.806875228881836
I0203 16:42:53.488353 139702543816448 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.177624225616455, loss=2.835163116455078
I0203 16:43:10.403805 139863983413056 spec.py:321] Evaluating on the training split.
I0203 16:43:21.014966 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 16:43:54.550976 139863983413056 spec.py:349] Evaluating on the test split.
I0203 16:43:56.199733 139863983413056 submission_runner.py:408] Time since start: 19539.08s, 	Step: 38438, 	{'train/accuracy': 0.602343738079071, 'train/loss': 1.6719639301300049, 'validation/accuracy': 0.5600799918174744, 'validation/loss': 1.897687554359436, 'validation/num_examples': 50000, 'test/accuracy': 0.44540002942085266, 'test/loss': 2.5424721240997314, 'test/num_examples': 10000, 'score': 17686.52992963791, 'total_duration': 19539.077089309692, 'accumulated_submission_time': 17686.52992963791, 'accumulated_eval_time': 1849.2419004440308, 'accumulated_logging_time': 1.2778377532958984}
I0203 16:43:56.222971 139702527031040 logging_writer.py:48] [38438] accumulated_eval_time=1849.241900, accumulated_logging_time=1.277838, accumulated_submission_time=17686.529930, global_step=38438, preemption_count=0, score=17686.529930, test/accuracy=0.445400, test/loss=2.542472, test/num_examples=10000, total_duration=19539.077089, train/accuracy=0.602344, train/loss=1.671964, validation/accuracy=0.560080, validation/loss=1.897688, validation/num_examples=50000
I0203 16:44:22.526164 139702543816448 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.108708143234253, loss=2.918638229370117
I0203 16:45:07.978695 139702527031040 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.9941570162773132, loss=3.9763734340667725
I0203 16:45:54.468026 139702543816448 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.1197757720947266, loss=2.847795009613037
I0203 16:46:40.917116 139702527031040 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.2520217895507812, loss=2.7847251892089844
I0203 16:47:27.045449 139702543816448 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.1355171203613281, loss=2.8106582164764404
I0203 16:48:13.259471 139702527031040 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.2194584608078003, loss=2.773672342300415
I0203 16:48:59.583276 139702543816448 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.229819416999817, loss=2.790537118911743
I0203 16:49:46.031485 139702527031040 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.2002230882644653, loss=2.7422783374786377
I0203 16:50:32.286654 139702543816448 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.13254714012146, loss=3.2518765926361084
I0203 16:50:56.238996 139863983413056 spec.py:321] Evaluating on the training split.
I0203 16:51:06.787757 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 16:51:39.962618 139863983413056 spec.py:349] Evaluating on the test split.
I0203 16:51:41.605250 139863983413056 submission_runner.py:408] Time since start: 20004.48s, 	Step: 39354, 	{'train/accuracy': 0.6241015195846558, 'train/loss': 1.555986762046814, 'validation/accuracy': 0.5655399560928345, 'validation/loss': 1.844379186630249, 'validation/num_examples': 50000, 'test/accuracy': 0.4482000172138214, 'test/loss': 2.5140278339385986, 'test/num_examples': 10000, 'score': 18106.487027406693, 'total_duration': 20004.48260617256, 'accumulated_submission_time': 18106.487027406693, 'accumulated_eval_time': 1894.6081516742706, 'accumulated_logging_time': 1.3122048377990723}
I0203 16:51:41.629639 139702527031040 logging_writer.py:48] [39354] accumulated_eval_time=1894.608152, accumulated_logging_time=1.312205, accumulated_submission_time=18106.487027, global_step=39354, preemption_count=0, score=18106.487027, test/accuracy=0.448200, test/loss=2.514028, test/num_examples=10000, total_duration=20004.482606, train/accuracy=0.624102, train/loss=1.555987, validation/accuracy=0.565540, validation/loss=1.844379, validation/num_examples=50000
I0203 16:52:01.252656 139702543816448 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9685729742050171, loss=4.635970115661621
I0203 16:52:45.647314 139702527031040 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.0988799333572388, loss=3.017026662826538
I0203 16:53:32.015295 139702543816448 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.9785965085029602, loss=3.6303529739379883
I0203 16:54:18.585954 139702527031040 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.2387605905532837, loss=2.6724207401275635
I0203 16:55:04.713381 139702543816448 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.1230909824371338, loss=2.741675615310669
I0203 16:55:50.881790 139702527031040 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.0514167547225952, loss=3.2039730548858643
I0203 16:56:36.993961 139702543816448 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.0974431037902832, loss=2.957207679748535
I0203 16:57:23.732225 139702527031040 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.0355589389801025, loss=3.1230053901672363
I0203 16:58:10.108198 139702543816448 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.1663671731948853, loss=2.6796298027038574
I0203 16:58:41.824978 139863983413056 spec.py:321] Evaluating on the training split.
I0203 16:58:52.319423 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 16:59:24.833780 139863983413056 spec.py:349] Evaluating on the test split.
I0203 16:59:26.470546 139863983413056 submission_runner.py:408] Time since start: 20469.35s, 	Step: 40270, 	{'train/accuracy': 0.6108984351158142, 'train/loss': 1.6063635349273682, 'validation/accuracy': 0.56277996301651, 'validation/loss': 1.844841718673706, 'validation/num_examples': 50000, 'test/accuracy': 0.44520002603530884, 'test/loss': 2.520315647125244, 'test/num_examples': 10000, 'score': 18526.620589256287, 'total_duration': 20469.3478808403, 'accumulated_submission_time': 18526.620589256287, 'accumulated_eval_time': 1939.2537033557892, 'accumulated_logging_time': 1.3499047756195068}
I0203 16:59:26.498526 139702527031040 logging_writer.py:48] [40270] accumulated_eval_time=1939.253703, accumulated_logging_time=1.349905, accumulated_submission_time=18526.620589, global_step=40270, preemption_count=0, score=18526.620589, test/accuracy=0.445200, test/loss=2.520316, test/num_examples=10000, total_duration=20469.347881, train/accuracy=0.610898, train/loss=1.606364, validation/accuracy=0.562780, validation/loss=1.844842, validation/num_examples=50000
I0203 16:59:39.416957 139702543816448 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.9585462808609009, loss=3.459651470184326
I0203 17:00:23.337853 139702527031040 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.3680557012557983, loss=2.7054603099823
I0203 17:01:09.903308 139702543816448 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.0003769397735596, loss=3.345001220703125
I0203 17:01:56.195877 139702527031040 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.8714191317558289, loss=4.8852128982543945
I0203 17:02:42.713031 139702543816448 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.9350666999816895, loss=5.377903938293457
I0203 17:03:29.106784 139702527031040 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.108365774154663, loss=2.8092122077941895
I0203 17:04:15.700910 139702543816448 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.9624717235565186, loss=5.045082092285156
I0203 17:05:02.100848 139702527031040 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.0625669956207275, loss=3.1009087562561035
I0203 17:05:48.652492 139702543816448 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.2336065769195557, loss=2.8395631313323975
I0203 17:06:26.810565 139863983413056 spec.py:321] Evaluating on the training split.
I0203 17:06:37.339497 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 17:07:09.896644 139863983413056 spec.py:349] Evaluating on the test split.
I0203 17:07:11.524735 139863983413056 submission_runner.py:408] Time since start: 20934.40s, 	Step: 41184, 	{'train/accuracy': 0.6096289157867432, 'train/loss': 1.6241741180419922, 'validation/accuracy': 0.5638999938964844, 'validation/loss': 1.85176420211792, 'validation/num_examples': 50000, 'test/accuracy': 0.44670000672340393, 'test/loss': 2.5229926109313965, 'test/num_examples': 10000, 'score': 18946.871083021164, 'total_duration': 20934.40208363533, 'accumulated_submission_time': 18946.871083021164, 'accumulated_eval_time': 1983.9678659439087, 'accumulated_logging_time': 1.3874144554138184}
I0203 17:07:11.550980 139702527031040 logging_writer.py:48] [41184] accumulated_eval_time=1983.967866, accumulated_logging_time=1.387414, accumulated_submission_time=18946.871083, global_step=41184, preemption_count=0, score=18946.871083, test/accuracy=0.446700, test/loss=2.522993, test/num_examples=10000, total_duration=20934.402084, train/accuracy=0.609629, train/loss=1.624174, validation/accuracy=0.563900, validation/loss=1.851764, validation/num_examples=50000
I0203 17:07:18.631604 139702543816448 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.2171740531921387, loss=2.9401726722717285
I0203 17:08:02.413400 139702527031040 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.8987172245979309, loss=4.4794816970825195
I0203 17:08:48.566840 139702543816448 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.1156457662582397, loss=2.6598644256591797
I0203 17:09:35.118288 139702527031040 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8894193768501282, loss=5.148108005523682
I0203 17:10:21.693824 139702543816448 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.9211969375610352, loss=5.388546466827393
I0203 17:11:07.889394 139702527031040 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.9606105089187622, loss=5.271581172943115
I0203 17:11:54.081431 139702543816448 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.9931986927986145, loss=4.188056945800781
I0203 17:12:40.397287 139702527031040 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.9116688370704651, loss=5.221702575683594
I0203 17:13:29.308712 139702543816448 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.0234179496765137, loss=3.8215436935424805
I0203 17:14:11.886497 139863983413056 spec.py:321] Evaluating on the training split.
I0203 17:14:22.306433 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 17:14:56.006101 139863983413056 spec.py:349] Evaluating on the test split.
I0203 17:14:57.639599 139863983413056 submission_runner.py:408] Time since start: 21400.52s, 	Step: 42094, 	{'train/accuracy': 0.6166796684265137, 'train/loss': 1.6326512098312378, 'validation/accuracy': 0.5676999688148499, 'validation/loss': 1.8969939947128296, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.5553810596466064, 'test/num_examples': 10000, 'score': 19367.147775888443, 'total_duration': 21400.516935825348, 'accumulated_submission_time': 19367.147775888443, 'accumulated_eval_time': 2029.7209577560425, 'accumulated_logging_time': 1.4249577522277832}
I0203 17:14:57.662727 139702527031040 logging_writer.py:48] [42094] accumulated_eval_time=2029.720958, accumulated_logging_time=1.424958, accumulated_submission_time=19367.147776, global_step=42094, preemption_count=0, score=19367.147776, test/accuracy=0.444500, test/loss=2.555381, test/num_examples=10000, total_duration=21400.516936, train/accuracy=0.616680, train/loss=1.632651, validation/accuracy=0.567700, validation/loss=1.896994, validation/num_examples=50000
I0203 17:15:00.581122 139702543816448 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.0856603384017944, loss=2.7922842502593994
I0203 17:15:43.310352 139702527031040 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1495879888534546, loss=2.675747871398926
I0203 17:16:29.360514 139702543816448 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.5212513208389282, loss=2.6923675537109375
I0203 17:17:16.085958 139702527031040 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.071658730506897, loss=2.9104666709899902
I0203 17:18:02.307196 139702543816448 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.16781747341156, loss=2.7883214950561523
I0203 17:18:49.530008 139702527031040 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.0539274215698242, loss=3.3222713470458984
I0203 17:19:36.489576 139702543816448 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.0896682739257812, loss=3.887086868286133
I0203 17:20:23.249897 139702527031040 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.2577852010726929, loss=2.8897151947021484
I0203 17:21:09.968182 139702543816448 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.0452513694763184, loss=5.347686290740967
I0203 17:21:56.764617 139702527031040 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.2771574258804321, loss=2.7041938304901123
I0203 17:21:57.951242 139863983413056 spec.py:321] Evaluating on the training split.
I0203 17:22:08.527386 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 17:22:39.806889 139863983413056 spec.py:349] Evaluating on the test split.
I0203 17:22:41.437134 139863983413056 submission_runner.py:408] Time since start: 21864.31s, 	Step: 43004, 	{'train/accuracy': 0.6131054759025574, 'train/loss': 1.5964422225952148, 'validation/accuracy': 0.5736199617385864, 'validation/loss': 1.7933223247528076, 'validation/num_examples': 50000, 'test/accuracy': 0.45340001583099365, 'test/loss': 2.4737699031829834, 'test/num_examples': 10000, 'score': 19787.377880573273, 'total_duration': 21864.3144903183, 'accumulated_submission_time': 19787.377880573273, 'accumulated_eval_time': 2073.206840276718, 'accumulated_logging_time': 1.4591336250305176}
I0203 17:22:41.464017 139702543816448 logging_writer.py:48] [43004] accumulated_eval_time=2073.206840, accumulated_logging_time=1.459134, accumulated_submission_time=19787.377881, global_step=43004, preemption_count=0, score=19787.377881, test/accuracy=0.453400, test/loss=2.473770, test/num_examples=10000, total_duration=21864.314490, train/accuracy=0.613105, train/loss=1.596442, validation/accuracy=0.573620, validation/loss=1.793322, validation/num_examples=50000
I0203 17:23:22.556051 139702527031040 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.2384778261184692, loss=2.652477502822876
I0203 17:24:08.517405 139702543816448 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.025244951248169, loss=4.335183143615723
I0203 17:24:54.699960 139702527031040 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.0093188285827637, loss=5.270820617675781
I0203 17:25:40.929059 139702543816448 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.124035120010376, loss=2.802025318145752
I0203 17:26:27.070311 139702527031040 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.0451279878616333, loss=5.2112812995910645
I0203 17:27:13.299790 139702543816448 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.169842004776001, loss=2.7363569736480713
I0203 17:27:59.366192 139702527031040 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.1645402908325195, loss=2.7342653274536133
I0203 17:28:45.779230 139702543816448 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.833261251449585, loss=4.9778642654418945
I0203 17:29:32.042916 139702527031040 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.2659682035446167, loss=2.9360392093658447
I0203 17:29:41.905105 139863983413056 spec.py:321] Evaluating on the training split.
I0203 17:29:52.682661 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 17:30:26.654922 139863983413056 spec.py:349] Evaluating on the test split.
I0203 17:30:28.289759 139863983413056 submission_runner.py:408] Time since start: 22331.17s, 	Step: 43923, 	{'train/accuracy': 0.6158202886581421, 'train/loss': 1.6060982942581177, 'validation/accuracy': 0.5713199973106384, 'validation/loss': 1.8300766944885254, 'validation/num_examples': 50000, 'test/accuracy': 0.4507000148296356, 'test/loss': 2.493374824523926, 'test/num_examples': 10000, 'score': 20207.759017944336, 'total_duration': 22331.16711997986, 'accumulated_submission_time': 20207.759017944336, 'accumulated_eval_time': 2119.591502904892, 'accumulated_logging_time': 1.496941328048706}
I0203 17:30:28.311887 139702543816448 logging_writer.py:48] [43923] accumulated_eval_time=2119.591503, accumulated_logging_time=1.496941, accumulated_submission_time=20207.759018, global_step=43923, preemption_count=0, score=20207.759018, test/accuracy=0.450700, test/loss=2.493375, test/num_examples=10000, total_duration=22331.167120, train/accuracy=0.615820, train/loss=1.606098, validation/accuracy=0.571320, validation/loss=1.830077, validation/num_examples=50000
I0203 17:31:01.197036 139702527031040 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.0362343788146973, loss=2.8262901306152344
I0203 17:31:46.664079 139702543816448 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.1140321493148804, loss=3.0011303424835205
I0203 17:32:33.111515 139702527031040 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.2097591161727905, loss=3.2730276584625244
I0203 17:33:19.363163 139702543816448 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.1416429281234741, loss=3.335172414779663
I0203 17:34:05.473221 139702527031040 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.425936222076416, loss=2.917614459991455
I0203 17:34:51.619889 139702543816448 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.1741101741790771, loss=2.772730588912964
I0203 17:35:37.835520 139702527031040 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.1990904808044434, loss=2.796431064605713
I0203 17:36:24.330305 139702543816448 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.9470486640930176, loss=5.242828369140625
I0203 17:37:10.443686 139702527031040 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.9879516363143921, loss=5.241023063659668
I0203 17:37:28.719977 139863983413056 spec.py:321] Evaluating on the training split.
I0203 17:37:39.135499 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 17:38:11.753098 139863983413056 spec.py:349] Evaluating on the test split.
I0203 17:38:13.385614 139863983413056 submission_runner.py:408] Time since start: 22796.26s, 	Step: 44841, 	{'train/accuracy': 0.6277539134025574, 'train/loss': 1.5462582111358643, 'validation/accuracy': 0.5756999850273132, 'validation/loss': 1.8153027296066284, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.492283821105957, 'test/num_examples': 10000, 'score': 20628.109059095383, 'total_duration': 22796.262968063354, 'accumulated_submission_time': 20628.109059095383, 'accumulated_eval_time': 2164.2571284770966, 'accumulated_logging_time': 1.5289440155029297}
I0203 17:38:13.407798 139702543816448 logging_writer.py:48] [44841] accumulated_eval_time=2164.257128, accumulated_logging_time=1.528944, accumulated_submission_time=20628.109059, global_step=44841, preemption_count=0, score=20628.109059, test/accuracy=0.459300, test/loss=2.492284, test/num_examples=10000, total_duration=22796.262968, train/accuracy=0.627754, train/loss=1.546258, validation/accuracy=0.575700, validation/loss=1.815303, validation/num_examples=50000
I0203 17:38:38.424007 139702527031040 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.8605608940124512, loss=4.479554653167725
I0203 17:39:23.342295 139702543816448 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.9427875876426697, loss=4.695085525512695
I0203 17:40:10.080825 139702527031040 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.0816582441329956, loss=3.1732709407806396
I0203 17:40:56.068392 139702543816448 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.215004563331604, loss=2.6272923946380615
I0203 17:41:42.214519 139702527031040 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.9952419400215149, loss=4.446805953979492
I0203 17:42:28.863683 139702543816448 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.2517523765563965, loss=2.7409939765930176
I0203 17:43:14.895873 139702527031040 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.9395430684089661, loss=4.602181434631348
I0203 17:44:01.287492 139702543816448 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.162227988243103, loss=3.2833244800567627
I0203 17:44:47.546853 139702527031040 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.189475655555725, loss=2.6268553733825684
I0203 17:45:13.671631 139863983413056 spec.py:321] Evaluating on the training split.
I0203 17:45:24.067152 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 17:45:57.711599 139863983413056 spec.py:349] Evaluating on the test split.
I0203 17:45:59.354112 139863983413056 submission_runner.py:408] Time since start: 23262.23s, 	Step: 45758, 	{'train/accuracy': 0.6309570074081421, 'train/loss': 1.5428788661956787, 'validation/accuracy': 0.5822799801826477, 'validation/loss': 1.779966950416565, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.4568777084350586, 'test/num_examples': 10000, 'score': 21048.3162214756, 'total_duration': 23262.23146867752, 'accumulated_submission_time': 21048.3162214756, 'accumulated_eval_time': 2209.939630508423, 'accumulated_logging_time': 1.5598695278167725}
I0203 17:45:59.378031 139702543816448 logging_writer.py:48] [45758] accumulated_eval_time=2209.939631, accumulated_logging_time=1.559870, accumulated_submission_time=21048.316221, global_step=45758, preemption_count=0, score=21048.316221, test/accuracy=0.462200, test/loss=2.456878, test/num_examples=10000, total_duration=23262.231469, train/accuracy=0.630957, train/loss=1.542879, validation/accuracy=0.582280, validation/loss=1.779967, validation/num_examples=50000
I0203 17:46:17.320718 139702527031040 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1698213815689087, loss=2.7014760971069336
I0203 17:47:01.351396 139702543816448 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.030016303062439, loss=3.5449371337890625
I0203 17:47:47.714562 139702527031040 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.1571803092956543, loss=2.6052565574645996
I0203 17:48:34.049349 139702543816448 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.1833547353744507, loss=2.6514248847961426
I0203 17:49:20.348508 139702527031040 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.9232885241508484, loss=4.4612507820129395
I0203 17:50:07.057891 139702543816448 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.0843349695205688, loss=3.226396083831787
I0203 17:50:53.161947 139702527031040 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.0227627754211426, loss=3.2902252674102783
I0203 17:51:39.543844 139702543816448 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.9329873919487, loss=5.326423645019531
I0203 17:52:25.694244 139702527031040 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1067768335342407, loss=5.129812240600586
I0203 17:52:59.403324 139863983413056 spec.py:321] Evaluating on the training split.
I0203 17:53:09.997292 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 17:53:40.159367 139863983413056 spec.py:349] Evaluating on the test split.
I0203 17:53:41.803641 139863983413056 submission_runner.py:408] Time since start: 23724.68s, 	Step: 46675, 	{'train/accuracy': 0.6208202838897705, 'train/loss': 1.5916742086410522, 'validation/accuracy': 0.574679970741272, 'validation/loss': 1.8139941692352295, 'validation/num_examples': 50000, 'test/accuracy': 0.45990002155303955, 'test/loss': 2.479991912841797, 'test/num_examples': 10000, 'score': 21468.28194952011, 'total_duration': 23724.68097090721, 'accumulated_submission_time': 21468.28194952011, 'accumulated_eval_time': 2252.3399090766907, 'accumulated_logging_time': 1.5934512615203857}
I0203 17:53:41.829185 139702543816448 logging_writer.py:48] [46675] accumulated_eval_time=2252.339909, accumulated_logging_time=1.593451, accumulated_submission_time=21468.281950, global_step=46675, preemption_count=0, score=21468.281950, test/accuracy=0.459900, test/loss=2.479992, test/num_examples=10000, total_duration=23724.680971, train/accuracy=0.620820, train/loss=1.591674, validation/accuracy=0.574680, validation/loss=1.813994, validation/num_examples=50000
I0203 17:53:52.675631 139702527031040 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.9109334349632263, loss=4.975370407104492
I0203 17:54:36.177448 139702543816448 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.1370927095413208, loss=2.6601006984710693
I0203 17:55:22.626101 139702527031040 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.1115703582763672, loss=2.6139073371887207
I0203 17:56:08.985693 139702543816448 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.183414101600647, loss=2.6021151542663574
I0203 17:56:55.168636 139702527031040 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.1596739292144775, loss=2.6660819053649902
I0203 17:57:41.524162 139702543816448 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.0728340148925781, loss=2.678382635116577
I0203 17:58:28.195814 139702527031040 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.139758586883545, loss=2.8440399169921875
I0203 17:59:14.427518 139702543816448 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1966345310211182, loss=2.5051121711730957
I0203 18:00:00.995296 139702527031040 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.2268753051757812, loss=2.5915658473968506
I0203 18:00:42.366167 139863983413056 spec.py:321] Evaluating on the training split.
I0203 18:00:52.856202 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 18:01:24.447457 139863983413056 spec.py:349] Evaluating on the test split.
I0203 18:01:26.092740 139863983413056 submission_runner.py:408] Time since start: 24188.97s, 	Step: 47590, 	{'train/accuracy': 0.6354101300239563, 'train/loss': 1.507873773574829, 'validation/accuracy': 0.5806800127029419, 'validation/loss': 1.768570065498352, 'validation/num_examples': 50000, 'test/accuracy': 0.46250003576278687, 'test/loss': 2.438570737838745, 'test/num_examples': 10000, 'score': 21888.7586414814, 'total_duration': 24188.970096588135, 'accumulated_submission_time': 21888.7586414814, 'accumulated_eval_time': 2296.0664880275726, 'accumulated_logging_time': 1.631007432937622}
I0203 18:01:26.120140 139702543816448 logging_writer.py:48] [47590] accumulated_eval_time=2296.066488, accumulated_logging_time=1.631007, accumulated_submission_time=21888.758641, global_step=47590, preemption_count=0, score=21888.758641, test/accuracy=0.462500, test/loss=2.438571, test/num_examples=10000, total_duration=24188.970097, train/accuracy=0.635410, train/loss=1.507874, validation/accuracy=0.580680, validation/loss=1.768570, validation/num_examples=50000
I0203 18:01:30.704972 139702527031040 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.2517622709274292, loss=2.544245719909668
I0203 18:02:13.589741 139702543816448 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2393440008163452, loss=2.613933801651001
I0203 18:02:59.519113 139702527031040 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.9164586067199707, loss=5.280664920806885
I0203 18:03:45.957001 139702543816448 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.9862864017486572, loss=3.3891074657440186
I0203 18:04:31.929090 139702527031040 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.2761540412902832, loss=5.239329814910889
I0203 18:05:18.130438 139702543816448 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.203289270401001, loss=2.609734058380127
I0203 18:06:04.387197 139702527031040 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.9921697378158569, loss=3.7855770587921143
I0203 18:06:50.530256 139702543816448 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.201439619064331, loss=2.5973706245422363
I0203 18:07:36.835304 139702527031040 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.1774227619171143, loss=2.885490894317627
I0203 18:08:22.985803 139702543816448 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0811333656311035, loss=2.9036054611206055
I0203 18:08:26.428234 139863983413056 spec.py:321] Evaluating on the training split.
I0203 18:08:37.014473 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 18:09:09.478602 139863983413056 spec.py:349] Evaluating on the test split.
I0203 18:09:11.121582 139863983413056 submission_runner.py:408] Time since start: 24654.00s, 	Step: 48509, 	{'train/accuracy': 0.6536523103713989, 'train/loss': 1.431609869003296, 'validation/accuracy': 0.5843600034713745, 'validation/loss': 1.7661563158035278, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.4309449195861816, 'test/num_examples': 10000, 'score': 22309.008142709732, 'total_duration': 24653.99893283844, 'accumulated_submission_time': 22309.008142709732, 'accumulated_eval_time': 2340.759823322296, 'accumulated_logging_time': 1.6684105396270752}
I0203 18:09:11.150736 139702527031040 logging_writer.py:48] [48509] accumulated_eval_time=2340.759823, accumulated_logging_time=1.668411, accumulated_submission_time=22309.008143, global_step=48509, preemption_count=0, score=22309.008143, test/accuracy=0.468900, test/loss=2.430945, test/num_examples=10000, total_duration=24653.998933, train/accuracy=0.653652, train/loss=1.431610, validation/accuracy=0.584360, validation/loss=1.766156, validation/num_examples=50000
I0203 18:09:49.994435 139702543816448 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.379440426826477, loss=2.814246654510498
I0203 18:10:36.533740 139702527031040 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.3575408458709717, loss=2.616511821746826
I0203 18:11:22.925522 139702543816448 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.0825239419937134, loss=3.2296714782714844
I0203 18:12:09.005457 139702527031040 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.099995732307434, loss=3.7251768112182617
I0203 18:12:55.239678 139702543816448 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.0566012859344482, loss=4.468613624572754
I0203 18:13:41.640449 139702527031040 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.0685207843780518, loss=5.055295467376709
I0203 18:14:27.923610 139702543816448 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.2618112564086914, loss=2.7346158027648926
I0203 18:15:14.266297 139702527031040 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.11249577999115, loss=2.8301026821136475
I0203 18:16:00.385691 139702543816448 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.216842532157898, loss=2.7077927589416504
I0203 18:16:11.230759 139863983413056 spec.py:321] Evaluating on the training split.
I0203 18:16:21.800510 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 18:16:56.758535 139863983413056 spec.py:349] Evaluating on the test split.
I0203 18:16:58.405708 139863983413056 submission_runner.py:408] Time since start: 25121.28s, 	Step: 49425, 	{'train/accuracy': 0.6244921684265137, 'train/loss': 1.5902440547943115, 'validation/accuracy': 0.5818600058555603, 'validation/loss': 1.7915534973144531, 'validation/num_examples': 50000, 'test/accuracy': 0.4668000340461731, 'test/loss': 2.4473612308502197, 'test/num_examples': 10000, 'score': 22729.027530670166, 'total_duration': 25121.2830452919, 'accumulated_submission_time': 22729.027530670166, 'accumulated_eval_time': 2387.934749364853, 'accumulated_logging_time': 1.710059642791748}
I0203 18:16:58.435224 139702527031040 logging_writer.py:48] [49425] accumulated_eval_time=2387.934749, accumulated_logging_time=1.710060, accumulated_submission_time=22729.027531, global_step=49425, preemption_count=0, score=22729.027531, test/accuracy=0.466800, test/loss=2.447361, test/num_examples=10000, total_duration=25121.283045, train/accuracy=0.624492, train/loss=1.590244, validation/accuracy=0.581860, validation/loss=1.791553, validation/num_examples=50000
I0203 18:17:30.146608 139702543816448 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1903566122055054, loss=2.6319658756256104
I0203 18:18:15.785145 139702527031040 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.176842212677002, loss=2.7945690155029297
I0203 18:19:02.002693 139702543816448 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.0200010538101196, loss=5.175051689147949
I0203 18:19:48.358755 139702527031040 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.1165251731872559, loss=2.821939468383789
I0203 18:20:34.993271 139702543816448 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.0699809789657593, loss=4.609602451324463
I0203 18:21:21.278580 139702527031040 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.9793189167976379, loss=5.242741107940674
I0203 18:22:07.926593 139702543816448 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.1570427417755127, loss=2.530229091644287
I0203 18:22:54.020497 139702527031040 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.0642400979995728, loss=3.9616334438323975
I0203 18:23:40.330571 139702543816448 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.1954096555709839, loss=2.6962428092956543
I0203 18:23:58.771651 139863983413056 spec.py:321] Evaluating on the training split.
I0203 18:24:09.182913 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 18:24:41.902915 139863983413056 spec.py:349] Evaluating on the test split.
I0203 18:24:43.546483 139863983413056 submission_runner.py:408] Time since start: 25586.42s, 	Step: 50342, 	{'train/accuracy': 0.6416601538658142, 'train/loss': 1.4782509803771973, 'validation/accuracy': 0.5866000056266785, 'validation/loss': 1.7392513751983643, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.4052207469940186, 'test/num_examples': 10000, 'score': 23149.305313825607, 'total_duration': 25586.423802137375, 'accumulated_submission_time': 23149.305313825607, 'accumulated_eval_time': 2432.709528207779, 'accumulated_logging_time': 1.7498483657836914}
I0203 18:24:43.574683 139702527031040 logging_writer.py:48] [50342] accumulated_eval_time=2432.709528, accumulated_logging_time=1.749848, accumulated_submission_time=23149.305314, global_step=50342, preemption_count=0, score=23149.305314, test/accuracy=0.470200, test/loss=2.405221, test/num_examples=10000, total_duration=25586.423802, train/accuracy=0.641660, train/loss=1.478251, validation/accuracy=0.586600, validation/loss=1.739251, validation/num_examples=50000
I0203 18:25:08.183927 139702543816448 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.0729936361312866, loss=5.302036762237549
I0203 18:25:52.998375 139702527031040 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.1885638236999512, loss=2.520843505859375
I0203 18:26:39.641263 139702543816448 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.3188403844833374, loss=2.6903390884399414
I0203 18:27:25.977549 139702527031040 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.1528496742248535, loss=3.0340383052825928
I0203 18:28:12.178250 139702543816448 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.275818109512329, loss=2.654325246810913
I0203 18:28:58.425307 139702527031040 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.950188159942627, loss=5.190411567687988
I0203 18:29:44.750630 139702543816448 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1628971099853516, loss=3.0256407260894775
I0203 18:30:31.048091 139702527031040 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.078592300415039, loss=2.890746831893921
I0203 18:31:17.346926 139702543816448 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.1412889957427979, loss=2.607337474822998
I0203 18:31:43.777912 139863983413056 spec.py:321] Evaluating on the training split.
I0203 18:31:54.398596 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 18:32:26.554928 139863983413056 spec.py:349] Evaluating on the test split.
I0203 18:32:28.199295 139863983413056 submission_runner.py:408] Time since start: 26051.08s, 	Step: 51259, 	{'train/accuracy': 0.6513866782188416, 'train/loss': 1.4301273822784424, 'validation/accuracy': 0.5900200009346008, 'validation/loss': 1.729769229888916, 'validation/num_examples': 50000, 'test/accuracy': 0.46970000863075256, 'test/loss': 2.399967670440674, 'test/num_examples': 10000, 'score': 23569.449481725693, 'total_duration': 26051.07665014267, 'accumulated_submission_time': 23569.449481725693, 'accumulated_eval_time': 2477.1309113502502, 'accumulated_logging_time': 1.788116216659546}
I0203 18:32:28.223811 139702527031040 logging_writer.py:48] [51259] accumulated_eval_time=2477.130911, accumulated_logging_time=1.788116, accumulated_submission_time=23569.449482, global_step=51259, preemption_count=0, score=23569.449482, test/accuracy=0.469700, test/loss=2.399968, test/num_examples=10000, total_duration=26051.076650, train/accuracy=0.651387, train/loss=1.430127, validation/accuracy=0.590020, validation/loss=1.729769, validation/num_examples=50000
I0203 18:32:45.738707 139702543816448 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.3212168216705322, loss=3.2870895862579346
I0203 18:33:29.943049 139702527031040 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.2199658155441284, loss=2.631340980529785
I0203 18:34:16.208198 139702543816448 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.1450262069702148, loss=2.7722721099853516
I0203 18:35:02.903681 139702527031040 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.224796175956726, loss=2.7585010528564453
I0203 18:35:48.878179 139702543816448 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.1146539449691772, loss=3.1615426540374756
I0203 18:36:35.170618 139702527031040 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.9897599816322327, loss=5.144595623016357
I0203 18:37:21.397884 139702543816448 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.1443731784820557, loss=4.737518310546875
I0203 18:38:07.756549 139702527031040 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.053601622581482, loss=5.292606353759766
I0203 18:38:54.062870 139702543816448 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.198717713356018, loss=2.6245574951171875
I0203 18:39:28.419281 139863983413056 spec.py:321] Evaluating on the training split.
I0203 18:39:38.744626 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 18:40:12.312805 139863983413056 spec.py:349] Evaluating on the test split.
I0203 18:40:13.953440 139863983413056 submission_runner.py:408] Time since start: 26516.83s, 	Step: 52176, 	{'train/accuracy': 0.6253125071525574, 'train/loss': 1.5959267616271973, 'validation/accuracy': 0.5811799764633179, 'validation/loss': 1.8141032457351685, 'validation/num_examples': 50000, 'test/accuracy': 0.460500031709671, 'test/loss': 2.463486433029175, 'test/num_examples': 10000, 'score': 23989.585379123688, 'total_duration': 26516.830800056458, 'accumulated_submission_time': 23989.585379123688, 'accumulated_eval_time': 2522.6650750637054, 'accumulated_logging_time': 1.8231792449951172}
I0203 18:40:13.978069 139702527031040 logging_writer.py:48] [52176] accumulated_eval_time=2522.665075, accumulated_logging_time=1.823179, accumulated_submission_time=23989.585379, global_step=52176, preemption_count=0, score=23989.585379, test/accuracy=0.460500, test/loss=2.463486, test/num_examples=10000, total_duration=26516.830800, train/accuracy=0.625313, train/loss=1.595927, validation/accuracy=0.581180, validation/loss=1.814103, validation/num_examples=50000
I0203 18:40:24.397456 139702543816448 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.1607908010482788, loss=2.538242816925049
I0203 18:41:08.099957 139702527031040 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.3278757333755493, loss=2.5969948768615723
I0203 18:41:54.148817 139702543816448 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.0845887660980225, loss=3.2555294036865234
I0203 18:42:40.485758 139702527031040 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.2892513275146484, loss=2.5257513523101807
I0203 18:43:27.117580 139702543816448 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.0256116390228271, loss=3.8711721897125244
I0203 18:44:13.397602 139702527031040 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.105292797088623, loss=5.202764987945557
I0203 18:44:59.934711 139702543816448 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.240350365638733, loss=2.7007439136505127
I0203 18:45:46.560911 139702527031040 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.1637591123580933, loss=2.4447615146636963
I0203 18:46:32.875340 139702543816448 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.188359022140503, loss=2.58015775680542
I0203 18:47:14.304551 139863983413056 spec.py:321] Evaluating on the training split.
I0203 18:47:24.769445 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 18:47:57.238256 139863983413056 spec.py:349] Evaluating on the test split.
I0203 18:47:58.897139 139863983413056 submission_runner.py:408] Time since start: 26981.77s, 	Step: 53091, 	{'train/accuracy': 0.6471874713897705, 'train/loss': 1.444928526878357, 'validation/accuracy': 0.5928999781608582, 'validation/loss': 1.692473292350769, 'validation/num_examples': 50000, 'test/accuracy': 0.4796000123023987, 'test/loss': 2.35530948638916, 'test/num_examples': 10000, 'score': 24409.854116678238, 'total_duration': 26981.77447628975, 'accumulated_submission_time': 24409.854116678238, 'accumulated_eval_time': 2567.257641553879, 'accumulated_logging_time': 1.8575630187988281}
I0203 18:47:58.929245 139702527031040 logging_writer.py:48] [53091] accumulated_eval_time=2567.257642, accumulated_logging_time=1.857563, accumulated_submission_time=24409.854117, global_step=53091, preemption_count=0, score=24409.854117, test/accuracy=0.479600, test/loss=2.355309, test/num_examples=10000, total_duration=26981.774476, train/accuracy=0.647187, train/loss=1.444929, validation/accuracy=0.592900, validation/loss=1.692473, validation/num_examples=50000
I0203 18:48:03.110906 139702543816448 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.156428575515747, loss=2.6427433490753174
I0203 18:48:45.905030 139702527031040 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2177091836929321, loss=2.738416910171509
I0203 18:49:32.160595 139702543816448 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.2397304773330688, loss=2.700890064239502
I0203 18:50:18.638175 139702527031040 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.1859184503555298, loss=2.6263413429260254
I0203 18:51:04.734303 139702543816448 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0894756317138672, loss=5.227989673614502
I0203 18:51:51.004158 139702527031040 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.9876654148101807, loss=4.939114093780518
I0203 18:52:37.377573 139702543816448 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.241838812828064, loss=2.5861618518829346
I0203 18:53:23.942181 139702527031040 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.0746885538101196, loss=3.283200263977051
I0203 18:54:10.123451 139702543816448 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.126254916191101, loss=3.5916757583618164
I0203 18:54:56.386639 139702527031040 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.3319834470748901, loss=2.5863993167877197
I0203 18:54:59.247589 139863983413056 spec.py:321] Evaluating on the training split.
I0203 18:55:09.942118 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 18:55:41.071857 139863983413056 spec.py:349] Evaluating on the test split.
I0203 18:55:42.712565 139863983413056 submission_runner.py:408] Time since start: 27445.59s, 	Step: 54008, 	{'train/accuracy': 0.6502734422683716, 'train/loss': 1.464459776878357, 'validation/accuracy': 0.5907399654388428, 'validation/loss': 1.7406047582626343, 'validation/num_examples': 50000, 'test/accuracy': 0.4711000323295593, 'test/loss': 2.407909393310547, 'test/num_examples': 10000, 'score': 24830.11319756508, 'total_duration': 27445.589923620224, 'accumulated_submission_time': 24830.11319756508, 'accumulated_eval_time': 2610.7226634025574, 'accumulated_logging_time': 1.9003996849060059}
I0203 18:55:42.737735 139702543816448 logging_writer.py:48] [54008] accumulated_eval_time=2610.722663, accumulated_logging_time=1.900400, accumulated_submission_time=24830.113198, global_step=54008, preemption_count=0, score=24830.113198, test/accuracy=0.471100, test/loss=2.407909, test/num_examples=10000, total_duration=27445.589924, train/accuracy=0.650273, train/loss=1.464460, validation/accuracy=0.590740, validation/loss=1.740605, validation/num_examples=50000
I0203 18:56:21.822359 139702527031040 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.382979154586792, loss=2.6335840225219727
I0203 18:57:08.088982 139702543816448 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.2741286754608154, loss=2.524242401123047
I0203 18:57:54.226783 139702527031040 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.998100757598877, loss=5.08469820022583
I0203 18:58:40.237913 139702543816448 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.0668882131576538, loss=4.320258140563965
I0203 18:59:26.479639 139702527031040 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.9153927564620972, loss=4.807159423828125
I0203 19:00:12.741783 139702543816448 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.3095744848251343, loss=2.8445017337799072
I0203 19:00:58.740914 139702527031040 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.9660888910293579, loss=4.5083136558532715
I0203 19:01:45.006297 139702543816448 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.1404657363891602, loss=2.9307661056518555
I0203 19:02:31.409570 139702527031040 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.1337426900863647, loss=2.9943253993988037
I0203 19:02:43.032040 139863983413056 spec.py:321] Evaluating on the training split.
I0203 19:02:53.343462 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 19:03:27.499454 139863983413056 spec.py:349] Evaluating on the test split.
I0203 19:03:29.138432 139863983413056 submission_runner.py:408] Time since start: 27912.02s, 	Step: 54927, 	{'train/accuracy': 0.6294726133346558, 'train/loss': 1.5730618238449097, 'validation/accuracy': 0.5880399942398071, 'validation/loss': 1.7772213220596313, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.4256205558776855, 'test/num_examples': 10000, 'score': 25250.349819660187, 'total_duration': 27912.015790462494, 'accumulated_submission_time': 25250.349819660187, 'accumulated_eval_time': 2656.8290503025055, 'accumulated_logging_time': 1.9344263076782227}
I0203 19:03:29.163647 139702543816448 logging_writer.py:48] [54927] accumulated_eval_time=2656.829050, accumulated_logging_time=1.934426, accumulated_submission_time=25250.349820, global_step=54927, preemption_count=0, score=25250.349820, test/accuracy=0.472000, test/loss=2.425621, test/num_examples=10000, total_duration=27912.015790, train/accuracy=0.629473, train/loss=1.573062, validation/accuracy=0.588040, validation/loss=1.777221, validation/num_examples=50000
I0203 19:04:00.032159 139702527031040 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.14212965965271, loss=2.5399508476257324
I0203 19:04:45.575535 139702543816448 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.2780128717422485, loss=2.6591691970825195
I0203 19:05:32.019236 139702527031040 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.0556474924087524, loss=5.238920211791992
I0203 19:06:18.282575 139702543816448 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.3221055269241333, loss=2.6154720783233643
I0203 19:07:04.204730 139702527031040 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.1612398624420166, loss=3.3558990955352783
I0203 19:07:50.428394 139702543816448 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.9982591867446899, loss=4.736015796661377
I0203 19:08:36.616722 139702527031040 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.2708758115768433, loss=2.8417088985443115
I0203 19:09:22.938601 139702543816448 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.2862699031829834, loss=2.6167876720428467
I0203 19:10:09.356399 139702527031040 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0816874504089355, loss=3.2007925510406494
I0203 19:10:29.385649 139863983413056 spec.py:321] Evaluating on the training split.
I0203 19:10:39.808762 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 19:11:11.011905 139863983413056 spec.py:349] Evaluating on the test split.
I0203 19:11:12.664698 139863983413056 submission_runner.py:408] Time since start: 28375.54s, 	Step: 55845, 	{'train/accuracy': 0.6401562094688416, 'train/loss': 1.5094945430755615, 'validation/accuracy': 0.5906800031661987, 'validation/loss': 1.7373265027999878, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.4103524684906006, 'test/num_examples': 10000, 'score': 25670.50795698166, 'total_duration': 28375.542023181915, 'accumulated_submission_time': 25670.50795698166, 'accumulated_eval_time': 2700.1080725193024, 'accumulated_logging_time': 1.9745028018951416}
I0203 19:11:12.704578 139702543816448 logging_writer.py:48] [55845] accumulated_eval_time=2700.108073, accumulated_logging_time=1.974503, accumulated_submission_time=25670.507957, global_step=55845, preemption_count=0, score=25670.507957, test/accuracy=0.474500, test/loss=2.410352, test/num_examples=10000, total_duration=28375.542023, train/accuracy=0.640156, train/loss=1.509495, validation/accuracy=0.590680, validation/loss=1.737327, validation/num_examples=50000
I0203 19:11:36.048718 139702527031040 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.355150818824768, loss=2.6833343505859375
I0203 19:12:20.623619 139702543816448 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.2392637729644775, loss=2.8784470558166504
I0203 19:13:07.418774 139702527031040 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.3041738271713257, loss=2.7686455249786377
I0203 19:13:53.268662 139702543816448 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.212289571762085, loss=2.6461944580078125
I0203 19:14:39.859590 139702527031040 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.2866544723510742, loss=2.4848570823669434
I0203 19:15:25.908702 139702543816448 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.3397176265716553, loss=2.523336410522461
I0203 19:16:12.344714 139702527031040 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.2290103435516357, loss=2.566417932510376
I0203 19:16:58.444979 139702543816448 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.4728596210479736, loss=2.616332769393921
I0203 19:17:44.606870 139702527031040 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.0244022607803345, loss=4.7122087478637695
I0203 19:18:13.003587 139863983413056 spec.py:321] Evaluating on the training split.
I0203 19:18:23.866879 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 19:18:53.614779 139863983413056 spec.py:349] Evaluating on the test split.
I0203 19:18:55.255373 139863983413056 submission_runner.py:408] Time since start: 28838.13s, 	Step: 56763, 	{'train/accuracy': 0.646484375, 'train/loss': 1.4634003639221191, 'validation/accuracy': 0.5952399969100952, 'validation/loss': 1.7165286540985107, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.4129831790924072, 'test/num_examples': 10000, 'score': 26090.748301029205, 'total_duration': 28838.13270163536, 'accumulated_submission_time': 26090.748301029205, 'accumulated_eval_time': 2742.3598341941833, 'accumulated_logging_time': 2.0240936279296875}
I0203 19:18:55.284359 139702543816448 logging_writer.py:48] [56763] accumulated_eval_time=2742.359834, accumulated_logging_time=2.024094, accumulated_submission_time=26090.748301, global_step=56763, preemption_count=0, score=26090.748301, test/accuracy=0.469500, test/loss=2.412983, test/num_examples=10000, total_duration=28838.132702, train/accuracy=0.646484, train/loss=1.463400, validation/accuracy=0.595240, validation/loss=1.716529, validation/num_examples=50000
I0203 19:19:11.125226 139702527031040 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.2907205820083618, loss=2.6481645107269287
I0203 19:19:55.105413 139702543816448 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.2833646535873413, loss=2.6197447776794434
I0203 19:20:41.409551 139702527031040 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.258625864982605, loss=2.5852959156036377
I0203 19:21:27.523053 139702543816448 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.3130826950073242, loss=2.6205360889434814
I0203 19:22:13.769314 139702527031040 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.3015753030776978, loss=2.4813334941864014
I0203 19:22:59.816038 139702543816448 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.2547482252120972, loss=2.4768803119659424
I0203 19:23:45.986471 139702527031040 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.2133837938308716, loss=2.996774435043335
I0203 19:24:32.123168 139702543816448 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.0583460330963135, loss=4.186925411224365
I0203 19:25:18.617841 139702527031040 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.450397253036499, loss=2.6336607933044434
I0203 19:25:55.578096 139863983413056 spec.py:321] Evaluating on the training split.
I0203 19:26:06.138146 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 19:26:39.508693 139863983413056 spec.py:349] Evaluating on the test split.
I0203 19:26:41.152826 139863983413056 submission_runner.py:408] Time since start: 29304.03s, 	Step: 57682, 	{'train/accuracy': 0.6556054353713989, 'train/loss': 1.3994473218917847, 'validation/accuracy': 0.599299967288971, 'validation/loss': 1.6665985584259033, 'validation/num_examples': 50000, 'test/accuracy': 0.4797000288963318, 'test/loss': 2.3530988693237305, 'test/num_examples': 10000, 'score': 26510.983321666718, 'total_duration': 29304.030174016953, 'accumulated_submission_time': 26510.983321666718, 'accumulated_eval_time': 2787.934552669525, 'accumulated_logging_time': 2.0629003047943115}
I0203 19:26:41.184530 139702543816448 logging_writer.py:48] [57682] accumulated_eval_time=2787.934553, accumulated_logging_time=2.062900, accumulated_submission_time=26510.983322, global_step=57682, preemption_count=0, score=26510.983322, test/accuracy=0.479700, test/loss=2.353099, test/num_examples=10000, total_duration=29304.030174, train/accuracy=0.655605, train/loss=1.399447, validation/accuracy=0.599300, validation/loss=1.666599, validation/num_examples=50000
I0203 19:26:49.088382 139702527031040 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.0991777181625366, loss=2.9621083736419678
I0203 19:27:32.159078 139702543816448 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.5685402154922485, loss=2.538512945175171
I0203 19:28:18.419265 139702527031040 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.2196851968765259, loss=2.7303836345672607
I0203 19:29:04.839063 139702543816448 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.3073186874389648, loss=5.16322135925293
I0203 19:29:51.142626 139702527031040 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.9942117929458618, loss=5.152411460876465
I0203 19:30:37.159522 139702543816448 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.286203145980835, loss=2.5547919273376465
I0203 19:31:23.271719 139702527031040 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.0380662679672241, loss=3.914201021194458
I0203 19:32:09.404226 139702543816448 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.2571372985839844, loss=2.547682523727417
I0203 19:32:55.652529 139702527031040 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.5346105098724365, loss=2.6984806060791016
I0203 19:33:41.796238 139702543816448 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.476387858390808, loss=2.765528440475464
I0203 19:33:41.810746 139863983413056 spec.py:321] Evaluating on the training split.
I0203 19:33:52.374620 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 19:34:26.479203 139863983413056 spec.py:349] Evaluating on the test split.
I0203 19:34:28.118705 139863983413056 submission_runner.py:408] Time since start: 29771.00s, 	Step: 58601, 	{'train/accuracy': 0.6381640434265137, 'train/loss': 1.4684827327728271, 'validation/accuracy': 0.5956400036811829, 'validation/loss': 1.700318694114685, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.3680901527404785, 'test/num_examples': 10000, 'score': 26931.549178361893, 'total_duration': 29770.996066331863, 'accumulated_submission_time': 26931.549178361893, 'accumulated_eval_time': 2834.242516517639, 'accumulated_logging_time': 2.105926275253296}
I0203 19:34:28.143912 139702527031040 logging_writer.py:48] [58601] accumulated_eval_time=2834.242517, accumulated_logging_time=2.105926, accumulated_submission_time=26931.549178, global_step=58601, preemption_count=0, score=26931.549178, test/accuracy=0.482000, test/loss=2.368090, test/num_examples=10000, total_duration=29770.996066, train/accuracy=0.638164, train/loss=1.468483, validation/accuracy=0.595640, validation/loss=1.700319, validation/num_examples=50000
I0203 19:35:10.315763 139702543816448 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.033040165901184, loss=5.174203872680664
I0203 19:35:56.563533 139702527031040 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.4211528301239014, loss=2.5533194541931152
I0203 19:36:42.911778 139702543816448 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.9354575276374817, loss=4.460402488708496
I0203 19:37:28.977059 139702527031040 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.0653315782546997, loss=4.254371643066406
I0203 19:38:15.446564 139702543816448 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.0292595624923706, loss=4.6337666511535645
I0203 19:39:01.622073 139702527031040 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.0867570638656616, loss=2.7877840995788574
I0203 19:39:47.957438 139702543816448 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.2935850620269775, loss=2.618027925491333
I0203 19:40:34.369075 139702527031040 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.3438280820846558, loss=2.581077814102173
I0203 19:41:20.419821 139702543816448 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.094070553779602, loss=4.584988117218018
I0203 19:41:28.385555 139863983413056 spec.py:321] Evaluating on the training split.
I0203 19:41:38.641414 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 19:42:10.507870 139863983413056 spec.py:349] Evaluating on the test split.
I0203 19:42:12.140919 139863983413056 submission_runner.py:408] Time since start: 30235.02s, 	Step: 59519, 	{'train/accuracy': 0.6510156393051147, 'train/loss': 1.446966528892517, 'validation/accuracy': 0.5954599976539612, 'validation/loss': 1.7019143104553223, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.367074728012085, 'test/num_examples': 10000, 'score': 27351.7313952446, 'total_duration': 30235.018276691437, 'accumulated_submission_time': 27351.7313952446, 'accumulated_eval_time': 2877.9978761672974, 'accumulated_logging_time': 2.1419429779052734}
I0203 19:42:12.166283 139702527031040 logging_writer.py:48] [59519] accumulated_eval_time=2877.997876, accumulated_logging_time=2.141943, accumulated_submission_time=27351.731395, global_step=59519, preemption_count=0, score=27351.731395, test/accuracy=0.481200, test/loss=2.367075, test/num_examples=10000, total_duration=30235.018277, train/accuracy=0.651016, train/loss=1.446967, validation/accuracy=0.595460, validation/loss=1.701914, validation/num_examples=50000
I0203 19:42:46.372969 139702543816448 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.2230736017227173, loss=2.5723214149475098
I0203 19:43:32.319425 139702527031040 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.2904070615768433, loss=3.074059009552002
I0203 19:44:18.647870 139702543816448 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.1068607568740845, loss=4.686554908752441
I0203 19:45:04.844305 139702527031040 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.3115159273147583, loss=2.5144734382629395
I0203 19:45:50.899097 139702543816448 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.3377629518508911, loss=3.033665895462036
I0203 19:46:37.440136 139702527031040 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.0233094692230225, loss=3.5454602241516113
I0203 19:47:23.576596 139702543816448 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.080154299736023, loss=3.2323737144470215
I0203 19:48:09.923835 139702527031040 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.304214596748352, loss=2.4490857124328613
I0203 19:48:56.126023 139702543816448 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.1399931907653809, loss=3.529855728149414
I0203 19:49:12.590973 139863983413056 spec.py:321] Evaluating on the training split.
I0203 19:49:22.994251 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 19:49:57.418802 139863983413056 spec.py:349] Evaluating on the test split.
I0203 19:49:59.058301 139863983413056 submission_runner.py:408] Time since start: 30701.94s, 	Step: 60437, 	{'train/accuracy': 0.6662304401397705, 'train/loss': 1.356255054473877, 'validation/accuracy': 0.5960400104522705, 'validation/loss': 1.6892080307006836, 'validation/num_examples': 50000, 'test/accuracy': 0.47950002551078796, 'test/loss': 2.367875814437866, 'test/num_examples': 10000, 'score': 27772.097522735596, 'total_duration': 30701.9356341362, 'accumulated_submission_time': 27772.097522735596, 'accumulated_eval_time': 2924.465174674988, 'accumulated_logging_time': 2.1778712272644043}
I0203 19:49:59.090353 139702527031040 logging_writer.py:48] [60437] accumulated_eval_time=2924.465175, accumulated_logging_time=2.177871, accumulated_submission_time=27772.097523, global_step=60437, preemption_count=0, score=27772.097523, test/accuracy=0.479500, test/loss=2.367876, test/num_examples=10000, total_duration=30701.935634, train/accuracy=0.666230, train/loss=1.356255, validation/accuracy=0.596040, validation/loss=1.689208, validation/num_examples=50000
I0203 19:50:25.771164 139702543816448 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.3545299768447876, loss=2.63805890083313
I0203 19:51:10.639715 139702527031040 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.0493308305740356, loss=4.7431864738464355
I0203 19:51:56.855423 139702543816448 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.32382071018219, loss=2.5557339191436768
I0203 19:52:42.924208 139702527031040 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.0706703662872314, loss=4.148717403411865
I0203 19:53:29.032851 139702543816448 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.2242316007614136, loss=2.942929983139038
I0203 19:54:15.082995 139702527031040 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.3688933849334717, loss=2.589644432067871
I0203 19:55:01.238765 139702543816448 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2993830442428589, loss=2.6863479614257812
I0203 19:55:47.197351 139702527031040 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.1588033437728882, loss=2.495283842086792
I0203 19:56:33.793850 139702543816448 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.1301409006118774, loss=3.0129923820495605
I0203 19:56:59.236221 139863983413056 spec.py:321] Evaluating on the training split.
I0203 19:57:09.812769 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 19:57:37.110622 139863983413056 spec.py:349] Evaluating on the test split.
I0203 19:57:38.755911 139863983413056 submission_runner.py:408] Time since start: 31161.63s, 	Step: 61357, 	{'train/accuracy': 0.6473046541213989, 'train/loss': 1.4790903329849243, 'validation/accuracy': 0.5974400043487549, 'validation/loss': 1.7262574434280396, 'validation/num_examples': 50000, 'test/accuracy': 0.47770002484321594, 'test/loss': 2.391491651535034, 'test/num_examples': 10000, 'score': 28192.18399953842, 'total_duration': 31161.633261203766, 'accumulated_submission_time': 28192.18399953842, 'accumulated_eval_time': 2963.984852075577, 'accumulated_logging_time': 2.220782995223999}
I0203 19:57:38.780805 139702527031040 logging_writer.py:48] [61357] accumulated_eval_time=2963.984852, accumulated_logging_time=2.220783, accumulated_submission_time=28192.184000, global_step=61357, preemption_count=0, score=28192.184000, test/accuracy=0.477700, test/loss=2.391492, test/num_examples=10000, total_duration=31161.633261, train/accuracy=0.647305, train/loss=1.479090, validation/accuracy=0.597440, validation/loss=1.726257, validation/num_examples=50000
I0203 19:57:57.134290 139702543816448 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.1821447610855103, loss=2.694269895553589
I0203 19:58:41.200502 139702527031040 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.0238418579101562, loss=4.35984992980957
I0203 19:59:27.742106 139702543816448 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.245158314704895, loss=2.556145191192627
I0203 20:00:14.253778 139702527031040 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.3638535737991333, loss=2.5657546520233154
I0203 20:01:00.765562 139702543816448 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.1600333452224731, loss=2.7141361236572266
I0203 20:01:47.166436 139702527031040 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.095357894897461, loss=3.5980043411254883
I0203 20:02:33.407941 139702543816448 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.405526876449585, loss=2.5929365158081055
I0203 20:03:19.561239 139702527031040 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.2060588598251343, loss=3.184488296508789
I0203 20:04:05.603090 139702543816448 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.256372332572937, loss=2.599174737930298
I0203 20:04:39.171796 139863983413056 spec.py:321] Evaluating on the training split.
I0203 20:04:49.575210 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 20:05:23.199322 139863983413056 spec.py:349] Evaluating on the test split.
I0203 20:05:24.833292 139863983413056 submission_runner.py:408] Time since start: 31627.71s, 	Step: 62274, 	{'train/accuracy': 0.6483398079872131, 'train/loss': 1.4805115461349487, 'validation/accuracy': 0.5988399982452393, 'validation/loss': 1.7147594690322876, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.3593976497650146, 'test/num_examples': 10000, 'score': 28612.51757788658, 'total_duration': 31627.710637569427, 'accumulated_submission_time': 28612.51757788658, 'accumulated_eval_time': 3009.6463346481323, 'accumulated_logging_time': 2.254441261291504}
I0203 20:05:24.865346 139702527031040 logging_writer.py:48] [62274] accumulated_eval_time=3009.646335, accumulated_logging_time=2.254441, accumulated_submission_time=28612.517578, global_step=62274, preemption_count=0, score=28612.517578, test/accuracy=0.484500, test/loss=2.359398, test/num_examples=10000, total_duration=31627.710638, train/accuracy=0.648340, train/loss=1.480512, validation/accuracy=0.598840, validation/loss=1.714759, validation/num_examples=50000
I0203 20:05:36.112658 139702543816448 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.3645219802856445, loss=2.472399950027466
I0203 20:06:19.810124 139702527031040 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.1858959197998047, loss=2.6281871795654297
I0203 20:07:05.800360 139702543816448 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.063152551651001, loss=4.959046363830566
I0203 20:07:52.410614 139702527031040 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.1277234554290771, loss=2.922192335128784
I0203 20:08:38.561643 139702543816448 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.1629459857940674, loss=2.821089267730713
I0203 20:09:24.853847 139702527031040 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.9917361736297607, loss=4.005134105682373
I0203 20:10:11.341026 139702543816448 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.257390022277832, loss=2.595766067504883
I0203 20:10:57.512517 139702527031040 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.1681585311889648, loss=3.700221538543701
I0203 20:11:43.732685 139702543816448 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.3023982048034668, loss=2.3546676635742188
I0203 20:12:25.152918 139863983413056 spec.py:321] Evaluating on the training split.
I0203 20:12:35.564103 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 20:13:06.179821 139863983413056 spec.py:349] Evaluating on the test split.
I0203 20:13:07.829109 139863983413056 submission_runner.py:408] Time since start: 32090.71s, 	Step: 63191, 	{'train/accuracy': 0.6765429377555847, 'train/loss': 1.3175029754638672, 'validation/accuracy': 0.6112399697303772, 'validation/loss': 1.6248478889465332, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.2709903717041016, 'test/num_examples': 10000, 'score': 29032.745688199997, 'total_duration': 32090.70645928383, 'accumulated_submission_time': 29032.745688199997, 'accumulated_eval_time': 3052.322532892227, 'accumulated_logging_time': 2.297640323638916}
I0203 20:13:07.855823 139702527031040 logging_writer.py:48] [63191] accumulated_eval_time=3052.322533, accumulated_logging_time=2.297640, accumulated_submission_time=29032.745688, global_step=63191, preemption_count=0, score=29032.745688, test/accuracy=0.490800, test/loss=2.270990, test/num_examples=10000, total_duration=32090.706459, train/accuracy=0.676543, train/loss=1.317503, validation/accuracy=0.611240, validation/loss=1.624848, validation/num_examples=50000
I0203 20:13:12.032713 139702543816448 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.035501480102539, loss=5.19235897064209
I0203 20:13:54.748380 139702527031040 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.128643274307251, loss=5.007324695587158
I0203 20:14:40.724697 139702543816448 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.1697651147842407, loss=2.8264341354370117
I0203 20:15:27.055224 139702527031040 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.2161296606063843, loss=2.7554407119750977
I0203 20:16:13.135078 139702543816448 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.269533395767212, loss=3.0213677883148193
I0203 20:16:59.264995 139702527031040 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.1630785465240479, loss=2.7302286624908447
I0203 20:17:45.907665 139702543816448 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.2476242780685425, loss=2.50791335105896
I0203 20:18:32.186603 139702527031040 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.106807827949524, loss=3.151754140853882
I0203 20:19:18.375596 139702543816448 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.9434832334518433, loss=4.8916449546813965
I0203 20:20:05.074570 139702527031040 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.0800914764404297, loss=5.105586528778076
I0203 20:20:07.932647 139863983413056 spec.py:321] Evaluating on the training split.
I0203 20:20:18.211394 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 20:20:50.268825 139863983413056 spec.py:349] Evaluating on the test split.
I0203 20:20:51.917894 139863983413056 submission_runner.py:408] Time since start: 32554.80s, 	Step: 64108, 	{'train/accuracy': 0.6566015481948853, 'train/loss': 1.3950915336608887, 'validation/accuracy': 0.6128999590873718, 'validation/loss': 1.6186541318893433, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.2900989055633545, 'test/num_examples': 10000, 'score': 29452.76300573349, 'total_duration': 32554.79523062706, 'accumulated_submission_time': 29452.76300573349, 'accumulated_eval_time': 3096.3077614307404, 'accumulated_logging_time': 2.3357009887695312}
I0203 20:20:51.948476 139702543816448 logging_writer.py:48] [64108] accumulated_eval_time=3096.307761, accumulated_logging_time=2.335701, accumulated_submission_time=29452.763006, global_step=64108, preemption_count=0, score=29452.763006, test/accuracy=0.491400, test/loss=2.290099, test/num_examples=10000, total_duration=32554.795231, train/accuracy=0.656602, train/loss=1.395092, validation/accuracy=0.612900, validation/loss=1.618654, validation/num_examples=50000
I0203 20:21:31.105915 139702527031040 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.9829950332641602, loss=3.788590669631958
I0203 20:22:17.199294 139702543816448 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.2050420045852661, loss=4.805357456207275
I0203 20:23:03.886397 139702527031040 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.288387656211853, loss=2.5892457962036133
I0203 20:23:50.015104 139702543816448 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.3505349159240723, loss=2.5047731399536133
I0203 20:24:36.339780 139702527031040 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.3699396848678589, loss=2.7312169075012207
I0203 20:25:22.783762 139702543816448 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.3176618814468384, loss=2.556610107421875
I0203 20:26:09.178768 139702527031040 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.2032824754714966, loss=2.5104076862335205
I0203 20:26:55.602945 139702543816448 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.1947523355484009, loss=5.091878890991211
I0203 20:27:42.126759 139702527031040 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.2308050394058228, loss=5.102867126464844
I0203 20:27:52.352604 139863983413056 spec.py:321] Evaluating on the training split.
I0203 20:28:02.821700 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 20:28:36.134856 139863983413056 spec.py:349] Evaluating on the test split.
I0203 20:28:37.772128 139863983413056 submission_runner.py:408] Time since start: 33020.65s, 	Step: 65024, 	{'train/accuracy': 0.6565819978713989, 'train/loss': 1.4120858907699585, 'validation/accuracy': 0.6054199934005737, 'validation/loss': 1.6623183488845825, 'validation/num_examples': 50000, 'test/accuracy': 0.48760002851486206, 'test/loss': 2.3212289810180664, 'test/num_examples': 10000, 'score': 29873.10894012451, 'total_duration': 33020.64948439598, 'accumulated_submission_time': 29873.10894012451, 'accumulated_eval_time': 3141.72727894783, 'accumulated_logging_time': 2.376920223236084}
I0203 20:28:37.798155 139702543816448 logging_writer.py:48] [65024] accumulated_eval_time=3141.727279, accumulated_logging_time=2.376920, accumulated_submission_time=29873.108940, global_step=65024, preemption_count=0, score=29873.108940, test/accuracy=0.487600, test/loss=2.321229, test/num_examples=10000, total_duration=33020.649484, train/accuracy=0.656582, train/loss=1.412086, validation/accuracy=0.605420, validation/loss=1.662318, validation/num_examples=50000
I0203 20:29:10.104153 139702527031040 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.1658746004104614, loss=5.061338424682617
I0203 20:29:56.248285 139702543816448 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.1425402164459229, loss=5.185450553894043
I0203 20:30:42.441192 139702527031040 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.3021825551986694, loss=2.616333246231079
I0203 20:31:28.541129 139702543816448 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.251142978668213, loss=2.425239324569702
I0203 20:32:14.747670 139702527031040 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.3090990781784058, loss=2.5228991508483887
I0203 20:33:00.860392 139702543816448 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.9750710725784302, loss=5.065138816833496
I0203 20:33:47.247725 139702527031040 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.2036594152450562, loss=2.3548154830932617
I0203 20:34:33.564862 139702543816448 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.0427772998809814, loss=3.474867343902588
I0203 20:35:19.597946 139702527031040 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.9431044459342957, loss=5.0272369384765625
I0203 20:35:38.129793 139863983413056 spec.py:321] Evaluating on the training split.
I0203 20:35:48.468297 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 20:36:21.838991 139863983413056 spec.py:349] Evaluating on the test split.
I0203 20:36:23.480018 139863983413056 submission_runner.py:408] Time since start: 33486.36s, 	Step: 65942, 	{'train/accuracy': 0.6722265481948853, 'train/loss': 1.4081395864486694, 'validation/accuracy': 0.6116799712181091, 'validation/loss': 1.6795889139175415, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.3212332725524902, 'test/num_examples': 10000, 'score': 30293.38259911537, 'total_duration': 33486.357377290726, 'accumulated_submission_time': 30293.38259911537, 'accumulated_eval_time': 3187.0775051116943, 'accumulated_logging_time': 2.4117984771728516}
I0203 20:36:23.509835 139702543816448 logging_writer.py:48] [65942] accumulated_eval_time=3187.077505, accumulated_logging_time=2.411798, accumulated_submission_time=30293.382599, global_step=65942, preemption_count=0, score=30293.382599, test/accuracy=0.487900, test/loss=2.321233, test/num_examples=10000, total_duration=33486.357377, train/accuracy=0.672227, train/loss=1.408140, validation/accuracy=0.611680, validation/loss=1.679589, validation/num_examples=50000
I0203 20:36:48.128845 139702527031040 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.3279402256011963, loss=2.419433355331421
I0203 20:37:33.005254 139702543816448 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.3919185400009155, loss=2.52518367767334
I0203 20:38:19.136686 139702527031040 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.2031092643737793, loss=2.9571404457092285
I0203 20:39:05.800691 139702543816448 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3390083312988281, loss=2.952514410018921
I0203 20:39:52.017966 139702527031040 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.2680038213729858, loss=3.337202787399292
I0203 20:40:38.092409 139702543816448 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.045367956161499, loss=4.875054359436035
I0203 20:41:24.231106 139702527031040 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.4275681972503662, loss=2.4592416286468506
I0203 20:42:10.577107 139702543816448 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.6481175422668457, loss=2.5586600303649902
I0203 20:42:56.723833 139702527031040 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.3870407342910767, loss=2.419846296310425
I0203 20:43:23.612615 139863983413056 spec.py:321] Evaluating on the training split.
I0203 20:43:34.135286 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 20:44:05.754428 139863983413056 spec.py:349] Evaluating on the test split.
I0203 20:44:07.390631 139863983413056 submission_runner.py:408] Time since start: 33950.27s, 	Step: 66860, 	{'train/accuracy': 0.6582812070846558, 'train/loss': 1.3994724750518799, 'validation/accuracy': 0.6097399592399597, 'validation/loss': 1.6316499710083008, 'validation/num_examples': 50000, 'test/accuracy': 0.4897000193595886, 'test/loss': 2.2837886810302734, 'test/num_examples': 10000, 'score': 30713.42664384842, 'total_duration': 33950.26797604561, 'accumulated_submission_time': 30713.42664384842, 'accumulated_eval_time': 3230.855504989624, 'accumulated_logging_time': 2.4521572589874268}
I0203 20:44:07.419942 139702543816448 logging_writer.py:48] [66860] accumulated_eval_time=3230.855505, accumulated_logging_time=2.452157, accumulated_submission_time=30713.426644, global_step=66860, preemption_count=0, score=30713.426644, test/accuracy=0.489700, test/loss=2.283789, test/num_examples=10000, total_duration=33950.267976, train/accuracy=0.658281, train/loss=1.399472, validation/accuracy=0.609740, validation/loss=1.631650, validation/num_examples=50000
I0203 20:44:24.526556 139702527031040 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.0797128677368164, loss=4.741785526275635
I0203 20:45:08.692967 139702543816448 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.1265023946762085, loss=5.093542575836182
I0203 20:45:54.787858 139702527031040 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.9830705523490906, loss=3.9899699687957764
I0203 20:46:41.029472 139702543816448 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.2388994693756104, loss=5.197551727294922
I0203 20:47:27.319772 139702527031040 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.3090516328811646, loss=2.4890458583831787
I0203 20:48:13.541388 139702543816448 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.1996214389801025, loss=3.035898208618164
I0203 20:48:59.661799 139702527031040 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.1647309064865112, loss=3.5854032039642334
I0203 20:49:46.289765 139702543816448 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.1138982772827148, loss=3.663656234741211
I0203 20:50:32.470777 139702527031040 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.0806691646575928, loss=3.8398683071136475
I0203 20:51:07.796047 139863983413056 spec.py:321] Evaluating on the training split.
I0203 20:51:18.003161 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 20:51:50.793715 139863983413056 spec.py:349] Evaluating on the test split.
I0203 20:51:52.437952 139863983413056 submission_runner.py:408] Time since start: 34415.32s, 	Step: 67778, 	{'train/accuracy': 0.6619336009025574, 'train/loss': 1.3762539625167847, 'validation/accuracy': 0.6098399758338928, 'validation/loss': 1.6209572553634644, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.292510986328125, 'test/num_examples': 10000, 'score': 31133.74489402771, 'total_duration': 34415.31527900696, 'accumulated_submission_time': 31133.74489402771, 'accumulated_eval_time': 3275.497382879257, 'accumulated_logging_time': 2.491576910018921}
I0203 20:51:52.471037 139702543816448 logging_writer.py:48] [67778] accumulated_eval_time=3275.497383, accumulated_logging_time=2.491577, accumulated_submission_time=31133.744894, global_step=67778, preemption_count=0, score=31133.744894, test/accuracy=0.489100, test/loss=2.292511, test/num_examples=10000, total_duration=34415.315279, train/accuracy=0.661934, train/loss=1.376254, validation/accuracy=0.609840, validation/loss=1.620957, validation/num_examples=50000
I0203 20:52:02.055876 139702527031040 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.339437484741211, loss=2.5192954540252686
I0203 20:52:45.266054 139702543816448 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.2230103015899658, loss=2.562965154647827
I0203 20:53:31.401004 139702527031040 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.2209281921386719, loss=3.4225611686706543
I0203 20:54:17.694895 139702543816448 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.2573186159133911, loss=4.939247131347656
I0203 20:55:03.869631 139702527031040 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.3750990629196167, loss=2.4093286991119385
I0203 20:55:49.965888 139702543816448 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.3059972524642944, loss=2.7660164833068848
I0203 20:56:36.151468 139702527031040 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.0525603294372559, loss=3.6352181434631348
I0203 20:57:22.411351 139702543816448 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.975264310836792, loss=4.802549362182617
I0203 20:58:08.636307 139702527031040 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.3152531385421753, loss=2.551767349243164
I0203 20:58:52.568254 139863983413056 spec.py:321] Evaluating on the training split.
I0203 20:59:02.963111 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 20:59:35.369186 139863983413056 spec.py:349] Evaluating on the test split.
I0203 20:59:37.011001 139863983413056 submission_runner.py:408] Time since start: 34879.89s, 	Step: 68697, 	{'train/accuracy': 0.6661718487739563, 'train/loss': 1.3747303485870361, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.637009620666504, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.307218551635742, 'test/num_examples': 10000, 'score': 31553.782564401627, 'total_duration': 34879.88833665848, 'accumulated_submission_time': 31553.782564401627, 'accumulated_eval_time': 3319.9401018619537, 'accumulated_logging_time': 2.5363433361053467}
I0203 20:59:37.042486 139702543816448 logging_writer.py:48] [68697] accumulated_eval_time=3319.940102, accumulated_logging_time=2.536343, accumulated_submission_time=31553.782564, global_step=68697, preemption_count=0, score=31553.782564, test/accuracy=0.490300, test/loss=2.307219, test/num_examples=10000, total_duration=34879.888337, train/accuracy=0.666172, train/loss=1.374730, validation/accuracy=0.610960, validation/loss=1.637010, validation/num_examples=50000
I0203 20:59:38.709817 139702527031040 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.1891992092132568, loss=5.0117340087890625
I0203 21:00:21.372068 139702543816448 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2550759315490723, loss=2.3441829681396484
I0203 21:01:07.352155 139702527031040 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.2921792268753052, loss=2.429182529449463
I0203 21:01:53.638803 139702543816448 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.487696886062622, loss=2.4442102909088135
I0203 21:02:40.028098 139702527031040 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.040420651435852, loss=3.277240514755249
I0203 21:03:26.419502 139702543816448 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.2471448183059692, loss=2.476325035095215
I0203 21:04:12.702685 139702527031040 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.0133287906646729, loss=4.546597480773926
I0203 21:04:58.947151 139702543816448 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.0357667207717896, loss=5.038846492767334
I0203 21:05:45.193850 139702527031040 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.2455791234970093, loss=2.431103467941284
I0203 21:06:31.381828 139702543816448 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.2761820554733276, loss=2.3259334564208984
I0203 21:06:37.471050 139863983413056 spec.py:321] Evaluating on the training split.
I0203 21:06:47.819850 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 21:07:15.310515 139863983413056 spec.py:349] Evaluating on the test split.
I0203 21:07:16.950716 139863983413056 submission_runner.py:408] Time since start: 35339.83s, 	Step: 69615, 	{'train/accuracy': 0.6862890720367432, 'train/loss': 1.303639531135559, 'validation/accuracy': 0.6137999892234802, 'validation/loss': 1.6325457096099854, 'validation/num_examples': 50000, 'test/accuracy': 0.49540001153945923, 'test/loss': 2.2856035232543945, 'test/num_examples': 10000, 'score': 31974.151923894882, 'total_duration': 35339.82807254791, 'accumulated_submission_time': 31974.151923894882, 'accumulated_eval_time': 3359.4197540283203, 'accumulated_logging_time': 2.5785627365112305}
I0203 21:07:16.986680 139702527031040 logging_writer.py:48] [69615] accumulated_eval_time=3359.419754, accumulated_logging_time=2.578563, accumulated_submission_time=31974.151924, global_step=69615, preemption_count=0, score=31974.151924, test/accuracy=0.495400, test/loss=2.285604, test/num_examples=10000, total_duration=35339.828073, train/accuracy=0.686289, train/loss=1.303640, validation/accuracy=0.613800, validation/loss=1.632546, validation/num_examples=50000
I0203 21:07:52.922849 139702543816448 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.2922637462615967, loss=2.282280683517456
I0203 21:08:39.032902 139702527031040 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.020195484161377, loss=4.3959197998046875
I0203 21:09:25.427814 139702543816448 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.0590834617614746, loss=4.661017417907715
I0203 21:10:11.698310 139702527031040 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.4113407135009766, loss=2.5923163890838623
I0203 21:10:58.053338 139702543816448 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.0427615642547607, loss=3.2872443199157715
I0203 21:11:44.258197 139702527031040 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.3723835945129395, loss=2.9068775177001953
I0203 21:12:30.609174 139702543816448 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.3663172721862793, loss=2.421081304550171
I0203 21:13:16.888868 139702527031040 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.322540283203125, loss=2.496091842651367
I0203 21:14:02.947401 139702543816448 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.2492517232894897, loss=2.359612464904785
I0203 21:14:17.037201 139863983413056 spec.py:321] Evaluating on the training split.
I0203 21:14:27.472347 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 21:15:00.951629 139863983413056 spec.py:349] Evaluating on the test split.
I0203 21:15:02.584881 139863983413056 submission_runner.py:408] Time since start: 35805.46s, 	Step: 70532, 	{'train/accuracy': 0.6647265553474426, 'train/loss': 1.3673946857452393, 'validation/accuracy': 0.618619978427887, 'validation/loss': 1.584282636642456, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.240478515625, 'test/num_examples': 10000, 'score': 32394.145018339157, 'total_duration': 35805.46223473549, 'accumulated_submission_time': 32394.145018339157, 'accumulated_eval_time': 3404.9674224853516, 'accumulated_logging_time': 2.6233327388763428}
I0203 21:15:02.614568 139702527031040 logging_writer.py:48] [70532] accumulated_eval_time=3404.967422, accumulated_logging_time=2.623333, accumulated_submission_time=32394.145018, global_step=70532, preemption_count=0, score=32394.145018, test/accuracy=0.501400, test/loss=2.240479, test/num_examples=10000, total_duration=35805.462235, train/accuracy=0.664727, train/loss=1.367395, validation/accuracy=0.618620, validation/loss=1.584283, validation/num_examples=50000
I0203 21:15:31.392540 139702543816448 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.3903876543045044, loss=2.4831855297088623
I0203 21:16:16.690312 139702527031040 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.2409900426864624, loss=3.089566469192505
I0203 21:17:03.140196 139702543816448 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.1766592264175415, loss=3.687613010406494
I0203 21:17:49.206235 139702527031040 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.9933404326438904, loss=4.218915939331055
I0203 21:18:35.437108 139702543816448 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.1080385446548462, loss=4.836065292358398
I0203 21:19:21.692931 139702527031040 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.0700387954711914, loss=4.0574493408203125
I0203 21:20:07.922695 139702543816448 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.3029041290283203, loss=2.5117383003234863
I0203 21:20:54.368800 139702527031040 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.3088995218276978, loss=2.9165804386138916
I0203 21:21:40.774640 139702543816448 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.151494026184082, loss=4.4972710609436035
I0203 21:22:02.828933 139863983413056 spec.py:321] Evaluating on the training split.
I0203 21:22:13.182502 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 21:22:47.841328 139863983413056 spec.py:349] Evaluating on the test split.
I0203 21:22:49.497583 139863983413056 submission_runner.py:408] Time since start: 36272.37s, 	Step: 71449, 	{'train/accuracy': 0.6814843416213989, 'train/loss': 1.3013520240783691, 'validation/accuracy': 0.6236599683761597, 'validation/loss': 1.567617654800415, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.2308449745178223, 'test/num_examples': 10000, 'score': 32814.301644325256, 'total_duration': 36272.37492394447, 'accumulated_submission_time': 32814.301644325256, 'accumulated_eval_time': 3451.6360535621643, 'accumulated_logging_time': 2.6626389026641846}
I0203 21:22:49.529623 139702527031040 logging_writer.py:48] [71449] accumulated_eval_time=3451.636054, accumulated_logging_time=2.662639, accumulated_submission_time=32814.301644, global_step=71449, preemption_count=0, score=32814.301644, test/accuracy=0.498400, test/loss=2.230845, test/num_examples=10000, total_duration=36272.374924, train/accuracy=0.681484, train/loss=1.301352, validation/accuracy=0.623660, validation/loss=1.567618, validation/num_examples=50000
I0203 21:23:11.216887 139702543816448 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.3290650844573975, loss=2.5514957904815674
I0203 21:23:55.712762 139702527031040 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.4543120861053467, loss=2.447692394256592
I0203 21:24:41.948031 139702543816448 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1954697370529175, loss=2.9890546798706055
I0203 21:25:28.426000 139702527031040 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.2115532159805298, loss=5.074473857879639
I0203 21:26:14.366612 139702543816448 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.3901677131652832, loss=2.3948819637298584
I0203 21:27:00.472678 139702527031040 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.185549020767212, loss=4.758999347686768
I0203 21:27:46.640349 139702543816448 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.2311102151870728, loss=2.7790510654449463
I0203 21:28:32.840931 139702527031040 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.2970730066299438, loss=2.7484805583953857
I0203 21:29:19.138603 139702543816448 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.0159821510314941, loss=4.737391471862793
I0203 21:29:49.895958 139863983413056 spec.py:321] Evaluating on the training split.
I0203 21:30:00.306094 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 21:30:35.103669 139863983413056 spec.py:349] Evaluating on the test split.
I0203 21:30:36.749281 139863983413056 submission_runner.py:408] Time since start: 36739.63s, 	Step: 72368, 	{'train/accuracy': 0.6871093511581421, 'train/loss': 1.261122465133667, 'validation/accuracy': 0.6153599619865417, 'validation/loss': 1.602860927581787, 'validation/num_examples': 50000, 'test/accuracy': 0.49630001187324524, 'test/loss': 2.2533340454101562, 'test/num_examples': 10000, 'score': 33234.60793232918, 'total_duration': 36739.626620054245, 'accumulated_submission_time': 33234.60793232918, 'accumulated_eval_time': 3498.4893431663513, 'accumulated_logging_time': 2.706322431564331}
I0203 21:30:36.784313 139702527031040 logging_writer.py:48] [72368] accumulated_eval_time=3498.489343, accumulated_logging_time=2.706322, accumulated_submission_time=33234.607932, global_step=72368, preemption_count=0, score=33234.607932, test/accuracy=0.496300, test/loss=2.253334, test/num_examples=10000, total_duration=36739.626620, train/accuracy=0.687109, train/loss=1.261122, validation/accuracy=0.615360, validation/loss=1.602861, validation/num_examples=50000
I0203 21:30:50.539061 139702543816448 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.3253639936447144, loss=2.405773639678955
I0203 21:31:34.194079 139702527031040 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2697786092758179, loss=3.5121750831604004
I0203 21:32:20.583232 139702543816448 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.151324987411499, loss=4.895493507385254
I0203 21:33:06.812018 139702527031040 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.321307897567749, loss=4.612618446350098
I0203 21:33:53.116821 139702543816448 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.2743510007858276, loss=4.846816062927246
I0203 21:34:39.094968 139702527031040 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.3826110363006592, loss=2.470470428466797
I0203 21:35:25.345631 139702543816448 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.118310809135437, loss=4.609790325164795
I0203 21:36:11.418112 139702527031040 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.2721858024597168, loss=2.4401118755340576
I0203 21:36:57.605458 139702543816448 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.3763271570205688, loss=2.5658607482910156
I0203 21:37:37.238137 139863983413056 spec.py:321] Evaluating on the training split.
I0203 21:37:47.695516 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 21:38:15.634319 139863983413056 spec.py:349] Evaluating on the test split.
I0203 21:38:17.274266 139863983413056 submission_runner.py:408] Time since start: 37200.15s, 	Step: 73287, 	{'train/accuracy': 0.6668750047683716, 'train/loss': 1.3529757261276245, 'validation/accuracy': 0.6220799684524536, 'validation/loss': 1.573674201965332, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.2361388206481934, 'test/num_examples': 10000, 'score': 33654.99967765808, 'total_duration': 37200.15159320831, 'accumulated_submission_time': 33654.99967765808, 'accumulated_eval_time': 3538.5254430770874, 'accumulated_logging_time': 2.7534587383270264}
I0203 21:38:17.302304 139702527031040 logging_writer.py:48] [73287] accumulated_eval_time=3538.525443, accumulated_logging_time=2.753459, accumulated_submission_time=33654.999678, global_step=73287, preemption_count=0, score=33654.999678, test/accuracy=0.501000, test/loss=2.236139, test/num_examples=10000, total_duration=37200.151593, train/accuracy=0.666875, train/loss=1.352976, validation/accuracy=0.622080, validation/loss=1.573674, validation/num_examples=50000
I0203 21:38:23.140750 139702543816448 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.3537174463272095, loss=2.572242021560669
I0203 21:39:06.057317 139702527031040 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.1617647409439087, loss=4.234084129333496
I0203 21:39:52.346024 139702543816448 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.3873779773712158, loss=2.4556732177734375
I0203 21:40:38.773626 139702527031040 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.133213996887207, loss=4.487252235412598
I0203 21:41:24.950062 139702543816448 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.1909810304641724, loss=3.609220266342163
I0203 21:42:11.259578 139702527031040 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.361348032951355, loss=2.4957520961761475
I0203 21:42:57.490002 139702543816448 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.3810099363327026, loss=2.3710920810699463
I0203 21:43:43.801035 139702527031040 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.2460107803344727, loss=2.9352307319641113
I0203 21:44:30.130244 139702543816448 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.3592630624771118, loss=2.4501495361328125
I0203 21:45:16.410936 139702527031040 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.2961796522140503, loss=2.4328644275665283
I0203 21:45:17.509742 139863983413056 spec.py:321] Evaluating on the training split.
I0203 21:45:27.795452 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 21:45:59.470459 139863983413056 spec.py:349] Evaluating on the test split.
I0203 21:46:01.116875 139863983413056 submission_runner.py:408] Time since start: 37663.99s, 	Step: 74204, 	{'train/accuracy': 0.6692773103713989, 'train/loss': 1.3801852464675903, 'validation/accuracy': 0.6154199838638306, 'validation/loss': 1.623300552368164, 'validation/num_examples': 50000, 'test/accuracy': 0.49230003356933594, 'test/loss': 2.295822858810425, 'test/num_examples': 10000, 'score': 34075.14587640762, 'total_duration': 37663.99420571327, 'accumulated_submission_time': 34075.14587640762, 'accumulated_eval_time': 3582.1325442790985, 'accumulated_logging_time': 2.7938594818115234}
I0203 21:46:01.150061 139702543816448 logging_writer.py:48] [74204] accumulated_eval_time=3582.132544, accumulated_logging_time=2.793859, accumulated_submission_time=34075.145876, global_step=74204, preemption_count=0, score=34075.145876, test/accuracy=0.492300, test/loss=2.295823, test/num_examples=10000, total_duration=37663.994206, train/accuracy=0.669277, train/loss=1.380185, validation/accuracy=0.615420, validation/loss=1.623301, validation/num_examples=50000
I0203 21:46:42.107331 139702527031040 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.1393897533416748, loss=3.28721284866333
I0203 21:47:28.310906 139702543816448 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.4220842123031616, loss=2.3978893756866455
I0203 21:48:14.613864 139702527031040 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.1171048879623413, loss=5.06660795211792
I0203 21:49:00.676017 139702543816448 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2157453298568726, loss=2.7590107917785645
I0203 21:49:47.028824 139702527031040 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.9327755570411682, loss=5.123307228088379
I0203 21:50:33.302580 139702543816448 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.366169810295105, loss=2.4360666275024414
I0203 21:51:19.474827 139702527031040 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1244529485702515, loss=3.95375394821167
I0203 21:52:05.617089 139702543816448 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.3202348947525024, loss=3.2304344177246094
I0203 21:52:52.159205 139702527031040 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.3759695291519165, loss=2.4593923091888428
I0203 21:53:01.546803 139863983413056 spec.py:321] Evaluating on the training split.
I0203 21:53:12.057297 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 21:53:43.579195 139863983413056 spec.py:349] Evaluating on the test split.
I0203 21:53:45.212874 139863983413056 submission_runner.py:408] Time since start: 38128.09s, 	Step: 75122, 	{'train/accuracy': 0.6792187094688416, 'train/loss': 1.3302925825119019, 'validation/accuracy': 0.6165399551391602, 'validation/loss': 1.6309008598327637, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.308547258377075, 'test/num_examples': 10000, 'score': 34495.48349118233, 'total_duration': 38128.09023118019, 'accumulated_submission_time': 34495.48349118233, 'accumulated_eval_time': 3625.7986521720886, 'accumulated_logging_time': 2.837161064147949}
I0203 21:53:45.241046 139702543816448 logging_writer.py:48] [75122] accumulated_eval_time=3625.798652, accumulated_logging_time=2.837161, accumulated_submission_time=34495.483491, global_step=75122, preemption_count=0, score=34495.483491, test/accuracy=0.494900, test/loss=2.308547, test/num_examples=10000, total_duration=38128.090231, train/accuracy=0.679219, train/loss=1.330293, validation/accuracy=0.616540, validation/loss=1.630901, validation/num_examples=50000
I0203 21:54:18.195353 139702527031040 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.3397200107574463, loss=2.407942533493042
I0203 21:55:03.888414 139702543816448 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.4454342126846313, loss=2.342560052871704
I0203 21:55:50.402042 139702527031040 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.1548465490341187, loss=4.843367576599121
I0203 21:56:36.552497 139702543816448 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.7916392087936401, loss=2.414957046508789
I0203 21:57:22.782354 139702527031040 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.3657488822937012, loss=2.808866262435913
I0203 21:58:08.902842 139702543816448 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.3590211868286133, loss=2.3578944206237793
I0203 21:58:55.235106 139702527031040 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.1506547927856445, loss=2.9502475261688232
I0203 21:59:41.618762 139702543816448 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.5239953994750977, loss=2.3848440647125244
I0203 22:00:27.913782 139702527031040 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.3076032400131226, loss=2.3295629024505615
I0203 22:00:45.601004 139863983413056 spec.py:321] Evaluating on the training split.
I0203 22:00:56.057172 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 22:01:28.453881 139863983413056 spec.py:349] Evaluating on the test split.
I0203 22:01:30.087008 139863983413056 submission_runner.py:408] Time since start: 38592.96s, 	Step: 76040, 	{'train/accuracy': 0.668652355670929, 'train/loss': 1.3684196472167969, 'validation/accuracy': 0.6186800003051758, 'validation/loss': 1.6112457513809204, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.2662360668182373, 'test/num_examples': 10000, 'score': 34915.78348207474, 'total_duration': 38592.96436548233, 'accumulated_submission_time': 34915.78348207474, 'accumulated_eval_time': 3670.284652233124, 'accumulated_logging_time': 2.877194404602051}
I0203 22:01:30.117183 139702543816448 logging_writer.py:48] [76040] accumulated_eval_time=3670.284652, accumulated_logging_time=2.877194, accumulated_submission_time=34915.783482, global_step=76040, preemption_count=0, score=34915.783482, test/accuracy=0.498800, test/loss=2.266236, test/num_examples=10000, total_duration=38592.964365, train/accuracy=0.668652, train/loss=1.368420, validation/accuracy=0.618680, validation/loss=1.611246, validation/num_examples=50000
I0203 22:01:55.573789 139702527031040 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.2309695482254028, loss=2.570624351501465
I0203 22:02:40.349208 139702543816448 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.14442777633667, loss=4.8419365882873535
I0203 22:03:26.699000 139702527031040 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.3878618478775024, loss=2.5390214920043945
I0203 22:04:13.091781 139702543816448 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.477031946182251, loss=2.4385881423950195
I0203 22:04:59.194099 139702527031040 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.3277533054351807, loss=2.877931594848633
I0203 22:05:45.408344 139702543816448 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.408259630203247, loss=5.065242767333984
I0203 22:06:31.553164 139702527031040 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.3112008571624756, loss=2.5062265396118164
I0203 22:07:17.599287 139702543816448 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.2630295753479004, loss=3.133274555206299
I0203 22:08:03.882845 139702527031040 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.3783237934112549, loss=2.4185235500335693
I0203 22:08:30.288820 139863983413056 spec.py:321] Evaluating on the training split.
I0203 22:08:40.679460 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 22:09:13.188180 139863983413056 spec.py:349] Evaluating on the test split.
I0203 22:09:14.833230 139863983413056 submission_runner.py:408] Time since start: 39057.71s, 	Step: 76959, 	{'train/accuracy': 0.6804882884025574, 'train/loss': 1.2870023250579834, 'validation/accuracy': 0.6269800066947937, 'validation/loss': 1.5367377996444702, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.2034332752227783, 'test/num_examples': 10000, 'score': 35335.89672112465, 'total_duration': 39057.71058821678, 'accumulated_submission_time': 35335.89672112465, 'accumulated_eval_time': 3714.829068660736, 'accumulated_logging_time': 2.9171230792999268}
I0203 22:09:14.864525 139702543816448 logging_writer.py:48] [76959] accumulated_eval_time=3714.829069, accumulated_logging_time=2.917123, accumulated_submission_time=35335.896721, global_step=76959, preemption_count=0, score=35335.896721, test/accuracy=0.509600, test/loss=2.203433, test/num_examples=10000, total_duration=39057.710588, train/accuracy=0.680488, train/loss=1.287002, validation/accuracy=0.626980, validation/loss=1.536738, validation/num_examples=50000
I0203 22:09:32.384274 139702527031040 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.2594969272613525, loss=2.378876209259033
I0203 22:10:16.616448 139702543816448 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.2252615690231323, loss=2.766880989074707
I0203 22:11:02.749524 139702527031040 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.1713694334030151, loss=5.124359130859375
I0203 22:11:48.678867 139702543816448 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.0855660438537598, loss=3.78867506980896
I0203 22:12:34.757619 139702527031040 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.3503634929656982, loss=2.446200132369995
I0203 22:13:20.858144 139702543816448 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.1910558938980103, loss=3.61171817779541
I0203 22:14:07.004856 139702527031040 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.255781888961792, loss=2.4892897605895996
I0203 22:14:53.172159 139702543816448 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.3932796716690063, loss=2.647923231124878
I0203 22:15:39.328923 139702527031040 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.5541306734085083, loss=2.3721349239349365
I0203 22:16:14.974983 139863983413056 spec.py:321] Evaluating on the training split.
I0203 22:16:25.512907 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 22:16:56.410680 139863983413056 spec.py:349] Evaluating on the test split.
I0203 22:16:58.044333 139863983413056 submission_runner.py:408] Time since start: 39520.92s, 	Step: 77879, 	{'train/accuracy': 0.6826757788658142, 'train/loss': 1.2964460849761963, 'validation/accuracy': 0.6247599720954895, 'validation/loss': 1.580952763557434, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.237107753753662, 'test/num_examples': 10000, 'score': 35755.94821023941, 'total_duration': 39520.92169165611, 'accumulated_submission_time': 35755.94821023941, 'accumulated_eval_time': 3757.8984265327454, 'accumulated_logging_time': 2.9589803218841553}
I0203 22:16:58.074905 139702543816448 logging_writer.py:48] [77879] accumulated_eval_time=3757.898427, accumulated_logging_time=2.958980, accumulated_submission_time=35755.948210, global_step=77879, preemption_count=0, score=35755.948210, test/accuracy=0.498800, test/loss=2.237108, test/num_examples=10000, total_duration=39520.921692, train/accuracy=0.682676, train/loss=1.296446, validation/accuracy=0.624760, validation/loss=1.580953, validation/num_examples=50000
I0203 22:17:07.247159 139702527031040 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.806416630744934, loss=2.4325942993164062
I0203 22:17:50.465340 139702543816448 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.3915926218032837, loss=4.056852340698242
I0203 22:18:36.871124 139702527031040 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.5281275510787964, loss=2.518568754196167
I0203 22:19:23.395305 139702543816448 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.350408911705017, loss=2.4308841228485107
I0203 22:20:09.721246 139702527031040 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.2107248306274414, loss=2.440941333770752
I0203 22:20:55.952329 139702543816448 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.2999701499938965, loss=4.428823947906494
I0203 22:21:42.130571 139702527031040 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.2350428104400635, loss=2.431074857711792
I0203 22:22:28.377725 139702543816448 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.271045446395874, loss=2.429086923599243
I0203 22:23:14.545773 139702527031040 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.342785120010376, loss=2.3411943912506104
I0203 22:23:58.380306 139863983413056 spec.py:321] Evaluating on the training split.
I0203 22:24:08.832441 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 22:24:41.898778 139863983413056 spec.py:349] Evaluating on the test split.
I0203 22:24:43.534016 139863983413056 submission_runner.py:408] Time since start: 39986.41s, 	Step: 78797, 	{'train/accuracy': 0.6805663704872131, 'train/loss': 1.3006861209869385, 'validation/accuracy': 0.6326799988746643, 'validation/loss': 1.5357210636138916, 'validation/num_examples': 50000, 'test/accuracy': 0.5067000389099121, 'test/loss': 2.2033281326293945, 'test/num_examples': 10000, 'score': 36176.19422531128, 'total_duration': 39986.41137290001, 'accumulated_submission_time': 36176.19422531128, 'accumulated_eval_time': 3803.0521445274353, 'accumulated_logging_time': 3.0000874996185303}
I0203 22:24:43.562568 139702543816448 logging_writer.py:48] [78797] accumulated_eval_time=3803.052145, accumulated_logging_time=3.000087, accumulated_submission_time=36176.194225, global_step=78797, preemption_count=0, score=36176.194225, test/accuracy=0.506700, test/loss=2.203328, test/num_examples=10000, total_duration=39986.411373, train/accuracy=0.680566, train/loss=1.300686, validation/accuracy=0.632680, validation/loss=1.535721, validation/num_examples=50000
I0203 22:24:45.236220 139702527031040 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.281348705291748, loss=2.8188376426696777
I0203 22:25:27.829227 139702543816448 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.136765956878662, loss=3.6991539001464844
I0203 22:26:13.752731 139702527031040 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.383650302886963, loss=2.5401716232299805
I0203 22:27:00.247737 139702543816448 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.3602924346923828, loss=2.439100980758667
I0203 22:27:46.334938 139702527031040 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.230276107788086, loss=4.023986339569092
I0203 22:28:32.734890 139702543816448 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.3227652311325073, loss=2.4453132152557373
I0203 22:29:18.958284 139702527031040 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.2371737957000732, loss=2.2570247650146484
I0203 22:30:05.696245 139702543816448 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.2988944053649902, loss=2.4431991577148438
I0203 22:30:51.971031 139702527031040 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.3072404861450195, loss=2.6912827491760254
I0203 22:31:38.249912 139702543816448 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.2036035060882568, loss=3.344731092453003
I0203 22:31:43.972291 139863983413056 spec.py:321] Evaluating on the training split.
I0203 22:31:54.281751 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 22:32:28.627661 139863983413056 spec.py:349] Evaluating on the test split.
I0203 22:32:30.275604 139863983413056 submission_runner.py:408] Time since start: 40453.15s, 	Step: 79714, 	{'train/accuracy': 0.6774023175239563, 'train/loss': 1.3380963802337646, 'validation/accuracy': 0.6273800134658813, 'validation/loss': 1.5836255550384521, 'validation/num_examples': 50000, 'test/accuracy': 0.5046000480651855, 'test/loss': 2.24481201171875, 'test/num_examples': 10000, 'score': 36596.54655098915, 'total_duration': 40453.15292882919, 'accumulated_submission_time': 36596.54655098915, 'accumulated_eval_time': 3849.35542011261, 'accumulated_logging_time': 3.0385146141052246}
I0203 22:32:30.308739 139702527031040 logging_writer.py:48] [79714] accumulated_eval_time=3849.355420, accumulated_logging_time=3.038515, accumulated_submission_time=36596.546551, global_step=79714, preemption_count=0, score=36596.546551, test/accuracy=0.504600, test/loss=2.244812, test/num_examples=10000, total_duration=40453.152929, train/accuracy=0.677402, train/loss=1.338096, validation/accuracy=0.627380, validation/loss=1.583626, validation/num_examples=50000
I0203 22:33:06.683818 139702543816448 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.1379592418670654, loss=4.486710548400879
I0203 22:33:52.650635 139702527031040 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.161746859550476, loss=3.7996468544006348
I0203 22:34:38.880436 139702543816448 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.4109495878219604, loss=2.7936501502990723
I0203 22:35:25.292439 139702527031040 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.32410728931427, loss=2.440446138381958
I0203 22:36:11.516455 139702543816448 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.2896313667297363, loss=2.319469451904297
I0203 22:36:57.848220 139702527031040 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.2165309190750122, loss=2.901625156402588
I0203 22:37:43.999258 139702543816448 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.3329570293426514, loss=3.5122549533843994
I0203 22:38:30.315999 139702527031040 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.3979804515838623, loss=2.3712034225463867
I0203 22:39:16.658047 139702543816448 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.2316569089889526, loss=4.937987804412842
I0203 22:39:30.693386 139863983413056 spec.py:321] Evaluating on the training split.
I0203 22:39:41.189285 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 22:40:12.152822 139863983413056 spec.py:349] Evaluating on the test split.
I0203 22:40:13.795916 139863983413056 submission_runner.py:408] Time since start: 40916.67s, 	Step: 80632, 	{'train/accuracy': 0.6942577958106995, 'train/loss': 1.2454754114151, 'validation/accuracy': 0.6340599656105042, 'validation/loss': 1.5173759460449219, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.1939358711242676, 'test/num_examples': 10000, 'score': 37016.87239527702, 'total_duration': 40916.673253536224, 'accumulated_submission_time': 37016.87239527702, 'accumulated_eval_time': 3892.457942724228, 'accumulated_logging_time': 3.081490993499756}
I0203 22:40:13.829226 139702527031040 logging_writer.py:48] [80632] accumulated_eval_time=3892.457943, accumulated_logging_time=3.081491, accumulated_submission_time=37016.872395, global_step=80632, preemption_count=0, score=37016.872395, test/accuracy=0.509600, test/loss=2.193936, test/num_examples=10000, total_duration=40916.673254, train/accuracy=0.694258, train/loss=1.245475, validation/accuracy=0.634060, validation/loss=1.517376, validation/num_examples=50000
I0203 22:40:42.622008 139702543816448 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.3313953876495361, loss=2.450136661529541
I0203 22:41:27.779757 139702527031040 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.5903836488723755, loss=2.617621421813965
I0203 22:42:14.121832 139702543816448 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.2061556577682495, loss=2.8022050857543945
I0203 22:43:00.501484 139702527031040 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.1702754497528076, loss=2.849829912185669
I0203 22:43:46.786592 139702543816448 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.4331579208374023, loss=2.3384227752685547
I0203 22:44:33.218920 139702527031040 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.2565369606018066, loss=4.28492546081543
I0203 22:45:19.465637 139702543816448 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.1749942302703857, loss=4.433773994445801
I0203 22:46:05.936523 139702527031040 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.1353238821029663, loss=4.9082231521606445
I0203 22:46:52.073364 139702543816448 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.2400943040847778, loss=3.493162155151367
I0203 22:47:14.120315 139863983413056 spec.py:321] Evaluating on the training split.
I0203 22:47:24.680087 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 22:47:56.703917 139863983413056 spec.py:349] Evaluating on the test split.
I0203 22:47:58.345249 139863983413056 submission_runner.py:408] Time since start: 41381.22s, 	Step: 81549, 	{'train/accuracy': 0.7085741758346558, 'train/loss': 1.1876144409179688, 'validation/accuracy': 0.6307399868965149, 'validation/loss': 1.532135248184204, 'validation/num_examples': 50000, 'test/accuracy': 0.5088000297546387, 'test/loss': 2.1980860233306885, 'test/num_examples': 10000, 'score': 37437.10391163826, 'total_duration': 41381.222581624985, 'accumulated_submission_time': 37437.10391163826, 'accumulated_eval_time': 3936.6828587055206, 'accumulated_logging_time': 3.125739812850952}
I0203 22:47:58.379621 139702527031040 logging_writer.py:48] [81549] accumulated_eval_time=3936.682859, accumulated_logging_time=3.125740, accumulated_submission_time=37437.103912, global_step=81549, preemption_count=0, score=37437.103912, test/accuracy=0.508800, test/loss=2.198086, test/num_examples=10000, total_duration=41381.222582, train/accuracy=0.708574, train/loss=1.187614, validation/accuracy=0.630740, validation/loss=1.532135, validation/num_examples=50000
I0203 22:48:20.072812 139702543816448 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.189123272895813, loss=3.321751832962036
I0203 22:49:04.918572 139702527031040 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.2631909847259521, loss=3.9678571224212646
I0203 22:49:51.326388 139702543816448 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.5888274908065796, loss=2.482424259185791
I0203 22:50:37.575856 139702527031040 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.4309297800064087, loss=2.3610880374908447
I0203 22:51:24.042036 139702543816448 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.2525715827941895, loss=2.3291189670562744
I0203 22:52:10.395077 139702527031040 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.4292675256729126, loss=2.484964609146118
I0203 22:52:56.544428 139702543816448 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.238504409790039, loss=3.5795865058898926
I0203 22:53:42.825258 139702527031040 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.2622630596160889, loss=3.168698787689209
I0203 22:54:29.204471 139702543816448 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.3887001276016235, loss=2.421253204345703
I0203 22:54:58.527605 139863983413056 spec.py:321] Evaluating on the training split.
I0203 22:55:09.184907 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 22:55:39.209803 139863983413056 spec.py:349] Evaluating on the test split.
I0203 22:55:40.847341 139863983413056 submission_runner.py:408] Time since start: 41843.72s, 	Step: 82465, 	{'train/accuracy': 0.6794531345367432, 'train/loss': 1.320271611213684, 'validation/accuracy': 0.6279999613761902, 'validation/loss': 1.5583600997924805, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.229015350341797, 'test/num_examples': 10000, 'score': 37857.19369125366, 'total_duration': 41843.72469615936, 'accumulated_submission_time': 37857.19369125366, 'accumulated_eval_time': 3979.00262093544, 'accumulated_logging_time': 3.1703832149505615}
I0203 22:55:40.876584 139702527031040 logging_writer.py:48] [82465] accumulated_eval_time=3979.002621, accumulated_logging_time=3.170383, accumulated_submission_time=37857.193691, global_step=82465, preemption_count=0, score=37857.193691, test/accuracy=0.505100, test/loss=2.229015, test/num_examples=10000, total_duration=41843.724696, train/accuracy=0.679453, train/loss=1.320272, validation/accuracy=0.628000, validation/loss=1.558360, validation/num_examples=50000
I0203 22:55:55.878777 139702543816448 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.2949546575546265, loss=2.355438232421875
I0203 22:56:39.910991 139702527031040 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.2657579183578491, loss=2.9328343868255615
I0203 22:57:26.161670 139702543816448 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.1177786588668823, loss=3.7519752979278564
I0203 22:58:12.258913 139702527031040 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.1142008304595947, loss=3.732272148132324
I0203 22:58:58.253447 139702543816448 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.137022614479065, loss=4.546727657318115
I0203 22:59:44.579840 139702527031040 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.4921976327896118, loss=2.4581267833709717
I0203 23:00:31.167507 139702543816448 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.2167658805847168, loss=4.843635082244873
I0203 23:01:17.513589 139702527031040 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.3256731033325195, loss=2.3722779750823975
I0203 23:02:03.481667 139702543816448 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.3262673616409302, loss=2.431157112121582
I0203 23:02:41.053986 139863983413056 spec.py:321] Evaluating on the training split.
I0203 23:02:51.696752 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 23:03:25.017489 139863983413056 spec.py:349] Evaluating on the test split.
I0203 23:03:26.669417 139863983413056 submission_runner.py:408] Time since start: 42309.55s, 	Step: 83383, 	{'train/accuracy': 0.6886913776397705, 'train/loss': 1.2884644269943237, 'validation/accuracy': 0.6291199922561646, 'validation/loss': 1.5523051023483276, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.218010187149048, 'test/num_examples': 10000, 'score': 38277.31278705597, 'total_duration': 42309.546770095825, 'accumulated_submission_time': 38277.31278705597, 'accumulated_eval_time': 4024.6180398464203, 'accumulated_logging_time': 3.2090845108032227}
I0203 23:03:26.701947 139702527031040 logging_writer.py:48] [83383] accumulated_eval_time=4024.618040, accumulated_logging_time=3.209085, accumulated_submission_time=38277.312787, global_step=83383, preemption_count=0, score=38277.312787, test/accuracy=0.509900, test/loss=2.218010, test/num_examples=10000, total_duration=42309.546770, train/accuracy=0.688691, train/loss=1.288464, validation/accuracy=0.629120, validation/loss=1.552305, validation/num_examples=50000
I0203 23:03:34.198948 139702543816448 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.3908442258834839, loss=2.5519063472747803
I0203 23:04:17.265902 139702527031040 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.6418694257736206, loss=2.3503241539001465
I0203 23:05:03.359089 139702543816448 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.319014072418213, loss=2.73002290725708
I0203 23:05:49.726618 139702527031040 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.3465512990951538, loss=2.8766937255859375
I0203 23:06:35.998288 139702543816448 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.3415366411209106, loss=2.458165407180786
I0203 23:07:22.360138 139702527031040 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.1904253959655762, loss=4.441189765930176
I0203 23:08:08.483321 139702543816448 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.3265960216522217, loss=2.3421454429626465
I0203 23:08:54.669051 139702527031040 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.2976069450378418, loss=2.2651522159576416
I0203 23:09:40.704952 139702543816448 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.3207645416259766, loss=2.6459028720855713
I0203 23:10:26.938790 139702527031040 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.48157799243927, loss=2.2587475776672363
I0203 23:10:26.952394 139863983413056 spec.py:321] Evaluating on the training split.
I0203 23:10:37.418553 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 23:11:09.761462 139863983413056 spec.py:349] Evaluating on the test split.
I0203 23:11:11.411486 139863983413056 submission_runner.py:408] Time since start: 42774.29s, 	Step: 84301, 	{'train/accuracy': 0.7104882597923279, 'train/loss': 1.1823593378067017, 'validation/accuracy': 0.6382799744606018, 'validation/loss': 1.511709451675415, 'validation/num_examples': 50000, 'test/accuracy': 0.5146000385284424, 'test/loss': 2.178715229034424, 'test/num_examples': 10000, 'score': 38697.50476980209, 'total_duration': 42774.288845300674, 'accumulated_submission_time': 38697.50476980209, 'accumulated_eval_time': 4069.0771267414093, 'accumulated_logging_time': 3.251221179962158}
I0203 23:11:11.440508 139702543816448 logging_writer.py:48] [84301] accumulated_eval_time=4069.077127, accumulated_logging_time=3.251221, accumulated_submission_time=38697.504770, global_step=84301, preemption_count=0, score=38697.504770, test/accuracy=0.514600, test/loss=2.178715, test/num_examples=10000, total_duration=42774.288845, train/accuracy=0.710488, train/loss=1.182359, validation/accuracy=0.638280, validation/loss=1.511709, validation/num_examples=50000
I0203 23:11:53.762667 139702527031040 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.4928675889968872, loss=2.3531088829040527
I0203 23:12:39.998243 139702543816448 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.3516473770141602, loss=2.2942113876342773
I0203 23:13:26.449600 139702527031040 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.3812706470489502, loss=2.4589462280273438
I0203 23:14:12.909762 139702543816448 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.2803325653076172, loss=3.8808891773223877
I0203 23:14:59.222155 139702527031040 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.738276481628418, loss=2.494913339614868
I0203 23:15:45.649717 139702543816448 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.316338300704956, loss=2.2521114349365234
I0203 23:16:32.278574 139702527031040 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.5261977910995483, loss=2.3957293033599854
I0203 23:17:18.764721 139702543816448 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.429238200187683, loss=3.3923888206481934
I0203 23:18:06.613329 139702527031040 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.45298433303833, loss=2.4155404567718506
I0203 23:18:11.630013 139863983413056 spec.py:321] Evaluating on the training split.
I0203 23:18:22.116236 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 23:18:54.285030 139863983413056 spec.py:349] Evaluating on the test split.
I0203 23:18:55.938131 139863983413056 submission_runner.py:408] Time since start: 43238.82s, 	Step: 85212, 	{'train/accuracy': 0.6768164038658142, 'train/loss': 1.3248145580291748, 'validation/accuracy': 0.6277599930763245, 'validation/loss': 1.5532524585723877, 'validation/num_examples': 50000, 'test/accuracy': 0.5046000480651855, 'test/loss': 2.2248823642730713, 'test/num_examples': 10000, 'score': 39117.637340545654, 'total_duration': 43238.81547117233, 'accumulated_submission_time': 39117.637340545654, 'accumulated_eval_time': 4113.385211467743, 'accumulated_logging_time': 3.2891671657562256}
I0203 23:18:55.972525 139702543816448 logging_writer.py:48] [85212] accumulated_eval_time=4113.385211, accumulated_logging_time=3.289167, accumulated_submission_time=39117.637341, global_step=85212, preemption_count=0, score=39117.637341, test/accuracy=0.504600, test/loss=2.224882, test/num_examples=10000, total_duration=43238.815471, train/accuracy=0.676816, train/loss=1.324815, validation/accuracy=0.627760, validation/loss=1.553252, validation/num_examples=50000
I0203 23:19:33.298513 139702527031040 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.0974582433700562, loss=4.356372356414795
I0203 23:20:19.392553 139702543816448 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.2014031410217285, loss=3.5774953365325928
I0203 23:21:05.974613 139702527031040 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.286825180053711, loss=2.628014087677002
I0203 23:21:52.366156 139702543816448 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.2294268608093262, loss=2.8768460750579834
I0203 23:22:38.483120 139702527031040 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.212785005569458, loss=2.5878376960754395
I0203 23:23:24.798024 139702543816448 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.4221066236495972, loss=2.52834415435791
I0203 23:24:11.261400 139702527031040 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.3548498153686523, loss=4.893044471740723
I0203 23:24:57.487826 139702543816448 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.0842174291610718, loss=4.88592529296875
I0203 23:25:43.883425 139702527031040 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.2593884468078613, loss=4.408621788024902
I0203 23:25:56.029889 139863983413056 spec.py:321] Evaluating on the training split.
I0203 23:26:06.678999 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 23:26:40.026756 139863983413056 spec.py:349] Evaluating on the test split.
I0203 23:26:41.667361 139863983413056 submission_runner.py:408] Time since start: 43704.54s, 	Step: 86128, 	{'train/accuracy': 0.6905273199081421, 'train/loss': 1.2369657754898071, 'validation/accuracy': 0.6373599767684937, 'validation/loss': 1.5001189708709717, 'validation/num_examples': 50000, 'test/accuracy': 0.5139999985694885, 'test/loss': 2.168210744857788, 'test/num_examples': 10000, 'score': 39537.63686776161, 'total_duration': 43704.54471921921, 'accumulated_submission_time': 39537.63686776161, 'accumulated_eval_time': 4159.022690296173, 'accumulated_logging_time': 3.333111047744751}
I0203 23:26:41.699670 139702543816448 logging_writer.py:48] [86128] accumulated_eval_time=4159.022690, accumulated_logging_time=3.333111, accumulated_submission_time=39537.636868, global_step=86128, preemption_count=0, score=39537.636868, test/accuracy=0.514000, test/loss=2.168211, test/num_examples=10000, total_duration=43704.544719, train/accuracy=0.690527, train/loss=1.236966, validation/accuracy=0.637360, validation/loss=1.500119, validation/num_examples=50000
I0203 23:27:12.133956 139702527031040 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.3646070957183838, loss=4.3689284324646
I0203 23:27:57.638473 139702543816448 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.4218871593475342, loss=2.3910298347473145
I0203 23:28:44.271221 139702527031040 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.3812049627304077, loss=2.2998881340026855
I0203 23:29:30.538541 139702543816448 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.4112604856491089, loss=2.549025058746338
I0203 23:30:17.223257 139702527031040 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.4522607326507568, loss=2.6386067867279053
I0203 23:31:03.663467 139702543816448 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.7370928525924683, loss=2.4679465293884277
I0203 23:31:50.085438 139702527031040 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.5244202613830566, loss=2.583146810531616
I0203 23:32:36.577689 139702543816448 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.3753851652145386, loss=2.302690029144287
I0203 23:33:22.932630 139702527031040 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.204703688621521, loss=5.030803680419922
I0203 23:33:42.069918 139863983413056 spec.py:321] Evaluating on the training split.
I0203 23:33:52.661093 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 23:34:25.447089 139863983413056 spec.py:349] Evaluating on the test split.
I0203 23:34:27.082231 139863983413056 submission_runner.py:408] Time since start: 44169.96s, 	Step: 87043, 	{'train/accuracy': 0.7060351371765137, 'train/loss': 1.2162786722183228, 'validation/accuracy': 0.64301997423172, 'validation/loss': 1.5114175081253052, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.167684316635132, 'test/num_examples': 10000, 'score': 39957.94800043106, 'total_duration': 44169.95958900452, 'accumulated_submission_time': 39957.94800043106, 'accumulated_eval_time': 4204.034997463226, 'accumulated_logging_time': 3.376688241958618}
I0203 23:34:27.112332 139702543816448 logging_writer.py:48] [87043] accumulated_eval_time=4204.034997, accumulated_logging_time=3.376688, accumulated_submission_time=39957.948000, global_step=87043, preemption_count=0, score=39957.948000, test/accuracy=0.519900, test/loss=2.167684, test/num_examples=10000, total_duration=44169.959589, train/accuracy=0.706035, train/loss=1.216279, validation/accuracy=0.643020, validation/loss=1.511418, validation/num_examples=50000
I0203 23:34:51.289845 139702527031040 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.3292933702468872, loss=3.706296920776367
I0203 23:35:36.302929 139702543816448 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.5618757009506226, loss=2.3465425968170166
I0203 23:36:22.605447 139702527031040 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.3447163105010986, loss=2.2164549827575684
I0203 23:37:08.835540 139702543816448 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.3528492450714111, loss=2.221480369567871
I0203 23:37:54.955130 139702527031040 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.1612991094589233, loss=4.147797584533691
I0203 23:38:41.391403 139702543816448 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.204298973083496, loss=3.4922196865081787
I0203 23:39:27.662052 139702527031040 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.360579490661621, loss=2.289480447769165
I0203 23:40:13.958213 139702543816448 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.3830857276916504, loss=2.336556911468506
I0203 23:41:00.109015 139702527031040 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.2666467428207397, loss=4.559643745422363
I0203 23:41:27.521720 139863983413056 spec.py:321] Evaluating on the training split.
I0203 23:41:37.930613 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 23:42:12.645370 139863983413056 spec.py:349] Evaluating on the test split.
I0203 23:42:14.295247 139863983413056 submission_runner.py:408] Time since start: 44637.17s, 	Step: 87961, 	{'train/accuracy': 0.6932812333106995, 'train/loss': 1.2655775547027588, 'validation/accuracy': 0.6387799978256226, 'validation/loss': 1.5086603164672852, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.170346975326538, 'test/num_examples': 10000, 'score': 40378.29867053032, 'total_duration': 44637.17258501053, 'accumulated_submission_time': 40378.29867053032, 'accumulated_eval_time': 4250.808509349823, 'accumulated_logging_time': 3.416672706604004}
I0203 23:42:14.330615 139702543816448 logging_writer.py:48] [87961] accumulated_eval_time=4250.808509, accumulated_logging_time=3.416673, accumulated_submission_time=40378.298671, global_step=87961, preemption_count=0, score=40378.298671, test/accuracy=0.517000, test/loss=2.170347, test/num_examples=10000, total_duration=44637.172585, train/accuracy=0.693281, train/loss=1.265578, validation/accuracy=0.638780, validation/loss=1.508660, validation/num_examples=50000
I0203 23:42:31.004749 139702527031040 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.1889400482177734, loss=4.456614971160889
I0203 23:43:15.255803 139702543816448 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.3672175407409668, loss=2.497175693511963
I0203 23:44:01.214101 139702527031040 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.4749072790145874, loss=2.3144593238830566
I0203 23:44:47.709867 139702543816448 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.6321061849594116, loss=2.2005908489227295
I0203 23:45:33.796643 139702527031040 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.4241892099380493, loss=2.384793996810913
I0203 23:46:20.094781 139702543816448 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.386792778968811, loss=2.720460891723633
I0203 23:47:06.146212 139702527031040 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.3512158393859863, loss=2.275045394897461
I0203 23:47:52.404428 139702543816448 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.377952218055725, loss=4.023064613342285
I0203 23:48:38.416738 139702527031040 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.2284022569656372, loss=4.787240028381348
I0203 23:49:14.716192 139863983413056 spec.py:321] Evaluating on the training split.
I0203 23:49:25.278701 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 23:49:57.283221 139863983413056 spec.py:349] Evaluating on the test split.
I0203 23:49:58.913422 139863983413056 submission_runner.py:408] Time since start: 45101.79s, 	Step: 88880, 	{'train/accuracy': 0.7005273103713989, 'train/loss': 1.252591609954834, 'validation/accuracy': 0.6423400044441223, 'validation/loss': 1.520622730255127, 'validation/num_examples': 50000, 'test/accuracy': 0.5182000398635864, 'test/loss': 2.1785120964050293, 'test/num_examples': 10000, 'score': 40798.624522686005, 'total_duration': 45101.790779829025, 'accumulated_submission_time': 40798.624522686005, 'accumulated_eval_time': 4295.005742549896, 'accumulated_logging_time': 3.462848424911499}
I0203 23:49:58.943787 139702543816448 logging_writer.py:48] [88880] accumulated_eval_time=4295.005743, accumulated_logging_time=3.462848, accumulated_submission_time=40798.624523, global_step=88880, preemption_count=0, score=40798.624523, test/accuracy=0.518200, test/loss=2.178512, test/num_examples=10000, total_duration=45101.790780, train/accuracy=0.700527, train/loss=1.252592, validation/accuracy=0.642340, validation/loss=1.520623, validation/num_examples=50000
I0203 23:50:07.693998 139702527031040 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.2426717281341553, loss=2.5694878101348877
I0203 23:50:50.941315 139702543816448 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.2831653356552124, loss=3.3654723167419434
I0203 23:51:37.298330 139702527031040 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.1663143634796143, loss=3.597245216369629
I0203 23:52:23.700636 139702543816448 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.5086859464645386, loss=2.2690348625183105
I0203 23:53:10.153138 139702527031040 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.5272661447525024, loss=2.3221704959869385
I0203 23:53:56.422741 139702543816448 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.3947077989578247, loss=2.402141809463501
I0203 23:54:42.669478 139702527031040 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.6004338264465332, loss=2.4123940467834473
I0203 23:55:29.114281 139702543816448 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.2588868141174316, loss=2.8488686084747314
I0203 23:56:15.556340 139702527031040 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.5247008800506592, loss=2.3506569862365723
I0203 23:56:59.305740 139863983413056 spec.py:321] Evaluating on the training split.
I0203 23:57:09.941268 139863983413056 spec.py:333] Evaluating on the validation split.
I0203 23:57:42.944459 139863983413056 spec.py:349] Evaluating on the test split.
I0203 23:57:44.585956 139863983413056 submission_runner.py:408] Time since start: 45567.46s, 	Step: 89796, 	{'train/accuracy': 0.7016210556030273, 'train/loss': 1.2490087747573853, 'validation/accuracy': 0.6393600106239319, 'validation/loss': 1.5409966707229614, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.1909217834472656, 'test/num_examples': 10000, 'score': 41218.92868351936, 'total_duration': 45567.463312625885, 'accumulated_submission_time': 41218.92868351936, 'accumulated_eval_time': 4340.285962820053, 'accumulated_logging_time': 3.503141403198242}
I0203 23:57:44.616896 139702543816448 logging_writer.py:48] [89796] accumulated_eval_time=4340.285963, accumulated_logging_time=3.503141, accumulated_submission_time=41218.928684, global_step=89796, preemption_count=0, score=41218.928684, test/accuracy=0.516200, test/loss=2.190922, test/num_examples=10000, total_duration=45567.463313, train/accuracy=0.701621, train/loss=1.249009, validation/accuracy=0.639360, validation/loss=1.540997, validation/num_examples=50000
I0203 23:57:46.703108 139702527031040 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.3975741863250732, loss=2.6926660537719727
I0203 23:58:29.255202 139702543816448 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.650099754333496, loss=2.2518434524536133
I0203 23:59:15.359791 139702527031040 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.54631507396698, loss=2.2041592597961426
I0204 00:00:01.962387 139702543816448 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.375295639038086, loss=2.2476940155029297
I0204 00:00:48.302827 139702527031040 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.4867877960205078, loss=2.1948342323303223
I0204 00:01:34.584311 139702543816448 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.4526536464691162, loss=2.3295326232910156
I0204 00:02:20.811659 139702527031040 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.3744728565216064, loss=2.7259633541107178
I0204 00:03:07.128370 139702543816448 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.3179504871368408, loss=2.5833585262298584
I0204 00:03:53.470186 139702527031040 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.2073509693145752, loss=2.9141836166381836
I0204 00:04:39.763023 139702543816448 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.6901962757110596, loss=2.3599905967712402
I0204 00:04:44.858772 139863983413056 spec.py:321] Evaluating on the training split.
I0204 00:04:55.222854 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 00:05:23.956795 139863983413056 spec.py:349] Evaluating on the test split.
I0204 00:05:25.601828 139863983413056 submission_runner.py:408] Time since start: 46028.48s, 	Step: 90713, 	{'train/accuracy': 0.69837886095047, 'train/loss': 1.2028743028640747, 'validation/accuracy': 0.6417199969291687, 'validation/loss': 1.4627127647399902, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.1296322345733643, 'test/num_examples': 10000, 'score': 41639.111610889435, 'total_duration': 46028.47916865349, 'accumulated_submission_time': 41639.111610889435, 'accumulated_eval_time': 4381.028985977173, 'accumulated_logging_time': 3.543948173522949}
I0204 00:05:25.636247 139702527031040 logging_writer.py:48] [90713] accumulated_eval_time=4381.028986, accumulated_logging_time=3.543948, accumulated_submission_time=41639.111611, global_step=90713, preemption_count=0, score=41639.111611, test/accuracy=0.521600, test/loss=2.129632, test/num_examples=10000, total_duration=46028.479169, train/accuracy=0.698379, train/loss=1.202874, validation/accuracy=0.641720, validation/loss=1.462713, validation/num_examples=50000
I0204 00:06:02.518401 139702543816448 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.296006202697754, loss=2.496901750564575
I0204 00:06:48.494329 139702527031040 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.358201026916504, loss=4.976665496826172
I0204 00:07:35.023785 139702543816448 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.4569071531295776, loss=2.3192214965820312
I0204 00:08:21.227361 139702527031040 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.3570642471313477, loss=2.3215596675872803
I0204 00:09:07.322351 139702543816448 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.4218144416809082, loss=2.838862895965576
I0204 00:09:53.475187 139702527031040 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.3615895509719849, loss=2.4658031463623047
I0204 00:10:40.045032 139702543816448 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.412867784500122, loss=2.3031222820281982
I0204 00:11:26.196373 139702527031040 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.3035086393356323, loss=3.7989308834075928
I0204 00:12:12.392231 139702543816448 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.407929539680481, loss=2.5872130393981934
I0204 00:12:25.797880 139863983413056 spec.py:321] Evaluating on the training split.
I0204 00:12:36.259988 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 00:13:10.448076 139863983413056 spec.py:349] Evaluating on the test split.
I0204 00:13:12.097655 139863983413056 submission_runner.py:408] Time since start: 46494.98s, 	Step: 91631, 	{'train/accuracy': 0.6951367259025574, 'train/loss': 1.2514410018920898, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.5035076141357422, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.1497442722320557, 'test/num_examples': 10000, 'score': 42059.214393138885, 'total_duration': 46494.97500014305, 'accumulated_submission_time': 42059.214393138885, 'accumulated_eval_time': 4427.328744649887, 'accumulated_logging_time': 3.5883829593658447}
I0204 00:13:12.126897 139702527031040 logging_writer.py:48] [91631] accumulated_eval_time=4427.328745, accumulated_logging_time=3.588383, accumulated_submission_time=42059.214393, global_step=91631, preemption_count=0, score=42059.214393, test/accuracy=0.524500, test/loss=2.149744, test/num_examples=10000, total_duration=46494.975000, train/accuracy=0.695137, train/loss=1.251441, validation/accuracy=0.641660, validation/loss=1.503508, validation/num_examples=50000
I0204 00:13:41.326434 139702543816448 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.2560538053512573, loss=2.9934210777282715
I0204 00:14:26.712968 139702527031040 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.2591900825500488, loss=4.901752471923828
I0204 00:15:13.173380 139702543816448 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.3747225999832153, loss=4.596241474151611
I0204 00:15:59.260163 139702527031040 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.3803870677947998, loss=2.127591133117676
I0204 00:16:45.478468 139702543816448 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.6746817827224731, loss=2.3915061950683594
I0204 00:17:31.693134 139702527031040 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.4095009565353394, loss=2.601705551147461
I0204 00:18:17.665493 139702543816448 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.346335768699646, loss=4.6558332443237305
I0204 00:19:03.942228 139702527031040 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.415439248085022, loss=2.36556339263916
I0204 00:19:50.150631 139702543816448 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.4645661115646362, loss=2.378335952758789
I0204 00:20:12.128724 139863983413056 spec.py:321] Evaluating on the training split.
I0204 00:20:22.518494 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 00:20:56.126041 139863983413056 spec.py:349] Evaluating on the test split.
I0204 00:20:57.758421 139863983413056 submission_runner.py:408] Time since start: 46960.64s, 	Step: 92549, 	{'train/accuracy': 0.703320324420929, 'train/loss': 1.215058445930481, 'validation/accuracy': 0.6431199908256531, 'validation/loss': 1.4973742961883545, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.147681713104248, 'test/num_examples': 10000, 'score': 42479.155719041824, 'total_duration': 46960.63577723503, 'accumulated_submission_time': 42479.155719041824, 'accumulated_eval_time': 4472.958468675613, 'accumulated_logging_time': 3.6285886764526367}
I0204 00:20:57.798104 139702527031040 logging_writer.py:48] [92549] accumulated_eval_time=4472.958469, accumulated_logging_time=3.628589, accumulated_submission_time=42479.155719, global_step=92549, preemption_count=0, score=42479.155719, test/accuracy=0.526100, test/loss=2.147682, test/num_examples=10000, total_duration=46960.635777, train/accuracy=0.703320, train/loss=1.215058, validation/accuracy=0.643120, validation/loss=1.497374, validation/num_examples=50000
I0204 00:21:19.474476 139702543816448 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.2982702255249023, loss=3.0011444091796875
I0204 00:22:04.029696 139702527031040 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.6439622640609741, loss=2.294888973236084
I0204 00:22:50.288198 139702543816448 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.5504852533340454, loss=2.337013006210327
I0204 00:23:36.613672 139702527031040 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.2309733629226685, loss=3.9597747325897217
I0204 00:24:22.875730 139702543816448 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.5815608501434326, loss=2.526273727416992
I0204 00:25:09.062057 139702527031040 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.3234113454818726, loss=3.0859439373016357
I0204 00:25:55.350787 139702543816448 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.7069907188415527, loss=2.2859880924224854
I0204 00:26:41.391302 139702527031040 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.408431053161621, loss=4.8752007484436035
I0204 00:27:27.595063 139702543816448 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.290920615196228, loss=3.1707165241241455
I0204 00:27:58.120959 139863983413056 spec.py:321] Evaluating on the training split.
I0204 00:28:08.491139 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 00:28:42.965461 139863983413056 spec.py:349] Evaluating on the test split.
I0204 00:28:44.606495 139863983413056 submission_runner.py:408] Time since start: 47427.48s, 	Step: 93468, 	{'train/accuracy': 0.7299999594688416, 'train/loss': 1.0730012655258179, 'validation/accuracy': 0.6488400101661682, 'validation/loss': 1.4530651569366455, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.1210744380950928, 'test/num_examples': 10000, 'score': 42899.42155408859, 'total_duration': 47427.48384642601, 'accumulated_submission_time': 42899.42155408859, 'accumulated_eval_time': 4519.443992853165, 'accumulated_logging_time': 3.6771814823150635}
I0204 00:28:44.640742 139702527031040 logging_writer.py:48] [93468] accumulated_eval_time=4519.443993, accumulated_logging_time=3.677181, accumulated_submission_time=42899.421554, global_step=93468, preemption_count=0, score=42899.421554, test/accuracy=0.520800, test/loss=2.121074, test/num_examples=10000, total_duration=47427.483846, train/accuracy=0.730000, train/loss=1.073001, validation/accuracy=0.648840, validation/loss=1.453065, validation/num_examples=50000
I0204 00:28:58.388138 139702543816448 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.7221169471740723, loss=2.341587781906128
I0204 00:29:41.984896 139702527031040 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.5745793581008911, loss=2.3350882530212402
I0204 00:30:28.290257 139702543816448 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.4094489812850952, loss=2.301933526992798
I0204 00:31:14.579720 139702527031040 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.1961203813552856, loss=3.762929677963257
I0204 00:32:01.243547 139702543816448 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.3339202404022217, loss=3.041944742202759
I0204 00:32:47.285229 139702527031040 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.4679001569747925, loss=2.186002731323242
I0204 00:33:33.547329 139702543816448 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.3614822626113892, loss=2.0791168212890625
I0204 00:34:20.012797 139702527031040 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.6691429615020752, loss=4.616843223571777
I0204 00:35:06.309625 139702543816448 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.2065300941467285, loss=3.883254051208496
I0204 00:35:44.731233 139863983413056 spec.py:321] Evaluating on the training split.
I0204 00:35:55.244377 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 00:36:28.650215 139863983413056 spec.py:349] Evaluating on the test split.
I0204 00:36:30.289789 139863983413056 submission_runner.py:408] Time since start: 47893.17s, 	Step: 94385, 	{'train/accuracy': 0.7069921493530273, 'train/loss': 1.1722054481506348, 'validation/accuracy': 0.6530399918556213, 'validation/loss': 1.4273556470870972, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.078791618347168, 'test/num_examples': 10000, 'score': 43319.45416808128, 'total_duration': 47893.16714167595, 'accumulated_submission_time': 43319.45416808128, 'accumulated_eval_time': 4565.002544879913, 'accumulated_logging_time': 3.720834970474243}
I0204 00:36:30.325044 139702527031040 logging_writer.py:48] [94385] accumulated_eval_time=4565.002545, accumulated_logging_time=3.720835, accumulated_submission_time=43319.454168, global_step=94385, preemption_count=0, score=43319.454168, test/accuracy=0.532300, test/loss=2.078792, test/num_examples=10000, total_duration=47893.167142, train/accuracy=0.706992, train/loss=1.172205, validation/accuracy=0.653040, validation/loss=1.427356, validation/num_examples=50000
I0204 00:36:36.986395 139702543816448 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.4933511018753052, loss=2.1492063999176025
I0204 00:37:20.137891 139702527031040 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.5341912508010864, loss=2.1839334964752197
I0204 00:38:06.015417 139702543816448 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.1944185495376587, loss=4.691222190856934
I0204 00:38:52.248090 139702527031040 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.4273509979248047, loss=2.280721426010132
I0204 00:39:38.419750 139702543816448 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.4023786783218384, loss=3.2606568336486816
I0204 00:40:25.127868 139702527031040 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.3542473316192627, loss=4.595361709594727
I0204 00:41:11.203586 139702543816448 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.540692925453186, loss=4.683542728424072
I0204 00:41:57.428638 139702527031040 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.4101743698120117, loss=2.3864970207214355
I0204 00:42:43.775902 139702543816448 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.464963436126709, loss=2.119925022125244
I0204 00:43:30.126102 139702527031040 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.379823923110962, loss=2.199974536895752
I0204 00:43:30.766243 139863983413056 spec.py:321] Evaluating on the training split.
I0204 00:43:41.512862 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 00:44:15.746787 139863983413056 spec.py:349] Evaluating on the test split.
I0204 00:44:17.392258 139863983413056 submission_runner.py:408] Time since start: 48360.27s, 	Step: 95303, 	{'train/accuracy': 0.712109386920929, 'train/loss': 1.1649584770202637, 'validation/accuracy': 0.6550599932670593, 'validation/loss': 1.4420679807662964, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.1036524772644043, 'test/num_examples': 10000, 'score': 43739.83526778221, 'total_duration': 48360.269610881805, 'accumulated_submission_time': 43739.83526778221, 'accumulated_eval_time': 4611.628629922867, 'accumulated_logging_time': 3.7676095962524414}
I0204 00:44:17.422947 139702543816448 logging_writer.py:48] [95303] accumulated_eval_time=4611.628630, accumulated_logging_time=3.767610, accumulated_submission_time=43739.835268, global_step=95303, preemption_count=0, score=43739.835268, test/accuracy=0.534800, test/loss=2.103652, test/num_examples=10000, total_duration=48360.269611, train/accuracy=0.712109, train/loss=1.164958, validation/accuracy=0.655060, validation/loss=1.442068, validation/num_examples=50000
I0204 00:44:58.794633 139702527031040 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.3544726371765137, loss=3.1661782264709473
I0204 00:45:44.918040 139702543816448 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.5162967443466187, loss=2.327589988708496
I0204 00:46:31.480867 139702527031040 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.4716103076934814, loss=2.542001724243164
I0204 00:47:17.796405 139702543816448 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.32530677318573, loss=2.4607038497924805
I0204 00:48:03.856540 139702527031040 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.5557925701141357, loss=2.3163013458251953
I0204 00:48:49.876236 139702543816448 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.2256183624267578, loss=4.775694847106934
I0204 00:49:36.076965 139702527031040 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.5113739967346191, loss=2.1887054443359375
I0204 00:50:22.703109 139702543816448 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.312974452972412, loss=4.768326282501221
I0204 00:51:09.202328 139702527031040 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.4768712520599365, loss=3.8397443294525146
I0204 00:51:17.602901 139863983413056 spec.py:321] Evaluating on the training split.
I0204 00:51:28.150720 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 00:52:00.913403 139863983413056 spec.py:349] Evaluating on the test split.
I0204 00:52:02.552393 139863983413056 submission_runner.py:408] Time since start: 48825.43s, 	Step: 96220, 	{'train/accuracy': 0.7240039110183716, 'train/loss': 1.1309791803359985, 'validation/accuracy': 0.6502199769020081, 'validation/loss': 1.4678243398666382, 'validation/num_examples': 50000, 'test/accuracy': 0.5303000211715698, 'test/loss': 2.125929832458496, 'test/num_examples': 10000, 'score': 44159.95757508278, 'total_duration': 48825.42972564697, 'accumulated_submission_time': 44159.95757508278, 'accumulated_eval_time': 4656.578102588654, 'accumulated_logging_time': 3.80673885345459}
I0204 00:52:02.590474 139702543816448 logging_writer.py:48] [96220] accumulated_eval_time=4656.578103, accumulated_logging_time=3.806739, accumulated_submission_time=44159.957575, global_step=96220, preemption_count=0, score=44159.957575, test/accuracy=0.530300, test/loss=2.125930, test/num_examples=10000, total_duration=48825.429726, train/accuracy=0.724004, train/loss=1.130979, validation/accuracy=0.650220, validation/loss=1.467824, validation/num_examples=50000
I0204 00:52:36.406384 139702527031040 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.4196593761444092, loss=2.243288040161133
I0204 00:53:22.446063 139702543816448 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.3549093008041382, loss=2.785580635070801
I0204 00:54:08.819108 139702527031040 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.2913386821746826, loss=4.539915084838867
I0204 00:54:55.046903 139702543816448 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.563560128211975, loss=2.292771339416504
I0204 00:55:41.108625 139702527031040 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.2045444250106812, loss=4.798976898193359
I0204 00:56:27.297169 139702543816448 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.6109453439712524, loss=2.3740291595458984
I0204 00:57:13.388290 139702527031040 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.4890227317810059, loss=2.147298812866211
I0204 00:57:59.604537 139702543816448 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.8595061302185059, loss=2.1064834594726562
I0204 00:58:45.849825 139702527031040 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.2076520919799805, loss=3.331305980682373
I0204 00:59:02.560491 139863983413056 spec.py:321] Evaluating on the training split.
I0204 00:59:12.875173 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 00:59:46.714279 139863983413056 spec.py:349] Evaluating on the test split.
I0204 00:59:48.367496 139863983413056 submission_runner.py:408] Time since start: 49291.24s, 	Step: 97138, 	{'train/accuracy': 0.7091991901397705, 'train/loss': 1.2087370157241821, 'validation/accuracy': 0.6511200070381165, 'validation/loss': 1.4547009468078613, 'validation/num_examples': 50000, 'test/accuracy': 0.5326000452041626, 'test/loss': 2.099363327026367, 'test/num_examples': 10000, 'score': 44579.869277477264, 'total_duration': 49291.24484491348, 'accumulated_submission_time': 44579.869277477264, 'accumulated_eval_time': 4702.385105133057, 'accumulated_logging_time': 3.8552112579345703}
I0204 00:59:48.400513 139702527031040 logging_writer.py:48] [97138] accumulated_eval_time=4702.385105, accumulated_logging_time=3.855211, accumulated_submission_time=44579.869277, global_step=97138, preemption_count=0, score=44579.869277, test/accuracy=0.532600, test/loss=2.099363, test/num_examples=10000, total_duration=49291.244845, train/accuracy=0.709199, train/loss=1.208737, validation/accuracy=0.651120, validation/loss=1.454701, validation/num_examples=50000
I0204 01:00:14.679711 139702543816448 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.3773527145385742, loss=2.403238296508789
I0204 01:00:59.897733 139702527031040 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.6207648515701294, loss=2.0350043773651123
I0204 01:01:46.684092 139702543816448 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.5230401754379272, loss=2.106335163116455
I0204 01:02:32.917353 139702527031040 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.300650715827942, loss=2.7706634998321533
I0204 01:03:19.608299 139702543816448 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.482163906097412, loss=2.2642312049865723
I0204 01:04:05.839349 139702527031040 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.1927790641784668, loss=3.859952926635742
I0204 01:04:52.136111 139702543816448 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.3870000839233398, loss=3.7912967205047607
I0204 01:05:38.585876 139702527031040 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.4002703428268433, loss=3.7601287364959717
I0204 01:06:24.779216 139702543816448 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.6477491855621338, loss=4.782283306121826
I0204 01:06:48.679202 139863983413056 spec.py:321] Evaluating on the training split.
I0204 01:06:59.175055 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 01:07:33.111506 139863983413056 spec.py:349] Evaluating on the test split.
I0204 01:07:34.757162 139863983413056 submission_runner.py:408] Time since start: 49757.63s, 	Step: 98053, 	{'train/accuracy': 0.714550793170929, 'train/loss': 1.1435582637786865, 'validation/accuracy': 0.658519983291626, 'validation/loss': 1.4108588695526123, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 2.0602378845214844, 'test/num_examples': 10000, 'score': 45000.0894985199, 'total_duration': 49757.634499788284, 'accumulated_submission_time': 45000.0894985199, 'accumulated_eval_time': 4748.463057041168, 'accumulated_logging_time': 3.8989665508270264}
I0204 01:07:34.795904 139702527031040 logging_writer.py:48] [98053] accumulated_eval_time=4748.463057, accumulated_logging_time=3.898967, accumulated_submission_time=45000.089499, global_step=98053, preemption_count=0, score=45000.089499, test/accuracy=0.538400, test/loss=2.060238, test/num_examples=10000, total_duration=49757.634500, train/accuracy=0.714551, train/loss=1.143558, validation/accuracy=0.658520, validation/loss=1.410859, validation/num_examples=50000
I0204 01:07:54.800101 139702543816448 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.468630075454712, loss=2.2518270015716553
I0204 01:08:39.131051 139702527031040 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.310772180557251, loss=2.8003041744232178
I0204 01:09:25.372190 139702543816448 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.2571921348571777, loss=3.1588754653930664
I0204 01:10:11.584481 139702527031040 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.4068320989608765, loss=2.171313524246216
I0204 01:10:57.874611 139702543816448 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.3235167264938354, loss=3.76552414894104
I0204 01:11:44.270846 139702527031040 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.4942303895950317, loss=2.0648033618927
I0204 01:12:30.532832 139702543816448 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.593233346939087, loss=2.243544340133667
I0204 01:13:16.651658 139702527031040 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.2388484477996826, loss=4.424121379852295
I0204 01:14:02.923816 139702543816448 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.4981966018676758, loss=2.260272979736328
I0204 01:14:34.800986 139863983413056 spec.py:321] Evaluating on the training split.
I0204 01:14:45.330909 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 01:15:14.814928 139863983413056 spec.py:349] Evaluating on the test split.
I0204 01:15:16.462455 139863983413056 submission_runner.py:408] Time since start: 50219.34s, 	Step: 98971, 	{'train/accuracy': 0.7219140529632568, 'train/loss': 1.1263810396194458, 'validation/accuracy': 0.6539799571037292, 'validation/loss': 1.4435505867004395, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.114023447036743, 'test/num_examples': 10000, 'score': 45420.03511428833, 'total_duration': 50219.339812517166, 'accumulated_submission_time': 45420.03511428833, 'accumulated_eval_time': 4790.124536275864, 'accumulated_logging_time': 3.9481842517852783}
I0204 01:15:16.493793 139702527031040 logging_writer.py:48] [98971] accumulated_eval_time=4790.124536, accumulated_logging_time=3.948184, accumulated_submission_time=45420.035114, global_step=98971, preemption_count=0, score=45420.035114, test/accuracy=0.527200, test/loss=2.114023, test/num_examples=10000, total_duration=50219.339813, train/accuracy=0.721914, train/loss=1.126381, validation/accuracy=0.653980, validation/loss=1.443551, validation/num_examples=50000
I0204 01:15:29.008886 139702543816448 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.3917555809020996, loss=2.970546007156372
I0204 01:16:12.890457 139702527031040 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.290714144706726, loss=3.1135292053222656
I0204 01:16:59.092580 139702543816448 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.3154317140579224, loss=3.2968239784240723
I0204 01:17:45.549278 139702527031040 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.1923198699951172, loss=3.377851963043213
I0204 01:18:31.930501 139702543816448 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.324516773223877, loss=4.197577953338623
I0204 01:19:18.264214 139702527031040 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.8107205629348755, loss=2.340869426727295
I0204 01:20:04.763524 139702543816448 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.520877718925476, loss=2.2174882888793945
I0204 01:20:50.769854 139702527031040 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.3873120546340942, loss=3.292375087738037
I0204 01:21:37.078176 139702543816448 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.3140208721160889, loss=4.764408111572266
I0204 01:22:16.805268 139863983413056 spec.py:321] Evaluating on the training split.
I0204 01:22:27.271672 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 01:23:01.900619 139863983413056 spec.py:349] Evaluating on the test split.
I0204 01:23:03.543320 139863983413056 submission_runner.py:408] Time since start: 50686.42s, 	Step: 99888, 	{'train/accuracy': 0.7099218368530273, 'train/loss': 1.1682158708572388, 'validation/accuracy': 0.6560999751091003, 'validation/loss': 1.4241340160369873, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.0836803913116455, 'test/num_examples': 10000, 'score': 45840.288721084595, 'total_duration': 50686.42065501213, 'accumulated_submission_time': 45840.288721084595, 'accumulated_eval_time': 4836.862557888031, 'accumulated_logging_time': 3.9894981384277344}
I0204 01:23:03.581270 139702527031040 logging_writer.py:48] [99888] accumulated_eval_time=4836.862558, accumulated_logging_time=3.989498, accumulated_submission_time=45840.288721, global_step=99888, preemption_count=0, score=45840.288721, test/accuracy=0.534100, test/loss=2.083680, test/num_examples=10000, total_duration=50686.420655, train/accuracy=0.709922, train/loss=1.168216, validation/accuracy=0.656100, validation/loss=1.424134, validation/num_examples=50000
I0204 01:23:08.995398 139702543816448 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.330940842628479, loss=2.1532671451568604
I0204 01:23:51.903207 139702527031040 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.6779379844665527, loss=2.509793758392334
I0204 01:24:38.238350 139702543816448 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.725399374961853, loss=2.2770564556121826
I0204 01:25:24.706543 139702527031040 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.3884352445602417, loss=3.806675434112549
I0204 01:26:10.725142 139702543816448 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.2627742290496826, loss=4.494466781616211
I0204 01:26:56.948546 139702527031040 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.4920414686203003, loss=2.4228909015655518
I0204 01:27:43.137383 139702543816448 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.6448687314987183, loss=2.3082830905914307
I0204 01:28:29.238309 139702527031040 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.2733612060546875, loss=4.237139701843262
I0204 01:29:15.207593 139702543816448 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.323252558708191, loss=2.5206222534179688
I0204 01:30:01.581394 139702527031040 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.5759814977645874, loss=2.308117628097534
I0204 01:30:04.100491 139863983413056 spec.py:321] Evaluating on the training split.
I0204 01:30:14.573145 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 01:30:49.254173 139863983413056 spec.py:349] Evaluating on the test split.
I0204 01:30:50.898991 139863983413056 submission_runner.py:408] Time since start: 51153.78s, 	Step: 100807, 	{'train/accuracy': 0.7173827886581421, 'train/loss': 1.1348272562026978, 'validation/accuracy': 0.6578800082206726, 'validation/loss': 1.4096122980117798, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.062699556350708, 'test/num_examples': 10000, 'score': 46260.74950814247, 'total_duration': 51153.77632880211, 'accumulated_submission_time': 46260.74950814247, 'accumulated_eval_time': 4883.661042928696, 'accumulated_logging_time': 4.037500381469727}
I0204 01:30:50.940520 139702543816448 logging_writer.py:48] [100807] accumulated_eval_time=4883.661043, accumulated_logging_time=4.037500, accumulated_submission_time=46260.749508, global_step=100807, preemption_count=0, score=46260.749508, test/accuracy=0.534000, test/loss=2.062700, test/num_examples=10000, total_duration=51153.776329, train/accuracy=0.717383, train/loss=1.134827, validation/accuracy=0.657880, validation/loss=1.409612, validation/num_examples=50000
I0204 01:31:30.627709 139702527031040 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.4368258714675903, loss=2.17997145652771
I0204 01:32:16.640121 139702543816448 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.441464900970459, loss=4.788266181945801
I0204 01:33:03.336447 139702527031040 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.2715412378311157, loss=3.243339776992798
I0204 01:33:49.383223 139702543816448 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.6310970783233643, loss=2.22908353805542
I0204 01:34:35.653771 139702527031040 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.4513930082321167, loss=2.5382113456726074
I0204 01:35:21.959920 139702543816448 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.7566972970962524, loss=2.267383337020874
I0204 01:36:07.970320 139702527031040 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.4402353763580322, loss=2.5077548027038574
I0204 01:36:54.419344 139702543816448 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.4286805391311646, loss=2.1791951656341553
I0204 01:37:40.573268 139702527031040 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.4672954082489014, loss=2.117537021636963
I0204 01:37:51.300188 139863983413056 spec.py:321] Evaluating on the training split.
I0204 01:38:01.755248 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 01:38:38.407484 139863983413056 spec.py:349] Evaluating on the test split.
I0204 01:38:40.045169 139863983413056 submission_runner.py:408] Time since start: 51622.92s, 	Step: 101725, 	{'train/accuracy': 0.7226757407188416, 'train/loss': 1.1128582954406738, 'validation/accuracy': 0.6645799875259399, 'validation/loss': 1.3956934213638306, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.032041311264038, 'test/num_examples': 10000, 'score': 46681.04964232445, 'total_duration': 51622.92252993584, 'accumulated_submission_time': 46681.04964232445, 'accumulated_eval_time': 4932.40603351593, 'accumulated_logging_time': 4.089587211608887}
I0204 01:38:40.076421 139702543816448 logging_writer.py:48] [101725] accumulated_eval_time=4932.406034, accumulated_logging_time=4.089587, accumulated_submission_time=46681.049642, global_step=101725, preemption_count=0, score=46681.049642, test/accuracy=0.541800, test/loss=2.032041, test/num_examples=10000, total_duration=51622.922530, train/accuracy=0.722676, train/loss=1.112858, validation/accuracy=0.664580, validation/loss=1.395693, validation/num_examples=50000
I0204 01:39:11.800324 139702527031040 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.2895840406417847, loss=4.120595455169678
I0204 01:39:57.451123 139702543816448 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.661019206047058, loss=2.2201173305511475
I0204 01:40:43.773854 139702527031040 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.3405710458755493, loss=4.6563286781311035
I0204 01:41:29.929057 139702543816448 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.331203579902649, loss=4.714069366455078
I0204 01:42:16.407609 139702527031040 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.5399519205093384, loss=2.219590902328491
I0204 01:43:02.617794 139702543816448 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.7575786113739014, loss=2.348928928375244
I0204 01:43:49.043174 139702527031040 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.4746884107589722, loss=4.730701446533203
I0204 01:44:35.203318 139702543816448 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.3396451473236084, loss=4.2669243812561035
I0204 01:45:21.916710 139702527031040 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.7403751611709595, loss=2.155416250228882
I0204 01:45:40.511510 139863983413056 spec.py:321] Evaluating on the training split.
I0204 01:45:51.149805 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 01:46:22.306993 139863983413056 spec.py:349] Evaluating on the test split.
I0204 01:46:23.952721 139863983413056 submission_runner.py:408] Time since start: 52086.83s, 	Step: 102642, 	{'train/accuracy': 0.7225781083106995, 'train/loss': 1.1422998905181885, 'validation/accuracy': 0.6594600081443787, 'validation/loss': 1.4308340549468994, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.086282730102539, 'test/num_examples': 10000, 'score': 47101.4250562191, 'total_duration': 52086.83007669449, 'accumulated_submission_time': 47101.4250562191, 'accumulated_eval_time': 4975.847240924835, 'accumulated_logging_time': 4.1319169998168945}
I0204 01:46:23.984838 139702543816448 logging_writer.py:48] [102642] accumulated_eval_time=4975.847241, accumulated_logging_time=4.131917, accumulated_submission_time=47101.425056, global_step=102642, preemption_count=0, score=47101.425056, test/accuracy=0.541500, test/loss=2.086283, test/num_examples=10000, total_duration=52086.830077, train/accuracy=0.722578, train/loss=1.142300, validation/accuracy=0.659460, validation/loss=1.430834, validation/num_examples=50000
I0204 01:46:48.621263 139702527031040 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.2660648822784424, loss=4.394008159637451
I0204 01:47:33.835837 139702543816448 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.603212594985962, loss=2.1409871578216553
I0204 01:48:20.297510 139702527031040 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.2098947763442993, loss=4.380709648132324
I0204 01:49:06.334324 139702543816448 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.4831453561782837, loss=2.3554251194000244
I0204 01:49:52.619297 139702527031040 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.3465291261672974, loss=2.4600489139556885
I0204 01:50:38.748972 139702543816448 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.3913795948028564, loss=4.696104049682617
I0204 01:51:24.950668 139702527031040 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.3306249380111694, loss=2.995738983154297
I0204 01:52:11.050567 139702543816448 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.345524549484253, loss=2.0875980854034424
I0204 01:52:57.289817 139702527031040 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.4740307331085205, loss=2.3895277976989746
I0204 01:53:24.200847 139863983413056 spec.py:321] Evaluating on the training split.
I0204 01:53:34.623284 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 01:54:08.914828 139863983413056 spec.py:349] Evaluating on the test split.
I0204 01:54:10.562632 139863983413056 submission_runner.py:408] Time since start: 52553.44s, 	Step: 103560, 	{'train/accuracy': 0.7223241925239563, 'train/loss': 1.1243679523468018, 'validation/accuracy': 0.6632999777793884, 'validation/loss': 1.3928260803222656, 'validation/num_examples': 50000, 'test/accuracy': 0.5383000373840332, 'test/loss': 2.04221248626709, 'test/num_examples': 10000, 'score': 47521.5824341774, 'total_duration': 52553.43996691704, 'accumulated_submission_time': 47521.5824341774, 'accumulated_eval_time': 5022.208994150162, 'accumulated_logging_time': 4.173872470855713}
I0204 01:54:10.601342 139702543816448 logging_writer.py:48] [103560] accumulated_eval_time=5022.208994, accumulated_logging_time=4.173872, accumulated_submission_time=47521.582434, global_step=103560, preemption_count=0, score=47521.582434, test/accuracy=0.538300, test/loss=2.042212, test/num_examples=10000, total_duration=52553.439967, train/accuracy=0.722324, train/loss=1.124368, validation/accuracy=0.663300, validation/loss=1.392826, validation/num_examples=50000
I0204 01:54:27.697366 139702527031040 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.2384916543960571, loss=3.2693185806274414
I0204 01:55:12.175120 139702543816448 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.299574613571167, loss=3.211738109588623
I0204 01:55:58.538845 139702527031040 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.7463489770889282, loss=2.1621522903442383
I0204 01:56:45.169850 139702543816448 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.5081071853637695, loss=2.2066211700439453
I0204 01:57:31.598401 139702527031040 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.5506324768066406, loss=2.2004401683807373
I0204 01:58:17.935293 139702543816448 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.3320382833480835, loss=4.731374740600586
I0204 01:59:04.154719 139702527031040 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.511565089225769, loss=2.0683960914611816
I0204 01:59:50.588770 139702543816448 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.6845808029174805, loss=2.2171642780303955
I0204 02:00:36.924764 139702527031040 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.315064549446106, loss=2.782050371170044
I0204 02:01:10.805660 139863983413056 spec.py:321] Evaluating on the training split.
I0204 02:01:21.027517 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 02:01:54.689006 139863983413056 spec.py:349] Evaluating on the test split.
I0204 02:01:56.323414 139863983413056 submission_runner.py:408] Time since start: 53019.20s, 	Step: 104475, 	{'train/accuracy': 0.73095703125, 'train/loss': 1.0725715160369873, 'validation/accuracy': 0.6665999889373779, 'validation/loss': 1.363909125328064, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.0053791999816895, 'test/num_examples': 10000, 'score': 47941.7282936573, 'total_duration': 53019.200771570206, 'accumulated_submission_time': 47941.7282936573, 'accumulated_eval_time': 5067.726749658585, 'accumulated_logging_time': 4.222776651382446}
I0204 02:01:56.360440 139702543816448 logging_writer.py:48] [104475] accumulated_eval_time=5067.726750, accumulated_logging_time=4.222777, accumulated_submission_time=47941.728294, global_step=104475, preemption_count=0, score=47941.728294, test/accuracy=0.547300, test/loss=2.005379, test/num_examples=10000, total_duration=53019.200772, train/accuracy=0.730957, train/loss=1.072572, validation/accuracy=0.666600, validation/loss=1.363909, validation/num_examples=50000
I0204 02:02:07.204774 139702527031040 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.4604148864746094, loss=2.976815938949585
I0204 02:02:50.478153 139702543816448 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.5847567319869995, loss=2.1173133850097656
I0204 02:03:36.620160 139702527031040 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.4385933876037598, loss=2.1543259620666504
I0204 02:04:23.094175 139702543816448 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.322576880455017, loss=3.977156639099121
I0204 02:05:09.014010 139702527031040 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.549825668334961, loss=2.1876306533813477
I0204 02:05:55.307651 139702543816448 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.6931111812591553, loss=2.1930532455444336
I0204 02:06:41.742718 139702527031040 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.4286378622055054, loss=3.499269723892212
I0204 02:07:28.102196 139702543816448 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.2075774669647217, loss=3.1652743816375732
I0204 02:08:14.274441 139702527031040 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.5490994453430176, loss=2.2720203399658203
I0204 02:08:56.411818 139863983413056 spec.py:321] Evaluating on the training split.
I0204 02:09:06.855113 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 02:09:39.444362 139863983413056 spec.py:349] Evaluating on the test split.
I0204 02:09:41.092702 139863983413056 submission_runner.py:408] Time since start: 53483.97s, 	Step: 105393, 	{'train/accuracy': 0.7487890720367432, 'train/loss': 1.0079954862594604, 'validation/accuracy': 0.6653000116348267, 'validation/loss': 1.3744615316390991, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 2.039724588394165, 'test/num_examples': 10000, 'score': 48361.7201230526, 'total_duration': 53483.970052719116, 'accumulated_submission_time': 48361.7201230526, 'accumulated_eval_time': 5112.407633304596, 'accumulated_logging_time': 4.270694017410278}
I0204 02:09:41.128718 139702543816448 logging_writer.py:48] [105393] accumulated_eval_time=5112.407633, accumulated_logging_time=4.270694, accumulated_submission_time=48361.720123, global_step=105393, preemption_count=0, score=48361.720123, test/accuracy=0.538400, test/loss=2.039725, test/num_examples=10000, total_duration=53483.970053, train/accuracy=0.748789, train/loss=1.007995, validation/accuracy=0.665300, validation/loss=1.374462, validation/num_examples=50000
I0204 02:09:44.465614 139702527031040 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.5337059497833252, loss=2.37044095993042
I0204 02:10:27.328544 139702543816448 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.443568468093872, loss=2.049548625946045
I0204 02:11:13.688088 139702527031040 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.3779637813568115, loss=4.338624000549316
I0204 02:11:59.930470 139702543816448 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.3041003942489624, loss=4.5297160148620605
I0204 02:12:46.251255 139702527031040 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.4196596145629883, loss=3.1711301803588867
I0204 02:13:32.725825 139702543816448 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.4185082912445068, loss=3.3833775520324707
I0204 02:14:19.146886 139702527031040 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.6302144527435303, loss=2.235468626022339
I0204 02:15:05.562811 139702543816448 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.3988083600997925, loss=3.0732672214508057
I0204 02:15:51.946790 139702527031040 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.5410932302474976, loss=2.105391502380371
I0204 02:16:38.118502 139702543816448 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.730557918548584, loss=2.0494463443756104
I0204 02:16:41.475993 139863983413056 spec.py:321] Evaluating on the training split.
I0204 02:16:51.942786 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 02:17:25.234166 139863983413056 spec.py:349] Evaluating on the test split.
I0204 02:17:26.870465 139863983413056 submission_runner.py:408] Time since start: 53949.75s, 	Step: 106309, 	{'train/accuracy': 0.7193945050239563, 'train/loss': 1.12610924243927, 'validation/accuracy': 0.6675999760627747, 'validation/loss': 1.3651163578033447, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.0198922157287598, 'test/num_examples': 10000, 'score': 48782.00963258743, 'total_duration': 53949.74782347679, 'accumulated_submission_time': 48782.00963258743, 'accumulated_eval_time': 5157.802113294601, 'accumulated_logging_time': 4.31640887260437}
I0204 02:17:26.906023 139702527031040 logging_writer.py:48] [106309] accumulated_eval_time=5157.802113, accumulated_logging_time=4.316409, accumulated_submission_time=48782.009633, global_step=106309, preemption_count=0, score=48782.009633, test/accuracy=0.545800, test/loss=2.019892, test/num_examples=10000, total_duration=53949.747823, train/accuracy=0.719395, train/loss=1.126109, validation/accuracy=0.667600, validation/loss=1.365116, validation/num_examples=50000
I0204 02:18:05.808456 139702543816448 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.5254113674163818, loss=2.1653270721435547
I0204 02:18:51.771074 139702527031040 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.6870492696762085, loss=2.244825839996338
I0204 02:19:38.181243 139702543816448 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.651867389678955, loss=2.067213535308838
I0204 02:20:24.477415 139702527031040 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.3425953388214111, loss=4.425268173217773
I0204 02:21:10.775446 139702543816448 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.4630074501037598, loss=2.313169002532959
I0204 02:21:56.933163 139702527031040 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.6289074420928955, loss=2.0606367588043213
I0204 02:22:43.322748 139702543816448 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.6233371496200562, loss=2.201761245727539
I0204 02:23:29.599381 139702527031040 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.6610045433044434, loss=2.239563465118408
I0204 02:24:15.793773 139702543816448 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.4398547410964966, loss=4.394596099853516
I0204 02:24:27.158834 139863983413056 spec.py:321] Evaluating on the training split.
I0204 02:24:37.575085 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 02:25:10.306653 139863983413056 spec.py:349] Evaluating on the test split.
I0204 02:25:11.951416 139863983413056 submission_runner.py:408] Time since start: 54414.83s, 	Step: 107226, 	{'train/accuracy': 0.7369921803474426, 'train/loss': 1.0469598770141602, 'validation/accuracy': 0.6708799600601196, 'validation/loss': 1.3509389162063599, 'validation/num_examples': 50000, 'test/accuracy': 0.5460000038146973, 'test/loss': 2.00260591506958, 'test/num_examples': 10000, 'score': 49202.203140735626, 'total_duration': 54414.82877445221, 'accumulated_submission_time': 49202.203140735626, 'accumulated_eval_time': 5202.594695091248, 'accumulated_logging_time': 4.361671686172485}
I0204 02:25:11.984833 139702527031040 logging_writer.py:48] [107226] accumulated_eval_time=5202.594695, accumulated_logging_time=4.361672, accumulated_submission_time=49202.203141, global_step=107226, preemption_count=0, score=49202.203141, test/accuracy=0.546000, test/loss=2.002606, test/num_examples=10000, total_duration=54414.828774, train/accuracy=0.736992, train/loss=1.046960, validation/accuracy=0.670880, validation/loss=1.350939, validation/num_examples=50000
I0204 02:25:43.279002 139702543816448 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.4658968448638916, loss=2.2187864780426025
I0204 02:26:28.606144 139702527031040 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.3420004844665527, loss=3.4178643226623535
I0204 02:27:15.120827 139702543816448 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.4218833446502686, loss=4.75538969039917
I0204 02:28:01.707745 139702527031040 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.4336597919464111, loss=3.3577654361724854
I0204 02:28:48.160081 139702543816448 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.4251081943511963, loss=2.7195727825164795
I0204 02:29:34.697910 139702527031040 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.804947853088379, loss=2.050076484680176
I0204 02:30:21.317594 139702543816448 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.5724833011627197, loss=2.2591662406921387
I0204 02:31:07.822870 139702527031040 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.4889096021652222, loss=2.348966360092163
I0204 02:31:54.314057 139702543816448 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.695208191871643, loss=2.0972766876220703
I0204 02:32:12.017630 139863983413056 spec.py:321] Evaluating on the training split.
I0204 02:32:22.305070 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 02:32:55.166392 139863983413056 spec.py:349] Evaluating on the test split.
I0204 02:32:56.805138 139863983413056 submission_runner.py:408] Time since start: 54879.68s, 	Step: 108140, 	{'train/accuracy': 0.7404687404632568, 'train/loss': 1.0386266708374023, 'validation/accuracy': 0.6669600009918213, 'validation/loss': 1.367197036743164, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.017230749130249, 'test/num_examples': 10000, 'score': 49622.17845416069, 'total_duration': 54879.68249583244, 'accumulated_submission_time': 49622.17845416069, 'accumulated_eval_time': 5247.3822016716, 'accumulated_logging_time': 4.404340744018555}
I0204 02:32:56.840278 139702527031040 logging_writer.py:48] [108140] accumulated_eval_time=5247.382202, accumulated_logging_time=4.404341, accumulated_submission_time=49622.178454, global_step=108140, preemption_count=0, score=49622.178454, test/accuracy=0.543900, test/loss=2.017231, test/num_examples=10000, total_duration=54879.682496, train/accuracy=0.740469, train/loss=1.038627, validation/accuracy=0.666960, validation/loss=1.367197, validation/num_examples=50000
I0204 02:33:22.279656 139702543816448 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.5263924598693848, loss=3.433729410171509
I0204 02:34:07.233844 139702527031040 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.6449958086013794, loss=2.3012068271636963
I0204 02:34:53.580613 139702543816448 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.5652543306350708, loss=4.597021102905273
I0204 02:35:40.017493 139702527031040 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.3739101886749268, loss=4.659663200378418
I0204 02:36:26.161227 139702543816448 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.4566168785095215, loss=4.1223063468933105
I0204 02:37:12.513005 139702527031040 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.3270010948181152, loss=4.443138599395752
I0204 02:37:58.644198 139702543816448 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.446903944015503, loss=4.1274518966674805
I0204 02:38:45.206430 139702527031040 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.4094215631484985, loss=3.852039337158203
I0204 02:39:31.347422 139702543816448 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.320217490196228, loss=3.867661237716675
I0204 02:39:57.202322 139863983413056 spec.py:321] Evaluating on the training split.
I0204 02:40:07.531096 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 02:40:44.809213 139863983413056 spec.py:349] Evaluating on the test split.
I0204 02:40:46.445879 139863983413056 submission_runner.py:408] Time since start: 55349.32s, 	Step: 109057, 	{'train/accuracy': 0.7318944931030273, 'train/loss': 1.066878318786621, 'validation/accuracy': 0.674839973449707, 'validation/loss': 1.330930471420288, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 1.9898630380630493, 'test/num_examples': 10000, 'score': 50042.48392677307, 'total_duration': 55349.32324099541, 'accumulated_submission_time': 50042.48392677307, 'accumulated_eval_time': 5296.62574672699, 'accumulated_logging_time': 4.448246955871582}
I0204 02:40:46.479585 139702527031040 logging_writer.py:48] [109057] accumulated_eval_time=5296.625747, accumulated_logging_time=4.448247, accumulated_submission_time=50042.483927, global_step=109057, preemption_count=0, score=50042.483927, test/accuracy=0.550300, test/loss=1.989863, test/num_examples=10000, total_duration=55349.323241, train/accuracy=0.731894, train/loss=1.066878, validation/accuracy=0.674840, validation/loss=1.330930, validation/num_examples=50000
I0204 02:41:04.809260 139702543816448 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.7412140369415283, loss=2.1297245025634766
I0204 02:41:49.098090 139702527031040 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.9570890665054321, loss=2.103008985519409
I0204 02:42:35.312030 139702543816448 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.6797841787338257, loss=1.9361284971237183
I0204 02:43:21.473656 139702527031040 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.6138023138046265, loss=2.79980206489563
I0204 02:44:07.733931 139702543816448 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.623207926750183, loss=2.122182846069336
I0204 02:44:53.889165 139702527031040 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.5642127990722656, loss=4.598287582397461
I0204 02:45:40.365174 139702543816448 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.4599518775939941, loss=4.6414899826049805
I0204 02:46:26.620566 139702527031040 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.4144755601882935, loss=4.561896324157715
I0204 02:47:12.958297 139702543816448 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.30941641330719, loss=3.469838857650757
I0204 02:47:46.730657 139863983413056 spec.py:321] Evaluating on the training split.
I0204 02:47:57.094035 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 02:48:31.521823 139863983413056 spec.py:349] Evaluating on the test split.
I0204 02:48:33.157346 139863983413056 submission_runner.py:408] Time since start: 55816.03s, 	Step: 109975, 	{'train/accuracy': 0.7314843535423279, 'train/loss': 1.0798393487930298, 'validation/accuracy': 0.6667199730873108, 'validation/loss': 1.3757458925247192, 'validation/num_examples': 50000, 'test/accuracy': 0.5470000505447388, 'test/loss': 2.032017946243286, 'test/num_examples': 10000, 'score': 50462.675322294235, 'total_duration': 55816.034700632095, 'accumulated_submission_time': 50462.675322294235, 'accumulated_eval_time': 5343.052444219589, 'accumulated_logging_time': 4.493457078933716}
I0204 02:48:33.192639 139702527031040 logging_writer.py:48] [109975] accumulated_eval_time=5343.052444, accumulated_logging_time=4.493457, accumulated_submission_time=50462.675322, global_step=109975, preemption_count=0, score=50462.675322, test/accuracy=0.547000, test/loss=2.032018, test/num_examples=10000, total_duration=55816.034701, train/accuracy=0.731484, train/loss=1.079839, validation/accuracy=0.666720, validation/loss=1.375746, validation/num_examples=50000
I0204 02:48:44.025434 139702543816448 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.30622136592865, loss=4.606291770935059
I0204 02:49:27.866877 139702527031040 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.5629323720932007, loss=2.433346748352051
I0204 02:50:14.305719 139702543816448 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.8937805891036987, loss=2.110198974609375
I0204 02:51:00.830358 139702527031040 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.402967929840088, loss=4.134005546569824
I0204 02:51:47.003422 139702543816448 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.6076833009719849, loss=3.109320878982544
I0204 02:52:33.205814 139702527031040 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.6269437074661255, loss=2.158977508544922
I0204 02:53:19.495476 139702543816448 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.380006194114685, loss=2.4804110527038574
I0204 02:54:05.615079 139702527031040 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.6696916818618774, loss=2.660862684249878
I0204 02:54:51.860804 139702543816448 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.6172531843185425, loss=2.2064402103424072
I0204 02:55:33.555286 139863983413056 spec.py:321] Evaluating on the training split.
I0204 02:55:44.122763 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 02:56:15.374336 139863983413056 spec.py:349] Evaluating on the test split.
I0204 02:56:17.010210 139863983413056 submission_runner.py:408] Time since start: 56279.89s, 	Step: 110892, 	{'train/accuracy': 0.7429296970367432, 'train/loss': 1.0271873474121094, 'validation/accuracy': 0.6739000082015991, 'validation/loss': 1.34721040725708, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 1.9919664859771729, 'test/num_examples': 10000, 'score': 50882.97862505913, 'total_duration': 56279.887543678284, 'accumulated_submission_time': 50882.97862505913, 'accumulated_eval_time': 5386.50735449791, 'accumulated_logging_time': 4.539769411087036}
I0204 02:56:17.051498 139702527031040 logging_writer.py:48] [110892] accumulated_eval_time=5386.507354, accumulated_logging_time=4.539769, accumulated_submission_time=50882.978625, global_step=110892, preemption_count=0, score=50882.978625, test/accuracy=0.552000, test/loss=1.991966, test/num_examples=10000, total_duration=56279.887544, train/accuracy=0.742930, train/loss=1.027187, validation/accuracy=0.673900, validation/loss=1.347210, validation/num_examples=50000
I0204 02:56:20.803089 139702543816448 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.6683894395828247, loss=2.1610171794891357
I0204 02:57:03.713346 139702527031040 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.6679326295852661, loss=2.0837724208831787
I0204 02:57:49.596623 139702543816448 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.7072842121124268, loss=2.1125600337982178
I0204 02:58:36.125980 139702527031040 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.1061339378356934, loss=2.5087060928344727
I0204 02:59:22.087440 139702543816448 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.4824661016464233, loss=4.710742950439453
I0204 03:00:08.910186 139702527031040 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.6995896100997925, loss=2.0383853912353516
I0204 03:00:55.177632 139702543816448 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.896419882774353, loss=2.2088093757629395
I0204 03:01:41.469375 139702527031040 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.3994085788726807, loss=3.5452396869659424
I0204 03:02:27.684575 139702543816448 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.499683141708374, loss=3.8622913360595703
I0204 03:03:14.033396 139702527031040 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.5700052976608276, loss=3.6431877613067627
I0204 03:03:17.418303 139863983413056 spec.py:321] Evaluating on the training split.
I0204 03:03:28.177088 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 03:04:03.781023 139863983413056 spec.py:349] Evaluating on the test split.
I0204 03:04:05.412233 139863983413056 submission_runner.py:408] Time since start: 56748.29s, 	Step: 111809, 	{'train/accuracy': 0.7333788871765137, 'train/loss': 1.0607457160949707, 'validation/accuracy': 0.6744999885559082, 'validation/loss': 1.3343209028244019, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 1.9874333143234253, 'test/num_examples': 10000, 'score': 51303.28646111488, 'total_duration': 56748.28958415985, 'accumulated_submission_time': 51303.28646111488, 'accumulated_eval_time': 5434.5012810230255, 'accumulated_logging_time': 4.5913405418396}
I0204 03:04:05.445787 139702543816448 logging_writer.py:48] [111809] accumulated_eval_time=5434.501281, accumulated_logging_time=4.591341, accumulated_submission_time=51303.286461, global_step=111809, preemption_count=0, score=51303.286461, test/accuracy=0.546800, test/loss=1.987433, test/num_examples=10000, total_duration=56748.289584, train/accuracy=0.733379, train/loss=1.060746, validation/accuracy=0.674500, validation/loss=1.334321, validation/num_examples=50000
I0204 03:04:44.297917 139702527031040 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.3936007022857666, loss=3.298482894897461
I0204 03:05:30.401938 139702543816448 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.5755513906478882, loss=2.4454345703125
I0204 03:06:16.681677 139702527031040 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.757677674293518, loss=2.040231466293335
I0204 03:07:02.776431 139702543816448 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.559294581413269, loss=3.9042978286743164
I0204 03:07:48.973314 139702527031040 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.736275553703308, loss=2.1031947135925293
I0204 03:08:35.289429 139702543816448 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.6827788352966309, loss=2.5172295570373535
I0204 03:09:21.474035 139702527031040 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.438378930091858, loss=3.056713342666626
I0204 03:10:08.424501 139702543816448 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.5425770282745361, loss=2.0845108032226562
I0204 03:10:54.585729 139702527031040 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.5562962293624878, loss=2.931025981903076
I0204 03:11:05.471064 139863983413056 spec.py:321] Evaluating on the training split.
I0204 03:11:15.884164 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 03:11:49.572569 139863983413056 spec.py:349] Evaluating on the test split.
I0204 03:11:51.202265 139863983413056 submission_runner.py:408] Time since start: 57214.08s, 	Step: 112725, 	{'train/accuracy': 0.732714831829071, 'train/loss': 1.0809401273727417, 'validation/accuracy': 0.6717199683189392, 'validation/loss': 1.3633190393447876, 'validation/num_examples': 50000, 'test/accuracy': 0.5455000400543213, 'test/loss': 2.0178678035736084, 'test/num_examples': 10000, 'score': 51723.2543463707, 'total_duration': 57214.07962059975, 'accumulated_submission_time': 51723.2543463707, 'accumulated_eval_time': 5480.232470989227, 'accumulated_logging_time': 4.634597301483154}
I0204 03:11:51.235083 139702543816448 logging_writer.py:48] [112725] accumulated_eval_time=5480.232471, accumulated_logging_time=4.634597, accumulated_submission_time=51723.254346, global_step=112725, preemption_count=0, score=51723.254346, test/accuracy=0.545500, test/loss=2.017868, test/num_examples=10000, total_duration=57214.079621, train/accuracy=0.732715, train/loss=1.080940, validation/accuracy=0.671720, validation/loss=1.363319, validation/num_examples=50000
I0204 03:12:22.900821 139702527031040 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.5915639400482178, loss=2.0915024280548096
I0204 03:13:08.547815 139702543816448 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.6727497577667236, loss=2.6522810459136963
I0204 03:13:54.849910 139702527031040 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.5487040281295776, loss=4.302472114562988
I0204 03:14:41.069590 139702543816448 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.624348759651184, loss=2.146820545196533
I0204 03:15:27.323814 139702527031040 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.6979349851608276, loss=2.469475030899048
I0204 03:16:13.778281 139702543816448 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.6624372005462646, loss=2.042069435119629
I0204 03:17:00.003388 139702527031040 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.6979272365570068, loss=2.2869207859039307
I0204 03:17:46.270113 139702543816448 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.9473130702972412, loss=2.046499013900757
I0204 03:18:32.589947 139702527031040 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.607264518737793, loss=2.037405490875244
I0204 03:18:51.429139 139863983413056 spec.py:321] Evaluating on the training split.
I0204 03:19:02.392118 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 03:19:36.255845 139863983413056 spec.py:349] Evaluating on the test split.
I0204 03:19:37.900841 139863983413056 submission_runner.py:408] Time since start: 57680.78s, 	Step: 113642, 	{'train/accuracy': 0.7489062547683716, 'train/loss': 0.9951205253601074, 'validation/accuracy': 0.67603999376297, 'validation/loss': 1.3217229843139648, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.9755213260650635, 'test/num_examples': 10000, 'score': 52143.389543771744, 'total_duration': 57680.778177022934, 'accumulated_submission_time': 52143.389543771744, 'accumulated_eval_time': 5526.704137802124, 'accumulated_logging_time': 4.677294731140137}
I0204 03:19:37.944783 139702543816448 logging_writer.py:48] [113642] accumulated_eval_time=5526.704138, accumulated_logging_time=4.677295, accumulated_submission_time=52143.389544, global_step=113642, preemption_count=0, score=52143.389544, test/accuracy=0.555000, test/loss=1.975521, test/num_examples=10000, total_duration=57680.778177, train/accuracy=0.748906, train/loss=0.995121, validation/accuracy=0.676040, validation/loss=1.321723, validation/num_examples=50000
I0204 03:20:02.534332 139702527031040 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.4309113025665283, loss=2.277311086654663
I0204 03:20:47.599036 139702543816448 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.5758020877838135, loss=2.3942694664001465
I0204 03:21:34.264616 139702527031040 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.8170009851455688, loss=2.1707053184509277
I0204 03:22:20.542800 139702543816448 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.5123003721237183, loss=4.2903733253479
I0204 03:23:06.969586 139702527031040 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.6386053562164307, loss=2.0663034915924072
I0204 03:23:53.209421 139702543816448 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.5825997591018677, loss=3.8337442874908447
I0204 03:24:39.622432 139702527031040 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.6303350925445557, loss=2.118525981903076
I0204 03:25:25.847705 139702543816448 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.6728579998016357, loss=4.402605056762695
I0204 03:26:11.967989 139702527031040 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.385454535484314, loss=2.782088279724121
I0204 03:26:38.331649 139863983413056 spec.py:321] Evaluating on the training split.
I0204 03:26:48.856681 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 03:27:22.469789 139863983413056 spec.py:349] Evaluating on the test split.
I0204 03:27:24.108464 139863983413056 submission_runner.py:408] Time since start: 58146.99s, 	Step: 114559, 	{'train/accuracy': 0.745898425579071, 'train/loss': 1.0103362798690796, 'validation/accuracy': 0.6803799867630005, 'validation/loss': 1.3071465492248535, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.9714163541793823, 'test/num_examples': 10000, 'score': 52563.71753501892, 'total_duration': 58146.98582029343, 'accumulated_submission_time': 52563.71753501892, 'accumulated_eval_time': 5572.480983495712, 'accumulated_logging_time': 4.731476306915283}
I0204 03:27:24.146686 139702543816448 logging_writer.py:48] [114559] accumulated_eval_time=5572.480983, accumulated_logging_time=4.731476, accumulated_submission_time=52563.717535, global_step=114559, preemption_count=0, score=52563.717535, test/accuracy=0.555000, test/loss=1.971416, test/num_examples=10000, total_duration=58146.985820, train/accuracy=0.745898, train/loss=1.010336, validation/accuracy=0.680380, validation/loss=1.307147, validation/num_examples=50000
I0204 03:27:41.649298 139702527031040 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.6318614482879639, loss=2.049686908721924
I0204 03:28:25.772760 139702543816448 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.5866707563400269, loss=2.937596559524536
I0204 03:29:12.188235 139702527031040 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.582157850265503, loss=3.9704947471618652
I0204 03:29:58.471621 139702543816448 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.5991071462631226, loss=3.0703351497650146
I0204 03:30:44.876516 139702527031040 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.6301112174987793, loss=2.258429527282715
I0204 03:31:31.296369 139702543816448 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.5670974254608154, loss=1.9602954387664795
I0204 03:32:17.301364 139702527031040 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.860573649406433, loss=2.0613455772399902
I0204 03:33:03.290764 139702543816448 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.5917049646377563, loss=2.1478922367095947
I0204 03:33:49.449928 139702527031040 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.5843168497085571, loss=2.136228084564209
I0204 03:34:24.165514 139863983413056 spec.py:321] Evaluating on the training split.
I0204 03:34:34.557893 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 03:35:08.057693 139863983413056 spec.py:349] Evaluating on the test split.
I0204 03:35:09.691652 139863983413056 submission_runner.py:408] Time since start: 58612.57s, 	Step: 115477, 	{'train/accuracy': 0.7442968487739563, 'train/loss': 1.0438237190246582, 'validation/accuracy': 0.6782799959182739, 'validation/loss': 1.3390940427780151, 'validation/num_examples': 50000, 'test/accuracy': 0.5524000525474548, 'test/loss': 1.993807315826416, 'test/num_examples': 10000, 'score': 52983.67902421951, 'total_duration': 58612.56900882721, 'accumulated_submission_time': 52983.67902421951, 'accumulated_eval_time': 5618.007117033005, 'accumulated_logging_time': 4.778652191162109}
I0204 03:35:09.729602 139702543816448 logging_writer.py:48] [115477] accumulated_eval_time=5618.007117, accumulated_logging_time=4.778652, accumulated_submission_time=52983.679024, global_step=115477, preemption_count=0, score=52983.679024, test/accuracy=0.552400, test/loss=1.993807, test/num_examples=10000, total_duration=58612.569009, train/accuracy=0.744297, train/loss=1.043824, validation/accuracy=0.678280, validation/loss=1.339094, validation/num_examples=50000
I0204 03:35:19.723366 139702527031040 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.522289514541626, loss=2.4876976013183594
I0204 03:36:03.218302 139702543816448 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.6619467735290527, loss=2.0237934589385986
I0204 03:36:49.531059 139702527031040 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.606967568397522, loss=1.9922382831573486
I0204 03:37:35.682254 139702543816448 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.4675272703170776, loss=3.9946043491363525
I0204 03:38:21.889099 139702527031040 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.5954430103302002, loss=1.9932959079742432
I0204 03:39:08.223476 139702543816448 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.5109105110168457, loss=3.2298479080200195
I0204 03:39:54.559907 139702527031040 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.8545341491699219, loss=2.1700613498687744
I0204 03:40:40.765769 139702543816448 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.4903976917266846, loss=3.4197700023651123
I0204 03:41:26.950211 139702527031040 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.732055425643921, loss=2.069885730743408
I0204 03:42:10.103512 139863983413056 spec.py:321] Evaluating on the training split.
I0204 03:42:20.743665 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 03:42:55.058933 139863983413056 spec.py:349] Evaluating on the test split.
I0204 03:42:56.693684 139863983413056 submission_runner.py:408] Time since start: 59079.57s, 	Step: 116394, 	{'train/accuracy': 0.7509765625, 'train/loss': 1.0075005292892456, 'validation/accuracy': 0.6782000064849854, 'validation/loss': 1.3190701007843018, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 1.96835458278656, 'test/num_examples': 10000, 'score': 53403.994445085526, 'total_duration': 59079.571038246155, 'accumulated_submission_time': 53403.994445085526, 'accumulated_eval_time': 5664.597292423248, 'accumulated_logging_time': 4.826295614242554}
I0204 03:42:56.730707 139702543816448 logging_writer.py:48] [116394] accumulated_eval_time=5664.597292, accumulated_logging_time=4.826296, accumulated_submission_time=53403.994445, global_step=116394, preemption_count=0, score=53403.994445, test/accuracy=0.554700, test/loss=1.968355, test/num_examples=10000, total_duration=59079.571038, train/accuracy=0.750977, train/loss=1.007501, validation/accuracy=0.678200, validation/loss=1.319070, validation/num_examples=50000
I0204 03:42:59.639375 139702527031040 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.6041138172149658, loss=2.617466926574707
I0204 03:43:42.122971 139702543816448 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.620997667312622, loss=2.0075623989105225
I0204 03:44:28.277561 139702527031040 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.437395691871643, loss=2.1845765113830566
I0204 03:45:14.986859 139702543816448 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.7065430879592896, loss=2.327974557876587
I0204 03:46:01.455322 139702527031040 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.514174461364746, loss=3.679403781890869
I0204 03:46:47.791536 139702543816448 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.7399585247039795, loss=2.2390687465667725
I0204 03:47:34.045002 139702527031040 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.7472339868545532, loss=2.015911340713501
I0204 03:48:20.510484 139702543816448 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.4841707944869995, loss=3.7959136962890625
I0204 03:49:06.740353 139702527031040 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.703955888748169, loss=2.066837787628174
I0204 03:49:53.278015 139702543816448 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.8186484575271606, loss=2.236781597137451
I0204 03:49:57.104111 139863983413056 spec.py:321] Evaluating on the training split.
I0204 03:50:08.086804 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 03:50:41.347320 139863983413056 spec.py:349] Evaluating on the test split.
I0204 03:50:42.987923 139863983413056 submission_runner.py:408] Time since start: 59545.87s, 	Step: 117310, 	{'train/accuracy': 0.7648828029632568, 'train/loss': 0.9348188638687134, 'validation/accuracy': 0.6816399693489075, 'validation/loss': 1.3038502931594849, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 1.9470292329788208, 'test/num_examples': 10000, 'score': 53824.31056809425, 'total_duration': 59545.865280628204, 'accumulated_submission_time': 53824.31056809425, 'accumulated_eval_time': 5710.4811136722565, 'accumulated_logging_time': 4.872780084609985}
I0204 03:50:43.022156 139702527031040 logging_writer.py:48] [117310] accumulated_eval_time=5710.481114, accumulated_logging_time=4.872780, accumulated_submission_time=53824.310568, global_step=117310, preemption_count=0, score=53824.310568, test/accuracy=0.555100, test/loss=1.947029, test/num_examples=10000, total_duration=59545.865281, train/accuracy=0.764883, train/loss=0.934819, validation/accuracy=0.681640, validation/loss=1.303850, validation/num_examples=50000
I0204 03:51:21.494746 139702543816448 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.755103349685669, loss=2.059717893600464
I0204 03:52:07.624765 139702527031040 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.928253173828125, loss=1.991716742515564
I0204 03:52:54.296769 139702543816448 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.5774728059768677, loss=2.626842498779297
I0204 03:53:40.374344 139702527031040 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.626006841659546, loss=1.9527430534362793
I0204 03:54:26.514910 139702543816448 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.6498829126358032, loss=2.00665020942688
I0204 03:55:12.859620 139702527031040 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.8120982646942139, loss=1.9649299383163452
I0204 03:55:58.978996 139702543816448 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.7082926034927368, loss=2.035036563873291
I0204 03:56:45.411008 139702527031040 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.9054116010665894, loss=2.0765769481658936
I0204 03:57:31.455114 139702543816448 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.8225769996643066, loss=2.0493927001953125
I0204 03:57:43.108653 139863983413056 spec.py:321] Evaluating on the training split.
I0204 03:57:53.805251 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 03:58:28.848193 139863983413056 spec.py:349] Evaluating on the test split.
I0204 03:58:30.482288 139863983413056 submission_runner.py:408] Time since start: 60013.36s, 	Step: 118227, 	{'train/accuracy': 0.7464257478713989, 'train/loss': 1.029775619506836, 'validation/accuracy': 0.6835199594497681, 'validation/loss': 1.3185490369796753, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9517545700073242, 'test/num_examples': 10000, 'score': 54244.33873510361, 'total_duration': 60013.35964846611, 'accumulated_submission_time': 54244.33873510361, 'accumulated_eval_time': 5757.854747772217, 'accumulated_logging_time': 4.916918992996216}
I0204 03:58:30.517199 139702527031040 logging_writer.py:48] [118227] accumulated_eval_time=5757.854748, accumulated_logging_time=4.916919, accumulated_submission_time=54244.338735, global_step=118227, preemption_count=0, score=54244.338735, test/accuracy=0.562100, test/loss=1.951755, test/num_examples=10000, total_duration=60013.359648, train/accuracy=0.746426, train/loss=1.029776, validation/accuracy=0.683520, validation/loss=1.318549, validation/num_examples=50000
I0204 03:59:01.406548 139702543816448 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.584800362586975, loss=2.0221498012542725
I0204 03:59:47.137757 139702527031040 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.6395421028137207, loss=4.528707981109619
I0204 04:00:33.434236 139702543816448 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.614522099494934, loss=3.6132187843322754
I0204 04:01:19.913896 139702527031040 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.7874819040298462, loss=2.071597099304199
I0204 04:02:05.991285 139702543816448 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.57895827293396, loss=3.926475763320923
I0204 04:02:52.227490 139702527031040 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.7079452276229858, loss=1.9831416606903076
I0204 04:03:38.687786 139702543816448 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.6940735578536987, loss=2.687819004058838
I0204 04:04:24.863720 139702527031040 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.777823805809021, loss=1.8998724222183228
I0204 04:05:11.070765 139702543816448 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.9504228830337524, loss=2.0552871227264404
I0204 04:05:30.542115 139863983413056 spec.py:321] Evaluating on the training split.
I0204 04:05:41.080183 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 04:06:12.609366 139863983413056 spec.py:349] Evaluating on the test split.
I0204 04:06:14.240212 139863983413056 submission_runner.py:408] Time since start: 60477.12s, 	Step: 119144, 	{'train/accuracy': 0.7531836032867432, 'train/loss': 0.9818204045295715, 'validation/accuracy': 0.6846799850463867, 'validation/loss': 1.2962895631790161, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 1.942766547203064, 'test/num_examples': 10000, 'score': 54664.30675196648, 'total_duration': 60477.11735010147, 'accumulated_submission_time': 54664.30675196648, 'accumulated_eval_time': 5801.55264043808, 'accumulated_logging_time': 4.960393190383911}
I0204 04:06:14.281043 139702527031040 logging_writer.py:48] [119144] accumulated_eval_time=5801.552640, accumulated_logging_time=4.960393, accumulated_submission_time=54664.306752, global_step=119144, preemption_count=0, score=54664.306752, test/accuracy=0.558200, test/loss=1.942767, test/num_examples=10000, total_duration=60477.117350, train/accuracy=0.753184, train/loss=0.981820, validation/accuracy=0.684680, validation/loss=1.296290, validation/num_examples=50000
I0204 04:06:38.049896 139702543816448 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.692069411277771, loss=1.9475915431976318
I0204 04:07:23.100717 139702527031040 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.7554566860198975, loss=2.0535335540771484
I0204 04:08:09.407282 139702543816448 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.6382781267166138, loss=2.4368672370910645
I0204 04:08:55.594905 139702527031040 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.563354253768921, loss=2.443650245666504
I0204 04:09:42.085443 139702543816448 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.726533055305481, loss=2.5284557342529297
I0204 04:10:28.309837 139702527031040 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.7081420421600342, loss=2.120213508605957
I0204 04:11:14.550919 139702543816448 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.5274150371551514, loss=3.38512921333313
I0204 04:12:01.074741 139702527031040 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.6040470600128174, loss=4.22042179107666
I0204 04:12:47.390052 139702543816448 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.897756814956665, loss=1.9404940605163574
I0204 04:13:14.376028 139863983413056 spec.py:321] Evaluating on the training split.
I0204 04:13:24.729531 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 04:13:56.768283 139863983413056 spec.py:349] Evaluating on the test split.
I0204 04:13:58.411981 139863983413056 submission_runner.py:408] Time since start: 60941.29s, 	Step: 120060, 	{'train/accuracy': 0.7615820169448853, 'train/loss': 1.0147477388381958, 'validation/accuracy': 0.6805799603462219, 'validation/loss': 1.354517936706543, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.008143424987793, 'test/num_examples': 10000, 'score': 55084.343988895416, 'total_duration': 60941.289311409, 'accumulated_submission_time': 55084.343988895416, 'accumulated_eval_time': 5845.588560342789, 'accumulated_logging_time': 5.0106000900268555}
I0204 04:13:58.452505 139702527031040 logging_writer.py:48] [120060] accumulated_eval_time=5845.588560, accumulated_logging_time=5.010600, accumulated_submission_time=55084.343989, global_step=120060, preemption_count=0, score=55084.343989, test/accuracy=0.556100, test/loss=2.008143, test/num_examples=10000, total_duration=60941.289311, train/accuracy=0.761582, train/loss=1.014748, validation/accuracy=0.680580, validation/loss=1.354518, validation/num_examples=50000
I0204 04:14:15.539389 139702543816448 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.5413464307785034, loss=3.407611846923828
I0204 04:14:59.438824 139702527031040 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.6186920404434204, loss=3.939357280731201
I0204 04:15:45.839024 139702543816448 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.4787814617156982, loss=3.5487942695617676
I0204 04:16:32.234421 139702527031040 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.6497864723205566, loss=2.055297374725342
I0204 04:17:18.644548 139702543816448 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.672796368598938, loss=2.007673501968384
I0204 04:18:04.823261 139702527031040 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.4623390436172485, loss=3.288090944290161
I0204 04:18:50.768976 139702543816448 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.7031148672103882, loss=2.0892109870910645
I0204 04:19:37.031590 139702527031040 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.8872913122177124, loss=1.9172399044036865
I0204 04:20:23.468649 139702543816448 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.7801785469055176, loss=2.019698143005371
I0204 04:20:58.617468 139863983413056 spec.py:321] Evaluating on the training split.
I0204 04:21:09.058907 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 04:21:43.162108 139863983413056 spec.py:349] Evaluating on the test split.
I0204 04:21:44.802696 139863983413056 submission_runner.py:408] Time since start: 61407.68s, 	Step: 120978, 	{'train/accuracy': 0.7485546469688416, 'train/loss': 1.011296033859253, 'validation/accuracy': 0.6860799789428711, 'validation/loss': 1.2949484586715698, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 1.9418689012527466, 'test/num_examples': 10000, 'score': 55504.44954395294, 'total_duration': 61407.6800467968, 'accumulated_submission_time': 55504.44954395294, 'accumulated_eval_time': 5891.7737855911255, 'accumulated_logging_time': 5.061231851577759}
I0204 04:21:44.839997 139702527031040 logging_writer.py:48] [120978] accumulated_eval_time=5891.773786, accumulated_logging_time=5.061232, accumulated_submission_time=55504.449544, global_step=120978, preemption_count=0, score=55504.449544, test/accuracy=0.563600, test/loss=1.941869, test/num_examples=10000, total_duration=61407.680047, train/accuracy=0.748555, train/loss=1.011296, validation/accuracy=0.686080, validation/loss=1.294948, validation/num_examples=50000
I0204 04:21:54.418999 139702543816448 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.734013557434082, loss=1.8921527862548828
I0204 04:22:37.485550 139702527031040 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.7866640090942383, loss=2.0244216918945312
I0204 04:23:23.804165 139702543816448 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.7433358430862427, loss=1.9003170728683472
I0204 04:24:10.087518 139702527031040 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.562182903289795, loss=3.5200557708740234
I0204 04:24:56.781948 139702543816448 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.6271237134933472, loss=3.66668963432312
I0204 04:25:42.924708 139702527031040 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.8347198963165283, loss=2.1035051345825195
I0204 04:26:29.428430 139702543816448 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.631212592124939, loss=3.773524045944214
I0204 04:27:15.818171 139702527031040 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.5626087188720703, loss=2.151613473892212
I0204 04:28:02.018972 139702543816448 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.8594273328781128, loss=1.8679882287979126
I0204 04:28:45.122241 139863983413056 spec.py:321] Evaluating on the training split.
I0204 04:28:55.486861 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 04:29:27.808423 139863983413056 spec.py:349] Evaluating on the test split.
I0204 04:29:29.450814 139863983413056 submission_runner.py:408] Time since start: 61872.33s, 	Step: 121895, 	{'train/accuracy': 0.7560937404632568, 'train/loss': 0.9640070796012878, 'validation/accuracy': 0.685259997844696, 'validation/loss': 1.2753068208694458, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 1.916074275970459, 'test/num_examples': 10000, 'score': 55924.674210071564, 'total_duration': 61872.32814979553, 'accumulated_submission_time': 55924.674210071564, 'accumulated_eval_time': 5936.102333545685, 'accumulated_logging_time': 5.1081626415252686}
I0204 04:29:29.494585 139702527031040 logging_writer.py:48] [121895] accumulated_eval_time=5936.102334, accumulated_logging_time=5.108163, accumulated_submission_time=55924.674210, global_step=121895, preemption_count=0, score=55924.674210, test/accuracy=0.563700, test/loss=1.916074, test/num_examples=10000, total_duration=61872.328150, train/accuracy=0.756094, train/loss=0.964007, validation/accuracy=0.685260, validation/loss=1.275307, validation/num_examples=50000
I0204 04:29:31.991518 139702543816448 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.5535706281661987, loss=3.69618558883667
I0204 04:30:14.906910 139702527031040 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.6919374465942383, loss=1.8803064823150635
I0204 04:31:00.791633 139702543816448 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.6663686037063599, loss=2.0632307529449463
I0204 04:31:47.061902 139702527031040 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.797670841217041, loss=4.503467559814453
I0204 04:32:33.307692 139702543816448 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.8187364339828491, loss=2.6225926876068115
I0204 04:33:20.002435 139702527031040 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.7027305364608765, loss=4.505962371826172
I0204 04:34:06.126336 139702543816448 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.7929099798202515, loss=1.9005048274993896
I0204 04:34:52.556465 139702527031040 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.5962660312652588, loss=4.514430999755859
I0204 04:35:38.716327 139702543816448 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.5971566438674927, loss=2.915804624557495
I0204 04:36:24.823391 139702527031040 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.558316946029663, loss=4.548924446105957
I0204 04:36:29.625480 139863983413056 spec.py:321] Evaluating on the training split.
I0204 04:36:40.084795 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 04:37:12.170089 139863983413056 spec.py:349] Evaluating on the test split.
I0204 04:37:13.816839 139863983413056 submission_runner.py:408] Time since start: 62336.69s, 	Step: 122812, 	{'train/accuracy': 0.7655858993530273, 'train/loss': 0.926956832408905, 'validation/accuracy': 0.6911399960517883, 'validation/loss': 1.2588539123535156, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 1.9093610048294067, 'test/num_examples': 10000, 'score': 56344.74541926384, 'total_duration': 62336.69419193268, 'accumulated_submission_time': 56344.74541926384, 'accumulated_eval_time': 5980.293684959412, 'accumulated_logging_time': 5.162423133850098}
I0204 04:37:13.856967 139702543816448 logging_writer.py:48] [122812] accumulated_eval_time=5980.293685, accumulated_logging_time=5.162423, accumulated_submission_time=56344.745419, global_step=122812, preemption_count=0, score=56344.745419, test/accuracy=0.568800, test/loss=1.909361, test/num_examples=10000, total_duration=62336.694192, train/accuracy=0.765586, train/loss=0.926957, validation/accuracy=0.691140, validation/loss=1.258854, validation/num_examples=50000
I0204 04:37:51.200142 139702527031040 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.796087384223938, loss=2.3836777210235596
I0204 04:38:37.126034 139702543816448 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.9977284669876099, loss=1.9149742126464844
I0204 04:39:23.348560 139702527031040 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.1360785961151123, loss=1.9220383167266846
I0204 04:40:09.584544 139702543816448 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.978962779045105, loss=4.356505393981934
I0204 04:40:55.626838 139702527031040 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.9372237920761108, loss=2.088297128677368
I0204 04:41:41.879825 139702543816448 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.5945409536361694, loss=3.012852191925049
I0204 04:42:28.232618 139702527031040 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.6877301931381226, loss=3.570368528366089
I0204 04:43:14.209734 139702543816448 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.7240748405456543, loss=1.9373619556427002
I0204 04:44:00.209094 139702527031040 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.508125901222229, loss=2.58341121673584
I0204 04:44:14.198348 139863983413056 spec.py:321] Evaluating on the training split.
I0204 04:44:24.478847 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 04:44:57.189221 139863983413056 spec.py:349] Evaluating on the test split.
I0204 04:44:58.823266 139863983413056 submission_runner.py:408] Time since start: 62801.70s, 	Step: 123732, 	{'train/accuracy': 0.7517968416213989, 'train/loss': 1.0083987712860107, 'validation/accuracy': 0.6892200112342834, 'validation/loss': 1.288521409034729, 'validation/num_examples': 50000, 'test/accuracy': 0.5617000460624695, 'test/loss': 1.9466270208358765, 'test/num_examples': 10000, 'score': 56765.029317855835, 'total_duration': 62801.70062446594, 'accumulated_submission_time': 56765.029317855835, 'accumulated_eval_time': 6024.918617486954, 'accumulated_logging_time': 5.211941957473755}
I0204 04:44:58.859601 139702543816448 logging_writer.py:48] [123732] accumulated_eval_time=6024.918617, accumulated_logging_time=5.211942, accumulated_submission_time=56765.029318, global_step=123732, preemption_count=0, score=56765.029318, test/accuracy=0.561700, test/loss=1.946627, test/num_examples=10000, total_duration=62801.700624, train/accuracy=0.751797, train/loss=1.008399, validation/accuracy=0.689220, validation/loss=1.288521, validation/num_examples=50000
I0204 04:45:27.629499 139702527031040 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.757887840270996, loss=4.5162353515625
I0204 04:46:13.311958 139702543816448 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.8361550569534302, loss=2.051063060760498
I0204 04:46:59.548594 139702527031040 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.6592332124710083, loss=3.989604949951172
I0204 04:47:45.904570 139702543816448 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.7624989748001099, loss=3.852717399597168
I0204 04:48:32.439538 139702527031040 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.7971446514129639, loss=2.1713454723358154
I0204 04:49:18.639694 139702543816448 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.7212352752685547, loss=4.4109296798706055
I0204 04:50:04.988632 139702527031040 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.818535327911377, loss=2.0228359699249268
I0204 04:50:51.396525 139702543816448 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.8435722589492798, loss=1.9014410972595215
I0204 04:51:37.729132 139702527031040 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.6629242897033691, loss=2.731567621231079
I0204 04:51:58.971270 139863983413056 spec.py:321] Evaluating on the training split.
I0204 04:52:09.714226 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 04:52:43.892526 139863983413056 spec.py:349] Evaluating on the test split.
I0204 04:52:45.529870 139863983413056 submission_runner.py:408] Time since start: 63268.41s, 	Step: 124648, 	{'train/accuracy': 0.7625976204872131, 'train/loss': 0.9537436366081238, 'validation/accuracy': 0.6960600018501282, 'validation/loss': 1.2512370347976685, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 1.9025684595108032, 'test/num_examples': 10000, 'score': 57185.08319354057, 'total_duration': 63268.40723109245, 'accumulated_submission_time': 57185.08319354057, 'accumulated_eval_time': 6071.47722363472, 'accumulated_logging_time': 5.257215738296509}
I0204 04:52:45.568931 139702543816448 logging_writer.py:48] [124648] accumulated_eval_time=6071.477224, accumulated_logging_time=5.257216, accumulated_submission_time=57185.083194, global_step=124648, preemption_count=0, score=57185.083194, test/accuracy=0.571900, test/loss=1.902568, test/num_examples=10000, total_duration=63268.407231, train/accuracy=0.762598, train/loss=0.953744, validation/accuracy=0.696060, validation/loss=1.251237, validation/num_examples=50000
I0204 04:53:07.651593 139702527031040 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.7669270038604736, loss=2.0459673404693604
I0204 04:53:52.412978 139702543816448 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.9320203065872192, loss=1.8838492631912231
I0204 04:54:38.838097 139702527031040 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.0691428184509277, loss=1.8794018030166626
I0204 04:55:25.082256 139702543816448 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.632867693901062, loss=2.986555576324463
I0204 04:56:11.585726 139702527031040 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.9160057306289673, loss=1.9009822607040405
I0204 04:56:57.785157 139702543816448 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.8166998624801636, loss=1.944925308227539
I0204 04:57:43.891908 139702527031040 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.0914881229400635, loss=1.9156922101974487
I0204 04:58:30.250877 139702543816448 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.8337677717208862, loss=2.379237413406372
I0204 04:59:16.428552 139702527031040 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.664019227027893, loss=4.154637813568115
I0204 04:59:46.031029 139863983413056 spec.py:321] Evaluating on the training split.
I0204 04:59:56.549639 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 05:00:28.402954 139863983413056 spec.py:349] Evaluating on the test split.
I0204 05:00:30.033803 139863983413056 submission_runner.py:408] Time since start: 63732.91s, 	Step: 125566, 	{'train/accuracy': 0.7717187404632568, 'train/loss': 0.9368151426315308, 'validation/accuracy': 0.6967399716377258, 'validation/loss': 1.2695817947387695, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 1.9237370491027832, 'test/num_examples': 10000, 'score': 57605.488095998764, 'total_duration': 63732.91115617752, 'accumulated_submission_time': 57605.488095998764, 'accumulated_eval_time': 6115.479986190796, 'accumulated_logging_time': 5.305594205856323}
I0204 05:00:30.074047 139702543816448 logging_writer.py:48] [125566] accumulated_eval_time=6115.479986, accumulated_logging_time=5.305594, accumulated_submission_time=57605.488096, global_step=125566, preemption_count=0, score=57605.488096, test/accuracy=0.568800, test/loss=1.923737, test/num_examples=10000, total_duration=63732.911156, train/accuracy=0.771719, train/loss=0.936815, validation/accuracy=0.696740, validation/loss=1.269582, validation/num_examples=50000
I0204 05:00:44.661090 139702527031040 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.7914798259735107, loss=1.8355727195739746
I0204 05:01:28.478264 139702543816448 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.8069109916687012, loss=3.4402518272399902
I0204 05:02:14.719845 139702527031040 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.806356430053711, loss=2.1388893127441406
I0204 05:03:00.979224 139702543816448 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.784458041191101, loss=2.964118719100952
I0204 05:03:47.243108 139702527031040 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.6800837516784668, loss=3.6025168895721436
I0204 05:04:33.392952 139702543816448 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.977901816368103, loss=1.9870574474334717
I0204 05:05:19.666602 139702527031040 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.7950608730316162, loss=1.9649362564086914
I0204 05:06:05.872794 139702543816448 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.2256522178649902, loss=2.020817995071411
I0204 05:06:52.333135 139702527031040 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.6641039848327637, loss=3.9785752296447754
I0204 05:07:30.481062 139863983413056 spec.py:321] Evaluating on the training split.
I0204 05:07:40.936869 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 05:08:12.780333 139863983413056 spec.py:349] Evaluating on the test split.
I0204 05:08:14.437990 139863983413056 submission_runner.py:408] Time since start: 64197.32s, 	Step: 126484, 	{'train/accuracy': 0.7656054496765137, 'train/loss': 0.9447650909423828, 'validation/accuracy': 0.6959199905395508, 'validation/loss': 1.2564334869384766, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.9149582386016846, 'test/num_examples': 10000, 'score': 58025.8363161087, 'total_duration': 64197.31534385681, 'accumulated_submission_time': 58025.8363161087, 'accumulated_eval_time': 6159.436917304993, 'accumulated_logging_time': 5.356076002120972}
I0204 05:08:14.481327 139702543816448 logging_writer.py:48] [126484] accumulated_eval_time=6159.436917, accumulated_logging_time=5.356076, accumulated_submission_time=58025.836316, global_step=126484, preemption_count=0, score=58025.836316, test/accuracy=0.569600, test/loss=1.914958, test/num_examples=10000, total_duration=64197.315344, train/accuracy=0.765605, train/loss=0.944765, validation/accuracy=0.695920, validation/loss=1.256433, validation/num_examples=50000
I0204 05:08:21.562300 139702527031040 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.691198706626892, loss=2.5279791355133057
I0204 05:09:04.507513 139702543816448 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.7088439464569092, loss=1.8666834831237793
I0204 05:09:50.613517 139702527031040 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.863764762878418, loss=3.012345314025879
I0204 05:10:37.033272 139702543816448 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.801967978477478, loss=1.9317612648010254
I0204 05:11:23.362603 139702527031040 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.8853329420089722, loss=2.01581072807312
I0204 05:12:09.433467 139702543816448 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.7793471813201904, loss=1.900879144668579
I0204 05:12:55.502379 139702527031040 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.7861878871917725, loss=2.27372145652771
I0204 05:13:41.490742 139702543816448 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.146796226501465, loss=1.9355335235595703
I0204 05:14:27.607882 139702527031040 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.7227715253829956, loss=3.3156628608703613
I0204 05:15:13.948292 139702543816448 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.9175937175750732, loss=1.8999338150024414
I0204 05:15:14.566017 139863983413056 spec.py:321] Evaluating on the training split.
I0204 05:15:24.931187 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 05:15:58.734334 139863983413056 spec.py:349] Evaluating on the test split.
I0204 05:16:00.368218 139863983413056 submission_runner.py:408] Time since start: 64663.25s, 	Step: 127403, 	{'train/accuracy': 0.7616015672683716, 'train/loss': 0.9457040429115295, 'validation/accuracy': 0.696399986743927, 'validation/loss': 1.245165467262268, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 1.8830746412277222, 'test/num_examples': 10000, 'score': 58445.861562252045, 'total_duration': 64663.24557733536, 'accumulated_submission_time': 58445.861562252045, 'accumulated_eval_time': 6205.239112854004, 'accumulated_logging_time': 5.409669399261475}
I0204 05:16:00.403168 139702527031040 logging_writer.py:48] [127403] accumulated_eval_time=6205.239113, accumulated_logging_time=5.409669, accumulated_submission_time=58445.861562, global_step=127403, preemption_count=0, score=58445.861562, test/accuracy=0.574800, test/loss=1.883075, test/num_examples=10000, total_duration=64663.245577, train/accuracy=0.761602, train/loss=0.945704, validation/accuracy=0.696400, validation/loss=1.245165, validation/num_examples=50000
I0204 05:16:41.907013 139702543816448 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.7171274423599243, loss=2.4539055824279785
I0204 05:17:28.411384 139702527031040 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.9078037738800049, loss=2.0728602409362793
I0204 05:18:14.802370 139702543816448 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.084838628768921, loss=1.9454405307769775
I0204 05:19:00.873111 139702527031040 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.5785801410675049, loss=3.7313485145568848
I0204 05:19:47.202196 139702543816448 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.7923033237457275, loss=1.921183466911316
I0204 05:20:33.680070 139702527031040 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.7063391208648682, loss=2.0984432697296143
I0204 05:21:20.047749 139702543816448 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.7199174165725708, loss=2.1205828189849854
I0204 05:22:06.332494 139702527031040 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.7932451963424683, loss=2.2464637756347656
I0204 05:22:52.512824 139702543816448 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.982738733291626, loss=1.960281491279602
I0204 05:23:00.428426 139863983413056 spec.py:321] Evaluating on the training split.
I0204 05:23:10.994477 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 05:23:42.120537 139863983413056 spec.py:349] Evaluating on the test split.
I0204 05:23:43.757923 139863983413056 submission_runner.py:408] Time since start: 65126.64s, 	Step: 128313, 	{'train/accuracy': 0.7695116996765137, 'train/loss': 0.9001871347427368, 'validation/accuracy': 0.6969000101089478, 'validation/loss': 1.226536512374878, 'validation/num_examples': 50000, 'test/accuracy': 0.5778000354766846, 'test/loss': 1.8673149347305298, 'test/num_examples': 10000, 'score': 58865.828120946884, 'total_duration': 65126.63528132439, 'accumulated_submission_time': 58865.828120946884, 'accumulated_eval_time': 6248.568606853485, 'accumulated_logging_time': 5.454848766326904}
I0204 05:23:43.794595 139702527031040 logging_writer.py:48] [128313] accumulated_eval_time=6248.568607, accumulated_logging_time=5.454849, accumulated_submission_time=58865.828121, global_step=128313, preemption_count=0, score=58865.828121, test/accuracy=0.577800, test/loss=1.867315, test/num_examples=10000, total_duration=65126.635281, train/accuracy=0.769512, train/loss=0.900187, validation/accuracy=0.696900, validation/loss=1.226537, validation/num_examples=50000
I0204 05:24:20.508959 139702543816448 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.8155884742736816, loss=1.7893235683441162
I0204 05:25:06.564251 139702527031040 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.7940045595169067, loss=2.122800827026367
I0204 05:25:53.007341 139702543816448 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.835995078086853, loss=1.9773577451705933
I0204 05:26:39.166970 139702527031040 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.011089324951172, loss=2.1406474113464355
I0204 05:27:25.403630 139702543816448 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.2385122776031494, loss=1.976957082748413
I0204 05:28:11.909736 139702527031040 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.8436297178268433, loss=1.937916874885559
I0204 05:28:58.268293 139702543816448 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.8076400756835938, loss=4.426574230194092
I0204 05:29:44.426988 139702527031040 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.8237464427947998, loss=2.9035568237304688
I0204 05:30:30.860182 139702543816448 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.7202351093292236, loss=4.0233659744262695
I0204 05:30:43.930443 139863983413056 spec.py:321] Evaluating on the training split.
I0204 05:30:54.365497 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 05:31:23.667650 139863983413056 spec.py:349] Evaluating on the test split.
I0204 05:31:25.307317 139863983413056 submission_runner.py:408] Time since start: 65588.18s, 	Step: 129230, 	{'train/accuracy': 0.7834765315055847, 'train/loss': 0.878718376159668, 'validation/accuracy': 0.7016800045967102, 'validation/loss': 1.2358816862106323, 'validation/num_examples': 50000, 'test/accuracy': 0.5782999992370605, 'test/loss': 1.8833563327789307, 'test/num_examples': 10000, 'score': 59285.90663409233, 'total_duration': 65588.18467664719, 'accumulated_submission_time': 59285.90663409233, 'accumulated_eval_time': 6289.945489883423, 'accumulated_logging_time': 5.501060485839844}
I0204 05:31:25.346229 139702527031040 logging_writer.py:48] [129230] accumulated_eval_time=6289.945490, accumulated_logging_time=5.501060, accumulated_submission_time=59285.906634, global_step=129230, preemption_count=0, score=59285.906634, test/accuracy=0.578300, test/loss=1.883356, test/num_examples=10000, total_duration=65588.184677, train/accuracy=0.783477, train/loss=0.878718, validation/accuracy=0.701680, validation/loss=1.235882, validation/num_examples=50000
I0204 05:31:55.002470 139702543816448 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.7366269826889038, loss=2.228104591369629
I0204 05:32:40.480603 139702527031040 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.8639472723007202, loss=2.7698616981506348
I0204 05:33:26.844902 139702543816448 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.9860453605651855, loss=1.919921636581421
I0204 05:34:12.881563 139702527031040 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.0968241691589355, loss=1.8437825441360474
I0204 05:34:59.064971 139702543816448 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.7026125192642212, loss=2.2541818618774414
I0204 05:35:45.180822 139702527031040 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.0774402618408203, loss=4.214578151702881
I0204 05:36:31.578255 139702543816448 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.9469530582427979, loss=4.104004383087158
I0204 05:37:17.716410 139702527031040 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.9541022777557373, loss=3.4797821044921875
I0204 05:38:03.929305 139702543816448 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.8090885877609253, loss=4.278257846832275
I0204 05:38:25.622764 139863983413056 spec.py:321] Evaluating on the training split.
I0204 05:38:36.005012 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 05:39:09.279967 139863983413056 spec.py:349] Evaluating on the test split.
I0204 05:39:10.927238 139863983413056 submission_runner.py:408] Time since start: 66053.80s, 	Step: 130148, 	{'train/accuracy': 0.7674999833106995, 'train/loss': 0.9318538308143616, 'validation/accuracy': 0.7002800107002258, 'validation/loss': 1.227251648902893, 'validation/num_examples': 50000, 'test/accuracy': 0.576200008392334, 'test/loss': 1.8715360164642334, 'test/num_examples': 10000, 'score': 59706.12528705597, 'total_duration': 66053.80458760262, 'accumulated_submission_time': 59706.12528705597, 'accumulated_eval_time': 6335.249958515167, 'accumulated_logging_time': 5.549408435821533}
I0204 05:39:10.962752 139702527031040 logging_writer.py:48] [130148] accumulated_eval_time=6335.249959, accumulated_logging_time=5.549408, accumulated_submission_time=59706.125287, global_step=130148, preemption_count=0, score=59706.125287, test/accuracy=0.576200, test/loss=1.871536, test/num_examples=10000, total_duration=66053.804588, train/accuracy=0.767500, train/loss=0.931854, validation/accuracy=0.700280, validation/loss=1.227252, validation/num_examples=50000
I0204 05:39:33.046721 139702543816448 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.8610848188400269, loss=1.929817795753479
I0204 05:40:17.470729 139702527031040 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.8308193683624268, loss=2.32480788230896
I0204 05:41:03.688583 139702543816448 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.728240728378296, loss=2.514220952987671
I0204 05:41:50.009779 139702527031040 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.9953773021697998, loss=1.9169890880584717
I0204 05:42:36.135115 139702543816448 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.7846698760986328, loss=2.9543840885162354
I0204 05:43:22.400238 139702527031040 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.0344791412353516, loss=1.9417192935943604
I0204 05:44:08.658042 139702543816448 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.7878828048706055, loss=2.708395481109619
I0204 05:44:54.782768 139702527031040 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.8815789222717285, loss=1.8463917970657349
I0204 05:45:40.867521 139702543816448 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.903090476989746, loss=3.28104567527771
I0204 05:46:10.951609 139863983413056 spec.py:321] Evaluating on the training split.
I0204 05:46:21.381824 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 05:46:55.130674 139863983413056 spec.py:349] Evaluating on the test split.
I0204 05:46:56.779095 139863983413056 submission_runner.py:408] Time since start: 66519.66s, 	Step: 131067, 	{'train/accuracy': 0.7759179472923279, 'train/loss': 0.9067143797874451, 'validation/accuracy': 0.7016400098800659, 'validation/loss': 1.23651921749115, 'validation/num_examples': 50000, 'test/accuracy': 0.576200008392334, 'test/loss': 1.8704854249954224, 'test/num_examples': 10000, 'score': 60126.05570912361, 'total_duration': 66519.6564245224, 'accumulated_submission_time': 60126.05570912361, 'accumulated_eval_time': 6381.077441215515, 'accumulated_logging_time': 5.593918085098267}
I0204 05:46:56.825379 139702527031040 logging_writer.py:48] [131067] accumulated_eval_time=6381.077441, accumulated_logging_time=5.593918, accumulated_submission_time=60126.055709, global_step=131067, preemption_count=0, score=60126.055709, test/accuracy=0.576200, test/loss=1.870485, test/num_examples=10000, total_duration=66519.656425, train/accuracy=0.775918, train/loss=0.906714, validation/accuracy=0.701640, validation/loss=1.236519, validation/num_examples=50000
I0204 05:47:10.995814 139702543816448 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.9540573358535767, loss=1.8226367235183716
I0204 05:47:54.759933 139702527031040 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.779492974281311, loss=2.382322311401367
I0204 05:48:41.057420 139702543816448 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.6092637777328491, loss=3.02093505859375
I0204 05:49:27.508289 139702527031040 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.0746350288391113, loss=1.9603420495986938
I0204 05:50:13.779667 139702543816448 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.966855525970459, loss=2.2339859008789062
I0204 05:51:00.119224 139702527031040 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.0466251373291016, loss=2.184483289718628
I0204 05:51:46.485250 139702543816448 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.215341567993164, loss=1.9331938028335571
I0204 05:52:32.756350 139702527031040 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.9336696863174438, loss=1.9650442600250244
I0204 05:53:19.072993 139702543816448 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.7592586278915405, loss=3.22894549369812
I0204 05:53:56.996958 139863983413056 spec.py:321] Evaluating on the training split.
I0204 05:54:07.617309 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 05:54:41.058180 139863983413056 spec.py:349] Evaluating on the test split.
I0204 05:54:42.688469 139863983413056 submission_runner.py:408] Time since start: 66985.57s, 	Step: 131984, 	{'train/accuracy': 0.7800976634025574, 'train/loss': 0.899075448513031, 'validation/accuracy': 0.7061600089073181, 'validation/loss': 1.231971263885498, 'validation/num_examples': 50000, 'test/accuracy': 0.584600031375885, 'test/loss': 1.8611094951629639, 'test/num_examples': 10000, 'score': 60546.168536663055, 'total_duration': 66985.56582260132, 'accumulated_submission_time': 60546.168536663055, 'accumulated_eval_time': 6426.768949747086, 'accumulated_logging_time': 5.6505677700042725}
I0204 05:54:42.728788 139702527031040 logging_writer.py:48] [131984] accumulated_eval_time=6426.768950, accumulated_logging_time=5.650568, accumulated_submission_time=60546.168537, global_step=131984, preemption_count=0, score=60546.168537, test/accuracy=0.584600, test/loss=1.861109, test/num_examples=10000, total_duration=66985.565823, train/accuracy=0.780098, train/loss=0.899075, validation/accuracy=0.706160, validation/loss=1.231971, validation/num_examples=50000
I0204 05:54:49.810905 139702543816448 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.03814959526062, loss=1.726043462753296
I0204 05:55:33.274665 139702527031040 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.9096641540527344, loss=1.666003942489624
I0204 05:56:19.176687 139702543816448 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.835905909538269, loss=2.3526933193206787
I0204 05:57:05.329928 139702527031040 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.735632300376892, loss=3.2195980548858643
I0204 05:57:52.038715 139702543816448 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.1923422813415527, loss=1.8717724084854126
I0204 05:58:37.755077 139702527031040 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.9600155353546143, loss=2.092130661010742
I0204 05:59:23.975573 139702543816448 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.179863929748535, loss=1.8347439765930176
I0204 06:00:10.647502 139702527031040 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.9599601030349731, loss=4.345561981201172
I0204 06:00:56.554372 139702543816448 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.8500972986221313, loss=1.8251415491104126
I0204 06:01:42.641185 139702527031040 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.09153413772583, loss=1.857155442237854
I0204 06:01:42.791144 139863983413056 spec.py:321] Evaluating on the training split.
I0204 06:01:53.204026 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 06:02:24.180859 139863983413056 spec.py:349] Evaluating on the test split.
I0204 06:02:25.819144 139863983413056 submission_runner.py:408] Time since start: 67448.70s, 	Step: 132902, 	{'train/accuracy': 0.7770116925239563, 'train/loss': 0.8806419372558594, 'validation/accuracy': 0.7057600021362305, 'validation/loss': 1.1938964128494263, 'validation/num_examples': 50000, 'test/accuracy': 0.5800999999046326, 'test/loss': 1.8424324989318848, 'test/num_examples': 10000, 'score': 60966.1720366478, 'total_duration': 67448.69650387764, 'accumulated_submission_time': 60966.1720366478, 'accumulated_eval_time': 6469.7969336509705, 'accumulated_logging_time': 5.7013936042785645}
I0204 06:02:25.854293 139702543816448 logging_writer.py:48] [132902] accumulated_eval_time=6469.796934, accumulated_logging_time=5.701394, accumulated_submission_time=60966.172037, global_step=132902, preemption_count=0, score=60966.172037, test/accuracy=0.580100, test/loss=1.842432, test/num_examples=10000, total_duration=67448.696504, train/accuracy=0.777012, train/loss=0.880642, validation/accuracy=0.705760, validation/loss=1.193896, validation/num_examples=50000
I0204 06:03:07.709540 139702527031040 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.9386799335479736, loss=1.8173948526382446
I0204 06:03:53.587390 139702543816448 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.650015950202942, loss=2.662844657897949
I0204 06:04:39.655040 139702527031040 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.097285270690918, loss=2.8445441722869873
I0204 06:05:25.648699 139702543816448 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.591588020324707, loss=4.272257328033447
I0204 06:06:11.838252 139702527031040 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.146094799041748, loss=1.871406078338623
I0204 06:06:57.981650 139702543816448 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.1908798217773438, loss=1.8398749828338623
I0204 06:07:44.166503 139702527031040 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.4809892177581787, loss=1.7901833057403564
I0204 06:08:30.323749 139702543816448 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.786533236503601, loss=2.5883522033691406
I0204 06:09:16.586285 139702527031040 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.265589714050293, loss=1.8299949169158936
I0204 06:09:25.887377 139863983413056 spec.py:321] Evaluating on the training split.
I0204 06:09:36.286849 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 06:10:07.823184 139863983413056 spec.py:349] Evaluating on the test split.
I0204 06:10:09.458394 139863983413056 submission_runner.py:408] Time since start: 67912.34s, 	Step: 133822, 	{'train/accuracy': 0.7735351324081421, 'train/loss': 0.9415649771690369, 'validation/accuracy': 0.7021200060844421, 'validation/loss': 1.2566030025482178, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 1.9204665422439575, 'test/num_examples': 10000, 'score': 61386.14681506157, 'total_duration': 67912.33575248718, 'accumulated_submission_time': 61386.14681506157, 'accumulated_eval_time': 6513.367963075638, 'accumulated_logging_time': 5.74661111831665}
I0204 06:10:09.494708 139702543816448 logging_writer.py:48] [133822] accumulated_eval_time=6513.367963, accumulated_logging_time=5.746611, accumulated_submission_time=61386.146815, global_step=133822, preemption_count=0, score=61386.146815, test/accuracy=0.573000, test/loss=1.920467, test/num_examples=10000, total_duration=67912.335752, train/accuracy=0.773535, train/loss=0.941565, validation/accuracy=0.702120, validation/loss=1.256603, validation/num_examples=50000
I0204 06:10:42.452440 139702527031040 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.141359567642212, loss=2.3638200759887695
I0204 06:11:28.017394 139702543816448 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.1162431240081787, loss=1.9854345321655273
I0204 06:12:14.262490 139702527031040 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.779400110244751, loss=2.3574047088623047
I0204 06:13:00.187020 139702543816448 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.9036418199539185, loss=2.6495778560638428
I0204 06:13:46.491910 139702527031040 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.234661102294922, loss=1.6911824941635132
I0204 06:14:32.406028 139702543816448 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.8131401538848877, loss=4.1888275146484375
I0204 06:15:18.617684 139702527031040 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.941341757774353, loss=2.2851555347442627
I0204 06:16:04.816392 139702543816448 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.094219207763672, loss=1.9399679899215698
I0204 06:16:51.096406 139702527031040 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.8496559858322144, loss=3.0067806243896484
I0204 06:17:09.687698 139863983413056 spec.py:321] Evaluating on the training split.
I0204 06:17:20.016891 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 06:17:53.024697 139863983413056 spec.py:349] Evaluating on the test split.
I0204 06:17:54.670539 139863983413056 submission_runner.py:408] Time since start: 68377.55s, 	Step: 134742, 	{'train/accuracy': 0.7837499976158142, 'train/loss': 0.8793671727180481, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.2217317819595337, 'validation/num_examples': 50000, 'test/accuracy': 0.5799000263214111, 'test/loss': 1.8586112260818481, 'test/num_examples': 10000, 'score': 61806.27965426445, 'total_duration': 68377.54789853096, 'accumulated_submission_time': 61806.27965426445, 'accumulated_eval_time': 6558.35079741478, 'accumulated_logging_time': 5.793826341629028}
I0204 06:17:54.707140 139702543816448 logging_writer.py:48] [134742] accumulated_eval_time=6558.350797, accumulated_logging_time=5.793826, accumulated_submission_time=61806.279654, global_step=134742, preemption_count=0, score=61806.279654, test/accuracy=0.579900, test/loss=1.858611, test/num_examples=10000, total_duration=68377.547899, train/accuracy=0.783750, train/loss=0.879367, validation/accuracy=0.706420, validation/loss=1.221732, validation/num_examples=50000
I0204 06:18:19.321327 139702527031040 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.9425957202911377, loss=1.9764466285705566
I0204 06:19:04.321578 139702543816448 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.101693630218506, loss=1.7847167253494263
I0204 06:19:50.552004 139702527031040 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.0208818912506104, loss=3.208064079284668
I0204 06:20:36.769730 139702543816448 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.917737364768982, loss=1.9412144422531128
I0204 06:21:23.208239 139702527031040 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.8396252393722534, loss=3.956990957260132
I0204 06:22:09.505745 139702543816448 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.0486786365509033, loss=1.8784517049789429
I0204 06:22:55.811382 139702527031040 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.7660737037658691, loss=3.163565158843994
I0204 06:23:42.050153 139702543816448 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.9266825914382935, loss=4.165904521942139
I0204 06:24:28.308287 139702527031040 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.0678539276123047, loss=4.124741554260254
I0204 06:24:54.708649 139863983413056 spec.py:321] Evaluating on the training split.
I0204 06:25:05.309567 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 06:25:36.844904 139863983413056 spec.py:349] Evaluating on the test split.
I0204 06:25:38.499471 139863983413056 submission_runner.py:408] Time since start: 68841.38s, 	Step: 135659, 	{'train/accuracy': 0.7777929306030273, 'train/loss': 0.8910472393035889, 'validation/accuracy': 0.708579957485199, 'validation/loss': 1.2002291679382324, 'validation/num_examples': 50000, 'test/accuracy': 0.5830000042915344, 'test/loss': 1.8402303457260132, 'test/num_examples': 10000, 'score': 62226.222628593445, 'total_duration': 68841.37681627274, 'accumulated_submission_time': 62226.222628593445, 'accumulated_eval_time': 6602.141601085663, 'accumulated_logging_time': 5.841196537017822}
I0204 06:25:38.540184 139702543816448 logging_writer.py:48] [135659] accumulated_eval_time=6602.141601, accumulated_logging_time=5.841197, accumulated_submission_time=62226.222629, global_step=135659, preemption_count=0, score=62226.222629, test/accuracy=0.583000, test/loss=1.840230, test/num_examples=10000, total_duration=68841.376816, train/accuracy=0.777793, train/loss=0.891047, validation/accuracy=0.708580, validation/loss=1.200229, validation/num_examples=50000
I0204 06:25:56.053941 139702527031040 logging_writer.py:48] [135700] global_step=135700, grad_norm=2.0771095752716064, loss=1.755126714706421
I0204 06:26:40.185702 139702543816448 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.273916721343994, loss=1.8294904232025146
I0204 06:27:26.540986 139702527031040 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.2707479000091553, loss=1.7458164691925049
I0204 06:28:12.909202 139702543816448 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.9349474906921387, loss=2.0420451164245605
I0204 06:28:58.934076 139702527031040 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.031902551651001, loss=1.9256277084350586
I0204 06:29:45.382559 139702543816448 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.008669137954712, loss=1.827864170074463
I0204 06:30:31.702453 139702527031040 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.9264143705368042, loss=2.4078314304351807
I0204 06:31:18.388056 139702543816448 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.7594655752182007, loss=2.570753335952759
I0204 06:32:04.544612 139702527031040 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.1897854804992676, loss=1.823933720588684
I0204 06:32:38.661605 139863983413056 spec.py:321] Evaluating on the training split.
I0204 06:32:49.256267 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 06:33:22.278229 139863983413056 spec.py:349] Evaluating on the test split.
I0204 06:33:23.911505 139863983413056 submission_runner.py:408] Time since start: 69306.79s, 	Step: 136576, 	{'train/accuracy': 0.7836328148841858, 'train/loss': 0.8509073257446289, 'validation/accuracy': 0.7106199860572815, 'validation/loss': 1.1824514865875244, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8325586318969727, 'test/num_examples': 10000, 'score': 62646.28657245636, 'total_duration': 69306.7888610363, 'accumulated_submission_time': 62646.28657245636, 'accumulated_eval_time': 6647.391499996185, 'accumulated_logging_time': 5.891319513320923}
I0204 06:33:23.951045 139702543816448 logging_writer.py:48] [136576] accumulated_eval_time=6647.391500, accumulated_logging_time=5.891320, accumulated_submission_time=62646.286572, global_step=136576, preemption_count=0, score=62646.286572, test/accuracy=0.587800, test/loss=1.832559, test/num_examples=10000, total_duration=69306.788861, train/accuracy=0.783633, train/loss=0.850907, validation/accuracy=0.710620, validation/loss=1.182451, validation/num_examples=50000
I0204 06:33:34.384296 139702527031040 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.1595869064331055, loss=1.7055846452713013
I0204 06:34:17.855043 139702543816448 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.087123394012451, loss=1.819860816001892
I0204 06:35:03.921046 139702527031040 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.881892442703247, loss=2.413285732269287
I0204 06:35:50.052625 139702543816448 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.0755105018615723, loss=1.7721331119537354
I0204 06:36:36.092594 139702527031040 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.178635835647583, loss=1.764711856842041
I0204 06:37:22.464436 139702543816448 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.0045924186706543, loss=2.1095128059387207
I0204 06:38:08.687514 139702527031040 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.1083807945251465, loss=1.995194673538208
I0204 06:38:54.842929 139702543816448 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.1634230613708496, loss=4.207584857940674
I0204 06:39:41.030181 139702527031040 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.95342218875885, loss=3.346247673034668
I0204 06:40:24.298219 139863983413056 spec.py:321] Evaluating on the training split.
I0204 06:40:34.768648 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 06:41:03.818476 139863983413056 spec.py:349] Evaluating on the test split.
I0204 06:41:05.448687 139863983413056 submission_runner.py:408] Time since start: 69768.33s, 	Step: 137494, 	{'train/accuracy': 0.7882031202316284, 'train/loss': 0.8359939455986023, 'validation/accuracy': 0.7127199769020081, 'validation/loss': 1.1701191663742065, 'validation/num_examples': 50000, 'test/accuracy': 0.5861999988555908, 'test/loss': 1.8159235715866089, 'test/num_examples': 10000, 'score': 63066.573899030685, 'total_duration': 69768.32604432106, 'accumulated_submission_time': 63066.573899030685, 'accumulated_eval_time': 6688.541975975037, 'accumulated_logging_time': 5.941382884979248}
I0204 06:41:05.488565 139702543816448 logging_writer.py:48] [137494] accumulated_eval_time=6688.541976, accumulated_logging_time=5.941383, accumulated_submission_time=63066.573899, global_step=137494, preemption_count=0, score=63066.573899, test/accuracy=0.586200, test/loss=1.815924, test/num_examples=10000, total_duration=69768.326044, train/accuracy=0.788203, train/loss=0.835994, validation/accuracy=0.712720, validation/loss=1.170119, validation/num_examples=50000
I0204 06:41:08.404004 139702527031040 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.0430803298950195, loss=1.7129236459732056
I0204 06:41:51.119244 139702543816448 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.7934223413467407, loss=2.1265227794647217
I0204 06:42:37.446712 139702527031040 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.0373165607452393, loss=3.240727424621582
I0204 06:43:23.651701 139702543816448 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.8839242458343506, loss=2.2738943099975586
I0204 06:44:10.113596 139702527031040 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.9826390743255615, loss=2.2579123973846436
I0204 06:44:56.218444 139702543816448 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.098543405532837, loss=1.9657803773880005
I0204 06:45:42.325692 139702527031040 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.3601512908935547, loss=2.8733115196228027
I0204 06:46:28.515227 139702543816448 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.1402804851531982, loss=1.764719009399414
I0204 06:47:14.682392 139702527031040 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.546576976776123, loss=1.7899703979492188
I0204 06:48:00.931028 139702543816448 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.047116756439209, loss=2.3417985439300537
I0204 06:48:05.475185 139863983413056 spec.py:321] Evaluating on the training split.
I0204 06:48:15.796639 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 06:48:51.134155 139863983413056 spec.py:349] Evaluating on the test split.
I0204 06:48:52.770061 139863983413056 submission_runner.py:408] Time since start: 70235.65s, 	Step: 138412, 	{'train/accuracy': 0.7888476252555847, 'train/loss': 0.8295444250106812, 'validation/accuracy': 0.7119199633598328, 'validation/loss': 1.1780586242675781, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.816422939300537, 'test/num_examples': 10000, 'score': 63486.502836704254, 'total_duration': 70235.64741063118, 'accumulated_submission_time': 63486.502836704254, 'accumulated_eval_time': 6735.836848020554, 'accumulated_logging_time': 5.990756034851074}
I0204 06:48:52.807627 139702527031040 logging_writer.py:48] [138412] accumulated_eval_time=6735.836848, accumulated_logging_time=5.990756, accumulated_submission_time=63486.502837, global_step=138412, preemption_count=0, score=63486.502837, test/accuracy=0.588800, test/loss=1.816423, test/num_examples=10000, total_duration=70235.647411, train/accuracy=0.788848, train/loss=0.829544, validation/accuracy=0.711920, validation/loss=1.178059, validation/num_examples=50000
I0204 06:49:29.967642 139702543816448 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.849618911743164, loss=2.813681125640869
I0204 06:50:16.070444 139702527031040 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.029820203781128, loss=1.6740696430206299
I0204 06:51:02.253929 139702543816448 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.1573193073272705, loss=4.074074745178223
I0204 06:51:48.355420 139702527031040 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.19484543800354, loss=1.8339236974716187
I0204 06:52:34.702407 139702543816448 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.009768009185791, loss=4.371685981750488
I0204 06:53:20.699639 139702527031040 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.137211322784424, loss=4.311813831329346
I0204 06:54:06.806173 139702543816448 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.1173269748687744, loss=1.7112271785736084
I0204 06:54:53.045910 139702527031040 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.0846593379974365, loss=1.7249795198440552
I0204 06:55:39.244266 139702543816448 logging_writer.py:48] [139300] global_step=139300, grad_norm=2.4984467029571533, loss=1.8209556341171265
I0204 06:55:53.137889 139863983413056 spec.py:321] Evaluating on the training split.
I0204 06:56:03.500966 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 06:56:37.450252 139863983413056 spec.py:349] Evaluating on the test split.
I0204 06:56:39.084070 139863983413056 submission_runner.py:408] Time since start: 70701.96s, 	Step: 139332, 	{'train/accuracy': 0.7895312309265137, 'train/loss': 0.8278987407684326, 'validation/accuracy': 0.7135799527168274, 'validation/loss': 1.1600850820541382, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.8050135374069214, 'test/num_examples': 10000, 'score': 63906.77509832382, 'total_duration': 70701.96142339706, 'accumulated_submission_time': 63906.77509832382, 'accumulated_eval_time': 6781.7830266952515, 'accumulated_logging_time': 6.037896156311035}
I0204 06:56:39.121284 139702527031040 logging_writer.py:48] [139332] accumulated_eval_time=6781.783027, accumulated_logging_time=6.037896, accumulated_submission_time=63906.775098, global_step=139332, preemption_count=0, score=63906.775098, test/accuracy=0.591800, test/loss=1.805014, test/num_examples=10000, total_duration=70701.961423, train/accuracy=0.789531, train/loss=0.827899, validation/accuracy=0.713580, validation/loss=1.160085, validation/num_examples=50000
I0204 06:57:07.895051 139702543816448 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.091592788696289, loss=1.8406007289886475
I0204 06:57:53.179865 139702527031040 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.6093087196350098, loss=1.6469788551330566
I0204 06:58:39.395576 139702543816448 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.328331470489502, loss=1.7610548734664917
I0204 06:59:25.406436 139702527031040 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.1348612308502197, loss=1.7521437406539917
I0204 07:00:11.720486 139702543816448 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.9688271284103394, loss=3.9273922443389893
I0204 07:00:57.727357 139702527031040 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.2178473472595215, loss=3.282219171524048
I0204 07:01:43.941962 139702543816448 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.119878053665161, loss=1.8341315984725952
I0204 07:02:30.078631 139702527031040 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.856990098953247, loss=3.3299002647399902
I0204 07:03:16.552297 139702543816448 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.1801891326904297, loss=1.9141863584518433
I0204 07:03:39.208456 139863983413056 spec.py:321] Evaluating on the training split.
I0204 07:03:49.706477 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 07:04:21.387638 139863983413056 spec.py:349] Evaluating on the test split.
I0204 07:04:23.026258 139863983413056 submission_runner.py:408] Time since start: 71165.90s, 	Step: 140251, 	{'train/accuracy': 0.7918164134025574, 'train/loss': 0.8284945487976074, 'validation/accuracy': 0.7123599648475647, 'validation/loss': 1.1808110475540161, 'validation/num_examples': 50000, 'test/accuracy': 0.5911000370979309, 'test/loss': 1.8107106685638428, 'test/num_examples': 10000, 'score': 64326.802830696106, 'total_duration': 71165.90361714363, 'accumulated_submission_time': 64326.802830696106, 'accumulated_eval_time': 6825.600820064545, 'accumulated_logging_time': 6.085220098495483}
I0204 07:04:23.067137 139702527031040 logging_writer.py:48] [140251] accumulated_eval_time=6825.600820, accumulated_logging_time=6.085220, accumulated_submission_time=64326.802831, global_step=140251, preemption_count=0, score=64326.802831, test/accuracy=0.591100, test/loss=1.810711, test/num_examples=10000, total_duration=71165.903617, train/accuracy=0.791816, train/loss=0.828495, validation/accuracy=0.712360, validation/loss=1.180811, validation/num_examples=50000
I0204 07:04:43.912949 139702543816448 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.100236177444458, loss=1.9380714893341064
I0204 07:05:28.368291 139702527031040 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.0004992485046387, loss=4.207367897033691
I0204 07:06:14.762651 139702543816448 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.2788937091827393, loss=2.023434638977051
I0204 07:07:01.020893 139702527031040 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.984562635421753, loss=3.383141040802002
I0204 07:07:47.258739 139702543816448 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.9584277868270874, loss=4.192487716674805
I0204 07:08:33.551653 139702527031040 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.1821651458740234, loss=1.887270212173462
I0204 07:09:19.791622 139702543816448 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.1429450511932373, loss=1.7994441986083984
I0204 07:10:06.052762 139702527031040 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.0781424045562744, loss=1.7914917469024658
I0204 07:10:52.045704 139702543816448 logging_writer.py:48] [141100] global_step=141100, grad_norm=2.100506544113159, loss=1.8314988613128662
I0204 07:11:23.237908 139863983413056 spec.py:321] Evaluating on the training split.
I0204 07:11:33.640538 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 07:12:09.070879 139863983413056 spec.py:349] Evaluating on the test split.
I0204 07:12:10.716454 139863983413056 submission_runner.py:408] Time since start: 71633.59s, 	Step: 141169, 	{'train/accuracy': 0.8020898103713989, 'train/loss': 0.7641717195510864, 'validation/accuracy': 0.714199960231781, 'validation/loss': 1.146799087524414, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.7914958000183105, 'test/num_examples': 10000, 'score': 64746.913219451904, 'total_duration': 71633.59379124641, 'accumulated_submission_time': 64746.913219451904, 'accumulated_eval_time': 6873.079336643219, 'accumulated_logging_time': 6.138157367706299}
I0204 07:12:10.758323 139702527031040 logging_writer.py:48] [141169] accumulated_eval_time=6873.079337, accumulated_logging_time=6.138157, accumulated_submission_time=64746.913219, global_step=141169, preemption_count=0, score=64746.913219, test/accuracy=0.592700, test/loss=1.791496, test/num_examples=10000, total_duration=71633.593791, train/accuracy=0.802090, train/loss=0.764172, validation/accuracy=0.714200, validation/loss=1.146799, validation/num_examples=50000
I0204 07:12:24.098691 139702543816448 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.828226089477539, loss=2.407893657684326
I0204 07:13:07.906197 139702527031040 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.074354410171509, loss=1.7383736371994019
I0204 07:13:54.556115 139702543816448 logging_writer.py:48] [141400] global_step=141400, grad_norm=4.198553085327148, loss=4.264314651489258
I0204 07:14:41.134614 139702527031040 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.30959415435791, loss=1.8032493591308594
I0204 07:15:27.315837 139702543816448 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.079786539077759, loss=1.6591355800628662
I0204 07:16:13.810700 139702527031040 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.0000219345092773, loss=1.8357274532318115
I0204 07:17:00.267430 139702543816448 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.265592336654663, loss=1.7682521343231201
I0204 07:17:46.710257 139702527031040 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.2711713314056396, loss=1.7657485008239746
I0204 07:18:33.019892 139702543816448 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.205246686935425, loss=1.7421575784683228
I0204 07:19:11.033485 139863983413056 spec.py:321] Evaluating on the training split.
I0204 07:19:21.553538 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 07:19:54.277409 139863983413056 spec.py:349] Evaluating on the test split.
I0204 07:19:55.927134 139863983413056 submission_runner.py:408] Time since start: 72098.80s, 	Step: 142084, 	{'train/accuracy': 0.7903515696525574, 'train/loss': 0.8418607115745544, 'validation/accuracy': 0.718459963798523, 'validation/loss': 1.1597744226455688, 'validation/num_examples': 50000, 'test/accuracy': 0.5960000157356262, 'test/loss': 1.8071895837783813, 'test/num_examples': 10000, 'score': 65167.130407333374, 'total_duration': 72098.80448961258, 'accumulated_submission_time': 65167.130407333374, 'accumulated_eval_time': 6917.972964763641, 'accumulated_logging_time': 6.189510822296143}
I0204 07:19:55.965467 139702527031040 logging_writer.py:48] [142084] accumulated_eval_time=6917.972965, accumulated_logging_time=6.189511, accumulated_submission_time=65167.130407, global_step=142084, preemption_count=0, score=65167.130407, test/accuracy=0.596000, test/loss=1.807190, test/num_examples=10000, total_duration=72098.804490, train/accuracy=0.790352, train/loss=0.841861, validation/accuracy=0.718460, validation/loss=1.159774, validation/num_examples=50000
I0204 07:20:03.042041 139702543816448 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.9392322301864624, loss=2.8725571632385254
I0204 07:20:46.135677 139702527031040 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.0410828590393066, loss=4.26609992980957
I0204 07:21:32.324152 139702543816448 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.196704626083374, loss=2.1387248039245605
I0204 07:22:18.644184 139702527031040 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.1394124031066895, loss=1.7509678602218628
I0204 07:23:05.058156 139702543816448 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.312051773071289, loss=3.7362682819366455
I0204 07:23:51.345658 139702527031040 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.3677661418914795, loss=4.001455783843994
I0204 07:24:38.024062 139702543816448 logging_writer.py:48] [142700] global_step=142700, grad_norm=1.982523798942566, loss=2.2678308486938477
I0204 07:25:24.291937 139702527031040 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.206221103668213, loss=4.203706741333008
I0204 07:26:10.925325 139702543816448 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.1649720668792725, loss=2.584122896194458
I0204 07:26:56.281951 139863983413056 spec.py:321] Evaluating on the training split.
I0204 07:27:06.825146 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 07:27:37.624896 139863983413056 spec.py:349] Evaluating on the test split.
I0204 07:27:39.265932 139863983413056 submission_runner.py:408] Time since start: 72562.14s, 	Step: 143000, 	{'train/accuracy': 0.7939453125, 'train/loss': 0.8365713953971863, 'validation/accuracy': 0.7152599692344666, 'validation/loss': 1.1836109161376953, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.820393681526184, 'test/num_examples': 10000, 'score': 65587.38845300674, 'total_duration': 72562.14328551292, 'accumulated_submission_time': 65587.38845300674, 'accumulated_eval_time': 6960.956964015961, 'accumulated_logging_time': 6.237833261489868}
I0204 07:27:39.305940 139702527031040 logging_writer.py:48] [143000] accumulated_eval_time=6960.956964, accumulated_logging_time=6.237833, accumulated_submission_time=65587.388453, global_step=143000, preemption_count=0, score=65587.388453, test/accuracy=0.596900, test/loss=1.820394, test/num_examples=10000, total_duration=72562.143286, train/accuracy=0.793945, train/loss=0.836571, validation/accuracy=0.715260, validation/loss=1.183611, validation/num_examples=50000
I0204 07:27:39.731231 139702543816448 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.2476885318756104, loss=2.2278711795806885
I0204 07:28:22.286771 139702527031040 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.19275164604187, loss=4.113480567932129
I0204 07:29:08.290292 139702543816448 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.123267889022827, loss=2.9706201553344727
I0204 07:29:54.853578 139702527031040 logging_writer.py:48] [143300] global_step=143300, grad_norm=2.162177324295044, loss=3.601639747619629
I0204 07:30:41.143255 139702543816448 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.040353775024414, loss=1.7911505699157715
I0204 07:31:27.243488 139702527031040 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.460068941116333, loss=1.7162582874298096
I0204 07:32:13.289002 139702543816448 logging_writer.py:48] [143600] global_step=143600, grad_norm=2.0959997177124023, loss=1.6666443347930908
I0204 07:32:59.523072 139702527031040 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.0011277198791504, loss=1.685706377029419
I0204 07:33:45.928320 139702543816448 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.4267678260803223, loss=4.160327911376953
I0204 07:34:32.140264 139702527031040 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.0261857509613037, loss=3.083528995513916
I0204 07:34:39.283037 139863983413056 spec.py:321] Evaluating on the training split.
I0204 07:34:49.666398 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 07:35:25.503667 139863983413056 spec.py:349] Evaluating on the test split.
I0204 07:35:27.140814 139863983413056 submission_runner.py:408] Time since start: 73030.02s, 	Step: 143917, 	{'train/accuracy': 0.8037304282188416, 'train/loss': 0.8105840682983398, 'validation/accuracy': 0.7167999744415283, 'validation/loss': 1.1841614246368408, 'validation/num_examples': 50000, 'test/accuracy': 0.5915000438690186, 'test/loss': 1.8317683935165405, 'test/num_examples': 10000, 'score': 66007.30506968498, 'total_duration': 73030.0181658268, 'accumulated_submission_time': 66007.30506968498, 'accumulated_eval_time': 7008.814731359482, 'accumulated_logging_time': 6.289177417755127}
I0204 07:35:27.179735 139702543816448 logging_writer.py:48] [143917] accumulated_eval_time=7008.814731, accumulated_logging_time=6.289177, accumulated_submission_time=66007.305070, global_step=143917, preemption_count=0, score=66007.305070, test/accuracy=0.591500, test/loss=1.831768, test/num_examples=10000, total_duration=73030.018166, train/accuracy=0.803730, train/loss=0.810584, validation/accuracy=0.716800, validation/loss=1.184161, validation/num_examples=50000
I0204 07:36:02.242026 139702527031040 logging_writer.py:48] [144000] global_step=144000, grad_norm=1.9976626634597778, loss=4.191102027893066
I0204 07:36:48.256461 139702543816448 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.061770439147949, loss=1.6896344423294067
I0204 07:37:34.474678 139702527031040 logging_writer.py:48] [144200] global_step=144200, grad_norm=1.96584153175354, loss=2.7989344596862793
I0204 07:38:20.762612 139702543816448 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.6384835243225098, loss=4.20346736907959
I0204 07:39:07.081756 139702527031040 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.027968406677246, loss=2.4501595497131348
I0204 07:39:53.474925 139702543816448 logging_writer.py:48] [144500] global_step=144500, grad_norm=1.9428642988204956, loss=3.3280701637268066
I0204 07:40:39.899759 139702527031040 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.9052703380584717, loss=2.440326690673828
I0204 07:41:26.273731 139702543816448 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.1175880432128906, loss=2.8104333877563477
I0204 07:42:12.427895 139702527031040 logging_writer.py:48] [144800] global_step=144800, grad_norm=1.9985206127166748, loss=2.2215635776519775
I0204 07:42:27.485631 139863983413056 spec.py:321] Evaluating on the training split.
I0204 07:42:38.082137 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 07:43:08.874303 139863983413056 spec.py:349] Evaluating on the test split.
I0204 07:43:10.522062 139863983413056 submission_runner.py:408] Time since start: 73493.40s, 	Step: 144834, 	{'train/accuracy': 0.7900585532188416, 'train/loss': 0.8303824663162231, 'validation/accuracy': 0.7147600054740906, 'validation/loss': 1.1643662452697754, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.8147989511489868, 'test/num_examples': 10000, 'score': 66427.55252289772, 'total_duration': 73493.39942002296, 'accumulated_submission_time': 66427.55252289772, 'accumulated_eval_time': 7051.8511662483215, 'accumulated_logging_time': 6.3378260135650635}
I0204 07:43:10.566455 139702543816448 logging_writer.py:48] [144834] accumulated_eval_time=7051.851166, accumulated_logging_time=6.337826, accumulated_submission_time=66427.552523, global_step=144834, preemption_count=0, score=66427.552523, test/accuracy=0.591000, test/loss=1.814799, test/num_examples=10000, total_duration=73493.399420, train/accuracy=0.790059, train/loss=0.830382, validation/accuracy=0.714760, validation/loss=1.164366, validation/num_examples=50000
I0204 07:43:38.515320 139702527031040 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.5043652057647705, loss=1.835485577583313
I0204 07:44:23.762797 139702543816448 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.149783134460449, loss=2.553546190261841
I0204 07:45:10.111125 139702527031040 logging_writer.py:48] [145100] global_step=145100, grad_norm=2.3753278255462646, loss=1.9603646993637085
I0204 07:45:56.231159 139702543816448 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.1145122051239014, loss=1.7194074392318726
I0204 07:46:42.411639 139702527031040 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.490264415740967, loss=4.163534164428711
I0204 07:47:28.456101 139702543816448 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.461437463760376, loss=1.7729828357696533
I0204 07:48:14.637334 139702527031040 logging_writer.py:48] [145500] global_step=145500, grad_norm=2.1822969913482666, loss=2.1999545097351074
I0204 07:49:00.729631 139702543816448 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.5045859813690186, loss=2.837928295135498
I0204 07:49:46.929168 139702527031040 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.3402111530303955, loss=1.6801378726959229
I0204 07:50:10.743892 139863983413056 spec.py:321] Evaluating on the training split.
I0204 07:50:21.269681 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 07:50:53.877810 139863983413056 spec.py:349] Evaluating on the test split.
I0204 07:50:55.521707 139863983413056 submission_runner.py:408] Time since start: 73958.40s, 	Step: 145753, 	{'train/accuracy': 0.8025780916213989, 'train/loss': 0.7942774295806885, 'validation/accuracy': 0.7219399809837341, 'validation/loss': 1.1464852094650269, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.7750864028930664, 'test/num_examples': 10000, 'score': 66847.67327642441, 'total_duration': 73958.39906048775, 'accumulated_submission_time': 66847.67327642441, 'accumulated_eval_time': 7096.628978729248, 'accumulated_logging_time': 6.390943288803101}
I0204 07:50:55.560505 139702543816448 logging_writer.py:48] [145753] accumulated_eval_time=7096.628979, accumulated_logging_time=6.390943, accumulated_submission_time=66847.673276, global_step=145753, preemption_count=0, score=66847.673276, test/accuracy=0.600600, test/loss=1.775086, test/num_examples=10000, total_duration=73958.399060, train/accuracy=0.802578, train/loss=0.794277, validation/accuracy=0.721940, validation/loss=1.146485, validation/num_examples=50000
I0204 07:51:15.563401 139702527031040 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.3007915019989014, loss=1.872727632522583
I0204 07:51:59.702542 139702543816448 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.3070430755615234, loss=4.156336784362793
I0204 07:52:45.981253 139702527031040 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.343573570251465, loss=1.7154141664505005
I0204 07:53:32.278735 139702543816448 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.2639265060424805, loss=1.687970757484436
I0204 07:54:18.587660 139702527031040 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.1855130195617676, loss=1.7114479541778564
I0204 07:55:04.823225 139702543816448 logging_writer.py:48] [146300] global_step=146300, grad_norm=2.273442268371582, loss=1.6747504472732544
I0204 07:55:51.427001 139702527031040 logging_writer.py:48] [146400] global_step=146400, grad_norm=2.0644609928131104, loss=2.8096468448638916
I0204 07:56:38.123744 139702543816448 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.550689458847046, loss=4.169613361358643
I0204 07:57:24.687487 139702527031040 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.1680054664611816, loss=1.6939343214035034
I0204 07:57:55.897148 139863983413056 spec.py:321] Evaluating on the training split.
I0204 07:58:06.471037 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 07:58:38.728859 139863983413056 spec.py:349] Evaluating on the test split.
I0204 07:58:40.372218 139863983413056 submission_runner.py:408] Time since start: 74423.25s, 	Step: 146669, 	{'train/accuracy': 0.8016601204872131, 'train/loss': 0.7725005149841309, 'validation/accuracy': 0.7207199931144714, 'validation/loss': 1.1352367401123047, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.7748686075210571, 'test/num_examples': 10000, 'score': 67267.95164108276, 'total_duration': 74423.24955821037, 'accumulated_submission_time': 67267.95164108276, 'accumulated_eval_time': 7141.1040625572205, 'accumulated_logging_time': 6.439411401748657}
I0204 07:58:40.419273 139702543816448 logging_writer.py:48] [146669] accumulated_eval_time=7141.104063, accumulated_logging_time=6.439411, accumulated_submission_time=67267.951641, global_step=146669, preemption_count=0, score=67267.951641, test/accuracy=0.600300, test/loss=1.774869, test/num_examples=10000, total_duration=74423.249558, train/accuracy=0.801660, train/loss=0.772501, validation/accuracy=0.720720, validation/loss=1.135237, validation/num_examples=50000
I0204 07:58:53.749276 139702527031040 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.6335606575012207, loss=1.7660940885543823
I0204 07:59:37.613692 139702543816448 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.3061940670013428, loss=4.155113220214844
I0204 08:00:24.220341 139702527031040 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.0557727813720703, loss=3.613088607788086
I0204 08:01:10.434982 139702543816448 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.2739853858947754, loss=2.6639604568481445
I0204 08:01:56.384321 139702527031040 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.07497239112854, loss=2.5797431468963623
I0204 08:02:42.466875 139702543816448 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.220062732696533, loss=1.6913498640060425
I0204 08:03:28.488818 139702527031040 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.150228261947632, loss=3.70647931098938
I0204 08:04:14.896983 139702543816448 logging_writer.py:48] [147400] global_step=147400, grad_norm=1.9861562252044678, loss=2.1203861236572266
I0204 08:05:01.066069 139702527031040 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.534477949142456, loss=3.6960818767547607
I0204 08:05:40.506852 139863983413056 spec.py:321] Evaluating on the training split.
I0204 08:05:50.833897 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 08:06:25.802159 139863983413056 spec.py:349] Evaluating on the test split.
I0204 08:06:27.444303 139863983413056 submission_runner.py:408] Time since start: 74890.32s, 	Step: 147587, 	{'train/accuracy': 0.7973241806030273, 'train/loss': 0.8021615147590637, 'validation/accuracy': 0.7207799553871155, 'validation/loss': 1.1381536722183228, 'validation/num_examples': 50000, 'test/accuracy': 0.5991000533103943, 'test/loss': 1.7693490982055664, 'test/num_examples': 10000, 'score': 67687.97954654694, 'total_duration': 74890.32166147232, 'accumulated_submission_time': 67687.97954654694, 'accumulated_eval_time': 7188.0415251255035, 'accumulated_logging_time': 6.497358798980713}
I0204 08:06:27.483739 139702543816448 logging_writer.py:48] [147587] accumulated_eval_time=7188.041525, accumulated_logging_time=6.497359, accumulated_submission_time=67687.979547, global_step=147587, preemption_count=0, score=67687.979547, test/accuracy=0.599100, test/loss=1.769349, test/num_examples=10000, total_duration=74890.321661, train/accuracy=0.797324, train/loss=0.802162, validation/accuracy=0.720780, validation/loss=1.138154, validation/num_examples=50000
I0204 08:06:33.319320 139702527031040 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.20632266998291, loss=3.9120593070983887
I0204 08:07:16.154473 139702543816448 logging_writer.py:48] [147700] global_step=147700, grad_norm=1.9731597900390625, loss=3.421627998352051
I0204 08:08:02.157140 139702527031040 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.4260506629943848, loss=4.068217754364014
I0204 08:08:48.407843 139702543816448 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.250828742980957, loss=1.6813766956329346
I0204 08:09:34.719498 139702527031040 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.3814520835876465, loss=1.7163233757019043
I0204 08:10:20.991509 139702543816448 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.229301691055298, loss=3.9021756649017334
I0204 08:11:07.257495 139702527031040 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.377969980239868, loss=1.7816836833953857
I0204 08:11:53.305401 139702543816448 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.48638916015625, loss=1.5875816345214844
I0204 08:12:39.439531 139702527031040 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.2244768142700195, loss=1.7389425039291382
I0204 08:13:25.534994 139702543816448 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.2820539474487305, loss=1.5701721906661987
I0204 08:13:27.482306 139863983413056 spec.py:321] Evaluating on the training split.
I0204 08:13:37.837594 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 08:14:10.857672 139863983413056 spec.py:349] Evaluating on the test split.
I0204 08:14:12.501217 139863983413056 submission_runner.py:408] Time since start: 75355.38s, 	Step: 148506, 	{'train/accuracy': 0.8033788800239563, 'train/loss': 0.7781947255134583, 'validation/accuracy': 0.7247799634933472, 'validation/loss': 1.1242923736572266, 'validation/num_examples': 50000, 'test/accuracy': 0.6051000356674194, 'test/loss': 1.7475770711898804, 'test/num_examples': 10000, 'score': 68107.91986322403, 'total_duration': 75355.3785700798, 'accumulated_submission_time': 68107.91986322403, 'accumulated_eval_time': 7233.060445070267, 'accumulated_logging_time': 6.546364784240723}
I0204 08:14:12.539546 139702527031040 logging_writer.py:48] [148506] accumulated_eval_time=7233.060445, accumulated_logging_time=6.546365, accumulated_submission_time=68107.919863, global_step=148506, preemption_count=0, score=68107.919863, test/accuracy=0.605100, test/loss=1.747577, test/num_examples=10000, total_duration=75355.378570, train/accuracy=0.803379, train/loss=0.778195, validation/accuracy=0.724780, validation/loss=1.124292, validation/num_examples=50000
I0204 08:14:52.559074 139702543816448 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.179624319076538, loss=3.517178535461426
I0204 08:15:38.568337 139702527031040 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.242917537689209, loss=3.348193883895874
I0204 08:16:25.130446 139702543816448 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.5211877822875977, loss=3.1776187419891357
I0204 08:17:11.481346 139702527031040 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.2144694328308105, loss=2.873384475708008
I0204 08:17:57.680015 139702543816448 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.0340158939361572, loss=3.8388161659240723
I0204 08:18:43.900500 139702527031040 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.245561122894287, loss=1.7486729621887207
I0204 08:19:30.265208 139702543816448 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.2214646339416504, loss=2.185450792312622
I0204 08:20:16.836888 139702527031040 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.500986337661743, loss=1.6522679328918457
I0204 08:21:03.104191 139702543816448 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.395002603530884, loss=1.7351504564285278
I0204 08:21:12.584199 139863983413056 spec.py:321] Evaluating on the training split.
I0204 08:21:23.019471 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 08:21:55.710404 139863983413056 spec.py:349] Evaluating on the test split.
I0204 08:21:57.352179 139863983413056 submission_runner.py:408] Time since start: 75820.23s, 	Step: 149422, 	{'train/accuracy': 0.8059765696525574, 'train/loss': 0.7608180046081543, 'validation/accuracy': 0.7228999733924866, 'validation/loss': 1.124770998954773, 'validation/num_examples': 50000, 'test/accuracy': 0.6030000448226929, 'test/loss': 1.751966118812561, 'test/num_examples': 10000, 'score': 68527.9065463543, 'total_duration': 75820.22951364517, 'accumulated_submission_time': 68527.9065463543, 'accumulated_eval_time': 7277.828405618668, 'accumulated_logging_time': 6.59432315826416}
I0204 08:21:57.399353 139702527031040 logging_writer.py:48] [149422] accumulated_eval_time=7277.828406, accumulated_logging_time=6.594323, accumulated_submission_time=68527.906546, global_step=149422, preemption_count=0, score=68527.906546, test/accuracy=0.603000, test/loss=1.751966, test/num_examples=10000, total_duration=75820.229514, train/accuracy=0.805977, train/loss=0.760818, validation/accuracy=0.722900, validation/loss=1.124771, validation/num_examples=50000
I0204 08:22:30.370281 139702543816448 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.12585711479187, loss=2.159015417098999
I0204 08:23:16.054559 139702527031040 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.3634390830993652, loss=1.5430513620376587
I0204 08:24:02.285817 139702543816448 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.28945255279541, loss=1.7291488647460938
I0204 08:24:48.369081 139702527031040 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.701718330383301, loss=1.694331407546997
I0204 08:25:34.487227 139702543816448 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.6605355739593506, loss=1.7599568367004395
I0204 08:26:20.544791 139702527031040 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.3148250579833984, loss=3.916060447692871
I0204 08:27:06.645301 139702543816448 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.294265031814575, loss=1.6778874397277832
I0204 08:27:53.197534 139702527031040 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.2554757595062256, loss=3.7811155319213867
I0204 08:28:39.379410 139702543816448 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.3954036235809326, loss=4.023458957672119
I0204 08:28:57.473883 139863983413056 spec.py:321] Evaluating on the training split.
I0204 08:29:07.856508 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 08:29:38.090413 139863983413056 spec.py:349] Evaluating on the test split.
I0204 08:29:39.732014 139863983413056 submission_runner.py:408] Time since start: 76282.61s, 	Step: 150341, 	{'train/accuracy': 0.818359375, 'train/loss': 0.733532190322876, 'validation/accuracy': 0.7276999950408936, 'validation/loss': 1.10873281955719, 'validation/num_examples': 50000, 'test/accuracy': 0.6095000505447388, 'test/loss': 1.739927053451538, 'test/num_examples': 10000, 'score': 68947.92167925835, 'total_duration': 76282.6093711853, 'accumulated_submission_time': 68947.92167925835, 'accumulated_eval_time': 7320.086533069611, 'accumulated_logging_time': 6.6522908210754395}
I0204 08:29:39.777222 139702527031040 logging_writer.py:48] [150341] accumulated_eval_time=7320.086533, accumulated_logging_time=6.652291, accumulated_submission_time=68947.921679, global_step=150341, preemption_count=0, score=68947.921679, test/accuracy=0.609500, test/loss=1.739927, test/num_examples=10000, total_duration=76282.609371, train/accuracy=0.818359, train/loss=0.733532, validation/accuracy=0.727700, validation/loss=1.108733, validation/num_examples=50000
I0204 08:30:04.790706 139702543816448 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.0735349655151367, loss=2.961109161376953
I0204 08:30:49.949584 139702527031040 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.5341362953186035, loss=1.611372709274292
I0204 08:31:36.246395 139702543816448 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.2928810119628906, loss=2.396636962890625
I0204 08:32:22.456352 139702527031040 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.713095188140869, loss=2.0254433155059814
I0204 08:33:08.701403 139702543816448 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.43460750579834, loss=1.6305029392242432
I0204 08:33:54.928045 139702527031040 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.4611740112304688, loss=1.7706843614578247
I0204 08:34:41.099382 139702543816448 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.1141510009765625, loss=2.4473538398742676
I0204 08:35:27.208794 139702527031040 logging_writer.py:48] [151100] global_step=151100, grad_norm=1.9856858253479004, loss=2.311246871948242
I0204 08:36:13.281027 139702543816448 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.2905540466308594, loss=1.5147141218185425
I0204 08:36:39.735955 139863983413056 spec.py:321] Evaluating on the training split.
I0204 08:36:50.016383 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 08:37:23.684086 139863983413056 spec.py:349] Evaluating on the test split.
I0204 08:37:25.321290 139863983413056 submission_runner.py:408] Time since start: 76748.20s, 	Step: 151259, 	{'train/accuracy': 0.8030859231948853, 'train/loss': 0.7721931338310242, 'validation/accuracy': 0.7271599769592285, 'validation/loss': 1.108335256576538, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.7397416830062866, 'test/num_examples': 10000, 'score': 69367.8230752945, 'total_duration': 76748.19864630699, 'accumulated_submission_time': 69367.8230752945, 'accumulated_eval_time': 7365.671882867813, 'accumulated_logging_time': 6.706914186477661}
I0204 08:37:25.362167 139702527031040 logging_writer.py:48] [151259] accumulated_eval_time=7365.671883, accumulated_logging_time=6.706914, accumulated_submission_time=69367.823075, global_step=151259, preemption_count=0, score=69367.823075, test/accuracy=0.609000, test/loss=1.739742, test/num_examples=10000, total_duration=76748.198646, train/accuracy=0.803086, train/loss=0.772193, validation/accuracy=0.727160, validation/loss=1.108335, validation/num_examples=50000
I0204 08:37:42.869284 139702543816448 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.316983938217163, loss=1.5721677541732788
I0204 08:38:27.389736 139702527031040 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.2861645221710205, loss=1.801149606704712
I0204 08:39:13.689805 139702543816448 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.3943607807159424, loss=3.7793235778808594
I0204 08:39:59.949871 139702527031040 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.3320224285125732, loss=1.6689887046813965
I0204 08:40:46.117640 139702543816448 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.295301914215088, loss=1.8686659336090088
I0204 08:41:32.177381 139702527031040 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.4202864170074463, loss=3.263422966003418
I0204 08:42:18.447490 139702543816448 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.7424795627593994, loss=3.8369498252868652
I0204 08:43:04.696983 139702527031040 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.242769718170166, loss=2.716628074645996
I0204 08:43:50.922855 139702543816448 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.4392173290252686, loss=1.6462748050689697
I0204 08:44:25.595859 139863983413056 spec.py:321] Evaluating on the training split.
I0204 08:44:36.166693 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 08:45:10.111277 139863983413056 spec.py:349] Evaluating on the test split.
I0204 08:45:11.755909 139863983413056 submission_runner.py:408] Time since start: 77214.63s, 	Step: 152177, 	{'train/accuracy': 0.8080468773841858, 'train/loss': 0.78282630443573, 'validation/accuracy': 0.7276399731636047, 'validation/loss': 1.1330397129058838, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.7696969509124756, 'test/num_examples': 10000, 'score': 69787.99724078178, 'total_duration': 77214.63326454163, 'accumulated_submission_time': 69787.99724078178, 'accumulated_eval_time': 7411.831959962845, 'accumulated_logging_time': 6.759262323379517}
I0204 08:45:11.795698 139702527031040 logging_writer.py:48] [152177] accumulated_eval_time=7411.831960, accumulated_logging_time=6.759262, accumulated_submission_time=69787.997241, global_step=152177, preemption_count=0, score=69787.997241, test/accuracy=0.603500, test/loss=1.769697, test/num_examples=10000, total_duration=77214.633265, train/accuracy=0.808047, train/loss=0.782826, validation/accuracy=0.727640, validation/loss=1.133040, validation/num_examples=50000
I0204 08:45:21.783961 139702543816448 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.977400779724121, loss=2.8026463985443115
I0204 08:46:05.128996 139702527031040 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.4414806365966797, loss=3.8144371509552
I0204 08:46:51.451342 139702543816448 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.9117395877838135, loss=3.8220276832580566
I0204 08:47:37.781094 139702527031040 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.644620895385742, loss=1.67997407913208
I0204 08:48:24.044217 139702543816448 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.610219717025757, loss=1.8182041645050049
I0204 08:49:10.375037 139702527031040 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.342332124710083, loss=1.8588535785675049
I0204 08:49:56.784410 139702543816448 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.2382850646972656, loss=2.061448574066162
I0204 08:50:43.071117 139702527031040 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.6096739768981934, loss=1.7723972797393799
I0204 08:51:29.246770 139702543816448 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.3619163036346436, loss=1.6051758527755737
I0204 08:52:11.893040 139863983413056 spec.py:321] Evaluating on the training split.
I0204 08:52:22.861965 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 08:52:55.241268 139863983413056 spec.py:349] Evaluating on the test split.
I0204 08:52:56.886871 139863983413056 submission_runner.py:408] Time since start: 77679.76s, 	Step: 153094, 	{'train/accuracy': 0.8205859065055847, 'train/loss': 0.7144888639450073, 'validation/accuracy': 0.7292799949645996, 'validation/loss': 1.1079001426696777, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7434462308883667, 'test/num_examples': 10000, 'score': 70208.0367231369, 'total_duration': 77679.76422834396, 'accumulated_submission_time': 70208.0367231369, 'accumulated_eval_time': 7456.8257756233215, 'accumulated_logging_time': 6.808479070663452}
I0204 08:52:56.930134 139702527031040 logging_writer.py:48] [153094] accumulated_eval_time=7456.825776, accumulated_logging_time=6.808479, accumulated_submission_time=70208.036723, global_step=153094, preemption_count=0, score=70208.036723, test/accuracy=0.607000, test/loss=1.743446, test/num_examples=10000, total_duration=77679.764228, train/accuracy=0.820586, train/loss=0.714489, validation/accuracy=0.729280, validation/loss=1.107900, validation/num_examples=50000
I0204 08:52:59.850204 139702543816448 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.593257188796997, loss=2.7234065532684326
I0204 08:53:42.557016 139702527031040 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.4047696590423584, loss=3.554976224899292
I0204 08:54:28.520468 139702543816448 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.2275850772857666, loss=2.533196210861206
I0204 08:55:14.890837 139702527031040 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.098940849304199, loss=2.9147982597351074
I0204 08:56:01.088379 139702543816448 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.4662749767303467, loss=2.7437219619750977
I0204 08:56:47.359688 139702527031040 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.4169692993164062, loss=1.7100311517715454
I0204 08:57:33.517882 139702543816448 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.9167215824127197, loss=1.5976924896240234
I0204 08:58:19.780401 139702527031040 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.6892709732055664, loss=1.5873315334320068
I0204 08:59:06.558196 139702543816448 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.2834837436676025, loss=1.9586464166641235
I0204 08:59:52.815579 139702527031040 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.517441511154175, loss=1.5893412828445435
I0204 08:59:57.160522 139863983413056 spec.py:321] Evaluating on the training split.
I0204 09:00:07.695332 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 09:00:41.735819 139863983413056 spec.py:349] Evaluating on the test split.
I0204 09:00:43.379941 139863983413056 submission_runner.py:408] Time since start: 78146.26s, 	Step: 154011, 	{'train/accuracy': 0.8125, 'train/loss': 0.7371184229850769, 'validation/accuracy': 0.7307400107383728, 'validation/loss': 1.0911295413970947, 'validation/num_examples': 50000, 'test/accuracy': 0.6048000454902649, 'test/loss': 1.7228014469146729, 'test/num_examples': 10000, 'score': 70628.20611071587, 'total_duration': 78146.25729894638, 'accumulated_submission_time': 70628.20611071587, 'accumulated_eval_time': 7503.045190811157, 'accumulated_logging_time': 6.8642542362213135}
I0204 09:00:43.422614 139702543816448 logging_writer.py:48] [154011] accumulated_eval_time=7503.045191, accumulated_logging_time=6.864254, accumulated_submission_time=70628.206111, global_step=154011, preemption_count=0, score=70628.206111, test/accuracy=0.604800, test/loss=1.722801, test/num_examples=10000, total_duration=78146.257299, train/accuracy=0.812500, train/loss=0.737118, validation/accuracy=0.730740, validation/loss=1.091130, validation/num_examples=50000
I0204 09:01:21.133280 139702527031040 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.2807106971740723, loss=1.6463661193847656
I0204 09:02:07.046002 139702543816448 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.1398885250091553, loss=1.6907169818878174
I0204 09:02:53.170404 139702527031040 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.534097194671631, loss=1.777777910232544
I0204 09:03:39.126787 139702543816448 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.3086390495300293, loss=2.1889054775238037
I0204 09:04:25.480535 139702527031040 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.7468066215515137, loss=3.803236484527588
I0204 09:05:11.810731 139702543816448 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.402200222015381, loss=1.668628215789795
I0204 09:05:57.826148 139702527031040 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.72825026512146, loss=1.4892427921295166
I0204 09:06:43.919945 139702543816448 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.3825759887695312, loss=2.249765634536743
I0204 09:07:30.077444 139702527031040 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.477083683013916, loss=2.019166946411133
I0204 09:07:43.639341 139863983413056 spec.py:321] Evaluating on the training split.
I0204 09:07:54.132591 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 09:08:28.304868 139863983413056 spec.py:349] Evaluating on the test split.
I0204 09:08:29.955883 139863983413056 submission_runner.py:408] Time since start: 78612.83s, 	Step: 154931, 	{'train/accuracy': 0.8117187023162842, 'train/loss': 0.7915002107620239, 'validation/accuracy': 0.7309199571609497, 'validation/loss': 1.1422063112258911, 'validation/num_examples': 50000, 'test/accuracy': 0.6079000234603882, 'test/loss': 1.7812516689300537, 'test/num_examples': 10000, 'score': 71048.3641808033, 'total_duration': 78612.83320403099, 'accumulated_submission_time': 71048.3641808033, 'accumulated_eval_time': 7549.361694574356, 'accumulated_logging_time': 6.916259765625}
I0204 09:08:30.002582 139702543816448 logging_writer.py:48] [154931] accumulated_eval_time=7549.361695, accumulated_logging_time=6.916260, accumulated_submission_time=71048.364181, global_step=154931, preemption_count=0, score=71048.364181, test/accuracy=0.607900, test/loss=1.781252, test/num_examples=10000, total_duration=78612.833204, train/accuracy=0.811719, train/loss=0.791500, validation/accuracy=0.730920, validation/loss=1.142206, validation/num_examples=50000
I0204 09:08:59.218994 139702527031040 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.5976274013519287, loss=3.9522645473480225
I0204 09:09:44.579026 139702543816448 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.47105073928833, loss=1.545881748199463
I0204 09:10:31.438077 139702527031040 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.162034273147583, loss=1.9555037021636963
I0204 09:11:17.807227 139702543816448 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.6410179138183594, loss=1.5871202945709229
I0204 09:12:03.862143 139702527031040 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.489532947540283, loss=2.792598247528076
I0204 09:12:50.072718 139702543816448 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.0666046142578125, loss=1.6281099319458008
I0204 09:13:36.196911 139702527031040 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.4565956592559814, loss=2.912121534347534
I0204 09:14:22.222718 139702543816448 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.684535503387451, loss=1.6518114805221558
I0204 09:15:08.104284 139702527031040 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.3472580909729004, loss=3.3703362941741943
I0204 09:15:30.323113 139863983413056 spec.py:321] Evaluating on the training split.
I0204 09:15:40.732890 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 09:16:11.781984 139863983413056 spec.py:349] Evaluating on the test split.
I0204 09:16:13.416150 139863983413056 submission_runner.py:408] Time since start: 79076.29s, 	Step: 155850, 	{'train/accuracy': 0.8199804425239563, 'train/loss': 0.7226928472518921, 'validation/accuracy': 0.7334399819374084, 'validation/loss': 1.1000254154205322, 'validation/num_examples': 50000, 'test/accuracy': 0.6098000407218933, 'test/loss': 1.736724853515625, 'test/num_examples': 10000, 'score': 71468.62477397919, 'total_duration': 79076.29350209236, 'accumulated_submission_time': 71468.62477397919, 'accumulated_eval_time': 7592.454738378525, 'accumulated_logging_time': 6.974083662033081}
I0204 09:16:13.458668 139702543816448 logging_writer.py:48] [155850] accumulated_eval_time=7592.454738, accumulated_logging_time=6.974084, accumulated_submission_time=71468.624774, global_step=155850, preemption_count=0, score=71468.624774, test/accuracy=0.609800, test/loss=1.736725, test/num_examples=10000, total_duration=79076.293502, train/accuracy=0.819980, train/loss=0.722693, validation/accuracy=0.733440, validation/loss=1.100025, validation/num_examples=50000
I0204 09:16:34.726580 139702527031040 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.4379019737243652, loss=2.6957478523254395
I0204 09:17:19.616863 139702543816448 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.4825222492218018, loss=1.6658368110656738
I0204 09:18:05.951232 139702527031040 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.5043163299560547, loss=3.393249988555908
I0204 09:18:51.976328 139702543816448 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.407954454421997, loss=2.8448469638824463
I0204 09:19:38.088663 139702527031040 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.0053532123565674, loss=3.0697999000549316
I0204 09:20:24.605606 139702543816448 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.7149078845977783, loss=3.0100574493408203
I0204 09:21:10.669542 139702527031040 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.614351272583008, loss=1.8314225673675537
I0204 09:21:56.779257 139702543816448 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.5897057056427, loss=1.6653157472610474
I0204 09:22:43.053381 139702527031040 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.3359596729278564, loss=1.7982977628707886
I0204 09:23:13.698327 139863983413056 spec.py:321] Evaluating on the training split.
I0204 09:23:24.010424 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 09:23:56.401534 139863983413056 spec.py:349] Evaluating on the test split.
I0204 09:23:58.044784 139863983413056 submission_runner.py:408] Time since start: 79540.92s, 	Step: 156768, 	{'train/accuracy': 0.8161718845367432, 'train/loss': 0.7212274074554443, 'validation/accuracy': 0.7339999675750732, 'validation/loss': 1.0783400535583496, 'validation/num_examples': 50000, 'test/accuracy': 0.6101000308990479, 'test/loss': 1.7127060890197754, 'test/num_examples': 10000, 'score': 71888.80693101883, 'total_duration': 79540.92209506035, 'accumulated_submission_time': 71888.80693101883, 'accumulated_eval_time': 7636.801145553589, 'accumulated_logging_time': 7.0256242752075195}
I0204 09:23:58.088405 139702543816448 logging_writer.py:48] [156768] accumulated_eval_time=7636.801146, accumulated_logging_time=7.025624, accumulated_submission_time=71888.806931, global_step=156768, preemption_count=0, score=71888.806931, test/accuracy=0.610100, test/loss=1.712706, test/num_examples=10000, total_duration=79540.922095, train/accuracy=0.816172, train/loss=0.721227, validation/accuracy=0.734000, validation/loss=1.078340, validation/num_examples=50000
I0204 09:24:11.844664 139702527031040 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.5928733348846436, loss=1.662132740020752
I0204 09:24:55.695876 139702543816448 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.6127026081085205, loss=3.6623616218566895
I0204 09:25:42.075115 139702527031040 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.66940975189209, loss=2.565959930419922
I0204 09:26:28.364019 139702543816448 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.7451138496398926, loss=1.6216691732406616
I0204 09:27:14.699014 139702527031040 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.406022071838379, loss=1.7418394088745117
I0204 09:28:00.768498 139702543816448 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.6118409633636475, loss=1.6389368772506714
I0204 09:28:47.222571 139702527031040 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.4478633403778076, loss=2.0393853187561035
I0204 09:29:33.142363 139702543816448 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.5437467098236084, loss=1.5373820066452026
I0204 09:30:19.264729 139702527031040 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.3480677604675293, loss=2.0173375606536865
I0204 09:30:58.260881 139863983413056 spec.py:321] Evaluating on the training split.
I0204 09:31:08.870142 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 09:31:42.340140 139863983413056 spec.py:349] Evaluating on the test split.
I0204 09:31:43.985424 139863983413056 submission_runner.py:408] Time since start: 80006.86s, 	Step: 157685, 	{'train/accuracy': 0.8190429210662842, 'train/loss': 0.7114618420600891, 'validation/accuracy': 0.7349599599838257, 'validation/loss': 1.075613021850586, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.7125824689865112, 'test/num_examples': 10000, 'score': 72308.92112541199, 'total_duration': 80006.86278057098, 'accumulated_submission_time': 72308.92112541199, 'accumulated_eval_time': 7682.525693893433, 'accumulated_logging_time': 7.078494548797607}
I0204 09:31:44.026436 139702543816448 logging_writer.py:48] [157685] accumulated_eval_time=7682.525694, accumulated_logging_time=7.078495, accumulated_submission_time=72308.921125, global_step=157685, preemption_count=0, score=72308.921125, test/accuracy=0.608000, test/loss=1.712582, test/num_examples=10000, total_duration=80006.862781, train/accuracy=0.819043, train/loss=0.711462, validation/accuracy=0.734960, validation/loss=1.075613, validation/num_examples=50000
I0204 09:31:50.692731 139702527031040 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.653745412826538, loss=1.552632212638855
I0204 09:32:33.455721 139702543816448 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.6221868991851807, loss=1.6802983283996582
I0204 09:33:19.667781 139702527031040 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.8148562908172607, loss=1.8598178625106812
I0204 09:34:06.434536 139702543816448 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.8609328269958496, loss=1.7593512535095215
I0204 09:34:52.528475 139702527031040 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.231077194213867, loss=1.5398930311203003
I0204 09:35:38.777501 139702543816448 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.4345757961273193, loss=1.8897438049316406
I0204 09:36:25.553673 139702527031040 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.631418466567993, loss=1.8384284973144531
I0204 09:37:11.953046 139702543816448 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.4051365852355957, loss=1.9590989351272583
I0204 09:37:58.418500 139702527031040 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.4849162101745605, loss=1.5421655178070068
I0204 09:38:44.070161 139863983413056 spec.py:321] Evaluating on the training split.
I0204 09:38:54.889922 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 09:39:27.288854 139863983413056 spec.py:349] Evaluating on the test split.
I0204 09:39:28.926610 139863983413056 submission_runner.py:408] Time since start: 80471.80s, 	Step: 158600, 	{'train/accuracy': 0.8244531154632568, 'train/loss': 0.6801031231880188, 'validation/accuracy': 0.7355999946594238, 'validation/loss': 1.0589427947998047, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.695249319076538, 'test/num_examples': 10000, 'score': 72728.9054980278, 'total_duration': 80471.80396866798, 'accumulated_submission_time': 72728.9054980278, 'accumulated_eval_time': 7727.382160902023, 'accumulated_logging_time': 7.130783319473267}
I0204 09:39:28.971646 139702543816448 logging_writer.py:48] [158600] accumulated_eval_time=7727.382161, accumulated_logging_time=7.130783, accumulated_submission_time=72728.905498, global_step=158600, preemption_count=0, score=72728.905498, test/accuracy=0.609600, test/loss=1.695249, test/num_examples=10000, total_duration=80471.803969, train/accuracy=0.824453, train/loss=0.680103, validation/accuracy=0.735600, validation/loss=1.058943, validation/num_examples=50000
I0204 09:39:29.398517 139702527031040 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.59816312789917, loss=1.6318190097808838
I0204 09:40:12.026959 139702543816448 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.46177339553833, loss=2.2360363006591797
I0204 09:40:58.134935 139702527031040 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.2837893962860107, loss=2.399142026901245
I0204 09:41:44.724299 139702543816448 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.5223608016967773, loss=1.5126677751541138
I0204 09:42:31.014765 139702527031040 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.6125710010528564, loss=2.9765779972076416
I0204 09:43:17.424475 139702543816448 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.5642757415771484, loss=3.2835166454315186
I0204 09:44:03.653171 139702527031040 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.500192403793335, loss=3.3344316482543945
I0204 09:44:49.995640 139702543816448 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.4792447090148926, loss=1.5847090482711792
I0204 09:45:36.228092 139702527031040 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.844378709793091, loss=3.5198192596435547
I0204 09:46:22.513494 139702543816448 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.4855809211730957, loss=2.1331870555877686
I0204 09:46:29.073824 139863983413056 spec.py:321] Evaluating on the training split.
I0204 09:46:39.500752 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 09:47:13.606400 139863983413056 spec.py:349] Evaluating on the test split.
I0204 09:47:15.240698 139863983413056 submission_runner.py:408] Time since start: 80938.12s, 	Step: 159516, 	{'train/accuracy': 0.8199218511581421, 'train/loss': 0.7040935158729553, 'validation/accuracy': 0.737559974193573, 'validation/loss': 1.0672333240509033, 'validation/num_examples': 50000, 'test/accuracy': 0.6140000224113464, 'test/loss': 1.6969444751739502, 'test/num_examples': 10000, 'score': 73148.94817018509, 'total_duration': 80938.11805319786, 'accumulated_submission_time': 73148.94817018509, 'accumulated_eval_time': 7773.549040794373, 'accumulated_logging_time': 7.186861276626587}
I0204 09:47:15.284480 139702527031040 logging_writer.py:48] [159516] accumulated_eval_time=7773.549041, accumulated_logging_time=7.186861, accumulated_submission_time=73148.948170, global_step=159516, preemption_count=0, score=73148.948170, test/accuracy=0.614000, test/loss=1.696944, test/num_examples=10000, total_duration=80938.118053, train/accuracy=0.819922, train/loss=0.704094, validation/accuracy=0.737560, validation/loss=1.067233, validation/num_examples=50000
I0204 09:47:50.771515 139702543816448 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.6888253688812256, loss=1.544690728187561
I0204 09:48:36.789088 139702527031040 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.562971830368042, loss=1.7756237983703613
I0204 09:49:23.303948 139702543816448 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.9632205963134766, loss=1.5167505741119385
I0204 09:50:09.612302 139702527031040 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.600008487701416, loss=1.567031979560852
I0204 09:50:55.835776 139702543816448 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.502992630004883, loss=1.7693564891815186
I0204 09:51:42.137249 139702527031040 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.64453387260437, loss=1.585459589958191
I0204 09:52:28.698759 139702543816448 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.549914836883545, loss=1.9680482149124146
I0204 09:53:14.898540 139702527031040 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.593496799468994, loss=1.5422347784042358
I0204 09:54:01.167531 139702543816448 logging_writer.py:48] [160400] global_step=160400, grad_norm=3.177403688430786, loss=1.605414628982544
I0204 09:54:15.372932 139863983413056 spec.py:321] Evaluating on the training split.
I0204 09:54:25.862079 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 09:54:59.313552 139863983413056 spec.py:349] Evaluating on the test split.
I0204 09:55:00.950136 139863983413056 submission_runner.py:408] Time since start: 81403.83s, 	Step: 160432, 	{'train/accuracy': 0.8233007788658142, 'train/loss': 0.6964321732521057, 'validation/accuracy': 0.7393199801445007, 'validation/loss': 1.0613622665405273, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.701716661453247, 'test/num_examples': 10000, 'score': 73568.97808933258, 'total_duration': 81403.82748365402, 'accumulated_submission_time': 73568.97808933258, 'accumulated_eval_time': 7819.126227378845, 'accumulated_logging_time': 7.240983247756958}
I0204 09:55:00.993298 139702527031040 logging_writer.py:48] [160432] accumulated_eval_time=7819.126227, accumulated_logging_time=7.240983, accumulated_submission_time=73568.978089, global_step=160432, preemption_count=0, score=73568.978089, test/accuracy=0.610700, test/loss=1.701717, test/num_examples=10000, total_duration=81403.827484, train/accuracy=0.823301, train/loss=0.696432, validation/accuracy=0.739320, validation/loss=1.061362, validation/num_examples=50000
I0204 09:55:29.773910 139702543816448 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.592100143432617, loss=3.322685718536377
I0204 09:56:14.906423 139702527031040 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.793936014175415, loss=3.279996871948242
I0204 09:57:01.325511 139702543816448 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.878528118133545, loss=2.889322280883789
I0204 09:57:47.573023 139702527031040 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.862955093383789, loss=3.5129282474517822
I0204 09:58:33.698925 139702543816448 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.519728660583496, loss=3.6189956665039062
I0204 09:59:19.953643 139702527031040 logging_writer.py:48] [161000] global_step=161000, grad_norm=3.0478649139404297, loss=1.9781649112701416
I0204 10:00:06.258882 139702543816448 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.7215464115142822, loss=1.522354245185852
I0204 10:00:52.674668 139702527031040 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.7441375255584717, loss=2.9694228172302246
I0204 10:01:39.008664 139702543816448 logging_writer.py:48] [161300] global_step=161300, grad_norm=3.802741527557373, loss=1.6422357559204102
I0204 10:02:01.316036 139863983413056 spec.py:321] Evaluating on the training split.
I0204 10:02:12.230449 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 10:02:42.340696 139863983413056 spec.py:349] Evaluating on the test split.
I0204 10:02:43.981450 139863983413056 submission_runner.py:408] Time since start: 81866.86s, 	Step: 161350, 	{'train/accuracy': 0.8231640458106995, 'train/loss': 0.701531708240509, 'validation/accuracy': 0.7382000088691711, 'validation/loss': 1.0777002573013306, 'validation/num_examples': 50000, 'test/accuracy': 0.6128000020980835, 'test/loss': 1.720652461051941, 'test/num_examples': 10000, 'score': 73989.23976898193, 'total_duration': 81866.85880875587, 'accumulated_submission_time': 73989.23976898193, 'accumulated_eval_time': 7861.79163479805, 'accumulated_logging_time': 7.296997785568237}
I0204 10:02:44.026780 139702527031040 logging_writer.py:48] [161350] accumulated_eval_time=7861.791635, accumulated_logging_time=7.296998, accumulated_submission_time=73989.239769, global_step=161350, preemption_count=0, score=73989.239769, test/accuracy=0.612800, test/loss=1.720652, test/num_examples=10000, total_duration=81866.858809, train/accuracy=0.823164, train/loss=0.701532, validation/accuracy=0.738200, validation/loss=1.077700, validation/num_examples=50000
I0204 10:03:05.302819 139702543816448 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.6415178775787354, loss=1.56065034866333
I0204 10:03:49.796910 139702527031040 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.883434295654297, loss=2.0831615924835205
I0204 10:04:36.245515 139702543816448 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.549696922302246, loss=1.4853649139404297
I0204 10:05:22.383677 139702527031040 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.602975368499756, loss=2.15921950340271
I0204 10:06:08.444708 139702543816448 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.9898924827575684, loss=4.017294406890869
I0204 10:06:54.644928 139702527031040 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.862314462661743, loss=1.797552466392517
I0204 10:07:40.926331 139702543816448 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.6439032554626465, loss=2.381852626800537
I0204 10:08:27.144816 139702527031040 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.6014742851257324, loss=3.3691117763519287
I0204 10:09:13.538834 139702543816448 logging_writer.py:48] [162200] global_step=162200, grad_norm=3.2056188583374023, loss=1.5435056686401367
I0204 10:09:44.002590 139863983413056 spec.py:321] Evaluating on the training split.
I0204 10:09:54.712985 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 10:10:27.906630 139863983413056 spec.py:349] Evaluating on the test split.
I0204 10:10:29.541968 139863983413056 submission_runner.py:408] Time since start: 82332.42s, 	Step: 162268, 	{'train/accuracy': 0.8293749690055847, 'train/loss': 0.6697791218757629, 'validation/accuracy': 0.7391200065612793, 'validation/loss': 1.0580641031265259, 'validation/num_examples': 50000, 'test/accuracy': 0.6132000088691711, 'test/loss': 1.7002379894256592, 'test/num_examples': 10000, 'score': 74409.15633821487, 'total_duration': 82332.41932559013, 'accumulated_submission_time': 74409.15633821487, 'accumulated_eval_time': 7907.331022977829, 'accumulated_logging_time': 7.35301947593689}
I0204 10:10:29.584248 139702527031040 logging_writer.py:48] [162268] accumulated_eval_time=7907.331023, accumulated_logging_time=7.353019, accumulated_submission_time=74409.156338, global_step=162268, preemption_count=0, score=74409.156338, test/accuracy=0.613200, test/loss=1.700238, test/num_examples=10000, total_duration=82332.419326, train/accuracy=0.829375, train/loss=0.669779, validation/accuracy=0.739120, validation/loss=1.058064, validation/num_examples=50000
I0204 10:10:43.333756 139702543816448 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.8235409259796143, loss=1.5949782133102417
I0204 10:11:27.089963 139702527031040 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.773134708404541, loss=1.444713830947876
I0204 10:12:13.249476 139702543816448 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.860271453857422, loss=1.54318106174469
I0204 10:12:59.466726 139702527031040 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.8093578815460205, loss=2.759840726852417
I0204 10:13:45.674119 139702543816448 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.440361499786377, loss=1.7503615617752075
I0204 10:14:31.689832 139702527031040 logging_writer.py:48] [162800] global_step=162800, grad_norm=3.820758104324341, loss=1.5206120014190674
I0204 10:15:17.747878 139702543816448 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.8950088024139404, loss=4.085947036743164
I0204 10:16:03.832027 139702527031040 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.628925085067749, loss=2.191683769226074
I0204 10:16:50.071055 139702543816448 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.757917642593384, loss=2.844801425933838
I0204 10:17:29.788716 139863983413056 spec.py:321] Evaluating on the training split.
I0204 10:17:40.405434 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 10:18:14.716665 139863983413056 spec.py:349] Evaluating on the test split.
I0204 10:18:16.358695 139863983413056 submission_runner.py:408] Time since start: 82799.24s, 	Step: 163188, 	{'train/accuracy': 0.8237695097923279, 'train/loss': 0.6979393362998962, 'validation/accuracy': 0.739579975605011, 'validation/loss': 1.063696026802063, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.7050611972808838, 'test/num_examples': 10000, 'score': 74829.30160307884, 'total_duration': 82799.23604655266, 'accumulated_submission_time': 74829.30160307884, 'accumulated_eval_time': 7953.901052236557, 'accumulated_logging_time': 7.404864072799683}
I0204 10:18:16.400360 139702527031040 logging_writer.py:48] [163188] accumulated_eval_time=7953.901052, accumulated_logging_time=7.404864, accumulated_submission_time=74829.301603, global_step=163188, preemption_count=0, score=74829.301603, test/accuracy=0.617000, test/loss=1.705061, test/num_examples=10000, total_duration=82799.236047, train/accuracy=0.823770, train/loss=0.697939, validation/accuracy=0.739580, validation/loss=1.063696, validation/num_examples=50000
I0204 10:18:21.811012 139702543816448 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.53187894821167, loss=1.7025225162506104
I0204 10:19:04.830496 139702527031040 logging_writer.py:48] [163300] global_step=163300, grad_norm=5.863553047180176, loss=1.5635385513305664
I0204 10:19:50.973240 139702543816448 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.7578673362731934, loss=1.5148398876190186
I0204 10:20:37.304278 139702527031040 logging_writer.py:48] [163500] global_step=163500, grad_norm=3.3666832447052, loss=1.4878474473953247
I0204 10:21:23.347386 139702543816448 logging_writer.py:48] [163600] global_step=163600, grad_norm=3.06947922706604, loss=1.4691660404205322
I0204 10:22:09.465150 139702527031040 logging_writer.py:48] [163700] global_step=163700, grad_norm=3.1626365184783936, loss=3.786093235015869
I0204 10:22:55.688705 139702543816448 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.518465042114258, loss=2.608593702316284
I0204 10:23:42.078221 139702527031040 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.63081693649292, loss=1.559959888458252
I0204 10:24:28.508670 139702543816448 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.532252788543701, loss=1.4160056114196777
I0204 10:25:14.565744 139702527031040 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.6659655570983887, loss=1.6570813655853271
I0204 10:25:16.503443 139863983413056 spec.py:321] Evaluating on the training split.
I0204 10:25:26.997870 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 10:26:00.315177 139863983413056 spec.py:349] Evaluating on the test split.
I0204 10:26:01.959271 139863983413056 submission_runner.py:408] Time since start: 83264.84s, 	Step: 164106, 	{'train/accuracy': 0.8280468583106995, 'train/loss': 0.6734815835952759, 'validation/accuracy': 0.7416799664497375, 'validation/loss': 1.0467333793640137, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.6836295127868652, 'test/num_examples': 10000, 'score': 75249.34499502182, 'total_duration': 83264.83662986755, 'accumulated_submission_time': 75249.34499502182, 'accumulated_eval_time': 7999.356866836548, 'accumulated_logging_time': 7.457672595977783}
I0204 10:26:02.000217 139702543816448 logging_writer.py:48] [164106] accumulated_eval_time=7999.356867, accumulated_logging_time=7.457673, accumulated_submission_time=75249.344995, global_step=164106, preemption_count=0, score=75249.344995, test/accuracy=0.617400, test/loss=1.683630, test/num_examples=10000, total_duration=83264.836630, train/accuracy=0.828047, train/loss=0.673482, validation/accuracy=0.741680, validation/loss=1.046733, validation/num_examples=50000
I0204 10:26:42.112607 139702527031040 logging_writer.py:48] [164200] global_step=164200, grad_norm=3.0343141555786133, loss=1.5763156414031982
I0204 10:27:28.206853 139702543816448 logging_writer.py:48] [164300] global_step=164300, grad_norm=3.21100115776062, loss=1.5698704719543457
I0204 10:28:14.287266 139702527031040 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.7646496295928955, loss=3.699413537979126
I0204 10:29:00.397283 139702543816448 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.5060017108917236, loss=1.5819339752197266
I0204 10:29:46.636350 139702527031040 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.821777820587158, loss=3.714900493621826
I0204 10:30:32.890929 139702543816448 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.698946475982666, loss=1.4857975244522095
I0204 10:31:19.021978 139702527031040 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.9729137420654297, loss=3.2535340785980225
I0204 10:32:05.352222 139702543816448 logging_writer.py:48] [164900] global_step=164900, grad_norm=3.0295352935791016, loss=1.501843810081482
I0204 10:32:51.740032 139702527031040 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.9902238845825195, loss=2.930687189102173
I0204 10:33:02.106997 139863983413056 spec.py:321] Evaluating on the training split.
I0204 10:33:12.538800 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 10:33:48.524803 139863983413056 spec.py:349] Evaluating on the test split.
I0204 10:33:50.165671 139863983413056 submission_runner.py:408] Time since start: 83733.04s, 	Step: 165024, 	{'train/accuracy': 0.8347070217132568, 'train/loss': 0.6491798162460327, 'validation/accuracy': 0.7430399656295776, 'validation/loss': 1.0401471853256226, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.6772472858428955, 'test/num_examples': 10000, 'score': 75669.39481902122, 'total_duration': 83733.0430316925, 'accumulated_submission_time': 75669.39481902122, 'accumulated_eval_time': 8047.415571212769, 'accumulated_logging_time': 7.507667779922485}
I0204 10:33:50.206834 139702543816448 logging_writer.py:48] [165024] accumulated_eval_time=8047.415571, accumulated_logging_time=7.507668, accumulated_submission_time=75669.394819, global_step=165024, preemption_count=0, score=75669.394819, test/accuracy=0.619100, test/loss=1.677247, test/num_examples=10000, total_duration=83733.043032, train/accuracy=0.834707, train/loss=0.649180, validation/accuracy=0.743040, validation/loss=1.040147, validation/num_examples=50000
I0204 10:34:22.312254 139702527031040 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.3218705654144287, loss=1.545292615890503
I0204 10:35:08.302491 139702543816448 logging_writer.py:48] [165200] global_step=165200, grad_norm=3.4298744201660156, loss=1.5451397895812988
I0204 10:35:54.482487 139702527031040 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.573800563812256, loss=3.2839784622192383
I0204 10:36:40.490604 139702543816448 logging_writer.py:48] [165400] global_step=165400, grad_norm=3.2031877040863037, loss=2.485166072845459
I0204 10:37:26.539814 139702527031040 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.6295888423919678, loss=1.8341095447540283
I0204 10:38:12.653297 139702543816448 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.862241268157959, loss=2.122375726699829
I0204 10:38:58.601571 139702527031040 logging_writer.py:48] [165700] global_step=165700, grad_norm=3.0256264209747314, loss=3.909404754638672
I0204 10:39:44.805630 139702543816448 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.7019360065460205, loss=2.6198394298553467
I0204 10:40:31.220143 139702527031040 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.932265520095825, loss=1.5910309553146362
I0204 10:40:50.181204 139863983413056 spec.py:321] Evaluating on the training split.
I0204 10:41:00.727159 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 10:41:35.272744 139863983413056 spec.py:349] Evaluating on the test split.
I0204 10:41:36.903185 139863983413056 submission_runner.py:408] Time since start: 84199.78s, 	Step: 165943, 	{'train/accuracy': 0.8305078148841858, 'train/loss': 0.6913408637046814, 'validation/accuracy': 0.7436800003051758, 'validation/loss': 1.0642290115356445, 'validation/num_examples': 50000, 'test/accuracy': 0.6195000410079956, 'test/loss': 1.6991186141967773, 'test/num_examples': 10000, 'score': 76089.31130671501, 'total_duration': 84199.780534029, 'accumulated_submission_time': 76089.31130671501, 'accumulated_eval_time': 8094.137541294098, 'accumulated_logging_time': 7.5582780838012695}
I0204 10:41:36.948827 139702543816448 logging_writer.py:48] [165943] accumulated_eval_time=8094.137541, accumulated_logging_time=7.558278, accumulated_submission_time=76089.311307, global_step=165943, preemption_count=0, score=76089.311307, test/accuracy=0.619500, test/loss=1.699119, test/num_examples=10000, total_duration=84199.780534, train/accuracy=0.830508, train/loss=0.691341, validation/accuracy=0.743680, validation/loss=1.064229, validation/num_examples=50000
I0204 10:42:01.161961 139702527031040 logging_writer.py:48] [166000] global_step=166000, grad_norm=3.145326852798462, loss=3.6489417552948
I0204 10:42:46.060544 139702543816448 logging_writer.py:48] [166100] global_step=166100, grad_norm=3.0558626651763916, loss=1.8479747772216797
I0204 10:43:32.228797 139702527031040 logging_writer.py:48] [166200] global_step=166200, grad_norm=3.214435577392578, loss=3.2746596336364746
I0204 10:44:18.132301 139702543816448 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.947575330734253, loss=1.545020580291748
I0204 10:45:04.601566 139702527031040 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.819532871246338, loss=2.766524076461792
I0204 10:45:50.891131 139702543816448 logging_writer.py:48] [166500] global_step=166500, grad_norm=3.0226120948791504, loss=3.2949206829071045
I0204 10:46:37.221665 139702527031040 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.8776326179504395, loss=3.5494885444641113
I0204 10:47:23.713507 139702543816448 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.755557060241699, loss=1.551128625869751
I0204 10:48:10.059079 139702527031040 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.82375431060791, loss=4.000555992126465
I0204 10:48:37.065393 139863983413056 spec.py:321] Evaluating on the training split.
I0204 10:48:47.678128 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 10:49:20.239240 139863983413056 spec.py:349] Evaluating on the test split.
I0204 10:49:21.877426 139863983413056 submission_runner.py:408] Time since start: 84664.75s, 	Step: 166860, 	{'train/accuracy': 0.83509761095047, 'train/loss': 0.6374510526657104, 'validation/accuracy': 0.7443199753761292, 'validation/loss': 1.0309221744537354, 'validation/num_examples': 50000, 'test/accuracy': 0.6202000379562378, 'test/loss': 1.6646264791488647, 'test/num_examples': 10000, 'score': 76509.37047219276, 'total_duration': 84664.75477600098, 'accumulated_submission_time': 76509.37047219276, 'accumulated_eval_time': 8138.949564218521, 'accumulated_logging_time': 7.613423585891724}
I0204 10:49:21.922783 139702543816448 logging_writer.py:48] [166860] accumulated_eval_time=8138.949564, accumulated_logging_time=7.613424, accumulated_submission_time=76509.370472, global_step=166860, preemption_count=0, score=76509.370472, test/accuracy=0.620200, test/loss=1.664626, test/num_examples=10000, total_duration=84664.754776, train/accuracy=0.835098, train/loss=0.637451, validation/accuracy=0.744320, validation/loss=1.030922, validation/num_examples=50000
I0204 10:49:39.005468 139702527031040 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.7180001735687256, loss=1.5809205770492554
I0204 10:50:23.704117 139702543816448 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.81312894821167, loss=1.421115517616272
I0204 10:51:09.708742 139702527031040 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.939491033554077, loss=3.099836826324463
I0204 10:51:55.725022 139702543816448 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.6656954288482666, loss=1.5107786655426025
I0204 10:52:41.883414 139702527031040 logging_writer.py:48] [167300] global_step=167300, grad_norm=3.5286285877227783, loss=1.523276925086975
I0204 10:53:27.981360 139702543816448 logging_writer.py:48] [167400] global_step=167400, grad_norm=3.4668545722961426, loss=1.4158306121826172
I0204 10:54:14.196945 139702527031040 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.5441651344299316, loss=2.004249095916748
I0204 10:55:00.420074 139702543816448 logging_writer.py:48] [167600] global_step=167600, grad_norm=3.025099515914917, loss=1.558809518814087
I0204 10:55:46.995006 139702527031040 logging_writer.py:48] [167700] global_step=167700, grad_norm=3.07720685005188, loss=3.891360282897949
I0204 10:56:22.277688 139863983413056 spec.py:321] Evaluating on the training split.
I0204 10:56:32.681622 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 10:57:06.360289 139863983413056 spec.py:349] Evaluating on the test split.
I0204 10:57:07.995631 139863983413056 submission_runner.py:408] Time since start: 85130.87s, 	Step: 167778, 	{'train/accuracy': 0.8340038657188416, 'train/loss': 0.6508774161338806, 'validation/accuracy': 0.7443599700927734, 'validation/loss': 1.0415518283843994, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.6723334789276123, 'test/num_examples': 10000, 'score': 76929.66366410255, 'total_duration': 85130.87297224998, 'accumulated_submission_time': 76929.66366410255, 'accumulated_eval_time': 8184.667482852936, 'accumulated_logging_time': 7.671286582946777}
I0204 10:57:08.039901 139702543816448 logging_writer.py:48] [167778] accumulated_eval_time=8184.667483, accumulated_logging_time=7.671287, accumulated_submission_time=76929.663664, global_step=167778, preemption_count=0, score=76929.663664, test/accuracy=0.623200, test/loss=1.672333, test/num_examples=10000, total_duration=85130.872972, train/accuracy=0.834004, train/loss=0.650877, validation/accuracy=0.744360, validation/loss=1.041552, validation/num_examples=50000
I0204 10:57:17.629899 139702527031040 logging_writer.py:48] [167800] global_step=167800, grad_norm=2.84566330909729, loss=1.5350362062454224
I0204 10:58:01.212229 139702543816448 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.7831358909606934, loss=2.622274398803711
I0204 10:58:47.400611 139702527031040 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.764791250228882, loss=1.4790904521942139
I0204 10:59:33.637447 139702543816448 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.5818192958831787, loss=1.4683018922805786
I0204 11:00:20.170614 139702527031040 logging_writer.py:48] [168200] global_step=168200, grad_norm=2.747292995452881, loss=1.5237672328948975
I0204 11:01:06.417682 139702543816448 logging_writer.py:48] [168300] global_step=168300, grad_norm=2.612567901611328, loss=1.7605695724487305
I0204 11:01:52.578509 139702527031040 logging_writer.py:48] [168400] global_step=168400, grad_norm=2.708162546157837, loss=2.069458484649658
I0204 11:02:38.777858 139702543816448 logging_writer.py:48] [168500] global_step=168500, grad_norm=2.7285711765289307, loss=1.468636393547058
I0204 11:03:25.249442 139702527031040 logging_writer.py:48] [168600] global_step=168600, grad_norm=3.4924018383026123, loss=1.846728801727295
I0204 11:04:08.021674 139863983413056 spec.py:321] Evaluating on the training split.
I0204 11:04:18.600413 139863983413056 spec.py:333] Evaluating on the validation split.
I0204 11:04:53.093655 139863983413056 spec.py:349] Evaluating on the test split.
I0204 11:04:54.734820 139863983413056 submission_runner.py:408] Time since start: 85597.61s, 	Step: 168694, 	{'train/accuracy': 0.8335155844688416, 'train/loss': 0.6408066153526306, 'validation/accuracy': 0.74617999792099, 'validation/loss': 1.0215263366699219, 'validation/num_examples': 50000, 'test/accuracy': 0.6210000514984131, 'test/loss': 1.6536414623260498, 'test/num_examples': 10000, 'score': 77349.58613371849, 'total_duration': 85597.61217308044, 'accumulated_submission_time': 77349.58613371849, 'accumulated_eval_time': 8231.38061952591, 'accumulated_logging_time': 7.726383686065674}
I0204 11:04:54.782373 139702543816448 logging_writer.py:48] [168694] accumulated_eval_time=8231.380620, accumulated_logging_time=7.726384, accumulated_submission_time=77349.586134, global_step=168694, preemption_count=0, score=77349.586134, test/accuracy=0.621000, test/loss=1.653641, test/num_examples=10000, total_duration=85597.612173, train/accuracy=0.833516, train/loss=0.640807, validation/accuracy=0.746180, validation/loss=1.021526, validation/num_examples=50000
I0204 11:04:57.699286 139702527031040 logging_writer.py:48] [168700] global_step=168700, grad_norm=2.707202672958374, loss=1.4381234645843506
I0204 11:05:40.411808 139702543816448 logging_writer.py:48] [168800] global_step=168800, grad_norm=3.103116035461426, loss=2.9152119159698486
I0204 11:06:26.684182 139702527031040 logging_writer.py:48] [168900] global_step=168900, grad_norm=3.369955062866211, loss=3.9116251468658447
I0204 11:07:13.424403 139702543816448 logging_writer.py:48] [169000] global_step=169000, grad_norm=2.959765672683716, loss=3.2364635467529297
I0204 11:07:45.347798 139702527031040 logging_writer.py:48] [169071] global_step=169071, preemption_count=0, score=77520.063144
I0204 11:07:45.993505 139863983413056 checkpoints.py:490] Saving checkpoint at step: 169071
I0204 11:07:47.133978 139863983413056 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_5/checkpoint_169071
I0204 11:07:47.150273 139863983413056 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_vit_jax/trial_5/checkpoint_169071.
I0204 11:07:48.056834 139863983413056 submission_runner.py:583] Tuning trial 5/5
I0204 11:07:48.057066 139863983413056 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0204 11:07:48.068151 139863983413056 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 39.676350355148315, 'total_duration': 68.48205900192261, 'accumulated_submission_time': 39.676350355148315, 'accumulated_eval_time': 28.80561327934265, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (851, {'train/accuracy': 0.034257810562849045, 'train/loss': 5.893809795379639, 'validation/accuracy': 0.027739999815821648, 'validation/loss': 5.958045482635498, 'validation/num_examples': 50000, 'test/accuracy': 0.023000001907348633, 'test/loss': 6.076109886169434, 'test/num_examples': 10000, 'score': 460.0015518665314, 'total_duration': 529.3016893863678, 'accumulated_submission_time': 460.0015518665314, 'accumulated_eval_time': 69.23552632331848, 'accumulated_logging_time': 0.01894378662109375, 'global_step': 851, 'preemption_count': 0}), (1762, {'train/accuracy': 0.07244140654802322, 'train/loss': 5.277771472930908, 'validation/accuracy': 0.06849999725818634, 'validation/loss': 5.329521656036377, 'validation/num_examples': 50000, 'test/accuracy': 0.05340000241994858, 'test/loss': 5.5414252281188965, 'test/num_examples': 10000, 'score': 879.9417262077332, 'total_duration': 993.3262569904327, 'accumulated_submission_time': 879.9417262077332, 'accumulated_eval_time': 113.24756622314453, 'accumulated_logging_time': 0.043344736099243164, 'global_step': 1762, 'preemption_count': 0}), (2680, {'train/accuracy': 0.11164062470197678, 'train/loss': 4.909753799438477, 'validation/accuracy': 0.10255999863147736, 'validation/loss': 4.9625959396362305, 'validation/num_examples': 50000, 'test/accuracy': 0.08150000125169754, 'test/loss': 5.237037658691406, 'test/num_examples': 10000, 'score': 1300.3221225738525, 'total_duration': 1457.4158778190613, 'accumulated_submission_time': 1300.3221225738525, 'accumulated_eval_time': 156.88109421730042, 'accumulated_logging_time': 0.06997036933898926, 'global_step': 2680, 'preemption_count': 0}), (3597, {'train/accuracy': 0.17169921100139618, 'train/loss': 4.300407409667969, 'validation/accuracy': 0.15811999142169952, 'validation/loss': 4.405792236328125, 'validation/num_examples': 50000, 'test/accuracy': 0.12030000239610672, 'test/loss': 4.7625908851623535, 'test/num_examples': 10000, 'score': 1720.324553012848, 'total_duration': 1917.7254054546356, 'accumulated_submission_time': 1720.324553012848, 'accumulated_eval_time': 197.1125226020813, 'accumulated_logging_time': 0.09761691093444824, 'global_step': 3597, 'preemption_count': 0}), (4512, {'train/accuracy': 0.2356249988079071, 'train/loss': 3.8305723667144775, 'validation/accuracy': 0.2140599936246872, 'validation/loss': 3.9363467693328857, 'validation/num_examples': 50000, 'test/accuracy': 0.16520000994205475, 'test/loss': 4.379038333892822, 'test/num_examples': 10000, 'score': 2140.311147928238, 'total_duration': 2382.7273893356323, 'accumulated_submission_time': 2140.311147928238, 'accumulated_eval_time': 242.04579830169678, 'accumulated_logging_time': 0.13115668296813965, 'global_step': 4512, 'preemption_count': 0}), (5427, {'train/accuracy': 0.26445311307907104, 'train/loss': 3.6391592025756836, 'validation/accuracy': 0.2471199929714203, 'validation/loss': 3.742400884628296, 'validation/num_examples': 50000, 'test/accuracy': 0.18620000779628754, 'test/loss': 4.206192493438721, 'test/num_examples': 10000, 'score': 2560.448935985565, 'total_duration': 2846.608163833618, 'accumulated_submission_time': 2560.448935985565, 'accumulated_eval_time': 285.71435475349426, 'accumulated_logging_time': 0.15716123580932617, 'global_step': 5427, 'preemption_count': 0}), (6344, {'train/accuracy': 0.3071679472923279, 'train/loss': 3.325169086456299, 'validation/accuracy': 0.27605998516082764, 'validation/loss': 3.493992328643799, 'validation/num_examples': 50000, 'test/accuracy': 0.21490001678466797, 'test/loss': 4.034944534301758, 'test/num_examples': 10000, 'score': 2980.7929248809814, 'total_duration': 3312.563648700714, 'accumulated_submission_time': 2980.7929248809814, 'accumulated_eval_time': 331.25124192237854, 'accumulated_logging_time': 0.1834251880645752, 'global_step': 6344, 'preemption_count': 0}), (7263, {'train/accuracy': 0.34326171875, 'train/loss': 3.0648884773254395, 'validation/accuracy': 0.3183799982070923, 'validation/loss': 3.202389717102051, 'validation/num_examples': 50000, 'test/accuracy': 0.24670001864433289, 'test/loss': 3.7745606899261475, 'test/num_examples': 10000, 'score': 3401.106611251831, 'total_duration': 3772.3128740787506, 'accumulated_submission_time': 3401.106611251831, 'accumulated_eval_time': 370.6109387874603, 'accumulated_logging_time': 0.21096396446228027, 'global_step': 7263, 'preemption_count': 0}), (8179, {'train/accuracy': 0.3716796934604645, 'train/loss': 2.930692672729492, 'validation/accuracy': 0.3402999937534332, 'validation/loss': 3.099444627761841, 'validation/num_examples': 50000, 'test/accuracy': 0.25950002670288086, 'test/loss': 3.666433095932007, 'test/num_examples': 10000, 'score': 3821.2906200885773, 'total_duration': 4238.481454372406, 'accumulated_submission_time': 3821.2906200885773, 'accumulated_eval_time': 416.5219187736511, 'accumulated_logging_time': 0.23738980293273926, 'global_step': 8179, 'preemption_count': 0}), (9097, {'train/accuracy': 0.3936523497104645, 'train/loss': 2.7883851528167725, 'validation/accuracy': 0.3587999939918518, 'validation/loss': 2.984739303588867, 'validation/num_examples': 50000, 'test/accuracy': 0.28550001978874207, 'test/loss': 3.554089307785034, 'test/num_examples': 10000, 'score': 4241.387323856354, 'total_duration': 4698.91641163826, 'accumulated_submission_time': 4241.387323856354, 'accumulated_eval_time': 456.7812805175781, 'accumulated_logging_time': 0.26792001724243164, 'global_step': 9097, 'preemption_count': 0}), (10015, {'train/accuracy': 0.4039062261581421, 'train/loss': 2.737269163131714, 'validation/accuracy': 0.3750399947166443, 'validation/loss': 2.905771493911743, 'validation/num_examples': 50000, 'test/accuracy': 0.28710001707077026, 'test/loss': 3.4907071590423584, 'test/num_examples': 10000, 'score': 4661.656677007675, 'total_duration': 5157.845973968506, 'accumulated_submission_time': 4661.656677007675, 'accumulated_eval_time': 495.365522146225, 'accumulated_logging_time': 0.29549193382263184, 'global_step': 10015, 'preemption_count': 0}), (10934, {'train/accuracy': 0.423164039850235, 'train/loss': 2.606679677963257, 'validation/accuracy': 0.3898399770259857, 'validation/loss': 2.784226894378662, 'validation/num_examples': 50000, 'test/accuracy': 0.2964000105857849, 'test/loss': 3.4158220291137695, 'test/num_examples': 10000, 'score': 5082.045745134354, 'total_duration': 5620.655026435852, 'accumulated_submission_time': 5082.045745134354, 'accumulated_eval_time': 537.7116749286652, 'accumulated_logging_time': 0.320845365524292, 'global_step': 10934, 'preemption_count': 0}), (11853, {'train/accuracy': 0.4403710961341858, 'train/loss': 2.523897886276245, 'validation/accuracy': 0.4095799922943115, 'validation/loss': 2.7042064666748047, 'validation/num_examples': 50000, 'test/accuracy': 0.31450000405311584, 'test/loss': 3.3171584606170654, 'test/num_examples': 10000, 'score': 5502.399238586426, 'total_duration': 6084.099897861481, 'accumulated_submission_time': 5502.399238586426, 'accumulated_eval_time': 580.7291917800903, 'accumulated_logging_time': 0.34621691703796387, 'global_step': 11853, 'preemption_count': 0}), (12770, {'train/accuracy': 0.47035154700279236, 'train/loss': 2.3473031520843506, 'validation/accuracy': 0.41335999965667725, 'validation/loss': 2.656049966812134, 'validation/num_examples': 50000, 'test/accuracy': 0.3165000081062317, 'test/loss': 3.3088340759277344, 'test/num_examples': 10000, 'score': 5922.57227897644, 'total_duration': 6547.986525058746, 'accumulated_submission_time': 5922.57227897644, 'accumulated_eval_time': 624.3629055023193, 'accumulated_logging_time': 0.3769981861114502, 'global_step': 12770, 'preemption_count': 0}), (13689, {'train/accuracy': 0.4599999785423279, 'train/loss': 2.3901679515838623, 'validation/accuracy': 0.42781999707221985, 'validation/loss': 2.5512709617614746, 'validation/num_examples': 50000, 'test/accuracy': 0.33740001916885376, 'test/loss': 3.1996219158172607, 'test/num_examples': 10000, 'score': 6342.573100805283, 'total_duration': 7010.78583407402, 'accumulated_submission_time': 6342.573100805283, 'accumulated_eval_time': 667.0851843357086, 'accumulated_logging_time': 0.40462446212768555, 'global_step': 13689, 'preemption_count': 0}), (14606, {'train/accuracy': 0.4755273461341858, 'train/loss': 2.356476306915283, 'validation/accuracy': 0.4366599917411804, 'validation/loss': 2.5436606407165527, 'validation/num_examples': 50000, 'test/accuracy': 0.3416000306606293, 'test/loss': 3.1861348152160645, 'test/num_examples': 10000, 'score': 6762.732401609421, 'total_duration': 7475.724648714066, 'accumulated_submission_time': 6762.732401609421, 'accumulated_eval_time': 711.7846443653107, 'accumulated_logging_time': 0.43683457374572754, 'global_step': 14606, 'preemption_count': 0}), (15523, {'train/accuracy': 0.49757811427116394, 'train/loss': 2.191378593444824, 'validation/accuracy': 0.44443997740745544, 'validation/loss': 2.4651525020599365, 'validation/num_examples': 50000, 'test/accuracy': 0.34470000863075256, 'test/loss': 3.0966248512268066, 'test/num_examples': 10000, 'score': 7182.700782775879, 'total_duration': 7939.937408447266, 'accumulated_submission_time': 7182.700782775879, 'accumulated_eval_time': 755.9537699222565, 'accumulated_logging_time': 0.4638805389404297, 'global_step': 15523, 'preemption_count': 0}), (16441, {'train/accuracy': 0.486152321100235, 'train/loss': 2.2722318172454834, 'validation/accuracy': 0.45513999462127686, 'validation/loss': 2.4447381496429443, 'validation/num_examples': 50000, 'test/accuracy': 0.3492000102996826, 'test/loss': 3.093480348587036, 'test/num_examples': 10000, 'score': 7602.853166103363, 'total_duration': 8405.024823188782, 'accumulated_submission_time': 7602.853166103363, 'accumulated_eval_time': 800.8106706142426, 'accumulated_logging_time': 0.4935455322265625, 'global_step': 16441, 'preemption_count': 0}), (17359, {'train/accuracy': 0.5013867020606995, 'train/loss': 2.1820547580718994, 'validation/accuracy': 0.4605799913406372, 'validation/loss': 2.3915188312530518, 'validation/num_examples': 50000, 'test/accuracy': 0.3612000048160553, 'test/loss': 3.0269052982330322, 'test/num_examples': 10000, 'score': 8023.229565858841, 'total_duration': 8865.672659635544, 'accumulated_submission_time': 8023.229565858841, 'accumulated_eval_time': 841.0064516067505, 'accumulated_logging_time': 0.5205333232879639, 'global_step': 17359, 'preemption_count': 0}), (18273, {'train/accuracy': 0.5225195288658142, 'train/loss': 2.098334789276123, 'validation/accuracy': 0.4728599786758423, 'validation/loss': 2.336740732192993, 'validation/num_examples': 50000, 'test/accuracy': 0.36230000853538513, 'test/loss': 3.014296293258667, 'test/num_examples': 10000, 'score': 8443.607513904572, 'total_duration': 9330.35705280304, 'accumulated_submission_time': 8443.607513904572, 'accumulated_eval_time': 885.2275066375732, 'accumulated_logging_time': 0.5578651428222656, 'global_step': 18273, 'preemption_count': 0}), (19190, {'train/accuracy': 0.5135741829872131, 'train/loss': 2.104053258895874, 'validation/accuracy': 0.47693997621536255, 'validation/loss': 2.3006768226623535, 'validation/num_examples': 50000, 'test/accuracy': 0.37050002813339233, 'test/loss': 2.9391238689422607, 'test/num_examples': 10000, 'score': 8863.76538324356, 'total_duration': 9794.63821029663, 'accumulated_submission_time': 8863.76538324356, 'accumulated_eval_time': 929.2668771743774, 'accumulated_logging_time': 0.5929629802703857, 'global_step': 19190, 'preemption_count': 0}), (20105, {'train/accuracy': 0.5260156393051147, 'train/loss': 2.0405845642089844, 'validation/accuracy': 0.4841799736022949, 'validation/loss': 2.2629568576812744, 'validation/num_examples': 50000, 'test/accuracy': 0.3801000118255615, 'test/loss': 2.9380943775177, 'test/num_examples': 10000, 'score': 9283.831866025925, 'total_duration': 10255.40760755539, 'accumulated_submission_time': 9283.831866025925, 'accumulated_eval_time': 969.8912837505341, 'accumulated_logging_time': 0.623236894607544, 'global_step': 20105, 'preemption_count': 0}), (21022, {'train/accuracy': 0.53369140625, 'train/loss': 2.0299012660980225, 'validation/accuracy': 0.4887999892234802, 'validation/loss': 2.2722764015197754, 'validation/num_examples': 50000, 'test/accuracy': 0.3781000077724457, 'test/loss': 2.9239919185638428, 'test/num_examples': 10000, 'score': 9703.953919887543, 'total_duration': 10718.971504211426, 'accumulated_submission_time': 9703.953919887543, 'accumulated_eval_time': 1013.2559487819672, 'accumulated_logging_time': 0.6527237892150879, 'global_step': 21022, 'preemption_count': 0}), (21939, {'train/accuracy': 0.5484570264816284, 'train/loss': 1.9116195440292358, 'validation/accuracy': 0.5009399652481079, 'validation/loss': 2.1616222858428955, 'validation/num_examples': 50000, 'test/accuracy': 0.3904000222682953, 'test/loss': 2.829202175140381, 'test/num_examples': 10000, 'score': 10124.150616884232, 'total_duration': 11180.589293718338, 'accumulated_submission_time': 10124.150616884232, 'accumulated_eval_time': 1054.5985553264618, 'accumulated_logging_time': 0.6823267936706543, 'global_step': 21939, 'preemption_count': 0}), (22852, {'train/accuracy': 0.5477929711341858, 'train/loss': 1.9292596578598022, 'validation/accuracy': 0.5063599944114685, 'validation/loss': 2.1407365798950195, 'validation/num_examples': 50000, 'test/accuracy': 0.3939000070095062, 'test/loss': 2.801138162612915, 'test/num_examples': 10000, 'score': 10544.085697889328, 'total_duration': 11644.321509361267, 'accumulated_submission_time': 10544.085697889328, 'accumulated_eval_time': 1098.3198697566986, 'accumulated_logging_time': 0.7099740505218506, 'global_step': 22852, 'preemption_count': 0}), (23768, {'train/accuracy': 0.5549609065055847, 'train/loss': 1.9112342596054077, 'validation/accuracy': 0.5130199790000916, 'validation/loss': 2.142449378967285, 'validation/num_examples': 50000, 'test/accuracy': 0.3993000090122223, 'test/loss': 2.8055648803710938, 'test/num_examples': 10000, 'score': 10964.318322658539, 'total_duration': 12105.310588121414, 'accumulated_submission_time': 10964.318322658539, 'accumulated_eval_time': 1138.9947321414948, 'accumulated_logging_time': 0.7432305812835693, 'global_step': 23768, 'preemption_count': 0}), (24687, {'train/accuracy': 0.5798242092132568, 'train/loss': 1.8216063976287842, 'validation/accuracy': 0.5144199728965759, 'validation/loss': 2.1380128860473633, 'validation/num_examples': 50000, 'test/accuracy': 0.39900001883506775, 'test/loss': 2.804219961166382, 'test/num_examples': 10000, 'score': 11384.320001840591, 'total_duration': 12568.191683530807, 'accumulated_submission_time': 11384.320001840591, 'accumulated_eval_time': 1181.7955298423767, 'accumulated_logging_time': 0.773090124130249, 'global_step': 24687, 'preemption_count': 0}), (25606, {'train/accuracy': 0.5553905963897705, 'train/loss': 1.961132287979126, 'validation/accuracy': 0.5149999856948853, 'validation/loss': 2.1607208251953125, 'validation/num_examples': 50000, 'test/accuracy': 0.4036000072956085, 'test/loss': 2.8055355548858643, 'test/num_examples': 10000, 'score': 11804.5945789814, 'total_duration': 13031.554756641388, 'accumulated_submission_time': 11804.5945789814, 'accumulated_eval_time': 1224.8035056591034, 'accumulated_logging_time': 0.8047606945037842, 'global_step': 25606, 'preemption_count': 0}), (26524, {'train/accuracy': 0.5643359422683716, 'train/loss': 1.8898110389709473, 'validation/accuracy': 0.5171599984169006, 'validation/loss': 2.1188228130340576, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.7826554775238037, 'test/num_examples': 10000, 'score': 12224.88507938385, 'total_duration': 13495.64342713356, 'accumulated_submission_time': 12224.88507938385, 'accumulated_eval_time': 1268.5251622200012, 'accumulated_logging_time': 0.8329160213470459, 'global_step': 26524, 'preemption_count': 0}), (27444, {'train/accuracy': 0.5781835913658142, 'train/loss': 1.8442374467849731, 'validation/accuracy': 0.5189599990844727, 'validation/loss': 2.119422197341919, 'validation/num_examples': 50000, 'test/accuracy': 0.4106000065803528, 'test/loss': 2.772930145263672, 'test/num_examples': 10000, 'score': 12644.999958276749, 'total_duration': 13959.151177167892, 'accumulated_submission_time': 12644.999958276749, 'accumulated_eval_time': 1311.8416454792023, 'accumulated_logging_time': 0.8607838153839111, 'global_step': 27444, 'preemption_count': 0}), (28361, {'train/accuracy': 0.5642968416213989, 'train/loss': 1.883542776107788, 'validation/accuracy': 0.5241999626159668, 'validation/loss': 2.089167356491089, 'validation/num_examples': 50000, 'test/accuracy': 0.41040003299713135, 'test/loss': 2.7521159648895264, 'test/num_examples': 10000, 'score': 13065.262185811996, 'total_duration': 14424.698271989822, 'accumulated_submission_time': 13065.262185811996, 'accumulated_eval_time': 1357.0431625843048, 'accumulated_logging_time': 0.895582914352417, 'global_step': 28361, 'preemption_count': 0}), (29278, {'train/accuracy': 0.5771288871765137, 'train/loss': 1.784113883972168, 'validation/accuracy': 0.5302799940109253, 'validation/loss': 2.028592348098755, 'validation/num_examples': 50000, 'test/accuracy': 0.42420002818107605, 'test/loss': 2.681459426879883, 'test/num_examples': 10000, 'score': 13485.303869009018, 'total_duration': 14888.231466531754, 'accumulated_submission_time': 13485.303869009018, 'accumulated_eval_time': 1400.4531652927399, 'accumulated_logging_time': 0.9288933277130127, 'global_step': 29278, 'preemption_count': 0}), (30193, {'train/accuracy': 0.5874413847923279, 'train/loss': 1.7344344854354858, 'validation/accuracy': 0.5366799831390381, 'validation/loss': 1.992333173751831, 'validation/num_examples': 50000, 'test/accuracy': 0.4198000133037567, 'test/loss': 2.670804500579834, 'test/num_examples': 10000, 'score': 13905.338715076447, 'total_duration': 15353.465184688568, 'accumulated_submission_time': 13905.338715076447, 'accumulated_eval_time': 1445.575161933899, 'accumulated_logging_time': 0.9585323333740234, 'global_step': 30193, 'preemption_count': 0}), (31110, {'train/accuracy': 0.5776171684265137, 'train/loss': 1.8119839429855347, 'validation/accuracy': 0.5379199981689453, 'validation/loss': 2.0045900344848633, 'validation/num_examples': 50000, 'test/accuracy': 0.4220000207424164, 'test/loss': 2.683004379272461, 'test/num_examples': 10000, 'score': 14325.543489933014, 'total_duration': 15818.188180923462, 'accumulated_submission_time': 14325.543489933014, 'accumulated_eval_time': 1490.0121433734894, 'accumulated_logging_time': 0.9918410778045654, 'global_step': 31110, 'preemption_count': 0}), (32027, {'train/accuracy': 0.5791406035423279, 'train/loss': 1.7816259860992432, 'validation/accuracy': 0.5400199890136719, 'validation/loss': 1.9844132661819458, 'validation/num_examples': 50000, 'test/accuracy': 0.4261000156402588, 'test/loss': 2.6556942462921143, 'test/num_examples': 10000, 'score': 14745.545434951782, 'total_duration': 16282.084006547928, 'accumulated_submission_time': 14745.545434951782, 'accumulated_eval_time': 1533.8274431228638, 'accumulated_logging_time': 1.0221738815307617, 'global_step': 32027, 'preemption_count': 0}), (32942, {'train/accuracy': 0.5919530987739563, 'train/loss': 1.6846200227737427, 'validation/accuracy': 0.5460000038146973, 'validation/loss': 1.936972737312317, 'validation/num_examples': 50000, 'test/accuracy': 0.428600013256073, 'test/loss': 2.6139447689056396, 'test/num_examples': 10000, 'score': 15165.510427713394, 'total_duration': 16746.997824668884, 'accumulated_submission_time': 15165.510427713394, 'accumulated_eval_time': 1578.6870546340942, 'accumulated_logging_time': 1.0637776851654053, 'global_step': 32942, 'preemption_count': 0}), (33857, {'train/accuracy': 0.5914843678474426, 'train/loss': 1.794201374053955, 'validation/accuracy': 0.5385199785232544, 'validation/loss': 2.034351110458374, 'validation/num_examples': 50000, 'test/accuracy': 0.42730000615119934, 'test/loss': 2.6944146156311035, 'test/num_examples': 10000, 'score': 15585.75400686264, 'total_duration': 17211.00025987625, 'accumulated_submission_time': 15585.75400686264, 'accumulated_eval_time': 1622.3610565662384, 'accumulated_logging_time': 1.1009063720703125, 'global_step': 33857, 'preemption_count': 0}), (34774, {'train/accuracy': 0.5895116925239563, 'train/loss': 1.7374787330627441, 'validation/accuracy': 0.5465199947357178, 'validation/loss': 1.960172414779663, 'validation/num_examples': 50000, 'test/accuracy': 0.43390002846717834, 'test/loss': 2.622077703475952, 'test/num_examples': 10000, 'score': 16005.865160703659, 'total_duration': 17675.351983308792, 'accumulated_submission_time': 16005.865160703659, 'accumulated_eval_time': 1666.5135188102722, 'accumulated_logging_time': 1.1385252475738525, 'global_step': 34774, 'preemption_count': 0}), (35690, {'train/accuracy': 0.5945702791213989, 'train/loss': 1.7534534931182861, 'validation/accuracy': 0.5510799884796143, 'validation/loss': 1.9650921821594238, 'validation/num_examples': 50000, 'test/accuracy': 0.43400001525878906, 'test/loss': 2.628279209136963, 'test/num_examples': 10000, 'score': 16425.89613389969, 'total_duration': 18141.204838514328, 'accumulated_submission_time': 16425.89613389969, 'accumulated_eval_time': 1712.2519705295563, 'accumulated_logging_time': 1.1743512153625488, 'global_step': 35690, 'preemption_count': 0}), (36606, {'train/accuracy': 0.6235546469688416, 'train/loss': 1.5661062002182007, 'validation/accuracy': 0.553059995174408, 'validation/loss': 1.8976249694824219, 'validation/num_examples': 50000, 'test/accuracy': 0.4409000277519226, 'test/loss': 2.563768148422241, 'test/num_examples': 10000, 'score': 16846.11967420578, 'total_duration': 18607.68721485138, 'accumulated_submission_time': 16846.11967420578, 'accumulated_eval_time': 1758.4259514808655, 'accumulated_logging_time': 1.2110042572021484, 'global_step': 36606, 'preemption_count': 0}), (37521, {'train/accuracy': 0.5977148413658142, 'train/loss': 1.7096517086029053, 'validation/accuracy': 0.5522400140762329, 'validation/loss': 1.9204543828964233, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.5619406700134277, 'test/num_examples': 10000, 'score': 17266.167387723923, 'total_duration': 19072.837619304657, 'accumulated_submission_time': 17266.167387723923, 'accumulated_eval_time': 1803.4459762573242, 'accumulated_logging_time': 1.2457661628723145, 'global_step': 37521, 'preemption_count': 0}), (38438, {'train/accuracy': 0.602343738079071, 'train/loss': 1.6719639301300049, 'validation/accuracy': 0.5600799918174744, 'validation/loss': 1.897687554359436, 'validation/num_examples': 50000, 'test/accuracy': 0.44540002942085266, 'test/loss': 2.5424721240997314, 'test/num_examples': 10000, 'score': 17686.52992963791, 'total_duration': 19539.077089309692, 'accumulated_submission_time': 17686.52992963791, 'accumulated_eval_time': 1849.2419004440308, 'accumulated_logging_time': 1.2778377532958984, 'global_step': 38438, 'preemption_count': 0}), (39354, {'train/accuracy': 0.6241015195846558, 'train/loss': 1.555986762046814, 'validation/accuracy': 0.5655399560928345, 'validation/loss': 1.844379186630249, 'validation/num_examples': 50000, 'test/accuracy': 0.4482000172138214, 'test/loss': 2.5140278339385986, 'test/num_examples': 10000, 'score': 18106.487027406693, 'total_duration': 20004.48260617256, 'accumulated_submission_time': 18106.487027406693, 'accumulated_eval_time': 1894.6081516742706, 'accumulated_logging_time': 1.3122048377990723, 'global_step': 39354, 'preemption_count': 0}), (40270, {'train/accuracy': 0.6108984351158142, 'train/loss': 1.6063635349273682, 'validation/accuracy': 0.56277996301651, 'validation/loss': 1.844841718673706, 'validation/num_examples': 50000, 'test/accuracy': 0.44520002603530884, 'test/loss': 2.520315647125244, 'test/num_examples': 10000, 'score': 18526.620589256287, 'total_duration': 20469.3478808403, 'accumulated_submission_time': 18526.620589256287, 'accumulated_eval_time': 1939.2537033557892, 'accumulated_logging_time': 1.3499047756195068, 'global_step': 40270, 'preemption_count': 0}), (41184, {'train/accuracy': 0.6096289157867432, 'train/loss': 1.6241741180419922, 'validation/accuracy': 0.5638999938964844, 'validation/loss': 1.85176420211792, 'validation/num_examples': 50000, 'test/accuracy': 0.44670000672340393, 'test/loss': 2.5229926109313965, 'test/num_examples': 10000, 'score': 18946.871083021164, 'total_duration': 20934.40208363533, 'accumulated_submission_time': 18946.871083021164, 'accumulated_eval_time': 1983.9678659439087, 'accumulated_logging_time': 1.3874144554138184, 'global_step': 41184, 'preemption_count': 0}), (42094, {'train/accuracy': 0.6166796684265137, 'train/loss': 1.6326512098312378, 'validation/accuracy': 0.5676999688148499, 'validation/loss': 1.8969939947128296, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.5553810596466064, 'test/num_examples': 10000, 'score': 19367.147775888443, 'total_duration': 21400.516935825348, 'accumulated_submission_time': 19367.147775888443, 'accumulated_eval_time': 2029.7209577560425, 'accumulated_logging_time': 1.4249577522277832, 'global_step': 42094, 'preemption_count': 0}), (43004, {'train/accuracy': 0.6131054759025574, 'train/loss': 1.5964422225952148, 'validation/accuracy': 0.5736199617385864, 'validation/loss': 1.7933223247528076, 'validation/num_examples': 50000, 'test/accuracy': 0.45340001583099365, 'test/loss': 2.4737699031829834, 'test/num_examples': 10000, 'score': 19787.377880573273, 'total_duration': 21864.3144903183, 'accumulated_submission_time': 19787.377880573273, 'accumulated_eval_time': 2073.206840276718, 'accumulated_logging_time': 1.4591336250305176, 'global_step': 43004, 'preemption_count': 0}), (43923, {'train/accuracy': 0.6158202886581421, 'train/loss': 1.6060982942581177, 'validation/accuracy': 0.5713199973106384, 'validation/loss': 1.8300766944885254, 'validation/num_examples': 50000, 'test/accuracy': 0.4507000148296356, 'test/loss': 2.493374824523926, 'test/num_examples': 10000, 'score': 20207.759017944336, 'total_duration': 22331.16711997986, 'accumulated_submission_time': 20207.759017944336, 'accumulated_eval_time': 2119.591502904892, 'accumulated_logging_time': 1.496941328048706, 'global_step': 43923, 'preemption_count': 0}), (44841, {'train/accuracy': 0.6277539134025574, 'train/loss': 1.5462582111358643, 'validation/accuracy': 0.5756999850273132, 'validation/loss': 1.8153027296066284, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.492283821105957, 'test/num_examples': 10000, 'score': 20628.109059095383, 'total_duration': 22796.262968063354, 'accumulated_submission_time': 20628.109059095383, 'accumulated_eval_time': 2164.2571284770966, 'accumulated_logging_time': 1.5289440155029297, 'global_step': 44841, 'preemption_count': 0}), (45758, {'train/accuracy': 0.6309570074081421, 'train/loss': 1.5428788661956787, 'validation/accuracy': 0.5822799801826477, 'validation/loss': 1.779966950416565, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.4568777084350586, 'test/num_examples': 10000, 'score': 21048.3162214756, 'total_duration': 23262.23146867752, 'accumulated_submission_time': 21048.3162214756, 'accumulated_eval_time': 2209.939630508423, 'accumulated_logging_time': 1.5598695278167725, 'global_step': 45758, 'preemption_count': 0}), (46675, {'train/accuracy': 0.6208202838897705, 'train/loss': 1.5916742086410522, 'validation/accuracy': 0.574679970741272, 'validation/loss': 1.8139941692352295, 'validation/num_examples': 50000, 'test/accuracy': 0.45990002155303955, 'test/loss': 2.479991912841797, 'test/num_examples': 10000, 'score': 21468.28194952011, 'total_duration': 23724.68097090721, 'accumulated_submission_time': 21468.28194952011, 'accumulated_eval_time': 2252.3399090766907, 'accumulated_logging_time': 1.5934512615203857, 'global_step': 46675, 'preemption_count': 0}), (47590, {'train/accuracy': 0.6354101300239563, 'train/loss': 1.507873773574829, 'validation/accuracy': 0.5806800127029419, 'validation/loss': 1.768570065498352, 'validation/num_examples': 50000, 'test/accuracy': 0.46250003576278687, 'test/loss': 2.438570737838745, 'test/num_examples': 10000, 'score': 21888.7586414814, 'total_duration': 24188.970096588135, 'accumulated_submission_time': 21888.7586414814, 'accumulated_eval_time': 2296.0664880275726, 'accumulated_logging_time': 1.631007432937622, 'global_step': 47590, 'preemption_count': 0}), (48509, {'train/accuracy': 0.6536523103713989, 'train/loss': 1.431609869003296, 'validation/accuracy': 0.5843600034713745, 'validation/loss': 1.7661563158035278, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.4309449195861816, 'test/num_examples': 10000, 'score': 22309.008142709732, 'total_duration': 24653.99893283844, 'accumulated_submission_time': 22309.008142709732, 'accumulated_eval_time': 2340.759823322296, 'accumulated_logging_time': 1.6684105396270752, 'global_step': 48509, 'preemption_count': 0}), (49425, {'train/accuracy': 0.6244921684265137, 'train/loss': 1.5902440547943115, 'validation/accuracy': 0.5818600058555603, 'validation/loss': 1.7915534973144531, 'validation/num_examples': 50000, 'test/accuracy': 0.4668000340461731, 'test/loss': 2.4473612308502197, 'test/num_examples': 10000, 'score': 22729.027530670166, 'total_duration': 25121.2830452919, 'accumulated_submission_time': 22729.027530670166, 'accumulated_eval_time': 2387.934749364853, 'accumulated_logging_time': 1.710059642791748, 'global_step': 49425, 'preemption_count': 0}), (50342, {'train/accuracy': 0.6416601538658142, 'train/loss': 1.4782509803771973, 'validation/accuracy': 0.5866000056266785, 'validation/loss': 1.7392513751983643, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.4052207469940186, 'test/num_examples': 10000, 'score': 23149.305313825607, 'total_duration': 25586.423802137375, 'accumulated_submission_time': 23149.305313825607, 'accumulated_eval_time': 2432.709528207779, 'accumulated_logging_time': 1.7498483657836914, 'global_step': 50342, 'preemption_count': 0}), (51259, {'train/accuracy': 0.6513866782188416, 'train/loss': 1.4301273822784424, 'validation/accuracy': 0.5900200009346008, 'validation/loss': 1.729769229888916, 'validation/num_examples': 50000, 'test/accuracy': 0.46970000863075256, 'test/loss': 2.399967670440674, 'test/num_examples': 10000, 'score': 23569.449481725693, 'total_duration': 26051.07665014267, 'accumulated_submission_time': 23569.449481725693, 'accumulated_eval_time': 2477.1309113502502, 'accumulated_logging_time': 1.788116216659546, 'global_step': 51259, 'preemption_count': 0}), (52176, {'train/accuracy': 0.6253125071525574, 'train/loss': 1.5959267616271973, 'validation/accuracy': 0.5811799764633179, 'validation/loss': 1.8141032457351685, 'validation/num_examples': 50000, 'test/accuracy': 0.460500031709671, 'test/loss': 2.463486433029175, 'test/num_examples': 10000, 'score': 23989.585379123688, 'total_duration': 26516.830800056458, 'accumulated_submission_time': 23989.585379123688, 'accumulated_eval_time': 2522.6650750637054, 'accumulated_logging_time': 1.8231792449951172, 'global_step': 52176, 'preemption_count': 0}), (53091, {'train/accuracy': 0.6471874713897705, 'train/loss': 1.444928526878357, 'validation/accuracy': 0.5928999781608582, 'validation/loss': 1.692473292350769, 'validation/num_examples': 50000, 'test/accuracy': 0.4796000123023987, 'test/loss': 2.35530948638916, 'test/num_examples': 10000, 'score': 24409.854116678238, 'total_duration': 26981.77447628975, 'accumulated_submission_time': 24409.854116678238, 'accumulated_eval_time': 2567.257641553879, 'accumulated_logging_time': 1.8575630187988281, 'global_step': 53091, 'preemption_count': 0}), (54008, {'train/accuracy': 0.6502734422683716, 'train/loss': 1.464459776878357, 'validation/accuracy': 0.5907399654388428, 'validation/loss': 1.7406047582626343, 'validation/num_examples': 50000, 'test/accuracy': 0.4711000323295593, 'test/loss': 2.407909393310547, 'test/num_examples': 10000, 'score': 24830.11319756508, 'total_duration': 27445.589923620224, 'accumulated_submission_time': 24830.11319756508, 'accumulated_eval_time': 2610.7226634025574, 'accumulated_logging_time': 1.9003996849060059, 'global_step': 54008, 'preemption_count': 0}), (54927, {'train/accuracy': 0.6294726133346558, 'train/loss': 1.5730618238449097, 'validation/accuracy': 0.5880399942398071, 'validation/loss': 1.7772213220596313, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.4256205558776855, 'test/num_examples': 10000, 'score': 25250.349819660187, 'total_duration': 27912.015790462494, 'accumulated_submission_time': 25250.349819660187, 'accumulated_eval_time': 2656.8290503025055, 'accumulated_logging_time': 1.9344263076782227, 'global_step': 54927, 'preemption_count': 0}), (55845, {'train/accuracy': 0.6401562094688416, 'train/loss': 1.5094945430755615, 'validation/accuracy': 0.5906800031661987, 'validation/loss': 1.7373265027999878, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.4103524684906006, 'test/num_examples': 10000, 'score': 25670.50795698166, 'total_duration': 28375.542023181915, 'accumulated_submission_time': 25670.50795698166, 'accumulated_eval_time': 2700.1080725193024, 'accumulated_logging_time': 1.9745028018951416, 'global_step': 55845, 'preemption_count': 0}), (56763, {'train/accuracy': 0.646484375, 'train/loss': 1.4634003639221191, 'validation/accuracy': 0.5952399969100952, 'validation/loss': 1.7165286540985107, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.4129831790924072, 'test/num_examples': 10000, 'score': 26090.748301029205, 'total_duration': 28838.13270163536, 'accumulated_submission_time': 26090.748301029205, 'accumulated_eval_time': 2742.3598341941833, 'accumulated_logging_time': 2.0240936279296875, 'global_step': 56763, 'preemption_count': 0}), (57682, {'train/accuracy': 0.6556054353713989, 'train/loss': 1.3994473218917847, 'validation/accuracy': 0.599299967288971, 'validation/loss': 1.6665985584259033, 'validation/num_examples': 50000, 'test/accuracy': 0.4797000288963318, 'test/loss': 2.3530988693237305, 'test/num_examples': 10000, 'score': 26510.983321666718, 'total_duration': 29304.030174016953, 'accumulated_submission_time': 26510.983321666718, 'accumulated_eval_time': 2787.934552669525, 'accumulated_logging_time': 2.0629003047943115, 'global_step': 57682, 'preemption_count': 0}), (58601, {'train/accuracy': 0.6381640434265137, 'train/loss': 1.4684827327728271, 'validation/accuracy': 0.5956400036811829, 'validation/loss': 1.700318694114685, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.3680901527404785, 'test/num_examples': 10000, 'score': 26931.549178361893, 'total_duration': 29770.996066331863, 'accumulated_submission_time': 26931.549178361893, 'accumulated_eval_time': 2834.242516517639, 'accumulated_logging_time': 2.105926275253296, 'global_step': 58601, 'preemption_count': 0}), (59519, {'train/accuracy': 0.6510156393051147, 'train/loss': 1.446966528892517, 'validation/accuracy': 0.5954599976539612, 'validation/loss': 1.7019143104553223, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.367074728012085, 'test/num_examples': 10000, 'score': 27351.7313952446, 'total_duration': 30235.018276691437, 'accumulated_submission_time': 27351.7313952446, 'accumulated_eval_time': 2877.9978761672974, 'accumulated_logging_time': 2.1419429779052734, 'global_step': 59519, 'preemption_count': 0}), (60437, {'train/accuracy': 0.6662304401397705, 'train/loss': 1.356255054473877, 'validation/accuracy': 0.5960400104522705, 'validation/loss': 1.6892080307006836, 'validation/num_examples': 50000, 'test/accuracy': 0.47950002551078796, 'test/loss': 2.367875814437866, 'test/num_examples': 10000, 'score': 27772.097522735596, 'total_duration': 30701.9356341362, 'accumulated_submission_time': 27772.097522735596, 'accumulated_eval_time': 2924.465174674988, 'accumulated_logging_time': 2.1778712272644043, 'global_step': 60437, 'preemption_count': 0}), (61357, {'train/accuracy': 0.6473046541213989, 'train/loss': 1.4790903329849243, 'validation/accuracy': 0.5974400043487549, 'validation/loss': 1.7262574434280396, 'validation/num_examples': 50000, 'test/accuracy': 0.47770002484321594, 'test/loss': 2.391491651535034, 'test/num_examples': 10000, 'score': 28192.18399953842, 'total_duration': 31161.633261203766, 'accumulated_submission_time': 28192.18399953842, 'accumulated_eval_time': 2963.984852075577, 'accumulated_logging_time': 2.220782995223999, 'global_step': 61357, 'preemption_count': 0}), (62274, {'train/accuracy': 0.6483398079872131, 'train/loss': 1.4805115461349487, 'validation/accuracy': 0.5988399982452393, 'validation/loss': 1.7147594690322876, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.3593976497650146, 'test/num_examples': 10000, 'score': 28612.51757788658, 'total_duration': 31627.710637569427, 'accumulated_submission_time': 28612.51757788658, 'accumulated_eval_time': 3009.6463346481323, 'accumulated_logging_time': 2.254441261291504, 'global_step': 62274, 'preemption_count': 0}), (63191, {'train/accuracy': 0.6765429377555847, 'train/loss': 1.3175029754638672, 'validation/accuracy': 0.6112399697303772, 'validation/loss': 1.6248478889465332, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.2709903717041016, 'test/num_examples': 10000, 'score': 29032.745688199997, 'total_duration': 32090.70645928383, 'accumulated_submission_time': 29032.745688199997, 'accumulated_eval_time': 3052.322532892227, 'accumulated_logging_time': 2.297640323638916, 'global_step': 63191, 'preemption_count': 0}), (64108, {'train/accuracy': 0.6566015481948853, 'train/loss': 1.3950915336608887, 'validation/accuracy': 0.6128999590873718, 'validation/loss': 1.6186541318893433, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.2900989055633545, 'test/num_examples': 10000, 'score': 29452.76300573349, 'total_duration': 32554.79523062706, 'accumulated_submission_time': 29452.76300573349, 'accumulated_eval_time': 3096.3077614307404, 'accumulated_logging_time': 2.3357009887695312, 'global_step': 64108, 'preemption_count': 0}), (65024, {'train/accuracy': 0.6565819978713989, 'train/loss': 1.4120858907699585, 'validation/accuracy': 0.6054199934005737, 'validation/loss': 1.6623183488845825, 'validation/num_examples': 50000, 'test/accuracy': 0.48760002851486206, 'test/loss': 2.3212289810180664, 'test/num_examples': 10000, 'score': 29873.10894012451, 'total_duration': 33020.64948439598, 'accumulated_submission_time': 29873.10894012451, 'accumulated_eval_time': 3141.72727894783, 'accumulated_logging_time': 2.376920223236084, 'global_step': 65024, 'preemption_count': 0}), (65942, {'train/accuracy': 0.6722265481948853, 'train/loss': 1.4081395864486694, 'validation/accuracy': 0.6116799712181091, 'validation/loss': 1.6795889139175415, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.3212332725524902, 'test/num_examples': 10000, 'score': 30293.38259911537, 'total_duration': 33486.357377290726, 'accumulated_submission_time': 30293.38259911537, 'accumulated_eval_time': 3187.0775051116943, 'accumulated_logging_time': 2.4117984771728516, 'global_step': 65942, 'preemption_count': 0}), (66860, {'train/accuracy': 0.6582812070846558, 'train/loss': 1.3994724750518799, 'validation/accuracy': 0.6097399592399597, 'validation/loss': 1.6316499710083008, 'validation/num_examples': 50000, 'test/accuracy': 0.4897000193595886, 'test/loss': 2.2837886810302734, 'test/num_examples': 10000, 'score': 30713.42664384842, 'total_duration': 33950.26797604561, 'accumulated_submission_time': 30713.42664384842, 'accumulated_eval_time': 3230.855504989624, 'accumulated_logging_time': 2.4521572589874268, 'global_step': 66860, 'preemption_count': 0}), (67778, {'train/accuracy': 0.6619336009025574, 'train/loss': 1.3762539625167847, 'validation/accuracy': 0.6098399758338928, 'validation/loss': 1.6209572553634644, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.292510986328125, 'test/num_examples': 10000, 'score': 31133.74489402771, 'total_duration': 34415.31527900696, 'accumulated_submission_time': 31133.74489402771, 'accumulated_eval_time': 3275.497382879257, 'accumulated_logging_time': 2.491576910018921, 'global_step': 67778, 'preemption_count': 0}), (68697, {'train/accuracy': 0.6661718487739563, 'train/loss': 1.3747303485870361, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.637009620666504, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.307218551635742, 'test/num_examples': 10000, 'score': 31553.782564401627, 'total_duration': 34879.88833665848, 'accumulated_submission_time': 31553.782564401627, 'accumulated_eval_time': 3319.9401018619537, 'accumulated_logging_time': 2.5363433361053467, 'global_step': 68697, 'preemption_count': 0}), (69615, {'train/accuracy': 0.6862890720367432, 'train/loss': 1.303639531135559, 'validation/accuracy': 0.6137999892234802, 'validation/loss': 1.6325457096099854, 'validation/num_examples': 50000, 'test/accuracy': 0.49540001153945923, 'test/loss': 2.2856035232543945, 'test/num_examples': 10000, 'score': 31974.151923894882, 'total_duration': 35339.82807254791, 'accumulated_submission_time': 31974.151923894882, 'accumulated_eval_time': 3359.4197540283203, 'accumulated_logging_time': 2.5785627365112305, 'global_step': 69615, 'preemption_count': 0}), (70532, {'train/accuracy': 0.6647265553474426, 'train/loss': 1.3673946857452393, 'validation/accuracy': 0.618619978427887, 'validation/loss': 1.584282636642456, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.240478515625, 'test/num_examples': 10000, 'score': 32394.145018339157, 'total_duration': 35805.46223473549, 'accumulated_submission_time': 32394.145018339157, 'accumulated_eval_time': 3404.9674224853516, 'accumulated_logging_time': 2.6233327388763428, 'global_step': 70532, 'preemption_count': 0}), (71449, {'train/accuracy': 0.6814843416213989, 'train/loss': 1.3013520240783691, 'validation/accuracy': 0.6236599683761597, 'validation/loss': 1.567617654800415, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.2308449745178223, 'test/num_examples': 10000, 'score': 32814.301644325256, 'total_duration': 36272.37492394447, 'accumulated_submission_time': 32814.301644325256, 'accumulated_eval_time': 3451.6360535621643, 'accumulated_logging_time': 2.6626389026641846, 'global_step': 71449, 'preemption_count': 0}), (72368, {'train/accuracy': 0.6871093511581421, 'train/loss': 1.261122465133667, 'validation/accuracy': 0.6153599619865417, 'validation/loss': 1.602860927581787, 'validation/num_examples': 50000, 'test/accuracy': 0.49630001187324524, 'test/loss': 2.2533340454101562, 'test/num_examples': 10000, 'score': 33234.60793232918, 'total_duration': 36739.626620054245, 'accumulated_submission_time': 33234.60793232918, 'accumulated_eval_time': 3498.4893431663513, 'accumulated_logging_time': 2.706322431564331, 'global_step': 72368, 'preemption_count': 0}), (73287, {'train/accuracy': 0.6668750047683716, 'train/loss': 1.3529757261276245, 'validation/accuracy': 0.6220799684524536, 'validation/loss': 1.573674201965332, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.2361388206481934, 'test/num_examples': 10000, 'score': 33654.99967765808, 'total_duration': 37200.15159320831, 'accumulated_submission_time': 33654.99967765808, 'accumulated_eval_time': 3538.5254430770874, 'accumulated_logging_time': 2.7534587383270264, 'global_step': 73287, 'preemption_count': 0}), (74204, {'train/accuracy': 0.6692773103713989, 'train/loss': 1.3801852464675903, 'validation/accuracy': 0.6154199838638306, 'validation/loss': 1.623300552368164, 'validation/num_examples': 50000, 'test/accuracy': 0.49230003356933594, 'test/loss': 2.295822858810425, 'test/num_examples': 10000, 'score': 34075.14587640762, 'total_duration': 37663.99420571327, 'accumulated_submission_time': 34075.14587640762, 'accumulated_eval_time': 3582.1325442790985, 'accumulated_logging_time': 2.7938594818115234, 'global_step': 74204, 'preemption_count': 0}), (75122, {'train/accuracy': 0.6792187094688416, 'train/loss': 1.3302925825119019, 'validation/accuracy': 0.6165399551391602, 'validation/loss': 1.6309008598327637, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.308547258377075, 'test/num_examples': 10000, 'score': 34495.48349118233, 'total_duration': 38128.09023118019, 'accumulated_submission_time': 34495.48349118233, 'accumulated_eval_time': 3625.7986521720886, 'accumulated_logging_time': 2.837161064147949, 'global_step': 75122, 'preemption_count': 0}), (76040, {'train/accuracy': 0.668652355670929, 'train/loss': 1.3684196472167969, 'validation/accuracy': 0.6186800003051758, 'validation/loss': 1.6112457513809204, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.2662360668182373, 'test/num_examples': 10000, 'score': 34915.78348207474, 'total_duration': 38592.96436548233, 'accumulated_submission_time': 34915.78348207474, 'accumulated_eval_time': 3670.284652233124, 'accumulated_logging_time': 2.877194404602051, 'global_step': 76040, 'preemption_count': 0}), (76959, {'train/accuracy': 0.6804882884025574, 'train/loss': 1.2870023250579834, 'validation/accuracy': 0.6269800066947937, 'validation/loss': 1.5367377996444702, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.2034332752227783, 'test/num_examples': 10000, 'score': 35335.89672112465, 'total_duration': 39057.71058821678, 'accumulated_submission_time': 35335.89672112465, 'accumulated_eval_time': 3714.829068660736, 'accumulated_logging_time': 2.9171230792999268, 'global_step': 76959, 'preemption_count': 0}), (77879, {'train/accuracy': 0.6826757788658142, 'train/loss': 1.2964460849761963, 'validation/accuracy': 0.6247599720954895, 'validation/loss': 1.580952763557434, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.237107753753662, 'test/num_examples': 10000, 'score': 35755.94821023941, 'total_duration': 39520.92169165611, 'accumulated_submission_time': 35755.94821023941, 'accumulated_eval_time': 3757.8984265327454, 'accumulated_logging_time': 2.9589803218841553, 'global_step': 77879, 'preemption_count': 0}), (78797, {'train/accuracy': 0.6805663704872131, 'train/loss': 1.3006861209869385, 'validation/accuracy': 0.6326799988746643, 'validation/loss': 1.5357210636138916, 'validation/num_examples': 50000, 'test/accuracy': 0.5067000389099121, 'test/loss': 2.2033281326293945, 'test/num_examples': 10000, 'score': 36176.19422531128, 'total_duration': 39986.41137290001, 'accumulated_submission_time': 36176.19422531128, 'accumulated_eval_time': 3803.0521445274353, 'accumulated_logging_time': 3.0000874996185303, 'global_step': 78797, 'preemption_count': 0}), (79714, {'train/accuracy': 0.6774023175239563, 'train/loss': 1.3380963802337646, 'validation/accuracy': 0.6273800134658813, 'validation/loss': 1.5836255550384521, 'validation/num_examples': 50000, 'test/accuracy': 0.5046000480651855, 'test/loss': 2.24481201171875, 'test/num_examples': 10000, 'score': 36596.54655098915, 'total_duration': 40453.15292882919, 'accumulated_submission_time': 36596.54655098915, 'accumulated_eval_time': 3849.35542011261, 'accumulated_logging_time': 3.0385146141052246, 'global_step': 79714, 'preemption_count': 0}), (80632, {'train/accuracy': 0.6942577958106995, 'train/loss': 1.2454754114151, 'validation/accuracy': 0.6340599656105042, 'validation/loss': 1.5173759460449219, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.1939358711242676, 'test/num_examples': 10000, 'score': 37016.87239527702, 'total_duration': 40916.673253536224, 'accumulated_submission_time': 37016.87239527702, 'accumulated_eval_time': 3892.457942724228, 'accumulated_logging_time': 3.081490993499756, 'global_step': 80632, 'preemption_count': 0}), (81549, {'train/accuracy': 0.7085741758346558, 'train/loss': 1.1876144409179688, 'validation/accuracy': 0.6307399868965149, 'validation/loss': 1.532135248184204, 'validation/num_examples': 50000, 'test/accuracy': 0.5088000297546387, 'test/loss': 2.1980860233306885, 'test/num_examples': 10000, 'score': 37437.10391163826, 'total_duration': 41381.222581624985, 'accumulated_submission_time': 37437.10391163826, 'accumulated_eval_time': 3936.6828587055206, 'accumulated_logging_time': 3.125739812850952, 'global_step': 81549, 'preemption_count': 0}), (82465, {'train/accuracy': 0.6794531345367432, 'train/loss': 1.320271611213684, 'validation/accuracy': 0.6279999613761902, 'validation/loss': 1.5583600997924805, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.229015350341797, 'test/num_examples': 10000, 'score': 37857.19369125366, 'total_duration': 41843.72469615936, 'accumulated_submission_time': 37857.19369125366, 'accumulated_eval_time': 3979.00262093544, 'accumulated_logging_time': 3.1703832149505615, 'global_step': 82465, 'preemption_count': 0}), (83383, {'train/accuracy': 0.6886913776397705, 'train/loss': 1.2884644269943237, 'validation/accuracy': 0.6291199922561646, 'validation/loss': 1.5523051023483276, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.218010187149048, 'test/num_examples': 10000, 'score': 38277.31278705597, 'total_duration': 42309.546770095825, 'accumulated_submission_time': 38277.31278705597, 'accumulated_eval_time': 4024.6180398464203, 'accumulated_logging_time': 3.2090845108032227, 'global_step': 83383, 'preemption_count': 0}), (84301, {'train/accuracy': 0.7104882597923279, 'train/loss': 1.1823593378067017, 'validation/accuracy': 0.6382799744606018, 'validation/loss': 1.511709451675415, 'validation/num_examples': 50000, 'test/accuracy': 0.5146000385284424, 'test/loss': 2.178715229034424, 'test/num_examples': 10000, 'score': 38697.50476980209, 'total_duration': 42774.288845300674, 'accumulated_submission_time': 38697.50476980209, 'accumulated_eval_time': 4069.0771267414093, 'accumulated_logging_time': 3.251221179962158, 'global_step': 84301, 'preemption_count': 0}), (85212, {'train/accuracy': 0.6768164038658142, 'train/loss': 1.3248145580291748, 'validation/accuracy': 0.6277599930763245, 'validation/loss': 1.5532524585723877, 'validation/num_examples': 50000, 'test/accuracy': 0.5046000480651855, 'test/loss': 2.2248823642730713, 'test/num_examples': 10000, 'score': 39117.637340545654, 'total_duration': 43238.81547117233, 'accumulated_submission_time': 39117.637340545654, 'accumulated_eval_time': 4113.385211467743, 'accumulated_logging_time': 3.2891671657562256, 'global_step': 85212, 'preemption_count': 0}), (86128, {'train/accuracy': 0.6905273199081421, 'train/loss': 1.2369657754898071, 'validation/accuracy': 0.6373599767684937, 'validation/loss': 1.5001189708709717, 'validation/num_examples': 50000, 'test/accuracy': 0.5139999985694885, 'test/loss': 2.168210744857788, 'test/num_examples': 10000, 'score': 39537.63686776161, 'total_duration': 43704.54471921921, 'accumulated_submission_time': 39537.63686776161, 'accumulated_eval_time': 4159.022690296173, 'accumulated_logging_time': 3.333111047744751, 'global_step': 86128, 'preemption_count': 0}), (87043, {'train/accuracy': 0.7060351371765137, 'train/loss': 1.2162786722183228, 'validation/accuracy': 0.64301997423172, 'validation/loss': 1.5114175081253052, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.167684316635132, 'test/num_examples': 10000, 'score': 39957.94800043106, 'total_duration': 44169.95958900452, 'accumulated_submission_time': 39957.94800043106, 'accumulated_eval_time': 4204.034997463226, 'accumulated_logging_time': 3.376688241958618, 'global_step': 87043, 'preemption_count': 0}), (87961, {'train/accuracy': 0.6932812333106995, 'train/loss': 1.2655775547027588, 'validation/accuracy': 0.6387799978256226, 'validation/loss': 1.5086603164672852, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.170346975326538, 'test/num_examples': 10000, 'score': 40378.29867053032, 'total_duration': 44637.17258501053, 'accumulated_submission_time': 40378.29867053032, 'accumulated_eval_time': 4250.808509349823, 'accumulated_logging_time': 3.416672706604004, 'global_step': 87961, 'preemption_count': 0}), (88880, {'train/accuracy': 0.7005273103713989, 'train/loss': 1.252591609954834, 'validation/accuracy': 0.6423400044441223, 'validation/loss': 1.520622730255127, 'validation/num_examples': 50000, 'test/accuracy': 0.5182000398635864, 'test/loss': 2.1785120964050293, 'test/num_examples': 10000, 'score': 40798.624522686005, 'total_duration': 45101.790779829025, 'accumulated_submission_time': 40798.624522686005, 'accumulated_eval_time': 4295.005742549896, 'accumulated_logging_time': 3.462848424911499, 'global_step': 88880, 'preemption_count': 0}), (89796, {'train/accuracy': 0.7016210556030273, 'train/loss': 1.2490087747573853, 'validation/accuracy': 0.6393600106239319, 'validation/loss': 1.5409966707229614, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.1909217834472656, 'test/num_examples': 10000, 'score': 41218.92868351936, 'total_duration': 45567.463312625885, 'accumulated_submission_time': 41218.92868351936, 'accumulated_eval_time': 4340.285962820053, 'accumulated_logging_time': 3.503141403198242, 'global_step': 89796, 'preemption_count': 0}), (90713, {'train/accuracy': 0.69837886095047, 'train/loss': 1.2028743028640747, 'validation/accuracy': 0.6417199969291687, 'validation/loss': 1.4627127647399902, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.1296322345733643, 'test/num_examples': 10000, 'score': 41639.111610889435, 'total_duration': 46028.47916865349, 'accumulated_submission_time': 41639.111610889435, 'accumulated_eval_time': 4381.028985977173, 'accumulated_logging_time': 3.543948173522949, 'global_step': 90713, 'preemption_count': 0}), (91631, {'train/accuracy': 0.6951367259025574, 'train/loss': 1.2514410018920898, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.5035076141357422, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.1497442722320557, 'test/num_examples': 10000, 'score': 42059.214393138885, 'total_duration': 46494.97500014305, 'accumulated_submission_time': 42059.214393138885, 'accumulated_eval_time': 4427.328744649887, 'accumulated_logging_time': 3.5883829593658447, 'global_step': 91631, 'preemption_count': 0}), (92549, {'train/accuracy': 0.703320324420929, 'train/loss': 1.215058445930481, 'validation/accuracy': 0.6431199908256531, 'validation/loss': 1.4973742961883545, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.147681713104248, 'test/num_examples': 10000, 'score': 42479.155719041824, 'total_duration': 46960.63577723503, 'accumulated_submission_time': 42479.155719041824, 'accumulated_eval_time': 4472.958468675613, 'accumulated_logging_time': 3.6285886764526367, 'global_step': 92549, 'preemption_count': 0}), (93468, {'train/accuracy': 0.7299999594688416, 'train/loss': 1.0730012655258179, 'validation/accuracy': 0.6488400101661682, 'validation/loss': 1.4530651569366455, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.1210744380950928, 'test/num_examples': 10000, 'score': 42899.42155408859, 'total_duration': 47427.48384642601, 'accumulated_submission_time': 42899.42155408859, 'accumulated_eval_time': 4519.443992853165, 'accumulated_logging_time': 3.6771814823150635, 'global_step': 93468, 'preemption_count': 0}), (94385, {'train/accuracy': 0.7069921493530273, 'train/loss': 1.1722054481506348, 'validation/accuracy': 0.6530399918556213, 'validation/loss': 1.4273556470870972, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.078791618347168, 'test/num_examples': 10000, 'score': 43319.45416808128, 'total_duration': 47893.16714167595, 'accumulated_submission_time': 43319.45416808128, 'accumulated_eval_time': 4565.002544879913, 'accumulated_logging_time': 3.720834970474243, 'global_step': 94385, 'preemption_count': 0}), (95303, {'train/accuracy': 0.712109386920929, 'train/loss': 1.1649584770202637, 'validation/accuracy': 0.6550599932670593, 'validation/loss': 1.4420679807662964, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.1036524772644043, 'test/num_examples': 10000, 'score': 43739.83526778221, 'total_duration': 48360.269610881805, 'accumulated_submission_time': 43739.83526778221, 'accumulated_eval_time': 4611.628629922867, 'accumulated_logging_time': 3.7676095962524414, 'global_step': 95303, 'preemption_count': 0}), (96220, {'train/accuracy': 0.7240039110183716, 'train/loss': 1.1309791803359985, 'validation/accuracy': 0.6502199769020081, 'validation/loss': 1.4678243398666382, 'validation/num_examples': 50000, 'test/accuracy': 0.5303000211715698, 'test/loss': 2.125929832458496, 'test/num_examples': 10000, 'score': 44159.95757508278, 'total_duration': 48825.42972564697, 'accumulated_submission_time': 44159.95757508278, 'accumulated_eval_time': 4656.578102588654, 'accumulated_logging_time': 3.80673885345459, 'global_step': 96220, 'preemption_count': 0}), (97138, {'train/accuracy': 0.7091991901397705, 'train/loss': 1.2087370157241821, 'validation/accuracy': 0.6511200070381165, 'validation/loss': 1.4547009468078613, 'validation/num_examples': 50000, 'test/accuracy': 0.5326000452041626, 'test/loss': 2.099363327026367, 'test/num_examples': 10000, 'score': 44579.869277477264, 'total_duration': 49291.24484491348, 'accumulated_submission_time': 44579.869277477264, 'accumulated_eval_time': 4702.385105133057, 'accumulated_logging_time': 3.8552112579345703, 'global_step': 97138, 'preemption_count': 0}), (98053, {'train/accuracy': 0.714550793170929, 'train/loss': 1.1435582637786865, 'validation/accuracy': 0.658519983291626, 'validation/loss': 1.4108588695526123, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 2.0602378845214844, 'test/num_examples': 10000, 'score': 45000.0894985199, 'total_duration': 49757.634499788284, 'accumulated_submission_time': 45000.0894985199, 'accumulated_eval_time': 4748.463057041168, 'accumulated_logging_time': 3.8989665508270264, 'global_step': 98053, 'preemption_count': 0}), (98971, {'train/accuracy': 0.7219140529632568, 'train/loss': 1.1263810396194458, 'validation/accuracy': 0.6539799571037292, 'validation/loss': 1.4435505867004395, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.114023447036743, 'test/num_examples': 10000, 'score': 45420.03511428833, 'total_duration': 50219.339812517166, 'accumulated_submission_time': 45420.03511428833, 'accumulated_eval_time': 4790.124536275864, 'accumulated_logging_time': 3.9481842517852783, 'global_step': 98971, 'preemption_count': 0}), (99888, {'train/accuracy': 0.7099218368530273, 'train/loss': 1.1682158708572388, 'validation/accuracy': 0.6560999751091003, 'validation/loss': 1.4241340160369873, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.0836803913116455, 'test/num_examples': 10000, 'score': 45840.288721084595, 'total_duration': 50686.42065501213, 'accumulated_submission_time': 45840.288721084595, 'accumulated_eval_time': 4836.862557888031, 'accumulated_logging_time': 3.9894981384277344, 'global_step': 99888, 'preemption_count': 0}), (100807, {'train/accuracy': 0.7173827886581421, 'train/loss': 1.1348272562026978, 'validation/accuracy': 0.6578800082206726, 'validation/loss': 1.4096122980117798, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.062699556350708, 'test/num_examples': 10000, 'score': 46260.74950814247, 'total_duration': 51153.77632880211, 'accumulated_submission_time': 46260.74950814247, 'accumulated_eval_time': 4883.661042928696, 'accumulated_logging_time': 4.037500381469727, 'global_step': 100807, 'preemption_count': 0}), (101725, {'train/accuracy': 0.7226757407188416, 'train/loss': 1.1128582954406738, 'validation/accuracy': 0.6645799875259399, 'validation/loss': 1.3956934213638306, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.032041311264038, 'test/num_examples': 10000, 'score': 46681.04964232445, 'total_duration': 51622.92252993584, 'accumulated_submission_time': 46681.04964232445, 'accumulated_eval_time': 4932.40603351593, 'accumulated_logging_time': 4.089587211608887, 'global_step': 101725, 'preemption_count': 0}), (102642, {'train/accuracy': 0.7225781083106995, 'train/loss': 1.1422998905181885, 'validation/accuracy': 0.6594600081443787, 'validation/loss': 1.4308340549468994, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.086282730102539, 'test/num_examples': 10000, 'score': 47101.4250562191, 'total_duration': 52086.83007669449, 'accumulated_submission_time': 47101.4250562191, 'accumulated_eval_time': 4975.847240924835, 'accumulated_logging_time': 4.1319169998168945, 'global_step': 102642, 'preemption_count': 0}), (103560, {'train/accuracy': 0.7223241925239563, 'train/loss': 1.1243679523468018, 'validation/accuracy': 0.6632999777793884, 'validation/loss': 1.3928260803222656, 'validation/num_examples': 50000, 'test/accuracy': 0.5383000373840332, 'test/loss': 2.04221248626709, 'test/num_examples': 10000, 'score': 47521.5824341774, 'total_duration': 52553.43996691704, 'accumulated_submission_time': 47521.5824341774, 'accumulated_eval_time': 5022.208994150162, 'accumulated_logging_time': 4.173872470855713, 'global_step': 103560, 'preemption_count': 0}), (104475, {'train/accuracy': 0.73095703125, 'train/loss': 1.0725715160369873, 'validation/accuracy': 0.6665999889373779, 'validation/loss': 1.363909125328064, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.0053791999816895, 'test/num_examples': 10000, 'score': 47941.7282936573, 'total_duration': 53019.200771570206, 'accumulated_submission_time': 47941.7282936573, 'accumulated_eval_time': 5067.726749658585, 'accumulated_logging_time': 4.222776651382446, 'global_step': 104475, 'preemption_count': 0}), (105393, {'train/accuracy': 0.7487890720367432, 'train/loss': 1.0079954862594604, 'validation/accuracy': 0.6653000116348267, 'validation/loss': 1.3744615316390991, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 2.039724588394165, 'test/num_examples': 10000, 'score': 48361.7201230526, 'total_duration': 53483.970052719116, 'accumulated_submission_time': 48361.7201230526, 'accumulated_eval_time': 5112.407633304596, 'accumulated_logging_time': 4.270694017410278, 'global_step': 105393, 'preemption_count': 0}), (106309, {'train/accuracy': 0.7193945050239563, 'train/loss': 1.12610924243927, 'validation/accuracy': 0.6675999760627747, 'validation/loss': 1.3651163578033447, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.0198922157287598, 'test/num_examples': 10000, 'score': 48782.00963258743, 'total_duration': 53949.74782347679, 'accumulated_submission_time': 48782.00963258743, 'accumulated_eval_time': 5157.802113294601, 'accumulated_logging_time': 4.31640887260437, 'global_step': 106309, 'preemption_count': 0}), (107226, {'train/accuracy': 0.7369921803474426, 'train/loss': 1.0469598770141602, 'validation/accuracy': 0.6708799600601196, 'validation/loss': 1.3509389162063599, 'validation/num_examples': 50000, 'test/accuracy': 0.5460000038146973, 'test/loss': 2.00260591506958, 'test/num_examples': 10000, 'score': 49202.203140735626, 'total_duration': 54414.82877445221, 'accumulated_submission_time': 49202.203140735626, 'accumulated_eval_time': 5202.594695091248, 'accumulated_logging_time': 4.361671686172485, 'global_step': 107226, 'preemption_count': 0}), (108140, {'train/accuracy': 0.7404687404632568, 'train/loss': 1.0386266708374023, 'validation/accuracy': 0.6669600009918213, 'validation/loss': 1.367197036743164, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.017230749130249, 'test/num_examples': 10000, 'score': 49622.17845416069, 'total_duration': 54879.68249583244, 'accumulated_submission_time': 49622.17845416069, 'accumulated_eval_time': 5247.3822016716, 'accumulated_logging_time': 4.404340744018555, 'global_step': 108140, 'preemption_count': 0}), (109057, {'train/accuracy': 0.7318944931030273, 'train/loss': 1.066878318786621, 'validation/accuracy': 0.674839973449707, 'validation/loss': 1.330930471420288, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 1.9898630380630493, 'test/num_examples': 10000, 'score': 50042.48392677307, 'total_duration': 55349.32324099541, 'accumulated_submission_time': 50042.48392677307, 'accumulated_eval_time': 5296.62574672699, 'accumulated_logging_time': 4.448246955871582, 'global_step': 109057, 'preemption_count': 0}), (109975, {'train/accuracy': 0.7314843535423279, 'train/loss': 1.0798393487930298, 'validation/accuracy': 0.6667199730873108, 'validation/loss': 1.3757458925247192, 'validation/num_examples': 50000, 'test/accuracy': 0.5470000505447388, 'test/loss': 2.032017946243286, 'test/num_examples': 10000, 'score': 50462.675322294235, 'total_duration': 55816.034700632095, 'accumulated_submission_time': 50462.675322294235, 'accumulated_eval_time': 5343.052444219589, 'accumulated_logging_time': 4.493457078933716, 'global_step': 109975, 'preemption_count': 0}), (110892, {'train/accuracy': 0.7429296970367432, 'train/loss': 1.0271873474121094, 'validation/accuracy': 0.6739000082015991, 'validation/loss': 1.34721040725708, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 1.9919664859771729, 'test/num_examples': 10000, 'score': 50882.97862505913, 'total_duration': 56279.887543678284, 'accumulated_submission_time': 50882.97862505913, 'accumulated_eval_time': 5386.50735449791, 'accumulated_logging_time': 4.539769411087036, 'global_step': 110892, 'preemption_count': 0}), (111809, {'train/accuracy': 0.7333788871765137, 'train/loss': 1.0607457160949707, 'validation/accuracy': 0.6744999885559082, 'validation/loss': 1.3343209028244019, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 1.9874333143234253, 'test/num_examples': 10000, 'score': 51303.28646111488, 'total_duration': 56748.28958415985, 'accumulated_submission_time': 51303.28646111488, 'accumulated_eval_time': 5434.5012810230255, 'accumulated_logging_time': 4.5913405418396, 'global_step': 111809, 'preemption_count': 0}), (112725, {'train/accuracy': 0.732714831829071, 'train/loss': 1.0809401273727417, 'validation/accuracy': 0.6717199683189392, 'validation/loss': 1.3633190393447876, 'validation/num_examples': 50000, 'test/accuracy': 0.5455000400543213, 'test/loss': 2.0178678035736084, 'test/num_examples': 10000, 'score': 51723.2543463707, 'total_duration': 57214.07962059975, 'accumulated_submission_time': 51723.2543463707, 'accumulated_eval_time': 5480.232470989227, 'accumulated_logging_time': 4.634597301483154, 'global_step': 112725, 'preemption_count': 0}), (113642, {'train/accuracy': 0.7489062547683716, 'train/loss': 0.9951205253601074, 'validation/accuracy': 0.67603999376297, 'validation/loss': 1.3217229843139648, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.9755213260650635, 'test/num_examples': 10000, 'score': 52143.389543771744, 'total_duration': 57680.778177022934, 'accumulated_submission_time': 52143.389543771744, 'accumulated_eval_time': 5526.704137802124, 'accumulated_logging_time': 4.677294731140137, 'global_step': 113642, 'preemption_count': 0}), (114559, {'train/accuracy': 0.745898425579071, 'train/loss': 1.0103362798690796, 'validation/accuracy': 0.6803799867630005, 'validation/loss': 1.3071465492248535, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.9714163541793823, 'test/num_examples': 10000, 'score': 52563.71753501892, 'total_duration': 58146.98582029343, 'accumulated_submission_time': 52563.71753501892, 'accumulated_eval_time': 5572.480983495712, 'accumulated_logging_time': 4.731476306915283, 'global_step': 114559, 'preemption_count': 0}), (115477, {'train/accuracy': 0.7442968487739563, 'train/loss': 1.0438237190246582, 'validation/accuracy': 0.6782799959182739, 'validation/loss': 1.3390940427780151, 'validation/num_examples': 50000, 'test/accuracy': 0.5524000525474548, 'test/loss': 1.993807315826416, 'test/num_examples': 10000, 'score': 52983.67902421951, 'total_duration': 58612.56900882721, 'accumulated_submission_time': 52983.67902421951, 'accumulated_eval_time': 5618.007117033005, 'accumulated_logging_time': 4.778652191162109, 'global_step': 115477, 'preemption_count': 0}), (116394, {'train/accuracy': 0.7509765625, 'train/loss': 1.0075005292892456, 'validation/accuracy': 0.6782000064849854, 'validation/loss': 1.3190701007843018, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 1.96835458278656, 'test/num_examples': 10000, 'score': 53403.994445085526, 'total_duration': 59079.571038246155, 'accumulated_submission_time': 53403.994445085526, 'accumulated_eval_time': 5664.597292423248, 'accumulated_logging_time': 4.826295614242554, 'global_step': 116394, 'preemption_count': 0}), (117310, {'train/accuracy': 0.7648828029632568, 'train/loss': 0.9348188638687134, 'validation/accuracy': 0.6816399693489075, 'validation/loss': 1.3038502931594849, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 1.9470292329788208, 'test/num_examples': 10000, 'score': 53824.31056809425, 'total_duration': 59545.865280628204, 'accumulated_submission_time': 53824.31056809425, 'accumulated_eval_time': 5710.4811136722565, 'accumulated_logging_time': 4.872780084609985, 'global_step': 117310, 'preemption_count': 0}), (118227, {'train/accuracy': 0.7464257478713989, 'train/loss': 1.029775619506836, 'validation/accuracy': 0.6835199594497681, 'validation/loss': 1.3185490369796753, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9517545700073242, 'test/num_examples': 10000, 'score': 54244.33873510361, 'total_duration': 60013.35964846611, 'accumulated_submission_time': 54244.33873510361, 'accumulated_eval_time': 5757.854747772217, 'accumulated_logging_time': 4.916918992996216, 'global_step': 118227, 'preemption_count': 0}), (119144, {'train/accuracy': 0.7531836032867432, 'train/loss': 0.9818204045295715, 'validation/accuracy': 0.6846799850463867, 'validation/loss': 1.2962895631790161, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 1.942766547203064, 'test/num_examples': 10000, 'score': 54664.30675196648, 'total_duration': 60477.11735010147, 'accumulated_submission_time': 54664.30675196648, 'accumulated_eval_time': 5801.55264043808, 'accumulated_logging_time': 4.960393190383911, 'global_step': 119144, 'preemption_count': 0}), (120060, {'train/accuracy': 0.7615820169448853, 'train/loss': 1.0147477388381958, 'validation/accuracy': 0.6805799603462219, 'validation/loss': 1.354517936706543, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.008143424987793, 'test/num_examples': 10000, 'score': 55084.343988895416, 'total_duration': 60941.289311409, 'accumulated_submission_time': 55084.343988895416, 'accumulated_eval_time': 5845.588560342789, 'accumulated_logging_time': 5.0106000900268555, 'global_step': 120060, 'preemption_count': 0}), (120978, {'train/accuracy': 0.7485546469688416, 'train/loss': 1.011296033859253, 'validation/accuracy': 0.6860799789428711, 'validation/loss': 1.2949484586715698, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 1.9418689012527466, 'test/num_examples': 10000, 'score': 55504.44954395294, 'total_duration': 61407.6800467968, 'accumulated_submission_time': 55504.44954395294, 'accumulated_eval_time': 5891.7737855911255, 'accumulated_logging_time': 5.061231851577759, 'global_step': 120978, 'preemption_count': 0}), (121895, {'train/accuracy': 0.7560937404632568, 'train/loss': 0.9640070796012878, 'validation/accuracy': 0.685259997844696, 'validation/loss': 1.2753068208694458, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 1.916074275970459, 'test/num_examples': 10000, 'score': 55924.674210071564, 'total_duration': 61872.32814979553, 'accumulated_submission_time': 55924.674210071564, 'accumulated_eval_time': 5936.102333545685, 'accumulated_logging_time': 5.1081626415252686, 'global_step': 121895, 'preemption_count': 0}), (122812, {'train/accuracy': 0.7655858993530273, 'train/loss': 0.926956832408905, 'validation/accuracy': 0.6911399960517883, 'validation/loss': 1.2588539123535156, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 1.9093610048294067, 'test/num_examples': 10000, 'score': 56344.74541926384, 'total_duration': 62336.69419193268, 'accumulated_submission_time': 56344.74541926384, 'accumulated_eval_time': 5980.293684959412, 'accumulated_logging_time': 5.162423133850098, 'global_step': 122812, 'preemption_count': 0}), (123732, {'train/accuracy': 0.7517968416213989, 'train/loss': 1.0083987712860107, 'validation/accuracy': 0.6892200112342834, 'validation/loss': 1.288521409034729, 'validation/num_examples': 50000, 'test/accuracy': 0.5617000460624695, 'test/loss': 1.9466270208358765, 'test/num_examples': 10000, 'score': 56765.029317855835, 'total_duration': 62801.70062446594, 'accumulated_submission_time': 56765.029317855835, 'accumulated_eval_time': 6024.918617486954, 'accumulated_logging_time': 5.211941957473755, 'global_step': 123732, 'preemption_count': 0}), (124648, {'train/accuracy': 0.7625976204872131, 'train/loss': 0.9537436366081238, 'validation/accuracy': 0.6960600018501282, 'validation/loss': 1.2512370347976685, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 1.9025684595108032, 'test/num_examples': 10000, 'score': 57185.08319354057, 'total_duration': 63268.40723109245, 'accumulated_submission_time': 57185.08319354057, 'accumulated_eval_time': 6071.47722363472, 'accumulated_logging_time': 5.257215738296509, 'global_step': 124648, 'preemption_count': 0}), (125566, {'train/accuracy': 0.7717187404632568, 'train/loss': 0.9368151426315308, 'validation/accuracy': 0.6967399716377258, 'validation/loss': 1.2695817947387695, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 1.9237370491027832, 'test/num_examples': 10000, 'score': 57605.488095998764, 'total_duration': 63732.91115617752, 'accumulated_submission_time': 57605.488095998764, 'accumulated_eval_time': 6115.479986190796, 'accumulated_logging_time': 5.305594205856323, 'global_step': 125566, 'preemption_count': 0}), (126484, {'train/accuracy': 0.7656054496765137, 'train/loss': 0.9447650909423828, 'validation/accuracy': 0.6959199905395508, 'validation/loss': 1.2564334869384766, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.9149582386016846, 'test/num_examples': 10000, 'score': 58025.8363161087, 'total_duration': 64197.31534385681, 'accumulated_submission_time': 58025.8363161087, 'accumulated_eval_time': 6159.436917304993, 'accumulated_logging_time': 5.356076002120972, 'global_step': 126484, 'preemption_count': 0}), (127403, {'train/accuracy': 0.7616015672683716, 'train/loss': 0.9457040429115295, 'validation/accuracy': 0.696399986743927, 'validation/loss': 1.245165467262268, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 1.8830746412277222, 'test/num_examples': 10000, 'score': 58445.861562252045, 'total_duration': 64663.24557733536, 'accumulated_submission_time': 58445.861562252045, 'accumulated_eval_time': 6205.239112854004, 'accumulated_logging_time': 5.409669399261475, 'global_step': 127403, 'preemption_count': 0}), (128313, {'train/accuracy': 0.7695116996765137, 'train/loss': 0.9001871347427368, 'validation/accuracy': 0.6969000101089478, 'validation/loss': 1.226536512374878, 'validation/num_examples': 50000, 'test/accuracy': 0.5778000354766846, 'test/loss': 1.8673149347305298, 'test/num_examples': 10000, 'score': 58865.828120946884, 'total_duration': 65126.63528132439, 'accumulated_submission_time': 58865.828120946884, 'accumulated_eval_time': 6248.568606853485, 'accumulated_logging_time': 5.454848766326904, 'global_step': 128313, 'preemption_count': 0}), (129230, {'train/accuracy': 0.7834765315055847, 'train/loss': 0.878718376159668, 'validation/accuracy': 0.7016800045967102, 'validation/loss': 1.2358816862106323, 'validation/num_examples': 50000, 'test/accuracy': 0.5782999992370605, 'test/loss': 1.8833563327789307, 'test/num_examples': 10000, 'score': 59285.90663409233, 'total_duration': 65588.18467664719, 'accumulated_submission_time': 59285.90663409233, 'accumulated_eval_time': 6289.945489883423, 'accumulated_logging_time': 5.501060485839844, 'global_step': 129230, 'preemption_count': 0}), (130148, {'train/accuracy': 0.7674999833106995, 'train/loss': 0.9318538308143616, 'validation/accuracy': 0.7002800107002258, 'validation/loss': 1.227251648902893, 'validation/num_examples': 50000, 'test/accuracy': 0.576200008392334, 'test/loss': 1.8715360164642334, 'test/num_examples': 10000, 'score': 59706.12528705597, 'total_duration': 66053.80458760262, 'accumulated_submission_time': 59706.12528705597, 'accumulated_eval_time': 6335.249958515167, 'accumulated_logging_time': 5.549408435821533, 'global_step': 130148, 'preemption_count': 0}), (131067, {'train/accuracy': 0.7759179472923279, 'train/loss': 0.9067143797874451, 'validation/accuracy': 0.7016400098800659, 'validation/loss': 1.23651921749115, 'validation/num_examples': 50000, 'test/accuracy': 0.576200008392334, 'test/loss': 1.8704854249954224, 'test/num_examples': 10000, 'score': 60126.05570912361, 'total_duration': 66519.6564245224, 'accumulated_submission_time': 60126.05570912361, 'accumulated_eval_time': 6381.077441215515, 'accumulated_logging_time': 5.593918085098267, 'global_step': 131067, 'preemption_count': 0}), (131984, {'train/accuracy': 0.7800976634025574, 'train/loss': 0.899075448513031, 'validation/accuracy': 0.7061600089073181, 'validation/loss': 1.231971263885498, 'validation/num_examples': 50000, 'test/accuracy': 0.584600031375885, 'test/loss': 1.8611094951629639, 'test/num_examples': 10000, 'score': 60546.168536663055, 'total_duration': 66985.56582260132, 'accumulated_submission_time': 60546.168536663055, 'accumulated_eval_time': 6426.768949747086, 'accumulated_logging_time': 5.6505677700042725, 'global_step': 131984, 'preemption_count': 0}), (132902, {'train/accuracy': 0.7770116925239563, 'train/loss': 0.8806419372558594, 'validation/accuracy': 0.7057600021362305, 'validation/loss': 1.1938964128494263, 'validation/num_examples': 50000, 'test/accuracy': 0.5800999999046326, 'test/loss': 1.8424324989318848, 'test/num_examples': 10000, 'score': 60966.1720366478, 'total_duration': 67448.69650387764, 'accumulated_submission_time': 60966.1720366478, 'accumulated_eval_time': 6469.7969336509705, 'accumulated_logging_time': 5.7013936042785645, 'global_step': 132902, 'preemption_count': 0}), (133822, {'train/accuracy': 0.7735351324081421, 'train/loss': 0.9415649771690369, 'validation/accuracy': 0.7021200060844421, 'validation/loss': 1.2566030025482178, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 1.9204665422439575, 'test/num_examples': 10000, 'score': 61386.14681506157, 'total_duration': 67912.33575248718, 'accumulated_submission_time': 61386.14681506157, 'accumulated_eval_time': 6513.367963075638, 'accumulated_logging_time': 5.74661111831665, 'global_step': 133822, 'preemption_count': 0}), (134742, {'train/accuracy': 0.7837499976158142, 'train/loss': 0.8793671727180481, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.2217317819595337, 'validation/num_examples': 50000, 'test/accuracy': 0.5799000263214111, 'test/loss': 1.8586112260818481, 'test/num_examples': 10000, 'score': 61806.27965426445, 'total_duration': 68377.54789853096, 'accumulated_submission_time': 61806.27965426445, 'accumulated_eval_time': 6558.35079741478, 'accumulated_logging_time': 5.793826341629028, 'global_step': 134742, 'preemption_count': 0}), (135659, {'train/accuracy': 0.7777929306030273, 'train/loss': 0.8910472393035889, 'validation/accuracy': 0.708579957485199, 'validation/loss': 1.2002291679382324, 'validation/num_examples': 50000, 'test/accuracy': 0.5830000042915344, 'test/loss': 1.8402303457260132, 'test/num_examples': 10000, 'score': 62226.222628593445, 'total_duration': 68841.37681627274, 'accumulated_submission_time': 62226.222628593445, 'accumulated_eval_time': 6602.141601085663, 'accumulated_logging_time': 5.841196537017822, 'global_step': 135659, 'preemption_count': 0}), (136576, {'train/accuracy': 0.7836328148841858, 'train/loss': 0.8509073257446289, 'validation/accuracy': 0.7106199860572815, 'validation/loss': 1.1824514865875244, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8325586318969727, 'test/num_examples': 10000, 'score': 62646.28657245636, 'total_duration': 69306.7888610363, 'accumulated_submission_time': 62646.28657245636, 'accumulated_eval_time': 6647.391499996185, 'accumulated_logging_time': 5.891319513320923, 'global_step': 136576, 'preemption_count': 0}), (137494, {'train/accuracy': 0.7882031202316284, 'train/loss': 0.8359939455986023, 'validation/accuracy': 0.7127199769020081, 'validation/loss': 1.1701191663742065, 'validation/num_examples': 50000, 'test/accuracy': 0.5861999988555908, 'test/loss': 1.8159235715866089, 'test/num_examples': 10000, 'score': 63066.573899030685, 'total_duration': 69768.32604432106, 'accumulated_submission_time': 63066.573899030685, 'accumulated_eval_time': 6688.541975975037, 'accumulated_logging_time': 5.941382884979248, 'global_step': 137494, 'preemption_count': 0}), (138412, {'train/accuracy': 0.7888476252555847, 'train/loss': 0.8295444250106812, 'validation/accuracy': 0.7119199633598328, 'validation/loss': 1.1780586242675781, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.816422939300537, 'test/num_examples': 10000, 'score': 63486.502836704254, 'total_duration': 70235.64741063118, 'accumulated_submission_time': 63486.502836704254, 'accumulated_eval_time': 6735.836848020554, 'accumulated_logging_time': 5.990756034851074, 'global_step': 138412, 'preemption_count': 0}), (139332, {'train/accuracy': 0.7895312309265137, 'train/loss': 0.8278987407684326, 'validation/accuracy': 0.7135799527168274, 'validation/loss': 1.1600850820541382, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.8050135374069214, 'test/num_examples': 10000, 'score': 63906.77509832382, 'total_duration': 70701.96142339706, 'accumulated_submission_time': 63906.77509832382, 'accumulated_eval_time': 6781.7830266952515, 'accumulated_logging_time': 6.037896156311035, 'global_step': 139332, 'preemption_count': 0}), (140251, {'train/accuracy': 0.7918164134025574, 'train/loss': 0.8284945487976074, 'validation/accuracy': 0.7123599648475647, 'validation/loss': 1.1808110475540161, 'validation/num_examples': 50000, 'test/accuracy': 0.5911000370979309, 'test/loss': 1.8107106685638428, 'test/num_examples': 10000, 'score': 64326.802830696106, 'total_duration': 71165.90361714363, 'accumulated_submission_time': 64326.802830696106, 'accumulated_eval_time': 6825.600820064545, 'accumulated_logging_time': 6.085220098495483, 'global_step': 140251, 'preemption_count': 0}), (141169, {'train/accuracy': 0.8020898103713989, 'train/loss': 0.7641717195510864, 'validation/accuracy': 0.714199960231781, 'validation/loss': 1.146799087524414, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.7914958000183105, 'test/num_examples': 10000, 'score': 64746.913219451904, 'total_duration': 71633.59379124641, 'accumulated_submission_time': 64746.913219451904, 'accumulated_eval_time': 6873.079336643219, 'accumulated_logging_time': 6.138157367706299, 'global_step': 141169, 'preemption_count': 0}), (142084, {'train/accuracy': 0.7903515696525574, 'train/loss': 0.8418607115745544, 'validation/accuracy': 0.718459963798523, 'validation/loss': 1.1597744226455688, 'validation/num_examples': 50000, 'test/accuracy': 0.5960000157356262, 'test/loss': 1.8071895837783813, 'test/num_examples': 10000, 'score': 65167.130407333374, 'total_duration': 72098.80448961258, 'accumulated_submission_time': 65167.130407333374, 'accumulated_eval_time': 6917.972964763641, 'accumulated_logging_time': 6.189510822296143, 'global_step': 142084, 'preemption_count': 0}), (143000, {'train/accuracy': 0.7939453125, 'train/loss': 0.8365713953971863, 'validation/accuracy': 0.7152599692344666, 'validation/loss': 1.1836109161376953, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.820393681526184, 'test/num_examples': 10000, 'score': 65587.38845300674, 'total_duration': 72562.14328551292, 'accumulated_submission_time': 65587.38845300674, 'accumulated_eval_time': 6960.956964015961, 'accumulated_logging_time': 6.237833261489868, 'global_step': 143000, 'preemption_count': 0}), (143917, {'train/accuracy': 0.8037304282188416, 'train/loss': 0.8105840682983398, 'validation/accuracy': 0.7167999744415283, 'validation/loss': 1.1841614246368408, 'validation/num_examples': 50000, 'test/accuracy': 0.5915000438690186, 'test/loss': 1.8317683935165405, 'test/num_examples': 10000, 'score': 66007.30506968498, 'total_duration': 73030.0181658268, 'accumulated_submission_time': 66007.30506968498, 'accumulated_eval_time': 7008.814731359482, 'accumulated_logging_time': 6.289177417755127, 'global_step': 143917, 'preemption_count': 0}), (144834, {'train/accuracy': 0.7900585532188416, 'train/loss': 0.8303824663162231, 'validation/accuracy': 0.7147600054740906, 'validation/loss': 1.1643662452697754, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.8147989511489868, 'test/num_examples': 10000, 'score': 66427.55252289772, 'total_duration': 73493.39942002296, 'accumulated_submission_time': 66427.55252289772, 'accumulated_eval_time': 7051.8511662483215, 'accumulated_logging_time': 6.3378260135650635, 'global_step': 144834, 'preemption_count': 0}), (145753, {'train/accuracy': 0.8025780916213989, 'train/loss': 0.7942774295806885, 'validation/accuracy': 0.7219399809837341, 'validation/loss': 1.1464852094650269, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.7750864028930664, 'test/num_examples': 10000, 'score': 66847.67327642441, 'total_duration': 73958.39906048775, 'accumulated_submission_time': 66847.67327642441, 'accumulated_eval_time': 7096.628978729248, 'accumulated_logging_time': 6.390943288803101, 'global_step': 145753, 'preemption_count': 0}), (146669, {'train/accuracy': 0.8016601204872131, 'train/loss': 0.7725005149841309, 'validation/accuracy': 0.7207199931144714, 'validation/loss': 1.1352367401123047, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.7748686075210571, 'test/num_examples': 10000, 'score': 67267.95164108276, 'total_duration': 74423.24955821037, 'accumulated_submission_time': 67267.95164108276, 'accumulated_eval_time': 7141.1040625572205, 'accumulated_logging_time': 6.439411401748657, 'global_step': 146669, 'preemption_count': 0}), (147587, {'train/accuracy': 0.7973241806030273, 'train/loss': 0.8021615147590637, 'validation/accuracy': 0.7207799553871155, 'validation/loss': 1.1381536722183228, 'validation/num_examples': 50000, 'test/accuracy': 0.5991000533103943, 'test/loss': 1.7693490982055664, 'test/num_examples': 10000, 'score': 67687.97954654694, 'total_duration': 74890.32166147232, 'accumulated_submission_time': 67687.97954654694, 'accumulated_eval_time': 7188.0415251255035, 'accumulated_logging_time': 6.497358798980713, 'global_step': 147587, 'preemption_count': 0}), (148506, {'train/accuracy': 0.8033788800239563, 'train/loss': 0.7781947255134583, 'validation/accuracy': 0.7247799634933472, 'validation/loss': 1.1242923736572266, 'validation/num_examples': 50000, 'test/accuracy': 0.6051000356674194, 'test/loss': 1.7475770711898804, 'test/num_examples': 10000, 'score': 68107.91986322403, 'total_duration': 75355.3785700798, 'accumulated_submission_time': 68107.91986322403, 'accumulated_eval_time': 7233.060445070267, 'accumulated_logging_time': 6.546364784240723, 'global_step': 148506, 'preemption_count': 0}), (149422, {'train/accuracy': 0.8059765696525574, 'train/loss': 0.7608180046081543, 'validation/accuracy': 0.7228999733924866, 'validation/loss': 1.124770998954773, 'validation/num_examples': 50000, 'test/accuracy': 0.6030000448226929, 'test/loss': 1.751966118812561, 'test/num_examples': 10000, 'score': 68527.9065463543, 'total_duration': 75820.22951364517, 'accumulated_submission_time': 68527.9065463543, 'accumulated_eval_time': 7277.828405618668, 'accumulated_logging_time': 6.59432315826416, 'global_step': 149422, 'preemption_count': 0}), (150341, {'train/accuracy': 0.818359375, 'train/loss': 0.733532190322876, 'validation/accuracy': 0.7276999950408936, 'validation/loss': 1.10873281955719, 'validation/num_examples': 50000, 'test/accuracy': 0.6095000505447388, 'test/loss': 1.739927053451538, 'test/num_examples': 10000, 'score': 68947.92167925835, 'total_duration': 76282.6093711853, 'accumulated_submission_time': 68947.92167925835, 'accumulated_eval_time': 7320.086533069611, 'accumulated_logging_time': 6.6522908210754395, 'global_step': 150341, 'preemption_count': 0}), (151259, {'train/accuracy': 0.8030859231948853, 'train/loss': 0.7721931338310242, 'validation/accuracy': 0.7271599769592285, 'validation/loss': 1.108335256576538, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.7397416830062866, 'test/num_examples': 10000, 'score': 69367.8230752945, 'total_duration': 76748.19864630699, 'accumulated_submission_time': 69367.8230752945, 'accumulated_eval_time': 7365.671882867813, 'accumulated_logging_time': 6.706914186477661, 'global_step': 151259, 'preemption_count': 0}), (152177, {'train/accuracy': 0.8080468773841858, 'train/loss': 0.78282630443573, 'validation/accuracy': 0.7276399731636047, 'validation/loss': 1.1330397129058838, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.7696969509124756, 'test/num_examples': 10000, 'score': 69787.99724078178, 'total_duration': 77214.63326454163, 'accumulated_submission_time': 69787.99724078178, 'accumulated_eval_time': 7411.831959962845, 'accumulated_logging_time': 6.759262323379517, 'global_step': 152177, 'preemption_count': 0}), (153094, {'train/accuracy': 0.8205859065055847, 'train/loss': 0.7144888639450073, 'validation/accuracy': 0.7292799949645996, 'validation/loss': 1.1079001426696777, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7434462308883667, 'test/num_examples': 10000, 'score': 70208.0367231369, 'total_duration': 77679.76422834396, 'accumulated_submission_time': 70208.0367231369, 'accumulated_eval_time': 7456.8257756233215, 'accumulated_logging_time': 6.808479070663452, 'global_step': 153094, 'preemption_count': 0}), (154011, {'train/accuracy': 0.8125, 'train/loss': 0.7371184229850769, 'validation/accuracy': 0.7307400107383728, 'validation/loss': 1.0911295413970947, 'validation/num_examples': 50000, 'test/accuracy': 0.6048000454902649, 'test/loss': 1.7228014469146729, 'test/num_examples': 10000, 'score': 70628.20611071587, 'total_duration': 78146.25729894638, 'accumulated_submission_time': 70628.20611071587, 'accumulated_eval_time': 7503.045190811157, 'accumulated_logging_time': 6.8642542362213135, 'global_step': 154011, 'preemption_count': 0}), (154931, {'train/accuracy': 0.8117187023162842, 'train/loss': 0.7915002107620239, 'validation/accuracy': 0.7309199571609497, 'validation/loss': 1.1422063112258911, 'validation/num_examples': 50000, 'test/accuracy': 0.6079000234603882, 'test/loss': 1.7812516689300537, 'test/num_examples': 10000, 'score': 71048.3641808033, 'total_duration': 78612.83320403099, 'accumulated_submission_time': 71048.3641808033, 'accumulated_eval_time': 7549.361694574356, 'accumulated_logging_time': 6.916259765625, 'global_step': 154931, 'preemption_count': 0}), (155850, {'train/accuracy': 0.8199804425239563, 'train/loss': 0.7226928472518921, 'validation/accuracy': 0.7334399819374084, 'validation/loss': 1.1000254154205322, 'validation/num_examples': 50000, 'test/accuracy': 0.6098000407218933, 'test/loss': 1.736724853515625, 'test/num_examples': 10000, 'score': 71468.62477397919, 'total_duration': 79076.29350209236, 'accumulated_submission_time': 71468.62477397919, 'accumulated_eval_time': 7592.454738378525, 'accumulated_logging_time': 6.974083662033081, 'global_step': 155850, 'preemption_count': 0}), (156768, {'train/accuracy': 0.8161718845367432, 'train/loss': 0.7212274074554443, 'validation/accuracy': 0.7339999675750732, 'validation/loss': 1.0783400535583496, 'validation/num_examples': 50000, 'test/accuracy': 0.6101000308990479, 'test/loss': 1.7127060890197754, 'test/num_examples': 10000, 'score': 71888.80693101883, 'total_duration': 79540.92209506035, 'accumulated_submission_time': 71888.80693101883, 'accumulated_eval_time': 7636.801145553589, 'accumulated_logging_time': 7.0256242752075195, 'global_step': 156768, 'preemption_count': 0}), (157685, {'train/accuracy': 0.8190429210662842, 'train/loss': 0.7114618420600891, 'validation/accuracy': 0.7349599599838257, 'validation/loss': 1.075613021850586, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.7125824689865112, 'test/num_examples': 10000, 'score': 72308.92112541199, 'total_duration': 80006.86278057098, 'accumulated_submission_time': 72308.92112541199, 'accumulated_eval_time': 7682.525693893433, 'accumulated_logging_time': 7.078494548797607, 'global_step': 157685, 'preemption_count': 0}), (158600, {'train/accuracy': 0.8244531154632568, 'train/loss': 0.6801031231880188, 'validation/accuracy': 0.7355999946594238, 'validation/loss': 1.0589427947998047, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.695249319076538, 'test/num_examples': 10000, 'score': 72728.9054980278, 'total_duration': 80471.80396866798, 'accumulated_submission_time': 72728.9054980278, 'accumulated_eval_time': 7727.382160902023, 'accumulated_logging_time': 7.130783319473267, 'global_step': 158600, 'preemption_count': 0}), (159516, {'train/accuracy': 0.8199218511581421, 'train/loss': 0.7040935158729553, 'validation/accuracy': 0.737559974193573, 'validation/loss': 1.0672333240509033, 'validation/num_examples': 50000, 'test/accuracy': 0.6140000224113464, 'test/loss': 1.6969444751739502, 'test/num_examples': 10000, 'score': 73148.94817018509, 'total_duration': 80938.11805319786, 'accumulated_submission_time': 73148.94817018509, 'accumulated_eval_time': 7773.549040794373, 'accumulated_logging_time': 7.186861276626587, 'global_step': 159516, 'preemption_count': 0}), (160432, {'train/accuracy': 0.8233007788658142, 'train/loss': 0.6964321732521057, 'validation/accuracy': 0.7393199801445007, 'validation/loss': 1.0613622665405273, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.701716661453247, 'test/num_examples': 10000, 'score': 73568.97808933258, 'total_duration': 81403.82748365402, 'accumulated_submission_time': 73568.97808933258, 'accumulated_eval_time': 7819.126227378845, 'accumulated_logging_time': 7.240983247756958, 'global_step': 160432, 'preemption_count': 0}), (161350, {'train/accuracy': 0.8231640458106995, 'train/loss': 0.701531708240509, 'validation/accuracy': 0.7382000088691711, 'validation/loss': 1.0777002573013306, 'validation/num_examples': 50000, 'test/accuracy': 0.6128000020980835, 'test/loss': 1.720652461051941, 'test/num_examples': 10000, 'score': 73989.23976898193, 'total_duration': 81866.85880875587, 'accumulated_submission_time': 73989.23976898193, 'accumulated_eval_time': 7861.79163479805, 'accumulated_logging_time': 7.296997785568237, 'global_step': 161350, 'preemption_count': 0}), (162268, {'train/accuracy': 0.8293749690055847, 'train/loss': 0.6697791218757629, 'validation/accuracy': 0.7391200065612793, 'validation/loss': 1.0580641031265259, 'validation/num_examples': 50000, 'test/accuracy': 0.6132000088691711, 'test/loss': 1.7002379894256592, 'test/num_examples': 10000, 'score': 74409.15633821487, 'total_duration': 82332.41932559013, 'accumulated_submission_time': 74409.15633821487, 'accumulated_eval_time': 7907.331022977829, 'accumulated_logging_time': 7.35301947593689, 'global_step': 162268, 'preemption_count': 0}), (163188, {'train/accuracy': 0.8237695097923279, 'train/loss': 0.6979393362998962, 'validation/accuracy': 0.739579975605011, 'validation/loss': 1.063696026802063, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.7050611972808838, 'test/num_examples': 10000, 'score': 74829.30160307884, 'total_duration': 82799.23604655266, 'accumulated_submission_time': 74829.30160307884, 'accumulated_eval_time': 7953.901052236557, 'accumulated_logging_time': 7.404864072799683, 'global_step': 163188, 'preemption_count': 0}), (164106, {'train/accuracy': 0.8280468583106995, 'train/loss': 0.6734815835952759, 'validation/accuracy': 0.7416799664497375, 'validation/loss': 1.0467333793640137, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.6836295127868652, 'test/num_examples': 10000, 'score': 75249.34499502182, 'total_duration': 83264.83662986755, 'accumulated_submission_time': 75249.34499502182, 'accumulated_eval_time': 7999.356866836548, 'accumulated_logging_time': 7.457672595977783, 'global_step': 164106, 'preemption_count': 0}), (165024, {'train/accuracy': 0.8347070217132568, 'train/loss': 0.6491798162460327, 'validation/accuracy': 0.7430399656295776, 'validation/loss': 1.0401471853256226, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.6772472858428955, 'test/num_examples': 10000, 'score': 75669.39481902122, 'total_duration': 83733.0430316925, 'accumulated_submission_time': 75669.39481902122, 'accumulated_eval_time': 8047.415571212769, 'accumulated_logging_time': 7.507667779922485, 'global_step': 165024, 'preemption_count': 0}), (165943, {'train/accuracy': 0.8305078148841858, 'train/loss': 0.6913408637046814, 'validation/accuracy': 0.7436800003051758, 'validation/loss': 1.0642290115356445, 'validation/num_examples': 50000, 'test/accuracy': 0.6195000410079956, 'test/loss': 1.6991186141967773, 'test/num_examples': 10000, 'score': 76089.31130671501, 'total_duration': 84199.780534029, 'accumulated_submission_time': 76089.31130671501, 'accumulated_eval_time': 8094.137541294098, 'accumulated_logging_time': 7.5582780838012695, 'global_step': 165943, 'preemption_count': 0}), (166860, {'train/accuracy': 0.83509761095047, 'train/loss': 0.6374510526657104, 'validation/accuracy': 0.7443199753761292, 'validation/loss': 1.0309221744537354, 'validation/num_examples': 50000, 'test/accuracy': 0.6202000379562378, 'test/loss': 1.6646264791488647, 'test/num_examples': 10000, 'score': 76509.37047219276, 'total_duration': 84664.75477600098, 'accumulated_submission_time': 76509.37047219276, 'accumulated_eval_time': 8138.949564218521, 'accumulated_logging_time': 7.613423585891724, 'global_step': 166860, 'preemption_count': 0}), (167778, {'train/accuracy': 0.8340038657188416, 'train/loss': 0.6508774161338806, 'validation/accuracy': 0.7443599700927734, 'validation/loss': 1.0415518283843994, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.6723334789276123, 'test/num_examples': 10000, 'score': 76929.66366410255, 'total_duration': 85130.87297224998, 'accumulated_submission_time': 76929.66366410255, 'accumulated_eval_time': 8184.667482852936, 'accumulated_logging_time': 7.671286582946777, 'global_step': 167778, 'preemption_count': 0}), (168694, {'train/accuracy': 0.8335155844688416, 'train/loss': 0.6408066153526306, 'validation/accuracy': 0.74617999792099, 'validation/loss': 1.0215263366699219, 'validation/num_examples': 50000, 'test/accuracy': 0.6210000514984131, 'test/loss': 1.6536414623260498, 'test/num_examples': 10000, 'score': 77349.58613371849, 'total_duration': 85597.61217308044, 'accumulated_submission_time': 77349.58613371849, 'accumulated_eval_time': 8231.38061952591, 'accumulated_logging_time': 7.726383686065674, 'global_step': 168694, 'preemption_count': 0})], 'global_step': 169071}
I0204 11:07:48.069149 139863983413056 submission_runner.py:586] Timing: 77520.06314373016
I0204 11:07:48.069227 139863983413056 submission_runner.py:588] Total number of evals: 185
I0204 11:07:48.069271 139863983413056 submission_runner.py:589] ====================
I0204 11:07:48.071421 139863983413056 submission_runner.py:673] Final imagenet_vit score: 77520.00591540337
