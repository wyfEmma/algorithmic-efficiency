python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=1480595982 --max_global_steps=80000 2>&1 | tee -a /logs/ogbg_jax_02-04-2024-21-11-51.log
I0204 21:12:12.587203 140205209478976 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_0/ogbg_jax.
I0204 21:12:13.629135 140205209478976 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0204 21:12:13.629917 140205209478976 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0204 21:12:13.630054 140205209478976 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0204 21:12:13.631066 140205209478976 submission_runner.py:542] Using RNG seed 1480595982
I0204 21:12:14.758861 140205209478976 submission_runner.py:551] --- Tuning run 1/5 ---
I0204 21:12:14.759059 140205209478976 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_1.
I0204 21:12:14.759406 140205209478976 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_1/hparams.json.
I0204 21:12:14.944319 140205209478976 submission_runner.py:206] Initializing dataset.
I0204 21:12:15.059231 140205209478976 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:12:15.068239 140205209478976 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0204 21:12:15.329286 140205209478976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0204 21:12:15.390151 140205209478976 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:12:15.470344 140205209478976 submission_runner.py:213] Initializing model.
I0204 21:12:20.129336 140205209478976 submission_runner.py:255] Initializing optimizer.
I0204 21:12:20.780411 140205209478976 submission_runner.py:262] Initializing metrics bundle.
I0204 21:12:20.780613 140205209478976 submission_runner.py:280] Initializing checkpoint and logger.
I0204 21:12:20.781604 140205209478976 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_1 with prefix checkpoint_
I0204 21:12:20.781743 140205209478976 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_1/meta_data_0.json.
I0204 21:12:20.781925 140205209478976 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0204 21:12:20.781985 140205209478976 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0204 21:12:21.116269 140205209478976 logger_utils.py:220] Unable to record git information. Continuing without it.
I0204 21:12:21.423709 140205209478976 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_1/flags_0.json.
I0204 21:12:21.433653 140205209478976 submission_runner.py:314] Starting training loop.
I0204 21:12:39.551244 140040923490048 logging_writer.py:48] [0] global_step=0, grad_norm=2.3602805137634277, loss=0.7395736575126648
I0204 21:12:39.568076 140205209478976 spec.py:321] Evaluating on the training split.
I0204 21:12:39.574325 140205209478976 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:12:39.578959 140205209478976 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0204 21:12:39.648173 140205209478976 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:14:33.782419 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 21:14:33.785835 140205209478976 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:14:33.789831 140205209478976 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0204 21:14:33.856511 140205209478976 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:16:07.409749 140205209478976 spec.py:349] Evaluating on the test split.
I0204 21:16:07.413168 140205209478976 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:16:07.417426 140205209478976 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0204 21:16:07.483200 140205209478976 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:17:44.309098 140205209478976 submission_runner.py:408] Time since start: 322.88s, 	Step: 1, 	{'train/accuracy': 0.5092470049858093, 'train/loss': 0.7391904592514038, 'train/mean_average_precision': 0.022183556073118962, 'validation/accuracy': 0.5064646005630493, 'validation/loss': 0.7422076463699341, 'validation/mean_average_precision': 0.025314419280320945, 'validation/num_examples': 43793, 'test/accuracy': 0.5047287344932556, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.026939659112222583, 'test/num_examples': 43793, 'score': 18.134385585784912, 'total_duration': 322.87538480758667, 'accumulated_submission_time': 18.134385585784912, 'accumulated_eval_time': 304.74095821380615, 'accumulated_logging_time': 0}
I0204 21:17:44.327278 140036477093632 logging_writer.py:48] [1] accumulated_eval_time=304.740958, accumulated_logging_time=0, accumulated_submission_time=18.134386, global_step=1, preemption_count=0, score=18.134386, test/accuracy=0.504729, test/loss=0.743874, test/mean_average_precision=0.026940, test/num_examples=43793, total_duration=322.875385, train/accuracy=0.509247, train/loss=0.739190, train/mean_average_precision=0.022184, validation/accuracy=0.506465, validation/loss=0.742208, validation/mean_average_precision=0.025314, validation/num_examples=43793
I0204 21:18:16.474880 140037844416256 logging_writer.py:48] [100] global_step=100, grad_norm=0.6410756707191467, loss=0.4908663332462311
I0204 21:18:48.657808 140036477093632 logging_writer.py:48] [200] global_step=200, grad_norm=0.40028145909309387, loss=0.36105412244796753
I0204 21:19:20.809056 140037844416256 logging_writer.py:48] [300] global_step=300, grad_norm=0.2900595963001251, loss=0.2593868374824524
I0204 21:19:52.912133 140036477093632 logging_writer.py:48] [400] global_step=400, grad_norm=0.19244447350502014, loss=0.17325295507907867
I0204 21:20:24.948832 140037844416256 logging_writer.py:48] [500] global_step=500, grad_norm=0.1175621822476387, loss=0.12124864757061005
I0204 21:20:56.385062 140036477093632 logging_writer.py:48] [600] global_step=600, grad_norm=0.0742836445569992, loss=0.08897362649440765
I0204 21:21:29.324540 140037844416256 logging_writer.py:48] [700] global_step=700, grad_norm=0.04829800873994827, loss=0.07373613864183426
I0204 21:21:44.413693 140205209478976 spec.py:321] Evaluating on the training split.
I0204 21:23:34.799711 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 21:23:37.786476 140205209478976 spec.py:349] Evaluating on the test split.
I0204 21:23:40.680433 140205209478976 submission_runner.py:408] Time since start: 679.25s, 	Step: 747, 	{'train/accuracy': 0.9867319464683533, 'train/loss': 0.07046488672494888, 'train/mean_average_precision': 0.03332047189447332, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07882650941610336, 'validation/mean_average_precision': 0.03370828574743398, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08166052401065826, 'test/mean_average_precision': 0.036926219938296194, 'test/num_examples': 43793, 'score': 258.18942499160767, 'total_duration': 679.2467248439789, 'accumulated_submission_time': 258.18942499160767, 'accumulated_eval_time': 421.0076684951782, 'accumulated_logging_time': 0.029463768005371094}
I0204 21:23:40.697993 140037861201664 logging_writer.py:48] [747] accumulated_eval_time=421.007668, accumulated_logging_time=0.029464, accumulated_submission_time=258.189425, global_step=747, preemption_count=0, score=258.189425, test/accuracy=0.983142, test/loss=0.081661, test/mean_average_precision=0.036926, test/num_examples=43793, total_duration=679.246725, train/accuracy=0.986732, train/loss=0.070465, train/mean_average_precision=0.033320, validation/accuracy=0.984118, validation/loss=0.078827, validation/mean_average_precision=0.033708, validation/num_examples=43793
I0204 21:23:58.036631 140037869594368 logging_writer.py:48] [800] global_step=800, grad_norm=0.0852346196770668, loss=0.0640549287199974
I0204 21:24:29.943479 140037861201664 logging_writer.py:48] [900] global_step=900, grad_norm=0.09359320253133774, loss=0.05869091674685478
I0204 21:25:01.684830 140037869594368 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.10805060714483261, loss=0.05908416956663132
I0204 21:25:33.712145 140037861201664 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.03873418644070625, loss=0.053865183144807816
I0204 21:26:05.785445 140037869594368 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.18394944071769714, loss=0.050147902220487595
I0204 21:26:37.389443 140037861201664 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.18000899255275726, loss=0.049618691205978394
I0204 21:27:09.501029 140037869594368 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.09363407641649246, loss=0.04955245554447174
I0204 21:27:40.898806 140205209478976 spec.py:321] Evaluating on the training split.
I0204 21:29:36.497120 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 21:29:39.519598 140205209478976 spec.py:349] Evaluating on the test split.
I0204 21:29:42.499634 140205209478976 submission_runner.py:408] Time since start: 1041.07s, 	Step: 1500, 	{'train/accuracy': 0.9873274564743042, 'train/loss': 0.04891827329993248, 'train/mean_average_precision': 0.08356941409016463, 'validation/accuracy': 0.9845482110977173, 'validation/loss': 0.05748947337269783, 'validation/mean_average_precision': 0.08481615630268999, 'validation/num_examples': 43793, 'test/accuracy': 0.9835611581802368, 'test/loss': 0.060405172407627106, 'test/mean_average_precision': 0.08839088611255327, 'test/num_examples': 43793, 'score': 498.358202457428, 'total_duration': 1041.0658912658691, 'accumulated_submission_time': 498.358202457428, 'accumulated_eval_time': 542.6084208488464, 'accumulated_logging_time': 0.060240745544433594}
I0204 21:29:42.515123 140037753956096 logging_writer.py:48] [1500] accumulated_eval_time=542.608421, accumulated_logging_time=0.060241, accumulated_submission_time=498.358202, global_step=1500, preemption_count=0, score=498.358202, test/accuracy=0.983561, test/loss=0.060405, test/mean_average_precision=0.088391, test/num_examples=43793, total_duration=1041.065891, train/accuracy=0.987327, train/loss=0.048918, train/mean_average_precision=0.083569, validation/accuracy=0.984548, validation/loss=0.057489, validation/mean_average_precision=0.084816, validation/num_examples=43793
I0204 21:29:42.889128 140037844416256 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.36238598823547363, loss=0.0509413443505764
I0204 21:30:15.414524 140037753956096 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.3347069025039673, loss=0.045058779418468475
I0204 21:30:47.290869 140037844416256 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.16034609079360962, loss=0.050992392003536224
I0204 21:31:19.363460 140037753956096 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.16988250613212585, loss=0.05439605191349983
I0204 21:31:51.479246 140037844416256 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.11209466308355331, loss=0.0497376024723053
I0204 21:32:23.715147 140037753956096 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.2999753952026367, loss=0.05470873415470123
I0204 21:32:56.231894 140037844416256 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.16854768991470337, loss=0.05156014859676361
I0204 21:33:28.430768 140037753956096 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.2646559178829193, loss=0.04239509254693985
I0204 21:33:42.617091 140205209478976 spec.py:321] Evaluating on the training split.
I0204 21:35:39.600393 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 21:35:42.622615 140205209478976 spec.py:349] Evaluating on the test split.
I0204 21:35:45.571386 140205209478976 submission_runner.py:408] Time since start: 1404.14s, 	Step: 2245, 	{'train/accuracy': 0.9878538846969604, 'train/loss': 0.04451540485024452, 'train/mean_average_precision': 0.12710063223144522, 'validation/accuracy': 0.9850134253501892, 'validation/loss': 0.053862106055021286, 'validation/mean_average_precision': 0.12863561760452455, 'validation/num_examples': 43793, 'test/accuracy': 0.9840468168258667, 'test/loss': 0.05668896064162254, 'test/mean_average_precision': 0.12697697458880344, 'test/num_examples': 43793, 'score': 738.4302983283997, 'total_duration': 1404.1376762390137, 'accumulated_submission_time': 738.4302983283997, 'accumulated_eval_time': 665.5626842975616, 'accumulated_logging_time': 0.08686161041259766}
I0204 21:35:45.587317 140037861201664 logging_writer.py:48] [2245] accumulated_eval_time=665.562684, accumulated_logging_time=0.086862, accumulated_submission_time=738.430298, global_step=2245, preemption_count=0, score=738.430298, test/accuracy=0.984047, test/loss=0.056689, test/mean_average_precision=0.126977, test/num_examples=43793, total_duration=1404.137676, train/accuracy=0.987854, train/loss=0.044515, train/mean_average_precision=0.127101, validation/accuracy=0.985013, validation/loss=0.053862, validation/mean_average_precision=0.128636, validation/num_examples=43793
I0204 21:36:03.791296 140037869594368 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.06781817227602005, loss=0.041154760867357254
I0204 21:36:36.172941 140037861201664 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.22892899811267853, loss=0.04757171496748924
I0204 21:37:08.504593 140037869594368 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.13504602015018463, loss=0.047458942979574203
I0204 21:37:40.949579 140037861201664 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.13381940126419067, loss=0.04486000910401344
I0204 21:38:13.543133 140037869594368 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.09543725848197937, loss=0.040155019611120224
I0204 21:38:46.519861 140037861201664 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.21646881103515625, loss=0.04422725364565849
I0204 21:39:19.178111 140037869594368 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.05434419587254524, loss=0.04391368851065636
I0204 21:39:45.668267 140205209478976 spec.py:321] Evaluating on the training split.
I0204 21:41:44.741628 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 21:41:47.784223 140205209478976 spec.py:349] Evaluating on the test split.
I0204 21:41:50.817030 140205209478976 submission_runner.py:408] Time since start: 1769.38s, 	Step: 2983, 	{'train/accuracy': 0.9879559278488159, 'train/loss': 0.04276910424232483, 'train/mean_average_precision': 0.1545525683655089, 'validation/accuracy': 0.9852378964424133, 'validation/loss': 0.052117809653282166, 'validation/mean_average_precision': 0.1480877293086972, 'validation/num_examples': 43793, 'test/accuracy': 0.9842873215675354, 'test/loss': 0.05477682128548622, 'test/mean_average_precision': 0.15463342800933638, 'test/num_examples': 43793, 'score': 978.4800200462341, 'total_duration': 1769.3833138942719, 'accumulated_submission_time': 978.4800200462341, 'accumulated_eval_time': 790.7114028930664, 'accumulated_logging_time': 0.11533927917480469}
I0204 21:41:50.833406 140037753956096 logging_writer.py:48] [2983] accumulated_eval_time=790.711403, accumulated_logging_time=0.115339, accumulated_submission_time=978.480020, global_step=2983, preemption_count=0, score=978.480020, test/accuracy=0.984287, test/loss=0.054777, test/mean_average_precision=0.154633, test/num_examples=43793, total_duration=1769.383314, train/accuracy=0.987956, train/loss=0.042769, train/mean_average_precision=0.154553, validation/accuracy=0.985238, validation/loss=0.052118, validation/mean_average_precision=0.148088, validation/num_examples=43793
I0204 21:41:56.685853 140037852808960 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.15997610986232758, loss=0.04175162315368652
I0204 21:42:29.546692 140037753956096 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.116461381316185, loss=0.043886687606573105
I0204 21:43:02.499122 140037852808960 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.09258104860782623, loss=0.046789392828941345
I0204 21:43:34.675740 140037753956096 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.04715855419635773, loss=0.04106904938817024
I0204 21:44:06.624573 140037852808960 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.1251945197582245, loss=0.0473746620118618
I0204 21:44:38.846193 140037753956096 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.09554830193519592, loss=0.04744253680109978
I0204 21:45:10.762951 140037852808960 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.10395302623510361, loss=0.04493187367916107
I0204 21:45:42.756106 140037753956096 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.09307338297367096, loss=0.04170418530702591
I0204 21:45:51.069027 140205209478976 spec.py:321] Evaluating on the training split.
I0204 21:47:48.268729 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 21:47:51.287030 140205209478976 spec.py:349] Evaluating on the test split.
I0204 21:47:54.232338 140205209478976 submission_runner.py:408] Time since start: 2132.80s, 	Step: 3727, 	{'train/accuracy': 0.9880806803703308, 'train/loss': 0.04140053316950798, 'train/mean_average_precision': 0.186444753574127, 'validation/accuracy': 0.9853852987289429, 'validation/loss': 0.0503360740840435, 'validation/mean_average_precision': 0.1722589120405069, 'validation/num_examples': 43793, 'test/accuracy': 0.9844751358032227, 'test/loss': 0.053038205951452255, 'test/mean_average_precision': 0.17465184291979313, 'test/num_examples': 43793, 'score': 1218.683366060257, 'total_duration': 2132.798628091812, 'accumulated_submission_time': 1218.683366060257, 'accumulated_eval_time': 913.8746719360352, 'accumulated_logging_time': 0.14537787437438965}
I0204 21:47:54.247621 140037861201664 logging_writer.py:48] [3727] accumulated_eval_time=913.874672, accumulated_logging_time=0.145378, accumulated_submission_time=1218.683366, global_step=3727, preemption_count=0, score=1218.683366, test/accuracy=0.984475, test/loss=0.053038, test/mean_average_precision=0.174652, test/num_examples=43793, total_duration=2132.798628, train/accuracy=0.988081, train/loss=0.041401, train/mean_average_precision=0.186445, validation/accuracy=0.985385, validation/loss=0.050336, validation/mean_average_precision=0.172259, validation/num_examples=43793
I0204 21:48:17.778960 140037869594368 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.1051747053861618, loss=0.044066570699214935
I0204 21:48:49.713835 140037861201664 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.06783568859100342, loss=0.04309220612049103
I0204 21:49:21.217267 140037869594368 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.09944193810224533, loss=0.03775789216160774
I0204 21:49:53.056349 140037861201664 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.07669855654239655, loss=0.041550248861312866
I0204 21:50:25.021173 140037869594368 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.04694705456495285, loss=0.04140816256403923
I0204 21:50:56.668433 140037861201664 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.04169671609997749, loss=0.04141068086028099
I0204 21:51:28.672172 140037869594368 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.27917706966400146, loss=0.047565147280693054
I0204 21:51:54.320417 140205209478976 spec.py:321] Evaluating on the training split.
I0204 21:53:53.957663 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 21:53:56.960544 140205209478976 spec.py:349] Evaluating on the test split.
I0204 21:54:00.007980 140205209478976 submission_runner.py:408] Time since start: 2498.57s, 	Step: 4481, 	{'train/accuracy': 0.9885159134864807, 'train/loss': 0.03997578099370003, 'train/mean_average_precision': 0.20383492520000512, 'validation/accuracy': 0.9855237007141113, 'validation/loss': 0.049245577305555344, 'validation/mean_average_precision': 0.1806705486653925, 'validation/num_examples': 43793, 'test/accuracy': 0.9846848845481873, 'test/loss': 0.05176909267902374, 'test/mean_average_precision': 0.18150076981735463, 'test/num_examples': 43793, 'score': 1458.725549697876, 'total_duration': 2498.5742666721344, 'accumulated_submission_time': 1458.725549697876, 'accumulated_eval_time': 1039.5621936321259, 'accumulated_logging_time': 0.17210650444030762}
I0204 21:54:00.025585 140037877987072 logging_writer.py:48] [4481] accumulated_eval_time=1039.562194, accumulated_logging_time=0.172107, accumulated_submission_time=1458.725550, global_step=4481, preemption_count=0, score=1458.725550, test/accuracy=0.984685, test/loss=0.051769, test/mean_average_precision=0.181501, test/num_examples=43793, total_duration=2498.574267, train/accuracy=0.988516, train/loss=0.039976, train/mean_average_precision=0.203835, validation/accuracy=0.985524, validation/loss=0.049246, validation/mean_average_precision=0.180671, validation/num_examples=43793
I0204 21:54:06.639657 140143457441536 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.057529136538505554, loss=0.04009304568171501
I0204 21:54:39.648392 140037877987072 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0713529959321022, loss=0.043816495686769485
I0204 21:55:12.175747 140143457441536 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.06564144045114517, loss=0.04023095592856407
I0204 21:55:44.613312 140037877987072 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.11592184007167816, loss=0.04521975293755531
I0204 21:56:16.479748 140143457441536 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.06466750055551529, loss=0.047737207263708115
I0204 21:56:48.040458 140037877987072 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0560365654528141, loss=0.042183250188827515
I0204 21:57:20.023816 140143457441536 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.03060619719326496, loss=0.039240941405296326
I0204 21:57:51.801634 140037877987072 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.029410431161522865, loss=0.03780417516827583
I0204 21:58:00.259097 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:00:01.014266 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:00:04.075474 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:00:07.060121 140205209478976 submission_runner.py:408] Time since start: 2865.63s, 	Step: 5228, 	{'train/accuracy': 0.9886970520019531, 'train/loss': 0.03851606324315071, 'train/mean_average_precision': 0.22704527736097163, 'validation/accuracy': 0.9857315421104431, 'validation/loss': 0.04844583570957184, 'validation/mean_average_precision': 0.19226982985119906, 'validation/num_examples': 43793, 'test/accuracy': 0.984925389289856, 'test/loss': 0.05113844946026802, 'test/mean_average_precision': 0.19083808361818078, 'test/num_examples': 43793, 'score': 1698.9222049713135, 'total_duration': 2865.626400947571, 'accumulated_submission_time': 1698.9222049713135, 'accumulated_eval_time': 1166.363187789917, 'accumulated_logging_time': 0.20289373397827148}
I0204 22:00:07.076030 140044326471424 logging_writer.py:48] [5228] accumulated_eval_time=1166.363188, accumulated_logging_time=0.202894, accumulated_submission_time=1698.922205, global_step=5228, preemption_count=0, score=1698.922205, test/accuracy=0.984925, test/loss=0.051138, test/mean_average_precision=0.190838, test/num_examples=43793, total_duration=2865.626401, train/accuracy=0.988697, train/loss=0.038516, train/mean_average_precision=0.227045, validation/accuracy=0.985732, validation/loss=0.048446, validation/mean_average_precision=0.192270, validation/num_examples=43793
I0204 22:00:29.644638 140044334864128 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.09481969475746155, loss=0.03975258767604828
I0204 22:01:00.735440 140044326471424 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.09328389167785645, loss=0.040431711822748184
I0204 22:01:32.005079 140044334864128 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.05417133867740631, loss=0.04459978640079498
I0204 22:02:03.352552 140044326471424 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0771087184548378, loss=0.03924993425607681
I0204 22:02:34.595111 140044334864128 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.0534273199737072, loss=0.0399932861328125
I0204 22:03:05.781987 140044326471424 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.06432252377271652, loss=0.04305338114500046
I0204 22:03:37.495468 140044334864128 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.045568715780973434, loss=0.04337136447429657
I0204 22:04:07.221807 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:06:07.422175 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:06:10.509934 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:06:13.492636 140205209478976 submission_runner.py:408] Time since start: 3232.06s, 	Step: 5996, 	{'train/accuracy': 0.9888204336166382, 'train/loss': 0.03822404518723488, 'train/mean_average_precision': 0.23757638562749245, 'validation/accuracy': 0.9858850240707397, 'validation/loss': 0.04780585691332817, 'validation/mean_average_precision': 0.2007561521562516, 'validation/num_examples': 43793, 'test/accuracy': 0.9850820899009705, 'test/loss': 0.05045550689101219, 'test/mean_average_precision': 0.20090724192004614, 'test/num_examples': 43793, 'score': 1939.038080215454, 'total_duration': 3232.058907032013, 'accumulated_submission_time': 1939.038080215454, 'accumulated_eval_time': 1292.633964061737, 'accumulated_logging_time': 0.22948431968688965}
I0204 22:06:13.508827 140037886379776 logging_writer.py:48] [5996] accumulated_eval_time=1292.633964, accumulated_logging_time=0.229484, accumulated_submission_time=1939.038080, global_step=5996, preemption_count=0, score=1939.038080, test/accuracy=0.985082, test/loss=0.050456, test/mean_average_precision=0.200907, test/num_examples=43793, total_duration=3232.058907, train/accuracy=0.988820, train/loss=0.038224, train/mean_average_precision=0.237576, validation/accuracy=0.985885, validation/loss=0.047806, validation/mean_average_precision=0.200756, validation/num_examples=43793
I0204 22:06:15.125948 140143465834240 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.03534214198589325, loss=0.0365452840924263
I0204 22:06:46.235635 140037886379776 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.05713159963488579, loss=0.044168464839458466
I0204 22:07:17.809289 140143465834240 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.04200454428792, loss=0.03832966089248657
I0204 22:07:50.207953 140037886379776 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.04805123433470726, loss=0.04191006347537041
I0204 22:08:21.809842 140143465834240 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.028696073219180107, loss=0.03939200937747955
I0204 22:08:53.165506 140037886379776 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.03360675275325775, loss=0.037655461579561234
I0204 22:09:24.512646 140143465834240 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.034239184111356735, loss=0.04156986251473427
I0204 22:09:55.635863 140037886379776 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.06109572947025299, loss=0.04323926568031311
I0204 22:10:13.578616 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:12:09.248667 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:12:12.285929 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:12:15.309057 140205209478976 submission_runner.py:408] Time since start: 3593.88s, 	Step: 6757, 	{'train/accuracy': 0.9890872836112976, 'train/loss': 0.0374721959233284, 'train/mean_average_precision': 0.2541572959008027, 'validation/accuracy': 0.9859747290611267, 'validation/loss': 0.04729709401726723, 'validation/mean_average_precision': 0.20384817768363916, 'validation/num_examples': 43793, 'test/accuracy': 0.9850327968597412, 'test/loss': 0.049796223640441895, 'test/mean_average_precision': 0.20342232567746538, 'test/num_examples': 43793, 'score': 2179.076942920685, 'total_duration': 3593.875339746475, 'accumulated_submission_time': 2179.076942920685, 'accumulated_eval_time': 1414.3643636703491, 'accumulated_logging_time': 0.256913423538208}
I0204 22:12:15.324935 140044326471424 logging_writer.py:48] [6757] accumulated_eval_time=1414.364364, accumulated_logging_time=0.256913, accumulated_submission_time=2179.076943, global_step=6757, preemption_count=0, score=2179.076943, test/accuracy=0.985033, test/loss=0.049796, test/mean_average_precision=0.203422, test/num_examples=43793, total_duration=3593.875340, train/accuracy=0.989087, train/loss=0.037472, train/mean_average_precision=0.254157, validation/accuracy=0.985975, validation/loss=0.047297, validation/mean_average_precision=0.203848, validation/num_examples=43793
I0204 22:12:29.253518 140143457441536 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.027089189738035202, loss=0.03774937987327576
I0204 22:13:01.171079 140044326471424 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.025139013305306435, loss=0.03810575231909752
I0204 22:13:32.676404 140143457441536 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.05683203414082527, loss=0.03938668966293335
I0204 22:14:03.942710 140044326471424 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.021533193066716194, loss=0.03499056026339531
I0204 22:14:35.244067 140143457441536 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.025728454813361168, loss=0.03847789019346237
I0204 22:15:06.389222 140044326471424 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.02234884537756443, loss=0.03825816139578819
I0204 22:15:37.536938 140143457441536 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.022181624546647072, loss=0.04051081836223602
I0204 22:16:08.857880 140044326471424 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.03635018691420555, loss=0.04126757010817528
I0204 22:16:15.463340 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:18:15.275515 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:18:18.295180 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:18:21.241801 140205209478976 submission_runner.py:408] Time since start: 3959.81s, 	Step: 7522, 	{'train/accuracy': 0.989159345626831, 'train/loss': 0.0369904600083828, 'train/mean_average_precision': 0.2583130676710853, 'validation/accuracy': 0.9860392808914185, 'validation/loss': 0.04717843979597092, 'validation/mean_average_precision': 0.21269932487038165, 'validation/num_examples': 43793, 'test/accuracy': 0.9851840138435364, 'test/loss': 0.04962729662656784, 'test/mean_average_precision': 0.212045926660142, 'test/num_examples': 43793, 'score': 2419.183886051178, 'total_duration': 3959.8080909252167, 'accumulated_submission_time': 2419.183886051178, 'accumulated_eval_time': 1540.1427791118622, 'accumulated_logging_time': 0.2850782871246338}
I0204 22:18:21.258641 140044334864128 logging_writer.py:48] [7522] accumulated_eval_time=1540.142779, accumulated_logging_time=0.285078, accumulated_submission_time=2419.183886, global_step=7522, preemption_count=0, score=2419.183886, test/accuracy=0.985184, test/loss=0.049627, test/mean_average_precision=0.212046, test/num_examples=43793, total_duration=3959.808091, train/accuracy=0.989159, train/loss=0.036990, train/mean_average_precision=0.258313, validation/accuracy=0.986039, validation/loss=0.047178, validation/mean_average_precision=0.212699, validation/num_examples=43793
I0204 22:18:46.695442 140143465834240 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.03954031318426132, loss=0.04192729666829109
I0204 22:19:18.232348 140044334864128 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03256422281265259, loss=0.04507606476545334
I0204 22:19:50.205406 140143465834240 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.02884492091834545, loss=0.03816714137792587
I0204 22:20:22.068424 140044334864128 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.0416237898170948, loss=0.04019427299499512
I0204 22:20:53.174744 140143465834240 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.02464781329035759, loss=0.03929124400019646
I0204 22:21:24.554357 140044334864128 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.02610277570784092, loss=0.04255183786153793
I0204 22:21:55.780741 140143465834240 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.06400856375694275, loss=0.03899405151605606
I0204 22:22:21.392066 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:24:20.767279 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:24:23.771611 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:24:26.671474 140205209478976 submission_runner.py:408] Time since start: 4325.24s, 	Step: 8281, 	{'train/accuracy': 0.9891362190246582, 'train/loss': 0.03711735084652901, 'train/mean_average_precision': 0.2612576523422221, 'validation/accuracy': 0.9862495064735413, 'validation/loss': 0.046459268778562546, 'validation/mean_average_precision': 0.21966237376798012, 'validation/num_examples': 43793, 'test/accuracy': 0.9853053092956543, 'test/loss': 0.04907968267798424, 'test/mean_average_precision': 0.2183831775864873, 'test/num_examples': 43793, 'score': 2659.2870647907257, 'total_duration': 4325.237766027451, 'accumulated_submission_time': 2659.2870647907257, 'accumulated_eval_time': 1665.422149181366, 'accumulated_logging_time': 0.31254100799560547}
I0204 22:24:26.689064 140037886379776 logging_writer.py:48] [8281] accumulated_eval_time=1665.422149, accumulated_logging_time=0.312541, accumulated_submission_time=2659.287065, global_step=8281, preemption_count=0, score=2659.287065, test/accuracy=0.985305, test/loss=0.049080, test/mean_average_precision=0.218383, test/num_examples=43793, total_duration=4325.237766, train/accuracy=0.989136, train/loss=0.037117, train/mean_average_precision=0.261258, validation/accuracy=0.986250, validation/loss=0.046459, validation/mean_average_precision=0.219662, validation/num_examples=43793
I0204 22:24:32.977051 140044326471424 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.029618609696626663, loss=0.0439799465239048
I0204 22:25:04.201451 140037886379776 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.027514221146702766, loss=0.03570140525698662
I0204 22:25:35.647384 140044326471424 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.029894616454839706, loss=0.04338237643241882
I0204 22:26:06.649725 140037886379776 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.03167421370744705, loss=0.03977539762854576
I0204 22:26:37.791139 140044326471424 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.02594497986137867, loss=0.039774369448423386
I0204 22:27:09.019398 140037886379776 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.02097865752875805, loss=0.03804832324385643
I0204 22:27:40.232254 140044326471424 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.037303902208805084, loss=0.04098895937204361
I0204 22:28:11.457557 140037886379776 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.03068951703608036, loss=0.037862978875637054
I0204 22:28:26.905427 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:30:27.776851 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:30:30.799030 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:30:33.717580 140205209478976 submission_runner.py:408] Time since start: 4692.28s, 	Step: 9050, 	{'train/accuracy': 0.9894993901252747, 'train/loss': 0.03580339998006821, 'train/mean_average_precision': 0.2808998759655631, 'validation/accuracy': 0.9862414002418518, 'validation/loss': 0.04602714627981186, 'validation/mean_average_precision': 0.2250249947677923, 'validation/num_examples': 43793, 'test/accuracy': 0.985432505607605, 'test/loss': 0.04857400059700012, 'test/mean_average_precision': 0.22985825223934117, 'test/num_examples': 43793, 'score': 2899.4726645946503, 'total_duration': 4692.283871173859, 'accumulated_submission_time': 2899.4726645946503, 'accumulated_eval_time': 1792.2342555522919, 'accumulated_logging_time': 0.34105992317199707}
I0204 22:30:33.734023 140143457441536 logging_writer.py:48] [9050] accumulated_eval_time=1792.234256, accumulated_logging_time=0.341060, accumulated_submission_time=2899.472665, global_step=9050, preemption_count=0, score=2899.472665, test/accuracy=0.985433, test/loss=0.048574, test/mean_average_precision=0.229858, test/num_examples=43793, total_duration=4692.283871, train/accuracy=0.989499, train/loss=0.035803, train/mean_average_precision=0.280900, validation/accuracy=0.986241, validation/loss=0.046027, validation/mean_average_precision=0.225025, validation/num_examples=43793
I0204 22:30:49.706696 140143465834240 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.02571278065443039, loss=0.04350157827138901
I0204 22:31:21.439581 140143457441536 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.03776772320270538, loss=0.03948034346103668
I0204 22:31:52.913782 140143465834240 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.03063330054283142, loss=0.0409080907702446
I0204 22:32:24.960926 140143457441536 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.026739923283457756, loss=0.03736642003059387
I0204 22:32:56.209197 140143465834240 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.03080672398209572, loss=0.038235776126384735
I0204 22:33:28.046514 140143457441536 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.02635527402162552, loss=0.0362287238240242
I0204 22:33:59.624703 140143465834240 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.04192772135138512, loss=0.0374622642993927
I0204 22:34:31.131116 140143457441536 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.03551106154918671, loss=0.04061759263277054
I0204 22:34:34.032812 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:36:36.875455 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:36:39.863354 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:36:42.765259 140205209478976 submission_runner.py:408] Time since start: 5061.33s, 	Step: 9810, 	{'train/accuracy': 0.9892947673797607, 'train/loss': 0.03601916506886482, 'train/mean_average_precision': 0.28290001463691294, 'validation/accuracy': 0.9861679077148438, 'validation/loss': 0.04629398137331009, 'validation/mean_average_precision': 0.2249200090916605, 'validation/num_examples': 43793, 'test/accuracy': 0.9851722121238708, 'test/loss': 0.04909360036253929, 'test/mean_average_precision': 0.2129779767804349, 'test/num_examples': 43793, 'score': 3139.7406141757965, 'total_duration': 5061.331539392471, 'accumulated_submission_time': 3139.7406141757965, 'accumulated_eval_time': 1920.9666464328766, 'accumulated_logging_time': 0.368741512298584}
I0204 22:36:42.782418 140037886379776 logging_writer.py:48] [9810] accumulated_eval_time=1920.966646, accumulated_logging_time=0.368742, accumulated_submission_time=3139.740614, global_step=9810, preemption_count=0, score=3139.740614, test/accuracy=0.985172, test/loss=0.049094, test/mean_average_precision=0.212978, test/num_examples=43793, total_duration=5061.331539, train/accuracy=0.989295, train/loss=0.036019, train/mean_average_precision=0.282900, validation/accuracy=0.986168, validation/loss=0.046294, validation/mean_average_precision=0.224920, validation/num_examples=43793
I0204 22:37:11.600810 140044326471424 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.023182759061455727, loss=0.03552757203578949
I0204 22:37:42.856583 140037886379776 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.02178242616355419, loss=0.0408141203224659
I0204 22:38:14.939033 140044326471424 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.02606731466948986, loss=0.03931553661823273
I0204 22:38:47.086785 140037886379776 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.034388985484838486, loss=0.04041104391217232
I0204 22:39:19.073556 140044326471424 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0281257014721632, loss=0.038470588624477386
I0204 22:39:50.748950 140037886379776 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.026414213702082634, loss=0.034318193793296814
I0204 22:40:22.656172 140044326471424 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.025659501552581787, loss=0.03758497163653374
I0204 22:40:42.908048 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:42:51.024725 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:42:54.431486 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:42:57.720503 140205209478976 submission_runner.py:408] Time since start: 5436.29s, 	Step: 10564, 	{'train/accuracy': 0.9896652698516846, 'train/loss': 0.03486141934990883, 'train/mean_average_precision': 0.2978428145748989, 'validation/accuracy': 0.9864857792854309, 'validation/loss': 0.04574143886566162, 'validation/mean_average_precision': 0.23948837319628052, 'validation/num_examples': 43793, 'test/accuracy': 0.9855567812919617, 'test/loss': 0.048363097012043, 'test/mean_average_precision': 0.24234612427888075, 'test/num_examples': 43793, 'score': 3379.834892272949, 'total_duration': 5436.286778688431, 'accumulated_submission_time': 3379.834892272949, 'accumulated_eval_time': 2055.779053211212, 'accumulated_logging_time': 0.3968634605407715}
I0204 22:42:57.739804 140037877987072 logging_writer.py:48] [10564] accumulated_eval_time=2055.779053, accumulated_logging_time=0.396863, accumulated_submission_time=3379.834892, global_step=10564, preemption_count=0, score=3379.834892, test/accuracy=0.985557, test/loss=0.048363, test/mean_average_precision=0.242346, test/num_examples=43793, total_duration=5436.286779, train/accuracy=0.989665, train/loss=0.034861, train/mean_average_precision=0.297843, validation/accuracy=0.986486, validation/loss=0.045741, validation/mean_average_precision=0.239488, validation/num_examples=43793
I0204 22:43:09.650784 140044334864128 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.022728299722075462, loss=0.04038120061159134
I0204 22:43:41.656275 140037877987072 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.022662809118628502, loss=0.03798110783100128
I0204 22:44:14.176145 140044334864128 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0223771333694458, loss=0.03587869554758072
I0204 22:44:45.910235 140037877987072 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.03385325148701668, loss=0.037715863436460495
I0204 22:45:17.833871 140044334864128 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.02774590626358986, loss=0.03817136213183403
I0204 22:45:49.868956 140037877987072 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03355294093489647, loss=0.03679859638214111
I0204 22:46:21.832591 140044334864128 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.0271004531532526, loss=0.03636524826288223
I0204 22:46:53.810959 140037877987072 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.03894473612308502, loss=0.03967812657356262
I0204 22:46:57.935800 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:49:02.845485 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:49:05.875216 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:49:08.833154 140205209478976 submission_runner.py:408] Time since start: 5807.40s, 	Step: 11314, 	{'train/accuracy': 0.9897768497467041, 'train/loss': 0.03419574350118637, 'train/mean_average_precision': 0.3297334600062838, 'validation/accuracy': 0.9864062070846558, 'validation/loss': 0.0457753986120224, 'validation/mean_average_precision': 0.23745007205009375, 'validation/num_examples': 43793, 'test/accuracy': 0.9855571985244751, 'test/loss': 0.04848429560661316, 'test/mean_average_precision': 0.24124910774038727, 'test/num_examples': 43793, 'score': 3619.9956657886505, 'total_duration': 5807.399445056915, 'accumulated_submission_time': 3619.9956657886505, 'accumulated_eval_time': 2186.6763796806335, 'accumulated_logging_time': 0.4272780418395996}
I0204 22:49:08.850773 140037886379776 logging_writer.py:48] [11314] accumulated_eval_time=2186.676380, accumulated_logging_time=0.427278, accumulated_submission_time=3619.995666, global_step=11314, preemption_count=0, score=3619.995666, test/accuracy=0.985557, test/loss=0.048484, test/mean_average_precision=0.241249, test/num_examples=43793, total_duration=5807.399445, train/accuracy=0.989777, train/loss=0.034196, train/mean_average_precision=0.329733, validation/accuracy=0.986406, validation/loss=0.045775, validation/mean_average_precision=0.237450, validation/num_examples=43793
I0204 22:49:36.646050 140143465834240 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.024053966626524925, loss=0.03776579722762108
I0204 22:50:08.475820 140037886379776 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.03709953650832176, loss=0.036807719618082047
I0204 22:50:40.019980 140143465834240 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.027803316712379456, loss=0.03352447599172592
I0204 22:51:11.770615 140037886379776 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.026705441996455193, loss=0.036362357437610626
I0204 22:51:44.061081 140143465834240 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.02386644296348095, loss=0.037601765245199203
I0204 22:52:16.665600 140037886379776 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.04921891540288925, loss=0.04022200033068657
I0204 22:52:48.599730 140143465834240 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.03375868871808052, loss=0.037933364510536194
I0204 22:53:09.126122 140205209478976 spec.py:321] Evaluating on the training split.
I0204 22:55:14.082383 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 22:55:17.124164 140205209478976 spec.py:349] Evaluating on the test split.
I0204 22:55:20.141430 140205209478976 submission_runner.py:408] Time since start: 6178.71s, 	Step: 12064, 	{'train/accuracy': 0.9900395274162292, 'train/loss': 0.033200040459632874, 'train/mean_average_precision': 0.35565118295834997, 'validation/accuracy': 0.986559271812439, 'validation/loss': 0.045159950852394104, 'validation/mean_average_precision': 0.24599681831864537, 'validation/num_examples': 43793, 'test/accuracy': 0.9856915473937988, 'test/loss': 0.0479581244289875, 'test/mean_average_precision': 0.2449247904619788, 'test/num_examples': 43793, 'score': 3860.2378079891205, 'total_duration': 6178.707715749741, 'accumulated_submission_time': 3860.2378079891205, 'accumulated_eval_time': 2317.691652059555, 'accumulated_logging_time': 0.45623111724853516}
I0204 22:55:20.160284 140044326471424 logging_writer.py:48] [12064] accumulated_eval_time=2317.691652, accumulated_logging_time=0.456231, accumulated_submission_time=3860.237808, global_step=12064, preemption_count=0, score=3860.237808, test/accuracy=0.985692, test/loss=0.047958, test/mean_average_precision=0.244925, test/num_examples=43793, total_duration=6178.707716, train/accuracy=0.990040, train/loss=0.033200, train/mean_average_precision=0.355651, validation/accuracy=0.986559, validation/loss=0.045160, validation/mean_average_precision=0.245997, validation/num_examples=43793
I0204 22:55:31.946680 140044334864128 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.03490408882498741, loss=0.03439087048172951
I0204 22:56:04.025937 140044326471424 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.03125607594847679, loss=0.03657097741961479
I0204 22:56:35.822257 140044334864128 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.029429206624627113, loss=0.03536355867981911
I0204 22:57:07.711711 140044326471424 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.03437275066971779, loss=0.03402300179004669
I0204 22:57:39.976427 140044334864128 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.03973388671875, loss=0.03472380340099335
I0204 22:58:11.685630 140044326471424 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.03186764940619469, loss=0.03510841354727745
I0204 22:58:42.967916 140044334864128 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.04765070229768753, loss=0.03448539227247238
I0204 22:59:14.574344 140044326471424 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.053911514580249786, loss=0.03665237873792648
I0204 22:59:20.285109 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:01:23.512775 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:01:26.512906 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:01:29.494389 140205209478976 submission_runner.py:408] Time since start: 6548.06s, 	Step: 12819, 	{'train/accuracy': 0.9901712536811829, 'train/loss': 0.03270101174712181, 'train/mean_average_precision': 0.3520209612511431, 'validation/accuracy': 0.9864898324012756, 'validation/loss': 0.04514045640826225, 'validation/mean_average_precision': 0.23950796320556614, 'validation/num_examples': 43793, 'test/accuracy': 0.985736608505249, 'test/loss': 0.04779091477394104, 'test/mean_average_precision': 0.2415669797862868, 'test/num_examples': 43793, 'score': 4100.330991983414, 'total_duration': 6548.0606780052185, 'accumulated_submission_time': 4100.330991983414, 'accumulated_eval_time': 2446.9008860588074, 'accumulated_logging_time': 0.4860799312591553}
I0204 23:01:29.511727 140037877987072 logging_writer.py:48] [12819] accumulated_eval_time=2446.900886, accumulated_logging_time=0.486080, accumulated_submission_time=4100.330992, global_step=12819, preemption_count=0, score=4100.330992, test/accuracy=0.985737, test/loss=0.047791, test/mean_average_precision=0.241567, test/num_examples=43793, total_duration=6548.060678, train/accuracy=0.990171, train/loss=0.032701, train/mean_average_precision=0.352021, validation/accuracy=0.986490, validation/loss=0.045140, validation/mean_average_precision=0.239508, validation/num_examples=43793
I0204 23:01:57.454054 140143465834240 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.05520495027303696, loss=0.03759161755442619
I0204 23:02:28.808240 140037877987072 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.04236428067088127, loss=0.04125238209962845
I0204 23:03:00.557228 140143465834240 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.029373466968536377, loss=0.040202196687459946
I0204 23:03:31.994440 140037877987072 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.045049380511045456, loss=0.032586902379989624
I0204 23:04:03.476784 140143465834240 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.05128595605492592, loss=0.037438225001096725
I0204 23:04:35.466311 140037877987072 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.04340686276555061, loss=0.03783252462744713
I0204 23:05:07.364921 140143465834240 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.029990633949637413, loss=0.0342494361102581
I0204 23:05:29.595505 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:07:30.361346 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:07:33.348801 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:07:36.314318 140205209478976 submission_runner.py:408] Time since start: 6914.88s, 	Step: 13572, 	{'train/accuracy': 0.9905293583869934, 'train/loss': 0.031600289046764374, 'train/mean_average_precision': 0.368745897355923, 'validation/accuracy': 0.9866258502006531, 'validation/loss': 0.04486170411109924, 'validation/mean_average_precision': 0.2575587049856996, 'validation/num_examples': 43793, 'test/accuracy': 0.9857243895530701, 'test/loss': 0.04759618267416954, 'test/mean_average_precision': 0.25348135402824057, 'test/num_examples': 43793, 'score': 4340.384344100952, 'total_duration': 6914.880608558655, 'accumulated_submission_time': 4340.384344100952, 'accumulated_eval_time': 2573.6196570396423, 'accumulated_logging_time': 0.514338493347168}
I0204 23:07:36.331869 140037886379776 logging_writer.py:48] [13572] accumulated_eval_time=2573.619657, accumulated_logging_time=0.514338, accumulated_submission_time=4340.384344, global_step=13572, preemption_count=0, score=4340.384344, test/accuracy=0.985724, test/loss=0.047596, test/mean_average_precision=0.253481, test/num_examples=43793, total_duration=6914.880609, train/accuracy=0.990529, train/loss=0.031600, train/mean_average_precision=0.368746, validation/accuracy=0.986626, validation/loss=0.044862, validation/mean_average_precision=0.257559, validation/num_examples=43793
I0204 23:07:45.474413 140044326471424 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.05618664622306824, loss=0.03487579897046089
I0204 23:08:17.160080 140037886379776 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.045794837176799774, loss=0.036282654851675034
I0204 23:08:48.969933 140044326471424 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03582824021577835, loss=0.03348972648382187
I0204 23:09:20.531370 140037886379776 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03471333533525467, loss=0.03335566073656082
I0204 23:09:52.734369 140044326471424 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.04481324926018715, loss=0.03586525470018387
I0204 23:10:24.410302 140037886379776 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03252614289522171, loss=0.0319582000374794
I0204 23:10:56.292641 140044326471424 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.03620016202330589, loss=0.03151566535234451
I0204 23:11:28.411477 140037886379776 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.04370715469121933, loss=0.03733551874756813
I0204 23:11:36.369948 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:13:42.235464 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:13:45.264339 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:13:48.283504 140205209478976 submission_runner.py:408] Time since start: 7286.85s, 	Step: 14326, 	{'train/accuracy': 0.9907206296920776, 'train/loss': 0.031035449355840683, 'train/mean_average_precision': 0.39466925296175914, 'validation/accuracy': 0.986632764339447, 'validation/loss': 0.04433930665254593, 'validation/mean_average_precision': 0.25108633559962107, 'validation/num_examples': 43793, 'test/accuracy': 0.9858802556991577, 'test/loss': 0.046953026205301285, 'test/mean_average_precision': 0.2496725852030368, 'test/num_examples': 43793, 'score': 4580.391752004623, 'total_duration': 7286.849791765213, 'accumulated_submission_time': 4580.391752004623, 'accumulated_eval_time': 2705.533165216446, 'accumulated_logging_time': 0.5428283214569092}
I0204 23:13:48.302048 140037877987072 logging_writer.py:48] [14326] accumulated_eval_time=2705.533165, accumulated_logging_time=0.542828, accumulated_submission_time=4580.391752, global_step=14326, preemption_count=0, score=4580.391752, test/accuracy=0.985880, test/loss=0.046953, test/mean_average_precision=0.249673, test/num_examples=43793, total_duration=7286.849792, train/accuracy=0.990721, train/loss=0.031035, train/mean_average_precision=0.394669, validation/accuracy=0.986633, validation/loss=0.044339, validation/mean_average_precision=0.251086, validation/num_examples=43793
I0204 23:14:12.374903 140143465834240 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.059641674160957336, loss=0.03589607775211334
I0204 23:14:44.349208 140037877987072 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.03795653209090233, loss=0.03306948393583298
I0204 23:15:16.389605 140143465834240 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.048538777977228165, loss=0.03720370680093765
I0204 23:15:48.083271 140037877987072 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.05970447510480881, loss=0.039614029228687286
I0204 23:16:19.806639 140143465834240 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.04325277730822563, loss=0.035445209592580795
I0204 23:16:51.414374 140037877987072 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.05493800342082977, loss=0.036575671285390854
I0204 23:17:23.190939 140143465834240 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.05262758210301399, loss=0.0351051390171051
I0204 23:17:48.329420 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:19:52.720927 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:19:55.758681 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:19:58.761515 140205209478976 submission_runner.py:408] Time since start: 7657.33s, 	Step: 15080, 	{'train/accuracy': 0.9904849529266357, 'train/loss': 0.031830884516239166, 'train/mean_average_precision': 0.37670945859015104, 'validation/accuracy': 0.9865093231201172, 'validation/loss': 0.04480632394552231, 'validation/mean_average_precision': 0.2508813830749123, 'validation/num_examples': 43793, 'test/accuracy': 0.9857193827629089, 'test/loss': 0.04746022820472717, 'test/mean_average_precision': 0.24386262425404154, 'test/num_examples': 43793, 'score': 4820.388184309006, 'total_duration': 7657.3278040885925, 'accumulated_submission_time': 4820.388184309006, 'accumulated_eval_time': 2835.9652168750763, 'accumulated_logging_time': 0.5726001262664795}
I0204 23:19:58.780049 140037886379776 logging_writer.py:48] [15080] accumulated_eval_time=2835.965217, accumulated_logging_time=0.572600, accumulated_submission_time=4820.388184, global_step=15080, preemption_count=0, score=4820.388184, test/accuracy=0.985719, test/loss=0.047460, test/mean_average_precision=0.243863, test/num_examples=43793, total_duration=7657.327804, train/accuracy=0.990485, train/loss=0.031831, train/mean_average_precision=0.376709, validation/accuracy=0.986509, validation/loss=0.044806, validation/mean_average_precision=0.250881, validation/num_examples=43793
I0204 23:20:05.541466 140044334864128 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.06575453281402588, loss=0.03920302912592888
I0204 23:20:37.295833 140037886379776 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.050126124173402786, loss=0.03984720632433891
I0204 23:21:09.539787 140044334864128 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.043624408543109894, loss=0.03503475710749626
I0204 23:21:42.071751 140037886379776 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.04754360020160675, loss=0.03554517403244972
I0204 23:22:14.221634 140044334864128 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.06997516751289368, loss=0.035855431109666824
I0204 23:22:46.037179 140037886379776 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.05681360512971878, loss=0.03516233712434769
I0204 23:23:18.296935 140044334864128 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.051938582211732864, loss=0.03258460387587547
I0204 23:23:50.511604 140037886379776 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.050246186554431915, loss=0.033882126212120056
I0204 23:23:59.045278 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:26:02.328403 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:26:05.401068 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:26:08.415602 140205209478976 submission_runner.py:408] Time since start: 8026.98s, 	Step: 15827, 	{'train/accuracy': 0.9903941750526428, 'train/loss': 0.03175380080938339, 'train/mean_average_precision': 0.38474196639505487, 'validation/accuracy': 0.9866757392883301, 'validation/loss': 0.04488752409815788, 'validation/mean_average_precision': 0.25469984764009845, 'validation/num_examples': 43793, 'test/accuracy': 0.985835611820221, 'test/loss': 0.04772009328007698, 'test/mean_average_precision': 0.24732493344320675, 'test/num_examples': 43793, 'score': 5060.620755910873, 'total_duration': 8026.981867313385, 'accumulated_submission_time': 5060.620755910873, 'accumulated_eval_time': 2965.335473537445, 'accumulated_logging_time': 0.6022679805755615}
I0204 23:26:08.434616 140044326471424 logging_writer.py:48] [15827] accumulated_eval_time=2965.335474, accumulated_logging_time=0.602268, accumulated_submission_time=5060.620756, global_step=15827, preemption_count=0, score=5060.620756, test/accuracy=0.985836, test/loss=0.047720, test/mean_average_precision=0.247325, test/num_examples=43793, total_duration=8026.981867, train/accuracy=0.990394, train/loss=0.031754, train/mean_average_precision=0.384742, validation/accuracy=0.986676, validation/loss=0.044888, validation/mean_average_precision=0.254700, validation/num_examples=43793
I0204 23:26:32.103186 140143465834240 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.05025879666209221, loss=0.032980188727378845
I0204 23:27:04.714361 140044326471424 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.07675426453351974, loss=0.0348934531211853
I0204 23:27:36.780925 140143465834240 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.045701250433921814, loss=0.03446754440665245
I0204 23:28:08.761585 140044326471424 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.056084126234054565, loss=0.03632180765271187
I0204 23:28:40.896599 140143465834240 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.057382822036743164, loss=0.035269808024168015
I0204 23:29:12.937684 140044326471424 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.0725637748837471, loss=0.033610984683036804
I0204 23:29:44.790750 140143465834240 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.06885053217411041, loss=0.033506810665130615
I0204 23:30:08.470186 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:32:13.185067 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:32:16.176324 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:32:19.120228 140205209478976 submission_runner.py:408] Time since start: 8397.69s, 	Step: 16575, 	{'train/accuracy': 0.9903529286384583, 'train/loss': 0.0319628044962883, 'train/mean_average_precision': 0.37372795584763974, 'validation/accuracy': 0.9867126941680908, 'validation/loss': 0.044516146183013916, 'validation/mean_average_precision': 0.26091488359638376, 'validation/num_examples': 43793, 'test/accuracy': 0.9857538938522339, 'test/loss': 0.047431498765945435, 'test/mean_average_precision': 0.2495560120245449, 'test/num_examples': 43793, 'score': 5300.623321056366, 'total_duration': 8397.686516284943, 'accumulated_submission_time': 5300.623321056366, 'accumulated_eval_time': 3095.9854731559753, 'accumulated_logging_time': 0.6344373226165771}
I0204 23:32:19.139095 140037877987072 logging_writer.py:48] [16575] accumulated_eval_time=3095.985473, accumulated_logging_time=0.634437, accumulated_submission_time=5300.623321, global_step=16575, preemption_count=0, score=5300.623321, test/accuracy=0.985754, test/loss=0.047431, test/mean_average_precision=0.249556, test/num_examples=43793, total_duration=8397.686516, train/accuracy=0.990353, train/loss=0.031963, train/mean_average_precision=0.373728, validation/accuracy=0.986713, validation/loss=0.044516, validation/mean_average_precision=0.260915, validation/num_examples=43793
I0204 23:32:27.366544 140037886379776 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.048946790397167206, loss=0.035764019936323166
I0204 23:32:58.945128 140037877987072 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.06630433350801468, loss=0.03442499041557312
I0204 23:33:30.786723 140037886379776 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.05997898057103157, loss=0.03614557906985283
I0204 23:34:02.550291 140037877987072 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.06683653593063354, loss=0.037184689193964005
I0204 23:34:34.100280 140037886379776 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.055159736424684525, loss=0.034448280930519104
I0204 23:35:05.956746 140037877987072 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.04796377569437027, loss=0.031152434647083282
I0204 23:35:38.110537 140037886379776 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.06142861768603325, loss=0.03457577899098396
I0204 23:36:10.097327 140037877987072 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.0684620663523674, loss=0.03568614274263382
I0204 23:36:19.143940 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:38:24.744477 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:38:27.768355 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:38:30.758386 140205209478976 submission_runner.py:408] Time since start: 8769.32s, 	Step: 17330, 	{'train/accuracy': 0.990510880947113, 'train/loss': 0.03144244849681854, 'train/mean_average_precision': 0.3802214268304177, 'validation/accuracy': 0.9867005348205566, 'validation/loss': 0.04471373185515404, 'validation/mean_average_precision': 0.261765784988841, 'validation/num_examples': 43793, 'test/accuracy': 0.9857543110847473, 'test/loss': 0.04779241234064102, 'test/mean_average_precision': 0.2385126380118646, 'test/num_examples': 43793, 'score': 5540.597485303879, 'total_duration': 8769.324578762054, 'accumulated_submission_time': 5540.597485303879, 'accumulated_eval_time': 3227.5997779369354, 'accumulated_logging_time': 0.6642558574676514}
I0204 23:38:30.776730 140044326471424 logging_writer.py:48] [17330] accumulated_eval_time=3227.599778, accumulated_logging_time=0.664256, accumulated_submission_time=5540.597485, global_step=17330, preemption_count=0, score=5540.597485, test/accuracy=0.985754, test/loss=0.047792, test/mean_average_precision=0.238513, test/num_examples=43793, total_duration=8769.324579, train/accuracy=0.990511, train/loss=0.031442, train/mean_average_precision=0.380221, validation/accuracy=0.986701, validation/loss=0.044714, validation/mean_average_precision=0.261766, validation/num_examples=43793
I0204 23:38:53.599234 140143465834240 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.05585060268640518, loss=0.03991692140698433
I0204 23:39:25.669735 140044326471424 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.09490590542554855, loss=0.0372956246137619
I0204 23:39:57.938867 140143465834240 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.05674220249056816, loss=0.034355279058218
I0204 23:40:30.102294 140044326471424 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.0555485300719738, loss=0.03555731102824211
I0204 23:41:02.608655 140143465834240 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.05412542447447777, loss=0.0346960611641407
I0204 23:41:34.532380 140044326471424 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.09933404624462128, loss=0.03548863157629967
I0204 23:42:06.835373 140143465834240 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.06857004761695862, loss=0.03868613392114639
I0204 23:42:31.006296 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:44:38.973280 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:44:42.047304 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:44:45.037608 140205209478976 submission_runner.py:408] Time since start: 9143.60s, 	Step: 18077, 	{'train/accuracy': 0.990651547908783, 'train/loss': 0.030918745324015617, 'train/mean_average_precision': 0.39857086045160495, 'validation/accuracy': 0.9868072867393494, 'validation/loss': 0.04433363303542137, 'validation/mean_average_precision': 0.26610207992878404, 'validation/num_examples': 43793, 'test/accuracy': 0.9859607219696045, 'test/loss': 0.04709821939468384, 'test/mean_average_precision': 0.25139896950538065, 'test/num_examples': 43793, 'score': 5780.7910261154175, 'total_duration': 9143.603893518448, 'accumulated_submission_time': 5780.7910261154175, 'accumulated_eval_time': 3361.631055355072, 'accumulated_logging_time': 0.6942794322967529}
I0204 23:44:45.058373 140037877987072 logging_writer.py:48] [18077] accumulated_eval_time=3361.631055, accumulated_logging_time=0.694279, accumulated_submission_time=5780.791026, global_step=18077, preemption_count=0, score=5780.791026, test/accuracy=0.985961, test/loss=0.047098, test/mean_average_precision=0.251399, test/num_examples=43793, total_duration=9143.603894, train/accuracy=0.990652, train/loss=0.030919, train/mean_average_precision=0.398571, validation/accuracy=0.986807, validation/loss=0.044334, validation/mean_average_precision=0.266102, validation/num_examples=43793
I0204 23:44:52.907670 140037886379776 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.0873621329665184, loss=0.03461179882287979
I0204 23:45:24.958866 140037877987072 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.06520183384418488, loss=0.032075561583042145
I0204 23:45:56.641801 140037886379776 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.06644591689109802, loss=0.03773527219891548
I0204 23:46:28.368976 140037877987072 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.0827474594116211, loss=0.032430075109004974
I0204 23:47:00.004647 140037886379776 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.07541080564260483, loss=0.03533590957522392
I0204 23:47:31.948105 140037877987072 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.0687156394124031, loss=0.03556689992547035
I0204 23:48:04.224198 140037886379776 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.06286931782960892, loss=0.038066670298576355
I0204 23:48:36.082499 140037877987072 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.06731542944908142, loss=0.03560783341526985
I0204 23:48:45.113819 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:50:50.962270 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:50:54.017303 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:50:56.958750 140205209478976 submission_runner.py:408] Time since start: 9515.53s, 	Step: 18829, 	{'train/accuracy': 0.9905492067337036, 'train/loss': 0.031019046902656555, 'train/mean_average_precision': 0.39520292134503615, 'validation/accuracy': 0.9866875410079956, 'validation/loss': 0.044885165989398956, 'validation/mean_average_precision': 0.2635644689347876, 'validation/num_examples': 43793, 'test/accuracy': 0.9857859015464783, 'test/loss': 0.04802625998854637, 'test/mean_average_precision': 0.24394026731538151, 'test/num_examples': 43793, 'score': 6020.8155081272125, 'total_duration': 9515.525021791458, 'accumulated_submission_time': 6020.8155081272125, 'accumulated_eval_time': 3493.475923061371, 'accumulated_logging_time': 0.7263171672821045}
I0204 23:50:56.977502 140044326471424 logging_writer.py:48] [18829] accumulated_eval_time=3493.475923, accumulated_logging_time=0.726317, accumulated_submission_time=6020.815508, global_step=18829, preemption_count=0, score=6020.815508, test/accuracy=0.985786, test/loss=0.048026, test/mean_average_precision=0.243940, test/num_examples=43793, total_duration=9515.525022, train/accuracy=0.990549, train/loss=0.031019, train/mean_average_precision=0.395203, validation/accuracy=0.986688, validation/loss=0.044885, validation/mean_average_precision=0.263564, validation/num_examples=43793
I0204 23:51:20.078273 140044334864128 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.0637342557311058, loss=0.034502264112234116
I0204 23:51:51.932517 140044326471424 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.05844244733452797, loss=0.034111183136701584
I0204 23:52:23.728789 140044334864128 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.07229618728160858, loss=0.03329375013709068
I0204 23:52:55.540115 140044326471424 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.05629613995552063, loss=0.03240435943007469
I0204 23:53:27.190983 140044334864128 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.06551549583673477, loss=0.03548295423388481
I0204 23:53:58.938615 140044326471424 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.05751587450504303, loss=0.03451220318675041
I0204 23:54:30.729357 140044334864128 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.09478554874658585, loss=0.03669505566358566
I0204 23:54:57.179368 140205209478976 spec.py:321] Evaluating on the training split.
I0204 23:56:59.810949 140205209478976 spec.py:333] Evaluating on the validation split.
I0204 23:57:02.885591 140205209478976 spec.py:349] Evaluating on the test split.
I0204 23:57:05.860236 140205209478976 submission_runner.py:408] Time since start: 9884.43s, 	Step: 19584, 	{'train/accuracy': 0.9906980395317078, 'train/loss': 0.03056776523590088, 'train/mean_average_precision': 0.4242602740789738, 'validation/accuracy': 0.9866716861724854, 'validation/loss': 0.04484669491648674, 'validation/mean_average_precision': 0.2587407145618029, 'validation/num_examples': 43793, 'test/accuracy': 0.9859375357627869, 'test/loss': 0.0473642535507679, 'test/mean_average_precision': 0.25391582333620416, 'test/num_examples': 43793, 'score': 6260.985202074051, 'total_duration': 9884.426526784897, 'accumulated_submission_time': 6260.985202074051, 'accumulated_eval_time': 3622.1567487716675, 'accumulated_logging_time': 0.7575254440307617}
I0204 23:57:05.879081 140037877987072 logging_writer.py:48] [19584] accumulated_eval_time=3622.156749, accumulated_logging_time=0.757525, accumulated_submission_time=6260.985202, global_step=19584, preemption_count=0, score=6260.985202, test/accuracy=0.985938, test/loss=0.047364, test/mean_average_precision=0.253916, test/num_examples=43793, total_duration=9884.426527, train/accuracy=0.990698, train/loss=0.030568, train/mean_average_precision=0.424260, validation/accuracy=0.986672, validation/loss=0.044847, validation/mean_average_precision=0.258741, validation/num_examples=43793
I0204 23:57:11.433808 140037886379776 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.059921685606241226, loss=0.03224695846438408
I0204 23:57:43.362000 140037877987072 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.05747510865330696, loss=0.032989174127578735
I0204 23:58:15.450807 140037886379776 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.06217112019658089, loss=0.03476845473051071
I0204 23:58:47.103710 140037877987072 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.058276671916246414, loss=0.03364817798137665
I0204 23:59:19.146162 140037886379776 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.060597505420446396, loss=0.03520849347114563
I0204 23:59:51.393249 140037877987072 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.05525888130068779, loss=0.03265756368637085
I0205 00:00:23.255336 140037886379776 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.07295972853899002, loss=0.0340103954076767
I0205 00:00:55.284583 140037877987072 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.0737919881939888, loss=0.03406529501080513
I0205 00:01:06.005526 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:03:08.375227 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:03:11.377660 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:03:14.376522 140205209478976 submission_runner.py:408] Time since start: 10252.94s, 	Step: 20334, 	{'train/accuracy': 0.9910008907318115, 'train/loss': 0.029481450095772743, 'train/mean_average_precision': 0.4198984412224145, 'validation/accuracy': 0.9867601990699768, 'validation/loss': 0.04445962235331535, 'validation/mean_average_precision': 0.2566778579806512, 'validation/num_examples': 43793, 'test/accuracy': 0.9859299659729004, 'test/loss': 0.04732656851410866, 'test/mean_average_precision': 0.2548134925721332, 'test/num_examples': 43793, 'score': 6501.081083774567, 'total_duration': 10252.942810297012, 'accumulated_submission_time': 6501.081083774567, 'accumulated_eval_time': 3750.5276980400085, 'accumulated_logging_time': 0.7873868942260742}
I0205 00:03:14.396458 140044326471424 logging_writer.py:48] [20334] accumulated_eval_time=3750.527698, accumulated_logging_time=0.787387, accumulated_submission_time=6501.081084, global_step=20334, preemption_count=0, score=6501.081084, test/accuracy=0.985930, test/loss=0.047327, test/mean_average_precision=0.254813, test/num_examples=43793, total_duration=10252.942810, train/accuracy=0.991001, train/loss=0.029481, train/mean_average_precision=0.419898, validation/accuracy=0.986760, validation/loss=0.044460, validation/mean_average_precision=0.256678, validation/num_examples=43793
I0205 00:03:36.217669 140143465834240 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.09064491838216782, loss=0.03834551200270653
I0205 00:04:08.725537 140044326471424 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.08718540519475937, loss=0.033503204584121704
I0205 00:04:40.874763 140143465834240 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.06594960391521454, loss=0.032952480018138885
I0205 00:05:12.802213 140044326471424 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.0632144957780838, loss=0.03421887010335922
I0205 00:05:44.302737 140143465834240 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.10452556610107422, loss=0.03404611721634865
I0205 00:06:16.352762 140044326471424 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.05792636051774025, loss=0.030833514407277107
I0205 00:06:48.194214 140143465834240 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.08357871323823929, loss=0.03558199107646942
I0205 00:07:14.580857 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:09:14.593880 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:09:17.589125 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:09:20.536941 140205209478976 submission_runner.py:408] Time since start: 10619.10s, 	Step: 21083, 	{'train/accuracy': 0.9911450147628784, 'train/loss': 0.028955401852726936, 'train/mean_average_precision': 0.438380159959826, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.04458526894450188, 'validation/mean_average_precision': 0.25754632620481266, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.04733564704656601, 'test/mean_average_precision': 0.25246322236122937, 'test/num_examples': 43793, 'score': 6741.23437333107, 'total_duration': 10619.10322880745, 'accumulated_submission_time': 6741.23437333107, 'accumulated_eval_time': 3876.483735561371, 'accumulated_logging_time': 0.8186118602752686}
I0205 00:09:20.555945 140037877987072 logging_writer.py:48] [21083] accumulated_eval_time=3876.483736, accumulated_logging_time=0.818612, accumulated_submission_time=6741.234373, global_step=21083, preemption_count=0, score=6741.234373, test/accuracy=0.985882, test/loss=0.047336, test/mean_average_precision=0.252463, test/num_examples=43793, total_duration=10619.103229, train/accuracy=0.991145, train/loss=0.028955, train/mean_average_precision=0.438380, validation/accuracy=0.986681, validation/loss=0.044585, validation/mean_average_precision=0.257546, validation/num_examples=43793
I0205 00:09:26.290175 140037886379776 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.0724463164806366, loss=0.036623548716306686
I0205 00:09:58.266709 140037877987072 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.07727023214101791, loss=0.03448374569416046
I0205 00:10:30.643261 140037886379776 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.11401589214801788, loss=0.03355091065168381
I0205 00:11:03.070826 140037877987072 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.10763151198625565, loss=0.03487171605229378
I0205 00:11:34.816381 140037886379776 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.0717037245631218, loss=0.03336891904473305
I0205 00:12:07.171021 140037877987072 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.06942757219076157, loss=0.0349314883351326
I0205 00:12:39.121603 140037886379776 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.08061850070953369, loss=0.031608857214450836
I0205 00:13:11.503403 140037877987072 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.07225669175386429, loss=0.033244095742702484
I0205 00:13:20.696818 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:15:19.981981 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:15:23.016587 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:15:26.048596 140205209478976 submission_runner.py:408] Time since start: 10984.61s, 	Step: 21830, 	{'train/accuracy': 0.9911766052246094, 'train/loss': 0.02919008769094944, 'train/mean_average_precision': 0.4514105935616376, 'validation/accuracy': 0.9867467880249023, 'validation/loss': 0.04480132460594177, 'validation/mean_average_precision': 0.26495023211404944, 'validation/num_examples': 43793, 'test/accuracy': 0.9859269857406616, 'test/loss': 0.04775208607316017, 'test/mean_average_precision': 0.2534212336427425, 'test/num_examples': 43793, 'score': 6981.344316244125, 'total_duration': 10984.614884376526, 'accumulated_submission_time': 6981.344316244125, 'accumulated_eval_time': 4001.835470676422, 'accumulated_logging_time': 0.8482942581176758}
I0205 00:15:26.067757 140044326471424 logging_writer.py:48] [21830] accumulated_eval_time=4001.835471, accumulated_logging_time=0.848294, accumulated_submission_time=6981.344316, global_step=21830, preemption_count=0, score=6981.344316, test/accuracy=0.985927, test/loss=0.047752, test/mean_average_precision=0.253421, test/num_examples=43793, total_duration=10984.614884, train/accuracy=0.991177, train/loss=0.029190, train/mean_average_precision=0.451411, validation/accuracy=0.986747, validation/loss=0.044801, validation/mean_average_precision=0.264950, validation/num_examples=43793
I0205 00:15:48.652850 140044334864128 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.07564987242221832, loss=0.03283606842160225
I0205 00:16:20.374646 140044326471424 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.1001870408654213, loss=0.031371038407087326
I0205 00:16:52.440994 140044334864128 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.07210543751716614, loss=0.03555002436041832
I0205 00:17:24.498500 140044326471424 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.08120227605104446, loss=0.03725987672805786
I0205 00:17:56.093579 140044334864128 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.06661402434110641, loss=0.03572322428226471
I0205 00:18:27.732216 140044326471424 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.06697525829076767, loss=0.0345374196767807
I0205 00:18:59.590164 140044334864128 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.07225504517555237, loss=0.037493277341127396
I0205 00:19:26.073437 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:21:28.137466 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:21:31.120124 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:21:34.072416 140205209478976 submission_runner.py:408] Time since start: 11352.64s, 	Step: 22585, 	{'train/accuracy': 0.9911478757858276, 'train/loss': 0.029226714745163918, 'train/mean_average_precision': 0.422738707506834, 'validation/accuracy': 0.9867796897888184, 'validation/loss': 0.04455733671784401, 'validation/mean_average_precision': 0.2584770706554278, 'validation/num_examples': 43793, 'test/accuracy': 0.9858440160751343, 'test/loss': 0.04732321575284004, 'test/mean_average_precision': 0.246868166257547, 'test/num_examples': 43793, 'score': 7221.317653656006, 'total_duration': 11352.638570547104, 'accumulated_submission_time': 7221.317653656006, 'accumulated_eval_time': 4129.834286689758, 'accumulated_logging_time': 0.8798651695251465}
I0205 00:21:34.091771 140037877987072 logging_writer.py:48] [22585] accumulated_eval_time=4129.834287, accumulated_logging_time=0.879865, accumulated_submission_time=7221.317654, global_step=22585, preemption_count=0, score=7221.317654, test/accuracy=0.985844, test/loss=0.047323, test/mean_average_precision=0.246868, test/num_examples=43793, total_duration=11352.638571, train/accuracy=0.991148, train/loss=0.029227, train/mean_average_precision=0.422739, validation/accuracy=0.986780, validation/loss=0.044557, validation/mean_average_precision=0.258477, validation/num_examples=43793
I0205 00:21:39.291649 140037886379776 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.07145470380783081, loss=0.03365037590265274
I0205 00:22:11.851441 140037877987072 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.08660182356834412, loss=0.03253293037414551
I0205 00:22:44.201526 140037886379776 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.07068394869565964, loss=0.035526979714632034
I0205 00:23:16.155277 140037877987072 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.06852462142705917, loss=0.031142491847276688
I0205 00:23:47.830094 140037886379776 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.06559798866510391, loss=0.03632492572069168
I0205 00:24:19.809083 140037877987072 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.07903726398944855, loss=0.03178757429122925
I0205 00:24:51.530712 140037886379776 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.0739012286067009, loss=0.03606734424829483
I0205 00:25:23.744451 140037877987072 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.06818601489067078, loss=0.03410094976425171
I0205 00:25:34.261255 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:27:40.316939 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:27:43.311486 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:27:46.256141 140205209478976 submission_runner.py:408] Time since start: 11724.82s, 	Step: 23334, 	{'train/accuracy': 0.9909042119979858, 'train/loss': 0.03012176603078842, 'train/mean_average_precision': 0.4214129838863844, 'validation/accuracy': 0.9866956472396851, 'validation/loss': 0.04435041919350624, 'validation/mean_average_precision': 0.26667981224204323, 'validation/num_examples': 43793, 'test/accuracy': 0.9857808351516724, 'test/loss': 0.047195274382829666, 'test/mean_average_precision': 0.24963429103863122, 'test/num_examples': 43793, 'score': 7461.457053661346, 'total_duration': 11724.822311639786, 'accumulated_submission_time': 7461.457053661346, 'accumulated_eval_time': 4261.829008817673, 'accumulated_logging_time': 0.9099385738372803}
I0205 00:27:46.274945 140044326471424 logging_writer.py:48] [23334] accumulated_eval_time=4261.829009, accumulated_logging_time=0.909939, accumulated_submission_time=7461.457054, global_step=23334, preemption_count=0, score=7461.457054, test/accuracy=0.985781, test/loss=0.047195, test/mean_average_precision=0.249634, test/num_examples=43793, total_duration=11724.822312, train/accuracy=0.990904, train/loss=0.030122, train/mean_average_precision=0.421413, validation/accuracy=0.986696, validation/loss=0.044350, validation/mean_average_precision=0.266680, validation/num_examples=43793
I0205 00:28:08.204645 140143465834240 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.08297999203205109, loss=0.03067256510257721
I0205 00:28:39.882328 140044326471424 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.07723312079906464, loss=0.032027989625930786
I0205 00:29:11.901183 140143465834240 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.06943442672491074, loss=0.03134533762931824
I0205 00:29:43.648916 140044326471424 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.08063764125108719, loss=0.033775247633457184
I0205 00:30:15.619077 140143465834240 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.07225335389375687, loss=0.03507142513990402
I0205 00:30:47.615873 140044326471424 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.08504080027341843, loss=0.03319210559129715
I0205 00:31:19.652412 140143465834240 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.09179043769836426, loss=0.03237537294626236
I0205 00:31:46.422365 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:33:49.026602 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:33:52.096372 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:33:55.152374 140205209478976 submission_runner.py:408] Time since start: 12093.72s, 	Step: 24086, 	{'train/accuracy': 0.9909349679946899, 'train/loss': 0.02982982248067856, 'train/mean_average_precision': 0.4233810275004899, 'validation/accuracy': 0.9868283867835999, 'validation/loss': 0.044438932090997696, 'validation/mean_average_precision': 0.27103600700796654, 'validation/num_examples': 43793, 'test/accuracy': 0.9860095381736755, 'test/loss': 0.04692890867590904, 'test/mean_average_precision': 0.2543803166805494, 'test/num_examples': 43793, 'score': 7701.260676383972, 'total_duration': 12093.718660354614, 'accumulated_submission_time': 7701.260676383972, 'accumulated_eval_time': 4390.558969974518, 'accumulated_logging_time': 1.252744436264038}
I0205 00:33:55.172753 140037886379776 logging_writer.py:48] [24086] accumulated_eval_time=4390.558970, accumulated_logging_time=1.252744, accumulated_submission_time=7701.260676, global_step=24086, preemption_count=0, score=7701.260676, test/accuracy=0.986010, test/loss=0.046929, test/mean_average_precision=0.254380, test/num_examples=43793, total_duration=12093.718660, train/accuracy=0.990935, train/loss=0.029830, train/mean_average_precision=0.423381, validation/accuracy=0.986828, validation/loss=0.044439, validation/mean_average_precision=0.271036, validation/num_examples=43793
I0205 00:34:00.113777 140044334864128 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.1024790108203888, loss=0.039097826927900314
I0205 00:34:32.803783 140037886379776 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.061169374734163284, loss=0.033228565007448196
I0205 00:35:05.106962 140044334864128 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.06572173535823822, loss=0.031635817140340805
I0205 00:35:37.174236 140037886379776 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.06659834086894989, loss=0.03586539998650551
I0205 00:36:09.622539 140044334864128 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.077895887196064, loss=0.034781377762556076
I0205 00:36:41.521248 140037886379776 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.07714717090129852, loss=0.0330127477645874
I0205 00:37:13.412593 140044334864128 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.07689310610294342, loss=0.03375523164868355
I0205 00:37:45.604370 140037886379776 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.06626460701227188, loss=0.0354170985519886
I0205 00:37:55.455698 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:39:57.184528 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:40:00.266475 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:40:03.360763 140205209478976 submission_runner.py:408] Time since start: 12461.93s, 	Step: 24831, 	{'train/accuracy': 0.9908654093742371, 'train/loss': 0.029955491423606873, 'train/mean_average_precision': 0.41462480755626085, 'validation/accuracy': 0.9866400361061096, 'validation/loss': 0.044650282710790634, 'validation/mean_average_precision': 0.2619743777067673, 'validation/num_examples': 43793, 'test/accuracy': 0.9857420921325684, 'test/loss': 0.047577787190675735, 'test/mean_average_precision': 0.24849465698805137, 'test/num_examples': 43793, 'score': 7941.512230634689, 'total_duration': 12461.927044153214, 'accumulated_submission_time': 7941.512230634689, 'accumulated_eval_time': 4518.463991880417, 'accumulated_logging_time': 1.2852094173431396}
I0205 00:40:03.380936 140037877987072 logging_writer.py:48] [24831] accumulated_eval_time=4518.463992, accumulated_logging_time=1.285209, accumulated_submission_time=7941.512231, global_step=24831, preemption_count=0, score=7941.512231, test/accuracy=0.985742, test/loss=0.047578, test/mean_average_precision=0.248495, test/num_examples=43793, total_duration=12461.927044, train/accuracy=0.990865, train/loss=0.029955, train/mean_average_precision=0.414625, validation/accuracy=0.986640, validation/loss=0.044650, validation/mean_average_precision=0.261974, validation/num_examples=43793
I0205 00:40:25.594755 140044326471424 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.07609088718891144, loss=0.03380962833762169
I0205 00:40:57.393524 140037877987072 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.07453669607639313, loss=0.03377588093280792
I0205 00:41:29.219312 140044326471424 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.1190221756696701, loss=0.03513628989458084
I0205 00:42:01.100083 140037877987072 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.09516806900501251, loss=0.031163252890110016
I0205 00:42:32.680098 140044326471424 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.07874831557273865, loss=0.03273316100239754
I0205 00:43:04.722891 140037877987072 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.08064928650856018, loss=0.03490647301077843
I0205 00:43:36.112058 140044326471424 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.06552965193986893, loss=0.033103685826063156
I0205 00:44:03.540909 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:46:02.717579 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:46:05.717314 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:46:08.680412 140205209478976 submission_runner.py:408] Time since start: 12827.25s, 	Step: 25587, 	{'train/accuracy': 0.9910185933113098, 'train/loss': 0.029385587200522423, 'train/mean_average_precision': 0.43237196485122104, 'validation/accuracy': 0.9868754744529724, 'validation/loss': 0.044676605612039566, 'validation/mean_average_precision': 0.2670693539503197, 'validation/num_examples': 43793, 'test/accuracy': 0.985908031463623, 'test/loss': 0.047651320695877075, 'test/mean_average_precision': 0.25267636504550534, 'test/num_examples': 43793, 'score': 8181.642039299011, 'total_duration': 12827.246604681015, 'accumulated_submission_time': 8181.642039299011, 'accumulated_eval_time': 4643.603357791901, 'accumulated_logging_time': 1.3162312507629395}
I0205 00:46:08.701948 140037886379776 logging_writer.py:48] [25587] accumulated_eval_time=4643.603358, accumulated_logging_time=1.316231, accumulated_submission_time=8181.642039, global_step=25587, preemption_count=0, score=8181.642039, test/accuracy=0.985908, test/loss=0.047651, test/mean_average_precision=0.252676, test/num_examples=43793, total_duration=12827.246605, train/accuracy=0.991019, train/loss=0.029386, train/mean_average_precision=0.432372, validation/accuracy=0.986875, validation/loss=0.044677, validation/mean_average_precision=0.267069, validation/num_examples=43793
I0205 00:46:13.216735 140044334864128 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.07718510180711746, loss=0.0329727865755558
I0205 00:46:45.242487 140037886379776 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.07949362695217133, loss=0.03260268270969391
I0205 00:47:17.366773 140044334864128 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.07175028324127197, loss=0.03351815044879913
I0205 00:47:49.108368 140037886379776 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.10210972279310226, loss=0.0351993627846241
I0205 00:48:21.873374 140044334864128 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.09802732616662979, loss=0.03454877436161041
I0205 00:48:54.373545 140037886379776 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.09510643035173416, loss=0.03551695868372917
I0205 00:49:26.851303 140044334864128 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.10014034807682037, loss=0.036364100873470306
I0205 00:49:59.491861 140037886379776 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.09299054741859436, loss=0.032475803047418594
I0205 00:50:08.913363 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:52:11.617770 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:52:14.640446 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:52:17.687951 140205209478976 submission_runner.py:408] Time since start: 13196.25s, 	Step: 26330, 	{'train/accuracy': 0.991351306438446, 'train/loss': 0.02848195470869541, 'train/mean_average_precision': 0.45723114206103377, 'validation/accuracy': 0.9867662787437439, 'validation/loss': 0.04438454657793045, 'validation/mean_average_precision': 0.27187496015310164, 'validation/num_examples': 43793, 'test/accuracy': 0.9859522581100464, 'test/loss': 0.047049105167388916, 'test/mean_average_precision': 0.2559921718547284, 'test/num_examples': 43793, 'score': 8421.819328069687, 'total_duration': 13196.254113912582, 'accumulated_submission_time': 8421.819328069687, 'accumulated_eval_time': 4772.377794981003, 'accumulated_logging_time': 1.3500714302062988}
I0205 00:52:17.708259 140037877987072 logging_writer.py:48] [26330] accumulated_eval_time=4772.377795, accumulated_logging_time=1.350071, accumulated_submission_time=8421.819328, global_step=26330, preemption_count=0, score=8421.819328, test/accuracy=0.985952, test/loss=0.047049, test/mean_average_precision=0.255992, test/num_examples=43793, total_duration=13196.254114, train/accuracy=0.991351, train/loss=0.028482, train/mean_average_precision=0.457231, validation/accuracy=0.986766, validation/loss=0.044385, validation/mean_average_precision=0.271875, validation/num_examples=43793
I0205 00:52:39.872705 140044326471424 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.08903100341558456, loss=0.03531816229224205
I0205 00:53:11.365186 140037877987072 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.07957813888788223, loss=0.0312572605907917
I0205 00:53:42.968287 140044326471424 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.09937267750501633, loss=0.030631477013230324
I0205 00:54:14.948544 140037877987072 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.07255516946315765, loss=0.033187102526426315
I0205 00:54:46.475055 140044326471424 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.06734344363212585, loss=0.031155375763773918
I0205 00:55:18.015971 140037877987072 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.06612465530633926, loss=0.03390668332576752
I0205 00:55:49.573013 140044326471424 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.07608360052108765, loss=0.03419675678014755
I0205 00:56:17.737469 140205209478976 spec.py:321] Evaluating on the training split.
I0205 00:58:21.397292 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 00:58:24.428715 140205209478976 spec.py:349] Evaluating on the test split.
I0205 00:58:27.490585 140205209478976 submission_runner.py:408] Time since start: 13566.06s, 	Step: 27090, 	{'train/accuracy': 0.9912184476852417, 'train/loss': 0.028624698519706726, 'train/mean_average_precision': 0.4483122074480636, 'validation/accuracy': 0.9867537021636963, 'validation/loss': 0.044528041034936905, 'validation/mean_average_precision': 0.2673226801916273, 'validation/num_examples': 43793, 'test/accuracy': 0.985910177230835, 'test/loss': 0.04741410166025162, 'test/mean_average_precision': 0.2518513987897968, 'test/num_examples': 43793, 'score': 8661.81754231453, 'total_duration': 13566.056858778, 'accumulated_submission_time': 8661.81754231453, 'accumulated_eval_time': 4902.130871295929, 'accumulated_logging_time': 1.381518840789795}
I0205 00:58:27.512823 140037886379776 logging_writer.py:48] [27090] accumulated_eval_time=4902.130871, accumulated_logging_time=1.381519, accumulated_submission_time=8661.817542, global_step=27090, preemption_count=0, score=8661.817542, test/accuracy=0.985910, test/loss=0.047414, test/mean_average_precision=0.251851, test/num_examples=43793, total_duration=13566.056859, train/accuracy=0.991218, train/loss=0.028625, train/mean_average_precision=0.448312, validation/accuracy=0.986754, validation/loss=0.044528, validation/mean_average_precision=0.267323, validation/num_examples=43793
I0205 00:58:30.982008 140143465834240 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.06693612039089203, loss=0.03171444684267044
I0205 00:59:02.580510 140037886379776 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.09138554334640503, loss=0.03523300960659981
I0205 00:59:33.985798 140143465834240 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.10352391749620438, loss=0.03127004951238632
I0205 01:00:05.573273 140037886379776 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.06741851568222046, loss=0.0318121500313282
I0205 01:00:37.090322 140143465834240 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.07522644102573395, loss=0.030873751267790794
I0205 01:01:08.743640 140037886379776 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.08805648982524872, loss=0.0342511422932148
I0205 01:01:40.272871 140143465834240 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.07663164287805557, loss=0.03447427228093147
I0205 01:02:12.282222 140037886379776 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.09871477633714676, loss=0.03425003960728645
I0205 01:02:27.794368 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:04:26.908707 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:04:30.220179 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:04:33.534929 140205209478976 submission_runner.py:408] Time since start: 13932.10s, 	Step: 27850, 	{'train/accuracy': 0.9915094375610352, 'train/loss': 0.027809351682662964, 'train/mean_average_precision': 0.4788816882412824, 'validation/accuracy': 0.9867618083953857, 'validation/loss': 0.04470362514257431, 'validation/mean_average_precision': 0.26950986838141033, 'validation/num_examples': 43793, 'test/accuracy': 0.9858503341674805, 'test/loss': 0.04746343940496445, 'test/mean_average_precision': 0.25180781502242594, 'test/num_examples': 43793, 'score': 8902.068484067917, 'total_duration': 13932.1010928154, 'accumulated_submission_time': 8902.068484067917, 'accumulated_eval_time': 5027.87125992775, 'accumulated_logging_time': 1.4148139953613281}
I0205 01:04:33.558218 140037877987072 logging_writer.py:48] [27850] accumulated_eval_time=5027.871260, accumulated_logging_time=1.414814, accumulated_submission_time=8902.068484, global_step=27850, preemption_count=0, score=8902.068484, test/accuracy=0.985850, test/loss=0.047463, test/mean_average_precision=0.251808, test/num_examples=43793, total_duration=13932.101093, train/accuracy=0.991509, train/loss=0.027809, train/mean_average_precision=0.478882, validation/accuracy=0.986762, validation/loss=0.044704, validation/mean_average_precision=0.269510, validation/num_examples=43793
I0205 01:04:50.047705 140044326471424 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.08566208183765411, loss=0.034255240112543106
I0205 01:05:22.149963 140037877987072 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.07253476977348328, loss=0.032033637166023254
I0205 01:05:54.178355 140044326471424 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.07478688657283783, loss=0.03283950686454773
I0205 01:06:25.883940 140037877987072 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.07824620604515076, loss=0.035221610218286514
I0205 01:06:57.151470 140044326471424 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.07390142232179642, loss=0.03277430310845375
I0205 01:07:29.326843 140037877987072 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.07838698476552963, loss=0.03202919289469719
I0205 01:08:01.057621 140044326471424 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.07456879317760468, loss=0.032645389437675476
I0205 01:08:32.826135 140037877987072 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.0689118504524231, loss=0.03085368126630783
I0205 01:08:33.787948 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:10:34.850370 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:10:37.952315 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:10:41.023624 140205209478976 submission_runner.py:408] Time since start: 14299.59s, 	Step: 28604, 	{'train/accuracy': 0.9915740489959717, 'train/loss': 0.02738937921822071, 'train/mean_average_precision': 0.47740207335549867, 'validation/accuracy': 0.9868312478065491, 'validation/loss': 0.04473751038312912, 'validation/mean_average_precision': 0.263646713481197, 'validation/num_examples': 43793, 'test/accuracy': 0.9858406782150269, 'test/loss': 0.04765476658940315, 'test/mean_average_precision': 0.25276134230487596, 'test/num_examples': 43793, 'score': 9142.265007972717, 'total_duration': 14299.589767217636, 'accumulated_submission_time': 9142.265007972717, 'accumulated_eval_time': 5155.106744766235, 'accumulated_logging_time': 1.451047420501709}
I0205 01:10:41.043810 140037886379776 logging_writer.py:48] [28604] accumulated_eval_time=5155.106745, accumulated_logging_time=1.451047, accumulated_submission_time=9142.265008, global_step=28604, preemption_count=0, score=9142.265008, test/accuracy=0.985841, test/loss=0.047655, test/mean_average_precision=0.252761, test/num_examples=43793, total_duration=14299.589767, train/accuracy=0.991574, train/loss=0.027389, train/mean_average_precision=0.477402, validation/accuracy=0.986831, validation/loss=0.044738, validation/mean_average_precision=0.263647, validation/num_examples=43793
I0205 01:11:11.781524 140044334864128 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.08541331440210342, loss=0.03215409442782402
I0205 01:11:43.341818 140037886379776 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.08105415105819702, loss=0.0330682173371315
I0205 01:12:15.164955 140044334864128 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.08955051004886627, loss=0.03314308077096939
I0205 01:12:46.698954 140037886379776 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.09350074082612991, loss=0.03427791967988014
I0205 01:13:18.715364 140044334864128 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.09274853765964508, loss=0.03286192938685417
I0205 01:13:50.286473 140037886379776 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.08805035799741745, loss=0.029895184561610222
I0205 01:14:22.561220 140044334864128 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.08540688455104828, loss=0.032848574221134186
I0205 01:14:41.027737 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:16:36.719472 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:16:39.761982 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:16:42.773869 140205209478976 submission_runner.py:408] Time since start: 14661.34s, 	Step: 29360, 	{'train/accuracy': 0.9915950894355774, 'train/loss': 0.027292786166071892, 'train/mean_average_precision': 0.48154575379357334, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.044787004590034485, 'validation/mean_average_precision': 0.26283345457999824, 'validation/num_examples': 43793, 'test/accuracy': 0.9858920574188232, 'test/loss': 0.0477716363966465, 'test/mean_average_precision': 0.24770145840423297, 'test/num_examples': 43793, 'score': 9382.218895435333, 'total_duration': 14661.340048074722, 'accumulated_submission_time': 9382.218895435333, 'accumulated_eval_time': 5276.8527200222015, 'accumulated_logging_time': 1.4818134307861328}
I0205 01:16:42.794573 140030420236032 logging_writer.py:48] [29360] accumulated_eval_time=5276.852720, accumulated_logging_time=1.481813, accumulated_submission_time=9382.218895, global_step=29360, preemption_count=0, score=9382.218895, test/accuracy=0.985892, test/loss=0.047772, test/mean_average_precision=0.247701, test/num_examples=43793, total_duration=14661.340048, train/accuracy=0.991595, train/loss=0.027293, train/mean_average_precision=0.481546, validation/accuracy=0.986778, validation/loss=0.044787, validation/mean_average_precision=0.262833, validation/num_examples=43793
I0205 01:16:56.394776 140037877987072 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.12099649012088776, loss=0.032413993030786514
I0205 01:17:28.345813 140030420236032 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.1015356257557869, loss=0.03562656790018082
I0205 01:18:00.132799 140037877987072 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.07141458243131638, loss=0.03209439292550087
I0205 01:18:32.126743 140030420236032 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.07400351762771606, loss=0.03226272389292717
I0205 01:19:04.154678 140037877987072 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.06999221444129944, loss=0.03078048676252365
I0205 01:19:36.291174 140030420236032 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.08121521770954132, loss=0.03290953114628792
I0205 01:20:08.281443 140037877987072 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.07785508036613464, loss=0.03480122983455658
I0205 01:20:40.372373 140030420236032 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.09729278832674026, loss=0.0333089604973793
I0205 01:20:42.914023 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:22:41.936636 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:22:44.982563 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:22:47.960753 140205209478976 submission_runner.py:408] Time since start: 15026.53s, 	Step: 30109, 	{'train/accuracy': 0.9915949702262878, 'train/loss': 0.02762727253139019, 'train/mean_average_precision': 0.4659208937616314, 'validation/accuracy': 0.9868267774581909, 'validation/loss': 0.04461052641272545, 'validation/mean_average_precision': 0.26542109410932735, 'validation/num_examples': 43793, 'test/accuracy': 0.9860133528709412, 'test/loss': 0.047491997480392456, 'test/mean_average_precision': 0.25493339906777207, 'test/num_examples': 43793, 'score': 9622.306058883667, 'total_duration': 15026.526914358139, 'accumulated_submission_time': 9622.306058883667, 'accumulated_eval_time': 5401.89927482605, 'accumulated_logging_time': 1.514939308166504}
I0205 01:22:47.981730 140044326471424 logging_writer.py:48] [30109] accumulated_eval_time=5401.899275, accumulated_logging_time=1.514939, accumulated_submission_time=9622.306059, global_step=30109, preemption_count=0, score=9622.306059, test/accuracy=0.986013, test/loss=0.047492, test/mean_average_precision=0.254933, test/num_examples=43793, total_duration=15026.526914, train/accuracy=0.991595, train/loss=0.027627, train/mean_average_precision=0.465921, validation/accuracy=0.986827, validation/loss=0.044611, validation/mean_average_precision=0.265421, validation/num_examples=43793
I0205 01:23:17.699722 140044334864128 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.12233274430036545, loss=0.03289756178855896
I0205 01:23:49.609115 140044326471424 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.07017527520656586, loss=0.034859150648117065
I0205 01:24:21.472344 140044334864128 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.07435470074415207, loss=0.03038218431174755
I0205 01:24:53.300034 140044326471424 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.08349604904651642, loss=0.030830005183815956
I0205 01:25:25.354920 140044334864128 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.08208497613668442, loss=0.03369145095348358
I0205 01:25:57.313646 140044326471424 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.08284087479114532, loss=0.03424286097288132
I0205 01:26:28.983873 140044334864128 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.09745774418115616, loss=0.03164457529783249
I0205 01:26:48.010776 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:28:48.979736 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:28:51.964818 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:28:54.933730 140205209478976 submission_runner.py:408] Time since start: 15393.50s, 	Step: 30861, 	{'train/accuracy': 0.9914368391036987, 'train/loss': 0.02820701152086258, 'train/mean_average_precision': 0.4585877945100001, 'validation/accuracy': 0.9867272973060608, 'validation/loss': 0.0447770431637764, 'validation/mean_average_precision': 0.26481675342029126, 'validation/num_examples': 43793, 'test/accuracy': 0.9858651161193848, 'test/loss': 0.04745037481188774, 'test/mean_average_precision': 0.2607415900214805, 'test/num_examples': 43793, 'score': 9862.304508924484, 'total_duration': 15393.499891757965, 'accumulated_submission_time': 9862.304508924484, 'accumulated_eval_time': 5528.822056770325, 'accumulated_logging_time': 1.5469331741333008}
I0205 01:28:54.954334 140030420236032 logging_writer.py:48] [30861] accumulated_eval_time=5528.822057, accumulated_logging_time=1.546933, accumulated_submission_time=9862.304509, global_step=30861, preemption_count=0, score=9862.304509, test/accuracy=0.985865, test/loss=0.047450, test/mean_average_precision=0.260742, test/num_examples=43793, total_duration=15393.499892, train/accuracy=0.991437, train/loss=0.028207, train/mean_average_precision=0.458588, validation/accuracy=0.986727, validation/loss=0.044777, validation/mean_average_precision=0.264817, validation/num_examples=43793
I0205 01:29:08.406527 140037877987072 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.06385794281959534, loss=0.02923590876162052
I0205 01:29:39.966331 140030420236032 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.07525045424699783, loss=0.03177598863840103
I0205 01:30:12.214450 140037877987072 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.07259093225002289, loss=0.029811261221766472
I0205 01:30:44.022777 140030420236032 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.06865237653255463, loss=0.030976345762610435
I0205 01:31:15.558856 140037877987072 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.11877105385065079, loss=0.03460995852947235
I0205 01:31:47.518819 140030420236032 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.0790325254201889, loss=0.03204752504825592
I0205 01:32:19.318436 140037877987072 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.10157565027475357, loss=0.03317354992032051
I0205 01:32:51.165617 140030420236032 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.13614633679389954, loss=0.03176906704902649
I0205 01:32:54.960837 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:34:52.965954 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:34:55.989127 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:34:58.935675 140205209478976 submission_runner.py:408] Time since start: 15757.50s, 	Step: 31613, 	{'train/accuracy': 0.9915117025375366, 'train/loss': 0.027849651873111725, 'train/mean_average_precision': 0.47196756425559555, 'validation/accuracy': 0.9867539405822754, 'validation/loss': 0.04462186247110367, 'validation/mean_average_precision': 0.26264033029363215, 'validation/num_examples': 43793, 'test/accuracy': 0.9859716296195984, 'test/loss': 0.047234199941158295, 'test/mean_average_precision': 0.25093760276090715, 'test/num_examples': 43793, 'score': 10102.278964281082, 'total_duration': 15757.501960992813, 'accumulated_submission_time': 10102.278964281082, 'accumulated_eval_time': 5652.796845912933, 'accumulated_logging_time': 1.580089807510376}
I0205 01:34:58.956398 140037886379776 logging_writer.py:48] [31613] accumulated_eval_time=5652.796846, accumulated_logging_time=1.580090, accumulated_submission_time=10102.278964, global_step=31613, preemption_count=0, score=10102.278964, test/accuracy=0.985972, test/loss=0.047234, test/mean_average_precision=0.250938, test/num_examples=43793, total_duration=15757.501961, train/accuracy=0.991512, train/loss=0.027850, train/mean_average_precision=0.471968, validation/accuracy=0.986754, validation/loss=0.044622, validation/mean_average_precision=0.262640, validation/num_examples=43793
I0205 01:35:27.037658 140044326471424 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.08871866017580032, loss=0.03165118023753166
I0205 01:35:58.455424 140037886379776 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.11074812710285187, loss=0.032927777618169785
I0205 01:36:29.892346 140044326471424 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.08417186886072159, loss=0.034338366240262985
I0205 01:37:01.568297 140037886379776 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.0917583480477333, loss=0.032034944742918015
I0205 01:37:33.253376 140044326471424 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.06929275393486023, loss=0.03145411238074303
I0205 01:38:04.928252 140037886379776 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.0742783322930336, loss=0.03262739256024361
I0205 01:38:36.185125 140044326471424 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.07914648205041885, loss=0.03154033049941063
I0205 01:38:59.195399 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:41:00.105868 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:41:03.194204 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:41:06.175096 140205209478976 submission_runner.py:408] Time since start: 16124.74s, 	Step: 32375, 	{'train/accuracy': 0.9914616346359253, 'train/loss': 0.027796823531389236, 'train/mean_average_precision': 0.4661418373102133, 'validation/accuracy': 0.9868665337562561, 'validation/loss': 0.04482589662075043, 'validation/mean_average_precision': 0.26490325607114407, 'validation/num_examples': 43793, 'test/accuracy': 0.9860483407974243, 'test/loss': 0.047689080238342285, 'test/mean_average_precision': 0.2538761775615324, 'test/num_examples': 43793, 'score': 10342.485783815384, 'total_duration': 16124.741267204285, 'accumulated_submission_time': 10342.485783815384, 'accumulated_eval_time': 5779.776381969452, 'accumulated_logging_time': 1.6129043102264404}
I0205 01:41:06.195772 140030420236032 logging_writer.py:48] [32375] accumulated_eval_time=5779.776382, accumulated_logging_time=1.612904, accumulated_submission_time=10342.485784, global_step=32375, preemption_count=0, score=10342.485784, test/accuracy=0.986048, test/loss=0.047689, test/mean_average_precision=0.253876, test/num_examples=43793, total_duration=16124.741267, train/accuracy=0.991462, train/loss=0.027797, train/mean_average_precision=0.466142, validation/accuracy=0.986867, validation/loss=0.044826, validation/mean_average_precision=0.264903, validation/num_examples=43793
I0205 01:41:14.757474 140037877987072 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.07155393064022064, loss=0.03168513998389244
I0205 01:41:46.621715 140030420236032 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.09991195797920227, loss=0.029261920601129532
I0205 01:42:18.970501 140037877987072 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.08051557838916779, loss=0.03429834172129631
I0205 01:42:50.973658 140030420236032 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.13185638189315796, loss=0.028859391808509827
I0205 01:43:22.615231 140037877987072 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.08265793323516846, loss=0.03109879605472088
I0205 01:43:54.106812 140030420236032 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.08720892667770386, loss=0.03362250700592995
I0205 01:44:25.839630 140037877987072 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.0924026221036911, loss=0.028826208785176277
I0205 01:44:58.000157 140030420236032 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.09247660636901855, loss=0.032638534903526306
I0205 01:45:06.403206 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:47:06.426005 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:47:09.478698 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:47:12.473016 140205209478976 submission_runner.py:408] Time since start: 16491.04s, 	Step: 33127, 	{'train/accuracy': 0.9914880990982056, 'train/loss': 0.02771720103919506, 'train/mean_average_precision': 0.47612648725667905, 'validation/accuracy': 0.9866778254508972, 'validation/loss': 0.04471953585743904, 'validation/mean_average_precision': 0.2619186380504609, 'validation/num_examples': 43793, 'test/accuracy': 0.9858246445655823, 'test/loss': 0.04736829549074173, 'test/mean_average_precision': 0.25734370976292115, 'test/num_examples': 43793, 'score': 10582.662477493286, 'total_duration': 16491.039191007614, 'accumulated_submission_time': 10582.662477493286, 'accumulated_eval_time': 5905.846033334732, 'accumulated_logging_time': 1.644451379776001}
I0205 01:47:12.494676 140037886379776 logging_writer.py:48] [33127] accumulated_eval_time=5905.846033, accumulated_logging_time=1.644451, accumulated_submission_time=10582.662477, global_step=33127, preemption_count=0, score=10582.662477, test/accuracy=0.985825, test/loss=0.047368, test/mean_average_precision=0.257344, test/num_examples=43793, total_duration=16491.039191, train/accuracy=0.991488, train/loss=0.027717, train/mean_average_precision=0.476126, validation/accuracy=0.986678, validation/loss=0.044720, validation/mean_average_precision=0.261919, validation/num_examples=43793
I0205 01:47:36.209520 140044334864128 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.07944497466087341, loss=0.0320194810628891
I0205 01:48:07.977152 140037886379776 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.11465142667293549, loss=0.03385138511657715
I0205 01:48:40.503022 140044334864128 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.0721750408411026, loss=0.031131230294704437
I0205 01:49:12.651718 140037886379776 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.10516276210546494, loss=0.028996679931879044
I0205 01:49:44.276310 140044334864128 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.08736199140548706, loss=0.033782415091991425
I0205 01:50:15.964190 140037886379776 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.09407840669155121, loss=0.03130114823579788
I0205 01:50:47.499632 140044334864128 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.14850251376628876, loss=0.03289075568318367
I0205 01:51:12.697309 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:53:12.423537 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:53:15.442604 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:53:18.396872 140205209478976 submission_runner.py:408] Time since start: 16856.96s, 	Step: 33880, 	{'train/accuracy': 0.9916728138923645, 'train/loss': 0.027090424671769142, 'train/mean_average_precision': 0.4776111416574321, 'validation/accuracy': 0.9869416356086731, 'validation/loss': 0.044754911214113235, 'validation/mean_average_precision': 0.27371316807948076, 'validation/num_examples': 43793, 'test/accuracy': 0.9860420227050781, 'test/loss': 0.04735885560512543, 'test/mean_average_precision': 0.2607110022505314, 'test/num_examples': 43793, 'score': 10822.833097219467, 'total_duration': 16856.96304345131, 'accumulated_submission_time': 10822.833097219467, 'accumulated_eval_time': 6031.545439004898, 'accumulated_logging_time': 1.6781425476074219}
I0205 01:53:18.419286 140030420236032 logging_writer.py:48] [33880] accumulated_eval_time=6031.545439, accumulated_logging_time=1.678143, accumulated_submission_time=10822.833097, global_step=33880, preemption_count=0, score=10822.833097, test/accuracy=0.986042, test/loss=0.047359, test/mean_average_precision=0.260711, test/num_examples=43793, total_duration=16856.963043, train/accuracy=0.991673, train/loss=0.027090, train/mean_average_precision=0.477611, validation/accuracy=0.986942, validation/loss=0.044755, validation/mean_average_precision=0.273713, validation/num_examples=43793
I0205 01:53:25.207160 140037877987072 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.08467509597539902, loss=0.031312648206949234
I0205 01:53:57.032750 140030420236032 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.08220972120761871, loss=0.0339529812335968
I0205 01:54:29.025770 140037877987072 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.09581819176673889, loss=0.03300958499312401
I0205 01:55:00.828034 140030420236032 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.12049077451229095, loss=0.03132349252700806
I0205 01:55:33.013534 140037877987072 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.10498637706041336, loss=0.03350379690527916
I0205 01:56:05.146614 140030420236032 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.08824100345373154, loss=0.03382473438978195
I0205 01:56:36.892121 140037877987072 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.08633454144001007, loss=0.0314205065369606
I0205 01:57:09.258559 140030420236032 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.09391533583402634, loss=0.034299157559871674
I0205 01:57:18.470201 140205209478976 spec.py:321] Evaluating on the training split.
I0205 01:59:18.893203 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 01:59:21.951988 140205209478976 spec.py:349] Evaluating on the test split.
I0205 01:59:24.961957 140205209478976 submission_runner.py:408] Time since start: 17223.53s, 	Step: 34630, 	{'train/accuracy': 0.9918762445449829, 'train/loss': 0.026501566171646118, 'train/mean_average_precision': 0.49593268746876096, 'validation/accuracy': 0.9868482947349548, 'validation/loss': 0.04476573318243027, 'validation/mean_average_precision': 0.2684190313777011, 'validation/num_examples': 43793, 'test/accuracy': 0.9858322143554688, 'test/loss': 0.047500334680080414, 'test/mean_average_precision': 0.25602074410434805, 'test/num_examples': 43793, 'score': 11062.853231668472, 'total_duration': 17223.528245449066, 'accumulated_submission_time': 11062.853231668472, 'accumulated_eval_time': 6158.037148237228, 'accumulated_logging_time': 1.71140456199646}
I0205 01:59:24.983819 140044326471424 logging_writer.py:48] [34630] accumulated_eval_time=6158.037148, accumulated_logging_time=1.711405, accumulated_submission_time=11062.853232, global_step=34630, preemption_count=0, score=11062.853232, test/accuracy=0.985832, test/loss=0.047500, test/mean_average_precision=0.256021, test/num_examples=43793, total_duration=17223.528245, train/accuracy=0.991876, train/loss=0.026502, train/mean_average_precision=0.495933, validation/accuracy=0.986848, validation/loss=0.044766, validation/mean_average_precision=0.268419, validation/num_examples=43793
I0205 01:59:47.950070 140044334864128 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.10465358197689056, loss=0.030336247757077217
I0205 02:00:19.902095 140044326471424 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.12438780069351196, loss=0.03416718170046806
I0205 02:00:51.853313 140044334864128 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.09549886733293533, loss=0.030401146039366722
I0205 02:01:23.713026 140044326471424 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.091630719602108, loss=0.031001878902316093
I0205 02:01:55.429186 140044334864128 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.08523835986852646, loss=0.02790583297610283
I0205 02:02:27.682782 140044326471424 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.09595830738544464, loss=0.03214887157082558
I0205 02:03:01.305989 140044334864128 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.09577865898609161, loss=0.02963465265929699
I0205 02:03:25.021586 140205209478976 spec.py:321] Evaluating on the training split.
I0205 02:05:27.072474 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 02:05:30.229349 140205209478976 spec.py:349] Evaluating on the test split.
I0205 02:05:33.210568 140205209478976 submission_runner.py:408] Time since start: 17591.78s, 	Step: 35374, 	{'train/accuracy': 0.9919270873069763, 'train/loss': 0.026157235726714134, 'train/mean_average_precision': 0.5077010842706066, 'validation/accuracy': 0.9868158102035522, 'validation/loss': 0.04501223936676979, 'validation/mean_average_precision': 0.2640386329595336, 'validation/num_examples': 43793, 'test/accuracy': 0.9859994649887085, 'test/loss': 0.04774508252739906, 'test/mean_average_precision': 0.2512933375163518, 'test/num_examples': 43793, 'score': 11302.85993719101, 'total_duration': 17591.77684688568, 'accumulated_submission_time': 11302.85993719101, 'accumulated_eval_time': 6286.22608089447, 'accumulated_logging_time': 1.7442903518676758}
I0205 02:05:33.232891 140030420236032 logging_writer.py:48] [35374] accumulated_eval_time=6286.226081, accumulated_logging_time=1.744290, accumulated_submission_time=11302.859937, global_step=35374, preemption_count=0, score=11302.859937, test/accuracy=0.985999, test/loss=0.047745, test/mean_average_precision=0.251293, test/num_examples=43793, total_duration=17591.776847, train/accuracy=0.991927, train/loss=0.026157, train/mean_average_precision=0.507701, validation/accuracy=0.986816, validation/loss=0.045012, validation/mean_average_precision=0.264039, validation/num_examples=43793
I0205 02:05:41.829069 140037886379776 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.12317392975091934, loss=0.03173442557454109
I0205 02:06:13.829188 140030420236032 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.078923299908638, loss=0.030523844063282013
I0205 02:06:46.171217 140037886379776 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.0990094318985939, loss=0.031501106917858124
I0205 02:07:18.364764 140030420236032 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.07806539535522461, loss=0.030315779149532318
I0205 02:07:50.591581 140037886379776 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.09080775827169418, loss=0.03238954022526741
I0205 02:08:22.617380 140030420236032 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.09051784873008728, loss=0.031058963388204575
I0205 02:08:54.622995 140037886379776 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.08336973935365677, loss=0.03130243346095085
I0205 02:09:26.979254 140030420236032 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.10000090301036835, loss=0.030553093180060387
I0205 02:09:33.328392 140205209478976 spec.py:321] Evaluating on the training split.
I0205 02:11:34.603112 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 02:11:37.631519 140205209478976 spec.py:349] Evaluating on the test split.
I0205 02:11:40.626906 140205209478976 submission_runner.py:408] Time since start: 17959.19s, 	Step: 36121, 	{'train/accuracy': 0.9923211336135864, 'train/loss': 0.024961713701486588, 'train/mean_average_precision': 0.5338029568777007, 'validation/accuracy': 0.9868872761726379, 'validation/loss': 0.04489753395318985, 'validation/mean_average_precision': 0.2611574561332976, 'validation/num_examples': 43793, 'test/accuracy': 0.9859430193901062, 'test/loss': 0.04782985523343086, 'test/mean_average_precision': 0.2534297712322391, 'test/num_examples': 43793, 'score': 11542.92440032959, 'total_duration': 17959.193194389343, 'accumulated_submission_time': 11542.92440032959, 'accumulated_eval_time': 6413.524546384811, 'accumulated_logging_time': 1.7778651714324951}
I0205 02:11:40.649357 140037877987072 logging_writer.py:48] [36121] accumulated_eval_time=6413.524546, accumulated_logging_time=1.777865, accumulated_submission_time=11542.924400, global_step=36121, preemption_count=0, score=11542.924400, test/accuracy=0.985943, test/loss=0.047830, test/mean_average_precision=0.253430, test/num_examples=43793, total_duration=17959.193194, train/accuracy=0.992321, train/loss=0.024962, train/mean_average_precision=0.533803, validation/accuracy=0.986887, validation/loss=0.044898, validation/mean_average_precision=0.261157, validation/num_examples=43793
I0205 02:12:06.537609 140044334864128 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.09654907137155533, loss=0.03233061358332634
I0205 02:12:38.511839 140037877987072 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.09547741711139679, loss=0.029303984716534615
I0205 02:13:10.457398 140044334864128 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.0984753891825676, loss=0.030909715220332146
I0205 02:13:42.382254 140037877987072 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.09907828271389008, loss=0.0286500733345747
I0205 02:14:13.981787 140044334864128 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.0847432091832161, loss=0.030589699745178223
I0205 02:14:46.588963 140037877987072 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.08731146156787872, loss=0.029164044186472893
I0205 02:15:19.143813 140044334864128 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.11345105618238449, loss=0.029809216037392616
I0205 02:15:40.714501 140205209478976 spec.py:321] Evaluating on the training split.
I0205 02:17:44.853701 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 02:17:48.979596 140205209478976 spec.py:349] Evaluating on the test split.
I0205 02:17:52.001095 140205209478976 submission_runner.py:408] Time since start: 18330.57s, 	Step: 36868, 	{'train/accuracy': 0.9920997619628906, 'train/loss': 0.025457335636019707, 'train/mean_average_precision': 0.5130001361147138, 'validation/accuracy': 0.9868791699409485, 'validation/loss': 0.04519825428724289, 'validation/mean_average_precision': 0.26327072933660084, 'validation/num_examples': 43793, 'test/accuracy': 0.986023485660553, 'test/loss': 0.04804118722677231, 'test/mean_average_precision': 0.25629805749865264, 'test/num_examples': 43793, 'score': 11782.954986095428, 'total_duration': 18330.567383527756, 'accumulated_submission_time': 11782.954986095428, 'accumulated_eval_time': 6544.811107635498, 'accumulated_logging_time': 1.8135063648223877}
I0205 02:17:52.024561 140037886379776 logging_writer.py:48] [36868] accumulated_eval_time=6544.811108, accumulated_logging_time=1.813506, accumulated_submission_time=11782.954986, global_step=36868, preemption_count=0, score=11782.954986, test/accuracy=0.986023, test/loss=0.048041, test/mean_average_precision=0.256298, test/num_examples=43793, total_duration=18330.567384, train/accuracy=0.992100, train/loss=0.025457, train/mean_average_precision=0.513000, validation/accuracy=0.986879, validation/loss=0.045198, validation/mean_average_precision=0.263271, validation/num_examples=43793
I0205 02:18:02.637627 140044326471424 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.07500253617763519, loss=0.028720015659928322
I0205 02:18:34.598033 140037886379776 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.08265503495931625, loss=0.0291795302182436
I0205 02:19:06.280468 140044326471424 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.07756378501653671, loss=0.02972576394677162
I0205 02:19:38.134352 140037886379776 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.0823962613940239, loss=0.033296868205070496
I0205 02:20:10.555436 140044326471424 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.0812983438372612, loss=0.030886977910995483
I0205 02:20:42.560027 140037886379776 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.0933217778801918, loss=0.029492242261767387
I0205 02:21:14.251977 140044326471424 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.09350728988647461, loss=0.03232733532786369
I0205 02:21:45.931988 140037886379776 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.0911078155040741, loss=0.03343917056918144
I0205 02:21:52.192391 140205209478976 spec.py:321] Evaluating on the training split.
I0205 02:23:51.814890 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 02:23:54.870070 140205209478976 spec.py:349] Evaluating on the test split.
I0205 02:23:57.857024 140205209478976 submission_runner.py:408] Time since start: 18696.42s, 	Step: 37621, 	{'train/accuracy': 0.991813063621521, 'train/loss': 0.026473114266991615, 'train/mean_average_precision': 0.5066786252637335, 'validation/accuracy': 0.9869157075881958, 'validation/loss': 0.04498588666319847, 'validation/mean_average_precision': 0.27089276405841906, 'validation/num_examples': 43793, 'test/accuracy': 0.9859792590141296, 'test/loss': 0.047808218747377396, 'test/mean_average_precision': 0.25641973641373816, 'test/num_examples': 43793, 'score': 12023.092227220535, 'total_duration': 18696.423310041428, 'accumulated_submission_time': 12023.092227220535, 'accumulated_eval_time': 6670.475694179535, 'accumulated_logging_time': 1.847543478012085}
I0205 02:23:57.879992 140030420236032 logging_writer.py:48] [37621] accumulated_eval_time=6670.475694, accumulated_logging_time=1.847543, accumulated_submission_time=12023.092227, global_step=37621, preemption_count=0, score=12023.092227, test/accuracy=0.985979, test/loss=0.047808, test/mean_average_precision=0.256420, test/num_examples=43793, total_duration=18696.423310, train/accuracy=0.991813, train/loss=0.026473, train/mean_average_precision=0.506679, validation/accuracy=0.986916, validation/loss=0.044986, validation/mean_average_precision=0.270893, validation/num_examples=43793
I0205 02:24:23.286464 140044334864128 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.08473992347717285, loss=0.029189076274633408
I0205 02:24:54.692227 140030420236032 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.09774509072303772, loss=0.029133012518286705
I0205 02:25:26.284967 140044334864128 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.0813378393650055, loss=0.029189791530370712
I0205 02:25:57.934061 140030420236032 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.1295599490404129, loss=0.028113339096307755
I0205 02:26:29.488556 140044334864128 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.09339433163404465, loss=0.02894025668501854
I0205 02:27:00.969269 140030420236032 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.0922020748257637, loss=0.03285570442676544
I0205 02:27:32.693297 140044334864128 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.08299624919891357, loss=0.029107853770256042
I0205 02:27:58.033667 140205209478976 spec.py:321] Evaluating on the training split.
I0205 02:29:57.256053 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 02:30:00.298630 140205209478976 spec.py:349] Evaluating on the test split.
I0205 02:30:03.375259 140205209478976 submission_runner.py:408] Time since start: 19061.94s, 	Step: 38381, 	{'train/accuracy': 0.9919701814651489, 'train/loss': 0.0260833278298378, 'train/mean_average_precision': 0.49382275664101644, 'validation/accuracy': 0.9869075417518616, 'validation/loss': 0.044809501618146896, 'validation/mean_average_precision': 0.2723583684879261, 'validation/num_examples': 43793, 'test/accuracy': 0.985987663269043, 'test/loss': 0.04771247133612633, 'test/mean_average_precision': 0.2526040420471432, 'test/num_examples': 43793, 'score': 12263.215424776077, 'total_duration': 19061.941546201706, 'accumulated_submission_time': 12263.215424776077, 'accumulated_eval_time': 6795.817242145538, 'accumulated_logging_time': 1.8814423084259033}
I0205 02:30:03.398281 140037877987072 logging_writer.py:48] [38381] accumulated_eval_time=6795.817242, accumulated_logging_time=1.881442, accumulated_submission_time=12263.215425, global_step=38381, preemption_count=0, score=12263.215425, test/accuracy=0.985988, test/loss=0.047712, test/mean_average_precision=0.252604, test/num_examples=43793, total_duration=19061.941546, train/accuracy=0.991970, train/loss=0.026083, train/mean_average_precision=0.493823, validation/accuracy=0.986908, validation/loss=0.044810, validation/mean_average_precision=0.272358, validation/num_examples=43793
I0205 02:30:09.850621 140037886379776 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.1030905619263649, loss=0.0301426462829113
I0205 02:30:41.126935 140037877987072 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.13126486539840698, loss=0.03207814320921898
I0205 02:31:12.919606 140037886379776 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.08410689234733582, loss=0.02934161014854908
I0205 02:31:44.248172 140037877987072 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.11842255294322968, loss=0.030607054010033607
I0205 02:32:16.271812 140037886379776 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.10094596445560455, loss=0.029281610623002052
I0205 02:32:47.736432 140037877987072 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.09755616635084152, loss=0.03061172366142273
I0205 02:33:19.649493 140037886379776 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.1125604435801506, loss=0.03346532955765724
I0205 02:33:51.111009 140037877987072 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.09144976735115051, loss=0.0323181226849556
I0205 02:34:03.597903 140205209478976 spec.py:321] Evaluating on the training split.
I0205 02:35:59.465724 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 02:36:02.610250 140205209478976 spec.py:349] Evaluating on the test split.
I0205 02:36:05.624996 140205209478976 submission_runner.py:408] Time since start: 19424.19s, 	Step: 39140, 	{'train/accuracy': 0.9919205904006958, 'train/loss': 0.026281286031007767, 'train/mean_average_precision': 0.5019598940760397, 'validation/accuracy': 0.9867547154426575, 'validation/loss': 0.0452037937939167, 'validation/mean_average_precision': 0.26647209985618087, 'validation/num_examples': 43793, 'test/accuracy': 0.9859459400177002, 'test/loss': 0.0479544922709465, 'test/mean_average_precision': 0.25299035868724007, 'test/num_examples': 43793, 'score': 12503.383272647858, 'total_duration': 19424.19128537178, 'accumulated_submission_time': 12503.383272647858, 'accumulated_eval_time': 6917.844294548035, 'accumulated_logging_time': 1.9166576862335205}
I0205 02:36:05.648149 140044326471424 logging_writer.py:48] [39140] accumulated_eval_time=6917.844295, accumulated_logging_time=1.916658, accumulated_submission_time=12503.383273, global_step=39140, preemption_count=0, score=12503.383273, test/accuracy=0.985946, test/loss=0.047954, test/mean_average_precision=0.252990, test/num_examples=43793, total_duration=19424.191285, train/accuracy=0.991921, train/loss=0.026281, train/mean_average_precision=0.501960, validation/accuracy=0.986755, validation/loss=0.045204, validation/mean_average_precision=0.266472, validation/num_examples=43793
I0205 02:36:25.046376 140044334864128 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.0845920667052269, loss=0.028042100369930267
I0205 02:36:56.840411 140044326471424 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.08988781273365021, loss=0.02855890803039074
I0205 02:37:28.593877 140044334864128 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.09102585911750793, loss=0.03108329139649868
I0205 02:38:00.386847 140044326471424 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.09948299825191498, loss=0.031241074204444885
I0205 02:38:32.122420 140044334864128 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.09949630498886108, loss=0.030425870791077614
I0205 02:39:04.108605 140044326471424 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.10085795819759369, loss=0.03034382313489914
I0205 02:39:35.590105 140044334864128 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.09419077634811401, loss=0.028475280851125717
I0205 02:40:05.942369 140205209478976 spec.py:321] Evaluating on the training split.
I0205 02:42:05.987783 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 02:42:09.057394 140205209478976 spec.py:349] Evaluating on the test split.
I0205 02:42:12.055676 140205209478976 submission_runner.py:408] Time since start: 19790.62s, 	Step: 39896, 	{'train/accuracy': 0.9918465614318848, 'train/loss': 0.026147423312067986, 'train/mean_average_precision': 0.5186325621587466, 'validation/accuracy': 0.9868633151054382, 'validation/loss': 0.0452602319419384, 'validation/mean_average_precision': 0.26759407914953914, 'validation/num_examples': 43793, 'test/accuracy': 0.9860289096832275, 'test/loss': 0.04782252758741379, 'test/mean_average_precision': 0.2570222310540754, 'test/num_examples': 43793, 'score': 12743.645746707916, 'total_duration': 19790.621960878372, 'accumulated_submission_time': 12743.645746707916, 'accumulated_eval_time': 7043.957558870316, 'accumulated_logging_time': 1.9517738819122314}
I0205 02:42:12.079258 140030420236032 logging_writer.py:48] [39896] accumulated_eval_time=7043.957559, accumulated_logging_time=1.951774, accumulated_submission_time=12743.645747, global_step=39896, preemption_count=0, score=12743.645747, test/accuracy=0.986029, test/loss=0.047823, test/mean_average_precision=0.257022, test/num_examples=43793, total_duration=19790.621961, train/accuracy=0.991847, train/loss=0.026147, train/mean_average_precision=0.518633, validation/accuracy=0.986863, validation/loss=0.045260, validation/mean_average_precision=0.267594, validation/num_examples=43793
I0205 02:42:13.666968 140037886379776 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.10946118086576462, loss=0.031201619654893875
I0205 02:42:45.168211 140030420236032 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.09422329068183899, loss=0.030460214242339134
I0205 02:43:16.741680 140037886379776 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.1168799102306366, loss=0.029992247000336647
I0205 02:43:47.916260 140030420236032 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.14038914442062378, loss=0.029765617102384567
I0205 02:44:19.407144 140037886379776 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.13443295657634735, loss=0.03367074579000473
I0205 02:44:50.805276 140030420236032 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.10674528777599335, loss=0.029083549976348877
I0205 02:45:22.250166 140037886379776 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.1011056900024414, loss=0.02977830171585083
I0205 02:45:53.830006 140030420236032 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.10300961136817932, loss=0.031163491308689117
I0205 02:46:12.312513 140205209478976 spec.py:321] Evaluating on the training split.
I0205 02:48:06.690182 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 02:48:09.739140 140205209478976 spec.py:349] Evaluating on the test split.
I0205 02:48:12.746847 140205209478976 submission_runner.py:408] Time since start: 20151.31s, 	Step: 40660, 	{'train/accuracy': 0.9921106100082397, 'train/loss': 0.025712454691529274, 'train/mean_average_precision': 0.50393287827044, 'validation/accuracy': 0.9866834878921509, 'validation/loss': 0.04541837424039841, 'validation/mean_average_precision': 0.2655199069094247, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.04785643890500069, 'test/mean_average_precision': 0.25835325361566364, 'test/num_examples': 43793, 'score': 12983.84726190567, 'total_duration': 20151.313011407852, 'accumulated_submission_time': 12983.84726190567, 'accumulated_eval_time': 7164.391726016998, 'accumulated_logging_time': 1.987480878829956}
I0205 02:48:12.771166 140044326471424 logging_writer.py:48] [40660] accumulated_eval_time=7164.391726, accumulated_logging_time=1.987481, accumulated_submission_time=12983.847262, global_step=40660, preemption_count=0, score=12983.847262, test/accuracy=0.985882, test/loss=0.047856, test/mean_average_precision=0.258353, test/num_examples=43793, total_duration=20151.313011, train/accuracy=0.992111, train/loss=0.025712, train/mean_average_precision=0.503933, validation/accuracy=0.986683, validation/loss=0.045418, validation/mean_average_precision=0.265520, validation/num_examples=43793
I0205 02:48:25.849953 140044334864128 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.0961422398686409, loss=0.03316294774413109
I0205 02:48:57.121306 140044326471424 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.11325433105230331, loss=0.02872881479561329
I0205 02:49:28.779611 140044334864128 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.08794769644737244, loss=0.0281284861266613
I0205 02:50:01.075449 140044326471424 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.10982223600149155, loss=0.03354501724243164
I0205 02:50:33.415341 140044334864128 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.1038828045129776, loss=0.0315096452832222
I0205 02:51:05.946849 140044326471424 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.08117848634719849, loss=0.03010418638586998
I0205 02:51:37.775057 140044334864128 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.11258985847234726, loss=0.030211571604013443
I0205 02:52:09.740018 140044326471424 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.09871033579111099, loss=0.031550198793411255
I0205 02:52:12.903497 140205209478976 spec.py:321] Evaluating on the training split.
I0205 02:54:11.371474 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 02:54:14.464334 140205209478976 spec.py:349] Evaluating on the test split.
I0205 02:54:17.463000 140205209478976 submission_runner.py:408] Time since start: 20516.03s, 	Step: 41411, 	{'train/accuracy': 0.9921404719352722, 'train/loss': 0.02532167360186577, 'train/mean_average_precision': 0.5251935954997733, 'validation/accuracy': 0.9867870211601257, 'validation/loss': 0.045663002878427505, 'validation/mean_average_precision': 0.265891050572245, 'validation/num_examples': 43793, 'test/accuracy': 0.9858701229095459, 'test/loss': 0.04856467247009277, 'test/mean_average_precision': 0.25416953442428014, 'test/num_examples': 43793, 'score': 13223.94919514656, 'total_duration': 20516.02917122841, 'accumulated_submission_time': 13223.94919514656, 'accumulated_eval_time': 7288.951065063477, 'accumulated_logging_time': 2.023068428039551}
I0205 02:54:17.485770 140030420236032 logging_writer.py:48] [41411] accumulated_eval_time=7288.951065, accumulated_logging_time=2.023068, accumulated_submission_time=13223.949195, global_step=41411, preemption_count=0, score=13223.949195, test/accuracy=0.985870, test/loss=0.048565, test/mean_average_precision=0.254170, test/num_examples=43793, total_duration=20516.029171, train/accuracy=0.992140, train/loss=0.025322, train/mean_average_precision=0.525194, validation/accuracy=0.986787, validation/loss=0.045663, validation/mean_average_precision=0.265891, validation/num_examples=43793
I0205 02:54:45.973099 140037886379776 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.1026507019996643, loss=0.0313505157828331
I0205 02:55:17.935923 140030420236032 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.09430450201034546, loss=0.02854166366159916
I0205 02:55:49.127608 140037886379776 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.09564296156167984, loss=0.02987711876630783
I0205 02:56:20.769417 140030420236032 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.11382860690355301, loss=0.032029375433921814
I0205 02:56:52.091423 140037886379776 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.10067601501941681, loss=0.02951492927968502
I0205 02:57:23.774548 140030420236032 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.10217629373073578, loss=0.029567625373601913
I0205 02:57:55.440592 140037886379776 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.1288660168647766, loss=0.03112524189054966
I0205 02:58:17.635135 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:00:19.382373 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:00:22.405472 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:00:25.451764 140205209478976 submission_runner.py:408] Time since start: 20884.02s, 	Step: 42171, 	{'train/accuracy': 0.992421567440033, 'train/loss': 0.02443872205913067, 'train/mean_average_precision': 0.5435246945200095, 'validation/accuracy': 0.9867402911186218, 'validation/loss': 0.045766741037368774, 'validation/mean_average_precision': 0.2670472319105248, 'validation/num_examples': 43793, 'test/accuracy': 0.9859126806259155, 'test/loss': 0.048624247312545776, 'test/mean_average_precision': 0.25526057373272015, 'test/num_examples': 43793, 'score': 13464.0667886734, 'total_duration': 20884.01805138588, 'accumulated_submission_time': 13464.0667886734, 'accumulated_eval_time': 7416.767649650574, 'accumulated_logging_time': 2.0580596923828125}
I0205 03:00:25.475736 140037877987072 logging_writer.py:48] [42171] accumulated_eval_time=7416.767650, accumulated_logging_time=2.058060, accumulated_submission_time=13464.066789, global_step=42171, preemption_count=0, score=13464.066789, test/accuracy=0.985913, test/loss=0.048624, test/mean_average_precision=0.255261, test/num_examples=43793, total_duration=20884.018051, train/accuracy=0.992422, train/loss=0.024439, train/mean_average_precision=0.543525, validation/accuracy=0.986740, validation/loss=0.045767, validation/mean_average_precision=0.267047, validation/num_examples=43793
I0205 03:00:35.212327 140044326471424 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.10083389282226562, loss=0.030989568680524826
I0205 03:01:07.046804 140037877987072 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.11779651790857315, loss=0.029799414798617363
I0205 03:01:38.831737 140044326471424 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.13851365447044373, loss=0.030538709834218025
I0205 03:02:10.622156 140037877987072 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.09118100255727768, loss=0.03100554086267948
I0205 03:02:42.715536 140044326471424 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.09046418964862823, loss=0.029130365699529648
I0205 03:03:15.104442 140037877987072 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.15048782527446747, loss=0.03014983795583248
I0205 03:03:47.281021 140044326471424 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.1066298857331276, loss=0.025736568495631218
I0205 03:04:19.547244 140037877987072 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.1067068949341774, loss=0.029984604567289352
I0205 03:04:25.584361 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:06:29.007853 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:06:32.040362 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:06:35.020091 140205209478976 submission_runner.py:408] Time since start: 21253.59s, 	Step: 42920, 	{'train/accuracy': 0.9927170872688293, 'train/loss': 0.0233779214322567, 'train/mean_average_precision': 0.5733930541995389, 'validation/accuracy': 0.9868364930152893, 'validation/loss': 0.04593338817358017, 'validation/mean_average_precision': 0.2704952414247853, 'validation/num_examples': 43793, 'test/accuracy': 0.9860095381736755, 'test/loss': 0.048736944794654846, 'test/mean_average_precision': 0.25686386270281447, 'test/num_examples': 43793, 'score': 13704.141545772552, 'total_duration': 21253.58636689186, 'accumulated_submission_time': 13704.141545772552, 'accumulated_eval_time': 7546.203330516815, 'accumulated_logging_time': 2.094203472137451}
I0205 03:06:35.044804 140037886379776 logging_writer.py:48] [42920] accumulated_eval_time=7546.203331, accumulated_logging_time=2.094203, accumulated_submission_time=13704.141546, global_step=42920, preemption_count=0, score=13704.141546, test/accuracy=0.986010, test/loss=0.048737, test/mean_average_precision=0.256864, test/num_examples=43793, total_duration=21253.586367, train/accuracy=0.992717, train/loss=0.023378, train/mean_average_precision=0.573393, validation/accuracy=0.986836, validation/loss=0.045933, validation/mean_average_precision=0.270495, validation/num_examples=43793
I0205 03:07:00.893979 140044334864128 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.09634065628051758, loss=0.02842474915087223
I0205 03:07:32.758675 140037886379776 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.1231488361954689, loss=0.03231499344110489
I0205 03:08:04.645940 140044334864128 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.10557682812213898, loss=0.030237993225455284
I0205 03:08:36.383315 140037886379776 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.09789816290140152, loss=0.027300255373120308
I0205 03:09:07.924836 140044334864128 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.10323496162891388, loss=0.0310670118778944
I0205 03:09:39.740380 140037886379776 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.09160435944795609, loss=0.028279606252908707
I0205 03:10:11.734117 140044334864128 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.12320145219564438, loss=0.03131301701068878
I0205 03:10:35.062885 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:12:32.651202 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:12:35.707230 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:12:38.723290 140205209478976 submission_runner.py:408] Time since start: 21617.29s, 	Step: 43675, 	{'train/accuracy': 0.9927061200141907, 'train/loss': 0.02339312806725502, 'train/mean_average_precision': 0.5666941320422774, 'validation/accuracy': 0.9868308305740356, 'validation/loss': 0.045705344527959824, 'validation/mean_average_precision': 0.26667728140623015, 'validation/num_examples': 43793, 'test/accuracy': 0.9858528971672058, 'test/loss': 0.04876760020852089, 'test/mean_average_precision': 0.25349718911025815, 'test/num_examples': 43793, 'score': 13944.12907576561, 'total_duration': 21617.289580583572, 'accumulated_submission_time': 13944.12907576561, 'accumulated_eval_time': 7669.8636927604675, 'accumulated_logging_time': 2.1300570964813232}
I0205 03:12:38.749372 140030420236032 logging_writer.py:48] [43675] accumulated_eval_time=7669.863693, accumulated_logging_time=2.130057, accumulated_submission_time=13944.129076, global_step=43675, preemption_count=0, score=13944.129076, test/accuracy=0.985853, test/loss=0.048768, test/mean_average_precision=0.253497, test/num_examples=43793, total_duration=21617.289581, train/accuracy=0.992706, train/loss=0.023393, train/mean_average_precision=0.566694, validation/accuracy=0.986831, validation/loss=0.045705, validation/mean_average_precision=0.266677, validation/num_examples=43793
I0205 03:12:47.160626 140037877987072 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.18584848940372467, loss=0.030324803665280342
I0205 03:13:19.089069 140030420236032 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.1363380253314972, loss=0.029554573819041252
I0205 03:13:51.500805 140037877987072 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.10574810206890106, loss=0.030684392899274826
I0205 03:14:23.533399 140030420236032 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.0989428386092186, loss=0.02851993776857853
I0205 03:14:55.046416 140037877987072 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.1123504638671875, loss=0.029002830386161804
I0205 03:15:27.108783 140030420236032 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.11165144294500351, loss=0.030086811631917953
I0205 03:15:58.923574 140037877987072 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.12719182670116425, loss=0.0316631980240345
I0205 03:16:31.024152 140030420236032 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.11147098243236542, loss=0.02748277597129345
I0205 03:16:38.757692 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:18:36.597531 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:18:39.653614 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:18:42.647753 140205209478976 submission_runner.py:408] Time since start: 21981.21s, 	Step: 44425, 	{'train/accuracy': 0.9929761290550232, 'train/loss': 0.02256980538368225, 'train/mean_average_precision': 0.5881898919000075, 'validation/accuracy': 0.9867346286773682, 'validation/loss': 0.046440139412879944, 'validation/mean_average_precision': 0.2643463754657173, 'validation/num_examples': 43793, 'test/accuracy': 0.9857774972915649, 'test/loss': 0.04942133650183678, 'test/mean_average_precision': 0.25029550253572547, 'test/num_examples': 43793, 'score': 14184.10730624199, 'total_duration': 21981.214041233063, 'accumulated_submission_time': 14184.10730624199, 'accumulated_eval_time': 7793.7537133693695, 'accumulated_logging_time': 2.167064905166626}
I0205 03:18:42.671404 140037886379776 logging_writer.py:48] [44425] accumulated_eval_time=7793.753713, accumulated_logging_time=2.167065, accumulated_submission_time=14184.107306, global_step=44425, preemption_count=0, score=14184.107306, test/accuracy=0.985777, test/loss=0.049421, test/mean_average_precision=0.250296, test/num_examples=43793, total_duration=21981.214041, train/accuracy=0.992976, train/loss=0.022570, train/mean_average_precision=0.588190, validation/accuracy=0.986735, validation/loss=0.046440, validation/mean_average_precision=0.264346, validation/num_examples=43793
I0205 03:19:07.021364 140044334864128 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.12303709238767624, loss=0.0322529673576355
I0205 03:19:39.420469 140037886379776 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.09658993035554886, loss=0.02863825485110283
I0205 03:20:11.883746 140044334864128 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.09764610230922699, loss=0.02837727591395378
I0205 03:20:43.872481 140037886379776 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.12747810781002045, loss=0.03195766359567642
I0205 03:21:15.939355 140044334864128 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.1362699568271637, loss=0.029390016570687294
I0205 03:21:47.540136 140037886379776 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.128694087266922, loss=0.028078172355890274
I0205 03:22:19.762526 140044334864128 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.11234665662050247, loss=0.0281937625259161
I0205 03:22:42.735147 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:24:41.879230 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:24:45.256344 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:24:48.633926 140205209478976 submission_runner.py:408] Time since start: 22347.20s, 	Step: 45172, 	{'train/accuracy': 0.9928836226463318, 'train/loss': 0.023163873702287674, 'train/mean_average_precision': 0.5603069323792206, 'validation/accuracy': 0.9867216348648071, 'validation/loss': 0.045841023325920105, 'validation/mean_average_precision': 0.2698805055719386, 'validation/num_examples': 43793, 'test/accuracy': 0.9857913851737976, 'test/loss': 0.0486513152718544, 'test/mean_average_precision': 0.2532316248694131, 'test/num_examples': 43793, 'score': 14424.139218568802, 'total_duration': 22347.200195789337, 'accumulated_submission_time': 14424.139218568802, 'accumulated_eval_time': 7919.652441978455, 'accumulated_logging_time': 2.2020323276519775}
I0205 03:24:48.661586 140030420236032 logging_writer.py:48] [45172] accumulated_eval_time=7919.652442, accumulated_logging_time=2.202032, accumulated_submission_time=14424.139219, global_step=45172, preemption_count=0, score=14424.139219, test/accuracy=0.985791, test/loss=0.048651, test/mean_average_precision=0.253232, test/num_examples=43793, total_duration=22347.200196, train/accuracy=0.992884, train/loss=0.023164, train/mean_average_precision=0.560307, validation/accuracy=0.986722, validation/loss=0.045841, validation/mean_average_precision=0.269881, validation/num_examples=43793
I0205 03:24:58.132863 140037877987072 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.11912406980991364, loss=0.029809990897774696
I0205 03:25:30.539587 140030420236032 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.11156992614269257, loss=0.027450116351246834
I0205 03:26:01.902046 140037877987072 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.112070731818676, loss=0.028355561196804047
I0205 03:26:33.296331 140030420236032 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.13543091714382172, loss=0.028912050649523735
I0205 03:27:04.763696 140037877987072 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.13385531306266785, loss=0.03350451588630676
I0205 03:27:36.846007 140030420236032 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.14966632425785065, loss=0.027788637205958366
I0205 03:28:08.527402 140037877987072 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.14025579392910004, loss=0.029732292518019676
I0205 03:28:40.176663 140030420236032 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.143802672624588, loss=0.028985196724534035
I0205 03:28:48.790568 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:30:45.419194 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:30:48.471198 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:30:51.486840 140205209478976 submission_runner.py:408] Time since start: 22710.05s, 	Step: 45928, 	{'train/accuracy': 0.9925437569618225, 'train/loss': 0.0238797627389431, 'train/mean_average_precision': 0.5548479854623927, 'validation/accuracy': 0.9868665337562561, 'validation/loss': 0.046299174427986145, 'validation/mean_average_precision': 0.2675998202951851, 'validation/num_examples': 43793, 'test/accuracy': 0.9859084486961365, 'test/loss': 0.04929661750793457, 'test/mean_average_precision': 0.2490588585001329, 'test/num_examples': 43793, 'score': 14664.235578775406, 'total_duration': 22710.05312728882, 'accumulated_submission_time': 14664.235578775406, 'accumulated_eval_time': 8042.348666667938, 'accumulated_logging_time': 2.241867780685425}
I0205 03:30:51.510809 140037886379776 logging_writer.py:48] [45928] accumulated_eval_time=8042.348667, accumulated_logging_time=2.241868, accumulated_submission_time=14664.235579, global_step=45928, preemption_count=0, score=14664.235579, test/accuracy=0.985908, test/loss=0.049297, test/mean_average_precision=0.249059, test/num_examples=43793, total_duration=22710.053127, train/accuracy=0.992544, train/loss=0.023880, train/mean_average_precision=0.554848, validation/accuracy=0.986867, validation/loss=0.046299, validation/mean_average_precision=0.267600, validation/num_examples=43793
I0205 03:31:14.689375 140044334864128 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.1116078794002533, loss=0.030720163136720657
I0205 03:31:46.512114 140037886379776 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.1066303700208664, loss=0.02801385335624218
I0205 03:32:18.159452 140044334864128 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.09953580796718597, loss=0.02812616340816021
I0205 03:32:49.738332 140037886379776 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.12112610787153244, loss=0.028141731396317482
I0205 03:33:21.877296 140044334864128 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.10580790787935257, loss=0.029992466792464256
I0205 03:33:53.565132 140037886379776 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.11887659132480621, loss=0.029787395149469376
I0205 03:34:25.333752 140044334864128 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.10600075870752335, loss=0.029313115403056145
I0205 03:34:51.649127 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:36:47.264137 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:36:50.342261 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:36:53.350089 140205209478976 submission_runner.py:408] Time since start: 23071.92s, 	Step: 46685, 	{'train/accuracy': 0.9926679134368896, 'train/loss': 0.02372351661324501, 'train/mean_average_precision': 0.5522737730499209, 'validation/accuracy': 0.986710250377655, 'validation/loss': 0.04607642814517021, 'validation/mean_average_precision': 0.26736929985496544, 'validation/num_examples': 43793, 'test/accuracy': 0.985787570476532, 'test/loss': 0.04892697185277939, 'test/mean_average_precision': 0.255223206791752, 'test/num_examples': 43793, 'score': 14904.341437578201, 'total_duration': 23071.91637802124, 'accumulated_submission_time': 14904.341437578201, 'accumulated_eval_time': 8164.049585580826, 'accumulated_logging_time': 2.278223991394043}
I0205 03:36:53.375319 140030420236032 logging_writer.py:48] [46685] accumulated_eval_time=8164.049586, accumulated_logging_time=2.278224, accumulated_submission_time=14904.341438, global_step=46685, preemption_count=0, score=14904.341438, test/accuracy=0.985788, test/loss=0.048927, test/mean_average_precision=0.255223, test/num_examples=43793, total_duration=23071.916378, train/accuracy=0.992668, train/loss=0.023724, train/mean_average_precision=0.552274, validation/accuracy=0.986710, validation/loss=0.046076, validation/mean_average_precision=0.267369, validation/num_examples=43793
I0205 03:36:58.504836 140044326471424 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.12125493586063385, loss=0.026686610653996468
I0205 03:37:30.317998 140030420236032 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.10993983596563339, loss=0.027908094227313995
I0205 03:38:02.205110 140044326471424 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.12268133461475372, loss=0.0295261163264513
I0205 03:38:34.114322 140030420236032 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.1137198880314827, loss=0.029255857691168785
I0205 03:39:05.787761 140044326471424 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.12545816600322723, loss=0.027948740869760513
I0205 03:39:37.835044 140030420236032 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.11900954693555832, loss=0.028241924941539764
I0205 03:40:10.010220 140044326471424 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.13553766906261444, loss=0.02779637835919857
I0205 03:40:42.155675 140030420236032 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.10341247171163559, loss=0.024767925962805748
I0205 03:40:53.674605 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:42:54.227923 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:42:57.226460 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:43:00.197043 140205209478976 submission_runner.py:408] Time since start: 23438.76s, 	Step: 47437, 	{'train/accuracy': 0.9925719499588013, 'train/loss': 0.023791460320353508, 'train/mean_average_precision': 0.5576811836200534, 'validation/accuracy': 0.9868466854095459, 'validation/loss': 0.04642287641763687, 'validation/mean_average_precision': 0.2657642561931333, 'validation/num_examples': 43793, 'test/accuracy': 0.9859118461608887, 'test/loss': 0.049479302018880844, 'test/mean_average_precision': 0.25251685623100334, 'test/num_examples': 43793, 'score': 15144.60926938057, 'total_duration': 23438.763329267502, 'accumulated_submission_time': 15144.60926938057, 'accumulated_eval_time': 8290.571984052658, 'accumulated_logging_time': 2.3142735958099365}
I0205 03:43:00.221990 140037886379776 logging_writer.py:48] [47437] accumulated_eval_time=8290.571984, accumulated_logging_time=2.314274, accumulated_submission_time=15144.609269, global_step=47437, preemption_count=0, score=15144.609269, test/accuracy=0.985912, test/loss=0.049479, test/mean_average_precision=0.252517, test/num_examples=43793, total_duration=23438.763329, train/accuracy=0.992572, train/loss=0.023791, train/mean_average_precision=0.557681, validation/accuracy=0.986847, validation/loss=0.046423, validation/mean_average_precision=0.265764, validation/num_examples=43793
I0205 03:43:20.535916 140044334864128 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.11612353473901749, loss=0.02755938097834587
I0205 03:43:52.380183 140037886379776 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.12530562281608582, loss=0.026231912896037102
I0205 03:44:24.365292 140044334864128 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.11960884183645248, loss=0.030340906232595444
I0205 03:44:56.429194 140037886379776 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.13236351311206818, loss=0.028656335547566414
I0205 03:45:28.248858 140044334864128 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.14056560397148132, loss=0.027687454596161842
I0205 03:45:59.986789 140037886379776 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.12087708711624146, loss=0.0276767797768116
I0205 03:46:32.265575 140044334864128 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.15608495473861694, loss=0.027065036818385124
I0205 03:47:00.411248 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:48:58.522162 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:49:01.697170 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:49:04.849130 140205209478976 submission_runner.py:408] Time since start: 23803.42s, 	Step: 48189, 	{'train/accuracy': 0.9928362369537354, 'train/loss': 0.023019516840577126, 'train/mean_average_precision': 0.558678018020372, 'validation/accuracy': 0.9867460131645203, 'validation/loss': 0.04643058776855469, 'validation/mean_average_precision': 0.27327413772628895, 'validation/num_examples': 43793, 'test/accuracy': 0.9858225584030151, 'test/loss': 0.04929651692509651, 'test/mean_average_precision': 0.25214949307651735, 'test/num_examples': 43793, 'score': 15384.76617383957, 'total_duration': 23803.415416240692, 'accumulated_submission_time': 15384.76617383957, 'accumulated_eval_time': 8415.009822368622, 'accumulated_logging_time': 2.3507189750671387}
I0205 03:49:04.874394 140037877987072 logging_writer.py:48] [48189] accumulated_eval_time=8415.009822, accumulated_logging_time=2.350719, accumulated_submission_time=15384.766174, global_step=48189, preemption_count=0, score=15384.766174, test/accuracy=0.985823, test/loss=0.049297, test/mean_average_precision=0.252149, test/num_examples=43793, total_duration=23803.415416, train/accuracy=0.992836, train/loss=0.023020, train/mean_average_precision=0.558678, validation/accuracy=0.986746, validation/loss=0.046431, validation/mean_average_precision=0.273274, validation/num_examples=43793
I0205 03:49:09.082513 140044326471424 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.10247039049863815, loss=0.026713205501437187
I0205 03:49:40.974560 140037877987072 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.1376875787973404, loss=0.030963361263275146
I0205 03:50:13.652346 140044326471424 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.13131576776504517, loss=0.02731647901237011
I0205 03:50:45.556472 140037877987072 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.1243402287364006, loss=0.02805688977241516
I0205 03:51:17.544720 140044326471424 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.15080535411834717, loss=0.027707960456609726
I0205 03:51:49.174462 140037877987072 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.11758621782064438, loss=0.028850601986050606
I0205 03:52:20.911668 140044326471424 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.12569555640220642, loss=0.027902215719223022
I0205 03:52:52.750667 140037877987072 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.15313538908958435, loss=0.027619322761893272
I0205 03:53:04.853594 140205209478976 spec.py:321] Evaluating on the training split.
I0205 03:55:00.124536 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 03:55:03.562266 140205209478976 spec.py:349] Evaluating on the test split.
I0205 03:55:07.013615 140205209478976 submission_runner.py:408] Time since start: 24165.58s, 	Step: 48939, 	{'train/accuracy': 0.9928126335144043, 'train/loss': 0.022829918190836906, 'train/mean_average_precision': 0.5758678220084759, 'validation/accuracy': 0.9868665337562561, 'validation/loss': 0.046328239142894745, 'validation/mean_average_precision': 0.2763794516470148, 'validation/num_examples': 43793, 'test/accuracy': 0.9858726859092712, 'test/loss': 0.049397435039281845, 'test/mean_average_precision': 0.2561368281989875, 'test/num_examples': 43793, 'score': 15624.714596033096, 'total_duration': 24165.579883813858, 'accumulated_submission_time': 15624.714596033096, 'accumulated_eval_time': 8537.169777154922, 'accumulated_logging_time': 2.38702654838562}
I0205 03:55:07.043062 140030420236032 logging_writer.py:48] [48939] accumulated_eval_time=8537.169777, accumulated_logging_time=2.387027, accumulated_submission_time=15624.714596, global_step=48939, preemption_count=0, score=15624.714596, test/accuracy=0.985873, test/loss=0.049397, test/mean_average_precision=0.256137, test/num_examples=43793, total_duration=24165.579884, train/accuracy=0.992813, train/loss=0.022830, train/mean_average_precision=0.575868, validation/accuracy=0.986867, validation/loss=0.046328, validation/mean_average_precision=0.276379, validation/num_examples=43793
I0205 03:55:27.081221 140044334864128 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.1094515323638916, loss=0.026204172521829605
I0205 03:55:59.512496 140030420236032 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.11296706646680832, loss=0.02581268735229969
I0205 03:56:31.612720 140044334864128 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.10637258738279343, loss=0.024852609261870384
I0205 03:57:04.216683 140030420236032 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.12154728919267654, loss=0.02622576244175434
I0205 03:57:36.455257 140044334864128 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1134282574057579, loss=0.026865767315030098
I0205 03:58:08.466089 140030420236032 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.143118754029274, loss=0.029773401096463203
I0205 03:58:40.623912 140044334864128 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.12070796638727188, loss=0.02670128270983696
I0205 03:59:07.078442 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:01:01.637992 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:01:04.714689 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:01:07.753557 140205209478976 submission_runner.py:408] Time since start: 24526.32s, 	Step: 49683, 	{'train/accuracy': 0.9930967688560486, 'train/loss': 0.021930977702140808, 'train/mean_average_precision': 0.5948772431380066, 'validation/accuracy': 0.986885666847229, 'validation/loss': 0.0467839241027832, 'validation/mean_average_precision': 0.2694161972938923, 'validation/num_examples': 43793, 'test/accuracy': 0.9858827590942383, 'test/loss': 0.049932315945625305, 'test/mean_average_precision': 0.25306111751823346, 'test/num_examples': 43793, 'score': 15864.714323282242, 'total_duration': 24526.31983613968, 'accumulated_submission_time': 15864.714323282242, 'accumulated_eval_time': 8657.844844341278, 'accumulated_logging_time': 2.4283759593963623}
I0205 04:01:07.778684 140037877987072 logging_writer.py:48] [49683] accumulated_eval_time=8657.844844, accumulated_logging_time=2.428376, accumulated_submission_time=15864.714323, global_step=49683, preemption_count=0, score=15864.714323, test/accuracy=0.985883, test/loss=0.049932, test/mean_average_precision=0.253061, test/num_examples=43793, total_duration=24526.319836, train/accuracy=0.993097, train/loss=0.021931, train/mean_average_precision=0.594877, validation/accuracy=0.986886, validation/loss=0.046784, validation/mean_average_precision=0.269416, validation/num_examples=43793
I0205 04:01:13.540649 140037886379776 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.14288963377475739, loss=0.027786115184426308
I0205 04:01:45.235851 140037877987072 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.11893589794635773, loss=0.025632472708821297
I0205 04:02:16.791724 140037886379776 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.11665887385606766, loss=0.029058178886771202
I0205 04:02:48.595459 140037877987072 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.12186115980148315, loss=0.02650856040418148
I0205 04:03:20.071762 140037886379776 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.14685273170471191, loss=0.03139087185263634
I0205 04:03:51.301638 140037877987072 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.1477174013853073, loss=0.027037370949983597
I0205 04:04:23.048064 140037886379776 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.12019379436969757, loss=0.026061460375785828
I0205 04:04:54.687257 140037877987072 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.1404123455286026, loss=0.028305841609835625
I0205 04:05:08.041183 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:07:02.436006 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:07:05.470319 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:07:08.485714 140205209478976 submission_runner.py:408] Time since start: 24887.05s, 	Step: 50443, 	{'train/accuracy': 0.9935494065284729, 'train/loss': 0.020813167095184326, 'train/mean_average_precision': 0.6246684589240922, 'validation/accuracy': 0.9867971539497375, 'validation/loss': 0.04655579850077629, 'validation/mean_average_precision': 0.27577702724714537, 'validation/num_examples': 43793, 'test/accuracy': 0.9857075810432434, 'test/loss': 0.04983345419168472, 'test/mean_average_precision': 0.25529045165687886, 'test/num_examples': 43793, 'score': 16104.945301055908, 'total_duration': 24887.052001953125, 'accumulated_submission_time': 16104.945301055908, 'accumulated_eval_time': 8778.289328336716, 'accumulated_logging_time': 2.4652960300445557}
I0205 04:07:08.510725 140044326471424 logging_writer.py:48] [50443] accumulated_eval_time=8778.289328, accumulated_logging_time=2.465296, accumulated_submission_time=16104.945301, global_step=50443, preemption_count=0, score=16104.945301, test/accuracy=0.985708, test/loss=0.049833, test/mean_average_precision=0.255290, test/num_examples=43793, total_duration=24887.052002, train/accuracy=0.993549, train/loss=0.020813, train/mean_average_precision=0.624668, validation/accuracy=0.986797, validation/loss=0.046556, validation/mean_average_precision=0.275777, validation/num_examples=43793
I0205 04:07:27.434624 140044334864128 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.14144042134284973, loss=0.02698514424264431
I0205 04:07:59.207023 140044326471424 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.13666297495365143, loss=0.026994094252586365
I0205 04:08:30.995574 140044334864128 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.12190611660480499, loss=0.026517482474446297
I0205 04:09:02.652710 140044326471424 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.13272811472415924, loss=0.03026840090751648
I0205 04:09:34.144647 140044334864128 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.12120820581912994, loss=0.027908790856599808
I0205 04:10:05.818273 140044326471424 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.12459073960781097, loss=0.027657149359583855
I0205 04:10:37.205969 140044334864128 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.12485045939683914, loss=0.027473388239741325
I0205 04:11:08.726140 140044326471424 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.13407284021377563, loss=0.026236332952976227
I0205 04:11:08.732343 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:13:04.843768 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:13:07.881356 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:13:10.834870 140205209478976 submission_runner.py:408] Time since start: 25249.40s, 	Step: 51201, 	{'train/accuracy': 0.9938346743583679, 'train/loss': 0.019873609766364098, 'train/mean_average_precision': 0.6402711633729733, 'validation/accuracy': 0.9867033958435059, 'validation/loss': 0.04749038815498352, 'validation/mean_average_precision': 0.26534099133944655, 'validation/num_examples': 43793, 'test/accuracy': 0.9858195781707764, 'test/loss': 0.05073053389787674, 'test/mean_average_precision': 0.24764507307342512, 'test/num_examples': 43793, 'score': 16345.135685920715, 'total_duration': 25249.40115904808, 'accumulated_submission_time': 16345.135685920715, 'accumulated_eval_time': 8900.391793251038, 'accumulated_logging_time': 2.501399278640747}
I0205 04:13:10.861109 140030420236032 logging_writer.py:48] [51201] accumulated_eval_time=8900.391793, accumulated_logging_time=2.501399, accumulated_submission_time=16345.135686, global_step=51201, preemption_count=0, score=16345.135686, test/accuracy=0.985820, test/loss=0.050731, test/mean_average_precision=0.247645, test/num_examples=43793, total_duration=25249.401159, train/accuracy=0.993835, train/loss=0.019874, train/mean_average_precision=0.640271, validation/accuracy=0.986703, validation/loss=0.047490, validation/mean_average_precision=0.265341, validation/num_examples=43793
I0205 04:13:42.364698 140037877987072 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.14748787879943848, loss=0.02469549886882305
I0205 04:14:14.660056 140030420236032 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.18628500401973724, loss=0.02711232751607895
I0205 04:14:47.085717 140037877987072 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.12724794447422028, loss=0.026551665738224983
I0205 04:15:19.703288 140030420236032 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.1363779753446579, loss=0.026551153510808945
I0205 04:15:52.173651 140037877987072 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.16020546853542328, loss=0.025932013988494873
I0205 04:16:24.526176 140030420236032 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.12467052787542343, loss=0.027510929852724075
I0205 04:16:56.641356 140037877987072 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.1481410413980484, loss=0.025864411145448685
I0205 04:17:10.957514 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:19:05.938312 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:19:08.991546 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:19:11.999033 140205209478976 submission_runner.py:408] Time since start: 25610.57s, 	Step: 51945, 	{'train/accuracy': 0.9937007427215576, 'train/loss': 0.020169697701931, 'train/mean_average_precision': 0.6355450158745422, 'validation/accuracy': 0.9866883754730225, 'validation/loss': 0.04740728810429573, 'validation/mean_average_precision': 0.26725297012395566, 'validation/num_examples': 43793, 'test/accuracy': 0.9858056902885437, 'test/loss': 0.0504932701587677, 'test/mean_average_precision': 0.25328637582410407, 'test/num_examples': 43793, 'score': 16585.19568347931, 'total_duration': 25610.565322637558, 'accumulated_submission_time': 16585.19568347931, 'accumulated_eval_time': 9021.43328166008, 'accumulated_logging_time': 2.5409858226776123}
I0205 04:19:12.024738 140037886379776 logging_writer.py:48] [51945] accumulated_eval_time=9021.433282, accumulated_logging_time=2.540986, accumulated_submission_time=16585.195683, global_step=51945, preemption_count=0, score=16585.195683, test/accuracy=0.985806, test/loss=0.050493, test/mean_average_precision=0.253286, test/num_examples=43793, total_duration=25610.565323, train/accuracy=0.993701, train/loss=0.020170, train/mean_average_precision=0.635545, validation/accuracy=0.986688, validation/loss=0.047407, validation/mean_average_precision=0.267253, validation/num_examples=43793
I0205 04:19:30.235669 140044334864128 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.13381430506706238, loss=0.026742855086922646
I0205 04:20:02.210376 140037886379776 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.154372438788414, loss=0.025650344789028168
I0205 04:20:34.285865 140044334864128 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.1472802609205246, loss=0.027066826820373535
I0205 04:21:06.147669 140037886379776 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.1581418514251709, loss=0.028158634901046753
I0205 04:21:37.856294 140044334864128 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.1369377076625824, loss=0.02743048407137394
I0205 04:22:09.579694 140037886379776 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.1252392679452896, loss=0.024152332916855812
I0205 04:22:41.470709 140044334864128 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.14336374402046204, loss=0.024451039731502533
I0205 04:23:12.005494 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:25:05.325023 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:25:08.341917 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:25:11.366699 140205209478976 submission_runner.py:408] Time since start: 25969.93s, 	Step: 52698, 	{'train/accuracy': 0.9938126802444458, 'train/loss': 0.01998131349682808, 'train/mean_average_precision': 0.6342326759145145, 'validation/accuracy': 0.9867126941680908, 'validation/loss': 0.047159794718027115, 'validation/mean_average_precision': 0.2665871337103819, 'validation/num_examples': 43793, 'test/accuracy': 0.9858128428459167, 'test/loss': 0.05055546015501022, 'test/mean_average_precision': 0.24676034265588734, 'test/num_examples': 43793, 'score': 16825.145767211914, 'total_duration': 25969.932988643646, 'accumulated_submission_time': 16825.145767211914, 'accumulated_eval_time': 9140.794448375702, 'accumulated_logging_time': 2.5773093700408936}
I0205 04:25:11.391716 140037877987072 logging_writer.py:48] [52698] accumulated_eval_time=9140.794448, accumulated_logging_time=2.577309, accumulated_submission_time=16825.145767, global_step=52698, preemption_count=0, score=16825.145767, test/accuracy=0.985813, test/loss=0.050555, test/mean_average_precision=0.246760, test/num_examples=43793, total_duration=25969.932989, train/accuracy=0.993813, train/loss=0.019981, train/mean_average_precision=0.634233, validation/accuracy=0.986713, validation/loss=0.047160, validation/mean_average_precision=0.266587, validation/num_examples=43793
I0205 04:25:12.363891 140044326471424 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.13215474784374237, loss=0.025300724431872368
I0205 04:25:44.372275 140037877987072 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.16819331049919128, loss=0.028909873217344284
I0205 04:26:15.937597 140044326471424 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.13633447885513306, loss=0.025994988158345222
I0205 04:26:47.188299 140037877987072 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.15294276177883148, loss=0.0287384781986475
I0205 04:27:18.731223 140044326471424 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.15231092274188995, loss=0.025568043813109398
I0205 04:27:50.182533 140037877987072 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.12589140236377716, loss=0.023572223260998726
I0205 04:28:21.970794 140044326471424 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.13092809915542603, loss=0.023201853036880493
I0205 04:28:53.337900 140037877987072 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.14134034514427185, loss=0.027003474533557892
I0205 04:29:11.383420 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:31:08.223065 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:31:11.591135 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:31:14.957448 140205209478976 submission_runner.py:408] Time since start: 26333.52s, 	Step: 53458, 	{'train/accuracy': 0.993604838848114, 'train/loss': 0.02053910121321678, 'train/mean_average_precision': 0.6163999862960505, 'validation/accuracy': 0.9866956472396851, 'validation/loss': 0.04780729115009308, 'validation/mean_average_precision': 0.26729791289631466, 'validation/num_examples': 43793, 'test/accuracy': 0.9857555627822876, 'test/loss': 0.050805043429136276, 'test/mean_average_precision': 0.25253103186839687, 'test/num_examples': 43793, 'score': 17065.106918811798, 'total_duration': 26333.523721456528, 'accumulated_submission_time': 17065.106918811798, 'accumulated_eval_time': 9264.368415594101, 'accumulated_logging_time': 2.6135025024414062}
I0205 04:31:14.985911 140037886379776 logging_writer.py:48] [53458] accumulated_eval_time=9264.368416, accumulated_logging_time=2.613503, accumulated_submission_time=17065.106919, global_step=53458, preemption_count=0, score=17065.106919, test/accuracy=0.985756, test/loss=0.050805, test/mean_average_precision=0.252531, test/num_examples=43793, total_duration=26333.523721, train/accuracy=0.993605, train/loss=0.020539, train/mean_average_precision=0.616400, validation/accuracy=0.986696, validation/loss=0.047807, validation/mean_average_precision=0.267298, validation/num_examples=43793
I0205 04:31:29.239063 140044334864128 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.15301184356212616, loss=0.026839707046747208
I0205 04:32:02.135038 140037886379776 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.13289745151996613, loss=0.02608180232346058
I0205 04:32:34.386599 140044334864128 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.16322214901447296, loss=0.02594137191772461
I0205 04:33:06.430847 140037886379776 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.13531990349292755, loss=0.02460429258644581
I0205 04:33:38.866661 140044334864128 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.14124800264835358, loss=0.025333361700177193
I0205 04:34:11.334123 140037886379776 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.1647292971611023, loss=0.0270000658929348
I0205 04:34:43.705980 140044334864128 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.14098797738552094, loss=0.02455006167292595
I0205 04:35:15.082626 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:37:13.218173 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:37:16.292211 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:37:19.326368 140205209478976 submission_runner.py:408] Time since start: 26697.89s, 	Step: 54198, 	{'train/accuracy': 0.993502676486969, 'train/loss': 0.02074066549539566, 'train/mean_average_precision': 0.6146754178331644, 'validation/accuracy': 0.986743152141571, 'validation/loss': 0.047809213399887085, 'validation/mean_average_precision': 0.26794072664347346, 'validation/num_examples': 43793, 'test/accuracy': 0.9857187271118164, 'test/loss': 0.05110371857881546, 'test/mean_average_precision': 0.2504868933628825, 'test/num_examples': 43793, 'score': 17305.169719457626, 'total_duration': 26697.89265561104, 'accumulated_submission_time': 17305.169719457626, 'accumulated_eval_time': 9388.612121343613, 'accumulated_logging_time': 2.655043125152588}
I0205 04:37:19.351813 140030420236032 logging_writer.py:48] [54198] accumulated_eval_time=9388.612121, accumulated_logging_time=2.655043, accumulated_submission_time=17305.169719, global_step=54198, preemption_count=0, score=17305.169719, test/accuracy=0.985719, test/loss=0.051104, test/mean_average_precision=0.250487, test/num_examples=43793, total_duration=26697.892656, train/accuracy=0.993503, train/loss=0.020741, train/mean_average_precision=0.614675, validation/accuracy=0.986743, validation/loss=0.047809, validation/mean_average_precision=0.267941, validation/num_examples=43793
I0205 04:37:20.357163 140044326471424 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.18276448547840118, loss=0.026890218257904053
I0205 04:37:51.927514 140030420236032 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.15644808113574982, loss=0.027757208794355392
I0205 04:38:23.702906 140044326471424 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.1954009234905243, loss=0.027367722243070602
I0205 04:38:55.336870 140030420236032 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.16674596071243286, loss=0.024409227073192596
I0205 04:39:27.100248 140044326471424 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.1632649153470993, loss=0.022934766486287117
I0205 04:39:59.060079 140030420236032 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.17526870965957642, loss=0.027438201010227203
I0205 04:40:30.911356 140044326471424 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2042076289653778, loss=0.025075148791074753
I0205 04:41:02.437498 140030420236032 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.15952973067760468, loss=0.027435341849923134
I0205 04:41:19.492835 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:43:16.798812 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:43:19.822890 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:43:22.883042 140205209478976 submission_runner.py:408] Time since start: 27061.45s, 	Step: 54955, 	{'train/accuracy': 0.9935206770896912, 'train/loss': 0.020450828596949577, 'train/mean_average_precision': 0.6152469816588861, 'validation/accuracy': 0.9867601990699768, 'validation/loss': 0.0481349416077137, 'validation/mean_average_precision': 0.2696156510939548, 'validation/num_examples': 43793, 'test/accuracy': 0.985710084438324, 'test/loss': 0.051288455724716187, 'test/mean_average_precision': 0.25568876173671473, 'test/num_examples': 43793, 'score': 17545.279770612717, 'total_duration': 27061.449331760406, 'accumulated_submission_time': 17545.279770612717, 'accumulated_eval_time': 9512.002286434174, 'accumulated_logging_time': 2.6914784908294678}
I0205 04:43:22.908808 140037877987072 logging_writer.py:48] [54955] accumulated_eval_time=9512.002286, accumulated_logging_time=2.691478, accumulated_submission_time=17545.279771, global_step=54955, preemption_count=0, score=17545.279771, test/accuracy=0.985710, test/loss=0.051288, test/mean_average_precision=0.255689, test/num_examples=43793, total_duration=27061.449332, train/accuracy=0.993521, train/loss=0.020451, train/mean_average_precision=0.615247, validation/accuracy=0.986760, validation/loss=0.048135, validation/mean_average_precision=0.269616, validation/num_examples=43793
I0205 04:43:37.504651 140037886379776 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.17649905383586884, loss=0.026161905378103256
I0205 04:44:09.216840 140037877987072 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.16608388721942902, loss=0.02617020159959793
I0205 04:44:41.236231 140037886379776 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.16647058725357056, loss=0.024523552507162094
I0205 04:45:12.790808 140037877987072 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.14674988389015198, loss=0.024106668308377266
I0205 04:45:44.561815 140037886379776 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.17622509598731995, loss=0.028502196073532104
I0205 04:46:16.333712 140037877987072 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.15628677606582642, loss=0.024456102401018143
I0205 04:46:48.032004 140037886379776 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.14629791676998138, loss=0.024521708488464355
I0205 04:47:19.739065 140037877987072 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1506621539592743, loss=0.025590846315026283
I0205 04:47:22.930902 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:49:15.418013 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:49:18.456524 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:49:21.502559 140205209478976 submission_runner.py:408] Time since start: 27420.07s, 	Step: 55711, 	{'train/accuracy': 0.9935681819915771, 'train/loss': 0.020248262211680412, 'train/mean_average_precision': 0.6322992889381533, 'validation/accuracy': 0.9867435693740845, 'validation/loss': 0.04861031472682953, 'validation/mean_average_precision': 0.2674156022578643, 'validation/num_examples': 43793, 'test/accuracy': 0.9857816696166992, 'test/loss': 0.05180561542510986, 'test/mean_average_precision': 0.25602466362856396, 'test/num_examples': 43793, 'score': 17785.269641399384, 'total_duration': 27420.068832159042, 'accumulated_submission_time': 17785.269641399384, 'accumulated_eval_time': 9630.57388138771, 'accumulated_logging_time': 2.7295162677764893}
I0205 04:49:21.533762 140044326471424 logging_writer.py:48] [55711] accumulated_eval_time=9630.573881, accumulated_logging_time=2.729516, accumulated_submission_time=17785.269641, global_step=55711, preemption_count=0, score=17785.269641, test/accuracy=0.985782, test/loss=0.051806, test/mean_average_precision=0.256025, test/num_examples=43793, total_duration=27420.068832, train/accuracy=0.993568, train/loss=0.020248, train/mean_average_precision=0.632299, validation/accuracy=0.986744, validation/loss=0.048610, validation/mean_average_precision=0.267416, validation/num_examples=43793
I0205 04:49:50.457651 140044334864128 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.15525589883327484, loss=0.023367594927549362
I0205 04:50:22.283095 140044326471424 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.17202290892601013, loss=0.022075019776821136
I0205 04:50:53.738136 140044334864128 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.13403476774692535, loss=0.02276422083377838
I0205 04:51:26.085678 140044326471424 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.20816950500011444, loss=0.025866640731692314
I0205 04:51:57.983254 140044334864128 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.17601387202739716, loss=0.025911247357726097
I0205 04:52:29.831202 140044326471424 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.13757337629795074, loss=0.022839544340968132
I0205 04:53:01.551526 140044334864128 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.15317891538143158, loss=0.02469007670879364
I0205 04:53:21.773325 140205209478976 spec.py:321] Evaluating on the training split.
I0205 04:55:19.347032 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 04:55:22.432844 140205209478976 spec.py:349] Evaluating on the test split.
I0205 04:55:25.466907 140205209478976 submission_runner.py:408] Time since start: 27784.03s, 	Step: 56465, 	{'train/accuracy': 0.9937283992767334, 'train/loss': 0.019526956602931023, 'train/mean_average_precision': 0.6458244588409702, 'validation/accuracy': 0.9866229891777039, 'validation/loss': 0.049001581966876984, 'validation/mean_average_precision': 0.2654806746801502, 'validation/num_examples': 43793, 'test/accuracy': 0.9857151508331299, 'test/loss': 0.052170172333717346, 'test/mean_average_precision': 0.24860228801934148, 'test/num_examples': 43793, 'score': 18025.476142644882, 'total_duration': 27784.033193588257, 'accumulated_submission_time': 18025.476142644882, 'accumulated_eval_time': 9754.26741719246, 'accumulated_logging_time': 2.7738380432128906}
I0205 04:55:25.492645 140030420236032 logging_writer.py:48] [56465] accumulated_eval_time=9754.267417, accumulated_logging_time=2.773838, accumulated_submission_time=18025.476143, global_step=56465, preemption_count=0, score=18025.476143, test/accuracy=0.985715, test/loss=0.052170, test/mean_average_precision=0.248602, test/num_examples=43793, total_duration=27784.033194, train/accuracy=0.993728, train/loss=0.019527, train/mean_average_precision=0.645824, validation/accuracy=0.986623, validation/loss=0.049002, validation/mean_average_precision=0.265481, validation/num_examples=43793
I0205 04:55:37.211737 140037886379776 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.15343090891838074, loss=0.024831600487232208
I0205 04:56:09.117223 140030420236032 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.17465052008628845, loss=0.02437620796263218
I0205 04:56:41.196675 140037886379776 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.15849685668945312, loss=0.0245783943682909
I0205 04:57:13.249172 140030420236032 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.16182146966457367, loss=0.023654315620660782
I0205 04:57:45.011309 140037886379776 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.17421497404575348, loss=0.02733842097222805
I0205 04:58:16.949934 140030420236032 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.1762407422065735, loss=0.02610800601541996
I0205 04:58:48.415615 140037886379776 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.14070187509059906, loss=0.021999729797244072
I0205 04:59:20.148870 140030420236032 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.15354357659816742, loss=0.024049701169133186
I0205 04:59:25.532718 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:01:18.906690 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:01:22.028124 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:01:25.145813 140205209478976 submission_runner.py:408] Time since start: 28143.71s, 	Step: 57218, 	{'train/accuracy': 0.994199812412262, 'train/loss': 0.01858672685921192, 'train/mean_average_precision': 0.6527285246331709, 'validation/accuracy': 0.9865426421165466, 'validation/loss': 0.04894685745239258, 'validation/mean_average_precision': 0.2711905727317665, 'validation/num_examples': 43793, 'test/accuracy': 0.9856405854225159, 'test/loss': 0.05209792032837868, 'test/mean_average_precision': 0.25158631821213995, 'test/num_examples': 43793, 'score': 18265.4832341671, 'total_duration': 28143.712094783783, 'accumulated_submission_time': 18265.4832341671, 'accumulated_eval_time': 9873.88046002388, 'accumulated_logging_time': 2.812410593032837}
I0205 05:01:25.175866 140037877987072 logging_writer.py:48] [57218] accumulated_eval_time=9873.880460, accumulated_logging_time=2.812411, accumulated_submission_time=18265.483234, global_step=57218, preemption_count=0, score=18265.483234, test/accuracy=0.985641, test/loss=0.052098, test/mean_average_precision=0.251586, test/num_examples=43793, total_duration=28143.712095, train/accuracy=0.994200, train/loss=0.018587, train/mean_average_precision=0.652729, validation/accuracy=0.986543, validation/loss=0.048947, validation/mean_average_precision=0.271191, validation/num_examples=43793
I0205 05:01:51.805960 140044334864128 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.15314942598342896, loss=0.023253394290804863
I0205 05:02:24.108487 140037877987072 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.16960826516151428, loss=0.02218261919915676
I0205 05:02:55.793618 140044334864128 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.15068650245666504, loss=0.025428077206015587
I0205 05:03:27.889955 140037877987072 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.15765081346035004, loss=0.022802848368883133
I0205 05:03:59.949535 140044334864128 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.18133309483528137, loss=0.02449670061469078
I0205 05:04:32.314140 140037877987072 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.15899115800857544, loss=0.02367617003619671
I0205 05:04:56.897201 140044334864128 logging_writer.py:48] [57878] global_step=57878, preemption_count=0, score=18477.159894
I0205 05:04:56.950819 140205209478976 checkpoints.py:490] Saving checkpoint at step: 57878
I0205 05:04:57.071324 140205209478976 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_1/checkpoint_57878
I0205 05:04:57.072450 140205209478976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_1/checkpoint_57878.
I0205 05:04:57.251782 140205209478976 submission_runner.py:583] Tuning trial 1/5
I0205 05:04:57.252016 140205209478976 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0205 05:04:57.261193 140205209478976 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5092470049858093, 'train/loss': 0.7391904592514038, 'train/mean_average_precision': 0.022183556073118962, 'validation/accuracy': 0.5064646005630493, 'validation/loss': 0.7422076463699341, 'validation/mean_average_precision': 0.025314419280320945, 'validation/num_examples': 43793, 'test/accuracy': 0.5047287344932556, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.026939659112222583, 'test/num_examples': 43793, 'score': 18.134385585784912, 'total_duration': 322.87538480758667, 'accumulated_submission_time': 18.134385585784912, 'accumulated_eval_time': 304.74095821380615, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (747, {'train/accuracy': 0.9867319464683533, 'train/loss': 0.07046488672494888, 'train/mean_average_precision': 0.03332047189447332, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07882650941610336, 'validation/mean_average_precision': 0.03370828574743398, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08166052401065826, 'test/mean_average_precision': 0.036926219938296194, 'test/num_examples': 43793, 'score': 258.18942499160767, 'total_duration': 679.2467248439789, 'accumulated_submission_time': 258.18942499160767, 'accumulated_eval_time': 421.0076684951782, 'accumulated_logging_time': 0.029463768005371094, 'global_step': 747, 'preemption_count': 0}), (1500, {'train/accuracy': 0.9873274564743042, 'train/loss': 0.04891827329993248, 'train/mean_average_precision': 0.08356941409016463, 'validation/accuracy': 0.9845482110977173, 'validation/loss': 0.05748947337269783, 'validation/mean_average_precision': 0.08481615630268999, 'validation/num_examples': 43793, 'test/accuracy': 0.9835611581802368, 'test/loss': 0.060405172407627106, 'test/mean_average_precision': 0.08839088611255327, 'test/num_examples': 43793, 'score': 498.358202457428, 'total_duration': 1041.0658912658691, 'accumulated_submission_time': 498.358202457428, 'accumulated_eval_time': 542.6084208488464, 'accumulated_logging_time': 0.060240745544433594, 'global_step': 1500, 'preemption_count': 0}), (2245, {'train/accuracy': 0.9878538846969604, 'train/loss': 0.04451540485024452, 'train/mean_average_precision': 0.12710063223144522, 'validation/accuracy': 0.9850134253501892, 'validation/loss': 0.053862106055021286, 'validation/mean_average_precision': 0.12863561760452455, 'validation/num_examples': 43793, 'test/accuracy': 0.9840468168258667, 'test/loss': 0.05668896064162254, 'test/mean_average_precision': 0.12697697458880344, 'test/num_examples': 43793, 'score': 738.4302983283997, 'total_duration': 1404.1376762390137, 'accumulated_submission_time': 738.4302983283997, 'accumulated_eval_time': 665.5626842975616, 'accumulated_logging_time': 0.08686161041259766, 'global_step': 2245, 'preemption_count': 0}), (2983, {'train/accuracy': 0.9879559278488159, 'train/loss': 0.04276910424232483, 'train/mean_average_precision': 0.1545525683655089, 'validation/accuracy': 0.9852378964424133, 'validation/loss': 0.052117809653282166, 'validation/mean_average_precision': 0.1480877293086972, 'validation/num_examples': 43793, 'test/accuracy': 0.9842873215675354, 'test/loss': 0.05477682128548622, 'test/mean_average_precision': 0.15463342800933638, 'test/num_examples': 43793, 'score': 978.4800200462341, 'total_duration': 1769.3833138942719, 'accumulated_submission_time': 978.4800200462341, 'accumulated_eval_time': 790.7114028930664, 'accumulated_logging_time': 0.11533927917480469, 'global_step': 2983, 'preemption_count': 0}), (3727, {'train/accuracy': 0.9880806803703308, 'train/loss': 0.04140053316950798, 'train/mean_average_precision': 0.186444753574127, 'validation/accuracy': 0.9853852987289429, 'validation/loss': 0.0503360740840435, 'validation/mean_average_precision': 0.1722589120405069, 'validation/num_examples': 43793, 'test/accuracy': 0.9844751358032227, 'test/loss': 0.053038205951452255, 'test/mean_average_precision': 0.17465184291979313, 'test/num_examples': 43793, 'score': 1218.683366060257, 'total_duration': 2132.798628091812, 'accumulated_submission_time': 1218.683366060257, 'accumulated_eval_time': 913.8746719360352, 'accumulated_logging_time': 0.14537787437438965, 'global_step': 3727, 'preemption_count': 0}), (4481, {'train/accuracy': 0.9885159134864807, 'train/loss': 0.03997578099370003, 'train/mean_average_precision': 0.20383492520000512, 'validation/accuracy': 0.9855237007141113, 'validation/loss': 0.049245577305555344, 'validation/mean_average_precision': 0.1806705486653925, 'validation/num_examples': 43793, 'test/accuracy': 0.9846848845481873, 'test/loss': 0.05176909267902374, 'test/mean_average_precision': 0.18150076981735463, 'test/num_examples': 43793, 'score': 1458.725549697876, 'total_duration': 2498.5742666721344, 'accumulated_submission_time': 1458.725549697876, 'accumulated_eval_time': 1039.5621936321259, 'accumulated_logging_time': 0.17210650444030762, 'global_step': 4481, 'preemption_count': 0}), (5228, {'train/accuracy': 0.9886970520019531, 'train/loss': 0.03851606324315071, 'train/mean_average_precision': 0.22704527736097163, 'validation/accuracy': 0.9857315421104431, 'validation/loss': 0.04844583570957184, 'validation/mean_average_precision': 0.19226982985119906, 'validation/num_examples': 43793, 'test/accuracy': 0.984925389289856, 'test/loss': 0.05113844946026802, 'test/mean_average_precision': 0.19083808361818078, 'test/num_examples': 43793, 'score': 1698.9222049713135, 'total_duration': 2865.626400947571, 'accumulated_submission_time': 1698.9222049713135, 'accumulated_eval_time': 1166.363187789917, 'accumulated_logging_time': 0.20289373397827148, 'global_step': 5228, 'preemption_count': 0}), (5996, {'train/accuracy': 0.9888204336166382, 'train/loss': 0.03822404518723488, 'train/mean_average_precision': 0.23757638562749245, 'validation/accuracy': 0.9858850240707397, 'validation/loss': 0.04780585691332817, 'validation/mean_average_precision': 0.2007561521562516, 'validation/num_examples': 43793, 'test/accuracy': 0.9850820899009705, 'test/loss': 0.05045550689101219, 'test/mean_average_precision': 0.20090724192004614, 'test/num_examples': 43793, 'score': 1939.038080215454, 'total_duration': 3232.058907032013, 'accumulated_submission_time': 1939.038080215454, 'accumulated_eval_time': 1292.633964061737, 'accumulated_logging_time': 0.22948431968688965, 'global_step': 5996, 'preemption_count': 0}), (6757, {'train/accuracy': 0.9890872836112976, 'train/loss': 0.0374721959233284, 'train/mean_average_precision': 0.2541572959008027, 'validation/accuracy': 0.9859747290611267, 'validation/loss': 0.04729709401726723, 'validation/mean_average_precision': 0.20384817768363916, 'validation/num_examples': 43793, 'test/accuracy': 0.9850327968597412, 'test/loss': 0.049796223640441895, 'test/mean_average_precision': 0.20342232567746538, 'test/num_examples': 43793, 'score': 2179.076942920685, 'total_duration': 3593.875339746475, 'accumulated_submission_time': 2179.076942920685, 'accumulated_eval_time': 1414.3643636703491, 'accumulated_logging_time': 0.256913423538208, 'global_step': 6757, 'preemption_count': 0}), (7522, {'train/accuracy': 0.989159345626831, 'train/loss': 0.0369904600083828, 'train/mean_average_precision': 0.2583130676710853, 'validation/accuracy': 0.9860392808914185, 'validation/loss': 0.04717843979597092, 'validation/mean_average_precision': 0.21269932487038165, 'validation/num_examples': 43793, 'test/accuracy': 0.9851840138435364, 'test/loss': 0.04962729662656784, 'test/mean_average_precision': 0.212045926660142, 'test/num_examples': 43793, 'score': 2419.183886051178, 'total_duration': 3959.8080909252167, 'accumulated_submission_time': 2419.183886051178, 'accumulated_eval_time': 1540.1427791118622, 'accumulated_logging_time': 0.2850782871246338, 'global_step': 7522, 'preemption_count': 0}), (8281, {'train/accuracy': 0.9891362190246582, 'train/loss': 0.03711735084652901, 'train/mean_average_precision': 0.2612576523422221, 'validation/accuracy': 0.9862495064735413, 'validation/loss': 0.046459268778562546, 'validation/mean_average_precision': 0.21966237376798012, 'validation/num_examples': 43793, 'test/accuracy': 0.9853053092956543, 'test/loss': 0.04907968267798424, 'test/mean_average_precision': 0.2183831775864873, 'test/num_examples': 43793, 'score': 2659.2870647907257, 'total_duration': 4325.237766027451, 'accumulated_submission_time': 2659.2870647907257, 'accumulated_eval_time': 1665.422149181366, 'accumulated_logging_time': 0.31254100799560547, 'global_step': 8281, 'preemption_count': 0}), (9050, {'train/accuracy': 0.9894993901252747, 'train/loss': 0.03580339998006821, 'train/mean_average_precision': 0.2808998759655631, 'validation/accuracy': 0.9862414002418518, 'validation/loss': 0.04602714627981186, 'validation/mean_average_precision': 0.2250249947677923, 'validation/num_examples': 43793, 'test/accuracy': 0.985432505607605, 'test/loss': 0.04857400059700012, 'test/mean_average_precision': 0.22985825223934117, 'test/num_examples': 43793, 'score': 2899.4726645946503, 'total_duration': 4692.283871173859, 'accumulated_submission_time': 2899.4726645946503, 'accumulated_eval_time': 1792.2342555522919, 'accumulated_logging_time': 0.34105992317199707, 'global_step': 9050, 'preemption_count': 0}), (9810, {'train/accuracy': 0.9892947673797607, 'train/loss': 0.03601916506886482, 'train/mean_average_precision': 0.28290001463691294, 'validation/accuracy': 0.9861679077148438, 'validation/loss': 0.04629398137331009, 'validation/mean_average_precision': 0.2249200090916605, 'validation/num_examples': 43793, 'test/accuracy': 0.9851722121238708, 'test/loss': 0.04909360036253929, 'test/mean_average_precision': 0.2129779767804349, 'test/num_examples': 43793, 'score': 3139.7406141757965, 'total_duration': 5061.331539392471, 'accumulated_submission_time': 3139.7406141757965, 'accumulated_eval_time': 1920.9666464328766, 'accumulated_logging_time': 0.368741512298584, 'global_step': 9810, 'preemption_count': 0}), (10564, {'train/accuracy': 0.9896652698516846, 'train/loss': 0.03486141934990883, 'train/mean_average_precision': 0.2978428145748989, 'validation/accuracy': 0.9864857792854309, 'validation/loss': 0.04574143886566162, 'validation/mean_average_precision': 0.23948837319628052, 'validation/num_examples': 43793, 'test/accuracy': 0.9855567812919617, 'test/loss': 0.048363097012043, 'test/mean_average_precision': 0.24234612427888075, 'test/num_examples': 43793, 'score': 3379.834892272949, 'total_duration': 5436.286778688431, 'accumulated_submission_time': 3379.834892272949, 'accumulated_eval_time': 2055.779053211212, 'accumulated_logging_time': 0.3968634605407715, 'global_step': 10564, 'preemption_count': 0}), (11314, {'train/accuracy': 0.9897768497467041, 'train/loss': 0.03419574350118637, 'train/mean_average_precision': 0.3297334600062838, 'validation/accuracy': 0.9864062070846558, 'validation/loss': 0.0457753986120224, 'validation/mean_average_precision': 0.23745007205009375, 'validation/num_examples': 43793, 'test/accuracy': 0.9855571985244751, 'test/loss': 0.04848429560661316, 'test/mean_average_precision': 0.24124910774038727, 'test/num_examples': 43793, 'score': 3619.9956657886505, 'total_duration': 5807.399445056915, 'accumulated_submission_time': 3619.9956657886505, 'accumulated_eval_time': 2186.6763796806335, 'accumulated_logging_time': 0.4272780418395996, 'global_step': 11314, 'preemption_count': 0}), (12064, {'train/accuracy': 0.9900395274162292, 'train/loss': 0.033200040459632874, 'train/mean_average_precision': 0.35565118295834997, 'validation/accuracy': 0.986559271812439, 'validation/loss': 0.045159950852394104, 'validation/mean_average_precision': 0.24599681831864537, 'validation/num_examples': 43793, 'test/accuracy': 0.9856915473937988, 'test/loss': 0.0479581244289875, 'test/mean_average_precision': 0.2449247904619788, 'test/num_examples': 43793, 'score': 3860.2378079891205, 'total_duration': 6178.707715749741, 'accumulated_submission_time': 3860.2378079891205, 'accumulated_eval_time': 2317.691652059555, 'accumulated_logging_time': 0.45623111724853516, 'global_step': 12064, 'preemption_count': 0}), (12819, {'train/accuracy': 0.9901712536811829, 'train/loss': 0.03270101174712181, 'train/mean_average_precision': 0.3520209612511431, 'validation/accuracy': 0.9864898324012756, 'validation/loss': 0.04514045640826225, 'validation/mean_average_precision': 0.23950796320556614, 'validation/num_examples': 43793, 'test/accuracy': 0.985736608505249, 'test/loss': 0.04779091477394104, 'test/mean_average_precision': 0.2415669797862868, 'test/num_examples': 43793, 'score': 4100.330991983414, 'total_duration': 6548.0606780052185, 'accumulated_submission_time': 4100.330991983414, 'accumulated_eval_time': 2446.9008860588074, 'accumulated_logging_time': 0.4860799312591553, 'global_step': 12819, 'preemption_count': 0}), (13572, {'train/accuracy': 0.9905293583869934, 'train/loss': 0.031600289046764374, 'train/mean_average_precision': 0.368745897355923, 'validation/accuracy': 0.9866258502006531, 'validation/loss': 0.04486170411109924, 'validation/mean_average_precision': 0.2575587049856996, 'validation/num_examples': 43793, 'test/accuracy': 0.9857243895530701, 'test/loss': 0.04759618267416954, 'test/mean_average_precision': 0.25348135402824057, 'test/num_examples': 43793, 'score': 4340.384344100952, 'total_duration': 6914.880608558655, 'accumulated_submission_time': 4340.384344100952, 'accumulated_eval_time': 2573.6196570396423, 'accumulated_logging_time': 0.514338493347168, 'global_step': 13572, 'preemption_count': 0}), (14326, {'train/accuracy': 0.9907206296920776, 'train/loss': 0.031035449355840683, 'train/mean_average_precision': 0.39466925296175914, 'validation/accuracy': 0.986632764339447, 'validation/loss': 0.04433930665254593, 'validation/mean_average_precision': 0.25108633559962107, 'validation/num_examples': 43793, 'test/accuracy': 0.9858802556991577, 'test/loss': 0.046953026205301285, 'test/mean_average_precision': 0.2496725852030368, 'test/num_examples': 43793, 'score': 4580.391752004623, 'total_duration': 7286.849791765213, 'accumulated_submission_time': 4580.391752004623, 'accumulated_eval_time': 2705.533165216446, 'accumulated_logging_time': 0.5428283214569092, 'global_step': 14326, 'preemption_count': 0}), (15080, {'train/accuracy': 0.9904849529266357, 'train/loss': 0.031830884516239166, 'train/mean_average_precision': 0.37670945859015104, 'validation/accuracy': 0.9865093231201172, 'validation/loss': 0.04480632394552231, 'validation/mean_average_precision': 0.2508813830749123, 'validation/num_examples': 43793, 'test/accuracy': 0.9857193827629089, 'test/loss': 0.04746022820472717, 'test/mean_average_precision': 0.24386262425404154, 'test/num_examples': 43793, 'score': 4820.388184309006, 'total_duration': 7657.3278040885925, 'accumulated_submission_time': 4820.388184309006, 'accumulated_eval_time': 2835.9652168750763, 'accumulated_logging_time': 0.5726001262664795, 'global_step': 15080, 'preemption_count': 0}), (15827, {'train/accuracy': 0.9903941750526428, 'train/loss': 0.03175380080938339, 'train/mean_average_precision': 0.38474196639505487, 'validation/accuracy': 0.9866757392883301, 'validation/loss': 0.04488752409815788, 'validation/mean_average_precision': 0.25469984764009845, 'validation/num_examples': 43793, 'test/accuracy': 0.985835611820221, 'test/loss': 0.04772009328007698, 'test/mean_average_precision': 0.24732493344320675, 'test/num_examples': 43793, 'score': 5060.620755910873, 'total_duration': 8026.981867313385, 'accumulated_submission_time': 5060.620755910873, 'accumulated_eval_time': 2965.335473537445, 'accumulated_logging_time': 0.6022679805755615, 'global_step': 15827, 'preemption_count': 0}), (16575, {'train/accuracy': 0.9903529286384583, 'train/loss': 0.0319628044962883, 'train/mean_average_precision': 0.37372795584763974, 'validation/accuracy': 0.9867126941680908, 'validation/loss': 0.044516146183013916, 'validation/mean_average_precision': 0.26091488359638376, 'validation/num_examples': 43793, 'test/accuracy': 0.9857538938522339, 'test/loss': 0.047431498765945435, 'test/mean_average_precision': 0.2495560120245449, 'test/num_examples': 43793, 'score': 5300.623321056366, 'total_duration': 8397.686516284943, 'accumulated_submission_time': 5300.623321056366, 'accumulated_eval_time': 3095.9854731559753, 'accumulated_logging_time': 0.6344373226165771, 'global_step': 16575, 'preemption_count': 0}), (17330, {'train/accuracy': 0.990510880947113, 'train/loss': 0.03144244849681854, 'train/mean_average_precision': 0.3802214268304177, 'validation/accuracy': 0.9867005348205566, 'validation/loss': 0.04471373185515404, 'validation/mean_average_precision': 0.261765784988841, 'validation/num_examples': 43793, 'test/accuracy': 0.9857543110847473, 'test/loss': 0.04779241234064102, 'test/mean_average_precision': 0.2385126380118646, 'test/num_examples': 43793, 'score': 5540.597485303879, 'total_duration': 8769.324578762054, 'accumulated_submission_time': 5540.597485303879, 'accumulated_eval_time': 3227.5997779369354, 'accumulated_logging_time': 0.6642558574676514, 'global_step': 17330, 'preemption_count': 0}), (18077, {'train/accuracy': 0.990651547908783, 'train/loss': 0.030918745324015617, 'train/mean_average_precision': 0.39857086045160495, 'validation/accuracy': 0.9868072867393494, 'validation/loss': 0.04433363303542137, 'validation/mean_average_precision': 0.26610207992878404, 'validation/num_examples': 43793, 'test/accuracy': 0.9859607219696045, 'test/loss': 0.04709821939468384, 'test/mean_average_precision': 0.25139896950538065, 'test/num_examples': 43793, 'score': 5780.7910261154175, 'total_duration': 9143.603893518448, 'accumulated_submission_time': 5780.7910261154175, 'accumulated_eval_time': 3361.631055355072, 'accumulated_logging_time': 0.6942794322967529, 'global_step': 18077, 'preemption_count': 0}), (18829, {'train/accuracy': 0.9905492067337036, 'train/loss': 0.031019046902656555, 'train/mean_average_precision': 0.39520292134503615, 'validation/accuracy': 0.9866875410079956, 'validation/loss': 0.044885165989398956, 'validation/mean_average_precision': 0.2635644689347876, 'validation/num_examples': 43793, 'test/accuracy': 0.9857859015464783, 'test/loss': 0.04802625998854637, 'test/mean_average_precision': 0.24394026731538151, 'test/num_examples': 43793, 'score': 6020.8155081272125, 'total_duration': 9515.525021791458, 'accumulated_submission_time': 6020.8155081272125, 'accumulated_eval_time': 3493.475923061371, 'accumulated_logging_time': 0.7263171672821045, 'global_step': 18829, 'preemption_count': 0}), (19584, {'train/accuracy': 0.9906980395317078, 'train/loss': 0.03056776523590088, 'train/mean_average_precision': 0.4242602740789738, 'validation/accuracy': 0.9866716861724854, 'validation/loss': 0.04484669491648674, 'validation/mean_average_precision': 0.2587407145618029, 'validation/num_examples': 43793, 'test/accuracy': 0.9859375357627869, 'test/loss': 0.0473642535507679, 'test/mean_average_precision': 0.25391582333620416, 'test/num_examples': 43793, 'score': 6260.985202074051, 'total_duration': 9884.426526784897, 'accumulated_submission_time': 6260.985202074051, 'accumulated_eval_time': 3622.1567487716675, 'accumulated_logging_time': 0.7575254440307617, 'global_step': 19584, 'preemption_count': 0}), (20334, {'train/accuracy': 0.9910008907318115, 'train/loss': 0.029481450095772743, 'train/mean_average_precision': 0.4198984412224145, 'validation/accuracy': 0.9867601990699768, 'validation/loss': 0.04445962235331535, 'validation/mean_average_precision': 0.2566778579806512, 'validation/num_examples': 43793, 'test/accuracy': 0.9859299659729004, 'test/loss': 0.04732656851410866, 'test/mean_average_precision': 0.2548134925721332, 'test/num_examples': 43793, 'score': 6501.081083774567, 'total_duration': 10252.942810297012, 'accumulated_submission_time': 6501.081083774567, 'accumulated_eval_time': 3750.5276980400085, 'accumulated_logging_time': 0.7873868942260742, 'global_step': 20334, 'preemption_count': 0}), (21083, {'train/accuracy': 0.9911450147628784, 'train/loss': 0.028955401852726936, 'train/mean_average_precision': 0.438380159959826, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.04458526894450188, 'validation/mean_average_precision': 0.25754632620481266, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.04733564704656601, 'test/mean_average_precision': 0.25246322236122937, 'test/num_examples': 43793, 'score': 6741.23437333107, 'total_duration': 10619.10322880745, 'accumulated_submission_time': 6741.23437333107, 'accumulated_eval_time': 3876.483735561371, 'accumulated_logging_time': 0.8186118602752686, 'global_step': 21083, 'preemption_count': 0}), (21830, {'train/accuracy': 0.9911766052246094, 'train/loss': 0.02919008769094944, 'train/mean_average_precision': 0.4514105935616376, 'validation/accuracy': 0.9867467880249023, 'validation/loss': 0.04480132460594177, 'validation/mean_average_precision': 0.26495023211404944, 'validation/num_examples': 43793, 'test/accuracy': 0.9859269857406616, 'test/loss': 0.04775208607316017, 'test/mean_average_precision': 0.2534212336427425, 'test/num_examples': 43793, 'score': 6981.344316244125, 'total_duration': 10984.614884376526, 'accumulated_submission_time': 6981.344316244125, 'accumulated_eval_time': 4001.835470676422, 'accumulated_logging_time': 0.8482942581176758, 'global_step': 21830, 'preemption_count': 0}), (22585, {'train/accuracy': 0.9911478757858276, 'train/loss': 0.029226714745163918, 'train/mean_average_precision': 0.422738707506834, 'validation/accuracy': 0.9867796897888184, 'validation/loss': 0.04455733671784401, 'validation/mean_average_precision': 0.2584770706554278, 'validation/num_examples': 43793, 'test/accuracy': 0.9858440160751343, 'test/loss': 0.04732321575284004, 'test/mean_average_precision': 0.246868166257547, 'test/num_examples': 43793, 'score': 7221.317653656006, 'total_duration': 11352.638570547104, 'accumulated_submission_time': 7221.317653656006, 'accumulated_eval_time': 4129.834286689758, 'accumulated_logging_time': 0.8798651695251465, 'global_step': 22585, 'preemption_count': 0}), (23334, {'train/accuracy': 0.9909042119979858, 'train/loss': 0.03012176603078842, 'train/mean_average_precision': 0.4214129838863844, 'validation/accuracy': 0.9866956472396851, 'validation/loss': 0.04435041919350624, 'validation/mean_average_precision': 0.26667981224204323, 'validation/num_examples': 43793, 'test/accuracy': 0.9857808351516724, 'test/loss': 0.047195274382829666, 'test/mean_average_precision': 0.24963429103863122, 'test/num_examples': 43793, 'score': 7461.457053661346, 'total_duration': 11724.822311639786, 'accumulated_submission_time': 7461.457053661346, 'accumulated_eval_time': 4261.829008817673, 'accumulated_logging_time': 0.9099385738372803, 'global_step': 23334, 'preemption_count': 0}), (24086, {'train/accuracy': 0.9909349679946899, 'train/loss': 0.02982982248067856, 'train/mean_average_precision': 0.4233810275004899, 'validation/accuracy': 0.9868283867835999, 'validation/loss': 0.044438932090997696, 'validation/mean_average_precision': 0.27103600700796654, 'validation/num_examples': 43793, 'test/accuracy': 0.9860095381736755, 'test/loss': 0.04692890867590904, 'test/mean_average_precision': 0.2543803166805494, 'test/num_examples': 43793, 'score': 7701.260676383972, 'total_duration': 12093.718660354614, 'accumulated_submission_time': 7701.260676383972, 'accumulated_eval_time': 4390.558969974518, 'accumulated_logging_time': 1.252744436264038, 'global_step': 24086, 'preemption_count': 0}), (24831, {'train/accuracy': 0.9908654093742371, 'train/loss': 0.029955491423606873, 'train/mean_average_precision': 0.41462480755626085, 'validation/accuracy': 0.9866400361061096, 'validation/loss': 0.044650282710790634, 'validation/mean_average_precision': 0.2619743777067673, 'validation/num_examples': 43793, 'test/accuracy': 0.9857420921325684, 'test/loss': 0.047577787190675735, 'test/mean_average_precision': 0.24849465698805137, 'test/num_examples': 43793, 'score': 7941.512230634689, 'total_duration': 12461.927044153214, 'accumulated_submission_time': 7941.512230634689, 'accumulated_eval_time': 4518.463991880417, 'accumulated_logging_time': 1.2852094173431396, 'global_step': 24831, 'preemption_count': 0}), (25587, {'train/accuracy': 0.9910185933113098, 'train/loss': 0.029385587200522423, 'train/mean_average_precision': 0.43237196485122104, 'validation/accuracy': 0.9868754744529724, 'validation/loss': 0.044676605612039566, 'validation/mean_average_precision': 0.2670693539503197, 'validation/num_examples': 43793, 'test/accuracy': 0.985908031463623, 'test/loss': 0.047651320695877075, 'test/mean_average_precision': 0.25267636504550534, 'test/num_examples': 43793, 'score': 8181.642039299011, 'total_duration': 12827.246604681015, 'accumulated_submission_time': 8181.642039299011, 'accumulated_eval_time': 4643.603357791901, 'accumulated_logging_time': 1.3162312507629395, 'global_step': 25587, 'preemption_count': 0}), (26330, {'train/accuracy': 0.991351306438446, 'train/loss': 0.02848195470869541, 'train/mean_average_precision': 0.45723114206103377, 'validation/accuracy': 0.9867662787437439, 'validation/loss': 0.04438454657793045, 'validation/mean_average_precision': 0.27187496015310164, 'validation/num_examples': 43793, 'test/accuracy': 0.9859522581100464, 'test/loss': 0.047049105167388916, 'test/mean_average_precision': 0.2559921718547284, 'test/num_examples': 43793, 'score': 8421.819328069687, 'total_duration': 13196.254113912582, 'accumulated_submission_time': 8421.819328069687, 'accumulated_eval_time': 4772.377794981003, 'accumulated_logging_time': 1.3500714302062988, 'global_step': 26330, 'preemption_count': 0}), (27090, {'train/accuracy': 0.9912184476852417, 'train/loss': 0.028624698519706726, 'train/mean_average_precision': 0.4483122074480636, 'validation/accuracy': 0.9867537021636963, 'validation/loss': 0.044528041034936905, 'validation/mean_average_precision': 0.2673226801916273, 'validation/num_examples': 43793, 'test/accuracy': 0.985910177230835, 'test/loss': 0.04741410166025162, 'test/mean_average_precision': 0.2518513987897968, 'test/num_examples': 43793, 'score': 8661.81754231453, 'total_duration': 13566.056858778, 'accumulated_submission_time': 8661.81754231453, 'accumulated_eval_time': 4902.130871295929, 'accumulated_logging_time': 1.381518840789795, 'global_step': 27090, 'preemption_count': 0}), (27850, {'train/accuracy': 0.9915094375610352, 'train/loss': 0.027809351682662964, 'train/mean_average_precision': 0.4788816882412824, 'validation/accuracy': 0.9867618083953857, 'validation/loss': 0.04470362514257431, 'validation/mean_average_precision': 0.26950986838141033, 'validation/num_examples': 43793, 'test/accuracy': 0.9858503341674805, 'test/loss': 0.04746343940496445, 'test/mean_average_precision': 0.25180781502242594, 'test/num_examples': 43793, 'score': 8902.068484067917, 'total_duration': 13932.1010928154, 'accumulated_submission_time': 8902.068484067917, 'accumulated_eval_time': 5027.87125992775, 'accumulated_logging_time': 1.4148139953613281, 'global_step': 27850, 'preemption_count': 0}), (28604, {'train/accuracy': 0.9915740489959717, 'train/loss': 0.02738937921822071, 'train/mean_average_precision': 0.47740207335549867, 'validation/accuracy': 0.9868312478065491, 'validation/loss': 0.04473751038312912, 'validation/mean_average_precision': 0.263646713481197, 'validation/num_examples': 43793, 'test/accuracy': 0.9858406782150269, 'test/loss': 0.04765476658940315, 'test/mean_average_precision': 0.25276134230487596, 'test/num_examples': 43793, 'score': 9142.265007972717, 'total_duration': 14299.589767217636, 'accumulated_submission_time': 9142.265007972717, 'accumulated_eval_time': 5155.106744766235, 'accumulated_logging_time': 1.451047420501709, 'global_step': 28604, 'preemption_count': 0}), (29360, {'train/accuracy': 0.9915950894355774, 'train/loss': 0.027292786166071892, 'train/mean_average_precision': 0.48154575379357334, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.044787004590034485, 'validation/mean_average_precision': 0.26283345457999824, 'validation/num_examples': 43793, 'test/accuracy': 0.9858920574188232, 'test/loss': 0.0477716363966465, 'test/mean_average_precision': 0.24770145840423297, 'test/num_examples': 43793, 'score': 9382.218895435333, 'total_duration': 14661.340048074722, 'accumulated_submission_time': 9382.218895435333, 'accumulated_eval_time': 5276.8527200222015, 'accumulated_logging_time': 1.4818134307861328, 'global_step': 29360, 'preemption_count': 0}), (30109, {'train/accuracy': 0.9915949702262878, 'train/loss': 0.02762727253139019, 'train/mean_average_precision': 0.4659208937616314, 'validation/accuracy': 0.9868267774581909, 'validation/loss': 0.04461052641272545, 'validation/mean_average_precision': 0.26542109410932735, 'validation/num_examples': 43793, 'test/accuracy': 0.9860133528709412, 'test/loss': 0.047491997480392456, 'test/mean_average_precision': 0.25493339906777207, 'test/num_examples': 43793, 'score': 9622.306058883667, 'total_duration': 15026.526914358139, 'accumulated_submission_time': 9622.306058883667, 'accumulated_eval_time': 5401.89927482605, 'accumulated_logging_time': 1.514939308166504, 'global_step': 30109, 'preemption_count': 0}), (30861, {'train/accuracy': 0.9914368391036987, 'train/loss': 0.02820701152086258, 'train/mean_average_precision': 0.4585877945100001, 'validation/accuracy': 0.9867272973060608, 'validation/loss': 0.0447770431637764, 'validation/mean_average_precision': 0.26481675342029126, 'validation/num_examples': 43793, 'test/accuracy': 0.9858651161193848, 'test/loss': 0.04745037481188774, 'test/mean_average_precision': 0.2607415900214805, 'test/num_examples': 43793, 'score': 9862.304508924484, 'total_duration': 15393.499891757965, 'accumulated_submission_time': 9862.304508924484, 'accumulated_eval_time': 5528.822056770325, 'accumulated_logging_time': 1.5469331741333008, 'global_step': 30861, 'preemption_count': 0}), (31613, {'train/accuracy': 0.9915117025375366, 'train/loss': 0.027849651873111725, 'train/mean_average_precision': 0.47196756425559555, 'validation/accuracy': 0.9867539405822754, 'validation/loss': 0.04462186247110367, 'validation/mean_average_precision': 0.26264033029363215, 'validation/num_examples': 43793, 'test/accuracy': 0.9859716296195984, 'test/loss': 0.047234199941158295, 'test/mean_average_precision': 0.25093760276090715, 'test/num_examples': 43793, 'score': 10102.278964281082, 'total_duration': 15757.501960992813, 'accumulated_submission_time': 10102.278964281082, 'accumulated_eval_time': 5652.796845912933, 'accumulated_logging_time': 1.580089807510376, 'global_step': 31613, 'preemption_count': 0}), (32375, {'train/accuracy': 0.9914616346359253, 'train/loss': 0.027796823531389236, 'train/mean_average_precision': 0.4661418373102133, 'validation/accuracy': 0.9868665337562561, 'validation/loss': 0.04482589662075043, 'validation/mean_average_precision': 0.26490325607114407, 'validation/num_examples': 43793, 'test/accuracy': 0.9860483407974243, 'test/loss': 0.047689080238342285, 'test/mean_average_precision': 0.2538761775615324, 'test/num_examples': 43793, 'score': 10342.485783815384, 'total_duration': 16124.741267204285, 'accumulated_submission_time': 10342.485783815384, 'accumulated_eval_time': 5779.776381969452, 'accumulated_logging_time': 1.6129043102264404, 'global_step': 32375, 'preemption_count': 0}), (33127, {'train/accuracy': 0.9914880990982056, 'train/loss': 0.02771720103919506, 'train/mean_average_precision': 0.47612648725667905, 'validation/accuracy': 0.9866778254508972, 'validation/loss': 0.04471953585743904, 'validation/mean_average_precision': 0.2619186380504609, 'validation/num_examples': 43793, 'test/accuracy': 0.9858246445655823, 'test/loss': 0.04736829549074173, 'test/mean_average_precision': 0.25734370976292115, 'test/num_examples': 43793, 'score': 10582.662477493286, 'total_duration': 16491.039191007614, 'accumulated_submission_time': 10582.662477493286, 'accumulated_eval_time': 5905.846033334732, 'accumulated_logging_time': 1.644451379776001, 'global_step': 33127, 'preemption_count': 0}), (33880, {'train/accuracy': 0.9916728138923645, 'train/loss': 0.027090424671769142, 'train/mean_average_precision': 0.4776111416574321, 'validation/accuracy': 0.9869416356086731, 'validation/loss': 0.044754911214113235, 'validation/mean_average_precision': 0.27371316807948076, 'validation/num_examples': 43793, 'test/accuracy': 0.9860420227050781, 'test/loss': 0.04735885560512543, 'test/mean_average_precision': 0.2607110022505314, 'test/num_examples': 43793, 'score': 10822.833097219467, 'total_duration': 16856.96304345131, 'accumulated_submission_time': 10822.833097219467, 'accumulated_eval_time': 6031.545439004898, 'accumulated_logging_time': 1.6781425476074219, 'global_step': 33880, 'preemption_count': 0}), (34630, {'train/accuracy': 0.9918762445449829, 'train/loss': 0.026501566171646118, 'train/mean_average_precision': 0.49593268746876096, 'validation/accuracy': 0.9868482947349548, 'validation/loss': 0.04476573318243027, 'validation/mean_average_precision': 0.2684190313777011, 'validation/num_examples': 43793, 'test/accuracy': 0.9858322143554688, 'test/loss': 0.047500334680080414, 'test/mean_average_precision': 0.25602074410434805, 'test/num_examples': 43793, 'score': 11062.853231668472, 'total_duration': 17223.528245449066, 'accumulated_submission_time': 11062.853231668472, 'accumulated_eval_time': 6158.037148237228, 'accumulated_logging_time': 1.71140456199646, 'global_step': 34630, 'preemption_count': 0}), (35374, {'train/accuracy': 0.9919270873069763, 'train/loss': 0.026157235726714134, 'train/mean_average_precision': 0.5077010842706066, 'validation/accuracy': 0.9868158102035522, 'validation/loss': 0.04501223936676979, 'validation/mean_average_precision': 0.2640386329595336, 'validation/num_examples': 43793, 'test/accuracy': 0.9859994649887085, 'test/loss': 0.04774508252739906, 'test/mean_average_precision': 0.2512933375163518, 'test/num_examples': 43793, 'score': 11302.85993719101, 'total_duration': 17591.77684688568, 'accumulated_submission_time': 11302.85993719101, 'accumulated_eval_time': 6286.22608089447, 'accumulated_logging_time': 1.7442903518676758, 'global_step': 35374, 'preemption_count': 0}), (36121, {'train/accuracy': 0.9923211336135864, 'train/loss': 0.024961713701486588, 'train/mean_average_precision': 0.5338029568777007, 'validation/accuracy': 0.9868872761726379, 'validation/loss': 0.04489753395318985, 'validation/mean_average_precision': 0.2611574561332976, 'validation/num_examples': 43793, 'test/accuracy': 0.9859430193901062, 'test/loss': 0.04782985523343086, 'test/mean_average_precision': 0.2534297712322391, 'test/num_examples': 43793, 'score': 11542.92440032959, 'total_duration': 17959.193194389343, 'accumulated_submission_time': 11542.92440032959, 'accumulated_eval_time': 6413.524546384811, 'accumulated_logging_time': 1.7778651714324951, 'global_step': 36121, 'preemption_count': 0}), (36868, {'train/accuracy': 0.9920997619628906, 'train/loss': 0.025457335636019707, 'train/mean_average_precision': 0.5130001361147138, 'validation/accuracy': 0.9868791699409485, 'validation/loss': 0.04519825428724289, 'validation/mean_average_precision': 0.26327072933660084, 'validation/num_examples': 43793, 'test/accuracy': 0.986023485660553, 'test/loss': 0.04804118722677231, 'test/mean_average_precision': 0.25629805749865264, 'test/num_examples': 43793, 'score': 11782.954986095428, 'total_duration': 18330.567383527756, 'accumulated_submission_time': 11782.954986095428, 'accumulated_eval_time': 6544.811107635498, 'accumulated_logging_time': 1.8135063648223877, 'global_step': 36868, 'preemption_count': 0}), (37621, {'train/accuracy': 0.991813063621521, 'train/loss': 0.026473114266991615, 'train/mean_average_precision': 0.5066786252637335, 'validation/accuracy': 0.9869157075881958, 'validation/loss': 0.04498588666319847, 'validation/mean_average_precision': 0.27089276405841906, 'validation/num_examples': 43793, 'test/accuracy': 0.9859792590141296, 'test/loss': 0.047808218747377396, 'test/mean_average_precision': 0.25641973641373816, 'test/num_examples': 43793, 'score': 12023.092227220535, 'total_duration': 18696.423310041428, 'accumulated_submission_time': 12023.092227220535, 'accumulated_eval_time': 6670.475694179535, 'accumulated_logging_time': 1.847543478012085, 'global_step': 37621, 'preemption_count': 0}), (38381, {'train/accuracy': 0.9919701814651489, 'train/loss': 0.0260833278298378, 'train/mean_average_precision': 0.49382275664101644, 'validation/accuracy': 0.9869075417518616, 'validation/loss': 0.044809501618146896, 'validation/mean_average_precision': 0.2723583684879261, 'validation/num_examples': 43793, 'test/accuracy': 0.985987663269043, 'test/loss': 0.04771247133612633, 'test/mean_average_precision': 0.2526040420471432, 'test/num_examples': 43793, 'score': 12263.215424776077, 'total_duration': 19061.941546201706, 'accumulated_submission_time': 12263.215424776077, 'accumulated_eval_time': 6795.817242145538, 'accumulated_logging_time': 1.8814423084259033, 'global_step': 38381, 'preemption_count': 0}), (39140, {'train/accuracy': 0.9919205904006958, 'train/loss': 0.026281286031007767, 'train/mean_average_precision': 0.5019598940760397, 'validation/accuracy': 0.9867547154426575, 'validation/loss': 0.0452037937939167, 'validation/mean_average_precision': 0.26647209985618087, 'validation/num_examples': 43793, 'test/accuracy': 0.9859459400177002, 'test/loss': 0.0479544922709465, 'test/mean_average_precision': 0.25299035868724007, 'test/num_examples': 43793, 'score': 12503.383272647858, 'total_duration': 19424.19128537178, 'accumulated_submission_time': 12503.383272647858, 'accumulated_eval_time': 6917.844294548035, 'accumulated_logging_time': 1.9166576862335205, 'global_step': 39140, 'preemption_count': 0}), (39896, {'train/accuracy': 0.9918465614318848, 'train/loss': 0.026147423312067986, 'train/mean_average_precision': 0.5186325621587466, 'validation/accuracy': 0.9868633151054382, 'validation/loss': 0.0452602319419384, 'validation/mean_average_precision': 0.26759407914953914, 'validation/num_examples': 43793, 'test/accuracy': 0.9860289096832275, 'test/loss': 0.04782252758741379, 'test/mean_average_precision': 0.2570222310540754, 'test/num_examples': 43793, 'score': 12743.645746707916, 'total_duration': 19790.621960878372, 'accumulated_submission_time': 12743.645746707916, 'accumulated_eval_time': 7043.957558870316, 'accumulated_logging_time': 1.9517738819122314, 'global_step': 39896, 'preemption_count': 0}), (40660, {'train/accuracy': 0.9921106100082397, 'train/loss': 0.025712454691529274, 'train/mean_average_precision': 0.50393287827044, 'validation/accuracy': 0.9866834878921509, 'validation/loss': 0.04541837424039841, 'validation/mean_average_precision': 0.2655199069094247, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.04785643890500069, 'test/mean_average_precision': 0.25835325361566364, 'test/num_examples': 43793, 'score': 12983.84726190567, 'total_duration': 20151.313011407852, 'accumulated_submission_time': 12983.84726190567, 'accumulated_eval_time': 7164.391726016998, 'accumulated_logging_time': 1.987480878829956, 'global_step': 40660, 'preemption_count': 0}), (41411, {'train/accuracy': 0.9921404719352722, 'train/loss': 0.02532167360186577, 'train/mean_average_precision': 0.5251935954997733, 'validation/accuracy': 0.9867870211601257, 'validation/loss': 0.045663002878427505, 'validation/mean_average_precision': 0.265891050572245, 'validation/num_examples': 43793, 'test/accuracy': 0.9858701229095459, 'test/loss': 0.04856467247009277, 'test/mean_average_precision': 0.25416953442428014, 'test/num_examples': 43793, 'score': 13223.94919514656, 'total_duration': 20516.02917122841, 'accumulated_submission_time': 13223.94919514656, 'accumulated_eval_time': 7288.951065063477, 'accumulated_logging_time': 2.023068428039551, 'global_step': 41411, 'preemption_count': 0}), (42171, {'train/accuracy': 0.992421567440033, 'train/loss': 0.02443872205913067, 'train/mean_average_precision': 0.5435246945200095, 'validation/accuracy': 0.9867402911186218, 'validation/loss': 0.045766741037368774, 'validation/mean_average_precision': 0.2670472319105248, 'validation/num_examples': 43793, 'test/accuracy': 0.9859126806259155, 'test/loss': 0.048624247312545776, 'test/mean_average_precision': 0.25526057373272015, 'test/num_examples': 43793, 'score': 13464.0667886734, 'total_duration': 20884.01805138588, 'accumulated_submission_time': 13464.0667886734, 'accumulated_eval_time': 7416.767649650574, 'accumulated_logging_time': 2.0580596923828125, 'global_step': 42171, 'preemption_count': 0}), (42920, {'train/accuracy': 0.9927170872688293, 'train/loss': 0.0233779214322567, 'train/mean_average_precision': 0.5733930541995389, 'validation/accuracy': 0.9868364930152893, 'validation/loss': 0.04593338817358017, 'validation/mean_average_precision': 0.2704952414247853, 'validation/num_examples': 43793, 'test/accuracy': 0.9860095381736755, 'test/loss': 0.048736944794654846, 'test/mean_average_precision': 0.25686386270281447, 'test/num_examples': 43793, 'score': 13704.141545772552, 'total_duration': 21253.58636689186, 'accumulated_submission_time': 13704.141545772552, 'accumulated_eval_time': 7546.203330516815, 'accumulated_logging_time': 2.094203472137451, 'global_step': 42920, 'preemption_count': 0}), (43675, {'train/accuracy': 0.9927061200141907, 'train/loss': 0.02339312806725502, 'train/mean_average_precision': 0.5666941320422774, 'validation/accuracy': 0.9868308305740356, 'validation/loss': 0.045705344527959824, 'validation/mean_average_precision': 0.26667728140623015, 'validation/num_examples': 43793, 'test/accuracy': 0.9858528971672058, 'test/loss': 0.04876760020852089, 'test/mean_average_precision': 0.25349718911025815, 'test/num_examples': 43793, 'score': 13944.12907576561, 'total_duration': 21617.289580583572, 'accumulated_submission_time': 13944.12907576561, 'accumulated_eval_time': 7669.8636927604675, 'accumulated_logging_time': 2.1300570964813232, 'global_step': 43675, 'preemption_count': 0}), (44425, {'train/accuracy': 0.9929761290550232, 'train/loss': 0.02256980538368225, 'train/mean_average_precision': 0.5881898919000075, 'validation/accuracy': 0.9867346286773682, 'validation/loss': 0.046440139412879944, 'validation/mean_average_precision': 0.2643463754657173, 'validation/num_examples': 43793, 'test/accuracy': 0.9857774972915649, 'test/loss': 0.04942133650183678, 'test/mean_average_precision': 0.25029550253572547, 'test/num_examples': 43793, 'score': 14184.10730624199, 'total_duration': 21981.214041233063, 'accumulated_submission_time': 14184.10730624199, 'accumulated_eval_time': 7793.7537133693695, 'accumulated_logging_time': 2.167064905166626, 'global_step': 44425, 'preemption_count': 0}), (45172, {'train/accuracy': 0.9928836226463318, 'train/loss': 0.023163873702287674, 'train/mean_average_precision': 0.5603069323792206, 'validation/accuracy': 0.9867216348648071, 'validation/loss': 0.045841023325920105, 'validation/mean_average_precision': 0.2698805055719386, 'validation/num_examples': 43793, 'test/accuracy': 0.9857913851737976, 'test/loss': 0.0486513152718544, 'test/mean_average_precision': 0.2532316248694131, 'test/num_examples': 43793, 'score': 14424.139218568802, 'total_duration': 22347.200195789337, 'accumulated_submission_time': 14424.139218568802, 'accumulated_eval_time': 7919.652441978455, 'accumulated_logging_time': 2.2020323276519775, 'global_step': 45172, 'preemption_count': 0}), (45928, {'train/accuracy': 0.9925437569618225, 'train/loss': 0.0238797627389431, 'train/mean_average_precision': 0.5548479854623927, 'validation/accuracy': 0.9868665337562561, 'validation/loss': 0.046299174427986145, 'validation/mean_average_precision': 0.2675998202951851, 'validation/num_examples': 43793, 'test/accuracy': 0.9859084486961365, 'test/loss': 0.04929661750793457, 'test/mean_average_precision': 0.2490588585001329, 'test/num_examples': 43793, 'score': 14664.235578775406, 'total_duration': 22710.05312728882, 'accumulated_submission_time': 14664.235578775406, 'accumulated_eval_time': 8042.348666667938, 'accumulated_logging_time': 2.241867780685425, 'global_step': 45928, 'preemption_count': 0}), (46685, {'train/accuracy': 0.9926679134368896, 'train/loss': 0.02372351661324501, 'train/mean_average_precision': 0.5522737730499209, 'validation/accuracy': 0.986710250377655, 'validation/loss': 0.04607642814517021, 'validation/mean_average_precision': 0.26736929985496544, 'validation/num_examples': 43793, 'test/accuracy': 0.985787570476532, 'test/loss': 0.04892697185277939, 'test/mean_average_precision': 0.255223206791752, 'test/num_examples': 43793, 'score': 14904.341437578201, 'total_duration': 23071.91637802124, 'accumulated_submission_time': 14904.341437578201, 'accumulated_eval_time': 8164.049585580826, 'accumulated_logging_time': 2.278223991394043, 'global_step': 46685, 'preemption_count': 0}), (47437, {'train/accuracy': 0.9925719499588013, 'train/loss': 0.023791460320353508, 'train/mean_average_precision': 0.5576811836200534, 'validation/accuracy': 0.9868466854095459, 'validation/loss': 0.04642287641763687, 'validation/mean_average_precision': 0.2657642561931333, 'validation/num_examples': 43793, 'test/accuracy': 0.9859118461608887, 'test/loss': 0.049479302018880844, 'test/mean_average_precision': 0.25251685623100334, 'test/num_examples': 43793, 'score': 15144.60926938057, 'total_duration': 23438.763329267502, 'accumulated_submission_time': 15144.60926938057, 'accumulated_eval_time': 8290.571984052658, 'accumulated_logging_time': 2.3142735958099365, 'global_step': 47437, 'preemption_count': 0}), (48189, {'train/accuracy': 0.9928362369537354, 'train/loss': 0.023019516840577126, 'train/mean_average_precision': 0.558678018020372, 'validation/accuracy': 0.9867460131645203, 'validation/loss': 0.04643058776855469, 'validation/mean_average_precision': 0.27327413772628895, 'validation/num_examples': 43793, 'test/accuracy': 0.9858225584030151, 'test/loss': 0.04929651692509651, 'test/mean_average_precision': 0.25214949307651735, 'test/num_examples': 43793, 'score': 15384.76617383957, 'total_duration': 23803.415416240692, 'accumulated_submission_time': 15384.76617383957, 'accumulated_eval_time': 8415.009822368622, 'accumulated_logging_time': 2.3507189750671387, 'global_step': 48189, 'preemption_count': 0}), (48939, {'train/accuracy': 0.9928126335144043, 'train/loss': 0.022829918190836906, 'train/mean_average_precision': 0.5758678220084759, 'validation/accuracy': 0.9868665337562561, 'validation/loss': 0.046328239142894745, 'validation/mean_average_precision': 0.2763794516470148, 'validation/num_examples': 43793, 'test/accuracy': 0.9858726859092712, 'test/loss': 0.049397435039281845, 'test/mean_average_precision': 0.2561368281989875, 'test/num_examples': 43793, 'score': 15624.714596033096, 'total_duration': 24165.579883813858, 'accumulated_submission_time': 15624.714596033096, 'accumulated_eval_time': 8537.169777154922, 'accumulated_logging_time': 2.38702654838562, 'global_step': 48939, 'preemption_count': 0}), (49683, {'train/accuracy': 0.9930967688560486, 'train/loss': 0.021930977702140808, 'train/mean_average_precision': 0.5948772431380066, 'validation/accuracy': 0.986885666847229, 'validation/loss': 0.0467839241027832, 'validation/mean_average_precision': 0.2694161972938923, 'validation/num_examples': 43793, 'test/accuracy': 0.9858827590942383, 'test/loss': 0.049932315945625305, 'test/mean_average_precision': 0.25306111751823346, 'test/num_examples': 43793, 'score': 15864.714323282242, 'total_duration': 24526.31983613968, 'accumulated_submission_time': 15864.714323282242, 'accumulated_eval_time': 8657.844844341278, 'accumulated_logging_time': 2.4283759593963623, 'global_step': 49683, 'preemption_count': 0}), (50443, {'train/accuracy': 0.9935494065284729, 'train/loss': 0.020813167095184326, 'train/mean_average_precision': 0.6246684589240922, 'validation/accuracy': 0.9867971539497375, 'validation/loss': 0.04655579850077629, 'validation/mean_average_precision': 0.27577702724714537, 'validation/num_examples': 43793, 'test/accuracy': 0.9857075810432434, 'test/loss': 0.04983345419168472, 'test/mean_average_precision': 0.25529045165687886, 'test/num_examples': 43793, 'score': 16104.945301055908, 'total_duration': 24887.052001953125, 'accumulated_submission_time': 16104.945301055908, 'accumulated_eval_time': 8778.289328336716, 'accumulated_logging_time': 2.4652960300445557, 'global_step': 50443, 'preemption_count': 0}), (51201, {'train/accuracy': 0.9938346743583679, 'train/loss': 0.019873609766364098, 'train/mean_average_precision': 0.6402711633729733, 'validation/accuracy': 0.9867033958435059, 'validation/loss': 0.04749038815498352, 'validation/mean_average_precision': 0.26534099133944655, 'validation/num_examples': 43793, 'test/accuracy': 0.9858195781707764, 'test/loss': 0.05073053389787674, 'test/mean_average_precision': 0.24764507307342512, 'test/num_examples': 43793, 'score': 16345.135685920715, 'total_duration': 25249.40115904808, 'accumulated_submission_time': 16345.135685920715, 'accumulated_eval_time': 8900.391793251038, 'accumulated_logging_time': 2.501399278640747, 'global_step': 51201, 'preemption_count': 0}), (51945, {'train/accuracy': 0.9937007427215576, 'train/loss': 0.020169697701931, 'train/mean_average_precision': 0.6355450158745422, 'validation/accuracy': 0.9866883754730225, 'validation/loss': 0.04740728810429573, 'validation/mean_average_precision': 0.26725297012395566, 'validation/num_examples': 43793, 'test/accuracy': 0.9858056902885437, 'test/loss': 0.0504932701587677, 'test/mean_average_precision': 0.25328637582410407, 'test/num_examples': 43793, 'score': 16585.19568347931, 'total_duration': 25610.565322637558, 'accumulated_submission_time': 16585.19568347931, 'accumulated_eval_time': 9021.43328166008, 'accumulated_logging_time': 2.5409858226776123, 'global_step': 51945, 'preemption_count': 0}), (52698, {'train/accuracy': 0.9938126802444458, 'train/loss': 0.01998131349682808, 'train/mean_average_precision': 0.6342326759145145, 'validation/accuracy': 0.9867126941680908, 'validation/loss': 0.047159794718027115, 'validation/mean_average_precision': 0.2665871337103819, 'validation/num_examples': 43793, 'test/accuracy': 0.9858128428459167, 'test/loss': 0.05055546015501022, 'test/mean_average_precision': 0.24676034265588734, 'test/num_examples': 43793, 'score': 16825.145767211914, 'total_duration': 25969.932988643646, 'accumulated_submission_time': 16825.145767211914, 'accumulated_eval_time': 9140.794448375702, 'accumulated_logging_time': 2.5773093700408936, 'global_step': 52698, 'preemption_count': 0}), (53458, {'train/accuracy': 0.993604838848114, 'train/loss': 0.02053910121321678, 'train/mean_average_precision': 0.6163999862960505, 'validation/accuracy': 0.9866956472396851, 'validation/loss': 0.04780729115009308, 'validation/mean_average_precision': 0.26729791289631466, 'validation/num_examples': 43793, 'test/accuracy': 0.9857555627822876, 'test/loss': 0.050805043429136276, 'test/mean_average_precision': 0.25253103186839687, 'test/num_examples': 43793, 'score': 17065.106918811798, 'total_duration': 26333.523721456528, 'accumulated_submission_time': 17065.106918811798, 'accumulated_eval_time': 9264.368415594101, 'accumulated_logging_time': 2.6135025024414062, 'global_step': 53458, 'preemption_count': 0}), (54198, {'train/accuracy': 0.993502676486969, 'train/loss': 0.02074066549539566, 'train/mean_average_precision': 0.6146754178331644, 'validation/accuracy': 0.986743152141571, 'validation/loss': 0.047809213399887085, 'validation/mean_average_precision': 0.26794072664347346, 'validation/num_examples': 43793, 'test/accuracy': 0.9857187271118164, 'test/loss': 0.05110371857881546, 'test/mean_average_precision': 0.2504868933628825, 'test/num_examples': 43793, 'score': 17305.169719457626, 'total_duration': 26697.89265561104, 'accumulated_submission_time': 17305.169719457626, 'accumulated_eval_time': 9388.612121343613, 'accumulated_logging_time': 2.655043125152588, 'global_step': 54198, 'preemption_count': 0}), (54955, {'train/accuracy': 0.9935206770896912, 'train/loss': 0.020450828596949577, 'train/mean_average_precision': 0.6152469816588861, 'validation/accuracy': 0.9867601990699768, 'validation/loss': 0.0481349416077137, 'validation/mean_average_precision': 0.2696156510939548, 'validation/num_examples': 43793, 'test/accuracy': 0.985710084438324, 'test/loss': 0.051288455724716187, 'test/mean_average_precision': 0.25568876173671473, 'test/num_examples': 43793, 'score': 17545.279770612717, 'total_duration': 27061.449331760406, 'accumulated_submission_time': 17545.279770612717, 'accumulated_eval_time': 9512.002286434174, 'accumulated_logging_time': 2.6914784908294678, 'global_step': 54955, 'preemption_count': 0}), (55711, {'train/accuracy': 0.9935681819915771, 'train/loss': 0.020248262211680412, 'train/mean_average_precision': 0.6322992889381533, 'validation/accuracy': 0.9867435693740845, 'validation/loss': 0.04861031472682953, 'validation/mean_average_precision': 0.2674156022578643, 'validation/num_examples': 43793, 'test/accuracy': 0.9857816696166992, 'test/loss': 0.05180561542510986, 'test/mean_average_precision': 0.25602466362856396, 'test/num_examples': 43793, 'score': 17785.269641399384, 'total_duration': 27420.068832159042, 'accumulated_submission_time': 17785.269641399384, 'accumulated_eval_time': 9630.57388138771, 'accumulated_logging_time': 2.7295162677764893, 'global_step': 55711, 'preemption_count': 0}), (56465, {'train/accuracy': 0.9937283992767334, 'train/loss': 0.019526956602931023, 'train/mean_average_precision': 0.6458244588409702, 'validation/accuracy': 0.9866229891777039, 'validation/loss': 0.049001581966876984, 'validation/mean_average_precision': 0.2654806746801502, 'validation/num_examples': 43793, 'test/accuracy': 0.9857151508331299, 'test/loss': 0.052170172333717346, 'test/mean_average_precision': 0.24860228801934148, 'test/num_examples': 43793, 'score': 18025.476142644882, 'total_duration': 27784.033193588257, 'accumulated_submission_time': 18025.476142644882, 'accumulated_eval_time': 9754.26741719246, 'accumulated_logging_time': 2.7738380432128906, 'global_step': 56465, 'preemption_count': 0}), (57218, {'train/accuracy': 0.994199812412262, 'train/loss': 0.01858672685921192, 'train/mean_average_precision': 0.6527285246331709, 'validation/accuracy': 0.9865426421165466, 'validation/loss': 0.04894685745239258, 'validation/mean_average_precision': 0.2711905727317665, 'validation/num_examples': 43793, 'test/accuracy': 0.9856405854225159, 'test/loss': 0.05209792032837868, 'test/mean_average_precision': 0.25158631821213995, 'test/num_examples': 43793, 'score': 18265.4832341671, 'total_duration': 28143.712094783783, 'accumulated_submission_time': 18265.4832341671, 'accumulated_eval_time': 9873.88046002388, 'accumulated_logging_time': 2.812410593032837, 'global_step': 57218, 'preemption_count': 0})], 'global_step': 57878}
I0205 05:04:57.261383 140205209478976 submission_runner.py:586] Timing: 18477.15989422798
I0205 05:04:57.261438 140205209478976 submission_runner.py:588] Total number of evals: 77
I0205 05:04:57.261481 140205209478976 submission_runner.py:589] ====================
I0205 05:04:57.261535 140205209478976 submission_runner.py:542] Using RNG seed 1480595982
I0205 05:04:57.329194 140205209478976 submission_runner.py:551] --- Tuning run 2/5 ---
I0205 05:04:57.329343 140205209478976 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_2.
I0205 05:04:57.329763 140205209478976 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_2/hparams.json.
I0205 05:04:57.468476 140205209478976 submission_runner.py:206] Initializing dataset.
I0205 05:04:57.561111 140205209478976 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 05:04:57.565193 140205209478976 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 05:04:57.978368 140205209478976 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 05:04:58.019246 140205209478976 submission_runner.py:213] Initializing model.
I0205 05:05:02.346516 140205209478976 submission_runner.py:255] Initializing optimizer.
I0205 05:05:02.960764 140205209478976 submission_runner.py:262] Initializing metrics bundle.
I0205 05:05:02.960952 140205209478976 submission_runner.py:280] Initializing checkpoint and logger.
I0205 05:05:02.961610 140205209478976 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_2 with prefix checkpoint_
I0205 05:05:02.961740 140205209478976 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_2/meta_data_0.json.
I0205 05:05:02.961935 140205209478976 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 05:05:02.961995 140205209478976 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 05:05:04.111395 140205209478976 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 05:05:05.244970 140205209478976 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_2/flags_0.json.
I0205 05:05:05.255066 140205209478976 submission_runner.py:314] Starting training loop.
I0205 05:05:17.500735 140021138208512 logging_writer.py:48] [0] global_step=0, grad_norm=2.362900495529175, loss=0.7395897507667542
I0205 05:05:17.512407 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:07:11.900582 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:07:14.937506 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:07:17.953880 140205209478976 submission_runner.py:408] Time since start: 132.70s, 	Step: 1, 	{'train/accuracy': 0.5092054009437561, 'train/loss': 0.7392170429229736, 'train/mean_average_precision': 0.02240319433195738, 'validation/accuracy': 0.5064647793769836, 'validation/loss': 0.7422076463699341, 'validation/mean_average_precision': 0.025301665127186908, 'validation/num_examples': 43793, 'test/accuracy': 0.5047284960746765, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.02696451860565442, 'test/num_examples': 43793, 'score': 12.25728178024292, 'total_duration': 132.69875383377075, 'accumulated_submission_time': 12.25728178024292, 'accumulated_eval_time': 120.44142413139343, 'accumulated_logging_time': 0}
I0205 05:07:17.964387 140021163808512 logging_writer.py:48] [1] accumulated_eval_time=120.441424, accumulated_logging_time=0, accumulated_submission_time=12.257282, global_step=1, preemption_count=0, score=12.257282, test/accuracy=0.504728, test/loss=0.743874, test/mean_average_precision=0.026965, test/num_examples=43793, total_duration=132.698754, train/accuracy=0.509205, train/loss=0.739217, train/mean_average_precision=0.022403, validation/accuracy=0.506465, validation/loss=0.742208, validation/mean_average_precision=0.025302, validation/num_examples=43793
I0205 05:07:50.243915 140030420236032 logging_writer.py:48] [100] global_step=100, grad_norm=0.48085740208625793, loss=0.44099918007850647
I0205 05:08:22.422766 140021163808512 logging_writer.py:48] [200] global_step=200, grad_norm=0.3829706311225891, loss=0.3336351215839386
I0205 05:08:54.855439 140030420236032 logging_writer.py:48] [300] global_step=300, grad_norm=0.27534329891204834, loss=0.22920480370521545
I0205 05:09:27.239287 140021163808512 logging_writer.py:48] [400] global_step=400, grad_norm=0.18350213766098022, loss=0.15001621842384338
I0205 05:09:59.785845 140030420236032 logging_writer.py:48] [500] global_step=500, grad_norm=0.10812724381685257, loss=0.10663183033466339
I0205 05:10:32.620341 140021163808512 logging_writer.py:48] [600] global_step=600, grad_norm=0.06689892709255219, loss=0.08002707362174988
I0205 05:11:05.281623 140030420236032 logging_writer.py:48] [700] global_step=700, grad_norm=0.050253335386514664, loss=0.06799724698066711
I0205 05:11:18.075227 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:13:13.561856 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:13:16.641561 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:13:19.620706 140205209478976 submission_runner.py:408] Time since start: 494.37s, 	Step: 740, 	{'train/accuracy': 0.9868019819259644, 'train/loss': 0.06725268810987473, 'train/mean_average_precision': 0.037721288701160274, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07590807974338531, 'validation/mean_average_precision': 0.03840245958394749, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07886271178722382, 'test/mean_average_precision': 0.041103354055522685, 'test/num_examples': 43793, 'score': 252.33542895317078, 'total_duration': 494.3655803203583, 'accumulated_submission_time': 252.33542895317078, 'accumulated_eval_time': 241.9868643283844, 'accumulated_logging_time': 0.0215451717376709}
I0205 05:13:19.636356 140020770670336 logging_writer.py:48] [740] accumulated_eval_time=241.986864, accumulated_logging_time=0.021545, accumulated_submission_time=252.335429, global_step=740, preemption_count=0, score=252.335429, test/accuracy=0.983142, test/loss=0.078863, test/mean_average_precision=0.041103, test/num_examples=43793, total_duration=494.365580, train/accuracy=0.986802, train/loss=0.067253, train/mean_average_precision=0.037721, validation/accuracy=0.984118, validation/loss=0.075908, validation/mean_average_precision=0.038402, validation/num_examples=43793
I0205 05:13:39.314830 140037886379776 logging_writer.py:48] [800] global_step=800, grad_norm=0.04187409207224846, loss=0.060570310801267624
I0205 05:14:11.669007 140020770670336 logging_writer.py:48] [900] global_step=900, grad_norm=0.026917049661278725, loss=0.05648479238152504
I0205 05:14:43.830475 140037886379776 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.04201820120215416, loss=0.05824505537748337
I0205 05:15:15.858263 140020770670336 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.023898396641016006, loss=0.0535726360976696
I0205 05:15:48.031974 140037886379776 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.03342544659972191, loss=0.04980652779340744
I0205 05:16:19.909415 140020770670336 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.044952720403671265, loss=0.04909820482134819
I0205 05:16:52.168096 140037886379776 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.028590066358447075, loss=0.0506843701004982
I0205 05:17:19.682942 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:19:16.032810 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:19:19.079936 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:19:22.005267 140205209478976 submission_runner.py:408] Time since start: 856.75s, 	Step: 1486, 	{'train/accuracy': 0.9867600202560425, 'train/loss': 0.05239196494221687, 'train/mean_average_precision': 0.07542910055882224, 'validation/accuracy': 0.9841382503509521, 'validation/loss': 0.06244411692023277, 'validation/mean_average_precision': 0.07567169206821818, 'validation/num_examples': 43793, 'test/accuracy': 0.9831559658050537, 'test/loss': 0.06585729122161865, 'test/mean_average_precision': 0.07697097941510322, 'test/num_examples': 43793, 'score': 492.3489384651184, 'total_duration': 856.7501437664032, 'accumulated_submission_time': 492.3489384651184, 'accumulated_eval_time': 364.3091459274292, 'accumulated_logging_time': 0.049912452697753906}
I0205 05:19:22.021215 140021163808512 logging_writer.py:48] [1486] accumulated_eval_time=364.309146, accumulated_logging_time=0.049912, accumulated_submission_time=492.348938, global_step=1486, preemption_count=0, score=492.348938, test/accuracy=0.983156, test/loss=0.065857, test/mean_average_precision=0.076971, test/num_examples=43793, total_duration=856.750144, train/accuracy=0.986760, train/loss=0.052392, train/mean_average_precision=0.075429, validation/accuracy=0.984138, validation/loss=0.062444, validation/mean_average_precision=0.075672, validation/num_examples=43793
I0205 05:19:26.988868 140030420236032 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.13690069317817688, loss=0.052592575550079346
I0205 05:19:58.585400 140021163808512 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.02700856886804104, loss=0.04705781489610672
I0205 05:20:30.480562 140030420236032 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0596429705619812, loss=0.05211305990815163
I0205 05:21:02.262605 140021163808512 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.052559126168489456, loss=0.05515534430742264
I0205 05:21:33.939444 140030420236032 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.03718617931008339, loss=0.0514829158782959
I0205 05:22:05.868763 140021163808512 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0678594782948494, loss=0.05463351681828499
I0205 05:22:38.267063 140030420236032 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.07821021229028702, loss=0.05304722115397453
I0205 05:23:09.949311 140021163808512 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.062360040843486786, loss=0.04381103441119194
I0205 05:23:22.290582 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:25:18.152102 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:25:21.111416 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:25:24.083254 140205209478976 submission_runner.py:408] Time since start: 1218.83s, 	Step: 2240, 	{'train/accuracy': 0.9872581958770752, 'train/loss': 0.045945290476083755, 'train/mean_average_precision': 0.1198747155496232, 'validation/accuracy': 0.9846627116203308, 'validation/loss': 0.0551317036151886, 'validation/mean_average_precision': 0.11798906748669641, 'validation/num_examples': 43793, 'test/accuracy': 0.9836491942405701, 'test/loss': 0.05832909047603607, 'test/mean_average_precision': 0.11707584962004593, 'test/num_examples': 43793, 'score': 732.587010383606, 'total_duration': 1218.8280143737793, 'accumulated_submission_time': 732.587010383606, 'accumulated_eval_time': 486.101655960083, 'accumulated_logging_time': 0.07718324661254883}
I0205 05:25:24.098880 140037877987072 logging_writer.py:48] [2240] accumulated_eval_time=486.101656, accumulated_logging_time=0.077183, accumulated_submission_time=732.587010, global_step=2240, preemption_count=0, score=732.587010, test/accuracy=0.983649, test/loss=0.058329, test/mean_average_precision=0.117076, test/num_examples=43793, total_duration=1218.828014, train/accuracy=0.987258, train/loss=0.045945, train/mean_average_precision=0.119875, validation/accuracy=0.984663, validation/loss=0.055132, validation/mean_average_precision=0.117989, validation/num_examples=43793
I0205 05:25:44.124691 140037886379776 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.07093475013971329, loss=0.0440746545791626
I0205 05:26:16.298141 140037877987072 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.06904095411300659, loss=0.049237120896577835
I0205 05:26:48.075159 140037886379776 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.030072901397943497, loss=0.048720043152570724
I0205 05:27:19.957667 140037877987072 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.03576216846704483, loss=0.04711239039897919
I0205 05:27:52.139246 140037886379776 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.03591058775782585, loss=0.04315827414393425
I0205 05:28:23.721263 140037877987072 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.13001872599124908, loss=0.048354048281908035
I0205 05:28:55.351354 140037886379776 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.019548818469047546, loss=0.04603522643446922
I0205 05:29:24.270216 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:31:20.512771 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:31:23.530211 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:31:26.555493 140205209478976 submission_runner.py:408] Time since start: 1581.30s, 	Step: 2992, 	{'train/accuracy': 0.9874991774559021, 'train/loss': 0.04441913962364197, 'train/mean_average_precision': 0.14990756717097473, 'validation/accuracy': 0.984927773475647, 'validation/loss': 0.0537693053483963, 'validation/mean_average_precision': 0.13893334565505383, 'validation/num_examples': 43793, 'test/accuracy': 0.9839326739311218, 'test/loss': 0.056828781962394714, 'test/mean_average_precision': 0.14079963923587263, 'test/num_examples': 43793, 'score': 972.7253069877625, 'total_duration': 1581.3003692626953, 'accumulated_submission_time': 972.7253069877625, 'accumulated_eval_time': 608.3868906497955, 'accumulated_logging_time': 0.10598874092102051}
I0205 05:31:26.570967 140020770670336 logging_writer.py:48] [2992] accumulated_eval_time=608.386891, accumulated_logging_time=0.105989, accumulated_submission_time=972.725307, global_step=2992, preemption_count=0, score=972.725307, test/accuracy=0.983933, test/loss=0.056829, test/mean_average_precision=0.140800, test/num_examples=43793, total_duration=1581.300369, train/accuracy=0.987499, train/loss=0.044419, train/mean_average_precision=0.149908, validation/accuracy=0.984928, validation/loss=0.053769, validation/mean_average_precision=0.138933, validation/num_examples=43793
I0205 05:31:29.446533 140030420236032 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0408841036260128, loss=0.043720752000808716
I0205 05:32:01.507088 140020770670336 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.020725851878523827, loss=0.04600553214550018
I0205 05:32:33.045080 140030420236032 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0267743319272995, loss=0.049195125699043274
I0205 05:33:04.694642 140020770670336 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.02314051054418087, loss=0.043495118618011475
I0205 05:33:36.538868 140030420236032 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.035759907215833664, loss=0.048977356404066086
I0205 05:34:08.102794 140020770670336 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.028464416041970253, loss=0.051014699041843414
I0205 05:34:39.589212 140030420236032 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.03492234647274017, loss=0.04663686081767082
I0205 05:35:11.226493 140020770670336 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.037422098219394684, loss=0.045592933893203735
I0205 05:35:26.576285 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:37:25.004812 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:37:28.042871 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:37:31.004727 140205209478976 submission_runner.py:408] Time since start: 1945.75s, 	Step: 3750, 	{'train/accuracy': 0.9879691004753113, 'train/loss': 0.04257238283753395, 'train/mean_average_precision': 0.1707366148038464, 'validation/accuracy': 0.985083281993866, 'validation/loss': 0.05229465290904045, 'validation/mean_average_precision': 0.15167389527715167, 'validation/num_examples': 43793, 'test/accuracy': 0.984113335609436, 'test/loss': 0.05512439087033272, 'test/mean_average_precision': 0.1527203936176814, 'test/num_examples': 43793, 'score': 1212.699630022049, 'total_duration': 1945.7496001720428, 'accumulated_submission_time': 1212.699630022049, 'accumulated_eval_time': 732.8152990341187, 'accumulated_logging_time': 0.1326751708984375}
I0205 05:37:31.020522 140021163808512 logging_writer.py:48] [3750] accumulated_eval_time=732.815299, accumulated_logging_time=0.132675, accumulated_submission_time=1212.699630, global_step=3750, preemption_count=0, score=1212.699630, test/accuracy=0.984113, test/loss=0.055124, test/mean_average_precision=0.152720, test/num_examples=43793, total_duration=1945.749600, train/accuracy=0.987969, train/loss=0.042572, train/mean_average_precision=0.170737, validation/accuracy=0.985083, validation/loss=0.052295, validation/mean_average_precision=0.151674, validation/num_examples=43793
I0205 05:37:47.328639 140037886379776 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.04026259481906891, loss=0.04629828780889511
I0205 05:38:19.112756 140021163808512 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.022441143169999123, loss=0.04643070325255394
I0205 05:38:51.124380 140037886379776 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.03292107954621315, loss=0.041371751576662064
I0205 05:39:23.166213 140021163808512 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.034134916961193085, loss=0.04366537928581238
I0205 05:39:55.024777 140037886379776 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.03442470356822014, loss=0.044111017137765884
I0205 05:40:26.604916 140021163808512 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.017791802063584328, loss=0.044145554304122925
I0205 05:40:58.252292 140037886379776 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.06300554424524307, loss=0.04796081781387329
I0205 05:41:29.966136 140021163808512 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.021414607763290405, loss=0.04286694526672363
I0205 05:41:31.227693 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:43:23.766648 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:43:26.820326 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:43:31.143127 140205209478976 submission_runner.py:408] Time since start: 2305.89s, 	Step: 4505, 	{'train/accuracy': 0.9882915616035461, 'train/loss': 0.04109819233417511, 'train/mean_average_precision': 0.19363046177628512, 'validation/accuracy': 0.9853597283363342, 'validation/loss': 0.05075986683368683, 'validation/mean_average_precision': 0.1696635347704308, 'validation/num_examples': 43793, 'test/accuracy': 0.9843845963478088, 'test/loss': 0.05361567437648773, 'test/mean_average_precision': 0.16837953043101717, 'test/num_examples': 43793, 'score': 1452.8758084774017, 'total_duration': 2305.887991666794, 'accumulated_submission_time': 1452.8758084774017, 'accumulated_eval_time': 852.7306742668152, 'accumulated_logging_time': 0.15938496589660645}
I0205 05:43:31.159734 140020770670336 logging_writer.py:48] [4505] accumulated_eval_time=852.730674, accumulated_logging_time=0.159385, accumulated_submission_time=1452.875808, global_step=4505, preemption_count=0, score=1452.875808, test/accuracy=0.984385, test/loss=0.053616, test/mean_average_precision=0.168380, test/num_examples=43793, total_duration=2305.887992, train/accuracy=0.988292, train/loss=0.041098, train/mean_average_precision=0.193630, validation/accuracy=0.985360, validation/loss=0.050760, validation/mean_average_precision=0.169664, validation/num_examples=43793
I0205 05:44:01.968216 140030420236032 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.01882997341454029, loss=0.0461738295853138
I0205 05:44:34.100159 140020770670336 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.03922615200281143, loss=0.04296603426337242
I0205 05:45:06.580195 140030420236032 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.029697898775339127, loss=0.04674733057618141
I0205 05:45:38.781016 140020770670336 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.019490739330649376, loss=0.04942445829510689
I0205 05:46:10.787715 140030420236032 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.01791391335427761, loss=0.04435817524790764
I0205 05:46:42.593979 140020770670336 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.01929789036512375, loss=0.0419900007545948
I0205 05:47:14.812049 140030420236032 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.011929976753890514, loss=0.040518809109926224
I0205 05:47:31.482045 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:49:23.100526 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:49:26.140546 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:49:29.168277 140205209478976 submission_runner.py:408] Time since start: 2663.91s, 	Step: 5253, 	{'train/accuracy': 0.9885271787643433, 'train/loss': 0.04024849832057953, 'train/mean_average_precision': 0.21536606923817142, 'validation/accuracy': 0.985521674156189, 'validation/loss': 0.049516282975673676, 'validation/mean_average_precision': 0.17605855944202745, 'validation/num_examples': 43793, 'test/accuracy': 0.9847253561019897, 'test/loss': 0.051736604422330856, 'test/mean_average_precision': 0.1818915693414284, 'test/num_examples': 43793, 'score': 1693.1669998168945, 'total_duration': 2663.9131495952606, 'accumulated_submission_time': 1693.1669998168945, 'accumulated_eval_time': 970.4168586730957, 'accumulated_logging_time': 0.18683815002441406}
I0205 05:49:29.184591 140037877987072 logging_writer.py:48] [5253] accumulated_eval_time=970.416859, accumulated_logging_time=0.186838, accumulated_submission_time=1693.167000, global_step=5253, preemption_count=0, score=1693.167000, test/accuracy=0.984725, test/loss=0.051737, test/mean_average_precision=0.181892, test/num_examples=43793, total_duration=2663.913150, train/accuracy=0.988527, train/loss=0.040248, train/mean_average_precision=0.215366, validation/accuracy=0.985522, validation/loss=0.049516, validation/mean_average_precision=0.176059, validation/num_examples=43793
I0205 05:49:45.653914 140037886379776 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.04814298823475838, loss=0.043045222759246826
I0205 05:50:18.661770 140037877987072 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.02356104552745819, loss=0.04269569367170334
I0205 05:50:50.530118 140037886379776 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.02545079030096531, loss=0.04602982476353645
I0205 05:51:22.373228 140037877987072 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.021275078877806664, loss=0.042160145938396454
I0205 05:51:53.980100 140037886379776 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.014285328797996044, loss=0.04205295816063881
I0205 05:52:25.719129 140037877987072 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.03383028879761696, loss=0.04543102905154228
I0205 05:52:57.462915 140037886379776 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.020481884479522705, loss=0.044695306569337845
I0205 05:53:29.439373 140037877987072 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.01464528776705265, loss=0.0390683077275753
I0205 05:53:29.444448 140205209478976 spec.py:321] Evaluating on the training split.
I0205 05:55:22.322974 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 05:55:25.310991 140205209478976 spec.py:349] Evaluating on the test split.
I0205 05:55:28.293252 140205209478976 submission_runner.py:408] Time since start: 3023.04s, 	Step: 6001, 	{'train/accuracy': 0.9886388182640076, 'train/loss': 0.03877423331141472, 'train/mean_average_precision': 0.2430892577206733, 'validation/accuracy': 0.9857465624809265, 'validation/loss': 0.04839358106255531, 'validation/mean_average_precision': 0.20521819519884538, 'validation/num_examples': 43793, 'test/accuracy': 0.9848209619522095, 'test/loss': 0.05105356499552727, 'test/mean_average_precision': 0.2013832889900116, 'test/num_examples': 43793, 'score': 1933.3945820331573, 'total_duration': 3023.0380108356476, 'accumulated_submission_time': 1933.3945820331573, 'accumulated_eval_time': 1089.2654836177826, 'accumulated_logging_time': 0.2155897617340088}
I0205 05:55:28.309238 140044360042240 logging_writer.py:48] [6001] accumulated_eval_time=1089.265484, accumulated_logging_time=0.215590, accumulated_submission_time=1933.394582, global_step=6001, preemption_count=0, score=1933.394582, test/accuracy=0.984821, test/loss=0.051054, test/mean_average_precision=0.201383, test/num_examples=43793, total_duration=3023.038011, train/accuracy=0.988639, train/loss=0.038774, train/mean_average_precision=0.243089, validation/accuracy=0.985747, validation/loss=0.048394, validation/mean_average_precision=0.205218, validation/num_examples=43793
I0205 05:56:00.117069 140142501164800 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.022228918969631195, loss=0.04603179916739464
I0205 05:56:31.660876 140044360042240 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.022617347538471222, loss=0.04043387621641159
I0205 05:57:03.343897 140142501164800 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.014728955924510956, loss=0.04306734353303909
I0205 05:57:35.025182 140044360042240 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.01720358058810234, loss=0.041452355682849884
I0205 05:58:06.701098 140142501164800 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.013957953080534935, loss=0.040208350867033005
I0205 05:58:38.696411 140044360042240 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.02884472906589508, loss=0.043277252465486526
I0205 05:59:10.729432 140142501164800 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.027856364846229553, loss=0.04494774714112282
I0205 05:59:28.537583 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:01:17.586401 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:01:20.652897 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:01:25.004355 140205209478976 submission_runner.py:408] Time since start: 3379.75s, 	Step: 6758, 	{'train/accuracy': 0.9886777997016907, 'train/loss': 0.03834982588887215, 'train/mean_average_precision': 0.2587568848428609, 'validation/accuracy': 0.9859300255775452, 'validation/loss': 0.047791145741939545, 'validation/mean_average_precision': 0.21494410461051575, 'validation/num_examples': 43793, 'test/accuracy': 0.985062301158905, 'test/loss': 0.05047466233372688, 'test/mean_average_precision': 0.21457699082593304, 'test/num_examples': 43793, 'score': 2173.5919332504272, 'total_duration': 3379.749230146408, 'accumulated_submission_time': 2173.5919332504272, 'accumulated_eval_time': 1205.7322103977203, 'accumulated_logging_time': 0.24257278442382812}
I0205 06:01:25.021370 140037877987072 logging_writer.py:48] [6758] accumulated_eval_time=1205.732210, accumulated_logging_time=0.242573, accumulated_submission_time=2173.591933, global_step=6758, preemption_count=0, score=2173.591933, test/accuracy=0.985062, test/loss=0.050475, test/mean_average_precision=0.214577, test/num_examples=43793, total_duration=3379.749230, train/accuracy=0.988678, train/loss=0.038350, train/mean_average_precision=0.258757, validation/accuracy=0.985930, validation/loss=0.047791, validation/mean_average_precision=0.214944, validation/num_examples=43793
I0205 06:01:38.702683 140037886379776 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.015329155139625072, loss=0.039557602256536484
I0205 06:02:10.346035 140037877987072 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.015649186447262764, loss=0.04029093682765961
I0205 06:02:41.784999 140037886379776 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0225930605083704, loss=0.0415181890130043
I0205 06:03:13.353541 140037877987072 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.012336362153291702, loss=0.03723068907856941
I0205 06:03:44.763779 140037886379776 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.01538564171642065, loss=0.03989779204130173
I0205 06:04:16.443663 140037877987072 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.014128588140010834, loss=0.04039552062749863
I0205 06:04:47.910974 140037886379776 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.025054534897208214, loss=0.0425744354724884
I0205 06:05:19.821872 140037877987072 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.021992215886712074, loss=0.04287073761224747
I0205 06:05:25.021943 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:07:19.208470 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:07:22.227025 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:07:25.190327 140205209478976 submission_runner.py:408] Time since start: 3739.94s, 	Step: 7517, 	{'train/accuracy': 0.9893007874488831, 'train/loss': 0.036982662975788116, 'train/mean_average_precision': 0.27333394809486294, 'validation/accuracy': 0.9862178564071655, 'validation/loss': 0.04694636911153793, 'validation/mean_average_precision': 0.2218923235563141, 'validation/num_examples': 43793, 'test/accuracy': 0.9852644801139832, 'test/loss': 0.04949888214468956, 'test/mean_average_precision': 0.22878059163496006, 'test/num_examples': 43793, 'score': 2413.561702489853, 'total_duration': 3739.935199022293, 'accumulated_submission_time': 2413.561702489853, 'accumulated_eval_time': 1325.9005455970764, 'accumulated_logging_time': 0.27036023139953613}
I0205 06:07:25.206969 140021163808512 logging_writer.py:48] [7517] accumulated_eval_time=1325.900546, accumulated_logging_time=0.270360, accumulated_submission_time=2413.561702, global_step=7517, preemption_count=0, score=2413.561702, test/accuracy=0.985264, test/loss=0.049499, test/mean_average_precision=0.228781, test/num_examples=43793, total_duration=3739.935199, train/accuracy=0.989301, train/loss=0.036983, train/mean_average_precision=0.273334, validation/accuracy=0.986218, validation/loss=0.046946, validation/mean_average_precision=0.221892, validation/num_examples=43793
I0205 06:07:51.835594 140142501164800 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.02110949717462063, loss=0.043655890971422195
I0205 06:08:23.653753 140021163808512 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.017846349626779556, loss=0.04567181318998337
I0205 06:08:55.859059 140142501164800 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.014111356809735298, loss=0.039380792528390884
I0205 06:09:27.545971 140021163808512 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.015388930216431618, loss=0.041189294308423996
I0205 06:09:59.279784 140142501164800 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.013363792560994625, loss=0.040979232639074326
I0205 06:10:31.153806 140021163808512 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.02115636132657528, loss=0.04377247393131256
I0205 06:11:02.916283 140142501164800 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.03966190665960312, loss=0.04126221314072609
I0205 06:11:25.289839 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:13:21.524789 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:13:24.574408 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:13:27.510650 140205209478976 submission_runner.py:408] Time since start: 4102.26s, 	Step: 8271, 	{'train/accuracy': 0.9892852902412415, 'train/loss': 0.036062296479940414, 'train/mean_average_precision': 0.3039532837698702, 'validation/accuracy': 0.9862227439880371, 'validation/loss': 0.04686782509088516, 'validation/mean_average_precision': 0.23472995254406653, 'validation/num_examples': 43793, 'test/accuracy': 0.9852551817893982, 'test/loss': 0.049763064831495285, 'test/mean_average_precision': 0.2241731692471199, 'test/num_examples': 43793, 'score': 2653.61355137825, 'total_duration': 4102.255522012711, 'accumulated_submission_time': 2653.61355137825, 'accumulated_eval_time': 1448.121309518814, 'accumulated_logging_time': 0.2982769012451172}
I0205 06:13:27.527394 140037877987072 logging_writer.py:48] [8271] accumulated_eval_time=1448.121310, accumulated_logging_time=0.298277, accumulated_submission_time=2653.613551, global_step=8271, preemption_count=0, score=2653.613551, test/accuracy=0.985255, test/loss=0.049763, test/mean_average_precision=0.224173, test/num_examples=43793, total_duration=4102.255522, train/accuracy=0.989285, train/loss=0.036062, train/mean_average_precision=0.303953, validation/accuracy=0.986223, validation/loss=0.046868, validation/mean_average_precision=0.234730, validation/num_examples=43793
I0205 06:13:37.139106 140037886379776 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.01666238158941269, loss=0.044823355972766876
I0205 06:14:08.808317 140037877987072 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.030056266114115715, loss=0.03923843428492546
I0205 06:14:40.658602 140037886379776 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.013854158110916615, loss=0.043835800141096115
I0205 06:15:12.710450 140037877987072 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.014074830338358879, loss=0.04046013578772545
I0205 06:15:44.588959 140037886379776 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.016129376366734505, loss=0.04114411771297455
I0205 06:16:16.808559 140037877987072 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.016592878848314285, loss=0.04012928158044815
I0205 06:16:48.556070 140037886379776 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.023191431537270546, loss=0.041573379188776016
I0205 06:17:21.039614 140037877987072 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.014825446531176567, loss=0.039406485855579376
I0205 06:17:27.594867 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:19:16.060572 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:19:19.102487 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:19:23.447376 140205209478976 submission_runner.py:408] Time since start: 4458.19s, 	Step: 9022, 	{'train/accuracy': 0.989479660987854, 'train/loss': 0.03526661917567253, 'train/mean_average_precision': 0.31980141372316095, 'validation/accuracy': 0.9863124489784241, 'validation/loss': 0.04625304415822029, 'validation/mean_average_precision': 0.2304908775967825, 'validation/num_examples': 43793, 'test/accuracy': 0.9854481220245361, 'test/loss': 0.04905175045132637, 'test/mean_average_precision': 0.23790814244357286, 'test/num_examples': 43793, 'score': 2893.6486990451813, 'total_duration': 4458.192250013351, 'accumulated_submission_time': 2893.6486990451813, 'accumulated_eval_time': 1563.973773241043, 'accumulated_logging_time': 0.32762908935546875}
I0205 06:19:23.465225 140030420236032 logging_writer.py:48] [9022] accumulated_eval_time=1563.973773, accumulated_logging_time=0.327629, accumulated_submission_time=2893.648699, global_step=9022, preemption_count=0, score=2893.648699, test/accuracy=0.985448, test/loss=0.049052, test/mean_average_precision=0.237908, test/num_examples=43793, total_duration=4458.192250, train/accuracy=0.989480, train/loss=0.035267, train/mean_average_precision=0.319801, validation/accuracy=0.986312, validation/loss=0.046253, validation/mean_average_precision=0.230491, validation/num_examples=43793
I0205 06:19:48.645937 140044360042240 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.014882458373904228, loss=0.04489697143435478
I0205 06:20:20.735371 140030420236032 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.015806183218955994, loss=0.04087173193693161
I0205 06:20:52.587429 140044360042240 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.01976129412651062, loss=0.042552050203084946
I0205 06:21:24.721499 140030420236032 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.013758176937699318, loss=0.03893830254673958
I0205 06:21:56.680596 140044360042240 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.017349453642964363, loss=0.03934084624052048
I0205 06:22:28.648972 140030420236032 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.01350492611527443, loss=0.03766905888915062
I0205 06:23:00.461403 140044360042240 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.023705236613750458, loss=0.03942273184657097
I0205 06:23:23.594574 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:25:15.160606 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:25:18.173783 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:25:21.141102 140205209478976 submission_runner.py:408] Time since start: 4815.89s, 	Step: 9774, 	{'train/accuracy': 0.9896937608718872, 'train/loss': 0.034345462918281555, 'train/mean_average_precision': 0.3432716140602922, 'validation/accuracy': 0.9864041805267334, 'validation/loss': 0.04587381333112717, 'validation/mean_average_precision': 0.2375307320474216, 'validation/num_examples': 43793, 'test/accuracy': 0.9855083227157593, 'test/loss': 0.048625972121953964, 'test/mean_average_precision': 0.23808249701418813, 'test/num_examples': 43793, 'score': 3133.746441602707, 'total_duration': 4815.885853528976, 'accumulated_submission_time': 3133.746441602707, 'accumulated_eval_time': 1681.5201325416565, 'accumulated_logging_time': 0.35713958740234375}
I0205 06:25:21.157986 140037886379776 logging_writer.py:48] [9774] accumulated_eval_time=1681.520133, accumulated_logging_time=0.357140, accumulated_submission_time=3133.746442, global_step=9774, preemption_count=0, score=3133.746442, test/accuracy=0.985508, test/loss=0.048626, test/mean_average_precision=0.238082, test/num_examples=43793, total_duration=4815.885854, train/accuracy=0.989694, train/loss=0.034345, train/mean_average_precision=0.343272, validation/accuracy=0.986404, validation/loss=0.045874, validation/mean_average_precision=0.237531, validation/num_examples=43793
I0205 06:25:29.674518 140142501164800 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.014001304283738136, loss=0.0412202887237072
I0205 06:26:01.726396 140037886379776 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.013141774572432041, loss=0.03735983371734619
I0205 06:26:33.267702 140142501164800 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.01819678209722042, loss=0.04204687103629112
I0205 06:27:05.431409 140037886379776 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.017647413536906242, loss=0.04099905490875244
I0205 06:27:37.540297 140142501164800 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.029839104041457176, loss=0.04210229963064194
I0205 06:28:09.396879 140037886379776 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.019653834402561188, loss=0.03982075676321983
I0205 06:28:41.319670 140142501164800 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.01538658607751131, loss=0.03683208301663399
I0205 06:29:13.595168 140037886379776 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.02033400908112526, loss=0.039774443954229355
I0205 06:29:21.335409 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:31:11.542871 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:31:14.604537 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:31:17.564630 140205209478976 submission_runner.py:408] Time since start: 5172.31s, 	Step: 10525, 	{'train/accuracy': 0.9900869131088257, 'train/loss': 0.03330640122294426, 'train/mean_average_precision': 0.3712344743367008, 'validation/accuracy': 0.9865828156471252, 'validation/loss': 0.045364007353782654, 'validation/mean_average_precision': 0.24889151383711278, 'validation/num_examples': 43793, 'test/accuracy': 0.9856384992599487, 'test/loss': 0.04809035733342171, 'test/mean_average_precision': 0.24709253057784347, 'test/num_examples': 43793, 'score': 3373.892038345337, 'total_duration': 5172.309501647949, 'accumulated_submission_time': 3373.892038345337, 'accumulated_eval_time': 1797.7493011951447, 'accumulated_logging_time': 0.3862593173980713}
I0205 06:31:17.581921 140037877987072 logging_writer.py:48] [10525] accumulated_eval_time=1797.749301, accumulated_logging_time=0.386259, accumulated_submission_time=3373.892038, global_step=10525, preemption_count=0, score=3373.892038, test/accuracy=0.985638, test/loss=0.048090, test/mean_average_precision=0.247093, test/num_examples=43793, total_duration=5172.309502, train/accuracy=0.990087, train/loss=0.033306, train/mean_average_precision=0.371234, validation/accuracy=0.986583, validation/loss=0.045364, validation/mean_average_precision=0.248892, validation/num_examples=43793
I0205 06:31:41.804119 140044360042240 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.018241899088025093, loss=0.041557829827070236
I0205 06:32:13.351684 140037877987072 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.016355715692043304, loss=0.04016215726733208
I0205 06:32:45.127475 140044360042240 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.01444312371313572, loss=0.037033870816230774
I0205 06:33:16.970265 140037877987072 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.01794837787747383, loss=0.039734505116939545
I0205 06:33:48.820937 140044360042240 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.016880987212061882, loss=0.03995993733406067
I0205 06:34:20.667627 140037877987072 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.015193392522633076, loss=0.03870183601975441
I0205 06:34:52.294444 140044360042240 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.013843781314790249, loss=0.03790327534079552
I0205 06:35:17.771675 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:37:10.463626 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:37:13.448779 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:37:17.805937 140205209478976 submission_runner.py:408] Time since start: 5532.55s, 	Step: 11281, 	{'train/accuracy': 0.9902627468109131, 'train/loss': 0.03286798298358917, 'train/mean_average_precision': 0.36300384868811686, 'validation/accuracy': 0.986607551574707, 'validation/loss': 0.045536480844020844, 'validation/mean_average_precision': 0.24634402796851296, 'validation/num_examples': 43793, 'test/accuracy': 0.9856376647949219, 'test/loss': 0.0483180433511734, 'test/mean_average_precision': 0.2483078179636221, 'test/num_examples': 43793, 'score': 3614.0496232509613, 'total_duration': 5532.550810575485, 'accumulated_submission_time': 3614.0496232509613, 'accumulated_eval_time': 1917.783516407013, 'accumulated_logging_time': 0.4158000946044922}
I0205 06:37:17.823091 140030420236032 logging_writer.py:48] [11281] accumulated_eval_time=1917.783516, accumulated_logging_time=0.415800, accumulated_submission_time=3614.049623, global_step=11281, preemption_count=0, score=3614.049623, test/accuracy=0.985638, test/loss=0.048318, test/mean_average_precision=0.248308, test/num_examples=43793, total_duration=5532.550811, train/accuracy=0.990263, train/loss=0.032868, train/mean_average_precision=0.363004, validation/accuracy=0.986608, validation/loss=0.045536, validation/mean_average_precision=0.246344, validation/num_examples=43793
I0205 06:37:24.167712 140037886379776 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.017288701608777046, loss=0.040832605212926865
I0205 06:37:56.460469 140030420236032 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.014612814411520958, loss=0.03949548304080963
I0205 06:38:28.222520 140037886379776 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.019290706142783165, loss=0.03856465965509415
I0205 06:38:59.652844 140030420236032 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.013782035559415817, loss=0.035506270825862885
I0205 06:39:31.891572 140037886379776 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.014726844616234303, loss=0.03816646710038185
I0205 06:40:04.013350 140030420236032 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.01650197245180607, loss=0.039752986282110214
I0205 06:40:36.032734 140037886379776 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.02028409205377102, loss=0.04154062643647194
I0205 06:41:07.788899 140030420236032 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.016237378120422363, loss=0.03975796699523926
I0205 06:41:17.848629 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:43:11.850861 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:43:14.883624 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:43:17.908805 140205209478976 submission_runner.py:408] Time since start: 5892.65s, 	Step: 12033, 	{'train/accuracy': 0.9904043674468994, 'train/loss': 0.03233281522989273, 'train/mean_average_precision': 0.3812291308726584, 'validation/accuracy': 0.9865823984146118, 'validation/loss': 0.04554494842886925, 'validation/mean_average_precision': 0.2456691085848675, 'validation/num_examples': 43793, 'test/accuracy': 0.9856178760528564, 'test/loss': 0.04854458197951317, 'test/mean_average_precision': 0.24102209608237854, 'test/num_examples': 43793, 'score': 3854.0442173480988, 'total_duration': 5892.653676509857, 'accumulated_submission_time': 3854.0442173480988, 'accumulated_eval_time': 2037.8436410427094, 'accumulated_logging_time': 0.4441356658935547}
I0205 06:43:17.926967 140037877987072 logging_writer.py:48] [12033] accumulated_eval_time=2037.843641, accumulated_logging_time=0.444136, accumulated_submission_time=3854.044217, global_step=12033, preemption_count=0, score=3854.044217, test/accuracy=0.985618, test/loss=0.048545, test/mean_average_precision=0.241022, test/num_examples=43793, total_duration=5892.653677, train/accuracy=0.990404, train/loss=0.032333, train/mean_average_precision=0.381229, validation/accuracy=0.986582, validation/loss=0.045545, validation/mean_average_precision=0.245669, validation/num_examples=43793
I0205 06:43:39.376394 140044360042240 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.01564452238380909, loss=0.036076728254556656
I0205 06:44:10.943631 140037877987072 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.018185079097747803, loss=0.0379987396299839
I0205 06:44:43.021117 140044360042240 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.015715258195996284, loss=0.037060949951410294
I0205 06:45:15.405273 140037877987072 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.013793068937957287, loss=0.036857970058918
I0205 06:45:47.154947 140044360042240 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.019519103690981865, loss=0.036685697734355927
I0205 06:46:18.932177 140037877987072 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.01644655503332615, loss=0.03708523511886597
I0205 06:46:50.557362 140044360042240 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.018140945583581924, loss=0.03649720549583435
I0205 06:47:18.202288 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:49:10.155726 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:49:13.252156 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:49:16.236915 140205209478976 submission_runner.py:408] Time since start: 6250.98s, 	Step: 12788, 	{'train/accuracy': 0.9902794361114502, 'train/loss': 0.0322381928563118, 'train/mean_average_precision': 0.3840976298833426, 'validation/accuracy': 0.986572265625, 'validation/loss': 0.045612987130880356, 'validation/mean_average_precision': 0.2533034074588097, 'validation/num_examples': 43793, 'test/accuracy': 0.9856410026550293, 'test/loss': 0.04857763648033142, 'test/mean_average_precision': 0.24597810309914628, 'test/num_examples': 43793, 'score': 4094.2885637283325, 'total_duration': 6250.9817860126495, 'accumulated_submission_time': 4094.2885637283325, 'accumulated_eval_time': 2155.8782205581665, 'accumulated_logging_time': 0.47352027893066406}
I0205 06:49:16.254533 140021163808512 logging_writer.py:48] [12788] accumulated_eval_time=2155.878221, accumulated_logging_time=0.473520, accumulated_submission_time=4094.288564, global_step=12788, preemption_count=0, score=4094.288564, test/accuracy=0.985641, test/loss=0.048578, test/mean_average_precision=0.245978, test/num_examples=43793, total_duration=6250.981786, train/accuracy=0.990279, train/loss=0.032238, train/mean_average_precision=0.384098, validation/accuracy=0.986572, validation/loss=0.045613, validation/mean_average_precision=0.253303, validation/num_examples=43793
I0205 06:49:20.564831 140030420236032 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.024640563875436783, loss=0.038245826959609985
I0205 06:49:53.324480 140021163808512 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.019956225529313087, loss=0.038809504359960556
I0205 06:50:25.838769 140030420236032 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.019128240644931793, loss=0.041984423995018005
I0205 06:50:58.148433 140021163808512 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.017040051519870758, loss=0.04142824932932854
I0205 06:51:30.379234 140030420236032 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.024107109755277634, loss=0.03577059134840965
I0205 06:52:02.488182 140021163808512 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.014968814328312874, loss=0.03799452260136604
I0205 06:52:34.654167 140030420236032 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.021449362859129906, loss=0.04006287455558777
I0205 06:53:06.900604 140021163808512 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.01394885778427124, loss=0.035388100892305374
I0205 06:53:16.268477 140205209478976 spec.py:321] Evaluating on the training split.
I0205 06:55:11.138642 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 06:55:14.173513 140205209478976 spec.py:349] Evaluating on the test split.
I0205 06:55:17.299318 140205209478976 submission_runner.py:408] Time since start: 6612.04s, 	Step: 13530, 	{'train/accuracy': 0.9906366467475891, 'train/loss': 0.03136277198791504, 'train/mean_average_precision': 0.40051991688965427, 'validation/accuracy': 0.9865929484367371, 'validation/loss': 0.04506342485547066, 'validation/mean_average_precision': 0.24803908969799324, 'validation/num_examples': 43793, 'test/accuracy': 0.9857252836227417, 'test/loss': 0.047978173941373825, 'test/mean_average_precision': 0.24740500670378202, 'test/num_examples': 43793, 'score': 4334.27161693573, 'total_duration': 6612.0441880226135, 'accumulated_submission_time': 4334.27161693573, 'accumulated_eval_time': 2276.909010410309, 'accumulated_logging_time': 0.5021927356719971}
I0205 06:55:17.321002 140037877987072 logging_writer.py:48] [13530] accumulated_eval_time=2276.909010, accumulated_logging_time=0.502193, accumulated_submission_time=4334.271617, global_step=13530, preemption_count=0, score=4334.271617, test/accuracy=0.985725, test/loss=0.047978, test/mean_average_precision=0.247405, test/num_examples=43793, total_duration=6612.044188, train/accuracy=0.990637, train/loss=0.031363, train/mean_average_precision=0.400520, validation/accuracy=0.986593, validation/loss=0.045063, validation/mean_average_precision=0.248039, validation/num_examples=43793
I0205 06:55:40.781335 140037886379776 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.02242959477007389, loss=0.03668377548456192
I0205 06:56:13.053874 140037877987072 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.019656673073768616, loss=0.037425678223371506
I0205 06:56:46.621528 140037886379776 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0207392405718565, loss=0.035295646637678146
I0205 06:57:18.789636 140037877987072 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.01525158155709505, loss=0.03454163298010826
I0205 06:57:50.772753 140037886379776 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.01753271371126175, loss=0.037320636212825775
I0205 06:58:22.918766 140037877987072 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.016400497406721115, loss=0.034633390605449677
I0205 06:58:54.753460 140037886379776 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.01765972375869751, loss=0.03431026637554169
I0205 06:59:17.304057 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:01:06.940361 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:01:10.343311 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:01:13.749648 140205209478976 submission_runner.py:408] Time since start: 6968.49s, 	Step: 14272, 	{'train/accuracy': 0.9906654953956604, 'train/loss': 0.030894337221980095, 'train/mean_average_precision': 0.43120313625059464, 'validation/accuracy': 0.9866920113563538, 'validation/loss': 0.045240096747875214, 'validation/mean_average_precision': 0.25559889707282346, 'validation/num_examples': 43793, 'test/accuracy': 0.9857500791549683, 'test/loss': 0.0482579730451107, 'test/mean_average_precision': 0.2457903663583589, 'test/num_examples': 43793, 'score': 4574.2215077877045, 'total_duration': 6968.494510889053, 'accumulated_submission_time': 4574.2215077877045, 'accumulated_eval_time': 2393.3545455932617, 'accumulated_logging_time': 0.5375001430511475}
I0205 07:01:13.769326 140030420236032 logging_writer.py:48] [14272] accumulated_eval_time=2393.354546, accumulated_logging_time=0.537500, accumulated_submission_time=4574.221508, global_step=14272, preemption_count=0, score=4574.221508, test/accuracy=0.985750, test/loss=0.048258, test/mean_average_precision=0.245790, test/num_examples=43793, total_duration=6968.494511, train/accuracy=0.990665, train/loss=0.030894, train/mean_average_precision=0.431203, validation/accuracy=0.986692, validation/loss=0.045240, validation/mean_average_precision=0.255599, validation/num_examples=43793
I0205 07:01:23.316061 140044360042240 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.020652666687965393, loss=0.038906265050172806
I0205 07:01:55.751772 140030420236032 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.026631543412804604, loss=0.036799654364585876
I0205 07:02:28.587647 140044360042240 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.015681192278862, loss=0.033929355442523956
I0205 07:03:01.634274 140030420236032 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.017360081896185875, loss=0.038111116737127304
I0205 07:03:34.449854 140044360042240 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.018217260017991066, loss=0.04035225883126259
I0205 07:04:06.820823 140030420236032 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.024493727833032608, loss=0.036084190011024475
I0205 07:04:39.353290 140044360042240 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.02152414806187153, loss=0.03782113641500473
I0205 07:05:11.600966 140030420236032 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.021256649866700172, loss=0.03684442117810249
I0205 07:05:13.886945 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:07:04.897861 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:07:07.948221 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:07:10.945184 140205209478976 submission_runner.py:408] Time since start: 7325.69s, 	Step: 15008, 	{'train/accuracy': 0.9905092716217041, 'train/loss': 0.030773861333727837, 'train/mean_average_precision': 0.43366181483048205, 'validation/accuracy': 0.9867232441902161, 'validation/loss': 0.04589784890413284, 'validation/mean_average_precision': 0.25760386529163765, 'validation/num_examples': 43793, 'test/accuracy': 0.9857884645462036, 'test/loss': 0.0488702729344368, 'test/mean_average_precision': 0.25056729325396826, 'test/num_examples': 43793, 'score': 4814.304394006729, 'total_duration': 7325.689950466156, 'accumulated_submission_time': 4814.304394006729, 'accumulated_eval_time': 2510.4126312732697, 'accumulated_logging_time': 0.5691132545471191}
I0205 07:07:10.963074 140021163808512 logging_writer.py:48] [15008] accumulated_eval_time=2510.412631, accumulated_logging_time=0.569113, accumulated_submission_time=4814.304394, global_step=15008, preemption_count=0, score=4814.304394, test/accuracy=0.985788, test/loss=0.048870, test/mean_average_precision=0.250567, test/num_examples=43793, total_duration=7325.689950, train/accuracy=0.990509, train/loss=0.030774, train/mean_average_precision=0.433662, validation/accuracy=0.986723, validation/loss=0.045898, validation/mean_average_precision=0.257604, validation/num_examples=43793
I0205 07:07:40.806371 140037886379776 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.02555547095835209, loss=0.0406777560710907
I0205 07:08:12.858048 140021163808512 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0259245578199625, loss=0.040409285575151443
I0205 07:08:44.740995 140037886379776 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.01946984976530075, loss=0.036462701857089996
I0205 07:09:16.519063 140021163808512 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.02201632410287857, loss=0.037302251905202866
I0205 07:09:48.712325 140037886379776 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.020668240264058113, loss=0.03680235147476196
I0205 07:10:20.767113 140021163808512 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.01898348517715931, loss=0.03646508604288101
I0205 07:10:52.756656 140037886379776 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.02010711096227169, loss=0.033777281641960144
I0205 07:11:11.169611 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:13:06.399758 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:13:09.584121 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:13:12.708736 140205209478976 submission_runner.py:408] Time since start: 7687.45s, 	Step: 15758, 	{'train/accuracy': 0.9911080598831177, 'train/loss': 0.029442239552736282, 'train/mean_average_precision': 0.4476567930154197, 'validation/accuracy': 0.9865406155586243, 'validation/loss': 0.04548534378409386, 'validation/mean_average_precision': 0.25222233620115586, 'validation/num_examples': 43793, 'test/accuracy': 0.9856599569320679, 'test/loss': 0.04849335923790932, 'test/mean_average_precision': 0.24102343525518574, 'test/num_examples': 43793, 'score': 5054.479521274567, 'total_duration': 7687.453608751297, 'accumulated_submission_time': 5054.479521274567, 'accumulated_eval_time': 2631.9517204761505, 'accumulated_logging_time': 0.5978469848632812}
I0205 07:13:12.727286 140030420236032 logging_writer.py:48] [15758] accumulated_eval_time=2631.951720, accumulated_logging_time=0.597847, accumulated_submission_time=5054.479521, global_step=15758, preemption_count=0, score=5054.479521, test/accuracy=0.985660, test/loss=0.048493, test/mean_average_precision=0.241023, test/num_examples=43793, total_duration=7687.453609, train/accuracy=0.991108, train/loss=0.029442, train/mean_average_precision=0.447657, validation/accuracy=0.986541, validation/loss=0.045485, validation/mean_average_precision=0.252222, validation/num_examples=43793
I0205 07:13:26.620051 140044360042240 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.021129082888364792, loss=0.035464201122522354
I0205 07:13:58.941361 140030420236032 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.02488704025745392, loss=0.035101987421512604
I0205 07:14:30.916485 140044360042240 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.021430496126413345, loss=0.03565708547830582
I0205 07:15:02.672296 140030420236032 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.021225787699222565, loss=0.035394683480262756
I0205 07:15:34.420955 140044360042240 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.0217116866260767, loss=0.0371418222784996
I0205 07:16:06.480953 140030420236032 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.022484323009848595, loss=0.03650352358818054
I0205 07:16:38.237893 140044360042240 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.022814186289906502, loss=0.0354808010160923
I0205 07:17:10.904170 140030420236032 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.019834155216813087, loss=0.03518640249967575
I0205 07:17:12.791875 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:19:06.282210 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:19:09.342662 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:19:12.326850 140205209478976 submission_runner.py:408] Time since start: 8047.07s, 	Step: 16507, 	{'train/accuracy': 0.9912184476852417, 'train/loss': 0.028480874374508858, 'train/mean_average_precision': 0.47126331260966237, 'validation/accuracy': 0.9866733551025391, 'validation/loss': 0.04558788985013962, 'validation/mean_average_precision': 0.26045790279198716, 'validation/num_examples': 43793, 'test/accuracy': 0.98580402135849, 'test/loss': 0.048654064536094666, 'test/mean_average_precision': 0.24589251090387632, 'test/num_examples': 43793, 'score': 5294.512858390808, 'total_duration': 8047.071723461151, 'accumulated_submission_time': 5294.512858390808, 'accumulated_eval_time': 2751.486648082733, 'accumulated_logging_time': 0.627873420715332}
I0205 07:19:12.345303 140021163808512 logging_writer.py:48] [16507] accumulated_eval_time=2751.486648, accumulated_logging_time=0.627873, accumulated_submission_time=5294.512858, global_step=16507, preemption_count=0, score=5294.512858, test/accuracy=0.985804, test/loss=0.048654, test/mean_average_precision=0.245893, test/num_examples=43793, total_duration=8047.071723, train/accuracy=0.991218, train/loss=0.028481, train/mean_average_precision=0.471263, validation/accuracy=0.986673, validation/loss=0.045588, validation/mean_average_precision=0.260458, validation/num_examples=43793
I0205 07:19:42.706269 140037877987072 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.021491600200533867, loss=0.037187863141298294
I0205 07:20:14.839292 140021163808512 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.023623529821634293, loss=0.03583977371454239
I0205 07:20:46.783131 140037877987072 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.02130330540239811, loss=0.03676288202404976
I0205 07:21:19.344485 140021163808512 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.024827435612678528, loss=0.03784019872546196
I0205 07:21:51.022864 140037877987072 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.019856669008731842, loss=0.03566961735486984
I0205 07:22:22.948187 140021163808512 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.01967025175690651, loss=0.03302416950464249
I0205 07:22:54.847500 140037877987072 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.024907832965254784, loss=0.03525470942258835
I0205 07:23:12.397727 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:25:02.170874 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:25:05.220313 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:25:08.240092 140205209478976 submission_runner.py:408] Time since start: 8402.98s, 	Step: 17256, 	{'train/accuracy': 0.9917227029800415, 'train/loss': 0.027211928740143776, 'train/mean_average_precision': 0.5129358647554417, 'validation/accuracy': 0.9868016242980957, 'validation/loss': 0.04518385976552963, 'validation/mean_average_precision': 0.26616701003676746, 'validation/num_examples': 43793, 'test/accuracy': 0.9859118461608887, 'test/loss': 0.04831591621041298, 'test/mean_average_precision': 0.25409906685934003, 'test/num_examples': 43793, 'score': 5534.532721757889, 'total_duration': 8402.984964370728, 'accumulated_submission_time': 5534.532721757889, 'accumulated_eval_time': 2867.3289663791656, 'accumulated_logging_time': 0.6588001251220703}
I0205 07:25:08.258540 140030420236032 logging_writer.py:48] [17256] accumulated_eval_time=2867.328966, accumulated_logging_time=0.658800, accumulated_submission_time=5534.532722, global_step=17256, preemption_count=0, score=5534.532722, test/accuracy=0.985912, test/loss=0.048316, test/mean_average_precision=0.254099, test/num_examples=43793, total_duration=8402.984964, train/accuracy=0.991723, train/loss=0.027212, train/mean_average_precision=0.512936, validation/accuracy=0.986802, validation/loss=0.045184, validation/mean_average_precision=0.266167, validation/num_examples=43793
I0205 07:25:22.640136 140044360042240 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.023018494248390198, loss=0.03599478304386139
I0205 07:25:54.238021 140030420236032 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.028858304023742676, loss=0.040685322135686874
I0205 07:26:26.382258 140044360042240 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.0347469262778759, loss=0.03792398050427437
I0205 07:26:58.183728 140030420236032 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.02329925075173378, loss=0.03548719733953476
I0205 07:27:29.955082 140044360042240 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.021745184436440468, loss=0.03499017283320427
I0205 07:28:01.812963 140030420236032 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.020118219777941704, loss=0.034854400902986526
I0205 07:28:33.506394 140044360042240 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.023493727669119835, loss=0.03535929322242737
I0205 07:29:05.595126 140030420236032 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.033608920872211456, loss=0.03895563632249832
I0205 07:29:08.483378 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:30:57.379741 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:31:00.673895 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:31:04.047147 140205209478976 submission_runner.py:408] Time since start: 8758.79s, 	Step: 18010, 	{'train/accuracy': 0.9915904998779297, 'train/loss': 0.027231523767113686, 'train/mean_average_precision': 0.520161026037464, 'validation/accuracy': 0.9867346286773682, 'validation/loss': 0.04604444280266762, 'validation/mean_average_precision': 0.261773279133514, 'validation/num_examples': 43793, 'test/accuracy': 0.9857736825942993, 'test/loss': 0.04926221817731857, 'test/mean_average_precision': 0.24607791155525627, 'test/num_examples': 43793, 'score': 5774.725650072098, 'total_duration': 8758.792022228241, 'accumulated_submission_time': 5774.725650072098, 'accumulated_eval_time': 2982.892689228058, 'accumulated_logging_time': 0.689293384552002}
I0205 07:31:04.065718 140021163808512 logging_writer.py:48] [18010] accumulated_eval_time=2982.892689, accumulated_logging_time=0.689293, accumulated_submission_time=5774.725650, global_step=18010, preemption_count=0, score=5774.725650, test/accuracy=0.985774, test/loss=0.049262, test/mean_average_precision=0.246078, test/num_examples=43793, total_duration=8758.792022, train/accuracy=0.991590, train/loss=0.027232, train/mean_average_precision=0.520161, validation/accuracy=0.986735, validation/loss=0.046044, validation/mean_average_precision=0.261773, validation/num_examples=43793
I0205 07:31:33.529313 140037877987072 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.024592457339167595, loss=0.034449007362127304
I0205 07:32:05.342962 140021163808512 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.024097474291920662, loss=0.03312946856021881
I0205 07:32:36.751976 140037877987072 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.02271278016269207, loss=0.03636239841580391
I0205 07:33:08.542423 140021163808512 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.032393574714660645, loss=0.03388473391532898
I0205 07:33:40.580615 140037877987072 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.029643619433045387, loss=0.03585319593548775
I0205 07:34:12.454721 140021163808512 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.02490762621164322, loss=0.035534366965293884
I0205 07:34:44.254173 140037877987072 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.026614144444465637, loss=0.03753514960408211
I0205 07:35:04.146779 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:36:56.589095 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:36:59.631040 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:37:02.709067 140205209478976 submission_runner.py:408] Time since start: 9117.45s, 	Step: 18764, 	{'train/accuracy': 0.9917588233947754, 'train/loss': 0.026817476376891136, 'train/mean_average_precision': 0.5093664431630106, 'validation/accuracy': 0.9866693019866943, 'validation/loss': 0.04563991352915764, 'validation/mean_average_precision': 0.2636575558792305, 'validation/num_examples': 43793, 'test/accuracy': 0.9858528971672058, 'test/loss': 0.048856575042009354, 'test/mean_average_precision': 0.2505815096182588, 'test/num_examples': 43793, 'score': 6014.775934457779, 'total_duration': 9117.453943014145, 'accumulated_submission_time': 6014.775934457779, 'accumulated_eval_time': 3101.454934358597, 'accumulated_logging_time': 0.7188436985015869}
I0205 07:37:02.727503 140030420236032 logging_writer.py:48] [18764] accumulated_eval_time=3101.454934, accumulated_logging_time=0.718844, accumulated_submission_time=6014.775934, global_step=18764, preemption_count=0, score=6014.775934, test/accuracy=0.985853, test/loss=0.048857, test/mean_average_precision=0.250582, test/num_examples=43793, total_duration=9117.453943, train/accuracy=0.991759, train/loss=0.026817, train/mean_average_precision=0.509366, validation/accuracy=0.986669, validation/loss=0.045640, validation/mean_average_precision=0.263658, validation/num_examples=43793
I0205 07:37:14.888083 140044360042240 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.0235470961779356, loss=0.035588063299655914
I0205 07:37:46.591314 140030420236032 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.025317879393696785, loss=0.03525588661432266
I0205 07:38:18.637470 140044360042240 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.023038256913423538, loss=0.03418200835585594
I0205 07:38:50.544894 140030420236032 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.02827555686235428, loss=0.033673468977212906
I0205 07:39:22.852149 140044360042240 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.026395311579108238, loss=0.0335262231528759
I0205 07:39:54.880850 140030420236032 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.026207612827420235, loss=0.03565821796655655
I0205 07:40:26.802428 140044360042240 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.028072595596313477, loss=0.035253603011369705
I0205 07:40:58.811934 140030420236032 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.032238639891147614, loss=0.03621675446629524
I0205 07:41:02.717714 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:42:49.820993 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:42:52.837029 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:42:55.780597 140205209478976 submission_runner.py:408] Time since start: 9470.53s, 	Step: 19513, 	{'train/accuracy': 0.9917542934417725, 'train/loss': 0.027237333357334137, 'train/mean_average_precision': 0.49226029719581205, 'validation/accuracy': 0.9866904020309448, 'validation/loss': 0.045660365372896194, 'validation/mean_average_precision': 0.25972865294506914, 'validation/num_examples': 43793, 'test/accuracy': 0.9858368635177612, 'test/loss': 0.04862479120492935, 'test/mean_average_precision': 0.2516486277602932, 'test/num_examples': 43793, 'score': 6254.735512256622, 'total_duration': 9470.525463342667, 'accumulated_submission_time': 6254.735512256622, 'accumulated_eval_time': 3214.5177624225616, 'accumulated_logging_time': 0.7481474876403809}
I0205 07:42:55.799359 140021163808512 logging_writer.py:48] [19513] accumulated_eval_time=3214.517762, accumulated_logging_time=0.748147, accumulated_submission_time=6254.735512, global_step=19513, preemption_count=0, score=6254.735512, test/accuracy=0.985837, test/loss=0.048625, test/mean_average_precision=0.251649, test/num_examples=43793, total_duration=9470.525463, train/accuracy=0.991754, train/loss=0.027237, train/mean_average_precision=0.492260, validation/accuracy=0.986690, validation/loss=0.045660, validation/mean_average_precision=0.259729, validation/num_examples=43793
I0205 07:43:23.483003 140037886379776 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.023477857932448387, loss=0.0325532965362072
I0205 07:43:54.899782 140021163808512 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.023564765229821205, loss=0.033286113291978836
I0205 07:44:26.365605 140037886379776 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.023611756041646004, loss=0.0349794402718544
I0205 07:44:57.518185 140021163808512 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.0266171395778656, loss=0.03429542854428291
I0205 07:45:29.353128 140037886379776 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.02374443970620632, loss=0.03488120809197426
I0205 07:46:00.616349 140021163808512 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.025329096242785454, loss=0.03250235691666603
I0205 07:46:32.076153 140037886379776 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.028291642665863037, loss=0.034450989216566086
I0205 07:46:56.033483 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:48:45.004567 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:48:48.008745 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:48:51.083005 140205209478976 submission_runner.py:408] Time since start: 9825.83s, 	Step: 20277, 	{'train/accuracy': 0.9916300177574158, 'train/loss': 0.027217315509915352, 'train/mean_average_precision': 0.5153293861606729, 'validation/accuracy': 0.9866891503334045, 'validation/loss': 0.04640466347336769, 'validation/mean_average_precision': 0.26223917540315783, 'validation/num_examples': 43793, 'test/accuracy': 0.9858145713806152, 'test/loss': 0.04971195012331009, 'test/mean_average_precision': 0.24593857869652774, 'test/num_examples': 43793, 'score': 6494.936870098114, 'total_duration': 9825.827875614166, 'accumulated_submission_time': 6494.936870098114, 'accumulated_eval_time': 3329.5672364234924, 'accumulated_logging_time': 0.7790920734405518}
I0205 07:48:51.103363 140030420236032 logging_writer.py:48] [20277] accumulated_eval_time=3329.567236, accumulated_logging_time=0.779092, accumulated_submission_time=6494.936870, global_step=20277, preemption_count=0, score=6494.936870, test/accuracy=0.985815, test/loss=0.049712, test/mean_average_precision=0.245939, test/num_examples=43793, total_duration=9825.827876, train/accuracy=0.991630, train/loss=0.027217, train/mean_average_precision=0.515329, validation/accuracy=0.986689, validation/loss=0.046405, validation/mean_average_precision=0.262239, validation/num_examples=43793
I0205 07:48:58.794310 140044360042240 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.02822752669453621, loss=0.03423197567462921
I0205 07:49:30.393890 140030420236032 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.029156437143683434, loss=0.03737908601760864
I0205 07:50:01.749626 140044360042240 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.03133032098412514, loss=0.033674873411655426
I0205 07:50:33.366404 140030420236032 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.025532763451337814, loss=0.033124394714832306
I0205 07:51:05.268851 140044360042240 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.025075191631913185, loss=0.03395271301269531
I0205 07:51:36.835592 140030420236032 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.025497451424598694, loss=0.03290557861328125
I0205 07:52:08.507910 140044360042240 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.022336894646286964, loss=0.0310401301831007
I0205 07:52:40.149726 140030420236032 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.029052603989839554, loss=0.03494999185204506
I0205 07:52:51.367039 140205209478976 spec.py:321] Evaluating on the training split.
I0205 07:54:39.991758 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 07:54:43.001407 140205209478976 spec.py:349] Evaluating on the test split.
I0205 07:54:45.992606 140205209478976 submission_runner.py:408] Time since start: 10180.74s, 	Step: 21037, 	{'train/accuracy': 0.9919770359992981, 'train/loss': 0.026440460234880447, 'train/mean_average_precision': 0.5115002049030158, 'validation/accuracy': 0.9867467880249023, 'validation/loss': 0.04570683836936951, 'validation/mean_average_precision': 0.26095220365104393, 'validation/num_examples': 43793, 'test/accuracy': 0.9858802556991577, 'test/loss': 0.048931680619716644, 'test/mean_average_precision': 0.2508172272557681, 'test/num_examples': 43793, 'score': 6735.167747020721, 'total_duration': 10180.737467765808, 'accumulated_submission_time': 6735.167747020721, 'accumulated_eval_time': 3444.192747116089, 'accumulated_logging_time': 0.8117268085479736}
I0205 07:54:46.012285 140021163808512 logging_writer.py:48] [21037] accumulated_eval_time=3444.192747, accumulated_logging_time=0.811727, accumulated_submission_time=6735.167747, global_step=21037, preemption_count=0, score=6735.167747, test/accuracy=0.985880, test/loss=0.048932, test/mean_average_precision=0.250817, test/num_examples=43793, total_duration=10180.737468, train/accuracy=0.991977, train/loss=0.026440, train/mean_average_precision=0.511500, validation/accuracy=0.986747, validation/loss=0.045707, validation/mean_average_precision=0.260952, validation/num_examples=43793
I0205 07:55:06.365336 140037877987072 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.03224791958928108, loss=0.035416129976511
I0205 07:55:38.261817 140021163808512 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.02497091516852379, loss=0.033567897975444794
I0205 07:56:09.969478 140037877987072 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.029874563217163086, loss=0.03236062824726105
I0205 07:56:41.596147 140021163808512 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.04086781293153763, loss=0.03522047773003578
I0205 07:57:13.119344 140037877987072 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.02615826576948166, loss=0.03211892768740654
I0205 07:57:45.029933 140021163808512 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.03135334327816963, loss=0.03448936715722084
I0205 07:58:16.349819 140037877987072 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.029056185856461525, loss=0.03265207260847092
I0205 07:58:46.169223 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:00:34.994026 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:00:37.996760 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:00:40.974278 140205209478976 submission_runner.py:408] Time since start: 10535.72s, 	Step: 21796, 	{'train/accuracy': 0.9917345643043518, 'train/loss': 0.026639902964234352, 'train/mean_average_precision': 0.5187101817819291, 'validation/accuracy': 0.986717164516449, 'validation/loss': 0.04679332301020622, 'validation/mean_average_precision': 0.2639815040254276, 'validation/num_examples': 43793, 'test/accuracy': 0.9857686161994934, 'test/loss': 0.05028944090008736, 'test/mean_average_precision': 0.24699562876448253, 'test/num_examples': 43793, 'score': 6975.293369054794, 'total_duration': 10535.719145298004, 'accumulated_submission_time': 6975.293369054794, 'accumulated_eval_time': 3558.997751712799, 'accumulated_logging_time': 0.842423677444458}
I0205 08:00:40.994369 140037886379776 logging_writer.py:48] [21796] accumulated_eval_time=3558.997752, accumulated_logging_time=0.842424, accumulated_submission_time=6975.293369, global_step=21796, preemption_count=0, score=6975.293369, test/accuracy=0.985769, test/loss=0.050289, test/mean_average_precision=0.246996, test/num_examples=43793, total_duration=10535.719145, train/accuracy=0.991735, train/loss=0.026640, train/mean_average_precision=0.518710, validation/accuracy=0.986717, validation/loss=0.046793, validation/mean_average_precision=0.263982, validation/num_examples=43793
I0205 08:00:42.584801 140044360042240 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.031007099896669388, loss=0.032603610306978226
I0205 08:01:14.567281 140037886379776 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.03179364651441574, loss=0.03235732764005661
I0205 08:01:46.643669 140044360042240 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.025328777730464935, loss=0.031253281980752945
I0205 08:02:18.473596 140037886379776 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.02724028192460537, loss=0.03530304506421089
I0205 08:02:50.354858 140044360042240 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.03651537001132965, loss=0.03597365692257881
I0205 08:03:22.430131 140037886379776 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.03240807354450226, loss=0.034670211374759674
I0205 08:03:54.497234 140044360042240 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.039318300783634186, loss=0.033727727830410004
I0205 08:04:26.574421 140037886379776 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.03500484675168991, loss=0.0354171097278595
I0205 08:04:41.232562 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:06:30.144290 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:06:33.166992 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:06:36.121463 140205209478976 submission_runner.py:408] Time since start: 10890.87s, 	Step: 22547, 	{'train/accuracy': 0.9921161532402039, 'train/loss': 0.02546873688697815, 'train/mean_average_precision': 0.5342317305319013, 'validation/accuracy': 0.986757755279541, 'validation/loss': 0.046293362975120544, 'validation/mean_average_precision': 0.2647903676022655, 'validation/num_examples': 43793, 'test/accuracy': 0.9858317971229553, 'test/loss': 0.049536049365997314, 'test/mean_average_precision': 0.24832765723280317, 'test/num_examples': 43793, 'score': 7215.500328779221, 'total_duration': 10890.866336345673, 'accumulated_submission_time': 7215.500328779221, 'accumulated_eval_time': 3673.886606693268, 'accumulated_logging_time': 0.8734378814697266}
I0205 08:06:36.140913 140021163808512 logging_writer.py:48] [22547] accumulated_eval_time=3673.886607, accumulated_logging_time=0.873438, accumulated_submission_time=7215.500329, global_step=22547, preemption_count=0, score=7215.500329, test/accuracy=0.985832, test/loss=0.049536, test/mean_average_precision=0.248328, test/num_examples=43793, total_duration=10890.866336, train/accuracy=0.992116, train/loss=0.025469, train/mean_average_precision=0.534232, validation/accuracy=0.986758, validation/loss=0.046293, validation/mean_average_precision=0.264790, validation/num_examples=43793
I0205 08:06:53.526494 140030420236032 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.03514866903424263, loss=0.033257316797971725
I0205 08:07:25.572430 140021163808512 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.028273923322558403, loss=0.031958069652318954
I0205 08:07:57.584671 140030420236032 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.0315820574760437, loss=0.03492021933197975
I0205 08:08:29.498106 140021163808512 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.027233783155679703, loss=0.030164387077093124
I0205 08:09:01.545331 140030420236032 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.03319301828742027, loss=0.034945931285619736
I0205 08:09:33.895085 140021163808512 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.028477659448981285, loss=0.03073287010192871
I0205 08:10:06.155334 140030420236032 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.03632127493619919, loss=0.034850262105464935
I0205 08:10:36.200670 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:12:26.265398 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:12:29.306032 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:12:32.256707 140205209478976 submission_runner.py:408] Time since start: 11247.00s, 	Step: 23294, 	{'train/accuracy': 0.9923154711723328, 'train/loss': 0.024891646578907967, 'train/mean_average_precision': 0.5460445997356278, 'validation/accuracy': 0.9866899847984314, 'validation/loss': 0.04657692462205887, 'validation/mean_average_precision': 0.254345836560073, 'validation/num_examples': 43793, 'test/accuracy': 0.9859139323234558, 'test/loss': 0.049807824194431305, 'test/mean_average_precision': 0.24592628014287862, 'test/num_examples': 43793, 'score': 7455.528498411179, 'total_duration': 11247.001574993134, 'accumulated_submission_time': 7455.528498411179, 'accumulated_eval_time': 3789.942592382431, 'accumulated_logging_time': 0.9038915634155273}
I0205 08:12:32.276282 140037886379776 logging_writer.py:48] [23294] accumulated_eval_time=3789.942592, accumulated_logging_time=0.903892, accumulated_submission_time=7455.528498, global_step=23294, preemption_count=0, score=7455.528498, test/accuracy=0.985914, test/loss=0.049808, test/mean_average_precision=0.245926, test/num_examples=43793, total_duration=11247.001575, train/accuracy=0.992315, train/loss=0.024892, train/mean_average_precision=0.546045, validation/accuracy=0.986690, validation/loss=0.046577, validation/mean_average_precision=0.254346, validation/num_examples=43793
I0205 08:12:34.569123 140044360042240 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.03038746677339077, loss=0.03381213918328285
I0205 08:13:06.700558 140037886379776 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.029438283294439316, loss=0.03065877966582775
I0205 08:13:38.437431 140044360042240 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.034330639988183975, loss=0.03244920074939728
I0205 08:14:10.432511 140037886379776 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.031760767102241516, loss=0.031556352972984314
I0205 08:14:42.102813 140044360042240 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.032180916517972946, loss=0.03238554298877716
I0205 08:15:14.428791 140037886379776 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.03208374232053757, loss=0.03374086320400238
I0205 08:15:46.751370 140044360042240 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.029712030664086342, loss=0.032463133335113525
I0205 08:16:18.667404 140037886379776 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.042568858712911606, loss=0.03252088651061058
I0205 08:16:32.482023 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:18:21.270312 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:18:24.318720 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:18:27.330165 140205209478976 submission_runner.py:408] Time since start: 11602.08s, 	Step: 24044, 	{'train/accuracy': 0.9923921823501587, 'train/loss': 0.024166883900761604, 'train/mean_average_precision': 0.5712409322426214, 'validation/accuracy': 0.9866307377815247, 'validation/loss': 0.04743630439043045, 'validation/mean_average_precision': 0.25264398668369664, 'validation/num_examples': 43793, 'test/accuracy': 0.9857686161994934, 'test/loss': 0.05093642324209213, 'test/mean_average_precision': 0.24634275661253566, 'test/num_examples': 43793, 'score': 7695.701915502548, 'total_duration': 11602.075031757355, 'accumulated_submission_time': 7695.701915502548, 'accumulated_eval_time': 3904.7906806468964, 'accumulated_logging_time': 0.9359946250915527}
I0205 08:18:27.353418 140021163808512 logging_writer.py:48] [24044] accumulated_eval_time=3904.790681, accumulated_logging_time=0.935995, accumulated_submission_time=7695.701916, global_step=24044, preemption_count=0, score=7695.701916, test/accuracy=0.985769, test/loss=0.050936, test/mean_average_precision=0.246343, test/num_examples=43793, total_duration=11602.075032, train/accuracy=0.992392, train/loss=0.024167, train/mean_average_precision=0.571241, validation/accuracy=0.986631, validation/loss=0.047436, validation/mean_average_precision=0.252644, validation/num_examples=43793
I0205 08:18:45.563735 140037877987072 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.04564901068806648, loss=0.037938281893730164
I0205 08:19:17.505600 140021163808512 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.031058693304657936, loss=0.03251564875245094
I0205 08:19:49.371751 140037877987072 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.03596606105566025, loss=0.030914081260561943
I0205 08:20:21.152697 140021163808512 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.03727677837014198, loss=0.03478788584470749
I0205 08:20:52.687143 140037877987072 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.03405505418777466, loss=0.034419234842061996
I0205 08:21:24.461283 140021163808512 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.03397288918495178, loss=0.032647062093019485
I0205 08:21:56.266943 140037877987072 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.03483228012919426, loss=0.031866349279880524
I0205 08:22:27.474077 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:24:18.665912 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:24:21.670402 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:24:24.668284 140205209478976 submission_runner.py:408] Time since start: 11959.41s, 	Step: 24798, 	{'train/accuracy': 0.9930984377861023, 'train/loss': 0.022470610216259956, 'train/mean_average_precision': 0.5961115454126245, 'validation/accuracy': 0.9866713285446167, 'validation/loss': 0.04684531316161156, 'validation/mean_average_precision': 0.2598697709526262, 'validation/num_examples': 43793, 'test/accuracy': 0.9857513904571533, 'test/loss': 0.05028628930449486, 'test/mean_average_precision': 0.24727313250568078, 'test/num_examples': 43793, 'score': 7935.791239023209, 'total_duration': 11959.413158655167, 'accumulated_submission_time': 7935.791239023209, 'accumulated_eval_time': 4021.9848453998566, 'accumulated_logging_time': 0.9708609580993652}
I0205 08:24:24.688128 140030420236032 logging_writer.py:48] [24798] accumulated_eval_time=4021.984845, accumulated_logging_time=0.970861, accumulated_submission_time=7935.791239, global_step=24798, preemption_count=0, score=7935.791239, test/accuracy=0.985751, test/loss=0.050286, test/mean_average_precision=0.247273, test/num_examples=43793, total_duration=11959.413159, train/accuracy=0.993098, train/loss=0.022471, train/mean_average_precision=0.596112, validation/accuracy=0.986671, validation/loss=0.046845, validation/mean_average_precision=0.259870, validation/num_examples=43793
I0205 08:24:25.674551 140044360042240 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.04277907684445381, loss=0.03425268456339836
I0205 08:24:57.494315 140030420236032 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.03873826935887337, loss=0.03222353756427765
I0205 08:25:29.257801 140044360042240 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.0354626402258873, loss=0.0318024680018425
I0205 08:26:02.013399 140030420236032 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.03781570494174957, loss=0.033310700207948685
I0205 08:26:34.513703 140044360042240 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.040396157652139664, loss=0.030478810891509056
I0205 08:27:07.324049 140030420236032 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.03339384123682976, loss=0.032382406294345856
I0205 08:27:40.095422 140044360042240 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.038153275847435, loss=0.03292250633239746
I0205 08:28:12.859896 140030420236032 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.03339478000998497, loss=0.031983647495508194
I0205 08:28:24.950930 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:30:23.577027 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:30:26.913190 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:30:30.259701 140205209478976 submission_runner.py:408] Time since start: 12325.00s, 	Step: 25538, 	{'train/accuracy': 0.9933809638023376, 'train/loss': 0.021842068061232567, 'train/mean_average_precision': 0.6266066179462839, 'validation/accuracy': 0.986694872379303, 'validation/loss': 0.04692692682147026, 'validation/mean_average_precision': 0.2572042716750418, 'validation/num_examples': 43793, 'test/accuracy': 0.9857223033905029, 'test/loss': 0.05055712163448334, 'test/mean_average_precision': 0.2435503236909003, 'test/num_examples': 43793, 'score': 8176.01961183548, 'total_duration': 12325.004558086395, 'accumulated_submission_time': 8176.01961183548, 'accumulated_eval_time': 4147.293561458588, 'accumulated_logging_time': 1.0030477046966553}
I0205 08:30:30.282522 140021163808512 logging_writer.py:48] [25538] accumulated_eval_time=4147.293561, accumulated_logging_time=1.003048, accumulated_submission_time=8176.019612, global_step=25538, preemption_count=0, score=8176.019612, test/accuracy=0.985722, test/loss=0.050557, test/mean_average_precision=0.243550, test/num_examples=43793, total_duration=12325.004558, train/accuracy=0.993381, train/loss=0.021842, train/mean_average_precision=0.626607, validation/accuracy=0.986695, validation/loss=0.046927, validation/mean_average_precision=0.257204, validation/num_examples=43793
I0205 08:30:50.410546 140037877987072 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.03391548991203308, loss=0.031520549207925797
I0205 08:31:22.362254 140021163808512 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.040286045521497726, loss=0.031243713572621346
I0205 08:31:54.254284 140037877987072 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.034445684403181076, loss=0.0324888601899147
I0205 08:32:26.115832 140021163808512 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.03930825740098953, loss=0.033723797649145126
I0205 08:32:58.201730 140037877987072 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.03887402266263962, loss=0.03313566744327545
I0205 08:33:30.191495 140021163808512 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.038372598588466644, loss=0.033102504909038544
I0205 08:34:02.232442 140037877987072 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.03694368153810501, loss=0.03363816812634468
I0205 08:34:30.566320 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:36:20.192925 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:36:23.218091 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:36:26.209098 140205209478976 submission_runner.py:408] Time since start: 12680.95s, 	Step: 26291, 	{'train/accuracy': 0.9928025603294373, 'train/loss': 0.0234451312571764, 'train/mean_average_precision': 0.5761827578002454, 'validation/accuracy': 0.9866213798522949, 'validation/loss': 0.04699945077300072, 'validation/mean_average_precision': 0.2511379755020969, 'validation/num_examples': 43793, 'test/accuracy': 0.9857041835784912, 'test/loss': 0.050397105515003204, 'test/mean_average_precision': 0.2480874887484809, 'test/num_examples': 43793, 'score': 8416.271092653275, 'total_duration': 12680.953973293304, 'accumulated_submission_time': 8416.271092653275, 'accumulated_eval_time': 4262.936295509338, 'accumulated_logging_time': 1.0376760959625244}
I0205 08:36:26.229738 140030420236032 logging_writer.py:48] [26291] accumulated_eval_time=4262.936296, accumulated_logging_time=1.037676, accumulated_submission_time=8416.271093, global_step=26291, preemption_count=0, score=8416.271093, test/accuracy=0.985704, test/loss=0.050397, test/mean_average_precision=0.248087, test/num_examples=43793, total_duration=12680.953973, train/accuracy=0.992803, train/loss=0.023445, train/mean_average_precision=0.576183, validation/accuracy=0.986621, validation/loss=0.046999, validation/mean_average_precision=0.251138, validation/num_examples=43793
I0205 08:36:29.583030 140037886379776 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.041413161903619766, loss=0.03205922618508339
I0205 08:37:01.423042 140030420236032 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.045270100235939026, loss=0.03358885645866394
I0205 08:37:33.571327 140037886379776 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.04490305483341217, loss=0.0314708910882473
I0205 08:38:06.128362 140030420236032 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.040291108191013336, loss=0.02958720177412033
I0205 08:38:38.070043 140037886379776 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.03553265333175659, loss=0.03141898661851883
I0205 08:39:09.995177 140030420236032 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.03417031466960907, loss=0.029861504212021828
I0205 08:39:42.153297 140037886379776 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.04006507247686386, loss=0.032714441418647766
I0205 08:40:14.204244 140030420236032 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.04716743901371956, loss=0.03373628109693527
I0205 08:40:26.461420 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:42:20.060531 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:42:23.113672 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:42:26.120796 140205209478976 submission_runner.py:408] Time since start: 13040.87s, 	Step: 27040, 	{'train/accuracy': 0.992655873298645, 'train/loss': 0.023609139025211334, 'train/mean_average_precision': 0.5897485965947075, 'validation/accuracy': 0.9866039156913757, 'validation/loss': 0.04807821661233902, 'validation/mean_average_precision': 0.25073438539267245, 'validation/num_examples': 43793, 'test/accuracy': 0.9857812523841858, 'test/loss': 0.0514933206140995, 'test/mean_average_precision': 0.24595441759522038, 'test/num_examples': 43793, 'score': 8656.47072982788, 'total_duration': 13040.865669727325, 'accumulated_submission_time': 8656.47072982788, 'accumulated_eval_time': 4382.595624923706, 'accumulated_logging_time': 1.0706696510314941}
I0205 08:42:26.141148 140037877987072 logging_writer.py:48] [27040] accumulated_eval_time=4382.595625, accumulated_logging_time=1.070670, accumulated_submission_time=8656.470730, global_step=27040, preemption_count=0, score=8656.470730, test/accuracy=0.985781, test/loss=0.051493, test/mean_average_precision=0.245954, test/num_examples=43793, total_duration=13040.865670, train/accuracy=0.992656, train/loss=0.023609, train/mean_average_precision=0.589749, validation/accuracy=0.986604, validation/loss=0.048078, validation/mean_average_precision=0.250734, validation/num_examples=43793
I0205 08:42:45.827585 140044360042240 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.046406541019678116, loss=0.03093404695391655
I0205 08:43:17.781861 140037877987072 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.03805181756615639, loss=0.0320068821310997
I0205 08:43:49.752704 140044360042240 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.037658579647541046, loss=0.03044496849179268
I0205 08:44:21.827621 140037877987072 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.035953864455223083, loss=0.030592629685997963
I0205 08:44:53.625728 140044360042240 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.04888468235731125, loss=0.03007390722632408
I0205 08:45:25.605055 140037877987072 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.0445660836994648, loss=0.03145386278629303
I0205 08:45:57.422027 140044360042240 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.0546136312186718, loss=0.032716281712055206
I0205 08:46:26.125951 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:48:14.436935 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:48:17.448211 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:48:20.397404 140205209478976 submission_runner.py:408] Time since start: 13395.14s, 	Step: 27791, 	{'train/accuracy': 0.9928238987922668, 'train/loss': 0.023239290341734886, 'train/mean_average_precision': 0.5776398059058571, 'validation/accuracy': 0.9865588545799255, 'validation/loss': 0.047662440687417984, 'validation/mean_average_precision': 0.2508783567306901, 'validation/num_examples': 43793, 'test/accuracy': 0.9857842326164246, 'test/loss': 0.05109025165438652, 'test/mean_average_precision': 0.24584770642931192, 'test/num_examples': 43793, 'score': 8896.424266338348, 'total_duration': 13395.142271757126, 'accumulated_submission_time': 8896.424266338348, 'accumulated_eval_time': 4496.867026567459, 'accumulated_logging_time': 1.102473258972168}
I0205 08:48:20.418856 140030420236032 logging_writer.py:48] [27791] accumulated_eval_time=4496.867027, accumulated_logging_time=1.102473, accumulated_submission_time=8896.424266, global_step=27791, preemption_count=0, score=8896.424266, test/accuracy=0.985784, test/loss=0.051090, test/mean_average_precision=0.245848, test/num_examples=43793, total_duration=13395.142272, train/accuracy=0.992824, train/loss=0.023239, train/mean_average_precision=0.577640, validation/accuracy=0.986559, validation/loss=0.047662, validation/mean_average_precision=0.250878, validation/num_examples=43793
I0205 08:48:23.556862 140037886379776 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.04569939151406288, loss=0.030855536460876465
I0205 08:48:55.387176 140030420236032 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.04206503555178642, loss=0.032275766134262085
I0205 08:49:27.401526 140037886379776 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.04468826577067375, loss=0.03175821527838707
I0205 08:49:59.684061 140030420236032 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.044240228831768036, loss=0.03207610547542572
I0205 08:50:31.605796 140037886379776 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.0417947955429554, loss=0.03195307403802872
I0205 08:51:03.659080 140030420236032 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.042957887053489685, loss=0.030664021149277687
I0205 08:51:36.316554 140037886379776 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.04684922471642494, loss=0.030850622802972794
I0205 08:52:09.249359 140030420236032 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.04062599688768387, loss=0.031036362051963806
I0205 08:52:20.557995 140205209478976 spec.py:321] Evaluating on the training split.
I0205 08:54:10.155942 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 08:54:13.350545 140205209478976 spec.py:349] Evaluating on the test split.
I0205 08:54:16.410743 140205209478976 submission_runner.py:408] Time since start: 13751.16s, 	Step: 28536, 	{'train/accuracy': 0.9926972389221191, 'train/loss': 0.023307790979743004, 'train/mean_average_precision': 0.5889419244248999, 'validation/accuracy': 0.9865353107452393, 'validation/loss': 0.04850964620709419, 'validation/mean_average_precision': 0.24961728863623658, 'validation/num_examples': 43793, 'test/accuracy': 0.9857046008110046, 'test/loss': 0.052114564925432205, 'test/mean_average_precision': 0.2429519902380678, 'test/num_examples': 43793, 'score': 9136.532777786255, 'total_duration': 13751.155616044998, 'accumulated_submission_time': 9136.532777786255, 'accumulated_eval_time': 4612.7197296619415, 'accumulated_logging_time': 1.1351377964019775}
I0205 08:54:16.431200 140021163808512 logging_writer.py:48] [28536] accumulated_eval_time=4612.719730, accumulated_logging_time=1.135138, accumulated_submission_time=9136.532778, global_step=28536, preemption_count=0, score=9136.532778, test/accuracy=0.985705, test/loss=0.052115, test/mean_average_precision=0.242952, test/num_examples=43793, total_duration=13751.155616, train/accuracy=0.992697, train/loss=0.023308, train/mean_average_precision=0.588942, validation/accuracy=0.986535, validation/loss=0.048510, validation/mean_average_precision=0.249617, validation/num_examples=43793
I0205 08:54:37.449914 140037877987072 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.040138546377420425, loss=0.02975497581064701
I0205 08:55:09.360052 140021163808512 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.04306420683860779, loss=0.029948294162750244
I0205 08:55:41.215058 140037877987072 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.047540027648210526, loss=0.03162049502134323
I0205 08:56:12.988224 140021163808512 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.0491039976477623, loss=0.031533483415842056
I0205 08:56:44.725047 140037877987072 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.04576163738965988, loss=0.032054588198661804
I0205 08:57:16.658459 140021163808512 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.044006019830703735, loss=0.03091745637357235
I0205 08:57:48.675100 140037877987072 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.043589428067207336, loss=0.02900642156600952
I0205 08:58:16.518197 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:00:10.599061 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:00:13.622465 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:00:16.597162 140205209478976 submission_runner.py:408] Time since start: 14111.34s, 	Step: 29288, 	{'train/accuracy': 0.9929196834564209, 'train/loss': 0.022844480350613594, 'train/mean_average_precision': 0.5824909793466204, 'validation/accuracy': 0.9865726828575134, 'validation/loss': 0.048228729516267776, 'validation/mean_average_precision': 0.24974791715845274, 'validation/num_examples': 43793, 'test/accuracy': 0.9856772422790527, 'test/loss': 0.05170305445790291, 'test/mean_average_precision': 0.2464958534102047, 'test/num_examples': 43793, 'score': 9376.589265823364, 'total_duration': 14111.342035531998, 'accumulated_submission_time': 9376.589265823364, 'accumulated_eval_time': 4732.798652887344, 'accumulated_logging_time': 1.1663379669189453}
I0205 09:00:16.617949 140037886379776 logging_writer.py:48] [29288] accumulated_eval_time=4732.798653, accumulated_logging_time=1.166338, accumulated_submission_time=9376.589266, global_step=29288, preemption_count=0, score=9376.589266, test/accuracy=0.985677, test/loss=0.051703, test/mean_average_precision=0.246496, test/num_examples=43793, total_duration=14111.342036, train/accuracy=0.992920, train/loss=0.022844, train/mean_average_precision=0.582491, validation/accuracy=0.986573, validation/loss=0.048229, validation/mean_average_precision=0.249748, validation/num_examples=43793
I0205 09:00:20.809406 140044360042240 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.0590115450322628, loss=0.03165784850716591
I0205 09:00:53.249168 140037886379776 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.04241578280925751, loss=0.03059764951467514
I0205 09:01:25.419997 140044360042240 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.044548701494932175, loss=0.0324605330824852
I0205 09:01:57.347033 140037886379776 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.046272993087768555, loss=0.029931411147117615
I0205 09:02:29.683445 140044360042240 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.049707625061273575, loss=0.03036203421652317
I0205 09:03:01.838360 140037886379776 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.043081607669591904, loss=0.029949624091386795
I0205 09:03:33.509004 140044360042240 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.04810177907347679, loss=0.030293602496385574
I0205 09:04:05.300439 140037886379776 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.049859244376420975, loss=0.03227076306939125
I0205 09:04:16.796001 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:06:02.219058 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:06:05.256403 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:06:08.207041 140205209478976 submission_runner.py:408] Time since start: 14462.95s, 	Step: 30037, 	{'train/accuracy': 0.9931166768074036, 'train/loss': 0.022161850705742836, 'train/mean_average_precision': 0.6110780190593907, 'validation/accuracy': 0.9865093231201172, 'validation/loss': 0.048746638000011444, 'validation/mean_average_precision': 0.24755324090593728, 'validation/num_examples': 43793, 'test/accuracy': 0.9855828881263733, 'test/loss': 0.05220925435423851, 'test/mean_average_precision': 0.23989250551440108, 'test/num_examples': 43793, 'score': 9616.7351603508, 'total_duration': 14462.951917409897, 'accumulated_submission_time': 9616.7351603508, 'accumulated_eval_time': 4844.209650993347, 'accumulated_logging_time': 1.1994884014129639}
I0205 09:06:08.228076 140030420236032 logging_writer.py:48] [30037] accumulated_eval_time=4844.209651, accumulated_logging_time=1.199488, accumulated_submission_time=9616.735160, global_step=30037, preemption_count=0, score=9616.735160, test/accuracy=0.985583, test/loss=0.052209, test/mean_average_precision=0.239893, test/num_examples=43793, total_duration=14462.951917, train/accuracy=0.993117, train/loss=0.022162, train/mean_average_precision=0.611078, validation/accuracy=0.986509, validation/loss=0.048747, validation/mean_average_precision=0.247553, validation/num_examples=43793
I0205 09:06:28.597859 140037877987072 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.04884743317961693, loss=0.031220603734254837
I0205 09:07:00.198871 140030420236032 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.05730227008461952, loss=0.03172529861330986
I0205 09:07:32.207784 140037877987072 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.05093945935368538, loss=0.03247859328985214
I0205 09:08:04.200816 140030420236032 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.045062318444252014, loss=0.02924380637705326
I0205 09:08:36.205866 140037877987072 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.056658171117305756, loss=0.029392603784799576
I0205 09:09:07.987216 140030420236032 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.07089263945817947, loss=0.031474560499191284
I0205 09:09:39.625141 140037877987072 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.04751979932188988, loss=0.03163670003414154
I0205 09:10:08.244539 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:11:58.051243 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:12:01.099691 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:12:04.130422 140205209478976 submission_runner.py:408] Time since start: 14818.88s, 	Step: 30790, 	{'train/accuracy': 0.9931876063346863, 'train/loss': 0.02141193114221096, 'train/mean_average_precision': 0.6239691444803845, 'validation/accuracy': 0.9865888953208923, 'validation/loss': 0.04952388256788254, 'validation/mean_average_precision': 0.2529329067237749, 'validation/num_examples': 43793, 'test/accuracy': 0.9857505559921265, 'test/loss': 0.05304334685206413, 'test/mean_average_precision': 0.24176353729688374, 'test/num_examples': 43793, 'score': 9856.721096038818, 'total_duration': 14818.875288009644, 'accumulated_submission_time': 9856.721096038818, 'accumulated_eval_time': 4960.09548664093, 'accumulated_logging_time': 1.2316246032714844}
I0205 09:12:04.152999 140037886379776 logging_writer.py:48] [30790] accumulated_eval_time=4960.095487, accumulated_logging_time=1.231625, accumulated_submission_time=9856.721096, global_step=30790, preemption_count=0, score=9856.721096, test/accuracy=0.985751, test/loss=0.053043, test/mean_average_precision=0.241764, test/num_examples=43793, total_duration=14818.875288, train/accuracy=0.993188, train/loss=0.021412, train/mean_average_precision=0.623969, validation/accuracy=0.986589, validation/loss=0.049524, validation/mean_average_precision=0.252933, validation/num_examples=43793
I0205 09:12:07.811181 140044360042240 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.04924778267741203, loss=0.029158245772123337
I0205 09:12:40.068703 140037886379776 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.043436914682388306, loss=0.028759924694895744
I0205 09:13:12.669227 140044360042240 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.04066469147801399, loss=0.028592929244041443
I0205 09:13:45.660153 140037886379776 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.05370429903268814, loss=0.027780191972851753
I0205 09:14:17.932582 140044360042240 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.04841577634215355, loss=0.030205052345991135
I0205 09:14:50.321351 140037886379776 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.04918662831187248, loss=0.031179599463939667
I0205 09:15:22.656202 140044360042240 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.06374695897102356, loss=0.029481327161192894
I0205 09:15:55.228107 140037886379776 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.055614836513996124, loss=0.030779831111431122
I0205 09:16:04.281091 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:17:49.834018 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:17:52.894097 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:17:55.959308 140205209478976 submission_runner.py:408] Time since start: 15170.70s, 	Step: 31529, 	{'train/accuracy': 0.9937662482261658, 'train/loss': 0.020005514845252037, 'train/mean_average_precision': 0.6605400162041181, 'validation/accuracy': 0.9865211248397827, 'validation/loss': 0.04902686923742294, 'validation/mean_average_precision': 0.2478563465140137, 'validation/num_examples': 43793, 'test/accuracy': 0.9855732321739197, 'test/loss': 0.05247371643781662, 'test/mean_average_precision': 0.24339744815671674, 'test/num_examples': 43793, 'score': 10096.81805896759, 'total_duration': 15170.70418548584, 'accumulated_submission_time': 10096.81805896759, 'accumulated_eval_time': 5071.77366065979, 'accumulated_logging_time': 1.265533208847046}
I0205 09:17:55.980734 140021163808512 logging_writer.py:48] [31529] accumulated_eval_time=5071.773661, accumulated_logging_time=1.265533, accumulated_submission_time=10096.818059, global_step=31529, preemption_count=0, score=10096.818059, test/accuracy=0.985573, test/loss=0.052474, test/mean_average_precision=0.243397, test/num_examples=43793, total_duration=15170.704185, train/accuracy=0.993766, train/loss=0.020006, train/mean_average_precision=0.660540, validation/accuracy=0.986521, validation/loss=0.049027, validation/mean_average_precision=0.247856, validation/num_examples=43793
I0205 09:18:19.225774 140030420236032 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.04743301123380661, loss=0.029700959101319313
I0205 09:18:52.032256 140021163808512 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.06218048557639122, loss=0.030425462871789932
I0205 09:19:24.480542 140030420236032 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.059602607041597366, loss=0.029966287314891815
I0205 09:19:56.857726 140021163808512 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.058149516582489014, loss=0.03150647506117821
I0205 09:20:28.987700 140030420236032 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05512750521302223, loss=0.02998666651546955
I0205 09:21:01.353710 140021163808512 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.047357287257909775, loss=0.02977660670876503
I0205 09:21:33.676562 140030420236032 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.0491754375398159, loss=0.030826488509774208
I0205 09:21:56.126762 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:23:48.914041 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:23:51.958595 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:23:54.977356 140205209478976 submission_runner.py:408] Time since start: 15529.72s, 	Step: 32270, 	{'train/accuracy': 0.9942206740379333, 'train/loss': 0.018979180604219437, 'train/mean_average_precision': 0.6804974564516768, 'validation/accuracy': 0.9864314198493958, 'validation/loss': 0.049711667001247406, 'validation/mean_average_precision': 0.24315312134865424, 'validation/num_examples': 43793, 'test/accuracy': 0.9855959415435791, 'test/loss': 0.05326930806040764, 'test/mean_average_precision': 0.23170831553398583, 'test/num_examples': 43793, 'score': 10336.932120800018, 'total_duration': 15529.722115755081, 'accumulated_submission_time': 10336.932120800018, 'accumulated_eval_time': 5190.624104499817, 'accumulated_logging_time': 1.299309253692627}
I0205 09:23:54.998480 140037877987072 logging_writer.py:48] [32270] accumulated_eval_time=5190.624104, accumulated_logging_time=1.299309, accumulated_submission_time=10336.932121, global_step=32270, preemption_count=0, score=10336.932121, test/accuracy=0.985596, test/loss=0.053269, test/mean_average_precision=0.231708, test/num_examples=43793, total_duration=15529.722116, train/accuracy=0.994221, train/loss=0.018979, train/mean_average_precision=0.680497, validation/accuracy=0.986431, validation/loss=0.049712, validation/mean_average_precision=0.243153, validation/num_examples=43793
I0205 09:24:05.089414 140037886379776 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.052014317363500595, loss=0.030557041987776756
I0205 09:24:36.902378 140037877987072 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.049383487552404404, loss=0.03007395938038826
I0205 09:25:08.846725 140037886379776 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.0538131408393383, loss=0.027640381827950478
I0205 09:25:41.062459 140037877987072 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.058640819042921066, loss=0.030989985913038254
I0205 09:26:12.643646 140037886379776 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.06688647717237473, loss=0.02876398339867592
I0205 09:26:44.515427 140037877987072 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.056403886526823044, loss=0.02926134131848812
I0205 09:27:16.173811 140037886379776 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.060035619884729385, loss=0.03180719539523125
I0205 09:27:47.940509 140037877987072 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.04998411238193512, loss=0.02720491774380207
I0205 09:27:55.287476 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:29:47.725538 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:29:50.762073 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:29:53.715604 140205209478976 submission_runner.py:408] Time since start: 15888.46s, 	Step: 33024, 	{'train/accuracy': 0.9941815733909607, 'train/loss': 0.01906203106045723, 'train/mean_average_precision': 0.6768720292775423, 'validation/accuracy': 0.9864902496337891, 'validation/loss': 0.049419961869716644, 'validation/mean_average_precision': 0.24561891526677285, 'validation/num_examples': 43793, 'test/accuracy': 0.985584557056427, 'test/loss': 0.05310051143169403, 'test/mean_average_precision': 0.2379802507963694, 'test/num_examples': 43793, 'score': 10577.190371990204, 'total_duration': 15888.460463523865, 'accumulated_submission_time': 10577.190371990204, 'accumulated_eval_time': 5309.0521721839905, 'accumulated_logging_time': 1.3314814567565918}
I0205 09:29:53.736130 140021163808512 logging_writer.py:48] [33024] accumulated_eval_time=5309.052172, accumulated_logging_time=1.331481, accumulated_submission_time=10577.190372, global_step=33024, preemption_count=0, score=10577.190372, test/accuracy=0.985585, test/loss=0.053101, test/mean_average_precision=0.237980, test/num_examples=43793, total_duration=15888.460464, train/accuracy=0.994182, train/loss=0.019062, train/mean_average_precision=0.676872, validation/accuracy=0.986490, validation/loss=0.049420, validation/mean_average_precision=0.245619, validation/num_examples=43793
I0205 09:30:18.183308 140030420236032 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.05676364526152611, loss=0.02993698976933956
I0205 09:30:50.385154 140021163808512 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.06046072021126747, loss=0.029086407274007797
I0205 09:31:21.841600 140030420236032 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.05311882868409157, loss=0.029748788103461266
I0205 09:31:53.356980 140021163808512 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.05215531587600708, loss=0.02890406735241413
I0205 09:32:25.132029 140030420236032 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.06156744435429573, loss=0.027912208810448647
I0205 09:32:56.811706 140021163808512 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.05902856960892677, loss=0.030835691839456558
I0205 09:33:28.449760 140030420236032 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.05253499373793602, loss=0.02890646830201149
I0205 09:33:53.792343 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:35:44.653549 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:35:47.740511 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:35:50.762965 140205209478976 submission_runner.py:408] Time since start: 16245.51s, 	Step: 33781, 	{'train/accuracy': 0.9938918352127075, 'train/loss': 0.019619356840848923, 'train/mean_average_precision': 0.6653318432091149, 'validation/accuracy': 0.9864837527275085, 'validation/loss': 0.04993182048201561, 'validation/mean_average_precision': 0.24321026504665458, 'validation/num_examples': 43793, 'test/accuracy': 0.9855904579162598, 'test/loss': 0.05348431318998337, 'test/mean_average_precision': 0.24136566361562156, 'test/num_examples': 43793, 'score': 10817.216018676758, 'total_duration': 16245.507838010788, 'accumulated_submission_time': 10817.216018676758, 'accumulated_eval_time': 5426.022752046585, 'accumulated_logging_time': 1.362900733947754}
I0205 09:35:50.784910 140037886379776 logging_writer.py:48] [33781] accumulated_eval_time=5426.022752, accumulated_logging_time=1.362901, accumulated_submission_time=10817.216019, global_step=33781, preemption_count=0, score=10817.216019, test/accuracy=0.985590, test/loss=0.053484, test/mean_average_precision=0.241366, test/num_examples=43793, total_duration=16245.507838, train/accuracy=0.993892, train/loss=0.019619, train/mean_average_precision=0.665332, validation/accuracy=0.986484, validation/loss=0.049932, validation/mean_average_precision=0.243210, validation/num_examples=43793
I0205 09:35:57.317276 140044360042240 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.06353984028100967, loss=0.030102714896202087
I0205 09:36:29.711235 140037886379776 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.0575467124581337, loss=0.0283825621008873
I0205 09:37:01.713923 140044360042240 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.06173231825232506, loss=0.03017854131758213
I0205 09:37:33.962990 140037886379776 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.05513875558972359, loss=0.03044476918876171
I0205 09:38:05.755998 140044360042240 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.053276579827070236, loss=0.029483813792467117
I0205 09:38:37.502486 140037886379776 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.0630783662199974, loss=0.030225098133087158
I0205 09:39:09.343167 140044360042240 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.06479647755622864, loss=0.03093874081969261
I0205 09:39:41.138121 140037886379776 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.053966183215379715, loss=0.029009420424699783
I0205 09:39:50.817652 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:41:40.713577 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:41:43.823530 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:41:46.811680 140205209478976 submission_runner.py:408] Time since start: 16601.56s, 	Step: 34531, 	{'train/accuracy': 0.9931476712226868, 'train/loss': 0.02184562385082245, 'train/mean_average_precision': 0.6065809626161704, 'validation/accuracy': 0.9863153100013733, 'validation/loss': 0.05028614401817322, 'validation/mean_average_precision': 0.2356881656238881, 'validation/num_examples': 43793, 'test/accuracy': 0.9854485392570496, 'test/loss': 0.05395681411027908, 'test/mean_average_precision': 0.23406512845939162, 'test/num_examples': 43793, 'score': 11057.218078613281, 'total_duration': 16601.556545495987, 'accumulated_submission_time': 11057.218078613281, 'accumulated_eval_time': 5542.016725063324, 'accumulated_logging_time': 1.3957459926605225}
I0205 09:41:46.833301 140030420236032 logging_writer.py:48] [34531] accumulated_eval_time=5542.016725, accumulated_logging_time=1.395746, accumulated_submission_time=11057.218079, global_step=34531, preemption_count=0, score=11057.218079, test/accuracy=0.985449, test/loss=0.053957, test/mean_average_precision=0.234065, test/num_examples=43793, total_duration=16601.556545, train/accuracy=0.993148, train/loss=0.021846, train/mean_average_precision=0.606581, validation/accuracy=0.986315, validation/loss=0.050286, validation/mean_average_precision=0.235688, validation/num_examples=43793
I0205 09:42:09.270995 140037877987072 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.07289142161607742, loss=0.031307876110076904
I0205 09:42:40.770619 140030420236032 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06233201175928116, loss=0.027981247752904892
I0205 09:43:12.586055 140037877987072 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.06019645184278488, loss=0.030524585396051407
I0205 09:43:43.993390 140030420236032 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.06465739756822586, loss=0.027754582464694977
I0205 09:44:15.894706 140037877987072 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.06338942795991898, loss=0.028580188751220703
I0205 09:44:47.615919 140030420236032 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.05364159494638443, loss=0.02700503170490265
I0205 09:45:19.264593 140037877987072 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.058161113411188126, loss=0.029660362750291824
I0205 09:45:47.041401 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:47:36.007717 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:47:39.013294 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:47:41.956717 140205209478976 submission_runner.py:408] Time since start: 16956.70s, 	Step: 35289, 	{'train/accuracy': 0.9932732582092285, 'train/loss': 0.021351510658860207, 'train/mean_average_precision': 0.6102445285731928, 'validation/accuracy': 0.9863283038139343, 'validation/loss': 0.05074532330036163, 'validation/mean_average_precision': 0.24225557996907987, 'validation/num_examples': 43793, 'test/accuracy': 0.9854581952095032, 'test/loss': 0.05426078289747238, 'test/mean_average_precision': 0.23705284058685896, 'test/num_examples': 43793, 'score': 11297.394924879074, 'total_duration': 16956.7015914917, 'accumulated_submission_time': 11297.394924879074, 'accumulated_eval_time': 5656.931999921799, 'accumulated_logging_time': 1.4285671710968018}
I0205 09:47:41.978296 140021163808512 logging_writer.py:48] [35289] accumulated_eval_time=5656.932000, accumulated_logging_time=1.428567, accumulated_submission_time=11297.394925, global_step=35289, preemption_count=0, score=11297.394925, test/accuracy=0.985458, test/loss=0.054261, test/mean_average_precision=0.237053, test/num_examples=43793, total_duration=16956.701591, train/accuracy=0.993273, train/loss=0.021352, train/mean_average_precision=0.610245, validation/accuracy=0.986328, validation/loss=0.050745, validation/mean_average_precision=0.242256, validation/num_examples=43793
I0205 09:47:45.805617 140044360042240 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.059832390397787094, loss=0.02888289839029312
I0205 09:48:17.512588 140021163808512 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.059789616614580154, loss=0.028305063024163246
I0205 09:48:48.872047 140044360042240 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.05611220747232437, loss=0.02807588130235672
I0205 09:49:20.350291 140021163808512 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.08292903751134872, loss=0.02910003997385502
I0205 09:49:51.970647 140044360042240 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.06529131531715393, loss=0.02907710336148739
I0205 09:50:23.492530 140021163808512 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.08006362617015839, loss=0.02940243110060692
I0205 09:50:55.298444 140044360042240 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.06582138687372208, loss=0.027886059135198593
I0205 09:51:27.552685 140021163808512 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06458184868097305, loss=0.028615761548280716
I0205 09:51:42.233738 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:53:32.056962 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:53:35.109362 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:53:38.152834 140205209478976 submission_runner.py:408] Time since start: 17312.90s, 	Step: 36047, 	{'train/accuracy': 0.9936595559120178, 'train/loss': 0.020167946815490723, 'train/mean_average_precision': 0.6438834465263861, 'validation/accuracy': 0.9863104224205017, 'validation/loss': 0.05070577934384346, 'validation/mean_average_precision': 0.23380648597369996, 'validation/num_examples': 43793, 'test/accuracy': 0.9854379892349243, 'test/loss': 0.05435900390148163, 'test/mean_average_precision': 0.23143665965904994, 'test/num_examples': 43793, 'score': 11537.618818044662, 'total_duration': 17312.897706270218, 'accumulated_submission_time': 11537.618818044662, 'accumulated_eval_time': 5772.85106253624, 'accumulated_logging_time': 1.4614310264587402}
I0205 09:53:38.174912 140037877987072 logging_writer.py:48] [36047] accumulated_eval_time=5772.851063, accumulated_logging_time=1.461431, accumulated_submission_time=11537.618818, global_step=36047, preemption_count=0, score=11537.618818, test/accuracy=0.985438, test/loss=0.054359, test/mean_average_precision=0.231437, test/num_examples=43793, total_duration=17312.897706, train/accuracy=0.993660, train/loss=0.020168, train/mean_average_precision=0.643883, validation/accuracy=0.986310, validation/loss=0.050706, validation/mean_average_precision=0.233806, validation/num_examples=43793
I0205 09:53:56.140727 140037886379776 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.05920110270380974, loss=0.028331760317087173
I0205 09:54:27.990952 140037877987072 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.06361746042966843, loss=0.028575364500284195
I0205 09:54:59.636081 140037886379776 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.059704940766096115, loss=0.027448920533061028
I0205 09:55:31.574095 140037877987072 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.06966390460729599, loss=0.02960326336324215
I0205 09:56:03.592705 140037886379776 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.06103730946779251, loss=0.02686935104429722
I0205 09:56:35.710896 140037877987072 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.06163688376545906, loss=0.02806680090725422
I0205 09:57:07.945318 140037886379776 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.061443693935871124, loss=0.028011763468384743
I0205 09:57:38.458111 140205209478976 spec.py:321] Evaluating on the training split.
I0205 09:59:29.635919 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 09:59:32.741738 140205209478976 spec.py:349] Evaluating on the test split.
I0205 09:59:35.744633 140205209478976 submission_runner.py:408] Time since start: 17670.49s, 	Step: 36797, 	{'train/accuracy': 0.9936636686325073, 'train/loss': 0.01991063542664051, 'train/mean_average_precision': 0.6476884772247021, 'validation/accuracy': 0.9864094853401184, 'validation/loss': 0.05145861953496933, 'validation/mean_average_precision': 0.2377665407309361, 'validation/num_examples': 43793, 'test/accuracy': 0.985491931438446, 'test/loss': 0.05515115708112717, 'test/mean_average_precision': 0.23462677071407081, 'test/num_examples': 43793, 'score': 11777.46779179573, 'total_duration': 17670.489493846893, 'accumulated_submission_time': 11777.46779179573, 'accumulated_eval_time': 5890.137528896332, 'accumulated_logging_time': 1.8974733352661133}
I0205 09:59:35.767427 140030420236032 logging_writer.py:48] [36797] accumulated_eval_time=5890.137529, accumulated_logging_time=1.897473, accumulated_submission_time=11777.467792, global_step=36797, preemption_count=0, score=11777.467792, test/accuracy=0.985492, test/loss=0.055151, test/mean_average_precision=0.234627, test/num_examples=43793, total_duration=17670.489494, train/accuracy=0.993664, train/loss=0.019911, train/mean_average_precision=0.647688, validation/accuracy=0.986409, validation/loss=0.051459, validation/mean_average_precision=0.237767, validation/num_examples=43793
I0205 09:59:37.070982 140044360042240 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.06263962388038635, loss=0.0280306376516819
I0205 10:00:08.856145 140030420236032 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06329807639122009, loss=0.026415646076202393
I0205 10:00:41.124758 140044360042240 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.06402498483657837, loss=0.027479518204927444
I0205 10:01:13.346662 140030420236032 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.065858855843544, loss=0.027531098574399948
I0205 10:01:45.450337 140044360042240 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.07767672836780548, loss=0.029321983456611633
I0205 10:02:17.342940 140030420236032 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06646950542926788, loss=0.028836430981755257
I0205 10:02:49.098365 140044360042240 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06589168310165405, loss=0.02730804868042469
I0205 10:03:20.903387 140030420236032 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06545038521289825, loss=0.028797563165426254
I0205 10:03:35.979980 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:05:22.560269 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:05:25.640651 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:05:28.668512 140205209478976 submission_runner.py:408] Time since start: 18023.41s, 	Step: 37549, 	{'train/accuracy': 0.9935628175735474, 'train/loss': 0.020100276917219162, 'train/mean_average_precision': 0.6463171003589824, 'validation/accuracy': 0.9863964915275574, 'validation/loss': 0.05192934721708298, 'validation/mean_average_precision': 0.23822748553626358, 'validation/num_examples': 43793, 'test/accuracy': 0.9854986667633057, 'test/loss': 0.05559105426073074, 'test/mean_average_precision': 0.23435272812689595, 'test/num_examples': 43793, 'score': 12017.64917397499, 'total_duration': 18023.413385868073, 'accumulated_submission_time': 12017.64917397499, 'accumulated_eval_time': 6002.826015472412, 'accumulated_logging_time': 1.9313278198242188}
I0205 10:05:28.690924 140021163808512 logging_writer.py:48] [37549] accumulated_eval_time=6002.826015, accumulated_logging_time=1.931328, accumulated_submission_time=12017.649174, global_step=37549, preemption_count=0, score=12017.649174, test/accuracy=0.985499, test/loss=0.055591, test/mean_average_precision=0.234353, test/num_examples=43793, total_duration=18023.413386, train/accuracy=0.993563, train/loss=0.020100, train/mean_average_precision=0.646317, validation/accuracy=0.986396, validation/loss=0.051929, validation/mean_average_precision=0.238227, validation/num_examples=43793
I0205 10:05:45.294836 140037886379776 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.08011340349912643, loss=0.03029930405318737
I0205 10:06:17.230555 140021163808512 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.07509145140647888, loss=0.027732430025935173
I0205 10:06:49.421453 140037886379776 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.07386859506368637, loss=0.02744140289723873
I0205 10:07:21.368058 140021163808512 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.07101232558488846, loss=0.027686448767781258
I0205 10:07:53.361313 140037886379776 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.06579101085662842, loss=0.02630006894469261
I0205 10:08:25.799658 140021163808512 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.06354920566082001, loss=0.027262510731816292
I0205 10:08:57.630048 140037886379776 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.07381503283977509, loss=0.028849193826317787
I0205 10:09:28.873899 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:11:17.496826 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:11:20.535600 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:11:23.495995 140205209478976 submission_runner.py:408] Time since start: 18378.24s, 	Step: 38299, 	{'train/accuracy': 0.9944259524345398, 'train/loss': 0.01781834289431572, 'train/mean_average_precision': 0.7018950843743552, 'validation/accuracy': 0.9862747192382812, 'validation/loss': 0.051901448518037796, 'validation/mean_average_precision': 0.23619965994096087, 'validation/num_examples': 43793, 'test/accuracy': 0.9853967428207397, 'test/loss': 0.05539665371179581, 'test/mean_average_precision': 0.23333045766374247, 'test/num_examples': 43793, 'score': 12257.801238059998, 'total_duration': 18378.24086880684, 'accumulated_submission_time': 12257.801238059998, 'accumulated_eval_time': 6117.448066949844, 'accumulated_logging_time': 1.9647307395935059}
I0205 10:11:23.518285 140037877987072 logging_writer.py:48] [38299] accumulated_eval_time=6117.448067, accumulated_logging_time=1.964731, accumulated_submission_time=12257.801238, global_step=38299, preemption_count=0, score=12257.801238, test/accuracy=0.985397, test/loss=0.055397, test/mean_average_precision=0.233330, test/num_examples=43793, total_duration=18378.240869, train/accuracy=0.994426, train/loss=0.017818, train/mean_average_precision=0.701895, validation/accuracy=0.986275, validation/loss=0.051901, validation/mean_average_precision=0.236200, validation/num_examples=43793
I0205 10:11:24.189295 140044360042240 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07372467964887619, loss=0.02670207992196083
I0205 10:11:55.918401 140037877987072 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06402663141489029, loss=0.026882803067564964
I0205 10:12:27.795369 140044360042240 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.07309164106845856, loss=0.02922763116657734
I0205 10:12:59.648836 140037877987072 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.0691007599234581, loss=0.027370667085051537
I0205 10:13:31.353892 140044360042240 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.07081007212400436, loss=0.02698138728737831
I0205 10:14:03.338850 140037877987072 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.06493724882602692, loss=0.02568717673420906
I0205 10:14:35.075120 140044360042240 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06785373389720917, loss=0.02707463689148426
I0205 10:15:06.980795 140037877987072 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.08779300004243851, loss=0.03026358224451542
I0205 10:15:23.731651 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:17:14.573605 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:17:17.671756 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:17:20.676269 140205209478976 submission_runner.py:408] Time since start: 18735.42s, 	Step: 39053, 	{'train/accuracy': 0.9949982762336731, 'train/loss': 0.01652163453400135, 'train/mean_average_precision': 0.7224225479696811, 'validation/accuracy': 0.9863018989562988, 'validation/loss': 0.052019424736499786, 'validation/mean_average_precision': 0.23460739858380092, 'validation/num_examples': 43793, 'test/accuracy': 0.985359251499176, 'test/loss': 0.055601317435503006, 'test/mean_average_precision': 0.23551709508652574, 'test/num_examples': 43793, 'score': 12497.982424497604, 'total_duration': 18735.421145915985, 'accumulated_submission_time': 12497.982424497604, 'accumulated_eval_time': 6234.392640352249, 'accumulated_logging_time': 1.999298334121704}
I0205 10:17:20.699137 140030420236032 logging_writer.py:48] [39053] accumulated_eval_time=6234.392640, accumulated_logging_time=1.999298, accumulated_submission_time=12497.982424, global_step=39053, preemption_count=0, score=12497.982424, test/accuracy=0.985359, test/loss=0.055601, test/mean_average_precision=0.235517, test/num_examples=43793, total_duration=18735.421146, train/accuracy=0.994998, train/loss=0.016522, train/mean_average_precision=0.722423, validation/accuracy=0.986302, validation/loss=0.052019, validation/mean_average_precision=0.234607, validation/num_examples=43793
I0205 10:17:37.297182 140037886379776 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.06959987431764603, loss=0.029464930295944214
I0205 10:18:10.321470 140030420236032 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.06919443607330322, loss=0.026153061538934708
I0205 10:18:42.223014 140037886379776 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.07229822129011154, loss=0.026815565302968025
I0205 10:19:14.510720 140030420236032 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.0662226527929306, loss=0.02718357928097248
I0205 10:19:46.413790 140037886379776 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07108160853385925, loss=0.02792835421860218
I0205 10:20:18.868598 140030420236032 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.07368794828653336, loss=0.027067553251981735
I0205 10:20:51.593389 140037886379776 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.0722605511546135, loss=0.027698438614606857
I0205 10:21:20.828660 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:23:11.949100 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:23:15.004199 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:23:18.075030 140205209478976 submission_runner.py:408] Time since start: 19092.82s, 	Step: 39792, 	{'train/accuracy': 0.9952055215835571, 'train/loss': 0.015768634155392647, 'train/mean_average_precision': 0.7366664288057573, 'validation/accuracy': 0.9863213896751404, 'validation/loss': 0.05276699364185333, 'validation/mean_average_precision': 0.23301486214369227, 'validation/num_examples': 43793, 'test/accuracy': 0.9853891134262085, 'test/loss': 0.05658691003918648, 'test/mean_average_precision': 0.2302643002277187, 'test/num_examples': 43793, 'score': 12738.079112768173, 'total_duration': 19092.81990480423, 'accumulated_submission_time': 12738.079112768173, 'accumulated_eval_time': 6351.638965606689, 'accumulated_logging_time': 2.03450345993042}
I0205 10:23:18.097839 140021163808512 logging_writer.py:48] [39792] accumulated_eval_time=6351.638966, accumulated_logging_time=2.034503, accumulated_submission_time=12738.079113, global_step=39792, preemption_count=0, score=12738.079113, test/accuracy=0.985389, test/loss=0.056587, test/mean_average_precision=0.230264, test/num_examples=43793, total_duration=19092.819905, train/accuracy=0.995206, train/loss=0.015769, train/mean_average_precision=0.736666, validation/accuracy=0.986321, validation/loss=0.052767, validation/mean_average_precision=0.233015, validation/num_examples=43793
I0205 10:23:21.016013 140044360042240 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.0627562552690506, loss=0.02634209766983986
I0205 10:23:53.265780 140021163808512 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.07519912719726562, loss=0.027612697333097458
I0205 10:24:25.558451 140044360042240 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0643848404288292, loss=0.02712729573249817
I0205 10:24:57.360867 140021163808512 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.07208116352558136, loss=0.027567071840167046
I0205 10:25:29.314903 140044360042240 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.07395912706851959, loss=0.026426255702972412
I0205 10:26:01.916182 140021163808512 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.08484233170747757, loss=0.02881360985338688
I0205 10:26:34.022670 140044360042240 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.07019796967506409, loss=0.026707055047154427
I0205 10:27:05.964818 140021163808512 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.07614051550626755, loss=0.02681385539472103
I0205 10:27:18.160270 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:29:09.858074 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:29:13.244141 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:29:16.547834 140205209478976 submission_runner.py:408] Time since start: 19451.29s, 	Step: 40539, 	{'train/accuracy': 0.9952002763748169, 'train/loss': 0.016081351786851883, 'train/mean_average_precision': 0.7377185027103813, 'validation/accuracy': 0.9861488342285156, 'validation/loss': 0.05301316827535629, 'validation/mean_average_precision': 0.23309463438415423, 'validation/num_examples': 43793, 'test/accuracy': 0.9852867722511292, 'test/loss': 0.056485746055841446, 'test/mean_average_precision': 0.23099852632288098, 'test/num_examples': 43793, 'score': 12978.109545946121, 'total_duration': 19451.29268836975, 'accumulated_submission_time': 12978.109545946121, 'accumulated_eval_time': 6470.0264637470245, 'accumulated_logging_time': 2.069436550140381}
I0205 10:29:16.574106 140030420236032 logging_writer.py:48] [40539] accumulated_eval_time=6470.026464, accumulated_logging_time=2.069437, accumulated_submission_time=12978.109546, global_step=40539, preemption_count=0, score=12978.109546, test/accuracy=0.985287, test/loss=0.056486, test/mean_average_precision=0.230999, test/num_examples=43793, total_duration=19451.292688, train/accuracy=0.995200, train/loss=0.016081, train/mean_average_precision=0.737719, validation/accuracy=0.986149, validation/loss=0.053013, validation/mean_average_precision=0.233095, validation/num_examples=43793
I0205 10:29:36.708009 140037886379776 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.07805977016687393, loss=0.02825227752327919
I0205 10:30:09.331262 140030420236032 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.0828232690691948, loss=0.029031045734882355
I0205 10:30:41.950623 140037886379776 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.06635835021734238, loss=0.026019524782896042
I0205 10:31:14.339547 140030420236032 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.06629449874162674, loss=0.025890571996569633
I0205 10:31:46.951313 140037886379776 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.07828940451145172, loss=0.029507119208574295
I0205 10:32:19.477158 140030420236032 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07280345261096954, loss=0.02829456888139248
I0205 10:32:52.323106 140037886379776 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.08271700143814087, loss=0.02737319841980934
I0205 10:33:16.713522 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:35:10.144782 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:35:13.188473 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:35:16.261644 140205209478976 submission_runner.py:408] Time since start: 19811.01s, 	Step: 41274, 	{'train/accuracy': 0.9948239326477051, 'train/loss': 0.016791634261608124, 'train/mean_average_precision': 0.7085789565068721, 'validation/accuracy': 0.9862288236618042, 'validation/loss': 0.053134433925151825, 'validation/mean_average_precision': 0.22880845618112994, 'validation/num_examples': 43793, 'test/accuracy': 0.9853672385215759, 'test/loss': 0.05689776688814163, 'test/mean_average_precision': 0.22833339206003106, 'test/num_examples': 43793, 'score': 13218.214102506638, 'total_duration': 19811.006506443024, 'accumulated_submission_time': 13218.214102506638, 'accumulated_eval_time': 6589.574536561966, 'accumulated_logging_time': 2.107372522354126}
I0205 10:35:16.286271 140021163808512 logging_writer.py:48] [41274] accumulated_eval_time=6589.574537, accumulated_logging_time=2.107373, accumulated_submission_time=13218.214103, global_step=41274, preemption_count=0, score=13218.214103, test/accuracy=0.985367, test/loss=0.056898, test/mean_average_precision=0.228333, test/num_examples=43793, total_duration=19811.006506, train/accuracy=0.994824, train/loss=0.016792, train/mean_average_precision=0.708579, validation/accuracy=0.986229, validation/loss=0.053134, validation/mean_average_precision=0.228808, validation/num_examples=43793
I0205 10:35:25.289891 140044360042240 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.08054119348526001, loss=0.026357218623161316
I0205 10:35:57.455649 140021163808512 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.0823821872472763, loss=0.028081612661480904
I0205 10:36:29.701831 140044360042240 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.10703980922698975, loss=0.027223331853747368
I0205 10:37:01.986088 140021163808512 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.0913751944899559, loss=0.02676215209066868
I0205 10:37:34.217721 140044360042240 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.07265396416187286, loss=0.026240231469273567
I0205 10:38:06.511142 140021163808512 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.07780108600854874, loss=0.02745431661605835
I0205 10:38:38.050064 140044360042240 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.07753202319145203, loss=0.02640279196202755
I0205 10:39:09.856724 140021163808512 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.07255738973617554, loss=0.026623820886015892
I0205 10:39:16.540412 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:41:05.882282 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:41:08.932214 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:41:11.886633 140205209478976 submission_runner.py:408] Time since start: 20166.63s, 	Step: 42022, 	{'train/accuracy': 0.9945333003997803, 'train/loss': 0.01730739325284958, 'train/mean_average_precision': 0.7135553817663034, 'validation/accuracy': 0.9861322045326233, 'validation/loss': 0.05359821394085884, 'validation/mean_average_precision': 0.2313976091081589, 'validation/num_examples': 43793, 'test/accuracy': 0.9854017496109009, 'test/loss': 0.057231880724430084, 'test/mean_average_precision': 0.2287596771324323, 'test/num_examples': 43793, 'score': 13458.43649482727, 'total_duration': 20166.63150715828, 'accumulated_submission_time': 13458.43649482727, 'accumulated_eval_time': 6704.9207146167755, 'accumulated_logging_time': 2.143451690673828}
I0205 10:41:11.910684 140037877987072 logging_writer.py:48] [42022] accumulated_eval_time=6704.920715, accumulated_logging_time=2.143452, accumulated_submission_time=13458.436495, global_step=42022, preemption_count=0, score=13458.436495, test/accuracy=0.985402, test/loss=0.057232, test/mean_average_precision=0.228760, test/num_examples=43793, total_duration=20166.631507, train/accuracy=0.994533, train/loss=0.017307, train/mean_average_precision=0.713555, validation/accuracy=0.986132, validation/loss=0.053598, validation/mean_average_precision=0.231398, validation/num_examples=43793
I0205 10:41:37.005674 140037886379776 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07662168145179749, loss=0.027403492480516434
I0205 10:42:09.075773 140037877987072 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.08071981370449066, loss=0.027290206402540207
I0205 10:42:40.915004 140037886379776 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07366280257701874, loss=0.026788415387272835
I0205 10:43:13.019908 140037877987072 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.09336832910776138, loss=0.026543395593762398
I0205 10:43:44.969044 140037886379776 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.08063564449548721, loss=0.027639267966151237
I0205 10:44:16.491027 140037877987072 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.09109597653150558, loss=0.027130916714668274
I0205 10:44:48.612277 140037886379776 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.08514557033777237, loss=0.027753643691539764
I0205 10:45:12.061915 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:47:03.077500 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:47:06.130217 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:47:09.114756 140205209478976 submission_runner.py:408] Time since start: 20523.86s, 	Step: 42774, 	{'train/accuracy': 0.9939397573471069, 'train/loss': 0.018936272710561752, 'train/mean_average_precision': 0.6697946583231249, 'validation/accuracy': 0.9862284064292908, 'validation/loss': 0.054160453379154205, 'validation/mean_average_precision': 0.23365620581876412, 'validation/num_examples': 43793, 'test/accuracy': 0.9853150248527527, 'test/loss': 0.058036379516124725, 'test/mean_average_precision': 0.22792961388349214, 'test/num_examples': 43793, 'score': 13698.556680202484, 'total_duration': 20523.85961985588, 'accumulated_submission_time': 13698.556680202484, 'accumulated_eval_time': 6821.973500728607, 'accumulated_logging_time': 2.1787869930267334}
I0205 10:47:09.138102 140021163808512 logging_writer.py:48] [42774] accumulated_eval_time=6821.973501, accumulated_logging_time=2.178787, accumulated_submission_time=13698.556680, global_step=42774, preemption_count=0, score=13698.556680, test/accuracy=0.985315, test/loss=0.058036, test/mean_average_precision=0.227930, test/num_examples=43793, total_duration=20523.859620, train/accuracy=0.993940, train/loss=0.018936, train/mean_average_precision=0.669795, validation/accuracy=0.986228, validation/loss=0.054160, validation/mean_average_precision=0.233656, validation/num_examples=43793
I0205 10:47:17.737455 140044360042240 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.06371082365512848, loss=0.02426152676343918
I0205 10:47:50.214737 140021163808512 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.07728254795074463, loss=0.026599977165460587
I0205 10:48:22.021231 140044360042240 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.08785898983478546, loss=0.025602372363209724
I0205 10:48:53.820447 140021163808512 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.09545844048261642, loss=0.027683136984705925
I0205 10:49:25.760937 140044360042240 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.07617843151092529, loss=0.02684897929430008
I0205 10:49:58.431654 140021163808512 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.07760122418403625, loss=0.02541581727564335
I0205 10:50:30.987447 140044360042240 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.08038797229528427, loss=0.027461424469947815
I0205 10:51:02.685332 140021163808512 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.06712967902421951, loss=0.025218330323696136
I0205 10:51:09.403277 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:52:58.750116 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:53:01.908957 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:53:04.918188 140205209478976 submission_runner.py:408] Time since start: 20879.66s, 	Step: 43522, 	{'train/accuracy': 0.9940671324729919, 'train/loss': 0.018478265032172203, 'train/mean_average_precision': 0.6751643354404507, 'validation/accuracy': 0.9861873984336853, 'validation/loss': 0.05490073561668396, 'validation/mean_average_precision': 0.22554508688219319, 'validation/num_examples': 43793, 'test/accuracy': 0.9853845238685608, 'test/loss': 0.05850997939705849, 'test/mean_average_precision': 0.2311539778899343, 'test/num_examples': 43793, 'score': 13938.790473937988, 'total_duration': 20879.66306090355, 'accumulated_submission_time': 13938.790473937988, 'accumulated_eval_time': 6937.488364696503, 'accumulated_logging_time': 2.2132139205932617}
I0205 10:53:04.941979 140030420236032 logging_writer.py:48] [43522] accumulated_eval_time=6937.488365, accumulated_logging_time=2.213214, accumulated_submission_time=13938.790474, global_step=43522, preemption_count=0, score=13938.790474, test/accuracy=0.985385, test/loss=0.058510, test/mean_average_precision=0.231154, test/num_examples=43793, total_duration=20879.663061, train/accuracy=0.994067, train/loss=0.018478, train/mean_average_precision=0.675164, validation/accuracy=0.986187, validation/loss=0.054901, validation/mean_average_precision=0.225545, validation/num_examples=43793
I0205 10:53:30.296467 140037877987072 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.09393097460269928, loss=0.02829429879784584
I0205 10:54:01.833671 140030420236032 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.07815655320882797, loss=0.026598820462822914
I0205 10:54:33.710480 140037877987072 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.08226600289344788, loss=0.026244312524795532
I0205 10:55:05.314625 140030420236032 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.08084698766469955, loss=0.026786793023347855
I0205 10:55:37.623543 140037877987072 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.09251418709754944, loss=0.025746379047632217
I0205 10:56:09.675985 140030420236032 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.08210834115743637, loss=0.02596837654709816
I0205 10:56:41.576436 140037877987072 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.090818852186203, loss=0.026977399364113808
I0205 10:57:05.006106 140205209478976 spec.py:321] Evaluating on the training split.
I0205 10:58:52.062885 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 10:58:55.096074 140205209478976 spec.py:349] Evaluating on the test split.
I0205 10:58:58.141448 140205209478976 submission_runner.py:408] Time since start: 21232.89s, 	Step: 44274, 	{'train/accuracy': 0.9948219656944275, 'train/loss': 0.016602113842964172, 'train/mean_average_precision': 0.7204996206656971, 'validation/accuracy': 0.9860494136810303, 'validation/loss': 0.054025713354349136, 'validation/mean_average_precision': 0.22890643751743953, 'validation/num_examples': 43793, 'test/accuracy': 0.9851073622703552, 'test/loss': 0.057887982577085495, 'test/mean_average_precision': 0.22672974356732076, 'test/num_examples': 43793, 'score': 14178.823575496674, 'total_duration': 21232.88632273674, 'accumulated_submission_time': 14178.823575496674, 'accumulated_eval_time': 7050.623664855957, 'accumulated_logging_time': 2.248157262802124}
I0205 10:58:58.164893 140021163808512 logging_writer.py:48] [44274] accumulated_eval_time=7050.623665, accumulated_logging_time=2.248157, accumulated_submission_time=14178.823575, global_step=44274, preemption_count=0, score=14178.823575, test/accuracy=0.985107, test/loss=0.057888, test/mean_average_precision=0.226730, test/num_examples=43793, total_duration=21232.886323, train/accuracy=0.994822, train/loss=0.016602, train/mean_average_precision=0.720500, validation/accuracy=0.986049, validation/loss=0.054026, validation/mean_average_precision=0.228906, validation/num_examples=43793
I0205 10:59:06.803937 140037886379776 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.11239807307720184, loss=0.02813177928328514
I0205 10:59:38.681602 140021163808512 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.07286253571510315, loss=0.024612635374069214
I0205 11:00:11.049189 140037886379776 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.08441570401191711, loss=0.027646580711007118
I0205 11:00:43.143993 140021163808512 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.08502518385648727, loss=0.025964103639125824
I0205 11:01:15.251592 140037886379776 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.07335896790027618, loss=0.025932110846042633
I0205 11:01:47.365245 140021163808512 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.08276994526386261, loss=0.02735978178679943
I0205 11:02:19.540002 140037886379776 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.07956868410110474, loss=0.02566532790660858
I0205 11:02:51.764628 140021163808512 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.09306470304727554, loss=0.024780523031949997
I0205 11:02:58.162898 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:04:43.444689 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:04:46.514003 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:04:49.523313 140205209478976 submission_runner.py:408] Time since start: 21584.27s, 	Step: 45021, 	{'train/accuracy': 0.9945811033248901, 'train/loss': 0.017048701643943787, 'train/mean_average_precision': 0.7150672404338416, 'validation/accuracy': 0.9861192107200623, 'validation/loss': 0.05487704649567604, 'validation/mean_average_precision': 0.22146424657360408, 'validation/num_examples': 43793, 'test/accuracy': 0.9852181673049927, 'test/loss': 0.05893776938319206, 'test/mean_average_precision': 0.21934709262506977, 'test/num_examples': 43793, 'score': 14418.789083957672, 'total_duration': 21584.268087387085, 'accumulated_submission_time': 14418.789083957672, 'accumulated_eval_time': 7161.983935594559, 'accumulated_logging_time': 2.284372329711914}
I0205 11:04:49.547569 140030420236032 logging_writer.py:48] [45021] accumulated_eval_time=7161.983936, accumulated_logging_time=2.284372, accumulated_submission_time=14418.789084, global_step=45021, preemption_count=0, score=14418.789084, test/accuracy=0.985218, test/loss=0.058938, test/mean_average_precision=0.219347, test/num_examples=43793, total_duration=21584.268087, train/accuracy=0.994581, train/loss=0.017049, train/mean_average_precision=0.715067, validation/accuracy=0.986119, validation/loss=0.054877, validation/mean_average_precision=0.221464, validation/num_examples=43793
I0205 11:05:15.102287 140044360042240 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.08929020911455154, loss=0.024572022259235382
I0205 11:05:46.698887 140030420236032 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08611225336790085, loss=0.026980627328157425
I0205 11:06:18.264786 140044360042240 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.08390668034553528, loss=0.02504807338118553
I0205 11:06:50.281094 140030420236032 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.083633191883564, loss=0.0261175986379385
I0205 11:07:22.249147 140044360042240 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0904177874326706, loss=0.025156645104289055
I0205 11:07:54.260199 140030420236032 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.09845267236232758, loss=0.02868466265499592
I0205 11:08:26.039649 140044360042240 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.08606813102960587, loss=0.02509177289903164
I0205 11:08:49.645769 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:10:34.542596 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:10:37.610509 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:10:40.604524 140205209478976 submission_runner.py:408] Time since start: 21935.35s, 	Step: 45776, 	{'train/accuracy': 0.994462788105011, 'train/loss': 0.017135147005319595, 'train/mean_average_precision': 0.7201120524918182, 'validation/accuracy': 0.9861781001091003, 'validation/loss': 0.05562358349561691, 'validation/mean_average_precision': 0.22999685633951691, 'validation/num_examples': 43793, 'test/accuracy': 0.9852989912033081, 'test/loss': 0.05964159220457077, 'test/mean_average_precision': 0.22465961636631798, 'test/num_examples': 43793, 'score': 14658.856129646301, 'total_duration': 21935.34939599037, 'accumulated_submission_time': 14658.856129646301, 'accumulated_eval_time': 7272.9426436424255, 'accumulated_logging_time': 2.3197762966156006}
I0205 11:10:40.629739 140037877987072 logging_writer.py:48] [45776] accumulated_eval_time=7272.942644, accumulated_logging_time=2.319776, accumulated_submission_time=14658.856130, global_step=45776, preemption_count=0, score=14658.856130, test/accuracy=0.985299, test/loss=0.059642, test/mean_average_precision=0.224660, test/num_examples=43793, total_duration=21935.349396, train/accuracy=0.994463, train/loss=0.017135, train/mean_average_precision=0.720112, validation/accuracy=0.986178, validation/loss=0.055624, validation/mean_average_precision=0.229997, validation/num_examples=43793
I0205 11:10:48.597203 140037886379776 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.09292931109666824, loss=0.0254620723426342
I0205 11:11:20.378997 140037877987072 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08332781493663788, loss=0.025300605222582817
I0205 11:11:52.142930 140037886379776 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.09236913919448853, loss=0.026752032339572906
I0205 11:12:23.846161 140037877987072 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.08786042779684067, loss=0.02540699951350689
I0205 11:12:55.582691 140037886379776 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.08920224010944366, loss=0.025845857337117195
I0205 11:13:27.885649 140037877987072 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.08931253105401993, loss=0.025188468396663666
I0205 11:14:00.290785 140037886379776 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08084514737129211, loss=0.026268361136317253
I0205 11:14:32.038037 140037877987072 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.08449222147464752, loss=0.025380250066518784
I0205 11:14:40.670546 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:16:28.260608 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:16:31.265363 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:16:34.205101 140205209478976 submission_runner.py:408] Time since start: 22288.95s, 	Step: 46528, 	{'train/accuracy': 0.9965863823890686, 'train/loss': 0.012139195576310158, 'train/mean_average_precision': 0.8180281852483683, 'validation/accuracy': 0.986148476600647, 'validation/loss': 0.05636822432279587, 'validation/mean_average_precision': 0.23150599813977804, 'validation/num_examples': 43793, 'test/accuracy': 0.9852783679962158, 'test/loss': 0.06038013845682144, 'test/mean_average_precision': 0.2273424703023917, 'test/num_examples': 43793, 'score': 14898.866291761398, 'total_duration': 22288.94997239113, 'accumulated_submission_time': 14898.866291761398, 'accumulated_eval_time': 7386.477149009705, 'accumulated_logging_time': 2.3562400341033936}
I0205 11:16:34.229429 140030420236032 logging_writer.py:48] [46528] accumulated_eval_time=7386.477149, accumulated_logging_time=2.356240, accumulated_submission_time=14898.866292, global_step=46528, preemption_count=0, score=14898.866292, test/accuracy=0.985278, test/loss=0.060380, test/mean_average_precision=0.227342, test/num_examples=43793, total_duration=22288.949972, train/accuracy=0.996586, train/loss=0.012139, train/mean_average_precision=0.818028, validation/accuracy=0.986148, validation/loss=0.056368, validation/mean_average_precision=0.231506, validation/num_examples=43793
I0205 11:16:57.293429 140044360042240 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08946673572063446, loss=0.025351904332637787
I0205 11:17:29.497853 140030420236032 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.09503010660409927, loss=0.02472333423793316
I0205 11:18:01.428548 140044360042240 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.09027066826820374, loss=0.024443544447422028
I0205 11:18:33.018628 140030420236032 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.08930247277021408, loss=0.02581516094505787
I0205 11:19:05.156363 140044360042240 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.08666977286338806, loss=0.026421668007969856
I0205 11:19:37.376409 140030420236032 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.08632072061300278, loss=0.02551395632326603
I0205 11:20:09.513984 140044360042240 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.09482259303331375, loss=0.025302162393927574
I0205 11:20:34.396127 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:22:18.826805 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:22:21.858110 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:22:24.898363 140205209478976 submission_runner.py:408] Time since start: 22639.64s, 	Step: 47279, 	{'train/accuracy': 0.9965313076972961, 'train/loss': 0.012453470379114151, 'train/mean_average_precision': 0.8025178704840774, 'validation/accuracy': 0.9861151576042175, 'validation/loss': 0.05602893605828285, 'validation/mean_average_precision': 0.23137236510486978, 'validation/num_examples': 43793, 'test/accuracy': 0.9851911664009094, 'test/loss': 0.060071028769016266, 'test/mean_average_precision': 0.22063199437236372, 'test/num_examples': 43793, 'score': 15139.000858545303, 'total_duration': 22639.643125772476, 'accumulated_submission_time': 15139.000858545303, 'accumulated_eval_time': 7496.9792284965515, 'accumulated_logging_time': 2.3927478790283203}
I0205 11:22:24.922645 140037877987072 logging_writer.py:48] [47279] accumulated_eval_time=7496.979228, accumulated_logging_time=2.392748, accumulated_submission_time=15139.000859, global_step=47279, preemption_count=0, score=15139.000859, test/accuracy=0.985191, test/loss=0.060071, test/mean_average_precision=0.220632, test/num_examples=43793, total_duration=22639.643126, train/accuracy=0.996531, train/loss=0.012453, train/mean_average_precision=0.802518, validation/accuracy=0.986115, validation/loss=0.056029, validation/mean_average_precision=0.231372, validation/num_examples=43793
I0205 11:22:31.943261 140037886379776 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.08492182940244675, loss=0.02417825721204281
I0205 11:23:04.209107 140037877987072 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.0822664201259613, loss=0.022555794566869736
I0205 11:23:36.074138 140037886379776 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.0954739898443222, loss=0.024316489696502686
I0205 11:24:08.185774 140037877987072 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.09194943308830261, loss=0.02473096176981926
I0205 11:24:40.195456 140037886379776 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.09543538838624954, loss=0.02744375728070736
I0205 11:25:11.974236 140037877987072 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.09741076081991196, loss=0.025362586602568626
I0205 11:25:43.982386 140037886379776 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.09102237224578857, loss=0.024541767314076424
I0205 11:26:16.297309 140037877987072 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.09516599774360657, loss=0.024177923798561096
I0205 11:26:25.051094 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:28:14.716910 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:28:17.773354 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:28:20.753373 140205209478976 submission_runner.py:408] Time since start: 22995.50s, 	Step: 48028, 	{'train/accuracy': 0.9962067008018494, 'train/loss': 0.013107025995850563, 'train/mean_average_precision': 0.7931497384251549, 'validation/accuracy': 0.986046552658081, 'validation/loss': 0.05627189204096794, 'validation/mean_average_precision': 0.22607762340226092, 'validation/num_examples': 43793, 'test/accuracy': 0.9850408434867859, 'test/loss': 0.060180965811014175, 'test/mean_average_precision': 0.21903934471072786, 'test/num_examples': 43793, 'score': 15379.096559047699, 'total_duration': 22995.49823999405, 'accumulated_submission_time': 15379.096559047699, 'accumulated_eval_time': 7612.681452512741, 'accumulated_logging_time': 2.4296257495880127}
I0205 11:28:20.780470 140021163808512 logging_writer.py:48] [48028] accumulated_eval_time=7612.681453, accumulated_logging_time=2.429626, accumulated_submission_time=15379.096559, global_step=48028, preemption_count=0, score=15379.096559, test/accuracy=0.985041, test/loss=0.060181, test/mean_average_precision=0.219039, test/num_examples=43793, total_duration=22995.498240, train/accuracy=0.996207, train/loss=0.013107, train/mean_average_precision=0.793150, validation/accuracy=0.986047, validation/loss=0.056272, validation/mean_average_precision=0.226078, validation/num_examples=43793
I0205 11:28:44.035736 140044360042240 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.09456916898488998, loss=0.024762699380517006
I0205 11:29:16.374305 140021163808512 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.07791884988546371, loss=0.024478403851389885
I0205 11:29:49.219349 140044360042240 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.10072434693574905, loss=0.02698427066206932
I0205 11:30:21.201973 140021163808512 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.09775836765766144, loss=0.024450447410345078
I0205 11:30:53.459851 140044360042240 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.08571001142263412, loss=0.024340519681572914
I0205 11:31:25.367427 140021163808512 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.0940711721777916, loss=0.024418992921710014
I0205 11:31:57.320428 140044360042240 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.10773079842329025, loss=0.02519655041396618
I0205 11:32:20.773089 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:34:07.499809 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:34:12.237572 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:34:15.241184 140205209478976 submission_runner.py:408] Time since start: 23349.99s, 	Step: 48773, 	{'train/accuracy': 0.9955082535743713, 'train/loss': 0.014244879595935345, 'train/mean_average_precision': 0.774797287980069, 'validation/accuracy': 0.986183762550354, 'validation/loss': 0.05732876434922218, 'validation/mean_average_precision': 0.228134299896944, 'validation/num_examples': 43793, 'test/accuracy': 0.9852885007858276, 'test/loss': 0.06146378815174103, 'test/mean_average_precision': 0.22184345036840447, 'test/num_examples': 43793, 'score': 15619.057337284088, 'total_duration': 23349.986057519913, 'accumulated_submission_time': 15619.057337284088, 'accumulated_eval_time': 7727.149502515793, 'accumulated_logging_time': 2.468273878097534}
I0205 11:34:15.266103 140030420236032 logging_writer.py:48] [48773] accumulated_eval_time=7727.149503, accumulated_logging_time=2.468274, accumulated_submission_time=15619.057337, global_step=48773, preemption_count=0, score=15619.057337, test/accuracy=0.985289, test/loss=0.061464, test/mean_average_precision=0.221843, test/num_examples=43793, total_duration=23349.986058, train/accuracy=0.995508, train/loss=0.014245, train/mean_average_precision=0.774797, validation/accuracy=0.986184, validation/loss=0.057329, validation/mean_average_precision=0.228134, validation/num_examples=43793
I0205 11:34:24.364124 140037886379776 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.08097139000892639, loss=0.024618012830615044
I0205 11:34:56.748699 140030420236032 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.08692122250795364, loss=0.025237254798412323
I0205 11:35:28.532047 140037886379776 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.08829272538423538, loss=0.02414112351834774
I0205 11:36:00.211807 140030420236032 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.08107887208461761, loss=0.023599114269018173
I0205 11:36:32.080356 140037886379776 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.09313768893480301, loss=0.023359445855021477
I0205 11:37:04.109134 140030420236032 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.08575093001127243, loss=0.022909706458449364
I0205 11:37:35.861126 140037886379776 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.09760328382253647, loss=0.02443757839500904
I0205 11:38:07.560868 140030420236032 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.08817339688539505, loss=0.026209907606244087
I0205 11:38:15.529443 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:39:58.599094 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:40:01.762662 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:40:04.809095 140205209478976 submission_runner.py:408] Time since start: 23699.55s, 	Step: 49526, 	{'train/accuracy': 0.9951686859130859, 'train/loss': 0.014958654530346394, 'train/mean_average_precision': 0.7747991965325125, 'validation/accuracy': 0.9860441088676453, 'validation/loss': 0.05705243721604347, 'validation/mean_average_precision': 0.2230743490184299, 'validation/num_examples': 43793, 'test/accuracy': 0.9851848483085632, 'test/loss': 0.06115218251943588, 'test/mean_average_precision': 0.21814090271004577, 'test/num_examples': 43793, 'score': 15859.288888454437, 'total_duration': 23699.553965330124, 'accumulated_submission_time': 15859.288888454437, 'accumulated_eval_time': 7836.429103851318, 'accumulated_logging_time': 2.5045437812805176}
I0205 11:40:04.834620 140021163808512 logging_writer.py:48] [49526] accumulated_eval_time=7836.429104, accumulated_logging_time=2.504544, accumulated_submission_time=15859.288888, global_step=49526, preemption_count=0, score=15859.288888, test/accuracy=0.985185, test/loss=0.061152, test/mean_average_precision=0.218141, test/num_examples=43793, total_duration=23699.553965, train/accuracy=0.995169, train/loss=0.014959, train/mean_average_precision=0.774799, validation/accuracy=0.986044, validation/loss=0.057052, validation/mean_average_precision=0.223074, validation/num_examples=43793
I0205 11:40:29.077420 140044360042240 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.08585001528263092, loss=0.02465691789984703
I0205 11:41:01.111339 140021163808512 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.09334433823823929, loss=0.024781905114650726
I0205 11:41:33.290935 140044360042240 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08380328118801117, loss=0.022419944405555725
I0205 11:42:05.685107 140021163808512 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.088596411049366, loss=0.024801459163427353
I0205 11:42:37.676480 140044360042240 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.08588086068630219, loss=0.02416984736919403
I0205 11:43:09.708010 140021163808512 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.10521790385246277, loss=0.026277119293808937
I0205 11:43:41.735952 140044360042240 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.09022265672683716, loss=0.024137694388628006
I0205 11:44:05.022516 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:45:49.824840 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:45:53.120546 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:45:56.430603 140205209478976 submission_runner.py:408] Time since start: 24051.18s, 	Step: 50272, 	{'train/accuracy': 0.9951609969139099, 'train/loss': 0.01507300790399313, 'train/mean_average_precision': 0.7670223106612885, 'validation/accuracy': 0.986040472984314, 'validation/loss': 0.05771508067846298, 'validation/mean_average_precision': 0.22505703540184746, 'validation/num_examples': 43793, 'test/accuracy': 0.9852353930473328, 'test/loss': 0.06162497401237488, 'test/mean_average_precision': 0.22136297501708904, 'test/num_examples': 43793, 'score': 16099.445001363754, 'total_duration': 24051.175461292267, 'accumulated_submission_time': 16099.445001363754, 'accumulated_eval_time': 7947.837135314941, 'accumulated_logging_time': 2.5418262481689453}
I0205 11:45:56.458625 140037877987072 logging_writer.py:48] [50272] accumulated_eval_time=7947.837135, accumulated_logging_time=2.541826, accumulated_submission_time=16099.445001, global_step=50272, preemption_count=0, score=16099.445001, test/accuracy=0.985235, test/loss=0.061625, test/mean_average_precision=0.221363, test/num_examples=43793, total_duration=24051.175461, train/accuracy=0.995161, train/loss=0.015073, train/mean_average_precision=0.767022, validation/accuracy=0.986040, validation/loss=0.057715, validation/mean_average_precision=0.225057, validation/num_examples=43793
I0205 11:46:05.948958 140037886379776 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.10815633833408356, loss=0.0237219650298357
I0205 11:46:38.213987 140037877987072 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.09501272439956665, loss=0.025078872218728065
I0205 11:47:10.611430 140037886379776 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.09085965156555176, loss=0.02422918751835823
I0205 11:47:42.682156 140037877987072 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.09051279723644257, loss=0.024024125188589096
I0205 11:48:14.884679 140037886379776 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.08695098757743835, loss=0.022660980001091957
I0205 11:48:47.141926 140037877987072 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.09942647814750671, loss=0.026211079210042953
I0205 11:49:19.363839 140037886379776 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.09943845123052597, loss=0.024474531412124634
I0205 11:49:51.897856 140037877987072 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.09347429126501083, loss=0.02412453666329384
I0205 11:49:56.683307 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:51:38.030177 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:51:41.103797 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:51:44.102865 140205209478976 submission_runner.py:408] Time since start: 24398.85s, 	Step: 51016, 	{'train/accuracy': 0.994962215423584, 'train/loss': 0.015288949012756348, 'train/mean_average_precision': 0.7472203813139899, 'validation/accuracy': 0.9861119389533997, 'validation/loss': 0.0582580640912056, 'validation/mean_average_precision': 0.220357172150594, 'validation/num_examples': 43793, 'test/accuracy': 0.9851894974708557, 'test/loss': 0.06237699091434479, 'test/mean_average_precision': 0.21689525458278233, 'test/num_examples': 43793, 'score': 16339.63740158081, 'total_duration': 24398.847739219666, 'accumulated_submission_time': 16339.63740158081, 'accumulated_eval_time': 8055.256649494171, 'accumulated_logging_time': 2.5816922187805176}
I0205 11:51:44.128044 140030420236032 logging_writer.py:48] [51016] accumulated_eval_time=8055.256649, accumulated_logging_time=2.581692, accumulated_submission_time=16339.637402, global_step=51016, preemption_count=0, score=16339.637402, test/accuracy=0.985189, test/loss=0.062377, test/mean_average_precision=0.216895, test/num_examples=43793, total_duration=24398.847739, train/accuracy=0.994962, train/loss=0.015289, train/mean_average_precision=0.747220, validation/accuracy=0.986112, validation/loss=0.058258, validation/mean_average_precision=0.220357, validation/num_examples=43793
I0205 11:52:11.227889 140044360042240 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.09912682324647903, loss=0.024401532486081123
I0205 11:52:43.527458 140030420236032 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.09666825830936432, loss=0.02362764999270439
I0205 11:53:15.840294 140044360042240 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.07297556102275848, loss=0.0220128633081913
I0205 11:53:47.645317 140030420236032 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.10091730207204819, loss=0.024418456479907036
I0205 11:54:19.584816 140044360042240 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.08895070850849152, loss=0.024021869525313377
I0205 11:54:51.546637 140030420236032 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.09178733825683594, loss=0.023779207840561867
I0205 11:55:23.787235 140044360042240 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09534986317157745, loss=0.023251663893461227
I0205 11:55:44.369236 140205209478976 spec.py:321] Evaluating on the training split.
I0205 11:57:29.691987 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 11:57:32.709627 140205209478976 spec.py:349] Evaluating on the test split.
I0205 11:57:35.735208 140205209478976 submission_runner.py:408] Time since start: 24750.48s, 	Step: 51765, 	{'train/accuracy': 0.9956825971603394, 'train/loss': 0.01382682379335165, 'train/mean_average_precision': 0.788454380210666, 'validation/accuracy': 0.9860782027244568, 'validation/loss': 0.05799631401896477, 'validation/mean_average_precision': 0.22232651831847636, 'validation/num_examples': 43793, 'test/accuracy': 0.9851473569869995, 'test/loss': 0.06215343251824379, 'test/mean_average_precision': 0.22124616404784242, 'test/num_examples': 43793, 'score': 16579.84620285034, 'total_duration': 24750.48007750511, 'accumulated_submission_time': 16579.84620285034, 'accumulated_eval_time': 8166.622575998306, 'accumulated_logging_time': 2.6193721294403076}
I0205 11:57:35.760118 140021163808512 logging_writer.py:48] [51765] accumulated_eval_time=8166.622576, accumulated_logging_time=2.619372, accumulated_submission_time=16579.846203, global_step=51765, preemption_count=0, score=16579.846203, test/accuracy=0.985147, test/loss=0.062153, test/mean_average_precision=0.221246, test/num_examples=43793, total_duration=24750.480078, train/accuracy=0.995683, train/loss=0.013827, train/mean_average_precision=0.788454, validation/accuracy=0.986078, validation/loss=0.057996, validation/mean_average_precision=0.222327, validation/num_examples=43793
I0205 11:57:47.649499 140037877987072 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.09265751391649246, loss=0.024119481444358826
I0205 11:58:20.000917 140021163808512 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.08727417886257172, loss=0.023257529363036156
I0205 11:58:51.684528 140037877987072 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.09898717701435089, loss=0.023630427196621895
I0205 11:59:23.682801 140021163808512 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.07476430386304855, loss=0.02280672825872898
I0205 11:59:55.680155 140037877987072 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.10474273562431335, loss=0.02419220097362995
I0205 12:00:27.821127 140021163808512 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.08749336004257202, loss=0.025047674775123596
I0205 12:00:59.763025 140037877987072 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.08633822947740555, loss=0.024592651054263115
I0205 12:01:31.344408 140021163808512 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.09484528750181198, loss=0.02240646630525589
I0205 12:01:35.781073 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:03:22.573993 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:03:25.916184 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:03:29.212809 140205209478976 submission_runner.py:408] Time since start: 25103.96s, 	Step: 52515, 	{'train/accuracy': 0.9962998032569885, 'train/loss': 0.012408302165567875, 'train/mean_average_precision': 0.8206180664414136, 'validation/accuracy': 0.9861164093017578, 'validation/loss': 0.058766286820173264, 'validation/mean_average_precision': 0.2223903278764082, 'validation/num_examples': 43793, 'test/accuracy': 0.9851554036140442, 'test/loss': 0.06296781450510025, 'test/mean_average_precision': 0.21867639481387877, 'test/num_examples': 43793, 'score': 16819.83504796028, 'total_duration': 25103.957667827606, 'accumulated_submission_time': 16819.83504796028, 'accumulated_eval_time': 8280.05424952507, 'accumulated_logging_time': 2.6561763286590576}
I0205 12:03:29.242218 140030420236032 logging_writer.py:48] [52515] accumulated_eval_time=8280.054250, accumulated_logging_time=2.656176, accumulated_submission_time=16819.835048, global_step=52515, preemption_count=0, score=16819.835048, test/accuracy=0.985155, test/loss=0.062968, test/mean_average_precision=0.218676, test/num_examples=43793, total_duration=25103.957668, train/accuracy=0.996300, train/loss=0.012408, train/mean_average_precision=0.820618, validation/accuracy=0.986116, validation/loss=0.058766, validation/mean_average_precision=0.222390, validation/num_examples=43793
I0205 12:03:57.205950 140044360042240 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.13063664734363556, loss=0.024249471724033356
I0205 12:04:29.760303 140030420236032 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.09017051756381989, loss=0.022339649498462677
I0205 12:05:02.130730 140044360042240 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.11198354512453079, loss=0.02455986477434635
I0205 12:05:34.381238 140030420236032 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.09408687800168991, loss=0.0233455840498209
I0205 12:06:06.615403 140044360042240 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.10413112491369247, loss=0.02535984106361866
I0205 12:06:38.323741 140030420236032 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.0956873744726181, loss=0.023099452257156372
I0205 12:07:09.997183 140044360042240 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.08734952658414841, loss=0.022501274943351746
I0205 12:07:29.324866 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:09:16.137790 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:09:19.158819 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:09:22.132323 140205209478976 submission_runner.py:408] Time since start: 25456.88s, 	Step: 53261, 	{'train/accuracy': 0.997327983379364, 'train/loss': 0.010309528559446335, 'train/mean_average_precision': 0.8675313433747215, 'validation/accuracy': 0.9860234260559082, 'validation/loss': 0.05928371846675873, 'validation/mean_average_precision': 0.21858522312938175, 'validation/num_examples': 43793, 'test/accuracy': 0.985207200050354, 'test/loss': 0.06347201764583588, 'test/mean_average_precision': 0.21946776122638742, 'test/num_examples': 43793, 'score': 17059.884666204453, 'total_duration': 25456.877187013626, 'accumulated_submission_time': 17059.884666204453, 'accumulated_eval_time': 8392.8616604805, 'accumulated_logging_time': 2.697401523590088}
I0205 12:09:22.158853 140037877987072 logging_writer.py:48] [53261] accumulated_eval_time=8392.861660, accumulated_logging_time=2.697402, accumulated_submission_time=17059.884666, global_step=53261, preemption_count=0, score=17059.884666, test/accuracy=0.985207, test/loss=0.063472, test/mean_average_precision=0.219468, test/num_examples=43793, total_duration=25456.877187, train/accuracy=0.997328, train/loss=0.010310, train/mean_average_precision=0.867531, validation/accuracy=0.986023, validation/loss=0.059284, validation/mean_average_precision=0.218585, validation/num_examples=43793
I0205 12:09:34.896982 140037886379776 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.10369163006544113, loss=0.02211332693696022
I0205 12:10:06.553442 140037877987072 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.08633039146661758, loss=0.024228375405073166
I0205 12:10:37.770689 140037886379776 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.09254920482635498, loss=0.023478571325540543
I0205 12:11:09.832697 140037877987072 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.08939382433891296, loss=0.023295721039175987
I0205 12:11:42.027011 140037886379776 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.10024438798427582, loss=0.023103345185518265
I0205 12:12:14.115928 140037877987072 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.08543530851602554, loss=0.022358128800988197
I0205 12:12:46.144350 140037886379776 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.09748575836420059, loss=0.023640422150492668
I0205 12:13:18.377447 140037877987072 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.0910157635807991, loss=0.024011604487895966
I0205 12:13:22.219153 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:15:08.915480 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:15:11.937229 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:15:14.924140 140205209478976 submission_runner.py:408] Time since start: 25809.67s, 	Step: 54013, 	{'train/accuracy': 0.9960364103317261, 'train/loss': 0.012853307649493217, 'train/mean_average_precision': 0.8124292085607783, 'validation/accuracy': 0.9860871434211731, 'validation/loss': 0.05909451097249985, 'validation/mean_average_precision': 0.21627753684474293, 'validation/num_examples': 43793, 'test/accuracy': 0.9851477742195129, 'test/loss': 0.06346726417541504, 'test/mean_average_precision': 0.21424649439036367, 'test/num_examples': 43793, 'score': 17299.912273406982, 'total_duration': 25809.669014453888, 'accumulated_submission_time': 17299.912273406982, 'accumulated_eval_time': 8505.566602230072, 'accumulated_logging_time': 2.735142230987549}
I0205 12:15:14.949688 140021163808512 logging_writer.py:48] [54013] accumulated_eval_time=8505.566602, accumulated_logging_time=2.735142, accumulated_submission_time=17299.912273, global_step=54013, preemption_count=0, score=17299.912273, test/accuracy=0.985148, test/loss=0.063467, test/mean_average_precision=0.214246, test/num_examples=43793, total_duration=25809.669014, train/accuracy=0.996036, train/loss=0.012853, train/mean_average_precision=0.812429, validation/accuracy=0.986087, validation/loss=0.059095, validation/mean_average_precision=0.216278, validation/num_examples=43793
I0205 12:15:43.015261 140030420236032 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.08214885741472244, loss=0.02250560373067856
I0205 12:16:15.583693 140021163808512 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.0879618301987648, loss=0.02379554696381092
I0205 12:16:48.062047 140030420236032 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.09023493528366089, loss=0.0236558448523283
I0205 12:17:20.060671 140021163808512 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.09164407104253769, loss=0.02365974523127079
I0205 12:17:51.590152 140030420236032 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.08977410197257996, loss=0.021826472133398056
I0205 12:18:23.230182 140021163808512 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.09060172736644745, loss=0.021983234211802483
I0205 12:18:55.766814 140030420236032 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.09717896580696106, loss=0.02330935187637806
I0205 12:19:14.997690 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:20:55.303815 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:20:58.392400 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:21:01.414767 140205209478976 submission_runner.py:408] Time since start: 26156.16s, 	Step: 54762, 	{'train/accuracy': 0.9976819157600403, 'train/loss': 0.009660778567194939, 'train/mean_average_precision': 0.8719512232661151, 'validation/accuracy': 0.9859962463378906, 'validation/loss': 0.05957746505737305, 'validation/mean_average_precision': 0.21746900532071675, 'validation/num_examples': 43793, 'test/accuracy': 0.9850918054580688, 'test/loss': 0.06392613053321838, 'test/mean_average_precision': 0.2151637094748986, 'test/num_examples': 43793, 'score': 17539.928025960922, 'total_duration': 26156.15963792801, 'accumulated_submission_time': 17539.928025960922, 'accumulated_eval_time': 8611.9836332798, 'accumulated_logging_time': 2.773247480392456}
I0205 12:21:01.440793 140037886379776 logging_writer.py:48] [54762] accumulated_eval_time=8611.983633, accumulated_logging_time=2.773247, accumulated_submission_time=17539.928026, global_step=54762, preemption_count=0, score=17539.928026, test/accuracy=0.985092, test/loss=0.063926, test/mean_average_precision=0.215164, test/num_examples=43793, total_duration=26156.159638, train/accuracy=0.997682, train/loss=0.009661, train/mean_average_precision=0.871951, validation/accuracy=0.985996, validation/loss=0.059577, validation/mean_average_precision=0.217469, validation/num_examples=43793
I0205 12:21:13.743521 140044360042240 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.0872713252902031, loss=0.02232624776661396
I0205 12:21:45.165618 140037886379776 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.10743720084428787, loss=0.024296922609210014
I0205 12:22:16.487811 140044360042240 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.10148731619119644, loss=0.02309405244886875
I0205 12:22:47.543678 140037886379776 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.09023568779230118, loss=0.023657482117414474
I0205 12:23:19.056936 140044360042240 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.08984547853469849, loss=0.02260424755513668
I0205 12:23:50.697337 140037886379776 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.07965495437383652, loss=0.022214040160179138
I0205 12:24:22.338178 140044360042240 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.09050845354795456, loss=0.024065526202321053
I0205 12:24:54.052809 140037886379776 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.08227889239788055, loss=0.022153131663799286
I0205 12:25:01.610354 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:26:43.242198 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:26:46.324140 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:26:49.294516 140205209478976 submission_runner.py:408] Time since start: 26504.04s, 	Step: 55525, 	{'train/accuracy': 0.9970630407333374, 'train/loss': 0.010564861819148064, 'train/mean_average_precision': 0.8574635624911153, 'validation/accuracy': 0.9860855340957642, 'validation/loss': 0.06065803021192551, 'validation/mean_average_precision': 0.21881870874851647, 'validation/num_examples': 43793, 'test/accuracy': 0.9851404428482056, 'test/loss': 0.06524888426065445, 'test/mean_average_precision': 0.21419798285350508, 'test/num_examples': 43793, 'score': 17780.066515922546, 'total_duration': 26504.03938817978, 'accumulated_submission_time': 17780.066515922546, 'accumulated_eval_time': 8719.667748212814, 'accumulated_logging_time': 2.8100857734680176}
I0205 12:26:49.320183 140021163808512 logging_writer.py:48] [55525] accumulated_eval_time=8719.667748, accumulated_logging_time=2.810086, accumulated_submission_time=17780.066516, global_step=55525, preemption_count=0, score=17780.066516, test/accuracy=0.985140, test/loss=0.065249, test/mean_average_precision=0.214198, test/num_examples=43793, total_duration=26504.039388, train/accuracy=0.997063, train/loss=0.010565, train/mean_average_precision=0.857464, validation/accuracy=0.986086, validation/loss=0.060658, validation/mean_average_precision=0.218819, validation/num_examples=43793
I0205 12:27:13.711579 140037877987072 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.08975053578615189, loss=0.022832686081528664
I0205 12:27:45.561052 140021163808512 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.10113471746444702, loss=0.023415572941303253
I0205 12:28:17.732582 140037877987072 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.09118110686540604, loss=0.02172875963151455
I0205 12:28:49.319363 140021163808512 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.07684975117444992, loss=0.021634971722960472
I0205 12:29:21.289873 140037877987072 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.08302971720695496, loss=0.02170400880277157
I0205 12:29:53.262252 140021163808512 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.08019517362117767, loss=0.022726649418473244
I0205 12:30:25.285777 140037877987072 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.09797044098377228, loss=0.023069893941283226
I0205 12:30:49.360946 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:32:36.940358 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:32:39.942716 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:32:42.948705 140205209478976 submission_runner.py:408] Time since start: 26857.69s, 	Step: 56277, 	{'train/accuracy': 0.9965944290161133, 'train/loss': 0.011259051039814949, 'train/mean_average_precision': 0.8476336181469304, 'validation/accuracy': 0.9860563278198242, 'validation/loss': 0.06092185154557228, 'validation/mean_average_precision': 0.21723699273463995, 'validation/num_examples': 43793, 'test/accuracy': 0.9851536750793457, 'test/loss': 0.06518924981355667, 'test/mean_average_precision': 0.2122587202567049, 'test/num_examples': 43793, 'score': 18020.076694488525, 'total_duration': 26857.69357442856, 'accumulated_submission_time': 18020.076694488525, 'accumulated_eval_time': 8833.25545835495, 'accumulated_logging_time': 2.8469173908233643}
I0205 12:32:42.974036 140030420236032 logging_writer.py:48] [56277] accumulated_eval_time=8833.255458, accumulated_logging_time=2.846917, accumulated_submission_time=18020.076694, global_step=56277, preemption_count=0, score=18020.076694, test/accuracy=0.985154, test/loss=0.065189, test/mean_average_precision=0.212259, test/num_examples=43793, total_duration=26857.693574, train/accuracy=0.996594, train/loss=0.011259, train/mean_average_precision=0.847634, validation/accuracy=0.986056, validation/loss=0.060922, validation/mean_average_precision=0.217237, validation/num_examples=43793
I0205 12:32:50.936234 140037886379776 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.08068275451660156, loss=0.02193182148039341
I0205 12:33:23.022311 140030420236032 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.08883506059646606, loss=0.02247050032019615
I0205 12:33:54.426624 140037886379776 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.09191641956567764, loss=0.022477461025118828
I0205 12:34:26.271735 140030420236032 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.0997110977768898, loss=0.022291921079158783
I0205 12:34:58.126865 140037886379776 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.07924792170524597, loss=0.02190643548965454
I0205 12:35:30.285658 140030420236032 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.08343503624200821, loss=0.02092360332608223
I0205 12:36:02.337281 140037886379776 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.0899876058101654, loss=0.023406686261296272
I0205 12:36:34.344688 140030420236032 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.09735815227031708, loss=0.022675327956676483
I0205 12:36:43.258728 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:38:30.299729 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:38:35.913960 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:38:39.160528 140205209478976 submission_runner.py:408] Time since start: 27213.91s, 	Step: 57029, 	{'train/accuracy': 0.9959918260574341, 'train/loss': 0.012544776313006878, 'train/mean_average_precision': 0.8311303477558817, 'validation/accuracy': 0.9860153198242188, 'validation/loss': 0.06112876161932945, 'validation/mean_average_precision': 0.21309542368130246, 'validation/num_examples': 43793, 'test/accuracy': 0.985084593296051, 'test/loss': 0.06547699123620987, 'test/mean_average_precision': 0.2110058729860776, 'test/num_examples': 43793, 'score': 18260.329787254333, 'total_duration': 27213.905391216278, 'accumulated_submission_time': 18260.329787254333, 'accumulated_eval_time': 8949.157200813293, 'accumulated_logging_time': 2.883453845977783}
I0205 12:38:39.190698 140037877987072 logging_writer.py:48] [57029] accumulated_eval_time=8949.157201, accumulated_logging_time=2.883454, accumulated_submission_time=18260.329787, global_step=57029, preemption_count=0, score=18260.329787, test/accuracy=0.985085, test/loss=0.065477, test/mean_average_precision=0.211006, test/num_examples=43793, total_duration=27213.905391, train/accuracy=0.995992, train/loss=0.012545, train/mean_average_precision=0.831130, validation/accuracy=0.986015, validation/loss=0.061129, validation/mean_average_precision=0.213095, validation/num_examples=43793
I0205 12:39:02.361541 140044360042240 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.0888737142086029, loss=0.020733574405312538
I0205 12:39:34.568885 140037877987072 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.08832412958145142, loss=0.02242201939225197
I0205 12:40:07.019533 140044360042240 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.08431222289800644, loss=0.021461933851242065
I0205 12:40:39.401187 140037877987072 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.08832638710737228, loss=0.021466515958309174
I0205 12:41:11.650352 140044360042240 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.07895165681838989, loss=0.023493073880672455
I0205 12:41:43.812824 140037877987072 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.08559872210025787, loss=0.02065690979361534
I0205 12:42:15.994588 140044360042240 logging_writer.py:48] [57700] global_step=57700, preemption_count=0, score=18477.080608
I0205 12:42:16.053817 140205209478976 checkpoints.py:490] Saving checkpoint at step: 57700
I0205 12:42:16.193334 140205209478976 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_2/checkpoint_57700
I0205 12:42:16.194458 140205209478976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_2/checkpoint_57700.
I0205 12:42:16.398288 140205209478976 submission_runner.py:583] Tuning trial 2/5
I0205 12:42:16.398569 140205209478976 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0205 12:42:16.407838 140205209478976 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5092054009437561, 'train/loss': 0.7392170429229736, 'train/mean_average_precision': 0.02240319433195738, 'validation/accuracy': 0.5064647793769836, 'validation/loss': 0.7422076463699341, 'validation/mean_average_precision': 0.025301665127186908, 'validation/num_examples': 43793, 'test/accuracy': 0.5047284960746765, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.02696451860565442, 'test/num_examples': 43793, 'score': 12.25728178024292, 'total_duration': 132.69875383377075, 'accumulated_submission_time': 12.25728178024292, 'accumulated_eval_time': 120.44142413139343, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (740, {'train/accuracy': 0.9868019819259644, 'train/loss': 0.06725268810987473, 'train/mean_average_precision': 0.037721288701160274, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07590807974338531, 'validation/mean_average_precision': 0.03840245958394749, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07886271178722382, 'test/mean_average_precision': 0.041103354055522685, 'test/num_examples': 43793, 'score': 252.33542895317078, 'total_duration': 494.3655803203583, 'accumulated_submission_time': 252.33542895317078, 'accumulated_eval_time': 241.9868643283844, 'accumulated_logging_time': 0.0215451717376709, 'global_step': 740, 'preemption_count': 0}), (1486, {'train/accuracy': 0.9867600202560425, 'train/loss': 0.05239196494221687, 'train/mean_average_precision': 0.07542910055882224, 'validation/accuracy': 0.9841382503509521, 'validation/loss': 0.06244411692023277, 'validation/mean_average_precision': 0.07567169206821818, 'validation/num_examples': 43793, 'test/accuracy': 0.9831559658050537, 'test/loss': 0.06585729122161865, 'test/mean_average_precision': 0.07697097941510322, 'test/num_examples': 43793, 'score': 492.3489384651184, 'total_duration': 856.7501437664032, 'accumulated_submission_time': 492.3489384651184, 'accumulated_eval_time': 364.3091459274292, 'accumulated_logging_time': 0.049912452697753906, 'global_step': 1486, 'preemption_count': 0}), (2240, {'train/accuracy': 0.9872581958770752, 'train/loss': 0.045945290476083755, 'train/mean_average_precision': 0.1198747155496232, 'validation/accuracy': 0.9846627116203308, 'validation/loss': 0.0551317036151886, 'validation/mean_average_precision': 0.11798906748669641, 'validation/num_examples': 43793, 'test/accuracy': 0.9836491942405701, 'test/loss': 0.05832909047603607, 'test/mean_average_precision': 0.11707584962004593, 'test/num_examples': 43793, 'score': 732.587010383606, 'total_duration': 1218.8280143737793, 'accumulated_submission_time': 732.587010383606, 'accumulated_eval_time': 486.101655960083, 'accumulated_logging_time': 0.07718324661254883, 'global_step': 2240, 'preemption_count': 0}), (2992, {'train/accuracy': 0.9874991774559021, 'train/loss': 0.04441913962364197, 'train/mean_average_precision': 0.14990756717097473, 'validation/accuracy': 0.984927773475647, 'validation/loss': 0.0537693053483963, 'validation/mean_average_precision': 0.13893334565505383, 'validation/num_examples': 43793, 'test/accuracy': 0.9839326739311218, 'test/loss': 0.056828781962394714, 'test/mean_average_precision': 0.14079963923587263, 'test/num_examples': 43793, 'score': 972.7253069877625, 'total_duration': 1581.3003692626953, 'accumulated_submission_time': 972.7253069877625, 'accumulated_eval_time': 608.3868906497955, 'accumulated_logging_time': 0.10598874092102051, 'global_step': 2992, 'preemption_count': 0}), (3750, {'train/accuracy': 0.9879691004753113, 'train/loss': 0.04257238283753395, 'train/mean_average_precision': 0.1707366148038464, 'validation/accuracy': 0.985083281993866, 'validation/loss': 0.05229465290904045, 'validation/mean_average_precision': 0.15167389527715167, 'validation/num_examples': 43793, 'test/accuracy': 0.984113335609436, 'test/loss': 0.05512439087033272, 'test/mean_average_precision': 0.1527203936176814, 'test/num_examples': 43793, 'score': 1212.699630022049, 'total_duration': 1945.7496001720428, 'accumulated_submission_time': 1212.699630022049, 'accumulated_eval_time': 732.8152990341187, 'accumulated_logging_time': 0.1326751708984375, 'global_step': 3750, 'preemption_count': 0}), (4505, {'train/accuracy': 0.9882915616035461, 'train/loss': 0.04109819233417511, 'train/mean_average_precision': 0.19363046177628512, 'validation/accuracy': 0.9853597283363342, 'validation/loss': 0.05075986683368683, 'validation/mean_average_precision': 0.1696635347704308, 'validation/num_examples': 43793, 'test/accuracy': 0.9843845963478088, 'test/loss': 0.05361567437648773, 'test/mean_average_precision': 0.16837953043101717, 'test/num_examples': 43793, 'score': 1452.8758084774017, 'total_duration': 2305.887991666794, 'accumulated_submission_time': 1452.8758084774017, 'accumulated_eval_time': 852.7306742668152, 'accumulated_logging_time': 0.15938496589660645, 'global_step': 4505, 'preemption_count': 0}), (5253, {'train/accuracy': 0.9885271787643433, 'train/loss': 0.04024849832057953, 'train/mean_average_precision': 0.21536606923817142, 'validation/accuracy': 0.985521674156189, 'validation/loss': 0.049516282975673676, 'validation/mean_average_precision': 0.17605855944202745, 'validation/num_examples': 43793, 'test/accuracy': 0.9847253561019897, 'test/loss': 0.051736604422330856, 'test/mean_average_precision': 0.1818915693414284, 'test/num_examples': 43793, 'score': 1693.1669998168945, 'total_duration': 2663.9131495952606, 'accumulated_submission_time': 1693.1669998168945, 'accumulated_eval_time': 970.4168586730957, 'accumulated_logging_time': 0.18683815002441406, 'global_step': 5253, 'preemption_count': 0}), (6001, {'train/accuracy': 0.9886388182640076, 'train/loss': 0.03877423331141472, 'train/mean_average_precision': 0.2430892577206733, 'validation/accuracy': 0.9857465624809265, 'validation/loss': 0.04839358106255531, 'validation/mean_average_precision': 0.20521819519884538, 'validation/num_examples': 43793, 'test/accuracy': 0.9848209619522095, 'test/loss': 0.05105356499552727, 'test/mean_average_precision': 0.2013832889900116, 'test/num_examples': 43793, 'score': 1933.3945820331573, 'total_duration': 3023.0380108356476, 'accumulated_submission_time': 1933.3945820331573, 'accumulated_eval_time': 1089.2654836177826, 'accumulated_logging_time': 0.2155897617340088, 'global_step': 6001, 'preemption_count': 0}), (6758, {'train/accuracy': 0.9886777997016907, 'train/loss': 0.03834982588887215, 'train/mean_average_precision': 0.2587568848428609, 'validation/accuracy': 0.9859300255775452, 'validation/loss': 0.047791145741939545, 'validation/mean_average_precision': 0.21494410461051575, 'validation/num_examples': 43793, 'test/accuracy': 0.985062301158905, 'test/loss': 0.05047466233372688, 'test/mean_average_precision': 0.21457699082593304, 'test/num_examples': 43793, 'score': 2173.5919332504272, 'total_duration': 3379.749230146408, 'accumulated_submission_time': 2173.5919332504272, 'accumulated_eval_time': 1205.7322103977203, 'accumulated_logging_time': 0.24257278442382812, 'global_step': 6758, 'preemption_count': 0}), (7517, {'train/accuracy': 0.9893007874488831, 'train/loss': 0.036982662975788116, 'train/mean_average_precision': 0.27333394809486294, 'validation/accuracy': 0.9862178564071655, 'validation/loss': 0.04694636911153793, 'validation/mean_average_precision': 0.2218923235563141, 'validation/num_examples': 43793, 'test/accuracy': 0.9852644801139832, 'test/loss': 0.04949888214468956, 'test/mean_average_precision': 0.22878059163496006, 'test/num_examples': 43793, 'score': 2413.561702489853, 'total_duration': 3739.935199022293, 'accumulated_submission_time': 2413.561702489853, 'accumulated_eval_time': 1325.9005455970764, 'accumulated_logging_time': 0.27036023139953613, 'global_step': 7517, 'preemption_count': 0}), (8271, {'train/accuracy': 0.9892852902412415, 'train/loss': 0.036062296479940414, 'train/mean_average_precision': 0.3039532837698702, 'validation/accuracy': 0.9862227439880371, 'validation/loss': 0.04686782509088516, 'validation/mean_average_precision': 0.23472995254406653, 'validation/num_examples': 43793, 'test/accuracy': 0.9852551817893982, 'test/loss': 0.049763064831495285, 'test/mean_average_precision': 0.2241731692471199, 'test/num_examples': 43793, 'score': 2653.61355137825, 'total_duration': 4102.255522012711, 'accumulated_submission_time': 2653.61355137825, 'accumulated_eval_time': 1448.121309518814, 'accumulated_logging_time': 0.2982769012451172, 'global_step': 8271, 'preemption_count': 0}), (9022, {'train/accuracy': 0.989479660987854, 'train/loss': 0.03526661917567253, 'train/mean_average_precision': 0.31980141372316095, 'validation/accuracy': 0.9863124489784241, 'validation/loss': 0.04625304415822029, 'validation/mean_average_precision': 0.2304908775967825, 'validation/num_examples': 43793, 'test/accuracy': 0.9854481220245361, 'test/loss': 0.04905175045132637, 'test/mean_average_precision': 0.23790814244357286, 'test/num_examples': 43793, 'score': 2893.6486990451813, 'total_duration': 4458.192250013351, 'accumulated_submission_time': 2893.6486990451813, 'accumulated_eval_time': 1563.973773241043, 'accumulated_logging_time': 0.32762908935546875, 'global_step': 9022, 'preemption_count': 0}), (9774, {'train/accuracy': 0.9896937608718872, 'train/loss': 0.034345462918281555, 'train/mean_average_precision': 0.3432716140602922, 'validation/accuracy': 0.9864041805267334, 'validation/loss': 0.04587381333112717, 'validation/mean_average_precision': 0.2375307320474216, 'validation/num_examples': 43793, 'test/accuracy': 0.9855083227157593, 'test/loss': 0.048625972121953964, 'test/mean_average_precision': 0.23808249701418813, 'test/num_examples': 43793, 'score': 3133.746441602707, 'total_duration': 4815.885853528976, 'accumulated_submission_time': 3133.746441602707, 'accumulated_eval_time': 1681.5201325416565, 'accumulated_logging_time': 0.35713958740234375, 'global_step': 9774, 'preemption_count': 0}), (10525, {'train/accuracy': 0.9900869131088257, 'train/loss': 0.03330640122294426, 'train/mean_average_precision': 0.3712344743367008, 'validation/accuracy': 0.9865828156471252, 'validation/loss': 0.045364007353782654, 'validation/mean_average_precision': 0.24889151383711278, 'validation/num_examples': 43793, 'test/accuracy': 0.9856384992599487, 'test/loss': 0.04809035733342171, 'test/mean_average_precision': 0.24709253057784347, 'test/num_examples': 43793, 'score': 3373.892038345337, 'total_duration': 5172.309501647949, 'accumulated_submission_time': 3373.892038345337, 'accumulated_eval_time': 1797.7493011951447, 'accumulated_logging_time': 0.3862593173980713, 'global_step': 10525, 'preemption_count': 0}), (11281, {'train/accuracy': 0.9902627468109131, 'train/loss': 0.03286798298358917, 'train/mean_average_precision': 0.36300384868811686, 'validation/accuracy': 0.986607551574707, 'validation/loss': 0.045536480844020844, 'validation/mean_average_precision': 0.24634402796851296, 'validation/num_examples': 43793, 'test/accuracy': 0.9856376647949219, 'test/loss': 0.0483180433511734, 'test/mean_average_precision': 0.2483078179636221, 'test/num_examples': 43793, 'score': 3614.0496232509613, 'total_duration': 5532.550810575485, 'accumulated_submission_time': 3614.0496232509613, 'accumulated_eval_time': 1917.783516407013, 'accumulated_logging_time': 0.4158000946044922, 'global_step': 11281, 'preemption_count': 0}), (12033, {'train/accuracy': 0.9904043674468994, 'train/loss': 0.03233281522989273, 'train/mean_average_precision': 0.3812291308726584, 'validation/accuracy': 0.9865823984146118, 'validation/loss': 0.04554494842886925, 'validation/mean_average_precision': 0.2456691085848675, 'validation/num_examples': 43793, 'test/accuracy': 0.9856178760528564, 'test/loss': 0.04854458197951317, 'test/mean_average_precision': 0.24102209608237854, 'test/num_examples': 43793, 'score': 3854.0442173480988, 'total_duration': 5892.653676509857, 'accumulated_submission_time': 3854.0442173480988, 'accumulated_eval_time': 2037.8436410427094, 'accumulated_logging_time': 0.4441356658935547, 'global_step': 12033, 'preemption_count': 0}), (12788, {'train/accuracy': 0.9902794361114502, 'train/loss': 0.0322381928563118, 'train/mean_average_precision': 0.3840976298833426, 'validation/accuracy': 0.986572265625, 'validation/loss': 0.045612987130880356, 'validation/mean_average_precision': 0.2533034074588097, 'validation/num_examples': 43793, 'test/accuracy': 0.9856410026550293, 'test/loss': 0.04857763648033142, 'test/mean_average_precision': 0.24597810309914628, 'test/num_examples': 43793, 'score': 4094.2885637283325, 'total_duration': 6250.9817860126495, 'accumulated_submission_time': 4094.2885637283325, 'accumulated_eval_time': 2155.8782205581665, 'accumulated_logging_time': 0.47352027893066406, 'global_step': 12788, 'preemption_count': 0}), (13530, {'train/accuracy': 0.9906366467475891, 'train/loss': 0.03136277198791504, 'train/mean_average_precision': 0.40051991688965427, 'validation/accuracy': 0.9865929484367371, 'validation/loss': 0.04506342485547066, 'validation/mean_average_precision': 0.24803908969799324, 'validation/num_examples': 43793, 'test/accuracy': 0.9857252836227417, 'test/loss': 0.047978173941373825, 'test/mean_average_precision': 0.24740500670378202, 'test/num_examples': 43793, 'score': 4334.27161693573, 'total_duration': 6612.0441880226135, 'accumulated_submission_time': 4334.27161693573, 'accumulated_eval_time': 2276.909010410309, 'accumulated_logging_time': 0.5021927356719971, 'global_step': 13530, 'preemption_count': 0}), (14272, {'train/accuracy': 0.9906654953956604, 'train/loss': 0.030894337221980095, 'train/mean_average_precision': 0.43120313625059464, 'validation/accuracy': 0.9866920113563538, 'validation/loss': 0.045240096747875214, 'validation/mean_average_precision': 0.25559889707282346, 'validation/num_examples': 43793, 'test/accuracy': 0.9857500791549683, 'test/loss': 0.0482579730451107, 'test/mean_average_precision': 0.2457903663583589, 'test/num_examples': 43793, 'score': 4574.2215077877045, 'total_duration': 6968.494510889053, 'accumulated_submission_time': 4574.2215077877045, 'accumulated_eval_time': 2393.3545455932617, 'accumulated_logging_time': 0.5375001430511475, 'global_step': 14272, 'preemption_count': 0}), (15008, {'train/accuracy': 0.9905092716217041, 'train/loss': 0.030773861333727837, 'train/mean_average_precision': 0.43366181483048205, 'validation/accuracy': 0.9867232441902161, 'validation/loss': 0.04589784890413284, 'validation/mean_average_precision': 0.25760386529163765, 'validation/num_examples': 43793, 'test/accuracy': 0.9857884645462036, 'test/loss': 0.0488702729344368, 'test/mean_average_precision': 0.25056729325396826, 'test/num_examples': 43793, 'score': 4814.304394006729, 'total_duration': 7325.689950466156, 'accumulated_submission_time': 4814.304394006729, 'accumulated_eval_time': 2510.4126312732697, 'accumulated_logging_time': 0.5691132545471191, 'global_step': 15008, 'preemption_count': 0}), (15758, {'train/accuracy': 0.9911080598831177, 'train/loss': 0.029442239552736282, 'train/mean_average_precision': 0.4476567930154197, 'validation/accuracy': 0.9865406155586243, 'validation/loss': 0.04548534378409386, 'validation/mean_average_precision': 0.25222233620115586, 'validation/num_examples': 43793, 'test/accuracy': 0.9856599569320679, 'test/loss': 0.04849335923790932, 'test/mean_average_precision': 0.24102343525518574, 'test/num_examples': 43793, 'score': 5054.479521274567, 'total_duration': 7687.453608751297, 'accumulated_submission_time': 5054.479521274567, 'accumulated_eval_time': 2631.9517204761505, 'accumulated_logging_time': 0.5978469848632812, 'global_step': 15758, 'preemption_count': 0}), (16507, {'train/accuracy': 0.9912184476852417, 'train/loss': 0.028480874374508858, 'train/mean_average_precision': 0.47126331260966237, 'validation/accuracy': 0.9866733551025391, 'validation/loss': 0.04558788985013962, 'validation/mean_average_precision': 0.26045790279198716, 'validation/num_examples': 43793, 'test/accuracy': 0.98580402135849, 'test/loss': 0.048654064536094666, 'test/mean_average_precision': 0.24589251090387632, 'test/num_examples': 43793, 'score': 5294.512858390808, 'total_duration': 8047.071723461151, 'accumulated_submission_time': 5294.512858390808, 'accumulated_eval_time': 2751.486648082733, 'accumulated_logging_time': 0.627873420715332, 'global_step': 16507, 'preemption_count': 0}), (17256, {'train/accuracy': 0.9917227029800415, 'train/loss': 0.027211928740143776, 'train/mean_average_precision': 0.5129358647554417, 'validation/accuracy': 0.9868016242980957, 'validation/loss': 0.04518385976552963, 'validation/mean_average_precision': 0.26616701003676746, 'validation/num_examples': 43793, 'test/accuracy': 0.9859118461608887, 'test/loss': 0.04831591621041298, 'test/mean_average_precision': 0.25409906685934003, 'test/num_examples': 43793, 'score': 5534.532721757889, 'total_duration': 8402.984964370728, 'accumulated_submission_time': 5534.532721757889, 'accumulated_eval_time': 2867.3289663791656, 'accumulated_logging_time': 0.6588001251220703, 'global_step': 17256, 'preemption_count': 0}), (18010, {'train/accuracy': 0.9915904998779297, 'train/loss': 0.027231523767113686, 'train/mean_average_precision': 0.520161026037464, 'validation/accuracy': 0.9867346286773682, 'validation/loss': 0.04604444280266762, 'validation/mean_average_precision': 0.261773279133514, 'validation/num_examples': 43793, 'test/accuracy': 0.9857736825942993, 'test/loss': 0.04926221817731857, 'test/mean_average_precision': 0.24607791155525627, 'test/num_examples': 43793, 'score': 5774.725650072098, 'total_duration': 8758.792022228241, 'accumulated_submission_time': 5774.725650072098, 'accumulated_eval_time': 2982.892689228058, 'accumulated_logging_time': 0.689293384552002, 'global_step': 18010, 'preemption_count': 0}), (18764, {'train/accuracy': 0.9917588233947754, 'train/loss': 0.026817476376891136, 'train/mean_average_precision': 0.5093664431630106, 'validation/accuracy': 0.9866693019866943, 'validation/loss': 0.04563991352915764, 'validation/mean_average_precision': 0.2636575558792305, 'validation/num_examples': 43793, 'test/accuracy': 0.9858528971672058, 'test/loss': 0.048856575042009354, 'test/mean_average_precision': 0.2505815096182588, 'test/num_examples': 43793, 'score': 6014.775934457779, 'total_duration': 9117.453943014145, 'accumulated_submission_time': 6014.775934457779, 'accumulated_eval_time': 3101.454934358597, 'accumulated_logging_time': 0.7188436985015869, 'global_step': 18764, 'preemption_count': 0}), (19513, {'train/accuracy': 0.9917542934417725, 'train/loss': 0.027237333357334137, 'train/mean_average_precision': 0.49226029719581205, 'validation/accuracy': 0.9866904020309448, 'validation/loss': 0.045660365372896194, 'validation/mean_average_precision': 0.25972865294506914, 'validation/num_examples': 43793, 'test/accuracy': 0.9858368635177612, 'test/loss': 0.04862479120492935, 'test/mean_average_precision': 0.2516486277602932, 'test/num_examples': 43793, 'score': 6254.735512256622, 'total_duration': 9470.525463342667, 'accumulated_submission_time': 6254.735512256622, 'accumulated_eval_time': 3214.5177624225616, 'accumulated_logging_time': 0.7481474876403809, 'global_step': 19513, 'preemption_count': 0}), (20277, {'train/accuracy': 0.9916300177574158, 'train/loss': 0.027217315509915352, 'train/mean_average_precision': 0.5153293861606729, 'validation/accuracy': 0.9866891503334045, 'validation/loss': 0.04640466347336769, 'validation/mean_average_precision': 0.26223917540315783, 'validation/num_examples': 43793, 'test/accuracy': 0.9858145713806152, 'test/loss': 0.04971195012331009, 'test/mean_average_precision': 0.24593857869652774, 'test/num_examples': 43793, 'score': 6494.936870098114, 'total_duration': 9825.827875614166, 'accumulated_submission_time': 6494.936870098114, 'accumulated_eval_time': 3329.5672364234924, 'accumulated_logging_time': 0.7790920734405518, 'global_step': 20277, 'preemption_count': 0}), (21037, {'train/accuracy': 0.9919770359992981, 'train/loss': 0.026440460234880447, 'train/mean_average_precision': 0.5115002049030158, 'validation/accuracy': 0.9867467880249023, 'validation/loss': 0.04570683836936951, 'validation/mean_average_precision': 0.26095220365104393, 'validation/num_examples': 43793, 'test/accuracy': 0.9858802556991577, 'test/loss': 0.048931680619716644, 'test/mean_average_precision': 0.2508172272557681, 'test/num_examples': 43793, 'score': 6735.167747020721, 'total_duration': 10180.737467765808, 'accumulated_submission_time': 6735.167747020721, 'accumulated_eval_time': 3444.192747116089, 'accumulated_logging_time': 0.8117268085479736, 'global_step': 21037, 'preemption_count': 0}), (21796, {'train/accuracy': 0.9917345643043518, 'train/loss': 0.026639902964234352, 'train/mean_average_precision': 0.5187101817819291, 'validation/accuracy': 0.986717164516449, 'validation/loss': 0.04679332301020622, 'validation/mean_average_precision': 0.2639815040254276, 'validation/num_examples': 43793, 'test/accuracy': 0.9857686161994934, 'test/loss': 0.05028944090008736, 'test/mean_average_precision': 0.24699562876448253, 'test/num_examples': 43793, 'score': 6975.293369054794, 'total_duration': 10535.719145298004, 'accumulated_submission_time': 6975.293369054794, 'accumulated_eval_time': 3558.997751712799, 'accumulated_logging_time': 0.842423677444458, 'global_step': 21796, 'preemption_count': 0}), (22547, {'train/accuracy': 0.9921161532402039, 'train/loss': 0.02546873688697815, 'train/mean_average_precision': 0.5342317305319013, 'validation/accuracy': 0.986757755279541, 'validation/loss': 0.046293362975120544, 'validation/mean_average_precision': 0.2647903676022655, 'validation/num_examples': 43793, 'test/accuracy': 0.9858317971229553, 'test/loss': 0.049536049365997314, 'test/mean_average_precision': 0.24832765723280317, 'test/num_examples': 43793, 'score': 7215.500328779221, 'total_duration': 10890.866336345673, 'accumulated_submission_time': 7215.500328779221, 'accumulated_eval_time': 3673.886606693268, 'accumulated_logging_time': 0.8734378814697266, 'global_step': 22547, 'preemption_count': 0}), (23294, {'train/accuracy': 0.9923154711723328, 'train/loss': 0.024891646578907967, 'train/mean_average_precision': 0.5460445997356278, 'validation/accuracy': 0.9866899847984314, 'validation/loss': 0.04657692462205887, 'validation/mean_average_precision': 0.254345836560073, 'validation/num_examples': 43793, 'test/accuracy': 0.9859139323234558, 'test/loss': 0.049807824194431305, 'test/mean_average_precision': 0.24592628014287862, 'test/num_examples': 43793, 'score': 7455.528498411179, 'total_duration': 11247.001574993134, 'accumulated_submission_time': 7455.528498411179, 'accumulated_eval_time': 3789.942592382431, 'accumulated_logging_time': 0.9038915634155273, 'global_step': 23294, 'preemption_count': 0}), (24044, {'train/accuracy': 0.9923921823501587, 'train/loss': 0.024166883900761604, 'train/mean_average_precision': 0.5712409322426214, 'validation/accuracy': 0.9866307377815247, 'validation/loss': 0.04743630439043045, 'validation/mean_average_precision': 0.25264398668369664, 'validation/num_examples': 43793, 'test/accuracy': 0.9857686161994934, 'test/loss': 0.05093642324209213, 'test/mean_average_precision': 0.24634275661253566, 'test/num_examples': 43793, 'score': 7695.701915502548, 'total_duration': 11602.075031757355, 'accumulated_submission_time': 7695.701915502548, 'accumulated_eval_time': 3904.7906806468964, 'accumulated_logging_time': 0.9359946250915527, 'global_step': 24044, 'preemption_count': 0}), (24798, {'train/accuracy': 0.9930984377861023, 'train/loss': 0.022470610216259956, 'train/mean_average_precision': 0.5961115454126245, 'validation/accuracy': 0.9866713285446167, 'validation/loss': 0.04684531316161156, 'validation/mean_average_precision': 0.2598697709526262, 'validation/num_examples': 43793, 'test/accuracy': 0.9857513904571533, 'test/loss': 0.05028628930449486, 'test/mean_average_precision': 0.24727313250568078, 'test/num_examples': 43793, 'score': 7935.791239023209, 'total_duration': 11959.413158655167, 'accumulated_submission_time': 7935.791239023209, 'accumulated_eval_time': 4021.9848453998566, 'accumulated_logging_time': 0.9708609580993652, 'global_step': 24798, 'preemption_count': 0}), (25538, {'train/accuracy': 0.9933809638023376, 'train/loss': 0.021842068061232567, 'train/mean_average_precision': 0.6266066179462839, 'validation/accuracy': 0.986694872379303, 'validation/loss': 0.04692692682147026, 'validation/mean_average_precision': 0.2572042716750418, 'validation/num_examples': 43793, 'test/accuracy': 0.9857223033905029, 'test/loss': 0.05055712163448334, 'test/mean_average_precision': 0.2435503236909003, 'test/num_examples': 43793, 'score': 8176.01961183548, 'total_duration': 12325.004558086395, 'accumulated_submission_time': 8176.01961183548, 'accumulated_eval_time': 4147.293561458588, 'accumulated_logging_time': 1.0030477046966553, 'global_step': 25538, 'preemption_count': 0}), (26291, {'train/accuracy': 0.9928025603294373, 'train/loss': 0.0234451312571764, 'train/mean_average_precision': 0.5761827578002454, 'validation/accuracy': 0.9866213798522949, 'validation/loss': 0.04699945077300072, 'validation/mean_average_precision': 0.2511379755020969, 'validation/num_examples': 43793, 'test/accuracy': 0.9857041835784912, 'test/loss': 0.050397105515003204, 'test/mean_average_precision': 0.2480874887484809, 'test/num_examples': 43793, 'score': 8416.271092653275, 'total_duration': 12680.953973293304, 'accumulated_submission_time': 8416.271092653275, 'accumulated_eval_time': 4262.936295509338, 'accumulated_logging_time': 1.0376760959625244, 'global_step': 26291, 'preemption_count': 0}), (27040, {'train/accuracy': 0.992655873298645, 'train/loss': 0.023609139025211334, 'train/mean_average_precision': 0.5897485965947075, 'validation/accuracy': 0.9866039156913757, 'validation/loss': 0.04807821661233902, 'validation/mean_average_precision': 0.25073438539267245, 'validation/num_examples': 43793, 'test/accuracy': 0.9857812523841858, 'test/loss': 0.0514933206140995, 'test/mean_average_precision': 0.24595441759522038, 'test/num_examples': 43793, 'score': 8656.47072982788, 'total_duration': 13040.865669727325, 'accumulated_submission_time': 8656.47072982788, 'accumulated_eval_time': 4382.595624923706, 'accumulated_logging_time': 1.0706696510314941, 'global_step': 27040, 'preemption_count': 0}), (27791, {'train/accuracy': 0.9928238987922668, 'train/loss': 0.023239290341734886, 'train/mean_average_precision': 0.5776398059058571, 'validation/accuracy': 0.9865588545799255, 'validation/loss': 0.047662440687417984, 'validation/mean_average_precision': 0.2508783567306901, 'validation/num_examples': 43793, 'test/accuracy': 0.9857842326164246, 'test/loss': 0.05109025165438652, 'test/mean_average_precision': 0.24584770642931192, 'test/num_examples': 43793, 'score': 8896.424266338348, 'total_duration': 13395.142271757126, 'accumulated_submission_time': 8896.424266338348, 'accumulated_eval_time': 4496.867026567459, 'accumulated_logging_time': 1.102473258972168, 'global_step': 27791, 'preemption_count': 0}), (28536, {'train/accuracy': 0.9926972389221191, 'train/loss': 0.023307790979743004, 'train/mean_average_precision': 0.5889419244248999, 'validation/accuracy': 0.9865353107452393, 'validation/loss': 0.04850964620709419, 'validation/mean_average_precision': 0.24961728863623658, 'validation/num_examples': 43793, 'test/accuracy': 0.9857046008110046, 'test/loss': 0.052114564925432205, 'test/mean_average_precision': 0.2429519902380678, 'test/num_examples': 43793, 'score': 9136.532777786255, 'total_duration': 13751.155616044998, 'accumulated_submission_time': 9136.532777786255, 'accumulated_eval_time': 4612.7197296619415, 'accumulated_logging_time': 1.1351377964019775, 'global_step': 28536, 'preemption_count': 0}), (29288, {'train/accuracy': 0.9929196834564209, 'train/loss': 0.022844480350613594, 'train/mean_average_precision': 0.5824909793466204, 'validation/accuracy': 0.9865726828575134, 'validation/loss': 0.048228729516267776, 'validation/mean_average_precision': 0.24974791715845274, 'validation/num_examples': 43793, 'test/accuracy': 0.9856772422790527, 'test/loss': 0.05170305445790291, 'test/mean_average_precision': 0.2464958534102047, 'test/num_examples': 43793, 'score': 9376.589265823364, 'total_duration': 14111.342035531998, 'accumulated_submission_time': 9376.589265823364, 'accumulated_eval_time': 4732.798652887344, 'accumulated_logging_time': 1.1663379669189453, 'global_step': 29288, 'preemption_count': 0}), (30037, {'train/accuracy': 0.9931166768074036, 'train/loss': 0.022161850705742836, 'train/mean_average_precision': 0.6110780190593907, 'validation/accuracy': 0.9865093231201172, 'validation/loss': 0.048746638000011444, 'validation/mean_average_precision': 0.24755324090593728, 'validation/num_examples': 43793, 'test/accuracy': 0.9855828881263733, 'test/loss': 0.05220925435423851, 'test/mean_average_precision': 0.23989250551440108, 'test/num_examples': 43793, 'score': 9616.7351603508, 'total_duration': 14462.951917409897, 'accumulated_submission_time': 9616.7351603508, 'accumulated_eval_time': 4844.209650993347, 'accumulated_logging_time': 1.1994884014129639, 'global_step': 30037, 'preemption_count': 0}), (30790, {'train/accuracy': 0.9931876063346863, 'train/loss': 0.02141193114221096, 'train/mean_average_precision': 0.6239691444803845, 'validation/accuracy': 0.9865888953208923, 'validation/loss': 0.04952388256788254, 'validation/mean_average_precision': 0.2529329067237749, 'validation/num_examples': 43793, 'test/accuracy': 0.9857505559921265, 'test/loss': 0.05304334685206413, 'test/mean_average_precision': 0.24176353729688374, 'test/num_examples': 43793, 'score': 9856.721096038818, 'total_duration': 14818.875288009644, 'accumulated_submission_time': 9856.721096038818, 'accumulated_eval_time': 4960.09548664093, 'accumulated_logging_time': 1.2316246032714844, 'global_step': 30790, 'preemption_count': 0}), (31529, {'train/accuracy': 0.9937662482261658, 'train/loss': 0.020005514845252037, 'train/mean_average_precision': 0.6605400162041181, 'validation/accuracy': 0.9865211248397827, 'validation/loss': 0.04902686923742294, 'validation/mean_average_precision': 0.2478563465140137, 'validation/num_examples': 43793, 'test/accuracy': 0.9855732321739197, 'test/loss': 0.05247371643781662, 'test/mean_average_precision': 0.24339744815671674, 'test/num_examples': 43793, 'score': 10096.81805896759, 'total_duration': 15170.70418548584, 'accumulated_submission_time': 10096.81805896759, 'accumulated_eval_time': 5071.77366065979, 'accumulated_logging_time': 1.265533208847046, 'global_step': 31529, 'preemption_count': 0}), (32270, {'train/accuracy': 0.9942206740379333, 'train/loss': 0.018979180604219437, 'train/mean_average_precision': 0.6804974564516768, 'validation/accuracy': 0.9864314198493958, 'validation/loss': 0.049711667001247406, 'validation/mean_average_precision': 0.24315312134865424, 'validation/num_examples': 43793, 'test/accuracy': 0.9855959415435791, 'test/loss': 0.05326930806040764, 'test/mean_average_precision': 0.23170831553398583, 'test/num_examples': 43793, 'score': 10336.932120800018, 'total_duration': 15529.722115755081, 'accumulated_submission_time': 10336.932120800018, 'accumulated_eval_time': 5190.624104499817, 'accumulated_logging_time': 1.299309253692627, 'global_step': 32270, 'preemption_count': 0}), (33024, {'train/accuracy': 0.9941815733909607, 'train/loss': 0.01906203106045723, 'train/mean_average_precision': 0.6768720292775423, 'validation/accuracy': 0.9864902496337891, 'validation/loss': 0.049419961869716644, 'validation/mean_average_precision': 0.24561891526677285, 'validation/num_examples': 43793, 'test/accuracy': 0.985584557056427, 'test/loss': 0.05310051143169403, 'test/mean_average_precision': 0.2379802507963694, 'test/num_examples': 43793, 'score': 10577.190371990204, 'total_duration': 15888.460463523865, 'accumulated_submission_time': 10577.190371990204, 'accumulated_eval_time': 5309.0521721839905, 'accumulated_logging_time': 1.3314814567565918, 'global_step': 33024, 'preemption_count': 0}), (33781, {'train/accuracy': 0.9938918352127075, 'train/loss': 0.019619356840848923, 'train/mean_average_precision': 0.6653318432091149, 'validation/accuracy': 0.9864837527275085, 'validation/loss': 0.04993182048201561, 'validation/mean_average_precision': 0.24321026504665458, 'validation/num_examples': 43793, 'test/accuracy': 0.9855904579162598, 'test/loss': 0.05348431318998337, 'test/mean_average_precision': 0.24136566361562156, 'test/num_examples': 43793, 'score': 10817.216018676758, 'total_duration': 16245.507838010788, 'accumulated_submission_time': 10817.216018676758, 'accumulated_eval_time': 5426.022752046585, 'accumulated_logging_time': 1.362900733947754, 'global_step': 33781, 'preemption_count': 0}), (34531, {'train/accuracy': 0.9931476712226868, 'train/loss': 0.02184562385082245, 'train/mean_average_precision': 0.6065809626161704, 'validation/accuracy': 0.9863153100013733, 'validation/loss': 0.05028614401817322, 'validation/mean_average_precision': 0.2356881656238881, 'validation/num_examples': 43793, 'test/accuracy': 0.9854485392570496, 'test/loss': 0.05395681411027908, 'test/mean_average_precision': 0.23406512845939162, 'test/num_examples': 43793, 'score': 11057.218078613281, 'total_duration': 16601.556545495987, 'accumulated_submission_time': 11057.218078613281, 'accumulated_eval_time': 5542.016725063324, 'accumulated_logging_time': 1.3957459926605225, 'global_step': 34531, 'preemption_count': 0}), (35289, {'train/accuracy': 0.9932732582092285, 'train/loss': 0.021351510658860207, 'train/mean_average_precision': 0.6102445285731928, 'validation/accuracy': 0.9863283038139343, 'validation/loss': 0.05074532330036163, 'validation/mean_average_precision': 0.24225557996907987, 'validation/num_examples': 43793, 'test/accuracy': 0.9854581952095032, 'test/loss': 0.05426078289747238, 'test/mean_average_precision': 0.23705284058685896, 'test/num_examples': 43793, 'score': 11297.394924879074, 'total_duration': 16956.7015914917, 'accumulated_submission_time': 11297.394924879074, 'accumulated_eval_time': 5656.931999921799, 'accumulated_logging_time': 1.4285671710968018, 'global_step': 35289, 'preemption_count': 0}), (36047, {'train/accuracy': 0.9936595559120178, 'train/loss': 0.020167946815490723, 'train/mean_average_precision': 0.6438834465263861, 'validation/accuracy': 0.9863104224205017, 'validation/loss': 0.05070577934384346, 'validation/mean_average_precision': 0.23380648597369996, 'validation/num_examples': 43793, 'test/accuracy': 0.9854379892349243, 'test/loss': 0.05435900390148163, 'test/mean_average_precision': 0.23143665965904994, 'test/num_examples': 43793, 'score': 11537.618818044662, 'total_duration': 17312.897706270218, 'accumulated_submission_time': 11537.618818044662, 'accumulated_eval_time': 5772.85106253624, 'accumulated_logging_time': 1.4614310264587402, 'global_step': 36047, 'preemption_count': 0}), (36797, {'train/accuracy': 0.9936636686325073, 'train/loss': 0.01991063542664051, 'train/mean_average_precision': 0.6476884772247021, 'validation/accuracy': 0.9864094853401184, 'validation/loss': 0.05145861953496933, 'validation/mean_average_precision': 0.2377665407309361, 'validation/num_examples': 43793, 'test/accuracy': 0.985491931438446, 'test/loss': 0.05515115708112717, 'test/mean_average_precision': 0.23462677071407081, 'test/num_examples': 43793, 'score': 11777.46779179573, 'total_duration': 17670.489493846893, 'accumulated_submission_time': 11777.46779179573, 'accumulated_eval_time': 5890.137528896332, 'accumulated_logging_time': 1.8974733352661133, 'global_step': 36797, 'preemption_count': 0}), (37549, {'train/accuracy': 0.9935628175735474, 'train/loss': 0.020100276917219162, 'train/mean_average_precision': 0.6463171003589824, 'validation/accuracy': 0.9863964915275574, 'validation/loss': 0.05192934721708298, 'validation/mean_average_precision': 0.23822748553626358, 'validation/num_examples': 43793, 'test/accuracy': 0.9854986667633057, 'test/loss': 0.05559105426073074, 'test/mean_average_precision': 0.23435272812689595, 'test/num_examples': 43793, 'score': 12017.64917397499, 'total_duration': 18023.413385868073, 'accumulated_submission_time': 12017.64917397499, 'accumulated_eval_time': 6002.826015472412, 'accumulated_logging_time': 1.9313278198242188, 'global_step': 37549, 'preemption_count': 0}), (38299, {'train/accuracy': 0.9944259524345398, 'train/loss': 0.01781834289431572, 'train/mean_average_precision': 0.7018950843743552, 'validation/accuracy': 0.9862747192382812, 'validation/loss': 0.051901448518037796, 'validation/mean_average_precision': 0.23619965994096087, 'validation/num_examples': 43793, 'test/accuracy': 0.9853967428207397, 'test/loss': 0.05539665371179581, 'test/mean_average_precision': 0.23333045766374247, 'test/num_examples': 43793, 'score': 12257.801238059998, 'total_duration': 18378.24086880684, 'accumulated_submission_time': 12257.801238059998, 'accumulated_eval_time': 6117.448066949844, 'accumulated_logging_time': 1.9647307395935059, 'global_step': 38299, 'preemption_count': 0}), (39053, {'train/accuracy': 0.9949982762336731, 'train/loss': 0.01652163453400135, 'train/mean_average_precision': 0.7224225479696811, 'validation/accuracy': 0.9863018989562988, 'validation/loss': 0.052019424736499786, 'validation/mean_average_precision': 0.23460739858380092, 'validation/num_examples': 43793, 'test/accuracy': 0.985359251499176, 'test/loss': 0.055601317435503006, 'test/mean_average_precision': 0.23551709508652574, 'test/num_examples': 43793, 'score': 12497.982424497604, 'total_duration': 18735.421145915985, 'accumulated_submission_time': 12497.982424497604, 'accumulated_eval_time': 6234.392640352249, 'accumulated_logging_time': 1.999298334121704, 'global_step': 39053, 'preemption_count': 0}), (39792, {'train/accuracy': 0.9952055215835571, 'train/loss': 0.015768634155392647, 'train/mean_average_precision': 0.7366664288057573, 'validation/accuracy': 0.9863213896751404, 'validation/loss': 0.05276699364185333, 'validation/mean_average_precision': 0.23301486214369227, 'validation/num_examples': 43793, 'test/accuracy': 0.9853891134262085, 'test/loss': 0.05658691003918648, 'test/mean_average_precision': 0.2302643002277187, 'test/num_examples': 43793, 'score': 12738.079112768173, 'total_duration': 19092.81990480423, 'accumulated_submission_time': 12738.079112768173, 'accumulated_eval_time': 6351.638965606689, 'accumulated_logging_time': 2.03450345993042, 'global_step': 39792, 'preemption_count': 0}), (40539, {'train/accuracy': 0.9952002763748169, 'train/loss': 0.016081351786851883, 'train/mean_average_precision': 0.7377185027103813, 'validation/accuracy': 0.9861488342285156, 'validation/loss': 0.05301316827535629, 'validation/mean_average_precision': 0.23309463438415423, 'validation/num_examples': 43793, 'test/accuracy': 0.9852867722511292, 'test/loss': 0.056485746055841446, 'test/mean_average_precision': 0.23099852632288098, 'test/num_examples': 43793, 'score': 12978.109545946121, 'total_duration': 19451.29268836975, 'accumulated_submission_time': 12978.109545946121, 'accumulated_eval_time': 6470.0264637470245, 'accumulated_logging_time': 2.069436550140381, 'global_step': 40539, 'preemption_count': 0}), (41274, {'train/accuracy': 0.9948239326477051, 'train/loss': 0.016791634261608124, 'train/mean_average_precision': 0.7085789565068721, 'validation/accuracy': 0.9862288236618042, 'validation/loss': 0.053134433925151825, 'validation/mean_average_precision': 0.22880845618112994, 'validation/num_examples': 43793, 'test/accuracy': 0.9853672385215759, 'test/loss': 0.05689776688814163, 'test/mean_average_precision': 0.22833339206003106, 'test/num_examples': 43793, 'score': 13218.214102506638, 'total_duration': 19811.006506443024, 'accumulated_submission_time': 13218.214102506638, 'accumulated_eval_time': 6589.574536561966, 'accumulated_logging_time': 2.107372522354126, 'global_step': 41274, 'preemption_count': 0}), (42022, {'train/accuracy': 0.9945333003997803, 'train/loss': 0.01730739325284958, 'train/mean_average_precision': 0.7135553817663034, 'validation/accuracy': 0.9861322045326233, 'validation/loss': 0.05359821394085884, 'validation/mean_average_precision': 0.2313976091081589, 'validation/num_examples': 43793, 'test/accuracy': 0.9854017496109009, 'test/loss': 0.057231880724430084, 'test/mean_average_precision': 0.2287596771324323, 'test/num_examples': 43793, 'score': 13458.43649482727, 'total_duration': 20166.63150715828, 'accumulated_submission_time': 13458.43649482727, 'accumulated_eval_time': 6704.9207146167755, 'accumulated_logging_time': 2.143451690673828, 'global_step': 42022, 'preemption_count': 0}), (42774, {'train/accuracy': 0.9939397573471069, 'train/loss': 0.018936272710561752, 'train/mean_average_precision': 0.6697946583231249, 'validation/accuracy': 0.9862284064292908, 'validation/loss': 0.054160453379154205, 'validation/mean_average_precision': 0.23365620581876412, 'validation/num_examples': 43793, 'test/accuracy': 0.9853150248527527, 'test/loss': 0.058036379516124725, 'test/mean_average_precision': 0.22792961388349214, 'test/num_examples': 43793, 'score': 13698.556680202484, 'total_duration': 20523.85961985588, 'accumulated_submission_time': 13698.556680202484, 'accumulated_eval_time': 6821.973500728607, 'accumulated_logging_time': 2.1787869930267334, 'global_step': 42774, 'preemption_count': 0}), (43522, {'train/accuracy': 0.9940671324729919, 'train/loss': 0.018478265032172203, 'train/mean_average_precision': 0.6751643354404507, 'validation/accuracy': 0.9861873984336853, 'validation/loss': 0.05490073561668396, 'validation/mean_average_precision': 0.22554508688219319, 'validation/num_examples': 43793, 'test/accuracy': 0.9853845238685608, 'test/loss': 0.05850997939705849, 'test/mean_average_precision': 0.2311539778899343, 'test/num_examples': 43793, 'score': 13938.790473937988, 'total_duration': 20879.66306090355, 'accumulated_submission_time': 13938.790473937988, 'accumulated_eval_time': 6937.488364696503, 'accumulated_logging_time': 2.2132139205932617, 'global_step': 43522, 'preemption_count': 0}), (44274, {'train/accuracy': 0.9948219656944275, 'train/loss': 0.016602113842964172, 'train/mean_average_precision': 0.7204996206656971, 'validation/accuracy': 0.9860494136810303, 'validation/loss': 0.054025713354349136, 'validation/mean_average_precision': 0.22890643751743953, 'validation/num_examples': 43793, 'test/accuracy': 0.9851073622703552, 'test/loss': 0.057887982577085495, 'test/mean_average_precision': 0.22672974356732076, 'test/num_examples': 43793, 'score': 14178.823575496674, 'total_duration': 21232.88632273674, 'accumulated_submission_time': 14178.823575496674, 'accumulated_eval_time': 7050.623664855957, 'accumulated_logging_time': 2.248157262802124, 'global_step': 44274, 'preemption_count': 0}), (45021, {'train/accuracy': 0.9945811033248901, 'train/loss': 0.017048701643943787, 'train/mean_average_precision': 0.7150672404338416, 'validation/accuracy': 0.9861192107200623, 'validation/loss': 0.05487704649567604, 'validation/mean_average_precision': 0.22146424657360408, 'validation/num_examples': 43793, 'test/accuracy': 0.9852181673049927, 'test/loss': 0.05893776938319206, 'test/mean_average_precision': 0.21934709262506977, 'test/num_examples': 43793, 'score': 14418.789083957672, 'total_duration': 21584.268087387085, 'accumulated_submission_time': 14418.789083957672, 'accumulated_eval_time': 7161.983935594559, 'accumulated_logging_time': 2.284372329711914, 'global_step': 45021, 'preemption_count': 0}), (45776, {'train/accuracy': 0.994462788105011, 'train/loss': 0.017135147005319595, 'train/mean_average_precision': 0.7201120524918182, 'validation/accuracy': 0.9861781001091003, 'validation/loss': 0.05562358349561691, 'validation/mean_average_precision': 0.22999685633951691, 'validation/num_examples': 43793, 'test/accuracy': 0.9852989912033081, 'test/loss': 0.05964159220457077, 'test/mean_average_precision': 0.22465961636631798, 'test/num_examples': 43793, 'score': 14658.856129646301, 'total_duration': 21935.34939599037, 'accumulated_submission_time': 14658.856129646301, 'accumulated_eval_time': 7272.9426436424255, 'accumulated_logging_time': 2.3197762966156006, 'global_step': 45776, 'preemption_count': 0}), (46528, {'train/accuracy': 0.9965863823890686, 'train/loss': 0.012139195576310158, 'train/mean_average_precision': 0.8180281852483683, 'validation/accuracy': 0.986148476600647, 'validation/loss': 0.05636822432279587, 'validation/mean_average_precision': 0.23150599813977804, 'validation/num_examples': 43793, 'test/accuracy': 0.9852783679962158, 'test/loss': 0.06038013845682144, 'test/mean_average_precision': 0.2273424703023917, 'test/num_examples': 43793, 'score': 14898.866291761398, 'total_duration': 22288.94997239113, 'accumulated_submission_time': 14898.866291761398, 'accumulated_eval_time': 7386.477149009705, 'accumulated_logging_time': 2.3562400341033936, 'global_step': 46528, 'preemption_count': 0}), (47279, {'train/accuracy': 0.9965313076972961, 'train/loss': 0.012453470379114151, 'train/mean_average_precision': 0.8025178704840774, 'validation/accuracy': 0.9861151576042175, 'validation/loss': 0.05602893605828285, 'validation/mean_average_precision': 0.23137236510486978, 'validation/num_examples': 43793, 'test/accuracy': 0.9851911664009094, 'test/loss': 0.060071028769016266, 'test/mean_average_precision': 0.22063199437236372, 'test/num_examples': 43793, 'score': 15139.000858545303, 'total_duration': 22639.643125772476, 'accumulated_submission_time': 15139.000858545303, 'accumulated_eval_time': 7496.9792284965515, 'accumulated_logging_time': 2.3927478790283203, 'global_step': 47279, 'preemption_count': 0}), (48028, {'train/accuracy': 0.9962067008018494, 'train/loss': 0.013107025995850563, 'train/mean_average_precision': 0.7931497384251549, 'validation/accuracy': 0.986046552658081, 'validation/loss': 0.05627189204096794, 'validation/mean_average_precision': 0.22607762340226092, 'validation/num_examples': 43793, 'test/accuracy': 0.9850408434867859, 'test/loss': 0.060180965811014175, 'test/mean_average_precision': 0.21903934471072786, 'test/num_examples': 43793, 'score': 15379.096559047699, 'total_duration': 22995.49823999405, 'accumulated_submission_time': 15379.096559047699, 'accumulated_eval_time': 7612.681452512741, 'accumulated_logging_time': 2.4296257495880127, 'global_step': 48028, 'preemption_count': 0}), (48773, {'train/accuracy': 0.9955082535743713, 'train/loss': 0.014244879595935345, 'train/mean_average_precision': 0.774797287980069, 'validation/accuracy': 0.986183762550354, 'validation/loss': 0.05732876434922218, 'validation/mean_average_precision': 0.228134299896944, 'validation/num_examples': 43793, 'test/accuracy': 0.9852885007858276, 'test/loss': 0.06146378815174103, 'test/mean_average_precision': 0.22184345036840447, 'test/num_examples': 43793, 'score': 15619.057337284088, 'total_duration': 23349.986057519913, 'accumulated_submission_time': 15619.057337284088, 'accumulated_eval_time': 7727.149502515793, 'accumulated_logging_time': 2.468273878097534, 'global_step': 48773, 'preemption_count': 0}), (49526, {'train/accuracy': 0.9951686859130859, 'train/loss': 0.014958654530346394, 'train/mean_average_precision': 0.7747991965325125, 'validation/accuracy': 0.9860441088676453, 'validation/loss': 0.05705243721604347, 'validation/mean_average_precision': 0.2230743490184299, 'validation/num_examples': 43793, 'test/accuracy': 0.9851848483085632, 'test/loss': 0.06115218251943588, 'test/mean_average_precision': 0.21814090271004577, 'test/num_examples': 43793, 'score': 15859.288888454437, 'total_duration': 23699.553965330124, 'accumulated_submission_time': 15859.288888454437, 'accumulated_eval_time': 7836.429103851318, 'accumulated_logging_time': 2.5045437812805176, 'global_step': 49526, 'preemption_count': 0}), (50272, {'train/accuracy': 0.9951609969139099, 'train/loss': 0.01507300790399313, 'train/mean_average_precision': 0.7670223106612885, 'validation/accuracy': 0.986040472984314, 'validation/loss': 0.05771508067846298, 'validation/mean_average_precision': 0.22505703540184746, 'validation/num_examples': 43793, 'test/accuracy': 0.9852353930473328, 'test/loss': 0.06162497401237488, 'test/mean_average_precision': 0.22136297501708904, 'test/num_examples': 43793, 'score': 16099.445001363754, 'total_duration': 24051.175461292267, 'accumulated_submission_time': 16099.445001363754, 'accumulated_eval_time': 7947.837135314941, 'accumulated_logging_time': 2.5418262481689453, 'global_step': 50272, 'preemption_count': 0}), (51016, {'train/accuracy': 0.994962215423584, 'train/loss': 0.015288949012756348, 'train/mean_average_precision': 0.7472203813139899, 'validation/accuracy': 0.9861119389533997, 'validation/loss': 0.0582580640912056, 'validation/mean_average_precision': 0.220357172150594, 'validation/num_examples': 43793, 'test/accuracy': 0.9851894974708557, 'test/loss': 0.06237699091434479, 'test/mean_average_precision': 0.21689525458278233, 'test/num_examples': 43793, 'score': 16339.63740158081, 'total_duration': 24398.847739219666, 'accumulated_submission_time': 16339.63740158081, 'accumulated_eval_time': 8055.256649494171, 'accumulated_logging_time': 2.5816922187805176, 'global_step': 51016, 'preemption_count': 0}), (51765, {'train/accuracy': 0.9956825971603394, 'train/loss': 0.01382682379335165, 'train/mean_average_precision': 0.788454380210666, 'validation/accuracy': 0.9860782027244568, 'validation/loss': 0.05799631401896477, 'validation/mean_average_precision': 0.22232651831847636, 'validation/num_examples': 43793, 'test/accuracy': 0.9851473569869995, 'test/loss': 0.06215343251824379, 'test/mean_average_precision': 0.22124616404784242, 'test/num_examples': 43793, 'score': 16579.84620285034, 'total_duration': 24750.48007750511, 'accumulated_submission_time': 16579.84620285034, 'accumulated_eval_time': 8166.622575998306, 'accumulated_logging_time': 2.6193721294403076, 'global_step': 51765, 'preemption_count': 0}), (52515, {'train/accuracy': 0.9962998032569885, 'train/loss': 0.012408302165567875, 'train/mean_average_precision': 0.8206180664414136, 'validation/accuracy': 0.9861164093017578, 'validation/loss': 0.058766286820173264, 'validation/mean_average_precision': 0.2223903278764082, 'validation/num_examples': 43793, 'test/accuracy': 0.9851554036140442, 'test/loss': 0.06296781450510025, 'test/mean_average_precision': 0.21867639481387877, 'test/num_examples': 43793, 'score': 16819.83504796028, 'total_duration': 25103.957667827606, 'accumulated_submission_time': 16819.83504796028, 'accumulated_eval_time': 8280.05424952507, 'accumulated_logging_time': 2.6561763286590576, 'global_step': 52515, 'preemption_count': 0}), (53261, {'train/accuracy': 0.997327983379364, 'train/loss': 0.010309528559446335, 'train/mean_average_precision': 0.8675313433747215, 'validation/accuracy': 0.9860234260559082, 'validation/loss': 0.05928371846675873, 'validation/mean_average_precision': 0.21858522312938175, 'validation/num_examples': 43793, 'test/accuracy': 0.985207200050354, 'test/loss': 0.06347201764583588, 'test/mean_average_precision': 0.21946776122638742, 'test/num_examples': 43793, 'score': 17059.884666204453, 'total_duration': 25456.877187013626, 'accumulated_submission_time': 17059.884666204453, 'accumulated_eval_time': 8392.8616604805, 'accumulated_logging_time': 2.697401523590088, 'global_step': 53261, 'preemption_count': 0}), (54013, {'train/accuracy': 0.9960364103317261, 'train/loss': 0.012853307649493217, 'train/mean_average_precision': 0.8124292085607783, 'validation/accuracy': 0.9860871434211731, 'validation/loss': 0.05909451097249985, 'validation/mean_average_precision': 0.21627753684474293, 'validation/num_examples': 43793, 'test/accuracy': 0.9851477742195129, 'test/loss': 0.06346726417541504, 'test/mean_average_precision': 0.21424649439036367, 'test/num_examples': 43793, 'score': 17299.912273406982, 'total_duration': 25809.669014453888, 'accumulated_submission_time': 17299.912273406982, 'accumulated_eval_time': 8505.566602230072, 'accumulated_logging_time': 2.735142230987549, 'global_step': 54013, 'preemption_count': 0}), (54762, {'train/accuracy': 0.9976819157600403, 'train/loss': 0.009660778567194939, 'train/mean_average_precision': 0.8719512232661151, 'validation/accuracy': 0.9859962463378906, 'validation/loss': 0.05957746505737305, 'validation/mean_average_precision': 0.21746900532071675, 'validation/num_examples': 43793, 'test/accuracy': 0.9850918054580688, 'test/loss': 0.06392613053321838, 'test/mean_average_precision': 0.2151637094748986, 'test/num_examples': 43793, 'score': 17539.928025960922, 'total_duration': 26156.15963792801, 'accumulated_submission_time': 17539.928025960922, 'accumulated_eval_time': 8611.9836332798, 'accumulated_logging_time': 2.773247480392456, 'global_step': 54762, 'preemption_count': 0}), (55525, {'train/accuracy': 0.9970630407333374, 'train/loss': 0.010564861819148064, 'train/mean_average_precision': 0.8574635624911153, 'validation/accuracy': 0.9860855340957642, 'validation/loss': 0.06065803021192551, 'validation/mean_average_precision': 0.21881870874851647, 'validation/num_examples': 43793, 'test/accuracy': 0.9851404428482056, 'test/loss': 0.06524888426065445, 'test/mean_average_precision': 0.21419798285350508, 'test/num_examples': 43793, 'score': 17780.066515922546, 'total_duration': 26504.03938817978, 'accumulated_submission_time': 17780.066515922546, 'accumulated_eval_time': 8719.667748212814, 'accumulated_logging_time': 2.8100857734680176, 'global_step': 55525, 'preemption_count': 0}), (56277, {'train/accuracy': 0.9965944290161133, 'train/loss': 0.011259051039814949, 'train/mean_average_precision': 0.8476336181469304, 'validation/accuracy': 0.9860563278198242, 'validation/loss': 0.06092185154557228, 'validation/mean_average_precision': 0.21723699273463995, 'validation/num_examples': 43793, 'test/accuracy': 0.9851536750793457, 'test/loss': 0.06518924981355667, 'test/mean_average_precision': 0.2122587202567049, 'test/num_examples': 43793, 'score': 18020.076694488525, 'total_duration': 26857.69357442856, 'accumulated_submission_time': 18020.076694488525, 'accumulated_eval_time': 8833.25545835495, 'accumulated_logging_time': 2.8469173908233643, 'global_step': 56277, 'preemption_count': 0}), (57029, {'train/accuracy': 0.9959918260574341, 'train/loss': 0.012544776313006878, 'train/mean_average_precision': 0.8311303477558817, 'validation/accuracy': 0.9860153198242188, 'validation/loss': 0.06112876161932945, 'validation/mean_average_precision': 0.21309542368130246, 'validation/num_examples': 43793, 'test/accuracy': 0.985084593296051, 'test/loss': 0.06547699123620987, 'test/mean_average_precision': 0.2110058729860776, 'test/num_examples': 43793, 'score': 18260.329787254333, 'total_duration': 27213.905391216278, 'accumulated_submission_time': 18260.329787254333, 'accumulated_eval_time': 8949.157200813293, 'accumulated_logging_time': 2.883453845977783, 'global_step': 57029, 'preemption_count': 0})], 'global_step': 57700}
I0205 12:42:16.408062 140205209478976 submission_runner.py:586] Timing: 18477.080607652664
I0205 12:42:16.408126 140205209478976 submission_runner.py:588] Total number of evals: 77
I0205 12:42:16.408175 140205209478976 submission_runner.py:589] ====================
I0205 12:42:16.408229 140205209478976 submission_runner.py:542] Using RNG seed 1480595982
I0205 12:42:16.478109 140205209478976 submission_runner.py:551] --- Tuning run 3/5 ---
I0205 12:42:16.478302 140205209478976 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_3.
I0205 12:42:16.478829 140205209478976 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_3/hparams.json.
I0205 12:42:16.622373 140205209478976 submission_runner.py:206] Initializing dataset.
I0205 12:42:16.715882 140205209478976 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 12:42:16.720531 140205209478976 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 12:42:16.862840 140205209478976 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 12:42:16.904289 140205209478976 submission_runner.py:213] Initializing model.
I0205 12:42:19.682024 140205209478976 submission_runner.py:255] Initializing optimizer.
I0205 12:42:20.279886 140205209478976 submission_runner.py:262] Initializing metrics bundle.
I0205 12:42:20.280133 140205209478976 submission_runner.py:280] Initializing checkpoint and logger.
I0205 12:42:20.280970 140205209478976 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_3 with prefix checkpoint_
I0205 12:42:20.281108 140205209478976 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_3/meta_data_0.json.
I0205 12:42:20.281343 140205209478976 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 12:42:20.281405 140205209478976 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 12:42:21.836159 140205209478976 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 12:42:23.327430 140205209478976 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_3/flags_0.json.
I0205 12:42:23.331743 140205209478976 submission_runner.py:314] Starting training loop.
I0205 12:42:34.731377 140002572695296 logging_writer.py:48] [0] global_step=0, grad_norm=2.357750415802002, loss=0.7395575046539307
I0205 12:42:34.740865 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:44:23.732250 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:44:26.801544 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:44:29.788191 140205209478976 submission_runner.py:408] Time since start: 126.46s, 	Step: 1, 	{'train/accuracy': 0.5091323852539062, 'train/loss': 0.7392957210540771, 'train/mean_average_precision': 0.022487030195939863, 'validation/accuracy': 0.5064647793769836, 'validation/loss': 0.7422075867652893, 'validation/mean_average_precision': 0.02529862965175552, 'validation/num_examples': 43793, 'test/accuracy': 0.5047287344932556, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.026928990047386532, 'test/num_examples': 43793, 'score': 11.409060716629028, 'total_duration': 126.4563901424408, 'accumulated_submission_time': 11.409060716629028, 'accumulated_eval_time': 115.04727101325989, 'accumulated_logging_time': 0}
I0205 12:44:29.798056 140002598295296 logging_writer.py:48] [1] accumulated_eval_time=115.047271, accumulated_logging_time=0, accumulated_submission_time=11.409061, global_step=1, preemption_count=0, score=11.409061, test/accuracy=0.504729, test/loss=0.743874, test/mean_average_precision=0.026929, test/num_examples=43793, total_duration=126.456390, train/accuracy=0.509132, train/loss=0.739296, train/mean_average_precision=0.022487, validation/accuracy=0.506465, validation/loss=0.742208, validation/mean_average_precision=0.025299, validation/num_examples=43793
I0205 12:45:03.348837 140020770670336 logging_writer.py:48] [100] global_step=100, grad_norm=0.6409674286842346, loss=0.49093008041381836
I0205 12:45:35.094467 140002598295296 logging_writer.py:48] [200] global_step=200, grad_norm=0.39967799186706543, loss=0.36154094338417053
I0205 12:46:07.230772 140020770670336 logging_writer.py:48] [300] global_step=300, grad_norm=0.28872114419937134, loss=0.26000508666038513
I0205 12:46:39.313546 140002598295296 logging_writer.py:48] [400] global_step=400, grad_norm=0.19085127115249634, loss=0.17341850697994232
I0205 12:47:11.491982 140020770670336 logging_writer.py:48] [500] global_step=500, grad_norm=0.11599373817443848, loss=0.12123337388038635
I0205 12:47:43.293739 140002598295296 logging_writer.py:48] [600] global_step=600, grad_norm=0.07261538505554199, loss=0.08865151554346085
I0205 12:48:15.113621 140020770670336 logging_writer.py:48] [700] global_step=700, grad_norm=0.053080279380083084, loss=0.07355888187885284
I0205 12:48:29.972070 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:50:17.061277 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:50:20.127041 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:50:23.108728 140205209478976 submission_runner.py:408] Time since start: 479.78s, 	Step: 748, 	{'train/accuracy': 0.9866933822631836, 'train/loss': 0.06980946660041809, 'train/mean_average_precision': 0.03583625443003626, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07773509621620178, 'validation/mean_average_precision': 0.0368130067090984, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08055828511714935, 'test/mean_average_precision': 0.0400774903969423, 'test/num_examples': 43793, 'score': 251.55146312713623, 'total_duration': 479.7769241333008, 'accumulated_submission_time': 251.55146312713623, 'accumulated_eval_time': 228.1838824748993, 'accumulated_logging_time': 0.02129340171813965}
I0205 12:50:23.126296 140002205157120 logging_writer.py:48] [748] accumulated_eval_time=228.183882, accumulated_logging_time=0.021293, accumulated_submission_time=251.551463, global_step=748, preemption_count=0, score=251.551463, test/accuracy=0.983142, test/loss=0.080558, test/mean_average_precision=0.040077, test/num_examples=43793, total_duration=479.776924, train/accuracy=0.986693, train/loss=0.069809, train/mean_average_precision=0.035836, validation/accuracy=0.984118, validation/loss=0.077735, validation/mean_average_precision=0.036813, validation/num_examples=43793
I0205 12:50:40.290897 140021163808512 logging_writer.py:48] [800] global_step=800, grad_norm=0.0985167846083641, loss=0.06374102085828781
I0205 12:51:12.286918 140002205157120 logging_writer.py:48] [900] global_step=900, grad_norm=0.13409113883972168, loss=0.05849568173289299
I0205 12:51:44.374607 140021163808512 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.07311197370290756, loss=0.05896162614226341
I0205 12:52:16.881022 140002205157120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.037872064858675, loss=0.0538005568087101
I0205 12:52:49.966060 140021163808512 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.16516636312007904, loss=0.04910873621702194
I0205 12:53:21.813972 140002205157120 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.12737032771110535, loss=0.047937631607055664
I0205 12:53:53.988243 140021163808512 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0746203064918518, loss=0.04993228614330292
I0205 12:54:23.337384 140205209478976 spec.py:321] Evaluating on the training split.
I0205 12:56:05.947966 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 12:56:09.065203 140205209478976 spec.py:349] Evaluating on the test split.
I0205 12:56:12.117925 140205209478976 submission_runner.py:408] Time since start: 828.79s, 	Step: 1492, 	{'train/accuracy': 0.987136721611023, 'train/loss': 0.04884883388876915, 'train/mean_average_precision': 0.08048615640400439, 'validation/accuracy': 0.9844698905944824, 'validation/loss': 0.05820437893271446, 'validation/mean_average_precision': 0.08047419959382764, 'validation/num_examples': 43793, 'test/accuracy': 0.9834933280944824, 'test/loss': 0.061417415738105774, 'test/mean_average_precision': 0.08071569313351043, 'test/num_examples': 43793, 'score': 491.73076152801514, 'total_duration': 828.7861225605011, 'accumulated_submission_time': 491.73076152801514, 'accumulated_eval_time': 336.9643814563751, 'accumulated_logging_time': 0.05037117004394531}
I0205 12:56:12.133837 140002598295296 logging_writer.py:48] [1492] accumulated_eval_time=336.964381, accumulated_logging_time=0.050371, accumulated_submission_time=491.730762, global_step=1492, preemption_count=0, score=491.730762, test/accuracy=0.983493, test/loss=0.061417, test/mean_average_precision=0.080716, test/num_examples=43793, total_duration=828.786123, train/accuracy=0.987137, train/loss=0.048849, train/mean_average_precision=0.080486, validation/accuracy=0.984470, validation/loss=0.058204, validation/mean_average_precision=0.080474, validation/num_examples=43793
I0205 12:56:15.031161 140020770670336 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.377105712890625, loss=0.05022694543004036
I0205 12:56:46.774276 140002598295296 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.2926956117153168, loss=0.043503597378730774
I0205 12:57:19.042534 140020770670336 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.21939487755298615, loss=0.05050567165017128
I0205 12:57:51.122083 140002598295296 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.20367082953453064, loss=0.05437785014510155
I0205 12:58:23.651205 140020770670336 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.1368390917778015, loss=0.04831048101186752
I0205 12:58:55.726572 140002598295296 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.22287358343601227, loss=0.05541272461414337
I0205 12:59:27.799922 140020770670336 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.16411162912845612, loss=0.05123642459511757
I0205 12:59:59.834544 140002598295296 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.23180118203163147, loss=0.040808770805597305
I0205 13:00:12.329109 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:01:58.776926 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:02:01.911776 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:02:04.874183 140205209478976 submission_runner.py:408] Time since start: 1181.54s, 	Step: 2240, 	{'train/accuracy': 0.987756609916687, 'train/loss': 0.04443162679672241, 'train/mean_average_precision': 0.12847178421730218, 'validation/accuracy': 0.9850073456764221, 'validation/loss': 0.053563233464956284, 'validation/mean_average_precision': 0.12322239156428166, 'validation/num_examples': 43793, 'test/accuracy': 0.9840232133865356, 'test/loss': 0.0565517283976078, 'test/mean_average_precision': 0.12192161108931347, 'test/num_examples': 43793, 'score': 731.8950526714325, 'total_duration': 1181.5423827171326, 'accumulated_submission_time': 731.8950526714325, 'accumulated_eval_time': 449.5094120502472, 'accumulated_logging_time': 0.07734990119934082}
I0205 13:02:04.889712 140002205157120 logging_writer.py:48] [2240] accumulated_eval_time=449.509412, accumulated_logging_time=0.077350, accumulated_submission_time=731.895053, global_step=2240, preemption_count=0, score=731.895053, test/accuracy=0.984023, test/loss=0.056552, test/mean_average_precision=0.121922, test/num_examples=43793, total_duration=1181.542383, train/accuracy=0.987757, train/loss=0.044432, train/mean_average_precision=0.128472, validation/accuracy=0.985007, validation/loss=0.053563, validation/mean_average_precision=0.123222, validation/num_examples=43793
I0205 13:02:24.867945 140030420236032 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.10427606105804443, loss=0.03987392038106918
I0205 13:02:57.523539 140002205157120 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.1942128688097, loss=0.04551342502236366
I0205 13:03:29.423795 140030420236032 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.17879652976989746, loss=0.04624346271157265
I0205 13:04:01.523776 140002205157120 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.1706865131855011, loss=0.043365154415369034
I0205 13:04:33.678699 140030420236032 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.08866114169359207, loss=0.0384124219417572
I0205 13:05:05.919851 140002205157120 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.2469484657049179, loss=0.04198672994971275
I0205 13:05:38.544775 140030420236032 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.07155317813158035, loss=0.041887395083904266
I0205 13:06:05.086973 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:07:55.117294 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:07:58.527523 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:08:01.852157 140205209478976 submission_runner.py:408] Time since start: 1538.52s, 	Step: 2982, 	{'train/accuracy': 0.9878631830215454, 'train/loss': 0.04359974339604378, 'train/mean_average_precision': 0.15402269006234956, 'validation/accuracy': 0.9849075078964233, 'validation/loss': 0.05237336456775665, 'validation/mean_average_precision': 0.13969176410434406, 'validation/num_examples': 43793, 'test/accuracy': 0.9840303659439087, 'test/loss': 0.05466533452272415, 'test/mean_average_precision': 0.14474327212571866, 'test/num_examples': 43793, 'score': 972.0586864948273, 'total_duration': 1538.52033162117, 'accumulated_submission_time': 972.0586864948273, 'accumulated_eval_time': 566.2745454311371, 'accumulated_logging_time': 0.10538744926452637}
I0205 13:08:01.871531 140002598295296 logging_writer.py:48] [2982] accumulated_eval_time=566.274545, accumulated_logging_time=0.105387, accumulated_submission_time=972.058686, global_step=2982, preemption_count=0, score=972.058686, test/accuracy=0.984030, test/loss=0.054665, test/mean_average_precision=0.144743, test/num_examples=43793, total_duration=1538.520332, train/accuracy=0.987863, train/loss=0.043600, train/mean_average_precision=0.154023, validation/accuracy=0.984908, validation/loss=0.052373, validation/mean_average_precision=0.139692, validation/num_examples=43793
I0205 13:08:08.184491 140020770670336 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.21902576088905334, loss=0.04033335670828819
I0205 13:08:40.948279 140002598295296 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.16947530210018158, loss=0.0419142059981823
I0205 13:09:13.784441 140020770670336 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.09253975003957748, loss=0.044779714196920395
I0205 13:09:46.366178 140002598295296 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.05728711560368538, loss=0.03843720629811287
I0205 13:10:19.500410 140020770670336 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.1723044067621231, loss=0.045125462114810944
I0205 13:10:52.163355 140002598295296 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0758490040898323, loss=0.045267798006534576
I0205 13:11:24.621599 140020770670336 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.12332454323768616, loss=0.042844682931900024
I0205 13:11:57.221993 140002598295296 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.12609319388866425, loss=0.04014081135392189
I0205 13:12:02.145506 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:13:48.507174 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:13:51.793837 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:13:55.045600 140205209478976 submission_runner.py:408] Time since start: 1891.71s, 	Step: 3716, 	{'train/accuracy': 0.9883297085762024, 'train/loss': 0.0408373698592186, 'train/mean_average_precision': 0.1751967621768912, 'validation/accuracy': 0.9854157567024231, 'validation/loss': 0.05042848363518715, 'validation/mean_average_precision': 0.16654359631809973, 'validation/num_examples': 43793, 'test/accuracy': 0.9845231771469116, 'test/loss': 0.053182750940322876, 'test/mean_average_precision': 0.16787737271235917, 'test/num_examples': 43793, 'score': 1212.2938175201416, 'total_duration': 1891.7137813568115, 'accumulated_submission_time': 1212.2938175201416, 'accumulated_eval_time': 679.1746046543121, 'accumulated_logging_time': 0.13896894454956055}
I0205 13:13:55.062901 140002205157120 logging_writer.py:48] [3716] accumulated_eval_time=679.174605, accumulated_logging_time=0.138969, accumulated_submission_time=1212.293818, global_step=3716, preemption_count=0, score=1212.293818, test/accuracy=0.984523, test/loss=0.053183, test/mean_average_precision=0.167877, test/num_examples=43793, total_duration=1891.713781, train/accuracy=0.988330, train/loss=0.040837, train/mean_average_precision=0.175197, validation/accuracy=0.985416, validation/loss=0.050428, validation/mean_average_precision=0.166544, validation/num_examples=43793
I0205 13:14:22.919947 140021163808512 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.10702493041753769, loss=0.04163232818245888
I0205 13:14:55.412130 140002205157120 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.08317313343286514, loss=0.040934570133686066
I0205 13:15:27.781978 140021163808512 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.11187351495027542, loss=0.03502972424030304
I0205 13:16:00.216140 140002205157120 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.08987873047590256, loss=0.03868884965777397
I0205 13:16:32.989230 140021163808512 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.05009221285581589, loss=0.039141781628131866
I0205 13:17:05.173925 140002205157120 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.045597076416015625, loss=0.03854343667626381
I0205 13:17:37.396040 140021163808512 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.24244986474514008, loss=0.043717607855796814
I0205 13:17:55.073155 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:19:40.768459 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:19:45.822491 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:19:48.862874 140205209478976 submission_runner.py:408] Time since start: 2245.53s, 	Step: 4456, 	{'train/accuracy': 0.9884167313575745, 'train/loss': 0.04014383628964424, 'train/mean_average_precision': 0.2029430039185905, 'validation/accuracy': 0.9853593111038208, 'validation/loss': 0.04974988102912903, 'validation/mean_average_precision': 0.17537489907493375, 'validation/num_examples': 43793, 'test/accuracy': 0.9844696521759033, 'test/loss': 0.05213036388158798, 'test/mean_average_precision': 0.18201119611679045, 'test/num_examples': 43793, 'score': 1452.2720143795013, 'total_duration': 2245.531072616577, 'accumulated_submission_time': 1452.2720143795013, 'accumulated_eval_time': 792.9642839431763, 'accumulated_logging_time': 0.16809701919555664}
I0205 13:19:48.879167 140002598295296 logging_writer.py:48] [4456] accumulated_eval_time=792.964284, accumulated_logging_time=0.168097, accumulated_submission_time=1452.272014, global_step=4456, preemption_count=0, score=1452.272014, test/accuracy=0.984470, test/loss=0.052130, test/mean_average_precision=0.182011, test/num_examples=43793, total_duration=2245.531073, train/accuracy=0.988417, train/loss=0.040144, train/mean_average_precision=0.202943, validation/accuracy=0.985359, validation/loss=0.049750, validation/mean_average_precision=0.175375, validation/num_examples=43793
I0205 13:20:03.531185 140030420236032 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.05686752125620842, loss=0.038280948996543884
I0205 13:20:35.929722 140002598295296 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.08394142985343933, loss=0.04146110266447067
I0205 13:21:08.057542 140030420236032 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0803828313946724, loss=0.03723504766821861
I0205 13:21:40.395246 140002598295296 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.14753605425357819, loss=0.043281883001327515
I0205 13:22:12.756886 140030420236032 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.07023121416568756, loss=0.04512626677751541
I0205 13:22:44.640145 140002598295296 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.05342372879385948, loss=0.03925448656082153
I0205 13:23:16.769264 140030420236032 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.06422073394060135, loss=0.03744146227836609
I0205 13:23:48.776136 140002598295296 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.03938054293394089, loss=0.0350545234978199
I0205 13:23:49.097375 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:25:31.559176 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:25:34.617715 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:25:37.693090 140205209478976 submission_runner.py:408] Time since start: 2594.36s, 	Step: 5202, 	{'train/accuracy': 0.9886823296546936, 'train/loss': 0.038618121296167374, 'train/mean_average_precision': 0.21325482904122076, 'validation/accuracy': 0.9857100248336792, 'validation/loss': 0.048392053693532944, 'validation/mean_average_precision': 0.19136392204456362, 'validation/num_examples': 43793, 'test/accuracy': 0.9848099946975708, 'test/loss': 0.05121665447950363, 'test/mean_average_precision': 0.1928349290109782, 'test/num_examples': 43793, 'score': 1692.4591073989868, 'total_duration': 2594.3612883090973, 'accumulated_submission_time': 1692.4591073989868, 'accumulated_eval_time': 901.5599522590637, 'accumulated_logging_time': 0.19564485549926758}
I0205 13:25:37.709283 140002205157120 logging_writer.py:48] [5202] accumulated_eval_time=901.559952, accumulated_logging_time=0.195645, accumulated_submission_time=1692.459107, global_step=5202, preemption_count=0, score=1692.459107, test/accuracy=0.984810, test/loss=0.051217, test/mean_average_precision=0.192835, test/num_examples=43793, total_duration=2594.361288, train/accuracy=0.988682, train/loss=0.038618, train/mean_average_precision=0.213255, validation/accuracy=0.985710, validation/loss=0.048392, validation/mean_average_precision=0.191364, validation/num_examples=43793
I0205 13:26:09.366838 140020770670336 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.08124475926160812, loss=0.03628680482506752
I0205 13:26:41.222279 140002205157120 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.09355519711971283, loss=0.03742361441254616
I0205 13:27:13.252845 140020770670336 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.07483863085508347, loss=0.042656559497117996
I0205 13:27:45.254892 140002205157120 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.07152681797742844, loss=0.03584122285246849
I0205 13:28:17.601035 140020770670336 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.03068302758038044, loss=0.03651830926537514
I0205 13:28:49.378666 140002205157120 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.05555807799100876, loss=0.040273915976285934
I0205 13:29:21.363066 140020770670336 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.060995519161224365, loss=0.04070672765374184
I0205 13:29:37.965824 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:31:21.075471 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:31:25.962200 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:31:28.949133 140205209478976 submission_runner.py:408] Time since start: 2945.62s, 	Step: 5953, 	{'train/accuracy': 0.9889214038848877, 'train/loss': 0.0379679910838604, 'train/mean_average_precision': 0.24135911960156448, 'validation/accuracy': 0.9860051274299622, 'validation/loss': 0.04773245379328728, 'validation/mean_average_precision': 0.20286832028184776, 'validation/num_examples': 43793, 'test/accuracy': 0.9849632978439331, 'test/loss': 0.0504787340760231, 'test/mean_average_precision': 0.197353427924065, 'test/num_examples': 43793, 'score': 1932.6851942539215, 'total_duration': 2945.617326259613, 'accumulated_submission_time': 1932.6851942539215, 'accumulated_eval_time': 1012.5432093143463, 'accumulated_logging_time': 0.2226417064666748}
I0205 13:31:28.965967 140002598295296 logging_writer.py:48] [5953] accumulated_eval_time=1012.543209, accumulated_logging_time=0.222642, accumulated_submission_time=1932.685194, global_step=5953, preemption_count=0, score=1932.685194, test/accuracy=0.984963, test/loss=0.050479, test/mean_average_precision=0.197353, test/num_examples=43793, total_duration=2945.617326, train/accuracy=0.988921, train/loss=0.037968, train/mean_average_precision=0.241359, validation/accuracy=0.986005, validation/loss=0.047732, validation/mean_average_precision=0.202868, validation/num_examples=43793
I0205 13:31:44.233318 140044376827648 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.03665800392627716, loss=0.03273073211312294
I0205 13:32:16.296352 140002598295296 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.06276209652423859, loss=0.04137042909860611
I0205 13:32:48.274507 140044376827648 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.03948812931776047, loss=0.034731727093458176
I0205 13:33:20.415843 140002598295296 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.03412399813532829, loss=0.038845643401145935
I0205 13:33:52.576103 140044376827648 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.04853782802820206, loss=0.03612915426492691
I0205 13:34:24.555524 140002598295296 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.02942313626408577, loss=0.03560437634587288
I0205 13:34:56.329175 140044376827648 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.06704571843147278, loss=0.03864867612719536
I0205 13:35:28.020234 140002598295296 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.050375696271657944, loss=0.040082965046167374
I0205 13:35:28.951590 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:37:11.042952 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:37:14.113271 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:37:17.097764 140205209478976 submission_runner.py:408] Time since start: 3293.77s, 	Step: 6704, 	{'train/accuracy': 0.989250123500824, 'train/loss': 0.036856867372989655, 'train/mean_average_precision': 0.2679609941091441, 'validation/accuracy': 0.9859207272529602, 'validation/loss': 0.047123752534389496, 'validation/mean_average_precision': 0.20606608966205492, 'validation/num_examples': 43793, 'test/accuracy': 0.9851145148277283, 'test/loss': 0.04972551763057709, 'test/mean_average_precision': 0.20891194351793108, 'test/num_examples': 43793, 'score': 2172.6391608715057, 'total_duration': 3293.7659606933594, 'accumulated_submission_time': 2172.6391608715057, 'accumulated_eval_time': 1120.6893513202667, 'accumulated_logging_time': 0.2512521743774414}
I0205 13:37:17.114668 140021163808512 logging_writer.py:48] [6704] accumulated_eval_time=1120.689351, accumulated_logging_time=0.251252, accumulated_submission_time=2172.639161, global_step=6704, preemption_count=0, score=2172.639161, test/accuracy=0.985115, test/loss=0.049726, test/mean_average_precision=0.208912, test/num_examples=43793, total_duration=3293.765961, train/accuracy=0.989250, train/loss=0.036857, train/mean_average_precision=0.267961, validation/accuracy=0.985921, validation/loss=0.047124, validation/mean_average_precision=0.206066, validation/num_examples=43793
I0205 13:37:48.189703 140030420236032 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.03615046292543411, loss=0.03497691825032234
I0205 13:38:20.320190 140021163808512 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.029616832733154297, loss=0.034978531301021576
I0205 13:38:53.001965 140030420236032 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.04411381855607033, loss=0.036623504012823105
I0205 13:39:25.694605 140021163808512 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.028695154935121536, loss=0.03188589587807655
I0205 13:39:58.148696 140030420236032 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.042085662484169006, loss=0.036041200160980225
I0205 13:40:30.505470 140021163808512 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.02786571905016899, loss=0.035015325993299484
I0205 13:41:02.950662 140030420236032 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.0338432751595974, loss=0.03777812048792839
I0205 13:41:17.418799 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:43:01.989939 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:43:06.901115 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:43:10.033649 140205209478976 submission_runner.py:408] Time since start: 3646.70s, 	Step: 7446, 	{'train/accuracy': 0.9892452955245972, 'train/loss': 0.03657560423016548, 'train/mean_average_precision': 0.2556209530754341, 'validation/accuracy': 0.9861403107643127, 'validation/loss': 0.04662451893091202, 'validation/mean_average_precision': 0.21814077435373458, 'validation/num_examples': 43793, 'test/accuracy': 0.9852392077445984, 'test/loss': 0.04920124635100365, 'test/mean_average_precision': 0.21603020952659968, 'test/num_examples': 43793, 'score': 2412.912478208542, 'total_duration': 3646.7018489837646, 'accumulated_submission_time': 2412.912478208542, 'accumulated_eval_time': 1233.3041589260101, 'accumulated_logging_time': 0.2791018486022949}
I0205 13:43:10.050650 140044351649536 logging_writer.py:48] [7446] accumulated_eval_time=1233.304159, accumulated_logging_time=0.279102, accumulated_submission_time=2412.912478, global_step=7446, preemption_count=0, score=2412.912478, test/accuracy=0.985239, test/loss=0.049201, test/mean_average_precision=0.216030, test/num_examples=43793, total_duration=3646.701849, train/accuracy=0.989245, train/loss=0.036576, train/mean_average_precision=0.255621, validation/accuracy=0.986140, validation/loss=0.046625, validation/mean_average_precision=0.218141, validation/num_examples=43793
I0205 13:43:27.415845 140044376827648 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.026849880814552307, loss=0.038117729127407074
I0205 13:43:59.671787 140044351649536 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.03718904033303261, loss=0.038550712168216705
I0205 13:44:31.819967 140044376827648 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.0365467369556427, loss=0.04249819740653038
I0205 13:45:04.328280 140044351649536 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.027782417833805084, loss=0.03459607809782028
I0205 13:45:36.622066 140044376827648 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.03670559450984001, loss=0.03584287315607071
I0205 13:46:08.876639 140044351649536 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.03676208481192589, loss=0.03659077733755112
I0205 13:46:41.043795 140044376827648 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.02677123248577118, loss=0.03942650184035301
I0205 13:47:10.217173 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:48:53.739211 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:48:56.775101 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:48:59.745360 140205209478976 submission_runner.py:408] Time since start: 3996.41s, 	Step: 8192, 	{'train/accuracy': 0.9891985654830933, 'train/loss': 0.0363474003970623, 'train/mean_average_precision': 0.27777547186001494, 'validation/accuracy': 0.9861192107200623, 'validation/loss': 0.04642857611179352, 'validation/mean_average_precision': 0.2241906615481037, 'validation/num_examples': 43793, 'test/accuracy': 0.985280454158783, 'test/loss': 0.04895807430148125, 'test/mean_average_precision': 0.22005215130504002, 'test/num_examples': 43793, 'score': 2653.0472116470337, 'total_duration': 3996.4135501384735, 'accumulated_submission_time': 2653.0472116470337, 'accumulated_eval_time': 1342.8322954177856, 'accumulated_logging_time': 0.30754566192626953}
I0205 13:48:59.762176 140021163808512 logging_writer.py:48] [8192] accumulated_eval_time=1342.832295, accumulated_logging_time=0.307546, accumulated_submission_time=2653.047212, global_step=8192, preemption_count=0, score=2653.047212, test/accuracy=0.985280, test/loss=0.048958, test/mean_average_precision=0.220052, test/num_examples=43793, total_duration=3996.413550, train/accuracy=0.989199, train/loss=0.036347, train/mean_average_precision=0.277775, validation/accuracy=0.986119, validation/loss=0.046429, validation/mean_average_precision=0.224191, validation/num_examples=43793
I0205 13:49:02.749665 140030420236032 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.07914484292268753, loss=0.0361240953207016
I0205 13:49:34.600233 140021163808512 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.034777577966451645, loss=0.04100022837519646
I0205 13:50:06.624686 140030420236032 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.04461030662059784, loss=0.033002667129039764
I0205 13:50:38.588252 140021163808512 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.036769092082977295, loss=0.040218617767095566
I0205 13:51:10.578985 140030420236032 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.039574407041072845, loss=0.036582667380571365
I0205 13:51:42.536417 140021163808512 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.03909428417682648, loss=0.0372900664806366
I0205 13:52:14.212098 140030420236032 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.02355155535042286, loss=0.0350801907479763
I0205 13:52:45.999375 140021163808512 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.05029768869280815, loss=0.037819404155015945
I0205 13:52:59.913814 140205209478976 spec.py:321] Evaluating on the training split.
I0205 13:54:42.681238 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 13:54:45.732051 140205209478976 spec.py:349] Evaluating on the test split.
I0205 13:54:50.524519 140205209478976 submission_runner.py:408] Time since start: 4347.19s, 	Step: 8944, 	{'train/accuracy': 0.9893425703048706, 'train/loss': 0.0358811616897583, 'train/mean_average_precision': 0.27784066435915766, 'validation/accuracy': 0.9862227439880371, 'validation/loss': 0.0463167168200016, 'validation/mean_average_precision': 0.21345663477538704, 'validation/num_examples': 43793, 'test/accuracy': 0.9853727221488953, 'test/loss': 0.04898221045732498, 'test/mean_average_precision': 0.21924437850548517, 'test/num_examples': 43793, 'score': 2893.168367624283, 'total_duration': 4347.192717552185, 'accumulated_submission_time': 2893.168367624283, 'accumulated_eval_time': 1453.4429540634155, 'accumulated_logging_time': 0.335129976272583}
I0205 13:54:50.541417 140020770670336 logging_writer.py:48] [8944] accumulated_eval_time=1453.442954, accumulated_logging_time=0.335130, accumulated_submission_time=2893.168368, global_step=8944, preemption_count=0, score=2893.168368, test/accuracy=0.985373, test/loss=0.048982, test/mean_average_precision=0.219244, test/num_examples=43793, total_duration=4347.192718, train/accuracy=0.989343, train/loss=0.035881, train/mean_average_precision=0.277841, validation/accuracy=0.986223, validation/loss=0.046317, validation/mean_average_precision=0.213457, validation/num_examples=43793
I0205 13:55:08.729030 140044351649536 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.025211209431290627, loss=0.03374561667442322
I0205 13:55:40.626524 140020770670336 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.02623622864484787, loss=0.04073994234204292
I0205 13:56:12.483943 140044351649536 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.04405834153294563, loss=0.03623753786087036
I0205 13:56:44.105292 140020770670336 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.05487794429063797, loss=0.038417525589466095
I0205 13:57:15.925959 140044351649536 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.03164270892739296, loss=0.03375500068068504
I0205 13:57:47.540536 140020770670336 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.04548463225364685, loss=0.035340964794158936
I0205 13:58:19.209757 140044351649536 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.02572489343583584, loss=0.03159759193658829
I0205 13:58:50.690026 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:00:32.632338 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:00:35.686551 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:00:38.672628 140205209478976 submission_runner.py:408] Time since start: 4695.34s, 	Step: 9700, 	{'train/accuracy': 0.989570677280426, 'train/loss': 0.03492804989218712, 'train/mean_average_precision': 0.2965503820496965, 'validation/accuracy': 0.9864078760147095, 'validation/loss': 0.04607466980814934, 'validation/mean_average_precision': 0.23116527704377005, 'validation/num_examples': 43793, 'test/accuracy': 0.9855323433876038, 'test/loss': 0.04869677126407623, 'test/mean_average_precision': 0.22627039185858266, 'test/num_examples': 43793, 'score': 3133.286482810974, 'total_duration': 4695.340826034546, 'accumulated_submission_time': 3133.286482810974, 'accumulated_eval_time': 1561.425509929657, 'accumulated_logging_time': 0.36286282539367676}
I0205 14:00:38.689856 140021163808512 logging_writer.py:48] [9700] accumulated_eval_time=1561.425510, accumulated_logging_time=0.362863, accumulated_submission_time=3133.286483, global_step=9700, preemption_count=0, score=3133.286483, test/accuracy=0.985532, test/loss=0.048697, test/mean_average_precision=0.226270, test/num_examples=43793, total_duration=4695.340826, train/accuracy=0.989571, train/loss=0.034928, train/mean_average_precision=0.296550, validation/accuracy=0.986408, validation/loss=0.046075, validation/mean_average_precision=0.231165, validation/num_examples=43793
I0205 14:00:39.029018 140044376827648 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.03566066548228264, loss=0.03375450521707535
I0205 14:01:11.107024 140021163808512 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.041087038815021515, loss=0.037089753895998
I0205 14:01:43.393094 140044376827648 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.023782728239893913, loss=0.03230869397521019
I0205 14:02:15.515060 140021163808512 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.03228756785392761, loss=0.038150303065776825
I0205 14:02:47.398736 140044376827648 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.03550124540925026, loss=0.0368504598736763
I0205 14:03:19.579366 140021163808512 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.044189780950546265, loss=0.03705340996384621
I0205 14:03:52.115604 140044376827648 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.025627020746469498, loss=0.034704871475696564
I0205 14:04:24.433196 140021163808512 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.027350034564733505, loss=0.0302235409617424
I0205 14:04:38.790313 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:06:22.810093 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:06:25.888288 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:06:28.924183 140205209478976 submission_runner.py:408] Time since start: 5045.59s, 	Step: 10446, 	{'train/accuracy': 0.9897487759590149, 'train/loss': 0.03432413935661316, 'train/mean_average_precision': 0.31451403243094755, 'validation/accuracy': 0.9864723682403564, 'validation/loss': 0.04550735652446747, 'validation/mean_average_precision': 0.2354921471673552, 'validation/num_examples': 43793, 'test/accuracy': 0.9855976104736328, 'test/loss': 0.04816288873553276, 'test/mean_average_precision': 0.23218881704430824, 'test/num_examples': 43793, 'score': 3373.3551876544952, 'total_duration': 5045.592380523682, 'accumulated_submission_time': 3373.3551876544952, 'accumulated_eval_time': 1671.5593321323395, 'accumulated_logging_time': 0.3925163745880127}
I0205 14:06:28.941386 140020770670336 logging_writer.py:48] [10446] accumulated_eval_time=1671.559332, accumulated_logging_time=0.392516, accumulated_submission_time=3373.355188, global_step=10446, preemption_count=0, score=3373.355188, test/accuracy=0.985598, test/loss=0.048163, test/mean_average_precision=0.232189, test/num_examples=43793, total_duration=5045.592381, train/accuracy=0.989749, train/loss=0.034324, train/mean_average_precision=0.314514, validation/accuracy=0.986472, validation/loss=0.045507, validation/mean_average_precision=0.235492, validation/num_examples=43793
I0205 14:06:46.621380 140044351649536 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.03373705595731735, loss=0.03456356003880501
I0205 14:07:18.732325 140020770670336 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.041261762380599976, loss=0.03754907846450806
I0205 14:07:50.710911 140044351649536 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.03295128419995308, loss=0.03509612753987312
I0205 14:08:22.520111 140020770670336 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.03766443952918053, loss=0.031603436917066574
I0205 14:08:54.455776 140044351649536 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.041211728006601334, loss=0.033928755670785904
I0205 14:09:27.157913 140020770670336 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.037002917379140854, loss=0.035297032445669174
I0205 14:09:59.987305 140044351649536 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.029049646109342575, loss=0.033124975860118866
I0205 14:10:29.110284 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:12:11.206465 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:12:14.246455 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:12:17.259093 140205209478976 submission_runner.py:408] Time since start: 5393.93s, 	Step: 11191, 	{'train/accuracy': 0.9900938272476196, 'train/loss': 0.03343943879008293, 'train/mean_average_precision': 0.3254542392372632, 'validation/accuracy': 0.9865438342094421, 'validation/loss': 0.04487171396613121, 'validation/mean_average_precision': 0.24166720918696438, 'validation/num_examples': 43793, 'test/accuracy': 0.9856485724449158, 'test/loss': 0.04768587276339531, 'test/mean_average_precision': 0.24127745643876747, 'test/num_examples': 43793, 'score': 3613.492337703705, 'total_duration': 5393.927290678024, 'accumulated_submission_time': 3613.492337703705, 'accumulated_eval_time': 1779.7080972194672, 'accumulated_logging_time': 0.420879602432251}
I0205 14:12:17.276420 140021163808512 logging_writer.py:48] [11191] accumulated_eval_time=1779.708097, accumulated_logging_time=0.420880, accumulated_submission_time=3613.492338, global_step=11191, preemption_count=0, score=3613.492338, test/accuracy=0.985649, test/loss=0.047686, test/mean_average_precision=0.241277, test/num_examples=43793, total_duration=5393.927291, train/accuracy=0.990094, train/loss=0.033439, train/mean_average_precision=0.325454, validation/accuracy=0.986544, validation/loss=0.044872, validation/mean_average_precision=0.241667, validation/num_examples=43793
I0205 14:12:20.496781 140044376827648 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.029068071395158768, loss=0.032832782715559006
I0205 14:12:52.272515 140021163808512 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.053938765078783035, loss=0.036300674080848694
I0205 14:13:24.496455 140044376827648 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.028775103390216827, loss=0.03474903479218483
I0205 14:13:56.421158 140021163808512 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.03840325400233269, loss=0.03309847414493561
I0205 14:14:28.213727 140044376827648 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.044387202709913254, loss=0.029845893383026123
I0205 14:15:00.285387 140021163808512 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.039005670696496964, loss=0.03316526114940643
I0205 14:15:32.333288 140044376827648 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.041671425104141235, loss=0.034535001963377
I0205 14:16:04.809721 140021163808512 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.039330676198005676, loss=0.03702453523874283
I0205 14:16:17.318874 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:18:05.819825 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:18:08.926142 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:18:11.946504 140205209478976 submission_runner.py:408] Time since start: 5748.61s, 	Step: 11940, 	{'train/accuracy': 0.9900937676429749, 'train/loss': 0.03291260823607445, 'train/mean_average_precision': 0.34256466207079067, 'validation/accuracy': 0.9863781929016113, 'validation/loss': 0.04511989280581474, 'validation/mean_average_precision': 0.23639738856923834, 'validation/num_examples': 43793, 'test/accuracy': 0.9855904579162598, 'test/loss': 0.04759938642382622, 'test/mean_average_precision': 0.23495691603311641, 'test/num_examples': 43793, 'score': 3853.5046513080597, 'total_duration': 5748.61470246315, 'accumulated_submission_time': 3853.5046513080597, 'accumulated_eval_time': 1894.3356800079346, 'accumulated_logging_time': 0.4488670825958252}
I0205 14:18:11.964153 140030420236032 logging_writer.py:48] [11940] accumulated_eval_time=1894.335680, accumulated_logging_time=0.448867, accumulated_submission_time=3853.504651, global_step=11940, preemption_count=0, score=3853.504651, test/accuracy=0.985590, test/loss=0.047599, test/mean_average_precision=0.234957, test/num_examples=43793, total_duration=5748.614702, train/accuracy=0.990094, train/loss=0.032913, train/mean_average_precision=0.342565, validation/accuracy=0.986378, validation/loss=0.045120, validation/mean_average_precision=0.236397, validation/num_examples=43793
I0205 14:18:32.283297 140044351649536 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.03401792794466019, loss=0.03504181653261185
I0205 14:19:04.584000 140030420236032 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.03602295368909836, loss=0.031122060492634773
I0205 14:19:36.766396 140044351649536 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.03361941874027252, loss=0.03336360678076744
I0205 14:20:09.136679 140030420236032 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.03269883245229721, loss=0.03252634033560753
I0205 14:20:41.668735 140044351649536 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.03971780836582184, loss=0.030377568677067757
I0205 14:21:13.520488 140030420236032 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.04548681154847145, loss=0.030737105756998062
I0205 14:21:45.452520 140044351649536 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.03823312744498253, loss=0.0313246063888073
I0205 14:22:11.948607 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:23:57.689075 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:24:00.771652 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:24:03.893249 140205209478976 submission_runner.py:408] Time since start: 6100.56s, 	Step: 12683, 	{'train/accuracy': 0.9903525710105896, 'train/loss': 0.03204910084605217, 'train/mean_average_precision': 0.37730289269277073, 'validation/accuracy': 0.9866164922714233, 'validation/loss': 0.044836658984422684, 'validation/mean_average_precision': 0.25276370255248154, 'validation/num_examples': 43793, 'test/accuracy': 0.9857320189476013, 'test/loss': 0.047577790915966034, 'test/mean_average_precision': 0.24766468234854286, 'test/num_examples': 43793, 'score': 4093.458253622055, 'total_duration': 6100.5614466667175, 'accumulated_submission_time': 4093.458253622055, 'accumulated_eval_time': 2006.2802784442902, 'accumulated_logging_time': 0.4773871898651123}
I0205 14:24:03.910906 140020770670336 logging_writer.py:48] [12683] accumulated_eval_time=2006.280278, accumulated_logging_time=0.477387, accumulated_submission_time=4093.458254, global_step=12683, preemption_count=0, score=4093.458254, test/accuracy=0.985732, test/loss=0.047578, test/mean_average_precision=0.247665, test/num_examples=43793, total_duration=6100.561447, train/accuracy=0.990353, train/loss=0.032049, train/mean_average_precision=0.377303, validation/accuracy=0.986616, validation/loss=0.044837, validation/mean_average_precision=0.252764, validation/num_examples=43793
I0205 14:24:09.793328 140021163808512 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.04822712764143944, loss=0.03031192161142826
I0205 14:24:41.733960 140020770670336 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.05940572917461395, loss=0.03314020484685898
I0205 14:25:13.693095 140021163808512 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.04622706025838852, loss=0.03251929208636284
I0205 14:25:45.666585 140020770670336 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.07819373160600662, loss=0.038030464202165604
I0205 14:26:18.663290 140021163808512 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.03922509774565697, loss=0.03685339540243149
I0205 14:26:51.928314 140020770670336 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.03385353088378906, loss=0.028752032667398453
I0205 14:27:25.132471 140021163808512 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0428135022521019, loss=0.033633626997470856
I0205 14:27:58.485085 140020770670336 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.046973664313554764, loss=0.034821875393390656
I0205 14:28:03.932505 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:29:52.400727 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:29:55.721210 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:29:58.988564 140205209478976 submission_runner.py:408] Time since start: 6455.66s, 	Step: 13417, 	{'train/accuracy': 0.9905537962913513, 'train/loss': 0.031089400872588158, 'train/mean_average_precision': 0.37240313133773384, 'validation/accuracy': 0.9866245985031128, 'validation/loss': 0.0445709228515625, 'validation/mean_average_precision': 0.24922931526205078, 'validation/num_examples': 43793, 'test/accuracy': 0.9857416749000549, 'test/loss': 0.04735759273171425, 'test/mean_average_precision': 0.24515711802698764, 'test/num_examples': 43793, 'score': 4333.447480678558, 'total_duration': 6455.656747102737, 'accumulated_submission_time': 4333.447480678558, 'accumulated_eval_time': 2121.3362860679626, 'accumulated_logging_time': 0.5057508945465088}
I0205 14:29:59.008198 140030420236032 logging_writer.py:48] [13417] accumulated_eval_time=2121.336286, accumulated_logging_time=0.505751, accumulated_submission_time=4333.447481, global_step=13417, preemption_count=0, score=4333.447481, test/accuracy=0.985742, test/loss=0.047358, test/mean_average_precision=0.245157, test/num_examples=43793, total_duration=6455.656747, train/accuracy=0.990554, train/loss=0.031089, train/mean_average_precision=0.372403, validation/accuracy=0.986625, validation/loss=0.044571, validation/mean_average_precision=0.249229, validation/num_examples=43793
I0205 14:30:26.383117 140044376827648 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.035789649933576584, loss=0.02988625317811966
I0205 14:30:59.092631 140030420236032 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.05325577035546303, loss=0.03048381768167019
I0205 14:31:31.692426 140044376827648 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.04790139198303223, loss=0.03162473812699318
I0205 14:32:04.849881 140030420236032 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.045493628829717636, loss=0.029274027794599533
I0205 14:32:37.351341 140044376827648 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.05835563689470291, loss=0.029241586104035378
I0205 14:33:10.170760 140030420236032 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.052355941385030746, loss=0.03199891000986099
I0205 14:33:42.948741 140044376827648 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.053614720702171326, loss=0.028412844985723495
I0205 14:33:59.241898 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:35:40.913381 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:35:43.992030 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:35:47.046495 140205209478976 submission_runner.py:408] Time since start: 6803.71s, 	Step: 14151, 	{'train/accuracy': 0.9907062649726868, 'train/loss': 0.030991319566965103, 'train/mean_average_precision': 0.37859976456395106, 'validation/accuracy': 0.9865682125091553, 'validation/loss': 0.044715654104948044, 'validation/mean_average_precision': 0.25239165412918096, 'validation/num_examples': 43793, 'test/accuracy': 0.9856612086296082, 'test/loss': 0.047581084072589874, 'test/mean_average_precision': 0.24578013946410093, 'test/num_examples': 43793, 'score': 4573.6459176540375, 'total_duration': 6803.714690446854, 'accumulated_submission_time': 4573.6459176540375, 'accumulated_eval_time': 2229.1408503055573, 'accumulated_logging_time': 0.5367867946624756}
I0205 14:35:47.065059 140020770670336 logging_writer.py:48] [14151] accumulated_eval_time=2229.140850, accumulated_logging_time=0.536787, accumulated_submission_time=4573.645918, global_step=14151, preemption_count=0, score=4573.645918, test/accuracy=0.985661, test/loss=0.047581, test/mean_average_precision=0.245780, test/num_examples=43793, total_duration=6803.714690, train/accuracy=0.990706, train/loss=0.030991, train/mean_average_precision=0.378600, validation/accuracy=0.986568, validation/loss=0.044716, validation/mean_average_precision=0.252392, validation/num_examples=43793
I0205 14:36:03.294846 140044351649536 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.05301140993833542, loss=0.028447657823562622
I0205 14:36:35.262868 140020770670336 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.07867538928985596, loss=0.03377837315201759
I0205 14:37:07.130822 140044351649536 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.08971460163593292, loss=0.03212406113743782
I0205 14:37:39.214253 140020770670336 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.04834359511733055, loss=0.028581960126757622
I0205 14:38:11.079536 140044351649536 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.06522704660892487, loss=0.0331927165389061
I0205 14:38:42.871940 140020770670336 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.06458679586648941, loss=0.03646999970078468
I0205 14:39:14.706901 140044351649536 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0559057891368866, loss=0.031848352402448654
I0205 14:39:46.461337 140020770670336 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.08337632566690445, loss=0.03291720524430275
I0205 14:39:47.096205 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:41:33.407565 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:41:38.511920 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:41:41.554785 140205209478976 submission_runner.py:408] Time since start: 7158.22s, 	Step: 14903, 	{'train/accuracy': 0.9905675649642944, 'train/loss': 0.03123924322426319, 'train/mean_average_precision': 0.39005511484235345, 'validation/accuracy': 0.9866558909416199, 'validation/loss': 0.04476241394877434, 'validation/mean_average_precision': 0.25344476467938665, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.047421928495168686, 'test/mean_average_precision': 0.2527017177476719, 'test/num_examples': 43793, 'score': 4813.645154476166, 'total_duration': 7158.222968816757, 'accumulated_submission_time': 4813.645154476166, 'accumulated_eval_time': 2343.599368095398, 'accumulated_logging_time': 0.567765474319458}
I0205 14:41:41.573112 140021163808512 logging_writer.py:48] [14903] accumulated_eval_time=2343.599368, accumulated_logging_time=0.567765, accumulated_submission_time=4813.645154, global_step=14903, preemption_count=0, score=4813.645154, test/accuracy=0.985814, test/loss=0.047422, test/mean_average_precision=0.252702, test/num_examples=43793, total_duration=7158.222969, train/accuracy=0.990568, train/loss=0.031239, train/mean_average_precision=0.390055, validation/accuracy=0.986656, validation/loss=0.044762, validation/mean_average_precision=0.253445, validation/num_examples=43793
I0205 14:42:12.881821 140030420236032 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.06036620959639549, loss=0.031371574848890305
I0205 14:42:44.940962 140021163808512 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.059185244143009186, loss=0.035736143589019775
I0205 14:43:17.220575 140030420236032 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.05245202034711838, loss=0.035618871450424194
I0205 14:43:49.600645 140021163808512 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.05997764691710472, loss=0.03155472129583359
I0205 14:44:21.828100 140030420236032 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.060205332934856415, loss=0.031950321048498154
I0205 14:44:53.462815 140021163808512 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.06969127804040909, loss=0.03245984762907028
I0205 14:45:25.540624 140030420236032 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.06599410623311996, loss=0.03231307491660118
I0205 14:45:41.861512 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:47:21.800165 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:47:24.851677 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:47:27.889288 140205209478976 submission_runner.py:408] Time since start: 7504.56s, 	Step: 15652, 	{'train/accuracy': 0.9906066060066223, 'train/loss': 0.031121844425797462, 'train/mean_average_precision': 0.38131995024157306, 'validation/accuracy': 0.9867772459983826, 'validation/loss': 0.04441023990511894, 'validation/mean_average_precision': 0.2583747608772624, 'validation/num_examples': 43793, 'test/accuracy': 0.9858461618423462, 'test/loss': 0.0471496619284153, 'test/mean_average_precision': 0.2551532981849261, 'test/num_examples': 43793, 'score': 5053.900189876556, 'total_duration': 7504.557371377945, 'accumulated_submission_time': 5053.900189876556, 'accumulated_eval_time': 2449.6269824504852, 'accumulated_logging_time': 0.5992722511291504}
I0205 14:47:27.908319 140020770670336 logging_writer.py:48] [15652] accumulated_eval_time=2449.626982, accumulated_logging_time=0.599272, accumulated_submission_time=5053.900190, global_step=15652, preemption_count=0, score=5053.900190, test/accuracy=0.985846, test/loss=0.047150, test/mean_average_precision=0.255153, test/num_examples=43793, total_duration=7504.557371, train/accuracy=0.990607, train/loss=0.031122, train/mean_average_precision=0.381320, validation/accuracy=0.986777, validation/loss=0.044410, validation/mean_average_precision=0.258375, validation/num_examples=43793
I0205 14:47:43.534693 140044376827648 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0528128556907177, loss=0.028067447245121002
I0205 14:48:15.777996 140020770670336 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0658341720700264, loss=0.030113831162452698
I0205 14:48:48.010693 140044376827648 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.06556220352649689, loss=0.028795430436730385
I0205 14:49:20.132818 140020770670336 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.08741749078035355, loss=0.03213083744049072
I0205 14:49:52.795238 140044376827648 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.055623240768909454, loss=0.030130909755825996
I0205 14:50:25.542773 140020770670336 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.07356718182563782, loss=0.0327397957444191
I0205 14:50:57.934774 140044376827648 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.07939664274454117, loss=0.03137093409895897
I0205 14:51:28.128320 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:53:11.933092 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:53:15.026415 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:53:20.017485 140205209478976 submission_runner.py:408] Time since start: 7856.69s, 	Step: 16395, 	{'train/accuracy': 0.9906516671180725, 'train/loss': 0.030857114121317863, 'train/mean_average_precision': 0.38694554789878394, 'validation/accuracy': 0.9865178465843201, 'validation/loss': 0.04490754380822182, 'validation/mean_average_precision': 0.2555116961913109, 'validation/num_examples': 43793, 'test/accuracy': 0.985654890537262, 'test/loss': 0.047570016235113144, 'test/mean_average_precision': 0.24621899625449672, 'test/num_examples': 43793, 'score': 5294.087451457977, 'total_duration': 7856.6856853961945, 'accumulated_submission_time': 5294.087451457977, 'accumulated_eval_time': 2561.516103744507, 'accumulated_logging_time': 0.6315131187438965}
I0205 14:53:20.035761 140030420236032 logging_writer.py:48] [16395] accumulated_eval_time=2561.516104, accumulated_logging_time=0.631513, accumulated_submission_time=5294.087451, global_step=16395, preemption_count=0, score=5294.087451, test/accuracy=0.985655, test/loss=0.047570, test/mean_average_precision=0.246219, test/num_examples=43793, total_duration=7856.685685, train/accuracy=0.990652, train/loss=0.030857, train/mean_average_precision=0.386946, validation/accuracy=0.986518, validation/loss=0.044908, validation/mean_average_precision=0.255512, validation/num_examples=43793
I0205 14:53:22.030452 140044351649536 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.08387788385152817, loss=0.030557498335838318
I0205 14:53:54.609467 140030420236032 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.06317894905805588, loss=0.029754823073744774
I0205 14:54:26.752573 140044351649536 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.07102056592702866, loss=0.032880645245313644
I0205 14:54:58.519345 140030420236032 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.06702443212270737, loss=0.030784400179982185
I0205 14:55:30.730404 140044351649536 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.059911977499723434, loss=0.03294624760746956
I0205 14:56:02.961726 140030420236032 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.12630881369113922, loss=0.03237360343337059
I0205 14:56:34.829322 140044351649536 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.06656999886035919, loss=0.03085116110742092
I0205 14:57:07.072715 140030420236032 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.06807847321033478, loss=0.026694146916270256
I0205 14:57:20.064517 140205209478976 spec.py:321] Evaluating on the training split.
I0205 14:59:02.591662 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 14:59:05.690822 140205209478976 spec.py:349] Evaluating on the test split.
I0205 14:59:08.703105 140205209478976 submission_runner.py:408] Time since start: 8205.37s, 	Step: 17141, 	{'train/accuracy': 0.990618884563446, 'train/loss': 0.03099219873547554, 'train/mean_average_precision': 0.38567776672741516, 'validation/accuracy': 0.9865024089813232, 'validation/loss': 0.044638730585575104, 'validation/mean_average_precision': 0.26013765900164426, 'validation/num_examples': 43793, 'test/accuracy': 0.9855812191963196, 'test/loss': 0.04733843356370926, 'test/mean_average_precision': 0.25248843629595097, 'test/num_examples': 43793, 'score': 5534.083811998367, 'total_duration': 8205.371302127838, 'accumulated_submission_time': 5534.083811998367, 'accumulated_eval_time': 2670.1546428203583, 'accumulated_logging_time': 0.6623170375823975}
I0205 14:59:08.722668 140020770670336 logging_writer.py:48] [17141] accumulated_eval_time=2670.154643, accumulated_logging_time=0.662317, accumulated_submission_time=5534.083812, global_step=17141, preemption_count=0, score=5534.083812, test/accuracy=0.985581, test/loss=0.047338, test/mean_average_precision=0.252488, test/num_examples=43793, total_duration=8205.371302, train/accuracy=0.990619, train/loss=0.030992, train/mean_average_precision=0.385678, validation/accuracy=0.986502, validation/loss=0.044639, validation/mean_average_precision=0.260138, validation/num_examples=43793
I0205 14:59:28.044925 140044376827648 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.07714841514825821, loss=0.031105972826480865
I0205 15:00:00.631771 140020770670336 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.05071267485618591, loss=0.030722742900252342
I0205 15:00:32.803173 140044376827648 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.07793234288692474, loss=0.035559289157390594
I0205 15:01:04.915461 140020770670336 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.10867419093847275, loss=0.032742228358983994
I0205 15:01:36.882396 140044376827648 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.06366966664791107, loss=0.03091944195330143
I0205 15:02:09.260025 140020770670336 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.06812234967947006, loss=0.030070725828409195
I0205 15:02:41.340608 140044376827648 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.060484178364276886, loss=0.03001927025616169
I0205 15:03:08.979732 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:04:52.869976 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:04:55.887157 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:05:00.782304 140205209478976 submission_runner.py:408] Time since start: 8557.45s, 	Step: 17887, 	{'train/accuracy': 0.9904924035072327, 'train/loss': 0.031084245070815086, 'train/mean_average_precision': 0.39302518302628175, 'validation/accuracy': 0.9865880608558655, 'validation/loss': 0.0455077663064003, 'validation/mean_average_precision': 0.2632867709525448, 'validation/num_examples': 43793, 'test/accuracy': 0.9856747388839722, 'test/loss': 0.048309292644262314, 'test/mean_average_precision': 0.24612998964632649, 'test/num_examples': 43793, 'score': 5774.310196876526, 'total_duration': 8557.450502157211, 'accumulated_submission_time': 5774.310196876526, 'accumulated_eval_time': 2781.9571702480316, 'accumulated_logging_time': 0.6927800178527832}
I0205 15:05:00.801353 140021163808512 logging_writer.py:48] [17887] accumulated_eval_time=2781.957170, accumulated_logging_time=0.692780, accumulated_submission_time=5774.310197, global_step=17887, preemption_count=0, score=5774.310197, test/accuracy=0.985675, test/loss=0.048309, test/mean_average_precision=0.246130, test/num_examples=43793, total_duration=8557.450502, train/accuracy=0.990492, train/loss=0.031084, train/mean_average_precision=0.393025, validation/accuracy=0.986588, validation/loss=0.045508, validation/mean_average_precision=0.263287, validation/num_examples=43793
I0205 15:05:05.356699 140030420236032 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.09503630548715591, loss=0.03142232820391655
I0205 15:05:37.587636 140021163808512 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.11104239523410797, loss=0.034852758049964905
I0205 15:06:09.624078 140030420236032 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.12196791172027588, loss=0.03145591914653778
I0205 15:06:41.722961 140021163808512 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.06937907636165619, loss=0.028236495330929756
I0205 15:07:13.934170 140030420236032 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.09366415441036224, loss=0.03275809437036514
I0205 15:07:45.908082 140021163808512 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.11428145319223404, loss=0.028647005558013916
I0205 15:08:18.100727 140030420236032 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.10086213052272797, loss=0.031042395159602165
I0205 15:08:50.560966 140021163808512 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.07521149516105652, loss=0.032625507563352585
I0205 15:09:00.832581 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:10:41.367129 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:10:44.559489 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:10:47.661747 140205209478976 submission_runner.py:408] Time since start: 8904.33s, 	Step: 18633, 	{'train/accuracy': 0.9910005927085876, 'train/loss': 0.02931615523993969, 'train/mean_average_precision': 0.42218359821230533, 'validation/accuracy': 0.9868016242980957, 'validation/loss': 0.04465004429221153, 'validation/mean_average_precision': 0.26313624471730956, 'validation/num_examples': 43793, 'test/accuracy': 0.9858941435813904, 'test/loss': 0.047337278723716736, 'test/mean_average_precision': 0.2532866247875107, 'test/num_examples': 43793, 'score': 6014.308893442154, 'total_duration': 8904.329939126968, 'accumulated_submission_time': 6014.308893442154, 'accumulated_eval_time': 2888.7862842082977, 'accumulated_logging_time': 0.7243075370788574}
I0205 15:10:47.680379 140020770670336 logging_writer.py:48] [18633] accumulated_eval_time=2888.786284, accumulated_logging_time=0.724308, accumulated_submission_time=6014.308893, global_step=18633, preemption_count=0, score=6014.308893, test/accuracy=0.985894, test/loss=0.047337, test/mean_average_precision=0.253287, test/num_examples=43793, total_duration=8904.329939, train/accuracy=0.991001, train/loss=0.029316, train/mean_average_precision=0.422184, validation/accuracy=0.986802, validation/loss=0.044650, validation/mean_average_precision=0.263136, validation/num_examples=43793
I0205 15:11:09.610370 140044351649536 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.09326198697090149, loss=0.03393392264842987
I0205 15:11:41.498348 140020770670336 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.10619977861642838, loss=0.03136143833398819
I0205 15:12:13.235522 140044351649536 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.08203637599945068, loss=0.030441606417298317
I0205 15:12:45.039383 140020770670336 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.08232802152633667, loss=0.029808878898620605
I0205 15:13:17.018368 140044351649536 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.10940475761890411, loss=0.029726507142186165
I0205 15:13:48.905750 140020770670336 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.0753149539232254, loss=0.02744712121784687
I0205 15:14:20.930524 140044351649536 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.06756652146577835, loss=0.03142162784934044
I0205 15:14:47.888141 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:16:33.444315 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:16:36.539107 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:16:39.530868 140205209478976 submission_runner.py:408] Time since start: 9256.20s, 	Step: 19385, 	{'train/accuracy': 0.991144597530365, 'train/loss': 0.028853613883256912, 'train/mean_average_precision': 0.4362776975542201, 'validation/accuracy': 0.9868060946464539, 'validation/loss': 0.044303517788648605, 'validation/mean_average_precision': 0.27145992009299347, 'validation/num_examples': 43793, 'test/accuracy': 0.9859779477119446, 'test/loss': 0.0470069982111454, 'test/mean_average_precision': 0.25648081370632786, 'test/num_examples': 43793, 'score': 6254.484782218933, 'total_duration': 9256.199064016342, 'accumulated_submission_time': 6254.484782218933, 'accumulated_eval_time': 3000.428964138031, 'accumulated_logging_time': 0.754981517791748}
I0205 15:16:39.550766 140021163808512 logging_writer.py:48] [19385] accumulated_eval_time=3000.428964, accumulated_logging_time=0.754982, accumulated_submission_time=6254.484782, global_step=19385, preemption_count=0, score=6254.484782, test/accuracy=0.985978, test/loss=0.047007, test/mean_average_precision=0.256481, test/num_examples=43793, total_duration=9256.199064, train/accuracy=0.991145, train/loss=0.028854, train/mean_average_precision=0.436278, validation/accuracy=0.986806, validation/loss=0.044304, validation/mean_average_precision=0.271460, validation/num_examples=43793
I0205 15:16:44.665776 140044376827648 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.07986476272344589, loss=0.029361432418227196
I0205 15:17:16.443404 140021163808512 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.07785342633724213, loss=0.032581303268671036
I0205 15:17:48.291794 140044376827648 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.0743965283036232, loss=0.028207877650856972
I0205 15:18:20.099205 140021163808512 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.062145452946424484, loss=0.028600994497537613
I0205 15:18:51.650068 140044376827648 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.11424782127141953, loss=0.03063289448618889
I0205 15:19:23.900476 140021163808512 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.11780153214931488, loss=0.028650321066379547
I0205 15:19:55.868226 140044376827648 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.07119805365800858, loss=0.03152315691113472
I0205 15:20:28.058887 140021163808512 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.06751106679439545, loss=0.027730463072657585
I0205 15:20:39.600569 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:22:19.569780 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:22:22.646682 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:22:25.660091 140205209478976 submission_runner.py:408] Time since start: 9602.33s, 	Step: 20137, 	{'train/accuracy': 0.9912317991256714, 'train/loss': 0.028409577906131744, 'train/mean_average_precision': 0.45106485488955966, 'validation/accuracy': 0.9867587685585022, 'validation/loss': 0.044544562697410583, 'validation/mean_average_precision': 0.2627217483865065, 'validation/num_examples': 43793, 'test/accuracy': 0.9858676195144653, 'test/loss': 0.04751855134963989, 'test/mean_average_precision': 0.2565515376288197, 'test/num_examples': 43793, 'score': 6494.502206325531, 'total_duration': 9602.328292369843, 'accumulated_submission_time': 6494.502206325531, 'accumulated_eval_time': 3106.488452196121, 'accumulated_logging_time': 0.7869946956634521}
I0205 15:22:25.679004 140020770670336 logging_writer.py:48] [20137] accumulated_eval_time=3106.488452, accumulated_logging_time=0.786995, accumulated_submission_time=6494.502206, global_step=20137, preemption_count=0, score=6494.502206, test/accuracy=0.985868, test/loss=0.047519, test/mean_average_precision=0.256552, test/num_examples=43793, total_duration=9602.328292, train/accuracy=0.991232, train/loss=0.028410, train/mean_average_precision=0.451065, validation/accuracy=0.986759, validation/loss=0.044545, validation/mean_average_precision=0.262722, validation/num_examples=43793
I0205 15:22:45.931757 140044351649536 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.08392026275396347, loss=0.030184544622898102
I0205 15:23:17.662787 140020770670336 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.09206973016262054, loss=0.030741849914193153
I0205 15:23:49.578596 140044351649536 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.10978636145591736, loss=0.03388574719429016
I0205 15:24:21.726507 140020770670336 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.0684845894575119, loss=0.02826816402375698
I0205 15:24:53.559988 140044351649536 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.11011210829019547, loss=0.028243497014045715
I0205 15:25:25.369769 140020770670336 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.06809720396995544, loss=0.02893480472266674
I0205 15:25:56.675495 140044351649536 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.0889686793088913, loss=0.02840498276054859
I0205 15:26:25.910923 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:28:09.161248 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:28:14.152244 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:28:17.162458 140205209478976 submission_runner.py:408] Time since start: 9953.83s, 	Step: 20892, 	{'train/accuracy': 0.9913666248321533, 'train/loss': 0.028140483424067497, 'train/mean_average_precision': 0.4477376798318661, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.04500306025147438, 'validation/mean_average_precision': 0.2594265373774643, 'validation/num_examples': 43793, 'test/accuracy': 0.985849916934967, 'test/loss': 0.04767508804798126, 'test/mean_average_precision': 0.2547641389335, 'test/num_examples': 43793, 'score': 6734.702629804611, 'total_duration': 9953.830657482147, 'accumulated_submission_time': 6734.702629804611, 'accumulated_eval_time': 3217.7399446964264, 'accumulated_logging_time': 0.8169877529144287}
I0205 15:28:17.182041 140021163808512 logging_writer.py:48] [20892] accumulated_eval_time=3217.739945, accumulated_logging_time=0.816988, accumulated_submission_time=6734.702630, global_step=20892, preemption_count=0, score=6734.702630, test/accuracy=0.985850, test/loss=0.047675, test/mean_average_precision=0.254764, test/num_examples=43793, total_duration=9953.830657, train/accuracy=0.991367, train/loss=0.028140, train/mean_average_precision=0.447738, validation/accuracy=0.986681, validation/loss=0.045003, validation/mean_average_precision=0.259427, validation/num_examples=43793
I0205 15:28:20.124293 140044376827648 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.08487552404403687, loss=0.027024703100323677
I0205 15:28:52.736020 140021163808512 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.1499277502298355, loss=0.031760696321725845
I0205 15:29:25.297204 140044376827648 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.09092923998832703, loss=0.032722119241952896
I0205 15:29:57.831654 140021163808512 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.07823760062456131, loss=0.030138202011585236
I0205 15:30:29.972608 140044376827648 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.0978226512670517, loss=0.02804834023118019
I0205 15:31:02.345562 140021163808512 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.08110078424215317, loss=0.03106151521205902
I0205 15:31:34.506869 140044376827648 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.10156834125518799, loss=0.028561988845467567
I0205 15:32:07.001258 140021163808512 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.0827149748802185, loss=0.03056662157177925
I0205 15:32:17.169137 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:34:03.452893 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:34:06.483462 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:34:09.496325 140205209478976 submission_runner.py:408] Time since start: 10306.16s, 	Step: 21632, 	{'train/accuracy': 0.9910850524902344, 'train/loss': 0.02917107380926609, 'train/mean_average_precision': 0.41867738324957493, 'validation/accuracy': 0.986506462097168, 'validation/loss': 0.044595010578632355, 'validation/mean_average_precision': 0.260642723999837, 'validation/num_examples': 43793, 'test/accuracy': 0.9857699275016785, 'test/loss': 0.047313250601291656, 'test/mean_average_precision': 0.25456625768344016, 'test/num_examples': 43793, 'score': 6974.657138586044, 'total_duration': 10306.164506912231, 'accumulated_submission_time': 6974.657138586044, 'accumulated_eval_time': 3330.0670692920685, 'accumulated_logging_time': 0.84893798828125}
I0205 15:34:09.516465 140030420236032 logging_writer.py:48] [21632] accumulated_eval_time=3330.067069, accumulated_logging_time=0.848938, accumulated_submission_time=6974.657139, global_step=21632, preemption_count=0, score=6974.657139, test/accuracy=0.985770, test/loss=0.047313, test/mean_average_precision=0.254566, test/num_examples=43793, total_duration=10306.164507, train/accuracy=0.991085, train/loss=0.029171, train/mean_average_precision=0.418677, validation/accuracy=0.986506, validation/loss=0.044595, validation/mean_average_precision=0.260643, validation/num_examples=43793
I0205 15:34:32.113408 140044351649536 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.10285976529121399, loss=0.028859401121735573
I0205 15:35:04.338450 140030420236032 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.08760599792003632, loss=0.028921382501721382
I0205 15:35:36.719230 140044351649536 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.1162361428141594, loss=0.029254799708724022
I0205 15:36:08.755704 140030420236032 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.09502483159303665, loss=0.02616194076836109
I0205 15:36:40.578775 140044351649536 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.08768584579229355, loss=0.03141291067004204
I0205 15:37:12.705825 140030420236032 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.10018680989742279, loss=0.03302587941288948
I0205 15:37:44.485022 140044351649536 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.09360111504793167, loss=0.03169868141412735
I0205 15:38:09.531898 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:39:52.951882 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:39:57.972886 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:40:00.933233 140205209478976 submission_runner.py:408] Time since start: 10657.60s, 	Step: 22380, 	{'train/accuracy': 0.9910112023353577, 'train/loss': 0.02939889393746853, 'train/mean_average_precision': 0.413999674447663, 'validation/accuracy': 0.9867098927497864, 'validation/loss': 0.04477539658546448, 'validation/mean_average_precision': 0.26131974641292277, 'validation/num_examples': 43793, 'test/accuracy': 0.9858149886131287, 'test/loss': 0.04739511385560036, 'test/mean_average_precision': 0.26023518949880103, 'test/num_examples': 43793, 'score': 7214.641557455063, 'total_duration': 10657.6014316082, 'accumulated_submission_time': 7214.641557455063, 'accumulated_eval_time': 3441.468363761902, 'accumulated_logging_time': 0.8800814151763916}
I0205 15:40:00.953858 140020770670336 logging_writer.py:48] [22380] accumulated_eval_time=3441.468364, accumulated_logging_time=0.880081, accumulated_submission_time=7214.641557, global_step=22380, preemption_count=0, score=7214.641557, test/accuracy=0.985815, test/loss=0.047395, test/mean_average_precision=0.260235, test/num_examples=43793, total_duration=10657.601432, train/accuracy=0.991011, train/loss=0.029399, train/mean_average_precision=0.414000, validation/accuracy=0.986710, validation/loss=0.044775, validation/mean_average_precision=0.261320, validation/num_examples=43793
I0205 15:40:07.712753 140044376827648 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.0963747501373291, loss=0.02980394847691059
I0205 15:40:39.503760 140020770670336 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.0902915671467781, loss=0.0325813889503479
I0205 15:41:11.448597 140044376827648 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.07751255482435226, loss=0.028492698445916176
I0205 15:41:43.444702 140020770670336 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.12575370073318481, loss=0.027413589879870415
I0205 15:42:15.294659 140044376827648 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.11008848994970322, loss=0.032226577401161194
I0205 15:42:47.142388 140020770670336 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.08040983229875565, loss=0.027526896446943283
I0205 15:43:18.797785 140044376827648 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.11904878914356232, loss=0.032666973769664764
I0205 15:43:50.841137 140020770670336 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.0865173190832138, loss=0.02741757594048977
I0205 15:44:00.960701 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:45:44.003887 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:45:47.053775 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:45:50.056958 140205209478976 submission_runner.py:408] Time since start: 11006.73s, 	Step: 23133, 	{'train/accuracy': 0.9910687804222107, 'train/loss': 0.02927219867706299, 'train/mean_average_precision': 0.42751952278599203, 'validation/accuracy': 0.9866611361503601, 'validation/loss': 0.04471036046743393, 'validation/mean_average_precision': 0.2611749950881226, 'validation/num_examples': 43793, 'test/accuracy': 0.9857193827629089, 'test/loss': 0.04752102494239807, 'test/mean_average_precision': 0.256449131398079, 'test/num_examples': 43793, 'score': 7454.6159999370575, 'total_duration': 11006.72515630722, 'accumulated_submission_time': 7454.6159999370575, 'accumulated_eval_time': 3550.5645759105682, 'accumulated_logging_time': 0.9132204055786133}
I0205 15:45:50.076559 140030420236032 logging_writer.py:48] [23133] accumulated_eval_time=3550.564576, accumulated_logging_time=0.913220, accumulated_submission_time=7454.616000, global_step=23133, preemption_count=0, score=7454.616000, test/accuracy=0.985719, test/loss=0.047521, test/mean_average_precision=0.256449, test/num_examples=43793, total_duration=11006.725156, train/accuracy=0.991069, train/loss=0.029272, train/mean_average_precision=0.427520, validation/accuracy=0.986661, validation/loss=0.044710, validation/mean_average_precision=0.261175, validation/num_examples=43793
I0205 15:46:11.963083 140044351649536 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.10556472837924957, loss=0.031950343400239944
I0205 15:46:43.824773 140030420236032 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.09933172166347504, loss=0.030182646587491035
I0205 15:47:15.706943 140044351649536 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.10312429815530777, loss=0.0269260723143816
I0205 15:47:47.692719 140030420236032 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.13095901906490326, loss=0.02871563658118248
I0205 15:48:19.723840 140044351649536 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.11512202024459839, loss=0.02700054459273815
I0205 15:48:51.839450 140030420236032 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.09372259676456451, loss=0.03005516342818737
I0205 15:49:24.122114 140044351649536 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.081783227622509, loss=0.030677534639835358
I0205 15:49:50.284654 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:51:37.049994 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:51:42.041320 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:51:45.038297 140205209478976 submission_runner.py:408] Time since start: 11361.71s, 	Step: 23882, 	{'train/accuracy': 0.9910465478897095, 'train/loss': 0.029097504913806915, 'train/mean_average_precision': 0.43902965739375505, 'validation/accuracy': 0.986792266368866, 'validation/loss': 0.04489562287926674, 'validation/mean_average_precision': 0.26428720016624674, 'validation/num_examples': 43793, 'test/accuracy': 0.9859809279441833, 'test/loss': 0.04757880046963692, 'test/mean_average_precision': 0.25833596487584526, 'test/num_examples': 43793, 'score': 7694.792117834091, 'total_duration': 11361.706496953964, 'accumulated_submission_time': 7694.792117834091, 'accumulated_eval_time': 3665.3181777000427, 'accumulated_logging_time': 0.9451286792755127}
I0205 15:51:45.058614 140020770670336 logging_writer.py:48] [23882] accumulated_eval_time=3665.318178, accumulated_logging_time=0.945129, accumulated_submission_time=7694.792118, global_step=23882, preemption_count=0, score=7694.792118, test/accuracy=0.985981, test/loss=0.047579, test/mean_average_precision=0.258336, test/num_examples=43793, total_duration=11361.706497, train/accuracy=0.991047, train/loss=0.029098, train/mean_average_precision=0.439030, validation/accuracy=0.986792, validation/loss=0.044896, validation/mean_average_precision=0.264287, validation/num_examples=43793
I0205 15:51:51.180660 140044376827648 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.09419485181570053, loss=0.02822345681488514
I0205 15:52:23.265266 140020770670336 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.11633724719285965, loss=0.028372567147016525
I0205 15:52:55.222785 140044376827648 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.17032328248023987, loss=0.035523030906915665
I0205 15:53:28.161210 140020770670336 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.09110015630722046, loss=0.02914714254438877
I0205 15:54:01.067157 140044376827648 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.09996577352285385, loss=0.027355458587408066
I0205 15:54:34.218792 140020770670336 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.10364295542240143, loss=0.032349761575460434
I0205 15:55:07.454437 140044376827648 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.10220783948898315, loss=0.031043771654367447
I0205 15:55:40.214149 140020770670336 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.0860457792878151, loss=0.028097517788410187
I0205 15:55:45.097951 140205209478976 spec.py:321] Evaluating on the training split.
I0205 15:57:27.763269 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 15:57:30.789292 140205209478976 spec.py:349] Evaluating on the test split.
I0205 15:57:33.709475 140205209478976 submission_runner.py:408] Time since start: 11710.38s, 	Step: 24616, 	{'train/accuracy': 0.9911187291145325, 'train/loss': 0.02876371331512928, 'train/mean_average_precision': 0.4408214315236216, 'validation/accuracy': 0.9866234064102173, 'validation/loss': 0.044969748705625534, 'validation/mean_average_precision': 0.2635825782785605, 'validation/num_examples': 43793, 'test/accuracy': 0.9857947826385498, 'test/loss': 0.04763353243470192, 'test/mean_average_precision': 0.25082574583198197, 'test/num_examples': 43793, 'score': 7934.79806470871, 'total_duration': 11710.377674818039, 'accumulated_submission_time': 7934.79806470871, 'accumulated_eval_time': 3773.929664850235, 'accumulated_logging_time': 0.9760401248931885}
I0205 15:57:33.729645 140030420236032 logging_writer.py:48] [24616] accumulated_eval_time=3773.929665, accumulated_logging_time=0.976040, accumulated_submission_time=7934.798065, global_step=24616, preemption_count=0, score=7934.798065, test/accuracy=0.985795, test/loss=0.047634, test/mean_average_precision=0.250826, test/num_examples=43793, total_duration=11710.377675, train/accuracy=0.991119, train/loss=0.028764, train/mean_average_precision=0.440821, validation/accuracy=0.986623, validation/loss=0.044970, validation/mean_average_precision=0.263583, validation/num_examples=43793
I0205 15:58:01.245152 140044351649536 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.10874573141336441, loss=0.029464690014719963
I0205 15:58:33.743217 140030420236032 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.10351777821779251, loss=0.03154090791940689
I0205 15:59:06.043524 140044351649536 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.09296248108148575, loss=0.03061380609869957
I0205 15:59:38.366418 140030420236032 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.11310149729251862, loss=0.02852451615035534
I0205 16:00:10.857328 140044351649536 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.15606874227523804, loss=0.031054023653268814
I0205 16:00:42.986653 140030420236032 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.09462522715330124, loss=0.02777894027531147
I0205 16:01:15.345527 140044351649536 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.12313175946474075, loss=0.028790755197405815
I0205 16:01:33.875655 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:03:19.402809 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:03:22.485798 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:03:27.461393 140205209478976 submission_runner.py:408] Time since start: 12064.13s, 	Step: 25358, 	{'train/accuracy': 0.9912756681442261, 'train/loss': 0.02825007773935795, 'train/mean_average_precision': 0.4428742902391496, 'validation/accuracy': 0.986655056476593, 'validation/loss': 0.04483070597052574, 'validation/mean_average_precision': 0.26376133340425956, 'validation/num_examples': 43793, 'test/accuracy': 0.9857442378997803, 'test/loss': 0.04766061156988144, 'test/mean_average_precision': 0.2516790343245108, 'test/num_examples': 43793, 'score': 8174.913177490234, 'total_duration': 12064.129575967789, 'accumulated_submission_time': 8174.913177490234, 'accumulated_eval_time': 3887.5153410434723, 'accumulated_logging_time': 1.0077130794525146}
I0205 16:03:27.481939 140021163808512 logging_writer.py:48] [25358] accumulated_eval_time=3887.515341, accumulated_logging_time=1.007713, accumulated_submission_time=8174.913177, global_step=25358, preemption_count=0, score=8174.913177, test/accuracy=0.985744, test/loss=0.047661, test/mean_average_precision=0.251679, test/num_examples=43793, total_duration=12064.129576, train/accuracy=0.991276, train/loss=0.028250, train/mean_average_precision=0.442874, validation/accuracy=0.986655, validation/loss=0.044831, validation/mean_average_precision=0.263761, validation/num_examples=43793
I0205 16:03:41.367604 140044376827648 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.1197405755519867, loss=0.02892005443572998
I0205 16:04:13.821404 140021163808512 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.09126380831003189, loss=0.028110641986131668
I0205 16:04:45.801694 140044376827648 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.08317654579877853, loss=0.02829763852059841
I0205 16:05:18.044098 140021163808512 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.09677001088857651, loss=0.028150606900453568
I0205 16:05:50.387877 140044376827648 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.09121841937303543, loss=0.030133431777358055
I0205 16:06:22.514807 140021163808512 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.09693355858325958, loss=0.03075270541012287
I0205 16:06:54.923477 140044376827648 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.10431232303380966, loss=0.031305499374866486
I0205 16:07:26.877799 140021163808512 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.09911955893039703, loss=0.03011430613696575
I0205 16:07:27.518438 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:09:10.264473 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:09:13.384560 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:09:16.387141 140205209478976 submission_runner.py:408] Time since start: 12413.06s, 	Step: 26103, 	{'train/accuracy': 0.9914022088050842, 'train/loss': 0.027766233310103416, 'train/mean_average_precision': 0.45234100839245117, 'validation/accuracy': 0.9866623878479004, 'validation/loss': 0.04479790851473808, 'validation/mean_average_precision': 0.2641685438259154, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.047423217445611954, 'test/mean_average_precision': 0.25438069850517014, 'test/num_examples': 43793, 'score': 8414.918694972992, 'total_duration': 12413.05533671379, 'accumulated_submission_time': 8414.918694972992, 'accumulated_eval_time': 3996.3839950561523, 'accumulated_logging_time': 1.039290189743042}
I0205 16:09:16.407360 140030420236032 logging_writer.py:48] [26103] accumulated_eval_time=3996.383995, accumulated_logging_time=1.039290, accumulated_submission_time=8414.918695, global_step=26103, preemption_count=0, score=8414.918695, test/accuracy=0.985849, test/loss=0.047423, test/mean_average_precision=0.254381, test/num_examples=43793, total_duration=12413.055337, train/accuracy=0.991402, train/loss=0.027766, train/mean_average_precision=0.452341, validation/accuracy=0.986662, validation/loss=0.044798, validation/mean_average_precision=0.264169, validation/num_examples=43793
I0205 16:09:48.676525 140044351649536 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.10233679413795471, loss=0.03215816989541054
I0205 16:10:20.988470 140030420236032 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.11239926517009735, loss=0.02893820032477379
I0205 16:10:53.492905 140044351649536 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.10999418050050735, loss=0.030498545616865158
I0205 16:11:25.721158 140030420236032 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.09141513705253601, loss=0.02615046314895153
I0205 16:11:57.812221 140044351649536 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.13226255774497986, loss=0.025873364880681038
I0205 16:12:30.178127 140030420236032 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.08534763753414154, loss=0.02823128178715706
I0205 16:13:02.341273 140044351649536 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.07593382894992828, loss=0.02636185847222805
I0205 16:13:16.530066 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:14:58.493214 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:15:01.709982 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:15:04.693578 140205209478976 submission_runner.py:408] Time since start: 12761.36s, 	Step: 26844, 	{'train/accuracy': 0.991820216178894, 'train/loss': 0.02651890367269516, 'train/mean_average_precision': 0.49922541465408793, 'validation/accuracy': 0.986660361289978, 'validation/loss': 0.044735804200172424, 'validation/mean_average_precision': 0.2714495760771454, 'validation/num_examples': 43793, 'test/accuracy': 0.9857585430145264, 'test/loss': 0.047469232231378555, 'test/mean_average_precision': 0.25685511484781937, 'test/num_examples': 43793, 'score': 8655.010743618011, 'total_duration': 12761.361777067184, 'accumulated_submission_time': 8655.010743618011, 'accumulated_eval_time': 4104.547461509705, 'accumulated_logging_time': 1.0705244541168213}
I0205 16:15:04.714049 140020770670336 logging_writer.py:48] [26844] accumulated_eval_time=4104.547462, accumulated_logging_time=1.070524, accumulated_submission_time=8655.010744, global_step=26844, preemption_count=0, score=8655.010744, test/accuracy=0.985759, test/loss=0.047469, test/mean_average_precision=0.256855, test/num_examples=43793, total_duration=12761.361777, train/accuracy=0.991820, train/loss=0.026519, train/mean_average_precision=0.499225, validation/accuracy=0.986660, validation/loss=0.044736, validation/mean_average_precision=0.271450, validation/num_examples=43793
I0205 16:15:22.979224 140021163808512 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.07719649374485016, loss=0.028586508706212044
I0205 16:15:55.153310 140020770670336 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.10251770913600922, loss=0.029890771955251694
I0205 16:16:27.294281 140021163808512 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.07365918159484863, loss=0.026633884757757187
I0205 16:16:59.605516 140020770670336 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.12296347320079803, loss=0.030178718268871307
I0205 16:17:32.058837 140021163808512 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.09891429543495178, loss=0.027435431256890297
I0205 16:18:04.049488 140020770670336 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.09826112538576126, loss=0.027913138270378113
I0205 16:18:35.913851 140021163808512 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.1298883557319641, loss=0.02584938518702984
I0205 16:19:04.972783 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:20:47.757765 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:20:50.783730 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:20:53.827883 140205209478976 submission_runner.py:408] Time since start: 13110.50s, 	Step: 27591, 	{'train/accuracy': 0.99174964427948, 'train/loss': 0.026884455233812332, 'train/mean_average_precision': 0.48900363211065134, 'validation/accuracy': 0.98653244972229, 'validation/loss': 0.045182015746831894, 'validation/mean_average_precision': 0.2685428630516191, 'validation/num_examples': 43793, 'test/accuracy': 0.9857147336006165, 'test/loss': 0.04805206507444382, 'test/mean_average_precision': 0.2612988077710391, 'test/num_examples': 43793, 'score': 8895.237357616425, 'total_duration': 13110.495953798294, 'accumulated_submission_time': 8895.237357616425, 'accumulated_eval_time': 4213.402389764786, 'accumulated_logging_time': 1.1035490036010742}
I0205 16:20:53.848288 140044351649536 logging_writer.py:48] [27591] accumulated_eval_time=4213.402390, accumulated_logging_time=1.103549, accumulated_submission_time=8895.237358, global_step=27591, preemption_count=0, score=8895.237358, test/accuracy=0.985715, test/loss=0.048052, test/mean_average_precision=0.261299, test/num_examples=43793, total_duration=13110.495954, train/accuracy=0.991750, train/loss=0.026884, train/mean_average_precision=0.489004, validation/accuracy=0.986532, validation/loss=0.045182, validation/mean_average_precision=0.268543, validation/num_examples=43793
I0205 16:20:57.108067 140044376827648 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.11069943010807037, loss=0.029045922681689262
I0205 16:21:29.130689 140044351649536 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.10046837478876114, loss=0.03057585284113884
I0205 16:22:01.369756 140044376827648 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.09841315448284149, loss=0.027504639700055122
I0205 16:22:33.641661 140044351649536 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.09825123101472855, loss=0.029657188802957535
I0205 16:23:05.710378 140044376827648 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.08567467331886292, loss=0.02796555869281292
I0205 16:23:37.518421 140044351649536 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.09788154810667038, loss=0.027739444747567177
I0205 16:24:09.635213 140044376827648 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.14617562294006348, loss=0.030862942337989807
I0205 16:24:41.491188 140044351649536 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.1260513961315155, loss=0.028794489800930023
I0205 16:24:53.862482 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:26:35.151293 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:26:38.207239 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:26:43.113370 140205209478976 submission_runner.py:408] Time since start: 13459.78s, 	Step: 28339, 	{'train/accuracy': 0.991543173789978, 'train/loss': 0.027303244918584824, 'train/mean_average_precision': 0.4728328304098546, 'validation/accuracy': 0.9867244958877563, 'validation/loss': 0.04520970955491066, 'validation/mean_average_precision': 0.267856911482501, 'validation/num_examples': 43793, 'test/accuracy': 0.9858225584030151, 'test/loss': 0.04836053401231766, 'test/mean_average_precision': 0.2572859701949634, 'test/num_examples': 43793, 'score': 9135.220917224884, 'total_duration': 13459.781569480896, 'accumulated_submission_time': 9135.220917224884, 'accumulated_eval_time': 4322.6532344818115, 'accumulated_logging_time': 1.1350586414337158}
I0205 16:26:43.134171 140021163808512 logging_writer.py:48] [28339] accumulated_eval_time=4322.653234, accumulated_logging_time=1.135059, accumulated_submission_time=9135.220917, global_step=28339, preemption_count=0, score=9135.220917, test/accuracy=0.985823, test/loss=0.048361, test/mean_average_precision=0.257286, test/num_examples=43793, total_duration=13459.781569, train/accuracy=0.991543, train/loss=0.027303, train/mean_average_precision=0.472833, validation/accuracy=0.986724, validation/loss=0.045210, validation/mean_average_precision=0.267857, validation/num_examples=43793
I0205 16:27:03.362361 140030420236032 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.14656901359558105, loss=0.028908155858516693
I0205 16:27:35.356388 140021163808512 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.08688745647668839, loss=0.028070613741874695
I0205 16:28:07.443596 140030420236032 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.09957684576511383, loss=0.02605333738029003
I0205 16:28:39.542668 140021163808512 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.10061708092689514, loss=0.0273623988032341
I0205 16:29:11.742353 140030420236032 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.10604052245616913, loss=0.030116090551018715
I0205 16:29:43.804177 140021163808512 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.09493108093738556, loss=0.02814605087041855
I0205 16:30:15.710471 140030420236032 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.09203852713108063, loss=0.03032296523451805
I0205 16:30:43.225615 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:32:23.896407 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:32:26.986030 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:32:29.977337 140205209478976 submission_runner.py:408] Time since start: 13806.65s, 	Step: 29087, 	{'train/accuracy': 0.9913496375083923, 'train/loss': 0.027921341359615326, 'train/mean_average_precision': 0.44670458985016304, 'validation/accuracy': 0.9866745471954346, 'validation/loss': 0.044872529804706573, 'validation/mean_average_precision': 0.2700355602727555, 'validation/num_examples': 43793, 'test/accuracy': 0.9858547449111938, 'test/loss': 0.04762396961450577, 'test/mean_average_precision': 0.2617479458606887, 'test/num_examples': 43793, 'score': 9375.281760692596, 'total_duration': 13806.64552783966, 'accumulated_submission_time': 9375.281760692596, 'accumulated_eval_time': 4429.404903411865, 'accumulated_logging_time': 1.1667847633361816}
I0205 16:32:29.998140 140020770670336 logging_writer.py:48] [29087] accumulated_eval_time=4429.404903, accumulated_logging_time=1.166785, accumulated_submission_time=9375.281761, global_step=29087, preemption_count=0, score=9375.281761, test/accuracy=0.985855, test/loss=0.047624, test/mean_average_precision=0.261748, test/num_examples=43793, total_duration=13806.645528, train/accuracy=0.991350, train/loss=0.027921, train/mean_average_precision=0.446705, validation/accuracy=0.986675, validation/loss=0.044873, validation/mean_average_precision=0.270036, validation/num_examples=43793
I0205 16:32:34.462486 140044351649536 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.09504880756139755, loss=0.027095802128314972
I0205 16:33:06.308544 140020770670336 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.08987165987491608, loss=0.025102123618125916
I0205 16:33:38.369201 140044351649536 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.21860232949256897, loss=0.029266677796840668
I0205 16:34:10.170054 140020770670336 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.09597575664520264, loss=0.027095617726445198
I0205 16:34:41.651059 140044351649536 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.08937204629182816, loss=0.030128158628940582
I0205 16:35:13.315272 140020770670336 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.08055995404720306, loss=0.027339478954672813
I0205 16:35:44.980538 140044351649536 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.09414103627204895, loss=0.02730037271976471
I0205 16:36:17.256569 140020770670336 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.0920509546995163, loss=0.026169229298830032
I0205 16:36:30.058723 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:38:10.853063 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:38:13.916673 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:38:16.921176 140205209478976 submission_runner.py:408] Time since start: 14153.59s, 	Step: 29841, 	{'train/accuracy': 0.9912735819816589, 'train/loss': 0.028208354488015175, 'train/mean_average_precision': 0.4498426126219626, 'validation/accuracy': 0.9865994453430176, 'validation/loss': 0.04536927491426468, 'validation/mean_average_precision': 0.2591981850120724, 'validation/num_examples': 43793, 'test/accuracy': 0.9858208894729614, 'test/loss': 0.04811793565750122, 'test/mean_average_precision': 0.25302340075831153, 'test/num_examples': 43793, 'score': 9615.312079906464, 'total_duration': 14153.5893740654, 'accumulated_submission_time': 9615.312079906464, 'accumulated_eval_time': 4536.267310619354, 'accumulated_logging_time': 1.1982874870300293}
I0205 16:38:16.942291 140030420236032 logging_writer.py:48] [29841] accumulated_eval_time=4536.267311, accumulated_logging_time=1.198287, accumulated_submission_time=9615.312080, global_step=29841, preemption_count=0, score=9615.312080, test/accuracy=0.985821, test/loss=0.048118, test/mean_average_precision=0.253023, test/num_examples=43793, total_duration=14153.589374, train/accuracy=0.991274, train/loss=0.028208, train/mean_average_precision=0.449843, validation/accuracy=0.986599, validation/loss=0.045369, validation/mean_average_precision=0.259198, validation/num_examples=43793
I0205 16:38:36.453713 140044376827648 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.098868727684021, loss=0.028205232694745064
I0205 16:39:08.461673 140030420236032 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.14068324863910675, loss=0.030678709968924522
I0205 16:39:40.316819 140044376827648 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.11737578362226486, loss=0.029176698997616768
I0205 16:40:12.247391 140030420236032 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.19978117942810059, loss=0.029761848971247673
I0205 16:40:44.434957 140044376827648 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.08503668010234833, loss=0.030185259878635406
I0205 16:41:17.051948 140030420236032 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.1020963191986084, loss=0.02582886628806591
I0205 16:41:49.223431 140044376827648 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.12384240329265594, loss=0.026519235223531723
I0205 16:42:17.221322 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:44:00.374926 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:44:03.534451 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:44:06.518795 140205209478976 submission_runner.py:408] Time since start: 14503.19s, 	Step: 30587, 	{'train/accuracy': 0.9914898872375488, 'train/loss': 0.027387216687202454, 'train/mean_average_precision': 0.47278088836117016, 'validation/accuracy': 0.986668050289154, 'validation/loss': 0.04530851170420647, 'validation/mean_average_precision': 0.26285704163105317, 'validation/num_examples': 43793, 'test/accuracy': 0.9857636094093323, 'test/loss': 0.0484786257147789, 'test/mean_average_precision': 0.254502359861954, 'test/num_examples': 43793, 'score': 9855.560412883759, 'total_duration': 14503.186990499496, 'accumulated_submission_time': 9855.560412883759, 'accumulated_eval_time': 4645.564736843109, 'accumulated_logging_time': 1.2307319641113281}
I0205 16:44:06.540799 140021163808512 logging_writer.py:48] [30587] accumulated_eval_time=4645.564737, accumulated_logging_time=1.230732, accumulated_submission_time=9855.560413, global_step=30587, preemption_count=0, score=9855.560413, test/accuracy=0.985764, test/loss=0.048479, test/mean_average_precision=0.254502, test/num_examples=43793, total_duration=14503.186990, train/accuracy=0.991490, train/loss=0.027387, train/mean_average_precision=0.472781, validation/accuracy=0.986668, validation/loss=0.045309, validation/mean_average_precision=0.262857, validation/num_examples=43793
I0205 16:44:11.013395 140044351649536 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.09585542231798172, loss=0.028734518215060234
I0205 16:44:42.740931 140021163808512 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.09568805247545242, loss=0.029588056728243828
I0205 16:45:14.939197 140044351649536 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.12942646443843842, loss=0.027734212577342987
I0205 16:45:46.888714 140021163808512 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.08471707254648209, loss=0.025562532246112823
I0205 16:46:19.062472 140044351649536 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.10070069879293442, loss=0.026898104697465897
I0205 16:46:51.685045 140021163808512 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.11681630462408066, loss=0.023897843435406685
I0205 16:47:24.310419 140044351649536 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.10868389159440994, loss=0.026628462597727776
I0205 16:47:56.714902 140021163808512 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.11525550484657288, loss=0.029235322028398514
I0205 16:48:06.525114 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:49:44.942469 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:49:48.059038 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:49:51.032041 140205209478976 submission_runner.py:408] Time since start: 14847.70s, 	Step: 31331, 	{'train/accuracy': 0.9915563464164734, 'train/loss': 0.027224678546190262, 'train/mean_average_precision': 0.4779938282806382, 'validation/accuracy': 0.9867269396781921, 'validation/loss': 0.04497452452778816, 'validation/mean_average_precision': 0.2681475382524397, 'validation/num_examples': 43793, 'test/accuracy': 0.9859392046928406, 'test/loss': 0.047643113881349564, 'test/mean_average_precision': 0.2611744513547663, 'test/num_examples': 43793, 'score': 10095.512593507767, 'total_duration': 14847.700211763382, 'accumulated_submission_time': 10095.512593507767, 'accumulated_eval_time': 4750.071592330933, 'accumulated_logging_time': 1.2652955055236816}
I0205 16:49:51.054604 140020770670336 logging_writer.py:48] [31331] accumulated_eval_time=4750.071592, accumulated_logging_time=1.265296, accumulated_submission_time=10095.512594, global_step=31331, preemption_count=0, score=10095.512594, test/accuracy=0.985939, test/loss=0.047643, test/mean_average_precision=0.261174, test/num_examples=43793, total_duration=14847.700212, train/accuracy=0.991556, train/loss=0.027225, train/mean_average_precision=0.477994, validation/accuracy=0.986727, validation/loss=0.044975, validation/mean_average_precision=0.268148, validation/num_examples=43793
I0205 16:50:13.856966 140044376827648 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.11731527745723724, loss=0.027372261509299278
I0205 16:50:45.681536 140020770670336 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.10343293100595474, loss=0.028206631541252136
I0205 16:51:17.793200 140044376827648 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.08476923406124115, loss=0.02683611959218979
I0205 16:51:49.456630 140020770670336 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.09479092061519623, loss=0.026970498263835907
I0205 16:52:21.221986 140044376827648 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.18109361827373505, loss=0.028530729934573174
I0205 16:52:53.283788 140020770670336 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.12622171640396118, loss=0.03007402829825878
I0205 16:53:24.719488 140044376827648 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.10239307582378387, loss=0.027072085067629814
I0205 16:53:51.222790 140205209478976 spec.py:321] Evaluating on the training split.
I0205 16:55:32.293044 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 16:55:35.331509 140205209478976 spec.py:349] Evaluating on the test split.
I0205 16:55:38.328359 140205209478976 submission_runner.py:408] Time since start: 15195.00s, 	Step: 32084, 	{'train/accuracy': 0.9915878772735596, 'train/loss': 0.02693975530564785, 'train/mean_average_precision': 0.4846764472143901, 'validation/accuracy': 0.9865304231643677, 'validation/loss': 0.04544617608189583, 'validation/mean_average_precision': 0.26817698552047364, 'validation/num_examples': 43793, 'test/accuracy': 0.9857879877090454, 'test/loss': 0.04806245118379593, 'test/mean_average_precision': 0.2527473212298942, 'test/num_examples': 43793, 'score': 10335.648624420166, 'total_duration': 15194.996549367905, 'accumulated_submission_time': 10335.648624420166, 'accumulated_eval_time': 4857.177111625671, 'accumulated_logging_time': 1.3001899719238281}
I0205 16:55:38.349709 140021163808512 logging_writer.py:48] [32084] accumulated_eval_time=4857.177112, accumulated_logging_time=1.300190, accumulated_submission_time=10335.648624, global_step=32084, preemption_count=0, score=10335.648624, test/accuracy=0.985788, test/loss=0.048062, test/mean_average_precision=0.252747, test/num_examples=43793, total_duration=15194.996549, train/accuracy=0.991588, train/loss=0.026940, train/mean_average_precision=0.484676, validation/accuracy=0.986530, validation/loss=0.045446, validation/mean_average_precision=0.268177, validation/num_examples=43793
I0205 16:55:43.777180 140044351649536 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.0978616327047348, loss=0.02738255262374878
I0205 16:56:15.372948 140021163808512 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.1101507917046547, loss=0.02810867875814438
I0205 16:56:46.899598 140044351649536 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.11555349826812744, loss=0.028454001992940903
I0205 16:57:18.568030 140021163808512 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.09763436764478683, loss=0.027571694925427437
I0205 16:57:50.144830 140044351649536 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.11050882935523987, loss=0.024755479767918587
I0205 16:58:21.886663 140021163808512 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.17590628564357758, loss=0.02923661842942238
I0205 16:58:53.538444 140044351649536 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.1365606039762497, loss=0.02352922037243843
I0205 16:59:25.305813 140021163808512 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.10471296310424805, loss=0.026285445317626
I0205 16:59:38.333580 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:01:17.518190 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:01:20.552156 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:01:23.526492 140205209478976 submission_runner.py:408] Time since start: 15540.19s, 	Step: 32842, 	{'train/accuracy': 0.9919490218162537, 'train/loss': 0.025746557861566544, 'train/mean_average_precision': 0.5127910015596717, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.04498951882123947, 'validation/mean_average_precision': 0.2654056363679506, 'validation/num_examples': 43793, 'test/accuracy': 0.9858031868934631, 'test/loss': 0.04785405099391937, 'test/mean_average_precision': 0.2626084049281989, 'test/num_examples': 43793, 'score': 10575.60150718689, 'total_duration': 15540.194693565369, 'accumulated_submission_time': 10575.60150718689, 'accumulated_eval_time': 4962.369988441467, 'accumulated_logging_time': 1.332615613937378}
I0205 17:01:23.547978 140030420236032 logging_writer.py:48] [32842] accumulated_eval_time=4962.369988, accumulated_logging_time=1.332616, accumulated_submission_time=10575.601507, global_step=32842, preemption_count=0, score=10575.601507, test/accuracy=0.985803, test/loss=0.047854, test/mean_average_precision=0.262608, test/num_examples=43793, total_duration=15540.194694, train/accuracy=0.991949, train/loss=0.025747, train/mean_average_precision=0.512791, validation/accuracy=0.986664, validation/loss=0.044990, validation/mean_average_precision=0.265406, validation/num_examples=43793
I0205 17:01:42.334259 140044376827648 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.10061853379011154, loss=0.031161494553089142
I0205 17:02:14.380601 140030420236032 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.08778740465641022, loss=0.022852830588817596
I0205 17:02:46.464447 140044376827648 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.09748227149248123, loss=0.02804727852344513
I0205 17:03:18.716361 140030420236032 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.10494859516620636, loss=0.02712499536573887
I0205 17:03:50.844854 140044376827648 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.10991908609867096, loss=0.028881000354886055
I0205 17:04:22.787688 140030420236032 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.09403876960277557, loss=0.025756023824214935
I0205 17:04:54.827180 140044376827648 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.14245352149009705, loss=0.02411157824099064
I0205 17:05:23.766853 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:07:03.072829 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:07:06.084997 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:07:09.068077 140205209478976 submission_runner.py:408] Time since start: 15885.74s, 	Step: 33592, 	{'train/accuracy': 0.9920307397842407, 'train/loss': 0.02541346289217472, 'train/mean_average_precision': 0.5067656067708026, 'validation/accuracy': 0.9868279695510864, 'validation/loss': 0.045044295489788055, 'validation/mean_average_precision': 0.26585007839346014, 'validation/num_examples': 43793, 'test/accuracy': 0.9859409332275391, 'test/loss': 0.047983698546886444, 'test/mean_average_precision': 0.25743621166626357, 'test/num_examples': 43793, 'score': 10815.790082454681, 'total_duration': 15885.7362678051, 'accumulated_submission_time': 10815.790082454681, 'accumulated_eval_time': 5067.671158790588, 'accumulated_logging_time': 1.3651981353759766}
I0205 17:07:09.090894 140020770670336 logging_writer.py:48] [33592] accumulated_eval_time=5067.671159, accumulated_logging_time=1.365198, accumulated_submission_time=10815.790082, global_step=33592, preemption_count=0, score=10815.790082, test/accuracy=0.985941, test/loss=0.047984, test/mean_average_precision=0.257436, test/num_examples=43793, total_duration=15885.736268, train/accuracy=0.992031, train/loss=0.025413, train/mean_average_precision=0.506766, validation/accuracy=0.986828, validation/loss=0.045044, validation/mean_average_precision=0.265850, validation/num_examples=43793
I0205 17:07:11.967778 140044351649536 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.1089085265994072, loss=0.02835916541516781
I0205 17:07:44.039455 140020770670336 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.09570088982582092, loss=0.02565423771739006
I0205 17:08:15.794704 140044351649536 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.11707491427659988, loss=0.0264731515198946
I0205 17:08:47.714314 140020770670336 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.10381876677274704, loss=0.02626398764550686
I0205 17:09:19.532150 140044351649536 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.1306162327528, loss=0.02894226275384426
I0205 17:09:51.132694 140020770670336 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.12414585798978806, loss=0.02846556156873703
I0205 17:10:22.807589 140044351649536 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.1365014910697937, loss=0.025436917319893837
I0205 17:10:54.429701 140020770670336 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.13560999929904938, loss=0.028530139476060867
I0205 17:11:09.358196 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:12:50.095701 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:12:53.085642 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:12:56.042189 140205209478976 submission_runner.py:408] Time since start: 16232.71s, 	Step: 34348, 	{'train/accuracy': 0.9923913478851318, 'train/loss': 0.024423692375421524, 'train/mean_average_precision': 0.5342409464257092, 'validation/accuracy': 0.9867200255393982, 'validation/loss': 0.04531063511967659, 'validation/mean_average_precision': 0.2671155695587615, 'validation/num_examples': 43793, 'test/accuracy': 0.985846996307373, 'test/loss': 0.04810245707631111, 'test/mean_average_precision': 0.25537366261511146, 'test/num_examples': 43793, 'score': 11056.026224136353, 'total_duration': 16232.710386753082, 'accumulated_submission_time': 11056.026224136353, 'accumulated_eval_time': 5174.355105876923, 'accumulated_logging_time': 1.3994545936584473}
I0205 17:12:56.069469 140021163808512 logging_writer.py:48] [34348] accumulated_eval_time=5174.355106, accumulated_logging_time=1.399455, accumulated_submission_time=11056.026224, global_step=34348, preemption_count=0, score=11056.026224, test/accuracy=0.985847, test/loss=0.048102, test/mean_average_precision=0.255374, test/num_examples=43793, total_duration=16232.710387, train/accuracy=0.992391, train/loss=0.024424, train/mean_average_precision=0.534241, validation/accuracy=0.986720, validation/loss=0.045311, validation/mean_average_precision=0.267116, validation/num_examples=43793
I0205 17:13:13.071710 140044376827648 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.12155725806951523, loss=0.029157226905226707
I0205 17:13:44.813624 140021163808512 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.09586897492408752, loss=0.026368863880634308
I0205 17:14:16.925343 140044376827648 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.15300892293453217, loss=0.030389362946152687
I0205 17:14:48.764653 140021163808512 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.10513388365507126, loss=0.026291292160749435
I0205 17:15:20.623862 140044376827648 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.10347259044647217, loss=0.029231872409582138
I0205 17:15:52.755061 140021163808512 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.10623486340045929, loss=0.025613758713006973
I0205 17:16:24.410926 140044376827648 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.11688755452632904, loss=0.025692539289593697
I0205 17:16:56.121596 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:18:40.109200 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:18:43.206649 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:18:46.257442 140205209478976 submission_runner.py:408] Time since start: 16582.93s, 	Step: 35099, 	{'train/accuracy': 0.9922991991043091, 'train/loss': 0.024864686653017998, 'train/mean_average_precision': 0.5406791037882595, 'validation/accuracy': 0.9865483045578003, 'validation/loss': 0.045412566512823105, 'validation/mean_average_precision': 0.27699245620698615, 'validation/num_examples': 43793, 'test/accuracy': 0.9857711791992188, 'test/loss': 0.04812723025679588, 'test/mean_average_precision': 0.2624014605046485, 'test/num_examples': 43793, 'score': 11296.04472565651, 'total_duration': 16582.92563867569, 'accumulated_submission_time': 11296.04472565651, 'accumulated_eval_time': 5284.490906953812, 'accumulated_logging_time': 1.4411022663116455}
I0205 17:18:46.279424 140020770670336 logging_writer.py:48] [35099] accumulated_eval_time=5284.490907, accumulated_logging_time=1.441102, accumulated_submission_time=11296.044726, global_step=35099, preemption_count=0, score=11296.044726, test/accuracy=0.985771, test/loss=0.048127, test/mean_average_precision=0.262401, test/num_examples=43793, total_duration=16582.925639, train/accuracy=0.992299, train/loss=0.024865, train/mean_average_precision=0.540679, validation/accuracy=0.986548, validation/loss=0.045413, validation/mean_average_precision=0.276992, validation/num_examples=43793
I0205 17:18:46.986301 140044351649536 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.10099717974662781, loss=0.022243628278374672
I0205 17:19:19.121351 140020770670336 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.09993889927864075, loss=0.026608766987919807
I0205 17:19:51.248587 140044351649536 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.10017306357622147, loss=0.024007413536310196
I0205 17:20:23.365071 140020770670336 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.09943646937608719, loss=0.025778241455554962
I0205 17:20:55.303337 140044351649536 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.11737778782844543, loss=0.025752373039722443
I0205 17:21:27.358747 140020770670336 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.1267349272966385, loss=0.02688651531934738
I0205 17:21:59.056035 140044351649536 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.1359413117170334, loss=0.026476867496967316
I0205 17:22:30.674477 140020770670336 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.10121665149927139, loss=0.02653844654560089
I0205 17:22:46.380156 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:24:28.967546 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:24:32.066194 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:24:35.003190 140205209478976 submission_runner.py:408] Time since start: 16931.67s, 	Step: 35851, 	{'train/accuracy': 0.9920918941497803, 'train/loss': 0.025452451780438423, 'train/mean_average_precision': 0.5169067383191193, 'validation/accuracy': 0.9867861866950989, 'validation/loss': 0.04577966406941414, 'validation/mean_average_precision': 0.27363462918792686, 'validation/num_examples': 43793, 'test/accuracy': 0.985917329788208, 'test/loss': 0.04885753244161606, 'test/mean_average_precision': 0.258611242636092, 'test/num_examples': 43793, 'score': 11536.114127397537, 'total_duration': 16931.671364068985, 'accumulated_submission_time': 11536.114127397537, 'accumulated_eval_time': 5393.113869190216, 'accumulated_logging_time': 1.4744434356689453}
I0205 17:24:35.026084 140021163808512 logging_writer.py:48] [35851] accumulated_eval_time=5393.113869, accumulated_logging_time=1.474443, accumulated_submission_time=11536.114127, global_step=35851, preemption_count=0, score=11536.114127, test/accuracy=0.985917, test/loss=0.048858, test/mean_average_precision=0.258611, test/num_examples=43793, total_duration=16931.671364, train/accuracy=0.992092, train/loss=0.025452, train/mean_average_precision=0.516907, validation/accuracy=0.986786, validation/loss=0.045780, validation/mean_average_precision=0.273635, validation/num_examples=43793
I0205 17:24:50.895633 140030420236032 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.13089899718761444, loss=0.024766799062490463
I0205 17:25:23.269467 140021163808512 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.12258773297071457, loss=0.025845389813184738
I0205 17:25:55.149872 140030420236032 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.11614957451820374, loss=0.02544238604605198
I0205 17:26:26.955854 140021163808512 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.12716354429721832, loss=0.026692314073443413
I0205 17:26:58.528682 140030420236032 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.10443445295095444, loss=0.024265915155410767
I0205 17:27:30.153023 140021163808512 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.1152760460972786, loss=0.02530531957745552
I0205 17:28:01.967059 140030420236032 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.09595692902803421, loss=0.022437268868088722
I0205 17:28:33.603146 140021163808512 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.10174371302127838, loss=0.02453569695353508
I0205 17:28:35.200015 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:30:18.856364 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:30:21.881377 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:30:24.825941 140205209478976 submission_runner.py:408] Time since start: 17281.49s, 	Step: 36606, 	{'train/accuracy': 0.9918672442436218, 'train/loss': 0.02592872828245163, 'train/mean_average_precision': 0.49946526637936217, 'validation/accuracy': 0.9868308305740356, 'validation/loss': 0.04560372978448868, 'validation/mean_average_precision': 0.27065348194802213, 'validation/num_examples': 43793, 'test/accuracy': 0.9859451055526733, 'test/loss': 0.04860323294997215, 'test/mean_average_precision': 0.2502696397802891, 'test/num_examples': 43793, 'score': 11776.257864952087, 'total_duration': 17281.49413871765, 'accumulated_submission_time': 11776.257864952087, 'accumulated_eval_time': 5502.7397446632385, 'accumulated_logging_time': 1.5083093643188477}
I0205 17:30:24.848272 140020770670336 logging_writer.py:48] [36606] accumulated_eval_time=5502.739745, accumulated_logging_time=1.508309, accumulated_submission_time=11776.257865, global_step=36606, preemption_count=0, score=11776.257865, test/accuracy=0.985945, test/loss=0.048603, test/mean_average_precision=0.250270, test/num_examples=43793, total_duration=17281.494139, train/accuracy=0.991867, train/loss=0.025929, train/mean_average_precision=0.499465, validation/accuracy=0.986831, validation/loss=0.045604, validation/mean_average_precision=0.270653, validation/num_examples=43793
I0205 17:30:55.094908 140044351649536 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.11438386142253876, loss=0.02531123347580433
I0205 17:31:28.388830 140020770670336 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.13504281640052795, loss=0.025012915953993797
I0205 17:32:02.473474 140044351649536 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.09802320599555969, loss=0.0235097948461771
I0205 17:32:34.122059 140020770670336 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.11173086613416672, loss=0.024105189368128777
I0205 17:33:05.955873 140044351649536 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.10364000499248505, loss=0.024405209347605705
I0205 17:33:37.915389 140020770670336 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.12235517054796219, loss=0.028438331559300423
I0205 17:34:10.221284 140044351649536 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.13142262399196625, loss=0.02655482105910778
I0205 17:34:25.109285 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:36:07.386809 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:36:10.370850 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:36:13.340756 140205209478976 submission_runner.py:408] Time since start: 17630.01s, 	Step: 37345, 	{'train/accuracy': 0.9918071031570435, 'train/loss': 0.026228440925478935, 'train/mean_average_precision': 0.4914974538833261, 'validation/accuracy': 0.986737072467804, 'validation/loss': 0.04582122340798378, 'validation/mean_average_precision': 0.269111434883714, 'validation/num_examples': 43793, 'test/accuracy': 0.9858027696609497, 'test/loss': 0.04891791194677353, 'test/mean_average_precision': 0.25877115480870727, 'test/num_examples': 43793, 'score': 12016.488486766815, 'total_duration': 17630.008954048157, 'accumulated_submission_time': 12016.488486766815, 'accumulated_eval_time': 5610.971168756485, 'accumulated_logging_time': 1.541703224182129}
I0205 17:36:13.363802 140021163808512 logging_writer.py:48] [37345] accumulated_eval_time=5610.971169, accumulated_logging_time=1.541703, accumulated_submission_time=12016.488487, global_step=37345, preemption_count=0, score=12016.488487, test/accuracy=0.985803, test/loss=0.048918, test/mean_average_precision=0.258771, test/num_examples=43793, total_duration=17630.008954, train/accuracy=0.991807, train/loss=0.026228, train/mean_average_precision=0.491497, validation/accuracy=0.986737, validation/loss=0.045821, validation/mean_average_precision=0.269111, validation/num_examples=43793
I0205 17:36:31.313233 140030420236032 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.1108257994055748, loss=0.02361147291958332
I0205 17:37:03.803970 140021163808512 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.10716722160577774, loss=0.027397718280553818
I0205 17:37:35.970425 140030420236032 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.13214793801307678, loss=0.030047984793782234
I0205 17:38:08.110718 140021163808512 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.11523954570293427, loss=0.024144671857357025
I0205 17:38:40.572918 140030420236032 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.13711285591125488, loss=0.024640211835503578
I0205 17:39:12.718698 140021163808512 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.11502520740032196, loss=0.02294304594397545
I0205 17:39:45.011782 140030420236032 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.11055652797222137, loss=0.023621641099452972
I0205 17:40:13.570293 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:41:50.793424 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:41:53.775429 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:41:56.741736 140205209478976 submission_runner.py:408] Time since start: 17973.41s, 	Step: 38090, 	{'train/accuracy': 0.991980254650116, 'train/loss': 0.025586925446987152, 'train/mean_average_precision': 0.5115828856662827, 'validation/accuracy': 0.9867179989814758, 'validation/loss': 0.045958761125802994, 'validation/mean_average_precision': 0.27139956490941525, 'validation/num_examples': 43793, 'test/accuracy': 0.9859261512756348, 'test/loss': 0.04866719990968704, 'test/mean_average_precision': 0.2615826043412462, 'test/num_examples': 43793, 'score': 12256.664202690125, 'total_duration': 17973.409933567047, 'accumulated_submission_time': 12256.664202690125, 'accumulated_eval_time': 5714.142570018768, 'accumulated_logging_time': 1.5757763385772705}
I0205 17:41:56.764155 140020770670336 logging_writer.py:48] [38090] accumulated_eval_time=5714.142570, accumulated_logging_time=1.575776, accumulated_submission_time=12256.664203, global_step=38090, preemption_count=0, score=12256.664203, test/accuracy=0.985926, test/loss=0.048667, test/mean_average_precision=0.261583, test/num_examples=43793, total_duration=17973.409934, train/accuracy=0.991980, train/loss=0.025587, train/mean_average_precision=0.511583, validation/accuracy=0.986718, validation/loss=0.045959, validation/mean_average_precision=0.271400, validation/num_examples=43793
I0205 17:42:00.253454 140044376827648 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.13675305247306824, loss=0.02429993450641632
I0205 17:42:32.271078 140020770670336 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.13667672872543335, loss=0.02825610153377056
I0205 17:43:04.265998 140044376827648 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.11306943744421005, loss=0.023928364738821983
I0205 17:43:35.990567 140020770670336 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.11826992779970169, loss=0.024009188637137413
I0205 17:44:08.250663 140044376827648 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.13263627886772156, loss=0.02684512548148632
I0205 17:44:41.056645 140020770670336 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.1199919730424881, loss=0.02386946976184845
I0205 17:45:14.163372 140044376827648 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.14241920411586761, loss=0.02463119849562645
I0205 17:45:46.554446 140020770670336 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.11501853913068771, loss=0.023890100419521332
I0205 17:45:56.977820 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:47:35.893785 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:47:38.954029 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:47:41.928405 140205209478976 submission_runner.py:408] Time since start: 18318.60s, 	Step: 38833, 	{'train/accuracy': 0.9920877814292908, 'train/loss': 0.025294775143265724, 'train/mean_average_precision': 0.521443788126027, 'validation/accuracy': 0.986579179763794, 'validation/loss': 0.04616735875606537, 'validation/mean_average_precision': 0.2693436348083421, 'validation/num_examples': 43793, 'test/accuracy': 0.9857484102249146, 'test/loss': 0.04892940819263458, 'test/mean_average_precision': 0.2565882521212089, 'test/num_examples': 43793, 'score': 12496.846721887589, 'total_duration': 18318.596489667892, 'accumulated_submission_time': 12496.846721887589, 'accumulated_eval_time': 5819.092994213104, 'accumulated_logging_time': 1.608827829360962}
I0205 17:47:41.951190 140021163808512 logging_writer.py:48] [38833] accumulated_eval_time=5819.092994, accumulated_logging_time=1.608828, accumulated_submission_time=12496.846722, global_step=38833, preemption_count=0, score=12496.846722, test/accuracy=0.985748, test/loss=0.048929, test/mean_average_precision=0.256588, test/num_examples=43793, total_duration=18318.596490, train/accuracy=0.992088, train/loss=0.025295, train/mean_average_precision=0.521444, validation/accuracy=0.986579, validation/loss=0.046167, validation/mean_average_precision=0.269344, validation/num_examples=43793
I0205 17:48:03.957586 140044351649536 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.1422826051712036, loss=0.025778213515877724
I0205 17:48:35.987713 140021163808512 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.116911880671978, loss=0.028440693393349648
I0205 17:49:08.523330 140044351649536 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.1278735250234604, loss=0.027667555958032608
I0205 17:49:41.480489 140021163808512 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.12574709951877594, loss=0.022410070523619652
I0205 17:50:14.615149 140044351649536 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.1190354973077774, loss=0.023397570475935936
I0205 17:50:47.692537 140021163808512 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.1271967887878418, loss=0.024904636666178703
I0205 17:51:20.746635 140044351649536 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.12784823775291443, loss=0.024868907406926155
I0205 17:51:42.243616 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:53:29.103063 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:53:32.123203 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:53:35.053489 140205209478976 submission_runner.py:408] Time since start: 18671.72s, 	Step: 39566, 	{'train/accuracy': 0.9922082424163818, 'train/loss': 0.024799220263957977, 'train/mean_average_precision': 0.5314044332998933, 'validation/accuracy': 0.9865458607673645, 'validation/loss': 0.04632498696446419, 'validation/mean_average_precision': 0.26710251393412404, 'validation/num_examples': 43793, 'test/accuracy': 0.985687792301178, 'test/loss': 0.04922351986169815, 'test/mean_average_precision': 0.25741389805365, 'test/num_examples': 43793, 'score': 12737.104538679123, 'total_duration': 18671.7216861248, 'accumulated_submission_time': 12737.104538679123, 'accumulated_eval_time': 5931.902855873108, 'accumulated_logging_time': 1.6432619094848633}
I0205 17:53:35.076426 140030420236032 logging_writer.py:48] [39566] accumulated_eval_time=5931.902856, accumulated_logging_time=1.643262, accumulated_submission_time=12737.104539, global_step=39566, preemption_count=0, score=12737.104539, test/accuracy=0.985688, test/loss=0.049224, test/mean_average_precision=0.257414, test/num_examples=43793, total_duration=18671.721686, train/accuracy=0.992208, train/loss=0.024799, train/mean_average_precision=0.531404, validation/accuracy=0.986546, validation/loss=0.046325, validation/mean_average_precision=0.267103, validation/num_examples=43793
I0205 17:53:46.567577 140044376827648 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.18530376255512238, loss=0.026045355945825577
I0205 17:54:18.981747 140030420236032 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.11099310964345932, loss=0.02510838769376278
I0205 17:54:51.074889 140044376827648 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.10603077709674835, loss=0.023021969944238663
I0205 17:55:23.429308 140030420236032 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.1139918863773346, loss=0.025626495480537415
I0205 17:55:56.110739 140044376827648 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.11353357136249542, loss=0.025159381330013275
I0205 17:56:28.873093 140030420236032 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.1400393694639206, loss=0.0245512705296278
I0205 17:57:01.340504 140044376827648 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.15407614409923553, loss=0.024201948195695877
I0205 17:57:33.923504 140030420236032 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.16052457690238953, loss=0.02770078554749489
I0205 17:57:35.300682 140205209478976 spec.py:321] Evaluating on the training split.
I0205 17:59:19.345388 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 17:59:22.391869 140205209478976 spec.py:349] Evaluating on the test split.
I0205 17:59:25.389875 140205209478976 submission_runner.py:408] Time since start: 19022.06s, 	Step: 40305, 	{'train/accuracy': 0.9925952553749084, 'train/loss': 0.023490697145462036, 'train/mean_average_precision': 0.5565252775312892, 'validation/accuracy': 0.986531674861908, 'validation/loss': 0.0463293083012104, 'validation/mean_average_precision': 0.26762152615204715, 'validation/num_examples': 43793, 'test/accuracy': 0.9857577085494995, 'test/loss': 0.049448490142822266, 'test/mean_average_precision': 0.2498049455988531, 'test/num_examples': 43793, 'score': 12977.295271873474, 'total_duration': 19022.058073043823, 'accumulated_submission_time': 12977.295271873474, 'accumulated_eval_time': 6041.992013454437, 'accumulated_logging_time': 1.677354335784912}
I0205 17:59:25.413900 140020770670336 logging_writer.py:48] [40305] accumulated_eval_time=6041.992013, accumulated_logging_time=1.677354, accumulated_submission_time=12977.295272, global_step=40305, preemption_count=0, score=12977.295272, test/accuracy=0.985758, test/loss=0.049448, test/mean_average_precision=0.249805, test/num_examples=43793, total_duration=19022.058073, train/accuracy=0.992595, train/loss=0.023491, train/mean_average_precision=0.556525, validation/accuracy=0.986532, validation/loss=0.046329, validation/mean_average_precision=0.267622, validation/num_examples=43793
I0205 17:59:56.459035 140044351649536 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.1493743509054184, loss=0.024542449042201042
I0205 18:00:28.752633 140020770670336 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.14651581645011902, loss=0.02490304596722126
I0205 18:01:00.996246 140044351649536 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.15570387244224548, loss=0.025623921304941177
I0205 18:01:33.120443 140020770670336 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.12781794369220734, loss=0.02802548184990883
I0205 18:02:04.677205 140044351649536 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.12760555744171143, loss=0.024742858484387398
I0205 18:02:36.488850 140020770670336 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.11748668551445007, loss=0.02211987040936947
I0205 18:03:08.811813 140044351649536 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.11659929901361465, loss=0.027500223368406296
I0205 18:03:25.446839 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:05:05.290446 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:05:08.295803 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:05:11.293826 140205209478976 submission_runner.py:408] Time since start: 19367.96s, 	Step: 41054, 	{'train/accuracy': 0.9928678274154663, 'train/loss': 0.022487075999379158, 'train/mean_average_precision': 0.581720308999962, 'validation/accuracy': 0.9867366552352905, 'validation/loss': 0.04618602618575096, 'validation/mean_average_precision': 0.2734461547168441, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.04916103556752205, 'test/mean_average_precision': 0.26506168285239545, 'test/num_examples': 43793, 'score': 13217.297476530075, 'total_duration': 19367.962020874023, 'accumulated_submission_time': 13217.297476530075, 'accumulated_eval_time': 6147.838949918747, 'accumulated_logging_time': 1.7124638557434082}
I0205 18:05:11.317554 140030420236032 logging_writer.py:48] [41054] accumulated_eval_time=6147.838950, accumulated_logging_time=1.712464, accumulated_submission_time=13217.297477, global_step=41054, preemption_count=0, score=13217.297477, test/accuracy=0.985814, test/loss=0.049161, test/mean_average_precision=0.265062, test/num_examples=43793, total_duration=19367.962021, train/accuracy=0.992868, train/loss=0.022487, train/mean_average_precision=0.581720, validation/accuracy=0.986737, validation/loss=0.046186, validation/mean_average_precision=0.273446, validation/num_examples=43793
I0205 18:05:26.444344 140044376827648 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.15642450749874115, loss=0.0275297649204731
I0205 18:05:58.127539 140030420236032 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.13195595145225525, loss=0.025344666093587875
I0205 18:06:29.880400 140044376827648 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.12335857003927231, loss=0.023474212735891342
I0205 18:07:01.651270 140030420236032 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.13725383579730988, loss=0.02726900577545166
I0205 18:07:33.442214 140044376827648 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.15298926830291748, loss=0.02632966637611389
I0205 18:08:05.279422 140030420236032 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.1572282910346985, loss=0.02293812297284603
I0205 18:08:37.136388 140044376827648 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.12920574843883514, loss=0.025649704039096832
I0205 18:09:08.657717 140030420236032 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.1332472264766693, loss=0.0254018884152174
I0205 18:09:11.485162 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:10:51.107870 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:10:54.103269 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:10:57.064708 140205209478976 submission_runner.py:408] Time since start: 19713.73s, 	Step: 41810, 	{'train/accuracy': 0.9927249550819397, 'train/loss': 0.022989705204963684, 'train/mean_average_precision': 0.5715105094814787, 'validation/accuracy': 0.9866583347320557, 'validation/loss': 0.04679886996746063, 'validation/mean_average_precision': 0.2642473790358029, 'validation/num_examples': 43793, 'test/accuracy': 0.9857454895973206, 'test/loss': 0.049967210739851, 'test/mean_average_precision': 0.2580405016937968, 'test/num_examples': 43793, 'score': 13457.432827711105, 'total_duration': 19713.732904434204, 'accumulated_submission_time': 13457.432827711105, 'accumulated_eval_time': 6253.418445587158, 'accumulated_logging_time': 1.7486231327056885}
I0205 18:10:57.088868 140020770670336 logging_writer.py:48] [41810] accumulated_eval_time=6253.418446, accumulated_logging_time=1.748623, accumulated_submission_time=13457.432828, global_step=41810, preemption_count=0, score=13457.432828, test/accuracy=0.985745, test/loss=0.049967, test/mean_average_precision=0.258041, test/num_examples=43793, total_duration=19713.732904, train/accuracy=0.992725, train/loss=0.022990, train/mean_average_precision=0.571511, validation/accuracy=0.986658, validation/loss=0.046799, validation/mean_average_precision=0.264247, validation/num_examples=43793
I0205 18:11:25.838286 140021163808512 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.14451606571674347, loss=0.02373724989593029
I0205 18:11:57.690629 140020770670336 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.1323026716709137, loss=0.024018798023462296
I0205 18:12:29.843207 140021163808512 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.1847776621580124, loss=0.027270261198282242
I0205 18:13:01.441859 140020770670336 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.1146886944770813, loss=0.025449419394135475
I0205 18:13:33.917151 140021163808512 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.11846910417079926, loss=0.025451648980379105
I0205 18:14:06.802577 140020770670336 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.14738361537456512, loss=0.026002926751971245
I0205 18:14:39.208845 140021163808512 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.1362641602754593, loss=0.025935493409633636
I0205 18:14:57.127969 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:16:43.931792 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:16:46.935815 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:16:49.906263 140205209478976 submission_runner.py:408] Time since start: 20066.57s, 	Step: 42556, 	{'train/accuracy': 0.992652952671051, 'train/loss': 0.023321157321333885, 'train/mean_average_precision': 0.5512938937985699, 'validation/accuracy': 0.9865283966064453, 'validation/loss': 0.04672816023230553, 'validation/mean_average_precision': 0.2636467984267341, 'validation/num_examples': 43793, 'test/accuracy': 0.9856923818588257, 'test/loss': 0.049496106803417206, 'test/mean_average_precision': 0.2557325618023192, 'test/num_examples': 43793, 'score': 13697.439344882965, 'total_duration': 20066.57444548607, 'accumulated_submission_time': 13697.439344882965, 'accumulated_eval_time': 6366.196691989899, 'accumulated_logging_time': 1.7837131023406982}
I0205 18:16:49.930657 140044351649536 logging_writer.py:48] [42556] accumulated_eval_time=6366.196692, accumulated_logging_time=1.783713, accumulated_submission_time=13697.439345, global_step=42556, preemption_count=0, score=13697.439345, test/accuracy=0.985692, test/loss=0.049496, test/mean_average_precision=0.255733, test/num_examples=43793, total_duration=20066.574445, train/accuracy=0.992653, train/loss=0.023321, train/mean_average_precision=0.551294, validation/accuracy=0.986528, validation/loss=0.046728, validation/mean_average_precision=0.263647, validation/num_examples=43793
I0205 18:17:04.334421 140044376827648 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.13491764664649963, loss=0.024618469178676605
I0205 18:17:36.388903 140044351649536 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.19940166175365448, loss=0.025354942306876183
I0205 18:18:08.499129 140044376827648 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.11629460752010345, loss=0.020303115248680115
I0205 18:18:40.423790 140044351649536 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.13554342091083527, loss=0.024075858294963837
I0205 18:19:12.468039 140044376827648 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.15947723388671875, loss=0.02233569137752056
I0205 18:19:44.425918 140044351649536 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.14733938872814178, loss=0.025244731456041336
I0205 18:20:16.588222 140044376827648 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.14028286933898926, loss=0.023811353370547295
I0205 18:20:48.466116 140044351649536 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.13492047786712646, loss=0.021913934499025345
I0205 18:20:50.062651 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:22:30.235436 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:22:33.334823 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:22:36.423664 140205209478976 submission_runner.py:408] Time since start: 20413.09s, 	Step: 43306, 	{'train/accuracy': 0.9925017356872559, 'train/loss': 0.023778589442372322, 'train/mean_average_precision': 0.5646825159913036, 'validation/accuracy': 0.9867951273918152, 'validation/loss': 0.0462379977107048, 'validation/mean_average_precision': 0.2821592979825692, 'validation/num_examples': 43793, 'test/accuracy': 0.985849916934967, 'test/loss': 0.04968702793121338, 'test/mean_average_precision': 0.26054839880737374, 'test/num_examples': 43793, 'score': 13937.537743330002, 'total_duration': 20413.091861009598, 'accumulated_submission_time': 13937.537743330002, 'accumulated_eval_time': 6472.557656049728, 'accumulated_logging_time': 1.8214752674102783}
I0205 18:22:36.447281 140020770670336 logging_writer.py:48] [43306] accumulated_eval_time=6472.557656, accumulated_logging_time=1.821475, accumulated_submission_time=13937.537743, global_step=43306, preemption_count=0, score=13937.537743, test/accuracy=0.985850, test/loss=0.049687, test/mean_average_precision=0.260548, test/num_examples=43793, total_duration=20413.091861, train/accuracy=0.992502, train/loss=0.023779, train/mean_average_precision=0.564683, validation/accuracy=0.986795, validation/loss=0.046238, validation/mean_average_precision=0.282159, validation/num_examples=43793
I0205 18:23:06.885224 140021163808512 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.1563120186328888, loss=0.02661295421421528
I0205 18:23:39.048088 140020770670336 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.1235666573047638, loss=0.02138286642730236
I0205 18:24:11.203731 140021163808512 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.15657761693000793, loss=0.02650534361600876
I0205 18:24:43.378966 140020770670336 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.20213866233825684, loss=0.024987827986478806
I0205 18:25:15.584757 140021163808512 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.18685714900493622, loss=0.02297571487724781
I0205 18:25:48.270926 140020770670336 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.15118101239204407, loss=0.02469033934175968
I0205 18:26:20.671450 140021163808512 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.11393056809902191, loss=0.023185772821307182
I0205 18:26:36.692974 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:28:18.266314 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:28:21.368790 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:28:24.447802 140205209478976 submission_runner.py:408] Time since start: 20761.12s, 	Step: 44051, 	{'train/accuracy': 0.9924307465553284, 'train/loss': 0.023861369118094444, 'train/mean_average_precision': 0.555371738352912, 'validation/accuracy': 0.9867119193077087, 'validation/loss': 0.04687708243727684, 'validation/mean_average_precision': 0.27326731123889797, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.05005699396133423, 'test/mean_average_precision': 0.25659462157829027, 'test/num_examples': 43793, 'score': 14177.752856016159, 'total_duration': 20761.1159992218, 'accumulated_submission_time': 14177.752856016159, 'accumulated_eval_time': 6580.312434911728, 'accumulated_logging_time': 1.8559012413024902}
I0205 18:28:24.472312 140044351649536 logging_writer.py:48] [44051] accumulated_eval_time=6580.312435, accumulated_logging_time=1.855901, accumulated_submission_time=14177.752856, global_step=44051, preemption_count=0, score=14177.752856, test/accuracy=0.985860, test/loss=0.050057, test/mean_average_precision=0.256595, test/num_examples=43793, total_duration=20761.115999, train/accuracy=0.992431, train/loss=0.023861, train/mean_average_precision=0.555372, validation/accuracy=0.986712, validation/loss=0.046877, validation/mean_average_precision=0.273267, validation/num_examples=43793
I0205 18:28:40.718353 140044376827648 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.14630571007728577, loss=0.02397109381854534
I0205 18:29:13.003786 140044351649536 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.12974858283996582, loss=0.024545874446630478
I0205 18:29:45.514641 140044376827648 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.1584208756685257, loss=0.026676276698708534
I0205 18:30:17.761610 140044351649536 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.1338462233543396, loss=0.02127945050597191
I0205 18:30:49.807299 140044376827648 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.12747135758399963, loss=0.02650853618979454
I0205 18:31:21.793608 140044351649536 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.15195505321025848, loss=0.023233508691191673
I0205 18:31:53.559167 140044376827648 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.117479607462883, loss=0.023229755461215973
I0205 18:32:24.485207 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:34:05.692966 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:34:08.696199 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:34:11.701859 140205209478976 submission_runner.py:408] Time since start: 21108.37s, 	Step: 44798, 	{'train/accuracy': 0.992325484752655, 'train/loss': 0.02401222288608551, 'train/mean_average_precision': 0.5586604499035724, 'validation/accuracy': 0.9867033958435059, 'validation/loss': 0.047204095870256424, 'validation/mean_average_precision': 0.26694024763712443, 'validation/num_examples': 43793, 'test/accuracy': 0.9858166575431824, 'test/loss': 0.05042057856917381, 'test/mean_average_precision': 0.2533213945513642, 'test/num_examples': 43793, 'score': 14417.734598636627, 'total_duration': 21108.370057582855, 'accumulated_submission_time': 14417.734598636627, 'accumulated_eval_time': 6687.529041051865, 'accumulated_logging_time': 1.8914201259613037}
I0205 18:34:11.725386 140020770670336 logging_writer.py:48] [44798] accumulated_eval_time=6687.529041, accumulated_logging_time=1.891420, accumulated_submission_time=14417.734599, global_step=44798, preemption_count=0, score=14417.734599, test/accuracy=0.985817, test/loss=0.050421, test/mean_average_precision=0.253321, test/num_examples=43793, total_duration=21108.370058, train/accuracy=0.992325, train/loss=0.024012, train/mean_average_precision=0.558660, validation/accuracy=0.986703, validation/loss=0.047204, validation/mean_average_precision=0.266940, validation/num_examples=43793
I0205 18:34:12.819447 140030420236032 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.17833822965621948, loss=0.026846881955862045
I0205 18:34:44.441943 140020770670336 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.13445398211479187, loss=0.02265853062272072
I0205 18:35:16.619410 140030420236032 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.14252635836601257, loss=0.022920912131667137
I0205 18:35:48.525406 140020770670336 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.14773711562156677, loss=0.023304227739572525
I0205 18:36:20.332209 140030420236032 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.14949077367782593, loss=0.025109166279435158
I0205 18:36:51.874776 140020770670336 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.19706185162067413, loss=0.022402716800570488
I0205 18:37:23.683051 140030420236032 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.15123584866523743, loss=0.023378368467092514
I0205 18:37:55.871334 140020770670336 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.1342458724975586, loss=0.02309887483716011
I0205 18:38:11.867344 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:39:52.935571 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:39:56.010637 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:39:59.003355 140205209478976 submission_runner.py:408] Time since start: 21455.67s, 	Step: 45551, 	{'train/accuracy': 0.9925184845924377, 'train/loss': 0.023408500477671623, 'train/mean_average_precision': 0.5609331101563908, 'validation/accuracy': 0.9865624904632568, 'validation/loss': 0.04778437688946724, 'validation/mean_average_precision': 0.26957855998027896, 'validation/num_examples': 43793, 'test/accuracy': 0.9856654405593872, 'test/loss': 0.0509008951485157, 'test/mean_average_precision': 0.25818216762065693, 'test/num_examples': 43793, 'score': 14657.845716714859, 'total_duration': 21455.67155122757, 'accumulated_submission_time': 14657.845716714859, 'accumulated_eval_time': 6794.665002822876, 'accumulated_logging_time': 1.9260263442993164}
I0205 18:39:59.027960 140021163808512 logging_writer.py:48] [45551] accumulated_eval_time=6794.665003, accumulated_logging_time=1.926026, accumulated_submission_time=14657.845717, global_step=45551, preemption_count=0, score=14657.845717, test/accuracy=0.985665, test/loss=0.050901, test/mean_average_precision=0.258182, test/num_examples=43793, total_duration=21455.671551, train/accuracy=0.992518, train/loss=0.023409, train/mean_average_precision=0.560933, validation/accuracy=0.986562, validation/loss=0.047784, validation/mean_average_precision=0.269579, validation/num_examples=43793
I0205 18:40:14.963681 140044351649536 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.14402984082698822, loss=0.027036121115088463
I0205 18:40:47.152280 140021163808512 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.24376603960990906, loss=0.022269967943429947
I0205 18:41:18.623864 140044351649536 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.16220836341381073, loss=0.023585164919495583
I0205 18:41:50.343581 140021163808512 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.14022377133369446, loss=0.022013099864125252
I0205 18:42:22.007426 140044351649536 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.14247174561023712, loss=0.024099625647068024
I0205 18:42:53.631642 140021163808512 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.12224052846431732, loss=0.022494632750749588
I0205 18:43:25.381098 140044351649536 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.14718057215213776, loss=0.023141007870435715
I0205 18:43:56.799268 140021163808512 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.17069114744663239, loss=0.02303689904510975
I0205 18:43:59.174837 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:45:37.877716 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:45:41.276156 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:45:44.704415 140205209478976 submission_runner.py:408] Time since start: 21801.37s, 	Step: 46308, 	{'train/accuracy': 0.9927812218666077, 'train/loss': 0.02257843129336834, 'train/mean_average_precision': 0.5735938940547376, 'validation/accuracy': 0.986531674861908, 'validation/loss': 0.047529514878988266, 'validation/mean_average_precision': 0.27112534172351554, 'validation/num_examples': 43793, 'test/accuracy': 0.9856970310211182, 'test/loss': 0.050856564193964005, 'test/mean_average_precision': 0.2524435396842576, 'test/num_examples': 43793, 'score': 14897.96158337593, 'total_duration': 21801.372572660446, 'accumulated_submission_time': 14897.96158337593, 'accumulated_eval_time': 6900.194499254227, 'accumulated_logging_time': 1.9617371559143066}
I0205 18:45:44.732323 140030420236032 logging_writer.py:48] [46308] accumulated_eval_time=6900.194499, accumulated_logging_time=1.961737, accumulated_submission_time=14897.961583, global_step=46308, preemption_count=0, score=14897.961583, test/accuracy=0.985697, test/loss=0.050857, test/mean_average_precision=0.252444, test/num_examples=43793, total_duration=21801.372573, train/accuracy=0.992781, train/loss=0.022578, train/mean_average_precision=0.573594, validation/accuracy=0.986532, validation/loss=0.047530, validation/mean_average_precision=0.271125, validation/num_examples=43793
I0205 18:46:15.579124 140044376827648 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.13399745523929596, loss=0.02293568104505539
I0205 18:46:47.745281 140030420236032 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.17621038854122162, loss=0.023930836468935013
I0205 18:47:20.304660 140044376827648 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.16093069314956665, loss=0.02388145588338375
I0205 18:47:52.473706 140030420236032 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.14370541274547577, loss=0.021020732820034027
I0205 18:48:25.173925 140044376827648 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.1395501047372818, loss=0.022335592657327652
I0205 18:48:57.789444 140030420236032 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.12563887238502502, loss=0.023954449221491814
I0205 18:49:30.314297 140044376827648 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.14916591346263885, loss=0.02281305566430092
I0205 18:49:44.991094 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:51:26.899135 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:51:29.991828 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:51:32.958147 140205209478976 submission_runner.py:408] Time since start: 22149.63s, 	Step: 47046, 	{'train/accuracy': 0.9932058453559875, 'train/loss': 0.021325523033738136, 'train/mean_average_precision': 0.6142257470793187, 'validation/accuracy': 0.9863327741622925, 'validation/loss': 0.04775543883442879, 'validation/mean_average_precision': 0.2685979413267775, 'validation/num_examples': 43793, 'test/accuracy': 0.9854986667633057, 'test/loss': 0.051068343222141266, 'test/mean_average_precision': 0.2488496460511818, 'test/num_examples': 43793, 'score': 15138.184672355652, 'total_duration': 22149.626344442368, 'accumulated_submission_time': 15138.184672355652, 'accumulated_eval_time': 7008.161518335342, 'accumulated_logging_time': 2.0013952255249023}
I0205 18:51:32.982307 140020770670336 logging_writer.py:48] [47046] accumulated_eval_time=7008.161518, accumulated_logging_time=2.001395, accumulated_submission_time=15138.184672, global_step=47046, preemption_count=0, score=15138.184672, test/accuracy=0.985499, test/loss=0.051068, test/mean_average_precision=0.248850, test/num_examples=43793, total_duration=22149.626344, train/accuracy=0.993206, train/loss=0.021326, train/mean_average_precision=0.614226, validation/accuracy=0.986333, validation/loss=0.047755, validation/mean_average_precision=0.268598, validation/num_examples=43793
I0205 18:51:50.689059 140021163808512 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.15157686173915863, loss=0.02233932912349701
I0205 18:52:22.275864 140020770670336 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.16642232239246368, loss=0.023440059274435043
I0205 18:52:54.345517 140021163808512 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.18899869918823242, loss=0.022129038348793983
I0205 18:53:26.628201 140020770670336 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.13716141879558563, loss=0.018573446199297905
I0205 18:53:58.793895 140021163808512 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.12776444852352142, loss=0.021537750959396362
I0205 18:54:30.387750 140020770670336 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.13969463109970093, loss=0.020715095102787018
I0205 18:55:02.401252 140021163808512 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.16716144979000092, loss=0.02313559129834175
I0205 18:55:33.151411 140205209478976 spec.py:321] Evaluating on the training split.
I0205 18:57:14.678188 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 18:57:17.720076 140205209478976 spec.py:349] Evaluating on the test split.
I0205 18:57:20.720208 140205209478976 submission_runner.py:408] Time since start: 22497.39s, 	Step: 47797, 	{'train/accuracy': 0.9933127760887146, 'train/loss': 0.020854709669947624, 'train/mean_average_precision': 0.6183224958318798, 'validation/accuracy': 0.9866700768470764, 'validation/loss': 0.04813152551651001, 'validation/mean_average_precision': 0.27352400561145834, 'validation/num_examples': 43793, 'test/accuracy': 0.9856852293014526, 'test/loss': 0.051686979830265045, 'test/mean_average_precision': 0.25225808267055216, 'test/num_examples': 43793, 'score': 15378.320225954056, 'total_duration': 22497.388407230377, 'accumulated_submission_time': 15378.320225954056, 'accumulated_eval_time': 7115.730270385742, 'accumulated_logging_time': 2.0384480953216553}
I0205 18:57:20.744946 140030420236032 logging_writer.py:48] [47797] accumulated_eval_time=7115.730270, accumulated_logging_time=2.038448, accumulated_submission_time=15378.320226, global_step=47797, preemption_count=0, score=15378.320226, test/accuracy=0.985685, test/loss=0.051687, test/mean_average_precision=0.252258, test/num_examples=43793, total_duration=22497.388407, train/accuracy=0.993313, train/loss=0.020855, train/mean_average_precision=0.618322, validation/accuracy=0.986670, validation/loss=0.048132, validation/mean_average_precision=0.273524, validation/num_examples=43793
I0205 18:57:22.006990 140044351649536 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.20424729585647583, loss=0.02383445017039776
I0205 18:57:53.850645 140030420236032 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1518581211566925, loss=0.02253407984972
I0205 18:58:26.523580 140044351649536 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.16555804014205933, loss=0.02233961597084999
I0205 18:58:59.327260 140030420236032 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.1576707363128662, loss=0.021493613719940186
I0205 18:59:31.195152 140044351649536 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.12389368563890457, loss=0.020376842468976974
I0205 19:00:03.089119 140030420236032 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.21379879117012024, loss=0.02532070502638817
I0205 19:00:34.714738 140044351649536 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.14186419546604156, loss=0.020267300307750702
I0205 19:01:06.653430 140030420236032 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.16626593470573425, loss=0.02305331453680992
I0205 19:01:20.895807 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:03:01.269360 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:03:04.296431 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:03:07.244852 140205209478976 submission_runner.py:408] Time since start: 22843.91s, 	Step: 48545, 	{'train/accuracy': 0.9937769770622253, 'train/loss': 0.01960907131433487, 'train/mean_average_precision': 0.657254856675352, 'validation/accuracy': 0.9864935278892517, 'validation/loss': 0.04848235100507736, 'validation/mean_average_precision': 0.26432682344356795, 'validation/num_examples': 43793, 'test/accuracy': 0.9857058525085449, 'test/loss': 0.05173896253108978, 'test/mean_average_precision': 0.2559660860530463, 'test/num_examples': 43793, 'score': 15618.439760684967, 'total_duration': 22843.91304540634, 'accumulated_submission_time': 15618.439760684967, 'accumulated_eval_time': 7222.079265594482, 'accumulated_logging_time': 2.07443904876709}
I0205 19:03:07.270199 140021163808512 logging_writer.py:48] [48545] accumulated_eval_time=7222.079266, accumulated_logging_time=2.074439, accumulated_submission_time=15618.439761, global_step=48545, preemption_count=0, score=15618.439761, test/accuracy=0.985706, test/loss=0.051739, test/mean_average_precision=0.255966, test/num_examples=43793, total_duration=22843.913045, train/accuracy=0.993777, train/loss=0.019609, train/mean_average_precision=0.657255, validation/accuracy=0.986494, validation/loss=0.048482, validation/mean_average_precision=0.264327, validation/num_examples=43793
I0205 19:03:25.869667 140044376827648 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.16052491962909698, loss=0.02201840840280056
I0205 19:03:58.400748 140021163808512 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.15092779695987701, loss=0.022179365158081055
I0205 19:04:30.268629 140044376827648 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.16608838737010956, loss=0.023159245029091835
I0205 19:05:02.338829 140021163808512 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.12899667024612427, loss=0.021306293085217476
I0205 19:05:34.501096 140044376827648 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.15568847954273224, loss=0.02119540609419346
I0205 19:06:06.799695 140021163808512 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.171932652592659, loss=0.01988028734922409
I0205 19:06:38.919229 140044376827648 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.14009782671928406, loss=0.019335806369781494
I0205 19:07:07.431297 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:08:48.169636 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:08:51.259944 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:08:54.271171 140205209478976 submission_runner.py:408] Time since start: 23190.94s, 	Step: 49290, 	{'train/accuracy': 0.9935559630393982, 'train/loss': 0.02026131935417652, 'train/mean_average_precision': 0.6167782802913997, 'validation/accuracy': 0.9866542816162109, 'validation/loss': 0.04876922816038132, 'validation/mean_average_precision': 0.2706925333525783, 'validation/num_examples': 43793, 'test/accuracy': 0.9856612086296082, 'test/loss': 0.052092064172029495, 'test/mean_average_precision': 0.2503146224776895, 'test/num_examples': 43793, 'score': 15858.267110586166, 'total_duration': 23190.93936944008, 'accumulated_submission_time': 15858.267110586166, 'accumulated_eval_time': 7328.919096469879, 'accumulated_logging_time': 2.4131574630737305}
I0205 19:08:54.295673 140030420236032 logging_writer.py:48] [49290] accumulated_eval_time=7328.919096, accumulated_logging_time=2.413157, accumulated_submission_time=15858.267111, global_step=49290, preemption_count=0, score=15858.267111, test/accuracy=0.985661, test/loss=0.052092, test/mean_average_precision=0.250315, test/num_examples=43793, total_duration=23190.939369, train/accuracy=0.993556, train/loss=0.020261, train/mean_average_precision=0.616778, validation/accuracy=0.986654, validation/loss=0.048769, validation/mean_average_precision=0.270693, validation/num_examples=43793
I0205 19:08:57.891256 140044351649536 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.14342351257801056, loss=0.01986459083855152
I0205 19:09:30.226783 140030420236032 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.17009666562080383, loss=0.020994920283555984
I0205 19:10:02.721407 140044351649536 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.16247594356536865, loss=0.023927856236696243
I0205 19:10:35.369307 140030420236032 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.15058843791484833, loss=0.020913833752274513
I0205 19:11:07.442440 140044351649536 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.194351926445961, loss=0.021322589367628098
I0205 19:11:39.727166 140030420236032 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.16104598343372345, loss=0.018763458356261253
I0205 19:12:12.305709 140044351649536 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.14295053482055664, loss=0.022770758718252182
I0205 19:12:44.321692 140030420236032 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.154710054397583, loss=0.02002834901213646
I0205 19:12:54.416595 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:14:37.693914 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:14:40.786019 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:14:43.702802 140205209478976 submission_runner.py:408] Time since start: 23540.37s, 	Step: 50032, 	{'train/accuracy': 0.9932066798210144, 'train/loss': 0.021182114258408546, 'train/mean_average_precision': 0.6060510859266648, 'validation/accuracy': 0.9864216446876526, 'validation/loss': 0.0490487702190876, 'validation/mean_average_precision': 0.2732556869305221, 'validation/num_examples': 43793, 'test/accuracy': 0.9856587052345276, 'test/loss': 0.0523470975458622, 'test/mean_average_precision': 0.2541978403734496, 'test/num_examples': 43793, 'score': 16098.356865167618, 'total_duration': 23540.37098479271, 'accumulated_submission_time': 16098.356865167618, 'accumulated_eval_time': 7438.20524597168, 'accumulated_logging_time': 2.4488024711608887}
I0205 19:14:43.728381 140020770670336 logging_writer.py:48] [50032] accumulated_eval_time=7438.205246, accumulated_logging_time=2.448802, accumulated_submission_time=16098.356865, global_step=50032, preemption_count=0, score=16098.356865, test/accuracy=0.985659, test/loss=0.052347, test/mean_average_precision=0.254198, test/num_examples=43793, total_duration=23540.370985, train/accuracy=0.993207, train/loss=0.021182, train/mean_average_precision=0.606051, validation/accuracy=0.986422, validation/loss=0.049049, validation/mean_average_precision=0.273256, validation/num_examples=43793
I0205 19:15:05.753166 140021163808512 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1754245012998581, loss=0.025277158245444298
I0205 19:15:37.715014 140020770670336 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.14799949526786804, loss=0.02019951120018959
I0205 19:16:09.640214 140021163808512 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.20067881047725677, loss=0.020444529131054878
I0205 19:16:41.786871 140020770670336 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.17847278714179993, loss=0.02325320988893509
I0205 19:17:14.084419 140021163808512 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.16736772656440735, loss=0.020811934024095535
I0205 19:17:46.123599 140020770670336 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.14537566900253296, loss=0.0205119326710701
I0205 19:18:18.138844 140021163808512 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.17296315729618073, loss=0.02030838280916214
I0205 19:18:43.894108 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:20:25.856382 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:20:28.849454 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:20:31.907591 140205209478976 submission_runner.py:408] Time since start: 23888.58s, 	Step: 50782, 	{'train/accuracy': 0.9932138323783875, 'train/loss': 0.02112257480621338, 'train/mean_average_precision': 0.6076637757132363, 'validation/accuracy': 0.9864943027496338, 'validation/loss': 0.04956536367535591, 'validation/mean_average_precision': 0.2643919542063089, 'validation/num_examples': 43793, 'test/accuracy': 0.9856283664703369, 'test/loss': 0.052835650742053986, 'test/mean_average_precision': 0.25240235104463826, 'test/num_examples': 43793, 'score': 16338.490710258484, 'total_duration': 23888.57576751709, 'accumulated_submission_time': 16338.490710258484, 'accumulated_eval_time': 7546.218659877777, 'accumulated_logging_time': 2.485704183578491}
I0205 19:20:31.932574 140044351649536 logging_writer.py:48] [50782] accumulated_eval_time=7546.218660, accumulated_logging_time=2.485704, accumulated_submission_time=16338.490710, global_step=50782, preemption_count=0, score=16338.490710, test/accuracy=0.985628, test/loss=0.052836, test/mean_average_precision=0.252402, test/num_examples=43793, total_duration=23888.575768, train/accuracy=0.993214, train/loss=0.021123, train/mean_average_precision=0.607664, validation/accuracy=0.986494, validation/loss=0.049565, validation/mean_average_precision=0.264392, validation/num_examples=43793
I0205 19:20:38.049144 140044376827648 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.21054506301879883, loss=0.023553898558020592
I0205 19:21:10.200014 140044351649536 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.22636674344539642, loss=0.022544197738170624
I0205 19:21:42.309573 140044376827648 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.18031854927539825, loss=0.02118084207177162
I0205 19:22:14.437358 140044351649536 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.16215351223945618, loss=0.02139309048652649
I0205 19:22:46.271349 140044376827648 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.17730920016765594, loss=0.020687194541096687
I0205 19:23:18.262931 140044351649536 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.17004305124282837, loss=0.01954927109181881
I0205 19:23:50.034974 140044376827648 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.18500451743602753, loss=0.02024577185511589
I0205 19:24:21.920065 140044351649536 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.17753377556800842, loss=0.020693445578217506
I0205 19:24:32.089204 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:26:14.977484 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:26:18.017932 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:26:21.023565 140205209478976 submission_runner.py:408] Time since start: 24237.69s, 	Step: 51533, 	{'train/accuracy': 0.9932173490524292, 'train/loss': 0.021080883219838142, 'train/mean_average_precision': 0.6193763045917757, 'validation/accuracy': 0.986382246017456, 'validation/loss': 0.049848511815071106, 'validation/mean_average_precision': 0.2682000829620698, 'validation/num_examples': 43793, 'test/accuracy': 0.9855205416679382, 'test/loss': 0.05324115231633186, 'test/mean_average_precision': 0.25270730641985173, 'test/num_examples': 43793, 'score': 16578.616221904755, 'total_duration': 24237.691730499268, 'accumulated_submission_time': 16578.616221904755, 'accumulated_eval_time': 7655.152938842773, 'accumulated_logging_time': 2.5216963291168213}
I0205 19:26:21.048589 140020770670336 logging_writer.py:48] [51533] accumulated_eval_time=7655.152939, accumulated_logging_time=2.521696, accumulated_submission_time=16578.616222, global_step=51533, preemption_count=0, score=16578.616222, test/accuracy=0.985521, test/loss=0.053241, test/mean_average_precision=0.252707, test/num_examples=43793, total_duration=24237.691730, train/accuracy=0.993217, train/loss=0.021081, train/mean_average_precision=0.619376, validation/accuracy=0.986382, validation/loss=0.049849, validation/mean_average_precision=0.268200, validation/num_examples=43793
I0205 19:26:43.285130 140030420236032 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.18737326562404633, loss=0.020946213975548744
I0205 19:27:15.843271 140020770670336 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2253933995962143, loss=0.019990995526313782
I0205 19:27:48.041911 140030420236032 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.1741776019334793, loss=0.02085716277360916
I0205 19:28:19.938150 140020770670336 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.17217276990413666, loss=0.01892780512571335
I0205 19:28:51.401445 140030420236032 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.20032666623592377, loss=0.021285315975546837
I0205 19:29:23.364550 140020770670336 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.17521628737449646, loss=0.019264718517661095
I0205 19:29:55.911109 140030420236032 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.20452657341957092, loss=0.02132740430533886
I0205 19:30:21.175478 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:31:56.664971 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:31:59.697211 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:32:02.815508 140205209478976 submission_runner.py:408] Time since start: 24579.48s, 	Step: 52279, 	{'train/accuracy': 0.9932751059532166, 'train/loss': 0.020680667832493782, 'train/mean_average_precision': 0.6200296858576568, 'validation/accuracy': 0.9865409731864929, 'validation/loss': 0.04983312636613846, 'validation/mean_average_precision': 0.2641053459337334, 'validation/num_examples': 43793, 'test/accuracy': 0.9856444001197815, 'test/loss': 0.05344433709979057, 'test/mean_average_precision': 0.2497437597936006, 'test/num_examples': 43793, 'score': 16818.712033748627, 'total_duration': 24579.483705997467, 'accumulated_submission_time': 16818.712033748627, 'accumulated_eval_time': 7756.792923927307, 'accumulated_logging_time': 2.5575668811798096}
I0205 19:32:02.841137 140021163808512 logging_writer.py:48] [52279] accumulated_eval_time=7756.792924, accumulated_logging_time=2.557567, accumulated_submission_time=16818.712034, global_step=52279, preemption_count=0, score=16818.712034, test/accuracy=0.985644, test/loss=0.053444, test/mean_average_precision=0.249744, test/num_examples=43793, total_duration=24579.483706, train/accuracy=0.993275, train/loss=0.020681, train/mean_average_precision=0.620030, validation/accuracy=0.986541, validation/loss=0.049833, validation/mean_average_precision=0.264105, validation/num_examples=43793
I0205 19:32:09.952358 140044376827648 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.19845819473266602, loss=0.021371539682149887
I0205 19:32:42.389098 140021163808512 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.16215527057647705, loss=0.020623210817575455
I0205 19:33:14.693637 140044376827648 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19027826189994812, loss=0.018061889335513115
I0205 19:33:46.785721 140021163808512 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.171725332736969, loss=0.017841486260294914
I0205 19:34:18.959638 140044376827648 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.16718299686908722, loss=0.019187534227967262
I0205 19:34:51.196654 140021163808512 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.24893631041049957, loss=0.022295478731393814
I0205 19:35:23.238181 140044376827648 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.17084133625030518, loss=0.020159181207418442
I0205 19:35:55.064850 140021163808512 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.24278271198272705, loss=0.022667501121759415
I0205 19:36:03.122397 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:37:43.076445 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:37:48.263750 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:37:51.249398 140205209478976 submission_runner.py:408] Time since start: 24927.92s, 	Step: 53026, 	{'train/accuracy': 0.9934790134429932, 'train/loss': 0.019983120262622833, 'train/mean_average_precision': 0.6333494312558867, 'validation/accuracy': 0.9864338040351868, 'validation/loss': 0.05018379166722298, 'validation/mean_average_precision': 0.26808323583810556, 'validation/num_examples': 43793, 'test/accuracy': 0.9855024218559265, 'test/loss': 0.05382246896624565, 'test/mean_average_precision': 0.24797876301901983, 'test/num_examples': 43793, 'score': 17058.962177991867, 'total_duration': 24927.917595624924, 'accumulated_submission_time': 17058.962177991867, 'accumulated_eval_time': 7864.91987657547, 'accumulated_logging_time': 2.594414710998535}
I0205 19:37:51.275787 140030420236032 logging_writer.py:48] [53026] accumulated_eval_time=7864.919877, accumulated_logging_time=2.594415, accumulated_submission_time=17058.962178, global_step=53026, preemption_count=0, score=17058.962178, test/accuracy=0.985502, test/loss=0.053822, test/mean_average_precision=0.247979, test/num_examples=43793, total_duration=24927.917596, train/accuracy=0.993479, train/loss=0.019983, train/mean_average_precision=0.633349, validation/accuracy=0.986434, validation/loss=0.050184, validation/mean_average_precision=0.268083, validation/num_examples=43793
I0205 19:38:15.413950 140044351649536 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.254878968000412, loss=0.01920272596180439
I0205 19:38:47.606474 140030420236032 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.1858339160680771, loss=0.01639728806912899
I0205 19:39:19.790647 140044351649536 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.21149873733520508, loss=0.017613353207707405
I0205 19:39:52.022175 140030420236032 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.19886431097984314, loss=0.01982266455888748
I0205 19:40:24.395520 140044351649536 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.19465231895446777, loss=0.020796293392777443
I0205 19:40:56.367295 140030420236032 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.20071455836296082, loss=0.020289955660700798
I0205 19:41:28.595496 140044351649536 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.19440820813179016, loss=0.019639108330011368
I0205 19:41:51.253316 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:43:31.776404 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:43:34.827265 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:43:37.797452 140205209478976 submission_runner.py:408] Time since start: 25274.47s, 	Step: 53772, 	{'train/accuracy': 0.9938586950302124, 'train/loss': 0.01891549676656723, 'train/mean_average_precision': 0.6609112039038493, 'validation/accuracy': 0.9863700866699219, 'validation/loss': 0.05027468875050545, 'validation/mean_average_precision': 0.2667339680899637, 'validation/num_examples': 43793, 'test/accuracy': 0.9854165315628052, 'test/loss': 0.054085321724414825, 'test/mean_average_precision': 0.2510609877039152, 'test/num_examples': 43793, 'score': 17298.90731549263, 'total_duration': 25274.465651512146, 'accumulated_submission_time': 17298.90731549263, 'accumulated_eval_time': 7971.4639711380005, 'accumulated_logging_time': 2.6330935955047607}
I0205 19:43:37.823208 140020770670336 logging_writer.py:48] [53772] accumulated_eval_time=7971.463971, accumulated_logging_time=2.633094, accumulated_submission_time=17298.907315, global_step=53772, preemption_count=0, score=17298.907315, test/accuracy=0.985417, test/loss=0.054085, test/mean_average_precision=0.251061, test/num_examples=43793, total_duration=25274.465652, train/accuracy=0.993859, train/loss=0.018915, train/mean_average_precision=0.660911, validation/accuracy=0.986370, validation/loss=0.050275, validation/mean_average_precision=0.266734, validation/num_examples=43793
I0205 19:43:47.245656 140044376827648 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1643434762954712, loss=0.017995573580265045
I0205 19:44:19.256740 140020770670336 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.1771479994058609, loss=0.018909739330410957
I0205 19:44:51.050390 140044376827648 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.23482432961463928, loss=0.02046145685017109
I0205 19:45:23.564099 140020770670336 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2049877643585205, loss=0.018539922311902046
I0205 19:45:55.516798 140044376827648 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.22210660576820374, loss=0.018782300874590874
I0205 19:46:27.562731 140020770670336 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.18836350739002228, loss=0.02173188142478466
I0205 19:46:59.716079 140044376827648 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.21518591046333313, loss=0.019991785287857056
I0205 19:47:31.844213 140020770670336 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.22759075462818146, loss=0.01870087906718254
I0205 19:47:37.888193 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:49:15.332805 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:49:18.353299 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:49:21.358190 140205209478976 submission_runner.py:408] Time since start: 25618.03s, 	Step: 54520, 	{'train/accuracy': 0.9943572282791138, 'train/loss': 0.017438994720578194, 'train/mean_average_precision': 0.6922163307395394, 'validation/accuracy': 0.9864155650138855, 'validation/loss': 0.05081336945295334, 'validation/mean_average_precision': 0.26253064975485707, 'validation/num_examples': 43793, 'test/accuracy': 0.9854194521903992, 'test/loss': 0.05472831428050995, 'test/mean_average_precision': 0.24891227325226248, 'test/num_examples': 43793, 'score': 17538.941791057587, 'total_duration': 25618.02638888359, 'accumulated_submission_time': 17538.941791057587, 'accumulated_eval_time': 8074.933919668198, 'accumulated_logging_time': 2.669600009918213}
I0205 19:49:21.383623 140021163808512 logging_writer.py:48] [54520] accumulated_eval_time=8074.933920, accumulated_logging_time=2.669600, accumulated_submission_time=17538.941791, global_step=54520, preemption_count=0, score=17538.941791, test/accuracy=0.985419, test/loss=0.054728, test/mean_average_precision=0.248912, test/num_examples=43793, total_duration=25618.026389, train/accuracy=0.994357, train/loss=0.017439, train/mean_average_precision=0.692216, validation/accuracy=0.986416, validation/loss=0.050813, validation/mean_average_precision=0.262531, validation/num_examples=43793
I0205 19:49:47.346539 140044351649536 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.22882629930973053, loss=0.015949638560414314
I0205 19:50:19.659649 140021163808512 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.20278608798980713, loss=0.02003607712686062
I0205 19:50:51.578058 140044351649536 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2329767942428589, loss=0.01866229623556137
I0205 19:51:23.839492 140021163808512 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.20044934749603271, loss=0.01916080340743065
I0205 19:51:55.920441 140044351649536 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.22090724110603333, loss=0.01904102973639965
I0205 19:52:28.011995 140021163808512 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.21271024644374847, loss=0.019341997802257538
I0205 19:52:59.939264 140044351649536 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.19175869226455688, loss=0.017721429467201233
I0205 19:53:21.521135 140205209478976 spec.py:321] Evaluating on the training split.
I0205 19:55:02.093599 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 19:55:05.250894 140205209478976 spec.py:349] Evaluating on the test split.
I0205 19:55:08.310757 140205209478976 submission_runner.py:408] Time since start: 25964.98s, 	Step: 55267, 	{'train/accuracy': 0.9947988390922546, 'train/loss': 0.016236860305070877, 'train/mean_average_precision': 0.7333053532778322, 'validation/accuracy': 0.9862548112869263, 'validation/loss': 0.051313333213329315, 'validation/mean_average_precision': 0.26274625230889226, 'validation/num_examples': 43793, 'test/accuracy': 0.9853533506393433, 'test/loss': 0.0552709624171257, 'test/mean_average_precision': 0.24841969844210535, 'test/num_examples': 43793, 'score': 17779.046933174133, 'total_duration': 25964.97895693779, 'accumulated_submission_time': 17779.046933174133, 'accumulated_eval_time': 8181.723499298096, 'accumulated_logging_time': 2.7075655460357666}
I0205 19:55:08.336519 140020770670336 logging_writer.py:48] [55267] accumulated_eval_time=8181.723499, accumulated_logging_time=2.707566, accumulated_submission_time=17779.046933, global_step=55267, preemption_count=0, score=17779.046933, test/accuracy=0.985353, test/loss=0.055271, test/mean_average_precision=0.248420, test/num_examples=43793, total_duration=25964.978957, train/accuracy=0.994799, train/loss=0.016237, train/mean_average_precision=0.733305, validation/accuracy=0.986255, validation/loss=0.051313, validation/mean_average_precision=0.262746, validation/num_examples=43793
I0205 19:55:19.225583 140044376827648 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.2149902731180191, loss=0.018798770383000374
I0205 19:55:51.174996 140020770670336 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.22784225642681122, loss=0.021656397730112076
I0205 19:56:23.220957 140044376827648 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.18819430470466614, loss=0.01786629669368267
I0205 19:56:55.296969 140020770670336 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.21472975611686707, loss=0.01776844449341297
I0205 19:57:27.255892 140044376827648 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1937279850244522, loss=0.018774308264255524
I0205 19:57:58.974233 140020770670336 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.19987313449382782, loss=0.016671324148774147
I0205 19:58:30.872195 140044376827648 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.19001157581806183, loss=0.015459053218364716
I0205 19:59:03.185445 140020770670336 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.18828147649765015, loss=0.015452032908797264
I0205 19:59:08.345159 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:00:50.630249 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:00:53.775926 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:00:56.892360 140205209478976 submission_runner.py:408] Time since start: 26313.56s, 	Step: 56017, 	{'train/accuracy': 0.9949828386306763, 'train/loss': 0.015798229724168777, 'train/mean_average_precision': 0.7273481342521675, 'validation/accuracy': 0.9862483143806458, 'validation/loss': 0.05185191333293915, 'validation/mean_average_precision': 0.26084115749890824, 'validation/num_examples': 43793, 'test/accuracy': 0.9853945970535278, 'test/loss': 0.05584893375635147, 'test/mean_average_precision': 0.25320181507136497, 'test/num_examples': 43793, 'score': 18019.023313760757, 'total_duration': 26313.56055831909, 'accumulated_submission_time': 18019.023313760757, 'accumulated_eval_time': 8290.270651817322, 'accumulated_logging_time': 2.7457399368286133}
I0205 20:00:56.919418 140021163808512 logging_writer.py:48] [56017] accumulated_eval_time=8290.270652, accumulated_logging_time=2.745740, accumulated_submission_time=18019.023314, global_step=56017, preemption_count=0, score=18019.023314, test/accuracy=0.985395, test/loss=0.055849, test/mean_average_precision=0.253202, test/num_examples=43793, total_duration=26313.560558, train/accuracy=0.994983, train/loss=0.015798, train/mean_average_precision=0.727348, validation/accuracy=0.986248, validation/loss=0.051852, validation/mean_average_precision=0.260841, validation/num_examples=43793
I0205 20:01:23.632369 140030420236032 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.19737787544727325, loss=0.018561746925115585
I0205 20:01:55.340272 140021163808512 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2234601080417633, loss=0.018832916393876076
I0205 20:02:27.526641 140030420236032 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.1995197832584381, loss=0.015140264295041561
I0205 20:02:59.555909 140021163808512 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.19286298751831055, loss=0.01660170778632164
I0205 20:03:31.901795 140030420236032 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.18963780999183655, loss=0.017638878896832466
I0205 20:04:03.942512 140021163808512 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.22036723792552948, loss=0.01829015463590622
I0205 20:04:35.979046 140030420236032 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.1783149093389511, loss=0.016774911433458328
I0205 20:04:57.175800 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:06:40.764609 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:06:43.896799 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:06:46.985586 140205209478976 submission_runner.py:408] Time since start: 26663.65s, 	Step: 56766, 	{'train/accuracy': 0.9939604997634888, 'train/loss': 0.018462734296917915, 'train/mean_average_precision': 0.6661507763472906, 'validation/accuracy': 0.9862211346626282, 'validation/loss': 0.05208123102784157, 'validation/mean_average_precision': 0.26548057966644023, 'validation/num_examples': 43793, 'test/accuracy': 0.9853697419166565, 'test/loss': 0.05587721988558769, 'test/mean_average_precision': 0.2519444893929446, 'test/num_examples': 43793, 'score': 18259.248502254486, 'total_duration': 26663.653777122498, 'accumulated_submission_time': 18259.248502254486, 'accumulated_eval_time': 8400.08038520813, 'accumulated_logging_time': 2.7842960357666016}
I0205 20:06:47.011582 140020770670336 logging_writer.py:48] [56766] accumulated_eval_time=8400.080385, accumulated_logging_time=2.784296, accumulated_submission_time=18259.248502, global_step=56766, preemption_count=0, score=18259.248502, test/accuracy=0.985370, test/loss=0.055877, test/mean_average_precision=0.251944, test/num_examples=43793, total_duration=26663.653777, train/accuracy=0.993960, train/loss=0.018463, train/mean_average_precision=0.666151, validation/accuracy=0.986221, validation/loss=0.052081, validation/mean_average_precision=0.265481, validation/num_examples=43793
I0205 20:06:59.003281 140044351649536 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.20971092581748962, loss=0.016925526782870293
I0205 20:07:31.040099 140020770670336 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2469034045934677, loss=0.019293783232569695
I0205 20:08:02.902910 140044351649536 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.21751585602760315, loss=0.019023016095161438
I0205 20:08:34.730730 140020770670336 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.20274226367473602, loss=0.01609296165406704
I0205 20:09:06.709084 140044351649536 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.22164496779441833, loss=0.017546286806464195
I0205 20:09:38.564783 140020770670336 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.19375291466712952, loss=0.015872078016400337
I0205 20:10:10.532698 140044351649536 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.24418765306472778, loss=0.015394491143524647
I0205 20:10:24.886939 140020770670336 logging_writer.py:48] [57446] global_step=57446, preemption_count=0, score=18477.078250
I0205 20:10:24.939374 140205209478976 checkpoints.py:490] Saving checkpoint at step: 57446
I0205 20:10:25.071170 140205209478976 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_3/checkpoint_57446
I0205 20:10:25.072241 140205209478976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_3/checkpoint_57446.
I0205 20:10:25.222273 140205209478976 submission_runner.py:583] Tuning trial 3/5
I0205 20:10:25.222497 140205209478976 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0205 20:10:25.226932 140205209478976 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5091323852539062, 'train/loss': 0.7392957210540771, 'train/mean_average_precision': 0.022487030195939863, 'validation/accuracy': 0.5064647793769836, 'validation/loss': 0.7422075867652893, 'validation/mean_average_precision': 0.02529862965175552, 'validation/num_examples': 43793, 'test/accuracy': 0.5047287344932556, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.026928990047386532, 'test/num_examples': 43793, 'score': 11.409060716629028, 'total_duration': 126.4563901424408, 'accumulated_submission_time': 11.409060716629028, 'accumulated_eval_time': 115.04727101325989, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (748, {'train/accuracy': 0.9866933822631836, 'train/loss': 0.06980946660041809, 'train/mean_average_precision': 0.03583625443003626, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07773509621620178, 'validation/mean_average_precision': 0.0368130067090984, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08055828511714935, 'test/mean_average_precision': 0.0400774903969423, 'test/num_examples': 43793, 'score': 251.55146312713623, 'total_duration': 479.7769241333008, 'accumulated_submission_time': 251.55146312713623, 'accumulated_eval_time': 228.1838824748993, 'accumulated_logging_time': 0.02129340171813965, 'global_step': 748, 'preemption_count': 0}), (1492, {'train/accuracy': 0.987136721611023, 'train/loss': 0.04884883388876915, 'train/mean_average_precision': 0.08048615640400439, 'validation/accuracy': 0.9844698905944824, 'validation/loss': 0.05820437893271446, 'validation/mean_average_precision': 0.08047419959382764, 'validation/num_examples': 43793, 'test/accuracy': 0.9834933280944824, 'test/loss': 0.061417415738105774, 'test/mean_average_precision': 0.08071569313351043, 'test/num_examples': 43793, 'score': 491.73076152801514, 'total_duration': 828.7861225605011, 'accumulated_submission_time': 491.73076152801514, 'accumulated_eval_time': 336.9643814563751, 'accumulated_logging_time': 0.05037117004394531, 'global_step': 1492, 'preemption_count': 0}), (2240, {'train/accuracy': 0.987756609916687, 'train/loss': 0.04443162679672241, 'train/mean_average_precision': 0.12847178421730218, 'validation/accuracy': 0.9850073456764221, 'validation/loss': 0.053563233464956284, 'validation/mean_average_precision': 0.12322239156428166, 'validation/num_examples': 43793, 'test/accuracy': 0.9840232133865356, 'test/loss': 0.0565517283976078, 'test/mean_average_precision': 0.12192161108931347, 'test/num_examples': 43793, 'score': 731.8950526714325, 'total_duration': 1181.5423827171326, 'accumulated_submission_time': 731.8950526714325, 'accumulated_eval_time': 449.5094120502472, 'accumulated_logging_time': 0.07734990119934082, 'global_step': 2240, 'preemption_count': 0}), (2982, {'train/accuracy': 0.9878631830215454, 'train/loss': 0.04359974339604378, 'train/mean_average_precision': 0.15402269006234956, 'validation/accuracy': 0.9849075078964233, 'validation/loss': 0.05237336456775665, 'validation/mean_average_precision': 0.13969176410434406, 'validation/num_examples': 43793, 'test/accuracy': 0.9840303659439087, 'test/loss': 0.05466533452272415, 'test/mean_average_precision': 0.14474327212571866, 'test/num_examples': 43793, 'score': 972.0586864948273, 'total_duration': 1538.52033162117, 'accumulated_submission_time': 972.0586864948273, 'accumulated_eval_time': 566.2745454311371, 'accumulated_logging_time': 0.10538744926452637, 'global_step': 2982, 'preemption_count': 0}), (3716, {'train/accuracy': 0.9883297085762024, 'train/loss': 0.0408373698592186, 'train/mean_average_precision': 0.1751967621768912, 'validation/accuracy': 0.9854157567024231, 'validation/loss': 0.05042848363518715, 'validation/mean_average_precision': 0.16654359631809973, 'validation/num_examples': 43793, 'test/accuracy': 0.9845231771469116, 'test/loss': 0.053182750940322876, 'test/mean_average_precision': 0.16787737271235917, 'test/num_examples': 43793, 'score': 1212.2938175201416, 'total_duration': 1891.7137813568115, 'accumulated_submission_time': 1212.2938175201416, 'accumulated_eval_time': 679.1746046543121, 'accumulated_logging_time': 0.13896894454956055, 'global_step': 3716, 'preemption_count': 0}), (4456, {'train/accuracy': 0.9884167313575745, 'train/loss': 0.04014383628964424, 'train/mean_average_precision': 0.2029430039185905, 'validation/accuracy': 0.9853593111038208, 'validation/loss': 0.04974988102912903, 'validation/mean_average_precision': 0.17537489907493375, 'validation/num_examples': 43793, 'test/accuracy': 0.9844696521759033, 'test/loss': 0.05213036388158798, 'test/mean_average_precision': 0.18201119611679045, 'test/num_examples': 43793, 'score': 1452.2720143795013, 'total_duration': 2245.531072616577, 'accumulated_submission_time': 1452.2720143795013, 'accumulated_eval_time': 792.9642839431763, 'accumulated_logging_time': 0.16809701919555664, 'global_step': 4456, 'preemption_count': 0}), (5202, {'train/accuracy': 0.9886823296546936, 'train/loss': 0.038618121296167374, 'train/mean_average_precision': 0.21325482904122076, 'validation/accuracy': 0.9857100248336792, 'validation/loss': 0.048392053693532944, 'validation/mean_average_precision': 0.19136392204456362, 'validation/num_examples': 43793, 'test/accuracy': 0.9848099946975708, 'test/loss': 0.05121665447950363, 'test/mean_average_precision': 0.1928349290109782, 'test/num_examples': 43793, 'score': 1692.4591073989868, 'total_duration': 2594.3612883090973, 'accumulated_submission_time': 1692.4591073989868, 'accumulated_eval_time': 901.5599522590637, 'accumulated_logging_time': 0.19564485549926758, 'global_step': 5202, 'preemption_count': 0}), (5953, {'train/accuracy': 0.9889214038848877, 'train/loss': 0.0379679910838604, 'train/mean_average_precision': 0.24135911960156448, 'validation/accuracy': 0.9860051274299622, 'validation/loss': 0.04773245379328728, 'validation/mean_average_precision': 0.20286832028184776, 'validation/num_examples': 43793, 'test/accuracy': 0.9849632978439331, 'test/loss': 0.0504787340760231, 'test/mean_average_precision': 0.197353427924065, 'test/num_examples': 43793, 'score': 1932.6851942539215, 'total_duration': 2945.617326259613, 'accumulated_submission_time': 1932.6851942539215, 'accumulated_eval_time': 1012.5432093143463, 'accumulated_logging_time': 0.2226417064666748, 'global_step': 5953, 'preemption_count': 0}), (6704, {'train/accuracy': 0.989250123500824, 'train/loss': 0.036856867372989655, 'train/mean_average_precision': 0.2679609941091441, 'validation/accuracy': 0.9859207272529602, 'validation/loss': 0.047123752534389496, 'validation/mean_average_precision': 0.20606608966205492, 'validation/num_examples': 43793, 'test/accuracy': 0.9851145148277283, 'test/loss': 0.04972551763057709, 'test/mean_average_precision': 0.20891194351793108, 'test/num_examples': 43793, 'score': 2172.6391608715057, 'total_duration': 3293.7659606933594, 'accumulated_submission_time': 2172.6391608715057, 'accumulated_eval_time': 1120.6893513202667, 'accumulated_logging_time': 0.2512521743774414, 'global_step': 6704, 'preemption_count': 0}), (7446, {'train/accuracy': 0.9892452955245972, 'train/loss': 0.03657560423016548, 'train/mean_average_precision': 0.2556209530754341, 'validation/accuracy': 0.9861403107643127, 'validation/loss': 0.04662451893091202, 'validation/mean_average_precision': 0.21814077435373458, 'validation/num_examples': 43793, 'test/accuracy': 0.9852392077445984, 'test/loss': 0.04920124635100365, 'test/mean_average_precision': 0.21603020952659968, 'test/num_examples': 43793, 'score': 2412.912478208542, 'total_duration': 3646.7018489837646, 'accumulated_submission_time': 2412.912478208542, 'accumulated_eval_time': 1233.3041589260101, 'accumulated_logging_time': 0.2791018486022949, 'global_step': 7446, 'preemption_count': 0}), (8192, {'train/accuracy': 0.9891985654830933, 'train/loss': 0.0363474003970623, 'train/mean_average_precision': 0.27777547186001494, 'validation/accuracy': 0.9861192107200623, 'validation/loss': 0.04642857611179352, 'validation/mean_average_precision': 0.2241906615481037, 'validation/num_examples': 43793, 'test/accuracy': 0.985280454158783, 'test/loss': 0.04895807430148125, 'test/mean_average_precision': 0.22005215130504002, 'test/num_examples': 43793, 'score': 2653.0472116470337, 'total_duration': 3996.4135501384735, 'accumulated_submission_time': 2653.0472116470337, 'accumulated_eval_time': 1342.8322954177856, 'accumulated_logging_time': 0.30754566192626953, 'global_step': 8192, 'preemption_count': 0}), (8944, {'train/accuracy': 0.9893425703048706, 'train/loss': 0.0358811616897583, 'train/mean_average_precision': 0.27784066435915766, 'validation/accuracy': 0.9862227439880371, 'validation/loss': 0.0463167168200016, 'validation/mean_average_precision': 0.21345663477538704, 'validation/num_examples': 43793, 'test/accuracy': 0.9853727221488953, 'test/loss': 0.04898221045732498, 'test/mean_average_precision': 0.21924437850548517, 'test/num_examples': 43793, 'score': 2893.168367624283, 'total_duration': 4347.192717552185, 'accumulated_submission_time': 2893.168367624283, 'accumulated_eval_time': 1453.4429540634155, 'accumulated_logging_time': 0.335129976272583, 'global_step': 8944, 'preemption_count': 0}), (9700, {'train/accuracy': 0.989570677280426, 'train/loss': 0.03492804989218712, 'train/mean_average_precision': 0.2965503820496965, 'validation/accuracy': 0.9864078760147095, 'validation/loss': 0.04607466980814934, 'validation/mean_average_precision': 0.23116527704377005, 'validation/num_examples': 43793, 'test/accuracy': 0.9855323433876038, 'test/loss': 0.04869677126407623, 'test/mean_average_precision': 0.22627039185858266, 'test/num_examples': 43793, 'score': 3133.286482810974, 'total_duration': 4695.340826034546, 'accumulated_submission_time': 3133.286482810974, 'accumulated_eval_time': 1561.425509929657, 'accumulated_logging_time': 0.36286282539367676, 'global_step': 9700, 'preemption_count': 0}), (10446, {'train/accuracy': 0.9897487759590149, 'train/loss': 0.03432413935661316, 'train/mean_average_precision': 0.31451403243094755, 'validation/accuracy': 0.9864723682403564, 'validation/loss': 0.04550735652446747, 'validation/mean_average_precision': 0.2354921471673552, 'validation/num_examples': 43793, 'test/accuracy': 0.9855976104736328, 'test/loss': 0.04816288873553276, 'test/mean_average_precision': 0.23218881704430824, 'test/num_examples': 43793, 'score': 3373.3551876544952, 'total_duration': 5045.592380523682, 'accumulated_submission_time': 3373.3551876544952, 'accumulated_eval_time': 1671.5593321323395, 'accumulated_logging_time': 0.3925163745880127, 'global_step': 10446, 'preemption_count': 0}), (11191, {'train/accuracy': 0.9900938272476196, 'train/loss': 0.03343943879008293, 'train/mean_average_precision': 0.3254542392372632, 'validation/accuracy': 0.9865438342094421, 'validation/loss': 0.04487171396613121, 'validation/mean_average_precision': 0.24166720918696438, 'validation/num_examples': 43793, 'test/accuracy': 0.9856485724449158, 'test/loss': 0.04768587276339531, 'test/mean_average_precision': 0.24127745643876747, 'test/num_examples': 43793, 'score': 3613.492337703705, 'total_duration': 5393.927290678024, 'accumulated_submission_time': 3613.492337703705, 'accumulated_eval_time': 1779.7080972194672, 'accumulated_logging_time': 0.420879602432251, 'global_step': 11191, 'preemption_count': 0}), (11940, {'train/accuracy': 0.9900937676429749, 'train/loss': 0.03291260823607445, 'train/mean_average_precision': 0.34256466207079067, 'validation/accuracy': 0.9863781929016113, 'validation/loss': 0.04511989280581474, 'validation/mean_average_precision': 0.23639738856923834, 'validation/num_examples': 43793, 'test/accuracy': 0.9855904579162598, 'test/loss': 0.04759938642382622, 'test/mean_average_precision': 0.23495691603311641, 'test/num_examples': 43793, 'score': 3853.5046513080597, 'total_duration': 5748.61470246315, 'accumulated_submission_time': 3853.5046513080597, 'accumulated_eval_time': 1894.3356800079346, 'accumulated_logging_time': 0.4488670825958252, 'global_step': 11940, 'preemption_count': 0}), (12683, {'train/accuracy': 0.9903525710105896, 'train/loss': 0.03204910084605217, 'train/mean_average_precision': 0.37730289269277073, 'validation/accuracy': 0.9866164922714233, 'validation/loss': 0.044836658984422684, 'validation/mean_average_precision': 0.25276370255248154, 'validation/num_examples': 43793, 'test/accuracy': 0.9857320189476013, 'test/loss': 0.047577790915966034, 'test/mean_average_precision': 0.24766468234854286, 'test/num_examples': 43793, 'score': 4093.458253622055, 'total_duration': 6100.5614466667175, 'accumulated_submission_time': 4093.458253622055, 'accumulated_eval_time': 2006.2802784442902, 'accumulated_logging_time': 0.4773871898651123, 'global_step': 12683, 'preemption_count': 0}), (13417, {'train/accuracy': 0.9905537962913513, 'train/loss': 0.031089400872588158, 'train/mean_average_precision': 0.37240313133773384, 'validation/accuracy': 0.9866245985031128, 'validation/loss': 0.0445709228515625, 'validation/mean_average_precision': 0.24922931526205078, 'validation/num_examples': 43793, 'test/accuracy': 0.9857416749000549, 'test/loss': 0.04735759273171425, 'test/mean_average_precision': 0.24515711802698764, 'test/num_examples': 43793, 'score': 4333.447480678558, 'total_duration': 6455.656747102737, 'accumulated_submission_time': 4333.447480678558, 'accumulated_eval_time': 2121.3362860679626, 'accumulated_logging_time': 0.5057508945465088, 'global_step': 13417, 'preemption_count': 0}), (14151, {'train/accuracy': 0.9907062649726868, 'train/loss': 0.030991319566965103, 'train/mean_average_precision': 0.37859976456395106, 'validation/accuracy': 0.9865682125091553, 'validation/loss': 0.044715654104948044, 'validation/mean_average_precision': 0.25239165412918096, 'validation/num_examples': 43793, 'test/accuracy': 0.9856612086296082, 'test/loss': 0.047581084072589874, 'test/mean_average_precision': 0.24578013946410093, 'test/num_examples': 43793, 'score': 4573.6459176540375, 'total_duration': 6803.714690446854, 'accumulated_submission_time': 4573.6459176540375, 'accumulated_eval_time': 2229.1408503055573, 'accumulated_logging_time': 0.5367867946624756, 'global_step': 14151, 'preemption_count': 0}), (14903, {'train/accuracy': 0.9905675649642944, 'train/loss': 0.03123924322426319, 'train/mean_average_precision': 0.39005511484235345, 'validation/accuracy': 0.9866558909416199, 'validation/loss': 0.04476241394877434, 'validation/mean_average_precision': 0.25344476467938665, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.047421928495168686, 'test/mean_average_precision': 0.2527017177476719, 'test/num_examples': 43793, 'score': 4813.645154476166, 'total_duration': 7158.222968816757, 'accumulated_submission_time': 4813.645154476166, 'accumulated_eval_time': 2343.599368095398, 'accumulated_logging_time': 0.567765474319458, 'global_step': 14903, 'preemption_count': 0}), (15652, {'train/accuracy': 0.9906066060066223, 'train/loss': 0.031121844425797462, 'train/mean_average_precision': 0.38131995024157306, 'validation/accuracy': 0.9867772459983826, 'validation/loss': 0.04441023990511894, 'validation/mean_average_precision': 0.2583747608772624, 'validation/num_examples': 43793, 'test/accuracy': 0.9858461618423462, 'test/loss': 0.0471496619284153, 'test/mean_average_precision': 0.2551532981849261, 'test/num_examples': 43793, 'score': 5053.900189876556, 'total_duration': 7504.557371377945, 'accumulated_submission_time': 5053.900189876556, 'accumulated_eval_time': 2449.6269824504852, 'accumulated_logging_time': 0.5992722511291504, 'global_step': 15652, 'preemption_count': 0}), (16395, {'train/accuracy': 0.9906516671180725, 'train/loss': 0.030857114121317863, 'train/mean_average_precision': 0.38694554789878394, 'validation/accuracy': 0.9865178465843201, 'validation/loss': 0.04490754380822182, 'validation/mean_average_precision': 0.2555116961913109, 'validation/num_examples': 43793, 'test/accuracy': 0.985654890537262, 'test/loss': 0.047570016235113144, 'test/mean_average_precision': 0.24621899625449672, 'test/num_examples': 43793, 'score': 5294.087451457977, 'total_duration': 7856.6856853961945, 'accumulated_submission_time': 5294.087451457977, 'accumulated_eval_time': 2561.516103744507, 'accumulated_logging_time': 0.6315131187438965, 'global_step': 16395, 'preemption_count': 0}), (17141, {'train/accuracy': 0.990618884563446, 'train/loss': 0.03099219873547554, 'train/mean_average_precision': 0.38567776672741516, 'validation/accuracy': 0.9865024089813232, 'validation/loss': 0.044638730585575104, 'validation/mean_average_precision': 0.26013765900164426, 'validation/num_examples': 43793, 'test/accuracy': 0.9855812191963196, 'test/loss': 0.04733843356370926, 'test/mean_average_precision': 0.25248843629595097, 'test/num_examples': 43793, 'score': 5534.083811998367, 'total_duration': 8205.371302127838, 'accumulated_submission_time': 5534.083811998367, 'accumulated_eval_time': 2670.1546428203583, 'accumulated_logging_time': 0.6623170375823975, 'global_step': 17141, 'preemption_count': 0}), (17887, {'train/accuracy': 0.9904924035072327, 'train/loss': 0.031084245070815086, 'train/mean_average_precision': 0.39302518302628175, 'validation/accuracy': 0.9865880608558655, 'validation/loss': 0.0455077663064003, 'validation/mean_average_precision': 0.2632867709525448, 'validation/num_examples': 43793, 'test/accuracy': 0.9856747388839722, 'test/loss': 0.048309292644262314, 'test/mean_average_precision': 0.24612998964632649, 'test/num_examples': 43793, 'score': 5774.310196876526, 'total_duration': 8557.450502157211, 'accumulated_submission_time': 5774.310196876526, 'accumulated_eval_time': 2781.9571702480316, 'accumulated_logging_time': 0.6927800178527832, 'global_step': 17887, 'preemption_count': 0}), (18633, {'train/accuracy': 0.9910005927085876, 'train/loss': 0.02931615523993969, 'train/mean_average_precision': 0.42218359821230533, 'validation/accuracy': 0.9868016242980957, 'validation/loss': 0.04465004429221153, 'validation/mean_average_precision': 0.26313624471730956, 'validation/num_examples': 43793, 'test/accuracy': 0.9858941435813904, 'test/loss': 0.047337278723716736, 'test/mean_average_precision': 0.2532866247875107, 'test/num_examples': 43793, 'score': 6014.308893442154, 'total_duration': 8904.329939126968, 'accumulated_submission_time': 6014.308893442154, 'accumulated_eval_time': 2888.7862842082977, 'accumulated_logging_time': 0.7243075370788574, 'global_step': 18633, 'preemption_count': 0}), (19385, {'train/accuracy': 0.991144597530365, 'train/loss': 0.028853613883256912, 'train/mean_average_precision': 0.4362776975542201, 'validation/accuracy': 0.9868060946464539, 'validation/loss': 0.044303517788648605, 'validation/mean_average_precision': 0.27145992009299347, 'validation/num_examples': 43793, 'test/accuracy': 0.9859779477119446, 'test/loss': 0.0470069982111454, 'test/mean_average_precision': 0.25648081370632786, 'test/num_examples': 43793, 'score': 6254.484782218933, 'total_duration': 9256.199064016342, 'accumulated_submission_time': 6254.484782218933, 'accumulated_eval_time': 3000.428964138031, 'accumulated_logging_time': 0.754981517791748, 'global_step': 19385, 'preemption_count': 0}), (20137, {'train/accuracy': 0.9912317991256714, 'train/loss': 0.028409577906131744, 'train/mean_average_precision': 0.45106485488955966, 'validation/accuracy': 0.9867587685585022, 'validation/loss': 0.044544562697410583, 'validation/mean_average_precision': 0.2627217483865065, 'validation/num_examples': 43793, 'test/accuracy': 0.9858676195144653, 'test/loss': 0.04751855134963989, 'test/mean_average_precision': 0.2565515376288197, 'test/num_examples': 43793, 'score': 6494.502206325531, 'total_duration': 9602.328292369843, 'accumulated_submission_time': 6494.502206325531, 'accumulated_eval_time': 3106.488452196121, 'accumulated_logging_time': 0.7869946956634521, 'global_step': 20137, 'preemption_count': 0}), (20892, {'train/accuracy': 0.9913666248321533, 'train/loss': 0.028140483424067497, 'train/mean_average_precision': 0.4477376798318661, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.04500306025147438, 'validation/mean_average_precision': 0.2594265373774643, 'validation/num_examples': 43793, 'test/accuracy': 0.985849916934967, 'test/loss': 0.04767508804798126, 'test/mean_average_precision': 0.2547641389335, 'test/num_examples': 43793, 'score': 6734.702629804611, 'total_duration': 9953.830657482147, 'accumulated_submission_time': 6734.702629804611, 'accumulated_eval_time': 3217.7399446964264, 'accumulated_logging_time': 0.8169877529144287, 'global_step': 20892, 'preemption_count': 0}), (21632, {'train/accuracy': 0.9910850524902344, 'train/loss': 0.02917107380926609, 'train/mean_average_precision': 0.41867738324957493, 'validation/accuracy': 0.986506462097168, 'validation/loss': 0.044595010578632355, 'validation/mean_average_precision': 0.260642723999837, 'validation/num_examples': 43793, 'test/accuracy': 0.9857699275016785, 'test/loss': 0.047313250601291656, 'test/mean_average_precision': 0.25456625768344016, 'test/num_examples': 43793, 'score': 6974.657138586044, 'total_duration': 10306.164506912231, 'accumulated_submission_time': 6974.657138586044, 'accumulated_eval_time': 3330.0670692920685, 'accumulated_logging_time': 0.84893798828125, 'global_step': 21632, 'preemption_count': 0}), (22380, {'train/accuracy': 0.9910112023353577, 'train/loss': 0.02939889393746853, 'train/mean_average_precision': 0.413999674447663, 'validation/accuracy': 0.9867098927497864, 'validation/loss': 0.04477539658546448, 'validation/mean_average_precision': 0.26131974641292277, 'validation/num_examples': 43793, 'test/accuracy': 0.9858149886131287, 'test/loss': 0.04739511385560036, 'test/mean_average_precision': 0.26023518949880103, 'test/num_examples': 43793, 'score': 7214.641557455063, 'total_duration': 10657.6014316082, 'accumulated_submission_time': 7214.641557455063, 'accumulated_eval_time': 3441.468363761902, 'accumulated_logging_time': 0.8800814151763916, 'global_step': 22380, 'preemption_count': 0}), (23133, {'train/accuracy': 0.9910687804222107, 'train/loss': 0.02927219867706299, 'train/mean_average_precision': 0.42751952278599203, 'validation/accuracy': 0.9866611361503601, 'validation/loss': 0.04471036046743393, 'validation/mean_average_precision': 0.2611749950881226, 'validation/num_examples': 43793, 'test/accuracy': 0.9857193827629089, 'test/loss': 0.04752102494239807, 'test/mean_average_precision': 0.256449131398079, 'test/num_examples': 43793, 'score': 7454.6159999370575, 'total_duration': 11006.72515630722, 'accumulated_submission_time': 7454.6159999370575, 'accumulated_eval_time': 3550.5645759105682, 'accumulated_logging_time': 0.9132204055786133, 'global_step': 23133, 'preemption_count': 0}), (23882, {'train/accuracy': 0.9910465478897095, 'train/loss': 0.029097504913806915, 'train/mean_average_precision': 0.43902965739375505, 'validation/accuracy': 0.986792266368866, 'validation/loss': 0.04489562287926674, 'validation/mean_average_precision': 0.26428720016624674, 'validation/num_examples': 43793, 'test/accuracy': 0.9859809279441833, 'test/loss': 0.04757880046963692, 'test/mean_average_precision': 0.25833596487584526, 'test/num_examples': 43793, 'score': 7694.792117834091, 'total_duration': 11361.706496953964, 'accumulated_submission_time': 7694.792117834091, 'accumulated_eval_time': 3665.3181777000427, 'accumulated_logging_time': 0.9451286792755127, 'global_step': 23882, 'preemption_count': 0}), (24616, {'train/accuracy': 0.9911187291145325, 'train/loss': 0.02876371331512928, 'train/mean_average_precision': 0.4408214315236216, 'validation/accuracy': 0.9866234064102173, 'validation/loss': 0.044969748705625534, 'validation/mean_average_precision': 0.2635825782785605, 'validation/num_examples': 43793, 'test/accuracy': 0.9857947826385498, 'test/loss': 0.04763353243470192, 'test/mean_average_precision': 0.25082574583198197, 'test/num_examples': 43793, 'score': 7934.79806470871, 'total_duration': 11710.377674818039, 'accumulated_submission_time': 7934.79806470871, 'accumulated_eval_time': 3773.929664850235, 'accumulated_logging_time': 0.9760401248931885, 'global_step': 24616, 'preemption_count': 0}), (25358, {'train/accuracy': 0.9912756681442261, 'train/loss': 0.02825007773935795, 'train/mean_average_precision': 0.4428742902391496, 'validation/accuracy': 0.986655056476593, 'validation/loss': 0.04483070597052574, 'validation/mean_average_precision': 0.26376133340425956, 'validation/num_examples': 43793, 'test/accuracy': 0.9857442378997803, 'test/loss': 0.04766061156988144, 'test/mean_average_precision': 0.2516790343245108, 'test/num_examples': 43793, 'score': 8174.913177490234, 'total_duration': 12064.129575967789, 'accumulated_submission_time': 8174.913177490234, 'accumulated_eval_time': 3887.5153410434723, 'accumulated_logging_time': 1.0077130794525146, 'global_step': 25358, 'preemption_count': 0}), (26103, {'train/accuracy': 0.9914022088050842, 'train/loss': 0.027766233310103416, 'train/mean_average_precision': 0.45234100839245117, 'validation/accuracy': 0.9866623878479004, 'validation/loss': 0.04479790851473808, 'validation/mean_average_precision': 0.2641685438259154, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.047423217445611954, 'test/mean_average_precision': 0.25438069850517014, 'test/num_examples': 43793, 'score': 8414.918694972992, 'total_duration': 12413.05533671379, 'accumulated_submission_time': 8414.918694972992, 'accumulated_eval_time': 3996.3839950561523, 'accumulated_logging_time': 1.039290189743042, 'global_step': 26103, 'preemption_count': 0}), (26844, {'train/accuracy': 0.991820216178894, 'train/loss': 0.02651890367269516, 'train/mean_average_precision': 0.49922541465408793, 'validation/accuracy': 0.986660361289978, 'validation/loss': 0.044735804200172424, 'validation/mean_average_precision': 0.2714495760771454, 'validation/num_examples': 43793, 'test/accuracy': 0.9857585430145264, 'test/loss': 0.047469232231378555, 'test/mean_average_precision': 0.25685511484781937, 'test/num_examples': 43793, 'score': 8655.010743618011, 'total_duration': 12761.361777067184, 'accumulated_submission_time': 8655.010743618011, 'accumulated_eval_time': 4104.547461509705, 'accumulated_logging_time': 1.0705244541168213, 'global_step': 26844, 'preemption_count': 0}), (27591, {'train/accuracy': 0.99174964427948, 'train/loss': 0.026884455233812332, 'train/mean_average_precision': 0.48900363211065134, 'validation/accuracy': 0.98653244972229, 'validation/loss': 0.045182015746831894, 'validation/mean_average_precision': 0.2685428630516191, 'validation/num_examples': 43793, 'test/accuracy': 0.9857147336006165, 'test/loss': 0.04805206507444382, 'test/mean_average_precision': 0.2612988077710391, 'test/num_examples': 43793, 'score': 8895.237357616425, 'total_duration': 13110.495953798294, 'accumulated_submission_time': 8895.237357616425, 'accumulated_eval_time': 4213.402389764786, 'accumulated_logging_time': 1.1035490036010742, 'global_step': 27591, 'preemption_count': 0}), (28339, {'train/accuracy': 0.991543173789978, 'train/loss': 0.027303244918584824, 'train/mean_average_precision': 0.4728328304098546, 'validation/accuracy': 0.9867244958877563, 'validation/loss': 0.04520970955491066, 'validation/mean_average_precision': 0.267856911482501, 'validation/num_examples': 43793, 'test/accuracy': 0.9858225584030151, 'test/loss': 0.04836053401231766, 'test/mean_average_precision': 0.2572859701949634, 'test/num_examples': 43793, 'score': 9135.220917224884, 'total_duration': 13459.781569480896, 'accumulated_submission_time': 9135.220917224884, 'accumulated_eval_time': 4322.6532344818115, 'accumulated_logging_time': 1.1350586414337158, 'global_step': 28339, 'preemption_count': 0}), (29087, {'train/accuracy': 0.9913496375083923, 'train/loss': 0.027921341359615326, 'train/mean_average_precision': 0.44670458985016304, 'validation/accuracy': 0.9866745471954346, 'validation/loss': 0.044872529804706573, 'validation/mean_average_precision': 0.2700355602727555, 'validation/num_examples': 43793, 'test/accuracy': 0.9858547449111938, 'test/loss': 0.04762396961450577, 'test/mean_average_precision': 0.2617479458606887, 'test/num_examples': 43793, 'score': 9375.281760692596, 'total_duration': 13806.64552783966, 'accumulated_submission_time': 9375.281760692596, 'accumulated_eval_time': 4429.404903411865, 'accumulated_logging_time': 1.1667847633361816, 'global_step': 29087, 'preemption_count': 0}), (29841, {'train/accuracy': 0.9912735819816589, 'train/loss': 0.028208354488015175, 'train/mean_average_precision': 0.4498426126219626, 'validation/accuracy': 0.9865994453430176, 'validation/loss': 0.04536927491426468, 'validation/mean_average_precision': 0.2591981850120724, 'validation/num_examples': 43793, 'test/accuracy': 0.9858208894729614, 'test/loss': 0.04811793565750122, 'test/mean_average_precision': 0.25302340075831153, 'test/num_examples': 43793, 'score': 9615.312079906464, 'total_duration': 14153.5893740654, 'accumulated_submission_time': 9615.312079906464, 'accumulated_eval_time': 4536.267310619354, 'accumulated_logging_time': 1.1982874870300293, 'global_step': 29841, 'preemption_count': 0}), (30587, {'train/accuracy': 0.9914898872375488, 'train/loss': 0.027387216687202454, 'train/mean_average_precision': 0.47278088836117016, 'validation/accuracy': 0.986668050289154, 'validation/loss': 0.04530851170420647, 'validation/mean_average_precision': 0.26285704163105317, 'validation/num_examples': 43793, 'test/accuracy': 0.9857636094093323, 'test/loss': 0.0484786257147789, 'test/mean_average_precision': 0.254502359861954, 'test/num_examples': 43793, 'score': 9855.560412883759, 'total_duration': 14503.186990499496, 'accumulated_submission_time': 9855.560412883759, 'accumulated_eval_time': 4645.564736843109, 'accumulated_logging_time': 1.2307319641113281, 'global_step': 30587, 'preemption_count': 0}), (31331, {'train/accuracy': 0.9915563464164734, 'train/loss': 0.027224678546190262, 'train/mean_average_precision': 0.4779938282806382, 'validation/accuracy': 0.9867269396781921, 'validation/loss': 0.04497452452778816, 'validation/mean_average_precision': 0.2681475382524397, 'validation/num_examples': 43793, 'test/accuracy': 0.9859392046928406, 'test/loss': 0.047643113881349564, 'test/mean_average_precision': 0.2611744513547663, 'test/num_examples': 43793, 'score': 10095.512593507767, 'total_duration': 14847.700211763382, 'accumulated_submission_time': 10095.512593507767, 'accumulated_eval_time': 4750.071592330933, 'accumulated_logging_time': 1.2652955055236816, 'global_step': 31331, 'preemption_count': 0}), (32084, {'train/accuracy': 0.9915878772735596, 'train/loss': 0.02693975530564785, 'train/mean_average_precision': 0.4846764472143901, 'validation/accuracy': 0.9865304231643677, 'validation/loss': 0.04544617608189583, 'validation/mean_average_precision': 0.26817698552047364, 'validation/num_examples': 43793, 'test/accuracy': 0.9857879877090454, 'test/loss': 0.04806245118379593, 'test/mean_average_precision': 0.2527473212298942, 'test/num_examples': 43793, 'score': 10335.648624420166, 'total_duration': 15194.996549367905, 'accumulated_submission_time': 10335.648624420166, 'accumulated_eval_time': 4857.177111625671, 'accumulated_logging_time': 1.3001899719238281, 'global_step': 32084, 'preemption_count': 0}), (32842, {'train/accuracy': 0.9919490218162537, 'train/loss': 0.025746557861566544, 'train/mean_average_precision': 0.5127910015596717, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.04498951882123947, 'validation/mean_average_precision': 0.2654056363679506, 'validation/num_examples': 43793, 'test/accuracy': 0.9858031868934631, 'test/loss': 0.04785405099391937, 'test/mean_average_precision': 0.2626084049281989, 'test/num_examples': 43793, 'score': 10575.60150718689, 'total_duration': 15540.194693565369, 'accumulated_submission_time': 10575.60150718689, 'accumulated_eval_time': 4962.369988441467, 'accumulated_logging_time': 1.332615613937378, 'global_step': 32842, 'preemption_count': 0}), (33592, {'train/accuracy': 0.9920307397842407, 'train/loss': 0.02541346289217472, 'train/mean_average_precision': 0.5067656067708026, 'validation/accuracy': 0.9868279695510864, 'validation/loss': 0.045044295489788055, 'validation/mean_average_precision': 0.26585007839346014, 'validation/num_examples': 43793, 'test/accuracy': 0.9859409332275391, 'test/loss': 0.047983698546886444, 'test/mean_average_precision': 0.25743621166626357, 'test/num_examples': 43793, 'score': 10815.790082454681, 'total_duration': 15885.7362678051, 'accumulated_submission_time': 10815.790082454681, 'accumulated_eval_time': 5067.671158790588, 'accumulated_logging_time': 1.3651981353759766, 'global_step': 33592, 'preemption_count': 0}), (34348, {'train/accuracy': 0.9923913478851318, 'train/loss': 0.024423692375421524, 'train/mean_average_precision': 0.5342409464257092, 'validation/accuracy': 0.9867200255393982, 'validation/loss': 0.04531063511967659, 'validation/mean_average_precision': 0.2671155695587615, 'validation/num_examples': 43793, 'test/accuracy': 0.985846996307373, 'test/loss': 0.04810245707631111, 'test/mean_average_precision': 0.25537366261511146, 'test/num_examples': 43793, 'score': 11056.026224136353, 'total_duration': 16232.710386753082, 'accumulated_submission_time': 11056.026224136353, 'accumulated_eval_time': 5174.355105876923, 'accumulated_logging_time': 1.3994545936584473, 'global_step': 34348, 'preemption_count': 0}), (35099, {'train/accuracy': 0.9922991991043091, 'train/loss': 0.024864686653017998, 'train/mean_average_precision': 0.5406791037882595, 'validation/accuracy': 0.9865483045578003, 'validation/loss': 0.045412566512823105, 'validation/mean_average_precision': 0.27699245620698615, 'validation/num_examples': 43793, 'test/accuracy': 0.9857711791992188, 'test/loss': 0.04812723025679588, 'test/mean_average_precision': 0.2624014605046485, 'test/num_examples': 43793, 'score': 11296.04472565651, 'total_duration': 16582.92563867569, 'accumulated_submission_time': 11296.04472565651, 'accumulated_eval_time': 5284.490906953812, 'accumulated_logging_time': 1.4411022663116455, 'global_step': 35099, 'preemption_count': 0}), (35851, {'train/accuracy': 0.9920918941497803, 'train/loss': 0.025452451780438423, 'train/mean_average_precision': 0.5169067383191193, 'validation/accuracy': 0.9867861866950989, 'validation/loss': 0.04577966406941414, 'validation/mean_average_precision': 0.27363462918792686, 'validation/num_examples': 43793, 'test/accuracy': 0.985917329788208, 'test/loss': 0.04885753244161606, 'test/mean_average_precision': 0.258611242636092, 'test/num_examples': 43793, 'score': 11536.114127397537, 'total_duration': 16931.671364068985, 'accumulated_submission_time': 11536.114127397537, 'accumulated_eval_time': 5393.113869190216, 'accumulated_logging_time': 1.4744434356689453, 'global_step': 35851, 'preemption_count': 0}), (36606, {'train/accuracy': 0.9918672442436218, 'train/loss': 0.02592872828245163, 'train/mean_average_precision': 0.49946526637936217, 'validation/accuracy': 0.9868308305740356, 'validation/loss': 0.04560372978448868, 'validation/mean_average_precision': 0.27065348194802213, 'validation/num_examples': 43793, 'test/accuracy': 0.9859451055526733, 'test/loss': 0.04860323294997215, 'test/mean_average_precision': 0.2502696397802891, 'test/num_examples': 43793, 'score': 11776.257864952087, 'total_duration': 17281.49413871765, 'accumulated_submission_time': 11776.257864952087, 'accumulated_eval_time': 5502.7397446632385, 'accumulated_logging_time': 1.5083093643188477, 'global_step': 36606, 'preemption_count': 0}), (37345, {'train/accuracy': 0.9918071031570435, 'train/loss': 0.026228440925478935, 'train/mean_average_precision': 0.4914974538833261, 'validation/accuracy': 0.986737072467804, 'validation/loss': 0.04582122340798378, 'validation/mean_average_precision': 0.269111434883714, 'validation/num_examples': 43793, 'test/accuracy': 0.9858027696609497, 'test/loss': 0.04891791194677353, 'test/mean_average_precision': 0.25877115480870727, 'test/num_examples': 43793, 'score': 12016.488486766815, 'total_duration': 17630.008954048157, 'accumulated_submission_time': 12016.488486766815, 'accumulated_eval_time': 5610.971168756485, 'accumulated_logging_time': 1.541703224182129, 'global_step': 37345, 'preemption_count': 0}), (38090, {'train/accuracy': 0.991980254650116, 'train/loss': 0.025586925446987152, 'train/mean_average_precision': 0.5115828856662827, 'validation/accuracy': 0.9867179989814758, 'validation/loss': 0.045958761125802994, 'validation/mean_average_precision': 0.27139956490941525, 'validation/num_examples': 43793, 'test/accuracy': 0.9859261512756348, 'test/loss': 0.04866719990968704, 'test/mean_average_precision': 0.2615826043412462, 'test/num_examples': 43793, 'score': 12256.664202690125, 'total_duration': 17973.409933567047, 'accumulated_submission_time': 12256.664202690125, 'accumulated_eval_time': 5714.142570018768, 'accumulated_logging_time': 1.5757763385772705, 'global_step': 38090, 'preemption_count': 0}), (38833, {'train/accuracy': 0.9920877814292908, 'train/loss': 0.025294775143265724, 'train/mean_average_precision': 0.521443788126027, 'validation/accuracy': 0.986579179763794, 'validation/loss': 0.04616735875606537, 'validation/mean_average_precision': 0.2693436348083421, 'validation/num_examples': 43793, 'test/accuracy': 0.9857484102249146, 'test/loss': 0.04892940819263458, 'test/mean_average_precision': 0.2565882521212089, 'test/num_examples': 43793, 'score': 12496.846721887589, 'total_duration': 18318.596489667892, 'accumulated_submission_time': 12496.846721887589, 'accumulated_eval_time': 5819.092994213104, 'accumulated_logging_time': 1.608827829360962, 'global_step': 38833, 'preemption_count': 0}), (39566, {'train/accuracy': 0.9922082424163818, 'train/loss': 0.024799220263957977, 'train/mean_average_precision': 0.5314044332998933, 'validation/accuracy': 0.9865458607673645, 'validation/loss': 0.04632498696446419, 'validation/mean_average_precision': 0.26710251393412404, 'validation/num_examples': 43793, 'test/accuracy': 0.985687792301178, 'test/loss': 0.04922351986169815, 'test/mean_average_precision': 0.25741389805365, 'test/num_examples': 43793, 'score': 12737.104538679123, 'total_duration': 18671.7216861248, 'accumulated_submission_time': 12737.104538679123, 'accumulated_eval_time': 5931.902855873108, 'accumulated_logging_time': 1.6432619094848633, 'global_step': 39566, 'preemption_count': 0}), (40305, {'train/accuracy': 0.9925952553749084, 'train/loss': 0.023490697145462036, 'train/mean_average_precision': 0.5565252775312892, 'validation/accuracy': 0.986531674861908, 'validation/loss': 0.0463293083012104, 'validation/mean_average_precision': 0.26762152615204715, 'validation/num_examples': 43793, 'test/accuracy': 0.9857577085494995, 'test/loss': 0.049448490142822266, 'test/mean_average_precision': 0.2498049455988531, 'test/num_examples': 43793, 'score': 12977.295271873474, 'total_duration': 19022.058073043823, 'accumulated_submission_time': 12977.295271873474, 'accumulated_eval_time': 6041.992013454437, 'accumulated_logging_time': 1.677354335784912, 'global_step': 40305, 'preemption_count': 0}), (41054, {'train/accuracy': 0.9928678274154663, 'train/loss': 0.022487075999379158, 'train/mean_average_precision': 0.581720308999962, 'validation/accuracy': 0.9867366552352905, 'validation/loss': 0.04618602618575096, 'validation/mean_average_precision': 0.2734461547168441, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.04916103556752205, 'test/mean_average_precision': 0.26506168285239545, 'test/num_examples': 43793, 'score': 13217.297476530075, 'total_duration': 19367.962020874023, 'accumulated_submission_time': 13217.297476530075, 'accumulated_eval_time': 6147.838949918747, 'accumulated_logging_time': 1.7124638557434082, 'global_step': 41054, 'preemption_count': 0}), (41810, {'train/accuracy': 0.9927249550819397, 'train/loss': 0.022989705204963684, 'train/mean_average_precision': 0.5715105094814787, 'validation/accuracy': 0.9866583347320557, 'validation/loss': 0.04679886996746063, 'validation/mean_average_precision': 0.2642473790358029, 'validation/num_examples': 43793, 'test/accuracy': 0.9857454895973206, 'test/loss': 0.049967210739851, 'test/mean_average_precision': 0.2580405016937968, 'test/num_examples': 43793, 'score': 13457.432827711105, 'total_duration': 19713.732904434204, 'accumulated_submission_time': 13457.432827711105, 'accumulated_eval_time': 6253.418445587158, 'accumulated_logging_time': 1.7486231327056885, 'global_step': 41810, 'preemption_count': 0}), (42556, {'train/accuracy': 0.992652952671051, 'train/loss': 0.023321157321333885, 'train/mean_average_precision': 0.5512938937985699, 'validation/accuracy': 0.9865283966064453, 'validation/loss': 0.04672816023230553, 'validation/mean_average_precision': 0.2636467984267341, 'validation/num_examples': 43793, 'test/accuracy': 0.9856923818588257, 'test/loss': 0.049496106803417206, 'test/mean_average_precision': 0.2557325618023192, 'test/num_examples': 43793, 'score': 13697.439344882965, 'total_duration': 20066.57444548607, 'accumulated_submission_time': 13697.439344882965, 'accumulated_eval_time': 6366.196691989899, 'accumulated_logging_time': 1.7837131023406982, 'global_step': 42556, 'preemption_count': 0}), (43306, {'train/accuracy': 0.9925017356872559, 'train/loss': 0.023778589442372322, 'train/mean_average_precision': 0.5646825159913036, 'validation/accuracy': 0.9867951273918152, 'validation/loss': 0.0462379977107048, 'validation/mean_average_precision': 0.2821592979825692, 'validation/num_examples': 43793, 'test/accuracy': 0.985849916934967, 'test/loss': 0.04968702793121338, 'test/mean_average_precision': 0.26054839880737374, 'test/num_examples': 43793, 'score': 13937.537743330002, 'total_duration': 20413.091861009598, 'accumulated_submission_time': 13937.537743330002, 'accumulated_eval_time': 6472.557656049728, 'accumulated_logging_time': 1.8214752674102783, 'global_step': 43306, 'preemption_count': 0}), (44051, {'train/accuracy': 0.9924307465553284, 'train/loss': 0.023861369118094444, 'train/mean_average_precision': 0.555371738352912, 'validation/accuracy': 0.9867119193077087, 'validation/loss': 0.04687708243727684, 'validation/mean_average_precision': 0.27326731123889797, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.05005699396133423, 'test/mean_average_precision': 0.25659462157829027, 'test/num_examples': 43793, 'score': 14177.752856016159, 'total_duration': 20761.1159992218, 'accumulated_submission_time': 14177.752856016159, 'accumulated_eval_time': 6580.312434911728, 'accumulated_logging_time': 1.8559012413024902, 'global_step': 44051, 'preemption_count': 0}), (44798, {'train/accuracy': 0.992325484752655, 'train/loss': 0.02401222288608551, 'train/mean_average_precision': 0.5586604499035724, 'validation/accuracy': 0.9867033958435059, 'validation/loss': 0.047204095870256424, 'validation/mean_average_precision': 0.26694024763712443, 'validation/num_examples': 43793, 'test/accuracy': 0.9858166575431824, 'test/loss': 0.05042057856917381, 'test/mean_average_precision': 0.2533213945513642, 'test/num_examples': 43793, 'score': 14417.734598636627, 'total_duration': 21108.370057582855, 'accumulated_submission_time': 14417.734598636627, 'accumulated_eval_time': 6687.529041051865, 'accumulated_logging_time': 1.8914201259613037, 'global_step': 44798, 'preemption_count': 0}), (45551, {'train/accuracy': 0.9925184845924377, 'train/loss': 0.023408500477671623, 'train/mean_average_precision': 0.5609331101563908, 'validation/accuracy': 0.9865624904632568, 'validation/loss': 0.04778437688946724, 'validation/mean_average_precision': 0.26957855998027896, 'validation/num_examples': 43793, 'test/accuracy': 0.9856654405593872, 'test/loss': 0.0509008951485157, 'test/mean_average_precision': 0.25818216762065693, 'test/num_examples': 43793, 'score': 14657.845716714859, 'total_duration': 21455.67155122757, 'accumulated_submission_time': 14657.845716714859, 'accumulated_eval_time': 6794.665002822876, 'accumulated_logging_time': 1.9260263442993164, 'global_step': 45551, 'preemption_count': 0}), (46308, {'train/accuracy': 0.9927812218666077, 'train/loss': 0.02257843129336834, 'train/mean_average_precision': 0.5735938940547376, 'validation/accuracy': 0.986531674861908, 'validation/loss': 0.047529514878988266, 'validation/mean_average_precision': 0.27112534172351554, 'validation/num_examples': 43793, 'test/accuracy': 0.9856970310211182, 'test/loss': 0.050856564193964005, 'test/mean_average_precision': 0.2524435396842576, 'test/num_examples': 43793, 'score': 14897.96158337593, 'total_duration': 21801.372572660446, 'accumulated_submission_time': 14897.96158337593, 'accumulated_eval_time': 6900.194499254227, 'accumulated_logging_time': 1.9617371559143066, 'global_step': 46308, 'preemption_count': 0}), (47046, {'train/accuracy': 0.9932058453559875, 'train/loss': 0.021325523033738136, 'train/mean_average_precision': 0.6142257470793187, 'validation/accuracy': 0.9863327741622925, 'validation/loss': 0.04775543883442879, 'validation/mean_average_precision': 0.2685979413267775, 'validation/num_examples': 43793, 'test/accuracy': 0.9854986667633057, 'test/loss': 0.051068343222141266, 'test/mean_average_precision': 0.2488496460511818, 'test/num_examples': 43793, 'score': 15138.184672355652, 'total_duration': 22149.626344442368, 'accumulated_submission_time': 15138.184672355652, 'accumulated_eval_time': 7008.161518335342, 'accumulated_logging_time': 2.0013952255249023, 'global_step': 47046, 'preemption_count': 0}), (47797, {'train/accuracy': 0.9933127760887146, 'train/loss': 0.020854709669947624, 'train/mean_average_precision': 0.6183224958318798, 'validation/accuracy': 0.9866700768470764, 'validation/loss': 0.04813152551651001, 'validation/mean_average_precision': 0.27352400561145834, 'validation/num_examples': 43793, 'test/accuracy': 0.9856852293014526, 'test/loss': 0.051686979830265045, 'test/mean_average_precision': 0.25225808267055216, 'test/num_examples': 43793, 'score': 15378.320225954056, 'total_duration': 22497.388407230377, 'accumulated_submission_time': 15378.320225954056, 'accumulated_eval_time': 7115.730270385742, 'accumulated_logging_time': 2.0384480953216553, 'global_step': 47797, 'preemption_count': 0}), (48545, {'train/accuracy': 0.9937769770622253, 'train/loss': 0.01960907131433487, 'train/mean_average_precision': 0.657254856675352, 'validation/accuracy': 0.9864935278892517, 'validation/loss': 0.04848235100507736, 'validation/mean_average_precision': 0.26432682344356795, 'validation/num_examples': 43793, 'test/accuracy': 0.9857058525085449, 'test/loss': 0.05173896253108978, 'test/mean_average_precision': 0.2559660860530463, 'test/num_examples': 43793, 'score': 15618.439760684967, 'total_duration': 22843.91304540634, 'accumulated_submission_time': 15618.439760684967, 'accumulated_eval_time': 7222.079265594482, 'accumulated_logging_time': 2.07443904876709, 'global_step': 48545, 'preemption_count': 0}), (49290, {'train/accuracy': 0.9935559630393982, 'train/loss': 0.02026131935417652, 'train/mean_average_precision': 0.6167782802913997, 'validation/accuracy': 0.9866542816162109, 'validation/loss': 0.04876922816038132, 'validation/mean_average_precision': 0.2706925333525783, 'validation/num_examples': 43793, 'test/accuracy': 0.9856612086296082, 'test/loss': 0.052092064172029495, 'test/mean_average_precision': 0.2503146224776895, 'test/num_examples': 43793, 'score': 15858.267110586166, 'total_duration': 23190.93936944008, 'accumulated_submission_time': 15858.267110586166, 'accumulated_eval_time': 7328.919096469879, 'accumulated_logging_time': 2.4131574630737305, 'global_step': 49290, 'preemption_count': 0}), (50032, {'train/accuracy': 0.9932066798210144, 'train/loss': 0.021182114258408546, 'train/mean_average_precision': 0.6060510859266648, 'validation/accuracy': 0.9864216446876526, 'validation/loss': 0.0490487702190876, 'validation/mean_average_precision': 0.2732556869305221, 'validation/num_examples': 43793, 'test/accuracy': 0.9856587052345276, 'test/loss': 0.0523470975458622, 'test/mean_average_precision': 0.2541978403734496, 'test/num_examples': 43793, 'score': 16098.356865167618, 'total_duration': 23540.37098479271, 'accumulated_submission_time': 16098.356865167618, 'accumulated_eval_time': 7438.20524597168, 'accumulated_logging_time': 2.4488024711608887, 'global_step': 50032, 'preemption_count': 0}), (50782, {'train/accuracy': 0.9932138323783875, 'train/loss': 0.02112257480621338, 'train/mean_average_precision': 0.6076637757132363, 'validation/accuracy': 0.9864943027496338, 'validation/loss': 0.04956536367535591, 'validation/mean_average_precision': 0.2643919542063089, 'validation/num_examples': 43793, 'test/accuracy': 0.9856283664703369, 'test/loss': 0.052835650742053986, 'test/mean_average_precision': 0.25240235104463826, 'test/num_examples': 43793, 'score': 16338.490710258484, 'total_duration': 23888.57576751709, 'accumulated_submission_time': 16338.490710258484, 'accumulated_eval_time': 7546.218659877777, 'accumulated_logging_time': 2.485704183578491, 'global_step': 50782, 'preemption_count': 0}), (51533, {'train/accuracy': 0.9932173490524292, 'train/loss': 0.021080883219838142, 'train/mean_average_precision': 0.6193763045917757, 'validation/accuracy': 0.986382246017456, 'validation/loss': 0.049848511815071106, 'validation/mean_average_precision': 0.2682000829620698, 'validation/num_examples': 43793, 'test/accuracy': 0.9855205416679382, 'test/loss': 0.05324115231633186, 'test/mean_average_precision': 0.25270730641985173, 'test/num_examples': 43793, 'score': 16578.616221904755, 'total_duration': 24237.691730499268, 'accumulated_submission_time': 16578.616221904755, 'accumulated_eval_time': 7655.152938842773, 'accumulated_logging_time': 2.5216963291168213, 'global_step': 51533, 'preemption_count': 0}), (52279, {'train/accuracy': 0.9932751059532166, 'train/loss': 0.020680667832493782, 'train/mean_average_precision': 0.6200296858576568, 'validation/accuracy': 0.9865409731864929, 'validation/loss': 0.04983312636613846, 'validation/mean_average_precision': 0.2641053459337334, 'validation/num_examples': 43793, 'test/accuracy': 0.9856444001197815, 'test/loss': 0.05344433709979057, 'test/mean_average_precision': 0.2497437597936006, 'test/num_examples': 43793, 'score': 16818.712033748627, 'total_duration': 24579.483705997467, 'accumulated_submission_time': 16818.712033748627, 'accumulated_eval_time': 7756.792923927307, 'accumulated_logging_time': 2.5575668811798096, 'global_step': 52279, 'preemption_count': 0}), (53026, {'train/accuracy': 0.9934790134429932, 'train/loss': 0.019983120262622833, 'train/mean_average_precision': 0.6333494312558867, 'validation/accuracy': 0.9864338040351868, 'validation/loss': 0.05018379166722298, 'validation/mean_average_precision': 0.26808323583810556, 'validation/num_examples': 43793, 'test/accuracy': 0.9855024218559265, 'test/loss': 0.05382246896624565, 'test/mean_average_precision': 0.24797876301901983, 'test/num_examples': 43793, 'score': 17058.962177991867, 'total_duration': 24927.917595624924, 'accumulated_submission_time': 17058.962177991867, 'accumulated_eval_time': 7864.91987657547, 'accumulated_logging_time': 2.594414710998535, 'global_step': 53026, 'preemption_count': 0}), (53772, {'train/accuracy': 0.9938586950302124, 'train/loss': 0.01891549676656723, 'train/mean_average_precision': 0.6609112039038493, 'validation/accuracy': 0.9863700866699219, 'validation/loss': 0.05027468875050545, 'validation/mean_average_precision': 0.2667339680899637, 'validation/num_examples': 43793, 'test/accuracy': 0.9854165315628052, 'test/loss': 0.054085321724414825, 'test/mean_average_precision': 0.2510609877039152, 'test/num_examples': 43793, 'score': 17298.90731549263, 'total_duration': 25274.465651512146, 'accumulated_submission_time': 17298.90731549263, 'accumulated_eval_time': 7971.4639711380005, 'accumulated_logging_time': 2.6330935955047607, 'global_step': 53772, 'preemption_count': 0}), (54520, {'train/accuracy': 0.9943572282791138, 'train/loss': 0.017438994720578194, 'train/mean_average_precision': 0.6922163307395394, 'validation/accuracy': 0.9864155650138855, 'validation/loss': 0.05081336945295334, 'validation/mean_average_precision': 0.26253064975485707, 'validation/num_examples': 43793, 'test/accuracy': 0.9854194521903992, 'test/loss': 0.05472831428050995, 'test/mean_average_precision': 0.24891227325226248, 'test/num_examples': 43793, 'score': 17538.941791057587, 'total_duration': 25618.02638888359, 'accumulated_submission_time': 17538.941791057587, 'accumulated_eval_time': 8074.933919668198, 'accumulated_logging_time': 2.669600009918213, 'global_step': 54520, 'preemption_count': 0}), (55267, {'train/accuracy': 0.9947988390922546, 'train/loss': 0.016236860305070877, 'train/mean_average_precision': 0.7333053532778322, 'validation/accuracy': 0.9862548112869263, 'validation/loss': 0.051313333213329315, 'validation/mean_average_precision': 0.26274625230889226, 'validation/num_examples': 43793, 'test/accuracy': 0.9853533506393433, 'test/loss': 0.0552709624171257, 'test/mean_average_precision': 0.24841969844210535, 'test/num_examples': 43793, 'score': 17779.046933174133, 'total_duration': 25964.97895693779, 'accumulated_submission_time': 17779.046933174133, 'accumulated_eval_time': 8181.723499298096, 'accumulated_logging_time': 2.7075655460357666, 'global_step': 55267, 'preemption_count': 0}), (56017, {'train/accuracy': 0.9949828386306763, 'train/loss': 0.015798229724168777, 'train/mean_average_precision': 0.7273481342521675, 'validation/accuracy': 0.9862483143806458, 'validation/loss': 0.05185191333293915, 'validation/mean_average_precision': 0.26084115749890824, 'validation/num_examples': 43793, 'test/accuracy': 0.9853945970535278, 'test/loss': 0.05584893375635147, 'test/mean_average_precision': 0.25320181507136497, 'test/num_examples': 43793, 'score': 18019.023313760757, 'total_duration': 26313.56055831909, 'accumulated_submission_time': 18019.023313760757, 'accumulated_eval_time': 8290.270651817322, 'accumulated_logging_time': 2.7457399368286133, 'global_step': 56017, 'preemption_count': 0}), (56766, {'train/accuracy': 0.9939604997634888, 'train/loss': 0.018462734296917915, 'train/mean_average_precision': 0.6661507763472906, 'validation/accuracy': 0.9862211346626282, 'validation/loss': 0.05208123102784157, 'validation/mean_average_precision': 0.26548057966644023, 'validation/num_examples': 43793, 'test/accuracy': 0.9853697419166565, 'test/loss': 0.05587721988558769, 'test/mean_average_precision': 0.2519444893929446, 'test/num_examples': 43793, 'score': 18259.248502254486, 'total_duration': 26663.653777122498, 'accumulated_submission_time': 18259.248502254486, 'accumulated_eval_time': 8400.08038520813, 'accumulated_logging_time': 2.7842960357666016, 'global_step': 56766, 'preemption_count': 0})], 'global_step': 57446}
I0205 20:10:25.227126 140205209478976 submission_runner.py:586] Timing: 18477.078250169754
I0205 20:10:25.227181 140205209478976 submission_runner.py:588] Total number of evals: 77
I0205 20:10:25.227224 140205209478976 submission_runner.py:589] ====================
I0205 20:10:25.227270 140205209478976 submission_runner.py:542] Using RNG seed 1480595982
I0205 20:10:25.291971 140205209478976 submission_runner.py:551] --- Tuning run 4/5 ---
I0205 20:10:25.292137 140205209478976 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_4.
I0205 20:10:25.294967 140205209478976 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_4/hparams.json.
I0205 20:10:25.428666 140205209478976 submission_runner.py:206] Initializing dataset.
I0205 20:10:25.517680 140205209478976 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 20:10:25.523429 140205209478976 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 20:10:25.660024 140205209478976 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 20:10:25.699268 140205209478976 submission_runner.py:213] Initializing model.
I0205 20:10:28.168909 140205209478976 submission_runner.py:255] Initializing optimizer.
I0205 20:10:28.753178 140205209478976 submission_runner.py:262] Initializing metrics bundle.
I0205 20:10:28.753371 140205209478976 submission_runner.py:280] Initializing checkpoint and logger.
I0205 20:10:28.754075 140205209478976 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_4 with prefix checkpoint_
I0205 20:10:28.754208 140205209478976 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_4/meta_data_0.json.
I0205 20:10:28.754416 140205209478976 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 20:10:28.754477 140205209478976 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 20:10:30.528592 140205209478976 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 20:10:32.272525 140205209478976 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_4/flags_0.json.
I0205 20:10:32.276870 140205209478976 submission_runner.py:314] Starting training loop.
I0205 20:10:44.450164 139983998789376 logging_writer.py:48] [0] global_step=0, grad_norm=2.357750415802002, loss=0.7395575046539307
I0205 20:10:44.462553 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:12:25.600238 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:12:29.020622 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:12:32.263760 140205209478976 submission_runner.py:408] Time since start: 119.99s, 	Step: 1, 	{'train/accuracy': 0.5091689229011536, 'train/loss': 0.7391471266746521, 'train/mean_average_precision': 0.022770563417320786, 'validation/accuracy': 0.5064647793769836, 'validation/loss': 0.7422076463699341, 'validation/mean_average_precision': 0.025298830402132374, 'validation/num_examples': 43793, 'test/accuracy': 0.5047287344932556, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.02693114158079488, 'test/num_examples': 43793, 'score': 12.185586929321289, 'total_duration': 119.98681426048279, 'accumulated_submission_time': 12.185586929321289, 'accumulated_eval_time': 107.80116128921509, 'accumulated_logging_time': 0}
I0205 20:12:32.274770 139984037992192 logging_writer.py:48] [1] accumulated_eval_time=107.801161, accumulated_logging_time=0, accumulated_submission_time=12.185587, global_step=1, preemption_count=0, score=12.185587, test/accuracy=0.504729, test/loss=0.743874, test/mean_average_precision=0.026931, test/num_examples=43793, total_duration=119.986814, train/accuracy=0.509169, train/loss=0.739147, train/mean_average_precision=0.022771, validation/accuracy=0.506465, validation/loss=0.742208, validation/mean_average_precision=0.025299, validation/num_examples=43793
I0205 20:13:04.849214 140021163808512 logging_writer.py:48] [100] global_step=100, grad_norm=0.11747941374778748, loss=0.1205483078956604
I0205 20:13:37.208879 139984037992192 logging_writer.py:48] [200] global_step=200, grad_norm=0.009640585631132126, loss=0.0510479100048542
I0205 20:14:09.798008 140021163808512 logging_writer.py:48] [300] global_step=300, grad_norm=0.03824050724506378, loss=0.058184608817100525
I0205 20:14:42.292352 139984037992192 logging_writer.py:48] [400] global_step=400, grad_norm=0.019507169723510742, loss=0.05381998419761658
I0205 20:15:14.652757 140021163808512 logging_writer.py:48] [500] global_step=500, grad_norm=0.01561992708593607, loss=0.05702965334057808
I0205 20:15:46.488511 139984037992192 logging_writer.py:48] [600] global_step=600, grad_norm=0.010656175203621387, loss=0.05191453918814659
I0205 20:16:18.214722 140021163808512 logging_writer.py:48] [700] global_step=700, grad_norm=0.01350459922105074, loss=0.05250623822212219
I0205 20:16:32.281278 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:18:13.857378 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:18:16.905849 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:18:19.892748 140205209478976 submission_runner.py:408] Time since start: 467.62s, 	Step: 745, 	{'train/accuracy': 0.9868181943893433, 'train/loss': 0.052752770483493805, 'train/mean_average_precision': 0.04831717073437651, 'validation/accuracy': 0.9841492176055908, 'validation/loss': 0.06301576644182205, 'validation/mean_average_precision': 0.04771911249913853, 'validation/num_examples': 43793, 'test/accuracy': 0.9831492304801941, 'test/loss': 0.06620319187641144, 'test/mean_average_precision': 0.0460700617813824, 'test/num_examples': 43793, 'score': 252.15884280204773, 'total_duration': 467.61569714546204, 'accumulated_submission_time': 252.15884280204773, 'accumulated_eval_time': 215.41246700286865, 'accumulated_logging_time': 0.024070024490356445}
I0205 20:18:19.907611 140002598295296 logging_writer.py:48] [745] accumulated_eval_time=215.412467, accumulated_logging_time=0.024070, accumulated_submission_time=252.158843, global_step=745, preemption_count=0, score=252.158843, test/accuracy=0.983149, test/loss=0.066203, test/mean_average_precision=0.046070, test/num_examples=43793, total_duration=467.615697, train/accuracy=0.986818, train/loss=0.052753, train/mean_average_precision=0.048317, validation/accuracy=0.984149, validation/loss=0.063016, validation/mean_average_precision=0.047719, validation/num_examples=43793
I0205 20:18:38.018442 140020770670336 logging_writer.py:48] [800] global_step=800, grad_norm=0.009882028214633465, loss=0.050166185945272446
I0205 20:19:10.835171 140002598295296 logging_writer.py:48] [900] global_step=900, grad_norm=0.012532622553408146, loss=0.0502597838640213
I0205 20:19:42.836374 140020770670336 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.010208196006715298, loss=0.05287357419729233
I0205 20:20:15.037302 140002598295296 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.009962816722691059, loss=0.05068330466747284
I0205 20:20:47.132579 140020770670336 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.007068973034620285, loss=0.04801720008254051
I0205 20:21:19.129959 140002598295296 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.009565146639943123, loss=0.04669918492436409
I0205 20:21:51.400584 140020770670336 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.01080055721104145, loss=0.0483054555952549
I0205 20:22:19.933039 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:23:59.920540 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:24:03.155025 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:24:06.189635 140205209478976 submission_runner.py:408] Time since start: 813.91s, 	Step: 1490, 	{'train/accuracy': 0.9869986772537231, 'train/loss': 0.04919382929801941, 'train/mean_average_precision': 0.07801795809218251, 'validation/accuracy': 0.9842535257339478, 'validation/loss': 0.05856533348560333, 'validation/mean_average_precision': 0.07444615138947651, 'validation/num_examples': 43793, 'test/accuracy': 0.9832949638366699, 'test/loss': 0.061599891632795334, 'test/mean_average_precision': 0.0745604968623183, 'test/num_examples': 43793, 'score': 492.1526997089386, 'total_duration': 813.9127051830292, 'accumulated_submission_time': 492.1526997089386, 'accumulated_eval_time': 321.669025182724, 'accumulated_logging_time': 0.05089735984802246}
I0205 20:24:06.204839 140002205157120 logging_writer.py:48] [1490] accumulated_eval_time=321.669025, accumulated_logging_time=0.050897, accumulated_submission_time=492.152700, global_step=1490, preemption_count=0, score=492.152700, test/accuracy=0.983295, test/loss=0.061600, test/mean_average_precision=0.074560, test/num_examples=43793, total_duration=813.912705, train/accuracy=0.986999, train/loss=0.049194, train/mean_average_precision=0.078018, validation/accuracy=0.984254, validation/loss=0.058565, validation/mean_average_precision=0.074446, validation/num_examples=43793
I0205 20:24:09.697328 140021163808512 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.016901027411222458, loss=0.05146395415067673
I0205 20:24:41.591626 140002205157120 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.009415901266038418, loss=0.04366301745176315
I0205 20:25:13.792901 140021163808512 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.006286883261054754, loss=0.050853706896305084
I0205 20:25:45.866473 140002205157120 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0076376209035515785, loss=0.05619155988097191
I0205 20:26:17.726253 140021163808512 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.005671478807926178, loss=0.05108298361301422
I0205 20:26:49.397882 140002205157120 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.02194143272936344, loss=0.05431997776031494
I0205 20:27:21.769066 140021163808512 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.011747443117201328, loss=0.051753684878349304
I0205 20:27:53.584090 140002205157120 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.014752629213035107, loss=0.04054926708340645
I0205 20:28:06.228726 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:29:47.613196 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:29:50.665796 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:29:53.626318 140205209478976 submission_runner.py:408] Time since start: 1161.35s, 	Step: 2240, 	{'train/accuracy': 0.9874287843704224, 'train/loss': 0.04568753018975258, 'train/mean_average_precision': 0.11178136272470766, 'validation/accuracy': 0.9846611022949219, 'validation/loss': 0.05566570162773132, 'validation/mean_average_precision': 0.1072617279992498, 'validation/num_examples': 43793, 'test/accuracy': 0.9836757183074951, 'test/loss': 0.059133052825927734, 'test/mean_average_precision': 0.10345936149153655, 'test/num_examples': 43793, 'score': 732.1452407836914, 'total_duration': 1161.3493824005127, 'accumulated_submission_time': 732.1452407836914, 'accumulated_eval_time': 429.0665693283081, 'accumulated_logging_time': 0.07750868797302246}
I0205 20:29:53.642716 140002598295296 logging_writer.py:48] [2240] accumulated_eval_time=429.066569, accumulated_logging_time=0.077509, accumulated_submission_time=732.145241, global_step=2240, preemption_count=0, score=732.145241, test/accuracy=0.983676, test/loss=0.059133, test/mean_average_precision=0.103459, test/num_examples=43793, total_duration=1161.349382, train/accuracy=0.987429, train/loss=0.045688, train/mean_average_precision=0.111781, validation/accuracy=0.984661, validation/loss=0.055666, validation/mean_average_precision=0.107262, validation/num_examples=43793
I0205 20:30:13.110829 140020770670336 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.016485031694173813, loss=0.04013601318001747
I0205 20:30:45.372761 140002598295296 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.012503400444984436, loss=0.043962713330984116
I0205 20:31:17.529799 140020770670336 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.007355565205216408, loss=0.045220620930194855
I0205 20:31:49.244330 140002598295296 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.013074837625026703, loss=0.04436580091714859
I0205 20:32:21.141911 140020770670336 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.020992804318666458, loss=0.03946796432137489
I0205 20:32:53.014872 140002598295296 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.024946412071585655, loss=0.042453132569789886
I0205 20:33:24.930048 140020770670336 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.011828899383544922, loss=0.042234912514686584
I0205 20:33:53.831519 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:35:30.743640 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:35:33.795492 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:35:36.738204 140205209478976 submission_runner.py:408] Time since start: 1504.46s, 	Step: 2992, 	{'train/accuracy': 0.9878295660018921, 'train/loss': 0.04290998727083206, 'train/mean_average_precision': 0.14909766748097233, 'validation/accuracy': 0.9849030375480652, 'validation/loss': 0.053014665842056274, 'validation/mean_average_precision': 0.12961651864892743, 'validation/num_examples': 43793, 'test/accuracy': 0.9839111566543579, 'test/loss': 0.055954184383153915, 'test/mean_average_precision': 0.13367850725457064, 'test/num_examples': 43793, 'score': 972.3031947612762, 'total_duration': 1504.4612703323364, 'accumulated_submission_time': 972.3031947612762, 'accumulated_eval_time': 531.9732112884521, 'accumulated_logging_time': 0.10485291481018066}
I0205 20:35:36.754148 139984133072640 logging_writer.py:48] [2992] accumulated_eval_time=531.973211, accumulated_logging_time=0.104853, accumulated_submission_time=972.303195, global_step=2992, preemption_count=0, score=972.303195, test/accuracy=0.983911, test/loss=0.055954, test/mean_average_precision=0.133679, test/num_examples=43793, total_duration=1504.461270, train/accuracy=0.987830, train/loss=0.042910, train/mean_average_precision=0.149098, validation/accuracy=0.984903, validation/loss=0.053015, validation/mean_average_precision=0.129617, validation/num_examples=43793
I0205 20:35:39.650221 140021163808512 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.03365589305758476, loss=0.04087202623486519
I0205 20:36:11.670414 139984133072640 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.008348004892468452, loss=0.041897013783454895
I0205 20:36:43.791531 140021163808512 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.018534047529101372, loss=0.04597391560673714
I0205 20:37:15.703369 139984133072640 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.012251414358615875, loss=0.0381452813744545
I0205 20:37:47.531766 140021163808512 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.04822465404868126, loss=0.04602088779211044
I0205 20:38:19.555767 139984133072640 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.04835974797606468, loss=0.04812018945813179
I0205 20:38:51.371380 140021163808512 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.031472090631723404, loss=0.04387402907013893
I0205 20:39:23.470242 139984133072640 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.04501648247241974, loss=0.04103810712695122
I0205 20:39:36.866981 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:41:16.532236 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:41:19.520211 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:41:22.480911 140205209478976 submission_runner.py:408] Time since start: 1850.20s, 	Step: 3743, 	{'train/accuracy': 0.9880852103233337, 'train/loss': 0.04129993915557861, 'train/mean_average_precision': 0.16831039361344796, 'validation/accuracy': 0.9852334856987, 'validation/loss': 0.0507333017885685, 'validation/mean_average_precision': 0.14941331378283598, 'validation/num_examples': 43793, 'test/accuracy': 0.9843466877937317, 'test/loss': 0.05367879569530487, 'test/mean_average_precision': 0.15127811692986542, 'test/num_examples': 43793, 'score': 1212.3853332996368, 'total_duration': 1850.2039790153503, 'accumulated_submission_time': 1212.3853332996368, 'accumulated_eval_time': 637.5870983600616, 'accumulated_logging_time': 0.13191795349121094}
I0205 20:41:22.496268 140002205157120 logging_writer.py:48] [3743] accumulated_eval_time=637.587098, accumulated_logging_time=0.131918, accumulated_submission_time=1212.385333, global_step=3743, preemption_count=0, score=1212.385333, test/accuracy=0.984347, test/loss=0.053679, test/mean_average_precision=0.151278, test/num_examples=43793, total_duration=1850.203979, train/accuracy=0.988085, train/loss=0.041300, train/mean_average_precision=0.168310, validation/accuracy=0.985233, validation/loss=0.050733, validation/mean_average_precision=0.149413, validation/num_examples=43793
I0205 20:41:40.905546 140020770670336 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.1768476665019989, loss=0.04536290094256401
I0205 20:42:12.550431 140002205157120 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.017617611214518547, loss=0.042479876428842545
I0205 20:42:44.115288 140020770670336 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.02723362110555172, loss=0.034789714962244034
I0205 20:43:15.901182 140002205157120 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0426136739552021, loss=0.03974521905183792
I0205 20:43:47.744826 140020770670336 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.03270554915070534, loss=0.03974466025829315
I0205 20:44:19.530420 140002205157120 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.019827470183372498, loss=0.04003551974892616
I0205 20:44:51.722218 140020770670336 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.054860103875398636, loss=0.04207107052206993
I0205 20:45:22.791750 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:47:03.635999 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:47:06.661597 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:47:09.639188 140205209478976 submission_runner.py:408] Time since start: 2197.36s, 	Step: 4498, 	{'train/accuracy': 0.9884041547775269, 'train/loss': 0.0404745452105999, 'train/mean_average_precision': 0.19482182188892394, 'validation/accuracy': 0.9853795766830444, 'validation/loss': 0.05005882307887077, 'validation/mean_average_precision': 0.15740818307328774, 'validation/num_examples': 43793, 'test/accuracy': 0.9844532608985901, 'test/loss': 0.05273740738630295, 'test/mean_average_precision': 0.15637513145419285, 'test/num_examples': 43793, 'score': 1452.6483166217804, 'total_duration': 2197.36225771904, 'accumulated_submission_time': 1452.6483166217804, 'accumulated_eval_time': 744.434494972229, 'accumulated_logging_time': 0.1595597267150879}
I0205 20:47:09.655029 139984133072640 logging_writer.py:48] [4498] accumulated_eval_time=744.434495, accumulated_logging_time=0.159560, accumulated_submission_time=1452.648317, global_step=4498, preemption_count=0, score=1452.648317, test/accuracy=0.984453, test/loss=0.052737, test/mean_average_precision=0.156375, test/num_examples=43793, total_duration=2197.362258, train/accuracy=0.988404, train/loss=0.040475, train/mean_average_precision=0.194822, validation/accuracy=0.985380, validation/loss=0.050059, validation/mean_average_precision=0.157408, validation/num_examples=43793
I0205 20:47:10.628858 140021163808512 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.01942191831767559, loss=0.03845975920557976
I0205 20:47:42.380846 139984133072640 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.035457637161016464, loss=0.04283876717090607
I0205 20:48:14.155145 140021163808512 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.03406127542257309, loss=0.03797272965312004
I0205 20:48:45.924299 139984133072640 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.04136918485164642, loss=0.043605197221040726
I0205 20:49:17.840944 140021163808512 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.04615679383277893, loss=0.047457847744226456
I0205 20:49:49.614353 139984133072640 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.044483333826065063, loss=0.0421840138733387
I0205 20:50:21.342813 140021163808512 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.03183552622795105, loss=0.03782045096158981
I0205 20:50:53.074047 139984133072640 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.05559418722987175, loss=0.03682313486933708
I0205 20:51:09.889292 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:52:50.613867 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:52:53.619122 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:52:56.557435 140205209478976 submission_runner.py:408] Time since start: 2544.28s, 	Step: 5254, 	{'train/accuracy': 0.9886635541915894, 'train/loss': 0.03942786902189255, 'train/mean_average_precision': 0.19027711590283466, 'validation/accuracy': 0.9854867458343506, 'validation/loss': 0.049740225076675415, 'validation/mean_average_precision': 0.16596522844191303, 'validation/num_examples': 43793, 'test/accuracy': 0.9845661520957947, 'test/loss': 0.05233199894428253, 'test/mean_average_precision': 0.17087559116697543, 'test/num_examples': 43793, 'score': 1692.851990699768, 'total_duration': 2544.280503988266, 'accumulated_submission_time': 1692.851990699768, 'accumulated_eval_time': 851.1025938987732, 'accumulated_logging_time': 0.18645191192626953}
I0205 20:52:56.573281 140002205157120 logging_writer.py:48] [5254] accumulated_eval_time=851.102594, accumulated_logging_time=0.186452, accumulated_submission_time=1692.851991, global_step=5254, preemption_count=0, score=1692.851991, test/accuracy=0.984566, test/loss=0.052332, test/mean_average_precision=0.170876, test/num_examples=43793, total_duration=2544.280504, train/accuracy=0.988664, train/loss=0.039428, train/mean_average_precision=0.190277, validation/accuracy=0.985487, validation/loss=0.049740, validation/mean_average_precision=0.165965, validation/num_examples=43793
I0205 20:53:11.498790 140002598295296 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.023190004751086235, loss=0.036871254444122314
I0205 20:53:42.856259 140002205157120 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.05843465030193329, loss=0.03892991691827774
I0205 20:54:14.913420 140002598295296 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.04311291128396988, loss=0.04414624720811844
I0205 20:54:46.471800 140002205157120 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.028194298967719078, loss=0.03742473945021629
I0205 20:55:18.872985 140002598295296 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.02384776435792446, loss=0.037760596722364426
I0205 20:55:51.069843 140002205157120 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04406921938061714, loss=0.04226779192686081
I0205 20:56:23.065845 140002598295296 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.03643355518579483, loss=0.04335615783929825
I0205 20:56:54.614470 140002205157120 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.02027333527803421, loss=0.03494073078036308
I0205 20:56:56.815098 140205209478976 spec.py:321] Evaluating on the training split.
I0205 20:58:34.633173 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 20:58:37.624614 140205209478976 spec.py:349] Evaluating on the test split.
I0205 20:58:40.582077 140205209478976 submission_runner.py:408] Time since start: 2888.31s, 	Step: 6008, 	{'train/accuracy': 0.9886347055435181, 'train/loss': 0.03914286568760872, 'train/mean_average_precision': 0.19986740394123362, 'validation/accuracy': 0.9856597185134888, 'validation/loss': 0.04885830730199814, 'validation/mean_average_precision': 0.18008463135684, 'validation/num_examples': 43793, 'test/accuracy': 0.9846979379653931, 'test/loss': 0.0519571416079998, 'test/mean_average_precision': 0.17276667524665232, 'test/num_examples': 43793, 'score': 1933.0624508857727, 'total_duration': 2888.3051455020905, 'accumulated_submission_time': 1933.0624508857727, 'accumulated_eval_time': 954.8695249557495, 'accumulated_logging_time': 0.21353363990783691}
I0205 20:58:40.598397 139984133072640 logging_writer.py:48] [6008] accumulated_eval_time=954.869525, accumulated_logging_time=0.213534, accumulated_submission_time=1933.062451, global_step=6008, preemption_count=0, score=1933.062451, test/accuracy=0.984698, test/loss=0.051957, test/mean_average_precision=0.172767, test/num_examples=43793, total_duration=2888.305146, train/accuracy=0.988635, train/loss=0.039143, train/mean_average_precision=0.199867, validation/accuracy=0.985660, validation/loss=0.048858, validation/mean_average_precision=0.180085, validation/num_examples=43793
I0205 20:59:10.688059 140020770670336 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.032409898936748505, loss=0.04364892840385437
I0205 20:59:42.863311 139984133072640 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.04007553309202194, loss=0.03768764063715935
I0205 21:00:15.509382 140020770670336 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.05696634203195572, loss=0.040340062230825424
I0205 21:00:47.660367 139984133072640 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.050821851938962936, loss=0.038315240293741226
I0205 21:01:19.680106 140020770670336 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.022970110177993774, loss=0.03742421045899391
I0205 21:01:51.436919 139984133072640 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.02997025102376938, loss=0.04045144468545914
I0205 21:02:24.277863 140020770670336 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.04134276136755943, loss=0.04290342330932617
I0205 21:02:40.826328 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:04:22.290265 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:04:25.304718 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:04:28.266206 140205209478976 submission_runner.py:408] Time since start: 3235.99s, 	Step: 6753, 	{'train/accuracy': 0.9883594512939453, 'train/loss': 0.04015161469578743, 'train/mean_average_precision': 0.192082496409469, 'validation/accuracy': 0.9853593111038208, 'validation/loss': 0.04985901713371277, 'validation/mean_average_precision': 0.16853419468052408, 'validation/num_examples': 43793, 'test/accuracy': 0.9844822883605957, 'test/loss': 0.05259844660758972, 'test/mean_average_precision': 0.16261873750826217, 'test/num_examples': 43793, 'score': 2173.258081674576, 'total_duration': 3235.989268064499, 'accumulated_submission_time': 2173.258081674576, 'accumulated_eval_time': 1062.309351682663, 'accumulated_logging_time': 0.24247026443481445}
I0205 21:04:28.282459 140021163808512 logging_writer.py:48] [6753] accumulated_eval_time=1062.309352, accumulated_logging_time=0.242470, accumulated_submission_time=2173.258082, global_step=6753, preemption_count=0, score=2173.258082, test/accuracy=0.984482, test/loss=0.052598, test/mean_average_precision=0.162619, test/num_examples=43793, total_duration=3235.989268, train/accuracy=0.988359, train/loss=0.040152, train/mean_average_precision=0.192082, validation/accuracy=0.985359, validation/loss=0.049859, validation/mean_average_precision=0.168534, validation/num_examples=43793
I0205 21:04:43.723124 140044334864128 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.04075159132480621, loss=0.03658748418092728
I0205 21:05:16.856481 140021163808512 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.02662520669400692, loss=0.03622286021709442
I0205 21:05:49.718416 140044334864128 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.024209238588809967, loss=0.038462162017822266
I0205 21:06:22.288790 140021163808512 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.030473576858639717, loss=0.033905092626810074
I0205 21:06:54.187990 140044334864128 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.025860300287604332, loss=0.036836687475442886
I0205 21:07:26.404473 140021163808512 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.08064460754394531, loss=0.040741123259067535
I0205 21:07:58.962682 140044334864128 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.02667267434298992, loss=0.04071395471692085
I0205 21:08:28.390505 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:10:06.720570 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:10:09.803229 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:10:12.795173 140205209478976 submission_runner.py:408] Time since start: 3580.52s, 	Step: 7492, 	{'train/accuracy': 0.9885475635528564, 'train/loss': 0.03908183053135872, 'train/mean_average_precision': 0.19995834331785922, 'validation/accuracy': 0.9856422543525696, 'validation/loss': 0.048810314387083054, 'validation/mean_average_precision': 0.18285298346840118, 'validation/num_examples': 43793, 'test/accuracy': 0.9846469759941101, 'test/loss': 0.05169987678527832, 'test/mean_average_precision': 0.18459043939620856, 'test/num_examples': 43793, 'score': 2413.3347930908203, 'total_duration': 3580.518236398697, 'accumulated_submission_time': 2413.3347930908203, 'accumulated_eval_time': 1166.7139718532562, 'accumulated_logging_time': 0.2698376178741455}
I0205 21:10:12.811614 140020770670336 logging_writer.py:48] [7492] accumulated_eval_time=1166.713972, accumulated_logging_time=0.269838, accumulated_submission_time=2413.334793, global_step=7492, preemption_count=0, score=2413.334793, test/accuracy=0.984647, test/loss=0.051700, test/mean_average_precision=0.184590, test/num_examples=43793, total_duration=3580.518236, train/accuracy=0.988548, train/loss=0.039082, train/mean_average_precision=0.199958, validation/accuracy=0.985642, validation/loss=0.048810, validation/mean_average_precision=0.182853, validation/num_examples=43793
I0205 21:10:15.765247 140142492776192 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.09865827858448029, loss=0.04108896479010582
I0205 21:10:48.529651 140020770670336 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.054843246936798096, loss=0.04120776802301407
I0205 21:11:20.608838 140142492776192 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03135880455374718, loss=0.04371229186654091
I0205 21:11:52.273729 140020770670336 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.03881483152508736, loss=0.03602927550673485
I0205 21:12:24.252243 140142492776192 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.048256535083055496, loss=0.03802055865526199
I0205 21:12:56.103204 140020770670336 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.06139671057462692, loss=0.03814620152115822
I0205 21:13:27.950993 140142492776192 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.03772462159395218, loss=0.041078295558691025
I0205 21:13:59.664216 140020770670336 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0679156631231308, loss=0.0370604433119297
I0205 21:14:13.108927 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:15:55.276721 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:15:58.309539 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:16:01.309244 140205209478976 submission_runner.py:408] Time since start: 3929.03s, 	Step: 8243, 	{'train/accuracy': 0.9886873364448547, 'train/loss': 0.03875712677836418, 'train/mean_average_precision': 0.21149383792245036, 'validation/accuracy': 0.9858253002166748, 'validation/loss': 0.048680491745471954, 'validation/mean_average_precision': 0.18361861906994822, 'validation/num_examples': 43793, 'test/accuracy': 0.9848407506942749, 'test/loss': 0.0517675057053566, 'test/mean_average_precision': 0.18188655629942915, 'test/num_examples': 43793, 'score': 2653.599544286728, 'total_duration': 3929.0323138237, 'accumulated_submission_time': 2653.599544286728, 'accumulated_eval_time': 1274.9142456054688, 'accumulated_logging_time': 0.29875755310058594}
I0205 21:16:01.326163 140021163808512 logging_writer.py:48] [8243] accumulated_eval_time=1274.914246, accumulated_logging_time=0.298758, accumulated_submission_time=2653.599544, global_step=8243, preemption_count=0, score=2653.599544, test/accuracy=0.984841, test/loss=0.051768, test/mean_average_precision=0.181887, test/num_examples=43793, total_duration=3929.032314, train/accuracy=0.988687, train/loss=0.038757, train/mean_average_precision=0.211494, validation/accuracy=0.985825, validation/loss=0.048680, validation/mean_average_precision=0.183619, validation/num_examples=43793
I0205 21:16:19.833583 140044334864128 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.03350452706217766, loss=0.04383065551519394
I0205 21:16:51.588362 140021163808512 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.02005019783973694, loss=0.0351884625852108
I0205 21:17:23.499580 140044334864128 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0552799254655838, loss=0.043557003140449524
I0205 21:17:54.880563 140021163808512 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.023280182853341103, loss=0.038079194724559784
I0205 21:18:26.449467 140044334864128 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.07835180312395096, loss=0.03998483717441559
I0205 21:18:57.756490 140021163808512 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.03328929841518402, loss=0.03717518225312233
I0205 21:19:29.002081 140044334864128 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.026506274938583374, loss=0.039841845631599426
I0205 21:20:00.934319 140021163808512 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0631926953792572, loss=0.038968704640865326
I0205 21:20:01.556201 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:21:43.786099 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:21:46.806864 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:21:49.827203 140205209478976 submission_runner.py:408] Time since start: 4277.55s, 	Step: 9003, 	{'train/accuracy': 0.9886422753334045, 'train/loss': 0.03899676725268364, 'train/mean_average_precision': 0.20870463578473877, 'validation/accuracy': 0.9857802391052246, 'validation/loss': 0.04873434081673622, 'validation/mean_average_precision': 0.18945347842751153, 'validation/num_examples': 43793, 'test/accuracy': 0.9847792387008667, 'test/loss': 0.05149604007601738, 'test/mean_average_precision': 0.19200215243796978, 'test/num_examples': 43793, 'score': 2893.7972707748413, 'total_duration': 4277.550271987915, 'accumulated_submission_time': 2893.7972707748413, 'accumulated_eval_time': 1383.1852016448975, 'accumulated_logging_time': 0.3282186985015869}
I0205 21:21:49.844054 140020770670336 logging_writer.py:48] [9003] accumulated_eval_time=1383.185202, accumulated_logging_time=0.328219, accumulated_submission_time=2893.797271, global_step=9003, preemption_count=0, score=2893.797271, test/accuracy=0.984779, test/loss=0.051496, test/mean_average_precision=0.192002, test/num_examples=43793, total_duration=4277.550272, train/accuracy=0.988642, train/loss=0.038997, train/mean_average_precision=0.208705, validation/accuracy=0.985780, validation/loss=0.048734, validation/mean_average_precision=0.189453, validation/num_examples=43793
I0205 21:22:20.800647 140142492776192 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.04476433992385864, loss=0.04442598670721054
I0205 21:22:52.400379 140020770670336 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.07698731124401093, loss=0.03995811566710472
I0205 21:23:23.895817 140142492776192 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.07239662110805511, loss=0.042491521686315536
I0205 21:23:55.599539 140020770670336 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.03400431573390961, loss=0.035888638347387314
I0205 21:24:27.367205 140142492776192 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.06572110205888748, loss=0.03913934528827667
I0205 21:24:59.304568 140020770670336 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.050788700580596924, loss=0.03520561009645462
I0205 21:25:31.551529 140142492776192 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.026308802887797356, loss=0.036147382110357285
I0205 21:25:49.980863 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:27:31.746869 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:27:34.813546 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:27:37.823469 140205209478976 submission_runner.py:408] Time since start: 4625.55s, 	Step: 9760, 	{'train/accuracy': 0.9888356328010559, 'train/loss': 0.03835595026612282, 'train/mean_average_precision': 0.2161694728775924, 'validation/accuracy': 0.9855862259864807, 'validation/loss': 0.048749759793281555, 'validation/mean_average_precision': 0.18400181375737143, 'validation/num_examples': 43793, 'test/accuracy': 0.9846423864364624, 'test/loss': 0.05163241922855377, 'test/mean_average_precision': 0.18344304914426365, 'test/num_examples': 43793, 'score': 3133.9032673835754, 'total_duration': 4625.546536207199, 'accumulated_submission_time': 3133.9032673835754, 'accumulated_eval_time': 1491.027760028839, 'accumulated_logging_time': 0.3559896945953369}
I0205 21:27:37.840740 140002598295296 logging_writer.py:48] [9760] accumulated_eval_time=1491.027760, accumulated_logging_time=0.355990, accumulated_submission_time=3133.903267, global_step=9760, preemption_count=0, score=3133.903267, test/accuracy=0.984642, test/loss=0.051632, test/mean_average_precision=0.183443, test/num_examples=43793, total_duration=4625.546536, train/accuracy=0.988836, train/loss=0.038356, train/mean_average_precision=0.216169, validation/accuracy=0.985586, validation/loss=0.048750, validation/mean_average_precision=0.184002, validation/num_examples=43793
I0205 21:27:51.924422 140021163808512 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.023281440138816833, loss=0.04151850566267967
I0205 21:28:24.512169 140002598295296 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.03110770881175995, loss=0.03618341684341431
I0205 21:28:56.413317 140021163808512 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.03541722521185875, loss=0.042221978306770325
I0205 21:29:28.607604 140002598295296 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0459517240524292, loss=0.03950290381908417
I0205 21:30:00.992136 140021163808512 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0399320013821125, loss=0.04298166558146477
I0205 21:30:32.946792 140002598295296 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.05046709254384041, loss=0.038694821298122406
I0205 21:31:05.120576 140021163808512 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.035225316882133484, loss=0.034953806549310684
I0205 21:31:37.380568 140002598295296 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.024642562493681908, loss=0.03778673708438873
I0205 21:31:38.005379 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:33:17.551462 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:33:20.685245 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:33:23.738869 140205209478976 submission_runner.py:408] Time since start: 4971.46s, 	Step: 10503, 	{'train/accuracy': 0.9887315034866333, 'train/loss': 0.038301631808280945, 'train/mean_average_precision': 0.22690173713484513, 'validation/accuracy': 0.9857413172721863, 'validation/loss': 0.04893196374177933, 'validation/mean_average_precision': 0.18965706404621746, 'validation/num_examples': 43793, 'test/accuracy': 0.9848563075065613, 'test/loss': 0.05198470503091812, 'test/mean_average_precision': 0.1847305208964719, 'test/num_examples': 43793, 'score': 3374.0375757217407, 'total_duration': 4971.461939096451, 'accumulated_submission_time': 3374.0375757217407, 'accumulated_eval_time': 1596.7612025737762, 'accumulated_logging_time': 0.384077787399292}
I0205 21:33:23.755683 140020770670336 logging_writer.py:48] [10503] accumulated_eval_time=1596.761203, accumulated_logging_time=0.384078, accumulated_submission_time=3374.037576, global_step=10503, preemption_count=0, score=3374.037576, test/accuracy=0.984856, test/loss=0.051985, test/mean_average_precision=0.184731, test/num_examples=43793, total_duration=4971.461939, train/accuracy=0.988732, train/loss=0.038302, train/mean_average_precision=0.226902, validation/accuracy=0.985741, validation/loss=0.048932, validation/mean_average_precision=0.189657, validation/num_examples=43793
I0205 21:33:55.340537 140142492776192 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.07118722051382065, loss=0.04218350350856781
I0205 21:34:26.999729 140020770670336 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.03636665269732475, loss=0.03864407166838646
I0205 21:34:58.635915 140142492776192 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.04760532081127167, loss=0.03779633343219757
I0205 21:35:30.263141 140020770670336 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.06264857947826385, loss=0.03895353153347969
I0205 21:36:01.977733 140142492776192 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.09792163223028183, loss=0.04000335931777954
I0205 21:36:33.835779 140020770670336 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.045434463769197464, loss=0.03768722340464592
I0205 21:37:06.600934 140142492776192 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.046304669231176376, loss=0.036949798464775085
I0205 21:37:23.770560 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:39:06.514741 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:39:09.653738 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:39:12.802140 140205209478976 submission_runner.py:408] Time since start: 5320.53s, 	Step: 11254, 	{'train/accuracy': 0.9888266921043396, 'train/loss': 0.038045983761548996, 'train/mean_average_precision': 0.22985548116505886, 'validation/accuracy': 0.9857701063156128, 'validation/loss': 0.04840954765677452, 'validation/mean_average_precision': 0.1863925213489288, 'validation/num_examples': 43793, 'test/accuracy': 0.9848538041114807, 'test/loss': 0.05127113312482834, 'test/mean_average_precision': 0.18452970367097346, 'test/num_examples': 43793, 'score': 3614.018998861313, 'total_duration': 5320.525203466415, 'accumulated_submission_time': 3614.018998861313, 'accumulated_eval_time': 1705.7927422523499, 'accumulated_logging_time': 0.4135866165161133}
I0205 21:39:12.820328 140021163808512 logging_writer.py:48] [11254] accumulated_eval_time=1705.792742, accumulated_logging_time=0.413587, accumulated_submission_time=3614.018999, global_step=11254, preemption_count=0, score=3614.018999, test/accuracy=0.984854, test/loss=0.051271, test/mean_average_precision=0.184530, test/num_examples=43793, total_duration=5320.525203, train/accuracy=0.988827, train/loss=0.038046, train/mean_average_precision=0.229855, validation/accuracy=0.985770, validation/loss=0.048410, validation/mean_average_precision=0.186393, validation/num_examples=43793
I0205 21:39:27.905940 140044334864128 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.04610065370798111, loss=0.04146023467183113
I0205 21:40:00.185800 140021163808512 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.03088279254734516, loss=0.03891032561659813
I0205 21:40:32.007651 140044334864128 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.02373693510890007, loss=0.037582337856292725
I0205 21:41:03.892080 140021163808512 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.04326072707772255, loss=0.03361745551228523
I0205 21:41:35.928930 140044334864128 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.05622556060552597, loss=0.03658406808972359
I0205 21:42:08.230689 140021163808512 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.029916247352957726, loss=0.03915242850780487
I0205 21:42:40.084834 140044334864128 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.03617766499519348, loss=0.04223068803548813
I0205 21:43:12.425945 140021163808512 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.02766977623105049, loss=0.04008108004927635
I0205 21:43:13.057989 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:44:51.930814 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:44:55.036518 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:44:58.071105 140205209478976 submission_runner.py:408] Time since start: 5665.79s, 	Step: 12003, 	{'train/accuracy': 0.988034188747406, 'train/loss': 0.040260523557662964, 'train/mean_average_precision': 0.21595434903574937, 'validation/accuracy': 0.9848340153694153, 'validation/loss': 0.05086250230669975, 'validation/mean_average_precision': 0.17195741729684832, 'validation/num_examples': 43793, 'test/accuracy': 0.9838365912437439, 'test/loss': 0.05383049696683884, 'test/mean_average_precision': 0.17677506274615856, 'test/num_examples': 43793, 'score': 3854.225952386856, 'total_duration': 5665.794174194336, 'accumulated_submission_time': 3854.225952386856, 'accumulated_eval_time': 1810.8058123588562, 'accumulated_logging_time': 0.4426114559173584}
I0205 21:44:58.088547 140020770670336 logging_writer.py:48] [12003] accumulated_eval_time=1810.805812, accumulated_logging_time=0.442611, accumulated_submission_time=3854.225952, global_step=12003, preemption_count=0, score=3854.225952, test/accuracy=0.983837, test/loss=0.053830, test/mean_average_precision=0.176775, test/num_examples=43793, total_duration=5665.794174, train/accuracy=0.988034, train/loss=0.040261, train/mean_average_precision=0.215954, validation/accuracy=0.984834, validation/loss=0.050863, validation/mean_average_precision=0.171957, validation/num_examples=43793
I0205 21:45:29.684206 140142492776192 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.019291311502456665, loss=0.03537340462207794
I0205 21:46:01.578864 140020770670336 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.031504590064287186, loss=0.037665918469429016
I0205 21:46:33.680594 140142492776192 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.026327461004257202, loss=0.037266191095113754
I0205 21:47:06.165071 140020770670336 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.031024351716041565, loss=0.03584748134016991
I0205 21:47:37.861411 140142492776192 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.05704779177904129, loss=0.03680844604969025
I0205 21:48:09.722614 140020770670336 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.05190158635377884, loss=0.03702908009290695
I0205 21:48:41.569982 140142492776192 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.053271640092134476, loss=0.03604117035865784
I0205 21:48:58.128198 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:50:38.441479 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:50:41.572937 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:50:44.644481 140205209478976 submission_runner.py:408] Time since start: 6012.37s, 	Step: 12752, 	{'train/accuracy': 0.9886979460716248, 'train/loss': 0.038513973355293274, 'train/mean_average_precision': 0.212413276491084, 'validation/accuracy': 0.9853869080543518, 'validation/loss': 0.0496855191886425, 'validation/mean_average_precision': 0.18165653618837238, 'validation/num_examples': 43793, 'test/accuracy': 0.9844094514846802, 'test/loss': 0.05285492539405823, 'test/mean_average_precision': 0.18299348088723388, 'test/num_examples': 43793, 'score': 4094.2330226898193, 'total_duration': 6012.367550611496, 'accumulated_submission_time': 4094.2330226898193, 'accumulated_eval_time': 1917.3220510482788, 'accumulated_logging_time': 0.47250795364379883}
I0205 21:50:44.662084 140002598295296 logging_writer.py:48] [12752] accumulated_eval_time=1917.322051, accumulated_logging_time=0.472508, accumulated_submission_time=4094.233023, global_step=12752, preemption_count=0, score=4094.233023, test/accuracy=0.984409, test/loss=0.052855, test/mean_average_precision=0.182993, test/num_examples=43793, total_duration=6012.367551, train/accuracy=0.988698, train/loss=0.038514, train/mean_average_precision=0.212413, validation/accuracy=0.985387, validation/loss=0.049686, validation/mean_average_precision=0.181657, validation/num_examples=43793
I0205 21:51:00.412787 140044334864128 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.04035075381398201, loss=0.03826846182346344
I0205 21:51:32.152943 140002598295296 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.043678559362888336, loss=0.03936708718538284
I0205 21:52:03.908985 140044334864128 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.05659852176904678, loss=0.045335207134485245
I0205 21:52:35.738686 140002598295296 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.050714876502752304, loss=0.04361339285969734
I0205 21:53:07.326603 140044334864128 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.03830808773636818, loss=0.03411480411887169
I0205 21:53:38.895815 140002598295296 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.04170433431863785, loss=0.03957625851035118
I0205 21:54:10.735978 140044334864128 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.03574199974536896, loss=0.0418071374297142
I0205 21:54:42.415434 140002598295296 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.040486160665750504, loss=0.03546438738703728
I0205 21:54:44.672779 140205209478976 spec.py:321] Evaluating on the training split.
I0205 21:56:26.737298 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 21:56:29.726540 140205209478976 spec.py:349] Evaluating on the test split.
I0205 21:56:32.756381 140205209478976 submission_runner.py:408] Time since start: 6360.48s, 	Step: 13508, 	{'train/accuracy': 0.98878014087677, 'train/loss': 0.03843506798148155, 'train/mean_average_precision': 0.2341694003337173, 'validation/accuracy': 0.9857327938079834, 'validation/loss': 0.04841131716966629, 'validation/mean_average_precision': 0.18682739612154003, 'validation/num_examples': 43793, 'test/accuracy': 0.9848297834396362, 'test/loss': 0.05098680779337883, 'test/mean_average_precision': 0.1925377068346132, 'test/num_examples': 43793, 'score': 4334.211813926697, 'total_duration': 6360.479429960251, 'accumulated_submission_time': 4334.211813926697, 'accumulated_eval_time': 2025.4055910110474, 'accumulated_logging_time': 0.5020263195037842}
I0205 21:56:32.773900 140021163808512 logging_writer.py:48] [13508] accumulated_eval_time=2025.405591, accumulated_logging_time=0.502026, accumulated_submission_time=4334.211814, global_step=13508, preemption_count=0, score=4334.211814, test/accuracy=0.984830, test/loss=0.050987, test/mean_average_precision=0.192538, test/num_examples=43793, total_duration=6360.479430, train/accuracy=0.988780, train/loss=0.038435, train/mean_average_precision=0.234169, validation/accuracy=0.985733, validation/loss=0.048411, validation/mean_average_precision=0.186827, validation/num_examples=43793
I0205 21:57:02.072801 140142492776192 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.04681912437081337, loss=0.03812291845679283
I0205 21:57:33.836642 140021163808512 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.024580713361501694, loss=0.037145595997571945
I0205 21:58:06.496823 140142492776192 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03027249127626419, loss=0.03485107421875
I0205 21:58:38.922910 140021163808512 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.04277601093053818, loss=0.03296097740530968
I0205 21:59:11.011201 140142492776192 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.05835191160440445, loss=0.040280573070049286
I0205 21:59:42.339480 140021163808512 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.0627778172492981, loss=0.03391176462173462
I0205 22:00:14.067305 140142492776192 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.023862438276410103, loss=0.033423446118831635
I0205 22:00:32.949025 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:02:14.708651 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:02:18.292344 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:02:21.746047 140205209478976 submission_runner.py:408] Time since start: 6709.47s, 	Step: 14261, 	{'train/accuracy': 0.9888355135917664, 'train/loss': 0.038366395980119705, 'train/mean_average_precision': 0.22154629846613177, 'validation/accuracy': 0.9857721328735352, 'validation/loss': 0.04902218282222748, 'validation/mean_average_precision': 0.18127468605978142, 'validation/num_examples': 43793, 'test/accuracy': 0.9848424196243286, 'test/loss': 0.05206082761287689, 'test/mean_average_precision': 0.17770151589102587, 'test/num_examples': 43793, 'score': 4574.353425979614, 'total_duration': 6709.469097614288, 'accumulated_submission_time': 4574.353425979614, 'accumulated_eval_time': 2134.2025485038757, 'accumulated_logging_time': 0.5312979221343994}
I0205 22:02:21.767105 140002598295296 logging_writer.py:48] [14261] accumulated_eval_time=2134.202549, accumulated_logging_time=0.531298, accumulated_submission_time=4574.353426, global_step=14261, preemption_count=0, score=4574.353426, test/accuracy=0.984842, test/loss=0.052061, test/mean_average_precision=0.177702, test/num_examples=43793, total_duration=6709.469098, train/accuracy=0.988836, train/loss=0.038366, train/mean_average_precision=0.221546, validation/accuracy=0.985772, validation/loss=0.049022, validation/mean_average_precision=0.181275, validation/num_examples=43793
I0205 22:02:35.079660 140020770670336 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.046531494706869125, loss=0.0410744845867157
I0205 22:03:08.537637 140002598295296 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.053728941828012466, loss=0.039015088230371475
I0205 22:03:41.218011 140020770670336 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.04626108705997467, loss=0.034626882523298264
I0205 22:04:13.766981 140002598295296 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.049344565719366074, loss=0.039417147636413574
I0205 22:04:46.378236 140020770670336 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.02111436054110527, loss=0.043066851794719696
I0205 22:05:19.127653 140002598295296 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03702988475561142, loss=0.037979912012815475
I0205 22:05:51.822706 140020770670336 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.031534675508737564, loss=0.03838633745908737
I0205 22:06:21.781892 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:08:06.323172 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:08:09.398761 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:08:12.426180 140205209478976 submission_runner.py:408] Time since start: 7060.15s, 	Step: 14993, 	{'train/accuracy': 0.9888824820518494, 'train/loss': 0.03792348504066467, 'train/mean_average_precision': 0.2257129817583256, 'validation/accuracy': 0.9858293533325195, 'validation/loss': 0.04814564436674118, 'validation/mean_average_precision': 0.19155246914254173, 'validation/num_examples': 43793, 'test/accuracy': 0.984917402267456, 'test/loss': 0.0510648749768734, 'test/mean_average_precision': 0.18411476735013166, 'test/num_examples': 43793, 'score': 4814.332322835922, 'total_duration': 7060.149246931076, 'accumulated_submission_time': 4814.332322835922, 'accumulated_eval_time': 2244.846806049347, 'accumulated_logging_time': 0.5642621517181396}
I0205 22:08:12.444565 140021163808512 logging_writer.py:48] [14993] accumulated_eval_time=2244.846806, accumulated_logging_time=0.564262, accumulated_submission_time=4814.332323, global_step=14993, preemption_count=0, score=4814.332323, test/accuracy=0.984917, test/loss=0.051065, test/mean_average_precision=0.184115, test/num_examples=43793, total_duration=7060.149247, train/accuracy=0.988882, train/loss=0.037923, train/mean_average_precision=0.225713, validation/accuracy=0.985829, validation/loss=0.048146, validation/mean_average_precision=0.191552, validation/num_examples=43793
I0205 22:08:15.092500 140142492776192 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03398844227194786, loss=0.037351157516241074
I0205 22:08:47.527365 140021163808512 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.056891072541475296, loss=0.04467512294650078
I0205 22:09:19.851531 140142492776192 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.04131216183304787, loss=0.04182431846857071
I0205 22:09:52.366587 140021163808512 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.046508900821208954, loss=0.03667343780398369
I0205 22:10:24.402364 140142492776192 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.05783491209149361, loss=0.040718626230955124
I0205 22:10:56.287294 140021163808512 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.026596523821353912, loss=0.03783675655722618
I0205 22:11:28.163075 140142492776192 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.02438974380493164, loss=0.03790697082877159
I0205 22:11:59.962165 140021163808512 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.027080677449703217, loss=0.033320505172014236
I0205 22:12:12.666121 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:13:51.460517 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:13:54.540877 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:13:57.621607 140205209478976 submission_runner.py:408] Time since start: 7405.34s, 	Step: 15741, 	{'train/accuracy': 0.9887647032737732, 'train/loss': 0.03873090818524361, 'train/mean_average_precision': 0.20830259584203645, 'validation/accuracy': 0.9856645464897156, 'validation/loss': 0.0490984246134758, 'validation/mean_average_precision': 0.1780559326371, 'validation/num_examples': 43793, 'test/accuracy': 0.9846630096435547, 'test/loss': 0.05220545455813408, 'test/mean_average_precision': 0.17087375222694276, 'test/num_examples': 43793, 'score': 5054.521810770035, 'total_duration': 7405.3446571826935, 'accumulated_submission_time': 5054.521810770035, 'accumulated_eval_time': 2349.802227497101, 'accumulated_logging_time': 0.5941531658172607}
I0205 22:13:57.640933 140002598295296 logging_writer.py:48] [15741] accumulated_eval_time=2349.802227, accumulated_logging_time=0.594153, accumulated_submission_time=5054.521811, global_step=15741, preemption_count=0, score=5054.521811, test/accuracy=0.984663, test/loss=0.052205, test/mean_average_precision=0.170874, test/num_examples=43793, total_duration=7405.344657, train/accuracy=0.988765, train/loss=0.038731, train/mean_average_precision=0.208303, validation/accuracy=0.985665, validation/loss=0.049098, validation/mean_average_precision=0.178056, validation/num_examples=43793
I0205 22:14:17.165444 140044334864128 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.08462489396333694, loss=0.03799309954047203
I0205 22:14:49.166738 140002598295296 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.048403918743133545, loss=0.03590512275695801
I0205 22:15:21.401311 140044334864128 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.032499682158231735, loss=0.03777841478586197
I0205 22:15:53.421455 140002598295296 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.02364613488316536, loss=0.03768523409962654
I0205 22:16:25.287402 140044334864128 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.03828110173344612, loss=0.04031907767057419
I0205 22:16:57.230948 140002598295296 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.04383878409862518, loss=0.0383867472410202
I0205 22:17:29.285679 140044334864128 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.11423241347074509, loss=0.039340779185295105
I0205 22:17:57.895220 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:19:38.416204 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:19:41.520680 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:19:44.590221 140205209478976 submission_runner.py:408] Time since start: 7752.31s, 	Step: 16489, 	{'train/accuracy': 0.9889504313468933, 'train/loss': 0.03789043799042702, 'train/mean_average_precision': 0.21556878954663675, 'validation/accuracy': 0.9857696890830994, 'validation/loss': 0.048670437186956406, 'validation/mean_average_precision': 0.18256488450566627, 'validation/num_examples': 43793, 'test/accuracy': 0.9848487377166748, 'test/loss': 0.05156475678086281, 'test/mean_average_precision': 0.17534660005543087, 'test/num_examples': 43793, 'score': 5294.745937824249, 'total_duration': 7752.313290119171, 'accumulated_submission_time': 5294.745937824249, 'accumulated_eval_time': 2456.4971837997437, 'accumulated_logging_time': 0.624169111251831}
I0205 22:19:44.608121 140020770670336 logging_writer.py:48] [16489] accumulated_eval_time=2456.497184, accumulated_logging_time=0.624169, accumulated_submission_time=5294.745938, global_step=16489, preemption_count=0, score=5294.745938, test/accuracy=0.984849, test/loss=0.051565, test/mean_average_precision=0.175347, test/num_examples=43793, total_duration=7752.313290, train/accuracy=0.988950, train/loss=0.037890, train/mean_average_precision=0.215569, validation/accuracy=0.985770, validation/loss=0.048670, validation/mean_average_precision=0.182565, validation/num_examples=43793
I0205 22:19:48.475778 140021163808512 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.03748483210802078, loss=0.03643595799803734
I0205 22:20:20.134305 140020770670336 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.030036872252821922, loss=0.03971024602651596
I0205 22:20:52.132204 140021163808512 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.04323624074459076, loss=0.037476759403944016
I0205 22:21:24.059980 140020770670336 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.03591148555278778, loss=0.03960777446627617
I0205 22:21:55.968223 140021163808512 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.04315001145005226, loss=0.039304398000240326
I0205 22:22:27.739249 140020770670336 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.03904455527663231, loss=0.03795282542705536
I0205 22:22:59.583098 140021163808512 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.048877354711294174, loss=0.03440817445516586
I0205 22:23:31.671896 140020770670336 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.03755772113800049, loss=0.037725355476140976
I0205 22:23:44.726696 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:25:23.360421 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:25:26.404808 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:25:29.473202 140205209478976 submission_runner.py:408] Time since start: 8097.20s, 	Step: 17242, 	{'train/accuracy': 0.9890180826187134, 'train/loss': 0.03764190897345543, 'train/mean_average_precision': 0.2201965623201106, 'validation/accuracy': 0.9857839345932007, 'validation/loss': 0.0482412688434124, 'validation/mean_average_precision': 0.18357224516518592, 'validation/num_examples': 43793, 'test/accuracy': 0.9849043488502502, 'test/loss': 0.05109306052327156, 'test/mean_average_precision': 0.18187410407147248, 'test/num_examples': 43793, 'score': 5534.832649469376, 'total_duration': 8097.196264505386, 'accumulated_submission_time': 5534.832649469376, 'accumulated_eval_time': 2561.2436380386353, 'accumulated_logging_time': 0.6534569263458252}
I0205 22:25:29.494777 140002205157120 logging_writer.py:48] [17242] accumulated_eval_time=2561.243638, accumulated_logging_time=0.653457, accumulated_submission_time=5534.832649, global_step=17242, preemption_count=0, score=5534.832649, test/accuracy=0.984904, test/loss=0.051093, test/mean_average_precision=0.181874, test/num_examples=43793, total_duration=8097.196265, train/accuracy=0.989018, train/loss=0.037642, train/mean_average_precision=0.220197, validation/accuracy=0.985784, validation/loss=0.048241, validation/mean_average_precision=0.183572, validation/num_examples=43793
I0205 22:25:48.802151 140044334864128 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.06087160483002663, loss=0.03820095583796501
I0205 22:26:20.821264 140002205157120 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.053588107228279114, loss=0.04424130171537399
I0205 22:26:52.873100 140044334864128 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.04293901100754738, loss=0.03969229385256767
I0205 22:27:25.161076 140002205157120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.044565267860889435, loss=0.03736454248428345
I0205 22:27:57.129729 140044334864128 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.026321863755583763, loss=0.03641260042786598
I0205 22:28:29.302901 140002205157120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.046087831258773804, loss=0.036309923976659775
I0205 22:29:01.516752 140044334864128 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.046250119805336, loss=0.03785126656293869
I0205 22:29:29.740375 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:31:12.066664 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:31:15.159023 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:31:18.230721 140205209478976 submission_runner.py:408] Time since start: 8445.95s, 	Step: 17989, 	{'train/accuracy': 0.9887692928314209, 'train/loss': 0.03804321587085724, 'train/mean_average_precision': 0.22739804622722806, 'validation/accuracy': 0.9855777025222778, 'validation/loss': 0.04910195618867874, 'validation/mean_average_precision': 0.18627496475117836, 'validation/num_examples': 43793, 'test/accuracy': 0.9846507906913757, 'test/loss': 0.05218307301402092, 'test/mean_average_precision': 0.1831122481482856, 'test/num_examples': 43793, 'score': 5775.0447998046875, 'total_duration': 8445.953789234161, 'accumulated_submission_time': 5775.0447998046875, 'accumulated_eval_time': 2669.7339446544647, 'accumulated_logging_time': 0.6886923313140869}
I0205 22:31:18.249486 140002598295296 logging_writer.py:48] [17989] accumulated_eval_time=2669.733945, accumulated_logging_time=0.688692, accumulated_submission_time=5775.044800, global_step=17989, preemption_count=0, score=5775.044800, test/accuracy=0.984651, test/loss=0.052183, test/mean_average_precision=0.183112, test/num_examples=43793, total_duration=8445.953789, train/accuracy=0.988769, train/loss=0.038043, train/mean_average_precision=0.227398, validation/accuracy=0.985578, validation/loss=0.049102, validation/mean_average_precision=0.186275, validation/num_examples=43793
I0205 22:31:22.272815 140020770670336 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.06229018792510033, loss=0.044526632875204086
I0205 22:31:53.950892 140002598295296 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.050040390342473984, loss=0.03889237344264984
I0205 22:32:25.634051 140020770670336 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.027409758418798447, loss=0.03560488298535347
I0205 22:32:57.772487 140002598295296 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.0659407451748848, loss=0.040512774139642715
I0205 22:33:29.510881 140020770670336 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.02471494860947132, loss=0.034731026738882065
I0205 22:34:01.668960 140002598295296 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.04652704671025276, loss=0.03873595967888832
I0205 22:34:33.629275 140020770670336 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.026049993932247162, loss=0.040489036589860916
I0205 22:35:05.639008 140002598295296 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.06632775813341141, loss=0.04200948029756546
I0205 22:35:18.326781 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:36:58.299798 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:37:01.350607 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:37:04.412945 140205209478976 submission_runner.py:408] Time since start: 8792.14s, 	Step: 18740, 	{'train/accuracy': 0.9890387058258057, 'train/loss': 0.03707514703273773, 'train/mean_average_precision': 0.24299820940443767, 'validation/accuracy': 0.9859284162521362, 'validation/loss': 0.04772140830755234, 'validation/mean_average_precision': 0.19121262066486283, 'validation/num_examples': 43793, 'test/accuracy': 0.984965443611145, 'test/loss': 0.050750963389873505, 'test/mean_average_precision': 0.1886865838804157, 'test/num_examples': 43793, 'score': 6015.091109275818, 'total_duration': 8792.135896921158, 'accumulated_submission_time': 6015.091109275818, 'accumulated_eval_time': 2775.819948196411, 'accumulated_logging_time': 0.7185335159301758}
I0205 22:37:04.431689 140002205157120 logging_writer.py:48] [18740] accumulated_eval_time=2775.819948, accumulated_logging_time=0.718534, accumulated_submission_time=6015.091109, global_step=18740, preemption_count=0, score=6015.091109, test/accuracy=0.984965, test/loss=0.050751, test/mean_average_precision=0.188687, test/num_examples=43793, total_duration=8792.135897, train/accuracy=0.989039, train/loss=0.037075, train/mean_average_precision=0.242998, validation/accuracy=0.985928, validation/loss=0.047721, validation/mean_average_precision=0.191213, validation/num_examples=43793
I0205 22:37:23.923934 140021163808512 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.0556381531059742, loss=0.041493579745292664
I0205 22:37:55.461292 140002205157120 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.054742373526096344, loss=0.0399799719452858
I0205 22:38:28.294679 140021163808512 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.02856994979083538, loss=0.037303876131772995
I0205 22:39:00.645515 140002205157120 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.02490304969251156, loss=0.036281753331422806
I0205 22:39:32.779079 140021163808512 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.029081810265779495, loss=0.033635154366493225
I0205 22:40:05.120118 140002205157120 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.02489229291677475, loss=0.03881007060408592
I0205 22:40:36.781320 140021163808512 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.06561888009309769, loss=0.039995431900024414
I0205 22:41:04.462847 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:42:42.849478 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:42:46.243321 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:42:49.577974 140205209478976 submission_runner.py:408] Time since start: 9137.30s, 	Step: 19487, 	{'train/accuracy': 0.9886627197265625, 'train/loss': 0.038342516869306564, 'train/mean_average_precision': 0.2338148047161424, 'validation/accuracy': 0.9854279160499573, 'validation/loss': 0.049056440591812134, 'validation/mean_average_precision': 0.18009676570737515, 'validation/num_examples': 43793, 'test/accuracy': 0.9846385717391968, 'test/loss': 0.051957037299871445, 'test/mean_average_precision': 0.17793733536948328, 'test/num_examples': 43793, 'score': 6255.089984178543, 'total_duration': 9137.301024913788, 'accumulated_submission_time': 6255.089984178543, 'accumulated_eval_time': 2880.9350163936615, 'accumulated_logging_time': 0.7496731281280518}
I0205 22:42:49.597982 140002598295296 logging_writer.py:48] [19487] accumulated_eval_time=2880.935016, accumulated_logging_time=0.749673, accumulated_submission_time=6255.089984, global_step=19487, preemption_count=0, score=6255.089984, test/accuracy=0.984639, test/loss=0.051957, test/mean_average_precision=0.177937, test/num_examples=43793, total_duration=9137.301025, train/accuracy=0.988663, train/loss=0.038343, train/mean_average_precision=0.233815, validation/accuracy=0.985428, validation/loss=0.049056, validation/mean_average_precision=0.180097, validation/num_examples=43793
I0205 22:42:54.174107 140020770670336 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.03962329775094986, loss=0.04162171855568886
I0205 22:43:26.729430 140002598295296 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.03888830542564392, loss=0.03383168578147888
I0205 22:43:59.154084 140020770670336 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.04426909610629082, loss=0.0357770174741745
I0205 22:44:31.656976 140002598295296 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.07786960899829865, loss=0.0379050150513649
I0205 22:45:04.039043 140020770670336 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.04434175044298172, loss=0.03833627700805664
I0205 22:45:36.225677 140002598295296 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.03063780441880226, loss=0.03972814232110977
I0205 22:46:08.369631 140020770670336 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.057162053883075714, loss=0.03612012788653374
I0205 22:46:40.309706 140002598295296 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.057227447628974915, loss=0.04160512611269951
I0205 22:46:49.821713 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:48:32.691404 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:48:35.740842 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:48:38.727524 140205209478976 submission_runner.py:408] Time since start: 9486.45s, 	Step: 20231, 	{'train/accuracy': 0.988877534866333, 'train/loss': 0.03777388483285904, 'train/mean_average_precision': 0.22414921413911615, 'validation/accuracy': 0.985895574092865, 'validation/loss': 0.048128772526979446, 'validation/mean_average_precision': 0.19038625336157286, 'validation/num_examples': 43793, 'test/accuracy': 0.9849616289138794, 'test/loss': 0.05130887031555176, 'test/mean_average_precision': 0.18830933830428093, 'test/num_examples': 43793, 'score': 6495.277824878693, 'total_duration': 9486.45058965683, 'accumulated_submission_time': 6495.277824878693, 'accumulated_eval_time': 2989.8407900333405, 'accumulated_logging_time': 0.7810525894165039}
I0205 22:48:38.748193 140002205157120 logging_writer.py:48] [20231] accumulated_eval_time=2989.840790, accumulated_logging_time=0.781053, accumulated_submission_time=6495.277825, global_step=20231, preemption_count=0, score=6495.277825, test/accuracy=0.984962, test/loss=0.051309, test/mean_average_precision=0.188309, test/num_examples=43793, total_duration=9486.450590, train/accuracy=0.988878, train/loss=0.037774, train/mean_average_precision=0.224149, validation/accuracy=0.985896, validation/loss=0.048129, validation/mean_average_precision=0.190386, validation/num_examples=43793
I0205 22:49:01.423912 140021163808512 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.030091963708400726, loss=0.03835064172744751
I0205 22:49:33.161149 140002205157120 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.05490428954362869, loss=0.0421450175344944
I0205 22:50:04.665968 140021163808512 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.03567492589354515, loss=0.03633442148566246
I0205 22:50:36.384901 140002205157120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.10272891074419022, loss=0.037723954766988754
I0205 22:51:07.967218 140021163808512 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.03030218929052353, loss=0.03911830857396126
I0205 22:51:39.698925 140002205157120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.039916448295116425, loss=0.036827124655246735
I0205 22:52:11.107741 140021163808512 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.02327348291873932, loss=0.033661022782325745
I0205 22:52:38.761399 140205209478976 spec.py:321] Evaluating on the training split.
I0205 22:54:19.270153 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 22:54:22.311335 140205209478976 spec.py:349] Evaluating on the test split.
I0205 22:54:25.262778 140205209478976 submission_runner.py:408] Time since start: 9832.99s, 	Step: 20989, 	{'train/accuracy': 0.9890268445014954, 'train/loss': 0.037681374698877335, 'train/mean_average_precision': 0.2302380819582901, 'validation/accuracy': 0.9859223365783691, 'validation/loss': 0.04777001217007637, 'validation/mean_average_precision': 0.19523066530024943, 'validation/num_examples': 43793, 'test/accuracy': 0.9849637150764465, 'test/loss': 0.05084994435310364, 'test/mean_average_precision': 0.19295053350812078, 'test/num_examples': 43793, 'score': 6735.258136510849, 'total_duration': 9832.98583817482, 'accumulated_submission_time': 6735.258136510849, 'accumulated_eval_time': 3096.3421173095703, 'accumulated_logging_time': 0.8141474723815918}
I0205 22:54:25.282810 140020770670336 logging_writer.py:48] [20989] accumulated_eval_time=3096.342117, accumulated_logging_time=0.814147, accumulated_submission_time=6735.258137, global_step=20989, preemption_count=0, score=6735.258137, test/accuracy=0.984964, test/loss=0.050850, test/mean_average_precision=0.192951, test/num_examples=43793, total_duration=9832.985838, train/accuracy=0.989027, train/loss=0.037681, train/mean_average_precision=0.230238, validation/accuracy=0.985922, validation/loss=0.047770, validation/mean_average_precision=0.195231, validation/num_examples=43793
I0205 22:54:29.076434 140044334864128 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.029783932492136955, loss=0.03939082473516464
I0205 22:55:00.949500 140020770670336 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.044160403311252594, loss=0.04110519587993622
I0205 22:55:32.514491 140044334864128 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.02873249724507332, loss=0.03775809705257416
I0205 22:56:04.611066 140020770670336 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.04818836972117424, loss=0.03627719357609749
I0205 22:56:36.562895 140044334864128 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.05505331978201866, loss=0.03915062174201012
I0205 22:57:08.475589 140020770670336 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.026424147188663483, loss=0.03724706545472145
I0205 22:57:40.187581 140044334864128 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.03497563302516937, loss=0.03740015625953674
I0205 22:58:12.300201 140020770670336 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.04560786485671997, loss=0.035815704613924026
I0205 22:58:25.502204 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:00:07.323395 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:00:10.402332 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:00:13.368716 140205209478976 submission_runner.py:408] Time since start: 10181.09s, 	Step: 21742, 	{'train/accuracy': 0.9889109134674072, 'train/loss': 0.03795648366212845, 'train/mean_average_precision': 0.228778463186817, 'validation/accuracy': 0.9857218265533447, 'validation/loss': 0.04850354790687561, 'validation/mean_average_precision': 0.18899655215754246, 'validation/num_examples': 43793, 'test/accuracy': 0.9847792387008667, 'test/loss': 0.051432929933071136, 'test/mean_average_precision': 0.1829139607213623, 'test/num_examples': 43793, 'score': 6975.445188045502, 'total_duration': 10181.09177494049, 'accumulated_submission_time': 6975.445188045502, 'accumulated_eval_time': 3204.208570957184, 'accumulated_logging_time': 0.8466379642486572}
I0205 23:00:13.388526 140002205157120 logging_writer.py:48] [21742] accumulated_eval_time=3204.208571, accumulated_logging_time=0.846638, accumulated_submission_time=6975.445188, global_step=21742, preemption_count=0, score=6975.445188, test/accuracy=0.984779, test/loss=0.051433, test/mean_average_precision=0.182914, test/num_examples=43793, total_duration=10181.091775, train/accuracy=0.988911, train/loss=0.037956, train/mean_average_precision=0.228778, validation/accuracy=0.985722, validation/loss=0.048504, validation/mean_average_precision=0.188997, validation/num_examples=43793
I0205 23:00:32.371254 140002598295296 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.046829383820295334, loss=0.03710348904132843
I0205 23:01:04.318599 140002205157120 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.029358241707086563, loss=0.03620053082704544
I0205 23:01:36.188247 140002598295296 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.03760220855474472, loss=0.03519275784492493
I0205 23:02:08.086711 140002205157120 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.02905367501080036, loss=0.03954816609621048
I0205 23:02:40.515057 140002598295296 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.03357746824622154, loss=0.0422290600836277
I0205 23:03:14.668130 140002205157120 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.031082352623343468, loss=0.037663377821445465
I0205 23:03:47.889049 140002598295296 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.09377913177013397, loss=0.03906655311584473
I0205 23:04:13.607299 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:05:53.254499 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:05:56.308839 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:05:59.312248 140205209478976 submission_runner.py:408] Time since start: 10527.04s, 	Step: 22481, 	{'train/accuracy': 0.9888846278190613, 'train/loss': 0.0379314087331295, 'train/mean_average_precision': 0.2274528755384141, 'validation/accuracy': 0.9856629371643066, 'validation/loss': 0.048899173736572266, 'validation/mean_average_precision': 0.18542973380631414, 'validation/num_examples': 43793, 'test/accuracy': 0.984722375869751, 'test/loss': 0.05183769017457962, 'test/mean_average_precision': 0.18563354908539184, 'test/num_examples': 43793, 'score': 7215.630878448486, 'total_duration': 10527.035320281982, 'accumulated_submission_time': 7215.630878448486, 'accumulated_eval_time': 3309.9134817123413, 'accumulated_logging_time': 0.8789269924163818}
I0205 23:05:59.331781 140021163808512 logging_writer.py:48] [22481] accumulated_eval_time=3309.913482, accumulated_logging_time=0.878927, accumulated_submission_time=7215.630878, global_step=22481, preemption_count=0, score=7215.630878, test/accuracy=0.984722, test/loss=0.051838, test/mean_average_precision=0.185634, test/num_examples=43793, total_duration=10527.035320, train/accuracy=0.988885, train/loss=0.037931, train/mean_average_precision=0.227453, validation/accuracy=0.985663, validation/loss=0.048899, validation/mean_average_precision=0.185430, validation/num_examples=43793
I0205 23:06:05.784043 140044334864128 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.04068385809659958, loss=0.041549455374479294
I0205 23:06:37.639202 140021163808512 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.09415896236896515, loss=0.0391567163169384
I0205 23:07:09.265354 140044334864128 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.07010933011770248, loss=0.03796377405524254
I0205 23:07:41.178993 140021163808512 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.03343324735760689, loss=0.04031466692686081
I0205 23:08:13.439706 140044334864128 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.03976860269904137, loss=0.03498650714755058
I0205 23:08:45.658116 140021163808512 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.06953233480453491, loss=0.04056267440319061
I0205 23:09:17.541416 140044334864128 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.061531636863946915, loss=0.03574948012828827
I0205 23:09:49.635879 140021163808512 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.05293763801455498, loss=0.04341176152229309
I0205 23:09:59.535815 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:11:38.103456 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:11:41.144931 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:11:44.145931 140205209478976 submission_runner.py:408] Time since start: 10871.87s, 	Step: 23233, 	{'train/accuracy': 0.9888824224472046, 'train/loss': 0.03774528577923775, 'train/mean_average_precision': 0.23477738939757817, 'validation/accuracy': 0.9858500957489014, 'validation/loss': 0.04834772273898125, 'validation/mean_average_precision': 0.19096310950721257, 'validation/num_examples': 43793, 'test/accuracy': 0.9848858118057251, 'test/loss': 0.05156517028808594, 'test/mean_average_precision': 0.18543619627329147, 'test/num_examples': 43793, 'score': 7455.803935050964, 'total_duration': 10871.868999242783, 'accumulated_submission_time': 7455.803935050964, 'accumulated_eval_time': 3414.523555278778, 'accumulated_logging_time': 0.9095168113708496}
I0205 23:11:44.165541 140002205157120 logging_writer.py:48] [23233] accumulated_eval_time=3414.523555, accumulated_logging_time=0.909517, accumulated_submission_time=7455.803935, global_step=23233, preemption_count=0, score=7455.803935, test/accuracy=0.984886, test/loss=0.051565, test/mean_average_precision=0.185436, test/num_examples=43793, total_duration=10871.868999, train/accuracy=0.988882, train/loss=0.037745, train/mean_average_precision=0.234777, validation/accuracy=0.985850, validation/loss=0.048348, validation/mean_average_precision=0.190963, validation/num_examples=43793
I0205 23:12:06.732431 140002598295296 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.0513184480369091, loss=0.03939634934067726
I0205 23:12:38.167431 140002205157120 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.04571932554244995, loss=0.035778481513261795
I0205 23:13:09.827360 140002598295296 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.051016680896282196, loss=0.03473462536931038
I0205 23:13:41.551203 140002205157120 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.03660402446985245, loss=0.036357082426548004
I0205 23:14:13.424193 140002598295296 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.03463141992688179, loss=0.03653668239712715
I0205 23:14:45.695649 140002205157120 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.04055756703019142, loss=0.041453637182712555
I0205 23:15:17.771321 140002598295296 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.08896415680646896, loss=0.03664097934961319
I0205 23:15:44.228524 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:17:21.652044 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:17:24.669123 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:17:27.650210 140205209478976 submission_runner.py:408] Time since start: 11215.37s, 	Step: 23985, 	{'train/accuracy': 0.9890407919883728, 'train/loss': 0.0375085286796093, 'train/mean_average_precision': 0.23146811830159914, 'validation/accuracy': 0.9859633445739746, 'validation/loss': 0.04745250940322876, 'validation/mean_average_precision': 0.19063053395112928, 'validation/num_examples': 43793, 'test/accuracy': 0.9850140810012817, 'test/loss': 0.050241898745298386, 'test/mean_average_precision': 0.18285316336152527, 'test/num_examples': 43793, 'score': 7695.835782766342, 'total_duration': 11215.373279094696, 'accumulated_submission_time': 7695.835782766342, 'accumulated_eval_time': 3517.945199251175, 'accumulated_logging_time': 0.9403893947601318}
I0205 23:17:27.670246 140020770670336 logging_writer.py:48] [23985] accumulated_eval_time=3517.945199, accumulated_logging_time=0.940389, accumulated_submission_time=7695.835783, global_step=23985, preemption_count=0, score=7695.835783, test/accuracy=0.985014, test/loss=0.050242, test/mean_average_precision=0.182853, test/num_examples=43793, total_duration=11215.373279, train/accuracy=0.989041, train/loss=0.037509, train/mean_average_precision=0.231468, validation/accuracy=0.985963, validation/loss=0.047453, validation/mean_average_precision=0.190631, validation/num_examples=43793
I0205 23:17:32.771290 140044334864128 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.05806932970881462, loss=0.03673087805509567
I0205 23:18:04.321275 140020770670336 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.11879283934831619, loss=0.045959025621414185
I0205 23:18:36.125809 140044334864128 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.03578567877411842, loss=0.03560350090265274
I0205 23:19:08.260947 140020770670336 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.03408293053507805, loss=0.03464435786008835
I0205 23:19:39.899692 140044334864128 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.03600779548287392, loss=0.04045417904853821
I0205 23:20:11.913468 140020770670336 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.03538838028907776, loss=0.03661973774433136
I0205 23:20:43.475293 140044334864128 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.034105025231838226, loss=0.03533794358372688
I0205 23:21:15.246099 140020770670336 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.03618830442428589, loss=0.03712077811360359
I0205 23:21:27.937220 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:23:09.513996 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:23:12.585358 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:23:15.510277 140205209478976 submission_runner.py:408] Time since start: 11563.23s, 	Step: 24741, 	{'train/accuracy': 0.9891539812088013, 'train/loss': 0.03674052655696869, 'train/mean_average_precision': 0.2390435111815593, 'validation/accuracy': 0.9858971834182739, 'validation/loss': 0.04799893498420715, 'validation/mean_average_precision': 0.1873103703045062, 'validation/num_examples': 43793, 'test/accuracy': 0.9850648045539856, 'test/loss': 0.05095173418521881, 'test/mean_average_precision': 0.19273518886644886, 'test/num_examples': 43793, 'score': 7936.069957971573, 'total_duration': 11563.233343362808, 'accumulated_submission_time': 7936.069957971573, 'accumulated_eval_time': 3625.518209695816, 'accumulated_logging_time': 0.9731786251068115}
I0205 23:23:15.530115 140002205157120 logging_writer.py:48] [24741] accumulated_eval_time=3625.518210, accumulated_logging_time=0.973179, accumulated_submission_time=7936.069958, global_step=24741, preemption_count=0, score=7936.069958, test/accuracy=0.985065, test/loss=0.050952, test/mean_average_precision=0.192735, test/num_examples=43793, total_duration=11563.233343, train/accuracy=0.989154, train/loss=0.036741, train/mean_average_precision=0.239044, validation/accuracy=0.985897, validation/loss=0.047999, validation/mean_average_precision=0.187310, validation/num_examples=43793
I0205 23:23:35.370259 140021163808512 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.028390968218445778, loss=0.04008523374795914
I0205 23:24:07.169961 140002205157120 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.08467774838209152, loss=0.039550311863422394
I0205 23:24:38.975612 140021163808512 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.05511036515235901, loss=0.03678111359477043
I0205 23:25:11.064136 140002205157120 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.06115901470184326, loss=0.042594265192747116
I0205 23:25:43.251320 140021163808512 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.03291521221399307, loss=0.03517360985279083
I0205 23:26:15.176499 140002205157120 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.0334215946495533, loss=0.03792475163936615
I0205 23:26:47.275334 140021163808512 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.03315272182226181, loss=0.0394032746553421
I0205 23:27:15.749800 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:28:54.793596 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:28:57.874883 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:29:00.835754 140205209478976 submission_runner.py:408] Time since start: 11908.56s, 	Step: 25490, 	{'train/accuracy': 0.9891579151153564, 'train/loss': 0.03681539371609688, 'train/mean_average_precision': 0.2493061489164509, 'validation/accuracy': 0.9859349131584167, 'validation/loss': 0.04808002710342407, 'validation/mean_average_precision': 0.1947319230197419, 'validation/num_examples': 43793, 'test/accuracy': 0.9850631356239319, 'test/loss': 0.05122765153646469, 'test/mean_average_precision': 0.18459982964451715, 'test/num_examples': 43793, 'score': 8175.831615447998, 'total_duration': 11908.558824062347, 'accumulated_submission_time': 8175.831615447998, 'accumulated_eval_time': 3730.604122877121, 'accumulated_logging_time': 1.4308359622955322}
I0205 23:29:00.856136 140002598295296 logging_writer.py:48] [25490] accumulated_eval_time=3730.604123, accumulated_logging_time=1.430836, accumulated_submission_time=8175.831615, global_step=25490, preemption_count=0, score=8175.831615, test/accuracy=0.985063, test/loss=0.051228, test/mean_average_precision=0.184600, test/num_examples=43793, total_duration=11908.558824, train/accuracy=0.989158, train/loss=0.036815, train/mean_average_precision=0.249306, validation/accuracy=0.985935, validation/loss=0.048080, validation/mean_average_precision=0.194732, validation/num_examples=43793
I0205 23:29:04.602034 140044334864128 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.04917507246136665, loss=0.03789854422211647
I0205 23:29:36.501267 140002598295296 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.03495895117521286, loss=0.035420823842287064
I0205 23:30:08.294330 140044334864128 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.04184018075466156, loss=0.03594110533595085
I0205 23:30:40.249054 140002598295296 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.02758822962641716, loss=0.038200169801712036
I0205 23:31:11.759989 140044334864128 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.03013550490140915, loss=0.03825635462999344
I0205 23:31:43.427000 140002598295296 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.042031727731227875, loss=0.038373854011297226
I0205 23:32:15.411794 140044334864128 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.04294634982943535, loss=0.03905647248029709
I0205 23:32:47.230676 140002598295296 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.06257728487253189, loss=0.04335935041308403
I0205 23:33:00.921321 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:34:41.686459 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:34:44.739990 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:34:47.774045 140205209478976 submission_runner.py:408] Time since start: 12255.50s, 	Step: 26244, 	{'train/accuracy': 0.9890082478523254, 'train/loss': 0.037123121321201324, 'train/mean_average_precision': 0.26194704652344725, 'validation/accuracy': 0.9857433438301086, 'validation/loss': 0.048029348254203796, 'validation/mean_average_precision': 0.19817879739783534, 'validation/num_examples': 43793, 'test/accuracy': 0.9848095774650574, 'test/loss': 0.051177769899368286, 'test/mean_average_precision': 0.1883528025284292, 'test/num_examples': 43793, 'score': 8415.865903615952, 'total_duration': 12255.497112989426, 'accumulated_submission_time': 8415.865903615952, 'accumulated_eval_time': 3837.456799507141, 'accumulated_logging_time': 1.4622154235839844}
I0205 23:34:47.793994 140002205157120 logging_writer.py:48] [26244] accumulated_eval_time=3837.456800, accumulated_logging_time=1.462215, accumulated_submission_time=8415.865904, global_step=26244, preemption_count=0, score=8415.865904, test/accuracy=0.984810, test/loss=0.051178, test/mean_average_precision=0.188353, test/num_examples=43793, total_duration=12255.497113, train/accuracy=0.989008, train/loss=0.037123, train/mean_average_precision=0.261947, validation/accuracy=0.985743, validation/loss=0.048029, validation/mean_average_precision=0.198179, validation/num_examples=43793
I0205 23:35:06.593925 140021163808512 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.06687229871749878, loss=0.03610430657863617
I0205 23:35:38.386596 140002205157120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.04488290846347809, loss=0.03929484263062477
I0205 23:36:10.413855 140021163808512 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.028587769716978073, loss=0.03543481230735779
I0205 23:36:41.871378 140002205157120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.03974989056587219, loss=0.03332015872001648
I0205 23:37:13.299031 140021163808512 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.02571875788271427, loss=0.036174703389406204
I0205 23:37:44.749820 140002205157120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.04644421115517616, loss=0.035317856818437576
I0205 23:38:16.326186 140021163808512 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.06123057380318642, loss=0.03909501060843468
I0205 23:38:47.784152 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:40:23.905653 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:40:27.077073 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:40:30.177384 140205209478976 submission_runner.py:408] Time since start: 12597.90s, 	Step: 27000, 	{'train/accuracy': 0.9892248511314392, 'train/loss': 0.036412276327610016, 'train/mean_average_precision': 0.24881678190256606, 'validation/accuracy': 0.9859369397163391, 'validation/loss': 0.047542754560709, 'validation/mean_average_precision': 0.18955104946096324, 'validation/num_examples': 43793, 'test/accuracy': 0.985078752040863, 'test/loss': 0.050518136471509933, 'test/mean_average_precision': 0.18626111746709237, 'test/num_examples': 43793, 'score': 8655.825356006622, 'total_duration': 12597.900453329086, 'accumulated_submission_time': 8655.825356006622, 'accumulated_eval_time': 3939.849988222122, 'accumulated_logging_time': 1.493131399154663}
I0205 23:40:30.197575 140002598295296 logging_writer.py:48] [27000] accumulated_eval_time=3939.849988, accumulated_logging_time=1.493131, accumulated_submission_time=8655.825356, global_step=27000, preemption_count=0, score=8655.825356, test/accuracy=0.985079, test/loss=0.050518, test/mean_average_precision=0.186261, test/num_examples=43793, total_duration=12597.900453, train/accuracy=0.989225, train/loss=0.036412, train/mean_average_precision=0.248817, validation/accuracy=0.985937, validation/loss=0.047543, validation/mean_average_precision=0.189551, validation/num_examples=43793
I0205 23:40:30.535381 140020770670336 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.05199260264635086, loss=0.03926262632012367
I0205 23:41:02.549829 140002598295296 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.039471421390771866, loss=0.03549042344093323
I0205 23:41:34.513795 140020770670336 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.05160596966743469, loss=0.040919229388237
I0205 23:42:06.810694 140002598295296 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.05167611688375473, loss=0.034746404737234116
I0205 23:42:38.266367 140020770670336 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.025440886616706848, loss=0.03527617081999779
I0205 23:43:10.302607 140002598295296 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.0337996669113636, loss=0.03413662686944008
I0205 23:43:42.304610 140020770670336 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.04360608756542206, loss=0.03833264857530594
I0205 23:44:14.723417 140002598295296 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.05324968323111534, loss=0.0398499071598053
I0205 23:44:30.192375 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:46:07.776243 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:46:10.886656 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:46:13.956970 140205209478976 submission_runner.py:408] Time since start: 12941.68s, 	Step: 27749, 	{'train/accuracy': 0.9891148209571838, 'train/loss': 0.036856137216091156, 'train/mean_average_precision': 0.24980815274093432, 'validation/accuracy': 0.9860550761222839, 'validation/loss': 0.04757341742515564, 'validation/mean_average_precision': 0.2024660222425728, 'validation/num_examples': 43793, 'test/accuracy': 0.985129714012146, 'test/loss': 0.05053034424781799, 'test/mean_average_precision': 0.1984891361260043, 'test/num_examples': 43793, 'score': 8895.789571285248, 'total_duration': 12941.680038928986, 'accumulated_submission_time': 8895.789571285248, 'accumulated_eval_time': 4043.614537715912, 'accumulated_logging_time': 1.5242531299591064}
I0205 23:46:13.977474 140002205157120 logging_writer.py:48] [27749] accumulated_eval_time=4043.614538, accumulated_logging_time=1.524253, accumulated_submission_time=8895.789571, global_step=27749, preemption_count=0, score=8895.789571, test/accuracy=0.985130, test/loss=0.050530, test/mean_average_precision=0.198489, test/num_examples=43793, total_duration=12941.680039, train/accuracy=0.989115, train/loss=0.036856, train/mean_average_precision=0.249808, validation/accuracy=0.986055, validation/loss=0.047573, validation/mean_average_precision=0.202466, validation/num_examples=43793
I0205 23:46:30.532865 140044334864128 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.058168090879917145, loss=0.03822708874940872
I0205 23:47:02.403830 140002205157120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.032781727612018585, loss=0.03771591559052467
I0205 23:47:34.385355 140044334864128 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.0951228141784668, loss=0.03787357732653618
I0205 23:48:06.152114 140002205157120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.03249441832304001, loss=0.03646700456738472
I0205 23:48:38.043470 140044334864128 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.05588233843445778, loss=0.04095674678683281
I0205 23:49:09.911710 140002205157120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.0557372011244297, loss=0.0348343551158905
I0205 23:49:42.411432 140044334864128 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.030291849747300148, loss=0.034739892929792404
I0205 23:50:14.238875 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:51:54.166710 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:51:57.269113 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:52:00.366055 140205209478976 submission_runner.py:408] Time since start: 13288.09s, 	Step: 28500, 	{'train/accuracy': 0.9891347885131836, 'train/loss': 0.03732499107718468, 'train/mean_average_precision': 0.22630107043646894, 'validation/accuracy': 0.9859467148780823, 'validation/loss': 0.04806974530220032, 'validation/mean_average_precision': 0.19213174303977623, 'validation/num_examples': 43793, 'test/accuracy': 0.9850189089775085, 'test/loss': 0.05103142559528351, 'test/mean_average_precision': 0.19004555406199725, 'test/num_examples': 43793, 'score': 9136.02029132843, 'total_duration': 13288.089123249054, 'accumulated_submission_time': 9136.02029132843, 'accumulated_eval_time': 4149.741677761078, 'accumulated_logging_time': 1.5556812286376953}
I0205 23:52:00.387602 140020770670336 logging_writer.py:48] [28500] accumulated_eval_time=4149.741678, accumulated_logging_time=1.555681, accumulated_submission_time=9136.020291, global_step=28500, preemption_count=0, score=9136.020291, test/accuracy=0.985019, test/loss=0.051031, test/mean_average_precision=0.190046, test/num_examples=43793, total_duration=13288.089123, train/accuracy=0.989135, train/loss=0.037325, train/mean_average_precision=0.226301, validation/accuracy=0.985947, validation/loss=0.048070, validation/mean_average_precision=0.192132, validation/num_examples=43793
I0205 23:52:00.765763 140021163808512 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.0377001091837883, loss=0.03745867684483528
I0205 23:52:33.488984 140020770670336 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.03400510177016258, loss=0.03549347072839737
I0205 23:53:05.803253 140021163808512 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.03573479875922203, loss=0.03629115968942642
I0205 23:53:37.513744 140020770670336 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.028968434780836105, loss=0.03799479827284813
I0205 23:54:09.459073 140021163808512 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.040519557893276215, loss=0.036911264061927795
I0205 23:54:41.248065 140020770670336 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.06348343938589096, loss=0.03912286087870598
I0205 23:55:13.251268 140021163808512 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.07346244156360626, loss=0.035762593150138855
I0205 23:55:45.001908 140020770670336 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.06256511062383652, loss=0.03166353330016136
I0205 23:56:00.648846 140205209478976 spec.py:321] Evaluating on the training split.
I0205 23:57:40.764415 140205209478976 spec.py:333] Evaluating on the validation split.
I0205 23:57:43.924834 140205209478976 spec.py:349] Evaluating on the test split.
I0205 23:57:46.948492 140205209478976 submission_runner.py:408] Time since start: 13634.67s, 	Step: 29250, 	{'train/accuracy': 0.9891011714935303, 'train/loss': 0.03728984668850899, 'train/mean_average_precision': 0.2516501756761238, 'validation/accuracy': 0.9858760833740234, 'validation/loss': 0.04750604182481766, 'validation/mean_average_precision': 0.19352562980530316, 'validation/num_examples': 43793, 'test/accuracy': 0.9850140810012817, 'test/loss': 0.05021721124649048, 'test/mean_average_precision': 0.18760906856632956, 'test/num_examples': 43793, 'score': 9376.250892162323, 'total_duration': 13634.671559333801, 'accumulated_submission_time': 9376.250892162323, 'accumulated_eval_time': 4256.041279792786, 'accumulated_logging_time': 1.5881264209747314}
I0205 23:57:46.969568 140002598295296 logging_writer.py:48] [29250] accumulated_eval_time=4256.041280, accumulated_logging_time=1.588126, accumulated_submission_time=9376.250892, global_step=29250, preemption_count=0, score=9376.250892, test/accuracy=0.985014, test/loss=0.050217, test/mean_average_precision=0.187609, test/num_examples=43793, total_duration=13634.671559, train/accuracy=0.989101, train/loss=0.037290, train/mean_average_precision=0.251650, validation/accuracy=0.985876, validation/loss=0.047506, validation/mean_average_precision=0.193526, validation/num_examples=43793
I0205 23:58:03.383114 140044334864128 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.050793491303920746, loss=0.038315389305353165
I0205 23:58:34.966322 140002598295296 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.037605807185173035, loss=0.037231650203466415
I0205 23:59:06.985975 140044334864128 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.052204735577106476, loss=0.041809387505054474
I0205 23:59:38.647661 140002598295296 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.067051462829113, loss=0.03671298921108246
I0206 00:00:10.803541 140044334864128 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.05064984783530235, loss=0.03630806878209114
I0206 00:00:42.623024 140002598295296 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.02240685559809208, loss=0.03422580659389496
I0206 00:01:14.504655 140044334864128 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.026952186599373817, loss=0.03714059665799141
I0206 00:01:46.284979 140002598295296 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.035834260284900665, loss=0.04014340043067932
I0206 00:01:47.249373 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:03:29.972396 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:03:32.996440 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:03:35.978615 140205209478976 submission_runner.py:408] Time since start: 13983.70s, 	Step: 30004, 	{'train/accuracy': 0.9890967011451721, 'train/loss': 0.036956485360860825, 'train/mean_average_precision': 0.23889701222262977, 'validation/accuracy': 0.9860494136810303, 'validation/loss': 0.04769286513328552, 'validation/mean_average_precision': 0.2032375966967203, 'validation/num_examples': 43793, 'test/accuracy': 0.9850319623947144, 'test/loss': 0.05084433779120445, 'test/mean_average_precision': 0.19017876820762847, 'test/num_examples': 43793, 'score': 9616.499011993408, 'total_duration': 13983.701684236526, 'accumulated_submission_time': 9616.499011993408, 'accumulated_eval_time': 4364.7704746723175, 'accumulated_logging_time': 1.6209070682525635}
I0206 00:03:35.999549 140002205157120 logging_writer.py:48] [30004] accumulated_eval_time=4364.770475, accumulated_logging_time=1.620907, accumulated_submission_time=9616.499012, global_step=30004, preemption_count=0, score=9616.499012, test/accuracy=0.985032, test/loss=0.050844, test/mean_average_precision=0.190179, test/num_examples=43793, total_duration=13983.701684, train/accuracy=0.989097, train/loss=0.036956, train/mean_average_precision=0.238897, validation/accuracy=0.986049, validation/loss=0.047693, validation/mean_average_precision=0.203238, validation/num_examples=43793
I0206 00:04:06.938297 140021163808512 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.1578674465417862, loss=0.040727607905864716
I0206 00:04:39.194306 140002205157120 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.07819922268390656, loss=0.038845013827085495
I0206 00:05:11.477437 140021163808512 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.055073924362659454, loss=0.04142458736896515
I0206 00:05:43.162938 140002205157120 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.07280237227678299, loss=0.034767139703035355
I0206 00:06:15.545489 140021163808512 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.0337069109082222, loss=0.032593995332717896
I0206 00:06:47.316560 140002205157120 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.063758485019207, loss=0.037746261805295944
I0206 00:07:20.148912 140021163808512 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.07157419621944427, loss=0.03917546197772026
I0206 00:07:36.194707 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:09:17.499191 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:09:20.910342 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:09:24.189934 140205209478976 submission_runner.py:408] Time since start: 14331.91s, 	Step: 30751, 	{'train/accuracy': 0.9891952872276306, 'train/loss': 0.03677273169159889, 'train/mean_average_precision': 0.24683130385525454, 'validation/accuracy': 0.9859515428543091, 'validation/loss': 0.04743628576397896, 'validation/mean_average_precision': 0.20348333767901491, 'validation/num_examples': 43793, 'test/accuracy': 0.9849410057067871, 'test/loss': 0.05041816085577011, 'test/mean_average_precision': 0.1896190516759259, 'test/num_examples': 43793, 'score': 9856.662395954132, 'total_duration': 14331.912987470627, 'accumulated_submission_time': 9856.662395954132, 'accumulated_eval_time': 4472.7656536102295, 'accumulated_logging_time': 1.652916431427002}
I0206 00:09:24.215265 140002598295296 logging_writer.py:48] [30751] accumulated_eval_time=4472.765654, accumulated_logging_time=1.652916, accumulated_submission_time=9856.662396, global_step=30751, preemption_count=0, score=9856.662396, test/accuracy=0.984941, test/loss=0.050418, test/mean_average_precision=0.189619, test/num_examples=43793, total_duration=14331.912987, train/accuracy=0.989195, train/loss=0.036773, train/mean_average_precision=0.246831, validation/accuracy=0.985952, validation/loss=0.047436, validation/mean_average_precision=0.203483, validation/num_examples=43793
I0206 00:09:40.575773 140044334864128 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05151334032416344, loss=0.03540549799799919
I0206 00:10:13.206270 140002598295296 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.02948959358036518, loss=0.033449869602918625
I0206 00:10:45.543906 140044334864128 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.036217428743839264, loss=0.03703465312719345
I0206 00:11:18.351242 140002598295296 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.09605678915977478, loss=0.033820055425167084
I0206 00:11:51.082766 140044334864128 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.08705487102270126, loss=0.036383144557476044
I0206 00:12:23.529721 140002598295296 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.0483117401599884, loss=0.04071424901485443
I0206 00:12:55.865583 140044334864128 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.0367814302444458, loss=0.03610435128211975
I0206 00:13:24.504368 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:15:09.404424 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:15:12.481946 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:15:15.529543 140205209478976 submission_runner.py:408] Time since start: 14683.25s, 	Step: 31490, 	{'train/accuracy': 0.9892831444740295, 'train/loss': 0.0365368127822876, 'train/mean_average_precision': 0.2541385304879885, 'validation/accuracy': 0.9860900044441223, 'validation/loss': 0.04710783809423447, 'validation/mean_average_precision': 0.20120274318131334, 'validation/num_examples': 43793, 'test/accuracy': 0.9851625561714172, 'test/loss': 0.050069410353899, 'test/mean_average_precision': 0.19461144235046873, 'test/num_examples': 43793, 'score': 10096.915367603302, 'total_duration': 14683.252610206604, 'accumulated_submission_time': 10096.915367603302, 'accumulated_eval_time': 4583.790787220001, 'accumulated_logging_time': 1.6917784214019775}
I0206 00:15:15.552727 140002205157120 logging_writer.py:48] [31490] accumulated_eval_time=4583.790787, accumulated_logging_time=1.691778, accumulated_submission_time=10096.915368, global_step=31490, preemption_count=0, score=10096.915368, test/accuracy=0.985163, test/loss=0.050069, test/mean_average_precision=0.194611, test/num_examples=43793, total_duration=14683.252610, train/accuracy=0.989283, train/loss=0.036537, train/mean_average_precision=0.254139, validation/accuracy=0.986090, validation/loss=0.047108, validation/mean_average_precision=0.201203, validation/num_examples=43793
I0206 00:15:19.156009 140020770670336 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.038554735481739044, loss=0.037263624370098114
I0206 00:15:51.295444 140002205157120 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.048906031996011734, loss=0.0364709235727787
I0206 00:16:23.451759 140020770670336 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.04199536144733429, loss=0.03670555353164673
I0206 00:16:55.541840 140002205157120 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.05406993255019188, loss=0.03782617673277855
I0206 00:17:27.379436 140020770670336 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.04117928445339203, loss=0.03876001760363579
I0206 00:17:59.433295 140002205157120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05369550734758377, loss=0.0356936976313591
I0206 00:18:31.713928 140020770670336 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.04463320970535278, loss=0.03802899643778801
I0206 00:19:03.549090 140002205157120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.07201742380857468, loss=0.03878040611743927
I0206 00:19:15.817482 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:20:55.724287 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:20:58.785706 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:21:01.874705 140205209478976 submission_runner.py:408] Time since start: 15029.60s, 	Step: 32239, 	{'train/accuracy': 0.9892269372940063, 'train/loss': 0.03683546185493469, 'train/mean_average_precision': 0.25522288133128135, 'validation/accuracy': 0.985957682132721, 'validation/loss': 0.04722733050584793, 'validation/mean_average_precision': 0.19437412916669136, 'validation/num_examples': 43793, 'test/accuracy': 0.9850172400474548, 'test/loss': 0.05006767064332962, 'test/mean_average_precision': 0.19555223572746927, 'test/num_examples': 43793, 'score': 10337.149015903473, 'total_duration': 15029.597774505615, 'accumulated_submission_time': 10337.149015903473, 'accumulated_eval_time': 4689.847971916199, 'accumulated_logging_time': 1.7260665893554688}
I0206 00:21:01.896972 140021163808512 logging_writer.py:48] [32239] accumulated_eval_time=4689.847972, accumulated_logging_time=1.726067, accumulated_submission_time=10337.149016, global_step=32239, preemption_count=0, score=10337.149016, test/accuracy=0.985017, test/loss=0.050068, test/mean_average_precision=0.195552, test/num_examples=43793, total_duration=15029.597775, train/accuracy=0.989227, train/loss=0.036835, train/mean_average_precision=0.255223, validation/accuracy=0.985958, validation/loss=0.047227, validation/mean_average_precision=0.194374, validation/num_examples=43793
I0206 00:21:21.677558 140044334864128 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.08849814534187317, loss=0.03764503821730614
I0206 00:21:53.577724 140021163808512 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.03892491012811661, loss=0.035570766776800156
I0206 00:22:26.698767 140044334864128 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.07380540668964386, loss=0.03269198536872864
I0206 00:22:58.613739 140021163808512 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.05578962713479996, loss=0.040696047246456146
I0206 00:23:30.569511 140044334864128 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.05212404578924179, loss=0.03480510413646698
I0206 00:24:02.353763 140021163808512 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.06911088526248932, loss=0.03577942028641701
I0206 00:24:34.325197 140044334864128 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.06736775487661362, loss=0.0389237254858017
I0206 00:25:02.013029 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:26:39.053235 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:26:42.076785 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:26:45.101971 140205209478976 submission_runner.py:408] Time since start: 15372.83s, 	Step: 32988, 	{'train/accuracy': 0.9894253015518188, 'train/loss': 0.03580518439412117, 'train/mean_average_precision': 0.2698218583076517, 'validation/accuracy': 0.9860225915908813, 'validation/loss': 0.04728809744119644, 'validation/mean_average_precision': 0.20049733686495858, 'validation/num_examples': 43793, 'test/accuracy': 0.9851486682891846, 'test/loss': 0.05022474750876427, 'test/mean_average_precision': 0.19327633019568238, 'test/num_examples': 43793, 'score': 10577.234008789062, 'total_duration': 15372.825038433075, 'accumulated_submission_time': 10577.234008789062, 'accumulated_eval_time': 4792.9368715286255, 'accumulated_logging_time': 1.7592804431915283}
I0206 00:26:45.123306 140002205157120 logging_writer.py:48] [32988] accumulated_eval_time=4792.936872, accumulated_logging_time=1.759280, accumulated_submission_time=10577.234009, global_step=32988, preemption_count=0, score=10577.234009, test/accuracy=0.985149, test/loss=0.050225, test/mean_average_precision=0.193276, test/num_examples=43793, total_duration=15372.825038, train/accuracy=0.989425, train/loss=0.035805, train/mean_average_precision=0.269822, validation/accuracy=0.986023, validation/loss=0.047288, validation/mean_average_precision=0.200497, validation/num_examples=43793
I0206 00:26:49.285147 140002598295296 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.08450955897569656, loss=0.03294088691473007
I0206 00:27:21.688513 140002205157120 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.06326723098754883, loss=0.038005389273166656
I0206 00:27:54.197859 140002598295296 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.035701580345630646, loss=0.03641954064369202
I0206 00:28:26.653034 140002205157120 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.05704609304666519, loss=0.03706663101911545
I0206 00:28:59.035114 140002598295296 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.05487397313117981, loss=0.03489890694618225
I0206 00:29:31.117513 140002205157120 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.04266805201768875, loss=0.03468809649348259
I0206 00:30:03.003846 140002598295296 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.05284208804368973, loss=0.038039494305849075
I0206 00:30:34.673948 140002205157120 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.07102230936288834, loss=0.033837880939245224
I0206 00:30:45.209432 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:32:21.730041 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:32:24.822103 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:32:27.804967 140205209478976 submission_runner.py:408] Time since start: 15715.53s, 	Step: 33734, 	{'train/accuracy': 0.9893887639045715, 'train/loss': 0.035785213112831116, 'train/mean_average_precision': 0.2618129740028855, 'validation/accuracy': 0.9860676527023315, 'validation/loss': 0.04695483669638634, 'validation/mean_average_precision': 0.19759185260774403, 'validation/num_examples': 43793, 'test/accuracy': 0.9850395321846008, 'test/loss': 0.04974985867738724, 'test/mean_average_precision': 0.1964414351587808, 'test/num_examples': 43793, 'score': 10817.288613796234, 'total_duration': 15715.528035879135, 'accumulated_submission_time': 10817.288613796234, 'accumulated_eval_time': 4895.532358884811, 'accumulated_logging_time': 1.7915773391723633}
I0206 00:32:27.826928 140020770670336 logging_writer.py:48] [33734] accumulated_eval_time=4895.532359, accumulated_logging_time=1.791577, accumulated_submission_time=10817.288614, global_step=33734, preemption_count=0, score=10817.288614, test/accuracy=0.985040, test/loss=0.049750, test/mean_average_precision=0.196441, test/num_examples=43793, total_duration=15715.528036, train/accuracy=0.989389, train/loss=0.035785, train/mean_average_precision=0.261813, validation/accuracy=0.986068, validation/loss=0.046955, validation/mean_average_precision=0.197592, validation/num_examples=43793
I0206 00:32:49.528874 140021163808512 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.07687902450561523, loss=0.03923451900482178
I0206 00:33:21.913773 140020770670336 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.061814188957214355, loss=0.03862921893596649
I0206 00:33:53.789323 140021163808512 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.05284843221306801, loss=0.03763684630393982
I0206 00:34:25.883116 140020770670336 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.04775336757302284, loss=0.03970407694578171
I0206 00:34:57.702716 140021163808512 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.03577135130763054, loss=0.03549661114811897
I0206 00:35:29.939400 140020770670336 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.03613843768835068, loss=0.038018230348825455
I0206 00:36:02.025190 140021163808512 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.036136750131845474, loss=0.04034842178225517
I0206 00:36:28.026865 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:38:05.022069 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:38:08.053343 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:38:13.337270 140205209478976 submission_runner.py:408] Time since start: 16061.06s, 	Step: 34482, 	{'train/accuracy': 0.9893254041671753, 'train/loss': 0.03624964505434036, 'train/mean_average_precision': 0.2509931044289288, 'validation/accuracy': 0.9861057996749878, 'validation/loss': 0.04736443981528282, 'validation/mean_average_precision': 0.19361712981863322, 'validation/num_examples': 43793, 'test/accuracy': 0.985135555267334, 'test/loss': 0.0503055602312088, 'test/mean_average_precision': 0.18777562192605082, 'test/num_examples': 43793, 'score': 11057.456293582916, 'total_duration': 16061.060340881348, 'accumulated_submission_time': 11057.456293582916, 'accumulated_eval_time': 5000.842721700668, 'accumulated_logging_time': 1.8258166313171387}
I0206 00:38:13.359912 140002205157120 logging_writer.py:48] [34482] accumulated_eval_time=5000.842722, accumulated_logging_time=1.825817, accumulated_submission_time=11057.456294, global_step=34482, preemption_count=0, score=11057.456294, test/accuracy=0.985136, test/loss=0.050306, test/mean_average_precision=0.187776, test/num_examples=43793, total_duration=16061.060341, train/accuracy=0.989325, train/loss=0.036250, train/mean_average_precision=0.250993, validation/accuracy=0.986106, validation/loss=0.047364, validation/mean_average_precision=0.193617, validation/num_examples=43793
I0206 00:38:19.428243 140002598295296 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0709642618894577, loss=0.03581741452217102
I0206 00:38:51.114970 140002205157120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.04006402567028999, loss=0.04035837575793266
I0206 00:39:22.918398 140002598295296 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.037852831184864044, loss=0.03464619070291519
I0206 00:39:54.944696 140002205157120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.03932907059788704, loss=0.03821280226111412
I0206 00:40:26.696186 140002598295296 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.06102150306105614, loss=0.03371390327811241
I0206 00:40:58.555095 140002205157120 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.043286703526973724, loss=0.03379708155989647
I0206 00:41:30.237187 140002598295296 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.11018993705511093, loss=0.031950753182172775
I0206 00:42:02.044771 140002205157120 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.029690731316804886, loss=0.035385649651288986
I0206 00:42:13.526154 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:43:50.269305 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:43:53.301073 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:43:56.291614 140205209478976 submission_runner.py:408] Time since start: 16404.01s, 	Step: 35237, 	{'train/accuracy': 0.9891011714935303, 'train/loss': 0.036694981157779694, 'train/mean_average_precision': 0.25369619866113313, 'validation/accuracy': 0.9861447811126709, 'validation/loss': 0.047388914972543716, 'validation/mean_average_precision': 0.20233305382149266, 'validation/num_examples': 43793, 'test/accuracy': 0.9851351380348206, 'test/loss': 0.05062321946024895, 'test/mean_average_precision': 0.19302576481803932, 'test/num_examples': 43793, 'score': 11297.589622735977, 'total_duration': 16404.014665842056, 'accumulated_submission_time': 11297.589622735977, 'accumulated_eval_time': 5103.608122110367, 'accumulated_logging_time': 1.8610637187957764}
I0206 00:43:56.313542 140021163808512 logging_writer.py:48] [35237] accumulated_eval_time=5103.608122, accumulated_logging_time=1.861064, accumulated_submission_time=11297.589623, global_step=35237, preemption_count=0, score=11297.589623, test/accuracy=0.985135, test/loss=0.050623, test/mean_average_precision=0.193026, test/num_examples=43793, total_duration=16404.014666, train/accuracy=0.989101, train/loss=0.036695, train/mean_average_precision=0.253696, validation/accuracy=0.986145, validation/loss=0.047389, validation/mean_average_precision=0.202333, validation/num_examples=43793
I0206 00:44:16.740421 140044334864128 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06003225967288017, loss=0.03289594128727913
I0206 00:44:48.603727 140021163808512 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.051669590175151825, loss=0.038036756217479706
I0206 00:45:20.955463 140044334864128 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.06574130803346634, loss=0.034421008080244064
I0206 00:45:52.843300 140021163808512 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.03842952847480774, loss=0.03592899441719055
I0206 00:46:24.903149 140044334864128 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.09222155809402466, loss=0.0375838503241539
I0206 00:46:56.668760 140021163808512 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.04253097251057625, loss=0.03666204586625099
I0206 00:47:28.730440 140044334864128 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.055043209344148636, loss=0.03497207909822464
I0206 00:47:56.481770 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:49:32.505311 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:49:35.567768 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:49:38.522381 140205209478976 submission_runner.py:408] Time since start: 16746.25s, 	Step: 35987, 	{'train/accuracy': 0.9890627861022949, 'train/loss': 0.03704715147614479, 'train/mean_average_precision': 0.23974588567854288, 'validation/accuracy': 0.9860416650772095, 'validation/loss': 0.047454286366701126, 'validation/mean_average_precision': 0.19409472872402248, 'validation/num_examples': 43793, 'test/accuracy': 0.9851229190826416, 'test/loss': 0.05034815892577171, 'test/mean_average_precision': 0.19721399984521867, 'test/num_examples': 43793, 'score': 11537.726133108139, 'total_duration': 16746.245433568954, 'accumulated_submission_time': 11537.726133108139, 'accumulated_eval_time': 5205.648674488068, 'accumulated_logging_time': 1.8951332569122314}
I0206 00:49:38.544492 140002205157120 logging_writer.py:48] [35987] accumulated_eval_time=5205.648674, accumulated_logging_time=1.895133, accumulated_submission_time=11537.726133, global_step=35987, preemption_count=0, score=11537.726133, test/accuracy=0.985123, test/loss=0.050348, test/mean_average_precision=0.197214, test/num_examples=43793, total_duration=16746.245434, train/accuracy=0.989063, train/loss=0.037047, train/mean_average_precision=0.239746, validation/accuracy=0.986042, validation/loss=0.047454, validation/mean_average_precision=0.194095, validation/num_examples=43793
I0206 00:49:43.114125 140002598295296 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.045182712376117706, loss=0.03466387838125229
I0206 00:50:15.210562 140002205157120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.051140278577804565, loss=0.03760087117552757
I0206 00:50:47.569085 140002598295296 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.053586605936288834, loss=0.039309311658144
I0206 00:51:19.208051 140002205157120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.057234346866607666, loss=0.03373629227280617
I0206 00:51:50.838069 140002598295296 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.04539612680673599, loss=0.03535746410489082
I0206 00:52:22.718404 140002205157120 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.055197250097990036, loss=0.033236462622880936
I0206 00:52:54.976384 140002598295296 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.09299765527248383, loss=0.036234449595212936
I0206 00:53:27.362984 140002205157120 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.03376959636807442, loss=0.0322231762111187
I0206 00:53:38.691879 140205209478976 spec.py:321] Evaluating on the training split.
I0206 00:55:19.463681 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 00:55:22.465765 140205209478976 spec.py:349] Evaluating on the test split.
I0206 00:55:25.434649 140205209478976 submission_runner.py:408] Time since start: 17093.16s, 	Step: 36737, 	{'train/accuracy': 0.9894454479217529, 'train/loss': 0.0358574204146862, 'train/mean_average_precision': 0.26194003030307045, 'validation/accuracy': 0.9862446784973145, 'validation/loss': 0.04672913998365402, 'validation/mean_average_precision': 0.21278031221063418, 'validation/num_examples': 43793, 'test/accuracy': 0.9852808713912964, 'test/loss': 0.04963654279708862, 'test/mean_average_precision': 0.20278154393467035, 'test/num_examples': 43793, 'score': 11777.842349767685, 'total_duration': 17093.15770673752, 'accumulated_submission_time': 11777.842349767685, 'accumulated_eval_time': 5312.391384601593, 'accumulated_logging_time': 1.928436040878296}
I0206 00:55:25.456939 140020770670336 logging_writer.py:48] [36737] accumulated_eval_time=5312.391385, accumulated_logging_time=1.928436, accumulated_submission_time=11777.842350, global_step=36737, preemption_count=0, score=11777.842350, test/accuracy=0.985281, test/loss=0.049637, test/mean_average_precision=0.202782, test/num_examples=43793, total_duration=17093.157707, train/accuracy=0.989445, train/loss=0.035857, train/mean_average_precision=0.261940, validation/accuracy=0.986245, validation/loss=0.046729, validation/mean_average_precision=0.212780, validation/num_examples=43793
I0206 00:55:45.894073 140021163808512 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.08606819808483124, loss=0.03516635671257973
I0206 00:56:17.976581 140020770670336 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.03679638355970383, loss=0.03197368234395981
I0206 00:56:49.817122 140021163808512 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.07180716097354889, loss=0.03517429903149605
I0206 00:57:21.896662 140020770670336 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.03398669511079788, loss=0.03247333690524101
I0206 00:57:53.437673 140021163808512 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.04950185865163803, loss=0.0385299026966095
I0206 00:58:25.458761 140020770670336 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.042076535522937775, loss=0.03791011869907379
I0206 00:58:58.141819 140021163808512 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.08357249200344086, loss=0.033062104135751724
I0206 00:59:25.449467 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:01:01.470638 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:01:04.516579 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:01:07.494859 140205209478976 submission_runner.py:408] Time since start: 17435.22s, 	Step: 37487, 	{'train/accuracy': 0.9894551038742065, 'train/loss': 0.03574817627668381, 'train/mean_average_precision': 0.25619943397522477, 'validation/accuracy': 0.9861322045326233, 'validation/loss': 0.0467582568526268, 'validation/mean_average_precision': 0.20661521290754128, 'validation/num_examples': 43793, 'test/accuracy': 0.9852720499038696, 'test/loss': 0.049628548324108124, 'test/mean_average_precision': 0.20916613384323424, 'test/num_examples': 43793, 'score': 12017.802143096924, 'total_duration': 17435.217926979065, 'accumulated_submission_time': 12017.802143096924, 'accumulated_eval_time': 5414.43673324585, 'accumulated_logging_time': 1.9633357524871826}
I0206 01:01:07.516943 139984133072640 logging_writer.py:48] [37487] accumulated_eval_time=5414.436733, accumulated_logging_time=1.963336, accumulated_submission_time=12017.802143, global_step=37487, preemption_count=0, score=12017.802143, test/accuracy=0.985272, test/loss=0.049629, test/mean_average_precision=0.209166, test/num_examples=43793, total_duration=17435.217927, train/accuracy=0.989455, train/loss=0.035748, train/mean_average_precision=0.256199, validation/accuracy=0.986132, validation/loss=0.046758, validation/mean_average_precision=0.206615, validation/num_examples=43793
I0206 01:01:12.012742 140002205157120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.04908762127161026, loss=0.03758738189935684
I0206 01:01:43.848228 139984133072640 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.04410277679562569, loss=0.03830110654234886
I0206 01:02:15.794424 140002205157120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.039992138743400574, loss=0.03462877497076988
I0206 01:02:47.495583 139984133072640 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.03784998506307602, loss=0.0330137275159359
I0206 01:03:19.338448 140002205157120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.048400890082120895, loss=0.03405066952109337
I0206 01:03:51.109503 139984133072640 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.032768670469522476, loss=0.032845739275217056
I0206 01:04:22.584995 140002205157120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.02887517586350441, loss=0.03469973802566528
I0206 01:04:53.891690 139984133072640 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.04363088309764862, loss=0.03943062573671341
I0206 01:05:07.580460 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:06:48.544918 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:06:51.581828 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:06:54.537411 140205209478976 submission_runner.py:408] Time since start: 17782.26s, 	Step: 38244, 	{'train/accuracy': 0.9893867373466492, 'train/loss': 0.03627531975507736, 'train/mean_average_precision': 0.26018173176115245, 'validation/accuracy': 0.9859263896942139, 'validation/loss': 0.04731174185872078, 'validation/mean_average_precision': 0.20246927034052514, 'validation/num_examples': 43793, 'test/accuracy': 0.9849477410316467, 'test/loss': 0.050186317414045334, 'test/mean_average_precision': 0.1923162326230432, 'test/num_examples': 43793, 'score': 12257.834669589996, 'total_duration': 17782.260478019714, 'accumulated_submission_time': 12257.834669589996, 'accumulated_eval_time': 5521.393639087677, 'accumulated_logging_time': 1.9962775707244873}
I0206 01:06:54.559645 140020770670336 logging_writer.py:48] [38244] accumulated_eval_time=5521.393639, accumulated_logging_time=1.996278, accumulated_submission_time=12257.834670, global_step=38244, preemption_count=0, score=12257.834670, test/accuracy=0.984948, test/loss=0.050186, test/mean_average_precision=0.192316, test/num_examples=43793, total_duration=17782.260478, train/accuracy=0.989387, train/loss=0.036275, train/mean_average_precision=0.260182, validation/accuracy=0.985926, validation/loss=0.047312, validation/mean_average_precision=0.202469, validation/num_examples=43793
I0206 01:07:12.689165 140021163808512 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.06534315645694733, loss=0.032657403498888016
I0206 01:07:44.204290 140020770670336 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.033721014857292175, loss=0.03456922248005867
I0206 01:08:15.929210 140021163808512 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.05766305699944496, loss=0.038359515368938446
I0206 01:08:47.596899 140020770670336 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.04009978473186493, loss=0.03486737608909607
I0206 01:09:19.419202 140021163808512 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.05272684246301651, loss=0.03601863607764244
I0206 01:09:50.981503 140020770670336 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.05592762678861618, loss=0.03240721672773361
I0206 01:10:22.703049 140021163808512 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.03863810747861862, loss=0.036205071955919266
I0206 01:10:54.591405 140020770670336 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.10496513545513153, loss=0.04262358695268631
I0206 01:10:54.596484 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:12:31.154456 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:12:34.201949 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:12:37.137799 140205209478976 submission_runner.py:408] Time since start: 18124.86s, 	Step: 39001, 	{'train/accuracy': 0.9894042611122131, 'train/loss': 0.036132972687482834, 'train/mean_average_precision': 0.2705461838739672, 'validation/accuracy': 0.9859158396720886, 'validation/loss': 0.047259051352739334, 'validation/mean_average_precision': 0.20614544073050622, 'validation/num_examples': 43793, 'test/accuracy': 0.9850168228149414, 'test/loss': 0.05015391483902931, 'test/mean_average_precision': 0.19393707504385768, 'test/num_examples': 43793, 'score': 12497.840120315552, 'total_duration': 18124.86086988449, 'accumulated_submission_time': 12497.840120315552, 'accumulated_eval_time': 5623.934894800186, 'accumulated_logging_time': 2.029609203338623}
I0206 01:12:37.161245 139984133072640 logging_writer.py:48] [39001] accumulated_eval_time=5623.934895, accumulated_logging_time=2.029609, accumulated_submission_time=12497.840120, global_step=39001, preemption_count=0, score=12497.840120, test/accuracy=0.985017, test/loss=0.050154, test/mean_average_precision=0.193937, test/num_examples=43793, total_duration=18124.860870, train/accuracy=0.989404, train/loss=0.036133, train/mean_average_precision=0.270546, validation/accuracy=0.985916, validation/loss=0.047259, validation/mean_average_precision=0.206145, validation/num_examples=43793
I0206 01:13:08.921995 140002205157120 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.04203606769442558, loss=0.038887809962034225
I0206 01:13:40.729104 139984133072640 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.03936414793133736, loss=0.03187669813632965
I0206 01:14:12.448483 140002205157120 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.03813602402806282, loss=0.03483787551522255
I0206 01:14:44.266344 139984133072640 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.04854663088917732, loss=0.03677787631750107
I0206 01:15:15.860409 140002205157120 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.04826643690466881, loss=0.03836902230978012
I0206 01:15:47.682397 139984133072640 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.033451057970523834, loss=0.03489568829536438
I0206 01:16:19.906548 140002205157120 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.046866852790117264, loss=0.03558187931776047
I0206 01:16:37.270568 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:18:15.688630 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:18:18.744990 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:18:21.699542 140205209478976 submission_runner.py:408] Time since start: 18469.42s, 	Step: 39756, 	{'train/accuracy': 0.9893486499786377, 'train/loss': 0.03603357449173927, 'train/mean_average_precision': 0.25614875944770504, 'validation/accuracy': 0.9859779477119446, 'validation/loss': 0.0470002144575119, 'validation/mean_average_precision': 0.20517942394232186, 'validation/num_examples': 43793, 'test/accuracy': 0.9850248098373413, 'test/loss': 0.05004764720797539, 'test/mean_average_precision': 0.19505340855467057, 'test/num_examples': 43793, 'score': 12737.917171955109, 'total_duration': 18469.422612428665, 'accumulated_submission_time': 12737.917171955109, 'accumulated_eval_time': 5728.363832473755, 'accumulated_logging_time': 2.065474271774292}
I0206 01:18:21.722257 140002598295296 logging_writer.py:48] [39756] accumulated_eval_time=5728.363832, accumulated_logging_time=2.065474, accumulated_submission_time=12737.917172, global_step=39756, preemption_count=0, score=12737.917172, test/accuracy=0.985025, test/loss=0.050048, test/mean_average_precision=0.195053, test/num_examples=43793, total_duration=18469.422612, train/accuracy=0.989349, train/loss=0.036034, train/mean_average_precision=0.256149, validation/accuracy=0.985978, validation/loss=0.047000, validation/mean_average_precision=0.205179, validation/num_examples=43793
I0206 01:18:35.897846 140021163808512 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.04254140332341194, loss=0.03372889384627342
I0206 01:19:07.738664 140002598295296 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.05823845788836479, loss=0.0376027412712574
I0206 01:19:39.668015 140021163808512 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0422838032245636, loss=0.03625046834349632
I0206 01:20:11.912711 140002598295296 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.06422010064125061, loss=0.03663943335413933
I0206 01:20:43.777600 140021163808512 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.09094808995723724, loss=0.036705635488033295
I0206 01:21:15.559128 140002598295296 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.09414871782064438, loss=0.04131336510181427
I0206 01:21:48.017854 140021163808512 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.05430682376027107, loss=0.036836881190538406
I0206 01:22:20.481634 140002598295296 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.07270848751068115, loss=0.034215398132801056
I0206 01:22:21.756862 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:24:01.744185 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:24:04.887541 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:24:08.024235 140205209478976 submission_runner.py:408] Time since start: 18815.75s, 	Step: 40505, 	{'train/accuracy': 0.9895033836364746, 'train/loss': 0.03543873503804207, 'train/mean_average_precision': 0.2753709398089208, 'validation/accuracy': 0.9860765933990479, 'validation/loss': 0.04671790078282356, 'validation/mean_average_precision': 0.20224206700190955, 'validation/num_examples': 43793, 'test/accuracy': 0.9851389527320862, 'test/loss': 0.0496848039329052, 'test/mean_average_precision': 0.1942995179941897, 'test/num_examples': 43793, 'score': 12977.919306516647, 'total_duration': 18815.74730205536, 'accumulated_submission_time': 12977.919306516647, 'accumulated_eval_time': 5834.631159543991, 'accumulated_logging_time': 2.10075044631958}
I0206 01:24:08.049116 139984133072640 logging_writer.py:48] [40505] accumulated_eval_time=5834.631160, accumulated_logging_time=2.100750, accumulated_submission_time=12977.919307, global_step=40505, preemption_count=0, score=12977.919307, test/accuracy=0.985139, test/loss=0.049685, test/mean_average_precision=0.194300, test/num_examples=43793, total_duration=18815.747302, train/accuracy=0.989503, train/loss=0.035439, train/mean_average_precision=0.275371, validation/accuracy=0.986077, validation/loss=0.046718, validation/mean_average_precision=0.202242, validation/num_examples=43793
I0206 01:24:39.005282 140002205157120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.04297114908695221, loss=0.03564935550093651
I0206 01:25:11.051378 139984133072640 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.05351446196436882, loss=0.03950142860412598
I0206 01:25:42.948532 140002205157120 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.03860366716980934, loss=0.034408822655677795
I0206 01:26:15.154410 139984133072640 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.05117165669798851, loss=0.03216244652867317
I0206 01:26:47.198160 140002205157120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.04713408276438713, loss=0.04032498598098755
I0206 01:27:19.489095 139984133072640 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.06753940135240555, loss=0.03713525831699371
I0206 01:27:51.520381 140002205157120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.04439571127295494, loss=0.037515949457883835
I0206 01:28:08.170897 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:29:44.641330 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:29:47.864428 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:29:50.945415 140205209478976 submission_runner.py:408] Time since start: 19158.67s, 	Step: 41252, 	{'train/accuracy': 0.9897860288619995, 'train/loss': 0.034546561539173126, 'train/mean_average_precision': 0.28537601240427235, 'validation/accuracy': 0.9862000346183777, 'validation/loss': 0.046651337295770645, 'validation/mean_average_precision': 0.21006526472080778, 'validation/num_examples': 43793, 'test/accuracy': 0.9852118492126465, 'test/loss': 0.04969169571995735, 'test/mean_average_precision': 0.19840804436460177, 'test/num_examples': 43793, 'score': 13218.008937835693, 'total_duration': 19158.668475151062, 'accumulated_submission_time': 13218.008937835693, 'accumulated_eval_time': 5937.405626773834, 'accumulated_logging_time': 2.137742519378662}
I0206 01:29:50.968659 140020770670336 logging_writer.py:48] [41252] accumulated_eval_time=5937.405627, accumulated_logging_time=2.137743, accumulated_submission_time=13218.008938, global_step=41252, preemption_count=0, score=13218.008938, test/accuracy=0.985212, test/loss=0.049692, test/mean_average_precision=0.198408, test/num_examples=43793, total_duration=19158.668475, train/accuracy=0.989786, train/loss=0.034547, train/mean_average_precision=0.285376, validation/accuracy=0.986200, validation/loss=0.046651, validation/mean_average_precision=0.210065, validation/num_examples=43793
I0206 01:30:06.611497 140021163808512 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.04449636489152908, loss=0.03510031849145889
I0206 01:30:38.538594 140020770670336 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.05264488607645035, loss=0.03924807906150818
I0206 01:31:10.592983 140021163808512 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.06175358593463898, loss=0.03789016231894493
I0206 01:31:42.341536 140020770670336 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.05567328631877899, loss=0.03613612428307533
I0206 01:32:14.461962 140021163808512 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.045436304062604904, loss=0.03638423979282379
I0206 01:32:46.326968 140020770670336 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.04403279349207878, loss=0.037210553884506226
I0206 01:33:18.235013 140021163808512 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.06998150050640106, loss=0.03487628325819969
I0206 01:33:50.218732 140020770670336 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.05610467121005058, loss=0.03445073589682579
I0206 01:33:51.225589 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:35:30.862147 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:35:34.176948 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:35:37.524612 140205209478976 submission_runner.py:408] Time since start: 19505.25s, 	Step: 42004, 	{'train/accuracy': 0.989495038986206, 'train/loss': 0.03562954440712929, 'train/mean_average_precision': 0.26692613512005703, 'validation/accuracy': 0.9862678050994873, 'validation/loss': 0.046522967517375946, 'validation/mean_average_precision': 0.21501858248592956, 'validation/num_examples': 43793, 'test/accuracy': 0.9853630065917969, 'test/loss': 0.049677565693855286, 'test/mean_average_precision': 0.19559475361363315, 'test/num_examples': 43793, 'score': 13458.234591960907, 'total_duration': 19505.247665166855, 'accumulated_submission_time': 13458.234591960907, 'accumulated_eval_time': 6043.70458650589, 'accumulated_logging_time': 2.17232346534729}
I0206 01:35:37.550508 140002205157120 logging_writer.py:48] [42004] accumulated_eval_time=6043.704587, accumulated_logging_time=2.172323, accumulated_submission_time=13458.234592, global_step=42004, preemption_count=0, score=13458.234592, test/accuracy=0.985363, test/loss=0.049678, test/mean_average_precision=0.195595, test/num_examples=43793, total_duration=19505.247665, train/accuracy=0.989495, train/loss=0.035630, train/mean_average_precision=0.266926, validation/accuracy=0.986268, validation/loss=0.046523, validation/mean_average_precision=0.215019, validation/num_examples=43793
I0206 01:36:09.389795 140002598295296 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.05348757654428482, loss=0.03978673368692398
I0206 01:36:42.128850 140002205157120 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07424087822437286, loss=0.0383877158164978
I0206 01:37:14.896003 140002598295296 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.047020480036735535, loss=0.03672463074326515
I0206 01:37:47.774824 140002205157120 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.04709596931934357, loss=0.03661850839853287
I0206 01:38:20.660065 140002598295296 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.03526362031698227, loss=0.037812549620866776
I0206 01:38:53.207877 140002205157120 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.044072821736335754, loss=0.036096230149269104
I0206 01:39:25.898691 140002598295296 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.09254483133554459, loss=0.03635452687740326
I0206 01:39:37.682105 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:41:15.857871 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:41:18.983867 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:41:22.024051 140205209478976 submission_runner.py:408] Time since start: 19849.75s, 	Step: 42737, 	{'train/accuracy': 0.9894113540649414, 'train/loss': 0.035757679492235184, 'train/mean_average_precision': 0.2688181003628976, 'validation/accuracy': 0.9861470460891724, 'validation/loss': 0.046720124781131744, 'validation/mean_average_precision': 0.21142738089231428, 'validation/num_examples': 43793, 'test/accuracy': 0.9852128624916077, 'test/loss': 0.049765896052122116, 'test/mean_average_precision': 0.20485735455653056, 'test/num_examples': 43793, 'score': 13698.329906463623, 'total_duration': 19849.74711751938, 'accumulated_submission_time': 13698.329906463623, 'accumulated_eval_time': 6148.046504974365, 'accumulated_logging_time': 2.210094928741455}
I0206 01:41:22.048013 139984133072640 logging_writer.py:48] [42737] accumulated_eval_time=6148.046505, accumulated_logging_time=2.210095, accumulated_submission_time=13698.329906, global_step=42737, preemption_count=0, score=13698.329906, test/accuracy=0.985213, test/loss=0.049766, test/mean_average_precision=0.204857, test/num_examples=43793, total_duration=19849.747118, train/accuracy=0.989411, train/loss=0.035758, train/mean_average_precision=0.268818, validation/accuracy=0.986147, validation/loss=0.046720, validation/mean_average_precision=0.211427, validation/num_examples=43793
I0206 01:41:42.370314 140020770670336 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.05564996972680092, loss=0.0325896218419075
I0206 01:42:14.157679 139984133072640 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.043884605169296265, loss=0.03689803555607796
I0206 01:42:45.751341 140020770670336 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.04701988399028778, loss=0.03317859768867493
I0206 01:43:17.440029 139984133072640 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.07027551531791687, loss=0.04058035463094711
I0206 01:43:48.720263 140020770670336 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.045156627893447876, loss=0.03445382043719292
I0206 01:44:20.619643 139984133072640 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.0471075139939785, loss=0.03232685849070549
I0206 01:44:52.669818 140020770670336 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.06093216687440872, loss=0.03891957178711891
I0206 01:45:22.191072 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:46:59.086185 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:47:02.310526 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:47:05.369927 140205209478976 submission_runner.py:408] Time since start: 20193.09s, 	Step: 43494, 	{'train/accuracy': 0.989499032497406, 'train/loss': 0.03540022671222687, 'train/mean_average_precision': 0.26506057007857436, 'validation/accuracy': 0.9862678050994873, 'validation/loss': 0.04665068909525871, 'validation/mean_average_precision': 0.21142511276955886, 'validation/num_examples': 43793, 'test/accuracy': 0.9853647351264954, 'test/loss': 0.049580205231904984, 'test/mean_average_precision': 0.20401457788142363, 'test/num_examples': 43793, 'score': 13938.442008972168, 'total_duration': 20193.092981815338, 'accumulated_submission_time': 13938.442008972168, 'accumulated_eval_time': 6251.2253041267395, 'accumulated_logging_time': 2.2450668811798096}
I0206 01:47:05.393171 140002205157120 logging_writer.py:48] [43494] accumulated_eval_time=6251.225304, accumulated_logging_time=2.245067, accumulated_submission_time=13938.442009, global_step=43494, preemption_count=0, score=13938.442009, test/accuracy=0.985365, test/loss=0.049580, test/mean_average_precision=0.204015, test/num_examples=43793, total_duration=20193.092982, train/accuracy=0.989499, train/loss=0.035400, train/mean_average_precision=0.265061, validation/accuracy=0.986268, validation/loss=0.046651, validation/mean_average_precision=0.211425, validation/num_examples=43793
I0206 01:47:07.641963 140002598295296 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.03787137567996979, loss=0.03302387893199921
I0206 01:47:39.528418 140002205157120 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.06710652261972427, loss=0.03871476650238037
I0206 01:48:10.898255 140002598295296 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.06295344233512878, loss=0.03680066764354706
I0206 01:48:42.514439 140002205157120 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.05281659960746765, loss=0.03555662930011749
I0206 01:49:14.528001 140002598295296 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.04567665234208107, loss=0.03643699735403061
I0206 01:49:46.072795 140002205157120 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.08214155584573746, loss=0.03307153284549713
I0206 01:50:18.057537 140002598295296 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.05761231482028961, loss=0.0354493074119091
I0206 01:50:49.555610 140002205157120 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.06445121020078659, loss=0.0384342297911644
I0206 01:51:05.578064 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:52:42.337773 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:52:45.364338 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:52:48.324325 140205209478976 submission_runner.py:408] Time since start: 20536.05s, 	Step: 44251, 	{'train/accuracy': 0.9895753860473633, 'train/loss': 0.03549180552363396, 'train/mean_average_precision': 0.27426659195040215, 'validation/accuracy': 0.9862418174743652, 'validation/loss': 0.0463673397898674, 'validation/mean_average_precision': 0.21045252702003892, 'validation/num_examples': 43793, 'test/accuracy': 0.9851962327957153, 'test/loss': 0.04952609911561012, 'test/mean_average_precision': 0.19897285175365154, 'test/num_examples': 43793, 'score': 14178.594394683838, 'total_duration': 20536.04729604721, 'accumulated_submission_time': 14178.594394683838, 'accumulated_eval_time': 6353.971425771713, 'accumulated_logging_time': 2.28100848197937}
I0206 01:52:48.348675 139984133072640 logging_writer.py:48] [44251] accumulated_eval_time=6353.971426, accumulated_logging_time=2.281008, accumulated_submission_time=14178.594395, global_step=44251, preemption_count=0, score=14178.594395, test/accuracy=0.985196, test/loss=0.049526, test/mean_average_precision=0.198973, test/num_examples=43793, total_duration=20536.047296, train/accuracy=0.989575, train/loss=0.035492, train/mean_average_precision=0.274267, validation/accuracy=0.986242, validation/loss=0.046367, validation/mean_average_precision=0.210453, validation/num_examples=43793
I0206 01:53:04.087157 140020770670336 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.09280576556921005, loss=0.03836320713162422
I0206 01:53:35.585597 139984133072640 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.047085437923669815, loss=0.033365700393915176
I0206 01:54:07.606684 140020770670336 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.10313662886619568, loss=0.038646433502435684
I0206 01:54:39.192747 139984133072640 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.03619880974292755, loss=0.0343191921710968
I0206 01:55:11.154547 140020770670336 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.03439481556415558, loss=0.03486447036266327
I0206 01:55:42.916814 139984133072640 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.07404827326536179, loss=0.03990232199430466
I0206 01:56:14.890071 140020770670336 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.13566485047340393, loss=0.035279978066682816
I0206 01:56:46.876770 139984133072640 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.06311646848917007, loss=0.03251415491104126
I0206 01:56:48.465120 140205209478976 spec.py:321] Evaluating on the training split.
I0206 01:58:27.342069 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 01:58:30.403832 140205209478976 spec.py:349] Evaluating on the test split.
I0206 01:58:33.367612 140205209478976 submission_runner.py:408] Time since start: 20881.09s, 	Step: 45006, 	{'train/accuracy': 0.9895598888397217, 'train/loss': 0.035108692944049835, 'train/mean_average_precision': 0.27760304847181394, 'validation/accuracy': 0.9861411452293396, 'validation/loss': 0.046678148210048676, 'validation/mean_average_precision': 0.20588797651640117, 'validation/num_examples': 43793, 'test/accuracy': 0.9851511716842651, 'test/loss': 0.04976167157292366, 'test/mean_average_precision': 0.19534209004671338, 'test/num_examples': 43793, 'score': 14418.677802801132, 'total_duration': 20881.09065937996, 'accumulated_submission_time': 14418.677802801132, 'accumulated_eval_time': 6458.873847723007, 'accumulated_logging_time': 2.3189632892608643}
I0206 01:58:33.391429 140002205157120 logging_writer.py:48] [45006] accumulated_eval_time=6458.873848, accumulated_logging_time=2.318963, accumulated_submission_time=14418.677803, global_step=45006, preemption_count=0, score=14418.677803, test/accuracy=0.985151, test/loss=0.049762, test/mean_average_precision=0.195342, test/num_examples=43793, total_duration=20881.090659, train/accuracy=0.989560, train/loss=0.035109, train/mean_average_precision=0.277603, validation/accuracy=0.986141, validation/loss=0.046678, validation/mean_average_precision=0.205888, validation/num_examples=43793
I0206 01:59:03.575873 140021163808512 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.03938617184758186, loss=0.033634964376688004
I0206 01:59:35.210984 140002205157120 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.06936410814523697, loss=0.03897945210337639
I0206 02:00:07.140855 140021163808512 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.057270996272563934, loss=0.03288460895419121
I0206 02:00:39.064111 140002205157120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.04925919324159622, loss=0.0349569171667099
I0206 02:01:10.725298 140021163808512 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.06685073673725128, loss=0.035251688212156296
I0206 02:01:42.204399 140002205157120 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.10653842240571976, loss=0.041774213314056396
I0206 02:02:13.670555 140021163808512 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.05114489421248436, loss=0.03166719526052475
I0206 02:02:33.509020 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:04:09.354885 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:04:12.373900 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:04:15.390009 140205209478976 submission_runner.py:408] Time since start: 21223.11s, 	Step: 45765, 	{'train/accuracy': 0.9897001385688782, 'train/loss': 0.03460755571722984, 'train/mean_average_precision': 0.27682914659793945, 'validation/accuracy': 0.9862962365150452, 'validation/loss': 0.0462343730032444, 'validation/mean_average_precision': 0.21518716704108146, 'validation/num_examples': 43793, 'test/accuracy': 0.9853546023368835, 'test/loss': 0.049391888082027435, 'test/mean_average_precision': 0.20383672222142435, 'test/num_examples': 43793, 'score': 14658.762327671051, 'total_duration': 21223.11307501793, 'accumulated_submission_time': 14658.762327671051, 'accumulated_eval_time': 6560.754787683487, 'accumulated_logging_time': 2.355625629425049}
I0206 02:04:15.413772 139984133072640 logging_writer.py:48] [45765] accumulated_eval_time=6560.754788, accumulated_logging_time=2.355626, accumulated_submission_time=14658.762328, global_step=45765, preemption_count=0, score=14658.762328, test/accuracy=0.985355, test/loss=0.049392, test/mean_average_precision=0.203837, test/num_examples=43793, total_duration=21223.113075, train/accuracy=0.989700, train/loss=0.034608, train/mean_average_precision=0.276829, validation/accuracy=0.986296, validation/loss=0.046234, validation/mean_average_precision=0.215187, validation/num_examples=43793
I0206 02:04:27.185974 140020770670336 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.04603366181254387, loss=0.03572402521967888
I0206 02:04:59.427845 139984133072640 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.0610404908657074, loss=0.03551540523767471
I0206 02:05:31.668336 140020770670336 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.043550748378038406, loss=0.037454430013895035
I0206 02:06:03.534899 139984133072640 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.0540720596909523, loss=0.033437103033065796
I0206 02:06:35.573504 140020770670336 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.05909198895096779, loss=0.0344439372420311
I0206 02:07:07.500979 139984133072640 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.06102549657225609, loss=0.03566576540470123
I0206 02:07:39.435619 140020770670336 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.058857422322034836, loss=0.03600115329027176
I0206 02:08:11.164587 139984133072640 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.07088557630777359, loss=0.03548545390367508
I0206 02:08:15.586373 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:09:53.478593 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:09:56.539507 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:09:59.530434 140205209478976 submission_runner.py:408] Time since start: 21567.25s, 	Step: 46515, 	{'train/accuracy': 0.9896975159645081, 'train/loss': 0.034646280109882355, 'train/mean_average_precision': 0.2963236467176596, 'validation/accuracy': 0.9863830804824829, 'validation/loss': 0.046199001371860504, 'validation/mean_average_precision': 0.22106585216394292, 'validation/num_examples': 43793, 'test/accuracy': 0.9853836894035339, 'test/loss': 0.04933399707078934, 'test/mean_average_precision': 0.207040467005581, 'test/num_examples': 43793, 'score': 14898.90408039093, 'total_duration': 21567.253492355347, 'accumulated_submission_time': 14898.90408039093, 'accumulated_eval_time': 6664.698795795441, 'accumulated_logging_time': 2.390665292739868}
I0206 02:09:59.555796 140002205157120 logging_writer.py:48] [46515] accumulated_eval_time=6664.698796, accumulated_logging_time=2.390665, accumulated_submission_time=14898.904080, global_step=46515, preemption_count=0, score=14898.904080, test/accuracy=0.985384, test/loss=0.049334, test/mean_average_precision=0.207040, test/num_examples=43793, total_duration=21567.253492, train/accuracy=0.989698, train/loss=0.034646, train/mean_average_precision=0.296324, validation/accuracy=0.986383, validation/loss=0.046199, validation/mean_average_precision=0.221066, validation/num_examples=43793
I0206 02:10:27.078392 140021163808512 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.0678013414144516, loss=0.03534999117255211
I0206 02:10:58.978174 140002205157120 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.05829687789082527, loss=0.03317354619503021
I0206 02:11:30.784513 140021163808512 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.04192619025707245, loss=0.032959356904029846
I0206 02:12:02.295227 140002205157120 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.07107267528772354, loss=0.03674687072634697
I0206 02:12:33.961214 140021163808512 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.053802426904439926, loss=0.0375041663646698
I0206 02:13:05.723345 140002205157120 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.08784845471382141, loss=0.0350341834127903
I0206 02:13:37.654555 140021163808512 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.04748564586043358, loss=0.03477510064840317
I0206 02:13:59.563289 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:15:39.248266 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:15:42.275781 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:15:45.263336 140205209478976 submission_runner.py:408] Time since start: 21912.99s, 	Step: 47270, 	{'train/accuracy': 0.9897758364677429, 'train/loss': 0.03435588255524635, 'train/mean_average_precision': 0.3042403984564562, 'validation/accuracy': 0.9862929582595825, 'validation/loss': 0.04597706347703934, 'validation/mean_average_precision': 0.212829559312736, 'validation/num_examples': 43793, 'test/accuracy': 0.9853179454803467, 'test/loss': 0.04889655113220215, 'test/mean_average_precision': 0.20305467845193023, 'test/num_examples': 43793, 'score': 15138.879612207413, 'total_duration': 21912.986404657364, 'accumulated_submission_time': 15138.879612207413, 'accumulated_eval_time': 6770.398800849915, 'accumulated_logging_time': 2.428494691848755}
I0206 02:15:45.287765 140002598295296 logging_writer.py:48] [47270] accumulated_eval_time=6770.398801, accumulated_logging_time=2.428495, accumulated_submission_time=15138.879612, global_step=47270, preemption_count=0, score=15138.879612, test/accuracy=0.985318, test/loss=0.048897, test/mean_average_precision=0.203055, test/num_examples=43793, total_duration=21912.986405, train/accuracy=0.989776, train/loss=0.034356, train/mean_average_precision=0.304240, validation/accuracy=0.986293, validation/loss=0.045977, validation/mean_average_precision=0.212830, validation/num_examples=43793
I0206 02:15:55.166422 140020770670336 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.06168357655405998, loss=0.035265002399683
I0206 02:16:26.974448 140002598295296 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.059445060789585114, loss=0.027247140184044838
I0206 02:16:58.721208 140020770670336 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.12383322417736053, loss=0.032438136637210846
I0206 02:17:30.611969 140002598295296 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.07054656744003296, loss=0.03212857246398926
I0206 02:18:02.323217 140020770670336 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.10860533267259598, loss=0.039640020579099655
I0206 02:18:34.064006 140002598295296 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.052185554057359695, loss=0.037040743976831436
I0206 02:19:05.597190 140020770670336 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.0476696714758873, loss=0.03481273725628853
I0206 02:19:37.289608 140002598295296 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.04147405922412872, loss=0.0322430394589901
I0206 02:19:45.525034 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:21:20.546921 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:21:23.548171 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:21:26.468605 140205209478976 submission_runner.py:408] Time since start: 22254.19s, 	Step: 48027, 	{'train/accuracy': 0.9896266460418701, 'train/loss': 0.03433692827820778, 'train/mean_average_precision': 0.286865701081385, 'validation/accuracy': 0.9862750768661499, 'validation/loss': 0.04656687006354332, 'validation/mean_average_precision': 0.2209927814720388, 'validation/num_examples': 43793, 'test/accuracy': 0.9853202700614929, 'test/loss': 0.049766506999731064, 'test/mean_average_precision': 0.20462477029053414, 'test/num_examples': 43793, 'score': 15379.08549618721, 'total_duration': 22254.191664218903, 'accumulated_submission_time': 15379.08549618721, 'accumulated_eval_time': 6871.342316389084, 'accumulated_logging_time': 2.464463233947754}
I0206 02:21:26.492785 139984133072640 logging_writer.py:48] [48027] accumulated_eval_time=6871.342316, accumulated_logging_time=2.464463, accumulated_submission_time=15379.085496, global_step=48027, preemption_count=0, score=15379.085496, test/accuracy=0.985320, test/loss=0.049767, test/mean_average_precision=0.204625, test/num_examples=43793, total_duration=22254.191664, train/accuracy=0.989627, train/loss=0.034337, train/mean_average_precision=0.286866, validation/accuracy=0.986275, validation/loss=0.046567, validation/mean_average_precision=0.220993, validation/num_examples=43793
I0206 02:21:49.798086 140021163808512 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.07807983458042145, loss=0.034812793135643005
I0206 02:22:21.363133 139984133072640 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.09666981548070908, loss=0.0333576500415802
I0206 02:22:52.772841 140021163808512 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.05047714337706566, loss=0.03996085003018379
I0206 02:23:24.267126 139984133072640 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.04664736986160278, loss=0.03287212550640106
I0206 02:23:55.679193 140021163808512 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.048208147287368774, loss=0.03438299894332886
I0206 02:24:28.300552 139984133072640 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.06948165595531464, loss=0.03283532336354256
I0206 02:25:00.093841 140021163808512 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.04650112986564636, loss=0.03494716063141823
I0206 02:25:26.539408 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:27:03.201800 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:27:06.361731 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:27:09.467888 140205209478976 submission_runner.py:408] Time since start: 22597.19s, 	Step: 48784, 	{'train/accuracy': 0.9900445342063904, 'train/loss': 0.03350134193897247, 'train/mean_average_precision': 0.3042157934923303, 'validation/accuracy': 0.9863396286964417, 'validation/loss': 0.046142980456352234, 'validation/mean_average_precision': 0.22079732841857616, 'validation/num_examples': 43793, 'test/accuracy': 0.9854194521903992, 'test/loss': 0.0493343286216259, 'test/mean_average_precision': 0.21046426470122526, 'test/num_examples': 43793, 'score': 15619.100360155106, 'total_duration': 22597.190956115723, 'accumulated_submission_time': 15619.100360155106, 'accumulated_eval_time': 6974.270753145218, 'accumulated_logging_time': 2.500919818878174}
I0206 02:27:09.492141 140002205157120 logging_writer.py:48] [48784] accumulated_eval_time=6974.270753, accumulated_logging_time=2.500920, accumulated_submission_time=15619.100360, global_step=48784, preemption_count=0, score=15619.100360, test/accuracy=0.985419, test/loss=0.049334, test/mean_average_precision=0.210464, test/num_examples=43793, total_duration=22597.190956, train/accuracy=0.990045, train/loss=0.033501, train/mean_average_precision=0.304216, validation/accuracy=0.986340, validation/loss=0.046143, validation/mean_average_precision=0.220797, validation/num_examples=43793
I0206 02:27:14.980935 140002598295296 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.06834730505943298, loss=0.03431026637554169
I0206 02:27:47.415035 140002205157120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.05248654633760452, loss=0.034670911729335785
I0206 02:28:19.354056 140002598295296 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.06551747024059296, loss=0.0337953120470047
I0206 02:28:51.307516 140002205157120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.07966474443674088, loss=0.03133160248398781
I0206 02:29:22.992126 140002598295296 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.052145835012197495, loss=0.03203214332461357
I0206 02:29:54.704670 140002205157120 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.05902613326907158, loss=0.03250303864479065
I0206 02:30:26.618477 140002598295296 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.0473569817841053, loss=0.03494083508849144
I0206 02:30:58.455366 140002205157120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.05938193202018738, loss=0.0380030982196331
I0206 02:31:09.561183 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:32:43.674092 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:32:46.718393 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:32:49.655341 140205209478976 submission_runner.py:408] Time since start: 22937.38s, 	Step: 49536, 	{'train/accuracy': 0.9898927807807922, 'train/loss': 0.03411620482802391, 'train/mean_average_precision': 0.30057846144930656, 'validation/accuracy': 0.986319363117218, 'validation/loss': 0.046313922852277756, 'validation/mean_average_precision': 0.2191926410291987, 'validation/num_examples': 43793, 'test/accuracy': 0.9853870272636414, 'test/loss': 0.049277063459157944, 'test/mean_average_precision': 0.21312256947102398, 'test/num_examples': 43793, 'score': 15859.136625528336, 'total_duration': 22937.37840652466, 'accumulated_submission_time': 15859.136625528336, 'accumulated_eval_time': 7074.364864110947, 'accumulated_logging_time': 2.5379621982574463}
I0206 02:32:49.679827 140020770670336 logging_writer.py:48] [49536] accumulated_eval_time=7074.364864, accumulated_logging_time=2.537962, accumulated_submission_time=15859.136626, global_step=49536, preemption_count=0, score=15859.136626, test/accuracy=0.985387, test/loss=0.049277, test/mean_average_precision=0.213123, test/num_examples=43793, total_duration=22937.378407, train/accuracy=0.989893, train/loss=0.034116, train/mean_average_precision=0.300578, validation/accuracy=0.986319, validation/loss=0.046314, validation/mean_average_precision=0.219193, validation/num_examples=43793
I0206 02:33:10.467773 140021163808512 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.06295912712812424, loss=0.0330624058842659
I0206 02:33:41.981366 140020770670336 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.06396626681089401, loss=0.03524712473154068
I0206 02:34:13.646035 140021163808512 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.06032740697264671, loss=0.031084995716810226
I0206 02:34:44.954921 140020770670336 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.05445168912410736, loss=0.036231305450201035
I0206 02:35:16.643707 140021163808512 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.07362858951091766, loss=0.03343966230750084
I0206 02:35:48.347821 140020770670336 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.0788164809346199, loss=0.039294466376304626
I0206 02:36:20.425711 140021163808512 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.05359964072704315, loss=0.03321372717618942
I0206 02:36:49.913392 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:38:28.412610 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:38:31.436098 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:38:34.403858 140205209478976 submission_runner.py:408] Time since start: 23282.13s, 	Step: 50294, 	{'train/accuracy': 0.9897878170013428, 'train/loss': 0.03433746472001076, 'train/mean_average_precision': 0.28569289379560725, 'validation/accuracy': 0.9864285588264465, 'validation/loss': 0.04585791379213333, 'validation/mean_average_precision': 0.2164103157897776, 'validation/num_examples': 43793, 'test/accuracy': 0.9856048226356506, 'test/loss': 0.048900358378887177, 'test/mean_average_precision': 0.2149699454561261, 'test/num_examples': 43793, 'score': 16099.339428424835, 'total_duration': 23282.12690448761, 'accumulated_submission_time': 16099.339428424835, 'accumulated_eval_time': 7178.855273962021, 'accumulated_logging_time': 2.573608636856079}
I0206 02:38:34.428473 140002205157120 logging_writer.py:48] [50294] accumulated_eval_time=7178.855274, accumulated_logging_time=2.573609, accumulated_submission_time=16099.339428, global_step=50294, preemption_count=0, score=16099.339428, test/accuracy=0.985605, test/loss=0.048900, test/mean_average_precision=0.214970, test/num_examples=43793, total_duration=23282.126904, train/accuracy=0.989788, train/loss=0.034337, train/mean_average_precision=0.285693, validation/accuracy=0.986429, validation/loss=0.045858, validation/mean_average_precision=0.216410, validation/num_examples=43793
I0206 02:38:36.707836 140002598295296 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.08184105902910233, loss=0.03202114999294281
I0206 02:39:08.854423 140002205157120 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.07375995814800262, loss=0.03782446309924126
I0206 02:39:40.838045 140002598295296 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.07945083826780319, loss=0.03312861919403076
I0206 02:40:13.089390 140002205157120 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.05376940593123436, loss=0.03469104319810867
I0206 02:40:44.912240 140002598295296 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.07802943885326385, loss=0.03299432247877121
I0206 02:41:17.298947 140002205157120 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.05915486067533493, loss=0.038059622049331665
I0206 02:41:49.679741 140002598295296 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.10248532891273499, loss=0.037873249500989914
I0206 02:42:21.993315 140002205157120 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.090094193816185, loss=0.03451348841190338
I0206 02:42:34.533912 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:44:11.862272 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:44:14.908682 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:44:17.886317 140205209478976 submission_runner.py:408] Time since start: 23625.61s, 	Step: 51040, 	{'train/accuracy': 0.9899283647537231, 'train/loss': 0.033983852714300156, 'train/mean_average_precision': 0.30221693932369764, 'validation/accuracy': 0.9863327741622925, 'validation/loss': 0.045761264860630035, 'validation/mean_average_precision': 0.21567340506284718, 'validation/num_examples': 43793, 'test/accuracy': 0.9853739738464355, 'test/loss': 0.048657987266778946, 'test/mean_average_precision': 0.20812532694284952, 'test/num_examples': 43793, 'score': 16339.413675069809, 'total_duration': 23625.6093814373, 'accumulated_submission_time': 16339.413675069809, 'accumulated_eval_time': 7282.207628250122, 'accumulated_logging_time': 2.6092591285705566}
I0206 02:44:17.912345 139984133072640 logging_writer.py:48] [51040] accumulated_eval_time=7282.207628, accumulated_logging_time=2.609259, accumulated_submission_time=16339.413675, global_step=51040, preemption_count=0, score=16339.413675, test/accuracy=0.985374, test/loss=0.048658, test/mean_average_precision=0.208125, test/num_examples=43793, total_duration=23625.609381, train/accuracy=0.989928, train/loss=0.033984, train/mean_average_precision=0.302217, validation/accuracy=0.986333, validation/loss=0.045761, validation/mean_average_precision=0.215673, validation/num_examples=43793
I0206 02:44:37.903290 140021163808512 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.06207221373915672, loss=0.03525179252028465
I0206 02:45:10.007239 139984133072640 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.06112685799598694, loss=0.032102812081575394
I0206 02:45:42.130330 140021163808512 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.08948364853858948, loss=0.03368218243122101
I0206 02:46:13.843207 139984133072640 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.05916855111718178, loss=0.03486282378435135
I0206 02:46:45.969698 140021163808512 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.08611162006855011, loss=0.03443058580160141
I0206 02:47:18.272757 139984133072640 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.05739330127835274, loss=0.03472549095749855
I0206 02:47:50.423067 140021163808512 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.116329625248909, loss=0.03115733154118061
I0206 02:48:18.148078 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:49:57.722713 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:50:00.747352 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:50:04.106266 140205209478976 submission_runner.py:408] Time since start: 23971.83s, 	Step: 51787, 	{'train/accuracy': 0.9897343516349792, 'train/loss': 0.034360747784376144, 'train/mean_average_precision': 0.2915450888909189, 'validation/accuracy': 0.986326277256012, 'validation/loss': 0.045857734978199005, 'validation/mean_average_precision': 0.2213827129714896, 'validation/num_examples': 43793, 'test/accuracy': 0.9853882789611816, 'test/loss': 0.04878430813550949, 'test/mean_average_precision': 0.20694046165501986, 'test/num_examples': 43793, 'score': 16579.61815905571, 'total_duration': 23971.82929778099, 'accumulated_submission_time': 16579.61815905571, 'accumulated_eval_time': 7388.1657457351685, 'accumulated_logging_time': 2.646596670150757}
I0206 02:50:04.134964 140002205157120 logging_writer.py:48] [51787] accumulated_eval_time=7388.165746, accumulated_logging_time=2.646597, accumulated_submission_time=16579.618159, global_step=51787, preemption_count=0, score=16579.618159, test/accuracy=0.985388, test/loss=0.048784, test/mean_average_precision=0.206940, test/num_examples=43793, total_duration=23971.829298, train/accuracy=0.989734, train/loss=0.034361, train/mean_average_precision=0.291545, validation/accuracy=0.986326, validation/loss=0.045858, validation/mean_average_precision=0.221383, validation/num_examples=43793
I0206 02:50:09.252392 140002598295296 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.05615423247218132, loss=0.03355507552623749
I0206 02:50:42.006825 140002205157120 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.05647139996290207, loss=0.0340435691177845
I0206 02:51:14.609698 140002598295296 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.056080304086208344, loss=0.0341610424220562
I0206 02:51:46.877929 140002205157120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.07992206513881683, loss=0.03350389003753662
I0206 02:52:19.461468 140002598295296 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.04500988870859146, loss=0.03436720371246338
I0206 02:52:51.537123 140002205157120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.0617879219353199, loss=0.038002729415893555
I0206 02:53:23.869231 140002598295296 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.0928674042224884, loss=0.03724776580929756
I0206 02:53:56.105037 140002205157120 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.1057460755109787, loss=0.031129227951169014
I0206 02:54:04.193306 140205209478976 spec.py:321] Evaluating on the training split.
I0206 02:55:40.902791 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 02:55:43.950762 140205209478976 spec.py:349] Evaluating on the test split.
I0206 02:55:46.928821 140205209478976 submission_runner.py:408] Time since start: 24314.65s, 	Step: 52526, 	{'train/accuracy': 0.9898606538772583, 'train/loss': 0.03372828662395477, 'train/mean_average_precision': 0.308960692285714, 'validation/accuracy': 0.9864127039909363, 'validation/loss': 0.04572121053934097, 'validation/mean_average_precision': 0.22213817610781364, 'validation/num_examples': 43793, 'test/accuracy': 0.9855268597602844, 'test/loss': 0.04860956221818924, 'test/mean_average_precision': 0.212095926354634, 'test/num_examples': 43793, 'score': 16819.64329266548, 'total_duration': 24314.65186572075, 'accumulated_submission_time': 16819.64329266548, 'accumulated_eval_time': 7490.901193618774, 'accumulated_logging_time': 2.6872761249542236}
I0206 02:55:46.954234 139984133072640 logging_writer.py:48] [52526] accumulated_eval_time=7490.901194, accumulated_logging_time=2.687276, accumulated_submission_time=16819.643293, global_step=52526, preemption_count=0, score=16819.643293, test/accuracy=0.985527, test/loss=0.048610, test/mean_average_precision=0.212096, test/num_examples=43793, total_duration=24314.651866, train/accuracy=0.989861, train/loss=0.033728, train/mean_average_precision=0.308961, validation/accuracy=0.986413, validation/loss=0.045721, validation/mean_average_precision=0.222138, validation/num_examples=43793
I0206 02:56:11.087590 140020770670336 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.06035981699824333, loss=0.029106050729751587
I0206 02:56:42.923076 139984133072640 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.07258003205060959, loss=0.03275926411151886
I0206 02:57:15.201506 140020770670336 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.0778469368815422, loss=0.038925886154174805
I0206 02:57:46.773447 139984133072640 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.060197360813617706, loss=0.033557869493961334
I0206 02:58:18.514978 140020770670336 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.08874824643135071, loss=0.038030121475458145
I0206 02:58:50.302355 139984133072640 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.0768989771604538, loss=0.03254073113203049
I0206 02:59:21.945430 140020770670336 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.08027415722608566, loss=0.030431441962718964
I0206 02:59:47.203629 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:01:28.087077 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:01:31.097974 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:01:34.062828 140205209478976 submission_runner.py:408] Time since start: 24661.79s, 	Step: 53280, 	{'train/accuracy': 0.9900662302970886, 'train/loss': 0.03331868350505829, 'train/mean_average_precision': 0.30609795083218194, 'validation/accuracy': 0.9864590167999268, 'validation/loss': 0.04557448998093605, 'validation/mean_average_precision': 0.22433592060434396, 'validation/num_examples': 43793, 'test/accuracy': 0.9855159521102905, 'test/loss': 0.04845355451107025, 'test/mean_average_precision': 0.21344609651030216, 'test/num_examples': 43793, 'score': 17059.861709833145, 'total_duration': 24661.785895824432, 'accumulated_submission_time': 17059.861709833145, 'accumulated_eval_time': 7597.760349988937, 'accumulated_logging_time': 2.723623275756836}
I0206 03:01:34.088193 140002205157120 logging_writer.py:48] [53280] accumulated_eval_time=7597.760350, accumulated_logging_time=2.723623, accumulated_submission_time=17059.861710, global_step=53280, preemption_count=0, score=17059.861710, test/accuracy=0.985516, test/loss=0.048454, test/mean_average_precision=0.213446, test/num_examples=43793, total_duration=24661.785896, train/accuracy=0.990066, train/loss=0.033319, train/mean_average_precision=0.306098, validation/accuracy=0.986459, validation/loss=0.045574, validation/mean_average_precision=0.224336, validation/num_examples=43793
I0206 03:01:40.767651 140021163808512 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.08434158563613892, loss=0.03031853772699833
I0206 03:02:12.247540 140002205157120 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.05981238931417465, loss=0.0350053608417511
I0206 03:02:44.112880 140021163808512 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.06455720216035843, loss=0.03427944704890251
I0206 03:03:15.773320 140002205157120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.10103942453861237, loss=0.033979449421167374
I0206 03:03:47.371253 140021163808512 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.09060147404670715, loss=0.03439783677458763
I0206 03:04:21.588712 140002205157120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.0709681510925293, loss=0.030557123944163322
I0206 03:04:53.993971 140021163808512 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.06138533726334572, loss=0.03342843055725098
I0206 03:05:25.845727 140002205157120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.08739377558231354, loss=0.035449784249067307
I0206 03:05:34.109999 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:07:12.000927 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:07:15.012479 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:07:17.984407 140205209478976 submission_runner.py:408] Time since start: 25005.71s, 	Step: 54027, 	{'train/accuracy': 0.9900087714195251, 'train/loss': 0.033369824290275574, 'train/mean_average_precision': 0.30490107379197173, 'validation/accuracy': 0.9865007996559143, 'validation/loss': 0.045682504773139954, 'validation/mean_average_precision': 0.22642147932658865, 'validation/num_examples': 43793, 'test/accuracy': 0.985465407371521, 'test/loss': 0.04903227835893631, 'test/mean_average_precision': 0.20949288392663823, 'test/num_examples': 43793, 'score': 17299.852272987366, 'total_duration': 25005.707471370697, 'accumulated_submission_time': 17299.852272987366, 'accumulated_eval_time': 7701.634706735611, 'accumulated_logging_time': 2.7599589824676514}
I0206 03:07:18.009701 139984133072640 logging_writer.py:48] [54027] accumulated_eval_time=7701.634707, accumulated_logging_time=2.759959, accumulated_submission_time=17299.852273, global_step=54027, preemption_count=0, score=17299.852273, test/accuracy=0.985465, test/loss=0.049032, test/mean_average_precision=0.209493, test/num_examples=43793, total_duration=25005.707471, train/accuracy=0.990009, train/loss=0.033370, train/mean_average_precision=0.304901, validation/accuracy=0.986501, validation/loss=0.045683, validation/mean_average_precision=0.226421, validation/num_examples=43793
I0206 03:07:41.634486 140020770670336 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.0870775654911995, loss=0.03272446244955063
I0206 03:08:13.605763 139984133072640 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.07811073958873749, loss=0.03455428034067154
I0206 03:08:45.212869 140020770670336 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.06267748773097992, loss=0.03529614582657814
I0206 03:09:16.788017 139984133072640 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.06357773393392563, loss=0.036040570586919785
I0206 03:09:48.954348 140020770670336 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.13901670277118683, loss=0.033996909856796265
I0206 03:10:20.789437 139984133072640 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.08607019484043121, loss=0.033607810735702515
I0206 03:10:52.754775 140020770670336 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.05916418880224228, loss=0.03435591608285904
I0206 03:11:18.032012 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:12:53.607248 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:12:56.603250 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:12:59.560324 140205209478976 submission_runner.py:408] Time since start: 25347.28s, 	Step: 54781, 	{'train/accuracy': 0.9901589155197144, 'train/loss': 0.03296329081058502, 'train/mean_average_precision': 0.3163235677717774, 'validation/accuracy': 0.9865801930427551, 'validation/loss': 0.045562393963336945, 'validation/mean_average_precision': 0.22805262338495752, 'validation/num_examples': 43793, 'test/accuracy': 0.9855460524559021, 'test/loss': 0.04861282929778099, 'test/mean_average_precision': 0.21454710113569686, 'test/num_examples': 43793, 'score': 17539.843241930008, 'total_duration': 25347.28339076042, 'accumulated_submission_time': 17539.843241930008, 'accumulated_eval_time': 7803.162973642349, 'accumulated_logging_time': 2.7964839935302734}
I0206 03:12:59.586262 140002205157120 logging_writer.py:48] [54781] accumulated_eval_time=7803.162974, accumulated_logging_time=2.796484, accumulated_submission_time=17539.843242, global_step=54781, preemption_count=0, score=17539.843242, test/accuracy=0.985546, test/loss=0.048613, test/mean_average_precision=0.214547, test/num_examples=43793, total_duration=25347.283391, train/accuracy=0.990159, train/loss=0.032963, train/mean_average_precision=0.316324, validation/accuracy=0.986580, validation/loss=0.045562, validation/mean_average_precision=0.228053, validation/num_examples=43793
I0206 03:13:05.983186 140002598295296 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.09948625415563583, loss=0.0317583903670311
I0206 03:13:37.769900 140002205157120 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.1259487271308899, loss=0.0360092893242836
I0206 03:14:09.627222 140002598295296 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.05961804836988449, loss=0.03333587571978569
I0206 03:14:41.620835 140002205157120 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.08617260307073593, loss=0.03553496673703194
I0206 03:15:13.665349 140002598295296 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.08278854936361313, loss=0.03371523320674896
I0206 03:15:45.449031 140002205157120 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.11464524269104004, loss=0.03212755173444748
I0206 03:16:17.577937 140002598295296 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.06141176074743271, loss=0.03682851418852806
I0206 03:16:49.481227 140002205157120 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.06124724820256233, loss=0.03212525323033333
I0206 03:16:59.665625 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:18:40.953750 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:18:43.946183 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:18:46.957844 140205209478976 submission_runner.py:408] Time since start: 25694.68s, 	Step: 55533, 	{'train/accuracy': 0.990270733833313, 'train/loss': 0.03255586326122284, 'train/mean_average_precision': 0.32371953494994204, 'validation/accuracy': 0.9865257740020752, 'validation/loss': 0.04533666744828224, 'validation/mean_average_precision': 0.23214892783435964, 'validation/num_examples': 43793, 'test/accuracy': 0.9855285286903381, 'test/loss': 0.04859936237335205, 'test/mean_average_precision': 0.21753470205782885, 'test/num_examples': 43793, 'score': 17779.891257047653, 'total_duration': 25694.680911540985, 'accumulated_submission_time': 17779.891257047653, 'accumulated_eval_time': 7910.455152988434, 'accumulated_logging_time': 2.8337621688842773}
I0206 03:18:46.983703 139984133072640 logging_writer.py:48] [55533] accumulated_eval_time=7910.455153, accumulated_logging_time=2.833762, accumulated_submission_time=17779.891257, global_step=55533, preemption_count=0, score=17779.891257, test/accuracy=0.985529, test/loss=0.048599, test/mean_average_precision=0.217535, test/num_examples=43793, total_duration=25694.680912, train/accuracy=0.990271, train/loss=0.032556, train/mean_average_precision=0.323720, validation/accuracy=0.986526, validation/loss=0.045337, validation/mean_average_precision=0.232149, validation/num_examples=43793
I0206 03:19:08.621534 140020770670336 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.07664044946432114, loss=0.0321982316672802
I0206 03:19:40.693542 139984133072640 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.07684686034917831, loss=0.03508457541465759
I0206 03:20:13.266930 140020770670336 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.10698258131742477, loss=0.030458636581897736
I0206 03:20:44.885424 139984133072640 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.11698421090841293, loss=0.028948599472641945
I0206 03:21:16.790563 140020770670336 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.06310935318470001, loss=0.0299579668790102
I0206 03:21:48.698305 139984133072640 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.07194962352514267, loss=0.033873964101076126
I0206 03:22:20.485224 140020770670336 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.07436297088861465, loss=0.03425462916493416
I0206 03:22:47.178109 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:24:23.137233 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:24:26.220631 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:24:29.242020 140205209478976 submission_runner.py:408] Time since start: 26036.97s, 	Step: 56285, 	{'train/accuracy': 0.9902195930480957, 'train/loss': 0.032643094658851624, 'train/mean_average_precision': 0.3289052356829938, 'validation/accuracy': 0.9864122867584229, 'validation/loss': 0.045647431164979935, 'validation/mean_average_precision': 0.23472729870815293, 'validation/num_examples': 43793, 'test/accuracy': 0.9855323433876038, 'test/loss': 0.04870383068919182, 'test/mean_average_precision': 0.2174664089875856, 'test/num_examples': 43793, 'score': 18020.05457186699, 'total_duration': 26036.965087890625, 'accumulated_submission_time': 18020.05457186699, 'accumulated_eval_time': 8012.519022226334, 'accumulated_logging_time': 2.870720386505127}
I0206 03:24:29.268585 140002205157120 logging_writer.py:48] [56285] accumulated_eval_time=8012.519022, accumulated_logging_time=2.870720, accumulated_submission_time=18020.054572, global_step=56285, preemption_count=0, score=18020.054572, test/accuracy=0.985532, test/loss=0.048704, test/mean_average_precision=0.217466, test/num_examples=43793, total_duration=26036.965088, train/accuracy=0.990220, train/loss=0.032643, train/mean_average_precision=0.328905, validation/accuracy=0.986412, validation/loss=0.045647, validation/mean_average_precision=0.234727, validation/num_examples=43793
I0206 03:24:34.361706 140002598295296 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.07308221608400345, loss=0.03064405359327793
I0206 03:25:06.032691 140002205157120 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.05757693573832512, loss=0.03175542876124382
I0206 03:25:38.021814 140002598295296 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.07601844519376755, loss=0.03136466071009636
I0206 03:26:09.940914 140002205157120 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.077063649892807, loss=0.03359203785657883
I0206 03:26:41.830343 140002598295296 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.0972406268119812, loss=0.032536283135414124
I0206 03:27:13.990722 140002205157120 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.08214008063077927, loss=0.02991768717765808
I0206 03:27:45.448125 140002598295296 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.09393139183521271, loss=0.03725840523838997
I0206 03:28:17.372222 140002205157120 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.07727213203907013, loss=0.03580569103360176
I0206 03:28:29.525848 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:30:08.316499 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:30:11.436558 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:30:14.484012 140205209478976 submission_runner.py:408] Time since start: 26382.21s, 	Step: 57039, 	{'train/accuracy': 0.9902625679969788, 'train/loss': 0.032707177102565765, 'train/mean_average_precision': 0.3176695183003912, 'validation/accuracy': 0.9864926934242249, 'validation/loss': 0.04545671492815018, 'validation/mean_average_precision': 0.2280582942071802, 'validation/num_examples': 43793, 'test/accuracy': 0.9855504631996155, 'test/loss': 0.04861481487751007, 'test/mean_average_precision': 0.2146260371624704, 'test/num_examples': 43793, 'score': 18260.280821561813, 'total_duration': 26382.207079172134, 'accumulated_submission_time': 18260.280821561813, 'accumulated_eval_time': 8117.47713804245, 'accumulated_logging_time': 2.908522367477417}
I0206 03:30:14.509582 139984133072640 logging_writer.py:48] [57039] accumulated_eval_time=8117.477138, accumulated_logging_time=2.908522, accumulated_submission_time=18260.280822, global_step=57039, preemption_count=0, score=18260.280822, test/accuracy=0.985550, test/loss=0.048615, test/mean_average_precision=0.214626, test/num_examples=43793, total_duration=26382.207079, train/accuracy=0.990263, train/loss=0.032707, train/mean_average_precision=0.317670, validation/accuracy=0.986493, validation/loss=0.045457, validation/mean_average_precision=0.228058, validation/num_examples=43793
I0206 03:30:34.450552 140020770670336 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.10217946022748947, loss=0.028825614601373672
I0206 03:31:06.669185 139984133072640 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.05780443921685219, loss=0.032747186720371246
I0206 03:31:38.242768 140020770670336 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.06551510095596313, loss=0.030967026948928833
I0206 03:32:09.555253 139984133072640 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.08769005537033081, loss=0.029658032581210136
I0206 03:32:41.560045 140020770670336 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.0767083615064621, loss=0.03666204214096069
I0206 03:33:14.286101 139984133072640 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.07890093326568604, loss=0.03251194581389427
I0206 03:33:46.804918 140020770670336 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.0950976014137268, loss=0.032574381679296494
I0206 03:33:51.311651 139984133072640 logging_writer.py:48] [57715] global_step=57715, preemption_count=0, score=18477.033698
I0206 03:33:51.365543 140205209478976 checkpoints.py:490] Saving checkpoint at step: 57715
I0206 03:33:51.489180 140205209478976 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_4/checkpoint_57715
I0206 03:33:51.490367 140205209478976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_4/checkpoint_57715.
I0206 03:33:51.686061 140205209478976 submission_runner.py:583] Tuning trial 4/5
I0206 03:33:51.686355 140205209478976 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0206 03:33:51.698243 140205209478976 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5091689229011536, 'train/loss': 0.7391471266746521, 'train/mean_average_precision': 0.022770563417320786, 'validation/accuracy': 0.5064647793769836, 'validation/loss': 0.7422076463699341, 'validation/mean_average_precision': 0.025298830402132374, 'validation/num_examples': 43793, 'test/accuracy': 0.5047287344932556, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.02693114158079488, 'test/num_examples': 43793, 'score': 12.185586929321289, 'total_duration': 119.98681426048279, 'accumulated_submission_time': 12.185586929321289, 'accumulated_eval_time': 107.80116128921509, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (745, {'train/accuracy': 0.9868181943893433, 'train/loss': 0.052752770483493805, 'train/mean_average_precision': 0.04831717073437651, 'validation/accuracy': 0.9841492176055908, 'validation/loss': 0.06301576644182205, 'validation/mean_average_precision': 0.04771911249913853, 'validation/num_examples': 43793, 'test/accuracy': 0.9831492304801941, 'test/loss': 0.06620319187641144, 'test/mean_average_precision': 0.0460700617813824, 'test/num_examples': 43793, 'score': 252.15884280204773, 'total_duration': 467.61569714546204, 'accumulated_submission_time': 252.15884280204773, 'accumulated_eval_time': 215.41246700286865, 'accumulated_logging_time': 0.024070024490356445, 'global_step': 745, 'preemption_count': 0}), (1490, {'train/accuracy': 0.9869986772537231, 'train/loss': 0.04919382929801941, 'train/mean_average_precision': 0.07801795809218251, 'validation/accuracy': 0.9842535257339478, 'validation/loss': 0.05856533348560333, 'validation/mean_average_precision': 0.07444615138947651, 'validation/num_examples': 43793, 'test/accuracy': 0.9832949638366699, 'test/loss': 0.061599891632795334, 'test/mean_average_precision': 0.0745604968623183, 'test/num_examples': 43793, 'score': 492.1526997089386, 'total_duration': 813.9127051830292, 'accumulated_submission_time': 492.1526997089386, 'accumulated_eval_time': 321.669025182724, 'accumulated_logging_time': 0.05089735984802246, 'global_step': 1490, 'preemption_count': 0}), (2240, {'train/accuracy': 0.9874287843704224, 'train/loss': 0.04568753018975258, 'train/mean_average_precision': 0.11178136272470766, 'validation/accuracy': 0.9846611022949219, 'validation/loss': 0.05566570162773132, 'validation/mean_average_precision': 0.1072617279992498, 'validation/num_examples': 43793, 'test/accuracy': 0.9836757183074951, 'test/loss': 0.059133052825927734, 'test/mean_average_precision': 0.10345936149153655, 'test/num_examples': 43793, 'score': 732.1452407836914, 'total_duration': 1161.3493824005127, 'accumulated_submission_time': 732.1452407836914, 'accumulated_eval_time': 429.0665693283081, 'accumulated_logging_time': 0.07750868797302246, 'global_step': 2240, 'preemption_count': 0}), (2992, {'train/accuracy': 0.9878295660018921, 'train/loss': 0.04290998727083206, 'train/mean_average_precision': 0.14909766748097233, 'validation/accuracy': 0.9849030375480652, 'validation/loss': 0.053014665842056274, 'validation/mean_average_precision': 0.12961651864892743, 'validation/num_examples': 43793, 'test/accuracy': 0.9839111566543579, 'test/loss': 0.055954184383153915, 'test/mean_average_precision': 0.13367850725457064, 'test/num_examples': 43793, 'score': 972.3031947612762, 'total_duration': 1504.4612703323364, 'accumulated_submission_time': 972.3031947612762, 'accumulated_eval_time': 531.9732112884521, 'accumulated_logging_time': 0.10485291481018066, 'global_step': 2992, 'preemption_count': 0}), (3743, {'train/accuracy': 0.9880852103233337, 'train/loss': 0.04129993915557861, 'train/mean_average_precision': 0.16831039361344796, 'validation/accuracy': 0.9852334856987, 'validation/loss': 0.0507333017885685, 'validation/mean_average_precision': 0.14941331378283598, 'validation/num_examples': 43793, 'test/accuracy': 0.9843466877937317, 'test/loss': 0.05367879569530487, 'test/mean_average_precision': 0.15127811692986542, 'test/num_examples': 43793, 'score': 1212.3853332996368, 'total_duration': 1850.2039790153503, 'accumulated_submission_time': 1212.3853332996368, 'accumulated_eval_time': 637.5870983600616, 'accumulated_logging_time': 0.13191795349121094, 'global_step': 3743, 'preemption_count': 0}), (4498, {'train/accuracy': 0.9884041547775269, 'train/loss': 0.0404745452105999, 'train/mean_average_precision': 0.19482182188892394, 'validation/accuracy': 0.9853795766830444, 'validation/loss': 0.05005882307887077, 'validation/mean_average_precision': 0.15740818307328774, 'validation/num_examples': 43793, 'test/accuracy': 0.9844532608985901, 'test/loss': 0.05273740738630295, 'test/mean_average_precision': 0.15637513145419285, 'test/num_examples': 43793, 'score': 1452.6483166217804, 'total_duration': 2197.36225771904, 'accumulated_submission_time': 1452.6483166217804, 'accumulated_eval_time': 744.434494972229, 'accumulated_logging_time': 0.1595597267150879, 'global_step': 4498, 'preemption_count': 0}), (5254, {'train/accuracy': 0.9886635541915894, 'train/loss': 0.03942786902189255, 'train/mean_average_precision': 0.19027711590283466, 'validation/accuracy': 0.9854867458343506, 'validation/loss': 0.049740225076675415, 'validation/mean_average_precision': 0.16596522844191303, 'validation/num_examples': 43793, 'test/accuracy': 0.9845661520957947, 'test/loss': 0.05233199894428253, 'test/mean_average_precision': 0.17087559116697543, 'test/num_examples': 43793, 'score': 1692.851990699768, 'total_duration': 2544.280503988266, 'accumulated_submission_time': 1692.851990699768, 'accumulated_eval_time': 851.1025938987732, 'accumulated_logging_time': 0.18645191192626953, 'global_step': 5254, 'preemption_count': 0}), (6008, {'train/accuracy': 0.9886347055435181, 'train/loss': 0.03914286568760872, 'train/mean_average_precision': 0.19986740394123362, 'validation/accuracy': 0.9856597185134888, 'validation/loss': 0.04885830730199814, 'validation/mean_average_precision': 0.18008463135684, 'validation/num_examples': 43793, 'test/accuracy': 0.9846979379653931, 'test/loss': 0.0519571416079998, 'test/mean_average_precision': 0.17276667524665232, 'test/num_examples': 43793, 'score': 1933.0624508857727, 'total_duration': 2888.3051455020905, 'accumulated_submission_time': 1933.0624508857727, 'accumulated_eval_time': 954.8695249557495, 'accumulated_logging_time': 0.21353363990783691, 'global_step': 6008, 'preemption_count': 0}), (6753, {'train/accuracy': 0.9883594512939453, 'train/loss': 0.04015161469578743, 'train/mean_average_precision': 0.192082496409469, 'validation/accuracy': 0.9853593111038208, 'validation/loss': 0.04985901713371277, 'validation/mean_average_precision': 0.16853419468052408, 'validation/num_examples': 43793, 'test/accuracy': 0.9844822883605957, 'test/loss': 0.05259844660758972, 'test/mean_average_precision': 0.16261873750826217, 'test/num_examples': 43793, 'score': 2173.258081674576, 'total_duration': 3235.989268064499, 'accumulated_submission_time': 2173.258081674576, 'accumulated_eval_time': 1062.309351682663, 'accumulated_logging_time': 0.24247026443481445, 'global_step': 6753, 'preemption_count': 0}), (7492, {'train/accuracy': 0.9885475635528564, 'train/loss': 0.03908183053135872, 'train/mean_average_precision': 0.19995834331785922, 'validation/accuracy': 0.9856422543525696, 'validation/loss': 0.048810314387083054, 'validation/mean_average_precision': 0.18285298346840118, 'validation/num_examples': 43793, 'test/accuracy': 0.9846469759941101, 'test/loss': 0.05169987678527832, 'test/mean_average_precision': 0.18459043939620856, 'test/num_examples': 43793, 'score': 2413.3347930908203, 'total_duration': 3580.518236398697, 'accumulated_submission_time': 2413.3347930908203, 'accumulated_eval_time': 1166.7139718532562, 'accumulated_logging_time': 0.2698376178741455, 'global_step': 7492, 'preemption_count': 0}), (8243, {'train/accuracy': 0.9886873364448547, 'train/loss': 0.03875712677836418, 'train/mean_average_precision': 0.21149383792245036, 'validation/accuracy': 0.9858253002166748, 'validation/loss': 0.048680491745471954, 'validation/mean_average_precision': 0.18361861906994822, 'validation/num_examples': 43793, 'test/accuracy': 0.9848407506942749, 'test/loss': 0.0517675057053566, 'test/mean_average_precision': 0.18188655629942915, 'test/num_examples': 43793, 'score': 2653.599544286728, 'total_duration': 3929.0323138237, 'accumulated_submission_time': 2653.599544286728, 'accumulated_eval_time': 1274.9142456054688, 'accumulated_logging_time': 0.29875755310058594, 'global_step': 8243, 'preemption_count': 0}), (9003, {'train/accuracy': 0.9886422753334045, 'train/loss': 0.03899676725268364, 'train/mean_average_precision': 0.20870463578473877, 'validation/accuracy': 0.9857802391052246, 'validation/loss': 0.04873434081673622, 'validation/mean_average_precision': 0.18945347842751153, 'validation/num_examples': 43793, 'test/accuracy': 0.9847792387008667, 'test/loss': 0.05149604007601738, 'test/mean_average_precision': 0.19200215243796978, 'test/num_examples': 43793, 'score': 2893.7972707748413, 'total_duration': 4277.550271987915, 'accumulated_submission_time': 2893.7972707748413, 'accumulated_eval_time': 1383.1852016448975, 'accumulated_logging_time': 0.3282186985015869, 'global_step': 9003, 'preemption_count': 0}), (9760, {'train/accuracy': 0.9888356328010559, 'train/loss': 0.03835595026612282, 'train/mean_average_precision': 0.2161694728775924, 'validation/accuracy': 0.9855862259864807, 'validation/loss': 0.048749759793281555, 'validation/mean_average_precision': 0.18400181375737143, 'validation/num_examples': 43793, 'test/accuracy': 0.9846423864364624, 'test/loss': 0.05163241922855377, 'test/mean_average_precision': 0.18344304914426365, 'test/num_examples': 43793, 'score': 3133.9032673835754, 'total_duration': 4625.546536207199, 'accumulated_submission_time': 3133.9032673835754, 'accumulated_eval_time': 1491.027760028839, 'accumulated_logging_time': 0.3559896945953369, 'global_step': 9760, 'preemption_count': 0}), (10503, {'train/accuracy': 0.9887315034866333, 'train/loss': 0.038301631808280945, 'train/mean_average_precision': 0.22690173713484513, 'validation/accuracy': 0.9857413172721863, 'validation/loss': 0.04893196374177933, 'validation/mean_average_precision': 0.18965706404621746, 'validation/num_examples': 43793, 'test/accuracy': 0.9848563075065613, 'test/loss': 0.05198470503091812, 'test/mean_average_precision': 0.1847305208964719, 'test/num_examples': 43793, 'score': 3374.0375757217407, 'total_duration': 4971.461939096451, 'accumulated_submission_time': 3374.0375757217407, 'accumulated_eval_time': 1596.7612025737762, 'accumulated_logging_time': 0.384077787399292, 'global_step': 10503, 'preemption_count': 0}), (11254, {'train/accuracy': 0.9888266921043396, 'train/loss': 0.038045983761548996, 'train/mean_average_precision': 0.22985548116505886, 'validation/accuracy': 0.9857701063156128, 'validation/loss': 0.04840954765677452, 'validation/mean_average_precision': 0.1863925213489288, 'validation/num_examples': 43793, 'test/accuracy': 0.9848538041114807, 'test/loss': 0.05127113312482834, 'test/mean_average_precision': 0.18452970367097346, 'test/num_examples': 43793, 'score': 3614.018998861313, 'total_duration': 5320.525203466415, 'accumulated_submission_time': 3614.018998861313, 'accumulated_eval_time': 1705.7927422523499, 'accumulated_logging_time': 0.4135866165161133, 'global_step': 11254, 'preemption_count': 0}), (12003, {'train/accuracy': 0.988034188747406, 'train/loss': 0.040260523557662964, 'train/mean_average_precision': 0.21595434903574937, 'validation/accuracy': 0.9848340153694153, 'validation/loss': 0.05086250230669975, 'validation/mean_average_precision': 0.17195741729684832, 'validation/num_examples': 43793, 'test/accuracy': 0.9838365912437439, 'test/loss': 0.05383049696683884, 'test/mean_average_precision': 0.17677506274615856, 'test/num_examples': 43793, 'score': 3854.225952386856, 'total_duration': 5665.794174194336, 'accumulated_submission_time': 3854.225952386856, 'accumulated_eval_time': 1810.8058123588562, 'accumulated_logging_time': 0.4426114559173584, 'global_step': 12003, 'preemption_count': 0}), (12752, {'train/accuracy': 0.9886979460716248, 'train/loss': 0.038513973355293274, 'train/mean_average_precision': 0.212413276491084, 'validation/accuracy': 0.9853869080543518, 'validation/loss': 0.0496855191886425, 'validation/mean_average_precision': 0.18165653618837238, 'validation/num_examples': 43793, 'test/accuracy': 0.9844094514846802, 'test/loss': 0.05285492539405823, 'test/mean_average_precision': 0.18299348088723388, 'test/num_examples': 43793, 'score': 4094.2330226898193, 'total_duration': 6012.367550611496, 'accumulated_submission_time': 4094.2330226898193, 'accumulated_eval_time': 1917.3220510482788, 'accumulated_logging_time': 0.47250795364379883, 'global_step': 12752, 'preemption_count': 0}), (13508, {'train/accuracy': 0.98878014087677, 'train/loss': 0.03843506798148155, 'train/mean_average_precision': 0.2341694003337173, 'validation/accuracy': 0.9857327938079834, 'validation/loss': 0.04841131716966629, 'validation/mean_average_precision': 0.18682739612154003, 'validation/num_examples': 43793, 'test/accuracy': 0.9848297834396362, 'test/loss': 0.05098680779337883, 'test/mean_average_precision': 0.1925377068346132, 'test/num_examples': 43793, 'score': 4334.211813926697, 'total_duration': 6360.479429960251, 'accumulated_submission_time': 4334.211813926697, 'accumulated_eval_time': 2025.4055910110474, 'accumulated_logging_time': 0.5020263195037842, 'global_step': 13508, 'preemption_count': 0}), (14261, {'train/accuracy': 0.9888355135917664, 'train/loss': 0.038366395980119705, 'train/mean_average_precision': 0.22154629846613177, 'validation/accuracy': 0.9857721328735352, 'validation/loss': 0.04902218282222748, 'validation/mean_average_precision': 0.18127468605978142, 'validation/num_examples': 43793, 'test/accuracy': 0.9848424196243286, 'test/loss': 0.05206082761287689, 'test/mean_average_precision': 0.17770151589102587, 'test/num_examples': 43793, 'score': 4574.353425979614, 'total_duration': 6709.469097614288, 'accumulated_submission_time': 4574.353425979614, 'accumulated_eval_time': 2134.2025485038757, 'accumulated_logging_time': 0.5312979221343994, 'global_step': 14261, 'preemption_count': 0}), (14993, {'train/accuracy': 0.9888824820518494, 'train/loss': 0.03792348504066467, 'train/mean_average_precision': 0.2257129817583256, 'validation/accuracy': 0.9858293533325195, 'validation/loss': 0.04814564436674118, 'validation/mean_average_precision': 0.19155246914254173, 'validation/num_examples': 43793, 'test/accuracy': 0.984917402267456, 'test/loss': 0.0510648749768734, 'test/mean_average_precision': 0.18411476735013166, 'test/num_examples': 43793, 'score': 4814.332322835922, 'total_duration': 7060.149246931076, 'accumulated_submission_time': 4814.332322835922, 'accumulated_eval_time': 2244.846806049347, 'accumulated_logging_time': 0.5642621517181396, 'global_step': 14993, 'preemption_count': 0}), (15741, {'train/accuracy': 0.9887647032737732, 'train/loss': 0.03873090818524361, 'train/mean_average_precision': 0.20830259584203645, 'validation/accuracy': 0.9856645464897156, 'validation/loss': 0.0490984246134758, 'validation/mean_average_precision': 0.1780559326371, 'validation/num_examples': 43793, 'test/accuracy': 0.9846630096435547, 'test/loss': 0.05220545455813408, 'test/mean_average_precision': 0.17087375222694276, 'test/num_examples': 43793, 'score': 5054.521810770035, 'total_duration': 7405.3446571826935, 'accumulated_submission_time': 5054.521810770035, 'accumulated_eval_time': 2349.802227497101, 'accumulated_logging_time': 0.5941531658172607, 'global_step': 15741, 'preemption_count': 0}), (16489, {'train/accuracy': 0.9889504313468933, 'train/loss': 0.03789043799042702, 'train/mean_average_precision': 0.21556878954663675, 'validation/accuracy': 0.9857696890830994, 'validation/loss': 0.048670437186956406, 'validation/mean_average_precision': 0.18256488450566627, 'validation/num_examples': 43793, 'test/accuracy': 0.9848487377166748, 'test/loss': 0.05156475678086281, 'test/mean_average_precision': 0.17534660005543087, 'test/num_examples': 43793, 'score': 5294.745937824249, 'total_duration': 7752.313290119171, 'accumulated_submission_time': 5294.745937824249, 'accumulated_eval_time': 2456.4971837997437, 'accumulated_logging_time': 0.624169111251831, 'global_step': 16489, 'preemption_count': 0}), (17242, {'train/accuracy': 0.9890180826187134, 'train/loss': 0.03764190897345543, 'train/mean_average_precision': 0.2201965623201106, 'validation/accuracy': 0.9857839345932007, 'validation/loss': 0.0482412688434124, 'validation/mean_average_precision': 0.18357224516518592, 'validation/num_examples': 43793, 'test/accuracy': 0.9849043488502502, 'test/loss': 0.05109306052327156, 'test/mean_average_precision': 0.18187410407147248, 'test/num_examples': 43793, 'score': 5534.832649469376, 'total_duration': 8097.196264505386, 'accumulated_submission_time': 5534.832649469376, 'accumulated_eval_time': 2561.2436380386353, 'accumulated_logging_time': 0.6534569263458252, 'global_step': 17242, 'preemption_count': 0}), (17989, {'train/accuracy': 0.9887692928314209, 'train/loss': 0.03804321587085724, 'train/mean_average_precision': 0.22739804622722806, 'validation/accuracy': 0.9855777025222778, 'validation/loss': 0.04910195618867874, 'validation/mean_average_precision': 0.18627496475117836, 'validation/num_examples': 43793, 'test/accuracy': 0.9846507906913757, 'test/loss': 0.05218307301402092, 'test/mean_average_precision': 0.1831122481482856, 'test/num_examples': 43793, 'score': 5775.0447998046875, 'total_duration': 8445.953789234161, 'accumulated_submission_time': 5775.0447998046875, 'accumulated_eval_time': 2669.7339446544647, 'accumulated_logging_time': 0.6886923313140869, 'global_step': 17989, 'preemption_count': 0}), (18740, {'train/accuracy': 0.9890387058258057, 'train/loss': 0.03707514703273773, 'train/mean_average_precision': 0.24299820940443767, 'validation/accuracy': 0.9859284162521362, 'validation/loss': 0.04772140830755234, 'validation/mean_average_precision': 0.19121262066486283, 'validation/num_examples': 43793, 'test/accuracy': 0.984965443611145, 'test/loss': 0.050750963389873505, 'test/mean_average_precision': 0.1886865838804157, 'test/num_examples': 43793, 'score': 6015.091109275818, 'total_duration': 8792.135896921158, 'accumulated_submission_time': 6015.091109275818, 'accumulated_eval_time': 2775.819948196411, 'accumulated_logging_time': 0.7185335159301758, 'global_step': 18740, 'preemption_count': 0}), (19487, {'train/accuracy': 0.9886627197265625, 'train/loss': 0.038342516869306564, 'train/mean_average_precision': 0.2338148047161424, 'validation/accuracy': 0.9854279160499573, 'validation/loss': 0.049056440591812134, 'validation/mean_average_precision': 0.18009676570737515, 'validation/num_examples': 43793, 'test/accuracy': 0.9846385717391968, 'test/loss': 0.051957037299871445, 'test/mean_average_precision': 0.17793733536948328, 'test/num_examples': 43793, 'score': 6255.089984178543, 'total_duration': 9137.301024913788, 'accumulated_submission_time': 6255.089984178543, 'accumulated_eval_time': 2880.9350163936615, 'accumulated_logging_time': 0.7496731281280518, 'global_step': 19487, 'preemption_count': 0}), (20231, {'train/accuracy': 0.988877534866333, 'train/loss': 0.03777388483285904, 'train/mean_average_precision': 0.22414921413911615, 'validation/accuracy': 0.985895574092865, 'validation/loss': 0.048128772526979446, 'validation/mean_average_precision': 0.19038625336157286, 'validation/num_examples': 43793, 'test/accuracy': 0.9849616289138794, 'test/loss': 0.05130887031555176, 'test/mean_average_precision': 0.18830933830428093, 'test/num_examples': 43793, 'score': 6495.277824878693, 'total_duration': 9486.45058965683, 'accumulated_submission_time': 6495.277824878693, 'accumulated_eval_time': 2989.8407900333405, 'accumulated_logging_time': 0.7810525894165039, 'global_step': 20231, 'preemption_count': 0}), (20989, {'train/accuracy': 0.9890268445014954, 'train/loss': 0.037681374698877335, 'train/mean_average_precision': 0.2302380819582901, 'validation/accuracy': 0.9859223365783691, 'validation/loss': 0.04777001217007637, 'validation/mean_average_precision': 0.19523066530024943, 'validation/num_examples': 43793, 'test/accuracy': 0.9849637150764465, 'test/loss': 0.05084994435310364, 'test/mean_average_precision': 0.19295053350812078, 'test/num_examples': 43793, 'score': 6735.258136510849, 'total_duration': 9832.98583817482, 'accumulated_submission_time': 6735.258136510849, 'accumulated_eval_time': 3096.3421173095703, 'accumulated_logging_time': 0.8141474723815918, 'global_step': 20989, 'preemption_count': 0}), (21742, {'train/accuracy': 0.9889109134674072, 'train/loss': 0.03795648366212845, 'train/mean_average_precision': 0.228778463186817, 'validation/accuracy': 0.9857218265533447, 'validation/loss': 0.04850354790687561, 'validation/mean_average_precision': 0.18899655215754246, 'validation/num_examples': 43793, 'test/accuracy': 0.9847792387008667, 'test/loss': 0.051432929933071136, 'test/mean_average_precision': 0.1829139607213623, 'test/num_examples': 43793, 'score': 6975.445188045502, 'total_duration': 10181.09177494049, 'accumulated_submission_time': 6975.445188045502, 'accumulated_eval_time': 3204.208570957184, 'accumulated_logging_time': 0.8466379642486572, 'global_step': 21742, 'preemption_count': 0}), (22481, {'train/accuracy': 0.9888846278190613, 'train/loss': 0.0379314087331295, 'train/mean_average_precision': 0.2274528755384141, 'validation/accuracy': 0.9856629371643066, 'validation/loss': 0.048899173736572266, 'validation/mean_average_precision': 0.18542973380631414, 'validation/num_examples': 43793, 'test/accuracy': 0.984722375869751, 'test/loss': 0.05183769017457962, 'test/mean_average_precision': 0.18563354908539184, 'test/num_examples': 43793, 'score': 7215.630878448486, 'total_duration': 10527.035320281982, 'accumulated_submission_time': 7215.630878448486, 'accumulated_eval_time': 3309.9134817123413, 'accumulated_logging_time': 0.8789269924163818, 'global_step': 22481, 'preemption_count': 0}), (23233, {'train/accuracy': 0.9888824224472046, 'train/loss': 0.03774528577923775, 'train/mean_average_precision': 0.23477738939757817, 'validation/accuracy': 0.9858500957489014, 'validation/loss': 0.04834772273898125, 'validation/mean_average_precision': 0.19096310950721257, 'validation/num_examples': 43793, 'test/accuracy': 0.9848858118057251, 'test/loss': 0.05156517028808594, 'test/mean_average_precision': 0.18543619627329147, 'test/num_examples': 43793, 'score': 7455.803935050964, 'total_duration': 10871.868999242783, 'accumulated_submission_time': 7455.803935050964, 'accumulated_eval_time': 3414.523555278778, 'accumulated_logging_time': 0.9095168113708496, 'global_step': 23233, 'preemption_count': 0}), (23985, {'train/accuracy': 0.9890407919883728, 'train/loss': 0.0375085286796093, 'train/mean_average_precision': 0.23146811830159914, 'validation/accuracy': 0.9859633445739746, 'validation/loss': 0.04745250940322876, 'validation/mean_average_precision': 0.19063053395112928, 'validation/num_examples': 43793, 'test/accuracy': 0.9850140810012817, 'test/loss': 0.050241898745298386, 'test/mean_average_precision': 0.18285316336152527, 'test/num_examples': 43793, 'score': 7695.835782766342, 'total_duration': 11215.373279094696, 'accumulated_submission_time': 7695.835782766342, 'accumulated_eval_time': 3517.945199251175, 'accumulated_logging_time': 0.9403893947601318, 'global_step': 23985, 'preemption_count': 0}), (24741, {'train/accuracy': 0.9891539812088013, 'train/loss': 0.03674052655696869, 'train/mean_average_precision': 0.2390435111815593, 'validation/accuracy': 0.9858971834182739, 'validation/loss': 0.04799893498420715, 'validation/mean_average_precision': 0.1873103703045062, 'validation/num_examples': 43793, 'test/accuracy': 0.9850648045539856, 'test/loss': 0.05095173418521881, 'test/mean_average_precision': 0.19273518886644886, 'test/num_examples': 43793, 'score': 7936.069957971573, 'total_duration': 11563.233343362808, 'accumulated_submission_time': 7936.069957971573, 'accumulated_eval_time': 3625.518209695816, 'accumulated_logging_time': 0.9731786251068115, 'global_step': 24741, 'preemption_count': 0}), (25490, {'train/accuracy': 0.9891579151153564, 'train/loss': 0.03681539371609688, 'train/mean_average_precision': 0.2493061489164509, 'validation/accuracy': 0.9859349131584167, 'validation/loss': 0.04808002710342407, 'validation/mean_average_precision': 0.1947319230197419, 'validation/num_examples': 43793, 'test/accuracy': 0.9850631356239319, 'test/loss': 0.05122765153646469, 'test/mean_average_precision': 0.18459982964451715, 'test/num_examples': 43793, 'score': 8175.831615447998, 'total_duration': 11908.558824062347, 'accumulated_submission_time': 8175.831615447998, 'accumulated_eval_time': 3730.604122877121, 'accumulated_logging_time': 1.4308359622955322, 'global_step': 25490, 'preemption_count': 0}), (26244, {'train/accuracy': 0.9890082478523254, 'train/loss': 0.037123121321201324, 'train/mean_average_precision': 0.26194704652344725, 'validation/accuracy': 0.9857433438301086, 'validation/loss': 0.048029348254203796, 'validation/mean_average_precision': 0.19817879739783534, 'validation/num_examples': 43793, 'test/accuracy': 0.9848095774650574, 'test/loss': 0.051177769899368286, 'test/mean_average_precision': 0.1883528025284292, 'test/num_examples': 43793, 'score': 8415.865903615952, 'total_duration': 12255.497112989426, 'accumulated_submission_time': 8415.865903615952, 'accumulated_eval_time': 3837.456799507141, 'accumulated_logging_time': 1.4622154235839844, 'global_step': 26244, 'preemption_count': 0}), (27000, {'train/accuracy': 0.9892248511314392, 'train/loss': 0.036412276327610016, 'train/mean_average_precision': 0.24881678190256606, 'validation/accuracy': 0.9859369397163391, 'validation/loss': 0.047542754560709, 'validation/mean_average_precision': 0.18955104946096324, 'validation/num_examples': 43793, 'test/accuracy': 0.985078752040863, 'test/loss': 0.050518136471509933, 'test/mean_average_precision': 0.18626111746709237, 'test/num_examples': 43793, 'score': 8655.825356006622, 'total_duration': 12597.900453329086, 'accumulated_submission_time': 8655.825356006622, 'accumulated_eval_time': 3939.849988222122, 'accumulated_logging_time': 1.493131399154663, 'global_step': 27000, 'preemption_count': 0}), (27749, {'train/accuracy': 0.9891148209571838, 'train/loss': 0.036856137216091156, 'train/mean_average_precision': 0.24980815274093432, 'validation/accuracy': 0.9860550761222839, 'validation/loss': 0.04757341742515564, 'validation/mean_average_precision': 0.2024660222425728, 'validation/num_examples': 43793, 'test/accuracy': 0.985129714012146, 'test/loss': 0.05053034424781799, 'test/mean_average_precision': 0.1984891361260043, 'test/num_examples': 43793, 'score': 8895.789571285248, 'total_duration': 12941.680038928986, 'accumulated_submission_time': 8895.789571285248, 'accumulated_eval_time': 4043.614537715912, 'accumulated_logging_time': 1.5242531299591064, 'global_step': 27749, 'preemption_count': 0}), (28500, {'train/accuracy': 0.9891347885131836, 'train/loss': 0.03732499107718468, 'train/mean_average_precision': 0.22630107043646894, 'validation/accuracy': 0.9859467148780823, 'validation/loss': 0.04806974530220032, 'validation/mean_average_precision': 0.19213174303977623, 'validation/num_examples': 43793, 'test/accuracy': 0.9850189089775085, 'test/loss': 0.05103142559528351, 'test/mean_average_precision': 0.19004555406199725, 'test/num_examples': 43793, 'score': 9136.02029132843, 'total_duration': 13288.089123249054, 'accumulated_submission_time': 9136.02029132843, 'accumulated_eval_time': 4149.741677761078, 'accumulated_logging_time': 1.5556812286376953, 'global_step': 28500, 'preemption_count': 0}), (29250, {'train/accuracy': 0.9891011714935303, 'train/loss': 0.03728984668850899, 'train/mean_average_precision': 0.2516501756761238, 'validation/accuracy': 0.9858760833740234, 'validation/loss': 0.04750604182481766, 'validation/mean_average_precision': 0.19352562980530316, 'validation/num_examples': 43793, 'test/accuracy': 0.9850140810012817, 'test/loss': 0.05021721124649048, 'test/mean_average_precision': 0.18760906856632956, 'test/num_examples': 43793, 'score': 9376.250892162323, 'total_duration': 13634.671559333801, 'accumulated_submission_time': 9376.250892162323, 'accumulated_eval_time': 4256.041279792786, 'accumulated_logging_time': 1.5881264209747314, 'global_step': 29250, 'preemption_count': 0}), (30004, {'train/accuracy': 0.9890967011451721, 'train/loss': 0.036956485360860825, 'train/mean_average_precision': 0.23889701222262977, 'validation/accuracy': 0.9860494136810303, 'validation/loss': 0.04769286513328552, 'validation/mean_average_precision': 0.2032375966967203, 'validation/num_examples': 43793, 'test/accuracy': 0.9850319623947144, 'test/loss': 0.05084433779120445, 'test/mean_average_precision': 0.19017876820762847, 'test/num_examples': 43793, 'score': 9616.499011993408, 'total_duration': 13983.701684236526, 'accumulated_submission_time': 9616.499011993408, 'accumulated_eval_time': 4364.7704746723175, 'accumulated_logging_time': 1.6209070682525635, 'global_step': 30004, 'preemption_count': 0}), (30751, {'train/accuracy': 0.9891952872276306, 'train/loss': 0.03677273169159889, 'train/mean_average_precision': 0.24683130385525454, 'validation/accuracy': 0.9859515428543091, 'validation/loss': 0.04743628576397896, 'validation/mean_average_precision': 0.20348333767901491, 'validation/num_examples': 43793, 'test/accuracy': 0.9849410057067871, 'test/loss': 0.05041816085577011, 'test/mean_average_precision': 0.1896190516759259, 'test/num_examples': 43793, 'score': 9856.662395954132, 'total_duration': 14331.912987470627, 'accumulated_submission_time': 9856.662395954132, 'accumulated_eval_time': 4472.7656536102295, 'accumulated_logging_time': 1.652916431427002, 'global_step': 30751, 'preemption_count': 0}), (31490, {'train/accuracy': 0.9892831444740295, 'train/loss': 0.0365368127822876, 'train/mean_average_precision': 0.2541385304879885, 'validation/accuracy': 0.9860900044441223, 'validation/loss': 0.04710783809423447, 'validation/mean_average_precision': 0.20120274318131334, 'validation/num_examples': 43793, 'test/accuracy': 0.9851625561714172, 'test/loss': 0.050069410353899, 'test/mean_average_precision': 0.19461144235046873, 'test/num_examples': 43793, 'score': 10096.915367603302, 'total_duration': 14683.252610206604, 'accumulated_submission_time': 10096.915367603302, 'accumulated_eval_time': 4583.790787220001, 'accumulated_logging_time': 1.6917784214019775, 'global_step': 31490, 'preemption_count': 0}), (32239, {'train/accuracy': 0.9892269372940063, 'train/loss': 0.03683546185493469, 'train/mean_average_precision': 0.25522288133128135, 'validation/accuracy': 0.985957682132721, 'validation/loss': 0.04722733050584793, 'validation/mean_average_precision': 0.19437412916669136, 'validation/num_examples': 43793, 'test/accuracy': 0.9850172400474548, 'test/loss': 0.05006767064332962, 'test/mean_average_precision': 0.19555223572746927, 'test/num_examples': 43793, 'score': 10337.149015903473, 'total_duration': 15029.597774505615, 'accumulated_submission_time': 10337.149015903473, 'accumulated_eval_time': 4689.847971916199, 'accumulated_logging_time': 1.7260665893554688, 'global_step': 32239, 'preemption_count': 0}), (32988, {'train/accuracy': 0.9894253015518188, 'train/loss': 0.03580518439412117, 'train/mean_average_precision': 0.2698218583076517, 'validation/accuracy': 0.9860225915908813, 'validation/loss': 0.04728809744119644, 'validation/mean_average_precision': 0.20049733686495858, 'validation/num_examples': 43793, 'test/accuracy': 0.9851486682891846, 'test/loss': 0.05022474750876427, 'test/mean_average_precision': 0.19327633019568238, 'test/num_examples': 43793, 'score': 10577.234008789062, 'total_duration': 15372.825038433075, 'accumulated_submission_time': 10577.234008789062, 'accumulated_eval_time': 4792.9368715286255, 'accumulated_logging_time': 1.7592804431915283, 'global_step': 32988, 'preemption_count': 0}), (33734, {'train/accuracy': 0.9893887639045715, 'train/loss': 0.035785213112831116, 'train/mean_average_precision': 0.2618129740028855, 'validation/accuracy': 0.9860676527023315, 'validation/loss': 0.04695483669638634, 'validation/mean_average_precision': 0.19759185260774403, 'validation/num_examples': 43793, 'test/accuracy': 0.9850395321846008, 'test/loss': 0.04974985867738724, 'test/mean_average_precision': 0.1964414351587808, 'test/num_examples': 43793, 'score': 10817.288613796234, 'total_duration': 15715.528035879135, 'accumulated_submission_time': 10817.288613796234, 'accumulated_eval_time': 4895.532358884811, 'accumulated_logging_time': 1.7915773391723633, 'global_step': 33734, 'preemption_count': 0}), (34482, {'train/accuracy': 0.9893254041671753, 'train/loss': 0.03624964505434036, 'train/mean_average_precision': 0.2509931044289288, 'validation/accuracy': 0.9861057996749878, 'validation/loss': 0.04736443981528282, 'validation/mean_average_precision': 0.19361712981863322, 'validation/num_examples': 43793, 'test/accuracy': 0.985135555267334, 'test/loss': 0.0503055602312088, 'test/mean_average_precision': 0.18777562192605082, 'test/num_examples': 43793, 'score': 11057.456293582916, 'total_duration': 16061.060340881348, 'accumulated_submission_time': 11057.456293582916, 'accumulated_eval_time': 5000.842721700668, 'accumulated_logging_time': 1.8258166313171387, 'global_step': 34482, 'preemption_count': 0}), (35237, {'train/accuracy': 0.9891011714935303, 'train/loss': 0.036694981157779694, 'train/mean_average_precision': 0.25369619866113313, 'validation/accuracy': 0.9861447811126709, 'validation/loss': 0.047388914972543716, 'validation/mean_average_precision': 0.20233305382149266, 'validation/num_examples': 43793, 'test/accuracy': 0.9851351380348206, 'test/loss': 0.05062321946024895, 'test/mean_average_precision': 0.19302576481803932, 'test/num_examples': 43793, 'score': 11297.589622735977, 'total_duration': 16404.014665842056, 'accumulated_submission_time': 11297.589622735977, 'accumulated_eval_time': 5103.608122110367, 'accumulated_logging_time': 1.8610637187957764, 'global_step': 35237, 'preemption_count': 0}), (35987, {'train/accuracy': 0.9890627861022949, 'train/loss': 0.03704715147614479, 'train/mean_average_precision': 0.23974588567854288, 'validation/accuracy': 0.9860416650772095, 'validation/loss': 0.047454286366701126, 'validation/mean_average_precision': 0.19409472872402248, 'validation/num_examples': 43793, 'test/accuracy': 0.9851229190826416, 'test/loss': 0.05034815892577171, 'test/mean_average_precision': 0.19721399984521867, 'test/num_examples': 43793, 'score': 11537.726133108139, 'total_duration': 16746.245433568954, 'accumulated_submission_time': 11537.726133108139, 'accumulated_eval_time': 5205.648674488068, 'accumulated_logging_time': 1.8951332569122314, 'global_step': 35987, 'preemption_count': 0}), (36737, {'train/accuracy': 0.9894454479217529, 'train/loss': 0.0358574204146862, 'train/mean_average_precision': 0.26194003030307045, 'validation/accuracy': 0.9862446784973145, 'validation/loss': 0.04672913998365402, 'validation/mean_average_precision': 0.21278031221063418, 'validation/num_examples': 43793, 'test/accuracy': 0.9852808713912964, 'test/loss': 0.04963654279708862, 'test/mean_average_precision': 0.20278154393467035, 'test/num_examples': 43793, 'score': 11777.842349767685, 'total_duration': 17093.15770673752, 'accumulated_submission_time': 11777.842349767685, 'accumulated_eval_time': 5312.391384601593, 'accumulated_logging_time': 1.928436040878296, 'global_step': 36737, 'preemption_count': 0}), (37487, {'train/accuracy': 0.9894551038742065, 'train/loss': 0.03574817627668381, 'train/mean_average_precision': 0.25619943397522477, 'validation/accuracy': 0.9861322045326233, 'validation/loss': 0.0467582568526268, 'validation/mean_average_precision': 0.20661521290754128, 'validation/num_examples': 43793, 'test/accuracy': 0.9852720499038696, 'test/loss': 0.049628548324108124, 'test/mean_average_precision': 0.20916613384323424, 'test/num_examples': 43793, 'score': 12017.802143096924, 'total_duration': 17435.217926979065, 'accumulated_submission_time': 12017.802143096924, 'accumulated_eval_time': 5414.43673324585, 'accumulated_logging_time': 1.9633357524871826, 'global_step': 37487, 'preemption_count': 0}), (38244, {'train/accuracy': 0.9893867373466492, 'train/loss': 0.03627531975507736, 'train/mean_average_precision': 0.26018173176115245, 'validation/accuracy': 0.9859263896942139, 'validation/loss': 0.04731174185872078, 'validation/mean_average_precision': 0.20246927034052514, 'validation/num_examples': 43793, 'test/accuracy': 0.9849477410316467, 'test/loss': 0.050186317414045334, 'test/mean_average_precision': 0.1923162326230432, 'test/num_examples': 43793, 'score': 12257.834669589996, 'total_duration': 17782.260478019714, 'accumulated_submission_time': 12257.834669589996, 'accumulated_eval_time': 5521.393639087677, 'accumulated_logging_time': 1.9962775707244873, 'global_step': 38244, 'preemption_count': 0}), (39001, {'train/accuracy': 0.9894042611122131, 'train/loss': 0.036132972687482834, 'train/mean_average_precision': 0.2705461838739672, 'validation/accuracy': 0.9859158396720886, 'validation/loss': 0.047259051352739334, 'validation/mean_average_precision': 0.20614544073050622, 'validation/num_examples': 43793, 'test/accuracy': 0.9850168228149414, 'test/loss': 0.05015391483902931, 'test/mean_average_precision': 0.19393707504385768, 'test/num_examples': 43793, 'score': 12497.840120315552, 'total_duration': 18124.86086988449, 'accumulated_submission_time': 12497.840120315552, 'accumulated_eval_time': 5623.934894800186, 'accumulated_logging_time': 2.029609203338623, 'global_step': 39001, 'preemption_count': 0}), (39756, {'train/accuracy': 0.9893486499786377, 'train/loss': 0.03603357449173927, 'train/mean_average_precision': 0.25614875944770504, 'validation/accuracy': 0.9859779477119446, 'validation/loss': 0.0470002144575119, 'validation/mean_average_precision': 0.20517942394232186, 'validation/num_examples': 43793, 'test/accuracy': 0.9850248098373413, 'test/loss': 0.05004764720797539, 'test/mean_average_precision': 0.19505340855467057, 'test/num_examples': 43793, 'score': 12737.917171955109, 'total_duration': 18469.422612428665, 'accumulated_submission_time': 12737.917171955109, 'accumulated_eval_time': 5728.363832473755, 'accumulated_logging_time': 2.065474271774292, 'global_step': 39756, 'preemption_count': 0}), (40505, {'train/accuracy': 0.9895033836364746, 'train/loss': 0.03543873503804207, 'train/mean_average_precision': 0.2753709398089208, 'validation/accuracy': 0.9860765933990479, 'validation/loss': 0.04671790078282356, 'validation/mean_average_precision': 0.20224206700190955, 'validation/num_examples': 43793, 'test/accuracy': 0.9851389527320862, 'test/loss': 0.0496848039329052, 'test/mean_average_precision': 0.1942995179941897, 'test/num_examples': 43793, 'score': 12977.919306516647, 'total_duration': 18815.74730205536, 'accumulated_submission_time': 12977.919306516647, 'accumulated_eval_time': 5834.631159543991, 'accumulated_logging_time': 2.10075044631958, 'global_step': 40505, 'preemption_count': 0}), (41252, {'train/accuracy': 0.9897860288619995, 'train/loss': 0.034546561539173126, 'train/mean_average_precision': 0.28537601240427235, 'validation/accuracy': 0.9862000346183777, 'validation/loss': 0.046651337295770645, 'validation/mean_average_precision': 0.21006526472080778, 'validation/num_examples': 43793, 'test/accuracy': 0.9852118492126465, 'test/loss': 0.04969169571995735, 'test/mean_average_precision': 0.19840804436460177, 'test/num_examples': 43793, 'score': 13218.008937835693, 'total_duration': 19158.668475151062, 'accumulated_submission_time': 13218.008937835693, 'accumulated_eval_time': 5937.405626773834, 'accumulated_logging_time': 2.137742519378662, 'global_step': 41252, 'preemption_count': 0}), (42004, {'train/accuracy': 0.989495038986206, 'train/loss': 0.03562954440712929, 'train/mean_average_precision': 0.26692613512005703, 'validation/accuracy': 0.9862678050994873, 'validation/loss': 0.046522967517375946, 'validation/mean_average_precision': 0.21501858248592956, 'validation/num_examples': 43793, 'test/accuracy': 0.9853630065917969, 'test/loss': 0.049677565693855286, 'test/mean_average_precision': 0.19559475361363315, 'test/num_examples': 43793, 'score': 13458.234591960907, 'total_duration': 19505.247665166855, 'accumulated_submission_time': 13458.234591960907, 'accumulated_eval_time': 6043.70458650589, 'accumulated_logging_time': 2.17232346534729, 'global_step': 42004, 'preemption_count': 0}), (42737, {'train/accuracy': 0.9894113540649414, 'train/loss': 0.035757679492235184, 'train/mean_average_precision': 0.2688181003628976, 'validation/accuracy': 0.9861470460891724, 'validation/loss': 0.046720124781131744, 'validation/mean_average_precision': 0.21142738089231428, 'validation/num_examples': 43793, 'test/accuracy': 0.9852128624916077, 'test/loss': 0.049765896052122116, 'test/mean_average_precision': 0.20485735455653056, 'test/num_examples': 43793, 'score': 13698.329906463623, 'total_duration': 19849.74711751938, 'accumulated_submission_time': 13698.329906463623, 'accumulated_eval_time': 6148.046504974365, 'accumulated_logging_time': 2.210094928741455, 'global_step': 42737, 'preemption_count': 0}), (43494, {'train/accuracy': 0.989499032497406, 'train/loss': 0.03540022671222687, 'train/mean_average_precision': 0.26506057007857436, 'validation/accuracy': 0.9862678050994873, 'validation/loss': 0.04665068909525871, 'validation/mean_average_precision': 0.21142511276955886, 'validation/num_examples': 43793, 'test/accuracy': 0.9853647351264954, 'test/loss': 0.049580205231904984, 'test/mean_average_precision': 0.20401457788142363, 'test/num_examples': 43793, 'score': 13938.442008972168, 'total_duration': 20193.092981815338, 'accumulated_submission_time': 13938.442008972168, 'accumulated_eval_time': 6251.2253041267395, 'accumulated_logging_time': 2.2450668811798096, 'global_step': 43494, 'preemption_count': 0}), (44251, {'train/accuracy': 0.9895753860473633, 'train/loss': 0.03549180552363396, 'train/mean_average_precision': 0.27426659195040215, 'validation/accuracy': 0.9862418174743652, 'validation/loss': 0.0463673397898674, 'validation/mean_average_precision': 0.21045252702003892, 'validation/num_examples': 43793, 'test/accuracy': 0.9851962327957153, 'test/loss': 0.04952609911561012, 'test/mean_average_precision': 0.19897285175365154, 'test/num_examples': 43793, 'score': 14178.594394683838, 'total_duration': 20536.04729604721, 'accumulated_submission_time': 14178.594394683838, 'accumulated_eval_time': 6353.971425771713, 'accumulated_logging_time': 2.28100848197937, 'global_step': 44251, 'preemption_count': 0}), (45006, {'train/accuracy': 0.9895598888397217, 'train/loss': 0.035108692944049835, 'train/mean_average_precision': 0.27760304847181394, 'validation/accuracy': 0.9861411452293396, 'validation/loss': 0.046678148210048676, 'validation/mean_average_precision': 0.20588797651640117, 'validation/num_examples': 43793, 'test/accuracy': 0.9851511716842651, 'test/loss': 0.04976167157292366, 'test/mean_average_precision': 0.19534209004671338, 'test/num_examples': 43793, 'score': 14418.677802801132, 'total_duration': 20881.09065937996, 'accumulated_submission_time': 14418.677802801132, 'accumulated_eval_time': 6458.873847723007, 'accumulated_logging_time': 2.3189632892608643, 'global_step': 45006, 'preemption_count': 0}), (45765, {'train/accuracy': 0.9897001385688782, 'train/loss': 0.03460755571722984, 'train/mean_average_precision': 0.27682914659793945, 'validation/accuracy': 0.9862962365150452, 'validation/loss': 0.0462343730032444, 'validation/mean_average_precision': 0.21518716704108146, 'validation/num_examples': 43793, 'test/accuracy': 0.9853546023368835, 'test/loss': 0.049391888082027435, 'test/mean_average_precision': 0.20383672222142435, 'test/num_examples': 43793, 'score': 14658.762327671051, 'total_duration': 21223.11307501793, 'accumulated_submission_time': 14658.762327671051, 'accumulated_eval_time': 6560.754787683487, 'accumulated_logging_time': 2.355625629425049, 'global_step': 45765, 'preemption_count': 0}), (46515, {'train/accuracy': 0.9896975159645081, 'train/loss': 0.034646280109882355, 'train/mean_average_precision': 0.2963236467176596, 'validation/accuracy': 0.9863830804824829, 'validation/loss': 0.046199001371860504, 'validation/mean_average_precision': 0.22106585216394292, 'validation/num_examples': 43793, 'test/accuracy': 0.9853836894035339, 'test/loss': 0.04933399707078934, 'test/mean_average_precision': 0.207040467005581, 'test/num_examples': 43793, 'score': 14898.90408039093, 'total_duration': 21567.253492355347, 'accumulated_submission_time': 14898.90408039093, 'accumulated_eval_time': 6664.698795795441, 'accumulated_logging_time': 2.390665292739868, 'global_step': 46515, 'preemption_count': 0}), (47270, {'train/accuracy': 0.9897758364677429, 'train/loss': 0.03435588255524635, 'train/mean_average_precision': 0.3042403984564562, 'validation/accuracy': 0.9862929582595825, 'validation/loss': 0.04597706347703934, 'validation/mean_average_precision': 0.212829559312736, 'validation/num_examples': 43793, 'test/accuracy': 0.9853179454803467, 'test/loss': 0.04889655113220215, 'test/mean_average_precision': 0.20305467845193023, 'test/num_examples': 43793, 'score': 15138.879612207413, 'total_duration': 21912.986404657364, 'accumulated_submission_time': 15138.879612207413, 'accumulated_eval_time': 6770.398800849915, 'accumulated_logging_time': 2.428494691848755, 'global_step': 47270, 'preemption_count': 0}), (48027, {'train/accuracy': 0.9896266460418701, 'train/loss': 0.03433692827820778, 'train/mean_average_precision': 0.286865701081385, 'validation/accuracy': 0.9862750768661499, 'validation/loss': 0.04656687006354332, 'validation/mean_average_precision': 0.2209927814720388, 'validation/num_examples': 43793, 'test/accuracy': 0.9853202700614929, 'test/loss': 0.049766506999731064, 'test/mean_average_precision': 0.20462477029053414, 'test/num_examples': 43793, 'score': 15379.08549618721, 'total_duration': 22254.191664218903, 'accumulated_submission_time': 15379.08549618721, 'accumulated_eval_time': 6871.342316389084, 'accumulated_logging_time': 2.464463233947754, 'global_step': 48027, 'preemption_count': 0}), (48784, {'train/accuracy': 0.9900445342063904, 'train/loss': 0.03350134193897247, 'train/mean_average_precision': 0.3042157934923303, 'validation/accuracy': 0.9863396286964417, 'validation/loss': 0.046142980456352234, 'validation/mean_average_precision': 0.22079732841857616, 'validation/num_examples': 43793, 'test/accuracy': 0.9854194521903992, 'test/loss': 0.0493343286216259, 'test/mean_average_precision': 0.21046426470122526, 'test/num_examples': 43793, 'score': 15619.100360155106, 'total_duration': 22597.190956115723, 'accumulated_submission_time': 15619.100360155106, 'accumulated_eval_time': 6974.270753145218, 'accumulated_logging_time': 2.500919818878174, 'global_step': 48784, 'preemption_count': 0}), (49536, {'train/accuracy': 0.9898927807807922, 'train/loss': 0.03411620482802391, 'train/mean_average_precision': 0.30057846144930656, 'validation/accuracy': 0.986319363117218, 'validation/loss': 0.046313922852277756, 'validation/mean_average_precision': 0.2191926410291987, 'validation/num_examples': 43793, 'test/accuracy': 0.9853870272636414, 'test/loss': 0.049277063459157944, 'test/mean_average_precision': 0.21312256947102398, 'test/num_examples': 43793, 'score': 15859.136625528336, 'total_duration': 22937.37840652466, 'accumulated_submission_time': 15859.136625528336, 'accumulated_eval_time': 7074.364864110947, 'accumulated_logging_time': 2.5379621982574463, 'global_step': 49536, 'preemption_count': 0}), (50294, {'train/accuracy': 0.9897878170013428, 'train/loss': 0.03433746472001076, 'train/mean_average_precision': 0.28569289379560725, 'validation/accuracy': 0.9864285588264465, 'validation/loss': 0.04585791379213333, 'validation/mean_average_precision': 0.2164103157897776, 'validation/num_examples': 43793, 'test/accuracy': 0.9856048226356506, 'test/loss': 0.048900358378887177, 'test/mean_average_precision': 0.2149699454561261, 'test/num_examples': 43793, 'score': 16099.339428424835, 'total_duration': 23282.12690448761, 'accumulated_submission_time': 16099.339428424835, 'accumulated_eval_time': 7178.855273962021, 'accumulated_logging_time': 2.573608636856079, 'global_step': 50294, 'preemption_count': 0}), (51040, {'train/accuracy': 0.9899283647537231, 'train/loss': 0.033983852714300156, 'train/mean_average_precision': 0.30221693932369764, 'validation/accuracy': 0.9863327741622925, 'validation/loss': 0.045761264860630035, 'validation/mean_average_precision': 0.21567340506284718, 'validation/num_examples': 43793, 'test/accuracy': 0.9853739738464355, 'test/loss': 0.048657987266778946, 'test/mean_average_precision': 0.20812532694284952, 'test/num_examples': 43793, 'score': 16339.413675069809, 'total_duration': 23625.6093814373, 'accumulated_submission_time': 16339.413675069809, 'accumulated_eval_time': 7282.207628250122, 'accumulated_logging_time': 2.6092591285705566, 'global_step': 51040, 'preemption_count': 0}), (51787, {'train/accuracy': 0.9897343516349792, 'train/loss': 0.034360747784376144, 'train/mean_average_precision': 0.2915450888909189, 'validation/accuracy': 0.986326277256012, 'validation/loss': 0.045857734978199005, 'validation/mean_average_precision': 0.2213827129714896, 'validation/num_examples': 43793, 'test/accuracy': 0.9853882789611816, 'test/loss': 0.04878430813550949, 'test/mean_average_precision': 0.20694046165501986, 'test/num_examples': 43793, 'score': 16579.61815905571, 'total_duration': 23971.82929778099, 'accumulated_submission_time': 16579.61815905571, 'accumulated_eval_time': 7388.1657457351685, 'accumulated_logging_time': 2.646596670150757, 'global_step': 51787, 'preemption_count': 0}), (52526, {'train/accuracy': 0.9898606538772583, 'train/loss': 0.03372828662395477, 'train/mean_average_precision': 0.308960692285714, 'validation/accuracy': 0.9864127039909363, 'validation/loss': 0.04572121053934097, 'validation/mean_average_precision': 0.22213817610781364, 'validation/num_examples': 43793, 'test/accuracy': 0.9855268597602844, 'test/loss': 0.04860956221818924, 'test/mean_average_precision': 0.212095926354634, 'test/num_examples': 43793, 'score': 16819.64329266548, 'total_duration': 24314.65186572075, 'accumulated_submission_time': 16819.64329266548, 'accumulated_eval_time': 7490.901193618774, 'accumulated_logging_time': 2.6872761249542236, 'global_step': 52526, 'preemption_count': 0}), (53280, {'train/accuracy': 0.9900662302970886, 'train/loss': 0.03331868350505829, 'train/mean_average_precision': 0.30609795083218194, 'validation/accuracy': 0.9864590167999268, 'validation/loss': 0.04557448998093605, 'validation/mean_average_precision': 0.22433592060434396, 'validation/num_examples': 43793, 'test/accuracy': 0.9855159521102905, 'test/loss': 0.04845355451107025, 'test/mean_average_precision': 0.21344609651030216, 'test/num_examples': 43793, 'score': 17059.861709833145, 'total_duration': 24661.785895824432, 'accumulated_submission_time': 17059.861709833145, 'accumulated_eval_time': 7597.760349988937, 'accumulated_logging_time': 2.723623275756836, 'global_step': 53280, 'preemption_count': 0}), (54027, {'train/accuracy': 0.9900087714195251, 'train/loss': 0.033369824290275574, 'train/mean_average_precision': 0.30490107379197173, 'validation/accuracy': 0.9865007996559143, 'validation/loss': 0.045682504773139954, 'validation/mean_average_precision': 0.22642147932658865, 'validation/num_examples': 43793, 'test/accuracy': 0.985465407371521, 'test/loss': 0.04903227835893631, 'test/mean_average_precision': 0.20949288392663823, 'test/num_examples': 43793, 'score': 17299.852272987366, 'total_duration': 25005.707471370697, 'accumulated_submission_time': 17299.852272987366, 'accumulated_eval_time': 7701.634706735611, 'accumulated_logging_time': 2.7599589824676514, 'global_step': 54027, 'preemption_count': 0}), (54781, {'train/accuracy': 0.9901589155197144, 'train/loss': 0.03296329081058502, 'train/mean_average_precision': 0.3163235677717774, 'validation/accuracy': 0.9865801930427551, 'validation/loss': 0.045562393963336945, 'validation/mean_average_precision': 0.22805262338495752, 'validation/num_examples': 43793, 'test/accuracy': 0.9855460524559021, 'test/loss': 0.04861282929778099, 'test/mean_average_precision': 0.21454710113569686, 'test/num_examples': 43793, 'score': 17539.843241930008, 'total_duration': 25347.28339076042, 'accumulated_submission_time': 17539.843241930008, 'accumulated_eval_time': 7803.162973642349, 'accumulated_logging_time': 2.7964839935302734, 'global_step': 54781, 'preemption_count': 0}), (55533, {'train/accuracy': 0.990270733833313, 'train/loss': 0.03255586326122284, 'train/mean_average_precision': 0.32371953494994204, 'validation/accuracy': 0.9865257740020752, 'validation/loss': 0.04533666744828224, 'validation/mean_average_precision': 0.23214892783435964, 'validation/num_examples': 43793, 'test/accuracy': 0.9855285286903381, 'test/loss': 0.04859936237335205, 'test/mean_average_precision': 0.21753470205782885, 'test/num_examples': 43793, 'score': 17779.891257047653, 'total_duration': 25694.680911540985, 'accumulated_submission_time': 17779.891257047653, 'accumulated_eval_time': 7910.455152988434, 'accumulated_logging_time': 2.8337621688842773, 'global_step': 55533, 'preemption_count': 0}), (56285, {'train/accuracy': 0.9902195930480957, 'train/loss': 0.032643094658851624, 'train/mean_average_precision': 0.3289052356829938, 'validation/accuracy': 0.9864122867584229, 'validation/loss': 0.045647431164979935, 'validation/mean_average_precision': 0.23472729870815293, 'validation/num_examples': 43793, 'test/accuracy': 0.9855323433876038, 'test/loss': 0.04870383068919182, 'test/mean_average_precision': 0.2174664089875856, 'test/num_examples': 43793, 'score': 18020.05457186699, 'total_duration': 26036.965087890625, 'accumulated_submission_time': 18020.05457186699, 'accumulated_eval_time': 8012.519022226334, 'accumulated_logging_time': 2.870720386505127, 'global_step': 56285, 'preemption_count': 0}), (57039, {'train/accuracy': 0.9902625679969788, 'train/loss': 0.032707177102565765, 'train/mean_average_precision': 0.3176695183003912, 'validation/accuracy': 0.9864926934242249, 'validation/loss': 0.04545671492815018, 'validation/mean_average_precision': 0.2280582942071802, 'validation/num_examples': 43793, 'test/accuracy': 0.9855504631996155, 'test/loss': 0.04861481487751007, 'test/mean_average_precision': 0.2146260371624704, 'test/num_examples': 43793, 'score': 18260.280821561813, 'total_duration': 26382.207079172134, 'accumulated_submission_time': 18260.280821561813, 'accumulated_eval_time': 8117.47713804245, 'accumulated_logging_time': 2.908522367477417, 'global_step': 57039, 'preemption_count': 0})], 'global_step': 57715}
I0206 03:33:51.698511 140205209478976 submission_runner.py:586] Timing: 18477.03369784355
I0206 03:33:51.698576 140205209478976 submission_runner.py:588] Total number of evals: 77
I0206 03:33:51.698628 140205209478976 submission_runner.py:589] ====================
I0206 03:33:51.698677 140205209478976 submission_runner.py:542] Using RNG seed 1480595982
I0206 03:33:51.767812 140205209478976 submission_runner.py:551] --- Tuning run 5/5 ---
I0206 03:33:51.768005 140205209478976 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_5.
I0206 03:33:51.768549 140205209478976 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_5/hparams.json.
I0206 03:33:51.909790 140205209478976 submission_runner.py:206] Initializing dataset.
I0206 03:33:52.002965 140205209478976 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0206 03:33:52.007604 140205209478976 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0206 03:33:52.144768 140205209478976 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0206 03:33:52.186713 140205209478976 submission_runner.py:213] Initializing model.
I0206 03:33:54.731115 140205209478976 submission_runner.py:255] Initializing optimizer.
I0206 03:33:55.359206 140205209478976 submission_runner.py:262] Initializing metrics bundle.
I0206 03:33:55.359476 140205209478976 submission_runner.py:280] Initializing checkpoint and logger.
I0206 03:33:55.360298 140205209478976 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_5 with prefix checkpoint_
I0206 03:33:55.360446 140205209478976 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_5/meta_data_0.json.
I0206 03:33:55.360689 140205209478976 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0206 03:33:55.360752 140205209478976 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 03:33:57.449990 140205209478976 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 03:33:59.464093 140205209478976 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_5/flags_0.json.
I0206 03:33:59.468986 140205209478976 submission_runner.py:314] Starting training loop.
I0206 03:34:13.037248 139965399324416 logging_writer.py:48] [0] global_step=0, grad_norm=1.6822483539581299, loss=0.7399396300315857
I0206 03:34:13.049617 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:35:50.707810 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:35:53.770874 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:35:56.770403 140205209478976 submission_runner.py:408] Time since start: 117.30s, 	Step: 1, 	{'train/accuracy': 0.5093092918395996, 'train/loss': 0.7391443848609924, 'train/mean_average_precision': 0.023277378646798894, 'validation/accuracy': 0.5064646005630493, 'validation/loss': 0.7422075867652893, 'validation/mean_average_precision': 0.025293153641848993, 'validation/num_examples': 43793, 'test/accuracy': 0.5047284960746765, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.026926999206144768, 'test/num_examples': 43793, 'score': 13.580633878707886, 'total_duration': 117.30141496658325, 'accumulated_submission_time': 13.580633878707886, 'accumulated_eval_time': 103.72073292732239, 'accumulated_logging_time': 0}
I0206 03:35:56.780119 139965416662784 logging_writer.py:48] [1] accumulated_eval_time=103.720733, accumulated_logging_time=0, accumulated_submission_time=13.580634, global_step=1, preemption_count=0, score=13.580634, test/accuracy=0.504728, test/loss=0.743874, test/mean_average_precision=0.026927, test/num_examples=43793, total_duration=117.301415, train/accuracy=0.509309, train/loss=0.739144, train/mean_average_precision=0.023277, validation/accuracy=0.506465, validation/loss=0.742208, validation/mean_average_precision=0.025293, validation/num_examples=43793
I0206 03:36:28.939266 140002205157120 logging_writer.py:48] [100] global_step=100, grad_norm=0.30844277143478394, loss=0.2863074839115143
I0206 03:37:00.742754 139965416662784 logging_writer.py:48] [200] global_step=200, grad_norm=0.09958939254283905, loss=0.10910574346780777
I0206 03:37:33.060539 140002205157120 logging_writer.py:48] [300] global_step=300, grad_norm=0.03186114877462387, loss=0.07073575258255005
I0206 03:38:05.286915 139965416662784 logging_writer.py:48] [400] global_step=400, grad_norm=0.03581967204809189, loss=0.059030041098594666
I0206 03:38:37.212120 140002205157120 logging_writer.py:48] [500] global_step=500, grad_norm=0.025683043524622917, loss=0.05906122922897339
I0206 03:39:09.206618 139965416662784 logging_writer.py:48] [600] global_step=600, grad_norm=0.017127005383372307, loss=0.05228336900472641
I0206 03:39:41.132441 140002205157120 logging_writer.py:48] [700] global_step=700, grad_norm=0.026687348261475563, loss=0.05071631819009781
I0206 03:39:57.034555 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:41:34.419025 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:41:37.547166 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:41:40.565886 140205209478976 submission_runner.py:408] Time since start: 461.10s, 	Step: 750, 	{'train/accuracy': 0.9868060946464539, 'train/loss': 0.05153829976916313, 'train/mean_average_precision': 0.05423567026451998, 'validation/accuracy': 0.9842458367347717, 'validation/loss': 0.06153523549437523, 'validation/mean_average_precision': 0.05384023588038083, 'validation/num_examples': 43793, 'test/accuracy': 0.9832671880722046, 'test/loss': 0.06482018530368805, 'test/mean_average_precision': 0.054599508015334225, 'test/num_examples': 43793, 'score': 253.80401229858398, 'total_duration': 461.0968973636627, 'accumulated_submission_time': 253.80401229858398, 'accumulated_eval_time': 207.2520191669464, 'accumulated_logging_time': 0.020728349685668945}
I0206 03:41:40.581046 139984037992192 logging_writer.py:48] [750] accumulated_eval_time=207.252019, accumulated_logging_time=0.020728, accumulated_submission_time=253.804012, global_step=750, preemption_count=0, score=253.804012, test/accuracy=0.983267, test/loss=0.064820, test/mean_average_precision=0.054600, test/num_examples=43793, total_duration=461.096897, train/accuracy=0.986806, train/loss=0.051538, train/mean_average_precision=0.054236, validation/accuracy=0.984246, validation/loss=0.061535, validation/mean_average_precision=0.053840, validation/num_examples=43793
I0206 03:41:56.975106 139984133072640 logging_writer.py:48] [800] global_step=800, grad_norm=0.014552038162946701, loss=0.04925369471311569
I0206 03:42:29.335838 139984037992192 logging_writer.py:48] [900] global_step=900, grad_norm=0.01639808528125286, loss=0.04921813681721687
I0206 03:43:01.103751 139984133072640 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.017486507073044777, loss=0.05031803995370865
I0206 03:43:33.215241 139984037992192 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.01739453710615635, loss=0.04940153658390045
I0206 03:44:05.324565 139984133072640 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.015124976634979248, loss=0.05005228891968727
I0206 03:44:37.559013 139984037992192 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.012625506147742271, loss=0.04688171669840813
I0206 03:45:10.483242 139984133072640 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.014980020932853222, loss=0.04948566481471062
I0206 03:45:40.850825 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:47:18.347341 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:47:21.542673 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:47:24.695487 140205209478976 submission_runner.py:408] Time since start: 805.23s, 	Step: 1495, 	{'train/accuracy': 0.9871118068695068, 'train/loss': 0.048001017421483994, 'train/mean_average_precision': 0.083969882400915, 'validation/accuracy': 0.9844828844070435, 'validation/loss': 0.057401418685913086, 'validation/mean_average_precision': 0.08528654023163375, 'validation/num_examples': 43793, 'test/accuracy': 0.9834954738616943, 'test/loss': 0.06077468395233154, 'test/mean_average_precision': 0.08407670770359991, 'test/num_examples': 43793, 'score': 494.04190468788147, 'total_duration': 805.2264924049377, 'accumulated_submission_time': 494.04190468788147, 'accumulated_eval_time': 311.0966327190399, 'accumulated_logging_time': 0.046759843826293945}
I0206 03:47:24.711474 139965425055488 logging_writer.py:48] [1495] accumulated_eval_time=311.096633, accumulated_logging_time=0.046760, accumulated_submission_time=494.041905, global_step=1495, preemption_count=0, score=494.041905, test/accuracy=0.983495, test/loss=0.060775, test/mean_average_precision=0.084077, test/num_examples=43793, total_duration=805.226492, train/accuracy=0.987112, train/loss=0.048001, train/mean_average_precision=0.083970, validation/accuracy=0.984483, validation/loss=0.057401, validation/mean_average_precision=0.085287, validation/num_examples=43793
I0206 03:47:26.645371 140002205157120 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.045317742973566055, loss=0.05273184925317764
I0206 03:47:58.978071 139965425055488 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.021235482767224312, loss=0.043040160089731216
I0206 03:48:31.046030 140002205157120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.01931009069085121, loss=0.051784589886665344
I0206 03:49:03.299953 139965425055488 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.01870260201394558, loss=0.054064348340034485
I0206 03:49:35.375965 140002205157120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.015295309945940971, loss=0.0496554970741272
I0206 03:50:07.484860 139965425055488 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.021184761077165604, loss=0.054395437240600586
I0206 03:50:39.292953 140002205157120 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.012230952270328999, loss=0.051681604236364365
I0206 03:51:11.450185 139965425055488 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.024575382471084595, loss=0.04076796770095825
I0206 03:51:24.868699 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:53:01.298503 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:53:04.499564 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:53:09.903881 140205209478976 submission_runner.py:408] Time since start: 1150.43s, 	Step: 2243, 	{'train/accuracy': 0.987299919128418, 'train/loss': 0.045526277273893356, 'train/mean_average_precision': 0.1279417429584762, 'validation/accuracy': 0.9847471714019775, 'validation/loss': 0.054682765156030655, 'validation/mean_average_precision': 0.12864685514709234, 'validation/num_examples': 43793, 'test/accuracy': 0.9837182760238647, 'test/loss': 0.05783096328377724, 'test/mean_average_precision': 0.12355053623446106, 'test/num_examples': 43793, 'score': 734.1674497127533, 'total_duration': 1150.4348917007446, 'accumulated_submission_time': 734.1674497127533, 'accumulated_eval_time': 416.1317677497864, 'accumulated_logging_time': 0.07377338409423828}
I0206 03:53:09.919157 139965416662784 logging_writer.py:48] [2243] accumulated_eval_time=416.131768, accumulated_logging_time=0.073773, accumulated_submission_time=734.167450, global_step=2243, preemption_count=0, score=734.167450, test/accuracy=0.983718, test/loss=0.057831, test/mean_average_precision=0.123551, test/num_examples=43793, total_duration=1150.434892, train/accuracy=0.987300, train/loss=0.045526, train/mean_average_precision=0.127942, validation/accuracy=0.984747, validation/loss=0.054683, validation/mean_average_precision=0.128647, validation/num_examples=43793
I0206 03:53:28.687373 139984133072640 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.01270817406475544, loss=0.04002317413687706
I0206 03:54:00.690408 139965416662784 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.019025009125471115, loss=0.04492993652820587
I0206 03:54:33.112511 139984133072640 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.030337419360876083, loss=0.048666853457689285
I0206 03:55:05.123287 139965416662784 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.018537696450948715, loss=0.044210948050022125
I0206 03:55:36.985768 139984133072640 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.009316569194197655, loss=0.038751546293497086
I0206 03:56:09.042506 139965416662784 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.022883793339133263, loss=0.04277721047401428
I0206 03:56:40.994501 139984133072640 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.011581413447856903, loss=0.04259629547595978
I0206 03:57:09.962873 140205209478976 spec.py:321] Evaluating on the training split.
I0206 03:58:45.552599 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 03:58:48.513195 140205209478976 spec.py:349] Evaluating on the test split.
I0206 03:58:51.511889 140205209478976 submission_runner.py:408] Time since start: 1492.04s, 	Step: 2991, 	{'train/accuracy': 0.9879700541496277, 'train/loss': 0.04237353801727295, 'train/mean_average_precision': 0.1602362821606831, 'validation/accuracy': 0.9851559400558472, 'validation/loss': 0.05190819129347801, 'validation/mean_average_precision': 0.1475911315728504, 'validation/num_examples': 43793, 'test/accuracy': 0.9842160940170288, 'test/loss': 0.05484679713845253, 'test/mean_average_precision': 0.14847681753521164, 'test/num_examples': 43793, 'score': 974.1777114868164, 'total_duration': 1492.042894601822, 'accumulated_submission_time': 974.1777114868164, 'accumulated_eval_time': 517.6807377338409, 'accumulated_logging_time': 0.10147762298583984}
I0206 03:58:51.528429 139965425055488 logging_writer.py:48] [2991] accumulated_eval_time=517.680738, accumulated_logging_time=0.101478, accumulated_submission_time=974.177711, global_step=2991, preemption_count=0, score=974.177711, test/accuracy=0.984216, test/loss=0.054847, test/mean_average_precision=0.148477, test/num_examples=43793, total_duration=1492.042895, train/accuracy=0.987970, train/loss=0.042374, train/mean_average_precision=0.160236, validation/accuracy=0.985156, validation/loss=0.051908, validation/mean_average_precision=0.147591, validation/num_examples=43793
I0206 03:58:54.830175 139984037992192 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.015730416402220726, loss=0.039539385586977005
I0206 03:59:27.430155 139965425055488 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.01508354116231203, loss=0.04284712299704552
I0206 03:59:59.718647 139984037992192 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.014150586910545826, loss=0.045711953192949295
I0206 04:00:31.958237 139965425055488 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.012131286785006523, loss=0.03909363970160484
I0206 04:01:04.182576 139984037992192 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.013585965149104595, loss=0.04489985853433609
I0206 04:01:36.258473 139965425055488 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.013884426094591618, loss=0.04683789238333702
I0206 04:02:08.424812 139984037992192 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.013825706206262112, loss=0.04277332127094269
I0206 04:02:40.629871 139965425055488 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.017524033784866333, loss=0.0404619537293911
I0206 04:02:51.796936 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:04:26.064899 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:04:29.165920 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:04:32.133486 140205209478976 submission_runner.py:408] Time since start: 1832.66s, 	Step: 3736, 	{'train/accuracy': 0.9882538318634033, 'train/loss': 0.04129573702812195, 'train/mean_average_precision': 0.1869888306182944, 'validation/accuracy': 0.9851595759391785, 'validation/loss': 0.050465673208236694, 'validation/mean_average_precision': 0.1660371829803102, 'validation/num_examples': 43793, 'test/accuracy': 0.984274685382843, 'test/loss': 0.053023021668195724, 'test/mean_average_precision': 0.16593629182887754, 'test/num_examples': 43793, 'score': 1214.414056301117, 'total_duration': 1832.6644973754883, 'accumulated_submission_time': 1214.414056301117, 'accumulated_eval_time': 618.0172493457794, 'accumulated_logging_time': 0.12940216064453125}
I0206 04:04:32.149093 139965416662784 logging_writer.py:48] [3736] accumulated_eval_time=618.017249, accumulated_logging_time=0.129402, accumulated_submission_time=1214.414056, global_step=3736, preemption_count=0, score=1214.414056, test/accuracy=0.984275, test/loss=0.053023, test/mean_average_precision=0.165936, test/num_examples=43793, total_duration=1832.664497, train/accuracy=0.988254, train/loss=0.041296, train/mean_average_precision=0.186989, validation/accuracy=0.985160, validation/loss=0.050466, validation/mean_average_precision=0.166037, validation/num_examples=43793
I0206 04:04:52.944876 140002205157120 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.014297964051365852, loss=0.04280724748969078
I0206 04:05:24.662425 139965416662784 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.011802253313362598, loss=0.041372548788785934
I0206 04:05:56.667247 140002205157120 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.015719037503004074, loss=0.03470147028565407
I0206 04:06:28.519987 139965416662784 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.011301489546895027, loss=0.03826403245329857
I0206 04:07:00.151614 140002205157120 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.012062432244420052, loss=0.03891856595873833
I0206 04:07:32.568535 139965416662784 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.012746201828122139, loss=0.038784462958574295
I0206 04:08:04.583200 140002205157120 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.023885568603873253, loss=0.042032863944768906
I0206 04:08:32.271031 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:10:14.834858 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:10:17.911612 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:10:20.933447 140205209478976 submission_runner.py:408] Time since start: 2181.46s, 	Step: 4488, 	{'train/accuracy': 0.9885920882225037, 'train/loss': 0.039246659725904465, 'train/mean_average_precision': 0.2131411379152453, 'validation/accuracy': 0.9855939149856567, 'validation/loss': 0.048895981162786484, 'validation/mean_average_precision': 0.18449098765003324, 'validation/num_examples': 43793, 'test/accuracy': 0.9847304224967957, 'test/loss': 0.05166628584265709, 'test/mean_average_precision': 0.18867037077657536, 'test/num_examples': 43793, 'score': 1454.5042502880096, 'total_duration': 2181.4644510746, 'accumulated_submission_time': 1454.5042502880096, 'accumulated_eval_time': 726.6796119213104, 'accumulated_logging_time': 0.15626215934753418}
I0206 04:10:20.949266 139984037992192 logging_writer.py:48] [4488] accumulated_eval_time=726.679612, accumulated_logging_time=0.156262, accumulated_submission_time=1454.504250, global_step=4488, preemption_count=0, score=1454.504250, test/accuracy=0.984730, test/loss=0.051666, test/mean_average_precision=0.188670, test/num_examples=43793, total_duration=2181.464451, train/accuracy=0.988592, train/loss=0.039247, train/mean_average_precision=0.213141, validation/accuracy=0.985594, validation/loss=0.048896, validation/mean_average_precision=0.184491, validation/num_examples=43793
I0206 04:10:25.188879 139984133072640 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.010781998746097088, loss=0.03784788399934769
I0206 04:10:57.295242 139984037992192 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.011751143261790276, loss=0.04184360057115555
I0206 04:11:28.969533 139984133072640 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.012472519651055336, loss=0.03713010624051094
I0206 04:12:01.151067 139984037992192 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.01695234701037407, loss=0.0414973683655262
I0206 04:12:32.999789 139984133072640 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.013207966461777687, loss=0.04404164478182793
I0206 04:13:04.760083 139984037992192 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.027507444843649864, loss=0.04008731618523598
I0206 04:13:36.536854 139984133072640 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.009887482970952988, loss=0.03719158098101616
I0206 04:14:08.411817 139984037992192 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.014089693315327168, loss=0.03477419167757034
I0206 04:14:21.089490 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:16:03.797864 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:16:06.826129 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:16:09.750977 140205209478976 submission_runner.py:408] Time since start: 2530.28s, 	Step: 5241, 	{'train/accuracy': 0.9889898300170898, 'train/loss': 0.03764519840478897, 'train/mean_average_precision': 0.23802269979823015, 'validation/accuracy': 0.9859138131141663, 'validation/loss': 0.04751823469996452, 'validation/mean_average_precision': 0.20319129931163368, 'validation/num_examples': 43793, 'test/accuracy': 0.9850661158561707, 'test/loss': 0.050238415598869324, 'test/mean_average_precision': 0.21379311876591287, 'test/num_examples': 43793, 'score': 1694.6122086048126, 'total_duration': 2530.281988620758, 'accumulated_submission_time': 1694.6122086048126, 'accumulated_eval_time': 835.3410527706146, 'accumulated_logging_time': 0.18371868133544922}
I0206 04:16:09.767073 139965425055488 logging_writer.py:48] [5241] accumulated_eval_time=835.341053, accumulated_logging_time=0.183719, accumulated_submission_time=1694.612209, global_step=5241, preemption_count=0, score=1694.612209, test/accuracy=0.985066, test/loss=0.050238, test/mean_average_precision=0.213793, test/num_examples=43793, total_duration=2530.281989, train/accuracy=0.988990, train/loss=0.037645, train/mean_average_precision=0.238023, validation/accuracy=0.985914, validation/loss=0.047518, validation/mean_average_precision=0.203191, validation/num_examples=43793
I0206 04:16:28.893030 140002205157120 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.018807757645845413, loss=0.03633781895041466
I0206 04:17:00.699589 139965425055488 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.014906426891684532, loss=0.0373707078397274
I0206 04:17:32.697659 140002205157120 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.012689858675003052, loss=0.04201529920101166
I0206 04:18:04.452207 139965425055488 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.013369196094572544, loss=0.03608693554997444
I0206 04:18:36.532266 140002205157120 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.01133925560861826, loss=0.03590358421206474
I0206 04:19:08.186599 139965425055488 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.027739020064473152, loss=0.04072580859065056
I0206 04:19:40.391341 140002205157120 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.016711238771677017, loss=0.040635935962200165
I0206 04:20:09.954267 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:21:51.908019 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:21:54.904101 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:21:57.913184 140205209478976 submission_runner.py:408] Time since start: 2878.44s, 	Step: 5993, 	{'train/accuracy': 0.9889129996299744, 'train/loss': 0.03743157163262367, 'train/mean_average_precision': 0.25126464969029233, 'validation/accuracy': 0.9860774278640747, 'validation/loss': 0.04659366235136986, 'validation/mean_average_precision': 0.21315192018549464, 'validation/num_examples': 43793, 'test/accuracy': 0.9853339791297913, 'test/loss': 0.049202386289834976, 'test/mean_average_precision': 0.2152423660366171, 'test/num_examples': 43793, 'score': 1934.768192768097, 'total_duration': 2878.444193124771, 'accumulated_submission_time': 1934.768192768097, 'accumulated_eval_time': 943.2999262809753, 'accumulated_logging_time': 0.2107102870941162}
I0206 04:21:57.929434 139984133072640 logging_writer.py:48] [5993] accumulated_eval_time=943.299926, accumulated_logging_time=0.210710, accumulated_submission_time=1934.768193, global_step=5993, preemption_count=0, score=1934.768193, test/accuracy=0.985334, test/loss=0.049202, test/mean_average_precision=0.215242, test/num_examples=43793, total_duration=2878.444193, train/accuracy=0.988913, train/loss=0.037432, train/mean_average_precision=0.251265, validation/accuracy=0.986077, validation/loss=0.046594, validation/mean_average_precision=0.213152, validation/num_examples=43793
I0206 04:22:00.473992 140044326475520 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.014020401984453201, loss=0.03387966752052307
I0206 04:22:32.459157 139984133072640 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.019684066995978355, loss=0.041867323219776154
I0206 04:23:04.124998 140044326475520 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.017333246767520905, loss=0.034078724682331085
I0206 04:23:36.157710 139984133072640 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.011996080167591572, loss=0.038318932056427
I0206 04:24:07.847677 140044326475520 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.017048221081495285, loss=0.03590839356184006
I0206 04:24:39.381274 139984133072640 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.012007098644971848, loss=0.03497575223445892
I0206 04:25:11.316107 140044326475520 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.01546530332416296, loss=0.038455016911029816
I0206 04:25:43.102888 139984133072640 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.01072589959949255, loss=0.03948044776916504
I0206 04:25:58.067359 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:27:31.965899 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:27:34.988269 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:27:38.010997 140205209478976 submission_runner.py:408] Time since start: 3218.54s, 	Step: 6748, 	{'train/accuracy': 0.989357590675354, 'train/loss': 0.035928696393966675, 'train/mean_average_precision': 0.27196293000153315, 'validation/accuracy': 0.9862214922904968, 'validation/loss': 0.04625379666686058, 'validation/mean_average_precision': 0.22629593197467981, 'validation/num_examples': 43793, 'test/accuracy': 0.9853845238685608, 'test/loss': 0.04884004965424538, 'test/mean_average_precision': 0.2211056133914604, 'test/num_examples': 43793, 'score': 2174.874867916107, 'total_duration': 3218.5420083999634, 'accumulated_submission_time': 2174.874867916107, 'accumulated_eval_time': 1043.2435188293457, 'accumulated_logging_time': 0.23788046836853027}
I0206 04:27:38.027296 139984037992192 logging_writer.py:48] [6748] accumulated_eval_time=1043.243519, accumulated_logging_time=0.237880, accumulated_submission_time=2174.874868, global_step=6748, preemption_count=0, score=2174.874868, test/accuracy=0.985385, test/loss=0.048840, test/mean_average_precision=0.221106, test/num_examples=43793, total_duration=3218.542008, train/accuracy=0.989358, train/loss=0.035929, train/mean_average_precision=0.271963, validation/accuracy=0.986221, validation/loss=0.046254, validation/mean_average_precision=0.226296, validation/num_examples=43793
I0206 04:27:55.023862 140142484387584 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.012660386972129345, loss=0.03387806564569473
I0206 04:28:27.389122 139984037992192 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.015360426157712936, loss=0.034699711948633194
I0206 04:29:00.005466 140142484387584 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.01995697245001793, loss=0.03589119389653206
I0206 04:29:32.110695 139984037992192 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.00973321683704853, loss=0.03148346766829491
I0206 04:30:04.473500 140142484387584 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.017324361950159073, loss=0.03589118644595146
I0206 04:30:36.939652 139984037992192 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.013824456371366978, loss=0.0340743325650692
I0206 04:31:08.880018 140142484387584 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.016984496265649796, loss=0.037222374230623245
I0206 04:31:38.327803 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:33:18.704138 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:33:21.732698 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:33:24.703795 140205209478976 submission_runner.py:408] Time since start: 3565.23s, 	Step: 7493, 	{'train/accuracy': 0.9894558787345886, 'train/loss': 0.03535552695393562, 'train/mean_average_precision': 0.3062496271384285, 'validation/accuracy': 0.9862868785858154, 'validation/loss': 0.04599423334002495, 'validation/mean_average_precision': 0.22793275281089398, 'validation/num_examples': 43793, 'test/accuracy': 0.9854055643081665, 'test/loss': 0.04870980978012085, 'test/mean_average_precision': 0.23485982806646893, 'test/num_examples': 43793, 'score': 2415.1428520679474, 'total_duration': 3565.2347972393036, 'accumulated_submission_time': 2415.1428520679474, 'accumulated_eval_time': 1149.6194591522217, 'accumulated_logging_time': 0.26660776138305664}
I0206 04:33:24.728537 139984133072640 logging_writer.py:48] [7493] accumulated_eval_time=1149.619459, accumulated_logging_time=0.266608, accumulated_submission_time=2415.142852, global_step=7493, preemption_count=0, score=2415.142852, test/accuracy=0.985406, test/loss=0.048710, test/mean_average_precision=0.234860, test/num_examples=43793, total_duration=3565.234797, train/accuracy=0.989456, train/loss=0.035356, train/mean_average_precision=0.306250, validation/accuracy=0.986287, validation/loss=0.045994, validation/mean_average_precision=0.227933, validation/num_examples=43793
I0206 04:33:27.327788 140044326475520 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.015678102150559425, loss=0.0383145771920681
I0206 04:33:59.553433 139984133072640 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01442435011267662, loss=0.038405414670705795
I0206 04:34:31.333617 140044326475520 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.020938191562891006, loss=0.041911233216524124
I0206 04:35:03.545028 139984133072640 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.020674584433436394, loss=0.034462809562683105
I0206 04:35:35.830932 140044326475520 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.02233206108212471, loss=0.03568100556731224
I0206 04:36:07.958102 139984133072640 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.01323289517313242, loss=0.03578264266252518
I0206 04:36:39.399230 140044326475520 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.015640901401638985, loss=0.038236431777477264
I0206 04:37:11.629955 139984133072640 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.03296368941664696, loss=0.03506886959075928
I0206 04:37:24.704723 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:39:04.041968 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:39:07.053075 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:39:09.995565 140205209478976 submission_runner.py:408] Time since start: 3910.53s, 	Step: 8242, 	{'train/accuracy': 0.989901602268219, 'train/loss': 0.033745985478162766, 'train/mean_average_precision': 0.33079067936035234, 'validation/accuracy': 0.9863956570625305, 'validation/loss': 0.045376237481832504, 'validation/mean_average_precision': 0.23738480475175228, 'validation/num_examples': 43793, 'test/accuracy': 0.9856169819831848, 'test/loss': 0.047817718237638474, 'test/mean_average_precision': 0.24193834660193864, 'test/num_examples': 43793, 'score': 2655.087381839752, 'total_duration': 3910.5265514850616, 'accumulated_submission_time': 2655.087381839752, 'accumulated_eval_time': 1254.9102308750153, 'accumulated_logging_time': 0.3024904727935791}
I0206 04:39:10.013140 139984037992192 logging_writer.py:48] [8242] accumulated_eval_time=1254.910231, accumulated_logging_time=0.302490, accumulated_submission_time=2655.087382, global_step=8242, preemption_count=0, score=2655.087382, test/accuracy=0.985617, test/loss=0.047818, test/mean_average_precision=0.241938, test/num_examples=43793, total_duration=3910.526551, train/accuracy=0.989902, train/loss=0.033746, train/mean_average_precision=0.330791, validation/accuracy=0.986396, validation/loss=0.045376, validation/mean_average_precision=0.237385, validation/num_examples=43793
I0206 04:39:28.688168 140002205157120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.02345743589103222, loss=0.04154709354043007
I0206 04:40:00.662629 139984037992192 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.015038191340863705, loss=0.03229851648211479
I0206 04:40:32.494660 140002205157120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.022489497438073158, loss=0.03974185883998871
I0206 04:41:04.366479 139984037992192 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.01836666837334633, loss=0.03562306612730026
I0206 04:41:36.087762 140002205157120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.017948441207408905, loss=0.03609679639339447
I0206 04:42:07.884233 139984037992192 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.017665071412920952, loss=0.03515436872839928
I0206 04:42:40.007975 140002205157120 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.02836192399263382, loss=0.036258041858673096
I0206 04:43:10.328365 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:44:48.860813 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:44:51.902368 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:44:54.857588 140205209478976 submission_runner.py:408] Time since start: 4255.39s, 	Step: 8996, 	{'train/accuracy': 0.99005526304245, 'train/loss': 0.03328986465930939, 'train/mean_average_precision': 0.3302585783276252, 'validation/accuracy': 0.9864711761474609, 'validation/loss': 0.04519089683890343, 'validation/mean_average_precision': 0.24074316745035115, 'validation/num_examples': 43793, 'test/accuracy': 0.9856843948364258, 'test/loss': 0.04783324897289276, 'test/mean_average_precision': 0.2460210322111171, 'test/num_examples': 43793, 'score': 2895.369548559189, 'total_duration': 4255.388598442078, 'accumulated_submission_time': 2895.369548559189, 'accumulated_eval_time': 1359.4394128322601, 'accumulated_logging_time': 0.33234596252441406}
I0206 04:44:54.874200 139984133072640 logging_writer.py:48] [8996] accumulated_eval_time=1359.439413, accumulated_logging_time=0.332346, accumulated_submission_time=2895.369549, global_step=8996, preemption_count=0, score=2895.369549, test/accuracy=0.985684, test/loss=0.047833, test/mean_average_precision=0.246021, test/num_examples=43793, total_duration=4255.388598, train/accuracy=0.990055, train/loss=0.033290, train/mean_average_precision=0.330259, validation/accuracy=0.986471, validation/loss=0.045191, validation/mean_average_precision=0.240743, validation/num_examples=43793
I0206 04:44:56.464972 140044326475520 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.022229965776205063, loss=0.034086257219314575
I0206 04:45:28.639155 139984133072640 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.017511436715722084, loss=0.04021569341421127
I0206 04:46:00.629106 140044326475520 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0238468199968338, loss=0.034620750695466995
I0206 04:46:32.557179 139984133072640 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.024616166949272156, loss=0.03759569674730301
I0206 04:47:04.530241 140044326475520 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.02163180336356163, loss=0.03309302031993866
I0206 04:47:36.723222 139984133072640 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.02859184518456459, loss=0.03429194539785385
I0206 04:48:08.499300 140044326475520 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.020265907049179077, loss=0.03151687979698181
I0206 04:48:40.243315 139984133072640 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.01841706596314907, loss=0.032655127346515656
I0206 04:48:55.160621 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:50:33.889667 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:50:36.946031 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:50:39.942476 140205209478976 submission_runner.py:408] Time since start: 4600.47s, 	Step: 9748, 	{'train/accuracy': 0.9901725649833679, 'train/loss': 0.03284863755106926, 'train/mean_average_precision': 0.3479799884843341, 'validation/accuracy': 0.9866335391998291, 'validation/loss': 0.04468999430537224, 'validation/mean_average_precision': 0.25269906028148437, 'validation/num_examples': 43793, 'test/accuracy': 0.9858174920082092, 'test/loss': 0.04727887734770775, 'test/mean_average_precision': 0.2526541590940708, 'test/num_examples': 43793, 'score': 3135.624180316925, 'total_duration': 4600.473388910294, 'accumulated_submission_time': 3135.624180316925, 'accumulated_eval_time': 1464.221135854721, 'accumulated_logging_time': 0.3602263927459717}
I0206 04:50:39.959036 140002205157120 logging_writer.py:48] [9748] accumulated_eval_time=1464.221136, accumulated_logging_time=0.360226, accumulated_submission_time=3135.624180, global_step=9748, preemption_count=0, score=3135.624180, test/accuracy=0.985817, test/loss=0.047279, test/mean_average_precision=0.252654, test/num_examples=43793, total_duration=4600.473389, train/accuracy=0.990173, train/loss=0.032849, train/mean_average_precision=0.347980, validation/accuracy=0.986634, validation/loss=0.044690, validation/mean_average_precision=0.252699, validation/num_examples=43793
I0206 04:50:56.973465 140142484387584 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.026276985183358192, loss=0.03562377765774727
I0206 04:51:29.027054 140002205157120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.021263865754008293, loss=0.0326019786298275
I0206 04:52:01.312149 140142484387584 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.03016049973666668, loss=0.03727172315120697
I0206 04:52:33.528919 140002205157120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.03065554052591324, loss=0.03474404662847519
I0206 04:53:05.234790 140142484387584 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.024725399911403656, loss=0.03679320216178894
I0206 04:53:36.931163 140002205157120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.027084045112133026, loss=0.03545157611370087
I0206 04:54:09.282770 140142484387584 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.02092207409441471, loss=0.030597494915127754
I0206 04:54:40.065257 140205209478976 spec.py:321] Evaluating on the training split.
I0206 04:56:17.532876 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 04:56:20.561149 140205209478976 spec.py:349] Evaluating on the test split.
I0206 04:56:23.562188 140205209478976 submission_runner.py:408] Time since start: 4944.09s, 	Step: 10498, 	{'train/accuracy': 0.9901142716407776, 'train/loss': 0.03304450958967209, 'train/mean_average_precision': 0.34253933158701405, 'validation/accuracy': 0.9865771532058716, 'validation/loss': 0.04460688680410385, 'validation/mean_average_precision': 0.25675631240173197, 'validation/num_examples': 43793, 'test/accuracy': 0.9857223033905029, 'test/loss': 0.0474354512989521, 'test/mean_average_precision': 0.24657643442000826, 'test/num_examples': 43793, 'score': 3375.697806596756, 'total_duration': 4944.093198299408, 'accumulated_submission_time': 3375.697806596756, 'accumulated_eval_time': 1567.718020439148, 'accumulated_logging_time': 0.3888866901397705}
I0206 04:56:23.579886 139984133072640 logging_writer.py:48] [10498] accumulated_eval_time=1567.718020, accumulated_logging_time=0.388887, accumulated_submission_time=3375.697807, global_step=10498, preemption_count=0, score=3375.697807, test/accuracy=0.985722, test/loss=0.047435, test/mean_average_precision=0.246576, test/num_examples=43793, total_duration=4944.093198, train/accuracy=0.990114, train/loss=0.033045, train/mean_average_precision=0.342539, validation/accuracy=0.986577, validation/loss=0.044607, validation/mean_average_precision=0.256756, validation/num_examples=43793
I0206 04:56:24.565294 140044326475520 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.024734623730182648, loss=0.03413873165845871
I0206 04:56:56.320154 139984133072640 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.02220223657786846, loss=0.036343805491924286
I0206 04:57:28.631275 140044326475520 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0206663366407156, loss=0.03478289768099785
I0206 04:58:01.015543 139984133072640 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.0189944040030241, loss=0.030404331162571907
I0206 04:58:33.299323 140044326475520 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.025415806099772453, loss=0.034351132810115814
I0206 04:59:05.259593 139984133072640 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.029336825013160706, loss=0.035072971135377884
I0206 04:59:37.567135 140044326475520 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.02131170965731144, loss=0.03211018815636635
I0206 05:00:10.330324 139984133072640 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.019789138808846474, loss=0.031937018036842346
I0206 05:00:23.789760 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:02:01.221368 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:02:06.803844 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:02:09.775083 140205209478976 submission_runner.py:408] Time since start: 5290.31s, 	Step: 11243, 	{'train/accuracy': 0.9903327226638794, 'train/loss': 0.0322994701564312, 'train/mean_average_precision': 0.3561079984004907, 'validation/accuracy': 0.9867005348205566, 'validation/loss': 0.04423731938004494, 'validation/mean_average_precision': 0.2580287784778692, 'validation/num_examples': 43793, 'test/accuracy': 0.9858258962631226, 'test/loss': 0.046979278326034546, 'test/mean_average_precision': 0.2500176737745309, 'test/num_examples': 43793, 'score': 3615.875280857086, 'total_duration': 5290.306088924408, 'accumulated_submission_time': 3615.875280857086, 'accumulated_eval_time': 1673.7032914161682, 'accumulated_logging_time': 0.4190025329589844}
I0206 05:02:09.793469 139984037992192 logging_writer.py:48] [11243] accumulated_eval_time=1673.703291, accumulated_logging_time=0.419003, accumulated_submission_time=3615.875281, global_step=11243, preemption_count=0, score=3615.875281, test/accuracy=0.985826, test/loss=0.046979, test/mean_average_precision=0.250018, test/num_examples=43793, total_duration=5290.306089, train/accuracy=0.990333, train/loss=0.032299, train/mean_average_precision=0.356108, validation/accuracy=0.986701, validation/loss=0.044237, validation/mean_average_precision=0.258029, validation/num_examples=43793
I0206 05:02:28.633244 140142484387584 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.026654744520783424, loss=0.03620753064751625
I0206 05:03:01.087188 139984037992192 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.01883431151509285, loss=0.034221623092889786
I0206 05:03:33.878646 140142484387584 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.022651053965091705, loss=0.032538916915655136
I0206 05:04:06.782178 139984037992192 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.020689528435468674, loss=0.030184805393218994
I0206 05:04:38.469877 140142484387584 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.027028527110815048, loss=0.032612089067697525
I0206 05:05:10.322272 139984037992192 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.029925037175416946, loss=0.03503594547510147
I0206 05:05:42.472281 140142484387584 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.03770161792635918, loss=0.03679851442575455
I0206 05:06:09.921443 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:07:44.311677 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:07:47.304624 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:07:50.325674 140205209478976 submission_runner.py:408] Time since start: 5630.86s, 	Step: 11987, 	{'train/accuracy': 0.990323007106781, 'train/loss': 0.032075900584459305, 'train/mean_average_precision': 0.3612385092120623, 'validation/accuracy': 0.9867829084396362, 'validation/loss': 0.04408801719546318, 'validation/mean_average_precision': 0.26028699428758134, 'validation/num_examples': 43793, 'test/accuracy': 0.9858882427215576, 'test/loss': 0.04693092778325081, 'test/mean_average_precision': 0.25750368349513364, 'test/num_examples': 43793, 'score': 3855.9695858955383, 'total_duration': 5630.856683969498, 'accumulated_submission_time': 3855.9695858955383, 'accumulated_eval_time': 1774.1074786186218, 'accumulated_logging_time': 0.4494631290435791}
I0206 05:07:50.343334 140002205157120 logging_writer.py:48] [11987] accumulated_eval_time=1774.107479, accumulated_logging_time=0.449463, accumulated_submission_time=3855.969586, global_step=11987, preemption_count=0, score=3855.969586, test/accuracy=0.985888, test/loss=0.046931, test/mean_average_precision=0.257504, test/num_examples=43793, total_duration=5630.856684, train/accuracy=0.990323, train/loss=0.032076, train/mean_average_precision=0.361239, validation/accuracy=0.986783, validation/loss=0.044088, validation/mean_average_precision=0.260287, validation/num_examples=43793
I0206 05:07:54.767327 140044326475520 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.029908131808042526, loss=0.03535006195306778
I0206 05:08:26.694300 140002205157120 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.025488387793302536, loss=0.030858423560857773
I0206 05:08:58.000771 140044326475520 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.026565907523036003, loss=0.03327900916337967
I0206 05:09:29.724562 140002205157120 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.027849605306982994, loss=0.03307687118649483
I0206 05:10:01.736227 140044326475520 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.027244092896580696, loss=0.030370991677045822
I0206 05:10:33.619854 140002205157120 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.025416335090994835, loss=0.03194478899240494
I0206 05:11:05.412930 140044326475520 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.023417778313159943, loss=0.031026620417833328
I0206 05:11:37.385567 140002205157120 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.03548872470855713, loss=0.030716532841324806
I0206 05:11:50.595839 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:13:25.823117 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:13:28.887650 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:13:31.949624 140205209478976 submission_runner.py:408] Time since start: 5972.48s, 	Step: 12742, 	{'train/accuracy': 0.9905083775520325, 'train/loss': 0.03149132803082466, 'train/mean_average_precision': 0.3642103863357995, 'validation/accuracy': 0.9867606163024902, 'validation/loss': 0.04409108683466911, 'validation/mean_average_precision': 0.2677271230835614, 'validation/num_examples': 43793, 'test/accuracy': 0.9859021306037903, 'test/loss': 0.04708637669682503, 'test/mean_average_precision': 0.25401200047356093, 'test/num_examples': 43793, 'score': 4096.190707445145, 'total_duration': 5972.480631828308, 'accumulated_submission_time': 4096.190707445145, 'accumulated_eval_time': 1875.4612171649933, 'accumulated_logging_time': 0.4779088497161865}
I0206 05:13:31.967792 139984037992192 logging_writer.py:48] [12742] accumulated_eval_time=1875.461217, accumulated_logging_time=0.477909, accumulated_submission_time=4096.190707, global_step=12742, preemption_count=0, score=4096.190707, test/accuracy=0.985902, test/loss=0.047086, test/mean_average_precision=0.254012, test/num_examples=43793, total_duration=5972.480632, train/accuracy=0.990508, train/loss=0.031491, train/mean_average_precision=0.364210, validation/accuracy=0.986761, validation/loss=0.044091, validation/mean_average_precision=0.267727, validation/num_examples=43793
I0206 05:13:51.607048 139984133072640 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.02688228152692318, loss=0.03233418986201286
I0206 05:14:24.230968 139984037992192 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.03851984068751335, loss=0.03316256031394005
I0206 05:14:57.069198 139984133072640 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.029389895498752594, loss=0.03714563325047493
I0206 05:15:29.497495 139984037992192 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.027933839708566666, loss=0.037405308336019516
I0206 05:16:01.735592 139984133072640 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.034388210624456406, loss=0.030081884935498238
I0206 05:16:34.203893 139984037992192 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.032042212784290314, loss=0.03390948846936226
I0206 05:17:06.170025 139984133072640 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.03177790716290474, loss=0.034651465713977814
I0206 05:17:31.988116 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:19:13.892371 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:19:17.075086 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:19:20.104591 140205209478976 submission_runner.py:408] Time since start: 6320.64s, 	Step: 13482, 	{'train/accuracy': 0.9906244874000549, 'train/loss': 0.03097464144229889, 'train/mean_average_precision': 0.389609197822945, 'validation/accuracy': 0.9867532849311829, 'validation/loss': 0.043991703540086746, 'validation/mean_average_precision': 0.2688608755476995, 'validation/num_examples': 43793, 'test/accuracy': 0.9860032200813293, 'test/loss': 0.046686749905347824, 'test/mean_average_precision': 0.2611335866362385, 'test/num_examples': 43793, 'score': 4336.178166389465, 'total_duration': 6320.635596513748, 'accumulated_submission_time': 4336.178166389465, 'accumulated_eval_time': 1983.5776450634003, 'accumulated_logging_time': 0.5073034763336182}
I0206 05:19:20.121788 140002205157120 logging_writer.py:48] [13482] accumulated_eval_time=1983.577645, accumulated_logging_time=0.507303, accumulated_submission_time=4336.178166, global_step=13482, preemption_count=0, score=4336.178166, test/accuracy=0.986003, test/loss=0.046687, test/mean_average_precision=0.261134, test/num_examples=43793, total_duration=6320.635597, train/accuracy=0.990624, train/loss=0.030975, train/mean_average_precision=0.389609, validation/accuracy=0.986753, validation/loss=0.043992, validation/mean_average_precision=0.268861, validation/num_examples=43793
I0206 05:19:26.604397 140044326475520 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.024752700701355934, loss=0.03048771433532238
I0206 05:19:58.246653 140002205157120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.051760848611593246, loss=0.03228805586695671
I0206 05:20:30.300755 140044326475520 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.026812126860022545, loss=0.03194580227136612
I0206 05:21:02.135744 140002205157120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.027401093393564224, loss=0.030112214386463165
I0206 05:21:33.909856 140044326475520 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03181435912847519, loss=0.02941129542887211
I0206 05:22:05.830327 140002205157120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03272726759314537, loss=0.03223651647567749
I0206 05:22:37.284026 140044326475520 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03400262072682381, loss=0.027907906100153923
I0206 05:23:09.305353 140002205157120 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.03280703350901604, loss=0.028808603063225746
I0206 05:23:20.294724 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:24:57.672523 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:25:00.785670 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:25:03.997870 140205209478976 submission_runner.py:408] Time since start: 6664.53s, 	Step: 14236, 	{'train/accuracy': 0.9906725287437439, 'train/loss': 0.030629856511950493, 'train/mean_average_precision': 0.39957078398726886, 'validation/accuracy': 0.986504077911377, 'validation/loss': 0.0440920814871788, 'validation/mean_average_precision': 0.2636364530613028, 'validation/num_examples': 43793, 'test/accuracy': 0.9858457446098328, 'test/loss': 0.04665074124932289, 'test/mean_average_precision': 0.25670494808584926, 'test/num_examples': 43793, 'score': 4576.319272518158, 'total_duration': 6664.528880357742, 'accumulated_submission_time': 4576.319272518158, 'accumulated_eval_time': 2087.2807455062866, 'accumulated_logging_time': 0.5352981090545654}
I0206 05:25:04.015763 139984037992192 logging_writer.py:48] [14236] accumulated_eval_time=2087.280746, accumulated_logging_time=0.535298, accumulated_submission_time=4576.319273, global_step=14236, preemption_count=0, score=4576.319273, test/accuracy=0.985846, test/loss=0.046651, test/mean_average_precision=0.256705, test/num_examples=43793, total_duration=6664.528880, train/accuracy=0.990673, train/loss=0.030630, train/mean_average_precision=0.399571, validation/accuracy=0.986504, validation/loss=0.044092, validation/mean_average_precision=0.263636, validation/num_examples=43793
I0206 05:25:24.720315 140142484387584 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.04339360445737839, loss=0.03409464284777641
I0206 05:25:56.672328 139984037992192 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.03151100128889084, loss=0.03258180245757103
I0206 05:26:28.591808 140142484387584 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.030820447951555252, loss=0.028874538838863373
I0206 05:27:00.195159 139984037992192 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.040167585015296936, loss=0.033755477517843246
I0206 05:27:32.080444 140142484387584 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.03308061137795448, loss=0.036837201565504074
I0206 05:28:03.887229 139984037992192 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03242111951112747, loss=0.032083604484796524
I0206 05:28:35.526702 140142484387584 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.046490855515003204, loss=0.03342660143971443
I0206 05:29:04.042280 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:30:45.305775 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:30:48.464221 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:30:51.598790 140205209478976 submission_runner.py:408] Time since start: 7012.13s, 	Step: 14991, 	{'train/accuracy': 0.9907553195953369, 'train/loss': 0.030214227735996246, 'train/mean_average_precision': 0.41221099028224745, 'validation/accuracy': 0.986757755279541, 'validation/loss': 0.044588346034288406, 'validation/mean_average_precision': 0.2677942141421415, 'validation/num_examples': 43793, 'test/accuracy': 0.9859707951545715, 'test/loss': 0.04723716154694557, 'test/mean_average_precision': 0.26380350888819126, 'test/num_examples': 43793, 'score': 4816.31423330307, 'total_duration': 7012.129799127579, 'accumulated_submission_time': 4816.31423330307, 'accumulated_eval_time': 2194.837212085724, 'accumulated_logging_time': 0.5642292499542236}
I0206 05:30:51.616765 139984133072640 logging_writer.py:48] [14991] accumulated_eval_time=2194.837212, accumulated_logging_time=0.564229, accumulated_submission_time=4816.314233, global_step=14991, preemption_count=0, score=4816.314233, test/accuracy=0.985971, test/loss=0.047237, test/mean_average_precision=0.263804, test/num_examples=43793, total_duration=7012.129799, train/accuracy=0.990755, train/loss=0.030214, train/mean_average_precision=0.412211, validation/accuracy=0.986758, validation/loss=0.044588, validation/mean_average_precision=0.267794, validation/num_examples=43793
I0206 05:30:54.875402 140002205157120 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0395100861787796, loss=0.03140607848763466
I0206 05:31:27.233461 139984133072640 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.03789006546139717, loss=0.03563528135418892
I0206 05:31:59.444437 140002205157120 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.03962065279483795, loss=0.03678184002637863
I0206 05:32:31.927327 139984133072640 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.03721277788281441, loss=0.03147789090871811
I0206 05:33:04.839443 140002205157120 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.03572779521346092, loss=0.0330205038189888
I0206 05:33:37.569328 139984133072640 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.04044969379901886, loss=0.033627964556217194
I0206 05:34:10.366424 140002205157120 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.040272727608680725, loss=0.03204150125384331
I0206 05:34:42.753122 139984133072640 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.043359946459531784, loss=0.02916555479168892
I0206 05:34:51.696692 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:36:26.528967 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:36:29.694675 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:36:32.799947 140205209478976 submission_runner.py:408] Time since start: 7353.33s, 	Step: 15729, 	{'train/accuracy': 0.9911349415779114, 'train/loss': 0.029304539784789085, 'train/mean_average_precision': 0.430178080129186, 'validation/accuracy': 0.9867480397224426, 'validation/loss': 0.044163040816783905, 'validation/mean_average_precision': 0.2715012183204673, 'validation/num_examples': 43793, 'test/accuracy': 0.9859463572502136, 'test/loss': 0.046861980110406876, 'test/mean_average_precision': 0.26072838119380015, 'test/num_examples': 43793, 'score': 5056.359443902969, 'total_duration': 7353.330956935883, 'accumulated_submission_time': 5056.359443902969, 'accumulated_eval_time': 2295.940425634384, 'accumulated_logging_time': 0.5946774482727051}
I0206 05:36:32.818248 140044326475520 logging_writer.py:48] [15729] accumulated_eval_time=2295.940426, accumulated_logging_time=0.594677, accumulated_submission_time=5056.359444, global_step=15729, preemption_count=0, score=5056.359444, test/accuracy=0.985946, test/loss=0.046862, test/mean_average_precision=0.260728, test/num_examples=43793, total_duration=7353.330957, train/accuracy=0.991135, train/loss=0.029305, train/mean_average_precision=0.430178, validation/accuracy=0.986748, validation/loss=0.044163, validation/mean_average_precision=0.271501, validation/num_examples=43793
I0206 05:36:56.094621 140142484387584 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.03194594383239746, loss=0.030599718913435936
I0206 05:37:28.623098 140044326475520 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.033141639083623886, loss=0.029055634513497353
I0206 05:38:00.884067 140142484387584 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.047757215797901154, loss=0.03184802457690239
I0206 05:38:33.097622 140044326475520 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.03316066786646843, loss=0.03149554878473282
I0206 05:39:05.480648 140142484387584 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.03520023822784424, loss=0.0338643342256546
I0206 05:39:37.934300 140044326475520 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.04097611457109451, loss=0.03265144303441048
I0206 05:40:10.582985 140142484387584 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.037085115909576416, loss=0.030178377404808998
I0206 05:40:32.801137 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:42:09.187055 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:42:12.303356 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:42:15.371275 140205209478976 submission_runner.py:408] Time since start: 7695.90s, 	Step: 16471, 	{'train/accuracy': 0.9909190535545349, 'train/loss': 0.02977074310183525, 'train/mean_average_precision': 0.40910289981395387, 'validation/accuracy': 0.9867866039276123, 'validation/loss': 0.044062357395887375, 'validation/mean_average_precision': 0.2620690775753606, 'validation/num_examples': 43793, 'test/accuracy': 0.9859893321990967, 'test/loss': 0.04678834602236748, 'test/mean_average_precision': 0.25604497180348645, 'test/num_examples': 43793, 'score': 5296.309223651886, 'total_duration': 7695.902285814285, 'accumulated_submission_time': 5296.309223651886, 'accumulated_eval_time': 2398.5105135440826, 'accumulated_logging_time': 0.6254196166992188}
I0206 05:42:15.399921 139984037992192 logging_writer.py:48] [16471] accumulated_eval_time=2398.510514, accumulated_logging_time=0.625420, accumulated_submission_time=5296.309224, global_step=16471, preemption_count=0, score=5296.309224, test/accuracy=0.985989, test/loss=0.046788, test/mean_average_precision=0.256045, test/num_examples=43793, total_duration=7695.902286, train/accuracy=0.990919, train/loss=0.029771, train/mean_average_precision=0.409103, validation/accuracy=0.986787, validation/loss=0.044062, validation/mean_average_precision=0.262069, validation/num_examples=43793
I0206 05:42:25.689602 139984133072640 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.03452588617801666, loss=0.030428536236286163
I0206 05:42:59.365786 139984037992192 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.04268968477845192, loss=0.032810427248477936
I0206 05:43:31.804388 139984133072640 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.04033169150352478, loss=0.030190253630280495
I0206 05:44:04.176896 139984037992192 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.04170360043644905, loss=0.03285739943385124
I0206 05:44:36.268134 139984133072640 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.04301728308200836, loss=0.033920761197805405
I0206 05:45:08.039633 139984037992192 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.03799382224678993, loss=0.03173435851931572
I0206 05:45:39.937689 139984133072640 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.04398048296570778, loss=0.02703731693327427
I0206 05:46:12.492848 139984037992192 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.03883272036910057, loss=0.032048363238573074
I0206 05:46:15.422003 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:47:52.092420 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:47:55.314206 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:47:58.429069 140205209478976 submission_runner.py:408] Time since start: 8038.96s, 	Step: 17210, 	{'train/accuracy': 0.9909037947654724, 'train/loss': 0.029957536607980728, 'train/mean_average_precision': 0.4211723660599582, 'validation/accuracy': 0.9867788553237915, 'validation/loss': 0.044044554233551025, 'validation/mean_average_precision': 0.2709214835239603, 'validation/num_examples': 43793, 'test/accuracy': 0.9859653115272522, 'test/loss': 0.046575117856264114, 'test/mean_average_precision': 0.26149841335593477, 'test/num_examples': 43793, 'score': 5536.299431324005, 'total_duration': 8038.960066795349, 'accumulated_submission_time': 5536.299431324005, 'accumulated_eval_time': 2501.5175173282623, 'accumulated_logging_time': 0.6656818389892578}
I0206 05:47:58.448312 140002205157120 logging_writer.py:48] [17210] accumulated_eval_time=2501.517517, accumulated_logging_time=0.665682, accumulated_submission_time=5536.299431, global_step=17210, preemption_count=0, score=5536.299431, test/accuracy=0.985965, test/loss=0.046575, test/mean_average_precision=0.261498, test/num_examples=43793, total_duration=8038.960067, train/accuracy=0.990904, train/loss=0.029958, train/mean_average_precision=0.421172, validation/accuracy=0.986779, validation/loss=0.044045, validation/mean_average_precision=0.270921, validation/num_examples=43793
I0206 05:48:27.668429 140142484387584 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.03159118443727493, loss=0.0314020998775959
I0206 05:48:59.738874 140002205157120 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.044368941336870193, loss=0.037511084228754044
I0206 05:49:31.904033 140142484387584 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.055166590958833694, loss=0.03426089510321617
I0206 05:50:04.347769 140002205157120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.04913266375660896, loss=0.031571079045534134
I0206 05:50:36.070712 140142484387584 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.039938464760780334, loss=0.03041527047753334
I0206 05:51:07.910514 140002205157120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.03786429017782211, loss=0.031174074858427048
I0206 05:51:40.414087 140142484387584 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.07138623297214508, loss=0.032511308789253235
I0206 05:51:58.595669 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:53:40.734347 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:53:43.761600 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:53:46.767337 140205209478976 submission_runner.py:408] Time since start: 8387.30s, 	Step: 17958, 	{'train/accuracy': 0.9907863736152649, 'train/loss': 0.03013172186911106, 'train/mean_average_precision': 0.4060708456613312, 'validation/accuracy': 0.9868494868278503, 'validation/loss': 0.04417216405272484, 'validation/mean_average_precision': 0.2710522605596167, 'validation/num_examples': 43793, 'test/accuracy': 0.986120343208313, 'test/loss': 0.04671439900994301, 'test/mean_average_precision': 0.26699967942228076, 'test/num_examples': 43793, 'score': 5776.414937496185, 'total_duration': 8387.298349618912, 'accumulated_submission_time': 5776.414937496185, 'accumulated_eval_time': 2609.689152956009, 'accumulated_logging_time': 0.6960053443908691}
I0206 05:53:46.786936 139984037992192 logging_writer.py:48] [17958] accumulated_eval_time=2609.689153, accumulated_logging_time=0.696005, accumulated_submission_time=5776.414937, global_step=17958, preemption_count=0, score=5776.414937, test/accuracy=0.986120, test/loss=0.046714, test/mean_average_precision=0.267000, test/num_examples=43793, total_duration=8387.298350, train/accuracy=0.990786, train/loss=0.030132, train/mean_average_precision=0.406071, validation/accuracy=0.986849, validation/loss=0.044172, validation/mean_average_precision=0.271052, validation/num_examples=43793
I0206 05:54:00.831030 139984133072640 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.046723559498786926, loss=0.03514469787478447
I0206 05:54:32.639668 139984037992192 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.06088295578956604, loss=0.03152993321418762
I0206 05:55:04.461379 139984133072640 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.03506508842110634, loss=0.029418662190437317
I0206 05:55:36.722869 139984037992192 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.0485992431640625, loss=0.03385479748249054
I0206 05:56:08.483673 139984133072640 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.04035290330648422, loss=0.029548978433012962
I0206 05:56:40.519306 139984037992192 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.03825508803129196, loss=0.03141132369637489
I0206 05:57:12.404927 139984133072640 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.040904734283685684, loss=0.03281376138329506
I0206 05:57:44.497623 139984037992192 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.04114767909049988, loss=0.03492210432887077
I0206 05:57:47.030962 140205209478976 spec.py:321] Evaluating on the training split.
I0206 05:59:26.038414 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 05:59:29.723915 140205209478976 spec.py:349] Evaluating on the test split.
I0206 05:59:33.366108 140205209478976 submission_runner.py:408] Time since start: 8733.90s, 	Step: 18709, 	{'train/accuracy': 0.9909077882766724, 'train/loss': 0.029821813106536865, 'train/mean_average_precision': 0.4070056823467816, 'validation/accuracy': 0.9868308305740356, 'validation/loss': 0.04389888420701027, 'validation/mean_average_precision': 0.27659673661417733, 'validation/num_examples': 43793, 'test/accuracy': 0.9861354827880859, 'test/loss': 0.04650556296110153, 'test/mean_average_precision': 0.2658340993299205, 'test/num_examples': 43793, 'score': 6016.625653028488, 'total_duration': 8733.897119998932, 'accumulated_submission_time': 6016.625653028488, 'accumulated_eval_time': 2716.024253845215, 'accumulated_logging_time': 0.7282171249389648}
I0206 05:59:33.384639 140002205157120 logging_writer.py:48] [18709] accumulated_eval_time=2716.024254, accumulated_logging_time=0.728217, accumulated_submission_time=6016.625653, global_step=18709, preemption_count=0, score=6016.625653, test/accuracy=0.986135, test/loss=0.046506, test/mean_average_precision=0.265834, test/num_examples=43793, total_duration=8733.897120, train/accuracy=0.990908, train/loss=0.029822, train/mean_average_precision=0.407006, validation/accuracy=0.986831, validation/loss=0.043899, validation/mean_average_precision=0.276597, validation/num_examples=43793
I0206 06:00:03.164296 140142484387584 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.04601278901100159, loss=0.03161406144499779
I0206 06:00:34.868365 140002205157120 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.04332766681909561, loss=0.03144388273358345
I0206 06:01:06.864730 140142484387584 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.03854001313447952, loss=0.029289159923791885
I0206 06:01:38.113446 140002205157120 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.04460716247558594, loss=0.029524605721235275
I0206 06:02:09.750734 140142484387584 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.04043946787714958, loss=0.029806893318891525
I0206 06:02:41.244740 140002205157120 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.04129478335380554, loss=0.032491110265254974
I0206 06:03:13.034498 140142484387584 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.04432754963636398, loss=0.02980046160519123
I0206 06:03:33.418209 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:05:09.652413 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:05:13.070547 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:05:16.419670 140205209478976 submission_runner.py:408] Time since start: 9076.95s, 	Step: 19465, 	{'train/accuracy': 0.9910405278205872, 'train/loss': 0.029201403260231018, 'train/mean_average_precision': 0.4292364006707121, 'validation/accuracy': 0.9868568181991577, 'validation/loss': 0.04404282569885254, 'validation/mean_average_precision': 0.2791245527026243, 'validation/num_examples': 43793, 'test/accuracy': 0.9860706329345703, 'test/loss': 0.04683166369795799, 'test/mean_average_precision': 0.25962619079546045, 'test/num_examples': 43793, 'score': 6256.626332998276, 'total_duration': 9076.950638532639, 'accumulated_submission_time': 6256.626332998276, 'accumulated_eval_time': 2819.025629043579, 'accumulated_logging_time': 0.7590713500976562}
I0206 06:05:16.440772 139963806111488 logging_writer.py:48] [19465] accumulated_eval_time=2819.025629, accumulated_logging_time=0.759071, accumulated_submission_time=6256.626333, global_step=19465, preemption_count=0, score=6256.626333, test/accuracy=0.986071, test/loss=0.046832, test/mean_average_precision=0.259626, test/num_examples=43793, total_duration=9076.950639, train/accuracy=0.991041, train/loss=0.029201, train/mean_average_precision=0.429236, validation/accuracy=0.986857, validation/loss=0.044043, validation/mean_average_precision=0.279125, validation/num_examples=43793
I0206 06:05:27.894599 139984133072640 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.04383889585733414, loss=0.03196563571691513
I0206 06:06:00.112131 139963806111488 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.0434037446975708, loss=0.02784872055053711
I0206 06:06:32.333179 139984133072640 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.03782227262854576, loss=0.029529232531785965
I0206 06:07:04.097565 139963806111488 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.053831469267606735, loss=0.0313710980117321
I0206 06:07:36.158528 139984133072640 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.046447403728961945, loss=0.030559442937374115
I0206 06:08:08.183151 139963806111488 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.05058121681213379, loss=0.03227786719799042
I0206 06:08:40.049503 139984133072640 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04471507668495178, loss=0.02825811877846718
I0206 06:09:12.069987 139963806111488 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.04345422238111496, loss=0.03146792948246002
I0206 06:09:16.474508 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:10:54.248200 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:10:57.292999 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:11:00.298331 140205209478976 submission_runner.py:408] Time since start: 9420.83s, 	Step: 20215, 	{'train/accuracy': 0.9910680651664734, 'train/loss': 0.02918209321796894, 'train/mean_average_precision': 0.439064296870055, 'validation/accuracy': 0.986634373664856, 'validation/loss': 0.044114645570516586, 'validation/mean_average_precision': 0.28133250854834996, 'validation/num_examples': 43793, 'test/accuracy': 0.9858280420303345, 'test/loss': 0.04660707712173462, 'test/mean_average_precision': 0.26924369568151874, 'test/num_examples': 43793, 'score': 6496.625156402588, 'total_duration': 9420.829341888428, 'accumulated_submission_time': 6496.625156402588, 'accumulated_eval_time': 2922.849404811859, 'accumulated_logging_time': 0.792198657989502}
I0206 06:11:00.318257 139984037992192 logging_writer.py:48] [20215] accumulated_eval_time=2922.849405, accumulated_logging_time=0.792199, accumulated_submission_time=6496.625156, global_step=20215, preemption_count=0, score=6496.625156, test/accuracy=0.985828, test/loss=0.046607, test/mean_average_precision=0.269244, test/num_examples=43793, total_duration=9420.829342, train/accuracy=0.991068, train/loss=0.029182, train/mean_average_precision=0.439064, validation/accuracy=0.986634, validation/loss=0.044115, validation/mean_average_precision=0.281333, validation/num_examples=43793
I0206 06:11:28.043576 140142484387584 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.0463285818696022, loss=0.03066210262477398
I0206 06:11:59.808537 139984037992192 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.045109208673238754, loss=0.035289179533720016
I0206 06:12:31.814774 140142484387584 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.04741712287068367, loss=0.028880765661597252
I0206 06:13:04.151741 139984037992192 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.04525888338685036, loss=0.029707971960306168
I0206 06:13:36.281615 140142484387584 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.04221916198730469, loss=0.0306047685444355
I0206 06:14:08.354068 139984037992192 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.050372540950775146, loss=0.03000682406127453
I0206 06:14:40.717347 140142484387584 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.044163890182971954, loss=0.027614189311861992
I0206 06:15:00.551755 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:16:34.138412 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:16:37.186410 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:16:40.187243 140205209478976 submission_runner.py:408] Time since start: 9760.72s, 	Step: 20963, 	{'train/accuracy': 0.9911518692970276, 'train/loss': 0.02880358137190342, 'train/mean_average_precision': 0.4462325316914583, 'validation/accuracy': 0.9868718385696411, 'validation/loss': 0.04390169680118561, 'validation/mean_average_precision': 0.2812189409328452, 'validation/num_examples': 43793, 'test/accuracy': 0.9860310554504395, 'test/loss': 0.04686373099684715, 'test/mean_average_precision': 0.26727420961545023, 'test/num_examples': 43793, 'score': 6736.826936483383, 'total_duration': 9760.718255281448, 'accumulated_submission_time': 6736.826936483383, 'accumulated_eval_time': 3022.484857082367, 'accumulated_logging_time': 0.8230326175689697}
I0206 06:16:40.206907 139963806111488 logging_writer.py:48] [20963] accumulated_eval_time=3022.484857, accumulated_logging_time=0.823033, accumulated_submission_time=6736.826936, global_step=20963, preemption_count=0, score=6736.826936, test/accuracy=0.986031, test/loss=0.046864, test/mean_average_precision=0.267274, test/num_examples=43793, total_duration=9760.718255, train/accuracy=0.991152, train/loss=0.028804, train/mean_average_precision=0.446233, validation/accuracy=0.986872, validation/loss=0.043902, validation/mean_average_precision=0.281219, validation/num_examples=43793
I0206 06:16:52.426324 139984133072640 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.046132586896419525, loss=0.03148344159126282
I0206 06:17:24.432690 139963806111488 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.05635857954621315, loss=0.0332116074860096
I0206 06:17:55.983285 139984133072640 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.04674944654107094, loss=0.030747024342417717
I0206 06:18:28.025881 139963806111488 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.054929159581661224, loss=0.02945183776319027
I0206 06:19:00.259882 139984133072640 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.04949750751256943, loss=0.031212996691465378
I0206 06:19:32.391881 139963806111488 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.049211129546165466, loss=0.030011514201760292
I0206 06:20:04.627492 139984133072640 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.051170531660318375, loss=0.03128698468208313
I0206 06:20:36.564178 139963806111488 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.04483480751514435, loss=0.02821318432688713
I0206 06:20:40.443758 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:22:16.056967 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:22:19.107243 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:22:22.117219 140205209478976 submission_runner.py:408] Time since start: 10102.65s, 	Step: 21713, 	{'train/accuracy': 0.9913423657417297, 'train/loss': 0.027976617217063904, 'train/mean_average_precision': 0.4702118638735022, 'validation/accuracy': 0.9869785904884338, 'validation/loss': 0.043873559683561325, 'validation/mean_average_precision': 0.285625691321616, 'validation/num_examples': 43793, 'test/accuracy': 0.9860352277755737, 'test/loss': 0.04697255417704582, 'test/mean_average_precision': 0.2679272339349347, 'test/num_examples': 43793, 'score': 6977.031692266464, 'total_duration': 10102.648232460022, 'accumulated_submission_time': 6977.031692266464, 'accumulated_eval_time': 3124.1582732200623, 'accumulated_logging_time': 0.8541834354400635}
I0206 06:22:22.136418 139984037992192 logging_writer.py:48] [21713] accumulated_eval_time=3124.158273, accumulated_logging_time=0.854183, accumulated_submission_time=6977.031692, global_step=21713, preemption_count=0, score=6977.031692, test/accuracy=0.986035, test/loss=0.046973, test/mean_average_precision=0.267927, test/num_examples=43793, total_duration=10102.648232, train/accuracy=0.991342, train/loss=0.027977, train/mean_average_precision=0.470212, validation/accuracy=0.986979, validation/loss=0.043874, validation/mean_average_precision=0.285626, validation/num_examples=43793
I0206 06:22:50.556120 140142484387584 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.043387312442064285, loss=0.030039826408028603
I0206 06:23:22.645688 139984037992192 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.04385325685143471, loss=0.029257722198963165
I0206 06:23:54.789620 140142484387584 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.052474409341812134, loss=0.02756274864077568
I0206 06:24:26.989932 139984037992192 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.062094178050756454, loss=0.03285776078701019
I0206 06:24:59.055485 140142484387584 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.051635660231113434, loss=0.0338699035346508
I0206 06:25:31.753760 139984037992192 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.04839326813817024, loss=0.03166307881474495
I0206 06:26:04.502324 140142484387584 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.062458571046590805, loss=0.03206362947821617
I0206 06:26:22.167603 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:28:02.277180 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:28:05.336203 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:28:08.343002 140205209478976 submission_runner.py:408] Time since start: 10448.87s, 	Step: 22456, 	{'train/accuracy': 0.9913394451141357, 'train/loss': 0.028038788586854935, 'train/mean_average_precision': 0.4604188786782695, 'validation/accuracy': 0.9869635701179504, 'validation/loss': 0.04374273493885994, 'validation/mean_average_precision': 0.28277668192121996, 'validation/num_examples': 43793, 'test/accuracy': 0.9862037301063538, 'test/loss': 0.04664532467722893, 'test/mean_average_precision': 0.2640839126761976, 'test/num_examples': 43793, 'score': 7217.031193494797, 'total_duration': 10448.873989105225, 'accumulated_submission_time': 7217.031193494797, 'accumulated_eval_time': 3230.3336248397827, 'accumulated_logging_time': 0.8844475746154785}
I0206 06:28:08.363442 139963806111488 logging_writer.py:48] [22456] accumulated_eval_time=3230.333625, accumulated_logging_time=0.884448, accumulated_submission_time=7217.031193, global_step=22456, preemption_count=0, score=7217.031193, test/accuracy=0.986204, test/loss=0.046645, test/mean_average_precision=0.264084, test/num_examples=43793, total_duration=10448.873989, train/accuracy=0.991339, train/loss=0.028039, train/mean_average_precision=0.460419, validation/accuracy=0.986964, validation/loss=0.043743, validation/mean_average_precision=0.282777, validation/num_examples=43793
I0206 06:28:22.873269 139984133072640 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.055678997188806534, loss=0.03419823572039604
I0206 06:28:55.072111 139963806111488 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.04881851375102997, loss=0.029576601460576057
I0206 06:29:27.596235 139984133072640 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.04771287739276886, loss=0.02889803983271122
I0206 06:29:59.881717 139963806111488 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.052007585763931274, loss=0.031564146280288696
I0206 06:30:31.948181 139984133072640 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.04897063598036766, loss=0.027376888319849968
I0206 06:31:04.072721 139963806111488 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.0494108721613884, loss=0.03300907090306282
I0206 06:31:35.458881 139984133072640 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.04566849768161774, loss=0.02856013923883438
I0206 06:32:07.501641 139963806111488 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.05610356852412224, loss=0.03370782360434532
I0206 06:32:08.488543 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:33:42.973815 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:33:46.397559 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:33:49.757316 140205209478976 submission_runner.py:408] Time since start: 10790.29s, 	Step: 23204, 	{'train/accuracy': 0.9916256070137024, 'train/loss': 0.02725934237241745, 'train/mean_average_precision': 0.4747280372458277, 'validation/accuracy': 0.9869043231010437, 'validation/loss': 0.044060613960027695, 'validation/mean_average_precision': 0.28456510734483886, 'validation/num_examples': 43793, 'test/accuracy': 0.986123263835907, 'test/loss': 0.04690439626574516, 'test/mean_average_precision': 0.26278994052956384, 'test/num_examples': 43793, 'score': 7457.124490976334, 'total_duration': 10790.28831577301, 'accumulated_submission_time': 7457.124490976334, 'accumulated_eval_time': 3331.6023383140564, 'accumulated_logging_time': 0.9160325527191162}
I0206 06:33:49.784759 139984037992192 logging_writer.py:48] [23204] accumulated_eval_time=3331.602338, accumulated_logging_time=0.916033, accumulated_submission_time=7457.124491, global_step=23204, preemption_count=0, score=7457.124491, test/accuracy=0.986123, test/loss=0.046904, test/mean_average_precision=0.262790, test/num_examples=43793, total_duration=10790.288316, train/accuracy=0.991626, train/loss=0.027259, train/mean_average_precision=0.474728, validation/accuracy=0.986904, validation/loss=0.044061, validation/mean_average_precision=0.284565, validation/num_examples=43793
I0206 06:34:21.254638 140002205157120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.05338487774133682, loss=0.030686452984809875
I0206 06:34:53.239579 139984037992192 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.04568212479352951, loss=0.027169430628418922
I0206 06:35:24.823850 140002205157120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0643790140748024, loss=0.02841578982770443
I0206 06:35:56.655362 139984037992192 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.053989022970199585, loss=0.027818521484732628
I0206 06:36:28.309086 140002205157120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.05759027972817421, loss=0.029839273542165756
I0206 06:37:00.704295 139984037992192 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.04748670384287834, loss=0.032934341579675674
I0206 06:37:32.480222 140002205157120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.05377316102385521, loss=0.030040675774216652
I0206 06:37:49.814926 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:39:28.601997 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:39:31.652915 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:39:34.631925 140205209478976 submission_runner.py:408] Time since start: 11135.16s, 	Step: 23956, 	{'train/accuracy': 0.9913517236709595, 'train/loss': 0.028187081217765808, 'train/mean_average_precision': 0.46011791682661285, 'validation/accuracy': 0.9868783354759216, 'validation/loss': 0.04426131770014763, 'validation/mean_average_precision': 0.280703321938136, 'validation/num_examples': 43793, 'test/accuracy': 0.985987663269043, 'test/loss': 0.047034889459609985, 'test/mean_average_precision': 0.26449449777889183, 'test/num_examples': 43793, 'score': 7697.121632099152, 'total_duration': 11135.16293668747, 'accumulated_submission_time': 7697.121632099152, 'accumulated_eval_time': 3436.4192943573, 'accumulated_logging_time': 0.9561116695404053}
I0206 06:39:34.651146 139963806111488 logging_writer.py:48] [23956] accumulated_eval_time=3436.419294, accumulated_logging_time=0.956112, accumulated_submission_time=7697.121632, global_step=23956, preemption_count=0, score=7697.121632, test/accuracy=0.985988, test/loss=0.047035, test/mean_average_precision=0.264494, test/num_examples=43793, total_duration=11135.162937, train/accuracy=0.991352, train/loss=0.028187, train/mean_average_precision=0.460118, validation/accuracy=0.986878, validation/loss=0.044261, validation/mean_average_precision=0.280703, validation/num_examples=43793
I0206 06:39:49.413647 139984133072640 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.06472600251436234, loss=0.028877219185233116
I0206 06:40:21.029242 139963806111488 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.06974244862794876, loss=0.0370054692029953
I0206 06:40:52.620047 139984133072640 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.05056586489081383, loss=0.03058343194425106
I0206 06:41:24.634701 139963806111488 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.05573917552828789, loss=0.028263257816433907
I0206 06:41:56.438971 139984133072640 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.05940897762775421, loss=0.03335318714380264
I0206 06:42:28.084438 139963806111488 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.05861697718501091, loss=0.03209419921040535
I0206 06:42:59.818410 139984133072640 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.05108015984296799, loss=0.029253555461764336
I0206 06:43:31.528815 139963806111488 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.05675193667411804, loss=0.030768267810344696
I0206 06:43:34.694681 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:45:12.637616 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:45:15.631063 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:45:18.607904 140205209478976 submission_runner.py:408] Time since start: 11479.14s, 	Step: 24711, 	{'train/accuracy': 0.9913532137870789, 'train/loss': 0.02813563123345375, 'train/mean_average_precision': 0.45984555097949037, 'validation/accuracy': 0.9870455861091614, 'validation/loss': 0.0438859760761261, 'validation/mean_average_precision': 0.28833704762538204, 'validation/num_examples': 43793, 'test/accuracy': 0.9862121343612671, 'test/loss': 0.04682972654700279, 'test/mean_average_precision': 0.26687861603947516, 'test/num_examples': 43793, 'score': 7937.132448196411, 'total_duration': 11479.138907432556, 'accumulated_submission_time': 7937.132448196411, 'accumulated_eval_time': 3540.332468509674, 'accumulated_logging_time': 0.987633228302002}
I0206 06:45:18.627665 139984037992192 logging_writer.py:48] [24711] accumulated_eval_time=3540.332469, accumulated_logging_time=0.987633, accumulated_submission_time=7937.132448, global_step=24711, preemption_count=0, score=7937.132448, test/accuracy=0.986212, test/loss=0.046830, test/mean_average_precision=0.266879, test/num_examples=43793, total_duration=11479.138907, train/accuracy=0.991353, train/loss=0.028136, train/mean_average_precision=0.459846, validation/accuracy=0.987046, validation/loss=0.043886, validation/mean_average_precision=0.288337, validation/num_examples=43793
I0206 06:45:47.439332 140142484387584 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.05302205681800842, loss=0.033792827278375626
I0206 06:46:19.535198 139984037992192 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.052693333476781845, loss=0.030715733766555786
I0206 06:46:51.794394 140142484387584 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.04973752051591873, loss=0.02961098961532116
I0206 06:47:23.550056 139984037992192 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.08454163372516632, loss=0.032678376883268356
I0206 06:47:55.457515 140142484387584 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.05832785740494728, loss=0.029224872589111328
I0206 06:48:27.479426 139984037992192 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.055105842649936676, loss=0.030141720548272133
I0206 06:48:59.216901 140142484387584 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.05862703546881676, loss=0.030497625470161438
I0206 06:49:18.782029 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:50:54.266580 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:50:57.329162 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:51:00.319352 140205209478976 submission_runner.py:408] Time since start: 11820.85s, 	Step: 25462, 	{'train/accuracy': 0.991429328918457, 'train/loss': 0.027779962867498398, 'train/mean_average_precision': 0.46719259355084936, 'validation/accuracy': 0.9870439767837524, 'validation/loss': 0.04369185119867325, 'validation/mean_average_precision': 0.28888979629004624, 'validation/num_examples': 43793, 'test/accuracy': 0.9861923456192017, 'test/loss': 0.04665488749742508, 'test/mean_average_precision': 0.2683779409036351, 'test/num_examples': 43793, 'score': 8177.254436969757, 'total_duration': 11820.850269317627, 'accumulated_submission_time': 8177.254436969757, 'accumulated_eval_time': 3641.869652032852, 'accumulated_logging_time': 1.0197508335113525}
I0206 06:51:00.339187 139963806111488 logging_writer.py:48] [25462] accumulated_eval_time=3641.869652, accumulated_logging_time=1.019751, accumulated_submission_time=8177.254437, global_step=25462, preemption_count=0, score=8177.254437, test/accuracy=0.986192, test/loss=0.046655, test/mean_average_precision=0.268378, test/num_examples=43793, total_duration=11820.850269, train/accuracy=0.991429, train/loss=0.027780, train/mean_average_precision=0.467193, validation/accuracy=0.987044, validation/loss=0.043692, validation/mean_average_precision=0.288890, validation/num_examples=43793
I0206 06:51:12.786162 139984133072640 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.05694863572716713, loss=0.029342999681830406
I0206 06:51:44.843137 139963806111488 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.04861697182059288, loss=0.029360057786107063
I0206 06:52:16.956810 139984133072640 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.0494920015335083, loss=0.02838253602385521
I0206 06:52:48.835157 139963806111488 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.056456420570611954, loss=0.030827956274151802
I0206 06:53:20.743724 139984133072640 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.04774199053645134, loss=0.030548591166734695
I0206 06:53:52.575448 139963806111488 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.05805009976029396, loss=0.03126266971230507
I0206 06:54:24.353834 139984133072640 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.06387335062026978, loss=0.030787575989961624
I0206 06:54:56.413619 139963806111488 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.0563603974878788, loss=0.03245212510228157
I0206 06:55:00.611875 140205209478976 spec.py:321] Evaluating on the training split.
I0206 06:56:38.280776 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 06:56:41.367428 140205209478976 spec.py:349] Evaluating on the test split.
I0206 06:56:44.342088 140205209478976 submission_runner.py:408] Time since start: 12164.87s, 	Step: 26214, 	{'train/accuracy': 0.991330623626709, 'train/loss': 0.028105879202485085, 'train/mean_average_precision': 0.4568819825250968, 'validation/accuracy': 0.9869639873504639, 'validation/loss': 0.04388206824660301, 'validation/mean_average_precision': 0.28561856725839535, 'validation/num_examples': 43793, 'test/accuracy': 0.9861291646957397, 'test/loss': 0.04659854993224144, 'test/mean_average_precision': 0.26752011398182646, 'test/num_examples': 43793, 'score': 8417.495745420456, 'total_duration': 12164.87309885025, 'accumulated_submission_time': 8417.495745420456, 'accumulated_eval_time': 3745.5998179912567, 'accumulated_logging_time': 1.0510282516479492}
I0206 06:56:44.362290 139984037992192 logging_writer.py:48] [26214] accumulated_eval_time=3745.599818, accumulated_logging_time=1.051028, accumulated_submission_time=8417.495745, global_step=26214, preemption_count=0, score=8417.495745, test/accuracy=0.986129, test/loss=0.046599, test/mean_average_precision=0.267520, test/num_examples=43793, total_duration=12164.873099, train/accuracy=0.991331, train/loss=0.028106, train/mean_average_precision=0.456882, validation/accuracy=0.986964, validation/loss=0.043882, validation/mean_average_precision=0.285619, validation/num_examples=43793
I0206 06:57:12.176727 140142484387584 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.05450914427638054, loss=0.029430052265524864
I0206 06:57:44.432265 139984037992192 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.06344945728778839, loss=0.032171521335840225
I0206 06:58:16.359823 140142484387584 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.05389558523893356, loss=0.02711714617908001
I0206 06:58:47.920618 139984037992192 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.06231964752078056, loss=0.026693785563111305
I0206 06:59:19.868484 140142484387584 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.05591422691941261, loss=0.030165450647473335
I0206 06:59:51.967267 139984037992192 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.05234019458293915, loss=0.02771643176674843
I0206 07:00:24.013100 140142484387584 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.05607503652572632, loss=0.030826009809970856
I0206 07:00:44.529265 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:02:19.512619 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:02:22.549699 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:02:25.561799 140205209478976 submission_runner.py:408] Time since start: 12506.09s, 	Step: 26964, 	{'train/accuracy': 0.9913989305496216, 'train/loss': 0.027905093505978584, 'train/mean_average_precision': 0.4681628812771156, 'validation/accuracy': 0.9868897199630737, 'validation/loss': 0.043874591588974, 'validation/mean_average_precision': 0.2818689651514881, 'validation/num_examples': 43793, 'test/accuracy': 0.986011266708374, 'test/loss': 0.04663465917110443, 'test/mean_average_precision': 0.2703870400534875, 'test/num_examples': 43793, 'score': 8657.630143404007, 'total_duration': 12506.092809200287, 'accumulated_submission_time': 8657.630143404007, 'accumulated_eval_time': 3846.632305622101, 'accumulated_logging_time': 1.0833914279937744}
I0206 07:02:25.582017 139984133072640 logging_writer.py:48] [26964] accumulated_eval_time=3846.632306, accumulated_logging_time=1.083391, accumulated_submission_time=8657.630143, global_step=26964, preemption_count=0, score=8657.630143, test/accuracy=0.986011, test/loss=0.046635, test/mean_average_precision=0.270387, test/num_examples=43793, total_duration=12506.092809, train/accuracy=0.991399, train/loss=0.027905, train/mean_average_precision=0.468163, validation/accuracy=0.986890, validation/loss=0.043875, validation/mean_average_precision=0.281869, validation/num_examples=43793
I0206 07:02:37.400371 140002205157120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.071836918592453, loss=0.030243605375289917
I0206 07:03:09.322169 139984133072640 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.05727483704686165, loss=0.027670687064528465
I0206 07:03:40.914050 140002205157120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.05594056844711304, loss=0.03142278268933296
I0206 07:04:12.832769 139984133072640 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.060845330357551575, loss=0.028777185827493668
I0206 07:04:44.601155 140002205157120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.05308083817362785, loss=0.030040932819247246
I0206 07:05:18.395224 139984133072640 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.06721751391887665, loss=0.026733700186014175
I0206 07:05:51.329266 140002205157120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.06480537354946136, loss=0.03192990645766258
I0206 07:06:23.287301 139984133072640 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.061667293310165405, loss=0.032063644379377365
I0206 07:06:25.819044 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:08:03.005914 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:08:06.069094 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:08:09.040405 140205209478976 submission_runner.py:408] Time since start: 12849.57s, 	Step: 27709, 	{'train/accuracy': 0.9916355013847351, 'train/loss': 0.027098597958683968, 'train/mean_average_precision': 0.4803630402502638, 'validation/accuracy': 0.986885666847229, 'validation/loss': 0.04390691965818405, 'validation/mean_average_precision': 0.28674508422373507, 'validation/num_examples': 43793, 'test/accuracy': 0.9860866665840149, 'test/loss': 0.046626582741737366, 'test/mean_average_precision': 0.26376158400125727, 'test/num_examples': 43793, 'score': 8897.8357629776, 'total_duration': 12849.571416378021, 'accumulated_submission_time': 8897.8357629776, 'accumulated_eval_time': 3949.8536190986633, 'accumulated_logging_time': 1.1148099899291992}
I0206 07:08:09.060670 139963806111488 logging_writer.py:48] [27709] accumulated_eval_time=3949.853619, accumulated_logging_time=1.114810, accumulated_submission_time=8897.835763, global_step=27709, preemption_count=0, score=8897.835763, test/accuracy=0.986087, test/loss=0.046627, test/mean_average_precision=0.263762, test/num_examples=43793, total_duration=12849.571416, train/accuracy=0.991636, train/loss=0.027099, train/mean_average_precision=0.480363, validation/accuracy=0.986886, validation/loss=0.043907, validation/mean_average_precision=0.286745, validation/num_examples=43793
I0206 07:08:38.735616 140142484387584 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.0574411004781723, loss=0.03011382557451725
I0206 07:09:10.522305 139963806111488 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.06345061957836151, loss=0.032355647534132004
I0206 07:09:42.295989 140142484387584 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.05100076273083687, loss=0.028721127659082413
I0206 07:10:14.439600 139963806111488 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.05713382735848427, loss=0.03021036460995674
I0206 07:10:46.653720 140142484387584 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.05375866964459419, loss=0.03111170418560505
I0206 07:11:18.946164 139963806111488 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.05173830687999725, loss=0.029699504375457764
I0206 07:11:51.162506 140142484387584 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.049851056188344955, loss=0.02862299419939518
I0206 07:12:09.258313 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:13:45.025038 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:13:48.073649 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:13:51.003846 140205209478976 submission_runner.py:408] Time since start: 13191.53s, 	Step: 28458, 	{'train/accuracy': 0.9917035102844238, 'train/loss': 0.026701265946030617, 'train/mean_average_precision': 0.49122608416394264, 'validation/accuracy': 0.9870139360427856, 'validation/loss': 0.043779049068689346, 'validation/mean_average_precision': 0.2853140712726124, 'validation/num_examples': 43793, 'test/accuracy': 0.9860950708389282, 'test/loss': 0.04675459489226341, 'test/mean_average_precision': 0.2691075067365001, 'test/num_examples': 43793, 'score': 9137.999815702438, 'total_duration': 13191.534855604172, 'accumulated_submission_time': 9137.999815702438, 'accumulated_eval_time': 4051.5991084575653, 'accumulated_logging_time': 1.148134708404541}
I0206 07:13:51.024690 139984037992192 logging_writer.py:48] [28458] accumulated_eval_time=4051.599108, accumulated_logging_time=1.148135, accumulated_submission_time=9137.999816, global_step=28458, preemption_count=0, score=9137.999816, test/accuracy=0.986095, test/loss=0.046755, test/mean_average_precision=0.269108, test/num_examples=43793, total_duration=13191.534856, train/accuracy=0.991704, train/loss=0.026701, train/mean_average_precision=0.491226, validation/accuracy=0.987014, validation/loss=0.043779, validation/mean_average_precision=0.285314, validation/num_examples=43793
I0206 07:14:04.683513 140002205157120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.07049202919006348, loss=0.028957076370716095
I0206 07:14:37.823007 139984037992192 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.05142763629555702, loss=0.02781953662633896
I0206 07:15:11.038511 140002205157120 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.05645691975951195, loss=0.02706967107951641
I0206 07:15:42.684289 139984037992192 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.056768205016851425, loss=0.02998044528067112
I0206 07:16:14.475900 140002205157120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.05285469442605972, loss=0.030560899525880814
I0206 07:16:46.209644 139984037992192 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.057415883988142014, loss=0.0317198745906353
I0206 07:17:18.443511 140002205157120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.05318983271718025, loss=0.028677962720394135
I0206 07:17:50.313555 139984037992192 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.05288438871502876, loss=0.025271479040384293
I0206 07:17:51.268075 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:19:26.792216 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:19:29.892075 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:19:32.911618 140205209478976 submission_runner.py:408] Time since start: 13533.44s, 	Step: 29204, 	{'train/accuracy': 0.9919525980949402, 'train/loss': 0.025991961359977722, 'train/mean_average_precision': 0.5057251405123364, 'validation/accuracy': 0.9870707392692566, 'validation/loss': 0.04379204288125038, 'validation/mean_average_precision': 0.290256895570826, 'validation/num_examples': 43793, 'test/accuracy': 0.986171305179596, 'test/loss': 0.046700723469257355, 'test/mean_average_precision': 0.2694446762523158, 'test/num_examples': 43793, 'score': 9378.210587263107, 'total_duration': 13533.442611694336, 'accumulated_submission_time': 9378.210587263107, 'accumulated_eval_time': 4153.242586612701, 'accumulated_logging_time': 1.1815519332885742}
I0206 07:19:32.932312 139984133072640 logging_writer.py:48] [29204] accumulated_eval_time=4153.242587, accumulated_logging_time=1.181552, accumulated_submission_time=9378.210587, global_step=29204, preemption_count=0, score=9378.210587, test/accuracy=0.986171, test/loss=0.046701, test/mean_average_precision=0.269445, test/num_examples=43793, total_duration=13533.442612, train/accuracy=0.991953, train/loss=0.025992, train/mean_average_precision=0.505725, validation/accuracy=0.987071, validation/loss=0.043792, validation/mean_average_precision=0.290257, validation/num_examples=43793
I0206 07:20:03.727525 140142484387584 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.07286533713340759, loss=0.030497891828417778
I0206 07:20:35.582313 139984133072640 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.05117185041308403, loss=0.027845872566103935
I0206 07:21:07.450635 140142484387584 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.05934758111834526, loss=0.03046293370425701
I0206 07:21:38.925395 139984133072640 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.0538942925632, loss=0.027755165472626686
I0206 07:22:10.792099 140142484387584 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.05770702660083771, loss=0.0283821951597929
I0206 07:22:42.324797 139984133072640 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.05744628980755806, loss=0.026204673573374748
I0206 07:23:13.988042 140142484387584 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.05590501055121422, loss=0.02900991402566433
I0206 07:23:33.017835 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:25:13.243063 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:25:16.416471 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:25:19.490280 140205209478976 submission_runner.py:408] Time since start: 13880.02s, 	Step: 29961, 	{'train/accuracy': 0.9919412136077881, 'train/loss': 0.025817345827817917, 'train/mean_average_precision': 0.5112644960667561, 'validation/accuracy': 0.9870638251304626, 'validation/loss': 0.043964121490716934, 'validation/mean_average_precision': 0.289960550844474, 'validation/num_examples': 43793, 'test/accuracy': 0.9861283302307129, 'test/loss': 0.04714837670326233, 'test/mean_average_precision': 0.27247323731981765, 'test/num_examples': 43793, 'score': 9618.264122486115, 'total_duration': 13880.021289348602, 'accumulated_submission_time': 9618.264122486115, 'accumulated_eval_time': 4259.714983224869, 'accumulated_logging_time': 1.2135634422302246}
I0206 07:25:19.512165 139984037992192 logging_writer.py:48] [29961] accumulated_eval_time=4259.714983, accumulated_logging_time=1.213563, accumulated_submission_time=9618.264122, global_step=29961, preemption_count=0, score=9618.264122, test/accuracy=0.986128, test/loss=0.047148, test/mean_average_precision=0.272473, test/num_examples=43793, total_duration=13880.021289, train/accuracy=0.991941, train/loss=0.025817, train/mean_average_precision=0.511264, validation/accuracy=0.987064, validation/loss=0.043964, validation/mean_average_precision=0.289961, validation/num_examples=43793
I0206 07:25:32.316409 140002205157120 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.055984847247600555, loss=0.031070785596966743
I0206 07:26:04.291125 139984037992192 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.05582660436630249, loss=0.030017077922821045
I0206 07:26:35.896480 140002205157120 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.08627687394618988, loss=0.030844829976558685
I0206 07:27:07.525873 139984037992192 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.058087754994630814, loss=0.030613981187343597
I0206 07:27:39.305814 140002205157120 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.056617870926856995, loss=0.026540368795394897
I0206 07:28:11.027294 139984037992192 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.07497359812259674, loss=0.026772011071443558
I0206 07:28:43.217833 140002205157120 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.057853102684020996, loss=0.029380153864622116
I0206 07:29:15.438047 139984037992192 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.059122852981090546, loss=0.03144744411110878
I0206 07:29:19.571973 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:30:57.482470 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:31:00.617788 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:31:03.809262 140205209478976 submission_runner.py:408] Time since start: 14224.34s, 	Step: 30714, 	{'train/accuracy': 0.9917917847633362, 'train/loss': 0.02630518563091755, 'train/mean_average_precision': 0.5038807477785993, 'validation/accuracy': 0.9869644045829773, 'validation/loss': 0.0440908707678318, 'validation/mean_average_precision': 0.2894453883646158, 'validation/num_examples': 43793, 'test/accuracy': 0.9860677123069763, 'test/loss': 0.04717359319329262, 'test/mean_average_precision': 0.2675294987951833, 'test/num_examples': 43793, 'score': 9858.291661024094, 'total_duration': 14224.340274810791, 'accumulated_submission_time': 9858.291661024094, 'accumulated_eval_time': 4363.952226638794, 'accumulated_logging_time': 1.246518850326538}
I0206 07:31:03.831080 139984133072640 logging_writer.py:48] [30714] accumulated_eval_time=4363.952227, accumulated_logging_time=1.246519, accumulated_submission_time=9858.291661, global_step=30714, preemption_count=0, score=9858.291661, test/accuracy=0.986068, test/loss=0.047174, test/mean_average_precision=0.267529, test/num_examples=43793, total_duration=14224.340275, train/accuracy=0.991792, train/loss=0.026305, train/mean_average_precision=0.503881, validation/accuracy=0.986964, validation/loss=0.044091, validation/mean_average_precision=0.289445, validation/num_examples=43793
I0206 07:31:32.541089 140142484387584 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.0629817545413971, loss=0.027529969811439514
I0206 07:32:04.878538 139984133072640 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.05672026798129082, loss=0.025092024356126785
I0206 07:32:36.970777 140142484387584 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.05909593030810356, loss=0.028413984924554825
I0206 07:33:09.279227 139984133072640 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.05423639342188835, loss=0.025920793414115906
I0206 07:33:41.325454 140142484387584 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.05716483294963837, loss=0.028268881142139435
I0206 07:34:13.518309 139984133072640 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.06858953088521957, loss=0.0315290167927742
I0206 07:34:45.676977 140142484387584 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.06995158642530441, loss=0.02913135103881359
I0206 07:35:03.890264 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:36:44.628899 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:36:47.789910 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:36:50.894375 140205209478976 submission_runner.py:408] Time since start: 14571.43s, 	Step: 31458, 	{'train/accuracy': 0.9918238520622253, 'train/loss': 0.026440177112817764, 'train/mean_average_precision': 0.4987664840541888, 'validation/accuracy': 0.9869713187217712, 'validation/loss': 0.04376460611820221, 'validation/mean_average_precision': 0.2861938749291255, 'validation/num_examples': 43793, 'test/accuracy': 0.9861093759536743, 'test/loss': 0.04669422656297684, 'test/mean_average_precision': 0.2704962118586615, 'test/num_examples': 43793, 'score': 10098.319123268127, 'total_duration': 14571.425382375717, 'accumulated_submission_time': 10098.319123268127, 'accumulated_eval_time': 4470.956292152405, 'accumulated_logging_time': 1.2795672416687012}
I0206 07:36:50.916167 139963806111488 logging_writer.py:48] [31458] accumulated_eval_time=4470.956292, accumulated_logging_time=1.279567, accumulated_submission_time=10098.319123, global_step=31458, preemption_count=0, score=10098.319123, test/accuracy=0.986109, test/loss=0.046694, test/mean_average_precision=0.270496, test/num_examples=43793, total_duration=14571.425382, train/accuracy=0.991824, train/loss=0.026440, train/mean_average_precision=0.498766, validation/accuracy=0.986971, validation/loss=0.043765, validation/mean_average_precision=0.286194, validation/num_examples=43793
I0206 07:37:04.894231 140002205157120 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.05674779787659645, loss=0.02904101461172104
I0206 07:37:36.964386 139963806111488 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.06927400082349777, loss=0.028170213103294373
I0206 07:38:09.552581 140002205157120 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.07040847092866898, loss=0.029127126559615135
I0206 07:38:41.703391 139963806111488 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.06871668249368668, loss=0.02960805408656597
I0206 07:39:14.152528 140002205157120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.05992817133665085, loss=0.030858542770147324
I0206 07:39:46.755634 139963806111488 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05820417031645775, loss=0.027932830154895782
I0206 07:40:18.813012 140002205157120 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.059974610805511475, loss=0.028870753943920135
I0206 07:40:51.167174 139963806111488 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.05935577303171158, loss=0.030467478558421135
I0206 07:40:51.172418 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:42:28.446842 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:42:31.587360 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:42:34.750120 140205209478976 submission_runner.py:408] Time since start: 14915.28s, 	Step: 32201, 	{'train/accuracy': 0.9917375445365906, 'train/loss': 0.026739291846752167, 'train/mean_average_precision': 0.48715062075297716, 'validation/accuracy': 0.9870427250862122, 'validation/loss': 0.043842099606990814, 'validation/mean_average_precision': 0.2848324278871966, 'validation/num_examples': 43793, 'test/accuracy': 0.9861093759536743, 'test/loss': 0.04696670547127724, 'test/mean_average_precision': 0.26856483920492724, 'test/num_examples': 43793, 'score': 10338.541726350784, 'total_duration': 14915.281131029129, 'accumulated_submission_time': 10338.541726350784, 'accumulated_eval_time': 4574.533927679062, 'accumulated_logging_time': 1.313849925994873}
I0206 07:42:34.771701 139984133072640 logging_writer.py:48] [32201] accumulated_eval_time=4574.533928, accumulated_logging_time=1.313850, accumulated_submission_time=10338.541726, global_step=32201, preemption_count=0, score=10338.541726, test/accuracy=0.986109, test/loss=0.046967, test/mean_average_precision=0.268565, test/num_examples=43793, total_duration=14915.281131, train/accuracy=0.991738, train/loss=0.026739, train/mean_average_precision=0.487151, validation/accuracy=0.987043, validation/loss=0.043842, validation/mean_average_precision=0.284832, validation/num_examples=43793
I0206 07:43:07.142812 140142484387584 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.06402547657489777, loss=0.028436172753572464
I0206 07:43:39.372636 139984133072640 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.06004928797483444, loss=0.02879607304930687
I0206 07:44:11.699355 140142484387584 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.05377136543393135, loss=0.026189405471086502
I0206 07:44:44.111956 139984133072640 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.07725371420383453, loss=0.03050910122692585
I0206 07:45:16.331701 140142484387584 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.06663684546947479, loss=0.02644273079931736
I0206 07:45:47.925704 139984133072640 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.05933928117156029, loss=0.027303596958518028
I0206 07:46:20.200627 140142484387584 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.06772021949291229, loss=0.030726896598935127
I0206 07:46:34.783190 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:48:08.010764 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:48:11.068415 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:48:16.602019 140205209478976 submission_runner.py:408] Time since start: 15257.13s, 	Step: 32947, 	{'train/accuracy': 0.9917527437210083, 'train/loss': 0.026688046753406525, 'train/mean_average_precision': 0.49590078381363595, 'validation/accuracy': 0.9871665239334106, 'validation/loss': 0.044212136417627335, 'validation/mean_average_precision': 0.29305686914002177, 'validation/num_examples': 43793, 'test/accuracy': 0.9862593412399292, 'test/loss': 0.047123465687036514, 'test/mean_average_precision': 0.2710033452222284, 'test/num_examples': 43793, 'score': 10578.52193403244, 'total_duration': 15257.13302230835, 'accumulated_submission_time': 10578.52193403244, 'accumulated_eval_time': 4676.352701425552, 'accumulated_logging_time': 1.3461999893188477}
I0206 07:48:16.625283 139984037992192 logging_writer.py:48] [32947] accumulated_eval_time=4676.352701, accumulated_logging_time=1.346200, accumulated_submission_time=10578.521934, global_step=32947, preemption_count=0, score=10578.521934, test/accuracy=0.986259, test/loss=0.047123, test/mean_average_precision=0.271003, test/num_examples=43793, total_duration=15257.133022, train/accuracy=0.991753, train/loss=0.026688, train/mean_average_precision=0.495901, validation/accuracy=0.987167, validation/loss=0.044212, validation/mean_average_precision=0.293057, validation/num_examples=43793
I0206 07:48:33.808404 140002205157120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.07763499766588211, loss=0.024311551824212074
I0206 07:49:05.585108 139984037992192 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.06348241120576859, loss=0.029462583363056183
I0206 07:49:37.113231 140002205157120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.06927159428596497, loss=0.028138065710663795
I0206 07:50:09.116963 139984037992192 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.06044146418571472, loss=0.02989160269498825
I0206 07:50:40.797094 140002205157120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.05549387261271477, loss=0.027252191677689552
I0206 07:51:12.385529 139984037992192 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.0673515796661377, loss=0.02562667429447174
I0206 07:51:43.841971 140002205157120 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.06566087156534195, loss=0.028748031705617905
I0206 07:52:15.759087 139984037992192 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.055486805737018585, loss=0.026779409497976303
I0206 07:52:16.717663 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:53:50.974092 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:53:54.487396 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:53:57.866969 140205209478976 submission_runner.py:408] Time since start: 15598.40s, 	Step: 33704, 	{'train/accuracy': 0.9918839931488037, 'train/loss': 0.026062043383717537, 'train/mean_average_precision': 0.5066806571838988, 'validation/accuracy': 0.9870265126228333, 'validation/loss': 0.04397330805659294, 'validation/mean_average_precision': 0.28619253154944974, 'validation/num_examples': 43793, 'test/accuracy': 0.9861788749694824, 'test/loss': 0.04694005474448204, 'test/mean_average_precision': 0.27111568274384507, 'test/num_examples': 43793, 'score': 10818.582146406174, 'total_duration': 15598.397938489914, 'accumulated_submission_time': 10818.582146406174, 'accumulated_eval_time': 4777.501918077469, 'accumulated_logging_time': 1.3808777332305908}
I0206 07:53:57.892154 139984133072640 logging_writer.py:48] [33704] accumulated_eval_time=4777.501918, accumulated_logging_time=1.380878, accumulated_submission_time=10818.582146, global_step=33704, preemption_count=0, score=10818.582146, test/accuracy=0.986179, test/loss=0.046940, test/mean_average_precision=0.271116, test/num_examples=43793, total_duration=15598.397938, train/accuracy=0.991884, train/loss=0.026062, train/mean_average_precision=0.506681, validation/accuracy=0.987027, validation/loss=0.043973, validation/mean_average_precision=0.286193, validation/num_examples=43793
I0206 07:54:29.635912 140142484387584 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.08751170337200165, loss=0.029131891205906868
I0206 07:55:01.331406 139984133072640 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.06529258191585541, loss=0.0286497063934803
I0206 07:55:32.872077 140142484387584 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.06332695484161377, loss=0.02988456003367901
I0206 07:56:04.650854 139984133072640 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.0709095448255539, loss=0.03051871620118618
I0206 07:56:36.202192 140142484387584 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.06445044279098511, loss=0.026602618396282196
I0206 07:57:08.276304 139984133072640 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.07660070806741714, loss=0.03054085373878479
I0206 07:57:40.037180 140142484387584 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.06434715539216995, loss=0.032192159444093704
I0206 07:57:57.888591 140205209478976 spec.py:321] Evaluating on the training split.
I0206 07:59:32.564860 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 07:59:35.588862 140205209478976 spec.py:349] Evaluating on the test split.
I0206 07:59:38.651305 140205209478976 submission_runner.py:408] Time since start: 15939.18s, 	Step: 34457, 	{'train/accuracy': 0.991776704788208, 'train/loss': 0.02636144682765007, 'train/mean_average_precision': 0.5051048917352821, 'validation/accuracy': 0.9870297312736511, 'validation/loss': 0.044214628636837006, 'validation/mean_average_precision': 0.290110529395656, 'validation/num_examples': 43793, 'test/accuracy': 0.9861131906509399, 'test/loss': 0.047194551676511765, 'test/mean_average_precision': 0.2731183121212644, 'test/num_examples': 43793, 'score': 11058.546568632126, 'total_duration': 15939.182308912277, 'accumulated_submission_time': 11058.546568632126, 'accumulated_eval_time': 4878.264587640762, 'accumulated_logging_time': 1.417754888534546}
I0206 07:59:38.673463 139963806111488 logging_writer.py:48] [34457] accumulated_eval_time=4878.264588, accumulated_logging_time=1.417755, accumulated_submission_time=11058.546569, global_step=34457, preemption_count=0, score=11058.546569, test/accuracy=0.986113, test/loss=0.047195, test/mean_average_precision=0.273118, test/num_examples=43793, total_duration=15939.182309, train/accuracy=0.991777, train/loss=0.026361, train/mean_average_precision=0.505105, validation/accuracy=0.987030, validation/loss=0.044215, validation/mean_average_precision=0.290111, validation/num_examples=43793
I0206 07:59:52.714061 139984037992192 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.05551471933722496, loss=0.026901716366410255
I0206 08:00:24.413741 139963806111488 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.06919534504413605, loss=0.03053615242242813
I0206 08:00:56.383109 139984037992192 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.061425477266311646, loss=0.026991376653313637
I0206 08:01:28.521544 139963806111488 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.06407919526100159, loss=0.03131941333413124
I0206 08:02:00.335663 139984037992192 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.06083957850933075, loss=0.026714494451880455
I0206 08:02:32.645488 139963806111488 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.06296640634536743, loss=0.027765264734625816
I0206 08:03:04.497812 139984037992192 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06187137961387634, loss=0.023976406082510948
I0206 08:03:36.582596 139963806111488 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.0691467672586441, loss=0.029386434704065323
I0206 08:03:38.825632 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:05:16.312844 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:05:19.334731 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:05:22.356588 140205209478976 submission_runner.py:408] Time since start: 16282.89s, 	Step: 35208, 	{'train/accuracy': 0.9921960234642029, 'train/loss': 0.025136025622487068, 'train/mean_average_precision': 0.5219924353902371, 'validation/accuracy': 0.9870204329490662, 'validation/loss': 0.04393310472369194, 'validation/mean_average_precision': 0.29234085819008016, 'validation/num_examples': 43793, 'test/accuracy': 0.9862593412399292, 'test/loss': 0.0469382219016552, 'test/mean_average_precision': 0.2796964435428016, 'test/num_examples': 43793, 'score': 11298.666824102402, 'total_duration': 16282.887593269348, 'accumulated_submission_time': 11298.666824102402, 'accumulated_eval_time': 4981.795501708984, 'accumulated_logging_time': 1.4510157108306885}
I0206 08:05:22.378666 139984133072640 logging_writer.py:48] [35208] accumulated_eval_time=4981.795502, accumulated_logging_time=1.451016, accumulated_submission_time=11298.666824, global_step=35208, preemption_count=0, score=11298.666824, test/accuracy=0.986259, test/loss=0.046938, test/mean_average_precision=0.279696, test/num_examples=43793, total_duration=16282.887593, train/accuracy=0.992196, train/loss=0.025136, train/mean_average_precision=0.521992, validation/accuracy=0.987020, validation/loss=0.043933, validation/mean_average_precision=0.292341, validation/num_examples=43793
I0206 08:05:53.066317 140142484387584 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.054113809019327164, loss=0.025608984753489494
I0206 08:06:25.352791 139984133072640 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.058254484087228775, loss=0.02734486572444439
I0206 08:06:58.048900 140142484387584 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.06592227518558502, loss=0.027868865057826042
I0206 08:07:29.959805 139984133072640 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.06485945731401443, loss=0.02853851579129696
I0206 08:08:02.001787 140142484387584 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.07348854094743729, loss=0.02716374583542347
I0206 08:08:34.243295 139984133072640 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.07541027665138245, loss=0.029474589973688126
I0206 08:09:06.639374 140142484387584 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.06709030270576477, loss=0.02784348838031292
I0206 08:09:22.660978 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:10:54.434059 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:10:57.490129 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:11:00.479992 140205209478976 submission_runner.py:408] Time since start: 16621.01s, 	Step: 35951, 	{'train/accuracy': 0.9921252131462097, 'train/loss': 0.02500178851187229, 'train/mean_average_precision': 0.5476866748148286, 'validation/accuracy': 0.9870662689208984, 'validation/loss': 0.044470176100730896, 'validation/mean_average_precision': 0.2893286274359, 'validation/num_examples': 43793, 'test/accuracy': 0.9862323999404907, 'test/loss': 0.047462280839681625, 'test/mean_average_precision': 0.2696253351743681, 'test/num_examples': 43793, 'score': 11538.917538404465, 'total_duration': 16621.011003494263, 'accumulated_submission_time': 11538.917538404465, 'accumulated_eval_time': 5079.614475250244, 'accumulated_logging_time': 1.4840147495269775}
I0206 08:11:00.502485 139963806111488 logging_writer.py:48] [35951] accumulated_eval_time=5079.614475, accumulated_logging_time=1.484015, accumulated_submission_time=11538.917538, global_step=35951, preemption_count=0, score=11538.917538, test/accuracy=0.986232, test/loss=0.047462, test/mean_average_precision=0.269625, test/num_examples=43793, total_duration=16621.011003, train/accuracy=0.992125, train/loss=0.025002, train/mean_average_precision=0.547687, validation/accuracy=0.987066, validation/loss=0.044470, validation/mean_average_precision=0.289329, validation/num_examples=43793
I0206 08:11:16.522417 139984037992192 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06976856291294098, loss=0.0267386045306921
I0206 08:11:48.477529 139963806111488 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.07334880530834198, loss=0.02758021466434002
I0206 08:12:20.573008 139984037992192 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.0733976885676384, loss=0.02916828542947769
I0206 08:12:52.847367 139963806111488 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.06067125126719475, loss=0.025429625064134598
I0206 08:13:24.983861 139984037992192 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.06961791217327118, loss=0.027413727715611458
I0206 08:13:56.858438 139963806111488 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.06291838735342026, loss=0.023620830848813057
I0206 08:14:28.852903 139984037992192 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.0676373615860939, loss=0.027406366541981697
I0206 08:15:00.402828 139963806111488 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.06407026201486588, loss=0.025893723592162132
I0206 08:15:00.721642 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:16:35.827230 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:16:38.833482 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:16:41.797046 140205209478976 submission_runner.py:408] Time since start: 16962.33s, 	Step: 36702, 	{'train/accuracy': 0.9924894571304321, 'train/loss': 0.024061957374215126, 'train/mean_average_precision': 0.5486724927922778, 'validation/accuracy': 0.987052857875824, 'validation/loss': 0.04396612569689751, 'validation/mean_average_precision': 0.29082902684215756, 'validation/num_examples': 43793, 'test/accuracy': 0.9861683249473572, 'test/loss': 0.04701868072152138, 'test/mean_average_precision': 0.2739539266716618, 'test/num_examples': 43793, 'score': 11779.103993415833, 'total_duration': 16962.328053236008, 'accumulated_submission_time': 11779.103993415833, 'accumulated_eval_time': 5180.689831018448, 'accumulated_logging_time': 1.5187046527862549}
I0206 08:16:41.820992 139984133072640 logging_writer.py:48] [36702] accumulated_eval_time=5180.689831, accumulated_logging_time=1.518705, accumulated_submission_time=11779.103993, global_step=36702, preemption_count=0, score=11779.103993, test/accuracy=0.986168, test/loss=0.047019, test/mean_average_precision=0.273954, test/num_examples=43793, total_duration=16962.328053, train/accuracy=0.992489, train/loss=0.024062, train/mean_average_precision=0.548672, validation/accuracy=0.987053, validation/loss=0.043966, validation/mean_average_precision=0.290829, validation/num_examples=43793
I0206 08:17:13.261144 140142484387584 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.0665934607386589, loss=0.02632022090256214
I0206 08:17:45.423536 139984133072640 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06942930072546005, loss=0.026137882843613625
I0206 08:18:17.710318 140142484387584 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.060128677636384964, loss=0.026001716032624245
I0206 08:18:49.802499 139984133072640 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.06277628988027573, loss=0.02675790712237358
I0206 08:19:21.845406 140142484387584 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.07161296159029007, loss=0.030231984332203865
I0206 08:19:54.162175 139984133072640 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06749673187732697, loss=0.028728174045681953
I0206 08:20:26.168382 140142484387584 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06436503678560257, loss=0.026870442554354668
I0206 08:20:41.843026 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:22:19.117074 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:22:22.159265 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:22:25.135402 140205209478976 submission_runner.py:408] Time since start: 17305.67s, 	Step: 37449, 	{'train/accuracy': 0.9924684762954712, 'train/loss': 0.02407103404402733, 'train/mean_average_precision': 0.5645409408952597, 'validation/accuracy': 0.9871202707290649, 'validation/loss': 0.044161438941955566, 'validation/mean_average_precision': 0.2904596654457011, 'validation/num_examples': 43793, 'test/accuracy': 0.9862315058708191, 'test/loss': 0.0470658540725708, 'test/mean_average_precision': 0.27402600030755137, 'test/num_examples': 43793, 'score': 12019.094329595566, 'total_duration': 17305.666412353516, 'accumulated_submission_time': 12019.094329595566, 'accumulated_eval_time': 5283.982161521912, 'accumulated_logging_time': 1.553828477859497}
I0206 08:22:25.157808 139963806111488 logging_writer.py:48] [37449] accumulated_eval_time=5283.982162, accumulated_logging_time=1.553828, accumulated_submission_time=12019.094330, global_step=37449, preemption_count=0, score=12019.094330, test/accuracy=0.986232, test/loss=0.047066, test/mean_average_precision=0.274026, test/num_examples=43793, total_duration=17305.666412, train/accuracy=0.992468, train/loss=0.024071, train/mean_average_precision=0.564541, validation/accuracy=0.987120, validation/loss=0.044161, validation/mean_average_precision=0.290460, validation/num_examples=43793
I0206 08:22:41.976915 140002205157120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06794817745685577, loss=0.02911127172410488
I0206 08:23:14.121492 139963806111488 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.06602361798286438, loss=0.03030788153409958
I0206 08:23:46.386106 140002205157120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.0571514256298542, loss=0.025282366201281548
I0206 08:24:18.324198 139963806111488 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.06966949999332428, loss=0.025926195085048676
I0206 08:24:49.985784 140002205157120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.07566925138235092, loss=0.025805383920669556
I0206 08:25:22.089493 139963806111488 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.07166633009910583, loss=0.02566426806151867
I0206 08:25:54.542365 140002205157120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.06816580146551132, loss=0.02519955299794674
I0206 08:26:25.175488 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:27:59.831048 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:28:03.011188 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:28:05.986778 140205209478976 submission_runner.py:408] Time since start: 17646.52s, 	Step: 38196, 	{'train/accuracy': 0.9922829866409302, 'train/loss': 0.024626431986689568, 'train/mean_average_precision': 0.533122242307951, 'validation/accuracy': 0.9870577454566956, 'validation/loss': 0.044651780277490616, 'validation/mean_average_precision': 0.2929242246556005, 'validation/num_examples': 43793, 'test/accuracy': 0.9862298369407654, 'test/loss': 0.0478082112967968, 'test/mean_average_precision': 0.2720654290337988, 'test/num_examples': 43793, 'score': 12259.080934286118, 'total_duration': 17646.517784833908, 'accumulated_submission_time': 12259.080934286118, 'accumulated_eval_time': 5384.793403148651, 'accumulated_logging_time': 1.5871310234069824}
I0206 08:28:06.011875 139984037992192 logging_writer.py:48] [38196] accumulated_eval_time=5384.793403, accumulated_logging_time=1.587131, accumulated_submission_time=12259.080934, global_step=38196, preemption_count=0, score=12259.080934, test/accuracy=0.986230, test/loss=0.047808, test/mean_average_precision=0.272065, test/num_examples=43793, total_duration=17646.517785, train/accuracy=0.992283, train/loss=0.024626, train/mean_average_precision=0.533122, validation/accuracy=0.987058, validation/loss=0.044652, validation/mean_average_precision=0.292924, validation/num_examples=43793
I0206 08:28:07.683738 139984133072640 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.06580878794193268, loss=0.030931813642382622
I0206 08:28:39.470727 139984037992192 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07008389383554459, loss=0.02525116316974163
I0206 08:29:11.533949 139984133072640 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06310825049877167, loss=0.026246801018714905
I0206 08:29:43.201494 139984037992192 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.06793759763240814, loss=0.028901437297463417
I0206 08:30:14.946216 139984133072640 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.08215666562318802, loss=0.02567967213690281
I0206 08:30:46.465090 139984037992192 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.07488024979829788, loss=0.026290837675333023
I0206 08:31:18.075870 139984133072640 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.08361193537712097, loss=0.026672611013054848
I0206 08:31:49.918384 139984037992192 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06785418838262558, loss=0.027449823915958405
I0206 08:32:06.262295 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:33:40.611517 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:33:43.652147 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:33:46.612718 140205209478976 submission_runner.py:408] Time since start: 17987.14s, 	Step: 38952, 	{'train/accuracy': 0.9922152757644653, 'train/loss': 0.024960581213235855, 'train/mean_average_precision': 0.5255462330459101, 'validation/accuracy': 0.986975371837616, 'validation/loss': 0.044149454683065414, 'validation/mean_average_precision': 0.2922564708468797, 'validation/num_examples': 43793, 'test/accuracy': 0.9861696362495422, 'test/loss': 0.04713044315576553, 'test/mean_average_precision': 0.27499161874293837, 'test/num_examples': 43793, 'score': 12499.300460100174, 'total_duration': 17987.14372611046, 'accumulated_submission_time': 12499.300460100174, 'accumulated_eval_time': 5485.143780708313, 'accumulated_logging_time': 1.6230242252349854}
I0206 08:33:46.635732 139963806111488 logging_writer.py:48] [38952] accumulated_eval_time=5485.143781, accumulated_logging_time=1.623024, accumulated_submission_time=12499.300460, global_step=38952, preemption_count=0, score=12499.300460, test/accuracy=0.986170, test/loss=0.047130, test/mean_average_precision=0.274992, test/num_examples=43793, total_duration=17987.143726, train/accuracy=0.992215, train/loss=0.024961, train/mean_average_precision=0.525546, validation/accuracy=0.986975, validation/loss=0.044149, validation/mean_average_precision=0.292256, validation/num_examples=43793
I0206 08:34:02.221310 140002205157120 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.0927802249789238, loss=0.031661149114370346
I0206 08:34:33.929131 139963806111488 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.0662262961268425, loss=0.02853449247777462
I0206 08:35:05.807179 140002205157120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.06376096606254578, loss=0.0242864228785038
I0206 08:35:37.116591 139963806111488 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.07117394357919693, loss=0.02575446479022503
I0206 08:36:08.847133 140002205157120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.06406283378601074, loss=0.026752669364213943
I0206 08:36:40.273525 139963806111488 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.06881362199783325, loss=0.02869688905775547
I0206 08:37:11.976678 140002205157120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.09631580859422684, loss=0.02757180482149124
I0206 08:37:43.801172 139963806111488 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.07352910190820694, loss=0.02741660736501217
I0206 08:37:46.646764 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:39:17.895427 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:39:20.886962 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:39:23.851540 140205209478976 submission_runner.py:408] Time since start: 18324.38s, 	Step: 39710, 	{'train/accuracy': 0.9921615123748779, 'train/loss': 0.02508951537311077, 'train/mean_average_precision': 0.5311539849439371, 'validation/accuracy': 0.9870626330375671, 'validation/loss': 0.044394511729478836, 'validation/mean_average_precision': 0.29393925036085516, 'validation/num_examples': 43793, 'test/accuracy': 0.9861927628517151, 'test/loss': 0.047573503106832504, 'test/mean_average_precision': 0.2701267080078491, 'test/num_examples': 43793, 'score': 12739.280121326447, 'total_duration': 18324.382556915283, 'accumulated_submission_time': 12739.280121326447, 'accumulated_eval_time': 5582.348515987396, 'accumulated_logging_time': 1.6569876670837402}
I0206 08:39:23.874239 139984133072640 logging_writer.py:48] [39710] accumulated_eval_time=5582.348516, accumulated_logging_time=1.656988, accumulated_submission_time=12739.280121, global_step=39710, preemption_count=0, score=12739.280121, test/accuracy=0.986193, test/loss=0.047574, test/mean_average_precision=0.270127, test/num_examples=43793, total_duration=18324.382557, train/accuracy=0.992162, train/loss=0.025090, train/mean_average_precision=0.531154, validation/accuracy=0.987063, validation/loss=0.044395, validation/mean_average_precision=0.293939, validation/num_examples=43793
I0206 08:39:52.877892 140142484387584 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.07274481654167175, loss=0.026179900392889977
I0206 08:40:24.633119 139984133072640 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.06742743402719498, loss=0.028329642489552498
I0206 08:40:56.186914 140142484387584 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.07186345010995865, loss=0.027987802401185036
I0206 08:41:27.938010 139984133072640 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.06089244410395622, loss=0.026640834286808968
I0206 08:41:59.593214 140142484387584 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.07147801667451859, loss=0.0262870192527771
I0206 08:42:31.005223 139984133072640 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.07890315353870392, loss=0.029815111309289932
I0206 08:43:02.569063 140142484387584 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.07438686490058899, loss=0.026854317635297775
I0206 08:43:24.001156 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:44:56.713219 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:44:59.727998 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:45:02.810636 140205209478976 submission_runner.py:408] Time since start: 18663.34s, 	Step: 40468, 	{'train/accuracy': 0.9923699498176575, 'train/loss': 0.024582475423812866, 'train/mean_average_precision': 0.5457115756354987, 'validation/accuracy': 0.9870626330375671, 'validation/loss': 0.04397698864340782, 'validation/mean_average_precision': 0.29008335261060875, 'validation/num_examples': 43793, 'test/accuracy': 0.9862176179885864, 'test/loss': 0.04706680774688721, 'test/mean_average_precision': 0.2757043997765046, 'test/num_examples': 43793, 'score': 12979.375619888306, 'total_duration': 18663.341647863388, 'accumulated_submission_time': 12979.375619888306, 'accumulated_eval_time': 5681.157953500748, 'accumulated_logging_time': 1.6909382343292236}
I0206 08:45:02.834708 139963806111488 logging_writer.py:48] [40468] accumulated_eval_time=5681.157954, accumulated_logging_time=1.690938, accumulated_submission_time=12979.375620, global_step=40468, preemption_count=0, score=12979.375620, test/accuracy=0.986218, test/loss=0.047067, test/mean_average_precision=0.275704, test/num_examples=43793, total_duration=18663.341648, train/accuracy=0.992370, train/loss=0.024582, train/mean_average_precision=0.545712, validation/accuracy=0.987063, validation/loss=0.043977, validation/mean_average_precision=0.290083, validation/num_examples=43793
I0206 08:45:13.258874 139984037992192 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.06691908091306686, loss=0.026064002886414528
I0206 08:45:45.090084 139963806111488 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.06598653644323349, loss=0.02695336379110813
I0206 08:46:17.398316 139984037992192 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.07816751301288605, loss=0.030013862997293472
I0206 08:46:49.338352 139963806111488 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.08280641585588455, loss=0.026569413021206856
I0206 08:47:21.726703 139984037992192 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.06750577688217163, loss=0.022694088518619537
I0206 08:47:53.828610 139963806111488 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.07962176203727722, loss=0.03186960518360138
I0206 08:48:26.015460 139984037992192 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.0766826719045639, loss=0.02852371335029602
I0206 08:48:58.174320 139963806111488 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.0698375403881073, loss=0.028388570994138718
I0206 08:49:03.044210 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:50:48.301749 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:50:51.777834 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:50:55.168593 140205209478976 submission_runner.py:408] Time since start: 19015.70s, 	Step: 41216, 	{'train/accuracy': 0.9924188256263733, 'train/loss': 0.02421451173722744, 'train/mean_average_precision': 0.5426725337847191, 'validation/accuracy': 0.9870244860649109, 'validation/loss': 0.044272493571043015, 'validation/mean_average_precision': 0.290808543338558, 'validation/num_examples': 43793, 'test/accuracy': 0.9861923456192017, 'test/loss': 0.04715505987405777, 'test/mean_average_precision': 0.2781001258652558, 'test/num_examples': 43793, 'score': 13219.553693056107, 'total_duration': 19015.69958639145, 'accumulated_submission_time': 13219.553693056107, 'accumulated_eval_time': 5793.282270669937, 'accumulated_logging_time': 1.726207971572876}
I0206 08:50:55.216642 139984133072640 logging_writer.py:48] [41216] accumulated_eval_time=5793.282271, accumulated_logging_time=1.726208, accumulated_submission_time=13219.553693, global_step=41216, preemption_count=0, score=13219.553693, test/accuracy=0.986192, test/loss=0.047155, test/mean_average_precision=0.278100, test/num_examples=43793, total_duration=19015.699586, train/accuracy=0.992419, train/loss=0.024215, train/mean_average_precision=0.542673, validation/accuracy=0.987024, validation/loss=0.044272, validation/mean_average_precision=0.290809, validation/num_examples=43793
I0206 08:51:23.146517 140002205157120 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.07913593202829361, loss=0.02625981532037258
I0206 08:51:55.630134 139984133072640 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.09211016446352005, loss=0.02939104288816452
I0206 08:52:28.128695 140002205157120 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.08351309597492218, loss=0.027997402474284172
I0206 08:52:59.936599 139984133072640 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.07588852196931839, loss=0.025529025122523308
I0206 08:53:31.846004 140002205157120 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.07047262787818909, loss=0.027567323297262192
I0206 08:54:03.747351 139984133072640 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.0787934958934784, loss=0.02754390984773636
I0206 08:54:35.858083 140002205157120 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.07576817274093628, loss=0.026424406096339226
I0206 08:54:55.358257 140205209478976 spec.py:321] Evaluating on the training split.
I0206 08:56:30.885445 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 08:56:33.980223 140205209478976 spec.py:349] Evaluating on the test split.
I0206 08:56:37.013622 140205209478976 submission_runner.py:408] Time since start: 19357.54s, 	Step: 41962, 	{'train/accuracy': 0.9924597144126892, 'train/loss': 0.023983314633369446, 'train/mean_average_precision': 0.5485732064088567, 'validation/accuracy': 0.9870756268501282, 'validation/loss': 0.0444074310362339, 'validation/mean_average_precision': 0.29814886448259775, 'validation/num_examples': 43793, 'test/accuracy': 0.9861965775489807, 'test/loss': 0.04757916182279587, 'test/mean_average_precision': 0.2741416501901633, 'test/num_examples': 43793, 'score': 13459.66164469719, 'total_duration': 19357.54460954666, 'accumulated_submission_time': 13459.66164469719, 'accumulated_eval_time': 5894.937569618225, 'accumulated_logging_time': 1.7861547470092773}
I0206 08:56:37.036980 139963806111488 logging_writer.py:48] [41962] accumulated_eval_time=5894.937570, accumulated_logging_time=1.786155, accumulated_submission_time=13459.661645, global_step=41962, preemption_count=0, score=13459.661645, test/accuracy=0.986197, test/loss=0.047579, test/mean_average_precision=0.274142, test/num_examples=43793, total_duration=19357.544610, train/accuracy=0.992460, train/loss=0.023983, train/mean_average_precision=0.548573, validation/accuracy=0.987076, validation/loss=0.044407, validation/mean_average_precision=0.298149, validation/num_examples=43793
I0206 08:56:49.559947 140142484387584 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.06353390961885452, loss=0.0258363988250494
I0206 08:57:21.814810 139963806111488 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.0788414478302002, loss=0.02779882401227951
I0206 08:57:53.858556 140142484387584 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07428237050771713, loss=0.02791261114180088
I0206 08:58:25.745905 139963806111488 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07509797811508179, loss=0.02664405107498169
I0206 08:58:57.867278 140142484387584 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.07509031891822815, loss=0.027887433767318726
I0206 08:59:29.989573 139963806111488 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.07330108433961868, loss=0.02818506769835949
I0206 09:00:02.228875 140142484387584 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.06582552939653397, loss=0.02650366723537445
I0206 09:00:34.109918 139963806111488 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.08686165511608124, loss=0.026214418932795525
I0206 09:00:37.263170 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:02:14.586909 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:02:17.665687 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:02:20.657708 140205209478976 submission_runner.py:408] Time since start: 19701.19s, 	Step: 42711, 	{'train/accuracy': 0.9925682544708252, 'train/loss': 0.02357172966003418, 'train/mean_average_precision': 0.5703393827897597, 'validation/accuracy': 0.9870427250862122, 'validation/loss': 0.04432450234889984, 'validation/mean_average_precision': 0.2951427843127839, 'validation/num_examples': 43793, 'test/accuracy': 0.9863145351409912, 'test/loss': 0.04708156734704971, 'test/mean_average_precision': 0.2796231683478654, 'test/num_examples': 43793, 'score': 13699.857156276703, 'total_duration': 19701.188717842102, 'accumulated_submission_time': 13699.857156276703, 'accumulated_eval_time': 5998.332057952881, 'accumulated_logging_time': 1.8204026222229004}
I0206 09:02:20.680351 139984037992192 logging_writer.py:48] [42711] accumulated_eval_time=5998.332058, accumulated_logging_time=1.820403, accumulated_submission_time=13699.857156, global_step=42711, preemption_count=0, score=13699.857156, test/accuracy=0.986315, test/loss=0.047082, test/mean_average_precision=0.279623, test/num_examples=43793, total_duration=19701.188718, train/accuracy=0.992568, train/loss=0.023572, train/mean_average_precision=0.570339, validation/accuracy=0.987043, validation/loss=0.044325, validation/mean_average_precision=0.295143, validation/num_examples=43793
I0206 09:02:49.544237 140002205157120 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.06935647875070572, loss=0.022428957745432854
I0206 09:03:21.565938 139984037992192 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.08658064901828766, loss=0.02758740447461605
I0206 09:03:53.693025 140002205157120 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.06745555996894836, loss=0.024797214195132256
I0206 09:04:25.520867 139984037992192 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.08139758557081223, loss=0.028676621615886688
I0206 09:04:57.539669 140002205157120 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.07601151615381241, loss=0.026472531259059906
I0206 09:05:28.955433 139984037992192 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.07280080020427704, loss=0.024294696748256683
I0206 09:06:00.903716 140002205157120 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.07227451354265213, loss=0.027315838262438774
I0206 09:06:20.758907 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:07:55.649036 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:07:58.645387 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:08:01.616079 140205209478976 submission_runner.py:408] Time since start: 20042.15s, 	Step: 43463, 	{'train/accuracy': 0.9928522109985352, 'train/loss': 0.022685980424284935, 'train/mean_average_precision': 0.587405296771667, 'validation/accuracy': 0.987025260925293, 'validation/loss': 0.04483186826109886, 'validation/mean_average_precision': 0.29264353230880247, 'validation/num_examples': 43793, 'test/accuracy': 0.986214280128479, 'test/loss': 0.04800967872142792, 'test/mean_average_precision': 0.2711062177589242, 'test/num_examples': 43793, 'score': 13939.90434885025, 'total_duration': 20042.147089719772, 'accumulated_submission_time': 13939.90434885025, 'accumulated_eval_time': 6099.189184188843, 'accumulated_logging_time': 1.8538603782653809}
I0206 09:08:01.639226 139963806111488 logging_writer.py:48] [43463] accumulated_eval_time=6099.189184, accumulated_logging_time=1.853860, accumulated_submission_time=13939.904349, global_step=43463, preemption_count=0, score=13939.904349, test/accuracy=0.986214, test/loss=0.048010, test/mean_average_precision=0.271106, test/num_examples=43793, total_duration=20042.147090, train/accuracy=0.992852, train/loss=0.022686, train/mean_average_precision=0.587405, validation/accuracy=0.987025, validation/loss=0.044832, validation/mean_average_precision=0.292644, validation/num_examples=43793
I0206 09:08:13.721787 140142484387584 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.07497701793909073, loss=0.02467847615480423
I0206 09:08:45.820347 139963806111488 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.07879366725683212, loss=0.029805632308125496
I0206 09:09:17.634957 140142484387584 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.07191936671733856, loss=0.02673936076462269
I0206 09:09:49.518732 139963806111488 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.07896848767995834, loss=0.025939134880900383
I0206 09:10:21.230742 140142484387584 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.08844228833913803, loss=0.027509864419698715
I0206 09:10:52.751675 139963806111488 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.07632587105035782, loss=0.025707796216011047
I0206 09:11:24.286233 140142484387584 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.08802839368581772, loss=0.026451351121068
I0206 09:11:55.869125 139963806111488 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.08867643773555756, loss=0.028045333921909332
I0206 09:12:01.790247 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:13:37.346622 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:13:40.432929 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:13:46.016074 140205209478976 submission_runner.py:408] Time since start: 20386.55s, 	Step: 44220, 	{'train/accuracy': 0.9930338859558105, 'train/loss': 0.022224726155400276, 'train/mean_average_precision': 0.6030117376077448, 'validation/accuracy': 0.9871328473091125, 'validation/loss': 0.04474617540836334, 'validation/mean_average_precision': 0.2994731789222761, 'validation/num_examples': 43793, 'test/accuracy': 0.9862829446792603, 'test/loss': 0.0478932149708271, 'test/mean_average_precision': 0.27737578476760955, 'test/num_examples': 43793, 'score': 14180.02293419838, 'total_duration': 20386.546948194504, 'accumulated_submission_time': 14180.02293419838, 'accumulated_eval_time': 6203.414827346802, 'accumulated_logging_time': 1.8894824981689453}
I0206 09:13:46.041740 139984037992192 logging_writer.py:48] [44220] accumulated_eval_time=6203.414827, accumulated_logging_time=1.889482, accumulated_submission_time=14180.022934, global_step=44220, preemption_count=0, score=14180.022934, test/accuracy=0.986283, test/loss=0.047893, test/mean_average_precision=0.277376, test/num_examples=43793, total_duration=20386.546948, train/accuracy=0.993034, train/loss=0.022225, train/mean_average_precision=0.603012, validation/accuracy=0.987133, validation/loss=0.044746, validation/mean_average_precision=0.299473, validation/num_examples=43793
I0206 09:14:12.207819 139984133072640 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.08614086359739304, loss=0.027977678924798965
I0206 09:14:43.531755 139984037992192 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.0728510394692421, loss=0.023583361878991127
I0206 09:15:15.590295 139984133072640 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.08160515129566193, loss=0.028883693739771843
I0206 09:15:47.905893 139984037992192 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.0856613963842392, loss=0.025657078251242638
I0206 09:16:20.029126 139984133072640 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.08011605590581894, loss=0.0244564451277256
I0206 09:16:51.610735 139984037992192 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.08595193177461624, loss=0.028803586959838867
I0206 09:17:23.569589 139984133072640 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.09775485098361969, loss=0.02715173363685608
I0206 09:17:46.154243 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:19:18.575301 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:19:21.677459 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:19:24.726724 140205209478976 submission_runner.py:408] Time since start: 20725.26s, 	Step: 44972, 	{'train/accuracy': 0.9930641651153564, 'train/loss': 0.02208307757973671, 'train/mean_average_precision': 0.6060911317596043, 'validation/accuracy': 0.987050473690033, 'validation/loss': 0.04487309977412224, 'validation/mean_average_precision': 0.2943794044568053, 'validation/num_examples': 43793, 'test/accuracy': 0.9862542748451233, 'test/loss': 0.04808291420340538, 'test/mean_average_precision': 0.27775466673846316, 'test/num_examples': 43793, 'score': 14420.104619264603, 'total_duration': 20725.25773501396, 'accumulated_submission_time': 14420.104619264603, 'accumulated_eval_time': 6301.98726439476, 'accumulated_logging_time': 1.9259374141693115}
I0206 09:19:24.750964 139963806111488 logging_writer.py:48] [44972] accumulated_eval_time=6301.987264, accumulated_logging_time=1.925937, accumulated_submission_time=14420.104619, global_step=44972, preemption_count=0, score=14420.104619, test/accuracy=0.986254, test/loss=0.048083, test/mean_average_precision=0.277755, test/num_examples=43793, total_duration=20725.257735, train/accuracy=0.993064, train/loss=0.022083, train/mean_average_precision=0.606091, validation/accuracy=0.987050, validation/loss=0.044873, validation/mean_average_precision=0.294379, validation/num_examples=43793
I0206 09:19:34.047664 140142484387584 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.07478111982345581, loss=0.02391950599849224
I0206 09:20:06.129650 139963806111488 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.07015984505414963, loss=0.024530209600925446
I0206 09:20:38.130552 140142484387584 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08276481926441193, loss=0.026516461744904518
I0206 09:21:10.165526 139963806111488 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.07460879534482956, loss=0.024107404053211212
I0206 09:21:41.801783 140142484387584 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.0894499272108078, loss=0.026837371289730072
I0206 09:22:13.914012 139963806111488 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.07935728877782822, loss=0.024759769439697266
I0206 09:22:45.583398 140142484387584 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.09491152316331863, loss=0.03187057748436928
I0206 09:23:17.527635 139963806111488 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.09253866225481033, loss=0.0243435837328434
I0206 09:23:24.821366 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:25:01.421334 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:25:04.592751 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:25:07.674785 140205209478976 submission_runner.py:408] Time since start: 21068.21s, 	Step: 45724, 	{'train/accuracy': 0.993030309677124, 'train/loss': 0.02232307381927967, 'train/mean_average_precision': 0.5868794150076238, 'validation/accuracy': 0.9870455861091614, 'validation/loss': 0.04456338658928871, 'validation/mean_average_precision': 0.2950067238532902, 'validation/num_examples': 43793, 'test/accuracy': 0.9862766265869141, 'test/loss': 0.04733329638838768, 'test/mean_average_precision': 0.2744227430076576, 'test/num_examples': 43793, 'score': 14660.14389538765, 'total_duration': 21068.20579099655, 'accumulated_submission_time': 14660.14389538765, 'accumulated_eval_time': 6404.840629816055, 'accumulated_logging_time': 1.961230993270874}
I0206 09:25:07.699989 139984133072640 logging_writer.py:48] [45724] accumulated_eval_time=6404.840630, accumulated_logging_time=1.961231, accumulated_submission_time=14660.143895, global_step=45724, preemption_count=0, score=14660.143895, test/accuracy=0.986277, test/loss=0.047333, test/mean_average_precision=0.274423, test/num_examples=43793, total_duration=21068.205791, train/accuracy=0.993030, train/loss=0.022323, train/mean_average_precision=0.586879, validation/accuracy=0.987046, validation/loss=0.044563, validation/mean_average_precision=0.295007, validation/num_examples=43793
I0206 09:25:32.412136 140002205157120 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.07661416381597519, loss=0.024562546983361244
I0206 09:26:04.644715 139984133072640 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.09150751680135727, loss=0.025956831872463226
I0206 09:26:36.437609 140002205157120 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.10031398385763168, loss=0.02676309272646904
I0206 09:27:08.556040 139984133072640 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.0750478059053421, loss=0.02471158653497696
I0206 09:27:40.378551 140002205157120 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.08047188073396683, loss=0.02499384805560112
I0206 09:28:12.421860 139984133072640 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.08632193505764008, loss=0.025064248591661453
I0206 09:28:44.928860 140002205157120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.07849441468715668, loss=0.024710414931178093
I0206 09:29:07.683452 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:30:45.883229 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:30:48.896886 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:30:51.915121 140205209478976 submission_runner.py:408] Time since start: 21412.45s, 	Step: 46471, 	{'train/accuracy': 0.9928067922592163, 'train/loss': 0.02292061410844326, 'train/mean_average_precision': 0.5767878589178458, 'validation/accuracy': 0.9869668483734131, 'validation/loss': 0.04495652765035629, 'validation/mean_average_precision': 0.2892812244489832, 'validation/num_examples': 43793, 'test/accuracy': 0.9861570000648499, 'test/loss': 0.0479879155755043, 'test/mean_average_precision': 0.27575870220349497, 'test/num_examples': 43793, 'score': 14900.095911979675, 'total_duration': 21412.446129083633, 'accumulated_submission_time': 14900.095911979675, 'accumulated_eval_time': 6509.072257757187, 'accumulated_logging_time': 1.9975545406341553}
I0206 09:30:51.938967 139963806111488 logging_writer.py:48] [46471] accumulated_eval_time=6509.072258, accumulated_logging_time=1.997555, accumulated_submission_time=14900.095912, global_step=46471, preemption_count=0, score=14900.095912, test/accuracy=0.986157, test/loss=0.047988, test/mean_average_precision=0.275759, test/num_examples=43793, total_duration=21412.446129, train/accuracy=0.992807, train/loss=0.022921, train/mean_average_precision=0.576788, validation/accuracy=0.986967, validation/loss=0.044957, validation/mean_average_precision=0.289281, validation/num_examples=43793
I0206 09:31:01.826021 139984037992192 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.08926396816968918, loss=0.026050910353660583
I0206 09:31:33.589392 139963806111488 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08557400852441788, loss=0.02611391246318817
I0206 09:32:05.557294 139984037992192 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.08019742369651794, loss=0.024366233497858047
I0206 09:32:36.975285 139963806111488 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.09215429425239563, loss=0.025061527267098427
I0206 09:33:08.679384 139984037992192 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.07430506497621536, loss=0.02651839330792427
I0206 09:33:40.736949 139963806111488 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.08732108771800995, loss=0.0270600076764822
I0206 09:34:12.946253 139984037992192 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.07877534627914429, loss=0.024650275707244873
I0206 09:34:45.081320 139963806111488 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.1020936518907547, loss=0.026007259264588356
I0206 09:34:52.136457 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:36:29.677643 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:36:32.681076 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:36:35.737819 140205209478976 submission_runner.py:408] Time since start: 21756.27s, 	Step: 47223, 	{'train/accuracy': 0.9927178621292114, 'train/loss': 0.02314264327287674, 'train/mean_average_precision': 0.5750837991380785, 'validation/accuracy': 0.9870151281356812, 'validation/loss': 0.0452580600976944, 'validation/mean_average_precision': 0.28985987010866954, 'validation/num_examples': 43793, 'test/accuracy': 0.9861999154090881, 'test/loss': 0.04842248931527138, 'test/mean_average_precision': 0.26999446926627463, 'test/num_examples': 43793, 'score': 15140.262208938599, 'total_duration': 21756.26882982254, 'accumulated_submission_time': 15140.262208938599, 'accumulated_eval_time': 6612.673577547073, 'accumulated_logging_time': 2.0322277545928955}
I0206 09:36:35.761887 140002205157120 logging_writer.py:48] [47223] accumulated_eval_time=6612.673578, accumulated_logging_time=2.032228, accumulated_submission_time=15140.262209, global_step=47223, preemption_count=0, score=15140.262209, test/accuracy=0.986200, test/loss=0.048422, test/mean_average_precision=0.269994, test/num_examples=43793, total_duration=21756.268830, train/accuracy=0.992718, train/loss=0.023143, train/mean_average_precision=0.575084, validation/accuracy=0.987015, validation/loss=0.045258, validation/mean_average_precision=0.289860, validation/num_examples=43793
I0206 09:37:01.047936 140142484387584 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.09752701967954636, loss=0.025840073823928833
I0206 09:37:33.030458 140002205157120 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.07189098000526428, loss=0.02090323530137539
I0206 09:38:05.136192 140142484387584 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.07822925597429276, loss=0.02424955740571022
I0206 09:38:37.464701 140002205157120 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.08381035178899765, loss=0.023491717875003815
I0206 09:39:09.683796 140142484387584 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.09073419868946075, loss=0.02682526782155037
I0206 09:39:43.752866 140002205157120 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.0976141020655632, loss=0.027236297726631165
I0206 09:40:15.850316 140142484387584 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.0859806165099144, loss=0.024779247120022774
I0206 09:40:36.010512 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:42:09.360934 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:42:12.407899 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:42:17.981913 140205209478976 submission_runner.py:408] Time since start: 22098.51s, 	Step: 47963, 	{'train/accuracy': 0.9928799867630005, 'train/loss': 0.02256070077419281, 'train/mean_average_precision': 0.5870601530022526, 'validation/accuracy': 0.9871381521224976, 'validation/loss': 0.04508497565984726, 'validation/mean_average_precision': 0.2975629992554879, 'validation/num_examples': 43793, 'test/accuracy': 0.9862197637557983, 'test/loss': 0.04828479886054993, 'test/mean_average_precision': 0.27362271948857514, 'test/num_examples': 43793, 'score': 15380.479937553406, 'total_duration': 22098.51292657852, 'accumulated_submission_time': 15380.479937553406, 'accumulated_eval_time': 6714.644959449768, 'accumulated_logging_time': 2.0670578479766846}
I0206 09:42:18.006755 139963806111488 logging_writer.py:48] [47963] accumulated_eval_time=6714.644959, accumulated_logging_time=2.067058, accumulated_submission_time=15380.479938, global_step=47963, preemption_count=0, score=15380.479938, test/accuracy=0.986220, test/loss=0.048285, test/mean_average_precision=0.273623, test/num_examples=43793, total_duration=22098.512927, train/accuracy=0.992880, train/loss=0.022561, train/mean_average_precision=0.587060, validation/accuracy=0.987138, validation/loss=0.045085, validation/mean_average_precision=0.297563, validation/num_examples=43793
I0206 09:42:30.223892 139984037992192 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.08836604654788971, loss=0.02522917278110981
I0206 09:43:02.626117 139963806111488 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.07746484130620956, loss=0.02337372489273548
I0206 09:43:35.091289 139984037992192 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.09151073545217514, loss=0.02465798333287239
I0206 09:44:07.579905 139963806111488 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.10388299822807312, loss=0.02711157500743866
I0206 09:44:39.429013 139984037992192 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.08086487650871277, loss=0.02312372997403145
I0206 09:45:11.971952 139963806111488 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.09434500336647034, loss=0.02508712373673916
I0206 09:45:44.408351 139984037992192 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.09078235924243927, loss=0.02475329302251339
I0206 09:46:16.886451 139963806111488 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.0841694325208664, loss=0.026520702987909317
I0206 09:46:18.227367 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:47:56.658745 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:47:59.756179 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:48:02.912984 140205209478976 submission_runner.py:408] Time since start: 22443.44s, 	Step: 48705, 	{'train/accuracy': 0.992908239364624, 'train/loss': 0.022484946995973587, 'train/mean_average_precision': 0.5866123590096386, 'validation/accuracy': 0.9870979189872742, 'validation/loss': 0.04502173885703087, 'validation/mean_average_precision': 0.2982286863077764, 'validation/num_examples': 43793, 'test/accuracy': 0.9861965775489807, 'test/loss': 0.048440057784318924, 'test/mean_average_precision': 0.26824022040142503, 'test/num_examples': 43793, 'score': 15620.667268753052, 'total_duration': 22443.443969726562, 'accumulated_submission_time': 15620.667268753052, 'accumulated_eval_time': 6819.330502986908, 'accumulated_logging_time': 2.104949951171875}
I0206 09:48:02.938872 140002205157120 logging_writer.py:48] [48705] accumulated_eval_time=6819.330503, accumulated_logging_time=2.104950, accumulated_submission_time=15620.667269, global_step=48705, preemption_count=0, score=15620.667269, test/accuracy=0.986197, test/loss=0.048440, test/mean_average_precision=0.268240, test/num_examples=43793, total_duration=22443.443970, train/accuracy=0.992908, train/loss=0.022485, train/mean_average_precision=0.586612, validation/accuracy=0.987098, validation/loss=0.045022, validation/mean_average_precision=0.298229, validation/num_examples=43793
I0206 09:48:33.265056 140142484387584 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.09439544379711151, loss=0.025407126173377037
I0206 09:49:05.342138 140002205157120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.09342566877603531, loss=0.02550448477268219
I0206 09:49:37.224966 140142484387584 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.07688478380441666, loss=0.024386493489146233
I0206 09:50:09.412313 140002205157120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.0850917398929596, loss=0.02192559652030468
I0206 09:50:41.435729 140142484387584 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.07250042259693146, loss=0.02186332643032074
I0206 09:51:13.448252 140002205157120 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.1012692004442215, loss=0.02322988212108612
I0206 09:51:45.783054 140142484387584 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.091622494161129, loss=0.02508716471493244
I0206 09:52:03.208581 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:53:35.827470 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:53:38.978562 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:53:42.116585 140205209478976 submission_runner.py:408] Time since start: 22782.65s, 	Step: 49454, 	{'train/accuracy': 0.9932429790496826, 'train/loss': 0.021441875025629997, 'train/mean_average_precision': 0.6142716018141637, 'validation/accuracy': 0.9870386719703674, 'validation/loss': 0.044959813356399536, 'validation/mean_average_precision': 0.2990870103980558, 'validation/num_examples': 43793, 'test/accuracy': 0.9861738085746765, 'test/loss': 0.048262231051921844, 'test/mean_average_precision': 0.2745028253297134, 'test/num_examples': 43793, 'score': 15860.90509223938, 'total_duration': 22782.647591114044, 'accumulated_submission_time': 15860.90509223938, 'accumulated_eval_time': 6918.238473892212, 'accumulated_logging_time': 2.142441749572754}
I0206 09:53:42.141731 139984037992192 logging_writer.py:48] [49454] accumulated_eval_time=6918.238474, accumulated_logging_time=2.142442, accumulated_submission_time=15860.905092, global_step=49454, preemption_count=0, score=15860.905092, test/accuracy=0.986174, test/loss=0.048262, test/mean_average_precision=0.274503, test/num_examples=43793, total_duration=22782.647591, train/accuracy=0.993243, train/loss=0.021442, train/mean_average_precision=0.614272, validation/accuracy=0.987039, validation/loss=0.044960, validation/mean_average_precision=0.299087, validation/num_examples=43793
I0206 09:53:57.238780 139984133072640 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.09374016523361206, loss=0.027910320088267326
I0206 09:54:29.525696 139984037992192 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.08978742361068726, loss=0.02430509589612484
I0206 09:55:01.847076 139984133072640 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.09025783836841583, loss=0.02536056749522686
I0206 09:55:34.237983 139984037992192 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08510582149028778, loss=0.02304321900010109
I0206 09:56:06.966852 139984133072640 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.09044815599918365, loss=0.026702750474214554
I0206 09:56:39.263172 139984037992192 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.10592833906412125, loss=0.02484884485602379
I0206 09:57:11.481690 139984133072640 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.09893643856048584, loss=0.028269583359360695
I0206 09:57:42.393829 140205209478976 spec.py:321] Evaluating on the training split.
I0206 09:59:18.337648 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 09:59:21.462278 140205209478976 spec.py:349] Evaluating on the test split.
I0206 09:59:24.555793 140205209478976 submission_runner.py:408] Time since start: 23125.09s, 	Step: 50198, 	{'train/accuracy': 0.993171751499176, 'train/loss': 0.021398043259978294, 'train/mean_average_precision': 0.6153558358088966, 'validation/accuracy': 0.9871393442153931, 'validation/loss': 0.045644935220479965, 'validation/mean_average_precision': 0.30043306845029527, 'validation/num_examples': 43793, 'test/accuracy': 0.9862896800041199, 'test/loss': 0.04900442436337471, 'test/mean_average_precision': 0.2771777891807733, 'test/num_examples': 43793, 'score': 16101.125497341156, 'total_duration': 23125.086805820465, 'accumulated_submission_time': 16101.125497341156, 'accumulated_eval_time': 7020.40039730072, 'accumulated_logging_time': 2.1796648502349854}
I0206 09:59:24.580561 139963806111488 logging_writer.py:48] [50198] accumulated_eval_time=7020.400397, accumulated_logging_time=2.179665, accumulated_submission_time=16101.125497, global_step=50198, preemption_count=0, score=16101.125497, test/accuracy=0.986290, test/loss=0.049004, test/mean_average_precision=0.277178, test/num_examples=43793, total_duration=23125.086806, train/accuracy=0.993172, train/loss=0.021398, train/mean_average_precision=0.615356, validation/accuracy=0.987139, validation/loss=0.045645, validation/mean_average_precision=0.300433, validation/num_examples=43793
I0206 09:59:25.560606 140002205157120 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.09160904586315155, loss=0.025426261126995087
I0206 09:59:57.444406 139963806111488 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.09735354036092758, loss=0.0234410110861063
I0206 10:00:29.311512 140002205157120 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.09022514522075653, loss=0.026738787069916725
I0206 10:01:01.836194 139963806111488 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.09704754501581192, loss=0.023835964500904083
I0206 10:01:34.228151 140002205157120 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.08189013600349426, loss=0.023900313302874565
I0206 10:02:07.894002 139963806111488 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.0949481651186943, loss=0.02369416505098343
I0206 10:02:39.688693 140002205157120 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.09944554418325424, loss=0.026237353682518005
I0206 10:03:11.666001 139963806111488 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.10567819327116013, loss=0.02577146328985691
I0206 10:03:24.788251 140205209478976 spec.py:321] Evaluating on the training split.
I0206 10:04:57.946645 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 10:05:00.983641 140205209478976 spec.py:349] Evaluating on the test split.
I0206 10:05:04.061190 140205209478976 submission_runner.py:408] Time since start: 23464.59s, 	Step: 50942, 	{'train/accuracy': 0.9934930801391602, 'train/loss': 0.02055634744465351, 'train/mean_average_precision': 0.6322967831703095, 'validation/accuracy': 0.9871681928634644, 'validation/loss': 0.045472003519535065, 'validation/mean_average_precision': 0.29601422460727383, 'validation/num_examples': 43793, 'test/accuracy': 0.986243724822998, 'test/loss': 0.04872037097811699, 'test/mean_average_precision': 0.27748623702265135, 'test/num_examples': 43793, 'score': 16341.301450490952, 'total_duration': 23464.592199087143, 'accumulated_submission_time': 16341.301450490952, 'accumulated_eval_time': 7119.6732885837555, 'accumulated_logging_time': 2.215346097946167}
I0206 10:05:04.086591 139984133072640 logging_writer.py:48] [50942] accumulated_eval_time=7119.673289, accumulated_logging_time=2.215346, accumulated_submission_time=16341.301450, global_step=50942, preemption_count=0, score=16341.301450, test/accuracy=0.986244, test/loss=0.048720, test/mean_average_precision=0.277486, test/num_examples=43793, total_duration=23464.592199, train/accuracy=0.993493, train/loss=0.020556, train/mean_average_precision=0.632297, validation/accuracy=0.987168, validation/loss=0.045472, validation/mean_average_precision=0.296014, validation/num_examples=43793
I0206 10:05:23.057928 140142484387584 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.09447281062602997, loss=0.025786567479372025
I0206 10:05:55.291938 139984133072640 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.08863706886768341, loss=0.025317976251244545
I0206 10:06:27.456421 140142484387584 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.08502144366502762, loss=0.023177916184067726
I0206 10:06:59.603260 139984133072640 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.0978921577334404, loss=0.022561129182577133
I0206 10:07:31.613118 140142484387584 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.10836923122406006, loss=0.02376794070005417
I0206 10:08:03.690124 139984133072640 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.09238969534635544, loss=0.023429641500115395
I0206 10:08:36.115323 140142484387584 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.10453999787569046, loss=0.02491227723658085
I0206 10:09:04.262727 140205209478976 spec.py:321] Evaluating on the training split.
I0206 10:10:41.592998 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 10:10:44.630795 140205209478976 spec.py:349] Evaluating on the test split.
I0206 10:10:47.629892 140205209478976 submission_runner.py:408] Time since start: 23808.16s, 	Step: 51688, 	{'train/accuracy': 0.9936957955360413, 'train/loss': 0.020005319267511368, 'train/mean_average_precision': 0.6529153022018896, 'validation/accuracy': 0.9870293140411377, 'validation/loss': 0.045481979846954346, 'validation/mean_average_precision': 0.29905867568132427, 'validation/num_examples': 43793, 'test/accuracy': 0.9861788749694824, 'test/loss': 0.04864748939871788, 'test/mean_average_precision': 0.2764061666842379, 'test/num_examples': 43793, 'score': 16581.445430278778, 'total_duration': 23808.160900831223, 'accumulated_submission_time': 16581.445430278778, 'accumulated_eval_time': 7223.04040813446, 'accumulated_logging_time': 2.2525784969329834}
I0206 10:10:47.664772 139984037992192 logging_writer.py:48] [51688] accumulated_eval_time=7223.040408, accumulated_logging_time=2.252578, accumulated_submission_time=16581.445430, global_step=51688, preemption_count=0, score=16581.445430, test/accuracy=0.986179, test/loss=0.048647, test/mean_average_precision=0.276406, test/num_examples=43793, total_duration=23808.160901, train/accuracy=0.993696, train/loss=0.020005, train/mean_average_precision=0.652915, validation/accuracy=0.987029, validation/loss=0.045482, validation/mean_average_precision=0.299059, validation/num_examples=43793
I0206 10:10:51.843808 140002205157120 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.10399416834115982, loss=0.02325080707669258
I0206 10:11:23.952572 139984037992192 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.10577850043773651, loss=0.026473408564925194
I0206 10:11:56.037295 140002205157120 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.08896008878946304, loss=0.022845711559057236
I0206 10:12:27.900400 139984037992192 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.09114294499158859, loss=0.02465727925300598
I0206 10:12:59.708381 140002205157120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.10021226108074188, loss=0.024077367037534714
I0206 10:13:31.623128 139984037992192 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.09195072948932648, loss=0.024707574397325516
I0206 10:14:03.153558 140002205157120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.09064295887947083, loss=0.025610337033867836
I0206 10:14:34.810483 139984037992192 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.09604711085557938, loss=0.025236379355192184
I0206 10:14:47.781300 140205209478976 spec.py:321] Evaluating on the training split.
I0206 10:16:26.577377 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 10:16:29.597755 140205209478976 spec.py:349] Evaluating on the test split.
I0206 10:16:32.629401 140205209478976 submission_runner.py:408] Time since start: 24153.16s, 	Step: 52442, 	{'train/accuracy': 0.9935854077339172, 'train/loss': 0.020275061950087547, 'train/mean_average_precision': 0.6460475938094383, 'validation/accuracy': 0.987106442451477, 'validation/loss': 0.04580055549740791, 'validation/mean_average_precision': 0.3005454513036418, 'validation/num_examples': 43793, 'test/accuracy': 0.9862736463546753, 'test/loss': 0.04913301393389702, 'test/mean_average_precision': 0.27781747098487997, 'test/num_examples': 43793, 'score': 16821.530868291855, 'total_duration': 24153.16040802002, 'accumulated_submission_time': 16821.530868291855, 'accumulated_eval_time': 7327.888458013535, 'accumulated_logging_time': 2.2985892295837402}
I0206 10:16:32.655512 139963806111488 logging_writer.py:48] [52442] accumulated_eval_time=7327.888458, accumulated_logging_time=2.298589, accumulated_submission_time=16821.530868, global_step=52442, preemption_count=0, score=16821.530868, test/accuracy=0.986274, test/loss=0.049133, test/mean_average_precision=0.277817, test/num_examples=43793, total_duration=24153.160408, train/accuracy=0.993585, train/loss=0.020275, train/mean_average_precision=0.646048, validation/accuracy=0.987106, validation/loss=0.045801, validation/mean_average_precision=0.300545, validation/num_examples=43793
I0206 10:16:51.606626 140142484387584 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.08540863543748856, loss=0.021045489236712456
I0206 10:17:23.824113 139963806111488 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.09264202415943146, loss=0.02039785124361515
I0206 10:17:55.831189 140142484387584 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.09500007331371307, loss=0.023306185379624367
I0206 10:18:28.093528 139963806111488 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.10718391090631485, loss=0.02584695629775524
I0206 10:19:00.201972 140142484387584 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.10026410222053528, loss=0.024253590032458305
I0206 10:19:32.426229 139963806111488 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.11094903200864792, loss=0.02601725235581398
I0206 10:20:04.784442 140142484387584 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.10303600877523422, loss=0.023182133212685585
I0206 10:20:32.936588 140205209478976 spec.py:321] Evaluating on the training split.
I0206 10:22:05.402285 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 10:22:08.443724 140205209478976 spec.py:349] Evaluating on the test split.
I0206 10:22:11.550369 140205209478976 submission_runner.py:408] Time since start: 24492.08s, 	Step: 53189, 	{'train/accuracy': 0.9934902787208557, 'train/loss': 0.020645245909690857, 'train/mean_average_precision': 0.6349567310737267, 'validation/accuracy': 0.9871125817298889, 'validation/loss': 0.04573584347963333, 'validation/mean_average_precision': 0.29954231350914534, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.049246788024902344, 'test/mean_average_precision': 0.2722941436238601, 'test/num_examples': 43793, 'score': 17061.780789375305, 'total_duration': 24492.081380605698, 'accumulated_submission_time': 17061.780789375305, 'accumulated_eval_time': 7426.502200841904, 'accumulated_logging_time': 2.3355820178985596}
I0206 10:22:11.585618 139984037992192 logging_writer.py:48] [53189] accumulated_eval_time=7426.502201, accumulated_logging_time=2.335582, accumulated_submission_time=17061.780789, global_step=53189, preemption_count=0, score=17061.780789, test/accuracy=0.986194, test/loss=0.049247, test/mean_average_precision=0.272294, test/num_examples=43793, total_duration=24492.081381, train/accuracy=0.993490, train/loss=0.020645, train/mean_average_precision=0.634957, validation/accuracy=0.987113, validation/loss=0.045736, validation/mean_average_precision=0.299542, validation/num_examples=43793
I0206 10:22:15.400694 139984133072640 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.0925838053226471, loss=0.02068311907351017
I0206 10:22:47.984530 139984037992192 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.0990806296467781, loss=0.020969439297914505
I0206 10:23:20.217409 139984133072640 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.09451361000537872, loss=0.02421778067946434
I0206 10:23:52.211596 139984037992192 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.09491550177335739, loss=0.023778269067406654
I0206 10:24:24.638659 139984133072640 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.10513054579496384, loss=0.023996179923415184
I0206 10:24:56.410570 139984037992192 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.10052333772182465, loss=0.022790294140577316
I0206 10:25:28.624071 139984133072640 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.08684714138507843, loss=0.02253791131079197
I0206 10:26:00.759694 139984037992192 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.09480065852403641, loss=0.023074593394994736
I0206 10:26:11.785847 140205209478976 spec.py:321] Evaluating on the training split.
I0206 10:27:50.276369 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 10:27:53.307470 140205209478976 spec.py:349] Evaluating on the test split.
I0206 10:27:56.337189 140205209478976 submission_runner.py:408] Time since start: 24836.87s, 	Step: 53935, 	{'train/accuracy': 0.9932562112808228, 'train/loss': 0.02114921435713768, 'train/mean_average_precision': 0.6126243232067949, 'validation/accuracy': 0.9869729280471802, 'validation/loss': 0.04588203504681587, 'validation/mean_average_precision': 0.2990511593052038, 'validation/num_examples': 43793, 'test/accuracy': 0.9861367344856262, 'test/loss': 0.04906720295548439, 'test/mean_average_precision': 0.27791338493800183, 'test/num_examples': 43793, 'score': 17301.95036458969, 'total_duration': 24836.868192195892, 'accumulated_submission_time': 17301.95036458969, 'accumulated_eval_time': 7531.053488969803, 'accumulated_logging_time': 2.3819081783294678}
I0206 10:27:56.363085 140002205157120 logging_writer.py:48] [53935] accumulated_eval_time=7531.053489, accumulated_logging_time=2.381908, accumulated_submission_time=17301.950365, global_step=53935, preemption_count=0, score=17301.950365, test/accuracy=0.986137, test/loss=0.049067, test/mean_average_precision=0.277913, test/num_examples=43793, total_duration=24836.868192, train/accuracy=0.993256, train/loss=0.021149, train/mean_average_precision=0.612624, validation/accuracy=0.986973, validation/loss=0.045882, validation/mean_average_precision=0.299051, validation/num_examples=43793
I0206 10:28:17.655596 140142484387584 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.11496343463659286, loss=0.024907367303967476
I0206 10:28:49.641218 140002205157120 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.10770254582166672, loss=0.02232537791132927
I0206 10:29:21.630815 140142484387584 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.10191462934017181, loss=0.024201558902859688
I0206 10:29:53.795413 140002205157120 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.10695608705282211, loss=0.025965142995119095
I0206 10:30:25.917765 140142484387584 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.12309308350086212, loss=0.026581358164548874
I0206 10:30:58.097465 140002205157120 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.13426533341407776, loss=0.02380271628499031
I0206 10:31:29.942150 140142484387584 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.09718835353851318, loss=0.02038317173719406
I0206 10:31:56.651980 140205209478976 spec.py:321] Evaluating on the training split.
I0206 10:33:33.007126 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 10:33:36.164405 140205209478976 spec.py:349] Evaluating on the test split.
I0206 10:33:39.241590 140205209478976 submission_runner.py:408] Time since start: 25179.77s, 	Step: 54684, 	{'train/accuracy': 0.9933159947395325, 'train/loss': 0.02094193734228611, 'train/mean_average_precision': 0.6317724607619654, 'validation/accuracy': 0.987216055393219, 'validation/loss': 0.04619571194052696, 'validation/mean_average_precision': 0.2959647151213805, 'validation/num_examples': 43793, 'test/accuracy': 0.9862450361251831, 'test/loss': 0.04962007701396942, 'test/mean_average_precision': 0.27874735065386286, 'test/num_examples': 43793, 'score': 17542.20799946785, 'total_duration': 25179.772585392, 'accumulated_submission_time': 17542.20799946785, 'accumulated_eval_time': 7633.643043994904, 'accumulated_logging_time': 2.4187357425689697}
I0206 10:33:39.268352 139984037992192 logging_writer.py:48] [54684] accumulated_eval_time=7633.643044, accumulated_logging_time=2.418736, accumulated_submission_time=17542.207999, global_step=54684, preemption_count=0, score=17542.207999, test/accuracy=0.986245, test/loss=0.049620, test/mean_average_precision=0.278747, test/num_examples=43793, total_duration=25179.772585, train/accuracy=0.993316, train/loss=0.020942, train/mean_average_precision=0.631772, validation/accuracy=0.987216, validation/loss=0.046196, validation/mean_average_precision=0.295965, validation/num_examples=43793
I0206 10:33:44.708270 139984133072640 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.10898853093385696, loss=0.024202393367886543
I0206 10:34:16.764590 139984037992192 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.1475154459476471, loss=0.021763524040579796
I0206 10:34:48.741216 139984133072640 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.11107007414102554, loss=0.025473682209849358
I0206 10:35:20.607503 139984037992192 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.0990857258439064, loss=0.02451426535844803
I0206 10:35:52.413608 139984133072640 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.12115183472633362, loss=0.023870427161455154
I0206 10:36:24.359499 139984037992192 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.11300375312566757, loss=0.021826623007655144
I0206 10:36:56.224976 139984133072640 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.11038531363010406, loss=0.02287663333117962
I0206 10:37:28.564599 139984037992192 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.10054141283035278, loss=0.027047837153077126
I0206 10:37:39.376595 140205209478976 spec.py:321] Evaluating on the training split.
I0206 10:39:16.714840 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 10:39:19.837008 140205209478976 spec.py:349] Evaluating on the test split.
I0206 10:39:22.903493 140205209478976 submission_runner.py:408] Time since start: 25523.43s, 	Step: 55434, 	{'train/accuracy': 0.9935021996498108, 'train/loss': 0.02031290903687477, 'train/mean_average_precision': 0.6375259346868289, 'validation/accuracy': 0.9871361255645752, 'validation/loss': 0.04621044546365738, 'validation/mean_average_precision': 0.29952755101861744, 'validation/num_examples': 43793, 'test/accuracy': 0.9863107204437256, 'test/loss': 0.049470916390419006, 'test/mean_average_precision': 0.277157897267368, 'test/num_examples': 43793, 'score': 17782.284705638885, 'total_duration': 25523.43450474739, 'accumulated_submission_time': 17782.284705638885, 'accumulated_eval_time': 7737.169899225235, 'accumulated_logging_time': 2.4568605422973633}
I0206 10:39:22.929204 140002205157120 logging_writer.py:48] [55434] accumulated_eval_time=7737.169899, accumulated_logging_time=2.456861, accumulated_submission_time=17782.284706, global_step=55434, preemption_count=0, score=17782.284706, test/accuracy=0.986311, test/loss=0.049471, test/mean_average_precision=0.277158, test/num_examples=43793, total_duration=25523.434505, train/accuracy=0.993502, train/loss=0.020313, train/mean_average_precision=0.637526, validation/accuracy=0.987136, validation/loss=0.046210, validation/mean_average_precision=0.299528, validation/num_examples=43793
I0206 10:39:44.500487 140142484387584 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.0988660380244255, loss=0.022911114618182182
I0206 10:40:16.871590 140002205157120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.10166022926568985, loss=0.0219719335436821
I0206 10:40:49.235120 140142484387584 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.11639723181724548, loss=0.025014841929078102
I0206 10:41:21.316116 140002205157120 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.12030506134033203, loss=0.021477295085787773
I0206 10:41:53.383411 140142484387584 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.10385101288557053, loss=0.01865619234740734
I0206 10:42:26.005861 140002205157120 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.12373736500740051, loss=0.02084493078291416
I0206 10:42:58.509291 140142484387584 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.10790146887302399, loss=0.02360239438712597
I0206 10:43:23.082262 140205209478976 spec.py:321] Evaluating on the training split.
I0206 10:45:01.538739 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 10:45:04.612967 140205209478976 spec.py:349] Evaluating on the test split.
I0206 10:45:07.601142 140205209478976 submission_runner.py:408] Time since start: 25868.13s, 	Step: 56177, 	{'train/accuracy': 0.9936658143997192, 'train/loss': 0.019820721819996834, 'train/mean_average_precision': 0.6528672863038035, 'validation/accuracy': 0.9871364831924438, 'validation/loss': 0.046166084706783295, 'validation/mean_average_precision': 0.30250379337983463, 'validation/num_examples': 43793, 'test/accuracy': 0.9862247705459595, 'test/loss': 0.04958441108465195, 'test/mean_average_precision': 0.2745376159234082, 'test/num_examples': 43793, 'score': 18022.405507564545, 'total_duration': 25868.13215303421, 'accumulated_submission_time': 18022.405507564545, 'accumulated_eval_time': 7841.688732147217, 'accumulated_logging_time': 2.4949471950531006}
I0206 10:45:07.631082 139963806111488 logging_writer.py:48] [56177] accumulated_eval_time=7841.688732, accumulated_logging_time=2.494947, accumulated_submission_time=18022.405508, global_step=56177, preemption_count=0, score=18022.405508, test/accuracy=0.986225, test/loss=0.049584, test/mean_average_precision=0.274538, test/num_examples=43793, total_duration=25868.132153, train/accuracy=0.993666, train/loss=0.019821, train/mean_average_precision=0.652867, validation/accuracy=0.987136, validation/loss=0.046166, validation/mean_average_precision=0.302504, validation/num_examples=43793
I0206 10:45:15.313112 139984133072640 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.10340992361307144, loss=0.024131370708346367
I0206 10:45:47.337361 139963806111488 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.10064136236906052, loss=0.019386645406484604
I0206 10:46:19.266309 139984133072640 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.10729888081550598, loss=0.021825848147273064
I0206 10:46:51.556527 139963806111488 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.09062132984399796, loss=0.021049046888947487
I0206 10:47:23.992441 139984133072640 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.11464232951402664, loss=0.023146890103816986
I0206 10:47:56.346468 139963806111488 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.10225071758031845, loss=0.02228270098567009
I0206 10:48:28.611887 139984133072640 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.09538424015045166, loss=0.02063981257379055
I0206 10:49:00.412274 139963806111488 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.11718490719795227, loss=0.0248627457767725
I0206 10:49:07.915998 140205209478976 spec.py:321] Evaluating on the training split.
I0206 10:50:42.191533 140205209478976 spec.py:333] Evaluating on the validation split.
I0206 10:50:45.235342 140205209478976 spec.py:349] Evaluating on the test split.
I0206 10:50:48.227182 140205209478976 submission_runner.py:408] Time since start: 26208.76s, 	Step: 56924, 	{'train/accuracy': 0.9937217235565186, 'train/loss': 0.019551070407032967, 'train/mean_average_precision': 0.6607373302507992, 'validation/accuracy': 0.9871413707733154, 'validation/loss': 0.04628141224384308, 'validation/mean_average_precision': 0.3001707093408799, 'validation/num_examples': 43793, 'test/accuracy': 0.9862277507781982, 'test/loss': 0.04944280534982681, 'test/mean_average_precision': 0.27963990690568774, 'test/num_examples': 43793, 'score': 18262.658086299896, 'total_duration': 26208.75817298889, 'accumulated_submission_time': 18262.658086299896, 'accumulated_eval_time': 7941.999848365784, 'accumulated_logging_time': 2.5373575687408447}
I0206 10:50:48.253075 139984037992192 logging_writer.py:48] [56924] accumulated_eval_time=7941.999848, accumulated_logging_time=2.537358, accumulated_submission_time=18262.658086, global_step=56924, preemption_count=0, score=18262.658086, test/accuracy=0.986228, test/loss=0.049443, test/mean_average_precision=0.279640, test/num_examples=43793, total_duration=26208.758173, train/accuracy=0.993722, train/loss=0.019551, train/mean_average_precision=0.660737, validation/accuracy=0.987141, validation/loss=0.046281, validation/mean_average_precision=0.300171, validation/num_examples=43793
I0206 10:51:13.226894 140002205157120 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.10977856069803238, loss=0.024089055135846138
I0206 10:51:45.030577 139984037992192 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.10301069170236588, loss=0.020863253623247147
I0206 10:52:17.274652 140002205157120 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.10267939418554306, loss=0.02239486761391163
I0206 10:52:49.516341 139984037992192 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.11273830384016037, loss=0.02119293436408043
I0206 10:53:21.805459 140002205157120 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.11591560393571854, loss=0.020414244383573532
I0206 10:53:53.914171 139984037992192 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.123027503490448, loss=0.023549219593405724
I0206 10:54:22.851033 140002205157120 logging_writer.py:48] [57590] global_step=57590, preemption_count=0, score=18477.211142
I0206 10:54:22.902905 140205209478976 checkpoints.py:490] Saving checkpoint at step: 57590
I0206 10:54:23.020934 140205209478976 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_5/checkpoint_57590
I0206 10:54:23.021892 140205209478976 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/ogbg_jax/trial_5/checkpoint_57590.
I0206 10:54:23.209166 140205209478976 submission_runner.py:583] Tuning trial 5/5
I0206 10:54:23.209397 140205209478976 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0206 10:54:23.213755 140205209478976 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5093092918395996, 'train/loss': 0.7391443848609924, 'train/mean_average_precision': 0.023277378646798894, 'validation/accuracy': 0.5064646005630493, 'validation/loss': 0.7422075867652893, 'validation/mean_average_precision': 0.025293153641848993, 'validation/num_examples': 43793, 'test/accuracy': 0.5047284960746765, 'test/loss': 0.7438743114471436, 'test/mean_average_precision': 0.026926999206144768, 'test/num_examples': 43793, 'score': 13.580633878707886, 'total_duration': 117.30141496658325, 'accumulated_submission_time': 13.580633878707886, 'accumulated_eval_time': 103.72073292732239, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (750, {'train/accuracy': 0.9868060946464539, 'train/loss': 0.05153829976916313, 'train/mean_average_precision': 0.05423567026451998, 'validation/accuracy': 0.9842458367347717, 'validation/loss': 0.06153523549437523, 'validation/mean_average_precision': 0.05384023588038083, 'validation/num_examples': 43793, 'test/accuracy': 0.9832671880722046, 'test/loss': 0.06482018530368805, 'test/mean_average_precision': 0.054599508015334225, 'test/num_examples': 43793, 'score': 253.80401229858398, 'total_duration': 461.0968973636627, 'accumulated_submission_time': 253.80401229858398, 'accumulated_eval_time': 207.2520191669464, 'accumulated_logging_time': 0.020728349685668945, 'global_step': 750, 'preemption_count': 0}), (1495, {'train/accuracy': 0.9871118068695068, 'train/loss': 0.048001017421483994, 'train/mean_average_precision': 0.083969882400915, 'validation/accuracy': 0.9844828844070435, 'validation/loss': 0.057401418685913086, 'validation/mean_average_precision': 0.08528654023163375, 'validation/num_examples': 43793, 'test/accuracy': 0.9834954738616943, 'test/loss': 0.06077468395233154, 'test/mean_average_precision': 0.08407670770359991, 'test/num_examples': 43793, 'score': 494.04190468788147, 'total_duration': 805.2264924049377, 'accumulated_submission_time': 494.04190468788147, 'accumulated_eval_time': 311.0966327190399, 'accumulated_logging_time': 0.046759843826293945, 'global_step': 1495, 'preemption_count': 0}), (2243, {'train/accuracy': 0.987299919128418, 'train/loss': 0.045526277273893356, 'train/mean_average_precision': 0.1279417429584762, 'validation/accuracy': 0.9847471714019775, 'validation/loss': 0.054682765156030655, 'validation/mean_average_precision': 0.12864685514709234, 'validation/num_examples': 43793, 'test/accuracy': 0.9837182760238647, 'test/loss': 0.05783096328377724, 'test/mean_average_precision': 0.12355053623446106, 'test/num_examples': 43793, 'score': 734.1674497127533, 'total_duration': 1150.4348917007446, 'accumulated_submission_time': 734.1674497127533, 'accumulated_eval_time': 416.1317677497864, 'accumulated_logging_time': 0.07377338409423828, 'global_step': 2243, 'preemption_count': 0}), (2991, {'train/accuracy': 0.9879700541496277, 'train/loss': 0.04237353801727295, 'train/mean_average_precision': 0.1602362821606831, 'validation/accuracy': 0.9851559400558472, 'validation/loss': 0.05190819129347801, 'validation/mean_average_precision': 0.1475911315728504, 'validation/num_examples': 43793, 'test/accuracy': 0.9842160940170288, 'test/loss': 0.05484679713845253, 'test/mean_average_precision': 0.14847681753521164, 'test/num_examples': 43793, 'score': 974.1777114868164, 'total_duration': 1492.042894601822, 'accumulated_submission_time': 974.1777114868164, 'accumulated_eval_time': 517.6807377338409, 'accumulated_logging_time': 0.10147762298583984, 'global_step': 2991, 'preemption_count': 0}), (3736, {'train/accuracy': 0.9882538318634033, 'train/loss': 0.04129573702812195, 'train/mean_average_precision': 0.1869888306182944, 'validation/accuracy': 0.9851595759391785, 'validation/loss': 0.050465673208236694, 'validation/mean_average_precision': 0.1660371829803102, 'validation/num_examples': 43793, 'test/accuracy': 0.984274685382843, 'test/loss': 0.053023021668195724, 'test/mean_average_precision': 0.16593629182887754, 'test/num_examples': 43793, 'score': 1214.414056301117, 'total_duration': 1832.6644973754883, 'accumulated_submission_time': 1214.414056301117, 'accumulated_eval_time': 618.0172493457794, 'accumulated_logging_time': 0.12940216064453125, 'global_step': 3736, 'preemption_count': 0}), (4488, {'train/accuracy': 0.9885920882225037, 'train/loss': 0.039246659725904465, 'train/mean_average_precision': 0.2131411379152453, 'validation/accuracy': 0.9855939149856567, 'validation/loss': 0.048895981162786484, 'validation/mean_average_precision': 0.18449098765003324, 'validation/num_examples': 43793, 'test/accuracy': 0.9847304224967957, 'test/loss': 0.05166628584265709, 'test/mean_average_precision': 0.18867037077657536, 'test/num_examples': 43793, 'score': 1454.5042502880096, 'total_duration': 2181.4644510746, 'accumulated_submission_time': 1454.5042502880096, 'accumulated_eval_time': 726.6796119213104, 'accumulated_logging_time': 0.15626215934753418, 'global_step': 4488, 'preemption_count': 0}), (5241, {'train/accuracy': 0.9889898300170898, 'train/loss': 0.03764519840478897, 'train/mean_average_precision': 0.23802269979823015, 'validation/accuracy': 0.9859138131141663, 'validation/loss': 0.04751823469996452, 'validation/mean_average_precision': 0.20319129931163368, 'validation/num_examples': 43793, 'test/accuracy': 0.9850661158561707, 'test/loss': 0.050238415598869324, 'test/mean_average_precision': 0.21379311876591287, 'test/num_examples': 43793, 'score': 1694.6122086048126, 'total_duration': 2530.281988620758, 'accumulated_submission_time': 1694.6122086048126, 'accumulated_eval_time': 835.3410527706146, 'accumulated_logging_time': 0.18371868133544922, 'global_step': 5241, 'preemption_count': 0}), (5993, {'train/accuracy': 0.9889129996299744, 'train/loss': 0.03743157163262367, 'train/mean_average_precision': 0.25126464969029233, 'validation/accuracy': 0.9860774278640747, 'validation/loss': 0.04659366235136986, 'validation/mean_average_precision': 0.21315192018549464, 'validation/num_examples': 43793, 'test/accuracy': 0.9853339791297913, 'test/loss': 0.049202386289834976, 'test/mean_average_precision': 0.2152423660366171, 'test/num_examples': 43793, 'score': 1934.768192768097, 'total_duration': 2878.444193124771, 'accumulated_submission_time': 1934.768192768097, 'accumulated_eval_time': 943.2999262809753, 'accumulated_logging_time': 0.2107102870941162, 'global_step': 5993, 'preemption_count': 0}), (6748, {'train/accuracy': 0.989357590675354, 'train/loss': 0.035928696393966675, 'train/mean_average_precision': 0.27196293000153315, 'validation/accuracy': 0.9862214922904968, 'validation/loss': 0.04625379666686058, 'validation/mean_average_precision': 0.22629593197467981, 'validation/num_examples': 43793, 'test/accuracy': 0.9853845238685608, 'test/loss': 0.04884004965424538, 'test/mean_average_precision': 0.2211056133914604, 'test/num_examples': 43793, 'score': 2174.874867916107, 'total_duration': 3218.5420083999634, 'accumulated_submission_time': 2174.874867916107, 'accumulated_eval_time': 1043.2435188293457, 'accumulated_logging_time': 0.23788046836853027, 'global_step': 6748, 'preemption_count': 0}), (7493, {'train/accuracy': 0.9894558787345886, 'train/loss': 0.03535552695393562, 'train/mean_average_precision': 0.3062496271384285, 'validation/accuracy': 0.9862868785858154, 'validation/loss': 0.04599423334002495, 'validation/mean_average_precision': 0.22793275281089398, 'validation/num_examples': 43793, 'test/accuracy': 0.9854055643081665, 'test/loss': 0.04870980978012085, 'test/mean_average_precision': 0.23485982806646893, 'test/num_examples': 43793, 'score': 2415.1428520679474, 'total_duration': 3565.2347972393036, 'accumulated_submission_time': 2415.1428520679474, 'accumulated_eval_time': 1149.6194591522217, 'accumulated_logging_time': 0.26660776138305664, 'global_step': 7493, 'preemption_count': 0}), (8242, {'train/accuracy': 0.989901602268219, 'train/loss': 0.033745985478162766, 'train/mean_average_precision': 0.33079067936035234, 'validation/accuracy': 0.9863956570625305, 'validation/loss': 0.045376237481832504, 'validation/mean_average_precision': 0.23738480475175228, 'validation/num_examples': 43793, 'test/accuracy': 0.9856169819831848, 'test/loss': 0.047817718237638474, 'test/mean_average_precision': 0.24193834660193864, 'test/num_examples': 43793, 'score': 2655.087381839752, 'total_duration': 3910.5265514850616, 'accumulated_submission_time': 2655.087381839752, 'accumulated_eval_time': 1254.9102308750153, 'accumulated_logging_time': 0.3024904727935791, 'global_step': 8242, 'preemption_count': 0}), (8996, {'train/accuracy': 0.99005526304245, 'train/loss': 0.03328986465930939, 'train/mean_average_precision': 0.3302585783276252, 'validation/accuracy': 0.9864711761474609, 'validation/loss': 0.04519089683890343, 'validation/mean_average_precision': 0.24074316745035115, 'validation/num_examples': 43793, 'test/accuracy': 0.9856843948364258, 'test/loss': 0.04783324897289276, 'test/mean_average_precision': 0.2460210322111171, 'test/num_examples': 43793, 'score': 2895.369548559189, 'total_duration': 4255.388598442078, 'accumulated_submission_time': 2895.369548559189, 'accumulated_eval_time': 1359.4394128322601, 'accumulated_logging_time': 0.33234596252441406, 'global_step': 8996, 'preemption_count': 0}), (9748, {'train/accuracy': 0.9901725649833679, 'train/loss': 0.03284863755106926, 'train/mean_average_precision': 0.3479799884843341, 'validation/accuracy': 0.9866335391998291, 'validation/loss': 0.04468999430537224, 'validation/mean_average_precision': 0.25269906028148437, 'validation/num_examples': 43793, 'test/accuracy': 0.9858174920082092, 'test/loss': 0.04727887734770775, 'test/mean_average_precision': 0.2526541590940708, 'test/num_examples': 43793, 'score': 3135.624180316925, 'total_duration': 4600.473388910294, 'accumulated_submission_time': 3135.624180316925, 'accumulated_eval_time': 1464.221135854721, 'accumulated_logging_time': 0.3602263927459717, 'global_step': 9748, 'preemption_count': 0}), (10498, {'train/accuracy': 0.9901142716407776, 'train/loss': 0.03304450958967209, 'train/mean_average_precision': 0.34253933158701405, 'validation/accuracy': 0.9865771532058716, 'validation/loss': 0.04460688680410385, 'validation/mean_average_precision': 0.25675631240173197, 'validation/num_examples': 43793, 'test/accuracy': 0.9857223033905029, 'test/loss': 0.0474354512989521, 'test/mean_average_precision': 0.24657643442000826, 'test/num_examples': 43793, 'score': 3375.697806596756, 'total_duration': 4944.093198299408, 'accumulated_submission_time': 3375.697806596756, 'accumulated_eval_time': 1567.718020439148, 'accumulated_logging_time': 0.3888866901397705, 'global_step': 10498, 'preemption_count': 0}), (11243, {'train/accuracy': 0.9903327226638794, 'train/loss': 0.0322994701564312, 'train/mean_average_precision': 0.3561079984004907, 'validation/accuracy': 0.9867005348205566, 'validation/loss': 0.04423731938004494, 'validation/mean_average_precision': 0.2580287784778692, 'validation/num_examples': 43793, 'test/accuracy': 0.9858258962631226, 'test/loss': 0.046979278326034546, 'test/mean_average_precision': 0.2500176737745309, 'test/num_examples': 43793, 'score': 3615.875280857086, 'total_duration': 5290.306088924408, 'accumulated_submission_time': 3615.875280857086, 'accumulated_eval_time': 1673.7032914161682, 'accumulated_logging_time': 0.4190025329589844, 'global_step': 11243, 'preemption_count': 0}), (11987, {'train/accuracy': 0.990323007106781, 'train/loss': 0.032075900584459305, 'train/mean_average_precision': 0.3612385092120623, 'validation/accuracy': 0.9867829084396362, 'validation/loss': 0.04408801719546318, 'validation/mean_average_precision': 0.26028699428758134, 'validation/num_examples': 43793, 'test/accuracy': 0.9858882427215576, 'test/loss': 0.04693092778325081, 'test/mean_average_precision': 0.25750368349513364, 'test/num_examples': 43793, 'score': 3855.9695858955383, 'total_duration': 5630.856683969498, 'accumulated_submission_time': 3855.9695858955383, 'accumulated_eval_time': 1774.1074786186218, 'accumulated_logging_time': 0.4494631290435791, 'global_step': 11987, 'preemption_count': 0}), (12742, {'train/accuracy': 0.9905083775520325, 'train/loss': 0.03149132803082466, 'train/mean_average_precision': 0.3642103863357995, 'validation/accuracy': 0.9867606163024902, 'validation/loss': 0.04409108683466911, 'validation/mean_average_precision': 0.2677271230835614, 'validation/num_examples': 43793, 'test/accuracy': 0.9859021306037903, 'test/loss': 0.04708637669682503, 'test/mean_average_precision': 0.25401200047356093, 'test/num_examples': 43793, 'score': 4096.190707445145, 'total_duration': 5972.480631828308, 'accumulated_submission_time': 4096.190707445145, 'accumulated_eval_time': 1875.4612171649933, 'accumulated_logging_time': 0.4779088497161865, 'global_step': 12742, 'preemption_count': 0}), (13482, {'train/accuracy': 0.9906244874000549, 'train/loss': 0.03097464144229889, 'train/mean_average_precision': 0.389609197822945, 'validation/accuracy': 0.9867532849311829, 'validation/loss': 0.043991703540086746, 'validation/mean_average_precision': 0.2688608755476995, 'validation/num_examples': 43793, 'test/accuracy': 0.9860032200813293, 'test/loss': 0.046686749905347824, 'test/mean_average_precision': 0.2611335866362385, 'test/num_examples': 43793, 'score': 4336.178166389465, 'total_duration': 6320.635596513748, 'accumulated_submission_time': 4336.178166389465, 'accumulated_eval_time': 1983.5776450634003, 'accumulated_logging_time': 0.5073034763336182, 'global_step': 13482, 'preemption_count': 0}), (14236, {'train/accuracy': 0.9906725287437439, 'train/loss': 0.030629856511950493, 'train/mean_average_precision': 0.39957078398726886, 'validation/accuracy': 0.986504077911377, 'validation/loss': 0.0440920814871788, 'validation/mean_average_precision': 0.2636364530613028, 'validation/num_examples': 43793, 'test/accuracy': 0.9858457446098328, 'test/loss': 0.04665074124932289, 'test/mean_average_precision': 0.25670494808584926, 'test/num_examples': 43793, 'score': 4576.319272518158, 'total_duration': 6664.528880357742, 'accumulated_submission_time': 4576.319272518158, 'accumulated_eval_time': 2087.2807455062866, 'accumulated_logging_time': 0.5352981090545654, 'global_step': 14236, 'preemption_count': 0}), (14991, {'train/accuracy': 0.9907553195953369, 'train/loss': 0.030214227735996246, 'train/mean_average_precision': 0.41221099028224745, 'validation/accuracy': 0.986757755279541, 'validation/loss': 0.044588346034288406, 'validation/mean_average_precision': 0.2677942141421415, 'validation/num_examples': 43793, 'test/accuracy': 0.9859707951545715, 'test/loss': 0.04723716154694557, 'test/mean_average_precision': 0.26380350888819126, 'test/num_examples': 43793, 'score': 4816.31423330307, 'total_duration': 7012.129799127579, 'accumulated_submission_time': 4816.31423330307, 'accumulated_eval_time': 2194.837212085724, 'accumulated_logging_time': 0.5642292499542236, 'global_step': 14991, 'preemption_count': 0}), (15729, {'train/accuracy': 0.9911349415779114, 'train/loss': 0.029304539784789085, 'train/mean_average_precision': 0.430178080129186, 'validation/accuracy': 0.9867480397224426, 'validation/loss': 0.044163040816783905, 'validation/mean_average_precision': 0.2715012183204673, 'validation/num_examples': 43793, 'test/accuracy': 0.9859463572502136, 'test/loss': 0.046861980110406876, 'test/mean_average_precision': 0.26072838119380015, 'test/num_examples': 43793, 'score': 5056.359443902969, 'total_duration': 7353.330956935883, 'accumulated_submission_time': 5056.359443902969, 'accumulated_eval_time': 2295.940425634384, 'accumulated_logging_time': 0.5946774482727051, 'global_step': 15729, 'preemption_count': 0}), (16471, {'train/accuracy': 0.9909190535545349, 'train/loss': 0.02977074310183525, 'train/mean_average_precision': 0.40910289981395387, 'validation/accuracy': 0.9867866039276123, 'validation/loss': 0.044062357395887375, 'validation/mean_average_precision': 0.2620690775753606, 'validation/num_examples': 43793, 'test/accuracy': 0.9859893321990967, 'test/loss': 0.04678834602236748, 'test/mean_average_precision': 0.25604497180348645, 'test/num_examples': 43793, 'score': 5296.309223651886, 'total_duration': 7695.902285814285, 'accumulated_submission_time': 5296.309223651886, 'accumulated_eval_time': 2398.5105135440826, 'accumulated_logging_time': 0.6254196166992188, 'global_step': 16471, 'preemption_count': 0}), (17210, {'train/accuracy': 0.9909037947654724, 'train/loss': 0.029957536607980728, 'train/mean_average_precision': 0.4211723660599582, 'validation/accuracy': 0.9867788553237915, 'validation/loss': 0.044044554233551025, 'validation/mean_average_precision': 0.2709214835239603, 'validation/num_examples': 43793, 'test/accuracy': 0.9859653115272522, 'test/loss': 0.046575117856264114, 'test/mean_average_precision': 0.26149841335593477, 'test/num_examples': 43793, 'score': 5536.299431324005, 'total_duration': 8038.960066795349, 'accumulated_submission_time': 5536.299431324005, 'accumulated_eval_time': 2501.5175173282623, 'accumulated_logging_time': 0.6656818389892578, 'global_step': 17210, 'preemption_count': 0}), (17958, {'train/accuracy': 0.9907863736152649, 'train/loss': 0.03013172186911106, 'train/mean_average_precision': 0.4060708456613312, 'validation/accuracy': 0.9868494868278503, 'validation/loss': 0.04417216405272484, 'validation/mean_average_precision': 0.2710522605596167, 'validation/num_examples': 43793, 'test/accuracy': 0.986120343208313, 'test/loss': 0.04671439900994301, 'test/mean_average_precision': 0.26699967942228076, 'test/num_examples': 43793, 'score': 5776.414937496185, 'total_duration': 8387.298349618912, 'accumulated_submission_time': 5776.414937496185, 'accumulated_eval_time': 2609.689152956009, 'accumulated_logging_time': 0.6960053443908691, 'global_step': 17958, 'preemption_count': 0}), (18709, {'train/accuracy': 0.9909077882766724, 'train/loss': 0.029821813106536865, 'train/mean_average_precision': 0.4070056823467816, 'validation/accuracy': 0.9868308305740356, 'validation/loss': 0.04389888420701027, 'validation/mean_average_precision': 0.27659673661417733, 'validation/num_examples': 43793, 'test/accuracy': 0.9861354827880859, 'test/loss': 0.04650556296110153, 'test/mean_average_precision': 0.2658340993299205, 'test/num_examples': 43793, 'score': 6016.625653028488, 'total_duration': 8733.897119998932, 'accumulated_submission_time': 6016.625653028488, 'accumulated_eval_time': 2716.024253845215, 'accumulated_logging_time': 0.7282171249389648, 'global_step': 18709, 'preemption_count': 0}), (19465, {'train/accuracy': 0.9910405278205872, 'train/loss': 0.029201403260231018, 'train/mean_average_precision': 0.4292364006707121, 'validation/accuracy': 0.9868568181991577, 'validation/loss': 0.04404282569885254, 'validation/mean_average_precision': 0.2791245527026243, 'validation/num_examples': 43793, 'test/accuracy': 0.9860706329345703, 'test/loss': 0.04683166369795799, 'test/mean_average_precision': 0.25962619079546045, 'test/num_examples': 43793, 'score': 6256.626332998276, 'total_duration': 9076.950638532639, 'accumulated_submission_time': 6256.626332998276, 'accumulated_eval_time': 2819.025629043579, 'accumulated_logging_time': 0.7590713500976562, 'global_step': 19465, 'preemption_count': 0}), (20215, {'train/accuracy': 0.9910680651664734, 'train/loss': 0.02918209321796894, 'train/mean_average_precision': 0.439064296870055, 'validation/accuracy': 0.986634373664856, 'validation/loss': 0.044114645570516586, 'validation/mean_average_precision': 0.28133250854834996, 'validation/num_examples': 43793, 'test/accuracy': 0.9858280420303345, 'test/loss': 0.04660707712173462, 'test/mean_average_precision': 0.26924369568151874, 'test/num_examples': 43793, 'score': 6496.625156402588, 'total_duration': 9420.829341888428, 'accumulated_submission_time': 6496.625156402588, 'accumulated_eval_time': 2922.849404811859, 'accumulated_logging_time': 0.792198657989502, 'global_step': 20215, 'preemption_count': 0}), (20963, {'train/accuracy': 0.9911518692970276, 'train/loss': 0.02880358137190342, 'train/mean_average_precision': 0.4462325316914583, 'validation/accuracy': 0.9868718385696411, 'validation/loss': 0.04390169680118561, 'validation/mean_average_precision': 0.2812189409328452, 'validation/num_examples': 43793, 'test/accuracy': 0.9860310554504395, 'test/loss': 0.04686373099684715, 'test/mean_average_precision': 0.26727420961545023, 'test/num_examples': 43793, 'score': 6736.826936483383, 'total_duration': 9760.718255281448, 'accumulated_submission_time': 6736.826936483383, 'accumulated_eval_time': 3022.484857082367, 'accumulated_logging_time': 0.8230326175689697, 'global_step': 20963, 'preemption_count': 0}), (21713, {'train/accuracy': 0.9913423657417297, 'train/loss': 0.027976617217063904, 'train/mean_average_precision': 0.4702118638735022, 'validation/accuracy': 0.9869785904884338, 'validation/loss': 0.043873559683561325, 'validation/mean_average_precision': 0.285625691321616, 'validation/num_examples': 43793, 'test/accuracy': 0.9860352277755737, 'test/loss': 0.04697255417704582, 'test/mean_average_precision': 0.2679272339349347, 'test/num_examples': 43793, 'score': 6977.031692266464, 'total_duration': 10102.648232460022, 'accumulated_submission_time': 6977.031692266464, 'accumulated_eval_time': 3124.1582732200623, 'accumulated_logging_time': 0.8541834354400635, 'global_step': 21713, 'preemption_count': 0}), (22456, {'train/accuracy': 0.9913394451141357, 'train/loss': 0.028038788586854935, 'train/mean_average_precision': 0.4604188786782695, 'validation/accuracy': 0.9869635701179504, 'validation/loss': 0.04374273493885994, 'validation/mean_average_precision': 0.28277668192121996, 'validation/num_examples': 43793, 'test/accuracy': 0.9862037301063538, 'test/loss': 0.04664532467722893, 'test/mean_average_precision': 0.2640839126761976, 'test/num_examples': 43793, 'score': 7217.031193494797, 'total_duration': 10448.873989105225, 'accumulated_submission_time': 7217.031193494797, 'accumulated_eval_time': 3230.3336248397827, 'accumulated_logging_time': 0.8844475746154785, 'global_step': 22456, 'preemption_count': 0}), (23204, {'train/accuracy': 0.9916256070137024, 'train/loss': 0.02725934237241745, 'train/mean_average_precision': 0.4747280372458277, 'validation/accuracy': 0.9869043231010437, 'validation/loss': 0.044060613960027695, 'validation/mean_average_precision': 0.28456510734483886, 'validation/num_examples': 43793, 'test/accuracy': 0.986123263835907, 'test/loss': 0.04690439626574516, 'test/mean_average_precision': 0.26278994052956384, 'test/num_examples': 43793, 'score': 7457.124490976334, 'total_duration': 10790.28831577301, 'accumulated_submission_time': 7457.124490976334, 'accumulated_eval_time': 3331.6023383140564, 'accumulated_logging_time': 0.9160325527191162, 'global_step': 23204, 'preemption_count': 0}), (23956, {'train/accuracy': 0.9913517236709595, 'train/loss': 0.028187081217765808, 'train/mean_average_precision': 0.46011791682661285, 'validation/accuracy': 0.9868783354759216, 'validation/loss': 0.04426131770014763, 'validation/mean_average_precision': 0.280703321938136, 'validation/num_examples': 43793, 'test/accuracy': 0.985987663269043, 'test/loss': 0.047034889459609985, 'test/mean_average_precision': 0.26449449777889183, 'test/num_examples': 43793, 'score': 7697.121632099152, 'total_duration': 11135.16293668747, 'accumulated_submission_time': 7697.121632099152, 'accumulated_eval_time': 3436.4192943573, 'accumulated_logging_time': 0.9561116695404053, 'global_step': 23956, 'preemption_count': 0}), (24711, {'train/accuracy': 0.9913532137870789, 'train/loss': 0.02813563123345375, 'train/mean_average_precision': 0.45984555097949037, 'validation/accuracy': 0.9870455861091614, 'validation/loss': 0.0438859760761261, 'validation/mean_average_precision': 0.28833704762538204, 'validation/num_examples': 43793, 'test/accuracy': 0.9862121343612671, 'test/loss': 0.04682972654700279, 'test/mean_average_precision': 0.26687861603947516, 'test/num_examples': 43793, 'score': 7937.132448196411, 'total_duration': 11479.138907432556, 'accumulated_submission_time': 7937.132448196411, 'accumulated_eval_time': 3540.332468509674, 'accumulated_logging_time': 0.987633228302002, 'global_step': 24711, 'preemption_count': 0}), (25462, {'train/accuracy': 0.991429328918457, 'train/loss': 0.027779962867498398, 'train/mean_average_precision': 0.46719259355084936, 'validation/accuracy': 0.9870439767837524, 'validation/loss': 0.04369185119867325, 'validation/mean_average_precision': 0.28888979629004624, 'validation/num_examples': 43793, 'test/accuracy': 0.9861923456192017, 'test/loss': 0.04665488749742508, 'test/mean_average_precision': 0.2683779409036351, 'test/num_examples': 43793, 'score': 8177.254436969757, 'total_duration': 11820.850269317627, 'accumulated_submission_time': 8177.254436969757, 'accumulated_eval_time': 3641.869652032852, 'accumulated_logging_time': 1.0197508335113525, 'global_step': 25462, 'preemption_count': 0}), (26214, {'train/accuracy': 0.991330623626709, 'train/loss': 0.028105879202485085, 'train/mean_average_precision': 0.4568819825250968, 'validation/accuracy': 0.9869639873504639, 'validation/loss': 0.04388206824660301, 'validation/mean_average_precision': 0.28561856725839535, 'validation/num_examples': 43793, 'test/accuracy': 0.9861291646957397, 'test/loss': 0.04659854993224144, 'test/mean_average_precision': 0.26752011398182646, 'test/num_examples': 43793, 'score': 8417.495745420456, 'total_duration': 12164.87309885025, 'accumulated_submission_time': 8417.495745420456, 'accumulated_eval_time': 3745.5998179912567, 'accumulated_logging_time': 1.0510282516479492, 'global_step': 26214, 'preemption_count': 0}), (26964, {'train/accuracy': 0.9913989305496216, 'train/loss': 0.027905093505978584, 'train/mean_average_precision': 0.4681628812771156, 'validation/accuracy': 0.9868897199630737, 'validation/loss': 0.043874591588974, 'validation/mean_average_precision': 0.2818689651514881, 'validation/num_examples': 43793, 'test/accuracy': 0.986011266708374, 'test/loss': 0.04663465917110443, 'test/mean_average_precision': 0.2703870400534875, 'test/num_examples': 43793, 'score': 8657.630143404007, 'total_duration': 12506.092809200287, 'accumulated_submission_time': 8657.630143404007, 'accumulated_eval_time': 3846.632305622101, 'accumulated_logging_time': 1.0833914279937744, 'global_step': 26964, 'preemption_count': 0}), (27709, {'train/accuracy': 0.9916355013847351, 'train/loss': 0.027098597958683968, 'train/mean_average_precision': 0.4803630402502638, 'validation/accuracy': 0.986885666847229, 'validation/loss': 0.04390691965818405, 'validation/mean_average_precision': 0.28674508422373507, 'validation/num_examples': 43793, 'test/accuracy': 0.9860866665840149, 'test/loss': 0.046626582741737366, 'test/mean_average_precision': 0.26376158400125727, 'test/num_examples': 43793, 'score': 8897.8357629776, 'total_duration': 12849.571416378021, 'accumulated_submission_time': 8897.8357629776, 'accumulated_eval_time': 3949.8536190986633, 'accumulated_logging_time': 1.1148099899291992, 'global_step': 27709, 'preemption_count': 0}), (28458, {'train/accuracy': 0.9917035102844238, 'train/loss': 0.026701265946030617, 'train/mean_average_precision': 0.49122608416394264, 'validation/accuracy': 0.9870139360427856, 'validation/loss': 0.043779049068689346, 'validation/mean_average_precision': 0.2853140712726124, 'validation/num_examples': 43793, 'test/accuracy': 0.9860950708389282, 'test/loss': 0.04675459489226341, 'test/mean_average_precision': 0.2691075067365001, 'test/num_examples': 43793, 'score': 9137.999815702438, 'total_duration': 13191.534855604172, 'accumulated_submission_time': 9137.999815702438, 'accumulated_eval_time': 4051.5991084575653, 'accumulated_logging_time': 1.148134708404541, 'global_step': 28458, 'preemption_count': 0}), (29204, {'train/accuracy': 0.9919525980949402, 'train/loss': 0.025991961359977722, 'train/mean_average_precision': 0.5057251405123364, 'validation/accuracy': 0.9870707392692566, 'validation/loss': 0.04379204288125038, 'validation/mean_average_precision': 0.290256895570826, 'validation/num_examples': 43793, 'test/accuracy': 0.986171305179596, 'test/loss': 0.046700723469257355, 'test/mean_average_precision': 0.2694446762523158, 'test/num_examples': 43793, 'score': 9378.210587263107, 'total_duration': 13533.442611694336, 'accumulated_submission_time': 9378.210587263107, 'accumulated_eval_time': 4153.242586612701, 'accumulated_logging_time': 1.1815519332885742, 'global_step': 29204, 'preemption_count': 0}), (29961, {'train/accuracy': 0.9919412136077881, 'train/loss': 0.025817345827817917, 'train/mean_average_precision': 0.5112644960667561, 'validation/accuracy': 0.9870638251304626, 'validation/loss': 0.043964121490716934, 'validation/mean_average_precision': 0.289960550844474, 'validation/num_examples': 43793, 'test/accuracy': 0.9861283302307129, 'test/loss': 0.04714837670326233, 'test/mean_average_precision': 0.27247323731981765, 'test/num_examples': 43793, 'score': 9618.264122486115, 'total_duration': 13880.021289348602, 'accumulated_submission_time': 9618.264122486115, 'accumulated_eval_time': 4259.714983224869, 'accumulated_logging_time': 1.2135634422302246, 'global_step': 29961, 'preemption_count': 0}), (30714, {'train/accuracy': 0.9917917847633362, 'train/loss': 0.02630518563091755, 'train/mean_average_precision': 0.5038807477785993, 'validation/accuracy': 0.9869644045829773, 'validation/loss': 0.0440908707678318, 'validation/mean_average_precision': 0.2894453883646158, 'validation/num_examples': 43793, 'test/accuracy': 0.9860677123069763, 'test/loss': 0.04717359319329262, 'test/mean_average_precision': 0.2675294987951833, 'test/num_examples': 43793, 'score': 9858.291661024094, 'total_duration': 14224.340274810791, 'accumulated_submission_time': 9858.291661024094, 'accumulated_eval_time': 4363.952226638794, 'accumulated_logging_time': 1.246518850326538, 'global_step': 30714, 'preemption_count': 0}), (31458, {'train/accuracy': 0.9918238520622253, 'train/loss': 0.026440177112817764, 'train/mean_average_precision': 0.4987664840541888, 'validation/accuracy': 0.9869713187217712, 'validation/loss': 0.04376460611820221, 'validation/mean_average_precision': 0.2861938749291255, 'validation/num_examples': 43793, 'test/accuracy': 0.9861093759536743, 'test/loss': 0.04669422656297684, 'test/mean_average_precision': 0.2704962118586615, 'test/num_examples': 43793, 'score': 10098.319123268127, 'total_duration': 14571.425382375717, 'accumulated_submission_time': 10098.319123268127, 'accumulated_eval_time': 4470.956292152405, 'accumulated_logging_time': 1.2795672416687012, 'global_step': 31458, 'preemption_count': 0}), (32201, {'train/accuracy': 0.9917375445365906, 'train/loss': 0.026739291846752167, 'train/mean_average_precision': 0.48715062075297716, 'validation/accuracy': 0.9870427250862122, 'validation/loss': 0.043842099606990814, 'validation/mean_average_precision': 0.2848324278871966, 'validation/num_examples': 43793, 'test/accuracy': 0.9861093759536743, 'test/loss': 0.04696670547127724, 'test/mean_average_precision': 0.26856483920492724, 'test/num_examples': 43793, 'score': 10338.541726350784, 'total_duration': 14915.281131029129, 'accumulated_submission_time': 10338.541726350784, 'accumulated_eval_time': 4574.533927679062, 'accumulated_logging_time': 1.313849925994873, 'global_step': 32201, 'preemption_count': 0}), (32947, {'train/accuracy': 0.9917527437210083, 'train/loss': 0.026688046753406525, 'train/mean_average_precision': 0.49590078381363595, 'validation/accuracy': 0.9871665239334106, 'validation/loss': 0.044212136417627335, 'validation/mean_average_precision': 0.29305686914002177, 'validation/num_examples': 43793, 'test/accuracy': 0.9862593412399292, 'test/loss': 0.047123465687036514, 'test/mean_average_precision': 0.2710033452222284, 'test/num_examples': 43793, 'score': 10578.52193403244, 'total_duration': 15257.13302230835, 'accumulated_submission_time': 10578.52193403244, 'accumulated_eval_time': 4676.352701425552, 'accumulated_logging_time': 1.3461999893188477, 'global_step': 32947, 'preemption_count': 0}), (33704, {'train/accuracy': 0.9918839931488037, 'train/loss': 0.026062043383717537, 'train/mean_average_precision': 0.5066806571838988, 'validation/accuracy': 0.9870265126228333, 'validation/loss': 0.04397330805659294, 'validation/mean_average_precision': 0.28619253154944974, 'validation/num_examples': 43793, 'test/accuracy': 0.9861788749694824, 'test/loss': 0.04694005474448204, 'test/mean_average_precision': 0.27111568274384507, 'test/num_examples': 43793, 'score': 10818.582146406174, 'total_duration': 15598.397938489914, 'accumulated_submission_time': 10818.582146406174, 'accumulated_eval_time': 4777.501918077469, 'accumulated_logging_time': 1.3808777332305908, 'global_step': 33704, 'preemption_count': 0}), (34457, {'train/accuracy': 0.991776704788208, 'train/loss': 0.02636144682765007, 'train/mean_average_precision': 0.5051048917352821, 'validation/accuracy': 0.9870297312736511, 'validation/loss': 0.044214628636837006, 'validation/mean_average_precision': 0.290110529395656, 'validation/num_examples': 43793, 'test/accuracy': 0.9861131906509399, 'test/loss': 0.047194551676511765, 'test/mean_average_precision': 0.2731183121212644, 'test/num_examples': 43793, 'score': 11058.546568632126, 'total_duration': 15939.182308912277, 'accumulated_submission_time': 11058.546568632126, 'accumulated_eval_time': 4878.264587640762, 'accumulated_logging_time': 1.417754888534546, 'global_step': 34457, 'preemption_count': 0}), (35208, {'train/accuracy': 0.9921960234642029, 'train/loss': 0.025136025622487068, 'train/mean_average_precision': 0.5219924353902371, 'validation/accuracy': 0.9870204329490662, 'validation/loss': 0.04393310472369194, 'validation/mean_average_precision': 0.29234085819008016, 'validation/num_examples': 43793, 'test/accuracy': 0.9862593412399292, 'test/loss': 0.0469382219016552, 'test/mean_average_precision': 0.2796964435428016, 'test/num_examples': 43793, 'score': 11298.666824102402, 'total_duration': 16282.887593269348, 'accumulated_submission_time': 11298.666824102402, 'accumulated_eval_time': 4981.795501708984, 'accumulated_logging_time': 1.4510157108306885, 'global_step': 35208, 'preemption_count': 0}), (35951, {'train/accuracy': 0.9921252131462097, 'train/loss': 0.02500178851187229, 'train/mean_average_precision': 0.5476866748148286, 'validation/accuracy': 0.9870662689208984, 'validation/loss': 0.044470176100730896, 'validation/mean_average_precision': 0.2893286274359, 'validation/num_examples': 43793, 'test/accuracy': 0.9862323999404907, 'test/loss': 0.047462280839681625, 'test/mean_average_precision': 0.2696253351743681, 'test/num_examples': 43793, 'score': 11538.917538404465, 'total_duration': 16621.011003494263, 'accumulated_submission_time': 11538.917538404465, 'accumulated_eval_time': 5079.614475250244, 'accumulated_logging_time': 1.4840147495269775, 'global_step': 35951, 'preemption_count': 0}), (36702, {'train/accuracy': 0.9924894571304321, 'train/loss': 0.024061957374215126, 'train/mean_average_precision': 0.5486724927922778, 'validation/accuracy': 0.987052857875824, 'validation/loss': 0.04396612569689751, 'validation/mean_average_precision': 0.29082902684215756, 'validation/num_examples': 43793, 'test/accuracy': 0.9861683249473572, 'test/loss': 0.04701868072152138, 'test/mean_average_precision': 0.2739539266716618, 'test/num_examples': 43793, 'score': 11779.103993415833, 'total_duration': 16962.328053236008, 'accumulated_submission_time': 11779.103993415833, 'accumulated_eval_time': 5180.689831018448, 'accumulated_logging_time': 1.5187046527862549, 'global_step': 36702, 'preemption_count': 0}), (37449, {'train/accuracy': 0.9924684762954712, 'train/loss': 0.02407103404402733, 'train/mean_average_precision': 0.5645409408952597, 'validation/accuracy': 0.9871202707290649, 'validation/loss': 0.044161438941955566, 'validation/mean_average_precision': 0.2904596654457011, 'validation/num_examples': 43793, 'test/accuracy': 0.9862315058708191, 'test/loss': 0.0470658540725708, 'test/mean_average_precision': 0.27402600030755137, 'test/num_examples': 43793, 'score': 12019.094329595566, 'total_duration': 17305.666412353516, 'accumulated_submission_time': 12019.094329595566, 'accumulated_eval_time': 5283.982161521912, 'accumulated_logging_time': 1.553828477859497, 'global_step': 37449, 'preemption_count': 0}), (38196, {'train/accuracy': 0.9922829866409302, 'train/loss': 0.024626431986689568, 'train/mean_average_precision': 0.533122242307951, 'validation/accuracy': 0.9870577454566956, 'validation/loss': 0.044651780277490616, 'validation/mean_average_precision': 0.2929242246556005, 'validation/num_examples': 43793, 'test/accuracy': 0.9862298369407654, 'test/loss': 0.0478082112967968, 'test/mean_average_precision': 0.2720654290337988, 'test/num_examples': 43793, 'score': 12259.080934286118, 'total_duration': 17646.517784833908, 'accumulated_submission_time': 12259.080934286118, 'accumulated_eval_time': 5384.793403148651, 'accumulated_logging_time': 1.5871310234069824, 'global_step': 38196, 'preemption_count': 0}), (38952, {'train/accuracy': 0.9922152757644653, 'train/loss': 0.024960581213235855, 'train/mean_average_precision': 0.5255462330459101, 'validation/accuracy': 0.986975371837616, 'validation/loss': 0.044149454683065414, 'validation/mean_average_precision': 0.2922564708468797, 'validation/num_examples': 43793, 'test/accuracy': 0.9861696362495422, 'test/loss': 0.04713044315576553, 'test/mean_average_precision': 0.27499161874293837, 'test/num_examples': 43793, 'score': 12499.300460100174, 'total_duration': 17987.14372611046, 'accumulated_submission_time': 12499.300460100174, 'accumulated_eval_time': 5485.143780708313, 'accumulated_logging_time': 1.6230242252349854, 'global_step': 38952, 'preemption_count': 0}), (39710, {'train/accuracy': 0.9921615123748779, 'train/loss': 0.02508951537311077, 'train/mean_average_precision': 0.5311539849439371, 'validation/accuracy': 0.9870626330375671, 'validation/loss': 0.044394511729478836, 'validation/mean_average_precision': 0.29393925036085516, 'validation/num_examples': 43793, 'test/accuracy': 0.9861927628517151, 'test/loss': 0.047573503106832504, 'test/mean_average_precision': 0.2701267080078491, 'test/num_examples': 43793, 'score': 12739.280121326447, 'total_duration': 18324.382556915283, 'accumulated_submission_time': 12739.280121326447, 'accumulated_eval_time': 5582.348515987396, 'accumulated_logging_time': 1.6569876670837402, 'global_step': 39710, 'preemption_count': 0}), (40468, {'train/accuracy': 0.9923699498176575, 'train/loss': 0.024582475423812866, 'train/mean_average_precision': 0.5457115756354987, 'validation/accuracy': 0.9870626330375671, 'validation/loss': 0.04397698864340782, 'validation/mean_average_precision': 0.29008335261060875, 'validation/num_examples': 43793, 'test/accuracy': 0.9862176179885864, 'test/loss': 0.04706680774688721, 'test/mean_average_precision': 0.2757043997765046, 'test/num_examples': 43793, 'score': 12979.375619888306, 'total_duration': 18663.341647863388, 'accumulated_submission_time': 12979.375619888306, 'accumulated_eval_time': 5681.157953500748, 'accumulated_logging_time': 1.6909382343292236, 'global_step': 40468, 'preemption_count': 0}), (41216, {'train/accuracy': 0.9924188256263733, 'train/loss': 0.02421451173722744, 'train/mean_average_precision': 0.5426725337847191, 'validation/accuracy': 0.9870244860649109, 'validation/loss': 0.044272493571043015, 'validation/mean_average_precision': 0.290808543338558, 'validation/num_examples': 43793, 'test/accuracy': 0.9861923456192017, 'test/loss': 0.04715505987405777, 'test/mean_average_precision': 0.2781001258652558, 'test/num_examples': 43793, 'score': 13219.553693056107, 'total_duration': 19015.69958639145, 'accumulated_submission_time': 13219.553693056107, 'accumulated_eval_time': 5793.282270669937, 'accumulated_logging_time': 1.726207971572876, 'global_step': 41216, 'preemption_count': 0}), (41962, {'train/accuracy': 0.9924597144126892, 'train/loss': 0.023983314633369446, 'train/mean_average_precision': 0.5485732064088567, 'validation/accuracy': 0.9870756268501282, 'validation/loss': 0.0444074310362339, 'validation/mean_average_precision': 0.29814886448259775, 'validation/num_examples': 43793, 'test/accuracy': 0.9861965775489807, 'test/loss': 0.04757916182279587, 'test/mean_average_precision': 0.2741416501901633, 'test/num_examples': 43793, 'score': 13459.66164469719, 'total_duration': 19357.54460954666, 'accumulated_submission_time': 13459.66164469719, 'accumulated_eval_time': 5894.937569618225, 'accumulated_logging_time': 1.7861547470092773, 'global_step': 41962, 'preemption_count': 0}), (42711, {'train/accuracy': 0.9925682544708252, 'train/loss': 0.02357172966003418, 'train/mean_average_precision': 0.5703393827897597, 'validation/accuracy': 0.9870427250862122, 'validation/loss': 0.04432450234889984, 'validation/mean_average_precision': 0.2951427843127839, 'validation/num_examples': 43793, 'test/accuracy': 0.9863145351409912, 'test/loss': 0.04708156734704971, 'test/mean_average_precision': 0.2796231683478654, 'test/num_examples': 43793, 'score': 13699.857156276703, 'total_duration': 19701.188717842102, 'accumulated_submission_time': 13699.857156276703, 'accumulated_eval_time': 5998.332057952881, 'accumulated_logging_time': 1.8204026222229004, 'global_step': 42711, 'preemption_count': 0}), (43463, {'train/accuracy': 0.9928522109985352, 'train/loss': 0.022685980424284935, 'train/mean_average_precision': 0.587405296771667, 'validation/accuracy': 0.987025260925293, 'validation/loss': 0.04483186826109886, 'validation/mean_average_precision': 0.29264353230880247, 'validation/num_examples': 43793, 'test/accuracy': 0.986214280128479, 'test/loss': 0.04800967872142792, 'test/mean_average_precision': 0.2711062177589242, 'test/num_examples': 43793, 'score': 13939.90434885025, 'total_duration': 20042.147089719772, 'accumulated_submission_time': 13939.90434885025, 'accumulated_eval_time': 6099.189184188843, 'accumulated_logging_time': 1.8538603782653809, 'global_step': 43463, 'preemption_count': 0}), (44220, {'train/accuracy': 0.9930338859558105, 'train/loss': 0.022224726155400276, 'train/mean_average_precision': 0.6030117376077448, 'validation/accuracy': 0.9871328473091125, 'validation/loss': 0.04474617540836334, 'validation/mean_average_precision': 0.2994731789222761, 'validation/num_examples': 43793, 'test/accuracy': 0.9862829446792603, 'test/loss': 0.0478932149708271, 'test/mean_average_precision': 0.27737578476760955, 'test/num_examples': 43793, 'score': 14180.02293419838, 'total_duration': 20386.546948194504, 'accumulated_submission_time': 14180.02293419838, 'accumulated_eval_time': 6203.414827346802, 'accumulated_logging_time': 1.8894824981689453, 'global_step': 44220, 'preemption_count': 0}), (44972, {'train/accuracy': 0.9930641651153564, 'train/loss': 0.02208307757973671, 'train/mean_average_precision': 0.6060911317596043, 'validation/accuracy': 0.987050473690033, 'validation/loss': 0.04487309977412224, 'validation/mean_average_precision': 0.2943794044568053, 'validation/num_examples': 43793, 'test/accuracy': 0.9862542748451233, 'test/loss': 0.04808291420340538, 'test/mean_average_precision': 0.27775466673846316, 'test/num_examples': 43793, 'score': 14420.104619264603, 'total_duration': 20725.25773501396, 'accumulated_submission_time': 14420.104619264603, 'accumulated_eval_time': 6301.98726439476, 'accumulated_logging_time': 1.9259374141693115, 'global_step': 44972, 'preemption_count': 0}), (45724, {'train/accuracy': 0.993030309677124, 'train/loss': 0.02232307381927967, 'train/mean_average_precision': 0.5868794150076238, 'validation/accuracy': 0.9870455861091614, 'validation/loss': 0.04456338658928871, 'validation/mean_average_precision': 0.2950067238532902, 'validation/num_examples': 43793, 'test/accuracy': 0.9862766265869141, 'test/loss': 0.04733329638838768, 'test/mean_average_precision': 0.2744227430076576, 'test/num_examples': 43793, 'score': 14660.14389538765, 'total_duration': 21068.20579099655, 'accumulated_submission_time': 14660.14389538765, 'accumulated_eval_time': 6404.840629816055, 'accumulated_logging_time': 1.961230993270874, 'global_step': 45724, 'preemption_count': 0}), (46471, {'train/accuracy': 0.9928067922592163, 'train/loss': 0.02292061410844326, 'train/mean_average_precision': 0.5767878589178458, 'validation/accuracy': 0.9869668483734131, 'validation/loss': 0.04495652765035629, 'validation/mean_average_precision': 0.2892812244489832, 'validation/num_examples': 43793, 'test/accuracy': 0.9861570000648499, 'test/loss': 0.0479879155755043, 'test/mean_average_precision': 0.27575870220349497, 'test/num_examples': 43793, 'score': 14900.095911979675, 'total_duration': 21412.446129083633, 'accumulated_submission_time': 14900.095911979675, 'accumulated_eval_time': 6509.072257757187, 'accumulated_logging_time': 1.9975545406341553, 'global_step': 46471, 'preemption_count': 0}), (47223, {'train/accuracy': 0.9927178621292114, 'train/loss': 0.02314264327287674, 'train/mean_average_precision': 0.5750837991380785, 'validation/accuracy': 0.9870151281356812, 'validation/loss': 0.0452580600976944, 'validation/mean_average_precision': 0.28985987010866954, 'validation/num_examples': 43793, 'test/accuracy': 0.9861999154090881, 'test/loss': 0.04842248931527138, 'test/mean_average_precision': 0.26999446926627463, 'test/num_examples': 43793, 'score': 15140.262208938599, 'total_duration': 21756.26882982254, 'accumulated_submission_time': 15140.262208938599, 'accumulated_eval_time': 6612.673577547073, 'accumulated_logging_time': 2.0322277545928955, 'global_step': 47223, 'preemption_count': 0}), (47963, {'train/accuracy': 0.9928799867630005, 'train/loss': 0.02256070077419281, 'train/mean_average_precision': 0.5870601530022526, 'validation/accuracy': 0.9871381521224976, 'validation/loss': 0.04508497565984726, 'validation/mean_average_precision': 0.2975629992554879, 'validation/num_examples': 43793, 'test/accuracy': 0.9862197637557983, 'test/loss': 0.04828479886054993, 'test/mean_average_precision': 0.27362271948857514, 'test/num_examples': 43793, 'score': 15380.479937553406, 'total_duration': 22098.51292657852, 'accumulated_submission_time': 15380.479937553406, 'accumulated_eval_time': 6714.644959449768, 'accumulated_logging_time': 2.0670578479766846, 'global_step': 47963, 'preemption_count': 0}), (48705, {'train/accuracy': 0.992908239364624, 'train/loss': 0.022484946995973587, 'train/mean_average_precision': 0.5866123590096386, 'validation/accuracy': 0.9870979189872742, 'validation/loss': 0.04502173885703087, 'validation/mean_average_precision': 0.2982286863077764, 'validation/num_examples': 43793, 'test/accuracy': 0.9861965775489807, 'test/loss': 0.048440057784318924, 'test/mean_average_precision': 0.26824022040142503, 'test/num_examples': 43793, 'score': 15620.667268753052, 'total_duration': 22443.443969726562, 'accumulated_submission_time': 15620.667268753052, 'accumulated_eval_time': 6819.330502986908, 'accumulated_logging_time': 2.104949951171875, 'global_step': 48705, 'preemption_count': 0}), (49454, {'train/accuracy': 0.9932429790496826, 'train/loss': 0.021441875025629997, 'train/mean_average_precision': 0.6142716018141637, 'validation/accuracy': 0.9870386719703674, 'validation/loss': 0.044959813356399536, 'validation/mean_average_precision': 0.2990870103980558, 'validation/num_examples': 43793, 'test/accuracy': 0.9861738085746765, 'test/loss': 0.048262231051921844, 'test/mean_average_precision': 0.2745028253297134, 'test/num_examples': 43793, 'score': 15860.90509223938, 'total_duration': 22782.647591114044, 'accumulated_submission_time': 15860.90509223938, 'accumulated_eval_time': 6918.238473892212, 'accumulated_logging_time': 2.142441749572754, 'global_step': 49454, 'preemption_count': 0}), (50198, {'train/accuracy': 0.993171751499176, 'train/loss': 0.021398043259978294, 'train/mean_average_precision': 0.6153558358088966, 'validation/accuracy': 0.9871393442153931, 'validation/loss': 0.045644935220479965, 'validation/mean_average_precision': 0.30043306845029527, 'validation/num_examples': 43793, 'test/accuracy': 0.9862896800041199, 'test/loss': 0.04900442436337471, 'test/mean_average_precision': 0.2771777891807733, 'test/num_examples': 43793, 'score': 16101.125497341156, 'total_duration': 23125.086805820465, 'accumulated_submission_time': 16101.125497341156, 'accumulated_eval_time': 7020.40039730072, 'accumulated_logging_time': 2.1796648502349854, 'global_step': 50198, 'preemption_count': 0}), (50942, {'train/accuracy': 0.9934930801391602, 'train/loss': 0.02055634744465351, 'train/mean_average_precision': 0.6322967831703095, 'validation/accuracy': 0.9871681928634644, 'validation/loss': 0.045472003519535065, 'validation/mean_average_precision': 0.29601422460727383, 'validation/num_examples': 43793, 'test/accuracy': 0.986243724822998, 'test/loss': 0.04872037097811699, 'test/mean_average_precision': 0.27748623702265135, 'test/num_examples': 43793, 'score': 16341.301450490952, 'total_duration': 23464.592199087143, 'accumulated_submission_time': 16341.301450490952, 'accumulated_eval_time': 7119.6732885837555, 'accumulated_logging_time': 2.215346097946167, 'global_step': 50942, 'preemption_count': 0}), (51688, {'train/accuracy': 0.9936957955360413, 'train/loss': 0.020005319267511368, 'train/mean_average_precision': 0.6529153022018896, 'validation/accuracy': 0.9870293140411377, 'validation/loss': 0.045481979846954346, 'validation/mean_average_precision': 0.29905867568132427, 'validation/num_examples': 43793, 'test/accuracy': 0.9861788749694824, 'test/loss': 0.04864748939871788, 'test/mean_average_precision': 0.2764061666842379, 'test/num_examples': 43793, 'score': 16581.445430278778, 'total_duration': 23808.160900831223, 'accumulated_submission_time': 16581.445430278778, 'accumulated_eval_time': 7223.04040813446, 'accumulated_logging_time': 2.2525784969329834, 'global_step': 51688, 'preemption_count': 0}), (52442, {'train/accuracy': 0.9935854077339172, 'train/loss': 0.020275061950087547, 'train/mean_average_precision': 0.6460475938094383, 'validation/accuracy': 0.987106442451477, 'validation/loss': 0.04580055549740791, 'validation/mean_average_precision': 0.3005454513036418, 'validation/num_examples': 43793, 'test/accuracy': 0.9862736463546753, 'test/loss': 0.04913301393389702, 'test/mean_average_precision': 0.27781747098487997, 'test/num_examples': 43793, 'score': 16821.530868291855, 'total_duration': 24153.16040802002, 'accumulated_submission_time': 16821.530868291855, 'accumulated_eval_time': 7327.888458013535, 'accumulated_logging_time': 2.2985892295837402, 'global_step': 52442, 'preemption_count': 0}), (53189, {'train/accuracy': 0.9934902787208557, 'train/loss': 0.020645245909690857, 'train/mean_average_precision': 0.6349567310737267, 'validation/accuracy': 0.9871125817298889, 'validation/loss': 0.04573584347963333, 'validation/mean_average_precision': 0.29954231350914534, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.049246788024902344, 'test/mean_average_precision': 0.2722941436238601, 'test/num_examples': 43793, 'score': 17061.780789375305, 'total_duration': 24492.081380605698, 'accumulated_submission_time': 17061.780789375305, 'accumulated_eval_time': 7426.502200841904, 'accumulated_logging_time': 2.3355820178985596, 'global_step': 53189, 'preemption_count': 0}), (53935, {'train/accuracy': 0.9932562112808228, 'train/loss': 0.02114921435713768, 'train/mean_average_precision': 0.6126243232067949, 'validation/accuracy': 0.9869729280471802, 'validation/loss': 0.04588203504681587, 'validation/mean_average_precision': 0.2990511593052038, 'validation/num_examples': 43793, 'test/accuracy': 0.9861367344856262, 'test/loss': 0.04906720295548439, 'test/mean_average_precision': 0.27791338493800183, 'test/num_examples': 43793, 'score': 17301.95036458969, 'total_duration': 24836.868192195892, 'accumulated_submission_time': 17301.95036458969, 'accumulated_eval_time': 7531.053488969803, 'accumulated_logging_time': 2.3819081783294678, 'global_step': 53935, 'preemption_count': 0}), (54684, {'train/accuracy': 0.9933159947395325, 'train/loss': 0.02094193734228611, 'train/mean_average_precision': 0.6317724607619654, 'validation/accuracy': 0.987216055393219, 'validation/loss': 0.04619571194052696, 'validation/mean_average_precision': 0.2959647151213805, 'validation/num_examples': 43793, 'test/accuracy': 0.9862450361251831, 'test/loss': 0.04962007701396942, 'test/mean_average_precision': 0.27874735065386286, 'test/num_examples': 43793, 'score': 17542.20799946785, 'total_duration': 25179.772585392, 'accumulated_submission_time': 17542.20799946785, 'accumulated_eval_time': 7633.643043994904, 'accumulated_logging_time': 2.4187357425689697, 'global_step': 54684, 'preemption_count': 0}), (55434, {'train/accuracy': 0.9935021996498108, 'train/loss': 0.02031290903687477, 'train/mean_average_precision': 0.6375259346868289, 'validation/accuracy': 0.9871361255645752, 'validation/loss': 0.04621044546365738, 'validation/mean_average_precision': 0.29952755101861744, 'validation/num_examples': 43793, 'test/accuracy': 0.9863107204437256, 'test/loss': 0.049470916390419006, 'test/mean_average_precision': 0.277157897267368, 'test/num_examples': 43793, 'score': 17782.284705638885, 'total_duration': 25523.43450474739, 'accumulated_submission_time': 17782.284705638885, 'accumulated_eval_time': 7737.169899225235, 'accumulated_logging_time': 2.4568605422973633, 'global_step': 55434, 'preemption_count': 0}), (56177, {'train/accuracy': 0.9936658143997192, 'train/loss': 0.019820721819996834, 'train/mean_average_precision': 0.6528672863038035, 'validation/accuracy': 0.9871364831924438, 'validation/loss': 0.046166084706783295, 'validation/mean_average_precision': 0.30250379337983463, 'validation/num_examples': 43793, 'test/accuracy': 0.9862247705459595, 'test/loss': 0.04958441108465195, 'test/mean_average_precision': 0.2745376159234082, 'test/num_examples': 43793, 'score': 18022.405507564545, 'total_duration': 25868.13215303421, 'accumulated_submission_time': 18022.405507564545, 'accumulated_eval_time': 7841.688732147217, 'accumulated_logging_time': 2.4949471950531006, 'global_step': 56177, 'preemption_count': 0}), (56924, {'train/accuracy': 0.9937217235565186, 'train/loss': 0.019551070407032967, 'train/mean_average_precision': 0.6607373302507992, 'validation/accuracy': 0.9871413707733154, 'validation/loss': 0.04628141224384308, 'validation/mean_average_precision': 0.3001707093408799, 'validation/num_examples': 43793, 'test/accuracy': 0.9862277507781982, 'test/loss': 0.04944280534982681, 'test/mean_average_precision': 0.27963990690568774, 'test/num_examples': 43793, 'score': 18262.658086299896, 'total_duration': 26208.75817298889, 'accumulated_submission_time': 18262.658086299896, 'accumulated_eval_time': 7941.999848365784, 'accumulated_logging_time': 2.5373575687408447, 'global_step': 56924, 'preemption_count': 0})], 'global_step': 57590}
I0206 10:54:23.213981 140205209478976 submission_runner.py:586] Timing: 18477.211141824722
I0206 10:54:23.214040 140205209478976 submission_runner.py:588] Total number of evals: 77
I0206 10:54:23.214084 140205209478976 submission_runner.py:589] ====================
I0206 10:54:23.216730 140205209478976 submission_runner.py:673] Final ogbg score: 18477.03369784355
