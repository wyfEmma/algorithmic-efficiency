python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=1618809895 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_02-14-2024-01-22-23.log
I0214 01:22:43.871513 139688679413568 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax.
I0214 01:22:44.927492 139688679413568 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0214 01:22:44.928672 139688679413568 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0214 01:22:44.928817 139688679413568 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0214 01:22:44.930093 139688679413568 submission_runner.py:542] Using RNG seed 1618809895
I0214 01:22:46.084530 139688679413568 submission_runner.py:551] --- Tuning run 1/5 ---
I0214 01:22:46.084729 139688679413568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_1.
I0214 01:22:46.085092 139688679413568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_1/hparams.json.
I0214 01:22:46.272248 139688679413568 submission_runner.py:206] Initializing dataset.
I0214 01:22:46.272463 139688679413568 submission_runner.py:213] Initializing model.
I0214 01:22:51.166112 139688679413568 submission_runner.py:255] Initializing optimizer.
I0214 01:22:52.424766 139688679413568 submission_runner.py:262] Initializing metrics bundle.
I0214 01:22:52.424969 139688679413568 submission_runner.py:280] Initializing checkpoint and logger.
I0214 01:22:52.426266 139688679413568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0214 01:22:52.426420 139688679413568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0214 01:22:52.426623 139688679413568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0214 01:22:52.426688 139688679413568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0214 01:22:52.730557 139688679413568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0214 01:22:53.004678 139688679413568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_1/flags_0.json.
I0214 01:22:53.019968 139688679413568 submission_runner.py:314] Starting training loop.
I0214 01:22:53.313838 139688679413568 input_pipeline.py:20] Loading split = train-clean-100
I0214 01:22:53.351563 139688679413568 input_pipeline.py:20] Loading split = train-clean-360
I0214 01:22:53.769334 139688679413568 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0214 01:23:54.939508 139514622240512 logging_writer.py:48] [0] global_step=0, grad_norm=54.19961929321289, loss=30.951793670654297
I0214 01:23:54.980074 139688679413568 spec.py:321] Evaluating on the training split.
I0214 01:23:55.154250 139688679413568 input_pipeline.py:20] Loading split = train-clean-100
I0214 01:23:55.189949 139688679413568 input_pipeline.py:20] Loading split = train-clean-360
I0214 01:23:55.587451 139688679413568 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0214 01:24:57.634253 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 01:24:57.749235 139688679413568 input_pipeline.py:20] Loading split = dev-clean
I0214 01:24:57.754404 139688679413568 input_pipeline.py:20] Loading split = dev-other
I0214 01:25:57.799297 139688679413568 spec.py:349] Evaluating on the test split.
I0214 01:25:57.915814 139688679413568 input_pipeline.py:20] Loading split = test-clean
I0214 01:26:33.193864 139688679413568 submission_runner.py:408] Time since start: 220.17s, 	Step: 1, 	{'train/ctc_loss': Array(30.99089, dtype=float32), 'train/wer': 0.9441777898090499, 'validation/ctc_loss': Array(30.141817, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 61.96003007888794, 'total_duration': 220.17135214805603, 'accumulated_submission_time': 61.96003007888794, 'accumulated_eval_time': 158.21125888824463, 'accumulated_logging_time': 0}
I0214 01:26:33.220394 139503761454848 logging_writer.py:48] [1] accumulated_eval_time=158.211259, accumulated_logging_time=0, accumulated_submission_time=61.960030, global_step=1, preemption_count=0, score=61.960030, test/ctc_loss=30.20883560180664, test/num_examples=2472, test/wer=0.908598, total_duration=220.171352, train/ctc_loss=30.990890502929688, train/wer=0.944178, validation/ctc_loss=30.141817092895508, validation/num_examples=5348, validation/wer=0.904371
I0214 01:28:11.805761 139517173384960 logging_writer.py:48] [100] global_step=100, grad_norm=34.302101135253906, loss=10.59990406036377
I0214 01:29:28.179047 139517181777664 logging_writer.py:48] [200] global_step=200, grad_norm=2.9327569007873535, loss=6.205972671508789
I0214 01:30:44.503917 139517173384960 logging_writer.py:48] [300] global_step=300, grad_norm=1.0351027250289917, loss=5.85424280166626
I0214 01:32:02.094583 139517181777664 logging_writer.py:48] [400] global_step=400, grad_norm=0.2664346396923065, loss=5.832924842834473
I0214 01:33:27.569368 139517173384960 logging_writer.py:48] [500] global_step=500, grad_norm=0.257377564907074, loss=5.806849479675293
I0214 01:34:52.363481 139517181777664 logging_writer.py:48] [600] global_step=600, grad_norm=0.4364609122276306, loss=5.813186168670654
I0214 01:36:21.889645 139517173384960 logging_writer.py:48] [700] global_step=700, grad_norm=0.3815106153488159, loss=5.810729503631592
I0214 01:37:49.776502 139517181777664 logging_writer.py:48] [800] global_step=800, grad_norm=0.3034002184867859, loss=5.784765720367432
I0214 01:39:15.124793 139517173384960 logging_writer.py:48] [900] global_step=900, grad_norm=0.5479650497436523, loss=5.784335136413574
I0214 01:40:43.242191 139517181777664 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.3331882953643799, loss=5.78719425201416
I0214 01:42:04.771521 139517232133888 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.27945786714553833, loss=5.769336700439453
I0214 01:43:20.969636 139517223741184 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.23603041470050812, loss=5.795950412750244
I0214 01:44:37.410956 139517232133888 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.3765675723552704, loss=5.795531749725342
I0214 01:45:56.093505 139517223741184 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.37125781178474426, loss=5.784595489501953
I0214 01:47:16.834103 139517232133888 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.2653813064098358, loss=5.772824287414551
I0214 01:48:44.845893 139517223741184 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.9574066400527954, loss=5.728864669799805
I0214 01:50:11.574332 139517232133888 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7584992051124573, loss=5.585361480712891
I0214 01:50:33.947125 139688679413568 spec.py:321] Evaluating on the training split.
I0214 01:51:11.582123 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 01:51:57.717713 139688679413568 spec.py:349] Evaluating on the test split.
I0214 01:52:20.951254 139688679413568 submission_runner.py:408] Time since start: 1767.93s, 	Step: 1729, 	{'train/ctc_loss': Array(6.527625, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.5964723, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.5954204, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1502.605571269989, 'total_duration': 1767.9254868030548, 'accumulated_submission_time': 1502.605571269989, 'accumulated_eval_time': 265.20961356163025, 'accumulated_logging_time': 0.041916847229003906}
I0214 01:52:20.984301 139517887493888 logging_writer.py:48] [1729] accumulated_eval_time=265.209614, accumulated_logging_time=0.041917, accumulated_submission_time=1502.605571, global_step=1729, preemption_count=0, score=1502.605571, test/ctc_loss=6.5954203605651855, test/num_examples=2472, test/wer=0.899580, total_duration=1767.925487, train/ctc_loss=6.52762508392334, train/wer=0.944636, validation/ctc_loss=6.596472263336182, validation/num_examples=5348, validation/wer=0.896618
I0214 01:53:15.843645 139517879101184 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7693156599998474, loss=5.499836444854736
I0214 01:54:32.177149 139517887493888 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8857636451721191, loss=5.289404392242432
I0214 01:55:50.863548 139517879101184 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9054967164993286, loss=4.727147579193115
I0214 01:57:16.873930 139518542853888 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.165263056755066, loss=4.067387580871582
I0214 01:58:33.130392 139518534461184 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.86118483543396, loss=3.8086166381835938
I0214 01:59:50.128042 139518542853888 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1050233840942383, loss=3.483325242996216
I0214 02:01:10.447780 139518534461184 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.1333309412002563, loss=3.4050841331481934
I0214 02:02:34.866347 139518542853888 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.941162645816803, loss=3.2035744190216064
I0214 02:04:02.046138 139518534461184 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9775840044021606, loss=3.0996298789978027
I0214 02:05:29.703398 139518542853888 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9635642766952515, loss=3.015256881713867
I0214 02:06:57.656534 139518534461184 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0476876497268677, loss=2.862222671508789
I0214 02:08:24.041071 139518542853888 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0897794961929321, loss=2.814951181411743
I0214 02:09:52.640875 139518534461184 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9158410429954529, loss=2.7392868995666504
I0214 02:11:19.890461 139518542853888 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9810250997543335, loss=2.75423526763916
I0214 02:12:35.969314 139518534461184 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0603007078170776, loss=2.6537327766418457
I0214 02:13:52.232643 139518542853888 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0164122581481934, loss=2.6215734481811523
I0214 02:15:08.471406 139518534461184 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.1874395608901978, loss=2.5731630325317383
I0214 02:16:21.376637 139688679413568 spec.py:321] Evaluating on the training split.
I0214 02:17:09.222463 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 02:17:58.202357 139688679413568 spec.py:349] Evaluating on the test split.
I0214 02:18:23.312182 139688679413568 submission_runner.py:408] Time since start: 3330.29s, 	Step: 3495, 	{'train/ctc_loss': Array(3.2871444, dtype=float32), 'train/wer': 0.6894920491170125, 'validation/ctc_loss': Array(3.6583788, dtype=float32), 'validation/wer': 0.7243403458296727, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.3570948, dtype=float32), 'test/wer': 0.6746694290414965, 'test/num_examples': 2472, 'score': 2942.91507768631, 'total_duration': 3330.286596775055, 'accumulated_submission_time': 2942.91507768631, 'accumulated_eval_time': 387.1395664215088, 'accumulated_logging_time': 0.09027791023254395}
I0214 02:18:23.343530 139518542853888 logging_writer.py:48] [3495] accumulated_eval_time=387.139566, accumulated_logging_time=0.090278, accumulated_submission_time=2942.915078, global_step=3495, preemption_count=0, score=2942.915078, test/ctc_loss=3.3570947647094727, test/num_examples=2472, test/wer=0.674669, total_duration=3330.286597, train/ctc_loss=3.287144422531128, train/wer=0.689492, validation/ctc_loss=3.658378839492798, validation/num_examples=5348, validation/wer=0.724340
I0214 02:18:28.028648 139518534461184 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9930740594863892, loss=2.5291926860809326
I0214 02:19:44.203065 139518542853888 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9062631726264954, loss=2.43607497215271
I0214 02:21:00.231069 139518534461184 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.112688660621643, loss=2.4593560695648193
I0214 02:22:19.492008 139518542853888 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.1225502490997314, loss=2.3955345153808594
I0214 02:23:43.979886 139518534461184 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9811543822288513, loss=2.351717233657837
I0214 02:25:12.321429 139518542853888 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.0264819860458374, loss=2.333559513092041
I0214 02:26:39.338904 139518534461184 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9310613870620728, loss=2.2759313583374023
I0214 02:28:01.109233 139518542853888 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8514966368675232, loss=2.3366482257843018
I0214 02:29:17.771983 139518534461184 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9776315689086914, loss=2.1732640266418457
I0214 02:30:34.141469 139518542853888 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8265399932861328, loss=2.166433572769165
I0214 02:31:55.870494 139518534461184 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8952954411506653, loss=2.130612373352051
I0214 02:33:21.455450 139518542853888 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.0655924081802368, loss=2.2055916786193848
I0214 02:34:50.816388 139518534461184 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8460404872894287, loss=2.1033689975738525
I0214 02:36:18.617564 139518542853888 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8048951029777527, loss=2.07633376121521
I0214 02:37:46.782498 139518534461184 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.0713862180709839, loss=2.0690760612487793
I0214 02:39:14.462307 139518542853888 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8140893578529358, loss=2.0463945865631104
I0214 02:40:45.125833 139518534461184 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.9638177156448364, loss=2.0049631595611572
I0214 02:42:10.272393 139518542853888 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.9072068929672241, loss=1.9324899911880493
I0214 02:42:23.638477 139688679413568 spec.py:321] Evaluating on the training split.
I0214 02:43:19.593328 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 02:44:13.213112 139688679413568 spec.py:349] Evaluating on the test split.
I0214 02:44:39.840126 139688679413568 submission_runner.py:408] Time since start: 4906.81s, 	Step: 5219, 	{'train/ctc_loss': Array(0.78254795, dtype=float32), 'train/wer': 0.2698947346226629, 'validation/ctc_loss': Array(1.1469847, dtype=float32), 'validation/wer': 0.3340799598366433, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8465067, dtype=float32), 'test/wer': 0.2771921272317348, 'test/num_examples': 2472, 'score': 4383.1235201358795, 'total_duration': 4906.813807249069, 'accumulated_submission_time': 4383.1235201358795, 'accumulated_eval_time': 523.3349103927612, 'accumulated_logging_time': 0.14176058769226074}
I0214 02:44:39.875443 139517662209792 logging_writer.py:48] [5219] accumulated_eval_time=523.334910, accumulated_logging_time=0.141761, accumulated_submission_time=4383.123520, global_step=5219, preemption_count=0, score=4383.123520, test/ctc_loss=0.8465067148208618, test/num_examples=2472, test/wer=0.277192, total_duration=4906.813807, train/ctc_loss=0.7825479507446289, train/wer=0.269895, validation/ctc_loss=1.1469846963882446, validation/num_examples=5348, validation/wer=0.334080
I0214 02:45:42.003899 139517653817088 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8859009742736816, loss=2.0362496376037598
I0214 02:46:57.924413 139517662209792 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.9363599419593811, loss=1.9745895862579346
I0214 02:48:14.195527 139517653817088 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9933122992515564, loss=1.9635887145996094
I0214 02:49:30.772192 139517662209792 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9488099813461304, loss=1.9933805465698242
I0214 02:50:59.275546 139517653817088 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8564228415489197, loss=1.9163087606430054
I0214 02:52:26.493398 139517662209792 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8765074610710144, loss=1.9295072555541992
I0214 02:53:53.757868 139517653817088 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7520005702972412, loss=1.978383183479309
I0214 02:55:24.126469 139517662209792 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7492278814315796, loss=1.8653291463851929
I0214 02:56:55.035027 139517653817088 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.9667234420776367, loss=1.8802376985549927
I0214 02:58:24.097779 139517662209792 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.9033151865005493, loss=1.8135441541671753
I0214 02:59:39.972432 139517653817088 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8610134720802307, loss=1.9231539964675903
I0214 03:00:57.755711 139517662209792 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7716105580329895, loss=1.846886157989502
I0214 03:02:17.794731 139517653817088 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7037111520767212, loss=1.8076043128967285
I0214 03:03:39.527095 139517662209792 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7932418584823608, loss=1.8491432666778564
I0214 03:05:07.825431 139517653817088 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.722387433052063, loss=1.8442955017089844
I0214 03:06:36.350147 139517662209792 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7945748567581177, loss=1.7954131364822388
I0214 03:08:05.157990 139517653817088 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.923717200756073, loss=1.7999541759490967
I0214 03:08:40.185674 139688679413568 spec.py:321] Evaluating on the training split.
I0214 03:09:36.471814 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 03:10:30.417102 139688679413568 spec.py:349] Evaluating on the test split.
I0214 03:10:56.633677 139688679413568 submission_runner.py:408] Time since start: 6483.61s, 	Step: 6943, 	{'train/ctc_loss': Array(0.5334083, dtype=float32), 'train/wer': 0.1861156782456618, 'validation/ctc_loss': Array(0.85424995, dtype=float32), 'validation/wer': 0.2556938316421599, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58340687, dtype=float32), 'test/wer': 0.19456462129059776, 'test/num_examples': 2472, 'score': 5823.346606016159, 'total_duration': 6483.608228683472, 'accumulated_submission_time': 5823.346606016159, 'accumulated_eval_time': 659.7774543762207, 'accumulated_logging_time': 0.1961519718170166}
I0214 03:10:56.670276 139517662209792 logging_writer.py:48] [6943] accumulated_eval_time=659.777454, accumulated_logging_time=0.196152, accumulated_submission_time=5823.346606, global_step=6943, preemption_count=0, score=5823.346606, test/ctc_loss=0.5834068655967712, test/num_examples=2472, test/wer=0.194565, total_duration=6483.608229, train/ctc_loss=0.5334082841873169, train/wer=0.186116, validation/ctc_loss=0.8542499542236328, validation/num_examples=5348, validation/wer=0.255694
I0214 03:11:40.533804 139517653817088 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8436051607131958, loss=1.798293113708496
I0214 03:12:56.295748 139517662209792 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9487977623939514, loss=1.76133394241333
I0214 03:14:18.136279 139517653817088 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.013264536857605, loss=1.7884037494659424
I0214 03:15:38.189351 139517662209792 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9186550378799438, loss=1.7756292819976807
I0214 03:16:55.436256 139517653817088 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8963354229927063, loss=1.7994933128356934
I0214 03:18:15.675584 139517662209792 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8783975839614868, loss=1.7611294984817505
I0214 03:19:40.401658 139517653817088 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8378236889839172, loss=1.7493544816970825
I0214 03:21:07.689480 139517662209792 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8382824659347534, loss=1.7787338495254517
I0214 03:22:35.348955 139517653817088 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7954198122024536, loss=1.7249634265899658
I0214 03:24:06.862587 139517662209792 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6731002926826477, loss=1.752616047859192
I0214 03:25:35.005608 139517653817088 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7222435474395752, loss=1.6940054893493652
I0214 03:27:00.904035 139517662209792 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6785380244255066, loss=1.6621445417404175
I0214 03:28:28.634010 139517653817088 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.8089149594306946, loss=1.7266088724136353
I0214 03:29:53.184289 139517334529792 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7888916730880737, loss=1.6995831727981567
I0214 03:31:08.931257 139517326137088 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6180629730224609, loss=1.6272996664047241
I0214 03:32:25.944368 139517334529792 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7354435324668884, loss=1.7077968120574951
I0214 03:33:47.708880 139517326137088 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7043598294258118, loss=1.6593626737594604
I0214 03:34:57.283395 139688679413568 spec.py:321] Evaluating on the training split.
I0214 03:35:51.427355 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 03:36:43.399325 139688679413568 spec.py:349] Evaluating on the test split.
I0214 03:37:10.079997 139688679413568 submission_runner.py:408] Time since start: 8057.05s, 	Step: 8684, 	{'train/ctc_loss': Array(0.45842528, dtype=float32), 'train/wer': 0.16366957315879885, 'validation/ctc_loss': Array(0.7623629, dtype=float32), 'validation/wer': 0.2306593162574703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50006676, dtype=float32), 'test/wer': 0.17006885625495094, 'test/num_examples': 2472, 'score': 7263.875959396362, 'total_duration': 8057.054099321365, 'accumulated_submission_time': 7263.875959396362, 'accumulated_eval_time': 792.5681960582733, 'accumulated_logging_time': 0.24925851821899414}
I0214 03:37:10.116517 139518112773888 logging_writer.py:48] [8684] accumulated_eval_time=792.568196, accumulated_logging_time=0.249259, accumulated_submission_time=7263.875959, global_step=8684, preemption_count=0, score=7263.875959, test/ctc_loss=0.5000667572021484, test/num_examples=2472, test/wer=0.170069, total_duration=8057.054099, train/ctc_loss=0.45842528343200684, train/wer=0.163670, validation/ctc_loss=0.7623628973960876, validation/num_examples=5348, validation/wer=0.230659
I0214 03:37:23.053257 139518104381184 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7132828235626221, loss=1.7144253253936768
I0214 03:38:38.912935 139518112773888 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.828683078289032, loss=1.6514418125152588
I0214 03:39:54.854646 139518104381184 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8688583374023438, loss=1.661333441734314
I0214 03:41:20.106224 139518112773888 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.0283355712890625, loss=1.6524261236190796
I0214 03:42:46.652211 139518104381184 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.75747150182724, loss=1.61902916431427
I0214 03:44:15.657641 139518112773888 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.0999165773391724, loss=1.684206485748291
I0214 03:45:44.226276 139518112773888 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7510120868682861, loss=1.614725112915039
I0214 03:47:02.588624 139518104381184 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6472304463386536, loss=1.6079015731811523
I0214 03:48:20.610208 139518112773888 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7864802479743958, loss=1.6154841184616089
I0214 03:49:41.182506 139518104381184 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8151945471763611, loss=1.6353118419647217
I0214 03:51:06.715403 139518112773888 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.59346604347229, loss=1.652463674545288
I0214 03:52:33.101458 139518104381184 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8145822882652283, loss=1.5795888900756836
I0214 03:54:02.852650 139518112773888 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8414708375930786, loss=1.6026463508605957
I0214 03:55:32.669206 139518104381184 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8008254766464233, loss=1.6278891563415527
I0214 03:57:02.659720 139518112773888 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7235705852508545, loss=1.5888906717300415
I0214 03:58:31.220832 139518104381184 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6946253180503845, loss=1.673402190208435
I0214 04:00:02.251274 139518112773888 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7844911217689514, loss=1.6349138021469116
I0214 04:01:10.471654 139688679413568 spec.py:321] Evaluating on the training split.
I0214 04:02:05.796183 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 04:02:59.501668 139688679413568 spec.py:349] Evaluating on the test split.
I0214 04:03:26.008651 139688679413568 submission_runner.py:408] Time since start: 9632.98s, 	Step: 10391, 	{'train/ctc_loss': Array(0.4250593, dtype=float32), 'train/wer': 0.15079140682098255, 'validation/ctc_loss': Array(0.6983394, dtype=float32), 'validation/wer': 0.21195825328016837, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45460317, dtype=float32), 'test/wer': 0.1544086283590275, 'test/num_examples': 2472, 'score': 8704.147989988327, 'total_duration': 9632.983120441437, 'accumulated_submission_time': 8704.147989988327, 'accumulated_eval_time': 928.0997140407562, 'accumulated_logging_time': 0.3020815849304199}
I0214 04:03:26.040985 139518542853888 logging_writer.py:48] [10391] accumulated_eval_time=928.099714, accumulated_logging_time=0.302082, accumulated_submission_time=8704.147990, global_step=10391, preemption_count=0, score=8704.147990, test/ctc_loss=0.4546031653881073, test/num_examples=2472, test/wer=0.154409, total_duration=9632.983120, train/ctc_loss=0.4250592887401581, train/wer=0.150791, validation/ctc_loss=0.6983394026756287, validation/num_examples=5348, validation/wer=0.211958
I0214 04:03:33.673208 139518534461184 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6895489692687988, loss=1.5433452129364014
I0214 04:04:49.173469 139518542853888 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7293559312820435, loss=1.635221004486084
I0214 04:06:04.820528 139518534461184 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8271686434745789, loss=1.5649449825286865
I0214 04:07:20.694173 139518542853888 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7138765454292297, loss=1.6075268983840942
I0214 04:08:46.338331 139518534461184 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.666913628578186, loss=1.6567500829696655
I0214 04:10:16.076703 139518542853888 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6427379846572876, loss=1.6195474863052368
I0214 04:11:44.552413 139518534461184 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5938433408737183, loss=1.5989413261413574
I0214 04:13:13.317894 139518542853888 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.647180438041687, loss=1.5694470405578613
I0214 04:14:43.373496 139518534461184 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.8876569271087646, loss=1.5884732007980347
I0214 04:16:13.517882 139518542853888 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7731950283050537, loss=1.5560219287872314
I0214 04:17:36.076377 139517165573888 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6919744610786438, loss=1.5132704973220825
I0214 04:18:54.450326 139517157181184 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7079817056655884, loss=1.5022358894348145
I0214 04:20:16.782142 139517165573888 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8302479982376099, loss=1.5546519756317139
I0214 04:21:37.901851 139517157181184 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7292553186416626, loss=1.569215178489685
I0214 04:23:04.799435 139517165573888 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6161141991615295, loss=1.6153779029846191
I0214 04:24:36.268885 139517157181184 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.8304381370544434, loss=1.595378041267395
I0214 04:26:05.259313 139517165573888 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7417469620704651, loss=1.55150306224823
I0214 04:27:26.140630 139688679413568 spec.py:321] Evaluating on the training split.
I0214 04:28:21.806166 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 04:29:15.661089 139688679413568 spec.py:349] Evaluating on the test split.
I0214 04:29:43.179174 139688679413568 submission_runner.py:408] Time since start: 11210.15s, 	Step: 12089, 	{'train/ctc_loss': Array(0.36514083, dtype=float32), 'train/wer': 0.13228256014293918, 'validation/ctc_loss': Array(0.65220815, dtype=float32), 'validation/wer': 0.19723490736360388, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41433823, dtype=float32), 'test/wer': 0.14096236264294273, 'test/num_examples': 2472, 'score': 10144.166356801987, 'total_duration': 11210.15414071083, 'accumulated_submission_time': 10144.166356801987, 'accumulated_eval_time': 1065.1332168579102, 'accumulated_logging_time': 0.350053071975708}
I0214 04:29:43.214576 139518251013888 logging_writer.py:48] [12089] accumulated_eval_time=1065.133217, accumulated_logging_time=0.350053, accumulated_submission_time=10144.166357, global_step=12089, preemption_count=0, score=10144.166357, test/ctc_loss=0.41433823108673096, test/num_examples=2472, test/wer=0.140962, total_duration=11210.154141, train/ctc_loss=0.365140825510025, train/wer=0.132283, validation/ctc_loss=0.652208149433136, validation/num_examples=5348, validation/wer=0.197235
I0214 04:29:52.346854 139518242621184 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6541653871536255, loss=1.5337014198303223
I0214 04:31:07.873443 139518251013888 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7055784463882446, loss=1.517216444015503
I0214 04:32:23.624929 139518242621184 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.8257459402084351, loss=1.4982258081436157
I0214 04:33:48.340303 139518251013888 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6053239703178406, loss=1.5316170454025269
I0214 04:35:04.543300 139518242621184 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6384049654006958, loss=1.5081764459609985
I0214 04:36:24.021223 139518251013888 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7425678968429565, loss=1.5214999914169312
I0214 04:37:42.023276 139518242621184 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6425595879554749, loss=1.5207581520080566
I0214 04:39:08.139870 139518251013888 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.8918103575706482, loss=1.504246711730957
I0214 04:40:37.325699 139518242621184 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6399073004722595, loss=1.5390197038650513
I0214 04:42:04.766038 139518251013888 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8495691418647766, loss=1.547964096069336
I0214 04:43:35.119195 139518242621184 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6212554574012756, loss=1.5516433715820312
I0214 04:45:02.435529 139518251013888 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.8878755569458008, loss=1.530633807182312
I0214 04:46:28.649606 139518242621184 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6269684433937073, loss=1.558510422706604
I0214 04:48:00.635328 139518251013888 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6508851647377014, loss=1.5239317417144775
I0214 04:49:16.296948 139518242621184 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7342666983604431, loss=1.4964592456817627
I0214 04:50:35.014991 139518251013888 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.7089894413948059, loss=1.4726741313934326
I0214 04:51:54.563835 139518242621184 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6042751669883728, loss=1.5401142835617065
I0214 04:53:19.372061 139518251013888 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.8383090496063232, loss=1.5231441259384155
I0214 04:53:43.705691 139688679413568 spec.py:321] Evaluating on the training split.
I0214 04:54:39.781424 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 04:55:32.660747 139688679413568 spec.py:349] Evaluating on the test split.
I0214 04:56:00.497573 139688679413568 submission_runner.py:408] Time since start: 12787.47s, 	Step: 13828, 	{'train/ctc_loss': Array(0.31292966, dtype=float32), 'train/wer': 0.11764216438284927, 'validation/ctc_loss': Array(0.6289795, dtype=float32), 'validation/wer': 0.19065043397665504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39117342, dtype=float32), 'test/wer': 0.1323908760384295, 'test/num_examples': 2472, 'score': 11584.574691057205, 'total_duration': 12787.471153974533, 'accumulated_submission_time': 11584.574691057205, 'accumulated_eval_time': 1201.91868019104, 'accumulated_logging_time': 0.4008769989013672}
I0214 04:56:00.532642 139518251013888 logging_writer.py:48] [13828] accumulated_eval_time=1201.918680, accumulated_logging_time=0.400877, accumulated_submission_time=11584.574691, global_step=13828, preemption_count=0, score=11584.574691, test/ctc_loss=0.39117342233657837, test/num_examples=2472, test/wer=0.132391, total_duration=12787.471154, train/ctc_loss=0.3129296600818634, train/wer=0.117642, validation/ctc_loss=0.628979504108429, validation/num_examples=5348, validation/wer=0.190650
I0214 04:56:55.549060 139518242621184 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6124131083488464, loss=1.4976955652236938
I0214 04:58:11.226256 139518251013888 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.702195942401886, loss=1.513817310333252
I0214 04:59:35.861684 139518242621184 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.619591236114502, loss=1.5546326637268066
I0214 05:01:05.606331 139518251013888 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6180617213249207, loss=1.5443620681762695
I0214 05:02:34.280031 139518242621184 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7276093363761902, loss=1.4988125562667847
I0214 05:04:00.046509 139518251013888 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7525038123130798, loss=1.521804690361023
I0214 05:05:21.816004 139518251013888 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6704712510108948, loss=1.5210286378860474
I0214 05:06:41.165136 139518242621184 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5637617111206055, loss=1.4354826211929321
I0214 05:07:59.810163 139518251013888 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6318254470825195, loss=1.4850918054580688
I0214 05:09:25.409399 139518242621184 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6840589642524719, loss=1.4809942245483398
I0214 05:10:54.759671 139518251013888 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.7406014204025269, loss=1.4694548845291138
I0214 05:12:21.129904 139518242621184 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6489629745483398, loss=1.4739831686019897
I0214 05:13:52.068894 139518251013888 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.8073453307151794, loss=1.4823319911956787
I0214 05:15:21.223521 139518242621184 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7629684805870056, loss=1.4409605264663696
I0214 05:16:53.780040 139518251013888 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.5990667939186096, loss=1.4735966920852661
I0214 05:18:23.080453 139518242621184 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.7374907732009888, loss=1.4701905250549316
I0214 05:19:47.944324 139517923333888 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.781517744064331, loss=1.4332345724105835
I0214 05:20:00.493378 139688679413568 spec.py:321] Evaluating on the training split.
I0214 05:20:55.543807 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 05:21:49.305041 139688679413568 spec.py:349] Evaluating on the test split.
I0214 05:22:15.925492 139688679413568 submission_runner.py:408] Time since start: 14362.90s, 	Step: 15518, 	{'train/ctc_loss': Array(0.28706837, dtype=float32), 'train/wer': 0.10611324207715102, 'validation/ctc_loss': Array(0.59261024, dtype=float32), 'validation/wer': 0.18042615638607026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3671398, dtype=float32), 'test/wer': 0.12562712002112406, 'test/num_examples': 2472, 'score': 13024.453626871109, 'total_duration': 14362.89952802658, 'accumulated_submission_time': 13024.453626871109, 'accumulated_eval_time': 1337.3448441028595, 'accumulated_logging_time': 0.4517827033996582}
I0214 05:22:15.960683 139518112773888 logging_writer.py:48] [15518] accumulated_eval_time=1337.344844, accumulated_logging_time=0.451783, accumulated_submission_time=13024.453627, global_step=15518, preemption_count=0, score=13024.453627, test/ctc_loss=0.3671397864818573, test/num_examples=2472, test/wer=0.125627, total_duration=14362.899528, train/ctc_loss=0.28706836700439453, train/wer=0.106113, validation/ctc_loss=0.592610239982605, validation/num_examples=5348, validation/wer=0.180426
I0214 05:23:18.469978 139518104381184 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6658616662025452, loss=1.444617509841919
I0214 05:24:34.002139 139518112773888 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6399540901184082, loss=1.4836505651474
I0214 05:25:49.581012 139518104381184 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.673378050327301, loss=1.4238070249557495
I0214 05:27:09.504211 139518112773888 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6222766041755676, loss=1.4999421834945679
I0214 05:28:37.617917 139518104381184 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5864240527153015, loss=1.445693016052246
I0214 05:30:05.542802 139518112773888 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6717128157615662, loss=1.4408714771270752
I0214 05:31:37.670553 139518104381184 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.5491122007369995, loss=1.4908909797668457
I0214 05:33:09.218587 139518112773888 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.5849522948265076, loss=1.4726977348327637
I0214 05:34:37.334012 139518104381184 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5972504615783691, loss=1.4196707010269165
I0214 05:36:05.556055 139518112773888 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7457869052886963, loss=1.4356615543365479
I0214 05:37:21.310111 139518104381184 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.7360926866531372, loss=1.4296828508377075
I0214 05:38:37.159210 139518112773888 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6213088631629944, loss=1.3868108987808228
I0214 05:40:00.037139 139518104381184 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6956813931465149, loss=1.448612093925476
I0214 05:41:21.917832 139518112773888 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.6957869529724121, loss=1.4492956399917603
I0214 05:42:51.164088 139518104381184 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7539933919906616, loss=1.4004294872283936
I0214 05:44:22.531111 139518112773888 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.7154955267906189, loss=1.4235769510269165
I0214 05:45:47.502086 139518104381184 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.8388116955757141, loss=1.4424571990966797
I0214 05:46:16.574245 139688679413568 spec.py:321] Evaluating on the training split.
I0214 05:47:12.012272 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 05:48:04.136461 139688679413568 spec.py:349] Evaluating on the test split.
I0214 05:48:30.872256 139688679413568 submission_runner.py:408] Time since start: 15937.85s, 	Step: 17235, 	{'train/ctc_loss': Array(0.28261438, dtype=float32), 'train/wer': 0.10724149581427277, 'validation/ctc_loss': Array(0.58001304, dtype=float32), 'validation/wer': 0.17476852969288548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3576985, dtype=float32), 'test/wer': 0.12150386935591981, 'test/num_examples': 2472, 'score': 14464.984570026398, 'total_duration': 15937.846490383148, 'accumulated_submission_time': 14464.984570026398, 'accumulated_eval_time': 1471.637094259262, 'accumulated_logging_time': 0.5028841495513916}
I0214 05:48:30.905822 139518112773888 logging_writer.py:48] [17235] accumulated_eval_time=1471.637094, accumulated_logging_time=0.502884, accumulated_submission_time=14464.984570, global_step=17235, preemption_count=0, score=14464.984570, test/ctc_loss=0.3576985001564026, test/num_examples=2472, test/wer=0.121504, total_duration=15937.846490, train/ctc_loss=0.2826143801212311, train/wer=0.107241, validation/ctc_loss=0.5800130367279053, validation/num_examples=5348, validation/wer=0.174769
I0214 05:49:20.726224 139518104381184 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.6468824148178101, loss=1.401824712753296
I0214 05:50:36.439381 139518112773888 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6354770064353943, loss=1.4573558568954468
I0214 05:51:56.819081 139518104381184 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6359082460403442, loss=1.4727067947387695
I0214 05:53:17.287238 139517165573888 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.5847928524017334, loss=1.4492058753967285
I0214 05:54:36.053001 139517157181184 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.6853016018867493, loss=1.4526630640029907
I0214 05:55:57.249699 139517165573888 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6523493528366089, loss=1.3665921688079834
I0214 05:57:20.579171 139517157181184 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.8325667977333069, loss=1.4413937330245972
I0214 05:58:49.539673 139517165573888 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.724215030670166, loss=1.4095195531845093
I0214 06:00:19.472858 139517157181184 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.6264392137527466, loss=1.4062286615371704
I0214 06:01:49.790645 139517165573888 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7360877990722656, loss=1.4125181436538696
I0214 06:03:21.455524 139517157181184 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.7553613781929016, loss=1.4351375102996826
I0214 06:04:52.386094 139517165573888 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.7704099416732788, loss=1.4464324712753296
I0214 06:06:20.021731 139517157181184 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5337972640991211, loss=1.4389482736587524
I0214 06:07:44.399459 139517165573888 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.7251754403114319, loss=1.3029720783233643
I0214 06:09:01.473740 139517157181184 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6643531918525696, loss=1.4491894245147705
I0214 06:10:17.313434 139517165573888 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6616309285163879, loss=1.4222323894500732
I0214 06:11:35.010816 139517157181184 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.660628616809845, loss=1.3682639598846436
I0214 06:12:31.409710 139688679413568 spec.py:321] Evaluating on the training split.
I0214 06:13:26.749820 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 06:14:19.322120 139688679413568 spec.py:349] Evaluating on the test split.
I0214 06:14:46.558200 139688679413568 submission_runner.py:408] Time since start: 17513.53s, 	Step: 18968, 	{'train/ctc_loss': Array(0.27884072, dtype=float32), 'train/wer': 0.1019963510404823, 'validation/ctc_loss': Array(0.5530038, dtype=float32), 'validation/wer': 0.1681068190814563, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33700582, dtype=float32), 'test/wer': 0.11465886702008815, 'test/num_examples': 2472, 'score': 15905.405641078949, 'total_duration': 17513.532631635666, 'accumulated_submission_time': 15905.405641078949, 'accumulated_eval_time': 1606.7800455093384, 'accumulated_logging_time': 0.55181884765625}
I0214 06:14:46.593071 139517165573888 logging_writer.py:48] [18968] accumulated_eval_time=1606.780046, accumulated_logging_time=0.551819, accumulated_submission_time=15905.405641, global_step=18968, preemption_count=0, score=15905.405641, test/ctc_loss=0.3370058238506317, test/num_examples=2472, test/wer=0.114659, total_duration=17513.532632, train/ctc_loss=0.27884072065353394, train/wer=0.101996, validation/ctc_loss=0.5530037879943848, validation/num_examples=5348, validation/wer=0.168107
I0214 06:15:11.575844 139517157181184 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9568519592285156, loss=1.4068286418914795
I0214 06:16:27.460605 139517165573888 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6111015677452087, loss=1.3701238632202148
I0214 06:17:45.025131 139517157181184 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.8749874234199524, loss=1.402929425239563
I0214 06:19:16.712241 139517165573888 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6796596050262451, loss=1.4405765533447266
I0214 06:20:42.853023 139517157181184 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.6887361407279968, loss=1.445043921470642
I0214 06:22:10.760807 139517165573888 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.614522397518158, loss=1.4039791822433472
I0214 06:23:38.429909 139517165573888 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.577582836151123, loss=1.3905513286590576
I0214 06:24:55.216957 139517157181184 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.143720030784607, loss=1.433258056640625
I0214 06:26:12.292878 139517165573888 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.898536741733551, loss=1.4176019430160522
I0214 06:27:32.314088 139517157181184 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5895626544952393, loss=1.3487292528152466
I0214 06:28:57.127485 139517165573888 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5360637903213501, loss=1.3306150436401367
I0214 06:30:27.660517 139517157181184 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.8196080923080444, loss=1.4083061218261719
I0214 06:31:58.833272 139517165573888 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6783298254013062, loss=1.3775330781936646
I0214 06:33:28.488655 139517157181184 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.6829613447189331, loss=1.3847951889038086
I0214 06:34:55.310619 139517165573888 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6499227285385132, loss=1.4443937540054321
I0214 06:36:23.688923 139517157181184 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5999929904937744, loss=1.4006050825119019
I0214 06:37:55.432319 139518112773888 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7224569320678711, loss=1.3842403888702393
I0214 06:38:46.937871 139688679413568 spec.py:321] Evaluating on the training split.
I0214 06:39:42.438572 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 06:40:35.304602 139688679413568 spec.py:349] Evaluating on the test split.
I0214 06:41:01.708284 139688679413568 submission_runner.py:408] Time since start: 19088.68s, 	Step: 20669, 	{'train/ctc_loss': Array(0.2795932, dtype=float32), 'train/wer': 0.10299416994222703, 'validation/ctc_loss': Array(0.5383996, dtype=float32), 'validation/wer': 0.16351120422487617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32689303, dtype=float32), 'test/wer': 0.11201836166798691, 'test/num_examples': 2472, 'score': 17345.66361284256, 'total_duration': 19088.68205499649, 'accumulated_submission_time': 17345.66361284256, 'accumulated_eval_time': 1741.5442523956299, 'accumulated_logging_time': 0.6050012111663818}
I0214 06:41:01.746340 139518542853888 logging_writer.py:48] [20669] accumulated_eval_time=1741.544252, accumulated_logging_time=0.605001, accumulated_submission_time=17345.663613, global_step=20669, preemption_count=0, score=17345.663613, test/ctc_loss=0.32689303159713745, test/num_examples=2472, test/wer=0.112018, total_duration=19088.682055, train/ctc_loss=0.27959319949150085, train/wer=0.102994, validation/ctc_loss=0.5383995771408081, validation/num_examples=5348, validation/wer=0.163511
I0214 06:41:26.126954 139518534461184 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7361152768135071, loss=1.357562780380249
I0214 06:42:41.877339 139518542853888 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.7315613031387329, loss=1.4323315620422363
I0214 06:43:57.439814 139518534461184 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.8576878905296326, loss=1.3404810428619385
I0214 06:45:13.003067 139518542853888 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6485605835914612, loss=1.3814074993133545
I0214 06:46:35.852578 139518534461184 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.7916432023048401, loss=1.3826417922973633
I0214 06:48:04.396487 139518542853888 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.7441940307617188, loss=1.3817156553268433
I0214 06:49:35.699459 139518534461184 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7662136554718018, loss=1.3511539697647095
I0214 06:51:04.433181 139518542853888 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6898905634880066, loss=1.4057693481445312
I0214 06:52:32.855675 139518534461184 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6561492085456848, loss=1.422593355178833
I0214 06:54:03.013598 139518542853888 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6171294450759888, loss=1.42215895652771
I0214 06:55:26.095995 139518542853888 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6583524346351624, loss=1.3561989068984985
I0214 06:56:41.803616 139518534461184 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6781753301620483, loss=1.3577446937561035
I0214 06:58:02.724284 139518542853888 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7183806300163269, loss=1.3971575498580933
I0214 06:59:23.327828 139518534461184 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6424990892410278, loss=1.3553932905197144
I0214 07:00:51.270838 139518542853888 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.6364011168479919, loss=1.3574029207229614
I0214 07:02:22.079514 139518534461184 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.767018735408783, loss=1.3673555850982666
I0214 07:03:52.827298 139518542853888 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.597412109375, loss=1.3189270496368408
I0214 07:05:02.325880 139688679413568 spec.py:321] Evaluating on the training split.
I0214 07:05:58.591982 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 07:06:51.128517 139688679413568 spec.py:349] Evaluating on the test split.
I0214 07:07:18.042106 139688679413568 submission_runner.py:408] Time since start: 20665.02s, 	Step: 22379, 	{'train/ctc_loss': Array(0.26681316, dtype=float32), 'train/wer': 0.09884247350550468, 'validation/ctc_loss': Array(0.53043646, dtype=float32), 'validation/wer': 0.1613002886741265, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32418817, dtype=float32), 'test/wer': 0.11061686267341012, 'test/num_examples': 2472, 'score': 18786.160464525223, 'total_duration': 20665.01668071747, 'accumulated_submission_time': 18786.160464525223, 'accumulated_eval_time': 1877.2550451755524, 'accumulated_logging_time': 0.659116268157959}
I0214 07:07:18.078368 139518542853888 logging_writer.py:48] [22379] accumulated_eval_time=1877.255045, accumulated_logging_time=0.659116, accumulated_submission_time=18786.160465, global_step=22379, preemption_count=0, score=18786.160465, test/ctc_loss=0.3241881728172302, test/num_examples=2472, test/wer=0.110617, total_duration=20665.016681, train/ctc_loss=0.26681315898895264, train/wer=0.098842, validation/ctc_loss=0.5304364562034607, validation/num_examples=5348, validation/wer=0.161300
I0214 07:07:34.834728 139518534461184 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7307391166687012, loss=1.3273460865020752
I0214 07:08:50.389938 139518542853888 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7686198353767395, loss=1.4243031740188599
I0214 07:10:06.679430 139518534461184 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.7978624105453491, loss=1.3290377855300903
I0214 07:11:35.841981 139517887493888 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.6935138702392578, loss=1.3780847787857056
I0214 07:12:53.923479 139517879101184 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.7315191030502319, loss=1.3440946340560913
I0214 07:14:10.004663 139517887493888 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.6978833675384521, loss=1.3466097116470337
I0214 07:15:31.340025 139517879101184 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.825659990310669, loss=1.3829454183578491
I0214 07:16:57.715267 139517887493888 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7741794586181641, loss=1.3432437181472778
I0214 07:18:26.940842 139517879101184 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.694990873336792, loss=1.3341293334960938
I0214 07:19:57.674966 139517887493888 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6396662592887878, loss=1.3793069124221802
I0214 07:21:24.987898 139517879101184 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.7203176021575928, loss=1.3693883419036865
I0214 07:22:56.694521 139517887493888 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7195913195610046, loss=1.2847492694854736
I0214 07:24:24.314294 139517879101184 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.762920618057251, loss=1.3480746746063232
I0214 07:25:57.227909 139517887493888 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6598816514015198, loss=1.3029736280441284
I0214 07:27:16.297846 139517879101184 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.6905103325843811, loss=1.3853482007980347
I0214 07:28:31.962635 139517887493888 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6140463948249817, loss=1.3635609149932861
I0214 07:29:52.012705 139517879101184 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.8318002223968506, loss=1.3242276906967163
I0214 07:31:15.342837 139517887493888 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.1740670204162598, loss=1.3984586000442505
I0214 07:31:18.164314 139688679413568 spec.py:321] Evaluating on the training split.
I0214 07:32:13.072469 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 07:33:07.179386 139688679413568 spec.py:349] Evaluating on the test split.
I0214 07:33:33.937960 139688679413568 submission_runner.py:408] Time since start: 22240.91s, 	Step: 24105, 	{'train/ctc_loss': Array(0.24471778, dtype=float32), 'train/wer': 0.09138655462184873, 'validation/ctc_loss': Array(0.52384573, dtype=float32), 'validation/wer': 0.1589252440213561, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31326857, dtype=float32), 'test/wer': 0.10620924989336421, 'test/num_examples': 2472, 'score': 20226.1611392498, 'total_duration': 22240.911847114563, 'accumulated_submission_time': 20226.1611392498, 'accumulated_eval_time': 2013.0226137638092, 'accumulated_logging_time': 0.7118988037109375}
I0214 07:33:33.980700 139518542853888 logging_writer.py:48] [24105] accumulated_eval_time=2013.022614, accumulated_logging_time=0.711899, accumulated_submission_time=20226.161139, global_step=24105, preemption_count=0, score=20226.161139, test/ctc_loss=0.3132685720920563, test/num_examples=2472, test/wer=0.106209, total_duration=22240.911847, train/ctc_loss=0.2447177767753601, train/wer=0.091387, validation/ctc_loss=0.5238457322120667, validation/num_examples=5348, validation/wer=0.158925
I0214 07:34:46.612022 139518534461184 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.8279610872268677, loss=1.3857879638671875
I0214 07:36:02.550619 139518542853888 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6531667113304138, loss=1.404603362083435
I0214 07:37:26.479468 139518534461184 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.6298403143882751, loss=1.3569055795669556
I0214 07:38:56.944900 139518542853888 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.677560031414032, loss=1.33533775806427
I0214 07:40:28.210759 139518534461184 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6233604550361633, loss=1.347335696220398
I0214 07:41:58.507565 139518542853888 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.7393856644630432, loss=1.3144136667251587
I0214 07:43:20.209974 139518542853888 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6025353670120239, loss=1.3346662521362305
I0214 07:44:40.089471 139518534461184 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.713165283203125, loss=1.3376513719558716
I0214 07:46:01.172287 139518542853888 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.013707160949707, loss=1.3181904554367065
I0214 07:47:24.319036 139518534461184 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6856061816215515, loss=1.3634066581726074
I0214 07:48:50.627726 139518542853888 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6396415829658508, loss=1.315664529800415
I0214 07:50:22.172391 139518534461184 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.7237115502357483, loss=1.3862547874450684
I0214 07:51:55.050493 139518542853888 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.7325781583786011, loss=1.35415518283844
I0214 07:53:25.353634 139518534461184 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8374844193458557, loss=1.3903429508209229
I0214 07:54:54.126271 139518542853888 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.7198471426963806, loss=1.321382761001587
I0214 07:56:23.868997 139518534461184 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6267278790473938, loss=1.392120599746704
I0214 07:57:34.144360 139688679413568 spec.py:321] Evaluating on the training split.
I0214 07:58:29.633485 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 07:59:23.456055 139688679413568 spec.py:349] Evaluating on the test split.
I0214 07:59:50.730425 139688679413568 submission_runner.py:408] Time since start: 23817.70s, 	Step: 25782, 	{'train/ctc_loss': Array(0.23044355, dtype=float32), 'train/wer': 0.08575601867550015, 'validation/ctc_loss': Array(0.5116114, dtype=float32), 'validation/wer': 0.15423308263417554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30197123, dtype=float32), 'test/wer': 0.10135478236142424, 'test/num_examples': 2472, 'score': 21666.239350557327, 'total_duration': 23817.70459675789, 'accumulated_submission_time': 21666.239350557327, 'accumulated_eval_time': 2149.6030519008636, 'accumulated_logging_time': 0.7754313945770264}
I0214 07:59:50.765968 139517744133888 logging_writer.py:48] [25782] accumulated_eval_time=2149.603052, accumulated_logging_time=0.775431, accumulated_submission_time=21666.239351, global_step=25782, preemption_count=0, score=21666.239351, test/ctc_loss=0.3019712269306183, test/num_examples=2472, test/wer=0.101355, total_duration=23817.704597, train/ctc_loss=0.2304435521364212, train/wer=0.085756, validation/ctc_loss=0.5116114020347595, validation/num_examples=5348, validation/wer=0.154233
I0214 08:00:05.145313 139517735741184 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.7462264895439148, loss=1.2918179035186768
I0214 08:01:20.651952 139517744133888 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6871570348739624, loss=1.282523512840271
I0214 08:02:36.313385 139517735741184 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7107880711555481, loss=1.391680121421814
I0214 08:03:51.886784 139517744133888 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.7622513771057129, loss=1.2835432291030884
I0214 08:05:14.832063 139517735741184 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6605648398399353, loss=1.3166258335113525
I0214 08:06:44.060894 139517744133888 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6918485760688782, loss=1.3366689682006836
I0214 08:08:14.997001 139517735741184 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.1057208776474, loss=1.3154394626617432
I0214 08:09:46.649596 139517744133888 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7938356995582581, loss=1.3230135440826416
I0214 08:11:15.537136 139517735741184 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.6789911389350891, loss=1.3991286754608154
I0214 08:12:44.613754 139517744133888 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.7093399167060852, loss=1.3013420104980469
I0214 08:14:15.890263 139517744133888 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.7304602265357971, loss=1.3396785259246826
I0214 08:15:32.661680 139517735741184 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7278954982757568, loss=1.305863380432129
I0214 08:16:51.552856 139517744133888 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7363954782485962, loss=1.3319319486618042
I0214 08:18:12.983303 139517735741184 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.631783127784729, loss=1.2889081239700317
I0214 08:19:38.348077 139517744133888 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8010329008102417, loss=1.3029890060424805
I0214 08:21:07.092336 139517735741184 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.7376796007156372, loss=1.305647611618042
I0214 08:22:37.749032 139517744133888 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.826511561870575, loss=1.3356062173843384
I0214 08:23:50.992405 139688679413568 spec.py:321] Evaluating on the training split.
I0214 08:24:46.995350 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 08:25:40.185355 139688679413568 spec.py:349] Evaluating on the test split.
I0214 08:26:06.730056 139688679413568 submission_runner.py:408] Time since start: 25393.70s, 	Step: 27484, 	{'train/ctc_loss': Array(0.22562861, dtype=float32), 'train/wer': 0.08264209223593909, 'validation/ctc_loss': Array(0.4992585, dtype=float32), 'validation/wer': 0.15019743765507787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30364478, dtype=float32), 'test/wer': 0.10111104340584567, 'test/num_examples': 2472, 'score': 23106.382368564606, 'total_duration': 25393.70451760292, 'accumulated_submission_time': 23106.382368564606, 'accumulated_eval_time': 2285.3351743221283, 'accumulated_logging_time': 0.8276486396789551}
I0214 08:26:06.769027 139518112773888 logging_writer.py:48] [27484] accumulated_eval_time=2285.335174, accumulated_logging_time=0.827649, accumulated_submission_time=23106.382369, global_step=27484, preemption_count=0, score=23106.382369, test/ctc_loss=0.3036447763442993, test/num_examples=2472, test/wer=0.101111, total_duration=25393.704518, train/ctc_loss=0.22562861442565918, train/wer=0.082642, validation/ctc_loss=0.49925848841667175, validation/num_examples=5348, validation/wer=0.150197
I0214 08:26:19.737083 139518104381184 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.6266818642616272, loss=1.3233120441436768
I0214 08:27:35.283986 139518112773888 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.7117738723754883, loss=1.36094069480896
I0214 08:28:54.097270 139518104381184 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.7053786516189575, loss=1.2301208972930908
I0214 08:30:21.403548 139518112773888 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6699866652488708, loss=1.3203673362731934
I0214 08:31:42.504405 139517457413888 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.7969267964363098, loss=1.2801860570907593
I0214 08:33:05.057152 139517449021184 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.7587093114852905, loss=1.3640412092208862
I0214 08:34:25.057106 139517457413888 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.6398658156394958, loss=1.2632756233215332
I0214 08:35:49.266009 139517449021184 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.7219922542572021, loss=1.3475226163864136
I0214 08:37:16.821501 139517457413888 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.1093037128448486, loss=1.3644616603851318
I0214 08:38:50.256350 139517449021184 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.6971880793571472, loss=1.3174941539764404
I0214 08:40:19.272343 139517457413888 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.654300332069397, loss=1.3398255109786987
I0214 08:41:48.938558 139517449021184 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.706691563129425, loss=1.3344056606292725
I0214 08:43:18.688452 139517457413888 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.6733176708221436, loss=1.2707053422927856
I0214 08:44:48.764559 139517449021184 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.8410477638244629, loss=1.3401209115982056
I0214 08:46:15.039401 139518112773888 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.6443918943405151, loss=1.315418004989624
I0214 08:47:32.660629 139518104381184 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6191008687019348, loss=1.2987899780273438
I0214 08:48:51.677159 139518112773888 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.688529908657074, loss=1.264947533607483
I0214 08:50:07.139040 139688679413568 spec.py:321] Evaluating on the training split.
I0214 08:51:01.436244 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 08:51:54.057658 139688679413568 spec.py:349] Evaluating on the test split.
I0214 08:52:21.522255 139688679413568 submission_runner.py:408] Time since start: 26968.50s, 	Step: 29193, 	{'train/ctc_loss': Array(0.23682073, dtype=float32), 'train/wer': 0.08620428150459351, 'validation/ctc_loss': Array(0.48714328, dtype=float32), 'validation/wer': 0.14619075663516032, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29264167, dtype=float32), 'test/wer': 0.09885645806674385, 'test/num_examples': 2472, 'score': 24546.66773033142, 'total_duration': 26968.495937108994, 'accumulated_submission_time': 24546.66773033142, 'accumulated_eval_time': 2419.71209859848, 'accumulated_logging_time': 0.8837933540344238}
I0214 08:52:21.560279 139518112773888 logging_writer.py:48] [29193] accumulated_eval_time=2419.712099, accumulated_logging_time=0.883793, accumulated_submission_time=24546.667730, global_step=29193, preemption_count=0, score=24546.667730, test/ctc_loss=0.29264166951179504, test/num_examples=2472, test/wer=0.098856, total_duration=26968.495937, train/ctc_loss=0.23682072758674622, train/wer=0.086204, validation/ctc_loss=0.48714327812194824, validation/num_examples=5348, validation/wer=0.146191
I0214 08:52:27.733262 139518104381184 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.771275520324707, loss=1.3133379220962524
I0214 08:53:43.640588 139518112773888 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.7264090776443481, loss=1.2623869180679321
I0214 08:54:59.336817 139518104381184 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.7674760222434998, loss=1.3216140270233154
I0214 08:56:25.184754 139518112773888 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.638058602809906, loss=1.2838441133499146
I0214 08:57:55.805155 139518104381184 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.7621570229530334, loss=1.2562425136566162
I0214 08:59:25.573074 139518112773888 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6821498870849609, loss=1.3447662591934204
I0214 09:00:55.944818 139518104381184 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.6364840865135193, loss=1.273688793182373
I0214 09:02:25.774309 139518112773888 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.9088386297225952, loss=1.262439489364624
I0214 09:03:42.953556 139518104381184 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7726925015449524, loss=1.3000441789627075
I0214 09:05:00.564504 139518112773888 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.6972737312316895, loss=1.3091213703155518
I0214 09:06:22.222796 139518104381184 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.6575078368186951, loss=1.2884373664855957
I0214 09:07:49.158118 139518112773888 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.7752861380577087, loss=1.3198541402816772
I0214 09:09:18.076142 139518104381184 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.6707004904747009, loss=1.3591196537017822
I0214 09:10:51.319453 139518112773888 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.7270534634590149, loss=1.3368664979934692
I0214 09:12:23.121764 139518104381184 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6952914595603943, loss=1.2927227020263672
I0214 09:13:56.460669 139518112773888 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.8516131639480591, loss=1.3293956518173218
I0214 09:15:25.656983 139518104381184 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.8155591487884521, loss=1.2491446733474731
I0214 09:16:21.715116 139688679413568 spec.py:321] Evaluating on the training split.
I0214 09:17:16.059187 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 09:18:08.829445 139688679413568 spec.py:349] Evaluating on the test split.
I0214 09:18:35.872541 139688679413568 submission_runner.py:408] Time since start: 28542.85s, 	Step: 30862, 	{'train/ctc_loss': Array(0.22417934, dtype=float32), 'train/wer': 0.08077634713156338, 'validation/ctc_loss': Array(0.48772222, dtype=float32), 'validation/wer': 0.14717553124728464, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28424188, dtype=float32), 'test/wer': 0.09465196108301342, 'test/num_examples': 2472, 'score': 25986.74062180519, 'total_duration': 28542.845601081848, 'accumulated_submission_time': 25986.74062180519, 'accumulated_eval_time': 2553.862573862076, 'accumulated_logging_time': 0.9391980171203613}
I0214 09:18:35.914638 139518542853888 logging_writer.py:48] [30862] accumulated_eval_time=2553.862574, accumulated_logging_time=0.939198, accumulated_submission_time=25986.740622, global_step=30862, preemption_count=0, score=25986.740622, test/ctc_loss=0.2842418849468231, test/num_examples=2472, test/wer=0.094652, total_duration=28542.845601, train/ctc_loss=0.22417934238910675, train/wer=0.080776, validation/ctc_loss=0.4877222180366516, validation/num_examples=5348, validation/wer=0.147176
I0214 09:19:08.870224 139518215173888 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.7792300581932068, loss=1.2575136423110962
I0214 09:20:27.973047 139518206781184 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.785519540309906, loss=1.288451075553894
I0214 09:21:44.447420 139518215173888 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6973568201065063, loss=1.2385245561599731
I0214 09:23:07.292655 139518206781184 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6317924857139587, loss=1.2530293464660645
I0214 09:24:33.693096 139518215173888 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.7068032026290894, loss=1.318374514579773
I0214 09:26:02.296980 139518206781184 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6654973030090332, loss=1.294266939163208
I0214 09:27:32.316989 139518215173888 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6802791357040405, loss=1.258148431777954
I0214 09:29:04.627823 139518206781184 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6799808144569397, loss=1.2770243883132935
I0214 09:30:35.946125 139518215173888 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6461001038551331, loss=1.263344407081604
I0214 09:32:04.220274 139518206781184 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.7727920413017273, loss=1.2590209245681763
I0214 09:33:35.639933 139518215173888 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.8162062168121338, loss=1.3208084106445312
I0214 09:34:58.888226 139517887493888 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.8352856636047363, loss=1.2728341817855835
I0214 09:36:17.906348 139517879101184 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.690506637096405, loss=1.3056976795196533
I0214 09:37:39.638722 139517887493888 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.8139384984970093, loss=1.2495466470718384
I0214 09:39:00.848119 139517879101184 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.7457748055458069, loss=1.2164435386657715
I0214 09:40:28.156965 139517887493888 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.7754217386245728, loss=1.278785228729248
I0214 09:41:59.665315 139517879101184 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.8130188584327698, loss=1.2910819053649902
I0214 09:42:36.135041 139688679413568 spec.py:321] Evaluating on the training split.
I0214 09:43:32.654522 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 09:44:25.000396 139688679413568 spec.py:349] Evaluating on the test split.
I0214 09:44:51.526257 139688679413568 submission_runner.py:408] Time since start: 30118.50s, 	Step: 32543, 	{'train/ctc_loss': Array(0.22964644, dtype=float32), 'train/wer': 0.08269935120640677, 'validation/ctc_loss': Array(0.47304323, dtype=float32), 'validation/wer': 0.14242544194174384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27928787, dtype=float32), 'test/wer': 0.09404261369406698, 'test/num_examples': 2472, 'score': 27426.87792801857, 'total_duration': 30118.500452518463, 'accumulated_submission_time': 27426.87792801857, 'accumulated_eval_time': 2689.2480068206787, 'accumulated_logging_time': 0.999396562576294}
I0214 09:44:51.567495 139517887493888 logging_writer.py:48] [32543] accumulated_eval_time=2689.248007, accumulated_logging_time=0.999397, accumulated_submission_time=27426.877928, global_step=32543, preemption_count=0, score=27426.877928, test/ctc_loss=0.2792878746986389, test/num_examples=2472, test/wer=0.094043, total_duration=30118.500453, train/ctc_loss=0.2296464443206787, train/wer=0.082699, validation/ctc_loss=0.4730432331562042, validation/num_examples=5348, validation/wer=0.142425
I0214 09:45:35.389718 139517879101184 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6742117404937744, loss=1.298592209815979
I0214 09:46:51.268126 139517887493888 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.710381031036377, loss=1.2580689191818237
I0214 09:48:14.450922 139517879101184 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.7492985129356384, loss=1.2826149463653564
I0214 09:49:46.703873 139517887493888 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.7435328364372253, loss=1.3103177547454834
I0214 09:51:14.626705 139517887493888 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.8188803195953369, loss=1.2311204671859741
I0214 09:52:33.047220 139517879101184 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.7042639851570129, loss=1.2320319414138794
I0214 09:53:50.165000 139517887493888 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.6173428893089294, loss=1.2090610265731812
I0214 09:55:13.532798 139517879101184 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.7165188193321228, loss=1.2552319765090942
I0214 09:56:42.996009 139517887493888 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.7293121814727783, loss=1.2547756433486938
I0214 09:58:12.375859 139517879101184 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.8496949672698975, loss=1.2270071506500244
I0214 09:59:40.233365 139517887493888 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.6657056212425232, loss=1.3066754341125488
I0214 10:01:11.313919 139517879101184 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.7357403039932251, loss=1.2497175931930542
I0214 10:02:44.366375 139517887493888 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.760524332523346, loss=1.2806416749954224
I0214 10:04:12.926475 139517879101184 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.0802713632583618, loss=1.2601970434188843
I0214 10:05:44.286810 139518542853888 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7505127191543579, loss=1.2682112455368042
I0214 10:07:00.442081 139518534461184 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.6804597973823547, loss=1.192491888999939
I0214 10:08:19.076398 139518542853888 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6848475933074951, loss=1.299500584602356
I0214 10:08:51.594602 139688679413568 spec.py:321] Evaluating on the training split.
I0214 10:09:49.696924 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 10:10:42.427753 139688679413568 spec.py:349] Evaluating on the test split.
I0214 10:11:09.918175 139688679413568 submission_runner.py:408] Time since start: 31696.89s, 	Step: 34241, 	{'train/ctc_loss': Array(0.22303934, dtype=float32), 'train/wer': 0.07963276745846522, 'validation/ctc_loss': Array(0.4615741, dtype=float32), 'validation/wer': 0.13925871573804996, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27463698, dtype=float32), 'test/wer': 0.09184896309385981, 'test/num_examples': 2472, 'score': 28866.8210606575, 'total_duration': 31696.89235520363, 'accumulated_submission_time': 28866.8210606575, 'accumulated_eval_time': 2827.5657880306244, 'accumulated_logging_time': 1.057802677154541}
I0214 10:11:09.961049 139518542853888 logging_writer.py:48] [34241] accumulated_eval_time=2827.565788, accumulated_logging_time=1.057803, accumulated_submission_time=28866.821061, global_step=34241, preemption_count=0, score=28866.821061, test/ctc_loss=0.27463698387145996, test/num_examples=2472, test/wer=0.091849, total_duration=31696.892355, train/ctc_loss=0.22303934395313263, train/wer=0.079633, validation/ctc_loss=0.46157410740852356, validation/num_examples=5348, validation/wer=0.139259
I0214 10:11:55.166791 139518534461184 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.7193835973739624, loss=1.2524735927581787
I0214 10:13:10.874379 139518542853888 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.7098989486694336, loss=1.2860978841781616
I0214 10:14:28.118952 139518534461184 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7963191270828247, loss=1.2259371280670166
I0214 10:15:55.683730 139518542853888 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.7568305730819702, loss=1.2185181379318237
I0214 10:17:26.163753 139518534461184 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.8723297119140625, loss=1.2389096021652222
I0214 10:18:56.201824 139518542853888 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6899351477622986, loss=1.3073532581329346
I0214 10:20:27.331326 139518534461184 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6864416599273682, loss=1.3210350275039673
I0214 10:21:58.269865 139518542853888 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7643070816993713, loss=1.2739512920379639
I0214 10:23:21.931855 139518542853888 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.699138879776001, loss=1.2125658988952637
I0214 10:24:39.304673 139518534461184 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.7355852723121643, loss=1.2609068155288696
I0214 10:25:57.970969 139518542853888 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.8258663415908813, loss=1.2806353569030762
I0214 10:27:20.574476 139518534461184 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.7333495020866394, loss=1.177213191986084
I0214 10:28:49.798871 139518542853888 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7079612612724304, loss=1.2822483777999878
I0214 10:30:22.143347 139518534461184 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7634067535400391, loss=1.2745552062988281
I0214 10:31:56.332380 139518542853888 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.0478465557098389, loss=1.2896950244903564
I0214 10:33:26.730410 139518534461184 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.8640685081481934, loss=1.2451387643814087
I0214 10:34:58.605531 139518542853888 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.7296775579452515, loss=1.2350679636001587
I0214 10:35:10.031365 139688679413568 spec.py:321] Evaluating on the training split.
I0214 10:36:06.390341 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 10:36:58.702629 139688679413568 spec.py:349] Evaluating on the test split.
I0214 10:37:25.642798 139688679413568 submission_runner.py:408] Time since start: 33272.62s, 	Step: 35914, 	{'train/ctc_loss': Array(0.17427287, dtype=float32), 'train/wer': 0.06601888010823775, 'validation/ctc_loss': Array(0.45529997, dtype=float32), 'validation/wer': 0.13775258986068337, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26810637, dtype=float32), 'test/wer': 0.09042715251965146, 'test/num_examples': 2472, 'score': 30306.810252904892, 'total_duration': 33272.61613845825, 'accumulated_submission_time': 30306.810252904892, 'accumulated_eval_time': 2963.1705458164215, 'accumulated_logging_time': 1.1170177459716797}
I0214 10:37:25.679932 139518542853888 logging_writer.py:48] [35914] accumulated_eval_time=2963.170546, accumulated_logging_time=1.117018, accumulated_submission_time=30306.810253, global_step=35914, preemption_count=0, score=30306.810253, test/ctc_loss=0.2681063711643219, test/num_examples=2472, test/wer=0.090427, total_duration=33272.616138, train/ctc_loss=0.17427286505699158, train/wer=0.066019, validation/ctc_loss=0.455299973487854, validation/num_examples=5348, validation/wer=0.137753
I0214 10:38:31.175841 139518534461184 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7718378901481628, loss=1.2316958904266357
I0214 10:39:49.977418 139518542853888 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.6467259526252747, loss=1.2470331192016602
I0214 10:41:07.380864 139518534461184 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.7158054709434509, loss=1.166877269744873
I0214 10:42:28.443687 139518542853888 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.7007554769515991, loss=1.223152995109558
I0214 10:43:51.484708 139518534461184 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.6627181172370911, loss=1.1832197904586792
I0214 10:45:18.975849 139518542853888 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.850519597530365, loss=1.2980409860610962
I0214 10:46:46.701638 139518534461184 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6560896635055542, loss=1.202842354774475
I0214 10:48:18.335840 139518542853888 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.6846243143081665, loss=1.2179780006408691
I0214 10:49:44.971189 139518534461184 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6915538311004639, loss=1.196399211883545
I0214 10:51:13.582569 139518542853888 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.7397756576538086, loss=1.2637310028076172
I0214 10:52:45.443110 139518534461184 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7525656819343567, loss=1.2089694738388062
I0214 10:54:15.887484 139518542853888 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.8182485699653625, loss=1.2027908563613892
I0214 10:55:34.126043 139518534461184 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.7082417607307434, loss=1.2052431106567383
I0214 10:56:50.786205 139518542853888 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.756402313709259, loss=1.2164982557296753
I0214 10:58:15.472702 139518534461184 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6761050224304199, loss=1.242021918296814
I0214 10:59:40.724364 139518542853888 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7704009413719177, loss=1.2463223934173584
I0214 11:01:09.224611 139518534461184 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.7968494296073914, loss=1.220005750656128
I0214 11:01:25.780003 139688679413568 spec.py:321] Evaluating on the training split.
I0214 11:02:21.211324 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 11:03:14.058948 139688679413568 spec.py:349] Evaluating on the test split.
I0214 11:03:41.630091 139688679413568 submission_runner.py:408] Time since start: 34848.60s, 	Step: 37620, 	{'train/ctc_loss': Array(0.19530179, dtype=float32), 'train/wer': 0.0711178222212836, 'validation/ctc_loss': Array(0.45110688, dtype=float32), 'validation/wer': 0.13534858124873284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2616381, dtype=float32), 'test/wer': 0.08977718197144192, 'test/num_examples': 2472, 'score': 31746.826417922974, 'total_duration': 34848.603251457214, 'accumulated_submission_time': 31746.826417922974, 'accumulated_eval_time': 3099.013829946518, 'accumulated_logging_time': 1.1702604293823242}
I0214 11:03:41.670371 139518542853888 logging_writer.py:48] [37620] accumulated_eval_time=3099.013830, accumulated_logging_time=1.170260, accumulated_submission_time=31746.826418, global_step=37620, preemption_count=0, score=31746.826418, test/ctc_loss=0.2616381049156189, test/num_examples=2472, test/wer=0.089777, total_duration=34848.603251, train/ctc_loss=0.19530178606510162, train/wer=0.071118, validation/ctc_loss=0.45110687613487244, validation/num_examples=5348, validation/wer=0.135349
I0214 11:04:42.779979 139518534461184 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.7630176544189453, loss=1.2081985473632812
I0214 11:05:58.289601 139518542853888 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.690854012966156, loss=1.264178991317749
I0214 11:07:23.431544 139518534461184 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6664842963218689, loss=1.2127083539962769
I0214 11:08:54.584510 139518542853888 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.8459335565567017, loss=1.2111108303070068
I0214 11:10:22.759847 139518534461184 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.7030792832374573, loss=1.249091386795044
I0214 11:11:45.819279 139518542853888 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.6886822581291199, loss=1.2069898843765259
I0214 11:13:05.932074 139518534461184 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.9051055312156677, loss=1.2290844917297363
I0214 11:14:25.900348 139518542853888 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6882217526435852, loss=1.2242966890335083
I0214 11:15:48.754873 139518534461184 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.703626811504364, loss=1.2569116353988647
I0214 11:17:14.438229 139518542853888 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.7869445085525513, loss=1.2557345628738403
I0214 11:18:45.768925 139518534461184 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.8497236371040344, loss=1.2328299283981323
I0214 11:20:17.023849 139518542853888 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.7132478952407837, loss=1.2141709327697754
I0214 11:21:47.132750 139518534461184 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.8058344125747681, loss=1.2916061878204346
I0214 11:23:17.502108 139518542853888 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6514719128608704, loss=1.2441269159317017
I0214 11:24:48.913338 139518534461184 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6776602864265442, loss=1.2141050100326538
I0214 11:26:14.747168 139518542853888 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6808979511260986, loss=1.2402093410491943
I0214 11:27:33.332572 139518534461184 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.7127026915550232, loss=1.2242062091827393
I0214 11:27:42.107761 139688679413568 spec.py:321] Evaluating on the training split.
I0214 11:28:36.371516 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 11:29:30.318699 139688679413568 spec.py:349] Evaluating on the test split.
I0214 11:29:57.686522 139688679413568 submission_runner.py:408] Time since start: 36424.66s, 	Step: 39313, 	{'train/ctc_loss': Array(0.2397092, dtype=float32), 'train/wer': 0.0867527649257172, 'validation/ctc_loss': Array(0.4422507, dtype=float32), 'validation/wer': 0.13199841663689815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25463516, dtype=float32), 'test/wer': 0.08488209128023887, 'test/num_examples': 2472, 'score': 33187.17974424362, 'total_duration': 36424.66038656235, 'accumulated_submission_time': 33187.17974424362, 'accumulated_eval_time': 3234.586481332779, 'accumulated_logging_time': 1.2274115085601807}
I0214 11:29:57.725723 139517959173888 logging_writer.py:48] [39313] accumulated_eval_time=3234.586481, accumulated_logging_time=1.227412, accumulated_submission_time=33187.179744, global_step=39313, preemption_count=0, score=33187.179744, test/ctc_loss=0.25463515520095825, test/num_examples=2472, test/wer=0.084882, total_duration=36424.660387, train/ctc_loss=0.23970919847488403, train/wer=0.086753, validation/ctc_loss=0.44225069880485535, validation/num_examples=5348, validation/wer=0.131998
I0214 11:31:04.165579 139517950781184 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.822960376739502, loss=1.2152512073516846
I0214 11:32:19.687994 139517959173888 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.6948609948158264, loss=1.2268084287643433
I0214 11:33:35.403758 139517950781184 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.6636973023414612, loss=1.2052897214889526
I0214 11:35:03.855521 139517959173888 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7830392718315125, loss=1.2071325778961182
I0214 11:36:33.640597 139517950781184 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.8018486499786377, loss=1.2406646013259888
I0214 11:38:03.936877 139517959173888 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7159905433654785, loss=1.1925156116485596
I0214 11:39:36.644097 139517950781184 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.8456892967224121, loss=1.2246155738830566
I0214 11:41:06.868912 139517959173888 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.745660662651062, loss=1.2008899450302124
I0214 11:42:35.648806 139517303813888 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7495653033256531, loss=1.1714991331100464
I0214 11:43:54.445241 139517295421184 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.8294360041618347, loss=1.201965570449829
I0214 11:45:15.060351 139517303813888 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.8155006170272827, loss=1.2191091775894165
I0214 11:46:35.062228 139517295421184 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.755810022354126, loss=1.21324622631073
I0214 11:47:59.921261 139517303813888 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6917399764060974, loss=1.1307097673416138
I0214 11:49:27.746711 139517295421184 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.933729350566864, loss=1.2429393529891968
I0214 11:50:57.061683 139517303813888 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7607271075248718, loss=1.223457932472229
I0214 11:52:28.761203 139517295421184 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.8814648985862732, loss=1.1678036451339722
I0214 11:53:58.056485 139688679413568 spec.py:321] Evaluating on the training split.
I0214 11:54:51.853066 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 11:55:44.455040 139688679413568 spec.py:349] Evaluating on the test split.
I0214 11:56:10.840210 139688679413568 submission_runner.py:408] Time since start: 37997.81s, 	Step: 41000, 	{'train/ctc_loss': Array(0.24868596, dtype=float32), 'train/wer': 0.08968174222952531, 'validation/ctc_loss': Array(0.43414676, dtype=float32), 'validation/wer': 0.12913098467806558, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25042498, dtype=float32), 'test/wer': 0.08313529543192574, 'test/num_examples': 2472, 'score': 34627.428320646286, 'total_duration': 37997.814823150635, 'accumulated_submission_time': 34627.428320646286, 'accumulated_eval_time': 3367.3648619651794, 'accumulated_logging_time': 1.2825305461883545}
I0214 11:56:10.877951 139518542853888 logging_writer.py:48] [41000] accumulated_eval_time=3367.364862, accumulated_logging_time=1.282531, accumulated_submission_time=34627.428321, global_step=41000, preemption_count=0, score=34627.428321, test/ctc_loss=0.25042498111724854, test/num_examples=2472, test/wer=0.083135, total_duration=37997.814823, train/ctc_loss=0.24868595600128174, train/wer=0.089682, validation/ctc_loss=0.4341467618942261, validation/num_examples=5348, validation/wer=0.129131
I0214 11:56:11.749686 139518534461184 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.880517840385437, loss=1.203489899635315
I0214 11:57:27.118472 139518542853888 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.6991766691207886, loss=1.174477219581604
I0214 11:58:46.338182 139518000133888 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.0461759567260742, loss=1.173224925994873
I0214 12:00:04.118014 139517991741184 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.7039884328842163, loss=1.221614956855774
I0214 12:01:23.484479 139518000133888 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.7619431614875793, loss=1.1120641231536865
I0214 12:02:43.526252 139517991741184 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6793921589851379, loss=1.242369294166565
I0214 12:04:09.586510 139518000133888 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.7281695008277893, loss=1.1637343168258667
I0214 12:05:36.918275 139517991741184 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.8889750838279724, loss=1.2371058464050293
I0214 12:07:07.478584 139518000133888 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.7055014371871948, loss=1.2370659112930298
I0214 12:08:36.499608 139517991741184 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.9256344437599182, loss=1.2075239419937134
I0214 12:10:07.190144 139518000133888 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.9935753345489502, loss=1.1909618377685547
I0214 12:11:37.708760 139517991741184 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.7196744084358215, loss=1.1729012727737427
I0214 12:13:07.051359 139518000133888 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.7002159953117371, loss=1.1519616842269897
I0214 12:14:33.651283 139518542853888 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7953815460205078, loss=1.1185070276260376
I0214 12:15:51.480051 139518534461184 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.863499104976654, loss=1.1674684286117554
I0214 12:17:09.871900 139518542853888 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.8085237741470337, loss=1.2429076433181763
I0214 12:18:33.205036 139518534461184 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.7330839037895203, loss=1.1520320177078247
I0214 12:19:58.791484 139518542853888 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.680435061454773, loss=1.19393789768219
I0214 12:20:10.971306 139688679413568 spec.py:321] Evaluating on the training split.
I0214 12:21:04.121869 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 12:21:56.188619 139688679413568 spec.py:349] Evaluating on the test split.
I0214 12:22:23.319528 139688679413568 submission_runner.py:408] Time since start: 39570.29s, 	Step: 42715, 	{'train/ctc_loss': Array(0.28319895, dtype=float32), 'train/wer': 0.10321086159191815, 'validation/ctc_loss': Array(0.42378053, dtype=float32), 'validation/wer': 0.12779864255577975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23770487, dtype=float32), 'test/wer': 0.07976357321308879, 'test/num_examples': 2472, 'score': 36067.43534851074, 'total_duration': 39570.2938041687, 'accumulated_submission_time': 36067.43534851074, 'accumulated_eval_time': 3499.707387447357, 'accumulated_logging_time': 1.3385326862335205}
I0214 12:22:23.360889 139518035973888 logging_writer.py:48] [42715] accumulated_eval_time=3499.707387, accumulated_logging_time=1.338533, accumulated_submission_time=36067.435349, global_step=42715, preemption_count=0, score=36067.435349, test/ctc_loss=0.23770487308502197, test/num_examples=2472, test/wer=0.079764, total_duration=39570.293804, train/ctc_loss=0.2831989526748657, train/wer=0.103211, validation/ctc_loss=0.42378053069114685, validation/num_examples=5348, validation/wer=0.127799
I0214 12:23:28.283092 139518027581184 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.6992340683937073, loss=1.1969633102416992
I0214 12:24:44.143335 139518035973888 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.6875495314598083, loss=1.1832890510559082
I0214 12:26:08.098112 139518027581184 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7436788082122803, loss=1.1885818243026733
I0214 12:27:36.234796 139518035973888 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.6936056613922119, loss=1.212579607963562
I0214 12:29:05.182572 139518027581184 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.9134781360626221, loss=1.2116395235061646
I0214 12:30:34.168303 139518035973888 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.857010006904602, loss=1.1582436561584473
I0214 12:31:50.658849 139518027581184 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.7731313705444336, loss=1.1716972589492798
I0214 12:33:06.584216 139518035973888 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7343838214874268, loss=1.185048222541809
I0214 12:34:26.607278 139518027581184 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.8024620413780212, loss=1.1497043371200562
I0214 12:35:51.777534 139518035973888 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7433942556381226, loss=1.1374002695083618
I0214 12:37:20.421859 139518027581184 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6104370355606079, loss=1.1322327852249146
I0214 12:38:48.394323 139518035973888 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.8110908269882202, loss=1.1122610569000244
I0214 12:40:19.109488 139518027581184 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.8302569389343262, loss=1.169574499130249
I0214 12:41:50.426034 139518035973888 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7561554908752441, loss=1.1560968160629272
I0214 12:43:20.934017 139518027581184 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.8799511194229126, loss=1.1636040210723877
I0214 12:44:55.326754 139518035973888 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7591491341590881, loss=1.1209819316864014
I0214 12:46:11.131865 139518027581184 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.8129187822341919, loss=1.1593486070632935
I0214 12:46:23.690294 139688679413568 spec.py:321] Evaluating on the training split.
I0214 12:47:15.824666 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 12:48:09.435903 139688679413568 spec.py:349] Evaluating on the test split.
I0214 12:48:36.151256 139688679413568 submission_runner.py:408] Time since start: 41143.13s, 	Step: 44418, 	{'train/ctc_loss': Array(0.2481551, dtype=float32), 'train/wer': 0.08901435981371689, 'validation/ctc_loss': Array(0.41698194, dtype=float32), 'validation/wer': 0.12533670602546898, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24019764, dtype=float32), 'test/wer': 0.08090102167245547, 'test/num_examples': 2472, 'score': 37507.68049240112, 'total_duration': 41143.12516140938, 'accumulated_submission_time': 37507.68049240112, 'accumulated_eval_time': 3632.162237882614, 'accumulated_logging_time': 1.3978519439697266}
I0214 12:48:36.192564 139518035973888 logging_writer.py:48] [44418] accumulated_eval_time=3632.162238, accumulated_logging_time=1.397852, accumulated_submission_time=37507.680492, global_step=44418, preemption_count=0, score=37507.680492, test/ctc_loss=0.24019764363765717, test/num_examples=2472, test/wer=0.080901, total_duration=41143.125161, train/ctc_loss=0.24815510213375092, train/wer=0.089014, validation/ctc_loss=0.41698193550109863, validation/num_examples=5348, validation/wer=0.125337
I0214 12:49:38.821387 139518027581184 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.7261422872543335, loss=1.139748215675354
I0214 12:50:54.557254 139518035973888 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.726853609085083, loss=1.1669261455535889
I0214 12:52:10.341392 139518027581184 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.8077861666679382, loss=1.2076568603515625
I0214 12:53:31.725270 139518035973888 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.9180663824081421, loss=1.2015491724014282
I0214 12:55:03.658149 139518027581184 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6759551167488098, loss=1.1802490949630737
I0214 12:56:34.647781 139518035973888 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.742058277130127, loss=1.1857126951217651
I0214 12:58:06.287457 139518027581184 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.7923753261566162, loss=1.1504895687103271
I0214 12:59:35.556360 139518035973888 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.6510319113731384, loss=1.1502715349197388
I0214 13:01:07.602828 139518027581184 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7834931015968323, loss=1.208602786064148
I0214 13:02:29.203976 139518035973888 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.8149973750114441, loss=1.1117503643035889
I0214 13:03:44.937577 139518027581184 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.919905960559845, loss=1.0834712982177734
I0214 13:05:03.296751 139518035973888 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.7987969517707825, loss=1.1625064611434937
I0214 13:06:23.838718 139518027581184 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.7596838474273682, loss=1.1045634746551514
I0214 13:07:51.645505 139518035973888 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.693570077419281, loss=1.1350762844085693
I0214 13:09:22.341070 139518027581184 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.6733657717704773, loss=1.0996171236038208
I0214 13:10:52.264769 139518035973888 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7559226751327515, loss=1.1623884439468384
I0214 13:12:22.313383 139518027581184 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7082951664924622, loss=1.1232988834381104
I0214 13:12:36.699448 139688679413568 spec.py:321] Evaluating on the training split.
I0214 13:13:30.882291 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 13:14:23.132090 139688679413568 spec.py:349] Evaluating on the test split.
I0214 13:14:49.841215 139688679413568 submission_runner.py:408] Time since start: 42716.81s, 	Step: 46117, 	{'train/ctc_loss': Array(0.22114661, dtype=float32), 'train/wer': 0.08169752708405242, 'validation/ctc_loss': Array(0.40540525, dtype=float32), 'validation/wer': 0.11971769794452436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23128738, dtype=float32), 'test/wer': 0.07724493733877684, 'test/num_examples': 2472, 'score': 38948.10423493385, 'total_duration': 42716.81401634216, 'accumulated_submission_time': 38948.10423493385, 'accumulated_eval_time': 3765.296792268753, 'accumulated_logging_time': 1.455432415008545}
I0214 13:14:49.884148 139516898621184 logging_writer.py:48] [46117] accumulated_eval_time=3765.296792, accumulated_logging_time=1.455432, accumulated_submission_time=38948.104235, global_step=46117, preemption_count=0, score=38948.104235, test/ctc_loss=0.2312873750925064, test/num_examples=2472, test/wer=0.077245, total_duration=42716.814016, train/ctc_loss=0.2211466133594513, train/wer=0.081698, validation/ctc_loss=0.4054052531719208, validation/num_examples=5348, validation/wer=0.119718
I0214 13:15:53.407944 139516890228480 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.8331421613693237, loss=1.093052625656128
I0214 13:17:09.069565 139516898621184 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.152997612953186, loss=1.1365176439285278
I0214 13:18:32.249337 139516898621184 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.8188578486442566, loss=1.1964977979660034
I0214 13:19:50.578097 139516890228480 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.7245957255363464, loss=1.0846953392028809
I0214 13:21:12.234325 139516898621184 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.7081454396247864, loss=1.1025564670562744
I0214 13:22:35.156861 139516890228480 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7476579546928406, loss=1.103101372718811
I0214 13:24:00.104820 139516898621184 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.8225290775299072, loss=1.133807897567749
I0214 13:25:31.715837 139516890228480 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7475756406784058, loss=1.1499223709106445
I0214 13:27:01.803366 139516898621184 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8256891369819641, loss=1.1514275074005127
I0214 13:28:30.683432 139516890228480 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.7674885392189026, loss=1.0840901136398315
I0214 13:29:59.578475 139516898621184 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.7551330327987671, loss=1.1403675079345703
I0214 13:31:30.593271 139516890228480 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7804188132286072, loss=1.1308984756469727
I0214 13:33:03.623709 139516898621184 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.8539159297943115, loss=1.102909803390503
I0214 13:34:20.134946 139516890228480 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.945860743522644, loss=1.1429071426391602
I0214 13:35:38.311807 139516898621184 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.7762181758880615, loss=1.1502001285552979
I0214 13:36:59.962218 139516890228480 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.7880145311355591, loss=1.1166858673095703
I0214 13:38:26.461588 139516898621184 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.8234433531761169, loss=1.0912607908248901
I0214 13:38:50.921251 139688679413568 spec.py:321] Evaluating on the training split.
I0214 13:39:45.277311 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 13:40:39.637739 139688679413568 spec.py:349] Evaluating on the test split.
I0214 13:41:07.243364 139688679413568 submission_runner.py:408] Time since start: 44294.22s, 	Step: 47828, 	{'train/ctc_loss': Array(0.18673953, dtype=float32), 'train/wer': 0.07121339003143047, 'validation/ctc_loss': Array(0.3940457, dtype=float32), 'validation/wer': 0.11766125684273536, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22929518, dtype=float32), 'test/wer': 0.07578250360530539, 'test/num_examples': 2472, 'score': 40389.05780625343, 'total_duration': 44294.217025756836, 'accumulated_submission_time': 40389.05780625343, 'accumulated_eval_time': 3901.612589597702, 'accumulated_logging_time': 1.514012098312378}
I0214 13:41:07.288419 139517820933888 logging_writer.py:48] [47828] accumulated_eval_time=3901.612590, accumulated_logging_time=1.514012, accumulated_submission_time=40389.057806, global_step=47828, preemption_count=0, score=40389.057806, test/ctc_loss=0.22929517924785614, test/num_examples=2472, test/wer=0.075783, total_duration=44294.217026, train/ctc_loss=0.18673953413963318, train/wer=0.071213, validation/ctc_loss=0.39404571056365967, validation/num_examples=5348, validation/wer=0.117661
I0214 13:42:02.836683 139517812541184 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.7439400553703308, loss=1.1191773414611816
I0214 13:43:18.709397 139517820933888 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.770534873008728, loss=1.1321909427642822
I0214 13:44:44.935012 139517812541184 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.7482879161834717, loss=1.1420674324035645
I0214 13:46:16.094723 139517820933888 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.7859355807304382, loss=1.1421550512313843
I0214 13:47:44.423272 139517812541184 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.0932048559188843, loss=1.1383792161941528
I0214 13:49:13.065490 139517820933888 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7939303517341614, loss=1.1142029762268066
I0214 13:50:33.861546 139517820933888 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.8059174418449402, loss=1.1301640272140503
I0214 13:51:51.762882 139517812541184 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.8195538520812988, loss=1.0522693395614624
I0214 13:53:12.659692 139517820933888 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.8903279304504395, loss=1.100053071975708
I0214 13:54:37.364893 139517812541184 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.8279147744178772, loss=1.0475398302078247
I0214 13:56:03.328742 139517820933888 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.8171042203903198, loss=1.0929840803146362
I0214 13:57:33.232263 139517812541184 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9503799080848694, loss=1.156651258468628
I0214 13:59:03.398164 139517820933888 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.8022110462188721, loss=1.1663291454315186
I0214 14:00:31.883733 139517812541184 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.9283154606819153, loss=1.1209876537322998
I0214 14:02:03.609116 139517820933888 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.8293706178665161, loss=1.0965771675109863
I0214 14:03:34.150749 139517812541184 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.10923433303833, loss=1.1528863906860352
I0214 14:05:00.495175 139517165573888 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.8196714520454407, loss=1.0648024082183838
I0214 14:05:07.762575 139688679413568 spec.py:321] Evaluating on the training split.
I0214 14:06:03.319626 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 14:06:56.089999 139688679413568 spec.py:349] Evaluating on the test split.
I0214 14:07:23.446342 139688679413568 submission_runner.py:408] Time since start: 45870.42s, 	Step: 49511, 	{'train/ctc_loss': Array(0.20579855, dtype=float32), 'train/wer': 0.0762191313604197, 'validation/ctc_loss': Array(0.39688087, dtype=float32), 'validation/wer': 0.1164158065979899, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2227393, dtype=float32), 'test/wer': 0.07385290354030832, 'test/num_examples': 2472, 'score': 41829.449006557465, 'total_duration': 45870.42122077942, 'accumulated_submission_time': 41829.449006557465, 'accumulated_eval_time': 4037.291281223297, 'accumulated_logging_time': 1.5761985778808594}
I0214 14:07:23.483905 139518251013888 logging_writer.py:48] [49511] accumulated_eval_time=4037.291281, accumulated_logging_time=1.576199, accumulated_submission_time=41829.449007, global_step=49511, preemption_count=0, score=41829.449007, test/ctc_loss=0.2227392941713333, test/num_examples=2472, test/wer=0.073853, total_duration=45870.421221, train/ctc_loss=0.20579855144023895, train/wer=0.076219, validation/ctc_loss=0.3968808650970459, validation/num_examples=5348, validation/wer=0.116416
I0214 14:08:31.989120 139518242621184 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.2680261135101318, loss=1.1346153020858765
I0214 14:09:47.633446 139518251013888 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.7722905874252319, loss=1.0615184307098389
I0214 14:11:03.247982 139518242621184 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7330688834190369, loss=1.0660189390182495
I0214 14:12:21.544694 139518251013888 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7900686264038086, loss=1.07173490524292
I0214 14:13:53.408947 139518242621184 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.887495756149292, loss=1.1824067831039429
I0214 14:15:23.188022 139518251013888 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.8937148451805115, loss=1.1198164224624634
I0214 14:16:55.468928 139518242621184 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.785388708114624, loss=1.0883643627166748
I0214 14:18:25.743856 139518251013888 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.9203968644142151, loss=1.1078879833221436
I0214 14:19:55.073982 139518242621184 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.7378470301628113, loss=1.1020854711532593
I0214 14:21:25.735777 139517923333888 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.842342734336853, loss=1.0848491191864014
I0214 14:22:43.248561 139517914941184 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.8764615654945374, loss=1.1070606708526611
I0214 14:24:02.384037 139517923333888 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8589604496955872, loss=1.0694293975830078
I0214 14:25:26.675894 139517914941184 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.9384645819664001, loss=1.1160119771957397
I0214 14:26:51.592357 139517923333888 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.773820161819458, loss=1.0466684103012085
I0214 14:28:22.372596 139517914941184 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.7890357375144958, loss=1.089272141456604
I0214 14:29:53.176178 139517923333888 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.8175133466720581, loss=1.095150351524353
I0214 14:31:23.376339 139517914941184 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.8883863091468811, loss=1.0998214483261108
I0214 14:31:23.841166 139688679413568 spec.py:321] Evaluating on the training split.
I0214 14:32:19.252066 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 14:33:13.794517 139688679413568 spec.py:349] Evaluating on the test split.
I0214 14:33:40.227245 139688679413568 submission_runner.py:408] Time since start: 47447.20s, 	Step: 51202, 	{'train/ctc_loss': Array(0.18011282, dtype=float32), 'train/wer': 0.06902271946400962, 'validation/ctc_loss': Array(0.38041675, dtype=float32), 'validation/wer': 0.11233188835359201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21779981, dtype=float32), 'test/wer': 0.07271545508094164, 'test/num_examples': 2472, 'score': 43269.72302722931, 'total_duration': 47447.20176792145, 'accumulated_submission_time': 43269.72302722931, 'accumulated_eval_time': 4173.671866893768, 'accumulated_logging_time': 1.6306617259979248}
I0214 14:33:40.270805 139517631493888 logging_writer.py:48] [51202] accumulated_eval_time=4173.671867, accumulated_logging_time=1.630662, accumulated_submission_time=43269.723027, global_step=51202, preemption_count=0, score=43269.723027, test/ctc_loss=0.2177998125553131, test/num_examples=2472, test/wer=0.072715, total_duration=47447.201768, train/ctc_loss=0.180112823843956, train/wer=0.069023, validation/ctc_loss=0.38041675090789795, validation/num_examples=5348, validation/wer=0.112332
I0214 14:34:55.413993 139517623101184 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.7540944814682007, loss=1.1044559478759766
I0214 14:36:11.110551 139517631493888 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.8012297749519348, loss=1.0829426050186157
I0214 14:37:41.512490 139517303813888 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7639917135238647, loss=1.0808931589126587
I0214 14:38:58.474133 139517295421184 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.826918363571167, loss=1.071526288986206
I0214 14:40:17.422609 139517303813888 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.8313637375831604, loss=1.0518120527267456
I0214 14:41:41.465467 139517295421184 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.7844783067703247, loss=1.0871622562408447
I0214 14:43:05.249860 139517303813888 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.7761386632919312, loss=1.0552096366882324
I0214 14:44:35.623590 139517295421184 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.7859789729118347, loss=1.0461708307266235
I0214 14:46:03.087580 139517303813888 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.92193204164505, loss=1.0464783906936646
I0214 14:47:33.387670 139517295421184 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.817264974117279, loss=1.0784032344818115
I0214 14:49:05.204961 139517303813888 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.9143304824829102, loss=1.0436279773712158
I0214 14:50:37.793584 139517295421184 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.8897444605827332, loss=1.071234107017517
I0214 14:52:08.210730 139517303813888 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.0156363248825073, loss=1.13336980342865
I0214 14:53:30.142172 139517303813888 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.9734680652618408, loss=1.0475366115570068
I0214 14:54:47.866809 139517295421184 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.7878991365432739, loss=1.0507242679595947
I0214 14:56:06.224534 139517303813888 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.9278011918067932, loss=1.031875491142273
I0214 14:57:26.295752 139517295421184 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.9222519993782043, loss=1.0574629306793213
I0214 14:57:40.249104 139688679413568 spec.py:321] Evaluating on the training split.
I0214 14:58:35.045322 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 14:59:28.837565 139688679413568 spec.py:349] Evaluating on the test split.
I0214 14:59:56.338612 139688679413568 submission_runner.py:408] Time since start: 49023.31s, 	Step: 52918, 	{'train/ctc_loss': Array(0.17563458, dtype=float32), 'train/wer': 0.06664750105772464, 'validation/ctc_loss': Array(0.37358853, dtype=float32), 'validation/wer': 0.11024648329262288, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21114749, dtype=float32), 'test/wer': 0.06944529076026243, 'test/num_examples': 2472, 'score': 44709.526916742325, 'total_duration': 49023.30984520912, 'accumulated_submission_time': 44709.526916742325, 'accumulated_eval_time': 4309.752601146698, 'accumulated_logging_time': 1.781308889389038}
I0214 14:59:56.391434 139517303813888 logging_writer.py:48] [52918] accumulated_eval_time=4309.752601, accumulated_logging_time=1.781309, accumulated_submission_time=44709.526917, global_step=52918, preemption_count=0, score=44709.526917, test/ctc_loss=0.2111474871635437, test/num_examples=2472, test/wer=0.069445, total_duration=49023.309845, train/ctc_loss=0.17563457787036896, train/wer=0.066648, validation/ctc_loss=0.37358853220939636, validation/num_examples=5348, validation/wer=0.110246
I0214 15:00:59.082489 139517295421184 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.1551599502563477, loss=1.0460466146469116
I0214 15:02:14.795151 139517303813888 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.8563950657844543, loss=1.0777148008346558
I0214 15:03:42.819820 139517295421184 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.8347050547599792, loss=1.0517826080322266
I0214 15:05:13.416406 139517303813888 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.8405312299728394, loss=1.040362000465393
I0214 15:06:45.124140 139517295421184 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.830060601234436, loss=1.0920021533966064
I0214 15:08:13.778250 139517303813888 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.7756679654121399, loss=1.0719192028045654
I0214 15:09:40.944414 139517303813888 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8050565123558044, loss=1.0748364925384521
I0214 15:10:57.280387 139517295421184 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.9700729846954346, loss=1.0195510387420654
I0214 15:12:14.411350 139517303813888 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.8219699859619141, loss=1.0382081270217896
I0214 15:13:36.305595 139517295421184 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.9450501799583435, loss=1.0341490507125854
I0214 15:15:01.618726 139517303813888 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8826979994773865, loss=1.0806829929351807
I0214 15:16:30.891053 139517295421184 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.0346544981002808, loss=1.1129961013793945
I0214 15:18:03.155575 139517303813888 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.9668433666229248, loss=1.0423920154571533
I0214 15:19:32.390917 139517295421184 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9416258335113525, loss=1.0670697689056396
I0214 15:21:00.807274 139517303813888 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.8185337781906128, loss=1.029267430305481
I0214 15:22:31.509353 139517295421184 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.7995409369468689, loss=1.0500446557998657
I0214 15:23:56.513345 139688679413568 spec.py:321] Evaluating on the training split.
I0214 15:24:51.003027 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 15:25:45.037000 139688679413568 spec.py:349] Evaluating on the test split.
I0214 15:26:11.380400 139688679413568 submission_runner.py:408] Time since start: 50598.35s, 	Step: 54594, 	{'train/ctc_loss': Array(0.16827415, dtype=float32), 'train/wer': 0.06451345755693581, 'validation/ctc_loss': Array(0.36485976, dtype=float32), 'validation/wer': 0.10857622831323556, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20493606, dtype=float32), 'test/wer': 0.07033900026405053, 'test/num_examples': 2472, 'score': 46149.565438985825, 'total_duration': 50598.354439258575, 'accumulated_submission_time': 46149.565438985825, 'accumulated_eval_time': 4444.613988637924, 'accumulated_logging_time': 1.8514833450317383}
I0214 15:26:11.422480 139517303813888 logging_writer.py:48] [54594] accumulated_eval_time=4444.613989, accumulated_logging_time=1.851483, accumulated_submission_time=46149.565439, global_step=54594, preemption_count=0, score=46149.565439, test/ctc_loss=0.20493605732917786, test/num_examples=2472, test/wer=0.070339, total_duration=50598.354439, train/ctc_loss=0.1682741492986679, train/wer=0.064513, validation/ctc_loss=0.36485975980758667, validation/num_examples=5348, validation/wer=0.108576
I0214 15:26:16.796478 139517295421184 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.8806686997413635, loss=1.0303435325622559
I0214 15:27:32.518763 139517303813888 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.856860876083374, loss=1.0844453573226929
I0214 15:28:48.257389 139517295421184 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.9192923307418823, loss=1.1118929386138916
I0214 15:30:04.400542 139517303813888 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.1374069452285767, loss=1.0514233112335205
I0214 15:31:20.215415 139517295421184 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.8549607396125793, loss=1.1156357526779175
I0214 15:32:49.774218 139517303813888 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.8049679398536682, loss=1.0056815147399902
I0214 15:34:18.495395 139517295421184 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.9293919205665588, loss=1.0598134994506836
I0214 15:35:46.586929 139517303813888 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.8452551364898682, loss=0.9965173602104187
I0214 15:37:13.890279 139517295421184 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.967967689037323, loss=1.031389594078064
I0214 15:38:42.262147 139517303813888 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.8408824801445007, loss=1.0870007276535034
I0214 15:40:12.945976 139517295421184 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.2487694025039673, loss=1.042534351348877
I0214 15:41:33.808594 139518251013888 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.7827050685882568, loss=1.0378656387329102
I0214 15:42:51.510791 139518242621184 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.9991966485977173, loss=1.0458853244781494
I0214 15:44:09.597064 139518251013888 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.8772426247596741, loss=1.0340007543563843
I0214 15:45:32.251025 139518242621184 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.1948583126068115, loss=1.0515958070755005
I0214 15:46:59.942659 139518251013888 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.225482702255249, loss=1.0038968324661255
I0214 15:48:30.006404 139518242621184 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.0643924474716187, loss=0.9818180203437805
I0214 15:50:00.364956 139518251013888 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.9338895678520203, loss=1.0251742601394653
I0214 15:50:11.464054 139688679413568 spec.py:321] Evaluating on the training split.
I0214 15:51:04.830233 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 15:51:58.278521 139688679413568 spec.py:349] Evaluating on the test split.
I0214 15:52:24.903339 139688679413568 submission_runner.py:408] Time since start: 52171.88s, 	Step: 56314, 	{'train/ctc_loss': Array(0.16584644, dtype=float32), 'train/wer': 0.06282980209012969, 'validation/ctc_loss': Array(0.35868764, dtype=float32), 'validation/wer': 0.10561224982380259, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19563222, dtype=float32), 'test/wer': 0.06605325696179391, 'test/num_examples': 2472, 'score': 47589.52333641052, 'total_duration': 52171.8772919178, 'accumulated_submission_time': 47589.52333641052, 'accumulated_eval_time': 4578.047208547592, 'accumulated_logging_time': 1.9099171161651611}
I0214 15:52:24.949374 139517662209792 logging_writer.py:48] [56314] accumulated_eval_time=4578.047209, accumulated_logging_time=1.909917, accumulated_submission_time=47589.523336, global_step=56314, preemption_count=0, score=47589.523336, test/ctc_loss=0.1956322193145752, test/num_examples=2472, test/wer=0.066053, total_duration=52171.877292, train/ctc_loss=0.16584643721580505, train/wer=0.062830, validation/ctc_loss=0.3586876392364502, validation/num_examples=5348, validation/wer=0.105612
I0214 15:53:30.940446 139517653817088 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.0187218189239502, loss=1.0550755262374878
I0214 15:54:46.963237 139517662209792 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9463489651679993, loss=1.0181667804718018
I0214 15:56:13.323697 139517653817088 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.0519702434539795, loss=1.0554354190826416
I0214 15:57:40.421852 139517662209792 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.9985617995262146, loss=0.9951866865158081
I0214 15:58:56.160349 139517653817088 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.9847535490989685, loss=0.9850557446479797
I0214 16:00:15.051884 139517662209792 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.8607491850852966, loss=0.9896292090415955
I0214 16:01:39.487226 139517653817088 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.9239462614059448, loss=1.038722276687622
I0214 16:03:08.096181 139517662209792 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.8557979464530945, loss=1.0361719131469727
I0214 16:04:37.656898 139517653817088 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9023913741111755, loss=0.9867546558380127
I0214 16:06:08.197206 139517662209792 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.0187451839447021, loss=1.0502413511276245
I0214 16:07:39.524155 139517653817088 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.9674642086029053, loss=0.9807891845703125
I0214 16:09:09.792545 139517662209792 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.864700973033905, loss=1.0132235288619995
I0214 16:10:38.750887 139517653817088 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.8267489075660706, loss=1.0465716123580933
I0214 16:12:11.305032 139517334529792 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.0306867361068726, loss=1.0249121189117432
I0214 16:13:29.796257 139517326137088 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.8547435998916626, loss=1.0157941579818726
I0214 16:14:48.164613 139517334529792 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.1393550634384155, loss=0.9923132061958313
I0214 16:16:07.358363 139517326137088 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.9166300892829895, loss=0.9505000114440918
I0214 16:16:24.913413 139688679413568 spec.py:321] Evaluating on the training split.
I0214 16:17:18.815478 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 16:18:11.169469 139688679413568 spec.py:349] Evaluating on the test split.
I0214 16:18:38.172398 139688679413568 submission_runner.py:408] Time since start: 53745.15s, 	Step: 58024, 	{'train/ctc_loss': Array(0.16346063, dtype=float32), 'train/wer': 0.06120737537982335, 'validation/ctc_loss': Array(0.35139942, dtype=float32), 'validation/wer': 0.10307307606901146, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19396327, dtype=float32), 'test/wer': 0.06371742530416591, 'test/num_examples': 2472, 'score': 49029.40422821045, 'total_duration': 53745.149267435074, 'accumulated_submission_time': 49029.40422821045, 'accumulated_eval_time': 4711.303059339523, 'accumulated_logging_time': 1.9719979763031006}
I0214 16:18:38.207699 139517334529792 logging_writer.py:48] [58024] accumulated_eval_time=4711.303059, accumulated_logging_time=1.971998, accumulated_submission_time=49029.404228, global_step=58024, preemption_count=0, score=49029.404228, test/ctc_loss=0.19396327435970306, test/num_examples=2472, test/wer=0.063717, total_duration=53745.149267, train/ctc_loss=0.1634606271982193, train/wer=0.061207, validation/ctc_loss=0.35139942169189453, validation/num_examples=5348, validation/wer=0.103073
I0214 16:19:36.469444 139517326137088 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.8829891085624695, loss=1.0218631029129028
I0214 16:20:52.407721 139517334529792 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.9979568123817444, loss=1.025282621383667
I0214 16:22:15.225557 139517326137088 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.8617106676101685, loss=1.0100433826446533
I0214 16:23:45.508845 139517334529792 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.120496392250061, loss=0.9706481099128723
I0214 16:25:14.655028 139517326137088 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.0554581880569458, loss=0.9692441821098328
I0214 16:26:44.910611 139517334529792 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.8638667464256287, loss=0.9929894804954529
I0214 16:28:14.808756 139517326137088 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.9943310022354126, loss=1.0066269636154175
I0214 16:29:35.323938 139517334529792 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.6085485219955444, loss=0.9765048027038574
I0214 16:30:53.691876 139517326137088 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.0972594022750854, loss=0.9994896054267883
I0214 16:32:15.979349 139517334529792 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.8496106863021851, loss=1.0192396640777588
I0214 16:33:41.449815 139517326137088 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.9720810055732727, loss=1.0048836469650269
I0214 16:35:09.496496 139517334529792 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.8973917365074158, loss=1.0053200721740723
I0214 16:36:39.883956 139517326137088 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.8986012935638428, loss=0.9857967495918274
I0214 16:38:07.611960 139517334529792 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9480394124984741, loss=1.0156267881393433
I0214 16:39:34.938370 139517326137088 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.8993760943412781, loss=1.0141799449920654
I0214 16:41:05.715696 139517334529792 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.1568174362182617, loss=0.9818137884140015
I0214 16:42:34.450728 139517326137088 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.9418277144432068, loss=0.992097020149231
I0214 16:42:38.461036 139688679413568 spec.py:321] Evaluating on the training split.
I0214 16:43:32.826379 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 16:44:27.280120 139688679413568 spec.py:349] Evaluating on the test split.
I0214 16:44:54.639724 139688679413568 submission_runner.py:408] Time since start: 55321.61s, 	Step: 59706, 	{'train/ctc_loss': Array(0.13584337, dtype=float32), 'train/wer': 0.05221714080749658, 'validation/ctc_loss': Array(0.3399327, dtype=float32), 'validation/wer': 0.09979049402859708, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18521993, dtype=float32), 'test/wer': 0.06188938313732659, 'test/num_examples': 2472, 'score': 50469.57779741287, 'total_duration': 55321.613298892975, 'accumulated_submission_time': 50469.57779741287, 'accumulated_eval_time': 4847.475305557251, 'accumulated_logging_time': 2.020612955093384}
I0214 16:44:54.689495 139516898621184 logging_writer.py:48] [59706] accumulated_eval_time=4847.475306, accumulated_logging_time=2.020613, accumulated_submission_time=50469.577797, global_step=59706, preemption_count=0, score=50469.577797, test/ctc_loss=0.1852199286222458, test/num_examples=2472, test/wer=0.061889, total_duration=55321.613299, train/ctc_loss=0.13584336638450623, train/wer=0.052217, validation/ctc_loss=0.33993270993232727, validation/num_examples=5348, validation/wer=0.099790
I0214 16:46:11.816030 139516898621184 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.9205809235572815, loss=0.9933953881263733
I0214 16:47:27.275048 139516890228480 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.9670262336730957, loss=0.9832435846328735
I0214 16:48:45.752636 139516898621184 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.9334757924079895, loss=1.0168709754943848
I0214 16:50:09.418421 139516890228480 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.2135285139083862, loss=1.0457457304000854
I0214 16:51:36.758236 139516898621184 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.9470085501670837, loss=1.0090807676315308
I0214 16:53:04.467247 139516890228480 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.8805839419364929, loss=1.0068912506103516
I0214 16:54:34.928630 139516898621184 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.1726155281066895, loss=1.0403685569763184
I0214 16:56:06.438138 139516890228480 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.9925085306167603, loss=0.9881513118743896
I0214 16:57:38.983531 139516898621184 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.9084084630012512, loss=1.023178219795227
I0214 16:59:10.512963 139516890228480 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.9902138710021973, loss=1.0200459957122803
I0214 17:00:41.296549 139516898621184 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.9011921882629395, loss=0.9640336632728577
I0214 17:01:59.720189 139516890228480 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0810778141021729, loss=0.9513452649116516
I0214 17:03:15.522419 139516898621184 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.9163168668746948, loss=0.9840328693389893
I0214 17:04:34.270463 139516890228480 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.9235507249832153, loss=1.0135300159454346
I0214 17:05:58.956932 139516898621184 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.9218685626983643, loss=0.997748851776123
I0214 17:07:29.373654 139516890228480 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.019454836845398, loss=0.9797342419624329
I0214 17:08:55.261576 139688679413568 spec.py:321] Evaluating on the training split.
I0214 17:09:48.518408 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 17:10:42.818652 139688679413568 spec.py:349] Evaluating on the test split.
I0214 17:11:10.410666 139688679413568 submission_runner.py:408] Time since start: 56897.38s, 	Step: 61398, 	{'train/ctc_loss': Array(0.13234085, dtype=float32), 'train/wer': 0.05043129021952444, 'validation/ctc_loss': Array(0.33066174, dtype=float32), 'validation/wer': 0.09668169574326346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18194032, dtype=float32), 'test/wer': 0.05998009465196108, 'test/num_examples': 2472, 'score': 51910.06654167175, 'total_duration': 56897.38406395912, 'accumulated_submission_time': 51910.06654167175, 'accumulated_eval_time': 4982.617788314819, 'accumulated_logging_time': 2.086501359939575}
I0214 17:11:10.455657 139517113661184 logging_writer.py:48] [61398] accumulated_eval_time=4982.617788, accumulated_logging_time=2.086501, accumulated_submission_time=51910.066542, global_step=61398, preemption_count=0, score=51910.066542, test/ctc_loss=0.18194031715393066, test/num_examples=2472, test/wer=0.059980, total_duration=56897.384064, train/ctc_loss=0.13234084844589233, train/wer=0.050431, validation/ctc_loss=0.33066174387931824, validation/num_examples=5348, validation/wer=0.096682
I0214 17:11:12.825801 139517105268480 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.9876744747161865, loss=0.9441143870353699
I0214 17:12:28.335105 139517113661184 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.879791259765625, loss=1.0091807842254639
I0214 17:13:44.030422 139517105268480 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.1191707849502563, loss=0.9481760859489441
I0214 17:15:12.496608 139517113661184 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.9210057258605957, loss=0.9848827123641968
I0214 17:16:45.751139 139516785981184 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.0266135931015015, loss=0.9567858576774597
I0214 17:18:02.657250 139516777588480 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.8762881755828857, loss=0.9453976154327393
I0214 17:19:20.414097 139516785981184 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.939455509185791, loss=0.9532498121261597
I0214 17:20:41.102439 139516777588480 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.0280272960662842, loss=1.0180652141571045
I0214 17:22:03.872810 139516785981184 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.8556575775146484, loss=0.9384679794311523
I0214 17:23:31.094470 139516777588480 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0219813585281372, loss=0.9565693736076355
I0214 17:25:01.454855 139516785981184 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.0389256477355957, loss=0.9999362826347351
I0214 17:26:30.618303 139516777588480 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.9865405559539795, loss=0.9575120210647583
I0214 17:28:00.181218 139516785981184 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.9420070648193359, loss=0.9754310846328735
I0214 17:29:32.399215 139516777588480 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.9554138779640198, loss=0.9427895545959473
I0214 17:31:03.372992 139516785981184 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.3400074243545532, loss=0.9891981482505798
I0214 17:32:26.951831 139517113661184 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.3567216396331787, loss=0.9836983680725098
I0214 17:33:45.908428 139517105268480 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.029607892036438, loss=0.9442222118377686
I0214 17:35:06.188585 139517113661184 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.1623846292495728, loss=0.9585860371589661
I0214 17:35:10.626642 139688679413568 spec.py:321] Evaluating on the training split.
I0214 17:36:05.449399 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 17:37:00.028151 139688679413568 spec.py:349] Evaluating on the test split.
I0214 17:37:27.139719 139688679413568 submission_runner.py:408] Time since start: 58474.11s, 	Step: 63107, 	{'train/ctc_loss': Array(0.11835707, dtype=float32), 'train/wer': 0.04549613890996741, 'validation/ctc_loss': Array(0.3270045, dtype=float32), 'validation/wer': 0.09408459407011209, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18186228, dtype=float32), 'test/wer': 0.0592691893648569, 'test/num_examples': 2472, 'score': 53350.15077781677, 'total_duration': 58474.11357855797, 'accumulated_submission_time': 53350.15077781677, 'accumulated_eval_time': 5119.124755144119, 'accumulated_logging_time': 2.1506528854370117}
I0214 17:37:27.182378 139517113661184 logging_writer.py:48] [63107] accumulated_eval_time=5119.124755, accumulated_logging_time=2.150653, accumulated_submission_time=53350.150778, global_step=63107, preemption_count=0, score=53350.150778, test/ctc_loss=0.18186227977275848, test/num_examples=2472, test/wer=0.059269, total_duration=58474.113579, train/ctc_loss=0.11835706979036331, train/wer=0.045496, validation/ctc_loss=0.32700449228286743, validation/num_examples=5348, validation/wer=0.094085
I0214 17:38:38.198351 139517105268480 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.0519112348556519, loss=0.9728084802627563
I0214 17:39:54.035236 139517113661184 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0447224378585815, loss=0.9512702226638794
I0214 17:41:20.361051 139517105268480 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.1640955209732056, loss=0.9713874459266663
I0214 17:42:49.988620 139517113661184 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.1004024744033813, loss=0.9980608820915222
I0214 17:44:15.993999 139517105268480 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.2093479633331299, loss=0.9710100889205933
I0214 17:45:46.937042 139517113661184 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.0153425931930542, loss=0.9585148096084595
I0214 17:47:18.044297 139517105268480 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.1797775030136108, loss=0.9234172701835632
I0214 17:48:46.534876 139517113661184 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.9660805463790894, loss=0.9677050113677979
I0214 17:50:02.416378 139517105268480 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1030752658843994, loss=0.9484977126121521
I0214 17:51:19.943977 139517113661184 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.653448462486267, loss=0.9368455410003662
I0214 17:52:42.418861 139517105268480 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.0232398509979248, loss=0.9517621994018555
I0214 17:54:08.803983 139517113661184 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1322695016860962, loss=0.9529215097427368
I0214 17:55:36.970429 139517105268480 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.1374695301055908, loss=0.9118763208389282
I0214 17:57:07.562461 139517113661184 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.062127947807312, loss=0.9640387296676636
I0214 17:58:40.323593 139517105268480 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.0112820863723755, loss=1.0014737844467163
I0214 18:00:10.443164 139517113661184 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.3663876056671143, loss=0.9393967986106873
I0214 18:01:27.517752 139688679413568 spec.py:321] Evaluating on the training split.
I0214 18:02:21.185600 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 18:03:15.410078 139688679413568 spec.py:349] Evaluating on the test split.
I0214 18:03:42.469048 139688679413568 submission_runner.py:408] Time since start: 60049.44s, 	Step: 64784, 	{'train/ctc_loss': Array(0.11423811, dtype=float32), 'train/wer': 0.04373224320615816, 'validation/ctc_loss': Array(0.31845886, dtype=float32), 'validation/wer': 0.09148749239696072, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17583282, dtype=float32), 'test/wer': 0.05711616192391282, 'test/num_examples': 2472, 'score': 54790.40292072296, 'total_duration': 60049.44139790535, 'accumulated_submission_time': 54790.40292072296, 'accumulated_eval_time': 5254.06840133667, 'accumulated_logging_time': 2.2103593349456787}
I0214 18:03:42.516443 139517113661184 logging_writer.py:48] [64784] accumulated_eval_time=5254.068401, accumulated_logging_time=2.210359, accumulated_submission_time=54790.402921, global_step=64784, preemption_count=0, score=54790.402921, test/ctc_loss=0.1758328229188919, test/num_examples=2472, test/wer=0.057116, total_duration=60049.441398, train/ctc_loss=0.11423810571432114, train/wer=0.043732, validation/ctc_loss=0.3184588551521301, validation/num_examples=5348, validation/wer=0.091487
I0214 18:03:55.386627 139517105268480 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.0245441198349, loss=0.9235665202140808
I0214 18:05:14.637363 139516785981184 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.1118111610412598, loss=0.923765242099762
I0214 18:06:31.144459 139516777588480 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.136851191520691, loss=0.9544832110404968
I0214 18:07:51.278212 139516785981184 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.0779443979263306, loss=0.976008415222168
I0214 18:09:14.880483 139516777588480 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.0539286136627197, loss=0.9374783635139465
I0214 18:10:40.072487 139516785981184 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.0909477472305298, loss=0.9306167960166931
I0214 18:12:09.716912 139516777588480 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.1889269351959229, loss=0.9308848977088928
I0214 18:13:39.263120 139516785981184 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0177241563796997, loss=0.9165942072868347
I0214 18:15:06.195823 139516777588480 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.0500051975250244, loss=0.9295441508293152
I0214 18:16:34.262784 139516785981184 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.1768670082092285, loss=0.964969277381897
I0214 18:18:03.948938 139516777588480 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.2129157781600952, loss=0.919137716293335
I0214 18:19:34.709666 139516785981184 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.990530252456665, loss=0.9039886593818665
I0214 18:20:58.041559 139516785981184 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.2005105018615723, loss=0.9449279308319092
I0214 18:22:17.424663 139516777588480 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.098403811454773, loss=0.9230494499206543
I0214 18:23:36.030856 139516785981184 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0415005683898926, loss=0.9405609965324402
I0214 18:24:56.767706 139516777588480 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.978938102722168, loss=0.9061920642852783
I0214 18:26:23.679866 139516785981184 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.1280927658081055, loss=0.9451119303703308
I0214 18:27:42.488360 139688679413568 spec.py:321] Evaluating on the training split.
I0214 18:28:36.866999 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 18:29:29.929222 139688679413568 spec.py:349] Evaluating on the test split.
I0214 18:29:57.075417 139688679413568 submission_runner.py:408] Time since start: 61624.05s, 	Step: 66491, 	{'train/ctc_loss': Array(0.10548726, dtype=float32), 'train/wer': 0.040025938597079674, 'validation/ctc_loss': Array(0.31323752, dtype=float32), 'validation/wer': 0.08986551068287361, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17403618, dtype=float32), 'test/wer': 0.05516625027928422, 'test/num_examples': 2472, 'score': 56230.28919124603, 'total_duration': 61624.050307273865, 'accumulated_submission_time': 56230.28919124603, 'accumulated_eval_time': 5388.6503438949585, 'accumulated_logging_time': 2.275513172149658}
I0214 18:29:57.118357 139517529093888 logging_writer.py:48] [66491] accumulated_eval_time=5388.650344, accumulated_logging_time=2.275513, accumulated_submission_time=56230.289191, global_step=66491, preemption_count=0, score=56230.289191, test/ctc_loss=0.1740361750125885, test/num_examples=2472, test/wer=0.055166, total_duration=61624.050307, train/ctc_loss=0.10548726469278336, train/wer=0.040026, validation/ctc_loss=0.3132375180721283, validation/num_examples=5348, validation/wer=0.089866
I0214 18:30:04.749949 139517520701184 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.3149676322937012, loss=0.9012369513511658
I0214 18:31:20.420539 139517529093888 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.9834498167037964, loss=0.8966993093490601
I0214 18:32:36.130047 139517520701184 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.3388750553131104, loss=0.9563767910003662
I0214 18:34:03.534941 139517529093888 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.0740383863449097, loss=0.9257166385650635
I0214 18:35:32.626260 139517520701184 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.2324227094650269, loss=0.8992042541503906
I0214 18:36:58.467381 139517201413888 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.0464181900024414, loss=0.9106706380844116
I0214 18:38:16.293313 139517193021184 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.0153554677963257, loss=0.918695330619812
I0214 18:39:37.993371 139517201413888 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.584387183189392, loss=0.9049923419952393
I0214 18:40:55.954566 139517193021184 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.5207291841506958, loss=0.8798313140869141
I0214 18:42:23.362076 139517201413888 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.013488531112671, loss=0.9180084466934204
I0214 18:43:52.077300 139517193021184 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.0154683589935303, loss=0.893688440322876
I0214 18:45:22.658655 139517201413888 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.002494215965271, loss=0.9029597043991089
I0214 18:46:53.175137 139517193021184 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1415307521820068, loss=0.9186275005340576
I0214 18:48:23.679901 139517201413888 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.2543343305587769, loss=0.9495928287506104
I0214 18:49:56.164223 139517193021184 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.4294490814208984, loss=0.9193592667579651
I0214 18:51:24.772871 139517201413888 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.282571792602539, loss=0.8990597128868103
I0214 18:52:42.587930 139517193021184 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.1589744091033936, loss=0.9163807034492493
I0214 18:53:57.083153 139688679413568 spec.py:321] Evaluating on the training split.
I0214 18:54:52.141479 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 18:55:46.464046 139688679413568 spec.py:349] Evaluating on the test split.
I0214 18:56:13.716323 139688679413568 submission_runner.py:408] Time since start: 63200.69s, 	Step: 68195, 	{'train/ctc_loss': Array(0.0994551, dtype=float32), 'train/wer': 0.03796641687789867, 'validation/ctc_loss': Array(0.30718902, dtype=float32), 'validation/wer': 0.08801181729534549, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16796601, dtype=float32), 'test/wer': 0.05463814920886397, 'test/num_examples': 2472, 'score': 57670.17176914215, 'total_duration': 63200.69050574303, 'accumulated_submission_time': 57670.17176914215, 'accumulated_eval_time': 5525.277727842331, 'accumulated_logging_time': 2.334338903427124}
I0214 18:56:13.761826 139517201413888 logging_writer.py:48] [68195] accumulated_eval_time=5525.277728, accumulated_logging_time=2.334339, accumulated_submission_time=57670.171769, global_step=68195, preemption_count=0, score=57670.171769, test/ctc_loss=0.16796600818634033, test/num_examples=2472, test/wer=0.054638, total_duration=63200.690506, train/ctc_loss=0.0994550958275795, train/wer=0.037966, validation/ctc_loss=0.307189017534256, validation/num_examples=5348, validation/wer=0.088012
I0214 18:56:18.389422 139517193021184 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.0492100715637207, loss=0.901585578918457
I0214 18:57:33.818244 139517201413888 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.1756614446640015, loss=0.8894091844558716
I0214 18:58:49.400571 139517193021184 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.2473031282424927, loss=0.8801528811454773
I0214 19:00:12.598667 139517201413888 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.2229769229888916, loss=0.8947251439094543
I0214 19:01:41.665897 139517193021184 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.265915870666504, loss=0.917250394821167
I0214 19:03:11.759757 139517201413888 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.364339828491211, loss=0.863554060459137
I0214 19:04:43.522462 139517193021184 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2711646556854248, loss=0.8674519062042236
I0214 19:06:11.427099 139517201413888 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.0698347091674805, loss=0.8649965524673462
I0214 19:07:43.061802 139517193021184 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.032949447631836, loss=0.8725855350494385
I0214 19:09:03.763369 139517201413888 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.2048652172088623, loss=0.9462891221046448
I0214 19:10:22.126451 139517193021184 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.4137146472930908, loss=0.9068899154663086
I0214 19:11:42.822138 139517201413888 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.0803730487823486, loss=0.9224216938018799
I0214 19:13:08.580289 139517193021184 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.1084450483322144, loss=0.8568282723426819
I0214 19:14:33.821712 139517201413888 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.107290267944336, loss=0.9333639740943909
I0214 19:16:03.918817 139517193021184 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.0518380403518677, loss=0.8658437728881836
I0214 19:17:33.507999 139517201413888 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.190471887588501, loss=0.8878970146179199
I0214 19:19:02.143970 139517193021184 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.0855437517166138, loss=0.8744564652442932
I0214 19:20:14.262064 139688679413568 spec.py:321] Evaluating on the training split.
I0214 19:21:08.319593 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 19:22:02.777912 139688679413568 spec.py:349] Evaluating on the test split.
I0214 19:22:30.631099 139688679413568 submission_runner.py:408] Time since start: 64777.60s, 	Step: 69881, 	{'train/ctc_loss': Array(0.08041966, dtype=float32), 'train/wer': 0.030858299457803058, 'validation/ctc_loss': Array(0.30663946, dtype=float32), 'validation/wer': 0.08763528582600384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16488022, dtype=float32), 'test/wer': 0.0529116649401824, 'test/num_examples': 2472, 'score': 59110.58864855766, 'total_duration': 64777.60359311104, 'accumulated_submission_time': 59110.58864855766, 'accumulated_eval_time': 5661.63925409317, 'accumulated_logging_time': 2.3968987464904785}
I0214 19:22:30.678253 139517201413888 logging_writer.py:48] [69881] accumulated_eval_time=5661.639254, accumulated_logging_time=2.396899, accumulated_submission_time=59110.588649, global_step=69881, preemption_count=0, score=59110.588649, test/ctc_loss=0.16488021612167358, test/num_examples=2472, test/wer=0.052912, total_duration=64777.603593, train/ctc_loss=0.08041965961456299, train/wer=0.030858, validation/ctc_loss=0.3066394627094269, validation/num_examples=5348, validation/wer=0.087635
I0214 19:22:45.843623 139517193021184 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.10530424118042, loss=0.8588813543319702
I0214 19:24:01.585436 139517201413888 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.0591164827346802, loss=0.8951338529586792
I0214 19:25:21.824365 139517529093888 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.1243540048599243, loss=0.8944443464279175
I0214 19:26:42.490022 139517520701184 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.0789748430252075, loss=0.8859715461730957
I0214 19:28:05.647493 139517529093888 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.290523648262024, loss=0.8982590436935425
I0214 19:29:26.391848 139517520701184 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.2438085079193115, loss=0.855832040309906
I0214 19:30:51.181667 139517529093888 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.085541009902954, loss=0.8491241931915283
I0214 19:32:20.531670 139517520701184 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.2141032218933105, loss=0.897315502166748
I0214 19:33:50.430028 139517529093888 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.0486642122268677, loss=0.9128725528717041
I0214 19:35:21.190495 139517520701184 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.2321107387542725, loss=0.9213829636573792
I0214 19:36:52.165814 139517529093888 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.1550182104110718, loss=0.8829967379570007
I0214 19:38:21.413956 139517520701184 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.1921746730804443, loss=0.8750807642936707
I0214 19:39:51.785411 139517529093888 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.2584608793258667, loss=0.8710225820541382
I0214 19:41:07.674643 139517520701184 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.3118890523910522, loss=0.8580135703086853
I0214 19:42:28.824474 139517529093888 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.3109095096588135, loss=0.9061310887336731
I0214 19:43:51.566783 139517520701184 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.1748669147491455, loss=0.8554122447967529
I0214 19:45:17.654111 139517529093888 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2039217948913574, loss=0.892493724822998
I0214 19:46:30.928566 139688679413568 spec.py:321] Evaluating on the training split.
I0214 19:47:25.163617 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 19:48:17.888240 139688679413568 spec.py:349] Evaluating on the test split.
I0214 19:48:44.597437 139688679413568 submission_runner.py:408] Time since start: 66351.57s, 	Step: 71588, 	{'train/ctc_loss': Array(0.07414443, dtype=float32), 'train/wer': 0.028042570786109555, 'validation/ctc_loss': Array(0.2998368, dtype=float32), 'validation/wer': 0.08489336435695183, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16116796, dtype=float32), 'test/wer': 0.051896085958605, 'test/num_examples': 2472, 'score': 60550.75520849228, 'total_duration': 66351.57162237167, 'accumulated_submission_time': 60550.75520849228, 'accumulated_eval_time': 5795.302344799042, 'accumulated_logging_time': 2.4600815773010254}
I0214 19:48:44.643462 139518327813888 logging_writer.py:48] [71588] accumulated_eval_time=5795.302345, accumulated_logging_time=2.460082, accumulated_submission_time=60550.755208, global_step=71588, preemption_count=0, score=60550.755208, test/ctc_loss=0.1611679643392563, test/num_examples=2472, test/wer=0.051896, total_duration=66351.571622, train/ctc_loss=0.07414443045854568, train/wer=0.028043, validation/ctc_loss=0.29983681440353394, validation/num_examples=5348, validation/wer=0.084893
I0214 19:48:54.524420 139518319421184 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.0715925693511963, loss=0.8224307298660278
I0214 19:50:10.469253 139518327813888 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.3208086490631104, loss=0.8949372172355652
I0214 19:51:27.073688 139518319421184 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.1175965070724487, loss=0.8019922375679016
I0214 19:52:55.795652 139518327813888 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.0719096660614014, loss=0.881926953792572
I0214 19:54:26.589784 139518319421184 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.2724043130874634, loss=0.9083862900733948
I0214 19:55:59.902706 139517672453888 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.2252376079559326, loss=0.8636955618858337
I0214 19:57:18.860094 139517664061184 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.0747805833816528, loss=0.8595699667930603
I0214 19:57:22.375324 139517672453888 logging_writer.py:48] [72206] global_step=72206, preemption_count=0, score=61068.425246
I0214 19:57:23.271322 139688679413568 checkpoints.py:490] Saving checkpoint at step: 72206
I0214 19:57:24.879291 139688679413568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_1/checkpoint_72206
I0214 19:57:24.919682 139688679413568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_1/checkpoint_72206.
I0214 19:57:28.307709 139688679413568 submission_runner.py:583] Tuning trial 1/5
I0214 19:57:28.308012 139688679413568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0214 19:57:28.332337 139688679413568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.99089, dtype=float32), 'train/wer': 0.9441777898090499, 'validation/ctc_loss': Array(30.141817, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 61.96003007888794, 'total_duration': 220.17135214805603, 'accumulated_submission_time': 61.96003007888794, 'accumulated_eval_time': 158.21125888824463, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1729, {'train/ctc_loss': Array(6.527625, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.5964723, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.5954204, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1502.605571269989, 'total_duration': 1767.9254868030548, 'accumulated_submission_time': 1502.605571269989, 'accumulated_eval_time': 265.20961356163025, 'accumulated_logging_time': 0.041916847229003906, 'global_step': 1729, 'preemption_count': 0}), (3495, {'train/ctc_loss': Array(3.2871444, dtype=float32), 'train/wer': 0.6894920491170125, 'validation/ctc_loss': Array(3.6583788, dtype=float32), 'validation/wer': 0.7243403458296727, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.3570948, dtype=float32), 'test/wer': 0.6746694290414965, 'test/num_examples': 2472, 'score': 2942.91507768631, 'total_duration': 3330.286596775055, 'accumulated_submission_time': 2942.91507768631, 'accumulated_eval_time': 387.1395664215088, 'accumulated_logging_time': 0.09027791023254395, 'global_step': 3495, 'preemption_count': 0}), (5219, {'train/ctc_loss': Array(0.78254795, dtype=float32), 'train/wer': 0.2698947346226629, 'validation/ctc_loss': Array(1.1469847, dtype=float32), 'validation/wer': 0.3340799598366433, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8465067, dtype=float32), 'test/wer': 0.2771921272317348, 'test/num_examples': 2472, 'score': 4383.1235201358795, 'total_duration': 4906.813807249069, 'accumulated_submission_time': 4383.1235201358795, 'accumulated_eval_time': 523.3349103927612, 'accumulated_logging_time': 0.14176058769226074, 'global_step': 5219, 'preemption_count': 0}), (6943, {'train/ctc_loss': Array(0.5334083, dtype=float32), 'train/wer': 0.1861156782456618, 'validation/ctc_loss': Array(0.85424995, dtype=float32), 'validation/wer': 0.2556938316421599, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58340687, dtype=float32), 'test/wer': 0.19456462129059776, 'test/num_examples': 2472, 'score': 5823.346606016159, 'total_duration': 6483.608228683472, 'accumulated_submission_time': 5823.346606016159, 'accumulated_eval_time': 659.7774543762207, 'accumulated_logging_time': 0.1961519718170166, 'global_step': 6943, 'preemption_count': 0}), (8684, {'train/ctc_loss': Array(0.45842528, dtype=float32), 'train/wer': 0.16366957315879885, 'validation/ctc_loss': Array(0.7623629, dtype=float32), 'validation/wer': 0.2306593162574703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50006676, dtype=float32), 'test/wer': 0.17006885625495094, 'test/num_examples': 2472, 'score': 7263.875959396362, 'total_duration': 8057.054099321365, 'accumulated_submission_time': 7263.875959396362, 'accumulated_eval_time': 792.5681960582733, 'accumulated_logging_time': 0.24925851821899414, 'global_step': 8684, 'preemption_count': 0}), (10391, {'train/ctc_loss': Array(0.4250593, dtype=float32), 'train/wer': 0.15079140682098255, 'validation/ctc_loss': Array(0.6983394, dtype=float32), 'validation/wer': 0.21195825328016837, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45460317, dtype=float32), 'test/wer': 0.1544086283590275, 'test/num_examples': 2472, 'score': 8704.147989988327, 'total_duration': 9632.983120441437, 'accumulated_submission_time': 8704.147989988327, 'accumulated_eval_time': 928.0997140407562, 'accumulated_logging_time': 0.3020815849304199, 'global_step': 10391, 'preemption_count': 0}), (12089, {'train/ctc_loss': Array(0.36514083, dtype=float32), 'train/wer': 0.13228256014293918, 'validation/ctc_loss': Array(0.65220815, dtype=float32), 'validation/wer': 0.19723490736360388, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41433823, dtype=float32), 'test/wer': 0.14096236264294273, 'test/num_examples': 2472, 'score': 10144.166356801987, 'total_duration': 11210.15414071083, 'accumulated_submission_time': 10144.166356801987, 'accumulated_eval_time': 1065.1332168579102, 'accumulated_logging_time': 0.350053071975708, 'global_step': 12089, 'preemption_count': 0}), (13828, {'train/ctc_loss': Array(0.31292966, dtype=float32), 'train/wer': 0.11764216438284927, 'validation/ctc_loss': Array(0.6289795, dtype=float32), 'validation/wer': 0.19065043397665504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39117342, dtype=float32), 'test/wer': 0.1323908760384295, 'test/num_examples': 2472, 'score': 11584.574691057205, 'total_duration': 12787.471153974533, 'accumulated_submission_time': 11584.574691057205, 'accumulated_eval_time': 1201.91868019104, 'accumulated_logging_time': 0.4008769989013672, 'global_step': 13828, 'preemption_count': 0}), (15518, {'train/ctc_loss': Array(0.28706837, dtype=float32), 'train/wer': 0.10611324207715102, 'validation/ctc_loss': Array(0.59261024, dtype=float32), 'validation/wer': 0.18042615638607026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3671398, dtype=float32), 'test/wer': 0.12562712002112406, 'test/num_examples': 2472, 'score': 13024.453626871109, 'total_duration': 14362.89952802658, 'accumulated_submission_time': 13024.453626871109, 'accumulated_eval_time': 1337.3448441028595, 'accumulated_logging_time': 0.4517827033996582, 'global_step': 15518, 'preemption_count': 0}), (17235, {'train/ctc_loss': Array(0.28261438, dtype=float32), 'train/wer': 0.10724149581427277, 'validation/ctc_loss': Array(0.58001304, dtype=float32), 'validation/wer': 0.17476852969288548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3576985, dtype=float32), 'test/wer': 0.12150386935591981, 'test/num_examples': 2472, 'score': 14464.984570026398, 'total_duration': 15937.846490383148, 'accumulated_submission_time': 14464.984570026398, 'accumulated_eval_time': 1471.637094259262, 'accumulated_logging_time': 0.5028841495513916, 'global_step': 17235, 'preemption_count': 0}), (18968, {'train/ctc_loss': Array(0.27884072, dtype=float32), 'train/wer': 0.1019963510404823, 'validation/ctc_loss': Array(0.5530038, dtype=float32), 'validation/wer': 0.1681068190814563, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33700582, dtype=float32), 'test/wer': 0.11465886702008815, 'test/num_examples': 2472, 'score': 15905.405641078949, 'total_duration': 17513.532631635666, 'accumulated_submission_time': 15905.405641078949, 'accumulated_eval_time': 1606.7800455093384, 'accumulated_logging_time': 0.55181884765625, 'global_step': 18968, 'preemption_count': 0}), (20669, {'train/ctc_loss': Array(0.2795932, dtype=float32), 'train/wer': 0.10299416994222703, 'validation/ctc_loss': Array(0.5383996, dtype=float32), 'validation/wer': 0.16351120422487617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32689303, dtype=float32), 'test/wer': 0.11201836166798691, 'test/num_examples': 2472, 'score': 17345.66361284256, 'total_duration': 19088.68205499649, 'accumulated_submission_time': 17345.66361284256, 'accumulated_eval_time': 1741.5442523956299, 'accumulated_logging_time': 0.6050012111663818, 'global_step': 20669, 'preemption_count': 0}), (22379, {'train/ctc_loss': Array(0.26681316, dtype=float32), 'train/wer': 0.09884247350550468, 'validation/ctc_loss': Array(0.53043646, dtype=float32), 'validation/wer': 0.1613002886741265, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32418817, dtype=float32), 'test/wer': 0.11061686267341012, 'test/num_examples': 2472, 'score': 18786.160464525223, 'total_duration': 20665.01668071747, 'accumulated_submission_time': 18786.160464525223, 'accumulated_eval_time': 1877.2550451755524, 'accumulated_logging_time': 0.659116268157959, 'global_step': 22379, 'preemption_count': 0}), (24105, {'train/ctc_loss': Array(0.24471778, dtype=float32), 'train/wer': 0.09138655462184873, 'validation/ctc_loss': Array(0.52384573, dtype=float32), 'validation/wer': 0.1589252440213561, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31326857, dtype=float32), 'test/wer': 0.10620924989336421, 'test/num_examples': 2472, 'score': 20226.1611392498, 'total_duration': 22240.911847114563, 'accumulated_submission_time': 20226.1611392498, 'accumulated_eval_time': 2013.0226137638092, 'accumulated_logging_time': 0.7118988037109375, 'global_step': 24105, 'preemption_count': 0}), (25782, {'train/ctc_loss': Array(0.23044355, dtype=float32), 'train/wer': 0.08575601867550015, 'validation/ctc_loss': Array(0.5116114, dtype=float32), 'validation/wer': 0.15423308263417554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30197123, dtype=float32), 'test/wer': 0.10135478236142424, 'test/num_examples': 2472, 'score': 21666.239350557327, 'total_duration': 23817.70459675789, 'accumulated_submission_time': 21666.239350557327, 'accumulated_eval_time': 2149.6030519008636, 'accumulated_logging_time': 0.7754313945770264, 'global_step': 25782, 'preemption_count': 0}), (27484, {'train/ctc_loss': Array(0.22562861, dtype=float32), 'train/wer': 0.08264209223593909, 'validation/ctc_loss': Array(0.4992585, dtype=float32), 'validation/wer': 0.15019743765507787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30364478, dtype=float32), 'test/wer': 0.10111104340584567, 'test/num_examples': 2472, 'score': 23106.382368564606, 'total_duration': 25393.70451760292, 'accumulated_submission_time': 23106.382368564606, 'accumulated_eval_time': 2285.3351743221283, 'accumulated_logging_time': 0.8276486396789551, 'global_step': 27484, 'preemption_count': 0}), (29193, {'train/ctc_loss': Array(0.23682073, dtype=float32), 'train/wer': 0.08620428150459351, 'validation/ctc_loss': Array(0.48714328, dtype=float32), 'validation/wer': 0.14619075663516032, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29264167, dtype=float32), 'test/wer': 0.09885645806674385, 'test/num_examples': 2472, 'score': 24546.66773033142, 'total_duration': 26968.495937108994, 'accumulated_submission_time': 24546.66773033142, 'accumulated_eval_time': 2419.71209859848, 'accumulated_logging_time': 0.8837933540344238, 'global_step': 29193, 'preemption_count': 0}), (30862, {'train/ctc_loss': Array(0.22417934, dtype=float32), 'train/wer': 0.08077634713156338, 'validation/ctc_loss': Array(0.48772222, dtype=float32), 'validation/wer': 0.14717553124728464, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28424188, dtype=float32), 'test/wer': 0.09465196108301342, 'test/num_examples': 2472, 'score': 25986.74062180519, 'total_duration': 28542.845601081848, 'accumulated_submission_time': 25986.74062180519, 'accumulated_eval_time': 2553.862573862076, 'accumulated_logging_time': 0.9391980171203613, 'global_step': 30862, 'preemption_count': 0}), (32543, {'train/ctc_loss': Array(0.22964644, dtype=float32), 'train/wer': 0.08269935120640677, 'validation/ctc_loss': Array(0.47304323, dtype=float32), 'validation/wer': 0.14242544194174384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27928787, dtype=float32), 'test/wer': 0.09404261369406698, 'test/num_examples': 2472, 'score': 27426.87792801857, 'total_duration': 30118.500452518463, 'accumulated_submission_time': 27426.87792801857, 'accumulated_eval_time': 2689.2480068206787, 'accumulated_logging_time': 0.999396562576294, 'global_step': 32543, 'preemption_count': 0}), (34241, {'train/ctc_loss': Array(0.22303934, dtype=float32), 'train/wer': 0.07963276745846522, 'validation/ctc_loss': Array(0.4615741, dtype=float32), 'validation/wer': 0.13925871573804996, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27463698, dtype=float32), 'test/wer': 0.09184896309385981, 'test/num_examples': 2472, 'score': 28866.8210606575, 'total_duration': 31696.89235520363, 'accumulated_submission_time': 28866.8210606575, 'accumulated_eval_time': 2827.5657880306244, 'accumulated_logging_time': 1.057802677154541, 'global_step': 34241, 'preemption_count': 0}), (35914, {'train/ctc_loss': Array(0.17427287, dtype=float32), 'train/wer': 0.06601888010823775, 'validation/ctc_loss': Array(0.45529997, dtype=float32), 'validation/wer': 0.13775258986068337, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26810637, dtype=float32), 'test/wer': 0.09042715251965146, 'test/num_examples': 2472, 'score': 30306.810252904892, 'total_duration': 33272.61613845825, 'accumulated_submission_time': 30306.810252904892, 'accumulated_eval_time': 2963.1705458164215, 'accumulated_logging_time': 1.1170177459716797, 'global_step': 35914, 'preemption_count': 0}), (37620, {'train/ctc_loss': Array(0.19530179, dtype=float32), 'train/wer': 0.0711178222212836, 'validation/ctc_loss': Array(0.45110688, dtype=float32), 'validation/wer': 0.13534858124873284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2616381, dtype=float32), 'test/wer': 0.08977718197144192, 'test/num_examples': 2472, 'score': 31746.826417922974, 'total_duration': 34848.603251457214, 'accumulated_submission_time': 31746.826417922974, 'accumulated_eval_time': 3099.013829946518, 'accumulated_logging_time': 1.1702604293823242, 'global_step': 37620, 'preemption_count': 0}), (39313, {'train/ctc_loss': Array(0.2397092, dtype=float32), 'train/wer': 0.0867527649257172, 'validation/ctc_loss': Array(0.4422507, dtype=float32), 'validation/wer': 0.13199841663689815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25463516, dtype=float32), 'test/wer': 0.08488209128023887, 'test/num_examples': 2472, 'score': 33187.17974424362, 'total_duration': 36424.66038656235, 'accumulated_submission_time': 33187.17974424362, 'accumulated_eval_time': 3234.586481332779, 'accumulated_logging_time': 1.2274115085601807, 'global_step': 39313, 'preemption_count': 0}), (41000, {'train/ctc_loss': Array(0.24868596, dtype=float32), 'train/wer': 0.08968174222952531, 'validation/ctc_loss': Array(0.43414676, dtype=float32), 'validation/wer': 0.12913098467806558, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25042498, dtype=float32), 'test/wer': 0.08313529543192574, 'test/num_examples': 2472, 'score': 34627.428320646286, 'total_duration': 37997.814823150635, 'accumulated_submission_time': 34627.428320646286, 'accumulated_eval_time': 3367.3648619651794, 'accumulated_logging_time': 1.2825305461883545, 'global_step': 41000, 'preemption_count': 0}), (42715, {'train/ctc_loss': Array(0.28319895, dtype=float32), 'train/wer': 0.10321086159191815, 'validation/ctc_loss': Array(0.42378053, dtype=float32), 'validation/wer': 0.12779864255577975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23770487, dtype=float32), 'test/wer': 0.07976357321308879, 'test/num_examples': 2472, 'score': 36067.43534851074, 'total_duration': 39570.2938041687, 'accumulated_submission_time': 36067.43534851074, 'accumulated_eval_time': 3499.707387447357, 'accumulated_logging_time': 1.3385326862335205, 'global_step': 42715, 'preemption_count': 0}), (44418, {'train/ctc_loss': Array(0.2481551, dtype=float32), 'train/wer': 0.08901435981371689, 'validation/ctc_loss': Array(0.41698194, dtype=float32), 'validation/wer': 0.12533670602546898, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24019764, dtype=float32), 'test/wer': 0.08090102167245547, 'test/num_examples': 2472, 'score': 37507.68049240112, 'total_duration': 41143.12516140938, 'accumulated_submission_time': 37507.68049240112, 'accumulated_eval_time': 3632.162237882614, 'accumulated_logging_time': 1.3978519439697266, 'global_step': 44418, 'preemption_count': 0}), (46117, {'train/ctc_loss': Array(0.22114661, dtype=float32), 'train/wer': 0.08169752708405242, 'validation/ctc_loss': Array(0.40540525, dtype=float32), 'validation/wer': 0.11971769794452436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23128738, dtype=float32), 'test/wer': 0.07724493733877684, 'test/num_examples': 2472, 'score': 38948.10423493385, 'total_duration': 42716.81401634216, 'accumulated_submission_time': 38948.10423493385, 'accumulated_eval_time': 3765.296792268753, 'accumulated_logging_time': 1.455432415008545, 'global_step': 46117, 'preemption_count': 0}), (47828, {'train/ctc_loss': Array(0.18673953, dtype=float32), 'train/wer': 0.07121339003143047, 'validation/ctc_loss': Array(0.3940457, dtype=float32), 'validation/wer': 0.11766125684273536, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22929518, dtype=float32), 'test/wer': 0.07578250360530539, 'test/num_examples': 2472, 'score': 40389.05780625343, 'total_duration': 44294.217025756836, 'accumulated_submission_time': 40389.05780625343, 'accumulated_eval_time': 3901.612589597702, 'accumulated_logging_time': 1.514012098312378, 'global_step': 47828, 'preemption_count': 0}), (49511, {'train/ctc_loss': Array(0.20579855, dtype=float32), 'train/wer': 0.0762191313604197, 'validation/ctc_loss': Array(0.39688087, dtype=float32), 'validation/wer': 0.1164158065979899, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2227393, dtype=float32), 'test/wer': 0.07385290354030832, 'test/num_examples': 2472, 'score': 41829.449006557465, 'total_duration': 45870.42122077942, 'accumulated_submission_time': 41829.449006557465, 'accumulated_eval_time': 4037.291281223297, 'accumulated_logging_time': 1.5761985778808594, 'global_step': 49511, 'preemption_count': 0}), (51202, {'train/ctc_loss': Array(0.18011282, dtype=float32), 'train/wer': 0.06902271946400962, 'validation/ctc_loss': Array(0.38041675, dtype=float32), 'validation/wer': 0.11233188835359201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21779981, dtype=float32), 'test/wer': 0.07271545508094164, 'test/num_examples': 2472, 'score': 43269.72302722931, 'total_duration': 47447.20176792145, 'accumulated_submission_time': 43269.72302722931, 'accumulated_eval_time': 4173.671866893768, 'accumulated_logging_time': 1.6306617259979248, 'global_step': 51202, 'preemption_count': 0}), (52918, {'train/ctc_loss': Array(0.17563458, dtype=float32), 'train/wer': 0.06664750105772464, 'validation/ctc_loss': Array(0.37358853, dtype=float32), 'validation/wer': 0.11024648329262288, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21114749, dtype=float32), 'test/wer': 0.06944529076026243, 'test/num_examples': 2472, 'score': 44709.526916742325, 'total_duration': 49023.30984520912, 'accumulated_submission_time': 44709.526916742325, 'accumulated_eval_time': 4309.752601146698, 'accumulated_logging_time': 1.781308889389038, 'global_step': 52918, 'preemption_count': 0}), (54594, {'train/ctc_loss': Array(0.16827415, dtype=float32), 'train/wer': 0.06451345755693581, 'validation/ctc_loss': Array(0.36485976, dtype=float32), 'validation/wer': 0.10857622831323556, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20493606, dtype=float32), 'test/wer': 0.07033900026405053, 'test/num_examples': 2472, 'score': 46149.565438985825, 'total_duration': 50598.354439258575, 'accumulated_submission_time': 46149.565438985825, 'accumulated_eval_time': 4444.613988637924, 'accumulated_logging_time': 1.8514833450317383, 'global_step': 54594, 'preemption_count': 0}), (56314, {'train/ctc_loss': Array(0.16584644, dtype=float32), 'train/wer': 0.06282980209012969, 'validation/ctc_loss': Array(0.35868764, dtype=float32), 'validation/wer': 0.10561224982380259, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19563222, dtype=float32), 'test/wer': 0.06605325696179391, 'test/num_examples': 2472, 'score': 47589.52333641052, 'total_duration': 52171.8772919178, 'accumulated_submission_time': 47589.52333641052, 'accumulated_eval_time': 4578.047208547592, 'accumulated_logging_time': 1.9099171161651611, 'global_step': 56314, 'preemption_count': 0}), (58024, {'train/ctc_loss': Array(0.16346063, dtype=float32), 'train/wer': 0.06120737537982335, 'validation/ctc_loss': Array(0.35139942, dtype=float32), 'validation/wer': 0.10307307606901146, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19396327, dtype=float32), 'test/wer': 0.06371742530416591, 'test/num_examples': 2472, 'score': 49029.40422821045, 'total_duration': 53745.149267435074, 'accumulated_submission_time': 49029.40422821045, 'accumulated_eval_time': 4711.303059339523, 'accumulated_logging_time': 1.9719979763031006, 'global_step': 58024, 'preemption_count': 0}), (59706, {'train/ctc_loss': Array(0.13584337, dtype=float32), 'train/wer': 0.05221714080749658, 'validation/ctc_loss': Array(0.3399327, dtype=float32), 'validation/wer': 0.09979049402859708, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18521993, dtype=float32), 'test/wer': 0.06188938313732659, 'test/num_examples': 2472, 'score': 50469.57779741287, 'total_duration': 55321.613298892975, 'accumulated_submission_time': 50469.57779741287, 'accumulated_eval_time': 4847.475305557251, 'accumulated_logging_time': 2.020612955093384, 'global_step': 59706, 'preemption_count': 0}), (61398, {'train/ctc_loss': Array(0.13234085, dtype=float32), 'train/wer': 0.05043129021952444, 'validation/ctc_loss': Array(0.33066174, dtype=float32), 'validation/wer': 0.09668169574326346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18194032, dtype=float32), 'test/wer': 0.05998009465196108, 'test/num_examples': 2472, 'score': 51910.06654167175, 'total_duration': 56897.38406395912, 'accumulated_submission_time': 51910.06654167175, 'accumulated_eval_time': 4982.617788314819, 'accumulated_logging_time': 2.086501359939575, 'global_step': 61398, 'preemption_count': 0}), (63107, {'train/ctc_loss': Array(0.11835707, dtype=float32), 'train/wer': 0.04549613890996741, 'validation/ctc_loss': Array(0.3270045, dtype=float32), 'validation/wer': 0.09408459407011209, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18186228, dtype=float32), 'test/wer': 0.0592691893648569, 'test/num_examples': 2472, 'score': 53350.15077781677, 'total_duration': 58474.11357855797, 'accumulated_submission_time': 53350.15077781677, 'accumulated_eval_time': 5119.124755144119, 'accumulated_logging_time': 2.1506528854370117, 'global_step': 63107, 'preemption_count': 0}), (64784, {'train/ctc_loss': Array(0.11423811, dtype=float32), 'train/wer': 0.04373224320615816, 'validation/ctc_loss': Array(0.31845886, dtype=float32), 'validation/wer': 0.09148749239696072, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17583282, dtype=float32), 'test/wer': 0.05711616192391282, 'test/num_examples': 2472, 'score': 54790.40292072296, 'total_duration': 60049.44139790535, 'accumulated_submission_time': 54790.40292072296, 'accumulated_eval_time': 5254.06840133667, 'accumulated_logging_time': 2.2103593349456787, 'global_step': 64784, 'preemption_count': 0}), (66491, {'train/ctc_loss': Array(0.10548726, dtype=float32), 'train/wer': 0.040025938597079674, 'validation/ctc_loss': Array(0.31323752, dtype=float32), 'validation/wer': 0.08986551068287361, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17403618, dtype=float32), 'test/wer': 0.05516625027928422, 'test/num_examples': 2472, 'score': 56230.28919124603, 'total_duration': 61624.050307273865, 'accumulated_submission_time': 56230.28919124603, 'accumulated_eval_time': 5388.6503438949585, 'accumulated_logging_time': 2.275513172149658, 'global_step': 66491, 'preemption_count': 0}), (68195, {'train/ctc_loss': Array(0.0994551, dtype=float32), 'train/wer': 0.03796641687789867, 'validation/ctc_loss': Array(0.30718902, dtype=float32), 'validation/wer': 0.08801181729534549, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16796601, dtype=float32), 'test/wer': 0.05463814920886397, 'test/num_examples': 2472, 'score': 57670.17176914215, 'total_duration': 63200.69050574303, 'accumulated_submission_time': 57670.17176914215, 'accumulated_eval_time': 5525.277727842331, 'accumulated_logging_time': 2.334338903427124, 'global_step': 68195, 'preemption_count': 0}), (69881, {'train/ctc_loss': Array(0.08041966, dtype=float32), 'train/wer': 0.030858299457803058, 'validation/ctc_loss': Array(0.30663946, dtype=float32), 'validation/wer': 0.08763528582600384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16488022, dtype=float32), 'test/wer': 0.0529116649401824, 'test/num_examples': 2472, 'score': 59110.58864855766, 'total_duration': 64777.60359311104, 'accumulated_submission_time': 59110.58864855766, 'accumulated_eval_time': 5661.63925409317, 'accumulated_logging_time': 2.3968987464904785, 'global_step': 69881, 'preemption_count': 0}), (71588, {'train/ctc_loss': Array(0.07414443, dtype=float32), 'train/wer': 0.028042570786109555, 'validation/ctc_loss': Array(0.2998368, dtype=float32), 'validation/wer': 0.08489336435695183, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16116796, dtype=float32), 'test/wer': 0.051896085958605, 'test/num_examples': 2472, 'score': 60550.75520849228, 'total_duration': 66351.57162237167, 'accumulated_submission_time': 60550.75520849228, 'accumulated_eval_time': 5795.302344799042, 'accumulated_logging_time': 2.4600815773010254, 'global_step': 71588, 'preemption_count': 0})], 'global_step': 72206}
I0214 19:57:28.332611 139688679413568 submission_runner.py:586] Timing: 61068.42524552345
I0214 19:57:28.332726 139688679413568 submission_runner.py:588] Total number of evals: 43
I0214 19:57:28.332783 139688679413568 submission_runner.py:589] ====================
I0214 19:57:28.332836 139688679413568 submission_runner.py:542] Using RNG seed 1618809895
I0214 19:57:28.335395 139688679413568 submission_runner.py:551] --- Tuning run 2/5 ---
I0214 19:57:28.335569 139688679413568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_2.
I0214 19:57:28.337203 139688679413568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_2/hparams.json.
I0214 19:57:28.338584 139688679413568 submission_runner.py:206] Initializing dataset.
I0214 19:57:28.338739 139688679413568 submission_runner.py:213] Initializing model.
I0214 19:57:31.951711 139688679413568 submission_runner.py:255] Initializing optimizer.
I0214 19:57:32.375672 139688679413568 submission_runner.py:262] Initializing metrics bundle.
I0214 19:57:32.375896 139688679413568 submission_runner.py:280] Initializing checkpoint and logger.
I0214 19:57:32.479938 139688679413568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_2 with prefix checkpoint_
I0214 19:57:32.480094 139688679413568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_2/meta_data_0.json.
I0214 19:57:32.480419 139688679413568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0214 19:57:32.480524 139688679413568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0214 19:57:33.039285 139688679413568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0214 19:57:33.501274 139688679413568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_2/flags_0.json.
I0214 19:57:33.635838 139688679413568 submission_runner.py:314] Starting training loop.
I0214 19:57:33.639046 139688679413568 input_pipeline.py:20] Loading split = train-clean-100
I0214 19:57:34.012197 139688679413568 input_pipeline.py:20] Loading split = train-clean-360
I0214 19:57:34.152814 139688679413568 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0214 19:58:08.771025 139518260672256 logging_writer.py:48] [0] global_step=0, grad_norm=55.655338287353516, loss=31.496856689453125
I0214 19:58:08.786525 139688679413568 spec.py:321] Evaluating on the training split.
I0214 19:58:51.648009 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 19:59:41.141837 139688679413568 spec.py:349] Evaluating on the test split.
I0214 20:00:06.529449 139688679413568 submission_runner.py:408] Time since start: 152.89s, 	Step: 1, 	{'train/ctc_loss': Array(31.28297, dtype=float32), 'train/wer': 0.9402553710331124, 'validation/ctc_loss': Array(30.141817, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 35.15062928199768, 'total_duration': 152.89069986343384, 'accumulated_submission_time': 35.15062928199768, 'accumulated_eval_time': 117.74000239372253, 'accumulated_logging_time': 0}
I0214 20:00:06.547867 139578135525120 logging_writer.py:48] [1] accumulated_eval_time=117.740002, accumulated_logging_time=0, accumulated_submission_time=35.150629, global_step=1, preemption_count=0, score=35.150629, test/ctc_loss=30.20883560180664, test/num_examples=2472, test/wer=0.908598, total_duration=152.890700, train/ctc_loss=31.282970428466797, train/wer=0.940255, validation/ctc_loss=30.141817092895508, validation/num_examples=5348, validation/wer=0.904371
I0214 20:01:48.705901 139517189850880 logging_writer.py:48] [100] global_step=100, grad_norm=3.546036720275879, loss=6.660452842712402
I0214 20:03:05.189395 139517198243584 logging_writer.py:48] [200] global_step=200, grad_norm=0.634671151638031, loss=5.923033714294434
I0214 20:04:21.694903 139517189850880 logging_writer.py:48] [300] global_step=300, grad_norm=0.59738689661026, loss=5.840603828430176
I0214 20:05:38.133114 139517198243584 logging_writer.py:48] [400] global_step=400, grad_norm=0.7159481048583984, loss=5.815854549407959
I0214 20:07:06.908145 139517189850880 logging_writer.py:48] [500] global_step=500, grad_norm=3.5868520736694336, loss=5.823735237121582
I0214 20:08:37.962409 139517198243584 logging_writer.py:48] [600] global_step=600, grad_norm=2.5879900455474854, loss=5.817413330078125
I0214 20:10:08.485433 139517189850880 logging_writer.py:48] [700] global_step=700, grad_norm=4.732459545135498, loss=5.807005882263184
I0214 20:11:38.781396 139517198243584 logging_writer.py:48] [800] global_step=800, grad_norm=2.85164475440979, loss=5.791976451873779
I0214 20:13:11.170334 139517189850880 logging_writer.py:48] [900] global_step=900, grad_norm=0.961694598197937, loss=5.786746025085449
I0214 20:14:41.636238 139517198243584 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.6366028785705566, loss=5.780921936035156
I0214 20:16:06.336630 139578135525120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4106183350086212, loss=5.738053798675537
I0214 20:17:23.481297 139578127132416 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.6531085968017578, loss=5.627452373504639
I0214 20:18:41.960636 139578135525120 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9649553894996643, loss=5.495645046234131
I0214 20:20:03.223064 139578127132416 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.478766918182373, loss=5.296218395233154
I0214 20:21:29.418605 139578135525120 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.0879878997802734, loss=4.521991729736328
I0214 20:23:01.186685 139578127132416 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.2401243448257446, loss=3.9856410026550293
I0214 20:24:07.445576 139688679413568 spec.py:321] Evaluating on the training split.
I0214 20:24:49.038375 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 20:25:36.851265 139688679413568 spec.py:349] Evaluating on the test split.
I0214 20:26:02.416218 139688679413568 submission_runner.py:408] Time since start: 1708.77s, 	Step: 1672, 	{'train/ctc_loss': Array(6.5428987, dtype=float32), 'train/wer': 0.8976848691695108, 'validation/ctc_loss': Array(6.410177, dtype=float32), 'validation/wer': 0.8668623343020169, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.379735, dtype=float32), 'test/wer': 0.8667357260374139, 'test/num_examples': 2472, 'score': 1475.9704265594482, 'total_duration': 1708.7733316421509, 'accumulated_submission_time': 1475.9704265594482, 'accumulated_eval_time': 232.7037012577057, 'accumulated_logging_time': 0.03008103370666504}
I0214 20:26:02.448273 139578135525120 logging_writer.py:48] [1672] accumulated_eval_time=232.703701, accumulated_logging_time=0.030081, accumulated_submission_time=1475.970427, global_step=1672, preemption_count=0, score=1475.970427, test/ctc_loss=6.379734992980957, test/num_examples=2472, test/wer=0.866736, total_duration=1708.773332, train/ctc_loss=6.542898654937744, train/wer=0.897685, validation/ctc_loss=6.410177230834961, validation/num_examples=5348, validation/wer=0.866862
I0214 20:26:24.563322 139578127132416 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.2578861713409424, loss=3.698420524597168
I0214 20:27:41.094281 139578135525120 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0317254066467285, loss=3.449544668197632
I0214 20:29:00.969509 139578127132416 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1265579462051392, loss=3.2944679260253906
I0214 20:30:29.204430 139578135525120 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.134478211402893, loss=3.1746089458465576
I0214 20:32:00.204864 139578135525120 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1077406406402588, loss=3.0779523849487305
I0214 20:33:16.381936 139578127132416 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.2198115587234497, loss=2.9980273246765137
I0214 20:34:35.295276 139578135525120 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.051957368850708, loss=2.8807623386383057
I0214 20:35:58.422504 139578127132416 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.2090063095092773, loss=2.830658197402954
I0214 20:37:26.250099 139578135525120 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8764724731445312, loss=2.7512590885162354
I0214 20:38:55.886584 139578127132416 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.335126280784607, loss=2.700580596923828
I0214 20:40:25.526469 139578135525120 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0923161506652832, loss=2.6340835094451904
I0214 20:41:56.655579 139578127132416 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9435584545135498, loss=2.555041790008545
I0214 20:43:27.808478 139578135525120 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0261001586914062, loss=2.5016543865203857
I0214 20:44:59.272537 139578127132416 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8118002414703369, loss=2.487936496734619
I0214 20:46:32.414558 139578135525120 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.2587834596633911, loss=2.435622453689575
I0214 20:47:49.867448 139578127132416 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.5813140869140625, loss=2.3579366207122803
I0214 20:49:09.247639 139578135525120 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8508110046386719, loss=2.3146812915802
I0214 20:50:03.060849 139688679413568 spec.py:321] Evaluating on the training split.
I0214 20:50:55.833839 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 20:51:48.664145 139688679413568 spec.py:349] Evaluating on the test split.
I0214 20:52:16.013853 139688679413568 submission_runner.py:408] Time since start: 3282.37s, 	Step: 3369, 	{'train/ctc_loss': Array(2.9755747, dtype=float32), 'train/wer': 0.6129117945215133, 'validation/ctc_loss': Array(2.9443042, dtype=float32), 'validation/wer': 0.5842320206223389, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5972764, dtype=float32), 'test/wer': 0.532935226372555, 'test/num_examples': 2472, 'score': 2916.4993340969086, 'total_duration': 3282.37225317955, 'accumulated_submission_time': 2916.4993340969086, 'accumulated_eval_time': 365.6510236263275, 'accumulated_logging_time': 0.07974767684936523}
I0214 20:52:16.047084 139578135525120 logging_writer.py:48] [3369] accumulated_eval_time=365.651024, accumulated_logging_time=0.079748, accumulated_submission_time=2916.499334, global_step=3369, preemption_count=0, score=2916.499334, test/ctc_loss=2.597276449203491, test/num_examples=2472, test/wer=0.532935, total_duration=3282.372253, train/ctc_loss=2.9755747318267822, train/wer=0.612912, validation/ctc_loss=2.9443042278289795, validation/num_examples=5348, validation/wer=0.584232
I0214 20:52:40.381297 139578127132416 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.1862497329711914, loss=2.264251232147217
I0214 20:53:56.253275 139578135525120 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8934999108314514, loss=2.289259433746338
I0214 20:55:13.876548 139578127132416 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.763604462146759, loss=2.182065486907959
I0214 20:56:42.102121 139578135525120 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.016811490058899, loss=2.166069507598877
I0214 20:58:14.221127 139578127132416 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9896475076675415, loss=2.0566768646240234
I0214 20:59:47.425648 139578135525120 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9699648022651672, loss=2.118276834487915
I0214 21:01:17.544913 139578127132416 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9248892664909363, loss=2.0648624897003174
I0214 21:02:46.346656 139578135525120 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7006480693817139, loss=1.9836015701293945
I0214 21:04:10.760754 139578135525120 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8321978449821472, loss=2.0041489601135254
I0214 21:05:31.245425 139578127132416 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8046751022338867, loss=2.021735191345215
I0214 21:06:50.999801 139578135525120 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7776193618774414, loss=2.010345935821533
I0214 21:08:12.776602 139578127132416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9127340912818909, loss=1.914911150932312
I0214 21:09:38.007645 139578135525120 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9172987341880798, loss=1.8713010549545288
I0214 21:11:08.720865 139578127132416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7883740067481995, loss=1.907867670059204
I0214 21:12:39.444693 139578135525120 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6205056309700012, loss=1.8635246753692627
I0214 21:14:11.181109 139578127132416 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6936256289482117, loss=1.8640974760055542
I0214 21:15:42.718254 139578135525120 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8446351885795593, loss=1.8538053035736084
I0214 21:16:16.300079 139688679413568 spec.py:321] Evaluating on the training split.
I0214 21:17:10.909028 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 21:18:03.958424 139688679413568 spec.py:349] Evaluating on the test split.
I0214 21:18:30.648257 139688679413568 submission_runner.py:408] Time since start: 4857.01s, 	Step: 5038, 	{'train/ctc_loss': Array(0.84934884, dtype=float32), 'train/wer': 0.27204680945020976, 'validation/ctc_loss': Array(0.9519117, dtype=float32), 'validation/wer': 0.28167450302673375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6751439, dtype=float32), 'test/wer': 0.22157902220055653, 'test/num_examples': 2472, 'score': 4356.664489269257, 'total_duration': 4857.005820035934, 'accumulated_submission_time': 4356.664489269257, 'accumulated_eval_time': 499.99262046813965, 'accumulated_logging_time': 0.13511443138122559}
I0214 21:18:30.681962 139578135525120 logging_writer.py:48] [5038] accumulated_eval_time=499.992620, accumulated_logging_time=0.135114, accumulated_submission_time=4356.664489, global_step=5038, preemption_count=0, score=4356.664489, test/ctc_loss=0.6751438975334167, test/num_examples=2472, test/wer=0.221579, total_duration=4857.005820, train/ctc_loss=0.8493488430976868, train/wer=0.272047, validation/ctc_loss=0.9519116878509521, validation/num_examples=5348, validation/wer=0.281675
I0214 21:19:18.479813 139578127132416 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6766945123672485, loss=1.824592113494873
I0214 21:20:39.257464 139578135525120 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.905387282371521, loss=1.785358190536499
I0214 21:21:58.742357 139578127132416 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6433074474334717, loss=1.8111767768859863
I0214 21:23:18.458548 139578135525120 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.822337806224823, loss=1.789097785949707
I0214 21:24:41.320567 139578127132416 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6664002537727356, loss=1.7177796363830566
I0214 21:26:09.822070 139578135525120 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9744217395782471, loss=1.7998974323272705
I0214 21:27:40.670115 139578127132416 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8645281791687012, loss=1.7352675199508667
I0214 21:29:12.053295 139578135525120 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6106752753257751, loss=1.6911180019378662
I0214 21:30:41.106799 139578127132416 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6500809788703918, loss=1.7509849071502686
I0214 21:32:14.761499 139578135525120 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7340775728225708, loss=1.7113642692565918
I0214 21:33:47.339835 139578127132416 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8798186182975769, loss=1.718933343887329
I0214 21:35:17.911635 139578135525120 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7470706105232239, loss=1.6369526386260986
I0214 21:36:37.265860 139578127132416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7169225811958313, loss=1.7050018310546875
I0214 21:37:57.367316 139578135525120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5855244398117065, loss=1.6444604396820068
I0214 21:39:16.854085 139578127132416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6927430629730225, loss=1.6729848384857178
I0214 21:40:43.354828 139578135525120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6177418828010559, loss=1.68214750289917
I0214 21:42:15.173161 139578127132416 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6226477026939392, loss=1.6610865592956543
I0214 21:42:30.740905 139688679413568 spec.py:321] Evaluating on the training split.
I0214 21:43:25.060580 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 21:44:19.735380 139688679413568 spec.py:349] Evaluating on the test split.
I0214 21:44:46.958805 139688679413568 submission_runner.py:408] Time since start: 6433.32s, 	Step: 6718, 	{'train/ctc_loss': Array(0.58703554, dtype=float32), 'train/wer': 0.20140974139371898, 'validation/ctc_loss': Array(0.7285059, dtype=float32), 'validation/wer': 0.2218735819728318, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47832748, dtype=float32), 'test/wer': 0.16011618223549245, 'test/num_examples': 2472, 'score': 5796.641132116318, 'total_duration': 6433.316517829895, 'accumulated_submission_time': 5796.641132116318, 'accumulated_eval_time': 636.2041306495667, 'accumulated_logging_time': 0.18589329719543457}
I0214 21:44:46.995443 139578135525120 logging_writer.py:48] [6718] accumulated_eval_time=636.204131, accumulated_logging_time=0.185893, accumulated_submission_time=5796.641132, global_step=6718, preemption_count=0, score=5796.641132, test/ctc_loss=0.4783274829387665, test/num_examples=2472, test/wer=0.160116, total_duration=6433.316518, train/ctc_loss=0.5870355367660522, train/wer=0.201410, validation/ctc_loss=0.7285059094429016, validation/num_examples=5348, validation/wer=0.221874
I0214 21:45:49.944283 139578127132416 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6310815811157227, loss=1.6315586566925049
I0214 21:47:05.661640 139578135525120 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6191092729568481, loss=1.5968644618988037
I0214 21:48:32.016785 139578127132416 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5995302796363831, loss=1.634187936782837
I0214 21:50:04.698934 139578135525120 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7727752923965454, loss=1.6035329103469849
I0214 21:51:33.051279 139578127132416 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6574514508247375, loss=1.5912840366363525
I0214 21:52:55.281874 139578135525120 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5927439332008362, loss=1.6147314310073853
I0214 21:54:12.362767 139578127132416 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7008399367332458, loss=1.6230381727218628
I0214 21:55:33.411235 139578135525120 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5858558416366577, loss=1.5960801839828491
I0214 21:56:52.896437 139578127132416 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.632207453250885, loss=1.6541637182235718
I0214 21:58:18.570238 139578135525120 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6276093125343323, loss=1.6252707242965698
I0214 21:59:51.278923 139578127132416 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8387790322303772, loss=1.5889275074005127
I0214 22:01:22.425845 139578135525120 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7855564951896667, loss=1.5887717008590698
I0214 22:02:51.322991 139578127132416 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9163538217544556, loss=1.619828224182129
I0214 22:04:22.507653 139578135525120 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6222085952758789, loss=1.4925594329833984
I0214 22:05:54.128990 139578127132416 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7476115226745605, loss=1.5883793830871582
I0214 22:07:21.162472 139578135525120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.8765326738357544, loss=1.523772954940796
I0214 22:08:39.210818 139578127132416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7971791625022888, loss=1.4737582206726074
I0214 22:08:47.140409 139688679413568 spec.py:321] Evaluating on the training split.
I0214 22:09:40.732302 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 22:10:33.987610 139688679413568 spec.py:349] Evaluating on the test split.
I0214 22:11:01.759031 139688679413568 submission_runner.py:408] Time since start: 8008.12s, 	Step: 8411, 	{'train/ctc_loss': Array(0.49299544, dtype=float32), 'train/wer': 0.17005069157397062, 'validation/ctc_loss': Array(0.6641883, dtype=float32), 'validation/wer': 0.20003475675101615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42482418, dtype=float32), 'test/wer': 0.14409034590620112, 'test/num_examples': 2472, 'score': 7236.699079275131, 'total_duration': 8008.11719751358, 'accumulated_submission_time': 7236.699079275131, 'accumulated_eval_time': 770.8168325424194, 'accumulated_logging_time': 0.24300336837768555}
I0214 22:11:01.791735 139578135525120 logging_writer.py:48] [8411] accumulated_eval_time=770.816833, accumulated_logging_time=0.243003, accumulated_submission_time=7236.699079, global_step=8411, preemption_count=0, score=7236.699079, test/ctc_loss=0.42482417821884155, test/num_examples=2472, test/wer=0.144090, total_duration=8008.117198, train/ctc_loss=0.4929954409599304, train/wer=0.170051, validation/ctc_loss=0.6641883254051208, validation/num_examples=5348, validation/wer=0.200035
I0214 22:12:09.823480 139578127132416 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5783225297927856, loss=1.5099347829818726
I0214 22:13:25.624564 139578135525120 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7642483115196228, loss=1.5061132907867432
I0214 22:14:41.461400 139578127132416 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7261297702789307, loss=1.5285931825637817
I0214 22:16:07.220104 139578135525120 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5798040628433228, loss=1.4956536293029785
I0214 22:17:39.123162 139578127132416 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5987759232521057, loss=1.5359656810760498
I0214 22:19:09.056438 139578135525120 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7515243291854858, loss=1.5476224422454834
I0214 22:20:41.315202 139578127132416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5744791626930237, loss=1.5203237533569336
I0214 22:22:11.375262 139578135525120 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5871458053588867, loss=1.4990441799163818
I0214 22:23:40.077305 139578135525120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6473755240440369, loss=1.4592753648757935
I0214 22:24:56.203404 139578127132416 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.683172345161438, loss=1.5484188795089722
I0214 22:26:11.920075 139578135525120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6591368913650513, loss=1.5072801113128662
I0214 22:27:33.175065 139578127132416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6397099494934082, loss=1.5009630918502808
I0214 22:28:55.973181 139578135525120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6729128360748291, loss=1.4383214712142944
I0214 22:30:23.428490 139578127132416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6710103750228882, loss=1.490456461906433
I0214 22:31:52.419740 139578135525120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5853493213653564, loss=1.5213066339492798
I0214 22:33:23.059369 139578127132416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5668193101882935, loss=1.4761669635772705
I0214 22:34:48.386952 139578135525120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6665578484535217, loss=1.5072321891784668
I0214 22:35:02.285854 139688679413568 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0214 22:36:21.233538 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 22:37:14.044819 139688679413568 spec.py:349] Evaluating on the test split.
I0214 22:37:41.644055 139688679413568 submission_runner.py:408] Time since start: 9608.00s, 	Step: 10118, 	{'train/ctc_loss': Array(0.29567704, dtype=float32), 'train/wer': 0.10973898996769257, 'validation/ctc_loss': Array(0.60095006, dtype=float32), 'validation/wer': 0.18386321287544533, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3799917, dtype=float32), 'test/wer': 0.1304003412345378, 'test/num_examples': 2472, 'score': 8677.11043047905, 'total_duration': 9608.002731323242, 'accumulated_submission_time': 8677.11043047905, 'accumulated_eval_time': 930.1696033477783, 'accumulated_logging_time': 0.2932603359222412}
I0214 22:37:41.677015 139578135525120 logging_writer.py:48] [10118] accumulated_eval_time=930.169603, accumulated_logging_time=0.293260, accumulated_submission_time=8677.110430, global_step=10118, preemption_count=0, score=8677.110430, test/ctc_loss=0.37999171018600464, test/num_examples=2472, test/wer=0.130400, total_duration=9608.002731, train/ctc_loss=0.2956770360469818, train/wer=0.109739, validation/ctc_loss=0.6009500622749329, validation/num_examples=5348, validation/wer=0.183863
I0214 22:38:44.381218 139578127132416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7626423239707947, loss=1.4945350885391235
I0214 22:40:04.060566 139578135525120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6241157054901123, loss=1.508161187171936
I0214 22:41:21.210884 139578127132416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5652436017990112, loss=1.4617247581481934
I0214 22:42:39.673926 139578135525120 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5805407762527466, loss=1.446916103363037
I0214 22:44:03.210618 139578127132416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6857378482818604, loss=1.427443504333496
I0214 22:45:23.080485 139578135525120 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6329740285873413, loss=1.4684048891067505
I0214 22:46:51.398116 139578127132416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7082151174545288, loss=1.5304327011108398
I0214 22:48:19.850953 139578135525120 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6563133597373962, loss=1.4649652242660522
I0214 22:49:50.452987 139578127132416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5869025588035583, loss=1.4498552083969116
I0214 22:51:22.913284 139578135525120 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5200271606445312, loss=1.4346121549606323
I0214 22:52:53.577395 139578127132416 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7826918363571167, loss=1.4216102361679077
I0214 22:54:23.468803 139578135525120 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5799981951713562, loss=1.4270381927490234
I0214 22:55:47.022944 139578135525120 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6461434960365295, loss=1.384198546409607
I0214 22:57:02.932999 139578127132416 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5983232259750366, loss=1.4785841703414917
I0214 22:58:19.916086 139578135525120 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6136350035667419, loss=1.4069405794143677
I0214 22:59:45.987035 139578127132416 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5695209503173828, loss=1.4100470542907715
I0214 23:01:14.070631 139578135525120 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6429843306541443, loss=1.381213903427124
I0214 23:01:42.366720 139688679413568 spec.py:321] Evaluating on the training split.
I0214 23:02:38.679162 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 23:03:31.949346 139688679413568 spec.py:349] Evaluating on the test split.
I0214 23:03:58.651974 139688679413568 submission_runner.py:408] Time since start: 11185.01s, 	Step: 11832, 	{'train/ctc_loss': Array(0.2633589, dtype=float32), 'train/wer': 0.0978113489841189, 'validation/ctc_loss': Array(0.5699334, dtype=float32), 'validation/wer': 0.17382237369300135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3490801, dtype=float32), 'test/wer': 0.1201836166798692, 'test/num_examples': 2472, 'score': 10117.716738939285, 'total_duration': 11185.009489297867, 'accumulated_submission_time': 10117.716738939285, 'accumulated_eval_time': 1066.4482853412628, 'accumulated_logging_time': 0.34238743782043457}
I0214 23:03:58.685502 139578135525120 logging_writer.py:48] [11832] accumulated_eval_time=1066.448285, accumulated_logging_time=0.342387, accumulated_submission_time=10117.716739, global_step=11832, preemption_count=0, score=10117.716739, test/ctc_loss=0.34908008575439453, test/num_examples=2472, test/wer=0.120184, total_duration=11185.009489, train/ctc_loss=0.2633588910102844, train/wer=0.097811, validation/ctc_loss=0.5699334144592285, validation/num_examples=5348, validation/wer=0.173822
I0214 23:04:51.036511 139578127132416 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5965604782104492, loss=1.4174309968948364
I0214 23:06:06.600449 139578135525120 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7976387739181519, loss=1.4541192054748535
I0214 23:07:32.778486 139578127132416 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5293377637863159, loss=1.384750485420227
I0214 23:09:06.096050 139578135525120 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6309191584587097, loss=1.4206148386001587
I0214 23:10:37.367171 139578127132416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7263532876968384, loss=1.4338994026184082
I0214 23:12:06.637750 139578135525120 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5266435742378235, loss=1.3813672065734863
I0214 23:13:27.567294 139578127132416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5920813679695129, loss=1.3681933879852295
I0214 23:14:46.505977 139578135525120 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5407434105873108, loss=1.3478854894638062
I0214 23:16:08.076761 139578127132416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5837739706039429, loss=1.3958652019500732
I0214 23:17:33.961658 139578135525120 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5351922512054443, loss=1.4125887155532837
I0214 23:19:01.014067 139578127132416 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5632228851318359, loss=1.3772716522216797
I0214 23:20:32.911603 139578135525120 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6238994598388672, loss=1.3699811697006226
I0214 23:22:02.165868 139578127132416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6310672163963318, loss=1.3887847661972046
I0214 23:23:34.699402 139578135525120 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5662185549736023, loss=1.3531081676483154
I0214 23:25:06.288004 139578127132416 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6055219173431396, loss=1.3716155290603638
I0214 23:26:41.754594 139578135525120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6516231894493103, loss=1.381919503211975
I0214 23:27:59.296587 139578127132416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.545421302318573, loss=1.3831558227539062
I0214 23:27:59.305949 139688679413568 spec.py:321] Evaluating on the training split.
I0214 23:28:54.593467 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 23:29:46.899241 139688679413568 spec.py:349] Evaluating on the test split.
I0214 23:30:13.998723 139688679413568 submission_runner.py:408] Time since start: 12760.36s, 	Step: 13501, 	{'train/ctc_loss': Array(0.25779513, dtype=float32), 'train/wer': 0.09635561193477629, 'validation/ctc_loss': Array(0.5493172, dtype=float32), 'validation/wer': 0.16707377120403177, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33060178, dtype=float32), 'test/wer': 0.11193711534946073, 'test/num_examples': 2472, 'score': 11558.25578045845, 'total_duration': 12760.356534957886, 'accumulated_submission_time': 11558.25578045845, 'accumulated_eval_time': 1201.1347353458405, 'accumulated_logging_time': 0.3927774429321289}
I0214 23:30:14.032501 139578135525120 logging_writer.py:48] [13501] accumulated_eval_time=1201.134735, accumulated_logging_time=0.392777, accumulated_submission_time=11558.255780, global_step=13501, preemption_count=0, score=11558.255780, test/ctc_loss=0.3306017816066742, test/num_examples=2472, test/wer=0.111937, total_duration=12760.356535, train/ctc_loss=0.257795125246048, train/wer=0.096356, validation/ctc_loss=0.5493171811103821, validation/num_examples=5348, validation/wer=0.167074
I0214 23:31:29.890679 139578127132416 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5476698875427246, loss=1.361814022064209
I0214 23:32:45.800961 139578135525120 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5996789336204529, loss=1.3776288032531738
I0214 23:34:01.617233 139578127132416 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6716780066490173, loss=1.3749842643737793
I0214 23:35:30.071262 139578135525120 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6014581322669983, loss=1.361100196838379
I0214 23:37:00.527383 139578127132416 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6916749477386475, loss=1.3614766597747803
I0214 23:38:30.977714 139578135525120 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.8531964421272278, loss=1.3754180669784546
I0214 23:40:00.326630 139578127132416 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6539933085441589, loss=1.4022085666656494
I0214 23:41:32.258402 139578135525120 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6304277777671814, loss=1.3889015913009644
I0214 23:43:04.424717 139578127132416 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.554355263710022, loss=1.4247397184371948
I0214 23:44:28.192917 139578135525120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6040875911712646, loss=1.393938422203064
I0214 23:45:46.576251 139578127132416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.623433530330658, loss=1.3689415454864502
I0214 23:47:10.514338 139578135525120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7718559503555298, loss=1.396135926246643
I0214 23:48:35.556172 139578127132416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.5393919348716736, loss=1.3036962747573853
I0214 23:50:06.681020 139578135525120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.584094762802124, loss=1.303836703300476
I0214 23:51:38.254317 139578127132416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6277573704719543, loss=1.3452849388122559
I0214 23:53:08.294311 139578135525120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.5607585906982422, loss=1.3642845153808594
I0214 23:54:14.765168 139688679413568 spec.py:321] Evaluating on the training split.
I0214 23:55:10.491145 139688679413568 spec.py:333] Evaluating on the validation split.
I0214 23:56:03.267637 139688679413568 spec.py:349] Evaluating on the test split.
I0214 23:56:31.142477 139688679413568 submission_runner.py:408] Time since start: 14337.50s, 	Step: 15177, 	{'train/ctc_loss': Array(0.22716044, dtype=float32), 'train/wer': 0.08648468152600385, 'validation/ctc_loss': Array(0.5236656, dtype=float32), 'validation/wer': 0.15900248124583644, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3165861, dtype=float32), 'test/wer': 0.10801698048057197, 'test/num_examples': 2472, 'score': 12998.907264947891, 'total_duration': 14337.500767707825, 'accumulated_submission_time': 12998.907264947891, 'accumulated_eval_time': 1337.5061967372894, 'accumulated_logging_time': 0.44394874572753906}
I0214 23:56:31.176844 139578135525120 logging_writer.py:48] [15177] accumulated_eval_time=1337.506197, accumulated_logging_time=0.443949, accumulated_submission_time=12998.907265, global_step=15177, preemption_count=0, score=12998.907265, test/ctc_loss=0.31658610701560974, test/num_examples=2472, test/wer=0.108017, total_duration=14337.500768, train/ctc_loss=0.22716043889522552, train/wer=0.086485, validation/ctc_loss=0.5236656069755554, validation/num_examples=5348, validation/wer=0.159002
I0214 23:56:49.326453 139578127132416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6654799580574036, loss=1.3843320608139038
I0214 23:58:05.015333 139578135525120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6223951578140259, loss=1.3226368427276611
I0214 23:59:24.332796 139578127132416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5535939931869507, loss=1.3740004301071167
I0215 00:00:54.554470 139578135525120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6349324584007263, loss=1.2912935018539429
I0215 00:02:13.024184 139578127132416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6602118015289307, loss=1.331282615661621
I0215 00:03:36.661958 139578135525120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6988399028778076, loss=1.3589365482330322
I0215 00:05:02.697568 139578127132416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7114914059638977, loss=1.2730293273925781
I0215 00:06:31.043421 139578135525120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6107701659202576, loss=1.3341434001922607
I0215 00:07:58.160399 139578127132416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5833272933959961, loss=1.2887192964553833
I0215 00:09:29.485770 139578135525120 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5146609544754028, loss=1.2835832834243774
I0215 00:11:04.201447 139578127132416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7036224603652954, loss=1.3924646377563477
I0215 00:12:38.038716 139578135525120 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6555160880088806, loss=1.2915282249450684
I0215 00:14:09.269922 139578127132416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5591190457344055, loss=1.3811187744140625
I0215 00:15:37.461256 139578135525120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6834127902984619, loss=1.341517686843872
I0215 00:16:54.783798 139578127132416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.7036621570587158, loss=1.306151032447815
I0215 00:18:14.620380 139578135525120 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6114094853401184, loss=1.3114053010940552
I0215 00:19:38.680727 139578127132416 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6325811147689819, loss=1.3641560077667236
I0215 00:20:31.362361 139688679413568 spec.py:321] Evaluating on the training split.
I0215 00:21:26.561966 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 00:22:19.591069 139688679413568 spec.py:349] Evaluating on the test split.
I0215 00:22:48.093080 139688679413568 submission_runner.py:408] Time since start: 15914.45s, 	Step: 16862, 	{'train/ctc_loss': Array(0.22985189, dtype=float32), 'train/wer': 0.0873535887743841, 'validation/ctc_loss': Array(0.5217149, dtype=float32), 'validation/wer': 0.1570329320215878, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3099842, dtype=float32), 'test/wer': 0.10523429407104991, 'test/num_examples': 2472, 'score': 14439.008511543274, 'total_duration': 15914.451095581055, 'accumulated_submission_time': 14439.008511543274, 'accumulated_eval_time': 1474.2307941913605, 'accumulated_logging_time': 0.49520206451416016}
I0215 00:22:48.125965 139578135525120 logging_writer.py:48] [16862] accumulated_eval_time=1474.230794, accumulated_logging_time=0.495202, accumulated_submission_time=14439.008512, global_step=16862, preemption_count=0, score=14439.008512, test/ctc_loss=0.3099842071533203, test/num_examples=2472, test/wer=0.105234, total_duration=15914.451096, train/ctc_loss=0.2298518866300583, train/wer=0.087354, validation/ctc_loss=0.5217149257659912, validation/num_examples=5348, validation/wer=0.157033
I0215 00:23:17.798842 139578127132416 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.723564863204956, loss=1.3401257991790771
I0215 00:24:33.724344 139578135525120 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6282602548599243, loss=1.311432957649231
I0215 00:25:53.786814 139578127132416 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5859078168869019, loss=1.2782541513442993
I0215 00:27:22.045130 139578135525120 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.5514746308326721, loss=1.315413236618042
I0215 00:28:55.297565 139578127132416 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.609928548336029, loss=1.3103768825531006
I0215 00:30:28.308644 139578135525120 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.689298689365387, loss=1.3408211469650269
I0215 00:31:59.821464 139578127132416 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6129292845726013, loss=1.3197808265686035
I0215 00:33:21.151095 139578135525120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.526096522808075, loss=1.2716851234436035
I0215 00:34:42.111653 139578127132416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.654977560043335, loss=1.2987686395645142
I0215 00:36:04.195692 139578135525120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6905365586280823, loss=1.2898480892181396
I0215 00:37:27.612381 139578127132416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7192791104316711, loss=1.351270079612732
I0215 00:38:56.988171 139578135525120 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6456443071365356, loss=1.2806323766708374
I0215 00:40:29.314735 139578127132416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.525951087474823, loss=1.2226526737213135
I0215 00:41:59.247065 139578135525120 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5983411073684692, loss=1.27080500125885
I0215 00:43:28.521962 139578127132416 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6354714632034302, loss=1.2713313102722168
I0215 00:44:57.832647 139578135525120 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6462834477424622, loss=1.322165608406067
I0215 00:46:26.400680 139578127132416 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5194966793060303, loss=1.298893690109253
I0215 00:46:48.643597 139688679413568 spec.py:321] Evaluating on the training split.
I0215 00:47:43.992195 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 00:48:37.787292 139688679413568 spec.py:349] Evaluating on the test split.
I0215 00:49:05.321046 139688679413568 submission_runner.py:408] Time since start: 17491.68s, 	Step: 18526, 	{'train/ctc_loss': Array(0.2079086, dtype=float32), 'train/wer': 0.08069496831824934, 'validation/ctc_loss': Array(0.49227136, dtype=float32), 'validation/wer': 0.15032294814485841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29305145, dtype=float32), 'test/wer': 0.09954705177421648, 'test/num_examples': 2472, 'score': 15879.444328546524, 'total_duration': 17491.679450511932, 'accumulated_submission_time': 15879.444328546524, 'accumulated_eval_time': 1610.9025394916534, 'accumulated_logging_time': 0.5452065467834473}
I0215 00:49:05.356404 139578135525120 logging_writer.py:48] [18526] accumulated_eval_time=1610.902539, accumulated_logging_time=0.545207, accumulated_submission_time=15879.444329, global_step=18526, preemption_count=0, score=15879.444329, test/ctc_loss=0.29305145144462585, test/num_examples=2472, test/wer=0.099547, total_duration=17491.679451, train/ctc_loss=0.20790860056877136, train/wer=0.080695, validation/ctc_loss=0.492271363735199, validation/num_examples=5348, validation/wer=0.150323
I0215 00:50:08.112837 139578135525120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.727662980556488, loss=1.2752971649169922
I0215 00:51:28.694002 139578127132416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.519133985042572, loss=1.3227943181991577
I0215 00:52:47.434695 139578135525120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.5464832186698914, loss=1.307971477508545
I0215 00:54:10.325497 139578127132416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6717007756233215, loss=1.2708592414855957
I0215 00:55:37.578103 139578135525120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5504314303398132, loss=1.2699635028839111
I0215 00:57:08.749439 139578127132416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.549362301826477, loss=1.297001600265503
I0215 00:58:38.173529 139578135525120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.622935950756073, loss=1.3008294105529785
I0215 01:00:08.940241 139578127132416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6077231168746948, loss=1.3126282691955566
I0215 01:01:42.036181 139578135525120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5589187741279602, loss=1.2270902395248413
I0215 01:03:12.367383 139578127132416 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.622417688369751, loss=1.3301955461502075
I0215 01:04:40.819063 139578135525120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.6416087746620178, loss=1.2811036109924316
I0215 01:06:00.175444 139578127132416 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6916502118110657, loss=1.2703649997711182
I0215 01:07:17.411130 139578135525120 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.6910690069198608, loss=1.3125711679458618
I0215 01:08:39.433535 139578127132416 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.8167287707328796, loss=1.2717026472091675
I0215 01:10:08.021759 139578135525120 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5890328884124756, loss=1.235687255859375
I0215 01:11:39.194238 139578127132416 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6133539080619812, loss=1.2064263820648193
I0215 01:13:04.726126 139578135525120 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6072641015052795, loss=1.2745733261108398
I0215 01:13:06.416560 139688679413568 spec.py:321] Evaluating on the training split.
I0215 01:14:01.687784 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 01:14:55.209411 139688679413568 spec.py:349] Evaluating on the test split.
I0215 01:15:22.466677 139688679413568 submission_runner.py:408] Time since start: 19068.82s, 	Step: 20203, 	{'train/ctc_loss': Array(0.22827052, dtype=float32), 'train/wer': 0.08320993218302, 'validation/ctc_loss': Array(0.48092973, dtype=float32), 'validation/wer': 0.1457949158596986, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28144392, dtype=float32), 'test/wer': 0.09552535900716999, 'test/num_examples': 2472, 'score': 17320.42205142975, 'total_duration': 19068.82412481308, 'accumulated_submission_time': 17320.42205142975, 'accumulated_eval_time': 1746.945957183838, 'accumulated_logging_time': 0.5986299514770508}
I0215 01:15:22.503735 139578135525120 logging_writer.py:48] [20203] accumulated_eval_time=1746.945957, accumulated_logging_time=0.598630, accumulated_submission_time=17320.422051, global_step=20203, preemption_count=0, score=17320.422051, test/ctc_loss=0.28144392371177673, test/num_examples=2472, test/wer=0.095525, total_duration=19068.824125, train/ctc_loss=0.2282705157995224, train/wer=0.083210, validation/ctc_loss=0.48092973232269287, validation/num_examples=5348, validation/wer=0.145795
I0215 01:16:36.614183 139578127132416 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.6761055588722229, loss=1.2796858549118042
I0215 01:17:52.435578 139578135525120 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6317274570465088, loss=1.3027414083480835
I0215 01:19:20.804657 139578127132416 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5640116333961487, loss=1.3059698343276978
I0215 01:20:55.785487 139578135525120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.5880882740020752, loss=1.2570208311080933
I0215 01:22:15.016100 139578127132416 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5995875597000122, loss=1.2472084760665894
I0215 01:23:34.590090 139578135525120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6111771464347839, loss=1.2368242740631104
I0215 01:24:55.918232 139578127132416 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6909384727478027, loss=1.1970546245574951
I0215 01:26:22.262167 139578135525120 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6315256953239441, loss=1.2442340850830078
I0215 01:27:52.984202 139578127132416 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.5460600256919861, loss=1.2333494424819946
I0215 01:29:23.947038 139578135525120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6035374402999878, loss=1.241855263710022
I0215 01:30:55.945686 139578127132416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6315790414810181, loss=1.2393068075180054
I0215 01:32:27.745358 139578135525120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7634783387184143, loss=1.2556525468826294
I0215 01:33:58.160094 139578127132416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5721921324729919, loss=1.2596315145492554
I0215 01:35:30.078132 139578135525120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5723764300346375, loss=1.324148416519165
I0215 01:36:55.138337 139578135525120 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5619633793830872, loss=1.2601299285888672
I0215 01:38:13.440378 139578127132416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6774123311042786, loss=1.2256962060928345
I0215 01:39:22.883726 139688679413568 spec.py:321] Evaluating on the training split.
I0215 01:40:18.891828 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 01:41:11.665482 139688679413568 spec.py:349] Evaluating on the test split.
I0215 01:41:39.881627 139688679413568 submission_runner.py:408] Time since start: 20646.24s, 	Step: 21887, 	{'train/ctc_loss': Array(0.2024693, dtype=float32), 'train/wer': 0.07828138252756574, 'validation/ctc_loss': Array(0.4742581, dtype=float32), 'validation/wer': 0.14387363990075017, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2813647, dtype=float32), 'test/wer': 0.09574878638311701, 'test/num_examples': 2472, 'score': 18760.71983551979, 'total_duration': 20646.238900899887, 'accumulated_submission_time': 18760.71983551979, 'accumulated_eval_time': 1883.9370305538177, 'accumulated_logging_time': 0.6513259410858154}
I0215 01:41:39.918896 139578135525120 logging_writer.py:48] [21887] accumulated_eval_time=1883.937031, accumulated_logging_time=0.651326, accumulated_submission_time=18760.719836, global_step=21887, preemption_count=0, score=18760.719836, test/ctc_loss=0.28136470913887024, test/num_examples=2472, test/wer=0.095749, total_duration=20646.238901, train/ctc_loss=0.20246930420398712, train/wer=0.078281, validation/ctc_loss=0.47425809502601624, validation/num_examples=5348, validation/wer=0.143874
I0215 01:41:50.579742 139578127132416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.704704225063324, loss=1.247619867324829
I0215 01:43:06.229634 139578135525120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6349834203720093, loss=1.2275587320327759
I0215 01:44:21.989268 139578127132416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5707013010978699, loss=1.220970869064331
I0215 01:45:50.665445 139578135525120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.5762107372283936, loss=1.2487542629241943
I0215 01:47:19.682368 139578127132416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6525970101356506, loss=1.1780349016189575
I0215 01:48:50.147677 139578135525120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7395086288452148, loss=1.233677864074707
I0215 01:50:22.618139 139578127132416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6729702949523926, loss=1.261892318725586
I0215 01:51:54.753505 139578135525120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6870709657669067, loss=1.2357077598571777
I0215 01:53:23.858603 139578135525120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.6029683947563171, loss=1.214546799659729
I0215 01:54:41.757353 139578127132416 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6117777228355408, loss=1.2022520303726196
I0215 01:56:00.752477 139578135525120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.5933042764663696, loss=1.2180343866348267
I0215 01:57:22.707759 139578127132416 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.5432599782943726, loss=1.194096565246582
I0215 01:58:51.065560 139578135525120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.6892341375350952, loss=1.2722851037979126
I0215 02:00:21.426216 139578127132416 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6153263449668884, loss=1.2356071472167969
I0215 02:01:53.030849 139578135525120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.647013247013092, loss=1.1877351999282837
I0215 02:03:21.902939 139578127132416 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.682410478591919, loss=1.1896929740905762
I0215 02:04:55.546153 139578135525120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6121690273284912, loss=1.246711015701294
I0215 02:05:40.495644 139688679413568 spec.py:321] Evaluating on the training split.
I0215 02:06:37.526946 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 02:07:32.547137 139688679413568 spec.py:349] Evaluating on the test split.
I0215 02:08:00.321485 139688679413568 submission_runner.py:408] Time since start: 22226.68s, 	Step: 23551, 	{'train/ctc_loss': Array(0.16744016, dtype=float32), 'train/wer': 0.06670042493657273, 'validation/ctc_loss': Array(0.45430538, dtype=float32), 'validation/wer': 0.13942284484007067, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2655093, dtype=float32), 'test/wer': 0.09130055044380801, 'test/num_examples': 2472, 'score': 20201.211846590042, 'total_duration': 22226.676994800568, 'accumulated_submission_time': 20201.211846590042, 'accumulated_eval_time': 2023.754251241684, 'accumulated_logging_time': 0.7084658145904541}
I0215 02:08:00.360488 139578135525120 logging_writer.py:48] [23551] accumulated_eval_time=2023.754251, accumulated_logging_time=0.708466, accumulated_submission_time=20201.211847, global_step=23551, preemption_count=0, score=20201.211847, test/ctc_loss=0.26550930738449097, test/num_examples=2472, test/wer=0.091301, total_duration=22226.676995, train/ctc_loss=0.16744016110897064, train/wer=0.066700, validation/ctc_loss=0.45430538058280945, validation/num_examples=5348, validation/wer=0.139423
I0215 02:08:38.215501 139578127132416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6821935772895813, loss=1.2193505764007568
I0215 02:09:57.912144 139578135525120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6265398859977722, loss=1.1760387420654297
I0215 02:11:15.278550 139578127132416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5894743800163269, loss=1.2954286336898804
I0215 02:12:34.920649 139578135525120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.7648082375526428, loss=1.2168515920639038
I0215 02:13:53.636649 139578127132416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5742994546890259, loss=1.2498785257339478
I0215 02:15:21.510698 139578135525120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.6499586701393127, loss=1.1879024505615234
I0215 02:16:51.274780 139578127132416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7002175450325012, loss=1.2234935760498047
I0215 02:18:23.733338 139578135525120 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.565316915512085, loss=1.2050343751907349
I0215 02:19:55.438438 139578127132416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.6051812767982483, loss=1.2410825490951538
I0215 02:21:25.565322 139578135525120 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.51962810754776, loss=1.2205833196640015
I0215 02:22:55.200459 139578127132416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5391440391540527, loss=1.1875563859939575
I0215 02:24:26.088670 139578135525120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.579479455947876, loss=1.2293164730072021
I0215 02:25:50.460334 139578135525120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6328469514846802, loss=1.1450941562652588
I0215 02:27:06.228773 139578127132416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.597154438495636, loss=1.2354391813278198
I0215 02:28:27.159858 139578135525120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6741549968719482, loss=1.1721572875976562
I0215 02:29:52.499355 139578127132416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6135950088500977, loss=1.173867106437683
I0215 02:31:22.677362 139578135525120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6240115165710449, loss=1.2194281816482544
I0215 02:32:00.736054 139688679413568 spec.py:321] Evaluating on the training split.
I0215 02:32:56.132118 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 02:33:51.010998 139688679413568 spec.py:349] Evaluating on the test split.
I0215 02:34:18.888217 139688679413568 submission_runner.py:408] Time since start: 23805.25s, 	Step: 25245, 	{'train/ctc_loss': Array(0.16750503, dtype=float32), 'train/wer': 0.06492667933146919, 'validation/ctc_loss': Array(0.45686898, dtype=float32), 'validation/wer': 0.13845737953406645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2632382, dtype=float32), 'test/wer': 0.09042715251965146, 'test/num_examples': 2472, 'score': 21641.502986192703, 'total_duration': 23805.24548983574, 'accumulated_submission_time': 21641.502986192703, 'accumulated_eval_time': 2161.89958691597, 'accumulated_logging_time': 0.764528751373291}
I0215 02:34:18.924760 139578135525120 logging_writer.py:48] [25245] accumulated_eval_time=2161.899587, accumulated_logging_time=0.764529, accumulated_submission_time=21641.502986, global_step=25245, preemption_count=0, score=21641.502986, test/ctc_loss=0.26323819160461426, test/num_examples=2472, test/wer=0.090427, total_duration=23805.245490, train/ctc_loss=0.16750502586364746, train/wer=0.064927, validation/ctc_loss=0.456868976354599, validation/num_examples=5348, validation/wer=0.138457
I0215 02:35:01.177851 139578127132416 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.6904617547988892, loss=1.277754783630371
I0215 02:36:16.713866 139578135525120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.8533684015274048, loss=1.2057186365127563
I0215 02:37:41.277020 139578127132416 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5776973366737366, loss=1.2455978393554688
I0215 02:39:12.603185 139578135525120 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.5114424824714661, loss=1.2336770296096802
I0215 02:40:45.249724 139578127132416 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6065367460250854, loss=1.2078499794006348
I0215 02:42:13.221609 139578135525120 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6437830328941345, loss=1.223438024520874
I0215 02:43:33.243646 139578127132416 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6111628413200378, loss=1.1530671119689941
I0215 02:44:53.760744 139578135525120 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.522377610206604, loss=1.186396598815918
I0215 02:46:18.632052 139578127132416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.533565878868103, loss=1.1869333982467651
I0215 02:47:46.510961 139578135525120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6686946749687195, loss=1.198783278465271
I0215 02:49:15.965547 139578127132416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5674954056739807, loss=1.2102916240692139
I0215 02:50:47.331045 139578135525120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6634069085121155, loss=1.1582680940628052
I0215 02:52:16.691777 139578127132416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5796546936035156, loss=1.1927859783172607
I0215 02:53:44.820403 139578135525120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.644389808177948, loss=1.2589149475097656
I0215 02:55:14.037547 139578127132416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6153446435928345, loss=1.2487151622772217
I0215 02:56:45.513690 139578135525120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6429338455200195, loss=1.1964236497879028
I0215 02:58:04.143640 139578127132416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6353955864906311, loss=1.2108503580093384
I0215 02:58:18.990184 139688679413568 spec.py:321] Evaluating on the training split.
I0215 02:59:13.849565 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 03:00:05.920718 139688679413568 spec.py:349] Evaluating on the test split.
I0215 03:00:32.253721 139688679413568 submission_runner.py:408] Time since start: 25378.61s, 	Step: 26921, 	{'train/ctc_loss': Array(0.15699868, dtype=float32), 'train/wer': 0.06478888410168031, 'validation/ctc_loss': Array(0.44483903, dtype=float32), 'validation/wer': 0.13478861137125037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25737813, dtype=float32), 'test/wer': 0.08878191456949606, 'test/num_examples': 2472, 'score': 23081.486287117004, 'total_duration': 25378.611304998398, 'accumulated_submission_time': 23081.486287117004, 'accumulated_eval_time': 2295.1566026210785, 'accumulated_logging_time': 0.8176376819610596}
I0215 03:00:32.291482 139578135525120 logging_writer.py:48] [26921] accumulated_eval_time=2295.156603, accumulated_logging_time=0.817638, accumulated_submission_time=23081.486287, global_step=26921, preemption_count=0, score=23081.486287, test/ctc_loss=0.25737813115119934, test/num_examples=2472, test/wer=0.088782, total_duration=25378.611305, train/ctc_loss=0.1569986790418625, train/wer=0.064789, validation/ctc_loss=0.4448390305042267, validation/num_examples=5348, validation/wer=0.134789
I0215 03:01:32.799088 139578127132416 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5121651887893677, loss=1.1427992582321167
I0215 03:02:48.712756 139578135525120 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.571117103099823, loss=1.1443841457366943
I0215 03:04:05.160531 139578127132416 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6507149338722229, loss=1.1955784559249878
I0215 03:05:35.234185 139578135525120 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5566824674606323, loss=1.1721198558807373
I0215 03:07:04.494591 139578127132416 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7332946062088013, loss=1.1466376781463623
I0215 03:08:32.493445 139578135525120 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5684211850166321, loss=1.2160251140594482
I0215 03:10:06.078921 139578127132416 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6630600094795227, loss=1.1960537433624268
I0215 03:11:39.347831 139578135525120 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.5700542330741882, loss=1.2371094226837158
I0215 03:13:08.359747 139578127132416 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6298046112060547, loss=1.1911439895629883
I0215 03:14:29.619412 139578135525120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6565975546836853, loss=1.1733850240707397
I0215 03:15:47.457521 139578127132416 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.635387659072876, loss=1.2161567211151123
I0215 03:17:07.477723 139578135525120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.5919750928878784, loss=1.196731448173523
I0215 03:18:31.239046 139578127132416 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.997684895992279, loss=1.2031549215316772
I0215 03:20:00.365409 139578135525120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5681861639022827, loss=1.1706452369689941
I0215 03:21:32.650933 139578127132416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.6497325301170349, loss=1.194378137588501
I0215 03:23:01.800072 139578135525120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5400957465171814, loss=1.1641796827316284
I0215 03:24:32.655817 139578127132416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.6694969534873962, loss=1.1580897569656372
I0215 03:24:32.664653 139688679413568 spec.py:321] Evaluating on the training split.
I0215 03:25:28.418172 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 03:26:21.618246 139688679413568 spec.py:349] Evaluating on the test split.
I0215 03:26:48.151339 139688679413568 submission_runner.py:408] Time since start: 26954.51s, 	Step: 28601, 	{'train/ctc_loss': Array(0.16138557, dtype=float32), 'train/wer': 0.06255288918605502, 'validation/ctc_loss': Array(0.43052354, dtype=float32), 'validation/wer': 0.13115846182067448, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24619368, dtype=float32), 'test/wer': 0.08398838177645075, 'test/num_examples': 2472, 'score': 24521.77783679962, 'total_duration': 26954.51037287712, 'accumulated_submission_time': 24521.77783679962, 'accumulated_eval_time': 2430.638184070587, 'accumulated_logging_time': 0.8726804256439209}
I0215 03:26:48.187323 139578135525120 logging_writer.py:48] [28601] accumulated_eval_time=2430.638184, accumulated_logging_time=0.872680, accumulated_submission_time=24521.777837, global_step=28601, preemption_count=0, score=24521.777837, test/ctc_loss=0.24619367718696594, test/num_examples=2472, test/wer=0.083988, total_duration=26954.510373, train/ctc_loss=0.16138556599617004, train/wer=0.062553, validation/ctc_loss=0.430523544549942, validation/num_examples=5348, validation/wer=0.131158
I0215 03:28:04.067842 139578127132416 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.5823089480400085, loss=1.1237612962722778
I0215 03:29:19.737473 139578135525120 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7020555734634399, loss=1.223544716835022
I0215 03:30:42.074949 139578135525120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.5938143134117126, loss=1.196837306022644
I0215 03:32:00.288005 139578127132416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.5805598497390747, loss=1.1715061664581299
I0215 03:33:22.376817 139578135525120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5810806155204773, loss=1.1824194192886353
I0215 03:34:49.226447 139578127132416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5537009835243225, loss=1.1720858812332153
I0215 03:36:16.728996 139578135525120 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6026260256767273, loss=1.1636756658554077
I0215 03:37:48.724212 139578127132416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5607994794845581, loss=1.1414248943328857
I0215 03:39:21.169611 139578135525120 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6209973096847534, loss=1.1588704586029053
I0215 03:40:52.365683 139578127132416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.5828610062599182, loss=1.1392083168029785
I0215 03:42:25.285515 139578135525120 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6401890516281128, loss=1.2177308797836304
I0215 03:43:54.441721 139578127132416 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.7170558571815491, loss=1.1498281955718994
I0215 03:45:23.700741 139578135525120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.6195194125175476, loss=1.2186105251312256
I0215 03:46:39.666784 139578127132416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7196252346038818, loss=1.1655797958374023
I0215 03:48:01.890643 139578135525120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.5548670291900635, loss=1.1652358770370483
I0215 03:49:23.926621 139578127132416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.7075186967849731, loss=1.1820237636566162
I0215 03:50:48.733518 139688679413568 spec.py:321] Evaluating on the training split.
I0215 03:51:45.589560 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 03:52:38.312679 139688679413568 spec.py:349] Evaluating on the test split.
I0215 03:53:05.574937 139688679413568 submission_runner.py:408] Time since start: 28531.93s, 	Step: 30300, 	{'train/ctc_loss': Array(0.15786675, dtype=float32), 'train/wer': 0.06232975061764892, 'validation/ctc_loss': Array(0.42404762, dtype=float32), 'validation/wer': 0.1281558647190013, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24171406, dtype=float32), 'test/wer': 0.08276968699855788, 'test/num_examples': 2472, 'score': 25962.238719701767, 'total_duration': 28531.932410240173, 'accumulated_submission_time': 25962.238719701767, 'accumulated_eval_time': 2567.4729528427124, 'accumulated_logging_time': 0.9269707202911377}
I0215 03:53:05.611521 139578135525120 logging_writer.py:48] [30300] accumulated_eval_time=2567.472953, accumulated_logging_time=0.926971, accumulated_submission_time=25962.238720, global_step=30300, preemption_count=0, score=25962.238720, test/ctc_loss=0.24171406030654907, test/num_examples=2472, test/wer=0.082770, total_duration=28531.932410, train/ctc_loss=0.15786674618721008, train/wer=0.062330, validation/ctc_loss=0.4240476191043854, validation/num_examples=5348, validation/wer=0.128156
I0215 03:53:06.566404 139578127132416 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.6430072784423828, loss=1.1711457967758179
I0215 03:54:21.919727 139578135525120 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.7698445916175842, loss=1.2142530679702759
I0215 03:55:37.539094 139578127132416 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6738613247871399, loss=1.1530532836914062
I0215 03:57:08.249180 139578135525120 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.8400936126708984, loss=1.185814380645752
I0215 03:58:36.306255 139578127132416 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.619213879108429, loss=1.173896074295044
I0215 04:00:06.185105 139578135525120 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.7690996527671814, loss=1.141725778579712
I0215 04:01:40.942674 139578135525120 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.5951845049858093, loss=1.162406086921692
I0215 04:02:58.374179 139578127132416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.5968161225318909, loss=1.1317343711853027
I0215 04:04:14.830836 139578135525120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6881332993507385, loss=1.160216212272644
I0215 04:05:35.419176 139578127132416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.5333943367004395, loss=1.1520001888275146
I0215 04:07:01.293050 139578135525120 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6338372230529785, loss=1.205475091934204
I0215 04:08:28.593102 139578127132416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6662600040435791, loss=1.1989389657974243
I0215 04:09:59.665895 139578135525120 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.5312982797622681, loss=1.2045865058898926
I0215 04:11:31.901871 139578127132416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6009536981582642, loss=1.1691182851791382
I0215 04:12:59.125795 139578135525120 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6853527426719666, loss=1.1137959957122803
I0215 04:14:28.497364 139578127132416 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.6444938778877258, loss=1.1459910869598389
I0215 04:15:58.049289 139578135525120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.5050572752952576, loss=1.1744349002838135
I0215 04:17:06.215985 139688679413568 spec.py:321] Evaluating on the training split.
I0215 04:18:02.509956 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 04:18:54.609349 139688679413568 spec.py:349] Evaluating on the test split.
I0215 04:19:21.890040 139688679413568 submission_runner.py:408] Time since start: 30108.25s, 	Step: 31982, 	{'train/ctc_loss': Array(0.1505317, dtype=float32), 'train/wer': 0.060234731827421936, 'validation/ctc_loss': Array(0.42554152, dtype=float32), 'validation/wer': 0.12888961835156454, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24244688, dtype=float32), 'test/wer': 0.08220096276887454, 'test/num_examples': 2472, 'score': 27402.670576810837, 'total_duration': 30108.24808359146, 'accumulated_submission_time': 27402.670576810837, 'accumulated_eval_time': 2703.1411702632904, 'accumulated_logging_time': 1.0707290172576904}
I0215 04:19:21.925317 139578135525120 logging_writer.py:48] [31982] accumulated_eval_time=2703.141170, accumulated_logging_time=1.070729, accumulated_submission_time=27402.670577, global_step=31982, preemption_count=0, score=27402.670577, test/ctc_loss=0.2424468845129013, test/num_examples=2472, test/wer=0.082201, total_duration=30108.248084, train/ctc_loss=0.15053169429302216, train/wer=0.060235, validation/ctc_loss=0.4255415201187134, validation/num_examples=5348, validation/wer=0.128890
I0215 04:19:36.343460 139578127132416 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6788888573646545, loss=1.1101220846176147
I0215 04:20:51.976896 139578135525120 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.5807746052742004, loss=1.137089729309082
I0215 04:22:07.602961 139578127132416 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6307080984115601, loss=1.1609776020050049
I0215 04:23:23.289936 139578135525120 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5897319316864014, loss=1.1875145435333252
I0215 04:24:46.501856 139578127132416 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.594366729259491, loss=1.1675169467926025
I0215 04:26:17.678633 139578135525120 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.5510779023170471, loss=1.1206177473068237
I0215 04:27:47.089208 139578127132416 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.7136930823326111, loss=1.166452407836914
I0215 04:29:16.629158 139578135525120 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.606336236000061, loss=1.138572096824646
I0215 04:30:46.802213 139578127132416 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.5724512338638306, loss=1.1510822772979736
I0215 04:32:13.208736 139578135525120 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.7431681752204895, loss=1.1283447742462158
I0215 04:33:37.909903 139578135525120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7004129886627197, loss=1.1121829748153687
I0215 04:34:53.635528 139578127132416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.5931646227836609, loss=1.110142707824707
I0215 04:36:11.086563 139578135525120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.5937588214874268, loss=1.078614592552185
I0215 04:37:32.786641 139578127132416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.6413900256156921, loss=1.1692721843719482
I0215 04:38:58.564530 139578135525120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.636543333530426, loss=1.1253122091293335
I0215 04:40:27.702149 139578127132416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.5560484528541565, loss=1.078818678855896
I0215 04:41:57.059876 139578135525120 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.5639838576316833, loss=1.163696527481079
I0215 04:43:22.603488 139688679413568 spec.py:321] Evaluating on the training split.
I0215 04:44:19.216452 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 04:45:13.007063 139688679413568 spec.py:349] Evaluating on the test split.
I0215 04:45:40.304673 139688679413568 submission_runner.py:408] Time since start: 31686.66s, 	Step: 33698, 	{'train/ctc_loss': Array(0.14140327, dtype=float32), 'train/wer': 0.05576077673158642, 'validation/ctc_loss': Array(0.4030115, dtype=float32), 'validation/wer': 0.12160035529123261, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23038326, dtype=float32), 'test/wer': 0.07793553104624946, 'test/num_examples': 2472, 'score': 28843.265655517578, 'total_duration': 31686.662852287292, 'accumulated_submission_time': 28843.265655517578, 'accumulated_eval_time': 2840.8364021778107, 'accumulated_logging_time': 1.121816635131836}
I0215 04:45:40.347185 139578135525120 logging_writer.py:48] [33698] accumulated_eval_time=2840.836402, accumulated_logging_time=1.121817, accumulated_submission_time=28843.265656, global_step=33698, preemption_count=0, score=28843.265656, test/ctc_loss=0.2303832620382309, test/num_examples=2472, test/wer=0.077936, total_duration=31686.662852, train/ctc_loss=0.14140327274799347, train/wer=0.055761, validation/ctc_loss=0.4030115008354187, validation/num_examples=5348, validation/wer=0.121600
I0215 04:45:42.748828 139578127132416 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.6602457761764526, loss=1.1573125123977661
I0215 04:46:58.257017 139578135525120 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.7124999165534973, loss=1.1605160236358643
I0215 04:48:14.498477 139578127132416 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5971714854240417, loss=1.1247259378433228
I0215 04:49:46.975475 139578135525120 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5991280674934387, loss=1.0999488830566406
I0215 04:51:07.669440 139578127132416 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.5751119256019592, loss=1.144650936126709
I0215 04:52:26.944807 139578135525120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6666621565818787, loss=1.1362208127975464
I0215 04:53:49.714750 139578127132416 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.680212676525116, loss=1.09652841091156
I0215 04:55:14.563503 139578135525120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.6013732552528381, loss=1.1532262563705444
I0215 04:56:43.749346 139578127132416 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.6545143127441406, loss=1.1562283039093018
I0215 04:58:14.531209 139578135525120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.700802743434906, loss=1.0820810794830322
I0215 04:59:44.767158 139578127132416 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.6610018610954285, loss=1.0818045139312744
I0215 05:01:12.663180 139578135525120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.5650474429130554, loss=1.1330957412719727
I0215 05:02:42.948787 139578127132416 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6328331828117371, loss=1.128449559211731
I0215 05:04:14.069097 139578135525120 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7915103435516357, loss=1.1579740047454834
I0215 05:05:35.556813 139578135525120 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.7365296483039856, loss=1.148877739906311
I0215 05:06:52.226537 139578127132416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6365403532981873, loss=1.141838788986206
I0215 05:08:12.181245 139578135525120 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6478200554847717, loss=1.108337640762329
I0215 05:09:39.304279 139578127132416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.5746976733207703, loss=1.0796842575073242
I0215 05:09:40.679611 139688679413568 spec.py:321] Evaluating on the training split.
I0215 05:10:35.664348 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 05:11:29.353467 139688679413568 spec.py:349] Evaluating on the test split.
I0215 05:11:56.346787 139688679413568 submission_runner.py:408] Time since start: 33262.70s, 	Step: 35403, 	{'train/ctc_loss': Array(0.12408025, dtype=float32), 'train/wer': 0.050719403598863295, 'validation/ctc_loss': Array(0.41467035, dtype=float32), 'validation/wer': 0.12207343329117468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22733217, dtype=float32), 'test/wer': 0.0772246257591453, 'test/num_examples': 2472, 'score': 30283.51073741913, 'total_duration': 33262.70490074158, 'accumulated_submission_time': 30283.51073741913, 'accumulated_eval_time': 2976.497559785843, 'accumulated_logging_time': 1.1860380172729492}
I0215 05:11:56.393527 139578135525120 logging_writer.py:48] [35403] accumulated_eval_time=2976.497560, accumulated_logging_time=1.186038, accumulated_submission_time=30283.510737, global_step=35403, preemption_count=0, score=30283.510737, test/ctc_loss=0.22733217477798462, test/num_examples=2472, test/wer=0.077225, total_duration=33262.704901, train/ctc_loss=0.12408024817705154, train/wer=0.050719, validation/ctc_loss=0.41467034816741943, validation/num_examples=5348, validation/wer=0.122073
I0215 05:13:10.404354 139578127132416 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.785946249961853, loss=1.1527704000473022
I0215 05:14:26.185437 139578135525120 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.6320079565048218, loss=1.1338225603103638
I0215 05:15:54.552157 139578127132416 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6039656400680542, loss=1.0990841388702393
I0215 05:17:22.667335 139578135525120 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.581258237361908, loss=1.1636388301849365
I0215 05:18:55.239936 139578127132416 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.606566309928894, loss=1.1125481128692627
I0215 05:20:25.435042 139578135525120 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7271032333374023, loss=1.146336555480957
I0215 05:21:50.772176 139578135525120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.7364336848258972, loss=1.1048202514648438
I0215 05:23:07.330539 139578127132416 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.6857126951217651, loss=1.111176609992981
I0215 05:24:25.331711 139578135525120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.5184253454208374, loss=1.0984396934509277
I0215 05:25:50.822607 139578127132416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.594627320766449, loss=1.1083606481552124
I0215 05:27:20.213303 139578135525120 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1351985931396484, loss=1.1033657789230347
I0215 05:28:52.078825 139578127132416 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.7868326306343079, loss=1.1017825603485107
I0215 05:30:25.360101 139578135525120 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.5824834108352661, loss=1.1197245121002197
I0215 05:31:55.587400 139578127132416 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.694487988948822, loss=1.0978552103042603
I0215 05:33:27.252819 139578135525120 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.6331713795661926, loss=1.1716914176940918
I0215 05:34:58.530529 139578127132416 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.613794207572937, loss=1.0499216318130493
I0215 05:35:56.372305 139688679413568 spec.py:321] Evaluating on the training split.
I0215 05:36:51.973098 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 05:37:45.691055 139688679413568 spec.py:349] Evaluating on the test split.
I0215 05:38:13.233636 139688679413568 submission_runner.py:408] Time since start: 34839.59s, 	Step: 37064, 	{'train/ctc_loss': Array(0.12518206, dtype=float32), 'train/wer': 0.04862967145843022, 'validation/ctc_loss': Array(0.3949641, dtype=float32), 'validation/wer': 0.11982389912818484, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22285151, dtype=float32), 'test/wer': 0.07529502569414824, 'test/num_examples': 2472, 'score': 31723.408026456833, 'total_duration': 34839.59212565422, 'accumulated_submission_time': 31723.408026456833, 'accumulated_eval_time': 3113.3532407283783, 'accumulated_logging_time': 1.2495365142822266}
I0215 05:38:13.273622 139578135525120 logging_writer.py:48] [37064] accumulated_eval_time=3113.353241, accumulated_logging_time=1.249537, accumulated_submission_time=31723.408026, global_step=37064, preemption_count=0, score=31723.408026, test/ctc_loss=0.22285151481628418, test/num_examples=2472, test/wer=0.075295, total_duration=34839.592126, train/ctc_loss=0.12518206238746643, train/wer=0.048630, validation/ctc_loss=0.3949640989303589, validation/num_examples=5348, validation/wer=0.119824
I0215 05:38:45.323370 139578135525120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.5813903212547302, loss=1.0629605054855347
I0215 05:40:03.663581 139578127132416 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.5651438236236572, loss=1.0620280504226685
I0215 05:41:25.893667 139578135525120 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.6918191909790039, loss=1.13530433177948
I0215 05:42:50.058523 139578127132416 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6551857590675354, loss=1.057071566581726
I0215 05:44:14.232383 139578135525120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.5710713863372803, loss=1.0861841440200806
I0215 05:45:42.139393 139578127132416 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6165958046913147, loss=1.111815333366394
I0215 05:47:13.678138 139578135525120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.6293094158172607, loss=1.0603525638580322
I0215 05:48:46.112716 139578127132416 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6488651633262634, loss=1.1305099725723267
I0215 05:50:20.154995 139578135525120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.5140079855918884, loss=1.0410856008529663
I0215 05:51:48.069169 139578127132416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.5370147824287415, loss=1.0404932498931885
I0215 05:53:20.512402 139578135525120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.6826493144035339, loss=1.0836085081100464
I0215 05:54:44.419368 139578135525120 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.6690672636032104, loss=1.0912914276123047
I0215 05:56:07.164781 139578127132416 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.7821987271308899, loss=1.1216819286346436
I0215 05:57:29.653180 139578135525120 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.7175939083099365, loss=1.152830958366394
I0215 05:58:56.285997 139578127132416 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.6246448159217834, loss=1.1469603776931763
I0215 06:00:24.862738 139578135525120 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.5282298922538757, loss=1.0888968706130981
I0215 06:01:54.069664 139578127132416 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6356921195983887, loss=1.0895024538040161
I0215 06:02:13.432593 139688679413568 spec.py:321] Evaluating on the training split.
I0215 06:03:07.859235 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 06:04:00.968976 139688679413568 spec.py:349] Evaluating on the test split.
I0215 06:04:28.718900 139688679413568 submission_runner.py:408] Time since start: 36415.08s, 	Step: 38724, 	{'train/ctc_loss': Array(0.1323574, dtype=float32), 'train/wer': 0.052375027050421984, 'validation/ctc_loss': Array(0.40408602, dtype=float32), 'validation/wer': 0.11951495023026347, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21985793, dtype=float32), 'test/wer': 0.07462474356630715, 'test/num_examples': 2472, 'score': 33163.48185658455, 'total_duration': 36415.07633471489, 'accumulated_submission_time': 33163.48185658455, 'accumulated_eval_time': 3248.6328341960907, 'accumulated_logging_time': 1.309312343597412}
I0215 06:04:28.760233 139578135525120 logging_writer.py:48] [38724] accumulated_eval_time=3248.632834, accumulated_logging_time=1.309312, accumulated_submission_time=33163.481857, global_step=38724, preemption_count=0, score=33163.481857, test/ctc_loss=0.21985793113708496, test/num_examples=2472, test/wer=0.074625, total_duration=36415.076335, train/ctc_loss=0.1323574036359787, train/wer=0.052375, validation/ctc_loss=0.40408602356910706, validation/num_examples=5348, validation/wer=0.119515
I0215 06:05:26.879486 139578127132416 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.6682677268981934, loss=1.043786644935608
I0215 06:06:42.545464 139578135525120 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.710317850112915, loss=1.1472162008285522
I0215 06:08:10.575226 139578127132416 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6160686016082764, loss=1.1273789405822754
I0215 06:09:39.695882 139578135525120 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6766332387924194, loss=1.0788835287094116
I0215 06:11:07.307318 139578135525120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6490772366523743, loss=1.0584813356399536
I0215 06:12:26.633369 139578127132416 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.5791553258895874, loss=1.0802446603775024
I0215 06:13:49.826681 139578135525120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.5917748212814331, loss=1.1185302734375
I0215 06:15:12.816560 139578127132416 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.6926408410072327, loss=1.1142619848251343
I0215 06:16:40.997317 139578135525120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.6412035226821899, loss=1.073649287223816
I0215 06:18:09.666510 139578127132416 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.650928795337677, loss=1.0307261943817139
I0215 06:19:39.784068 139578135525120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.6750566959381104, loss=1.1009268760681152
I0215 06:21:10.165551 139578127132416 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.6591740846633911, loss=1.0355852842330933
I0215 06:22:41.621566 139578135525120 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.6334090828895569, loss=1.088431477546692
I0215 06:24:10.998309 139578127132416 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.5577738881111145, loss=1.0684858560562134
I0215 06:25:42.872006 139578135525120 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7259225845336914, loss=1.0875790119171143
I0215 06:27:00.476097 139578127132416 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6383002400398254, loss=1.0874871015548706
I0215 06:28:21.113936 139578135525120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7429029941558838, loss=1.0925172567367554
I0215 06:28:29.149327 139688679413568 spec.py:321] Evaluating on the training split.
I0215 06:29:24.464617 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 06:30:17.138741 139688679413568 spec.py:349] Evaluating on the test split.
I0215 06:30:43.684386 139688679413568 submission_runner.py:408] Time since start: 37990.04s, 	Step: 40411, 	{'train/ctc_loss': Array(0.12066688, dtype=float32), 'train/wer': 0.047393106754334835, 'validation/ctc_loss': Array(0.39027813, dtype=float32), 'validation/wer': 0.11585583672050745, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21655618, dtype=float32), 'test/wer': 0.07176081083825889, 'test/num_examples': 2472, 'score': 34603.7865755558, 'total_duration': 37990.041987895966, 'accumulated_submission_time': 34603.7865755558, 'accumulated_eval_time': 3383.161405324936, 'accumulated_logging_time': 1.3682467937469482}
I0215 06:30:43.721245 139578135525120 logging_writer.py:48] [40411] accumulated_eval_time=3383.161405, accumulated_logging_time=1.368247, accumulated_submission_time=34603.786576, global_step=40411, preemption_count=0, score=34603.786576, test/ctc_loss=0.21655617654323578, test/num_examples=2472, test/wer=0.071761, total_duration=37990.041988, train/ctc_loss=0.12066688388586044, train/wer=0.047393, validation/ctc_loss=0.3902781307697296, validation/num_examples=5348, validation/wer=0.115856
I0215 06:31:51.569242 139578127132416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7950483560562134, loss=1.044649362564087
I0215 06:33:07.213081 139578135525120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.7298006415367126, loss=1.0433719158172607
I0215 06:34:32.497634 139578127132416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.6805598139762878, loss=1.0492608547210693
I0215 06:36:02.410833 139578135525120 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7726152539253235, loss=1.1323151588439941
I0215 06:37:31.939076 139578127132416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.6667532324790955, loss=1.1445794105529785
I0215 06:39:04.476175 139578135525120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.741923987865448, loss=1.0532101392745972
I0215 06:40:35.146074 139578127132416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.6070539355278015, loss=1.091556191444397
I0215 06:42:06.856671 139578135525120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.713436484336853, loss=1.0436121225357056
I0215 06:43:23.945269 139578127132416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6669554710388184, loss=1.0457266569137573
I0215 06:44:44.330705 139578135525120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.6717340350151062, loss=1.0447590351104736
I0215 06:46:05.406690 139578127132416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6275190114974976, loss=1.0379146337509155
I0215 06:47:28.936267 139578135525120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.7561914920806885, loss=1.0628759860992432
I0215 06:48:58.881222 139578127132416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7673497200012207, loss=1.0706919431686401
I0215 06:50:28.024884 139578135525120 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.7217857837677002, loss=1.048263669013977
I0215 06:51:56.328530 139578127132416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6290651559829712, loss=1.0660210847854614
I0215 06:53:25.285711 139578135525120 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.5998377203941345, loss=1.014664649963379
I0215 06:54:43.703616 139688679413568 spec.py:321] Evaluating on the training split.
I0215 06:55:40.678162 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 06:56:34.874577 139688679413568 spec.py:349] Evaluating on the test split.
I0215 06:57:01.598492 139688679413568 submission_runner.py:408] Time since start: 39567.96s, 	Step: 42087, 	{'train/ctc_loss': Array(0.11153517, dtype=float32), 'train/wer': 0.045562737252487946, 'validation/ctc_loss': Array(0.3821042, dtype=float32), 'validation/wer': 0.11345182810855692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21440364, dtype=float32), 'test/wer': 0.07194361505494282, 'test/num_examples': 2472, 'score': 36043.68631386757, 'total_duration': 39567.956351041794, 'accumulated_submission_time': 36043.68631386757, 'accumulated_eval_time': 3521.0500218868256, 'accumulated_logging_time': 1.421659231185913}
I0215 06:57:01.637651 139578135525120 logging_writer.py:48] [42087] accumulated_eval_time=3521.050022, accumulated_logging_time=1.421659, accumulated_submission_time=36043.686314, global_step=42087, preemption_count=0, score=36043.686314, test/ctc_loss=0.2144036442041397, test/num_examples=2472, test/wer=0.071944, total_duration=39567.956351, train/ctc_loss=0.11153516918420792, train/wer=0.045563, validation/ctc_loss=0.38210418820381165, validation/num_examples=5348, validation/wer=0.113452
I0215 06:57:12.276781 139578127132416 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.6407231092453003, loss=1.0456844568252563
I0215 06:58:27.858677 139578135525120 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.7349058389663696, loss=1.0153064727783203
I0215 06:59:48.644845 139578135525120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7289645671844482, loss=1.0785433053970337
I0215 07:01:08.054668 139578127132416 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.627289891242981, loss=1.0632203817367554
I0215 07:02:27.353170 139578135525120 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.6870951652526855, loss=1.0735795497894287
I0215 07:03:54.311671 139578127132416 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.6284947395324707, loss=1.0438867807388306
I0215 07:05:23.056085 139578135525120 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.6100645661354065, loss=1.0503613948822021
I0215 07:06:51.340500 139578127132416 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.6533433794975281, loss=1.061289668083191
I0215 07:08:23.854157 139578135525120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.5437226295471191, loss=1.0486267805099487
I0215 07:09:53.316034 139578127132416 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6646525859832764, loss=1.0403711795806885
I0215 07:11:25.899781 139578135525120 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.620617151260376, loss=1.0464757680892944
I0215 07:12:56.434288 139578127132416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.7575076222419739, loss=1.0793697834014893
I0215 07:14:25.925226 139578135525120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7214589715003967, loss=1.1070010662078857
I0215 07:15:43.570263 139578127132416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.6002256274223328, loss=0.9966881275177002
I0215 07:17:03.737890 139578135525120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.59598308801651, loss=1.055597186088562
I0215 07:18:26.069296 139578127132416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.6490992903709412, loss=1.0541646480560303
I0215 07:19:54.386451 139578135525120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7545150518417358, loss=1.0445688962936401
I0215 07:21:01.918382 139688679413568 spec.py:321] Evaluating on the training split.
I0215 07:21:58.946325 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 07:22:52.026595 139688679413568 spec.py:349] Evaluating on the test split.
I0215 07:23:19.869118 139688679413568 submission_runner.py:408] Time since start: 41146.23s, 	Step: 43777, 	{'train/ctc_loss': Array(0.1301486, dtype=float32), 'train/wer': 0.048602912239275875, 'validation/ctc_loss': Array(0.38293865, dtype=float32), 'validation/wer': 0.11291116753719455, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20935404, dtype=float32), 'test/wer': 0.06942497918063088, 'test/num_examples': 2472, 'score': 37483.879881858826, 'total_duration': 41146.22732448578, 'accumulated_submission_time': 37483.879881858826, 'accumulated_eval_time': 3658.9948778152466, 'accumulated_logging_time': 1.4797418117523193}
I0215 07:23:19.905958 139578135525120 logging_writer.py:48] [43777] accumulated_eval_time=3658.994878, accumulated_logging_time=1.479742, accumulated_submission_time=37483.879882, global_step=43777, preemption_count=0, score=37483.879882, test/ctc_loss=0.20935404300689697, test/num_examples=2472, test/wer=0.069425, total_duration=41146.227324, train/ctc_loss=0.13014860451221466, train/wer=0.048603, validation/ctc_loss=0.3829386532306671, validation/num_examples=5348, validation/wer=0.112911
I0215 07:23:38.120319 139578127132416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6376417875289917, loss=0.995769739151001
I0215 07:24:53.758677 139578135525120 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.722445547580719, loss=1.0587118864059448
I0215 07:26:11.174469 139578127132416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.5662587285041809, loss=0.999182939529419
I0215 07:27:40.119318 139578135525120 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.6848756670951843, loss=1.0792264938354492
I0215 07:29:12.739946 139578127132416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.6438820362091064, loss=1.0661468505859375
I0215 07:30:46.013709 139578135525120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.6435624361038208, loss=1.0512807369232178
I0215 07:32:02.818999 139578127132416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6506553292274475, loss=1.0098282098770142
I0215 07:33:22.134015 139578135525120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.6875199675559998, loss=1.0740958452224731
I0215 07:34:44.120452 139578127132416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.815467119216919, loss=1.059678316116333
I0215 07:36:08.713587 139578135525120 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.6355206370353699, loss=1.003291368484497
I0215 07:37:37.101583 139578127132416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.798312246799469, loss=0.9927091002464294
I0215 07:39:07.779274 139578135525120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6209726929664612, loss=1.0599641799926758
I0215 07:40:39.141309 139578127132416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6676061749458313, loss=1.0696779489517212
I0215 07:42:09.995136 139578135525120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.7098215222358704, loss=1.082171082496643
I0215 07:43:42.867132 139578127132416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.8966735005378723, loss=1.0390625
I0215 07:45:13.377067 139578135525120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7100824117660522, loss=0.9932570457458496
I0215 07:46:36.724720 139578135525120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.6022477149963379, loss=1.0062445402145386
I0215 07:47:19.929821 139688679413568 spec.py:321] Evaluating on the training split.
I0215 07:48:16.626759 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 07:49:10.954643 139688679413568 spec.py:349] Evaluating on the test split.
I0215 07:49:39.004189 139688679413568 submission_runner.py:408] Time since start: 42725.36s, 	Step: 45457, 	{'train/ctc_loss': Array(0.08889699, dtype=float32), 'train/wer': 0.035825625038456016, 'validation/ctc_loss': Array(0.37530112, dtype=float32), 'validation/wer': 0.1112312579047472, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2068121, dtype=float32), 'test/wer': 0.06912030548615766, 'test/num_examples': 2472, 'score': 38923.82216382027, 'total_duration': 42725.361545324326, 'accumulated_submission_time': 38923.82216382027, 'accumulated_eval_time': 3798.0624623298645, 'accumulated_logging_time': 1.5324275493621826}
I0215 07:49:39.044052 139578135525120 logging_writer.py:48] [45457] accumulated_eval_time=3798.062462, accumulated_logging_time=1.532428, accumulated_submission_time=38923.822164, global_step=45457, preemption_count=0, score=38923.822164, test/ctc_loss=0.20681209862232208, test/num_examples=2472, test/wer=0.069120, total_duration=42725.361545, train/ctc_loss=0.0888969898223877, train/wer=0.035826, validation/ctc_loss=0.3753011226654053, validation/num_examples=5348, validation/wer=0.111231
I0215 07:50:12.329713 139578127132416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.5447292327880859, loss=0.9966871738433838
I0215 07:51:27.943385 139578135525120 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.7992403507232666, loss=1.0320404767990112
I0215 07:52:43.727419 139578127132416 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.8170862197875977, loss=1.0777169466018677
I0215 07:54:08.620189 139578135525120 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.6032272577285767, loss=0.9993348717689514
I0215 07:55:41.686363 139578127132416 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.6015058755874634, loss=1.0350861549377441
I0215 07:57:09.463812 139578135525120 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6867328882217407, loss=1.0385059118270874
I0215 07:58:41.367815 139578127132416 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.9081663489341736, loss=1.0662873983383179
I0215 08:00:10.850873 139578135525120 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.8658245801925659, loss=1.0166101455688477
I0215 08:01:40.738976 139578127132416 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.6278811097145081, loss=1.0378773212432861
I0215 08:03:08.138167 139578135525120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7483038306236267, loss=1.026769757270813
I0215 08:04:27.066346 139578127132416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6366395354270935, loss=1.009135127067566
I0215 08:05:45.069105 139578135525120 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.6090489029884338, loss=1.0198326110839844
I0215 08:07:06.216918 139578127132416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7174383997917175, loss=0.9755658507347107
I0215 08:08:33.684517 139578135525120 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.8158925175666809, loss=1.0501856803894043
I0215 08:10:04.939136 139578127132416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.6732949018478394, loss=1.0500121116638184
I0215 08:11:33.806471 139578135525120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.6730135083198547, loss=1.0427851676940918
I0215 08:13:04.604854 139578127132416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.617843508720398, loss=0.9933188557624817
I0215 08:13:39.876318 139688679413568 spec.py:321] Evaluating on the training split.
I0215 08:14:36.351464 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 08:15:31.070649 139688679413568 spec.py:349] Evaluating on the test split.
I0215 08:15:57.889647 139688679413568 submission_runner.py:408] Time since start: 44304.25s, 	Step: 47141, 	{'train/ctc_loss': Array(0.09471288, dtype=float32), 'train/wer': 0.038131185077919565, 'validation/ctc_loss': Array(0.3694132, dtype=float32), 'validation/wer': 0.10856657366017552, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19967549, dtype=float32), 'test/wer': 0.06662198119147726, 'test/num_examples': 2472, 'score': 40364.569650650024, 'total_duration': 44304.246056079865, 'accumulated_submission_time': 40364.569650650024, 'accumulated_eval_time': 3936.06809425354, 'accumulated_logging_time': 1.5900306701660156}
I0215 08:15:57.936221 139578135525120 logging_writer.py:48] [47141] accumulated_eval_time=3936.068094, accumulated_logging_time=1.590031, accumulated_submission_time=40364.569651, global_step=47141, preemption_count=0, score=40364.569651, test/ctc_loss=0.19967548549175262, test/num_examples=2472, test/wer=0.066622, total_duration=44304.246056, train/ctc_loss=0.09471287578344345, train/wer=0.038131, validation/ctc_loss=0.36941319704055786, validation/num_examples=5348, validation/wer=0.108567
I0215 08:16:43.448537 139578127132416 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.6054558157920837, loss=1.053898572921753
I0215 08:17:59.304892 139578135525120 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7049424648284912, loss=1.0466502904891968
I0215 08:19:21.325296 139578135525120 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.6733564138412476, loss=1.0389715433120728
I0215 08:20:40.597798 139578127132416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.7354976534843445, loss=1.0425922870635986
I0215 08:21:59.216222 139578135525120 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.8064565658569336, loss=1.0164823532104492
I0215 08:23:24.540523 139578127132416 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.5659600496292114, loss=0.9725338220596313
I0215 08:24:52.180299 139578135525120 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.6558815836906433, loss=1.00364089012146
I0215 08:26:24.404648 139578127132416 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.5414569973945618, loss=1.0416555404663086
I0215 08:27:55.474108 139578135525120 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7197610139846802, loss=1.0386666059494019
I0215 08:29:28.010663 139578127132416 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.7605031728744507, loss=1.0033146142959595
I0215 08:30:59.308393 139578135525120 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.8950533270835876, loss=1.034024953842163
I0215 08:32:31.016918 139578127132416 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.7789250016212463, loss=1.0472793579101562
I0215 08:34:01.597698 139578135525120 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.8620862364768982, loss=1.082083821296692
I0215 08:35:22.444365 139578135525120 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.693517804145813, loss=1.0237438678741455
I0215 08:36:40.442242 139578127132416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.7619802355766296, loss=1.0209875106811523
I0215 08:38:03.998205 139578135525120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.6215068697929382, loss=0.9827876091003418
I0215 08:39:29.407764 139578127132416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.597734272480011, loss=0.9599540829658508
I0215 08:39:58.005613 139688679413568 spec.py:321] Evaluating on the training split.
I0215 08:40:52.207252 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 08:41:45.750002 139688679413568 spec.py:349] Evaluating on the test split.
I0215 08:42:13.636113 139688679413568 submission_runner.py:408] Time since start: 45879.99s, 	Step: 48834, 	{'train/ctc_loss': Array(0.11606108, dtype=float32), 'train/wer': 0.04643084837426078, 'validation/ctc_loss': Array(0.36678696, dtype=float32), 'validation/wer': 0.108219006150014, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1959807, dtype=float32), 'test/wer': 0.0662563727581094, 'test/num_examples': 2472, 'score': 41804.55584001541, 'total_duration': 45879.99431490898, 'accumulated_submission_time': 41804.55584001541, 'accumulated_eval_time': 4071.6926686763763, 'accumulated_logging_time': 1.653782606124878}
I0215 08:42:13.676930 139578135525120 logging_writer.py:48] [48834] accumulated_eval_time=4071.692669, accumulated_logging_time=1.653783, accumulated_submission_time=41804.555840, global_step=48834, preemption_count=0, score=41804.555840, test/ctc_loss=0.19598069787025452, test/num_examples=2472, test/wer=0.066256, total_duration=45879.994315, train/ctc_loss=0.11606107652187347, train/wer=0.046431, validation/ctc_loss=0.3667869567871094, validation/num_examples=5348, validation/wer=0.108219
I0215 08:43:04.443275 139578127132416 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.8183634281158447, loss=0.9932974576950073
I0215 08:44:20.042366 139578135525120 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.721474289894104, loss=1.0304362773895264
I0215 08:45:45.559118 139578127132416 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.6416348814964294, loss=1.0168700218200684
I0215 08:47:16.508336 139578135525120 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.7863322496414185, loss=0.9951929450035095
I0215 08:48:49.641868 139578127132416 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.6811480522155762, loss=1.0220649242401123
I0215 08:50:19.145283 139578135525120 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.7980729341506958, loss=0.9648690819740295
I0215 08:51:44.197924 139578135525120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.6881938576698303, loss=0.9630740880966187
I0215 08:53:03.008310 139578127132416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6325176954269409, loss=1.01833176612854
I0215 08:54:24.496212 139578135525120 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.6538072824478149, loss=0.9676337838172913
I0215 08:55:48.823207 139578127132416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7296775579452515, loss=0.9373605847358704
I0215 08:57:18.181327 139578135525120 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.718367874622345, loss=0.9779554605484009
I0215 08:58:48.649139 139578127132416 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.6930508017539978, loss=0.9894663691520691
I0215 09:00:17.052353 139578135525120 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.604033350944519, loss=0.9705697298049927
I0215 09:01:48.623307 139578127132416 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.7293980121612549, loss=1.001286506652832
I0215 09:03:20.524459 139578135525120 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.7122131586074829, loss=1.0356457233428955
I0215 09:04:53.950624 139578127132416 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.5893536806106567, loss=1.0397356748580933
I0215 09:06:13.685247 139688679413568 spec.py:321] Evaluating on the training split.
I0215 09:07:08.386751 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 09:08:02.731359 139688679413568 spec.py:349] Evaluating on the test split.
I0215 09:08:29.302911 139688679413568 submission_runner.py:408] Time since start: 47455.66s, 	Step: 50487, 	{'train/ctc_loss': Array(0.11821188, dtype=float32), 'train/wer': 0.046224055806009794, 'validation/ctc_loss': Array(0.3563446, dtype=float32), 'validation/wer': 0.10460816590555819, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19380894, dtype=float32), 'test/wer': 0.06410334531716531, 'test/num_examples': 2472, 'score': 43244.480036735535, 'total_duration': 47455.66095995903, 'accumulated_submission_time': 43244.480036735535, 'accumulated_eval_time': 4207.30445432663, 'accumulated_logging_time': 1.7124838829040527}
I0215 09:08:29.353988 139578135525120 logging_writer.py:48] [50487] accumulated_eval_time=4207.304454, accumulated_logging_time=1.712484, accumulated_submission_time=43244.480037, global_step=50487, preemption_count=0, score=43244.480037, test/ctc_loss=0.19380894303321838, test/num_examples=2472, test/wer=0.064103, total_duration=47455.660960, train/ctc_loss=0.11821188032627106, train/wer=0.046224, validation/ctc_loss=0.356344610452652, validation/num_examples=5348, validation/wer=0.104608
I0215 09:08:40.052121 139578127132416 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.7476564049720764, loss=0.9363188147544861
I0215 09:09:55.786995 139578135525120 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.7459162473678589, loss=0.9781538844108582
I0215 09:11:11.471338 139578127132416 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.754440426826477, loss=0.9209945797920227
I0215 09:12:27.249197 139578135525120 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.7352887988090515, loss=0.9777348041534424
I0215 09:13:52.301649 139578127132416 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.6379101872444153, loss=0.9909735918045044
I0215 09:15:21.732094 139578135525120 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.6661021113395691, loss=0.9881592988967896
I0215 09:16:51.185698 139578127132416 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.8383240103721619, loss=0.9606060981750488
I0215 09:18:20.364945 139578135525120 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.7141586542129517, loss=1.0373802185058594
I0215 09:19:52.073455 139578127132416 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.6354700922966003, loss=1.0076814889907837
I0215 09:21:22.213198 139578135525120 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.6545498371124268, loss=0.9676456451416016
I0215 09:22:57.974800 139578135525120 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7416014075279236, loss=0.9855219721794128
I0215 09:24:15.134770 139578127132416 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.6811652779579163, loss=0.9795205593109131
I0215 09:25:34.685091 139578135525120 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.8376348614692688, loss=0.9632428884506226
I0215 09:26:55.734509 139578127132416 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.7728974223136902, loss=0.9724922180175781
I0215 09:28:19.816927 139578135525120 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.6956565976142883, loss=0.9996642470359802
I0215 09:29:49.602380 139578127132416 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2794990539550781, loss=0.9800045490264893
I0215 09:31:22.186468 139578135525120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.6564361453056335, loss=0.9806849956512451
I0215 09:32:30.141043 139688679413568 spec.py:321] Evaluating on the training split.
I0215 09:33:23.487799 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 09:34:17.819707 139688679413568 spec.py:349] Evaluating on the test split.
I0215 09:34:44.169902 139688679413568 submission_runner.py:408] Time since start: 49030.53s, 	Step: 52173, 	{'train/ctc_loss': Array(0.13081229, dtype=float32), 'train/wer': 0.05299215704473733, 'validation/ctc_loss': Array(0.36061352, dtype=float32), 'validation/wer': 0.10450196472189772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19001366, dtype=float32), 'test/wer': 0.06310807791521947, 'test/num_examples': 2472, 'score': 44685.18374609947, 'total_duration': 49030.528040885925, 'accumulated_submission_time': 44685.18374609947, 'accumulated_eval_time': 4341.327328681946, 'accumulated_logging_time': 1.7806909084320068}
I0215 09:34:44.209895 139578135525120 logging_writer.py:48] [52173] accumulated_eval_time=4341.327329, accumulated_logging_time=1.780691, accumulated_submission_time=44685.183746, global_step=52173, preemption_count=0, score=44685.183746, test/ctc_loss=0.19001366198062897, test/num_examples=2472, test/wer=0.063108, total_duration=49030.528041, train/ctc_loss=0.13081228733062744, train/wer=0.052992, validation/ctc_loss=0.36061352491378784, validation/num_examples=5348, validation/wer=0.104502
I0215 09:35:05.446035 139578127132416 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.6682368516921997, loss=0.9493951201438904
I0215 09:36:21.136028 139578135525120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.6425284147262573, loss=0.9536767601966858
I0215 09:37:38.910997 139578127132416 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.7617360949516296, loss=1.0203710794448853
I0215 09:39:09.791210 139578135525120 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.8295062780380249, loss=0.989619791507721
I0215 09:40:35.025187 139578135525120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.7517372965812683, loss=0.9875119924545288
I0215 09:41:53.264204 139578127132416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.6115942597389221, loss=0.9807003140449524
I0215 09:43:15.698464 139578135525120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.6037704944610596, loss=0.9599428176879883
I0215 09:44:41.740495 139578127132416 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.7397674918174744, loss=0.9698803424835205
I0215 09:46:08.603166 139578135525120 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.9216055274009705, loss=0.983720064163208
I0215 09:47:39.252187 139578127132416 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.755603015422821, loss=0.9800207018852234
I0215 09:49:10.393337 139578135525120 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.8693521618843079, loss=0.993535578250885
I0215 09:50:43.144323 139578127132416 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.813329815864563, loss=0.9963070154190063
I0215 09:52:12.796620 139578135525120 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.8805615305900574, loss=0.9999401569366455
I0215 09:53:42.059547 139578127132416 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.7731950283050537, loss=0.9457429647445679
I0215 09:55:10.066834 139578135525120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.6497890949249268, loss=0.9740089178085327
I0215 09:56:26.269967 139578127132416 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.7303997278213501, loss=0.9655663967132568
I0215 09:57:42.313852 139578135525120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.6993269324302673, loss=0.9511757493019104
I0215 09:58:44.506287 139688679413568 spec.py:321] Evaluating on the training split.
I0215 09:59:37.909508 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 10:00:31.405698 139688679413568 spec.py:349] Evaluating on the test split.
I0215 10:00:59.881975 139688679413568 submission_runner.py:408] Time since start: 50606.24s, 	Step: 53875, 	{'train/ctc_loss': Array(0.11086977, dtype=float32), 'train/wer': 0.04350275853973471, 'validation/ctc_loss': Array(0.35066342, dtype=float32), 'validation/wer': 0.10255172480376917, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19238628, dtype=float32), 'test/wer': 0.06377836004306055, 'test/num_examples': 2472, 'score': 46125.39481854439, 'total_duration': 50606.23990535736, 'accumulated_submission_time': 46125.39481854439, 'accumulated_eval_time': 4476.6968767642975, 'accumulated_logging_time': 1.838486671447754}
I0215 10:00:59.924970 139578135525120 logging_writer.py:48] [53875] accumulated_eval_time=4476.696877, accumulated_logging_time=1.838487, accumulated_submission_time=46125.394819, global_step=53875, preemption_count=0, score=46125.394819, test/ctc_loss=0.19238628447055817, test/num_examples=2472, test/wer=0.063778, total_duration=50606.239905, train/ctc_loss=0.11086976528167725, train/wer=0.043503, validation/ctc_loss=0.350663423538208, validation/num_examples=5348, validation/wer=0.102552
I0215 10:01:19.607964 139578127132416 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.7217170596122742, loss=0.9474174976348877
I0215 10:02:35.397699 139578135525120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.6333240270614624, loss=0.9824896454811096
I0215 10:03:53.579796 139578127132416 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.7775148153305054, loss=0.9792699813842773
I0215 10:05:24.787542 139578135525120 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.7132552862167358, loss=0.9584813714027405
I0215 10:06:56.062623 139578127132416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9007946252822876, loss=1.0183370113372803
I0215 10:08:28.872596 139578135525120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.7129656672477722, loss=0.9710957407951355
I0215 10:10:02.423440 139578127132416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.6068695187568665, loss=0.9384914040565491
I0215 10:11:35.945703 139578135525120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.7914195656776428, loss=0.9454895853996277
I0215 10:12:54.365473 139578127132416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.940040647983551, loss=0.9902899265289307
I0215 10:14:13.543971 139578135525120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.6208703517913818, loss=0.9899210929870605
I0215 10:15:32.291330 139578127132416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.6816768050193787, loss=0.9333579540252686
I0215 10:16:57.679495 139578135525120 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.7222943305969238, loss=0.9929820895195007
I0215 10:18:27.019966 139578127132416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.665857195854187, loss=0.8860259056091309
I0215 10:19:57.467306 139578135525120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.7639645338058472, loss=0.952767014503479
I0215 10:21:27.857484 139578127132416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.8385117650032043, loss=0.9933724999427795
I0215 10:22:58.813150 139578135525120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.6996110081672668, loss=0.9701240062713623
I0215 10:24:29.843168 139578127132416 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.634472668170929, loss=0.9627305865287781
I0215 10:25:00.484997 139688679413568 spec.py:321] Evaluating on the training split.
I0215 10:25:55.616579 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 10:26:48.971903 139688679413568 spec.py:349] Evaluating on the test split.
I0215 10:27:16.107171 139688679413568 submission_runner.py:408] Time since start: 52182.47s, 	Step: 55536, 	{'train/ctc_loss': Array(0.09881861, dtype=float32), 'train/wer': 0.03991368798431099, 'validation/ctc_loss': Array(0.3454917, dtype=float32), 'validation/wer': 0.09919190553887446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18675168, dtype=float32), 'test/wer': 0.06197062945585278, 'test/num_examples': 2472, 'score': 47565.87292122841, 'total_duration': 52182.46559405327, 'accumulated_submission_time': 47565.87292122841, 'accumulated_eval_time': 4612.31334400177, 'accumulated_logging_time': 1.8981482982635498}
I0215 10:27:16.148731 139578135525120 logging_writer.py:48] [55536] accumulated_eval_time=4612.313344, accumulated_logging_time=1.898148, accumulated_submission_time=47565.872921, global_step=55536, preemption_count=0, score=47565.872921, test/ctc_loss=0.18675167858600616, test/num_examples=2472, test/wer=0.061971, total_duration=52182.465594, train/ctc_loss=0.09881860762834549, train/wer=0.039914, validation/ctc_loss=0.3454917073249817, validation/num_examples=5348, validation/wer=0.099192
I0215 10:28:05.508651 139578127132416 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.0046268701553345, loss=0.9779560565948486
I0215 10:29:26.741523 139578135525120 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.6909928917884827, loss=0.9775884747505188
I0215 10:30:46.488193 139578127132416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.6670385003089905, loss=0.920964241027832
I0215 10:32:09.019322 139578135525120 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.7046080231666565, loss=0.9363802671432495
I0215 10:33:31.278603 139578127132416 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.7149686813354492, loss=0.9486241936683655
I0215 10:35:02.268843 139578135525120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.6599321365356445, loss=0.9444015622138977
I0215 10:36:31.887181 139578127132416 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.6900584101676941, loss=0.9305509328842163
I0215 10:38:04.829587 139578135525120 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.7952601909637451, loss=0.9700925946235657
I0215 10:39:36.097835 139578127132416 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.7667566537857056, loss=0.9422613978385925
I0215 10:41:05.971676 139578135525120 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9090455770492554, loss=0.9568485617637634
I0215 10:42:37.589022 139578127132416 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.7017541527748108, loss=0.9453018307685852
I0215 10:44:06.753423 139578135525120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.6682965159416199, loss=0.9573565721511841
I0215 10:45:23.669775 139578127132416 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.768195629119873, loss=0.9643039703369141
I0215 10:46:42.741593 139578135525120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7348693013191223, loss=0.9248136878013611
I0215 10:48:05.987404 139578127132416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.6671009659767151, loss=0.9757102131843567
I0215 10:49:35.483946 139578135525120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.7943797707557678, loss=0.9858842492103577
I0215 10:51:05.646017 139578127132416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.8176937103271484, loss=0.8877381086349487
I0215 10:51:16.168877 139688679413568 spec.py:321] Evaluating on the training split.
I0215 10:52:11.041115 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 10:53:03.996067 139688679413568 spec.py:349] Evaluating on the test split.
I0215 10:53:31.058071 139688679413568 submission_runner.py:408] Time since start: 53757.42s, 	Step: 57214, 	{'train/ctc_loss': Array(0.0818679, dtype=float32), 'train/wer': 0.03281551613199742, 'validation/ctc_loss': Array(0.34621996, dtype=float32), 'validation/wer': 0.09928845206947488, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18420044, dtype=float32), 'test/wer': 0.05939105884264619, 'test/num_examples': 2472, 'score': 49005.80888533592, 'total_duration': 53757.41523528099, 'accumulated_submission_time': 49005.80888533592, 'accumulated_eval_time': 4747.195596456528, 'accumulated_logging_time': 1.9564628601074219}
I0215 10:53:31.099154 139578135525120 logging_writer.py:48] [57214] accumulated_eval_time=4747.195596, accumulated_logging_time=1.956463, accumulated_submission_time=49005.808885, global_step=57214, preemption_count=0, score=49005.808885, test/ctc_loss=0.1842004358768463, test/num_examples=2472, test/wer=0.059391, total_duration=53757.415235, train/ctc_loss=0.08186789602041245, train/wer=0.032816, validation/ctc_loss=0.3462199568748474, validation/num_examples=5348, validation/wer=0.099288
I0215 10:54:36.860806 139578127132416 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.7861484885215759, loss=0.8738649487495422
I0215 10:55:52.831975 139578135525120 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.7272005677223206, loss=0.9331891536712646
I0215 10:57:17.887104 139578127132416 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.650386393070221, loss=0.957471489906311
I0215 10:58:47.574850 139578135525120 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.7541135549545288, loss=0.9082942605018616
I0215 11:00:19.500651 139578135525120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.7070205807685852, loss=0.9010050892829895
I0215 11:01:38.769438 139578127132416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.6999940872192383, loss=0.9217479825019836
I0215 11:02:55.213591 139578135525120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.6189796924591064, loss=0.8925542831420898
I0215 11:04:15.325461 139578127132416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.7257382869720459, loss=0.9449678659439087
I0215 11:05:42.703788 139578135525120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.6810611486434937, loss=0.9407749176025391
I0215 11:07:11.803836 139578127132416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.7332378625869751, loss=0.9235056042671204
I0215 11:08:43.204907 139578135525120 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.8955260515213013, loss=0.9241793155670166
I0215 11:10:14.163367 139578127132416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.6978650689125061, loss=0.86998051404953
I0215 11:11:45.217732 139578135525120 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.9152265191078186, loss=0.9368666410446167
I0215 11:13:15.043033 139578127132416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.667131245136261, loss=0.8903672099113464
I0215 11:14:46.590879 139578135525120 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.6808034181594849, loss=0.9116479754447937
I0215 11:16:10.314154 139578135525120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7736684083938599, loss=0.9457732439041138
I0215 11:17:26.653238 139578127132416 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.835389256477356, loss=0.9485173225402832
I0215 11:17:31.686808 139688679413568 spec.py:321] Evaluating on the training split.
I0215 11:18:26.654583 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 11:19:21.323050 139688679413568 spec.py:349] Evaluating on the test split.
I0215 11:19:48.135277 139688679413568 submission_runner.py:408] Time since start: 55334.49s, 	Step: 58908, 	{'train/ctc_loss': Array(0.09207707, dtype=float32), 'train/wer': 0.038660805828843174, 'validation/ctc_loss': Array(0.34705952, dtype=float32), 'validation/wer': 0.099771184722477, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18286462, dtype=float32), 'test/wer': 0.05979729043527715, 'test/num_examples': 2472, 'score': 50446.31366991997, 'total_duration': 55334.49369978905, 'accumulated_submission_time': 50446.31366991997, 'accumulated_eval_time': 4883.638375282288, 'accumulated_logging_time': 2.0142009258270264}
I0215 11:19:48.179354 139578135525120 logging_writer.py:48] [58908] accumulated_eval_time=4883.638375, accumulated_logging_time=2.014201, accumulated_submission_time=50446.313670, global_step=58908, preemption_count=0, score=50446.313670, test/ctc_loss=0.18286462128162384, test/num_examples=2472, test/wer=0.059797, total_duration=55334.493700, train/ctc_loss=0.09207706898450851, train/wer=0.038661, validation/ctc_loss=0.3470595180988312, validation/num_examples=5348, validation/wer=0.099771
I0215 11:20:58.546726 139578127132416 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.8205705285072327, loss=0.8727582097053528
I0215 11:22:14.546406 139578135525120 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.8644071817398071, loss=0.9278949499130249
I0215 11:23:34.012978 139578127132416 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.9292821884155273, loss=0.9780957102775574
I0215 11:25:02.625564 139578135525120 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.8340237140655518, loss=0.8959819674491882
I0215 11:26:34.921335 139578127132416 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9042469263076782, loss=0.9070006012916565
I0215 11:28:06.342051 139578135525120 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.7971576452255249, loss=0.9440253973007202
I0215 11:29:37.938343 139578127132416 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.6741278767585754, loss=0.9144948124885559
I0215 11:31:10.634383 139578135525120 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.7633272409439087, loss=0.948103129863739
I0215 11:32:36.804733 139578135525120 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.0503604412078857, loss=0.8812831044197083
I0215 11:33:54.417931 139578127132416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.8287468552589417, loss=0.9236117601394653
I0215 11:35:13.789966 139578135525120 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.8547419905662537, loss=0.9239526391029358
I0215 11:36:38.943909 139578127132416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.7316386699676514, loss=0.9070055484771729
I0215 11:38:07.177294 139578135525120 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.7804597020149231, loss=0.917339026927948
I0215 11:39:40.056771 139578127132416 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.9853602647781372, loss=0.946681797504425
I0215 11:41:10.986937 139578135525120 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.6419417262077332, loss=0.905051052570343
I0215 11:42:39.489987 139578127132416 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.7328878045082092, loss=0.9033907651901245
I0215 11:43:48.704466 139688679413568 spec.py:321] Evaluating on the training split.
I0215 11:44:44.342329 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 11:45:37.782937 139688679413568 spec.py:349] Evaluating on the test split.
I0215 11:46:04.709514 139688679413568 submission_runner.py:408] Time since start: 56911.07s, 	Step: 60577, 	{'train/ctc_loss': Array(0.07713567, dtype=float32), 'train/wer': 0.030381777350016043, 'validation/ctc_loss': Array(0.34140152, dtype=float32), 'validation/wer': 0.09714511909014549, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17879401, dtype=float32), 'test/wer': 0.057705197733227714, 'test/num_examples': 2472, 'score': 51886.75729846954, 'total_duration': 56911.06728410721, 'accumulated_submission_time': 51886.75729846954, 'accumulated_eval_time': 5019.637094259262, 'accumulated_logging_time': 2.0751595497131348}
I0215 11:46:04.753308 139578135525120 logging_writer.py:48] [60577] accumulated_eval_time=5019.637094, accumulated_logging_time=2.075160, accumulated_submission_time=51886.757298, global_step=60577, preemption_count=0, score=51886.757298, test/ctc_loss=0.1787940114736557, test/num_examples=2472, test/wer=0.057705, total_duration=56911.067284, train/ctc_loss=0.07713566720485687, train/wer=0.030382, validation/ctc_loss=0.34140151739120483, validation/num_examples=5348, validation/wer=0.097145
I0215 11:46:23.062025 139578127132416 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.962241530418396, loss=0.9264140129089355
I0215 11:47:38.687368 139578135525120 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.694076418876648, loss=0.908222496509552
I0215 11:48:58.175032 139578135525120 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.7816131711006165, loss=0.8923618197441101
I0215 11:50:19.077320 139578127132416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.7889271378517151, loss=0.8920980095863342
I0215 11:51:38.924570 139578135525120 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.010710597038269, loss=0.8758905529975891
I0215 11:53:04.435313 139578127132416 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.7677353024482727, loss=0.9473363757133484
I0215 11:54:31.382862 139578135525120 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.7643004059791565, loss=0.8891324400901794
I0215 11:56:02.535674 139578127132416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.7833746075630188, loss=0.9084902405738831
I0215 11:57:34.481318 139578135525120 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.7967230677604675, loss=0.9092504382133484
I0215 11:59:05.685665 139578127132416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.9263851642608643, loss=0.941564679145813
I0215 12:00:36.036523 139578135525120 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.7779252529144287, loss=0.9104970693588257
I0215 12:02:05.200955 139578127132416 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.9481865167617798, loss=0.8642849922180176
I0215 12:03:40.569921 139578135525120 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.7687733769416809, loss=0.915980875492096
I0215 12:04:59.531338 139578127132416 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.7188935279846191, loss=0.8875985741615295
I0215 12:06:18.246217 139578135525120 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.7375127077102661, loss=0.9163103103637695
I0215 12:07:40.959923 139578127132416 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.7607648968696594, loss=1.005697250366211
I0215 12:09:08.068842 139578135525120 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.9166589379310608, loss=0.8782423734664917
I0215 12:10:05.154044 139688679413568 spec.py:321] Evaluating on the training split.
I0215 12:11:01.058509 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 12:11:55.044802 139688679413568 spec.py:349] Evaluating on the test split.
I0215 12:12:22.757253 139688679413568 submission_runner.py:408] Time since start: 58489.11s, 	Step: 62265, 	{'train/ctc_loss': Array(0.07505095, dtype=float32), 'train/wer': 0.031054039476766677, 'validation/ctc_loss': Array(0.33734754, dtype=float32), 'validation/wer': 0.09526246174343725, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17833206, dtype=float32), 'test/wer': 0.0573192777202283, 'test/num_examples': 2472, 'score': 53327.07553648949, 'total_duration': 58489.11463499069, 'accumulated_submission_time': 53327.07553648949, 'accumulated_eval_time': 5157.233544111252, 'accumulated_logging_time': 2.1355743408203125}
I0215 12:12:22.804962 139578135525120 logging_writer.py:48] [62265] accumulated_eval_time=5157.233544, accumulated_logging_time=2.135574, accumulated_submission_time=53327.075536, global_step=62265, preemption_count=0, score=53327.075536, test/ctc_loss=0.17833206057548523, test/num_examples=2472, test/wer=0.057319, total_duration=58489.114635, train/ctc_loss=0.075050950050354, train/wer=0.031054, validation/ctc_loss=0.33734753727912903, validation/num_examples=5348, validation/wer=0.095262
I0215 12:12:50.179750 139578127132416 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.7695425152778625, loss=0.8610560894012451
I0215 12:14:05.972940 139578135525120 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.9293017387390137, loss=0.8997421264648438
I0215 12:15:24.521386 139578127132416 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.7070499062538147, loss=0.8842279314994812
I0215 12:16:53.695375 139578135525120 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.7158657908439636, loss=0.8810182213783264
I0215 12:18:25.872221 139578127132416 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.8731002807617188, loss=0.8808445930480957
I0215 12:19:54.965697 139578135525120 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.8513874411582947, loss=0.864528238773346
I0215 12:21:20.650933 139578135525120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.7253466844558716, loss=0.8878562450408936
I0215 12:22:40.063114 139578127132416 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0160185098648071, loss=0.8558050990104675
I0215 12:24:04.790261 139578135525120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.8717172741889954, loss=0.9412386417388916
I0215 12:25:31.198348 139578127132416 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.2285242080688477, loss=0.9081273674964905
I0215 12:27:01.794248 139578135525120 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.761938750743866, loss=0.9192571043968201
I0215 12:28:30.290415 139578127132416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.7030515670776367, loss=0.8724471926689148
I0215 12:29:58.767222 139578135525120 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.9533716440200806, loss=0.8501714468002319
I0215 12:31:31.488887 139578127132416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.8358874320983887, loss=0.9148371815681458
I0215 12:33:02.051064 139578135525120 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.705049455165863, loss=0.9183135628700256
I0215 12:34:34.001394 139578127132416 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.7536098957061768, loss=0.8440918326377869
I0215 12:36:01.518748 139578135525120 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.7845665812492371, loss=0.8960477709770203
I0215 12:36:22.754316 139688679413568 spec.py:321] Evaluating on the training split.
I0215 12:37:18.015929 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 12:38:12.392437 139688679413568 spec.py:349] Evaluating on the test split.
I0215 12:38:40.203734 139688679413568 submission_runner.py:408] Time since start: 60066.56s, 	Step: 63929, 	{'train/ctc_loss': Array(0.06894256, dtype=float32), 'train/wer': 0.02790050100310249, 'validation/ctc_loss': Array(0.33377695, dtype=float32), 'validation/wer': 0.09337980439672901, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17739047, dtype=float32), 'test/wer': 0.05746145877764914, 'test/num_examples': 2472, 'score': 54766.94156455994, 'total_duration': 60066.56161522865, 'accumulated_submission_time': 54766.94156455994, 'accumulated_eval_time': 5294.676733493805, 'accumulated_logging_time': 2.2014431953430176}
I0215 12:38:40.244698 139578135525120 logging_writer.py:48] [63929] accumulated_eval_time=5294.676733, accumulated_logging_time=2.201443, accumulated_submission_time=54766.941565, global_step=63929, preemption_count=0, score=54766.941565, test/ctc_loss=0.1773904711008072, test/num_examples=2472, test/wer=0.057461, total_duration=60066.561615, train/ctc_loss=0.06894256174564362, train/wer=0.027901, validation/ctc_loss=0.33377695083618164, validation/num_examples=5348, validation/wer=0.093380
I0215 12:39:34.666217 139578127132416 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.7743624448776245, loss=0.8902900218963623
I0215 12:40:50.647605 139578135525120 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.8067923784255981, loss=0.8767290711402893
I0215 12:42:06.370513 139578127132416 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.8315709829330444, loss=0.8858948349952698
I0215 12:43:26.777783 139578135525120 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.9318782687187195, loss=0.9090161323547363
I0215 12:44:58.349030 139578127132416 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.7288361191749573, loss=0.8716129660606384
I0215 12:46:29.756197 139578135525120 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.8228784799575806, loss=0.8858122229576111
I0215 12:47:59.199461 139578127132416 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.6314380764961243, loss=0.9039812684059143
I0215 12:49:32.119485 139578135525120 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.7142871618270874, loss=0.8725051879882812
I0215 12:51:04.318139 139578127132416 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.9736614227294922, loss=0.8838098645210266
I0215 12:52:36.310418 139578135525120 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.7741239070892334, loss=0.8837202787399292
I0215 12:53:53.552783 139578127132416 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.7720091938972473, loss=0.8872191905975342
I0215 12:55:11.604429 139578135525120 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.9010588526725769, loss=0.8861749768257141
I0215 12:56:31.421978 139578127132416 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.8891762495040894, loss=0.8609511256217957
I0215 12:57:54.757266 139578135525120 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.8580707907676697, loss=0.8388088941574097
I0215 12:59:24.926023 139578127132416 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.0189200639724731, loss=0.8719959855079651
I0215 13:00:57.175453 139578135525120 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.7319286465644836, loss=0.8753495216369629
I0215 13:02:29.054778 139578127132416 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.7435933947563171, loss=0.8869619369506836
I0215 13:02:40.960396 139688679413568 spec.py:321] Evaluating on the training split.
I0215 13:03:34.285645 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 13:04:27.457476 139688679413568 spec.py:349] Evaluating on the test split.
I0215 13:04:55.137787 139688679413568 submission_runner.py:408] Time since start: 61641.50s, 	Step: 65615, 	{'train/ctc_loss': Array(0.07285346, dtype=float32), 'train/wer': 0.028964368773148813, 'validation/ctc_loss': Array(0.33438018, dtype=float32), 'validation/wer': 0.09347635092732942, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17490257, dtype=float32), 'test/wer': 0.05733958929985985, 'test/num_examples': 2472, 'score': 56207.575248003006, 'total_duration': 61641.49627780914, 'accumulated_submission_time': 56207.575248003006, 'accumulated_eval_time': 5428.8484699726105, 'accumulated_logging_time': 2.258314847946167}
I0215 13:04:55.183760 139578135525120 logging_writer.py:48] [65615] accumulated_eval_time=5428.848470, accumulated_logging_time=2.258315, accumulated_submission_time=56207.575248, global_step=65615, preemption_count=0, score=56207.575248, test/ctc_loss=0.17490257322788239, test/num_examples=2472, test/wer=0.057340, total_duration=61641.496278, train/ctc_loss=0.0728534609079361, train/wer=0.028964, validation/ctc_loss=0.334380179643631, validation/num_examples=5348, validation/wer=0.093476
I0215 13:06:00.316000 139578127132416 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.9136438369750977, loss=0.8766823410987854
I0215 13:07:16.108451 139578135525120 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.7183534502983093, loss=0.8826963305473328
I0215 13:08:44.500490 139578127132416 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.6646363735198975, loss=0.8571021556854248
I0215 13:10:08.908642 139578135525120 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.8365409970283508, loss=0.8692707419395447
I0215 13:11:28.596245 139578127132416 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.8400130271911621, loss=0.8943811058998108
I0215 13:12:48.224873 139578135525120 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.8219047784805298, loss=0.8698887228965759
I0215 13:14:13.865466 139578127132416 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.845209002494812, loss=0.8544883131980896
I0215 13:15:41.088784 139578135525120 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.7552241086959839, loss=0.8517712354660034
I0215 13:17:10.768667 139578127132416 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.9929544925689697, loss=0.8209367394447327
I0215 13:18:40.709169 139578135525120 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.8237005472183228, loss=0.8784079551696777
I0215 13:20:09.357805 139578127132416 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.9196965098381042, loss=0.8727416396141052
I0215 13:21:38.957417 139578135525120 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.9575340151786804, loss=0.8842864036560059
I0215 13:23:11.364390 139578127132416 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.733047366142273, loss=0.8668335676193237
I0215 13:24:39.141497 139578135525120 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.7981569766998291, loss=0.8995676636695862
I0215 13:25:58.101023 139578127132416 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.10724937915802, loss=0.8822131156921387
I0215 13:27:17.728232 139578135525120 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.7988149523735046, loss=0.865187406539917
I0215 13:28:41.184701 139578127132416 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.756969690322876, loss=0.8758190870285034
I0215 13:28:55.226439 139688679413568 spec.py:321] Evaluating on the training split.
I0215 13:29:49.930834 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 13:30:43.614125 139688679413568 spec.py:349] Evaluating on the test split.
I0215 13:31:10.445888 139688679413568 submission_runner.py:408] Time since start: 63216.80s, 	Step: 67319, 	{'train/ctc_loss': Array(0.07366558, dtype=float32), 'train/wer': 0.028987808149433657, 'validation/ctc_loss': Array(0.33114108, dtype=float32), 'validation/wer': 0.09242399374378482, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1737761, dtype=float32), 'test/wer': 0.05597871346454614, 'test/num_examples': 2472, 'score': 57647.53215622902, 'total_duration': 63216.80353283882, 'accumulated_submission_time': 57647.53215622902, 'accumulated_eval_time': 5564.061456441879, 'accumulated_logging_time': 2.3223886489868164}
I0215 13:31:10.491521 139578135525120 logging_writer.py:48] [67319] accumulated_eval_time=5564.061456, accumulated_logging_time=2.322389, accumulated_submission_time=57647.532156, global_step=67319, preemption_count=0, score=57647.532156, test/ctc_loss=0.17377610504627228, test/num_examples=2472, test/wer=0.055979, total_duration=63216.803533, train/ctc_loss=0.07366558164358139, train/wer=0.028988, validation/ctc_loss=0.33114108443260193, validation/num_examples=5348, validation/wer=0.092424
I0215 13:32:12.789019 139578127132416 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.093618392944336, loss=0.8919232487678528
I0215 13:33:28.456704 139578135525120 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.7922969460487366, loss=0.949221670627594
I0215 13:34:53.219141 139578127132416 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.9480122327804565, loss=0.8881022334098816
I0215 13:36:20.963705 139578135525120 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.702667772769928, loss=0.877771258354187
I0215 13:37:51.000416 139578127132416 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.7594342231750488, loss=0.8999159336090088
I0215 13:39:19.052465 139578135525120 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.7945935726165771, loss=0.878317654132843
I0215 13:40:48.747709 139578135525120 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.784008800983429, loss=0.840150773525238
I0215 13:42:08.912684 139578127132416 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.8332259058952332, loss=0.8220983743667603
I0215 13:43:28.931772 139578135525120 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.777972936630249, loss=0.8587659597396851
I0215 13:44:48.718545 139578127132416 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.6820750832557678, loss=0.8263930678367615
I0215 13:46:14.145741 139578135525120 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3583247661590576, loss=0.8528919219970703
I0215 13:47:44.443463 139578127132416 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.0030877590179443, loss=0.8660071492195129
I0215 13:49:13.666806 139578135525120 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.8767846822738647, loss=0.8758581280708313
I0215 13:50:44.722033 139578127132416 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.7254902124404907, loss=0.8523039817810059
I0215 13:52:16.813308 139578135525120 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.7841992378234863, loss=0.8749315142631531
I0215 13:53:49.269089 139578127132416 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.7492663264274597, loss=0.8278363943099976
I0215 13:55:11.469477 139688679413568 spec.py:321] Evaluating on the training split.
I0215 13:56:06.083057 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 13:57:00.223664 139688679413568 spec.py:349] Evaluating on the test split.
I0215 13:57:27.323328 139688679413568 submission_runner.py:408] Time since start: 64793.68s, 	Step: 68991, 	{'train/ctc_loss': Array(0.0607663, dtype=float32), 'train/wer': 0.024274253346490714, 'validation/ctc_loss': Array(0.32713333, dtype=float32), 'validation/wer': 0.09186402386630237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17034464, dtype=float32), 'test/wer': 0.05545061239412589, 'test/num_examples': 2472, 'score': 59088.42539978027, 'total_duration': 64793.68052625656, 'accumulated_submission_time': 59088.42539978027, 'accumulated_eval_time': 5699.908368587494, 'accumulated_logging_time': 2.387371301651001}
I0215 13:57:27.370887 139578135525120 logging_writer.py:48] [68991] accumulated_eval_time=5699.908369, accumulated_logging_time=2.387371, accumulated_submission_time=59088.425400, global_step=68991, preemption_count=0, score=59088.425400, test/ctc_loss=0.17034463584423065, test/num_examples=2472, test/wer=0.055451, total_duration=64793.680526, train/ctc_loss=0.060766298323869705, train/wer=0.024274, validation/ctc_loss=0.32713332772254944, validation/num_examples=5348, validation/wer=0.091864
I0215 13:57:34.990054 139578127132416 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.3596875667572021, loss=0.8045477867126465
I0215 13:58:57.810680 139578135525120 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.8972514867782593, loss=0.8819251656532288
I0215 14:00:14.810164 139578127132416 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.9452103972434998, loss=0.8999500870704651
I0215 14:01:35.932413 139578135525120 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.180259346961975, loss=0.8566544055938721
I0215 14:03:01.764734 139578127132416 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.8787823915481567, loss=0.8709786534309387
I0215 14:04:33.714601 139578135525120 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.7518301010131836, loss=0.8943283557891846
I0215 14:06:02.641104 139578127132416 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.8009477257728577, loss=0.8706419467926025
I0215 14:07:32.477470 139578135525120 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.8101644515991211, loss=0.8645948171615601
I0215 14:09:03.746134 139578127132416 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.8128629326820374, loss=0.8515697121620178
I0215 14:10:35.633583 139578135525120 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.7799689173698425, loss=0.8407081961631775
I0215 14:12:07.638041 139578127132416 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.7189710140228271, loss=0.8809638023376465
I0215 14:13:33.840985 139578135525120 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.7218016982078552, loss=0.841590166091919
I0215 14:14:51.182312 139578127132416 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.8510163426399231, loss=0.8579159379005432
I0215 14:16:14.116130 139578135525120 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.420783519744873, loss=0.8679659366607666
I0215 14:17:40.554845 139578127132416 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1794898509979248, loss=0.8456956744194031
I0215 14:19:10.357812 139578135525120 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.7160158157348633, loss=0.8388277888298035
I0215 14:20:43.273161 139578127132416 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.098552942276001, loss=0.8551095128059387
I0215 14:21:27.671717 139688679413568 spec.py:321] Evaluating on the training split.
I0215 14:22:21.634566 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 14:23:15.420591 139688679413568 spec.py:349] Evaluating on the test split.
I0215 14:23:42.350383 139688679413568 submission_runner.py:408] Time since start: 66368.71s, 	Step: 70651, 	{'train/ctc_loss': Array(0.06327435, dtype=float32), 'train/wer': 0.02436748237245956, 'validation/ctc_loss': Array(0.3291911, dtype=float32), 'validation/wer': 0.09122681676433958, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17073686, dtype=float32), 'test/wer': 0.0549021997440741, 'test/num_examples': 2472, 'score': 60528.64407491684, 'total_duration': 66368.70748090744, 'accumulated_submission_time': 60528.64407491684, 'accumulated_eval_time': 5834.580038309097, 'accumulated_logging_time': 2.4519057273864746}
I0215 14:23:42.394080 139578135525120 logging_writer.py:48] [70651] accumulated_eval_time=5834.580038, accumulated_logging_time=2.451906, accumulated_submission_time=60528.644075, global_step=70651, preemption_count=0, score=60528.644075, test/ctc_loss=0.1707368642091751, test/num_examples=2472, test/wer=0.054902, total_duration=66368.707481, train/ctc_loss=0.06327435374259949, train/wer=0.024367, validation/ctc_loss=0.32919108867645264, validation/num_examples=5348, validation/wer=0.091227
I0215 14:24:20.573158 139578127132416 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.7183605432510376, loss=0.8266631960868835
I0215 14:25:36.373820 139578135525120 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.8376114368438721, loss=0.9332447052001953
I0215 14:26:55.765001 139578127132416 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.8406012058258057, loss=0.8712852001190186
I0215 14:28:25.980087 139578135525120 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.8222240805625916, loss=0.8700207471847534
I0215 14:29:57.195127 139578135525120 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.9757726192474365, loss=0.8782835006713867
I0215 14:31:13.592834 139578127132416 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.813252329826355, loss=0.8953381180763245
I0215 14:32:32.873831 139578135525120 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.8463122844696045, loss=0.8563870787620544
I0215 14:32:42.497550 139578127132416 logging_writer.py:48] [71314] global_step=71314, preemption_count=0, score=61068.681362
I0215 14:32:43.429105 139688679413568 checkpoints.py:490] Saving checkpoint at step: 71314
I0215 14:32:44.981023 139688679413568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_2/checkpoint_71314
I0215 14:32:45.017146 139688679413568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_2/checkpoint_71314.
I0215 14:32:48.351841 139688679413568 submission_runner.py:583] Tuning trial 2/5
I0215 14:32:48.352133 139688679413568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0215 14:32:48.376689 139688679413568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.28297, dtype=float32), 'train/wer': 0.9402553710331124, 'validation/ctc_loss': Array(30.141817, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 35.15062928199768, 'total_duration': 152.89069986343384, 'accumulated_submission_time': 35.15062928199768, 'accumulated_eval_time': 117.74000239372253, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1672, {'train/ctc_loss': Array(6.5428987, dtype=float32), 'train/wer': 0.8976848691695108, 'validation/ctc_loss': Array(6.410177, dtype=float32), 'validation/wer': 0.8668623343020169, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.379735, dtype=float32), 'test/wer': 0.8667357260374139, 'test/num_examples': 2472, 'score': 1475.9704265594482, 'total_duration': 1708.7733316421509, 'accumulated_submission_time': 1475.9704265594482, 'accumulated_eval_time': 232.7037012577057, 'accumulated_logging_time': 0.03008103370666504, 'global_step': 1672, 'preemption_count': 0}), (3369, {'train/ctc_loss': Array(2.9755747, dtype=float32), 'train/wer': 0.6129117945215133, 'validation/ctc_loss': Array(2.9443042, dtype=float32), 'validation/wer': 0.5842320206223389, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5972764, dtype=float32), 'test/wer': 0.532935226372555, 'test/num_examples': 2472, 'score': 2916.4993340969086, 'total_duration': 3282.37225317955, 'accumulated_submission_time': 2916.4993340969086, 'accumulated_eval_time': 365.6510236263275, 'accumulated_logging_time': 0.07974767684936523, 'global_step': 3369, 'preemption_count': 0}), (5038, {'train/ctc_loss': Array(0.84934884, dtype=float32), 'train/wer': 0.27204680945020976, 'validation/ctc_loss': Array(0.9519117, dtype=float32), 'validation/wer': 0.28167450302673375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6751439, dtype=float32), 'test/wer': 0.22157902220055653, 'test/num_examples': 2472, 'score': 4356.664489269257, 'total_duration': 4857.005820035934, 'accumulated_submission_time': 4356.664489269257, 'accumulated_eval_time': 499.99262046813965, 'accumulated_logging_time': 0.13511443138122559, 'global_step': 5038, 'preemption_count': 0}), (6718, {'train/ctc_loss': Array(0.58703554, dtype=float32), 'train/wer': 0.20140974139371898, 'validation/ctc_loss': Array(0.7285059, dtype=float32), 'validation/wer': 0.2218735819728318, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47832748, dtype=float32), 'test/wer': 0.16011618223549245, 'test/num_examples': 2472, 'score': 5796.641132116318, 'total_duration': 6433.316517829895, 'accumulated_submission_time': 5796.641132116318, 'accumulated_eval_time': 636.2041306495667, 'accumulated_logging_time': 0.18589329719543457, 'global_step': 6718, 'preemption_count': 0}), (8411, {'train/ctc_loss': Array(0.49299544, dtype=float32), 'train/wer': 0.17005069157397062, 'validation/ctc_loss': Array(0.6641883, dtype=float32), 'validation/wer': 0.20003475675101615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42482418, dtype=float32), 'test/wer': 0.14409034590620112, 'test/num_examples': 2472, 'score': 7236.699079275131, 'total_duration': 8008.11719751358, 'accumulated_submission_time': 7236.699079275131, 'accumulated_eval_time': 770.8168325424194, 'accumulated_logging_time': 0.24300336837768555, 'global_step': 8411, 'preemption_count': 0}), (10118, {'train/ctc_loss': Array(0.29567704, dtype=float32), 'train/wer': 0.10973898996769257, 'validation/ctc_loss': Array(0.60095006, dtype=float32), 'validation/wer': 0.18386321287544533, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3799917, dtype=float32), 'test/wer': 0.1304003412345378, 'test/num_examples': 2472, 'score': 8677.11043047905, 'total_duration': 9608.002731323242, 'accumulated_submission_time': 8677.11043047905, 'accumulated_eval_time': 930.1696033477783, 'accumulated_logging_time': 0.2932603359222412, 'global_step': 10118, 'preemption_count': 0}), (11832, {'train/ctc_loss': Array(0.2633589, dtype=float32), 'train/wer': 0.0978113489841189, 'validation/ctc_loss': Array(0.5699334, dtype=float32), 'validation/wer': 0.17382237369300135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3490801, dtype=float32), 'test/wer': 0.1201836166798692, 'test/num_examples': 2472, 'score': 10117.716738939285, 'total_duration': 11185.009489297867, 'accumulated_submission_time': 10117.716738939285, 'accumulated_eval_time': 1066.4482853412628, 'accumulated_logging_time': 0.34238743782043457, 'global_step': 11832, 'preemption_count': 0}), (13501, {'train/ctc_loss': Array(0.25779513, dtype=float32), 'train/wer': 0.09635561193477629, 'validation/ctc_loss': Array(0.5493172, dtype=float32), 'validation/wer': 0.16707377120403177, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33060178, dtype=float32), 'test/wer': 0.11193711534946073, 'test/num_examples': 2472, 'score': 11558.25578045845, 'total_duration': 12760.356534957886, 'accumulated_submission_time': 11558.25578045845, 'accumulated_eval_time': 1201.1347353458405, 'accumulated_logging_time': 0.3927774429321289, 'global_step': 13501, 'preemption_count': 0}), (15177, {'train/ctc_loss': Array(0.22716044, dtype=float32), 'train/wer': 0.08648468152600385, 'validation/ctc_loss': Array(0.5236656, dtype=float32), 'validation/wer': 0.15900248124583644, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3165861, dtype=float32), 'test/wer': 0.10801698048057197, 'test/num_examples': 2472, 'score': 12998.907264947891, 'total_duration': 14337.500767707825, 'accumulated_submission_time': 12998.907264947891, 'accumulated_eval_time': 1337.5061967372894, 'accumulated_logging_time': 0.44394874572753906, 'global_step': 15177, 'preemption_count': 0}), (16862, {'train/ctc_loss': Array(0.22985189, dtype=float32), 'train/wer': 0.0873535887743841, 'validation/ctc_loss': Array(0.5217149, dtype=float32), 'validation/wer': 0.1570329320215878, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3099842, dtype=float32), 'test/wer': 0.10523429407104991, 'test/num_examples': 2472, 'score': 14439.008511543274, 'total_duration': 15914.451095581055, 'accumulated_submission_time': 14439.008511543274, 'accumulated_eval_time': 1474.2307941913605, 'accumulated_logging_time': 0.49520206451416016, 'global_step': 16862, 'preemption_count': 0}), (18526, {'train/ctc_loss': Array(0.2079086, dtype=float32), 'train/wer': 0.08069496831824934, 'validation/ctc_loss': Array(0.49227136, dtype=float32), 'validation/wer': 0.15032294814485841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29305145, dtype=float32), 'test/wer': 0.09954705177421648, 'test/num_examples': 2472, 'score': 15879.444328546524, 'total_duration': 17491.679450511932, 'accumulated_submission_time': 15879.444328546524, 'accumulated_eval_time': 1610.9025394916534, 'accumulated_logging_time': 0.5452065467834473, 'global_step': 18526, 'preemption_count': 0}), (20203, {'train/ctc_loss': Array(0.22827052, dtype=float32), 'train/wer': 0.08320993218302, 'validation/ctc_loss': Array(0.48092973, dtype=float32), 'validation/wer': 0.1457949158596986, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28144392, dtype=float32), 'test/wer': 0.09552535900716999, 'test/num_examples': 2472, 'score': 17320.42205142975, 'total_duration': 19068.82412481308, 'accumulated_submission_time': 17320.42205142975, 'accumulated_eval_time': 1746.945957183838, 'accumulated_logging_time': 0.5986299514770508, 'global_step': 20203, 'preemption_count': 0}), (21887, {'train/ctc_loss': Array(0.2024693, dtype=float32), 'train/wer': 0.07828138252756574, 'validation/ctc_loss': Array(0.4742581, dtype=float32), 'validation/wer': 0.14387363990075017, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2813647, dtype=float32), 'test/wer': 0.09574878638311701, 'test/num_examples': 2472, 'score': 18760.71983551979, 'total_duration': 20646.238900899887, 'accumulated_submission_time': 18760.71983551979, 'accumulated_eval_time': 1883.9370305538177, 'accumulated_logging_time': 0.6513259410858154, 'global_step': 21887, 'preemption_count': 0}), (23551, {'train/ctc_loss': Array(0.16744016, dtype=float32), 'train/wer': 0.06670042493657273, 'validation/ctc_loss': Array(0.45430538, dtype=float32), 'validation/wer': 0.13942284484007067, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2655093, dtype=float32), 'test/wer': 0.09130055044380801, 'test/num_examples': 2472, 'score': 20201.211846590042, 'total_duration': 22226.676994800568, 'accumulated_submission_time': 20201.211846590042, 'accumulated_eval_time': 2023.754251241684, 'accumulated_logging_time': 0.7084658145904541, 'global_step': 23551, 'preemption_count': 0}), (25245, {'train/ctc_loss': Array(0.16750503, dtype=float32), 'train/wer': 0.06492667933146919, 'validation/ctc_loss': Array(0.45686898, dtype=float32), 'validation/wer': 0.13845737953406645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2632382, dtype=float32), 'test/wer': 0.09042715251965146, 'test/num_examples': 2472, 'score': 21641.502986192703, 'total_duration': 23805.24548983574, 'accumulated_submission_time': 21641.502986192703, 'accumulated_eval_time': 2161.89958691597, 'accumulated_logging_time': 0.764528751373291, 'global_step': 25245, 'preemption_count': 0}), (26921, {'train/ctc_loss': Array(0.15699868, dtype=float32), 'train/wer': 0.06478888410168031, 'validation/ctc_loss': Array(0.44483903, dtype=float32), 'validation/wer': 0.13478861137125037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25737813, dtype=float32), 'test/wer': 0.08878191456949606, 'test/num_examples': 2472, 'score': 23081.486287117004, 'total_duration': 25378.611304998398, 'accumulated_submission_time': 23081.486287117004, 'accumulated_eval_time': 2295.1566026210785, 'accumulated_logging_time': 0.8176376819610596, 'global_step': 26921, 'preemption_count': 0}), (28601, {'train/ctc_loss': Array(0.16138557, dtype=float32), 'train/wer': 0.06255288918605502, 'validation/ctc_loss': Array(0.43052354, dtype=float32), 'validation/wer': 0.13115846182067448, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24619368, dtype=float32), 'test/wer': 0.08398838177645075, 'test/num_examples': 2472, 'score': 24521.77783679962, 'total_duration': 26954.51037287712, 'accumulated_submission_time': 24521.77783679962, 'accumulated_eval_time': 2430.638184070587, 'accumulated_logging_time': 0.8726804256439209, 'global_step': 28601, 'preemption_count': 0}), (30300, {'train/ctc_loss': Array(0.15786675, dtype=float32), 'train/wer': 0.06232975061764892, 'validation/ctc_loss': Array(0.42404762, dtype=float32), 'validation/wer': 0.1281558647190013, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24171406, dtype=float32), 'test/wer': 0.08276968699855788, 'test/num_examples': 2472, 'score': 25962.238719701767, 'total_duration': 28531.932410240173, 'accumulated_submission_time': 25962.238719701767, 'accumulated_eval_time': 2567.4729528427124, 'accumulated_logging_time': 0.9269707202911377, 'global_step': 30300, 'preemption_count': 0}), (31982, {'train/ctc_loss': Array(0.1505317, dtype=float32), 'train/wer': 0.060234731827421936, 'validation/ctc_loss': Array(0.42554152, dtype=float32), 'validation/wer': 0.12888961835156454, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24244688, dtype=float32), 'test/wer': 0.08220096276887454, 'test/num_examples': 2472, 'score': 27402.670576810837, 'total_duration': 30108.24808359146, 'accumulated_submission_time': 27402.670576810837, 'accumulated_eval_time': 2703.1411702632904, 'accumulated_logging_time': 1.0707290172576904, 'global_step': 31982, 'preemption_count': 0}), (33698, {'train/ctc_loss': Array(0.14140327, dtype=float32), 'train/wer': 0.05576077673158642, 'validation/ctc_loss': Array(0.4030115, dtype=float32), 'validation/wer': 0.12160035529123261, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23038326, dtype=float32), 'test/wer': 0.07793553104624946, 'test/num_examples': 2472, 'score': 28843.265655517578, 'total_duration': 31686.662852287292, 'accumulated_submission_time': 28843.265655517578, 'accumulated_eval_time': 2840.8364021778107, 'accumulated_logging_time': 1.121816635131836, 'global_step': 33698, 'preemption_count': 0}), (35403, {'train/ctc_loss': Array(0.12408025, dtype=float32), 'train/wer': 0.050719403598863295, 'validation/ctc_loss': Array(0.41467035, dtype=float32), 'validation/wer': 0.12207343329117468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22733217, dtype=float32), 'test/wer': 0.0772246257591453, 'test/num_examples': 2472, 'score': 30283.51073741913, 'total_duration': 33262.70490074158, 'accumulated_submission_time': 30283.51073741913, 'accumulated_eval_time': 2976.497559785843, 'accumulated_logging_time': 1.1860380172729492, 'global_step': 35403, 'preemption_count': 0}), (37064, {'train/ctc_loss': Array(0.12518206, dtype=float32), 'train/wer': 0.04862967145843022, 'validation/ctc_loss': Array(0.3949641, dtype=float32), 'validation/wer': 0.11982389912818484, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22285151, dtype=float32), 'test/wer': 0.07529502569414824, 'test/num_examples': 2472, 'score': 31723.408026456833, 'total_duration': 34839.59212565422, 'accumulated_submission_time': 31723.408026456833, 'accumulated_eval_time': 3113.3532407283783, 'accumulated_logging_time': 1.2495365142822266, 'global_step': 37064, 'preemption_count': 0}), (38724, {'train/ctc_loss': Array(0.1323574, dtype=float32), 'train/wer': 0.052375027050421984, 'validation/ctc_loss': Array(0.40408602, dtype=float32), 'validation/wer': 0.11951495023026347, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21985793, dtype=float32), 'test/wer': 0.07462474356630715, 'test/num_examples': 2472, 'score': 33163.48185658455, 'total_duration': 36415.07633471489, 'accumulated_submission_time': 33163.48185658455, 'accumulated_eval_time': 3248.6328341960907, 'accumulated_logging_time': 1.309312343597412, 'global_step': 38724, 'preemption_count': 0}), (40411, {'train/ctc_loss': Array(0.12066688, dtype=float32), 'train/wer': 0.047393106754334835, 'validation/ctc_loss': Array(0.39027813, dtype=float32), 'validation/wer': 0.11585583672050745, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21655618, dtype=float32), 'test/wer': 0.07176081083825889, 'test/num_examples': 2472, 'score': 34603.7865755558, 'total_duration': 37990.041987895966, 'accumulated_submission_time': 34603.7865755558, 'accumulated_eval_time': 3383.161405324936, 'accumulated_logging_time': 1.3682467937469482, 'global_step': 40411, 'preemption_count': 0}), (42087, {'train/ctc_loss': Array(0.11153517, dtype=float32), 'train/wer': 0.045562737252487946, 'validation/ctc_loss': Array(0.3821042, dtype=float32), 'validation/wer': 0.11345182810855692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21440364, dtype=float32), 'test/wer': 0.07194361505494282, 'test/num_examples': 2472, 'score': 36043.68631386757, 'total_duration': 39567.956351041794, 'accumulated_submission_time': 36043.68631386757, 'accumulated_eval_time': 3521.0500218868256, 'accumulated_logging_time': 1.421659231185913, 'global_step': 42087, 'preemption_count': 0}), (43777, {'train/ctc_loss': Array(0.1301486, dtype=float32), 'train/wer': 0.048602912239275875, 'validation/ctc_loss': Array(0.38293865, dtype=float32), 'validation/wer': 0.11291116753719455, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20935404, dtype=float32), 'test/wer': 0.06942497918063088, 'test/num_examples': 2472, 'score': 37483.879881858826, 'total_duration': 41146.22732448578, 'accumulated_submission_time': 37483.879881858826, 'accumulated_eval_time': 3658.9948778152466, 'accumulated_logging_time': 1.4797418117523193, 'global_step': 43777, 'preemption_count': 0}), (45457, {'train/ctc_loss': Array(0.08889699, dtype=float32), 'train/wer': 0.035825625038456016, 'validation/ctc_loss': Array(0.37530112, dtype=float32), 'validation/wer': 0.1112312579047472, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2068121, dtype=float32), 'test/wer': 0.06912030548615766, 'test/num_examples': 2472, 'score': 38923.82216382027, 'total_duration': 42725.361545324326, 'accumulated_submission_time': 38923.82216382027, 'accumulated_eval_time': 3798.0624623298645, 'accumulated_logging_time': 1.5324275493621826, 'global_step': 45457, 'preemption_count': 0}), (47141, {'train/ctc_loss': Array(0.09471288, dtype=float32), 'train/wer': 0.038131185077919565, 'validation/ctc_loss': Array(0.3694132, dtype=float32), 'validation/wer': 0.10856657366017552, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19967549, dtype=float32), 'test/wer': 0.06662198119147726, 'test/num_examples': 2472, 'score': 40364.569650650024, 'total_duration': 44304.246056079865, 'accumulated_submission_time': 40364.569650650024, 'accumulated_eval_time': 3936.06809425354, 'accumulated_logging_time': 1.5900306701660156, 'global_step': 47141, 'preemption_count': 0}), (48834, {'train/ctc_loss': Array(0.11606108, dtype=float32), 'train/wer': 0.04643084837426078, 'validation/ctc_loss': Array(0.36678696, dtype=float32), 'validation/wer': 0.108219006150014, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1959807, dtype=float32), 'test/wer': 0.0662563727581094, 'test/num_examples': 2472, 'score': 41804.55584001541, 'total_duration': 45879.99431490898, 'accumulated_submission_time': 41804.55584001541, 'accumulated_eval_time': 4071.6926686763763, 'accumulated_logging_time': 1.653782606124878, 'global_step': 48834, 'preemption_count': 0}), (50487, {'train/ctc_loss': Array(0.11821188, dtype=float32), 'train/wer': 0.046224055806009794, 'validation/ctc_loss': Array(0.3563446, dtype=float32), 'validation/wer': 0.10460816590555819, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19380894, dtype=float32), 'test/wer': 0.06410334531716531, 'test/num_examples': 2472, 'score': 43244.480036735535, 'total_duration': 47455.66095995903, 'accumulated_submission_time': 43244.480036735535, 'accumulated_eval_time': 4207.30445432663, 'accumulated_logging_time': 1.7124838829040527, 'global_step': 50487, 'preemption_count': 0}), (52173, {'train/ctc_loss': Array(0.13081229, dtype=float32), 'train/wer': 0.05299215704473733, 'validation/ctc_loss': Array(0.36061352, dtype=float32), 'validation/wer': 0.10450196472189772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19001366, dtype=float32), 'test/wer': 0.06310807791521947, 'test/num_examples': 2472, 'score': 44685.18374609947, 'total_duration': 49030.528040885925, 'accumulated_submission_time': 44685.18374609947, 'accumulated_eval_time': 4341.327328681946, 'accumulated_logging_time': 1.7806909084320068, 'global_step': 52173, 'preemption_count': 0}), (53875, {'train/ctc_loss': Array(0.11086977, dtype=float32), 'train/wer': 0.04350275853973471, 'validation/ctc_loss': Array(0.35066342, dtype=float32), 'validation/wer': 0.10255172480376917, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19238628, dtype=float32), 'test/wer': 0.06377836004306055, 'test/num_examples': 2472, 'score': 46125.39481854439, 'total_duration': 50606.23990535736, 'accumulated_submission_time': 46125.39481854439, 'accumulated_eval_time': 4476.6968767642975, 'accumulated_logging_time': 1.838486671447754, 'global_step': 53875, 'preemption_count': 0}), (55536, {'train/ctc_loss': Array(0.09881861, dtype=float32), 'train/wer': 0.03991368798431099, 'validation/ctc_loss': Array(0.3454917, dtype=float32), 'validation/wer': 0.09919190553887446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18675168, dtype=float32), 'test/wer': 0.06197062945585278, 'test/num_examples': 2472, 'score': 47565.87292122841, 'total_duration': 52182.46559405327, 'accumulated_submission_time': 47565.87292122841, 'accumulated_eval_time': 4612.31334400177, 'accumulated_logging_time': 1.8981482982635498, 'global_step': 55536, 'preemption_count': 0}), (57214, {'train/ctc_loss': Array(0.0818679, dtype=float32), 'train/wer': 0.03281551613199742, 'validation/ctc_loss': Array(0.34621996, dtype=float32), 'validation/wer': 0.09928845206947488, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18420044, dtype=float32), 'test/wer': 0.05939105884264619, 'test/num_examples': 2472, 'score': 49005.80888533592, 'total_duration': 53757.41523528099, 'accumulated_submission_time': 49005.80888533592, 'accumulated_eval_time': 4747.195596456528, 'accumulated_logging_time': 1.9564628601074219, 'global_step': 57214, 'preemption_count': 0}), (58908, {'train/ctc_loss': Array(0.09207707, dtype=float32), 'train/wer': 0.038660805828843174, 'validation/ctc_loss': Array(0.34705952, dtype=float32), 'validation/wer': 0.099771184722477, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18286462, dtype=float32), 'test/wer': 0.05979729043527715, 'test/num_examples': 2472, 'score': 50446.31366991997, 'total_duration': 55334.49369978905, 'accumulated_submission_time': 50446.31366991997, 'accumulated_eval_time': 4883.638375282288, 'accumulated_logging_time': 2.0142009258270264, 'global_step': 58908, 'preemption_count': 0}), (60577, {'train/ctc_loss': Array(0.07713567, dtype=float32), 'train/wer': 0.030381777350016043, 'validation/ctc_loss': Array(0.34140152, dtype=float32), 'validation/wer': 0.09714511909014549, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17879401, dtype=float32), 'test/wer': 0.057705197733227714, 'test/num_examples': 2472, 'score': 51886.75729846954, 'total_duration': 56911.06728410721, 'accumulated_submission_time': 51886.75729846954, 'accumulated_eval_time': 5019.637094259262, 'accumulated_logging_time': 2.0751595497131348, 'global_step': 60577, 'preemption_count': 0}), (62265, {'train/ctc_loss': Array(0.07505095, dtype=float32), 'train/wer': 0.031054039476766677, 'validation/ctc_loss': Array(0.33734754, dtype=float32), 'validation/wer': 0.09526246174343725, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17833206, dtype=float32), 'test/wer': 0.0573192777202283, 'test/num_examples': 2472, 'score': 53327.07553648949, 'total_duration': 58489.11463499069, 'accumulated_submission_time': 53327.07553648949, 'accumulated_eval_time': 5157.233544111252, 'accumulated_logging_time': 2.1355743408203125, 'global_step': 62265, 'preemption_count': 0}), (63929, {'train/ctc_loss': Array(0.06894256, dtype=float32), 'train/wer': 0.02790050100310249, 'validation/ctc_loss': Array(0.33377695, dtype=float32), 'validation/wer': 0.09337980439672901, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17739047, dtype=float32), 'test/wer': 0.05746145877764914, 'test/num_examples': 2472, 'score': 54766.94156455994, 'total_duration': 60066.56161522865, 'accumulated_submission_time': 54766.94156455994, 'accumulated_eval_time': 5294.676733493805, 'accumulated_logging_time': 2.2014431953430176, 'global_step': 63929, 'preemption_count': 0}), (65615, {'train/ctc_loss': Array(0.07285346, dtype=float32), 'train/wer': 0.028964368773148813, 'validation/ctc_loss': Array(0.33438018, dtype=float32), 'validation/wer': 0.09347635092732942, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17490257, dtype=float32), 'test/wer': 0.05733958929985985, 'test/num_examples': 2472, 'score': 56207.575248003006, 'total_duration': 61641.49627780914, 'accumulated_submission_time': 56207.575248003006, 'accumulated_eval_time': 5428.8484699726105, 'accumulated_logging_time': 2.258314847946167, 'global_step': 65615, 'preemption_count': 0}), (67319, {'train/ctc_loss': Array(0.07366558, dtype=float32), 'train/wer': 0.028987808149433657, 'validation/ctc_loss': Array(0.33114108, dtype=float32), 'validation/wer': 0.09242399374378482, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1737761, dtype=float32), 'test/wer': 0.05597871346454614, 'test/num_examples': 2472, 'score': 57647.53215622902, 'total_duration': 63216.80353283882, 'accumulated_submission_time': 57647.53215622902, 'accumulated_eval_time': 5564.061456441879, 'accumulated_logging_time': 2.3223886489868164, 'global_step': 67319, 'preemption_count': 0}), (68991, {'train/ctc_loss': Array(0.0607663, dtype=float32), 'train/wer': 0.024274253346490714, 'validation/ctc_loss': Array(0.32713333, dtype=float32), 'validation/wer': 0.09186402386630237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17034464, dtype=float32), 'test/wer': 0.05545061239412589, 'test/num_examples': 2472, 'score': 59088.42539978027, 'total_duration': 64793.68052625656, 'accumulated_submission_time': 59088.42539978027, 'accumulated_eval_time': 5699.908368587494, 'accumulated_logging_time': 2.387371301651001, 'global_step': 68991, 'preemption_count': 0}), (70651, {'train/ctc_loss': Array(0.06327435, dtype=float32), 'train/wer': 0.02436748237245956, 'validation/ctc_loss': Array(0.3291911, dtype=float32), 'validation/wer': 0.09122681676433958, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17073686, dtype=float32), 'test/wer': 0.0549021997440741, 'test/num_examples': 2472, 'score': 60528.64407491684, 'total_duration': 66368.70748090744, 'accumulated_submission_time': 60528.64407491684, 'accumulated_eval_time': 5834.580038309097, 'accumulated_logging_time': 2.4519057273864746, 'global_step': 70651, 'preemption_count': 0})], 'global_step': 71314}
I0215 14:32:48.377022 139688679413568 submission_runner.py:586] Timing: 61068.68136191368
I0215 14:32:48.377111 139688679413568 submission_runner.py:588] Total number of evals: 43
I0215 14:32:48.377178 139688679413568 submission_runner.py:589] ====================
I0215 14:32:48.377252 139688679413568 submission_runner.py:542] Using RNG seed 1618809895
I0215 14:32:48.379717 139688679413568 submission_runner.py:551] --- Tuning run 3/5 ---
I0215 14:32:48.379853 139688679413568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_3.
I0215 14:32:48.381412 139688679413568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_3/hparams.json.
I0215 14:32:48.383483 139688679413568 submission_runner.py:206] Initializing dataset.
I0215 14:32:48.383634 139688679413568 submission_runner.py:213] Initializing model.
I0215 14:32:51.938374 139688679413568 submission_runner.py:255] Initializing optimizer.
I0215 14:32:52.402678 139688679413568 submission_runner.py:262] Initializing metrics bundle.
I0215 14:32:52.402899 139688679413568 submission_runner.py:280] Initializing checkpoint and logger.
I0215 14:32:52.408009 139688679413568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_3 with prefix checkpoint_
I0215 14:32:52.408164 139688679413568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_3/meta_data_0.json.
I0215 14:32:52.408481 139688679413568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0215 14:32:52.408585 139688679413568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 14:32:52.990776 139688679413568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 14:32:53.501976 139688679413568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_3/flags_0.json.
I0215 14:32:53.522630 139688679413568 submission_runner.py:314] Starting training loop.
I0215 14:32:53.526015 139688679413568 input_pipeline.py:20] Loading split = train-clean-100
I0215 14:32:53.924812 139688679413568 input_pipeline.py:20] Loading split = train-clean-360
I0215 14:32:54.062304 139688679413568 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0215 14:33:28.468839 139518475712256 logging_writer.py:48] [0] global_step=0, grad_norm=50.38165283203125, loss=30.986305236816406
I0215 14:33:28.490440 139688679413568 spec.py:321] Evaluating on the training split.
I0215 14:34:11.628559 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 14:35:00.630799 139688679413568 spec.py:349] Evaluating on the test split.
I0215 14:35:26.024347 139688679413568 submission_runner.py:408] Time since start: 152.50s, 	Step: 1, 	{'train/ctc_loss': Array(31.732792, dtype=float32), 'train/wer': 0.9418907725440389, 'validation/ctc_loss': Array(30.141817, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 34.96774101257324, 'total_duration': 152.49866819381714, 'accumulated_submission_time': 34.96774101257324, 'accumulated_eval_time': 117.53086447715759, 'accumulated_logging_time': 0}
I0215 14:35:26.043973 139578135525120 logging_writer.py:48] [1] accumulated_eval_time=117.530864, accumulated_logging_time=0, accumulated_submission_time=34.967741, global_step=1, preemption_count=0, score=34.967741, test/ctc_loss=30.20883560180664, test/num_examples=2472, test/wer=0.908598, total_duration=152.498668, train/ctc_loss=31.732791900634766, train/wer=0.941891, validation/ctc_loss=30.141817092895508, validation/num_examples=5348, validation/wer=0.904371
I0215 14:37:07.358570 139518484104960 logging_writer.py:48] [100] global_step=100, grad_norm=26.344825744628906, loss=10.763748168945312
I0215 14:38:23.674366 139518492497664 logging_writer.py:48] [200] global_step=200, grad_norm=1.853766679763794, loss=6.20756196975708
I0215 14:39:40.135510 139518484104960 logging_writer.py:48] [300] global_step=300, grad_norm=0.6397425532341003, loss=5.858920097351074
I0215 14:40:56.386636 139518492497664 logging_writer.py:48] [400] global_step=400, grad_norm=0.4963670074939728, loss=5.82548713684082
I0215 14:42:24.130575 139518484104960 logging_writer.py:48] [500] global_step=500, grad_norm=0.6286103129386902, loss=5.8002471923828125
I0215 14:43:52.360501 139518492497664 logging_writer.py:48] [600] global_step=600, grad_norm=0.49764856696128845, loss=5.80533504486084
I0215 14:45:21.037035 139518484104960 logging_writer.py:48] [700] global_step=700, grad_norm=0.42585980892181396, loss=5.800042629241943
I0215 14:46:54.505473 139518492497664 logging_writer.py:48] [800] global_step=800, grad_norm=0.579330325126648, loss=5.780876159667969
I0215 14:48:22.546731 139518484104960 logging_writer.py:48] [900] global_step=900, grad_norm=0.713940441608429, loss=5.7713942527771
I0215 14:49:51.322939 139518492497664 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.2752150595188141, loss=5.7827372550964355
I0215 14:51:16.191755 139578135525120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.3402789831161499, loss=5.778223514556885
I0215 14:52:32.803134 139578127132416 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9510720372200012, loss=5.793951034545898
I0215 14:53:54.031187 139578135525120 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.39018091559410095, loss=5.799106121063232
I0215 14:55:19.983075 139578127132416 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9166831374168396, loss=5.786224842071533
I0215 14:56:46.371464 139578135525120 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7473002076148987, loss=5.776411533355713
I0215 14:58:17.542656 139578127132416 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.3506128787994385, loss=5.707302570343018
I0215 14:59:27.037736 139688679413568 spec.py:321] Evaluating on the training split.
I0215 15:00:05.991052 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 15:00:54.489034 139688679413568 spec.py:349] Evaluating on the test split.
I0215 15:01:19.113415 139688679413568 submission_runner.py:408] Time since start: 1705.58s, 	Step: 1680, 	{'train/ctc_loss': Array(6.382445, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(6.4972925, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.5108743, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1475.8832149505615, 'total_duration': 1705.5838894844055, 'accumulated_submission_time': 1475.8832149505615, 'accumulated_eval_time': 229.59972071647644, 'accumulated_logging_time': 0.03151512145996094}
I0215 15:01:19.148703 139578135525120 logging_writer.py:48] [1680] accumulated_eval_time=229.599721, accumulated_logging_time=0.031515, accumulated_submission_time=1475.883215, global_step=1680, preemption_count=0, score=1475.883215, test/ctc_loss=6.510874271392822, test/num_examples=2472, test/wer=0.899580, total_duration=1705.583889, train/ctc_loss=6.382444858551025, train/wer=0.939190, validation/ctc_loss=6.497292518615723, validation/num_examples=5348, validation/wer=0.896618
I0215 15:01:35.317724 139578127132416 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8767745494842529, loss=5.584002494812012
I0215 15:02:51.702609 139578135525120 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.6480998992919922, loss=5.455385208129883
I0215 15:04:09.927475 139578127132416 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.041231632232666, loss=5.270468711853027
I0215 15:05:40.435541 139578135525120 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.117654800415039, loss=4.661880970001221
I0215 15:07:08.516368 139578135525120 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1740140914916992, loss=4.11597204208374
I0215 15:08:29.247140 139578127132416 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.9378835558891296, loss=3.7633156776428223
I0215 15:09:50.164217 139578135525120 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.3249353170394897, loss=3.482794761657715
I0215 15:11:12.932391 139578127132416 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.1400748491287231, loss=3.3512730598449707
I0215 15:12:41.670714 139578135525120 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9284381866455078, loss=3.1851646900177
I0215 15:14:11.389981 139578127132416 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9307014346122742, loss=3.062694787979126
I0215 15:15:42.901938 139578135525120 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0016719102859497, loss=3.0111052989959717
I0215 15:17:14.043872 139578127132416 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9766339063644409, loss=2.913301706314087
I0215 15:18:43.209019 139578135525120 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.1352163553237915, loss=2.8247528076171875
I0215 15:20:16.295004 139578127132416 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0535343885421753, loss=2.7791357040405273
I0215 15:21:47.375683 139578135525120 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9934089183807373, loss=2.6860921382904053
I0215 15:23:04.091931 139578127132416 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0024523735046387, loss=2.6180620193481445
I0215 15:24:22.472998 139578135525120 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1891380548477173, loss=2.609672784805298
I0215 15:25:19.650396 139688679413568 spec.py:321] Evaluating on the training split.
I0215 15:26:03.966508 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 15:26:53.719208 139688679413568 spec.py:349] Evaluating on the test split.
I0215 15:27:20.818230 139688679413568 submission_runner.py:408] Time since start: 3267.29s, 	Step: 3371, 	{'train/ctc_loss': Array(3.8691866, dtype=float32), 'train/wer': 0.7855008984725966, 'validation/ctc_loss': Array(3.8361292, dtype=float32), 'validation/wer': 0.7482549214593973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.5268924, dtype=float32), 'test/wer': 0.6986777161659862, 'test/num_examples': 2472, 'score': 2916.303174495697, 'total_duration': 3267.289516210556, 'accumulated_submission_time': 2916.303174495697, 'accumulated_eval_time': 350.76149916648865, 'accumulated_logging_time': 0.0831305980682373}
I0215 15:27:20.853848 139578135525120 logging_writer.py:48] [3371] accumulated_eval_time=350.761499, accumulated_logging_time=0.083131, accumulated_submission_time=2916.303174, global_step=3371, preemption_count=0, score=2916.303174, test/ctc_loss=3.5268924236297607, test/num_examples=2472, test/wer=0.698678, total_duration=3267.289516, train/ctc_loss=3.8691866397857666, train/wer=0.785501, validation/ctc_loss=3.8361291885375977, validation/num_examples=5348, validation/wer=0.748255
I0215 15:27:43.676161 139578127132416 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.1262168884277344, loss=2.5445549488067627
I0215 15:28:59.711414 139578135525120 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9838374257087708, loss=2.545867443084717
I0215 15:30:17.373631 139578127132416 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.159591555595398, loss=2.462282419204712
I0215 15:31:49.454157 139578135525120 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.2013037204742432, loss=2.4296042919158936
I0215 15:33:18.006193 139578127132416 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0702555179595947, loss=2.358818769454956
I0215 15:34:49.188221 139578135525120 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9803295135498047, loss=2.379566192626953
I0215 15:36:22.084332 139578127132416 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.1276942491531372, loss=2.351778745651245
I0215 15:37:55.799709 139578135525120 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.18091881275177, loss=2.2894723415374756
I0215 15:39:17.734213 139578135525120 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9808955192565918, loss=2.2968621253967285
I0215 15:40:36.766471 139578127132416 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.2973836660385132, loss=2.232734203338623
I0215 15:42:00.049630 139578135525120 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8927989602088928, loss=2.26735258102417
I0215 15:43:26.740220 139578127132416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9787705540657043, loss=2.104168176651001
I0215 15:44:55.183687 139578135525120 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9498200416564941, loss=2.1190989017486572
I0215 15:46:27.536851 139578127132416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8958034515380859, loss=2.120828628540039
I0215 15:47:58.659817 139578135525120 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.875097930431366, loss=2.070605516433716
I0215 15:49:29.797394 139578127132416 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8643679022789001, loss=2.0398871898651123
I0215 15:50:57.877030 139578135525120 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9055060744285583, loss=2.121084690093994
I0215 15:51:21.467381 139688679413568 spec.py:321] Evaluating on the training split.
I0215 15:52:16.526556 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 15:53:11.287085 139688679413568 spec.py:349] Evaluating on the test split.
I0215 15:53:38.340095 139688679413568 submission_runner.py:408] Time since start: 4844.81s, 	Step: 5028, 	{'train/ctc_loss': Array(1.2378957, dtype=float32), 'train/wer': 0.3726033340564353, 'validation/ctc_loss': Array(1.2712711, dtype=float32), 'validation/wer': 0.359597207874335, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.95981365, dtype=float32), 'test/wer': 0.30034732801169944, 'test/num_examples': 2472, 'score': 4356.832535982132, 'total_duration': 4844.810940742493, 'accumulated_submission_time': 4356.832535982132, 'accumulated_eval_time': 487.6277189254761, 'accumulated_logging_time': 0.13818788528442383}
I0215 15:53:38.372457 139578135525120 logging_writer.py:48] [5028] accumulated_eval_time=487.627719, accumulated_logging_time=0.138188, accumulated_submission_time=4356.832536, global_step=5028, preemption_count=0, score=4356.832536, test/ctc_loss=0.95981365442276, test/num_examples=2472, test/wer=0.300347, total_duration=4844.810941, train/ctc_loss=1.2378957271575928, train/wer=0.372603, validation/ctc_loss=1.2712711095809937, validation/num_examples=5348, validation/wer=0.359597
I0215 15:54:33.912191 139578127132416 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8724865913391113, loss=1.9885438680648804
I0215 15:55:54.332641 139578135525120 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.0762196779251099, loss=1.9506179094314575
I0215 15:57:14.370176 139578127132416 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9896116256713867, loss=2.0321195125579834
I0215 15:58:38.155867 139578135525120 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8707035183906555, loss=1.9917476177215576
I0215 16:00:04.037124 139578127132416 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9518536329269409, loss=1.9531325101852417
I0215 16:01:35.595304 139578135525120 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7833541631698608, loss=1.9499492645263672
I0215 16:03:07.003117 139578127132416 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8617293238639832, loss=1.9003616571426392
I0215 16:04:40.768641 139578135525120 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9190252423286438, loss=1.8860849142074585
I0215 16:06:09.210209 139578127132416 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8679101467132568, loss=1.8886510133743286
I0215 16:07:39.540585 139578135525120 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.726646900177002, loss=1.8028966188430786
I0215 16:09:08.111122 139578127132416 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.741446852684021, loss=1.8699171543121338
I0215 16:10:41.517819 139578135525120 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8044547438621521, loss=1.821487545967102
I0215 16:12:00.221530 139578127132416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7928609251976013, loss=1.8546042442321777
I0215 16:13:21.566803 139578135525120 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.037489652633667, loss=1.8477199077606201
I0215 16:14:43.990445 139578127132416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.9111254811286926, loss=1.909242868423462
I0215 16:16:11.445846 139578135525120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7376214861869812, loss=1.843488097190857
I0215 16:17:39.716310 139688679413568 spec.py:321] Evaluating on the training split.
I0215 16:18:34.113713 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 16:19:28.614866 139688679413568 spec.py:349] Evaluating on the test split.
I0215 16:19:57.597849 139688679413568 submission_runner.py:408] Time since start: 6424.07s, 	Step: 6698, 	{'train/ctc_loss': Array(0.73739135, dtype=float32), 'train/wer': 0.2415693151924872, 'validation/ctc_loss': Array(0.87928903, dtype=float32), 'validation/wer': 0.26074321519256205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6022405, dtype=float32), 'test/wer': 0.19952064672069547, 'test/num_examples': 2472, 'score': 5798.092703580856, 'total_duration': 6424.067903518677, 'accumulated_submission_time': 5798.092703580856, 'accumulated_eval_time': 625.5019948482513, 'accumulated_logging_time': 0.18807625770568848}
I0215 16:19:57.634019 139578135525120 logging_writer.py:48] [6698] accumulated_eval_time=625.501995, accumulated_logging_time=0.188076, accumulated_submission_time=5798.092704, global_step=6698, preemption_count=0, score=5798.092704, test/ctc_loss=0.6022405028343201, test/num_examples=2472, test/wer=0.199521, total_duration=6424.067904, train/ctc_loss=0.7373913526535034, train/wer=0.241569, validation/ctc_loss=0.8792890310287476, validation/num_examples=5348, validation/wer=0.260743
I0215 16:20:00.016926 139578127132416 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7972412109375, loss=1.798822045326233
I0215 16:21:15.967949 139578135525120 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7492715120315552, loss=1.8344950675964355
I0215 16:22:33.536768 139578127132416 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9215514063835144, loss=1.7796534299850464
I0215 16:24:04.451123 139578135525120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6851262450218201, loss=1.7866004705429077
I0215 16:25:33.652113 139578127132416 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7426073551177979, loss=1.7662838697433472
I0215 16:27:05.240063 139578135525120 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8636534810066223, loss=1.7585701942443848
I0215 16:28:28.554662 139578135525120 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.781459629535675, loss=1.782799243927002
I0215 16:29:48.461516 139578127132416 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.9234576225280762, loss=1.7418053150177002
I0215 16:31:10.031177 139578135525120 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.9292816519737244, loss=1.752505898475647
I0215 16:32:37.505937 139578127132416 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8349205851554871, loss=1.7281570434570312
I0215 16:34:07.223239 139578135525120 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8977904319763184, loss=1.737648367881775
I0215 16:35:37.736135 139578127132416 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8648126721382141, loss=1.7270182371139526
I0215 16:37:07.502773 139578135525120 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7918668389320374, loss=1.737460970878601
I0215 16:38:37.093327 139578127132416 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8332035541534424, loss=1.7319624423980713
I0215 16:40:09.809123 139578135525120 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7633527517318726, loss=1.6889318227767944
I0215 16:41:38.119254 139578127132416 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.8392152786254883, loss=1.758833885192871
I0215 16:43:02.795840 139578135525120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7862715721130371, loss=1.7298915386199951
I0215 16:43:58.088345 139688679413568 spec.py:321] Evaluating on the training split.
I0215 16:44:51.533625 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 16:45:45.275944 139688679413568 spec.py:349] Evaluating on the test split.
I0215 16:46:13.527289 139688679413568 submission_runner.py:408] Time since start: 8000.00s, 	Step: 8371, 	{'train/ctc_loss': Array(0.6227412, dtype=float32), 'train/wer': 0.20609788163204645, 'validation/ctc_loss': Array(0.7817897, dtype=float32), 'validation/wer': 0.23225233401237727, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5196976, dtype=float32), 'test/wer': 0.17047508784758192, 'test/num_examples': 2472, 'score': 7238.464210033417, 'total_duration': 7999.998436927795, 'accumulated_submission_time': 7238.464210033417, 'accumulated_eval_time': 760.9348003864288, 'accumulated_logging_time': 0.24220538139343262}
I0215 16:46:13.565373 139578135525120 logging_writer.py:48] [8371] accumulated_eval_time=760.934800, accumulated_logging_time=0.242205, accumulated_submission_time=7238.464210, global_step=8371, preemption_count=0, score=7238.464210, test/ctc_loss=0.5196976065635681, test/num_examples=2472, test/wer=0.170475, total_duration=7999.998437, train/ctc_loss=0.6227412223815918, train/wer=0.206098, validation/ctc_loss=0.7817897200584412, validation/num_examples=5348, validation/wer=0.232252
I0215 16:46:36.387306 139578127132416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7845158576965332, loss=1.5976330041885376
I0215 16:47:52.053902 139578135525120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7422435283660889, loss=1.7017266750335693
I0215 16:49:07.726227 139578127132416 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7679508328437805, loss=1.6866158246994019
I0215 16:50:29.605961 139578135525120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7511820793151855, loss=1.66831374168396
I0215 16:51:58.726812 139578127132416 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.8295415043830872, loss=1.6031662225723267
I0215 16:53:29.921876 139578135525120 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6316370964050293, loss=1.7129528522491455
I0215 16:55:01.411456 139578127132416 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6521022915840149, loss=1.6038316488265991
I0215 16:56:33.677666 139578135525120 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6591899394989014, loss=1.6543055772781372
I0215 16:58:04.688788 139578127132416 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7524528503417969, loss=1.6855623722076416
I0215 16:59:35.696875 139578135525120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6967840194702148, loss=1.625110149383545
I0215 17:00:54.666843 139578127132416 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6188908815383911, loss=1.5731145143508911
I0215 17:02:15.976763 139578135525120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8817965388298035, loss=1.5749008655548096
I0215 17:03:38.435859 139578127132416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7071505188941956, loss=1.6535578966140747
I0215 17:05:02.922933 139578135525120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.9193029403686523, loss=1.6524466276168823
I0215 17:06:34.892768 139578127132416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.747525155544281, loss=1.5802192687988281
I0215 17:08:05.032233 139578135525120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9162703156471252, loss=1.6197731494903564
I0215 17:09:35.800815 139578127132416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6873137950897217, loss=1.5699039697647095
I0215 17:10:14.150318 139688679413568 spec.py:321] Evaluating on the training split.
I0215 17:11:08.145084 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 17:12:02.450361 139688679413568 spec.py:349] Evaluating on the test split.
I0215 17:12:29.640449 139688679413568 submission_runner.py:408] Time since start: 9576.11s, 	Step: 10043, 	{'train/ctc_loss': Array(0.6011116, dtype=float32), 'train/wer': 0.20390464054965843, 'validation/ctc_loss': Array(0.70295954, dtype=float32), 'validation/wer': 0.21234443940257006, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4573212, dtype=float32), 'test/wer': 0.15495704100907928, 'test/num_examples': 2472, 'score': 8678.964599847794, 'total_duration': 9576.111248016357, 'accumulated_submission_time': 8678.964599847794, 'accumulated_eval_time': 896.4183924198151, 'accumulated_logging_time': 0.2991371154785156}
I0215 17:12:29.678370 139578135525120 logging_writer.py:48] [10043] accumulated_eval_time=896.418392, accumulated_logging_time=0.299137, accumulated_submission_time=8678.964600, global_step=10043, preemption_count=0, score=8678.964600, test/ctc_loss=0.4573211967945099, test/num_examples=2472, test/wer=0.154957, total_duration=9576.111248, train/ctc_loss=0.6011115908622742, train/wer=0.203905, validation/ctc_loss=0.7029595375061035, validation/num_examples=5348, validation/wer=0.212344
I0215 17:13:13.535916 139578127132416 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7272462844848633, loss=1.5829484462738037
I0215 17:14:29.209067 139578135525120 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8089655041694641, loss=1.6005779504776
I0215 17:15:58.455306 139578135525120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6763893961906433, loss=1.6463946104049683
I0215 17:17:16.672523 139578127132416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6489172577857971, loss=1.5460777282714844
I0215 17:18:37.411996 139578135525120 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7065934538841248, loss=1.583003282546997
I0215 17:20:00.438879 139578127132416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6401961445808411, loss=1.5681865215301514
I0215 17:21:26.761032 139578135525120 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7066749930381775, loss=1.6124647855758667
I0215 17:22:58.225723 139578127132416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8029325008392334, loss=1.618220567703247
I0215 17:24:32.048281 139578135525120 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6301494836807251, loss=1.5524967908859253
I0215 17:26:01.075084 139578127132416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6171066761016846, loss=1.5723903179168701
I0215 17:27:27.889307 139578135525120 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5664770603179932, loss=1.5439826250076294
I0215 17:28:57.460222 139578127132416 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5974211692810059, loss=1.5541248321533203
I0215 17:30:28.195879 139578135525120 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.662452220916748, loss=1.6055243015289307
I0215 17:31:53.032140 139578135525120 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7106155753135681, loss=1.5441360473632812
I0215 17:33:11.595570 139578127132416 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8303741216659546, loss=1.5332142114639282
I0215 17:34:33.017402 139578135525120 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5537313222885132, loss=1.5370492935180664
I0215 17:35:58.221819 139578127132416 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7547789812088013, loss=1.5421119928359985
I0215 17:36:29.740496 139688679413568 spec.py:321] Evaluating on the training split.
I0215 17:37:22.889190 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 17:38:16.167958 139688679413568 spec.py:349] Evaluating on the test split.
I0215 17:38:43.229950 139688679413568 submission_runner.py:408] Time since start: 11149.70s, 	Step: 11739, 	{'train/ctc_loss': Array(0.51052195, dtype=float32), 'train/wer': 0.17512704591602068, 'validation/ctc_loss': Array(0.6549365, dtype=float32), 'validation/wer': 0.19736041785338443, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4183106, dtype=float32), 'test/wer': 0.14236386163751955, 'test/num_examples': 2472, 'score': 10118.942187547684, 'total_duration': 11149.701045513153, 'accumulated_submission_time': 10118.942187547684, 'accumulated_eval_time': 1029.9016468524933, 'accumulated_logging_time': 0.35513925552368164}
I0215 17:38:43.263067 139578135525120 logging_writer.py:48] [11739] accumulated_eval_time=1029.901647, accumulated_logging_time=0.355139, accumulated_submission_time=10118.942188, global_step=11739, preemption_count=0, score=10118.942188, test/ctc_loss=0.41831061244010925, test/num_examples=2472, test/wer=0.142364, total_duration=11149.701046, train/ctc_loss=0.5105219483375549, train/wer=0.175127, validation/ctc_loss=0.6549364924430847, validation/num_examples=5348, validation/wer=0.197360
I0215 17:39:30.183682 139578127132416 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.8053733706474304, loss=1.5729247331619263
I0215 17:40:45.908610 139578135525120 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7383880019187927, loss=1.5768516063690186
I0215 17:42:05.460788 139578127132416 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6672674417495728, loss=1.5847591161727905
I0215 17:43:37.133017 139578135525120 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.0894945859909058, loss=1.4579503536224365
I0215 17:45:08.057205 139578127132416 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7856410145759583, loss=1.551953673362732
I0215 17:46:39.004209 139578135525120 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6688399314880371, loss=1.5340487957000732
I0215 17:48:07.730784 139578135525120 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.7263731956481934, loss=1.5197339057922363
I0215 17:49:23.836560 139578127132416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8452029824256897, loss=1.5732980966567993
I0215 17:50:43.222660 139578135525120 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7930396795272827, loss=1.4799485206604004
I0215 17:52:07.578764 139578127132416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.8802599906921387, loss=1.5730957984924316
I0215 17:53:30.340991 139578135525120 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.683746874332428, loss=1.5700186491012573
I0215 17:54:59.304299 139578127132416 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.590064525604248, loss=1.555587887763977
I0215 17:56:29.649652 139578135525120 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6921102404594421, loss=1.4742085933685303
I0215 17:57:59.465559 139578127132416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9073920845985413, loss=1.567605972290039
I0215 17:59:32.389983 139578135525120 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.7197524309158325, loss=1.523316740989685
I0215 18:01:04.865310 139578127132416 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.8397630453109741, loss=1.6018567085266113
I0215 18:02:38.228784 139578135525120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6836006045341492, loss=1.521539330482483
I0215 18:02:43.454651 139688679413568 spec.py:321] Evaluating on the training split.
I0215 18:03:38.373472 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 18:04:32.871252 139688679413568 spec.py:349] Evaluating on the test split.
I0215 18:05:00.262580 139688679413568 submission_runner.py:408] Time since start: 12726.73s, 	Step: 13408, 	{'train/ctc_loss': Array(0.51378894, dtype=float32), 'train/wer': 0.1716898590868122, 'validation/ctc_loss': Array(0.63236403, dtype=float32), 'validation/wer': 0.19217586916014173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40097797, dtype=float32), 'test/wer': 0.13553917088131945, 'test/num_examples': 2472, 'score': 11559.051738500595, 'total_duration': 12726.73373579979, 'accumulated_submission_time': 11559.051738500595, 'accumulated_eval_time': 1166.7034318447113, 'accumulated_logging_time': 0.40659093856811523}
I0215 18:05:00.299350 139578135525120 logging_writer.py:48] [13408] accumulated_eval_time=1166.703432, accumulated_logging_time=0.406591, accumulated_submission_time=11559.051739, global_step=13408, preemption_count=0, score=11559.051739, test/ctc_loss=0.4009779691696167, test/num_examples=2472, test/wer=0.135539, total_duration=12726.733736, train/ctc_loss=0.5137889385223389, train/wer=0.171690, validation/ctc_loss=0.63236403465271, validation/num_examples=5348, validation/wer=0.192176
I0215 18:06:10.562268 139578127132416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.813095211982727, loss=1.5277875661849976
I0215 18:07:26.385054 139578135525120 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.243605136871338, loss=1.5691401958465576
I0215 18:08:42.048162 139578127132416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.8669830560684204, loss=1.5207206010818481
I0215 18:09:58.248189 139578135525120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7256714105606079, loss=1.4824392795562744
I0215 18:11:27.140831 139578127132416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5971004962921143, loss=1.4598301649093628
I0215 18:12:58.808263 139578135525120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.640404999256134, loss=1.5112701654434204
I0215 18:14:27.395248 139578127132416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.7333317399024963, loss=1.5031371116638184
I0215 18:15:59.434597 139578135525120 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7511292099952698, loss=1.531725287437439
I0215 18:17:32.052547 139578127132416 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6296381950378418, loss=1.5144224166870117
I0215 18:19:03.132775 139578135525120 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6659236550331116, loss=1.5320940017700195
I0215 18:20:25.742078 139578135525120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7224227786064148, loss=1.5565637350082397
I0215 18:21:44.301317 139578127132416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5817572474479675, loss=1.4352169036865234
I0215 18:23:07.425608 139578135525120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.5486084818840027, loss=1.458655834197998
I0215 18:24:31.813231 139578127132416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6856817007064819, loss=1.4289823770523071
I0215 18:26:03.306219 139578135525120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6050013303756714, loss=1.5217251777648926
I0215 18:27:33.291862 139578127132416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6653488278388977, loss=1.4455022811889648
I0215 18:29:01.092404 139688679413568 spec.py:321] Evaluating on the training split.
I0215 18:29:55.639074 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 18:30:49.582276 139688679413568 spec.py:349] Evaluating on the test split.
I0215 18:31:17.217640 139688679413568 submission_runner.py:408] Time since start: 14303.69s, 	Step: 15097, 	{'train/ctc_loss': Array(0.42208117, dtype=float32), 'train/wer': 0.14745705920803281, 'validation/ctc_loss': Array(0.6005494, dtype=float32), 'validation/wer': 0.18247294283479923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37557527, dtype=float32), 'test/wer': 0.12836918327138302, 'test/num_examples': 2472, 'score': 12999.760911226273, 'total_duration': 14303.689206123352, 'accumulated_submission_time': 12999.760911226273, 'accumulated_eval_time': 1302.8229024410248, 'accumulated_logging_time': 0.46058177947998047}
I0215 18:31:17.256125 139578135525120 logging_writer.py:48] [15097] accumulated_eval_time=1302.822902, accumulated_logging_time=0.460582, accumulated_submission_time=12999.760911, global_step=15097, preemption_count=0, score=12999.760911, test/ctc_loss=0.3755752742290497, test/num_examples=2472, test/wer=0.128369, total_duration=14303.689206, train/ctc_loss=0.4220811724662781, train/wer=0.147457, validation/ctc_loss=0.6005493998527527, validation/num_examples=5348, validation/wer=0.182473
I0215 18:31:20.386954 139578127132416 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.7362664937973022, loss=1.4814746379852295
I0215 18:32:36.051599 139578135525120 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7330555319786072, loss=1.4980660676956177
I0215 18:33:52.027529 139578127132416 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.7000144124031067, loss=1.4852076768875122
I0215 18:35:22.413756 139578135525120 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.7503088116645813, loss=1.4987640380859375
I0215 18:36:52.360016 139578135525120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6407614350318909, loss=1.4646859169006348
I0215 18:38:12.735952 139578127132416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.585415780544281, loss=1.3901370763778687
I0215 18:39:34.807449 139578135525120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6820771098136902, loss=1.474515438079834
I0215 18:40:58.496434 139578127132416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7252482175827026, loss=1.4409905672073364
I0215 18:42:24.682154 139578135525120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.651774525642395, loss=1.4551416635513306
I0215 18:43:54.700187 139578127132416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7605533003807068, loss=1.4813541173934937
I0215 18:45:24.384521 139578135525120 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7275498509407043, loss=1.425062894821167
I0215 18:46:54.191102 139578127132416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6858623623847961, loss=1.4832572937011719
I0215 18:48:26.215498 139578135525120 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.797142744064331, loss=1.4332743883132935
I0215 18:49:54.909821 139578127132416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.8540672659873962, loss=1.4708285331726074
I0215 18:51:28.251063 139578135525120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6796545386314392, loss=1.489194393157959
I0215 18:52:48.523644 139578127132416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6788989901542664, loss=1.409651756286621
I0215 18:54:09.030782 139578135525120 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.7998547554016113, loss=1.4295098781585693
I0215 18:55:17.614842 139688679413568 spec.py:321] Evaluating on the training split.
I0215 18:56:10.823720 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 18:57:04.574340 139688679413568 spec.py:349] Evaluating on the test split.
I0215 18:57:32.084805 139688679413568 submission_runner.py:408] Time since start: 15878.56s, 	Step: 16782, 	{'train/ctc_loss': Array(0.42158687, dtype=float32), 'train/wer': 0.1475592811097697, 'validation/ctc_loss': Array(0.5759482, dtype=float32), 'validation/wer': 0.17342653291753962, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35843527, dtype=float32), 'test/wer': 0.11979769666686978, 'test/num_examples': 2472, 'score': 14440.03445315361, 'total_duration': 15878.557571172714, 'accumulated_submission_time': 14440.03445315361, 'accumulated_eval_time': 1437.2882940769196, 'accumulated_logging_time': 0.5167298316955566}
I0215 18:57:32.114387 139578135525120 logging_writer.py:48] [16782] accumulated_eval_time=1437.288294, accumulated_logging_time=0.516730, accumulated_submission_time=14440.034453, global_step=16782, preemption_count=0, score=14440.034453, test/ctc_loss=0.3584352731704712, test/num_examples=2472, test/wer=0.119798, total_duration=15878.557571, train/ctc_loss=0.42158687114715576, train/wer=0.147559, validation/ctc_loss=0.575948178768158, validation/num_examples=5348, validation/wer=0.173427
I0215 18:57:46.484350 139578127132416 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6319092512130737, loss=1.436136245727539
I0215 18:59:02.154954 139578135525120 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.9335909485816956, loss=1.4325330257415771
I0215 19:00:17.828703 139578127132416 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5980709791183472, loss=1.4317609071731567
I0215 19:01:46.945558 139578135525120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.7105359435081482, loss=1.4368149042129517
I0215 19:03:18.624182 139578127132416 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6693811416625977, loss=1.4680125713348389
I0215 19:04:47.125608 139578135525120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5927224159240723, loss=1.4173362255096436
I0215 19:06:19.395930 139578127132416 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6827832460403442, loss=1.429862141609192
I0215 19:07:49.924523 139578135525120 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6940456628799438, loss=1.4448723793029785
I0215 19:09:11.635356 139578135525120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.7341216206550598, loss=1.421456217765808
I0215 19:10:28.881703 139578127132416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7668637037277222, loss=1.4644149541854858
I0215 19:11:49.750974 139578135525120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6302365064620972, loss=1.413909912109375
I0215 19:13:11.246765 139578127132416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7690947651863098, loss=1.4141671657562256
I0215 19:14:40.405359 139578135525120 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7264907360076904, loss=1.419685959815979
I0215 19:16:02.710943 139578127132416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7182736992835999, loss=1.4436724185943604
I0215 19:17:25.393077 139578135525120 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6307541131973267, loss=1.462471604347229
I0215 19:18:47.945600 139578127132416 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.7337823510169983, loss=1.412782073020935
I0215 19:20:10.257974 139578135525120 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.7464896440505981, loss=1.4511914253234863
I0215 19:21:31.684212 139578127132416 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6558994650840759, loss=1.444342851638794
I0215 19:21:32.160016 139688679413568 spec.py:321] Evaluating on the training split.
I0215 19:22:23.794020 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 19:23:14.212861 139688679413568 spec.py:349] Evaluating on the test split.
I0215 19:23:40.132725 139688679413568 submission_runner.py:408] Time since start: 17446.60s, 	Step: 18502, 	{'train/ctc_loss': Array(0.40602422, dtype=float32), 'train/wer': 0.14388951610198858, 'validation/ctc_loss': Array(0.56187373, dtype=float32), 'validation/wer': 0.16890815528543981, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.346281, dtype=float32), 'test/wer': 0.11701501025734772, 'test/num_examples': 2472, 'score': 15880.001097917557, 'total_duration': 17446.604358911514, 'accumulated_submission_time': 15880.001097917557, 'accumulated_eval_time': 1565.2552840709686, 'accumulated_logging_time': 0.5599899291992188}
I0215 19:23:40.170736 139578135525120 logging_writer.py:48] [18502] accumulated_eval_time=1565.255284, accumulated_logging_time=0.559990, accumulated_submission_time=15880.001098, global_step=18502, preemption_count=0, score=15880.001098, test/ctc_loss=0.3462809920310974, test/num_examples=2472, test/wer=0.117015, total_duration=17446.604359, train/ctc_loss=0.4060242176055908, train/wer=0.143890, validation/ctc_loss=0.561873733997345, validation/num_examples=5348, validation/wer=0.168908
I0215 19:24:58.180712 139578135525120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6569164991378784, loss=1.3599238395690918
I0215 19:26:13.928215 139578127132416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6452099680900574, loss=1.4904857873916626
I0215 19:27:29.686894 139578135525120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6206173300743103, loss=1.3913377523422241
I0215 19:28:45.527764 139578127132416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7340102195739746, loss=1.4159728288650513
I0215 19:30:01.401602 139578135525120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6957356929779053, loss=1.3939824104309082
I0215 19:31:17.486276 139578127132416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.7944273948669434, loss=1.4051018953323364
I0215 19:32:33.306073 139578135525120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6546835899353027, loss=1.384897232055664
I0215 19:33:50.536515 139578127132416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6524192690849304, loss=1.389380931854248
I0215 19:35:13.560333 139578135525120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.6056625843048096, loss=1.4275075197219849
I0215 19:36:36.107691 139578127132416 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6539252400398254, loss=1.4676610231399536
I0215 19:37:59.916007 139578135525120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.6715254187583923, loss=1.39722740650177
I0215 19:39:15.613197 139578127132416 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6268025040626526, loss=1.3875068426132202
I0215 19:40:31.346021 139578135525120 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.682144284248352, loss=1.410155177116394
I0215 19:41:47.038796 139578127132416 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.7131003737449646, loss=1.4598509073257446
I0215 19:43:02.749599 139578135525120 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6273115277290344, loss=1.3502146005630493
I0215 19:44:18.494636 139578127132416 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.5584490895271301, loss=1.3922301530838013
I0215 19:45:34.425355 139578135525120 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.590070366859436, loss=1.3952103853225708
I0215 19:46:50.169335 139578127132416 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.6565829515457153, loss=1.4687693119049072
I0215 19:47:40.694422 139688679413568 spec.py:321] Evaluating on the training split.
I0215 19:48:42.994393 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 19:49:32.435967 139688679413568 spec.py:349] Evaluating on the test split.
I0215 19:49:57.733979 139688679413568 submission_runner.py:408] Time since start: 19024.21s, 	Step: 20363, 	{'train/ctc_loss': Array(0.26609316, dtype=float32), 'train/wer': 0.0986750889965472, 'validation/ctc_loss': Array(0.5398281, dtype=float32), 'validation/wer': 0.16431254042885968, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32706234, dtype=float32), 'test/wer': 0.11146994901793512, 'test/num_examples': 2472, 'score': 17320.446226596832, 'total_duration': 19024.20640349388, 'accumulated_submission_time': 17320.446226596832, 'accumulated_eval_time': 1702.2899096012115, 'accumulated_logging_time': 0.6148602962493896}
I0215 19:49:57.766702 139578135525120 logging_writer.py:48] [20363] accumulated_eval_time=1702.289910, accumulated_logging_time=0.614860, accumulated_submission_time=17320.446227, global_step=20363, preemption_count=0, score=17320.446227, test/ctc_loss=0.32706233859062195, test/num_examples=2472, test/wer=0.111470, total_duration=19024.206403, train/ctc_loss=0.2660931646823883, train/wer=0.098675, validation/ctc_loss=0.5398281216621399, validation/num_examples=5348, validation/wer=0.164313
I0215 19:50:26.481034 139578127132416 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.0124903917312622, loss=1.4429891109466553
I0215 19:51:42.075386 139578135525120 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6269710659980774, loss=1.4538999795913696
I0215 19:53:01.009421 139578135525120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7354466915130615, loss=1.3997493982315063
I0215 19:54:16.576955 139578127132416 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.6741570830345154, loss=1.3658515214920044
I0215 19:55:32.100341 139578135525120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6355003714561462, loss=1.4685070514678955
I0215 19:56:47.708662 139578127132416 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.741769552230835, loss=1.398403525352478
I0215 19:58:03.430036 139578135525120 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.825761079788208, loss=1.427599549293518
I0215 19:59:19.155161 139578127132416 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.8374326229095459, loss=1.4230222702026367
I0215 20:00:34.876829 139578135525120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6849650144577026, loss=1.3591318130493164
I0215 20:01:55.692783 139578127132416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.8131148219108582, loss=1.367896556854248
I0215 20:03:17.345712 139578135525120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7415550351142883, loss=1.4180339574813843
I0215 20:04:39.402331 139578127132416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6947854161262512, loss=1.3930963277816772
I0215 20:06:00.975848 139578135525120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6490522027015686, loss=1.4036365747451782
I0215 20:07:20.825239 139578135525120 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6997730731964111, loss=1.4089362621307373
I0215 20:08:36.407483 139578127132416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.596005380153656, loss=1.3308018445968628
I0215 20:09:52.014590 139578135525120 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.6382540464401245, loss=1.3624988794326782
I0215 20:11:07.713413 139578127132416 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6622153520584106, loss=1.3876997232437134
I0215 20:12:23.416837 139578135525120 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5682936906814575, loss=1.3696484565734863
I0215 20:13:39.113643 139578127132416 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7071850895881653, loss=1.365685224533081
I0215 20:13:58.437159 139688679413568 spec.py:321] Evaluating on the training split.
I0215 20:14:53.114000 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 20:15:43.873397 139688679413568 spec.py:349] Evaluating on the test split.
I0215 20:16:09.304320 139688679413568 submission_runner.py:408] Time since start: 20595.78s, 	Step: 22227, 	{'train/ctc_loss': Array(0.2531626, dtype=float32), 'train/wer': 0.09213394914396124, 'validation/ctc_loss': Array(0.5354374, dtype=float32), 'validation/wer': 0.16039275128648253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32297328, dtype=float32), 'test/wer': 0.1081997846972559, 'test/num_examples': 2472, 'score': 18761.039699316025, 'total_duration': 20595.775464057922, 'accumulated_submission_time': 18761.039699316025, 'accumulated_eval_time': 1833.1509010791779, 'accumulated_logging_time': 0.6625058650970459}
I0215 20:16:09.337410 139578135525120 logging_writer.py:48] [22227] accumulated_eval_time=1833.150901, accumulated_logging_time=0.662506, accumulated_submission_time=18761.039699, global_step=22227, preemption_count=0, score=18761.039699, test/ctc_loss=0.3229732811450958, test/num_examples=2472, test/wer=0.108200, total_duration=20595.775464, train/ctc_loss=0.25316259264945984, train/wer=0.092134, validation/ctc_loss=0.5354374051094055, validation/num_examples=5348, validation/wer=0.160393
I0215 20:17:05.323925 139578127132416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.7123533487319946, loss=1.4165641069412231
I0215 20:18:21.131201 139578135525120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6253919005393982, loss=1.3620901107788086
I0215 20:19:37.113081 139578127132416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6084061861038208, loss=1.3855091333389282
I0215 20:20:52.877812 139578135525120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6345813274383545, loss=1.3690074682235718
I0215 20:22:12.187909 139578135525120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.678508460521698, loss=1.3492878675460815
I0215 20:23:27.879973 139578127132416 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.7678607106208801, loss=1.3449124097824097
I0215 20:24:43.565916 139578135525120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.7382919192314148, loss=1.3563543558120728
I0215 20:25:59.351952 139578127132416 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6948871612548828, loss=1.3815414905548096
I0215 20:27:15.131613 139578135525120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7432118058204651, loss=1.3874013423919678
I0215 20:28:30.855377 139578127132416 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.748354434967041, loss=1.3994790315628052
I0215 20:29:48.348324 139578135525120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8648593425750732, loss=1.3339753150939941
I0215 20:31:11.337111 139578127132416 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.6249033212661743, loss=1.3767457008361816
I0215 20:32:33.675518 139578135525120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6897647976875305, loss=1.2639055252075195
I0215 20:33:56.099865 139578127132416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6492406129837036, loss=1.2981321811676025
I0215 20:35:21.031236 139578135525120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5961959362030029, loss=1.3178716897964478
I0215 20:36:36.796104 139578127132416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.8545884490013123, loss=1.361440658569336
I0215 20:37:52.518572 139578135525120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.7847690582275391, loss=1.3639456033706665
I0215 20:39:08.278658 139578127132416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6917179226875305, loss=1.3390365839004517
I0215 20:40:09.923580 139688679413568 spec.py:321] Evaluating on the training split.
I0215 20:41:02.859651 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 20:41:53.114020 139688679413568 spec.py:349] Evaluating on the test split.
I0215 20:42:18.330169 139688679413568 submission_runner.py:408] Time since start: 22164.80s, 	Step: 24083, 	{'train/ctc_loss': Array(0.24372207, dtype=float32), 'train/wer': 0.09096693115433355, 'validation/ctc_loss': Array(0.516554, dtype=float32), 'validation/wer': 0.15621228651148422, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30685455, dtype=float32), 'test/wer': 0.10417809193020941, 'test/num_examples': 2472, 'score': 20201.548386335373, 'total_duration': 22164.801256656647, 'accumulated_submission_time': 20201.548386335373, 'accumulated_eval_time': 1961.5512776374817, 'accumulated_logging_time': 0.7109498977661133}
I0215 20:42:18.363400 139578135525120 logging_writer.py:48] [24083] accumulated_eval_time=1961.551278, accumulated_logging_time=0.710950, accumulated_submission_time=20201.548386, global_step=24083, preemption_count=0, score=20201.548386, test/ctc_loss=0.3068545460700989, test/num_examples=2472, test/wer=0.104178, total_duration=22164.801257, train/ctc_loss=0.243722066283226, train/wer=0.090967, validation/ctc_loss=0.5165539979934692, validation/num_examples=5348, validation/wer=0.156212
I0215 20:42:31.988111 139578127132416 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7664205431938171, loss=1.3440158367156982
I0215 20:43:47.569111 139578135525120 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7285385727882385, loss=1.3958579301834106
I0215 20:45:03.316554 139578127132416 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6760390996932983, loss=1.4007837772369385
I0215 20:46:19.034349 139578135525120 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8246349692344666, loss=1.3217815160751343
I0215 20:47:34.778287 139578127132416 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8465414643287659, loss=1.4340221881866455
I0215 20:48:54.301239 139578135525120 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6246148347854614, loss=1.3987524509429932
I0215 20:50:16.206521 139578127132416 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.663210928440094, loss=1.2991344928741455
I0215 20:51:36.584120 139578135525120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.5982820987701416, loss=1.2929165363311768
I0215 20:52:52.281657 139578127132416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.609011709690094, loss=1.3710503578186035
I0215 20:54:08.053292 139578135525120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6536498665809631, loss=1.347463607788086
I0215 20:55:23.864672 139578127132416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6102009415626526, loss=1.3111717700958252
I0215 20:56:39.573452 139578135525120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.7530762553215027, loss=1.338370680809021
I0215 20:57:55.426982 139578127132416 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.7909154891967773, loss=1.3834750652313232
I0215 20:59:16.225302 139578135525120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.720829963684082, loss=1.3470027446746826
I0215 21:00:39.357902 139578127132416 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.7644349336624146, loss=1.3637676239013672
I0215 21:02:01.758192 139578135525120 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6936721205711365, loss=1.3681962490081787
I0215 21:03:22.864914 139578127132416 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.7978706359863281, loss=1.3672058582305908
I0215 21:04:45.026178 139578135525120 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.7526752948760986, loss=1.3045083284378052
I0215 21:06:00.670862 139578127132416 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.7308089137077332, loss=1.2807241678237915
I0215 21:06:18.691389 139688679413568 spec.py:321] Evaluating on the training split.
I0215 21:07:11.984483 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 21:08:01.504032 139688679413568 spec.py:349] Evaluating on the test split.
I0215 21:08:27.032982 139688679413568 submission_runner.py:408] Time since start: 23733.51s, 	Step: 25925, 	{'train/ctc_loss': Array(0.2315767, dtype=float32), 'train/wer': 0.08636087293767426, 'validation/ctc_loss': Array(0.5072757, dtype=float32), 'validation/wer': 0.15324830802205122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29840583, dtype=float32), 'test/wer': 0.10157820973737128, 'test/num_examples': 2472, 'score': 21641.7920794487, 'total_duration': 23733.50555229187, 'accumulated_submission_time': 21641.7920794487, 'accumulated_eval_time': 2089.888088941574, 'accumulated_logging_time': 0.7663748264312744}
I0215 21:08:27.066723 139578135525120 logging_writer.py:48] [25925] accumulated_eval_time=2089.888089, accumulated_logging_time=0.766375, accumulated_submission_time=21641.792079, global_step=25925, preemption_count=0, score=21641.792079, test/ctc_loss=0.29840582609176636, test/num_examples=2472, test/wer=0.101578, total_duration=23733.505552, train/ctc_loss=0.23157669603824615, train/wer=0.086361, validation/ctc_loss=0.5072757005691528, validation/num_examples=5348, validation/wer=0.153248
I0215 21:09:24.501615 139578127132416 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6124630570411682, loss=1.33769953250885
I0215 21:10:40.173768 139578135525120 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.7271682620048523, loss=1.2751573324203491
I0215 21:11:55.950350 139578127132416 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6206992268562317, loss=1.3069955110549927
I0215 21:13:11.663244 139578135525120 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6972925066947937, loss=1.3392090797424316
I0215 21:14:27.451812 139578127132416 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.7409414052963257, loss=1.2776830196380615
I0215 21:15:43.200786 139578135525120 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6787323951721191, loss=1.2487151622772217
I0215 21:17:04.514634 139578127132416 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.8296661376953125, loss=1.33747398853302
I0215 21:18:26.388495 139578135525120 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6963092684745789, loss=1.365617036819458
I0215 21:19:49.993959 139578135525120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6736173033714294, loss=1.2979754209518433
I0215 21:21:05.741124 139578127132416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6575203537940979, loss=1.278678297996521
I0215 21:22:21.492707 139578135525120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7424006462097168, loss=1.2815498113632202
I0215 21:23:37.484264 139578127132416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6472188830375671, loss=1.2960957288742065
I0215 21:24:53.176873 139578135525120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.7234193086624146, loss=1.3286960124969482
I0215 21:26:08.954070 139578127132416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6938024759292603, loss=1.288062334060669
I0215 21:27:24.826144 139578135525120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.8277513980865479, loss=1.3194655179977417
I0215 21:28:46.872740 139578127132416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7938722372055054, loss=1.3313524723052979
I0215 21:30:09.268965 139578135525120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.9604712724685669, loss=1.3103610277175903
I0215 21:31:30.934096 139578127132416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.7371702194213867, loss=1.308752417564392
I0215 21:32:27.255690 139688679413568 spec.py:321] Evaluating on the training split.
I0215 21:33:20.521384 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 21:34:10.628639 139688679413568 spec.py:349] Evaluating on the test split.
I0215 21:34:36.236590 139688679413568 submission_runner.py:408] Time since start: 25302.71s, 	Step: 27770, 	{'train/ctc_loss': Array(0.2280684, dtype=float32), 'train/wer': 0.08473699921110424, 'validation/ctc_loss': Array(0.49400762, dtype=float32), 'validation/wer': 0.14941541075721443, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29091525, dtype=float32), 'test/wer': 0.09796274856295574, 'test/num_examples': 2472, 'score': 23081.903094768524, 'total_duration': 25302.70827627182, 'accumulated_submission_time': 23081.903094768524, 'accumulated_eval_time': 2218.8633258342743, 'accumulated_logging_time': 0.8157093524932861}
I0215 21:34:36.277079 139578135525120 logging_writer.py:48] [27770] accumulated_eval_time=2218.863326, accumulated_logging_time=0.815709, accumulated_submission_time=23081.903095, global_step=27770, preemption_count=0, score=23081.903095, test/ctc_loss=0.29091525077819824, test/num_examples=2472, test/wer=0.097963, total_duration=25302.708276, train/ctc_loss=0.22806839644908905, train/wer=0.084737, validation/ctc_loss=0.4940076172351837, validation/num_examples=5348, validation/wer=0.149415
I0215 21:34:59.738999 139578127132416 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6671195030212402, loss=1.3442708253860474
I0215 21:36:18.737533 139578135525120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6236372590065002, loss=1.2834488153457642
I0215 21:37:34.427853 139578127132416 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6906648278236389, loss=1.3448665142059326
I0215 21:38:50.117078 139578135525120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.8156644701957703, loss=1.2626944780349731
I0215 21:40:06.000528 139578127132416 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.7728302478790283, loss=1.3516918420791626
I0215 21:41:21.755342 139578135525120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.7632326483726501, loss=1.3451886177062988
I0215 21:42:40.842385 139578127132416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.6986554861068726, loss=1.3112609386444092
I0215 21:44:04.467386 139578135525120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.692933201789856, loss=1.2935528755187988
I0215 21:45:26.095507 139578127132416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.6870501637458801, loss=1.313608169555664
I0215 21:46:48.132352 139578135525120 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.6370866894721985, loss=1.2781736850738525
I0215 21:48:10.988957 139578127132416 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.8345162272453308, loss=1.3093754053115845
I0215 21:49:32.412297 139578135525120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.6295986771583557, loss=1.2881375551223755
I0215 21:50:48.204029 139578127132416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6146698594093323, loss=1.2673063278198242
I0215 21:52:03.819556 139578135525120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.7308197617530823, loss=1.2714639902114868
I0215 21:53:19.580296 139578127132416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.7089422941207886, loss=1.3086017370224
I0215 21:54:35.314403 139578135525120 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.7142603993415833, loss=1.3133600950241089
I0215 21:55:51.387251 139578127132416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.6376814246177673, loss=1.3452939987182617
I0215 21:57:09.145418 139578135525120 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.7194519639015198, loss=1.2670351266860962
I0215 21:58:31.937775 139578127132416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.7540799379348755, loss=1.2914540767669678
I0215 21:58:36.462937 139688679413568 spec.py:321] Evaluating on the training split.
I0215 21:59:30.639675 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 22:00:21.299277 139688679413568 spec.py:349] Evaluating on the test split.
I0215 22:00:47.563050 139688679413568 submission_runner.py:408] Time since start: 26874.03s, 	Step: 29607, 	{'train/ctc_loss': Array(0.21079952, dtype=float32), 'train/wer': 0.07880682626964503, 'validation/ctc_loss': Array(0.48705956, dtype=float32), 'validation/wer': 0.14607490079843982, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28334573, dtype=float32), 'test/wer': 0.09485507687932891, 'test/num_examples': 2472, 'score': 24522.007591724396, 'total_duration': 26874.034334897995, 'accumulated_submission_time': 24522.007591724396, 'accumulated_eval_time': 2349.9573764801025, 'accumulated_logging_time': 0.8724699020385742}
I0215 22:00:47.605869 139578135525120 logging_writer.py:48] [29607] accumulated_eval_time=2349.957376, accumulated_logging_time=0.872470, accumulated_submission_time=24522.007592, global_step=29607, preemption_count=0, score=24522.007592, test/ctc_loss=0.2833457291126251, test/num_examples=2472, test/wer=0.094855, total_duration=26874.034335, train/ctc_loss=0.21079951524734497, train/wer=0.078807, validation/ctc_loss=0.4870595633983612, validation/num_examples=5348, validation/wer=0.146075
I0215 22:01:58.664885 139578127132416 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.7895010113716125, loss=1.3536595106124878
I0215 22:03:14.442517 139578135525120 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.6956196427345276, loss=1.2640644311904907
I0215 22:04:33.799765 139578135525120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.9410611987113953, loss=1.2489705085754395
I0215 22:05:49.397667 139578127132416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6092313528060913, loss=1.300461769104004
I0215 22:07:05.024923 139578135525120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.672591507434845, loss=1.2863596677780151
I0215 22:08:20.743133 139578127132416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.8198933601379395, loss=1.3041385412216187
I0215 22:09:36.460736 139578135525120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.8448922634124756, loss=1.3085949420928955
I0215 22:10:52.217600 139578127132416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.7333480715751648, loss=1.315293788909912
I0215 22:12:13.538704 139578135525120 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.7157794833183289, loss=1.333638310432434
I0215 22:13:36.273938 139578127132416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.70865797996521, loss=1.2800930738449097
I0215 22:14:58.796161 139578135525120 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.7399182319641113, loss=1.2671349048614502
I0215 22:16:21.207617 139578127132416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.7407850623130798, loss=1.2923369407653809
I0215 22:17:47.766851 139578135525120 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.9744725227355957, loss=1.2412834167480469
I0215 22:19:03.466810 139578127132416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6523500680923462, loss=1.2579182386398315
I0215 22:20:19.163078 139578135525120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6941906809806824, loss=1.2822548151016235
I0215 22:21:34.846567 139578127132416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6548274159431458, loss=1.2441426515579224
I0215 22:22:50.542099 139578135525120 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6613032817840576, loss=1.2875436544418335
I0215 22:24:06.199092 139578127132416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.7265931963920593, loss=1.3239418268203735
I0215 22:24:48.218556 139688679413568 spec.py:321] Evaluating on the training split.
I0215 22:25:41.738964 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 22:26:32.331780 139688679413568 spec.py:349] Evaluating on the test split.
I0215 22:26:57.873590 139688679413568 submission_runner.py:408] Time since start: 28444.34s, 	Step: 31457, 	{'train/ctc_loss': Array(0.24844179, dtype=float32), 'train/wer': 0.08890363063939927, 'validation/ctc_loss': Array(0.48052293, dtype=float32), 'validation/wer': 0.14382536663544995, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28156066, dtype=float32), 'test/wer': 0.09574878638311701, 'test/num_examples': 2472, 'score': 25962.538994789124, 'total_duration': 28444.344913959503, 'accumulated_submission_time': 25962.538994789124, 'accumulated_eval_time': 2479.606421470642, 'accumulated_logging_time': 0.9324443340301514}
I0215 22:26:57.915858 139578135525120 logging_writer.py:48] [31457] accumulated_eval_time=2479.606421, accumulated_logging_time=0.932444, accumulated_submission_time=25962.538995, global_step=31457, preemption_count=0, score=25962.538995, test/ctc_loss=0.28156065940856934, test/num_examples=2472, test/wer=0.095749, total_duration=28444.344914, train/ctc_loss=0.24844178557395935, train/wer=0.088904, validation/ctc_loss=0.48052293062210083, validation/num_examples=5348, validation/wer=0.143825
I0215 22:27:31.148127 139578127132416 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6768072247505188, loss=1.2657818794250488
I0215 22:28:46.785192 139578135525120 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.622256338596344, loss=1.2566505670547485
I0215 22:30:02.832837 139578127132416 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6766325831413269, loss=1.2734013795852661
I0215 22:31:18.528749 139578135525120 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.7886537313461304, loss=1.2523243427276611
I0215 22:32:34.172188 139578127132416 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7201941013336182, loss=1.3182028532028198
I0215 22:33:53.296468 139578135525120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.7099428772926331, loss=1.2069205045700073
I0215 22:35:08.955625 139578127132416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.699873149394989, loss=1.2750357389450073
I0215 22:36:24.686457 139578135525120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6425915956497192, loss=1.3352606296539307
I0215 22:37:40.436019 139578127132416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.7011601328849792, loss=1.2415603399276733
I0215 22:38:56.310838 139578135525120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.72873455286026, loss=1.2634261846542358
I0215 22:40:12.120506 139578127132416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6573687791824341, loss=1.2476873397827148
I0215 22:41:33.290383 139578135525120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6439555883407593, loss=1.2174164056777954
I0215 22:42:55.978051 139578127132416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.6929729580879211, loss=1.2724915742874146
I0215 22:44:18.218909 139578135525120 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.6641267538070679, loss=1.3027626276016235
I0215 22:45:41.452148 139578127132416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.7229003310203552, loss=1.2969844341278076
I0215 22:47:04.640888 139578135525120 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.1029926538467407, loss=1.306576132774353
I0215 22:48:20.386245 139578127132416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6853806376457214, loss=1.2119724750518799
I0215 22:49:36.125094 139578135525120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.7301632761955261, loss=1.221448540687561
I0215 22:50:51.950132 139578127132416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.6357181668281555, loss=1.2353378534317017
I0215 22:50:58.478804 139688679413568 spec.py:321] Evaluating on the training split.
I0215 22:51:52.454445 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 22:52:42.668674 139688679413568 spec.py:349] Evaluating on the test split.
I0215 22:53:08.536064 139688679413568 submission_runner.py:408] Time since start: 30015.01s, 	Step: 33310, 	{'train/ctc_loss': Array(0.20790268, dtype=float32), 'train/wer': 0.07849532162816839, 'validation/ctc_loss': Array(0.46711764, dtype=float32), 'validation/wer': 0.14012763451345375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27785712, dtype=float32), 'test/wer': 0.09339264314585745, 'test/num_examples': 2472, 'score': 27403.022592306137, 'total_duration': 30015.006851911545, 'accumulated_submission_time': 27403.022592306137, 'accumulated_eval_time': 2609.6571514606476, 'accumulated_logging_time': 0.989710807800293}
I0215 22:53:08.575385 139578135525120 logging_writer.py:48] [33310] accumulated_eval_time=2609.657151, accumulated_logging_time=0.989711, accumulated_submission_time=27403.022592, global_step=33310, preemption_count=0, score=27403.022592, test/ctc_loss=0.27785712480545044, test/num_examples=2472, test/wer=0.093393, total_duration=30015.006852, train/ctc_loss=0.2079026848077774, train/wer=0.078495, validation/ctc_loss=0.46711763739585876, validation/num_examples=5348, validation/wer=0.140128
I0215 22:54:17.368961 139578127132416 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.7760545015335083, loss=1.3073458671569824
I0215 22:55:33.110238 139578135525120 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6970218420028687, loss=1.251907229423523
I0215 22:56:48.825113 139578127132416 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.842763364315033, loss=1.2775084972381592
I0215 22:58:04.592867 139578135525120 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.6537930369377136, loss=1.2726842164993286
I0215 22:59:23.031786 139578127132416 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.7200337052345276, loss=1.2664968967437744
I0215 23:00:45.705882 139578135525120 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.7354798316955566, loss=1.2866824865341187
I0215 23:02:09.720570 139578135525120 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7580713033676147, loss=1.2720305919647217
I0215 23:03:25.536305 139578127132416 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.7451404929161072, loss=1.2643945217132568
I0215 23:04:41.416530 139578135525120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.8229925632476807, loss=1.294060468673706
I0215 23:05:57.268907 139578127132416 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.6575842499732971, loss=1.2422736883163452
I0215 23:07:13.119005 139578135525120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.5939634442329407, loss=1.2618225812911987
I0215 23:08:28.957642 139578127132416 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7013749480247498, loss=1.2786372900009155
I0215 23:09:45.343056 139578135525120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.6460822224617004, loss=1.2918165922164917
I0215 23:11:08.858928 139578127132416 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7640102505683899, loss=1.246556043624878
I0215 23:12:30.743445 139578135525120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.8028109669685364, loss=1.2673460245132446
I0215 23:13:53.347388 139578127132416 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.8202999830245972, loss=1.292277455329895
I0215 23:15:15.808727 139578135525120 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7738932967185974, loss=1.2695508003234863
I0215 23:16:36.228668 139578135525120 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.6987484097480774, loss=1.2435187101364136
I0215 23:17:09.185160 139688679413568 spec.py:321] Evaluating on the training split.
I0215 23:18:03.272966 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 23:18:54.278162 139688679413568 spec.py:349] Evaluating on the test split.
I0215 23:19:20.107093 139688679413568 submission_runner.py:408] Time since start: 31586.58s, 	Step: 35145, 	{'train/ctc_loss': Array(0.18601915, dtype=float32), 'train/wer': 0.07065032078015412, 'validation/ctc_loss': Array(0.45886686, dtype=float32), 'validation/wer': 0.13640093843227744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26721686, dtype=float32), 'test/wer': 0.09050839883817764, 'test/num_examples': 2472, 'score': 28843.551401615143, 'total_duration': 31586.579282283783, 'accumulated_submission_time': 28843.551401615143, 'accumulated_eval_time': 2740.5739209651947, 'accumulated_logging_time': 1.044804573059082}
I0215 23:19:20.140875 139578135525120 logging_writer.py:48] [35145] accumulated_eval_time=2740.573921, accumulated_logging_time=1.044805, accumulated_submission_time=28843.551402, global_step=35145, preemption_count=0, score=28843.551402, test/ctc_loss=0.26721686124801636, test/num_examples=2472, test/wer=0.090508, total_duration=31586.579282, train/ctc_loss=0.1860191524028778, train/wer=0.070650, validation/ctc_loss=0.4588668644428253, validation/num_examples=5348, validation/wer=0.136401
I0215 23:20:02.405596 139578127132416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6858160495758057, loss=1.2634806632995605
I0215 23:21:18.020544 139578135525120 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.7409546375274658, loss=1.236397385597229
I0215 23:22:33.720111 139578127132416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.961022138595581, loss=1.2632886171340942
I0215 23:23:49.475085 139578135525120 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6701982021331787, loss=1.2777827978134155
I0215 23:25:05.239057 139578127132416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.8314943313598633, loss=1.2532721757888794
I0215 23:26:20.994099 139578135525120 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.7156068682670593, loss=1.189226508140564
I0215 23:27:42.944425 139578127132416 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.7107842564582825, loss=1.2755491733551025
I0215 23:29:04.960917 139578135525120 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.0100330114364624, loss=1.2995870113372803
I0215 23:30:28.755088 139578127132416 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6321834325790405, loss=1.2180768251419067
I0215 23:31:51.200918 139578135525120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.7614343762397766, loss=1.2440764904022217
I0215 23:33:06.895712 139578127132416 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.9422395825386047, loss=1.1649885177612305
I0215 23:34:22.745504 139578135525120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.6351351737976074, loss=1.2157235145568848
I0215 23:35:38.477607 139578127132416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.5795575380325317, loss=1.18057382106781
I0215 23:36:54.061996 139578135525120 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.7262172698974609, loss=1.209998607635498
I0215 23:38:09.654298 139578127132416 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6492315530776978, loss=1.1944324970245361
I0215 23:39:31.325206 139578135525120 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.646703839302063, loss=1.2167770862579346
I0215 23:40:53.343661 139578127132416 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.7087600827217102, loss=1.238038420677185
I0215 23:42:15.700699 139578135525120 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.8738222122192383, loss=1.2659013271331787
I0215 23:43:20.348205 139688679413568 spec.py:321] Evaluating on the training split.
I0215 23:44:14.723183 139688679413568 spec.py:333] Evaluating on the validation split.
I0215 23:45:05.389274 139688679413568 spec.py:349] Evaluating on the test split.
I0215 23:45:31.009496 139688679413568 submission_runner.py:408] Time since start: 33157.48s, 	Step: 36979, 	{'train/ctc_loss': Array(0.178505, dtype=float32), 'train/wer': 0.06665957955723284, 'validation/ctc_loss': Array(0.4410436, dtype=float32), 'validation/wer': 0.1324618399837802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25798786, dtype=float32), 'test/wer': 0.08608047447850019, 'test/num_examples': 2472, 'score': 30283.677778959274, 'total_duration': 33157.48137187958, 'accumulated_submission_time': 30283.677778959274, 'accumulated_eval_time': 2871.2297463417053, 'accumulated_logging_time': 1.0952837467193604}
I0215 23:45:31.052381 139578135525120 logging_writer.py:48] [36979] accumulated_eval_time=2871.229746, accumulated_logging_time=1.095284, accumulated_submission_time=30283.677779, global_step=36979, preemption_count=0, score=30283.677779, test/ctc_loss=0.2579878568649292, test/num_examples=2472, test/wer=0.086080, total_duration=33157.481372, train/ctc_loss=0.17850500345230103, train/wer=0.066660, validation/ctc_loss=0.44104358553886414, validation/num_examples=5348, validation/wer=0.132462
I0215 23:45:47.732748 139578127132416 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.725417971611023, loss=1.2205021381378174
I0215 23:47:06.716095 139578135525120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.6575055718421936, loss=1.1771917343139648
I0215 23:48:22.511630 139578127132416 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.77168869972229, loss=1.1886547803878784
I0215 23:49:38.336763 139578135525120 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.813835859298706, loss=1.2514994144439697
I0215 23:50:54.441843 139578127132416 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.0985877513885498, loss=1.2402828931808472
I0215 23:52:10.278633 139578135525120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7412177324295044, loss=1.2141530513763428
I0215 23:53:26.091059 139578127132416 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6770890951156616, loss=1.1881532669067383
I0215 23:54:45.830213 139578135525120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.6995174288749695, loss=1.2506130933761597
I0215 23:56:08.975352 139578127132416 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.7901624441146851, loss=1.266882061958313
I0215 23:57:32.305487 139578135525120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.7199695110321045, loss=1.2084152698516846
I0215 23:58:54.720627 139578127132416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.9205315113067627, loss=1.1978188753128052
I0216 00:00:17.807526 139578135525120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.6837856769561768, loss=1.242924451828003
I0216 00:01:37.088753 139578135525120 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.7436860799789429, loss=1.2462892532348633
I0216 00:02:52.768460 139578127132416 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.7404179573059082, loss=1.2088819742202759
I0216 00:04:08.549466 139578135525120 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.8518197536468506, loss=1.2595131397247314
I0216 00:05:24.515419 139578127132416 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.6531915664672852, loss=1.1996772289276123
I0216 00:06:40.272853 139578135525120 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.6596036553382874, loss=1.1852864027023315
I0216 00:07:56.852116 139578127132416 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.690466046333313, loss=1.2508552074432373
I0216 00:09:18.945562 139578135525120 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.9479719996452332, loss=1.2114887237548828
I0216 00:09:31.378844 139688679413568 spec.py:321] Evaluating on the training split.
I0216 00:10:24.948819 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 00:11:15.712721 139688679413568 spec.py:349] Evaluating on the test split.
I0216 00:11:41.539750 139688679413568 submission_runner.py:408] Time since start: 34728.01s, 	Step: 38817, 	{'train/ctc_loss': Array(0.17209044, dtype=float32), 'train/wer': 0.06707041343669251, 'validation/ctc_loss': Array(0.43647054, dtype=float32), 'validation/wer': 0.13019299651467026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25531837, dtype=float32), 'test/wer': 0.08693356082302521, 'test/num_examples': 2472, 'score': 31723.920412540436, 'total_duration': 34728.01073908806, 'accumulated_submission_time': 31723.920412540436, 'accumulated_eval_time': 3001.384289264679, 'accumulated_logging_time': 1.1554756164550781}
I0216 00:11:41.580304 139578135525120 logging_writer.py:48] [38817] accumulated_eval_time=3001.384289, accumulated_logging_time=1.155476, accumulated_submission_time=31723.920413, global_step=38817, preemption_count=0, score=31723.920413, test/ctc_loss=0.25531837344169617, test/num_examples=2472, test/wer=0.086934, total_duration=34728.010739, train/ctc_loss=0.17209044098854065, train/wer=0.067070, validation/ctc_loss=0.43647053837776184, validation/num_examples=5348, validation/wer=0.130193
I0216 00:12:44.873651 139578127132416 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.8401746153831482, loss=1.2492948770523071
I0216 00:14:00.451769 139578135525120 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7305331826210022, loss=1.273706078529358
I0216 00:15:16.034199 139578127132416 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.7393829822540283, loss=1.2175251245498657
I0216 00:16:35.130255 139578135525120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.7363232374191284, loss=1.2072750329971313
I0216 00:17:50.837687 139578127132416 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.6304923892021179, loss=1.2012015581130981
I0216 00:19:06.665600 139578135525120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.6603440642356873, loss=1.2234668731689453
I0216 00:20:22.410024 139578127132416 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.691932737827301, loss=1.2281473875045776
I0216 00:21:38.150924 139578135525120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.7376428842544556, loss=1.2530626058578491
I0216 00:22:56.590816 139578127132416 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7312757968902588, loss=1.1741727590560913
I0216 00:24:19.236036 139578135525120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.749953031539917, loss=1.2115588188171387
I0216 00:25:43.134783 139578127132416 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7099631428718567, loss=1.1949517726898193
I0216 00:27:07.017099 139578135525120 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.8396193385124207, loss=1.239546298980713
I0216 00:28:30.038913 139578127132416 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.832063615322113, loss=1.2383952140808105
I0216 00:29:53.959795 139578135525120 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7617471218109131, loss=1.239659070968628
I0216 00:31:09.798180 139578127132416 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7667732834815979, loss=1.2398350238800049
I0216 00:32:25.687525 139578135525120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7998902201652527, loss=1.1901581287384033
I0216 00:33:41.441959 139578127132416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7022375464439392, loss=1.2076170444488525
I0216 00:34:57.273725 139578135525120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.7881947755813599, loss=1.1643024682998657
I0216 00:35:41.578159 139688679413568 spec.py:321] Evaluating on the training split.
I0216 00:36:36.841577 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 00:37:27.207538 139688679413568 spec.py:349] Evaluating on the test split.
I0216 00:37:52.887518 139688679413568 submission_runner.py:408] Time since start: 36299.36s, 	Step: 40660, 	{'train/ctc_loss': Array(0.18338138, dtype=float32), 'train/wer': 0.06747332908774165, 'validation/ctc_loss': Array(0.4277794, dtype=float32), 'validation/wer': 0.12773105998435946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24571325, dtype=float32), 'test/wer': 0.08276968699855788, 'test/num_examples': 2472, 'score': 33163.837792634964, 'total_duration': 36299.36015820503, 'accumulated_submission_time': 33163.837792634964, 'accumulated_eval_time': 3132.688977241516, 'accumulated_logging_time': 1.2110445499420166}
I0216 00:37:52.929884 139578135525120 logging_writer.py:48] [40660] accumulated_eval_time=3132.688977, accumulated_logging_time=1.211045, accumulated_submission_time=33163.837793, global_step=40660, preemption_count=0, score=33163.837793, test/ctc_loss=0.2457132488489151, test/num_examples=2472, test/wer=0.082770, total_duration=36299.360158, train/ctc_loss=0.18338137865066528, train/wer=0.067473, validation/ctc_loss=0.4277794063091278, validation/num_examples=5348, validation/wer=0.127731
I0216 00:38:23.904479 139578127132416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.8033058047294617, loss=1.2487047910690308
I0216 00:39:39.902548 139578135525120 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7224223017692566, loss=1.2275111675262451
I0216 00:40:55.550854 139578127132416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.6512762308120728, loss=1.1953022480010986
I0216 00:42:11.405421 139578135525120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.7529643774032593, loss=1.1740015745162964
I0216 00:43:30.488109 139578127132416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.7012825012207031, loss=1.2128210067749023
I0216 00:44:56.631528 139578135525120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.7010594606399536, loss=1.2135932445526123
I0216 00:46:12.408601 139578127132416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.9833216071128845, loss=1.1775970458984375
I0216 00:47:28.143305 139578135525120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.717149019241333, loss=1.1776269674301147
I0216 00:48:43.908055 139578127132416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6995944976806641, loss=1.1565093994140625
I0216 00:49:59.741619 139578135525120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.8197943568229675, loss=1.2072821855545044
I0216 00:51:15.510528 139578127132416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.838830292224884, loss=1.2106825113296509
I0216 00:52:34.395735 139578135525120 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.8032606244087219, loss=1.1757268905639648
I0216 00:53:57.712777 139578127132416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.8420510292053223, loss=1.1868352890014648
I0216 00:55:20.505012 139578135525120 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.8057970404624939, loss=1.1480438709259033
I0216 00:56:43.479763 139578127132416 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.8387643694877625, loss=1.151451587677002
I0216 00:58:06.371382 139578135525120 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.8278776407241821, loss=1.176882266998291
I0216 00:59:27.575928 139578135525120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7077716588973999, loss=1.1502416133880615
I0216 01:00:43.404612 139578127132416 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.7733710408210754, loss=1.2004327774047852
I0216 01:01:53.480140 139688679413568 spec.py:321] Evaluating on the training split.
I0216 01:02:47.889707 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 01:03:39.028615 139688679413568 spec.py:349] Evaluating on the test split.
I0216 01:04:04.939771 139688679413568 submission_runner.py:408] Time since start: 37871.41s, 	Step: 42494, 	{'train/ctc_loss': Array(0.1785813, dtype=float32), 'train/wer': 0.06711494210506916, 'validation/ctc_loss': Array(0.42371103, dtype=float32), 'validation/wer': 0.126253898066173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2410632, dtype=float32), 'test/wer': 0.08031198586314058, 'test/num_examples': 2472, 'score': 34604.3055870533, 'total_duration': 37871.41095805168, 'accumulated_submission_time': 34604.3055870533, 'accumulated_eval_time': 3264.1424934864044, 'accumulated_logging_time': 1.2691049575805664}
I0216 01:04:04.980244 139578135525120 logging_writer.py:48] [42494] accumulated_eval_time=3264.142493, accumulated_logging_time=1.269105, accumulated_submission_time=34604.305587, global_step=42494, preemption_count=0, score=34604.305587, test/ctc_loss=0.2410632073879242, test/num_examples=2472, test/wer=0.080312, total_duration=37871.410958, train/ctc_loss=0.17858129739761353, train/wer=0.067115, validation/ctc_loss=0.42371103167533875, validation/num_examples=5348, validation/wer=0.126254
I0216 01:04:10.383398 139578127132416 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.8118024468421936, loss=1.215423822402954
I0216 01:05:26.074534 139578135525120 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.7076889276504517, loss=1.1694930791854858
I0216 01:06:41.932981 139578127132416 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7737323641777039, loss=1.1924278736114502
I0216 01:07:57.678188 139578135525120 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.837539792060852, loss=1.1514891386032104
I0216 01:09:13.457234 139578127132416 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.8326751589775085, loss=1.1762765645980835
I0216 01:10:32.339427 139578135525120 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7890211343765259, loss=1.1697990894317627
I0216 01:11:54.930140 139578127132416 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.7649277448654175, loss=1.1812124252319336
I0216 01:13:17.550330 139578135525120 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.7347270250320435, loss=1.1527066230773926
I0216 01:14:40.610406 139578135525120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7492244839668274, loss=1.1597789525985718
I0216 01:15:56.400538 139578127132416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.8192840814590454, loss=1.16975998878479
I0216 01:17:12.086381 139578135525120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7631876468658447, loss=1.17470121383667
I0216 01:18:27.758267 139578127132416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.8312838673591614, loss=1.1790411472320557
I0216 01:19:43.445098 139578135525120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.8866157531738281, loss=1.1342114210128784
I0216 01:20:59.262936 139578127132416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.7423798441886902, loss=1.1486725807189941
I0216 01:22:19.549921 139578135525120 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7552384734153748, loss=1.179445743560791
I0216 01:23:41.932863 139578127132416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.6906980276107788, loss=1.1031609773635864
I0216 01:25:05.551136 139578135525120 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.6438154578208923, loss=1.1388427019119263
I0216 01:26:27.899820 139578127132416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.6802764534950256, loss=1.1806687116622925
I0216 01:27:53.417111 139578135525120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7444509267807007, loss=1.1656711101531982
I0216 01:28:05.264972 139688679413568 spec.py:321] Evaluating on the training split.
I0216 01:29:00.594366 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 01:29:51.635145 139688679413568 spec.py:349] Evaluating on the test split.
I0216 01:30:17.989122 139688679413568 submission_runner.py:408] Time since start: 39444.46s, 	Step: 44317, 	{'train/ctc_loss': Array(0.15933198, dtype=float32), 'train/wer': 0.06103466511816004, 'validation/ctc_loss': Array(0.4057534, dtype=float32), 'validation/wer': 0.12189964953609392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2344677, dtype=float32), 'test/wer': 0.07941827635935246, 'test/num_examples': 2472, 'score': 36044.50428843498, 'total_duration': 39444.46064519882, 'accumulated_submission_time': 36044.50428843498, 'accumulated_eval_time': 3396.860850095749, 'accumulated_logging_time': 1.3300681114196777}
I0216 01:30:18.033379 139578135525120 logging_writer.py:48] [44317] accumulated_eval_time=3396.860850, accumulated_logging_time=1.330068, accumulated_submission_time=36044.504288, global_step=44317, preemption_count=0, score=36044.504288, test/ctc_loss=0.23446770012378693, test/num_examples=2472, test/wer=0.079418, total_duration=39444.460645, train/ctc_loss=0.15933197736740112, train/wer=0.061035, validation/ctc_loss=0.40575340390205383, validation/num_examples=5348, validation/wer=0.121900
I0216 01:31:21.608789 139578127132416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.680949330329895, loss=1.1881871223449707
I0216 01:32:37.352359 139578135525120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.742718517780304, loss=1.1644607782363892
I0216 01:33:53.087537 139578127132416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.7726872563362122, loss=1.1653854846954346
I0216 01:35:08.932913 139578135525120 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.0103559494018555, loss=1.117314100265503
I0216 01:36:24.716967 139578127132416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.8205195665359497, loss=1.1684538125991821
I0216 01:37:40.448233 139578135525120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.7980403304100037, loss=1.1746596097946167
I0216 01:38:56.919718 139578127132416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.735059380531311, loss=1.1616833209991455
I0216 01:40:19.887469 139578135525120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.7245098948478699, loss=1.1224749088287354
I0216 01:41:42.972799 139578127132416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.7522000074386597, loss=1.1278741359710693
I0216 01:43:07.003249 139578135525120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7196030020713806, loss=1.1723237037658691
I0216 01:44:27.415468 139578135525120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.9686880111694336, loss=1.173511028289795
I0216 01:45:43.235399 139578127132416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8712711930274963, loss=1.1214430332183838
I0216 01:46:58.935456 139578135525120 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.8221362233161926, loss=1.1191450357437134
I0216 01:48:14.670982 139578127132416 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.7529120445251465, loss=1.0932389497756958
I0216 01:49:30.417132 139578135525120 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.7559510469436646, loss=1.1944001913070679
I0216 01:50:47.573561 139578127132416 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.7443715929985046, loss=1.1185351610183716
I0216 01:52:11.425494 139578135525120 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7908983826637268, loss=1.1814252138137817
I0216 01:53:35.222982 139578127132416 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.8080758452415466, loss=1.1163979768753052
I0216 01:54:18.088272 139688679413568 spec.py:321] Evaluating on the training split.
I0216 01:55:12.785887 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 01:56:03.544139 139688679413568 spec.py:349] Evaluating on the test split.
I0216 01:56:28.981976 139688679413568 submission_runner.py:408] Time since start: 41015.45s, 	Step: 46154, 	{'train/ctc_loss': Array(0.15425502, dtype=float32), 'train/wer': 0.059206374867668304, 'validation/ctc_loss': Array(0.39746863, dtype=float32), 'validation/wer': 0.11965977002616411, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22770649, dtype=float32), 'test/wer': 0.07643247415351492, 'test/num_examples': 2472, 'score': 37484.47999191284, 'total_duration': 41015.45380330086, 'accumulated_submission_time': 37484.47999191284, 'accumulated_eval_time': 3527.749038219452, 'accumulated_logging_time': 1.3891162872314453}
I0216 01:56:29.024270 139578135525120 logging_writer.py:48] [46154] accumulated_eval_time=3527.749038, accumulated_logging_time=1.389116, accumulated_submission_time=37484.479992, global_step=46154, preemption_count=0, score=37484.479992, test/ctc_loss=0.22770649194717407, test/num_examples=2472, test/wer=0.076432, total_duration=41015.453803, train/ctc_loss=0.15425501763820648, train/wer=0.059206, validation/ctc_loss=0.397468626499176, validation/num_examples=5348, validation/wer=0.119660
I0216 01:57:04.545578 139578127132416 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.8587368130683899, loss=1.1426218748092651
I0216 01:58:20.201299 139578135525120 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.8418961763381958, loss=1.1117197275161743
I0216 01:59:39.435852 139578135525120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.6929337978363037, loss=1.165144920349121
I0216 02:00:54.939462 139578127132416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.7248684763908386, loss=1.1237905025482178
I0216 02:02:10.940095 139578135525120 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.685560405254364, loss=1.128271222114563
I0216 02:03:26.702579 139578127132416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7745891213417053, loss=1.1354570388793945
I0216 02:04:42.477906 139578135525120 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.7674937844276428, loss=1.157038927078247
I0216 02:05:59.037027 139578127132416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7461658120155334, loss=1.1204166412353516
I0216 02:07:21.490551 139578135525120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8492259979248047, loss=1.128437876701355
I0216 02:08:45.221957 139578127132416 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.1192984580993652, loss=1.059490442276001
I0216 02:10:07.436040 139578135525120 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.8163098096847534, loss=1.141622543334961
I0216 02:11:30.325086 139578127132416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7425133585929871, loss=1.1767029762268066
I0216 02:12:54.655935 139578135525120 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.7798836827278137, loss=1.1197524070739746
I0216 02:14:10.421797 139578127132416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.9303159713745117, loss=1.1330119371414185
I0216 02:15:26.239710 139578135525120 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.8852345943450928, loss=1.1179425716400146
I0216 02:16:42.238559 139578127132416 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.9639834761619568, loss=1.1049058437347412
I0216 02:17:57.952667 139578135525120 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.7446115016937256, loss=1.1533589363098145
I0216 02:19:13.704322 139578127132416 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.0162447690963745, loss=1.1240308284759521
I0216 02:20:29.178699 139688679413568 spec.py:321] Evaluating on the training split.
I0216 02:21:24.019340 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 02:22:15.000156 139688679413568 spec.py:349] Evaluating on the test split.
I0216 02:22:40.879127 139688679413568 submission_runner.py:408] Time since start: 42587.35s, 	Step: 47997, 	{'train/ctc_loss': Array(0.13490936, dtype=float32), 'train/wer': 0.05189839995775466, 'validation/ctc_loss': Array(0.39513963, dtype=float32), 'validation/wer': 0.11727507072033366, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22139996, dtype=float32), 'test/wer': 0.07484817094225418, 'test/num_examples': 2472, 'score': 38924.55057263374, 'total_duration': 42587.35040640831, 'accumulated_submission_time': 38924.55057263374, 'accumulated_eval_time': 3659.4434146881104, 'accumulated_logging_time': 1.4481637477874756}
I0216 02:22:40.917410 139578135525120 logging_writer.py:48] [47997] accumulated_eval_time=3659.443415, accumulated_logging_time=1.448164, accumulated_submission_time=38924.550573, global_step=47997, preemption_count=0, score=38924.550573, test/ctc_loss=0.2213999629020691, test/num_examples=2472, test/wer=0.074848, total_duration=42587.350406, train/ctc_loss=0.13490936160087585, train/wer=0.051898, validation/ctc_loss=0.3951396346092224, validation/num_examples=5348, validation/wer=0.117275
I0216 02:22:44.053060 139578127132416 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7446479797363281, loss=1.1215062141418457
I0216 02:23:59.628716 139578135525120 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.8889838457107544, loss=1.11865234375
I0216 02:25:15.372691 139578127132416 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.9422446489334106, loss=1.183768630027771
I0216 02:26:31.099915 139578135525120 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.9039839506149292, loss=1.1040997505187988
I0216 02:27:46.811305 139578127132416 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7700343728065491, loss=1.1259760856628418
I0216 02:29:06.085976 139578135525120 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.9726089835166931, loss=1.1162590980529785
I0216 02:30:21.937233 139578127132416 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0387485027313232, loss=1.1186388731002808
I0216 02:31:37.689895 139578135525120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.8473159670829773, loss=1.0942436456680298
I0216 02:32:53.485526 139578127132416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.8735725283622742, loss=1.0809972286224365
I0216 02:34:09.516731 139578135525120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.9275076389312744, loss=1.093373417854309
I0216 02:35:28.038976 139578127132416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.736801028251648, loss=1.1659393310546875
I0216 02:36:50.469614 139578135525120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.7143545150756836, loss=1.122645378112793
I0216 02:38:13.957509 139578127132416 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.7316053509712219, loss=1.0940991640090942
I0216 02:39:36.648167 139578135525120 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.6962025165557861, loss=1.0866093635559082
I0216 02:40:59.495465 139578127132416 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.0102555751800537, loss=1.11448335647583
I0216 02:42:21.133735 139578135525120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.7286828756332397, loss=1.0616607666015625
I0216 02:43:36.874694 139578127132416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.7152963280677795, loss=1.11171293258667
I0216 02:44:52.648724 139578135525120 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.7455189228057861, loss=1.0846047401428223
I0216 02:46:08.435255 139578127132416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.8141828179359436, loss=1.003995418548584
I0216 02:46:41.383680 139688679413568 spec.py:321] Evaluating on the training split.
I0216 02:47:36.595534 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 02:48:28.017855 139688679413568 spec.py:349] Evaluating on the test split.
I0216 02:48:53.916962 139688679413568 submission_runner.py:408] Time since start: 44160.39s, 	Step: 49845, 	{'train/ctc_loss': Array(0.13607302, dtype=float32), 'train/wer': 0.051533911551074195, 'validation/ctc_loss': Array(0.38606805, dtype=float32), 'validation/wer': 0.11457176786352183, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21825966, dtype=float32), 'test/wer': 0.07375134564215059, 'test/num_examples': 2472, 'score': 40364.931837558746, 'total_duration': 44160.38792181015, 'accumulated_submission_time': 40364.931837558746, 'accumulated_eval_time': 3791.9704139232635, 'accumulated_logging_time': 1.503833293914795}
I0216 02:48:53.955381 139578135525120 logging_writer.py:48] [49845] accumulated_eval_time=3791.970414, accumulated_logging_time=1.503833, accumulated_submission_time=40364.931838, global_step=49845, preemption_count=0, score=40364.931838, test/ctc_loss=0.21825966238975525, test/num_examples=2472, test/wer=0.073751, total_duration=44160.387922, train/ctc_loss=0.1360730230808258, train/wer=0.051534, validation/ctc_loss=0.38606804609298706, validation/num_examples=5348, validation/wer=0.114572
I0216 02:49:36.361239 139578127132416 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.8316259384155273, loss=1.1263116598129272
I0216 02:50:52.414482 139578135525120 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.8862501978874207, loss=1.0911415815353394
I0216 02:52:08.170047 139578127132416 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.0139849185943604, loss=1.0763784646987915
I0216 02:53:24.060318 139578135525120 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.7679353356361389, loss=1.1223379373550415
I0216 02:54:42.991818 139578127132416 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.7399143576622009, loss=1.1127707958221436
I0216 02:56:06.152640 139578135525120 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.8588659763336182, loss=1.080385684967041
I0216 02:57:30.185740 139578135525120 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.9010245203971863, loss=1.0275808572769165
I0216 02:58:46.068539 139578127132416 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.9358959794044495, loss=1.132961392402649
I0216 03:00:01.972429 139578135525120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8834328651428223, loss=1.0459944009780884
I0216 03:01:17.864487 139578127132416 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.902509868144989, loss=1.056611180305481
I0216 03:02:33.627656 139578135525120 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.8351448774337769, loss=1.0496140718460083
I0216 03:03:49.408718 139578127132416 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.0526134967803955, loss=1.1287841796875
I0216 03:05:11.699286 139578135525120 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.8437719941139221, loss=1.0957540273666382
I0216 03:06:33.981917 139578127132416 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.7652732133865356, loss=1.0743850469589233
I0216 03:07:56.559861 139578135525120 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.9242324829101562, loss=1.1403039693832397
I0216 03:09:19.423219 139578127132416 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.8813338875770569, loss=1.070290207862854
I0216 03:10:46.129594 139578135525120 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.8395437598228455, loss=1.117335557937622
I0216 03:12:01.900104 139578127132416 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.8277881145477295, loss=1.118638038635254
I0216 03:12:54.519232 139688679413568 spec.py:321] Evaluating on the training split.
I0216 03:13:48.704010 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 03:14:39.753159 139688679413568 spec.py:349] Evaluating on the test split.
I0216 03:15:05.827178 139688679413568 submission_runner.py:408] Time since start: 45732.30s, 	Step: 51671, 	{'train/ctc_loss': Array(0.13753137, dtype=float32), 'train/wer': 0.05211096404743254, 'validation/ctc_loss': Array(0.3752853, dtype=float32), 'validation/wer': 0.11043957635382373, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21176472, dtype=float32), 'test/wer': 0.07182174557715354, 'test/num_examples': 2472, 'score': 41805.41350221634, 'total_duration': 45732.29967999458, 'accumulated_submission_time': 41805.41350221634, 'accumulated_eval_time': 3923.273575782776, 'accumulated_logging_time': 1.5587153434753418}
I0216 03:15:05.868319 139578135525120 logging_writer.py:48] [51671] accumulated_eval_time=3923.273576, accumulated_logging_time=1.558715, accumulated_submission_time=41805.413502, global_step=51671, preemption_count=0, score=41805.413502, test/ctc_loss=0.21176472306251526, test/num_examples=2472, test/wer=0.071822, total_duration=45732.299680, train/ctc_loss=0.1375313699245453, train/wer=0.052111, validation/ctc_loss=0.3752852976322174, validation/num_examples=5348, validation/wer=0.110440
I0216 03:15:28.552695 139578127132416 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.9585651159286499, loss=1.0932830572128296
I0216 03:16:44.056684 139578135525120 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.8357396125793457, loss=1.0560299158096313
I0216 03:17:59.832077 139578127132416 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.7672027349472046, loss=1.0966074466705322
I0216 03:19:15.558274 139578135525120 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8346626162528992, loss=1.0782470703125
I0216 03:20:31.325488 139578127132416 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.7678623199462891, loss=1.044779896736145
I0216 03:21:47.006483 139578135525120 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.7885656952857971, loss=1.0895076990127563
I0216 03:23:09.266273 139578127132416 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.8919179439544678, loss=1.0792372226715088
I0216 03:24:32.830627 139578135525120 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.7751941084861755, loss=1.0937215089797974
I0216 03:25:55.669389 139578127132416 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.9303035140037537, loss=1.0774288177490234
I0216 03:27:17.248259 139578135525120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.8481857776641846, loss=1.0605286359786987
I0216 03:28:33.140250 139578127132416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.826082706451416, loss=1.0497428178787231
I0216 03:29:49.070335 139578135525120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.7893319129943848, loss=1.0281987190246582
I0216 03:31:04.935693 139578127132416 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.7661029100418091, loss=1.0387635231018066
I0216 03:32:20.881607 139578135525120 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.0852197408676147, loss=1.0599101781845093
I0216 03:33:36.762965 139578127132416 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.8261377215385437, loss=1.092524528503418
I0216 03:34:59.671547 139578135525120 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.8061591982841492, loss=1.0559110641479492
I0216 03:36:22.641282 139578127132416 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.9712237119674683, loss=1.0661003589630127
I0216 03:37:45.109616 139578135525120 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.9186549186706543, loss=1.0895434617996216
I0216 03:39:06.707966 139688679413568 spec.py:321] Evaluating on the training split.
I0216 03:40:00.483097 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 03:40:51.453808 139688679413568 spec.py:349] Evaluating on the test split.
I0216 03:41:17.419717 139688679413568 submission_runner.py:408] Time since start: 47303.89s, 	Step: 53500, 	{'train/ctc_loss': Array(0.12048063, dtype=float32), 'train/wer': 0.04511261977815764, 'validation/ctc_loss': Array(0.37096584, dtype=float32), 'validation/wer': 0.10881759463973661, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20651865, dtype=float32), 'test/wer': 0.06838908861942193, 'test/num_examples': 2472, 'score': 43246.1699757576, 'total_duration': 47303.89137673378, 'accumulated_submission_time': 43246.1699757576, 'accumulated_eval_time': 4053.9796431064606, 'accumulated_logging_time': 1.6170992851257324}
I0216 03:41:17.459782 139578135525120 logging_writer.py:48] [53500] accumulated_eval_time=4053.979643, accumulated_logging_time=1.617099, accumulated_submission_time=43246.169976, global_step=53500, preemption_count=0, score=43246.169976, test/ctc_loss=0.20651865005493164, test/num_examples=2472, test/wer=0.068389, total_duration=47303.891377, train/ctc_loss=0.12048063427209854, train/wer=0.045113, validation/ctc_loss=0.370965838432312, validation/num_examples=5348, validation/wer=0.108818
I0216 03:41:18.319631 139578127132416 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.8724397420883179, loss=1.0606107711791992
I0216 03:42:37.412451 139578135525120 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.172075629234314, loss=1.0623054504394531
I0216 03:43:52.949199 139578127132416 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.8148122429847717, loss=0.9982870817184448
I0216 03:45:08.588424 139578135525120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.7670862078666687, loss=1.0381203889846802
I0216 03:46:24.217316 139578127132416 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.885428249835968, loss=1.0646030902862549
I0216 03:47:39.879364 139578135525120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8826797604560852, loss=1.0493179559707642
I0216 03:48:59.379535 139578127132416 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.0772244930267334, loss=0.9961604475975037
I0216 03:50:22.073919 139578135525120 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.7976067662239075, loss=1.051282525062561
I0216 03:51:45.258577 139578127132416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9385166764259338, loss=1.077343225479126
I0216 03:53:08.604827 139578135525120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.8046857714653015, loss=1.0271605253219604
I0216 03:54:31.842547 139578127132416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.8692044615745544, loss=1.1226236820220947
I0216 03:55:57.186351 139578135525120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.9483814239501953, loss=1.0324589014053345
I0216 03:57:12.921940 139578127132416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.8760015964508057, loss=1.0915400981903076
I0216 03:58:28.639502 139578135525120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.9647380113601685, loss=1.0763611793518066
I0216 03:59:44.326109 139578127132416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.9760584831237793, loss=0.9996161460876465
I0216 04:00:59.992289 139578135525120 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.8369361758232117, loss=1.0441205501556396
I0216 04:02:15.640554 139578127132416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.8167887926101685, loss=1.0300945043563843
I0216 04:03:37.072817 139578135525120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.9114607572555542, loss=1.0898675918579102
I0216 04:04:59.903779 139578127132416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.8805431127548218, loss=1.044650673866272
I0216 04:05:18.333658 139688679413568 spec.py:321] Evaluating on the training split.
I0216 04:06:13.530459 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 04:07:03.993719 139688679413568 spec.py:349] Evaluating on the test split.
I0216 04:07:29.501776 139688679413568 submission_runner.py:408] Time since start: 48875.97s, 	Step: 55323, 	{'train/ctc_loss': Array(0.11976799, dtype=float32), 'train/wer': 0.04654713130715187, 'validation/ctc_loss': Array(0.35839888, dtype=float32), 'validation/wer': 0.10489780549735946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19546556, dtype=float32), 'test/wer': 0.06597201064326773, 'test/num_examples': 2472, 'score': 44686.96074914932, 'total_duration': 48875.97346353531, 'accumulated_submission_time': 44686.96074914932, 'accumulated_eval_time': 4185.1421592235565, 'accumulated_logging_time': 1.6726500988006592}
I0216 04:07:29.540557 139578135525120 logging_writer.py:48] [55323] accumulated_eval_time=4185.142159, accumulated_logging_time=1.672650, accumulated_submission_time=44686.960749, global_step=55323, preemption_count=0, score=44686.960749, test/ctc_loss=0.1954655647277832, test/num_examples=2472, test/wer=0.065972, total_duration=48875.973464, train/ctc_loss=0.11976798623800278, train/wer=0.046547, validation/ctc_loss=0.3583988845348358, validation/num_examples=5348, validation/wer=0.104898
I0216 04:08:28.508217 139578127132416 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.0654386281967163, loss=1.0327955484390259
I0216 04:09:44.233059 139578135525120 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.9424624443054199, loss=1.0724040269851685
I0216 04:11:00.077624 139578127132416 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.9791033267974854, loss=1.049744963645935
I0216 04:12:19.369402 139578135525120 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.8524043560028076, loss=1.0142015218734741
I0216 04:13:35.103960 139578127132416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.9548244476318359, loss=1.0390758514404297
I0216 04:14:50.764122 139578135525120 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.9007947444915771, loss=1.049299955368042
I0216 04:16:06.461744 139578127132416 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.9747727513313293, loss=1.0284229516983032
I0216 04:17:22.094070 139578135525120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.8479377031326294, loss=0.9874839782714844
I0216 04:18:42.774791 139578127132416 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.011343240737915, loss=1.0641907453536987
I0216 04:20:06.234415 139578135525120 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.0856696367263794, loss=1.0201886892318726
I0216 04:21:29.991024 139578127132416 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.9505876898765564, loss=1.0637444257736206
I0216 04:22:54.292369 139578135525120 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9434515833854675, loss=1.01092529296875
I0216 04:24:17.945848 139578127132416 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.973088264465332, loss=1.0411990880966187
I0216 04:25:40.054752 139578135525120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.9081743359565735, loss=1.0245060920715332
I0216 04:26:55.758693 139578127132416 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.8376479744911194, loss=0.9985647797584534
I0216 04:28:11.617380 139578135525120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.8398547768592834, loss=0.9971792101860046
I0216 04:29:27.346452 139578127132416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.8517656326293945, loss=1.05998694896698
I0216 04:30:43.067911 139578135525120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.8974100947380066, loss=1.0317085981369019
I0216 04:31:29.647934 139688679413568 spec.py:321] Evaluating on the training split.
I0216 04:32:24.209310 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 04:33:15.772783 139688679413568 spec.py:349] Evaluating on the test split.
I0216 04:33:41.897252 139688679413568 submission_runner.py:408] Time since start: 50448.37s, 	Step: 57163, 	{'train/ctc_loss': Array(0.13565873, dtype=float32), 'train/wer': 0.04769781455932176, 'validation/ctc_loss': Array(0.35523295, dtype=float32), 'validation/wer': 0.10475298570145881, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19099317, dtype=float32), 'test/wer': 0.06446895375053317, 'test/num_examples': 2472, 'score': 46126.98453450203, 'total_duration': 50448.36845588684, 'accumulated_submission_time': 46126.98453450203, 'accumulated_eval_time': 4317.38533616066, 'accumulated_logging_time': 1.7271640300750732}
I0216 04:33:41.937659 139578135525120 logging_writer.py:48] [57163] accumulated_eval_time=4317.385336, accumulated_logging_time=1.727164, accumulated_submission_time=46126.984535, global_step=57163, preemption_count=0, score=46126.984535, test/ctc_loss=0.19099317491054535, test/num_examples=2472, test/wer=0.064469, total_duration=50448.368456, train/ctc_loss=0.13565872609615326, train/wer=0.047698, validation/ctc_loss=0.35523295402526855, validation/num_examples=5348, validation/wer=0.104753
I0216 04:34:10.706730 139578127132416 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.0081870555877686, loss=0.9492557644844055
I0216 04:35:26.478558 139578135525120 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.007656216621399, loss=1.0194374322891235
I0216 04:36:42.291614 139578127132416 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.9683746695518494, loss=1.0372663736343384
I0216 04:37:58.080156 139578135525120 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.0470097064971924, loss=1.0764869451522827
I0216 04:39:14.030787 139578127132416 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.010519027709961, loss=1.0354177951812744
I0216 04:40:39.131878 139578135525120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.8801093697547913, loss=0.9745457172393799
I0216 04:41:54.900801 139578127132416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.97499680519104, loss=0.9955672025680542
I0216 04:43:10.612515 139578135525120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.9585497975349426, loss=1.0473674535751343
I0216 04:44:26.640259 139578127132416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.9107166528701782, loss=1.0010780096054077
I0216 04:45:42.520120 139578135525120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.8732731342315674, loss=1.039218783378601
I0216 04:46:58.363226 139578127132416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.9045144319534302, loss=1.0321472883224487
I0216 04:48:19.160396 139578135525120 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.9756543040275574, loss=0.9829784035682678
I0216 04:49:43.085304 139578127132416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.9328019618988037, loss=0.9948300123214722
I0216 04:51:05.899584 139578135525120 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.0023057460784912, loss=1.0295593738555908
I0216 04:52:29.274205 139578127132416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.8563677072525024, loss=0.9859212040901184
I0216 04:53:52.606654 139578135525120 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.0331165790557861, loss=1.0197868347167969
I0216 04:55:12.283441 139578135525120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.9116826057434082, loss=0.9910686612129211
I0216 04:56:27.940609 139578127132416 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.024573564529419, loss=0.9967542886734009
I0216 04:57:42.461189 139688679413568 spec.py:321] Evaluating on the training split.
I0216 04:58:38.064057 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 04:59:28.938508 139688679413568 spec.py:349] Evaluating on the test split.
I0216 04:59:54.468470 139688679413568 submission_runner.py:408] Time since start: 52020.94s, 	Step: 59000, 	{'train/ctc_loss': Array(0.0892466, dtype=float32), 'train/wer': 0.035003695946778365, 'validation/ctc_loss': Array(0.3397383, dtype=float32), 'validation/wer': 0.0990181217837937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18731277, dtype=float32), 'test/wer': 0.06197062945585278, 'test/num_examples': 2472, 'score': 47567.41899180412, 'total_duration': 52020.93994116783, 'accumulated_submission_time': 47567.41899180412, 'accumulated_eval_time': 4449.386815786362, 'accumulated_logging_time': 1.7888824939727783}
I0216 04:59:54.509266 139578135525120 logging_writer.py:48] [59000] accumulated_eval_time=4449.386816, accumulated_logging_time=1.788882, accumulated_submission_time=47567.418992, global_step=59000, preemption_count=0, score=47567.418992, test/ctc_loss=0.1873127669095993, test/num_examples=2472, test/wer=0.061971, total_duration=52020.939941, train/ctc_loss=0.08924660086631775, train/wer=0.035004, validation/ctc_loss=0.33973830938339233, validation/num_examples=5348, validation/wer=0.099018
I0216 04:59:55.385689 139578127132416 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9482306838035583, loss=1.0056573152542114
I0216 05:01:11.292199 139578135525120 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.8243064284324646, loss=0.9657101035118103
I0216 05:02:27.159688 139578127132416 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.8719098567962646, loss=1.0406426191329956
I0216 05:03:43.071012 139578135525120 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.0940600633621216, loss=0.9958962798118591
I0216 05:04:58.932171 139578127132416 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.4685033559799194, loss=0.9936838150024414
I0216 05:06:21.541510 139578135525120 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.9187381863594055, loss=1.0395647287368774
I0216 05:07:44.058089 139578127132416 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.0002455711364746, loss=0.9521964192390442
I0216 05:09:06.801050 139578135525120 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.944291889667511, loss=1.0064189434051514
I0216 05:10:29.007645 139578135525120 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.8904944658279419, loss=0.9668169617652893
I0216 05:11:44.898780 139578127132416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.9080448150634766, loss=0.9698380827903748
I0216 05:13:00.596328 139578135525120 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.9659065008163452, loss=1.0006648302078247
I0216 05:14:16.344695 139578127132416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.9233758449554443, loss=0.9847822785377502
I0216 05:15:32.104996 139578135525120 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.0049245357513428, loss=0.980241060256958
I0216 05:16:49.310105 139578127132416 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.9209970831871033, loss=0.9831732511520386
I0216 05:18:13.777712 139578135525120 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.003588080406189, loss=1.0247998237609863
I0216 05:19:37.509306 139578127132416 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.9611712098121643, loss=0.9695714116096497
I0216 05:21:00.589420 139578135525120 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.05303955078125, loss=0.9975485801696777
I0216 05:22:23.831664 139578127132416 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.0077335834503174, loss=0.9802460670471191
I0216 05:23:48.249689 139578135525120 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.001289963722229, loss=0.9574240446090698
I0216 05:23:54.766012 139688679413568 spec.py:321] Evaluating on the training split.
I0216 05:24:49.326415 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 05:25:40.192931 139688679413568 spec.py:349] Evaluating on the test split.
I0216 05:26:06.015414 139688679413568 submission_runner.py:408] Time since start: 53592.49s, 	Step: 60810, 	{'train/ctc_loss': Array(0.08853921, dtype=float32), 'train/wer': 0.033519204361033665, 'validation/ctc_loss': Array(0.3364041, dtype=float32), 'validation/wer': 0.09721270166156579, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18456773, dtype=float32), 'test/wer': 0.05998009465196108, 'test/num_examples': 2472, 'score': 49007.59355258942, 'total_duration': 53592.487142562866, 'accumulated_submission_time': 49007.59355258942, 'accumulated_eval_time': 4580.6306529045105, 'accumulated_logging_time': 1.845522403717041}
I0216 05:26:06.055791 139578135525120 logging_writer.py:48] [60810] accumulated_eval_time=4580.630653, accumulated_logging_time=1.845522, accumulated_submission_time=49007.593553, global_step=60810, preemption_count=0, score=49007.593553, test/ctc_loss=0.18456773459911346, test/num_examples=2472, test/wer=0.059980, total_duration=53592.487143, train/ctc_loss=0.08853920549154282, train/wer=0.033519, validation/ctc_loss=0.33640408515930176, validation/num_examples=5348, validation/wer=0.097213
I0216 05:27:14.738514 139578127132416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.9960529804229736, loss=0.9855121374130249
I0216 05:28:30.398919 139578135525120 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.0451459884643555, loss=0.97664475440979
I0216 05:29:46.187805 139578127132416 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.005319356918335, loss=1.021521806716919
I0216 05:31:02.035996 139578135525120 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.167322039604187, loss=0.9890875816345215
I0216 05:32:17.869806 139578127132416 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.0333304405212402, loss=1.0264503955841064
I0216 05:33:33.911794 139578135525120 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.9636301398277283, loss=1.024863362312317
I0216 05:34:52.783717 139578127132416 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.1978118419647217, loss=0.9910121560096741
I0216 05:36:16.028743 139578135525120 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.9698975682258606, loss=0.986458420753479
I0216 05:37:40.746968 139578127132416 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.279603123664856, loss=0.9443513751029968
I0216 05:39:07.925934 139578135525120 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.1614031791687012, loss=0.9881383180618286
I0216 05:40:23.653325 139578127132416 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.111006736755371, loss=0.9171547293663025
I0216 05:41:39.468694 139578135525120 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.8986587524414062, loss=0.8856044411659241
I0216 05:42:55.250510 139578127132416 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.3598613739013672, loss=1.0472818613052368
I0216 05:44:11.001166 139578135525120 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.9928717017173767, loss=0.973056972026825
I0216 05:45:26.787834 139578127132416 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.9648666381835938, loss=0.9940958619117737
I0216 05:46:47.142685 139578135525120 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.916422963142395, loss=0.9137018322944641
I0216 05:48:10.613714 139578127132416 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.8920705914497375, loss=0.9368555545806885
I0216 05:49:33.548917 139578135525120 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.1923854351043701, loss=0.9698775410652161
I0216 05:50:06.634463 139688679413568 spec.py:321] Evaluating on the training split.
I0216 05:51:00.326887 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 05:51:50.911578 139688679413568 spec.py:349] Evaluating on the test split.
I0216 05:52:16.494313 139688679413568 submission_runner.py:408] Time since start: 55162.97s, 	Step: 62641, 	{'train/ctc_loss': Array(0.10741339, dtype=float32), 'train/wer': 0.04095844747351398, 'validation/ctc_loss': Array(0.32483196, dtype=float32), 'validation/wer': 0.09384322774361104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17749391, dtype=float32), 'test/wer': 0.057969248268437835, 'test/num_examples': 2472, 'score': 50448.088452100754, 'total_duration': 55162.96579504013, 'accumulated_submission_time': 50448.088452100754, 'accumulated_eval_time': 4710.4846367836, 'accumulated_logging_time': 1.9030003547668457}
I0216 05:52:16.540333 139578135525120 logging_writer.py:48] [62641] accumulated_eval_time=4710.484637, accumulated_logging_time=1.903000, accumulated_submission_time=50448.088452, global_step=62641, preemption_count=0, score=50448.088452, test/ctc_loss=0.17749391496181488, test/num_examples=2472, test/wer=0.057969, total_duration=55162.965795, train/ctc_loss=0.1074133887887001, train/wer=0.040958, validation/ctc_loss=0.3248319625854492, validation/num_examples=5348, validation/wer=0.093843
I0216 05:53:02.011813 139578127132416 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.0203324556350708, loss=0.9524763226509094
I0216 05:54:17.845306 139578135525120 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.0633140802383423, loss=0.951815664768219
I0216 05:55:37.066882 139578135525120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.9999191761016846, loss=0.963436484336853
I0216 05:56:52.790541 139578127132416 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0456501245498657, loss=0.9787240028381348
I0216 05:58:08.521199 139578135525120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.9706814885139465, loss=0.9432229995727539
I0216 05:59:24.310774 139578127132416 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.0637421607971191, loss=0.951485812664032
I0216 06:00:40.158563 139578135525120 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.1686275005340576, loss=0.9193040728569031
I0216 06:01:59.361659 139578127132416 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.014263391494751, loss=0.9515601396560669
I0216 06:03:22.807723 139578135525120 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.9925300478935242, loss=0.9386110901832581
I0216 06:04:46.413859 139578127132416 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.3124723434448242, loss=0.9966669082641602
I0216 06:06:09.840411 139578135525120 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.0809451341629028, loss=0.912463366985321
I0216 06:07:32.997090 139578127132416 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.040158987045288, loss=0.9480788111686707
I0216 06:08:56.393828 139578135525120 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.0229021310806274, loss=0.9553678631782532
I0216 06:10:12.129654 139578127132416 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.0718315839767456, loss=0.9565868377685547
I0216 06:11:27.774490 139578135525120 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.0003899335861206, loss=0.908710777759552
I0216 06:12:43.673511 139578127132416 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.0551542043685913, loss=0.8929803967475891
I0216 06:13:59.423145 139578135525120 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.9856225252151489, loss=0.9217429161071777
I0216 06:15:17.408679 139578127132416 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.060079574584961, loss=0.9408093690872192
I0216 06:16:17.209300 139688679413568 spec.py:321] Evaluating on the training split.
I0216 06:17:10.664608 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 06:18:01.210373 139688679413568 spec.py:349] Evaluating on the test split.
I0216 06:18:27.254645 139688679413568 submission_runner.py:408] Time since start: 56733.73s, 	Step: 64473, 	{'train/ctc_loss': Array(0.10360827, dtype=float32), 'train/wer': 0.039054374563204035, 'validation/ctc_loss': Array(0.3186228, dtype=float32), 'validation/wer': 0.09156472962144105, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17470016, dtype=float32), 'test/wer': 0.0573192777202283, 'test/num_examples': 2472, 'score': 51888.67530345917, 'total_duration': 56733.72640347481, 'accumulated_submission_time': 51888.67530345917, 'accumulated_eval_time': 4840.52444434166, 'accumulated_logging_time': 1.9645371437072754}
I0216 06:18:27.295422 139578135525120 logging_writer.py:48] [64473] accumulated_eval_time=4840.524444, accumulated_logging_time=1.964537, accumulated_submission_time=51888.675303, global_step=64473, preemption_count=0, score=51888.675303, test/ctc_loss=0.17470015585422516, test/num_examples=2472, test/wer=0.057319, total_duration=56733.726403, train/ctc_loss=0.10360827296972275, train/wer=0.039054, validation/ctc_loss=0.31862279772758484, validation/num_examples=5348, validation/wer=0.091565
I0216 06:18:48.564881 139578127132416 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.2478008270263672, loss=0.9938206076622009
I0216 06:20:04.422115 139578135525120 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.1024079322814941, loss=0.9490681290626526
I0216 06:21:20.217741 139578127132416 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.2971484661102295, loss=0.9558991193771362
I0216 06:22:35.959779 139578135525120 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.1260825395584106, loss=0.9801925420761108
I0216 06:23:56.510389 139578135525120 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.075116753578186, loss=0.9395314455032349
I0216 06:25:12.252724 139578127132416 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.1863598823547363, loss=0.9860203862190247
I0216 06:26:28.062266 139578135525120 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.9719558954238892, loss=0.898568332195282
I0216 06:27:43.958895 139578127132416 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.1699738502502441, loss=0.8907753229141235
I0216 06:28:59.680811 139578135525120 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.9903922080993652, loss=0.9438326954841614
I0216 06:30:16.932592 139578127132416 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.072525978088379, loss=0.9146569967269897
I0216 06:31:40.842493 139578135525120 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0906028747558594, loss=0.9345517754554749
I0216 06:33:04.750178 139578127132416 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.9401968121528625, loss=0.9422413110733032
I0216 06:34:28.929644 139578135525120 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.2151089906692505, loss=0.9284054636955261
I0216 06:35:53.245666 139578127132416 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.9759620428085327, loss=0.9184517860412598
I0216 06:37:17.212181 139578135525120 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.0943907499313354, loss=0.9246528148651123
I0216 06:38:37.466961 139578135525120 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.0254571437835693, loss=0.8858832716941833
I0216 06:39:53.225474 139578127132416 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.3922550678253174, loss=0.9249807596206665
I0216 06:41:08.990946 139578135525120 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.1591532230377197, loss=0.9465834498405457
I0216 06:42:24.770045 139578127132416 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.2445157766342163, loss=0.9651060104370117
I0216 06:42:27.510604 139688679413568 spec.py:321] Evaluating on the training split.
I0216 06:43:19.485812 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 06:44:10.878955 139688679413568 spec.py:349] Evaluating on the test split.
I0216 06:44:36.869511 139688679413568 submission_runner.py:408] Time since start: 58303.34s, 	Step: 66305, 	{'train/ctc_loss': Array(0.12077924, dtype=float32), 'train/wer': 0.047228092993009264, 'validation/ctc_loss': Array(0.31349313, dtype=float32), 'validation/wer': 0.08982689207063344, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16910523, dtype=float32), 'test/wer': 0.05605995978307233, 'test/num_examples': 2472, 'score': 53328.806980371475, 'total_duration': 58303.341222286224, 'accumulated_submission_time': 53328.806980371475, 'accumulated_eval_time': 4969.87771320343, 'accumulated_logging_time': 2.021014928817749}
I0216 06:44:36.912150 139578135525120 logging_writer.py:48] [66305] accumulated_eval_time=4969.877713, accumulated_logging_time=2.021015, accumulated_submission_time=53328.806980, global_step=66305, preemption_count=0, score=53328.806980, test/ctc_loss=0.16910523176193237, test/num_examples=2472, test/wer=0.056060, total_duration=58303.341222, train/ctc_loss=0.12077923864126205, train/wer=0.047228, validation/ctc_loss=0.31349313259124756, validation/num_examples=5348, validation/wer=0.089827
I0216 06:45:49.475673 139578127132416 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.260658621788025, loss=0.965940535068512
I0216 06:47:05.177441 139578135525120 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.9944218993186951, loss=0.8596481084823608
I0216 06:48:20.866108 139578127132416 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.0773645639419556, loss=0.9422513842582703
I0216 06:49:38.256691 139578135525120 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.0428483486175537, loss=0.9173579216003418
I0216 06:51:02.963417 139578127132416 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.0924313068389893, loss=0.9140484929084778
I0216 06:52:26.309424 139578135525120 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.291696548461914, loss=0.9400012493133545
I0216 06:53:48.933954 139578135525120 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.5589423179626465, loss=0.9378728270530701
I0216 06:55:04.661351 139578127132416 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.085317850112915, loss=0.919137179851532
I0216 06:56:20.725615 139578135525120 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.9995298385620117, loss=0.8916313052177429
I0216 06:57:36.511941 139578127132416 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.0281307697296143, loss=0.8674560189247131
I0216 06:58:52.239742 139578135525120 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.5036282539367676, loss=0.9272044897079468
I0216 07:00:11.918641 139578127132416 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.157564640045166, loss=0.9387013912200928
I0216 07:01:35.951076 139578135525120 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.316247582435608, loss=0.897536039352417
I0216 07:02:59.506267 139578127132416 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.0991164445877075, loss=0.8948601484298706
I0216 07:04:23.916710 139578135525120 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.201849341392517, loss=0.9006578922271729
I0216 07:05:47.834833 139578127132416 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.0977340936660767, loss=0.8466622829437256
I0216 07:07:13.905576 139578135525120 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.0620853900909424, loss=0.8897483348846436
I0216 07:08:29.565499 139578127132416 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.1141633987426758, loss=0.8534433841705322
I0216 07:08:36.866480 139688679413568 spec.py:321] Evaluating on the training split.
I0216 07:09:29.077998 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 07:10:20.905871 139688679413568 spec.py:349] Evaluating on the test split.
I0216 07:10:47.205549 139688679413568 submission_runner.py:408] Time since start: 59873.68s, 	Step: 68111, 	{'train/ctc_loss': Array(0.09411724, dtype=float32), 'train/wer': 0.03501274466244183, 'validation/ctc_loss': Array(0.30283234, dtype=float32), 'validation/wer': 0.08693049615262076, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16567686, dtype=float32), 'test/wer': 0.054313163934759205, 'test/num_examples': 2472, 'score': 54768.677815914154, 'total_duration': 59873.677282094955, 'accumulated_submission_time': 54768.677815914154, 'accumulated_eval_time': 5100.211203813553, 'accumulated_logging_time': 2.079517364501953}
I0216 07:10:47.246002 139578135525120 logging_writer.py:48] [68111] accumulated_eval_time=5100.211204, accumulated_logging_time=2.079517, accumulated_submission_time=54768.677816, global_step=68111, preemption_count=0, score=54768.677816, test/ctc_loss=0.16567686200141907, test/num_examples=2472, test/wer=0.054313, total_duration=59873.677282, train/ctc_loss=0.09411723911762238, train/wer=0.035013, validation/ctc_loss=0.30283233523368835, validation/num_examples=5348, validation/wer=0.086930
I0216 07:11:55.331308 139578127132416 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.0966836214065552, loss=0.8840796947479248
I0216 07:13:11.317445 139578135525120 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.3014146089553833, loss=0.9172912836074829
I0216 07:14:27.116837 139578127132416 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.1718964576721191, loss=0.9311903119087219
I0216 07:15:42.975766 139578135525120 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.0679768323898315, loss=0.8759599328041077
I0216 07:16:58.876638 139578127132416 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.0639845132827759, loss=0.8949544429779053
I0216 07:18:20.748728 139578135525120 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.0964429378509521, loss=0.9123600125312805
I0216 07:19:45.466127 139578127132416 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.0977749824523926, loss=0.8991706371307373
I0216 07:21:09.336231 139578135525120 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.2048591375350952, loss=0.8632871508598328
I0216 07:22:33.332662 139578127132416 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.078210473060608, loss=0.8836124539375305
I0216 07:23:52.838274 139578135525120 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.3484402894973755, loss=0.927492618560791
I0216 07:25:08.694148 139578127132416 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.1253830194473267, loss=0.9019521474838257
I0216 07:26:24.436350 139578135525120 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.1190979480743408, loss=0.895426869392395
I0216 07:27:40.170333 139578127132416 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.0791504383087158, loss=0.8689683079719543
I0216 07:28:56.146960 139578135525120 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.0699020624160767, loss=0.8811152577400208
I0216 07:30:20.441679 139578127132416 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1659647226333618, loss=0.8962377905845642
I0216 07:31:44.778222 139578135525120 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.3013802766799927, loss=0.8895924091339111
I0216 07:33:09.053931 139578127132416 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.2226585149765015, loss=0.8482043743133545
I0216 07:34:33.262442 139578135525120 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.4274812936782837, loss=0.871053159236908
I0216 07:34:48.032099 139688679413568 spec.py:321] Evaluating on the training split.
I0216 07:35:41.274392 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 07:36:33.557298 139688679413568 spec.py:349] Evaluating on the test split.
I0216 07:36:59.784165 139688679413568 submission_runner.py:408] Time since start: 61446.26s, 	Step: 69919, 	{'train/ctc_loss': Array(0.09114316, dtype=float32), 'train/wer': 0.035440907178034824, 'validation/ctc_loss': Array(0.30461708, dtype=float32), 'validation/wer': 0.08602295876497679, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16325577, dtype=float32), 'test/wer': 0.05230231755123596, 'test/num_examples': 2472, 'score': 56209.37894654274, 'total_duration': 61446.25600028038, 'accumulated_submission_time': 56209.37894654274, 'accumulated_eval_time': 5231.957772254944, 'accumulated_logging_time': 2.138704776763916}
I0216 07:36:59.827181 139578135525120 logging_writer.py:48] [69919] accumulated_eval_time=5231.957772, accumulated_logging_time=2.138705, accumulated_submission_time=56209.378947, global_step=69919, preemption_count=0, score=56209.378947, test/ctc_loss=0.16325576603412628, test/num_examples=2472, test/wer=0.052302, total_duration=61446.256000, train/ctc_loss=0.0911431610584259, train/wer=0.035441, validation/ctc_loss=0.3046170771121979, validation/num_examples=5348, validation/wer=0.086023
I0216 07:38:01.871199 139578127132416 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1992390155792236, loss=0.9012477993965149
I0216 07:39:20.984772 139578135525120 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.2597285509109497, loss=0.9022399187088013
I0216 07:40:36.674921 139578127132416 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.0786433219909668, loss=0.8856536149978638
I0216 07:41:52.517685 139578135525120 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.0933020114898682, loss=0.9038487672805786
I0216 07:43:08.270545 139578127132416 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.3760650157928467, loss=0.8834165334701538
I0216 07:44:26.074438 139578135525120 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1810756921768188, loss=0.8797574639320374
I0216 07:45:50.285157 139578127132416 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.0371507406234741, loss=0.8775098919868469
I0216 07:47:13.775037 139578135525120 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.1599950790405273, loss=0.8862041234970093
I0216 07:48:39.027160 139578127132416 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.1326550245285034, loss=0.9581704139709473
I0216 07:50:02.508179 139578135525120 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.1954172849655151, loss=0.9075747132301331
I0216 07:51:27.254689 139578127132416 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.186444878578186, loss=0.9066022634506226
I0216 07:52:51.943171 139578135525120 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.0805870294570923, loss=0.8743978142738342
I0216 07:54:07.718742 139578127132416 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.1870293617248535, loss=0.9150827527046204
I0216 07:55:23.451952 139578135525120 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.2756620645523071, loss=0.8603052496910095
I0216 07:56:39.285876 139578127132416 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.5441038608551025, loss=0.8738105297088623
I0216 07:57:55.121320 139578135525120 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.0444884300231934, loss=0.8790699243545532
I0216 07:59:13.775417 139578127132416 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.1510484218597412, loss=0.8301602005958557
I0216 08:00:38.467475 139578135525120 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1711398363113403, loss=0.8898688554763794
I0216 08:01:00.573456 139688679413568 spec.py:321] Evaluating on the training split.
I0216 08:01:55.077483 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 08:02:47.210616 139688679413568 spec.py:349] Evaluating on the test split.
I0216 08:03:14.412140 139688679413568 submission_runner.py:408] Time since start: 63020.88s, 	Step: 71727, 	{'train/ctc_loss': Array(0.06677296, dtype=float32), 'train/wer': 0.025365145116604353, 'validation/ctc_loss': Array(0.29442066, dtype=float32), 'validation/wer': 0.08297208839800342, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15865384, dtype=float32), 'test/wer': 0.051144557512237725, 'test/num_examples': 2472, 'score': 57650.042269945145, 'total_duration': 63020.883692502975, 'accumulated_submission_time': 57650.042269945145, 'accumulated_eval_time': 5365.79065990448, 'accumulated_logging_time': 2.1979687213897705}
I0216 08:03:14.455088 139578135525120 logging_writer.py:48] [71727] accumulated_eval_time=5365.790660, accumulated_logging_time=2.197969, accumulated_submission_time=57650.042270, global_step=71727, preemption_count=0, score=57650.042270, test/ctc_loss=0.1586538404226303, test/num_examples=2472, test/wer=0.051145, total_duration=63020.883693, train/ctc_loss=0.0667729601264, train/wer=0.025365, validation/ctc_loss=0.29442065954208374, validation/num_examples=5348, validation/wer=0.082972
I0216 08:04:10.393723 139578127132416 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.154478669166565, loss=0.8530566096305847
I0216 08:05:26.148099 139578135525120 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.5034099817276, loss=0.8272286653518677
I0216 08:06:41.872696 139578127132416 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.3416056632995605, loss=0.9250670671463013
I0216 08:08:01.031523 139578135525120 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.1381657123565674, loss=0.8528193831443787
I0216 08:09:16.815010 139578127132416 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.0497338771820068, loss=0.8538284301757812
I0216 08:10:32.638254 139578135525120 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.1976839303970337, loss=0.8538099527359009
I0216 08:11:48.486276 139578127132416 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.2073328495025635, loss=0.8720282912254333
I0216 08:13:04.338766 139578135525120 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.1036478281021118, loss=0.8752707839012146
I0216 08:14:22.238161 139578127132416 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.3592578172683716, loss=0.8695975542068481
I0216 08:15:45.616892 139578135525120 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.3310279846191406, loss=0.8633861541748047
I0216 08:17:08.836982 139578127132416 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.1334832906723022, loss=0.8777263760566711
I0216 08:18:32.032537 139578135525120 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.1134368181228638, loss=0.8921794891357422
I0216 08:19:56.048292 139578127132416 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.2033228874206543, loss=0.8993586301803589
I0216 08:21:20.299907 139578135525120 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.107953667640686, loss=0.8373891711235046
I0216 08:22:41.125775 139578135525120 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.4886531829833984, loss=0.9007194638252258
I0216 08:23:56.801036 139578127132416 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.3842768669128418, loss=0.8494511246681213
I0216 08:25:12.611427 139578135525120 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.2012357711791992, loss=0.8466365933418274
I0216 08:26:28.367178 139578127132416 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.2104450464248657, loss=0.8355830311775208
I0216 08:27:15.040695 139688679413568 spec.py:321] Evaluating on the training split.
I0216 08:28:09.172162 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 08:29:01.099618 139688679413568 spec.py:349] Evaluating on the test split.
I0216 08:29:27.810883 139688679413568 submission_runner.py:408] Time since start: 64594.28s, 	Step: 73563, 	{'train/ctc_loss': Array(0.07948388, dtype=float32), 'train/wer': 0.030399894908537586, 'validation/ctc_loss': Array(0.29232764, dtype=float32), 'validation/wer': 0.08214178823483978, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15709297, dtype=float32), 'test/wer': 0.050393029065870454, 'test/num_examples': 2472, 'score': 59090.54529213905, 'total_duration': 64594.28295874596, 'accumulated_submission_time': 59090.54529213905, 'accumulated_eval_time': 5498.555616617203, 'accumulated_logging_time': 2.2566795349121094}
I0216 08:29:27.855299 139578135525120 logging_writer.py:48] [73563] accumulated_eval_time=5498.555617, accumulated_logging_time=2.256680, accumulated_submission_time=59090.545292, global_step=73563, preemption_count=0, score=59090.545292, test/ctc_loss=0.15709297358989716, test/num_examples=2472, test/wer=0.050393, total_duration=64594.282959, train/ctc_loss=0.07948388159275055, train/wer=0.030400, validation/ctc_loss=0.2923276424407959, validation/num_examples=5348, validation/wer=0.082142
I0216 08:29:56.610686 139578127132416 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.1568732261657715, loss=0.8698322772979736
I0216 08:31:12.317843 139578135525120 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.1941267251968384, loss=0.8397201299667358
I0216 08:32:27.878567 139578127132416 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.2869410514831543, loss=0.8508872389793396
I0216 08:33:43.498797 139578135525120 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.4836188554763794, loss=0.8688408136367798
I0216 08:35:04.696163 139578127132416 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.2340768575668335, loss=0.8720807433128357
I0216 08:36:28.466070 139578135525120 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.6921941041946411, loss=0.8622453212738037
I0216 08:37:52.486150 139578135525120 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.2632291316986084, loss=0.878217339515686
I0216 08:39:08.239667 139578127132416 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.1345728635787964, loss=0.8044813275337219
I0216 08:40:23.994236 139578135525120 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.1805092096328735, loss=0.8444002866744995
I0216 08:41:39.720612 139578127132416 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.260345220565796, loss=0.8454713225364685
I0216 08:42:55.433714 139578135525120 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2446022033691406, loss=0.8661894202232361
I0216 08:44:16.825213 139578127132416 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.2507928609848022, loss=0.8603830337524414
I0216 08:45:41.548974 139578135525120 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.2459906339645386, loss=0.8874344825744629
I0216 08:47:06.378309 139578127132416 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.2488752603530884, loss=0.8437671661376953
I0216 08:48:30.685163 139578135525120 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.1480607986450195, loss=0.814259946346283
I0216 08:49:57.408658 139578127132416 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.2382564544677734, loss=0.8196600675582886
I0216 08:51:24.248292 139578135525120 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.3494325876235962, loss=0.8208674192428589
I0216 08:52:40.087286 139578127132416 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.6075434684753418, loss=0.8679661154747009
I0216 08:53:28.245800 139688679413568 spec.py:321] Evaluating on the training split.
I0216 08:54:22.373003 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 08:55:14.392780 139688679413568 spec.py:349] Evaluating on the test split.
I0216 08:55:40.831970 139688679413568 submission_runner.py:408] Time since start: 66167.30s, 	Step: 75365, 	{'train/ctc_loss': Array(0.06929343, dtype=float32), 'train/wer': 0.025832999331997328, 'validation/ctc_loss': Array(0.29072267, dtype=float32), 'validation/wer': 0.08138872529615648, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15438108, dtype=float32), 'test/wer': 0.05033209432697581, 'test/num_examples': 2472, 'score': 60530.85288310051, 'total_duration': 66167.30356693268, 'accumulated_submission_time': 60530.85288310051, 'accumulated_eval_time': 5631.136070251465, 'accumulated_logging_time': 2.3183770179748535}
I0216 08:55:40.879323 139578135525120 logging_writer.py:48] [75365] accumulated_eval_time=5631.136070, accumulated_logging_time=2.318377, accumulated_submission_time=60530.852883, global_step=75365, preemption_count=0, score=60530.852883, test/ctc_loss=0.15438108146190643, test/num_examples=2472, test/wer=0.050332, total_duration=66167.303567, train/ctc_loss=0.06929343193769455, train/wer=0.025833, validation/ctc_loss=0.29072266817092896, validation/num_examples=5348, validation/wer=0.081389
I0216 08:56:08.145466 139578127132416 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.425952434539795, loss=0.8792169690132141
I0216 08:57:23.915040 139578135525120 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.325217604637146, loss=0.8898230791091919
I0216 08:58:39.680557 139578127132416 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.0909311771392822, loss=0.8512330055236816
I0216 08:59:55.445910 139578135525120 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.3191330432891846, loss=0.8421127200126648
I0216 09:01:11.205531 139578127132416 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.2323410511016846, loss=0.8434012532234192
I0216 09:02:34.421446 139578135525120 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.4615594148635864, loss=0.8791696429252625
I0216 09:03:58.325809 139578127132416 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.2416337728500366, loss=0.8203890919685364
I0216 09:04:38.168568 139578135525120 logging_writer.py:48] [76049] global_step=76049, preemption_count=0, score=61068.082364
I0216 09:04:39.039531 139688679413568 checkpoints.py:490] Saving checkpoint at step: 76049
I0216 09:04:40.633846 139688679413568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_3/checkpoint_76049
I0216 09:04:40.668933 139688679413568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_3/checkpoint_76049.
I0216 09:04:44.275884 139688679413568 submission_runner.py:583] Tuning trial 3/5
I0216 09:04:44.276159 139688679413568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0216 09:04:44.303263 139688679413568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.732792, dtype=float32), 'train/wer': 0.9418907725440389, 'validation/ctc_loss': Array(30.141817, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 34.96774101257324, 'total_duration': 152.49866819381714, 'accumulated_submission_time': 34.96774101257324, 'accumulated_eval_time': 117.53086447715759, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1680, {'train/ctc_loss': Array(6.382445, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(6.4972925, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.5108743, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1475.8832149505615, 'total_duration': 1705.5838894844055, 'accumulated_submission_time': 1475.8832149505615, 'accumulated_eval_time': 229.59972071647644, 'accumulated_logging_time': 0.03151512145996094, 'global_step': 1680, 'preemption_count': 0}), (3371, {'train/ctc_loss': Array(3.8691866, dtype=float32), 'train/wer': 0.7855008984725966, 'validation/ctc_loss': Array(3.8361292, dtype=float32), 'validation/wer': 0.7482549214593973, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.5268924, dtype=float32), 'test/wer': 0.6986777161659862, 'test/num_examples': 2472, 'score': 2916.303174495697, 'total_duration': 3267.289516210556, 'accumulated_submission_time': 2916.303174495697, 'accumulated_eval_time': 350.76149916648865, 'accumulated_logging_time': 0.0831305980682373, 'global_step': 3371, 'preemption_count': 0}), (5028, {'train/ctc_loss': Array(1.2378957, dtype=float32), 'train/wer': 0.3726033340564353, 'validation/ctc_loss': Array(1.2712711, dtype=float32), 'validation/wer': 0.359597207874335, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.95981365, dtype=float32), 'test/wer': 0.30034732801169944, 'test/num_examples': 2472, 'score': 4356.832535982132, 'total_duration': 4844.810940742493, 'accumulated_submission_time': 4356.832535982132, 'accumulated_eval_time': 487.6277189254761, 'accumulated_logging_time': 0.13818788528442383, 'global_step': 5028, 'preemption_count': 0}), (6698, {'train/ctc_loss': Array(0.73739135, dtype=float32), 'train/wer': 0.2415693151924872, 'validation/ctc_loss': Array(0.87928903, dtype=float32), 'validation/wer': 0.26074321519256205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6022405, dtype=float32), 'test/wer': 0.19952064672069547, 'test/num_examples': 2472, 'score': 5798.092703580856, 'total_duration': 6424.067903518677, 'accumulated_submission_time': 5798.092703580856, 'accumulated_eval_time': 625.5019948482513, 'accumulated_logging_time': 0.18807625770568848, 'global_step': 6698, 'preemption_count': 0}), (8371, {'train/ctc_loss': Array(0.6227412, dtype=float32), 'train/wer': 0.20609788163204645, 'validation/ctc_loss': Array(0.7817897, dtype=float32), 'validation/wer': 0.23225233401237727, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5196976, dtype=float32), 'test/wer': 0.17047508784758192, 'test/num_examples': 2472, 'score': 7238.464210033417, 'total_duration': 7999.998436927795, 'accumulated_submission_time': 7238.464210033417, 'accumulated_eval_time': 760.9348003864288, 'accumulated_logging_time': 0.24220538139343262, 'global_step': 8371, 'preemption_count': 0}), (10043, {'train/ctc_loss': Array(0.6011116, dtype=float32), 'train/wer': 0.20390464054965843, 'validation/ctc_loss': Array(0.70295954, dtype=float32), 'validation/wer': 0.21234443940257006, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4573212, dtype=float32), 'test/wer': 0.15495704100907928, 'test/num_examples': 2472, 'score': 8678.964599847794, 'total_duration': 9576.111248016357, 'accumulated_submission_time': 8678.964599847794, 'accumulated_eval_time': 896.4183924198151, 'accumulated_logging_time': 0.2991371154785156, 'global_step': 10043, 'preemption_count': 0}), (11739, {'train/ctc_loss': Array(0.51052195, dtype=float32), 'train/wer': 0.17512704591602068, 'validation/ctc_loss': Array(0.6549365, dtype=float32), 'validation/wer': 0.19736041785338443, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4183106, dtype=float32), 'test/wer': 0.14236386163751955, 'test/num_examples': 2472, 'score': 10118.942187547684, 'total_duration': 11149.701045513153, 'accumulated_submission_time': 10118.942187547684, 'accumulated_eval_time': 1029.9016468524933, 'accumulated_logging_time': 0.35513925552368164, 'global_step': 11739, 'preemption_count': 0}), (13408, {'train/ctc_loss': Array(0.51378894, dtype=float32), 'train/wer': 0.1716898590868122, 'validation/ctc_loss': Array(0.63236403, dtype=float32), 'validation/wer': 0.19217586916014173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40097797, dtype=float32), 'test/wer': 0.13553917088131945, 'test/num_examples': 2472, 'score': 11559.051738500595, 'total_duration': 12726.73373579979, 'accumulated_submission_time': 11559.051738500595, 'accumulated_eval_time': 1166.7034318447113, 'accumulated_logging_time': 0.40659093856811523, 'global_step': 13408, 'preemption_count': 0}), (15097, {'train/ctc_loss': Array(0.42208117, dtype=float32), 'train/wer': 0.14745705920803281, 'validation/ctc_loss': Array(0.6005494, dtype=float32), 'validation/wer': 0.18247294283479923, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37557527, dtype=float32), 'test/wer': 0.12836918327138302, 'test/num_examples': 2472, 'score': 12999.760911226273, 'total_duration': 14303.689206123352, 'accumulated_submission_time': 12999.760911226273, 'accumulated_eval_time': 1302.8229024410248, 'accumulated_logging_time': 0.46058177947998047, 'global_step': 15097, 'preemption_count': 0}), (16782, {'train/ctc_loss': Array(0.42158687, dtype=float32), 'train/wer': 0.1475592811097697, 'validation/ctc_loss': Array(0.5759482, dtype=float32), 'validation/wer': 0.17342653291753962, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35843527, dtype=float32), 'test/wer': 0.11979769666686978, 'test/num_examples': 2472, 'score': 14440.03445315361, 'total_duration': 15878.557571172714, 'accumulated_submission_time': 14440.03445315361, 'accumulated_eval_time': 1437.2882940769196, 'accumulated_logging_time': 0.5167298316955566, 'global_step': 16782, 'preemption_count': 0}), (18502, {'train/ctc_loss': Array(0.40602422, dtype=float32), 'train/wer': 0.14388951610198858, 'validation/ctc_loss': Array(0.56187373, dtype=float32), 'validation/wer': 0.16890815528543981, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.346281, dtype=float32), 'test/wer': 0.11701501025734772, 'test/num_examples': 2472, 'score': 15880.001097917557, 'total_duration': 17446.604358911514, 'accumulated_submission_time': 15880.001097917557, 'accumulated_eval_time': 1565.2552840709686, 'accumulated_logging_time': 0.5599899291992188, 'global_step': 18502, 'preemption_count': 0}), (20363, {'train/ctc_loss': Array(0.26609316, dtype=float32), 'train/wer': 0.0986750889965472, 'validation/ctc_loss': Array(0.5398281, dtype=float32), 'validation/wer': 0.16431254042885968, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32706234, dtype=float32), 'test/wer': 0.11146994901793512, 'test/num_examples': 2472, 'score': 17320.446226596832, 'total_duration': 19024.20640349388, 'accumulated_submission_time': 17320.446226596832, 'accumulated_eval_time': 1702.2899096012115, 'accumulated_logging_time': 0.6148602962493896, 'global_step': 20363, 'preemption_count': 0}), (22227, {'train/ctc_loss': Array(0.2531626, dtype=float32), 'train/wer': 0.09213394914396124, 'validation/ctc_loss': Array(0.5354374, dtype=float32), 'validation/wer': 0.16039275128648253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32297328, dtype=float32), 'test/wer': 0.1081997846972559, 'test/num_examples': 2472, 'score': 18761.039699316025, 'total_duration': 20595.775464057922, 'accumulated_submission_time': 18761.039699316025, 'accumulated_eval_time': 1833.1509010791779, 'accumulated_logging_time': 0.6625058650970459, 'global_step': 22227, 'preemption_count': 0}), (24083, {'train/ctc_loss': Array(0.24372207, dtype=float32), 'train/wer': 0.09096693115433355, 'validation/ctc_loss': Array(0.516554, dtype=float32), 'validation/wer': 0.15621228651148422, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30685455, dtype=float32), 'test/wer': 0.10417809193020941, 'test/num_examples': 2472, 'score': 20201.548386335373, 'total_duration': 22164.801256656647, 'accumulated_submission_time': 20201.548386335373, 'accumulated_eval_time': 1961.5512776374817, 'accumulated_logging_time': 0.7109498977661133, 'global_step': 24083, 'preemption_count': 0}), (25925, {'train/ctc_loss': Array(0.2315767, dtype=float32), 'train/wer': 0.08636087293767426, 'validation/ctc_loss': Array(0.5072757, dtype=float32), 'validation/wer': 0.15324830802205122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29840583, dtype=float32), 'test/wer': 0.10157820973737128, 'test/num_examples': 2472, 'score': 21641.7920794487, 'total_duration': 23733.50555229187, 'accumulated_submission_time': 21641.7920794487, 'accumulated_eval_time': 2089.888088941574, 'accumulated_logging_time': 0.7663748264312744, 'global_step': 25925, 'preemption_count': 0}), (27770, {'train/ctc_loss': Array(0.2280684, dtype=float32), 'train/wer': 0.08473699921110424, 'validation/ctc_loss': Array(0.49400762, dtype=float32), 'validation/wer': 0.14941541075721443, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29091525, dtype=float32), 'test/wer': 0.09796274856295574, 'test/num_examples': 2472, 'score': 23081.903094768524, 'total_duration': 25302.70827627182, 'accumulated_submission_time': 23081.903094768524, 'accumulated_eval_time': 2218.8633258342743, 'accumulated_logging_time': 0.8157093524932861, 'global_step': 27770, 'preemption_count': 0}), (29607, {'train/ctc_loss': Array(0.21079952, dtype=float32), 'train/wer': 0.07880682626964503, 'validation/ctc_loss': Array(0.48705956, dtype=float32), 'validation/wer': 0.14607490079843982, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28334573, dtype=float32), 'test/wer': 0.09485507687932891, 'test/num_examples': 2472, 'score': 24522.007591724396, 'total_duration': 26874.034334897995, 'accumulated_submission_time': 24522.007591724396, 'accumulated_eval_time': 2349.9573764801025, 'accumulated_logging_time': 0.8724699020385742, 'global_step': 29607, 'preemption_count': 0}), (31457, {'train/ctc_loss': Array(0.24844179, dtype=float32), 'train/wer': 0.08890363063939927, 'validation/ctc_loss': Array(0.48052293, dtype=float32), 'validation/wer': 0.14382536663544995, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28156066, dtype=float32), 'test/wer': 0.09574878638311701, 'test/num_examples': 2472, 'score': 25962.538994789124, 'total_duration': 28444.344913959503, 'accumulated_submission_time': 25962.538994789124, 'accumulated_eval_time': 2479.606421470642, 'accumulated_logging_time': 0.9324443340301514, 'global_step': 31457, 'preemption_count': 0}), (33310, {'train/ctc_loss': Array(0.20790268, dtype=float32), 'train/wer': 0.07849532162816839, 'validation/ctc_loss': Array(0.46711764, dtype=float32), 'validation/wer': 0.14012763451345375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27785712, dtype=float32), 'test/wer': 0.09339264314585745, 'test/num_examples': 2472, 'score': 27403.022592306137, 'total_duration': 30015.006851911545, 'accumulated_submission_time': 27403.022592306137, 'accumulated_eval_time': 2609.6571514606476, 'accumulated_logging_time': 0.989710807800293, 'global_step': 33310, 'preemption_count': 0}), (35145, {'train/ctc_loss': Array(0.18601915, dtype=float32), 'train/wer': 0.07065032078015412, 'validation/ctc_loss': Array(0.45886686, dtype=float32), 'validation/wer': 0.13640093843227744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26721686, dtype=float32), 'test/wer': 0.09050839883817764, 'test/num_examples': 2472, 'score': 28843.551401615143, 'total_duration': 31586.579282283783, 'accumulated_submission_time': 28843.551401615143, 'accumulated_eval_time': 2740.5739209651947, 'accumulated_logging_time': 1.044804573059082, 'global_step': 35145, 'preemption_count': 0}), (36979, {'train/ctc_loss': Array(0.178505, dtype=float32), 'train/wer': 0.06665957955723284, 'validation/ctc_loss': Array(0.4410436, dtype=float32), 'validation/wer': 0.1324618399837802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25798786, dtype=float32), 'test/wer': 0.08608047447850019, 'test/num_examples': 2472, 'score': 30283.677778959274, 'total_duration': 33157.48137187958, 'accumulated_submission_time': 30283.677778959274, 'accumulated_eval_time': 2871.2297463417053, 'accumulated_logging_time': 1.0952837467193604, 'global_step': 36979, 'preemption_count': 0}), (38817, {'train/ctc_loss': Array(0.17209044, dtype=float32), 'train/wer': 0.06707041343669251, 'validation/ctc_loss': Array(0.43647054, dtype=float32), 'validation/wer': 0.13019299651467026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25531837, dtype=float32), 'test/wer': 0.08693356082302521, 'test/num_examples': 2472, 'score': 31723.920412540436, 'total_duration': 34728.01073908806, 'accumulated_submission_time': 31723.920412540436, 'accumulated_eval_time': 3001.384289264679, 'accumulated_logging_time': 1.1554756164550781, 'global_step': 38817, 'preemption_count': 0}), (40660, {'train/ctc_loss': Array(0.18338138, dtype=float32), 'train/wer': 0.06747332908774165, 'validation/ctc_loss': Array(0.4277794, dtype=float32), 'validation/wer': 0.12773105998435946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24571325, dtype=float32), 'test/wer': 0.08276968699855788, 'test/num_examples': 2472, 'score': 33163.837792634964, 'total_duration': 36299.36015820503, 'accumulated_submission_time': 33163.837792634964, 'accumulated_eval_time': 3132.688977241516, 'accumulated_logging_time': 1.2110445499420166, 'global_step': 40660, 'preemption_count': 0}), (42494, {'train/ctc_loss': Array(0.1785813, dtype=float32), 'train/wer': 0.06711494210506916, 'validation/ctc_loss': Array(0.42371103, dtype=float32), 'validation/wer': 0.126253898066173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2410632, dtype=float32), 'test/wer': 0.08031198586314058, 'test/num_examples': 2472, 'score': 34604.3055870533, 'total_duration': 37871.41095805168, 'accumulated_submission_time': 34604.3055870533, 'accumulated_eval_time': 3264.1424934864044, 'accumulated_logging_time': 1.2691049575805664, 'global_step': 42494, 'preemption_count': 0}), (44317, {'train/ctc_loss': Array(0.15933198, dtype=float32), 'train/wer': 0.06103466511816004, 'validation/ctc_loss': Array(0.4057534, dtype=float32), 'validation/wer': 0.12189964953609392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2344677, dtype=float32), 'test/wer': 0.07941827635935246, 'test/num_examples': 2472, 'score': 36044.50428843498, 'total_duration': 39444.46064519882, 'accumulated_submission_time': 36044.50428843498, 'accumulated_eval_time': 3396.860850095749, 'accumulated_logging_time': 1.3300681114196777, 'global_step': 44317, 'preemption_count': 0}), (46154, {'train/ctc_loss': Array(0.15425502, dtype=float32), 'train/wer': 0.059206374867668304, 'validation/ctc_loss': Array(0.39746863, dtype=float32), 'validation/wer': 0.11965977002616411, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22770649, dtype=float32), 'test/wer': 0.07643247415351492, 'test/num_examples': 2472, 'score': 37484.47999191284, 'total_duration': 41015.45380330086, 'accumulated_submission_time': 37484.47999191284, 'accumulated_eval_time': 3527.749038219452, 'accumulated_logging_time': 1.3891162872314453, 'global_step': 46154, 'preemption_count': 0}), (47997, {'train/ctc_loss': Array(0.13490936, dtype=float32), 'train/wer': 0.05189839995775466, 'validation/ctc_loss': Array(0.39513963, dtype=float32), 'validation/wer': 0.11727507072033366, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22139996, dtype=float32), 'test/wer': 0.07484817094225418, 'test/num_examples': 2472, 'score': 38924.55057263374, 'total_duration': 42587.35040640831, 'accumulated_submission_time': 38924.55057263374, 'accumulated_eval_time': 3659.4434146881104, 'accumulated_logging_time': 1.4481637477874756, 'global_step': 47997, 'preemption_count': 0}), (49845, {'train/ctc_loss': Array(0.13607302, dtype=float32), 'train/wer': 0.051533911551074195, 'validation/ctc_loss': Array(0.38606805, dtype=float32), 'validation/wer': 0.11457176786352183, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21825966, dtype=float32), 'test/wer': 0.07375134564215059, 'test/num_examples': 2472, 'score': 40364.931837558746, 'total_duration': 44160.38792181015, 'accumulated_submission_time': 40364.931837558746, 'accumulated_eval_time': 3791.9704139232635, 'accumulated_logging_time': 1.503833293914795, 'global_step': 49845, 'preemption_count': 0}), (51671, {'train/ctc_loss': Array(0.13753137, dtype=float32), 'train/wer': 0.05211096404743254, 'validation/ctc_loss': Array(0.3752853, dtype=float32), 'validation/wer': 0.11043957635382373, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21176472, dtype=float32), 'test/wer': 0.07182174557715354, 'test/num_examples': 2472, 'score': 41805.41350221634, 'total_duration': 45732.29967999458, 'accumulated_submission_time': 41805.41350221634, 'accumulated_eval_time': 3923.273575782776, 'accumulated_logging_time': 1.5587153434753418, 'global_step': 51671, 'preemption_count': 0}), (53500, {'train/ctc_loss': Array(0.12048063, dtype=float32), 'train/wer': 0.04511261977815764, 'validation/ctc_loss': Array(0.37096584, dtype=float32), 'validation/wer': 0.10881759463973661, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20651865, dtype=float32), 'test/wer': 0.06838908861942193, 'test/num_examples': 2472, 'score': 43246.1699757576, 'total_duration': 47303.89137673378, 'accumulated_submission_time': 43246.1699757576, 'accumulated_eval_time': 4053.9796431064606, 'accumulated_logging_time': 1.6170992851257324, 'global_step': 53500, 'preemption_count': 0}), (55323, {'train/ctc_loss': Array(0.11976799, dtype=float32), 'train/wer': 0.04654713130715187, 'validation/ctc_loss': Array(0.35839888, dtype=float32), 'validation/wer': 0.10489780549735946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19546556, dtype=float32), 'test/wer': 0.06597201064326773, 'test/num_examples': 2472, 'score': 44686.96074914932, 'total_duration': 48875.97346353531, 'accumulated_submission_time': 44686.96074914932, 'accumulated_eval_time': 4185.1421592235565, 'accumulated_logging_time': 1.6726500988006592, 'global_step': 55323, 'preemption_count': 0}), (57163, {'train/ctc_loss': Array(0.13565873, dtype=float32), 'train/wer': 0.04769781455932176, 'validation/ctc_loss': Array(0.35523295, dtype=float32), 'validation/wer': 0.10475298570145881, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19099317, dtype=float32), 'test/wer': 0.06446895375053317, 'test/num_examples': 2472, 'score': 46126.98453450203, 'total_duration': 50448.36845588684, 'accumulated_submission_time': 46126.98453450203, 'accumulated_eval_time': 4317.38533616066, 'accumulated_logging_time': 1.7271640300750732, 'global_step': 57163, 'preemption_count': 0}), (59000, {'train/ctc_loss': Array(0.0892466, dtype=float32), 'train/wer': 0.035003695946778365, 'validation/ctc_loss': Array(0.3397383, dtype=float32), 'validation/wer': 0.0990181217837937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18731277, dtype=float32), 'test/wer': 0.06197062945585278, 'test/num_examples': 2472, 'score': 47567.41899180412, 'total_duration': 52020.93994116783, 'accumulated_submission_time': 47567.41899180412, 'accumulated_eval_time': 4449.386815786362, 'accumulated_logging_time': 1.7888824939727783, 'global_step': 59000, 'preemption_count': 0}), (60810, {'train/ctc_loss': Array(0.08853921, dtype=float32), 'train/wer': 0.033519204361033665, 'validation/ctc_loss': Array(0.3364041, dtype=float32), 'validation/wer': 0.09721270166156579, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18456773, dtype=float32), 'test/wer': 0.05998009465196108, 'test/num_examples': 2472, 'score': 49007.59355258942, 'total_duration': 53592.487142562866, 'accumulated_submission_time': 49007.59355258942, 'accumulated_eval_time': 4580.6306529045105, 'accumulated_logging_time': 1.845522403717041, 'global_step': 60810, 'preemption_count': 0}), (62641, {'train/ctc_loss': Array(0.10741339, dtype=float32), 'train/wer': 0.04095844747351398, 'validation/ctc_loss': Array(0.32483196, dtype=float32), 'validation/wer': 0.09384322774361104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17749391, dtype=float32), 'test/wer': 0.057969248268437835, 'test/num_examples': 2472, 'score': 50448.088452100754, 'total_duration': 55162.96579504013, 'accumulated_submission_time': 50448.088452100754, 'accumulated_eval_time': 4710.4846367836, 'accumulated_logging_time': 1.9030003547668457, 'global_step': 62641, 'preemption_count': 0}), (64473, {'train/ctc_loss': Array(0.10360827, dtype=float32), 'train/wer': 0.039054374563204035, 'validation/ctc_loss': Array(0.3186228, dtype=float32), 'validation/wer': 0.09156472962144105, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17470016, dtype=float32), 'test/wer': 0.0573192777202283, 'test/num_examples': 2472, 'score': 51888.67530345917, 'total_duration': 56733.72640347481, 'accumulated_submission_time': 51888.67530345917, 'accumulated_eval_time': 4840.52444434166, 'accumulated_logging_time': 1.9645371437072754, 'global_step': 64473, 'preemption_count': 0}), (66305, {'train/ctc_loss': Array(0.12077924, dtype=float32), 'train/wer': 0.047228092993009264, 'validation/ctc_loss': Array(0.31349313, dtype=float32), 'validation/wer': 0.08982689207063344, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16910523, dtype=float32), 'test/wer': 0.05605995978307233, 'test/num_examples': 2472, 'score': 53328.806980371475, 'total_duration': 58303.341222286224, 'accumulated_submission_time': 53328.806980371475, 'accumulated_eval_time': 4969.87771320343, 'accumulated_logging_time': 2.021014928817749, 'global_step': 66305, 'preemption_count': 0}), (68111, {'train/ctc_loss': Array(0.09411724, dtype=float32), 'train/wer': 0.03501274466244183, 'validation/ctc_loss': Array(0.30283234, dtype=float32), 'validation/wer': 0.08693049615262076, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16567686, dtype=float32), 'test/wer': 0.054313163934759205, 'test/num_examples': 2472, 'score': 54768.677815914154, 'total_duration': 59873.677282094955, 'accumulated_submission_time': 54768.677815914154, 'accumulated_eval_time': 5100.211203813553, 'accumulated_logging_time': 2.079517364501953, 'global_step': 68111, 'preemption_count': 0}), (69919, {'train/ctc_loss': Array(0.09114316, dtype=float32), 'train/wer': 0.035440907178034824, 'validation/ctc_loss': Array(0.30461708, dtype=float32), 'validation/wer': 0.08602295876497679, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16325577, dtype=float32), 'test/wer': 0.05230231755123596, 'test/num_examples': 2472, 'score': 56209.37894654274, 'total_duration': 61446.25600028038, 'accumulated_submission_time': 56209.37894654274, 'accumulated_eval_time': 5231.957772254944, 'accumulated_logging_time': 2.138704776763916, 'global_step': 69919, 'preemption_count': 0}), (71727, {'train/ctc_loss': Array(0.06677296, dtype=float32), 'train/wer': 0.025365145116604353, 'validation/ctc_loss': Array(0.29442066, dtype=float32), 'validation/wer': 0.08297208839800342, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15865384, dtype=float32), 'test/wer': 0.051144557512237725, 'test/num_examples': 2472, 'score': 57650.042269945145, 'total_duration': 63020.883692502975, 'accumulated_submission_time': 57650.042269945145, 'accumulated_eval_time': 5365.79065990448, 'accumulated_logging_time': 2.1979687213897705, 'global_step': 71727, 'preemption_count': 0}), (73563, {'train/ctc_loss': Array(0.07948388, dtype=float32), 'train/wer': 0.030399894908537586, 'validation/ctc_loss': Array(0.29232764, dtype=float32), 'validation/wer': 0.08214178823483978, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15709297, dtype=float32), 'test/wer': 0.050393029065870454, 'test/num_examples': 2472, 'score': 59090.54529213905, 'total_duration': 64594.28295874596, 'accumulated_submission_time': 59090.54529213905, 'accumulated_eval_time': 5498.555616617203, 'accumulated_logging_time': 2.2566795349121094, 'global_step': 73563, 'preemption_count': 0}), (75365, {'train/ctc_loss': Array(0.06929343, dtype=float32), 'train/wer': 0.025832999331997328, 'validation/ctc_loss': Array(0.29072267, dtype=float32), 'validation/wer': 0.08138872529615648, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15438108, dtype=float32), 'test/wer': 0.05033209432697581, 'test/num_examples': 2472, 'score': 60530.85288310051, 'total_duration': 66167.30356693268, 'accumulated_submission_time': 60530.85288310051, 'accumulated_eval_time': 5631.136070251465, 'accumulated_logging_time': 2.3183770179748535, 'global_step': 75365, 'preemption_count': 0})], 'global_step': 76049}
I0216 09:04:44.303503 139688679413568 submission_runner.py:586] Timing: 61068.082364320755
I0216 09:04:44.303589 139688679413568 submission_runner.py:588] Total number of evals: 43
I0216 09:04:44.303647 139688679413568 submission_runner.py:589] ====================
I0216 09:04:44.303713 139688679413568 submission_runner.py:542] Using RNG seed 1618809895
I0216 09:04:44.306512 139688679413568 submission_runner.py:551] --- Tuning run 4/5 ---
I0216 09:04:44.306691 139688679413568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_4.
I0216 09:04:44.308311 139688679413568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_4/hparams.json.
I0216 09:04:44.310827 139688679413568 submission_runner.py:206] Initializing dataset.
I0216 09:04:44.310958 139688679413568 submission_runner.py:213] Initializing model.
I0216 09:04:48.600243 139688679413568 submission_runner.py:255] Initializing optimizer.
I0216 09:04:49.012425 139688679413568 submission_runner.py:262] Initializing metrics bundle.
I0216 09:04:49.012630 139688679413568 submission_runner.py:280] Initializing checkpoint and logger.
I0216 09:04:49.017896 139688679413568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_4 with prefix checkpoint_
I0216 09:04:49.018026 139688679413568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_4/meta_data_0.json.
I0216 09:04:49.018238 139688679413568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0216 09:04:49.018305 139688679413568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0216 09:04:49.632977 139688679413568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0216 09:04:50.222810 139688679413568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_4/flags_0.json.
I0216 09:04:50.243284 139688679413568 submission_runner.py:314] Starting training loop.
I0216 09:04:50.246512 139688679413568 input_pipeline.py:20] Loading split = train-clean-100
I0216 09:04:50.292788 139688679413568 input_pipeline.py:20] Loading split = train-clean-360
I0216 09:04:50.429385 139688679413568 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0216 09:05:25.302939 139518484104960 logging_writer.py:48] [0] global_step=0, grad_norm=51.327003479003906, loss=31.740528106689453
I0216 09:05:25.323714 139688679413568 spec.py:321] Evaluating on the training split.
I0216 09:06:08.059402 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 09:06:56.241769 139688679413568 spec.py:349] Evaluating on the test split.
I0216 09:07:20.909616 139688679413568 submission_runner.py:408] Time since start: 150.66s, 	Step: 1, 	{'train/ctc_loss': Array(30.807152, dtype=float32), 'train/wer': 0.9418356167355484, 'validation/ctc_loss': Array(30.141817, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 35.08036541938782, 'total_duration': 150.66368436813354, 'accumulated_submission_time': 35.08036541938782, 'accumulated_eval_time': 115.58325505256653, 'accumulated_logging_time': 0}
I0216 09:07:20.926258 139578135525120 logging_writer.py:48] [1] accumulated_eval_time=115.583255, accumulated_logging_time=0, accumulated_submission_time=35.080365, global_step=1, preemption_count=0, score=35.080365, test/ctc_loss=30.20883560180664, test/num_examples=2472, test/wer=0.908598, total_duration=150.663684, train/ctc_loss=30.807151794433594, train/wer=0.941836, validation/ctc_loss=30.141817092895508, validation/num_examples=5348, validation/wer=0.904371
I0216 09:09:02.119286 139518492497664 logging_writer.py:48] [100] global_step=100, grad_norm=2.913667917251587, loss=5.805973052978516
I0216 09:10:17.846783 139518500890368 logging_writer.py:48] [200] global_step=200, grad_norm=3.2766165733337402, loss=5.800156593322754
I0216 09:11:33.595828 139518492497664 logging_writer.py:48] [300] global_step=300, grad_norm=2.9775240421295166, loss=5.621324062347412
I0216 09:12:49.533600 139518500890368 logging_writer.py:48] [400] global_step=400, grad_norm=2.081550359725952, loss=5.553656101226807
I0216 09:14:05.286832 139518492497664 logging_writer.py:48] [500] global_step=500, grad_norm=1.397841453552246, loss=5.501115798950195
I0216 09:15:22.049865 139518500890368 logging_writer.py:48] [600] global_step=600, grad_norm=2.8577001094818115, loss=5.4544997215271
I0216 09:16:46.796907 139518492497664 logging_writer.py:48] [700] global_step=700, grad_norm=1.5390961170196533, loss=5.293056488037109
I0216 09:18:10.902408 139518500890368 logging_writer.py:48] [800] global_step=800, grad_norm=0.9051762819290161, loss=4.628292560577393
I0216 09:19:36.328510 139518492497664 logging_writer.py:48] [900] global_step=900, grad_norm=1.5698236227035522, loss=4.071779727935791
I0216 09:21:01.391065 139518500890368 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.9744226932525635, loss=3.807518482208252
I0216 09:22:22.789524 139578135525120 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.7680771350860596, loss=3.5237410068511963
I0216 09:23:38.246023 139578127132416 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9502373337745667, loss=3.3035824298858643
I0216 09:24:53.630088 139578135525120 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0069748163223267, loss=3.154344320297241
I0216 09:26:09.107748 139578127132416 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.3719654083251953, loss=3.0891993045806885
I0216 09:27:28.435886 139578135525120 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.383478045463562, loss=2.9872732162475586
I0216 09:28:52.735280 139578127132416 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.6706013679504395, loss=2.922539710998535
I0216 09:30:17.708000 139578135525120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8865236043930054, loss=2.842646598815918
I0216 09:31:20.910293 139688679413568 spec.py:321] Evaluating on the training split.
I0216 09:32:08.626999 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 09:32:58.982556 139688679413568 spec.py:349] Evaluating on the test split.
I0216 09:33:25.290918 139688679413568 submission_runner.py:408] Time since start: 1715.04s, 	Step: 1774, 	{'train/ctc_loss': Array(3.0434783, dtype=float32), 'train/wer': 0.6561460888525483, 'validation/ctc_loss': Array(2.9196923, dtype=float32), 'validation/wer': 0.6208521196790794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5539382, dtype=float32), 'test/wer': 0.5610870757418804, 'test/num_examples': 2472, 'score': 1474.9875185489655, 'total_duration': 1715.042043685913, 'accumulated_submission_time': 1474.9875185489655, 'accumulated_eval_time': 239.9583158493042, 'accumulated_logging_time': 0.027961015701293945}
I0216 09:33:25.321460 139578135525120 logging_writer.py:48] [1774] accumulated_eval_time=239.958316, accumulated_logging_time=0.027961, accumulated_submission_time=1474.987519, global_step=1774, preemption_count=0, score=1474.987519, test/ctc_loss=2.553938150405884, test/num_examples=2472, test/wer=0.561087, total_duration=1715.042044, train/ctc_loss=3.04347825050354, train/wer=0.656146, validation/ctc_loss=2.919692277908325, validation/num_examples=5348, validation/wer=0.620852
I0216 09:33:45.747747 139578127132416 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.703598976135254, loss=2.7220799922943115
I0216 09:35:01.334864 139578135525120 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.969099998474121, loss=2.646609306335449
I0216 09:36:16.960517 139578127132416 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.6619975566864014, loss=2.6072330474853516
I0216 09:37:36.010737 139578135525120 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8173549771308899, loss=2.5399367809295654
I0216 09:38:51.558711 139578127132416 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5439144372940063, loss=2.5041472911834717
I0216 09:40:07.154905 139578135525120 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1969945430755615, loss=2.4116311073303223
I0216 09:41:22.664912 139578127132416 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5497117638587952, loss=2.3861024379730225
I0216 09:42:38.357610 139578135525120 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9204754829406738, loss=2.4227216243743896
I0216 09:44:01.712492 139578127132416 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7650419473648071, loss=2.3566017150878906
I0216 09:45:26.413948 139578135525120 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8340929746627808, loss=2.3417136669158936
I0216 09:46:51.150614 139578127132416 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.2266383171081543, loss=2.330498456954956
I0216 09:48:16.066349 139578135525120 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5530482530593872, loss=2.2786850929260254
I0216 09:49:40.288636 139578127132416 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.523354172706604, loss=2.3568155765533447
I0216 09:51:06.845606 139578135525120 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8931569457054138, loss=2.2034904956817627
I0216 09:52:22.144232 139578127132416 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7068654298782349, loss=2.1746578216552734
I0216 09:53:37.481436 139578135525120 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9193998575210571, loss=2.1629481315612793
I0216 09:54:52.878992 139578127132416 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.3110164403915405, loss=2.281735897064209
I0216 09:56:08.304126 139578135525120 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9752169251441956, loss=2.1983468532562256
I0216 09:57:24.724386 139578127132416 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.1243263483047485, loss=2.185772657394409
I0216 09:57:26.098746 139688679413568 spec.py:321] Evaluating on the training split.
I0216 09:58:18.884735 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 09:59:10.527162 139688679413568 spec.py:349] Evaluating on the test split.
I0216 09:59:36.853795 139688679413568 submission_runner.py:408] Time since start: 3286.60s, 	Step: 3603, 	{'train/ctc_loss': Array(1.1530465, dtype=float32), 'train/wer': 0.35323060101821163, 'validation/ctc_loss': Array(1.2137563, dtype=float32), 'validation/wer': 0.34704615889628004, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9011528, dtype=float32), 'test/wer': 0.2833262242804623, 'test/num_examples': 2472, 'score': 2915.676682949066, 'total_duration': 3286.6037788391113, 'accumulated_submission_time': 2915.676682949066, 'accumulated_eval_time': 370.70670461654663, 'accumulated_logging_time': 0.07816886901855469}
I0216 09:59:36.890367 139578135525120 logging_writer.py:48] [3603] accumulated_eval_time=370.706705, accumulated_logging_time=0.078169, accumulated_submission_time=2915.676683, global_step=3603, preemption_count=0, score=2915.676683, test/ctc_loss=0.9011527895927429, test/num_examples=2472, test/wer=0.283326, total_duration=3286.603779, train/ctc_loss=1.1530464887619019, train/wer=0.353231, validation/ctc_loss=1.2137563228607178, validation/num_examples=5348, validation/wer=0.347046
I0216 10:00:50.668398 139578127132416 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.2026909589767456, loss=2.178276777267456
I0216 10:02:05.979791 139578135525120 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0794082880020142, loss=2.0966968536376953
I0216 10:03:21.442891 139578127132416 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8355351686477661, loss=2.147723436355591
I0216 10:04:39.173329 139578135525120 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6059512495994568, loss=2.0410094261169434
I0216 10:06:02.441959 139578127132416 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5669964551925659, loss=2.1134748458862305
I0216 10:07:22.620890 139578135525120 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9313837289810181, loss=2.1276469230651855
I0216 10:08:38.198801 139578127132416 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5322274565696716, loss=2.1254844665527344
I0216 10:09:53.687733 139578135525120 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.4619081914424896, loss=2.081368923187256
I0216 10:11:09.158056 139578127132416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8136304616928101, loss=2.0556528568267822
I0216 10:12:24.594642 139578135525120 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6906247735023499, loss=2.1118674278259277
I0216 10:13:48.288120 139578127132416 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.1136870384216309, loss=2.0058906078338623
I0216 10:15:12.569064 139578135525120 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7969916462898254, loss=2.112159013748169
I0216 10:16:35.874810 139578127132416 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6151717901229858, loss=2.0543532371520996
I0216 10:17:59.820256 139578135525120 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.4673008918762207, loss=2.091731071472168
I0216 10:19:24.651478 139578127132416 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.46728482842445374, loss=2.0548360347747803
I0216 10:20:47.642710 139578135525120 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.4553961455821991, loss=2.020845651626587
I0216 10:22:03.432655 139578127132416 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.0065518617630005, loss=2.055539131164551
I0216 10:23:18.893804 139578135525120 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5852522850036621, loss=2.047025442123413
I0216 10:23:37.494977 139688679413568 spec.py:321] Evaluating on the training split.
I0216 10:24:30.449625 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 10:25:22.101159 139688679413568 spec.py:349] Evaluating on the test split.
I0216 10:25:48.569542 139688679413568 submission_runner.py:408] Time since start: 4858.32s, 	Step: 5426, 	{'train/ctc_loss': Array(1.018678, dtype=float32), 'train/wer': 0.31768502774297475, 'validation/ctc_loss': Array(1.0831897, dtype=float32), 'validation/wer': 0.3157071550633828, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7714357, dtype=float32), 'test/wer': 0.2490605894420409, 'test/num_examples': 2472, 'score': 4356.196928501129, 'total_duration': 4858.320121049881, 'accumulated_submission_time': 4356.196928501129, 'accumulated_eval_time': 501.77516174316406, 'accumulated_logging_time': 0.13093829154968262}
I0216 10:25:48.602614 139578135525120 logging_writer.py:48] [5426] accumulated_eval_time=501.775162, accumulated_logging_time=0.130938, accumulated_submission_time=4356.196929, global_step=5426, preemption_count=0, score=4356.196929, test/ctc_loss=0.7714356780052185, test/num_examples=2472, test/wer=0.249061, total_duration=4858.320121, train/ctc_loss=1.0186779499053955, train/wer=0.317685, validation/ctc_loss=1.0831897258758545, validation/num_examples=5348, validation/wer=0.315707
I0216 10:26:45.127849 139578127132416 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6827182769775391, loss=2.0065879821777344
I0216 10:28:00.669109 139578135525120 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.42355144023895264, loss=2.0151355266571045
I0216 10:29:16.218788 139578127132416 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.4408082962036133, loss=1.9855320453643799
I0216 10:30:31.670940 139578135525120 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9479650259017944, loss=2.049957752227783
I0216 10:31:51.970664 139578127132416 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.2669332027435303, loss=2.0764660835266113
I0216 10:33:17.963757 139578135525120 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6864026784896851, loss=1.98067307472229
I0216 10:34:42.088152 139578127132416 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5851519107818604, loss=2.0084774494171143
I0216 10:36:07.533349 139578135525120 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.1451196670532227, loss=2.0131802558898926
I0216 10:37:22.862551 139578127132416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9797149300575256, loss=2.04758620262146
I0216 10:38:38.284609 139578135525120 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.1484876871109009, loss=2.0134847164154053
I0216 10:39:53.883189 139578127132416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6929764151573181, loss=1.9671474695205688
I0216 10:41:09.289250 139578135525120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5756703019142151, loss=2.1039984226226807
I0216 10:42:29.106592 139578127132416 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8218309283256531, loss=2.0075676441192627
I0216 10:43:53.708828 139578135525120 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.0014747381210327, loss=1.989329218864441
I0216 10:45:17.805646 139578127132416 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.44774848222732544, loss=1.94785737991333
I0216 10:46:41.044204 139578135525120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.452495813369751, loss=2.004384994506836
I0216 10:48:05.301332 139578127132416 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8279339075088501, loss=1.9793673753738403
I0216 10:49:29.939728 139578135525120 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4906366765499115, loss=1.9985272884368896
I0216 10:49:48.798968 139688679413568 spec.py:321] Evaluating on the training split.
I0216 10:50:42.890947 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 10:51:34.814991 139688679413568 spec.py:349] Evaluating on the test split.
I0216 10:52:00.606690 139688679413568 submission_runner.py:408] Time since start: 6430.36s, 	Step: 7221, 	{'train/ctc_loss': Array(0.879369, dtype=float32), 'train/wer': 0.284893824790215, 'validation/ctc_loss': Array(0.98021287, dtype=float32), 'validation/wer': 0.2919180899234386, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6883703, dtype=float32), 'test/wer': 0.22862714033270368, 'test/num_examples': 2472, 'score': 5796.309594631195, 'total_duration': 6430.357679367065, 'accumulated_submission_time': 5796.309594631195, 'accumulated_eval_time': 633.5774214267731, 'accumulated_logging_time': 0.18009305000305176}
I0216 10:52:00.640631 139578135525120 logging_writer.py:48] [7221] accumulated_eval_time=633.577421, accumulated_logging_time=0.180093, accumulated_submission_time=5796.309595, global_step=7221, preemption_count=0, score=5796.309595, test/ctc_loss=0.6883702874183655, test/num_examples=2472, test/wer=0.228627, total_duration=6430.357679, train/ctc_loss=0.8793690204620361, train/wer=0.284894, validation/ctc_loss=0.9802128672599792, validation/num_examples=5348, validation/wer=0.291918
I0216 10:53:00.801775 139578127132416 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.8298912644386292, loss=1.931349515914917
I0216 10:54:16.234788 139578135525120 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.496749609708786, loss=1.9620832204818726
I0216 10:55:31.623594 139578127132416 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.335580825805664, loss=1.9435290098190308
I0216 10:56:47.030805 139578135525120 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5431243181228638, loss=1.987825870513916
I0216 10:58:02.663863 139578127132416 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7110137939453125, loss=2.0015594959259033
I0216 10:59:18.002014 139578135525120 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7869875431060791, loss=1.9891018867492676
I0216 11:00:35.564422 139578127132416 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8989866971969604, loss=1.9495841264724731
I0216 11:01:59.268641 139578135525120 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.1112984418869019, loss=1.9871505498886108
I0216 11:03:23.016546 139578127132416 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7491934299468994, loss=1.866592288017273
I0216 11:04:47.208791 139578135525120 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7766684889793396, loss=1.9862654209136963
I0216 11:06:09.250576 139578135525120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5515322685241699, loss=1.9133166074752808
I0216 11:07:24.544004 139578127132416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.63679039478302, loss=1.922092318534851
I0216 11:08:40.035743 139578135525120 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0959032773971558, loss=1.93287193775177
I0216 11:09:55.516257 139578127132416 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.1801793575286865, loss=1.9536079168319702
I0216 11:11:11.026183 139578135525120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.9740928411483765, loss=1.9791505336761475
I0216 11:12:30.031642 139578127132416 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.1310499906539917, loss=1.9538475275039673
I0216 11:13:53.045761 139578135525120 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5299744606018066, loss=1.8336641788482666
I0216 11:15:17.129477 139578127132416 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.3637113571166992, loss=1.9023396968841553
I0216 11:16:00.666564 139688679413568 spec.py:321] Evaluating on the training split.
I0216 11:16:53.267014 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 11:17:44.801291 139688679413568 spec.py:349] Evaluating on the test split.
I0216 11:18:10.675619 139688679413568 submission_runner.py:408] Time since start: 8000.43s, 	Step: 9054, 	{'train/ctc_loss': Array(0.9016759, dtype=float32), 'train/wer': 0.29293339521930845, 'validation/ctc_loss': Array(0.95180404, dtype=float32), 'validation/wer': 0.282427565965417, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6508851, dtype=float32), 'test/wer': 0.21485588934251418, 'test/num_examples': 2472, 'score': 7236.248072147369, 'total_duration': 8000.426544189453, 'accumulated_submission_time': 7236.248072147369, 'accumulated_eval_time': 763.5807249546051, 'accumulated_logging_time': 0.23284149169921875}
I0216 11:18:10.708175 139578135525120 logging_writer.py:48] [9054] accumulated_eval_time=763.580725, accumulated_logging_time=0.232841, accumulated_submission_time=7236.248072, global_step=9054, preemption_count=0, score=7236.248072, test/ctc_loss=0.6508851051330566, test/num_examples=2472, test/wer=0.214856, total_duration=8000.426544, train/ctc_loss=0.9016758799552917, train/wer=0.292933, validation/ctc_loss=0.9518040418624878, validation/num_examples=5348, validation/wer=0.282428
I0216 11:18:46.171916 139578127132416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9469202160835266, loss=1.981859803199768
I0216 11:20:01.556611 139578135525120 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5431622862815857, loss=1.8582535982131958
I0216 11:21:20.664822 139578135525120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9771901965141296, loss=1.9112310409545898
I0216 11:22:36.092005 139578127132416 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.2743656635284424, loss=1.9404975175857544
I0216 11:23:51.606452 139578135525120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7260515093803406, loss=1.8491679430007935
I0216 11:25:07.029226 139578127132416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5719195008277893, loss=1.9367402791976929
I0216 11:26:22.460050 139578135525120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.84212327003479, loss=1.9187402725219727
I0216 11:27:38.987874 139578127132416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.688366711139679, loss=1.8944146633148193
I0216 11:29:02.384239 139578135525120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6059759259223938, loss=1.931545615196228
I0216 11:30:25.932885 139578127132416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.44589343667030334, loss=1.9013148546218872
I0216 11:31:49.711387 139578135525120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.8065693974494934, loss=1.8548500537872314
I0216 11:33:13.169936 139578127132416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6716824769973755, loss=1.8854275941848755
I0216 11:34:40.540614 139578135525120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5932027101516724, loss=1.8760154247283936
I0216 11:35:55.937716 139578127132416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5339998602867126, loss=1.8676546812057495
I0216 11:37:11.325480 139578135525120 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8689936995506287, loss=1.8917665481567383
I0216 11:38:26.666281 139578127132416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7043834924697876, loss=1.900701642036438
I0216 11:39:41.970035 139578135525120 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5891786217689514, loss=1.8882465362548828
I0216 11:40:57.358645 139578127132416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.916379451751709, loss=1.9216012954711914
I0216 11:42:11.336155 139688679413568 spec.py:321] Evaluating on the training split.
I0216 11:43:05.692637 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 11:43:58.137515 139688679413568 spec.py:349] Evaluating on the test split.
I0216 11:44:24.340829 139688679413568 submission_runner.py:408] Time since start: 9574.09s, 	Step: 10891, 	{'train/ctc_loss': Array(0.8089695, dtype=float32), 'train/wer': 0.2691599672493282, 'validation/ctc_loss': Array(0.95127535, dtype=float32), 'validation/wer': 0.2845902082508665, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.65417844, dtype=float32), 'test/wer': 0.21981191477261186, 'test/num_examples': 2472, 'score': 8676.789829492569, 'total_duration': 9574.090694189072, 'accumulated_submission_time': 8676.789829492569, 'accumulated_eval_time': 896.5786190032959, 'accumulated_logging_time': 0.2828640937805176}
I0216 11:44:24.374944 139578135525120 logging_writer.py:48] [10891] accumulated_eval_time=896.578619, accumulated_logging_time=0.282864, accumulated_submission_time=8676.789829, global_step=10891, preemption_count=0, score=8676.789829, test/ctc_loss=0.6541784405708313, test/num_examples=2472, test/wer=0.219812, total_duration=9574.090694, train/ctc_loss=0.8089694976806641, train/wer=0.269160, validation/ctc_loss=0.9512753486633301, validation/num_examples=5348, validation/wer=0.284590
I0216 11:44:31.995214 139578127132416 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6179400086402893, loss=1.920291781425476
I0216 11:45:47.471775 139578135525120 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9533421397209167, loss=1.9318939447402954
I0216 11:47:02.888512 139578127132416 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.8095213174819946, loss=1.8987658023834229
I0216 11:48:18.611822 139578135525120 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.8429531455039978, loss=1.8680052757263184
I0216 11:49:37.496312 139578127132416 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.162900447845459, loss=1.9168059825897217
I0216 11:50:58.313041 139578135525120 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7171881198883057, loss=1.803393006324768
I0216 11:52:13.739814 139578127132416 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.9574883580207825, loss=1.9272223711013794
I0216 11:53:29.154728 139578135525120 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.0685259103775024, loss=1.866235375404358
I0216 11:54:44.604304 139578127132416 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6870689988136292, loss=1.8842753171920776
I0216 11:56:00.101615 139578135525120 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7949504852294922, loss=1.9663958549499512
I0216 11:57:19.474413 139578127132416 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5806151032447815, loss=1.8539472818374634
I0216 11:58:42.495253 139578135525120 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.683597207069397, loss=1.857742428779602
I0216 12:00:05.568411 139578127132416 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.1626582145690918, loss=1.8449699878692627
I0216 12:01:29.295174 139578135525120 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.2230795621871948, loss=1.883193016052246
I0216 12:02:52.870327 139578127132416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5216050148010254, loss=1.8973385095596313
I0216 12:04:16.539325 139578135525120 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.9298601746559143, loss=1.861175537109375
I0216 12:05:32.042162 139578127132416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.45994019508361816, loss=1.8634907007217407
I0216 12:06:47.744433 139578135525120 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.0508263111114502, loss=1.8889992237091064
I0216 12:08:03.369280 139578127132416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.7980777025222778, loss=1.8587403297424316
I0216 12:08:24.973142 139688679413568 spec.py:321] Evaluating on the training split.
I0216 12:09:17.784690 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 12:10:09.433656 139688679413568 spec.py:349] Evaluating on the test split.
I0216 12:10:35.781537 139688679413568 submission_runner.py:408] Time since start: 11145.53s, 	Step: 12730, 	{'train/ctc_loss': Array(0.8335272, dtype=float32), 'train/wer': 0.2669985010888317, 'validation/ctc_loss': Array(0.91189045, dtype=float32), 'validation/wer': 0.2689013970282978, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6159002, dtype=float32), 'test/wer': 0.20378607844332053, 'test/num_examples': 2472, 'score': 10117.304208755493, 'total_duration': 11145.531847715378, 'accumulated_submission_time': 10117.304208755493, 'accumulated_eval_time': 1027.3806738853455, 'accumulated_logging_time': 0.3326706886291504}
I0216 12:10:35.817310 139578135525120 logging_writer.py:48] [12730] accumulated_eval_time=1027.380674, accumulated_logging_time=0.332671, accumulated_submission_time=10117.304209, global_step=12730, preemption_count=0, score=10117.304209, test/ctc_loss=0.6159002184867859, test/num_examples=2472, test/wer=0.203786, total_duration=11145.531848, train/ctc_loss=0.8335272073745728, train/wer=0.266999, validation/ctc_loss=0.91189044713974, validation/num_examples=5348, validation/wer=0.268901
I0216 12:11:29.319394 139578127132416 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6119387745857239, loss=1.8811578750610352
I0216 12:12:44.899255 139578135525120 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.8831468224525452, loss=1.857743740081787
I0216 12:14:00.337536 139578127132416 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5288060903549194, loss=1.8428661823272705
I0216 12:15:15.846328 139578135525120 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.846746027469635, loss=1.8468061685562134
I0216 12:16:37.968764 139578127132416 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.1437920331954956, loss=1.9157108068466187
I0216 12:18:01.283258 139578135525120 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6577988862991333, loss=1.899914026260376
I0216 12:19:27.156028 139578135525120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5561590790748596, loss=1.847853660583496
I0216 12:20:42.855756 139578127132416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5912269353866577, loss=1.8148592710494995
I0216 12:21:58.212149 139578135525120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5655393004417419, loss=1.859270453453064
I0216 12:23:13.669779 139578127132416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5679240822792053, loss=1.8313689231872559
I0216 12:24:29.020732 139578135525120 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.155728816986084, loss=1.8841278553009033
I0216 12:25:45.726024 139578127132416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.8711047768592834, loss=1.8063772916793823
I0216 12:27:09.256493 139578135525120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7251461148262024, loss=1.8507825136184692
I0216 12:28:33.232475 139578127132416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5530715584754944, loss=1.8684942722320557
I0216 12:29:58.228008 139578135525120 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.8241945505142212, loss=1.8540385961532593
I0216 12:31:20.750792 139578127132416 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6113160252571106, loss=1.8185737133026123
I0216 12:32:44.717483 139578135525120 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.9448164701461792, loss=1.8818074464797974
I0216 12:34:05.160035 139578135525120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6558400392532349, loss=1.8890026807785034
I0216 12:34:35.825139 139688679413568 spec.py:321] Evaluating on the training split.
I0216 12:35:29.147624 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 12:36:21.148841 139688679413568 spec.py:349] Evaluating on the test split.
I0216 12:36:47.099359 139688679413568 submission_runner.py:408] Time since start: 12716.85s, 	Step: 14542, 	{'train/ctc_loss': Array(0.81453586, dtype=float32), 'train/wer': 0.2609976846979583, 'validation/ctc_loss': Array(0.8834435, dtype=float32), 'validation/wer': 0.26353340992691426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6003142, dtype=float32), 'test/wer': 0.19864724879653892, 'test/num_examples': 2472, 'score': 11557.22633099556, 'total_duration': 12716.850360155106, 'accumulated_submission_time': 11557.22633099556, 'accumulated_eval_time': 1158.6492466926575, 'accumulated_logging_time': 0.3859102725982666}
I0216 12:36:47.136836 139578135525120 logging_writer.py:48] [14542] accumulated_eval_time=1158.649247, accumulated_logging_time=0.385910, accumulated_submission_time=11557.226331, global_step=14542, preemption_count=0, score=11557.226331, test/ctc_loss=0.600314199924469, test/num_examples=2472, test/wer=0.198647, total_duration=12716.850360, train/ctc_loss=0.8145358562469482, train/wer=0.260998, validation/ctc_loss=0.8834434747695923, validation/num_examples=5348, validation/wer=0.263533
I0216 12:37:31.616059 139578127132416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7206749320030212, loss=1.8115174770355225
I0216 12:38:47.221533 139578135525120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6438091397285461, loss=1.877347707748413
I0216 12:40:02.686189 139578127132416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.5608745813369751, loss=1.801537275314331
I0216 12:41:18.120758 139578135525120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.5312582850456238, loss=1.7533323764801025
I0216 12:42:33.516311 139578127132416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4642414152622223, loss=1.7624584436416626
I0216 12:43:53.265830 139578135525120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6954793334007263, loss=1.811753511428833
I0216 12:45:17.216134 139578127132416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6194859743118286, loss=1.846811056137085
I0216 12:46:41.287990 139578135525120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.5706890821456909, loss=1.857570767402649
I0216 12:48:04.615596 139578127132416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.7759897708892822, loss=1.8649247884750366
I0216 12:49:27.787283 139578135525120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6140739321708679, loss=1.7756233215332031
I0216 12:50:43.265561 139578127132416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.713570773601532, loss=1.8291884660720825
I0216 12:51:58.648777 139578135525120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6314342617988586, loss=1.792396903038025
I0216 12:53:14.078096 139578127132416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9729713201522827, loss=1.8088325262069702
I0216 12:54:29.620875 139578135525120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.5549796223640442, loss=1.8339322805404663
I0216 12:55:48.329786 139578127132416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.8298087120056152, loss=1.8705713748931885
I0216 12:57:12.353662 139578135525120 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.8891582489013672, loss=1.8019554615020752
I0216 12:58:35.005570 139578127132416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.5744579434394836, loss=1.862466812133789
I0216 12:59:57.994280 139578135525120 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.502599835395813, loss=1.7999907732009888
I0216 13:00:47.754210 139688679413568 spec.py:321] Evaluating on the training split.
I0216 13:01:42.170804 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 13:02:33.820897 139688679413568 spec.py:349] Evaluating on the test split.
I0216 13:03:00.136878 139688679413568 submission_runner.py:408] Time since start: 14289.89s, 	Step: 16361, 	{'train/ctc_loss': Array(0.7476235, dtype=float32), 'train/wer': 0.24288179939316654, 'validation/ctc_loss': Array(0.8414512, dtype=float32), 'validation/wer': 0.250461009683617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5641267, dtype=float32), 'test/wer': 0.18853208214002803, 'test/num_examples': 2472, 'score': 12997.759685993195, 'total_duration': 14289.888085842133, 'accumulated_submission_time': 12997.759685993195, 'accumulated_eval_time': 1291.026449918747, 'accumulated_logging_time': 0.43868160247802734}
I0216 13:03:00.176245 139578135525120 logging_writer.py:48] [16361] accumulated_eval_time=1291.026450, accumulated_logging_time=0.438682, accumulated_submission_time=12997.759686, global_step=16361, preemption_count=0, score=12997.759686, test/ctc_loss=0.5641266703605652, test/num_examples=2472, test/wer=0.188532, total_duration=14289.888086, train/ctc_loss=0.7476235032081604, train/wer=0.242882, validation/ctc_loss=0.8414512276649475, validation/num_examples=5348, validation/wer=0.250461
I0216 13:03:30.384922 139578127132416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5306775569915771, loss=1.7877906560897827
I0216 13:04:49.269785 139578135525120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.932344913482666, loss=1.80391263961792
I0216 13:06:04.665913 139578127132416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.8873592615127563, loss=1.8124668598175049
I0216 13:07:20.188040 139578135525120 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6991668939590454, loss=1.8032121658325195
I0216 13:08:35.693856 139578127132416 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.5076196193695068, loss=1.7652291059494019
I0216 13:09:51.128143 139578135525120 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7745581865310669, loss=1.850229263305664
I0216 13:11:08.675853 139578127132416 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.8664084672927856, loss=1.8144792318344116
I0216 13:12:31.193936 139578135525120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.8483526110649109, loss=1.853259801864624
I0216 13:13:55.678048 139578127132416 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.5303562879562378, loss=1.8452190160751343
I0216 13:15:19.026303 139578135525120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5401135683059692, loss=1.7757604122161865
I0216 13:16:42.735460 139578127132416 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.5674008727073669, loss=1.7435911893844604
I0216 13:18:06.410925 139578135525120 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8041627407073975, loss=1.7791476249694824
I0216 13:19:26.052150 139578135525120 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.1582975387573242, loss=1.7644835710525513
I0216 13:20:41.504892 139578127132416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.672136127948761, loss=1.79732084274292
I0216 13:21:57.051678 139578135525120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.7454204559326172, loss=1.785297155380249
I0216 13:23:12.454229 139578127132416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7759851217269897, loss=1.8331810235977173
I0216 13:24:27.958050 139578135525120 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7393186092376709, loss=1.7765171527862549
I0216 13:25:49.657027 139578127132416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5268833637237549, loss=1.7445852756500244
I0216 13:27:00.455236 139688679413568 spec.py:321] Evaluating on the training split.
I0216 13:27:53.862196 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 13:28:46.635250 139688679413568 spec.py:349] Evaluating on the test split.
I0216 13:29:13.957277 139688679413568 submission_runner.py:408] Time since start: 15863.71s, 	Step: 18186, 	{'train/ctc_loss': Array(0.6964414, dtype=float32), 'train/wer': 0.22979803678212796, 'validation/ctc_loss': Array(0.83934134, dtype=float32), 'validation/wer': 0.24964036417351343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5645912, dtype=float32), 'test/wer': 0.18782117685292385, 'test/num_examples': 2472, 'score': 14437.954718589783, 'total_duration': 15863.708154201508, 'accumulated_submission_time': 14437.954718589783, 'accumulated_eval_time': 1424.5227282047272, 'accumulated_logging_time': 0.4945368766784668}
I0216 13:29:13.988742 139578135525120 logging_writer.py:48] [18186] accumulated_eval_time=1424.522728, accumulated_logging_time=0.494537, accumulated_submission_time=14437.954719, global_step=18186, preemption_count=0, score=14437.954719, test/ctc_loss=0.5645912289619446, test/num_examples=2472, test/wer=0.187821, total_duration=15863.708154, train/ctc_loss=0.6964414119720459, train/wer=0.229798, validation/ctc_loss=0.8393413424491882, validation/num_examples=5348, validation/wer=0.249640
I0216 13:29:25.590478 139578127132416 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5560542345046997, loss=1.8075575828552246
I0216 13:30:41.035504 139578135525120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.8362480998039246, loss=1.7453484535217285
I0216 13:31:56.558716 139578127132416 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.815990686416626, loss=1.8243684768676758
I0216 13:33:12.069446 139578135525120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5831699371337891, loss=1.7655607461929321
I0216 13:34:31.017514 139578135525120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6536486148834229, loss=1.7343827486038208
I0216 13:35:46.629149 139578127132416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9274794459342957, loss=1.770424246788025
I0216 13:37:02.183003 139578135525120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.650240957736969, loss=1.786070466041565
I0216 13:38:17.774534 139578127132416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.8204113841056824, loss=1.7705508470535278
I0216 13:39:33.356072 139578135525120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5769951939582825, loss=1.6923160552978516
I0216 13:40:54.695347 139578127132416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6069064736366272, loss=1.7279881238937378
I0216 13:42:18.852673 139578135525120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5096138715744019, loss=1.74380362033844
I0216 13:43:43.034654 139578127132416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.46548524498939514, loss=1.7760132551193237
I0216 13:45:07.564744 139578135525120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.4795042872428894, loss=1.7705081701278687
I0216 13:46:32.174690 139578127132416 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6292451024055481, loss=1.7826131582260132
I0216 13:47:56.630817 139578135525120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7016655206680298, loss=1.7311310768127441
I0216 13:49:12.081813 139578127132416 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.39650994539260864, loss=1.7629563808441162
I0216 13:50:27.557701 139578135525120 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.656635582447052, loss=1.7037264108657837
I0216 13:51:43.057759 139578127132416 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5528489947319031, loss=1.859302043914795
I0216 13:52:58.650025 139578135525120 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.4824577271938324, loss=1.7343940734863281
I0216 13:53:14.242674 139688679413568 spec.py:321] Evaluating on the training split.
I0216 13:54:07.008969 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 13:54:58.210592 139688679413568 spec.py:349] Evaluating on the test split.
I0216 13:55:24.689067 139688679413568 submission_runner.py:408] Time since start: 17434.44s, 	Step: 20022, 	{'train/ctc_loss': Array(0.6533358, dtype=float32), 'train/wer': 0.21529764532533727, 'validation/ctc_loss': Array(0.8104945, dtype=float32), 'validation/wer': 0.24206146152138022, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54033, dtype=float32), 'test/wer': 0.17943249446509454, 'test/num_examples': 2472, 'score': 15878.121164798737, 'total_duration': 17434.440548181534, 'accumulated_submission_time': 15878.121164798737, 'accumulated_eval_time': 1554.9639530181885, 'accumulated_logging_time': 0.5451233386993408}
I0216 13:55:24.723989 139578135525120 logging_writer.py:48] [20022] accumulated_eval_time=1554.963953, accumulated_logging_time=0.545123, accumulated_submission_time=15878.121165, global_step=20022, preemption_count=0, score=15878.121165, test/ctc_loss=0.5403299927711487, test/num_examples=2472, test/wer=0.179432, total_duration=17434.440548, train/ctc_loss=0.6533358097076416, train/wer=0.215298, validation/ctc_loss=0.8104944825172424, validation/num_examples=5348, validation/wer=0.242061
I0216 13:56:24.262273 139578127132416 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.47497349977493286, loss=1.7532416582107544
I0216 13:57:39.735988 139578135525120 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.49005699157714844, loss=1.730891227722168
I0216 13:58:55.323238 139578127132416 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.6168751120567322, loss=1.763922929763794
I0216 14:00:10.985769 139578135525120 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.5464795231819153, loss=1.7932889461517334
I0216 14:01:32.686561 139578127132416 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9228576421737671, loss=1.777322769165039
I0216 14:02:59.457005 139578135525120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.49655595421791077, loss=1.7414757013320923
I0216 14:04:15.069678 139578127132416 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.833988606929779, loss=1.7892241477966309
I0216 14:05:30.536755 139578135525120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.629345178604126, loss=1.7828859090805054
I0216 14:06:45.933521 139578127132416 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7279698848724365, loss=1.6988558769226074
I0216 14:08:01.666832 139578135525120 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.42501193284988403, loss=1.7383898496627808
I0216 14:09:17.224869 139578127132416 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.5181669592857361, loss=1.7141950130462646
I0216 14:10:40.283764 139578135525120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6241625547409058, loss=1.6829341650009155
I0216 14:12:03.662165 139578127132416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6988433003425598, loss=1.7804290056228638
I0216 14:13:28.371889 139578135525120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8038106560707092, loss=1.7699830532073975
I0216 14:14:51.733064 139578127132416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7712104916572571, loss=1.7451708316802979
I0216 14:16:14.744677 139578135525120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.4967699646949768, loss=1.7221620082855225
I0216 14:17:35.643201 139578135525120 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.545432448387146, loss=1.7234735488891602
I0216 14:18:51.391721 139578127132416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.58644700050354, loss=1.675718903541565
I0216 14:19:25.026222 139688679413568 spec.py:321] Evaluating on the training split.
I0216 14:20:17.975991 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 14:21:10.234003 139688679413568 spec.py:349] Evaluating on the test split.
I0216 14:21:36.413379 139688679413568 submission_runner.py:408] Time since start: 19006.16s, 	Step: 21846, 	{'train/ctc_loss': Array(0.7111166, dtype=float32), 'train/wer': 0.23846792097414393, 'validation/ctc_loss': Array(0.8012005, dtype=float32), 'validation/wer': 0.23961883429718953, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52948534, dtype=float32), 'test/wer': 0.1805090081855666, 'test/num_examples': 2472, 'score': 17318.338774442673, 'total_duration': 19006.164157390594, 'accumulated_submission_time': 17318.338774442673, 'accumulated_eval_time': 1686.3452100753784, 'accumulated_logging_time': 0.5957059860229492}
I0216 14:21:36.448925 139578135525120 logging_writer.py:48] [21846] accumulated_eval_time=1686.345210, accumulated_logging_time=0.595706, accumulated_submission_time=17318.338774, global_step=21846, preemption_count=0, score=17318.338774, test/ctc_loss=0.5294853448867798, test/num_examples=2472, test/wer=0.180509, total_duration=19006.164157, train/ctc_loss=0.71111661195755, train/wer=0.238468, validation/ctc_loss=0.8012005090713501, validation/num_examples=5348, validation/wer=0.239619
I0216 14:22:18.002243 139578127132416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.6685476899147034, loss=1.7284202575683594
I0216 14:23:33.686268 139578135525120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7685713768005371, loss=1.6815061569213867
I0216 14:24:49.347394 139578127132416 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.1376925706863403, loss=1.703352451324463
I0216 14:26:05.086795 139578135525120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.5286546945571899, loss=1.6743426322937012
I0216 14:27:23.320218 139578127132416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.587133526802063, loss=1.6940690279006958
I0216 14:28:46.718589 139578135525120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7379252910614014, loss=1.7646329402923584
I0216 14:30:09.624619 139578127132416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8721402287483215, loss=1.7285645008087158
I0216 14:31:33.610996 139578135525120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.5902124643325806, loss=1.7177375555038452
I0216 14:32:57.254399 139578135525120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.7492581009864807, loss=1.7138900756835938
I0216 14:34:12.793397 139578127132416 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.2233377695083618, loss=1.7410043478012085
I0216 14:35:28.579839 139578135525120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.7008129358291626, loss=1.7081022262573242
I0216 14:36:44.304850 139578127132416 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6739941239356995, loss=1.643242597579956
I0216 14:38:00.049452 139578135525120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.6528472900390625, loss=1.7262324094772339
I0216 14:39:19.282119 139578127132416 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.5960381031036377, loss=1.6539807319641113
I0216 14:40:41.647255 139578135525120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.5625638961791992, loss=1.698400855064392
I0216 14:42:04.505961 139578127132416 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.6678223609924316, loss=1.7219467163085938
I0216 14:43:28.485939 139578135525120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6795221567153931, loss=1.635339379310608
I0216 14:44:51.936321 139578127132416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.9037500023841858, loss=1.6629369258880615
I0216 14:45:36.708178 139688679413568 spec.py:321] Evaluating on the training split.
I0216 14:46:30.418969 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 14:47:22.909075 139688679413568 spec.py:349] Evaluating on the test split.
I0216 14:47:49.573521 139688679413568 submission_runner.py:408] Time since start: 20579.32s, 	Step: 23656, 	{'train/ctc_loss': Array(0.6636916, dtype=float32), 'train/wer': 0.21857476503706377, 'validation/ctc_loss': Array(0.7818518, dtype=float32), 'validation/wer': 0.2352549311140504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5160569, dtype=float32), 'test/wer': 0.17411086060162898, 'test/num_examples': 2472, 'score': 18758.5142223835, 'total_duration': 20579.324452877045, 'accumulated_submission_time': 18758.5142223835, 'accumulated_eval_time': 1819.2048013210297, 'accumulated_logging_time': 0.6472318172454834}
I0216 14:47:49.611524 139578135525120 logging_writer.py:48] [23656] accumulated_eval_time=1819.204801, accumulated_logging_time=0.647232, accumulated_submission_time=18758.514222, global_step=23656, preemption_count=0, score=18758.514222, test/ctc_loss=0.5160568952560425, test/num_examples=2472, test/wer=0.174111, total_duration=20579.324453, train/ctc_loss=0.6636915802955627, train/wer=0.218575, validation/ctc_loss=0.7818518280982971, validation/num_examples=5348, validation/wer=0.235255
I0216 14:48:27.481915 139578135525120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.4644545316696167, loss=1.6833566427230835
I0216 14:49:42.937670 139578127132416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5998235940933228, loss=1.6629832983016968
I0216 14:50:58.483995 139578135525120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.5929761528968811, loss=1.6704756021499634
I0216 14:52:14.028083 139578127132416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5925440192222595, loss=1.6667917966842651
I0216 14:53:29.788976 139578135525120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.6129499673843384, loss=1.7317476272583008
I0216 14:54:49.432727 139578127132416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6283634305000305, loss=1.7154711484909058
I0216 14:56:12.725696 139578135525120 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6776428818702698, loss=1.7019972801208496
I0216 14:57:37.183432 139578127132416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.5113493800163269, loss=1.7137850522994995
I0216 14:59:00.120401 139578135525120 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0191822052001953, loss=1.688729166984558
I0216 15:00:22.790074 139578127132416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6287542581558228, loss=1.6770999431610107
I0216 15:01:46.719162 139578135525120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.5857383608818054, loss=1.6144295930862427
I0216 15:03:06.968868 139578135525120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.7066642642021179, loss=1.6668070554733276
I0216 15:04:22.569462 139578127132416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6277034878730774, loss=1.6781363487243652
I0216 15:05:38.141978 139578135525120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6511077880859375, loss=1.6801989078521729
I0216 15:06:53.680837 139578127132416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.7020516395568848, loss=1.6494358777999878
I0216 15:08:09.154594 139578135525120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6473676562309265, loss=1.6685764789581299
I0216 15:09:30.399562 139578127132416 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5905246138572693, loss=1.6595529317855835
I0216 15:10:53.927670 139578135525120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.724062442779541, loss=1.7155771255493164
I0216 15:11:50.209235 139688679413568 spec.py:321] Evaluating on the training split.
I0216 15:12:44.629163 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 15:13:36.675370 139688679413568 spec.py:349] Evaluating on the test split.
I0216 15:14:02.892051 139688679413568 submission_runner.py:408] Time since start: 22152.64s, 	Step: 25469, 	{'train/ctc_loss': Array(0.6760904, dtype=float32), 'train/wer': 0.21828787155469956, 'validation/ctc_loss': Array(0.7688105, dtype=float32), 'validation/wer': 0.22999314519632738, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50244045, dtype=float32), 'test/wer': 0.16795645197326997, 'test/num_examples': 2472, 'score': 20199.02674293518, 'total_duration': 22152.642586946487, 'accumulated_submission_time': 20199.02674293518, 'accumulated_eval_time': 1951.8814685344696, 'accumulated_logging_time': 0.7016665935516357}
I0216 15:14:02.930423 139578135525120 logging_writer.py:48] [25469] accumulated_eval_time=1951.881469, accumulated_logging_time=0.701667, accumulated_submission_time=20199.026743, global_step=25469, preemption_count=0, score=20199.026743, test/ctc_loss=0.5024404525756836, test/num_examples=2472, test/wer=0.167956, total_duration=22152.642587, train/ctc_loss=0.67609041929245, train/wer=0.218288, validation/ctc_loss=0.768810510635376, validation/num_examples=5348, validation/wer=0.229993
I0216 15:14:27.109547 139578127132416 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8533016443252563, loss=1.6926093101501465
I0216 15:15:42.664000 139578135525120 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6944065690040588, loss=1.6690493822097778
I0216 15:16:58.065740 139578127132416 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.4973908066749573, loss=1.6387847661972046
I0216 15:18:17.163025 139578135525120 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.541548490524292, loss=1.6509284973144531
I0216 15:19:32.676016 139578127132416 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.8984591960906982, loss=1.650345802307129
I0216 15:20:48.316300 139578135525120 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5026818513870239, loss=1.6559131145477295
I0216 15:22:03.967297 139578127132416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6409499049186707, loss=1.6827762126922607
I0216 15:23:19.676623 139578135525120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.7334450483322144, loss=1.6353124380111694
I0216 15:24:40.236140 139578127132416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.48898428678512573, loss=1.6309925317764282
I0216 15:26:03.907061 139578135525120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.7632051110267639, loss=1.6831001043319702
I0216 15:27:27.587723 139578127132416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6613321304321289, loss=1.6674851179122925
I0216 15:28:52.180262 139578135525120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.8392673134803772, loss=1.7085661888122559
I0216 15:30:15.633135 139578127132416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.8887147307395935, loss=1.6729869842529297
I0216 15:31:41.126906 139578135525120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5382022261619568, loss=1.6648727655410767
I0216 15:32:56.589653 139578127132416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.867769718170166, loss=1.5947778224945068
I0216 15:34:12.022974 139578135525120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.607077956199646, loss=1.6632916927337646
I0216 15:35:27.547732 139578127132416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.5140588879585266, loss=1.6003625392913818
I0216 15:36:43.113045 139578135525120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6902257800102234, loss=1.6901382207870483
I0216 15:37:59.162523 139578127132416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6499043703079224, loss=1.6087290048599243
I0216 15:38:03.468034 139688679413568 spec.py:321] Evaluating on the training split.
I0216 15:38:57.822273 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 15:39:50.054979 139688679413568 spec.py:349] Evaluating on the test split.
I0216 15:40:16.766834 139688679413568 submission_runner.py:408] Time since start: 23726.52s, 	Step: 27307, 	{'train/ctc_loss': Array(0.5880113, dtype=float32), 'train/wer': 0.1996346712314533, 'validation/ctc_loss': Array(0.7525327, dtype=float32), 'validation/wer': 0.22395898703380093, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49013776, dtype=float32), 'test/wer': 0.16436130237848598, 'test/num_examples': 2472, 'score': 21639.47647380829, 'total_duration': 23726.517678260803, 'accumulated_submission_time': 21639.47647380829, 'accumulated_eval_time': 2085.1744623184204, 'accumulated_logging_time': 0.7583460807800293}
I0216 15:40:16.802426 139578135525120 logging_writer.py:48] [27307] accumulated_eval_time=2085.174462, accumulated_logging_time=0.758346, accumulated_submission_time=21639.476474, global_step=27307, preemption_count=0, score=21639.476474, test/ctc_loss=0.4901377558708191, test/num_examples=2472, test/wer=0.164361, total_duration=23726.517678, train/ctc_loss=0.5880113244056702, train/wer=0.199635, validation/ctc_loss=0.7525327205657959, validation/num_examples=5348, validation/wer=0.223959
I0216 15:41:27.622821 139578127132416 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5626495480537415, loss=1.664196252822876
I0216 15:42:43.114238 139578135525120 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5724809765815735, loss=1.7125641107559204
I0216 15:43:58.949908 139578127132416 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8044391870498657, loss=1.709588646888733
I0216 15:45:15.312715 139578135525120 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.5559613704681396, loss=1.6915100812911987
I0216 15:46:38.686567 139578127132416 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.4872382581233978, loss=1.669472575187683
I0216 15:47:58.058444 139578135525120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.5673756003379822, loss=1.67439603805542
I0216 15:49:13.534565 139578127132416 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6383665204048157, loss=1.6206815242767334
I0216 15:50:29.062364 139578135525120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.7518554925918579, loss=1.667900562286377
I0216 15:51:44.422957 139578127132416 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.582606852054596, loss=1.7088900804519653
I0216 15:52:59.873154 139578135525120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5173023343086243, loss=1.6044501066207886
I0216 15:54:20.318023 139578127132416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.8424710035324097, loss=1.6692070960998535
I0216 15:55:43.373537 139578135525120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5376129746437073, loss=1.6369144916534424
I0216 15:57:06.399430 139578127132416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.8815822005271912, loss=1.6391905546188354
I0216 15:58:30.402420 139578135525120 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.7764773964881897, loss=1.607803225517273
I0216 15:59:54.812562 139578127132416 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7050119638442993, loss=1.6583274602890015
I0216 16:01:17.560225 139578135525120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.5888702273368835, loss=1.634472131729126
I0216 16:02:33.078636 139578127132416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6539965867996216, loss=1.6137101650238037
I0216 16:03:48.692320 139578135525120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.7262347340583801, loss=1.6326273679733276
I0216 16:04:17.157249 139688679413568 spec.py:321] Evaluating on the training split.
I0216 16:05:09.528455 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 16:06:01.411999 139688679413568 spec.py:349] Evaluating on the test split.
I0216 16:06:27.548331 139688679413568 submission_runner.py:408] Time since start: 25297.30s, 	Step: 29139, 	{'train/ctc_loss': Array(0.5951339, dtype=float32), 'train/wer': 0.20340781530110189, 'validation/ctc_loss': Array(0.74104714, dtype=float32), 'validation/wer': 0.22305144964615697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47743583, dtype=float32), 'test/wer': 0.1630613612820669, 'test/num_examples': 2472, 'score': 23079.745376586914, 'total_duration': 25297.29909467697, 'accumulated_submission_time': 23079.745376586914, 'accumulated_eval_time': 2215.55966258049, 'accumulated_logging_time': 0.8103816509246826}
I0216 16:06:27.590792 139578135525120 logging_writer.py:48] [29139] accumulated_eval_time=2215.559663, accumulated_logging_time=0.810382, accumulated_submission_time=23079.745377, global_step=29139, preemption_count=0, score=23079.745377, test/ctc_loss=0.477435827255249, test/num_examples=2472, test/wer=0.163061, total_duration=25297.299095, train/ctc_loss=0.595133900642395, train/wer=0.203408, validation/ctc_loss=0.7410471439361572, validation/num_examples=5348, validation/wer=0.223051
I0216 16:07:14.389381 139578127132416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.7738836407661438, loss=1.656832218170166
I0216 16:08:30.119220 139578135525120 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.7050678730010986, loss=1.61001718044281
I0216 16:09:45.778698 139578127132416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.7263696193695068, loss=1.70396089553833
I0216 16:11:01.377615 139578135525120 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.624154269695282, loss=1.5712515115737915
I0216 16:12:22.193521 139578127132416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6207307577133179, loss=1.6859755516052246
I0216 16:13:45.683869 139578135525120 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.7788971662521362, loss=1.6655651330947876
I0216 16:15:09.539367 139578127132416 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.8542127013206482, loss=1.6515229940414429
I0216 16:16:34.172001 139578135525120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.6615421175956726, loss=1.6319262981414795
I0216 16:17:49.679970 139578127132416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6330872774124146, loss=1.7000939846038818
I0216 16:19:05.509733 139578135525120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.6954151391983032, loss=1.6558860540390015
I0216 16:20:21.069968 139578127132416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.5236193537712097, loss=1.649660587310791
I0216 16:21:36.627572 139578135525120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.5234537124633789, loss=1.6573214530944824
I0216 16:22:52.303578 139578127132416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5742332935333252, loss=1.691085934638977
I0216 16:24:15.702632 139578135525120 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.5242270231246948, loss=1.5985020399093628
I0216 16:25:39.579390 139578127132416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.7536978721618652, loss=1.6417144536972046
I0216 16:27:02.816067 139578135525120 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.7915104627609253, loss=1.6394139528274536
I0216 16:28:26.551492 139578127132416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.5391111373901367, loss=1.6017203330993652
I0216 16:29:52.944403 139578135525120 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.6419408321380615, loss=1.5820122957229614
I0216 16:30:28.074075 139688679413568 spec.py:321] Evaluating on the training split.
I0216 16:31:20.063200 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 16:32:11.410624 139688679413568 spec.py:349] Evaluating on the test split.
I0216 16:32:37.817971 139688679413568 submission_runner.py:408] Time since start: 26867.57s, 	Step: 30948, 	{'train/ctc_loss': Array(0.5952117, dtype=float32), 'train/wer': 0.20037932075385728, 'validation/ctc_loss': Array(0.70316494, dtype=float32), 'validation/wer': 0.21365747221873582, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46406707, dtype=float32), 'test/wer': 0.15609448946844595, 'test/num_examples': 2472, 'score': 24520.142600536346, 'total_duration': 26867.569379091263, 'accumulated_submission_time': 24520.142600536346, 'accumulated_eval_time': 2345.2983088493347, 'accumulated_logging_time': 0.8707478046417236}
I0216 16:32:37.860228 139578135525120 logging_writer.py:48] [30948] accumulated_eval_time=2345.298309, accumulated_logging_time=0.870748, accumulated_submission_time=24520.142601, global_step=30948, preemption_count=0, score=24520.142601, test/ctc_loss=0.4640670716762543, test/num_examples=2472, test/wer=0.156094, total_duration=26867.569379, train/ctc_loss=0.5952116847038269, train/wer=0.200379, validation/ctc_loss=0.7031649351119995, validation/num_examples=5348, validation/wer=0.213657
I0216 16:33:17.786986 139578127132416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.5654375553131104, loss=1.5520200729370117
I0216 16:34:33.502883 139578135525120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.5778310894966125, loss=1.6270962953567505
I0216 16:35:49.050241 139578127132416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.4940306544303894, loss=1.5815467834472656
I0216 16:37:04.625693 139578135525120 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.7007030248641968, loss=1.5699613094329834
I0216 16:38:20.247781 139578127132416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6080138087272644, loss=1.5902589559555054
I0216 16:39:35.901231 139578135525120 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.5652554035186768, loss=1.5774058103561401
I0216 16:40:51.481653 139578127132416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.5558225512504578, loss=1.5858720541000366
I0216 16:42:13.429722 139578135525120 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.5357691645622253, loss=1.585974931716919
I0216 16:43:37.397368 139578127132416 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.539819598197937, loss=1.6021472215652466
I0216 16:45:01.139178 139578135525120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7132158875465393, loss=1.5913841724395752
I0216 16:46:22.353215 139578135525120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.5018901228904724, loss=1.622621774673462
I0216 16:47:37.747862 139578127132416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.8597272634506226, loss=1.645185112953186
I0216 16:48:53.254714 139578135525120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6413190364837646, loss=1.6366499662399292
I0216 16:50:08.940296 139578127132416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.511061429977417, loss=1.6322811841964722
I0216 16:51:24.466607 139578135525120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.7760323286056519, loss=1.6763324737548828
I0216 16:52:39.985231 139578127132416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.5544655919075012, loss=1.630389928817749
I0216 16:53:58.190327 139578135525120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.5639581680297852, loss=1.6235079765319824
I0216 16:55:21.464353 139578127132416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.5741078853607178, loss=1.6188050508499146
I0216 16:56:38.383914 139688679413568 spec.py:321] Evaluating on the training split.
I0216 16:57:42.187047 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 16:58:34.113206 139688679413568 spec.py:349] Evaluating on the test split.
I0216 16:59:00.167249 139688679413568 submission_runner.py:408] Time since start: 28449.92s, 	Step: 32794, 	{'train/ctc_loss': Array(0.42188603, dtype=float32), 'train/wer': 0.14991927881854528, 'validation/ctc_loss': Array(0.6963656, dtype=float32), 'validation/wer': 0.20946735279067746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45321754, dtype=float32), 'test/wer': 0.1542461357219751, 'test/num_examples': 2472, 'score': 25960.58037161827, 'total_duration': 28449.918375730515, 'accumulated_submission_time': 25960.58037161827, 'accumulated_eval_time': 2487.0760929584503, 'accumulated_logging_time': 0.9291160106658936}
I0216 16:59:00.202501 139578135525120 logging_writer.py:48] [32794] accumulated_eval_time=2487.076093, accumulated_logging_time=0.929116, accumulated_submission_time=25960.580372, global_step=32794, preemption_count=0, score=25960.580372, test/ctc_loss=0.4532175362110138, test/num_examples=2472, test/wer=0.154246, total_duration=28449.918376, train/ctc_loss=0.42188602685928345, train/wer=0.149919, validation/ctc_loss=0.6963655948638916, validation/num_examples=5348, validation/wer=0.209467
I0216 16:59:05.578454 139578127132416 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.6085826754570007, loss=1.595907211303711
I0216 17:00:20.980087 139578135525120 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.5355837345123291, loss=1.5909417867660522
I0216 17:01:40.152695 139578135525120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5401317477226257, loss=1.5348553657531738
I0216 17:02:55.779648 139578127132416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6932729482650757, loss=1.5526385307312012
I0216 17:04:11.341251 139578135525120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.6825598478317261, loss=1.6007673740386963
I0216 17:05:26.956716 139578127132416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.777149498462677, loss=1.580283522605896
I0216 17:06:42.536638 139578135525120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6026241183280945, loss=1.5589582920074463
I0216 17:08:00.258117 139578127132416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.8101964592933655, loss=1.6081331968307495
I0216 17:09:23.096453 139578135525120 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.7693665027618408, loss=1.6089082956314087
I0216 17:10:46.993860 139578127132416 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.8018136024475098, loss=1.6650179624557495
I0216 17:12:10.937831 139578135525120 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.693323016166687, loss=1.6312289237976074
I0216 17:13:34.768674 139578127132416 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.6010847687721252, loss=1.6465071439743042
I0216 17:15:00.414238 139578135525120 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5158423185348511, loss=1.6293295621871948
I0216 17:16:15.909559 139578127132416 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.5893046855926514, loss=1.598684310913086
I0216 17:17:31.406930 139578135525120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.5292982459068298, loss=1.6060622930526733
I0216 17:18:46.948171 139578127132416 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.8456142544746399, loss=1.5440213680267334
I0216 17:20:02.490931 139578135525120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.5388568043708801, loss=1.5128302574157715
I0216 17:21:18.193226 139578127132416 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.6121352314949036, loss=1.5857454538345337
I0216 17:22:40.094233 139578135525120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.5229687094688416, loss=1.527783751487732
I0216 17:23:00.521634 139688679413568 spec.py:321] Evaluating on the training split.
I0216 17:23:56.468632 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 17:24:48.746451 139688679413568 spec.py:349] Evaluating on the test split.
I0216 17:25:15.268954 139688679413568 submission_runner.py:408] Time since start: 30025.02s, 	Step: 34626, 	{'train/ctc_loss': Array(0.3637514, dtype=float32), 'train/wer': 0.1303431418243586, 'validation/ctc_loss': Array(0.67736506, dtype=float32), 'validation/wer': 0.20642613707676416, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42954934, dtype=float32), 'test/wer': 0.14575589543598805, 'test/num_examples': 2472, 'score': 27400.81459593773, 'total_duration': 30025.020079612732, 'accumulated_submission_time': 27400.81459593773, 'accumulated_eval_time': 2621.817850112915, 'accumulated_logging_time': 0.9812886714935303}
I0216 17:25:15.305627 139578135525120 logging_writer.py:48] [34626] accumulated_eval_time=2621.817850, accumulated_logging_time=0.981289, accumulated_submission_time=27400.814596, global_step=34626, preemption_count=0, score=27400.814596, test/ctc_loss=0.42954933643341064, test/num_examples=2472, test/wer=0.145756, total_duration=30025.020080, train/ctc_loss=0.3637514114379883, train/wer=0.130343, validation/ctc_loss=0.6773650646209717, validation/num_examples=5348, validation/wer=0.206426
I0216 17:26:11.924681 139578127132416 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.5455637574195862, loss=1.5500727891921997
I0216 17:27:27.454555 139578135525120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6863294243812561, loss=1.605402946472168
I0216 17:28:43.155930 139578127132416 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.7400558590888977, loss=1.648522973060608
I0216 17:29:58.755741 139578135525120 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.8991422653198242, loss=1.5751978158950806
I0216 17:31:17.759705 139578135525120 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.545714795589447, loss=1.5378453731536865
I0216 17:32:33.299597 139578127132416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6799869537353516, loss=1.6305326223373413
I0216 17:33:48.848687 139578135525120 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.7202959060668945, loss=1.517443299293518
I0216 17:35:04.446194 139578127132416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.7520300149917603, loss=1.552527904510498
I0216 17:36:19.927751 139578135525120 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7903938293457031, loss=1.5393288135528564
I0216 17:37:40.253292 139578127132416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.5526961088180542, loss=1.5025931596755981
I0216 17:39:03.307253 139578135525120 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.5919677019119263, loss=1.5807760953903198
I0216 17:40:27.864389 139578127132416 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.5479008555412292, loss=1.553215742111206
I0216 17:41:50.749317 139578135525120 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.6585144996643066, loss=1.4791616201400757
I0216 17:43:14.611902 139578127132416 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6511708498001099, loss=1.5703041553497314
I0216 17:44:37.085740 139578135525120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.6462351083755493, loss=1.5356885194778442
I0216 17:45:52.665246 139578127132416 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.6310338973999023, loss=1.5152654647827148
I0216 17:47:08.116446 139578135525120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.5416335463523865, loss=1.5497394800186157
I0216 17:48:23.681139 139578127132416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.8583406805992126, loss=1.5444729328155518
I0216 17:49:15.348401 139688679413568 spec.py:321] Evaluating on the training split.
I0216 17:50:10.390307 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 17:51:01.667175 139688679413568 spec.py:349] Evaluating on the test split.
I0216 17:51:27.547700 139688679413568 submission_runner.py:408] Time since start: 31597.30s, 	Step: 36470, 	{'train/ctc_loss': Array(0.3567816, dtype=float32), 'train/wer': 0.12728226377488133, 'validation/ctc_loss': Array(0.66085625, dtype=float32), 'validation/wer': 0.20038232426117766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41625735, dtype=float32), 'test/wer': 0.1412467247577844, 'test/num_examples': 2472, 'score': 28840.77037382126, 'total_duration': 31597.29881477356, 'accumulated_submission_time': 28840.77037382126, 'accumulated_eval_time': 2754.01162815094, 'accumulated_logging_time': 1.035853385925293}
I0216 17:51:27.586098 139578135525120 logging_writer.py:48] [36470] accumulated_eval_time=2754.011628, accumulated_logging_time=1.035853, accumulated_submission_time=28840.770374, global_step=36470, preemption_count=0, score=28840.770374, test/ctc_loss=0.4162573516368866, test/num_examples=2472, test/wer=0.141247, total_duration=31597.298815, train/ctc_loss=0.35678160190582275, train/wer=0.127282, validation/ctc_loss=0.6608562469482422, validation/num_examples=5348, validation/wer=0.200382
I0216 17:51:50.955317 139578127132416 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6936312317848206, loss=1.5381090641021729
I0216 17:53:06.424023 139578135525120 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6229799389839172, loss=1.575617790222168
I0216 17:54:21.904580 139578127132416 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.725144624710083, loss=1.5568232536315918
I0216 17:55:37.291524 139578135525120 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6533299684524536, loss=1.5644830465316772
I0216 17:56:56.364616 139578127132416 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.5655150413513184, loss=1.6306052207946777
I0216 17:58:20.616939 139578135525120 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6566303968429565, loss=1.4950520992279053
I0216 17:59:45.663477 139578135525120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.6161471605300903, loss=1.5200486183166504
I0216 18:01:01.120030 139578127132416 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.6302459239959717, loss=1.4397791624069214
I0216 18:02:16.640479 139578135525120 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.6573528051376343, loss=1.5916916131973267
I0216 18:03:32.075020 139578127132416 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6535203456878662, loss=1.5270864963531494
I0216 18:04:47.367883 139578135525120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8318182826042175, loss=1.5685625076293945
I0216 18:06:05.119919 139578127132416 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.5504071712493896, loss=1.5517158508300781
I0216 18:07:29.041326 139578135525120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.6737421751022339, loss=1.536520004272461
I0216 18:08:53.180848 139578127132416 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.5987488031387329, loss=1.6195919513702393
I0216 18:10:16.729356 139578135525120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6002708077430725, loss=1.4837403297424316
I0216 18:11:40.329006 139578127132416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.6413483023643494, loss=1.512328028678894
I0216 18:13:04.305631 139578135525120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.6004977822303772, loss=1.5836981534957886
I0216 18:14:24.425809 139578135525120 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.541881263256073, loss=1.5130466222763062
I0216 18:15:28.269036 139688679413568 spec.py:321] Evaluating on the training split.
I0216 18:16:23.613752 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 18:17:15.075379 139688679413568 spec.py:349] Evaluating on the test split.
I0216 18:17:40.971479 139688679413568 submission_runner.py:408] Time since start: 33170.72s, 	Step: 38286, 	{'train/ctc_loss': Array(0.3339406, dtype=float32), 'train/wer': 0.12178276727282134, 'validation/ctc_loss': Array(0.637265, dtype=float32), 'validation/wer': 0.19318960773144617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40459725, dtype=float32), 'test/wer': 0.1378546909593159, 'test/num_examples': 2472, 'score': 30281.36931490898, 'total_duration': 33170.72305393219, 'accumulated_submission_time': 30281.36931490898, 'accumulated_eval_time': 2886.7089653015137, 'accumulated_logging_time': 1.089693307876587}
I0216 18:17:41.013131 139578135525120 logging_writer.py:48] [38286] accumulated_eval_time=2886.708965, accumulated_logging_time=1.089693, accumulated_submission_time=30281.369315, global_step=38286, preemption_count=0, score=30281.369315, test/ctc_loss=0.4045972526073456, test/num_examples=2472, test/wer=0.137855, total_duration=33170.723054, train/ctc_loss=0.3339405953884125, train/wer=0.121783, validation/ctc_loss=0.6372650265693665, validation/num_examples=5348, validation/wer=0.193190
I0216 18:17:52.377945 139578127132416 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.5084936022758484, loss=1.4743022918701172
I0216 18:19:07.795441 139578135525120 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.661722719669342, loss=1.5506627559661865
I0216 18:20:23.232754 139578127132416 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.5284501314163208, loss=1.534705638885498
I0216 18:21:38.716226 139578135525120 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.7963060140609741, loss=1.5687679052352905
I0216 18:22:54.330307 139578127132416 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.5562067627906799, loss=1.5359302759170532
I0216 18:24:13.466896 139578135525120 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.5749716758728027, loss=1.498854637145996
I0216 18:25:36.145923 139578127132416 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.8220998644828796, loss=1.5466498136520386
I0216 18:26:59.454985 139578135525120 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6786141395568848, loss=1.491983413696289
I0216 18:28:22.612555 139578127132416 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.5827903747558594, loss=1.4819718599319458
I0216 18:29:45.149311 139578135525120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6072673797607422, loss=1.547014832496643
I0216 18:31:01.112145 139578127132416 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.5516229271888733, loss=1.5509852170944214
I0216 18:32:16.872717 139578135525120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.5892534852027893, loss=1.5495760440826416
I0216 18:33:32.449982 139578127132416 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8001723885536194, loss=1.5290759801864624
I0216 18:34:48.105178 139578135525120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.5191580653190613, loss=1.509276032447815
I0216 18:36:10.306509 139578127132416 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.6303523778915405, loss=1.4880050420761108
I0216 18:37:34.758889 139578135525120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.6281484365463257, loss=1.6134666204452515
I0216 18:38:58.768847 139578127132416 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.6328547596931458, loss=1.5080453157424927
I0216 18:40:22.783133 139578135525120 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.6635528206825256, loss=1.5172460079193115
I0216 18:41:41.445589 139688679413568 spec.py:321] Evaluating on the training split.
I0216 18:42:38.688094 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 18:43:30.686152 139688679413568 spec.py:349] Evaluating on the test split.
I0216 18:43:56.698376 139688679413568 submission_runner.py:408] Time since start: 34746.45s, 	Step: 40096, 	{'train/ctc_loss': Array(0.3399412, dtype=float32), 'train/wer': 0.12416167131580509, 'validation/ctc_loss': Array(0.62865573, dtype=float32), 'validation/wer': 0.19244619944582292, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39331612, dtype=float32), 'test/wer': 0.13308146974590213, 'test/num_examples': 2472, 'score': 31721.717066049576, 'total_duration': 34746.4487221241, 'accumulated_submission_time': 31721.717066049576, 'accumulated_eval_time': 3021.955439567566, 'accumulated_logging_time': 1.148374080657959}
I0216 18:43:56.736879 139578135525120 logging_writer.py:48] [40096] accumulated_eval_time=3021.955440, accumulated_logging_time=1.148374, accumulated_submission_time=31721.717066, global_step=40096, preemption_count=0, score=31721.717066, test/ctc_loss=0.3933161199092865, test/num_examples=2472, test/wer=0.133081, total_duration=34746.448722, train/ctc_loss=0.33994120359420776, train/wer=0.124162, validation/ctc_loss=0.628655731678009, validation/num_examples=5348, validation/wer=0.192446
I0216 18:44:00.620936 139578127132416 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.632093071937561, loss=1.4990721940994263
I0216 18:45:19.558411 139578135525120 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6092755794525146, loss=1.5314429998397827
I0216 18:46:35.109631 139578127132416 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6002132892608643, loss=1.5348799228668213
I0216 18:47:50.615650 139578135525120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6005354523658752, loss=1.4855245351791382
I0216 18:49:06.488349 139578127132416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6322985291481018, loss=1.4768385887145996
I0216 18:50:22.162206 139578135525120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.5301212668418884, loss=1.4861016273498535
I0216 18:51:43.260047 139578127132416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.5292321443557739, loss=1.4854907989501953
I0216 18:53:06.025780 139578135525120 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.5832207202911377, loss=1.568217396736145
I0216 18:54:30.008689 139578127132416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.7770216464996338, loss=1.5152015686035156
I0216 18:55:53.581674 139578135525120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.577495276927948, loss=1.5022588968276978
I0216 18:57:16.633399 139578127132416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5473747849464417, loss=1.53111732006073
I0216 18:58:42.505471 139578135525120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.597018301486969, loss=1.462345004081726
I0216 18:59:58.079175 139578127132416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.5867130160331726, loss=1.4798712730407715
I0216 19:01:13.694380 139578135525120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.5217604637145996, loss=1.465987205505371
I0216 19:02:29.303314 139578127132416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6966562271118164, loss=1.4733216762542725
I0216 19:03:44.865459 139578135525120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.5792060494422913, loss=1.510038137435913
I0216 19:05:01.576807 139578127132416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.6659058928489685, loss=1.506333827972412
I0216 19:06:24.702033 139578135525120 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.5228604078292847, loss=1.4972004890441895
I0216 19:07:48.542784 139578127132416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6062746644020081, loss=1.515318512916565
I0216 19:07:57.230885 139688679413568 spec.py:321] Evaluating on the training split.
I0216 19:08:52.891889 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 19:09:45.625024 139688679413568 spec.py:349] Evaluating on the test split.
I0216 19:10:12.362843 139688679413568 submission_runner.py:408] Time since start: 36322.11s, 	Step: 41912, 	{'train/ctc_loss': Array(0.3111691, dtype=float32), 'train/wer': 0.11212839858547488, 'validation/ctc_loss': Array(0.60674435, dtype=float32), 'validation/wer': 0.18461627581412862, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3796361, dtype=float32), 'test/wer': 0.1285316759084354, 'test/num_examples': 2472, 'score': 33162.12484502792, 'total_duration': 36322.11321473122, 'accumulated_submission_time': 33162.12484502792, 'accumulated_eval_time': 3157.081080198288, 'accumulated_logging_time': 1.2041044235229492}
I0216 19:10:12.406245 139578135525120 logging_writer.py:48] [41912] accumulated_eval_time=3157.081080, accumulated_logging_time=1.204104, accumulated_submission_time=33162.124845, global_step=41912, preemption_count=0, score=33162.124845, test/ctc_loss=0.37963610887527466, test/num_examples=2472, test/wer=0.128532, total_duration=36322.113215, train/ctc_loss=0.3111690878868103, train/wer=0.112128, validation/ctc_loss=0.6067443490028381, validation/num_examples=5348, validation/wer=0.184616
I0216 19:11:19.553064 139578127132416 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6566826701164246, loss=1.4915214776992798
I0216 19:12:35.187190 139578135525120 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.6661581993103027, loss=1.4758548736572266
I0216 19:13:50.718885 139578127132416 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.5494688749313354, loss=1.458620548248291
I0216 19:15:09.702759 139578135525120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.5789605379104614, loss=1.3973414897918701
I0216 19:16:25.218878 139578127132416 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.5783877968788147, loss=1.4763743877410889
I0216 19:17:40.735342 139578135525120 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.5820358395576477, loss=1.5136687755584717
I0216 19:18:56.249383 139578127132416 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.6794425845146179, loss=1.4313033819198608
I0216 19:20:11.866178 139578135525120 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.578763484954834, loss=1.44091796875
I0216 19:21:31.640753 139578127132416 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.6307117342948914, loss=1.5548732280731201
I0216 19:22:55.226500 139578135525120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.6783899068832397, loss=1.467283844947815
I0216 19:24:19.527348 139578127132416 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6246107220649719, loss=1.4953843355178833
I0216 19:25:42.707525 139578135525120 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.5904269218444824, loss=1.4864333868026733
I0216 19:27:05.980602 139578127132416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.5517197847366333, loss=1.4526262283325195
I0216 19:28:30.085454 139578135525120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.6408844590187073, loss=1.4185140132904053
I0216 19:29:45.694702 139578127132416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.5617159605026245, loss=1.4478427171707153
I0216 19:31:01.293841 139578135525120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.5762009024620056, loss=1.423000693321228
I0216 19:32:17.076993 139578127132416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.5636621713638306, loss=1.461632251739502
I0216 19:33:32.803024 139578135525120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7592204809188843, loss=1.4340972900390625
I0216 19:34:12.606121 139688679413568 spec.py:321] Evaluating on the training split.
I0216 19:35:07.576614 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 19:35:59.623885 139688679413568 spec.py:349] Evaluating on the test split.
I0216 19:36:26.427245 139688679413568 submission_runner.py:408] Time since start: 37896.18s, 	Step: 43754, 	{'train/ctc_loss': Array(0.3599174, dtype=float32), 'train/wer': 0.12500478243997354, 'validation/ctc_loss': Array(0.59408325, dtype=float32), 'validation/wer': 0.1794317271208859, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36802, dtype=float32), 'test/wer': 0.12361627363760079, 'test/num_examples': 2472, 'score': 34602.23737287521, 'total_duration': 37896.17702317238, 'accumulated_submission_time': 34602.23737287521, 'accumulated_eval_time': 3290.895377635956, 'accumulated_logging_time': 1.264575481414795}
I0216 19:36:26.467389 139578135525120 logging_writer.py:48] [43754] accumulated_eval_time=3290.895378, accumulated_logging_time=1.264575, accumulated_submission_time=34602.237373, global_step=43754, preemption_count=0, score=34602.237373, test/ctc_loss=0.3680199980735779, test/num_examples=2472, test/wer=0.123616, total_duration=37896.177023, train/ctc_loss=0.35991740226745605, train/wer=0.125005, validation/ctc_loss=0.5940832495689392, validation/num_examples=5348, validation/wer=0.179432
I0216 19:37:01.899653 139578127132416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.5632923245429993, loss=1.4125474691390991
I0216 19:38:17.469613 139578135525120 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7367406487464905, loss=1.48979914188385
I0216 19:39:33.391386 139578127132416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.5958683490753174, loss=1.4640780687332153
I0216 19:40:49.075098 139578135525120 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.6818673610687256, loss=1.475386142730713
I0216 19:42:08.212293 139578127132416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.46699678897857666, loss=1.3877573013305664
I0216 19:43:34.738452 139578135525120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7073276042938232, loss=1.4262374639511108
I0216 19:44:50.297987 139578127132416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.7170593738555908, loss=1.4263173341751099
I0216 19:46:05.931885 139578135525120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.5369628667831421, loss=1.4212651252746582
I0216 19:47:21.442697 139578127132416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.6100374460220337, loss=1.4578778743743896
I0216 19:48:37.038810 139578135525120 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.7965465784072876, loss=1.4380767345428467
I0216 19:49:54.389725 139578127132416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.5882247686386108, loss=1.441012978553772
I0216 19:51:18.314459 139578135525120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.7225390076637268, loss=1.4445329904556274
I0216 19:52:41.622008 139578127132416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.7238267064094543, loss=1.4449998140335083
I0216 19:54:04.916026 139578135525120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.5367155075073242, loss=1.4422916173934937
I0216 19:55:28.925251 139578127132416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.5661036372184753, loss=1.482863187789917
I0216 19:56:52.424197 139578135525120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.6993362903594971, loss=1.4871379137039185
I0216 19:58:13.151939 139578135525120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.5689738988876343, loss=1.3850113153457642
I0216 19:59:28.697122 139578127132416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.6716793179512024, loss=1.4138644933700562
I0216 20:00:26.527573 139688679413568 spec.py:321] Evaluating on the training split.
I0216 20:01:21.333830 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 20:02:13.215213 139688679413568 spec.py:349] Evaluating on the test split.
I0216 20:02:39.425253 139688679413568 submission_runner.py:408] Time since start: 39469.18s, 	Step: 45578, 	{'train/ctc_loss': Array(0.29631695, dtype=float32), 'train/wer': 0.10954912157708044, 'validation/ctc_loss': Array(0.5799549, dtype=float32), 'validation/wer': 0.17433407030518358, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35863748, dtype=float32), 'test/wer': 0.12239757885970792, 'test/num_examples': 2472, 'score': 36042.2100276947, 'total_duration': 39469.1763818264, 'accumulated_submission_time': 36042.2100276947, 'accumulated_eval_time': 3423.7875752449036, 'accumulated_logging_time': 1.323909044265747}
I0216 20:02:39.463750 139578135525120 logging_writer.py:48] [45578] accumulated_eval_time=3423.787575, accumulated_logging_time=1.323909, accumulated_submission_time=36042.210028, global_step=45578, preemption_count=0, score=36042.210028, test/ctc_loss=0.3586374819278717, test/num_examples=2472, test/wer=0.122398, total_duration=39469.176382, train/ctc_loss=0.2963169515132904, train/wer=0.109549, validation/ctc_loss=0.5799549221992493, validation/num_examples=5348, validation/wer=0.174334
I0216 20:02:56.873137 139578127132416 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.5439074039459229, loss=1.4029954671859741
I0216 20:04:12.378426 139578135525120 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.6067779064178467, loss=1.3961271047592163
I0216 20:05:27.884091 139578127132416 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.5217630863189697, loss=1.3838227987289429
I0216 20:06:43.557376 139578135525120 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.6758232116699219, loss=1.3935859203338623
I0216 20:08:00.427941 139578127132416 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6603718400001526, loss=1.4566948413848877
I0216 20:09:23.416363 139578135525120 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.641495406627655, loss=1.43143630027771
I0216 20:10:47.233458 139578127132416 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.5641778707504272, loss=1.4360600709915161
I0216 20:12:11.117388 139578135525120 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.5135006308555603, loss=1.4333659410476685
I0216 20:13:34.672557 139578135525120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.6709243655204773, loss=1.432350993156433
I0216 20:14:50.247430 139578127132416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6747394800186157, loss=1.4055734872817993
I0216 20:16:05.733897 139578135525120 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.6063799858093262, loss=1.3976365327835083
I0216 20:17:21.254313 139578127132416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.6099411845207214, loss=1.3997355699539185
I0216 20:18:36.735579 139578135525120 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.7395361661911011, loss=1.4323114156723022
I0216 20:19:57.255683 139578127132416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7086338400840759, loss=1.4037445783615112
I0216 20:21:21.659886 139578135525120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.6139029264450073, loss=1.4728261232376099
I0216 20:22:45.893249 139578127132416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.529391884803772, loss=1.4166569709777832
I0216 20:24:08.633357 139578135525120 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.5225180983543396, loss=1.3890966176986694
I0216 20:25:31.864566 139578127132416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.5556241869926453, loss=1.4469035863876343
I0216 20:26:42.443992 139688679413568 spec.py:321] Evaluating on the training split.
I0216 20:27:37.682680 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 20:28:29.400877 139688679413568 spec.py:349] Evaluating on the test split.
I0216 20:28:55.813357 139688679413568 submission_runner.py:408] Time since start: 41045.56s, 	Step: 47381, 	{'train/ctc_loss': Array(0.28037357, dtype=float32), 'train/wer': 0.10386011495524543, 'validation/ctc_loss': Array(0.56814826, dtype=float32), 'validation/wer': 0.17082943124438824, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3452232, dtype=float32), 'test/wer': 0.1170962565758739, 'test/num_examples': 2472, 'score': 37485.10423183441, 'total_duration': 41045.56417584419, 'accumulated_submission_time': 37485.10423183441, 'accumulated_eval_time': 3557.1513023376465, 'accumulated_logging_time': 1.3801522254943848}
I0216 20:28:55.857236 139578135525120 logging_writer.py:48] [47381] accumulated_eval_time=3557.151302, accumulated_logging_time=1.380152, accumulated_submission_time=37485.104232, global_step=47381, preemption_count=0, score=37485.104232, test/ctc_loss=0.34522318840026855, test/num_examples=2472, test/wer=0.117096, total_duration=41045.564176, train/ctc_loss=0.28037357330322266, train/wer=0.103860, validation/ctc_loss=0.5681482553482056, validation/num_examples=5348, validation/wer=0.170829
I0216 20:29:11.015800 139578127132416 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.7377721667289734, loss=1.4493718147277832
I0216 20:30:27.019815 139578135525120 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.6995497941970825, loss=1.390380859375
I0216 20:31:42.636945 139578127132416 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.7349516153335571, loss=1.4197286367416382
I0216 20:32:58.229114 139578135525120 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.5802081823348999, loss=1.3386149406433105
I0216 20:34:13.956896 139578127132416 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.6105017066001892, loss=1.4096709489822388
I0216 20:35:29.635990 139578135525120 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6030325889587402, loss=1.3696599006652832
I0216 20:36:45.425398 139578127132416 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.5690588355064392, loss=1.4510546922683716
I0216 20:38:06.464939 139578135525120 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.6732094287872314, loss=1.410109519958496
I0216 20:39:31.038004 139578127132416 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.7538559436798096, loss=1.4250755310058594
I0216 20:40:53.903852 139578135525120 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.6505100727081299, loss=1.4082067012786865
I0216 20:42:17.305179 139578127132416 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.6811463832855225, loss=1.408867359161377
I0216 20:43:36.656802 139578135525120 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.641181230545044, loss=1.4671876430511475
I0216 20:44:52.040090 139578127132416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.5875039100646973, loss=1.3316890001296997
I0216 20:46:07.815019 139578135525120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.5505280494689941, loss=1.3879246711730957
I0216 20:47:23.547141 139578127132416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.6113314628601074, loss=1.4188189506530762
I0216 20:48:39.026768 139578135525120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.5685730576515198, loss=1.3270939588546753
I0216 20:50:02.370688 139578127132416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.6085535883903503, loss=1.4310832023620605
I0216 20:51:26.866735 139578135525120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.6889747381210327, loss=1.3930003643035889
I0216 20:52:50.122114 139578127132416 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.711887538433075, loss=1.3398855924606323
I0216 20:52:56.350134 139688679413568 spec.py:321] Evaluating on the training split.
I0216 20:53:52.096014 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 20:54:44.476365 139688679413568 spec.py:349] Evaluating on the test split.
I0216 20:55:11.353558 139688679413568 submission_runner.py:408] Time since start: 42621.10s, 	Step: 49209, 	{'train/ctc_loss': Array(0.2589628, dtype=float32), 'train/wer': 0.09479179338541363, 'validation/ctc_loss': Array(0.55032986, dtype=float32), 'validation/wer': 0.16804889116309604, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33619353, dtype=float32), 'test/wer': 0.11274957853472264, 'test/num_examples': 2472, 'score': 38925.508009433746, 'total_duration': 42621.10450196266, 'accumulated_submission_time': 38925.508009433746, 'accumulated_eval_time': 3692.1489946842194, 'accumulated_logging_time': 1.4441523551940918}
I0216 20:55:11.391099 139578135525120 logging_writer.py:48] [49209] accumulated_eval_time=3692.148995, accumulated_logging_time=1.444152, accumulated_submission_time=38925.508009, global_step=49209, preemption_count=0, score=38925.508009, test/ctc_loss=0.3361935317516327, test/num_examples=2472, test/wer=0.112750, total_duration=42621.104502, train/ctc_loss=0.25896281003952026, train/wer=0.094792, validation/ctc_loss=0.550329864025116, validation/num_examples=5348, validation/wer=0.168049
I0216 20:56:20.842402 139578127132416 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.5632821321487427, loss=1.377640962600708
I0216 20:57:36.537537 139578135525120 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.5936495065689087, loss=1.3950550556182861
I0216 20:58:55.576628 139578135525120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.5111541152000427, loss=1.3204636573791504
I0216 21:00:11.140158 139578127132416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.5977879762649536, loss=1.3953347206115723
I0216 21:01:26.778200 139578135525120 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.7325424551963806, loss=1.3640621900558472
I0216 21:02:42.430527 139578127132416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.49661633372306824, loss=1.3599956035614014
I0216 21:03:58.462629 139578135525120 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.5808854699134827, loss=1.366930603981018
I0216 21:05:20.447929 139578127132416 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.6494016051292419, loss=1.3550642728805542
I0216 21:06:44.943542 139578135525120 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.6170399785041809, loss=1.3504424095153809
I0216 21:08:08.693714 139578127132416 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.7181894779205322, loss=1.367933750152588
I0216 21:09:32.954814 139578135525120 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.5149150490760803, loss=1.4288866519927979
I0216 21:10:56.499711 139578127132416 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.6713006496429443, loss=1.3531684875488281
I0216 21:12:20.094492 139578135525120 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.5964404344558716, loss=1.3046280145645142
I0216 21:13:35.634579 139578127132416 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6542011499404907, loss=1.3192598819732666
I0216 21:14:51.318902 139578135525120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.5817224979400635, loss=1.3717225790023804
I0216 21:16:06.957932 139578127132416 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.6370265483856201, loss=1.360539197921753
I0216 21:17:22.535203 139578135525120 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.5444149374961853, loss=1.3219730854034424
I0216 21:18:38.641126 139578127132416 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.7049649357795715, loss=1.3337589502334595
I0216 21:19:11.520667 139688679413568 spec.py:321] Evaluating on the training split.
I0216 21:20:06.540076 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 21:20:58.147539 139688679413568 spec.py:349] Evaluating on the test split.
I0216 21:21:24.546130 139688679413568 submission_runner.py:408] Time since start: 44194.30s, 	Step: 51041, 	{'train/ctc_loss': Array(0.24425602, dtype=float32), 'train/wer': 0.09251374987281033, 'validation/ctc_loss': Array(0.53289413, dtype=float32), 'validation/wer': 0.16152234569450746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32483763, dtype=float32), 'test/wer': 0.10919505209920176, 'test/num_examples': 2472, 'score': 40365.552735090256, 'total_duration': 44194.29576420784, 'accumulated_submission_time': 40365.552735090256, 'accumulated_eval_time': 3825.167426109314, 'accumulated_logging_time': 1.4977757930755615}
I0216 21:21:24.590340 139578135525120 logging_writer.py:48] [51041] accumulated_eval_time=3825.167426, accumulated_logging_time=1.497776, accumulated_submission_time=40365.552735, global_step=51041, preemption_count=0, score=40365.552735, test/ctc_loss=0.3248376250267029, test/num_examples=2472, test/wer=0.109195, total_duration=44194.295764, train/ctc_loss=0.24425601959228516, train/wer=0.092514, validation/ctc_loss=0.5328941345214844, validation/num_examples=5348, validation/wer=0.161522
I0216 21:22:09.836439 139578127132416 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.5974347591400146, loss=1.3375662565231323
I0216 21:23:25.336614 139578135525120 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.6030359268188477, loss=1.3248862028121948
I0216 21:24:40.952784 139578127132416 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.7362928986549377, loss=1.3639100790023804
I0216 21:25:56.594297 139578135525120 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.6390661597251892, loss=1.3176053762435913
I0216 21:27:21.940242 139578135525120 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7521682977676392, loss=1.3986434936523438
I0216 21:28:37.431044 139578127132416 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.5591904520988464, loss=1.3954647779464722
I0216 21:29:52.867062 139578135525120 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.6028887629508972, loss=1.3539258241653442
I0216 21:31:08.431101 139578127132416 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.6935626864433289, loss=1.354218602180481
I0216 21:32:24.013395 139578135525120 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.5823899507522583, loss=1.3276952505111694
I0216 21:33:39.601569 139578127132416 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.6405640244483948, loss=1.3254468441009521
I0216 21:35:01.968729 139578135525120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.545433759689331, loss=1.318203330039978
I0216 21:36:26.221961 139578127132416 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.7519767880439758, loss=1.3212021589279175
I0216 21:37:50.055860 139578135525120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.585605263710022, loss=1.3558621406555176
I0216 21:39:13.376111 139578127132416 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.675603449344635, loss=1.3235715627670288
I0216 21:40:37.602380 139578135525120 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.606715977191925, loss=1.332108974456787
I0216 21:41:58.214706 139578135525120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6703010201454163, loss=1.3293280601501465
I0216 21:43:13.782435 139578127132416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.5864487886428833, loss=1.2406703233718872
I0216 21:44:29.405851 139578135525120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.6222473978996277, loss=1.3142364025115967
I0216 21:45:24.982761 139688679413568 spec.py:321] Evaluating on the training split.
I0216 21:46:19.879086 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 21:47:12.343392 139688679413568 spec.py:349] Evaluating on the test split.
I0216 21:47:39.398754 139688679413568 submission_runner.py:408] Time since start: 45769.15s, 	Step: 52875, 	{'train/ctc_loss': Array(0.25342596, dtype=float32), 'train/wer': 0.09347378746361591, 'validation/ctc_loss': Array(0.51060355, dtype=float32), 'validation/wer': 0.15436824777701613, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31024233, dtype=float32), 'test/wer': 0.10484837405805049, 'test/num_examples': 2472, 'score': 41805.859117269516, 'total_duration': 45769.149518728256, 'accumulated_submission_time': 41805.859117269516, 'accumulated_eval_time': 3959.5775384902954, 'accumulated_logging_time': 1.5582191944122314}
I0216 21:47:39.438807 139578135525120 logging_writer.py:48] [52875] accumulated_eval_time=3959.577538, accumulated_logging_time=1.558219, accumulated_submission_time=41805.859117, global_step=52875, preemption_count=0, score=41805.859117, test/ctc_loss=0.31024232506752014, test/num_examples=2472, test/wer=0.104848, total_duration=45769.149519, train/ctc_loss=0.2534259557723999, train/wer=0.093474, validation/ctc_loss=0.5106035470962524, validation/num_examples=5348, validation/wer=0.154368
I0216 21:47:59.101666 139578127132416 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.6746799349784851, loss=1.3565846681594849
I0216 21:49:14.555597 139578135525120 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.6059156656265259, loss=1.3228161334991455
I0216 21:50:30.252759 139578127132416 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.5446910262107849, loss=1.2957392930984497
I0216 21:51:45.883208 139578135525120 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.6078535914421082, loss=1.333966851234436
I0216 21:53:08.948600 139578127132416 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.6997027397155762, loss=1.3221561908721924
I0216 21:54:33.682337 139578135525120 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.6431785821914673, loss=1.3429627418518066
I0216 21:55:56.626013 139578127132416 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.7471423745155334, loss=1.3595020771026611
I0216 21:57:21.132371 139578135525120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.6408872604370117, loss=1.342660903930664
I0216 21:58:36.600867 139578127132416 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.6155943870544434, loss=1.3127444982528687
I0216 21:59:51.987046 139578135525120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.6418120265007019, loss=1.2637412548065186
I0216 22:01:07.472641 139578127132416 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.5992782711982727, loss=1.2727454900741577
I0216 22:02:23.082068 139578135525120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.6092430949211121, loss=1.333279013633728
I0216 22:03:41.279334 139578127132416 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.693842351436615, loss=1.3733543157577515
I0216 22:05:05.221492 139578135525120 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.7081271409988403, loss=1.3010159730911255
I0216 22:06:30.143848 139578127132416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.6491245031356812, loss=1.347572922706604
I0216 22:07:52.932829 139578135525120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.7151055335998535, loss=1.2687907218933105
I0216 22:09:16.382601 139578127132416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.5446023941040039, loss=1.3417093753814697
I0216 22:10:42.293697 139578135525120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.5915701389312744, loss=1.3027961254119873
I0216 22:11:40.041839 139688679413568 spec.py:321] Evaluating on the training split.
I0216 22:12:34.823034 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 22:13:26.952790 139688679413568 spec.py:349] Evaluating on the test split.
I0216 22:13:53.079970 139688679413568 submission_runner.py:408] Time since start: 47342.83s, 	Step: 54678, 	{'train/ctc_loss': Array(0.23427062, dtype=float32), 'train/wer': 0.08568783792312709, 'validation/ctc_loss': Array(0.4877075, dtype=float32), 'validation/wer': 0.148855440879732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29198828, dtype=float32), 'test/wer': 0.09842991489448134, 'test/num_examples': 2472, 'score': 43246.37415957451, 'total_duration': 47342.83117246628, 'accumulated_submission_time': 43246.37415957451, 'accumulated_eval_time': 4092.6102225780487, 'accumulated_logging_time': 1.617872714996338}
I0216 22:13:53.123255 139578135525120 logging_writer.py:48] [54678] accumulated_eval_time=4092.610223, accumulated_logging_time=1.617873, accumulated_submission_time=43246.374160, global_step=54678, preemption_count=0, score=43246.374160, test/ctc_loss=0.2919882833957672, test/num_examples=2472, test/wer=0.098430, total_duration=47342.831172, train/ctc_loss=0.2342706173658371, train/wer=0.085688, validation/ctc_loss=0.4877074956893921, validation/num_examples=5348, validation/wer=0.148855
I0216 22:14:10.514230 139578127132416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.7195358872413635, loss=1.288389801979065
I0216 22:15:26.012893 139578135525120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.7469538450241089, loss=1.3076720237731934
I0216 22:16:41.512141 139578127132416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.7035977840423584, loss=1.3165783882141113
I0216 22:17:57.019368 139578135525120 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.56088787317276, loss=1.2928109169006348
I0216 22:19:12.648031 139578127132416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.593832790851593, loss=1.2483627796173096
I0216 22:20:28.257641 139578135525120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.643311619758606, loss=1.3430659770965576
I0216 22:21:48.269827 139578127132416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.791208028793335, loss=1.2938536405563354
I0216 22:23:12.762320 139578135525120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.6170634031295776, loss=1.2721596956253052
I0216 22:24:37.507365 139578127132416 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.554091215133667, loss=1.3042064905166626
I0216 22:26:01.706496 139578135525120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.7281123399734497, loss=1.2948840856552124
I0216 22:27:22.282706 139578135525120 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.6684530377388, loss=1.295015811920166
I0216 22:28:37.782384 139578127132416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.6589778661727905, loss=1.267061710357666
I0216 22:29:53.285475 139578135525120 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.614841878414154, loss=1.2667933702468872
I0216 22:31:08.803296 139578127132416 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.606425940990448, loss=1.2861813306808472
I0216 22:32:24.295090 139578135525120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.5864797830581665, loss=1.2523927688598633
I0216 22:33:46.777008 139578127132416 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.6858788728713989, loss=1.2991472482681274
I0216 22:35:10.415536 139578135525120 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.6816344857215881, loss=1.262946605682373
I0216 22:36:32.895923 139578127132416 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.7215202450752258, loss=1.2871434688568115
I0216 22:37:53.321902 139688679413568 spec.py:321] Evaluating on the training split.
I0216 22:38:49.352374 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 22:39:40.719100 139688679413568 spec.py:349] Evaluating on the test split.
I0216 22:40:07.290246 139688679413568 submission_runner.py:408] Time since start: 48917.04s, 	Step: 56498, 	{'train/ctc_loss': Array(0.22292952, dtype=float32), 'train/wer': 0.08287230030942161, 'validation/ctc_loss': Array(0.46993592, dtype=float32), 'validation/wer': 0.143149540921247, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28381628, dtype=float32), 'test/wer': 0.09558629374606463, 'test/num_examples': 2472, 'score': 44686.488366127014, 'total_duration': 48917.04101586342, 'accumulated_submission_time': 44686.488366127014, 'accumulated_eval_time': 4226.572660923004, 'accumulated_logging_time': 1.6772680282592773}
I0216 22:40:07.328079 139578135525120 logging_writer.py:48] [56498] accumulated_eval_time=4226.572661, accumulated_logging_time=1.677268, accumulated_submission_time=44686.488366, global_step=56498, preemption_count=0, score=44686.488366, test/ctc_loss=0.28381627798080444, test/num_examples=2472, test/wer=0.095586, total_duration=48917.041016, train/ctc_loss=0.22292952239513397, train/wer=0.082872, validation/ctc_loss=0.46993592381477356, validation/num_examples=5348, validation/wer=0.143150
I0216 22:40:09.704614 139578127132416 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.613175094127655, loss=1.2427812814712524
I0216 22:41:25.219389 139578135525120 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.6620351672172546, loss=1.2901803255081177
I0216 22:42:44.287137 139578135525120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.7232076525688171, loss=1.2893986701965332
I0216 22:43:59.776433 139578127132416 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.7688946723937988, loss=1.2499127388000488
I0216 22:45:15.740443 139578135525120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.6438946723937988, loss=1.2934632301330566
I0216 22:46:31.453552 139578127132416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.6867702603340149, loss=1.2992966175079346
I0216 22:47:47.111143 139578135525120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.6940510272979736, loss=1.2925083637237549
I0216 22:49:07.744482 139578127132416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.6677207350730896, loss=1.2328770160675049
I0216 22:50:30.883428 139578135525120 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.6891137957572937, loss=1.254746913909912
I0216 22:51:53.660785 139578127132416 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.6490778923034668, loss=1.246667504310608
I0216 22:53:18.064286 139578135525120 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.6040737628936768, loss=1.291769027709961
I0216 22:54:41.613718 139578127132416 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.7145545482635498, loss=1.2982840538024902
I0216 22:56:07.418218 139578135525120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.816798746585846, loss=1.2190744876861572
I0216 22:57:23.033646 139578127132416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.5828422904014587, loss=1.1989758014678955
I0216 22:58:38.687392 139578135525120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.6651895046234131, loss=1.1833585500717163
I0216 22:59:54.258650 139578127132416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.5860356688499451, loss=1.2635371685028076
I0216 23:01:10.041663 139578135525120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.6563515663146973, loss=1.2807457447052002
I0216 23:02:25.649991 139578127132416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.6514264345169067, loss=1.227257490158081
I0216 23:03:48.546574 139578135525120 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.7267476916313171, loss=1.1912472248077393
I0216 23:04:07.898757 139688679413568 spec.py:321] Evaluating on the training split.
I0216 23:05:02.728904 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 23:05:54.523772 139688679413568 spec.py:349] Evaluating on the test split.
I0216 23:06:21.025897 139688679413568 submission_runner.py:408] Time since start: 50490.78s, 	Step: 58324, 	{'train/ctc_loss': Array(0.2015512, dtype=float32), 'train/wer': 0.07597622419593453, 'validation/ctc_loss': Array(0.45896965, dtype=float32), 'validation/wer': 0.1386118539830271, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27153856, dtype=float32), 'test/wer': 0.09111774622712408, 'test/num_examples': 2472, 'score': 46126.97168445587, 'total_duration': 50490.77713441849, 'accumulated_submission_time': 46126.97168445587, 'accumulated_eval_time': 4359.694350004196, 'accumulated_logging_time': 1.7328250408172607}
I0216 23:06:21.068383 139578135525120 logging_writer.py:48] [58324] accumulated_eval_time=4359.694350, accumulated_logging_time=1.732825, accumulated_submission_time=46126.971684, global_step=58324, preemption_count=0, score=46126.971684, test/ctc_loss=0.27153855562210083, test/num_examples=2472, test/wer=0.091118, total_duration=50490.777134, train/ctc_loss=0.20155119895935059, train/wer=0.075976, validation/ctc_loss=0.4589696526527405, validation/num_examples=5348, validation/wer=0.138612
I0216 23:07:19.166820 139578127132416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.6116980910301208, loss=1.156230092048645
I0216 23:08:34.726824 139578135525120 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.6804752945899963, loss=1.2676666975021362
I0216 23:09:50.400136 139578127132416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.6839953064918518, loss=1.2138867378234863
I0216 23:11:06.798547 139578135525120 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.6604306101799011, loss=1.2617920637130737
I0216 23:12:26.154894 139578135525120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.699733316898346, loss=1.2459080219268799
I0216 23:13:41.641054 139578127132416 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.6318477988243103, loss=1.235884666442871
I0216 23:14:57.176009 139578135525120 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.6435765624046326, loss=1.1976028680801392
I0216 23:16:12.781821 139578127132416 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.731891930103302, loss=1.2348122596740723
I0216 23:17:28.327503 139578135525120 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.7422420978546143, loss=1.2588350772857666
I0216 23:18:49.306678 139578127132416 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.6475223302841187, loss=1.2396246194839478
I0216 23:20:12.957260 139578135525120 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.5794659852981567, loss=1.2575033903121948
I0216 23:21:36.702565 139578127132416 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.6530126929283142, loss=1.2298617362976074
I0216 23:23:00.035058 139578135525120 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.6738729476928711, loss=1.179921269416809
I0216 23:24:23.972167 139578127132416 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.608326256275177, loss=1.193306803703308
I0216 23:25:46.020741 139578135525120 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.7734564542770386, loss=1.1768535375595093
I0216 23:27:01.704285 139578127132416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.6502624154090881, loss=1.1806228160858154
I0216 23:28:17.351927 139578135525120 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.8718833923339844, loss=1.1896954774856567
I0216 23:29:33.083372 139578127132416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.672653079032898, loss=1.2300816774368286
I0216 23:30:21.159576 139688679413568 spec.py:321] Evaluating on the training split.
I0216 23:31:15.782012 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 23:32:07.036586 139688679413568 spec.py:349] Evaluating on the test split.
I0216 23:32:33.192488 139688679413568 submission_runner.py:408] Time since start: 52062.94s, 	Step: 60165, 	{'train/ctc_loss': Array(0.18237127, dtype=float32), 'train/wer': 0.06818781602643274, 'validation/ctc_loss': Array(0.4402338, dtype=float32), 'validation/wer': 0.13266458769804107, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26202387, dtype=float32), 'test/wer': 0.08662888712855199, 'test/num_examples': 2472, 'score': 47566.9754588604, 'total_duration': 52062.943118810654, 'accumulated_submission_time': 47566.9754588604, 'accumulated_eval_time': 4491.721256494522, 'accumulated_logging_time': 1.7927451133728027}
I0216 23:32:33.236609 139578135525120 logging_writer.py:48] [60165] accumulated_eval_time=4491.721256, accumulated_logging_time=1.792745, accumulated_submission_time=47566.975459, global_step=60165, preemption_count=0, score=47566.975459, test/ctc_loss=0.2620238661766052, test/num_examples=2472, test/wer=0.086629, total_duration=52062.943119, train/ctc_loss=0.18237127363681793, train/wer=0.068188, validation/ctc_loss=0.4402337968349457, validation/num_examples=5348, validation/wer=0.132665
I0216 23:33:00.457926 139578127132416 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.6582401990890503, loss=1.2753795385360718
I0216 23:34:15.965092 139578135525120 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.5806590914726257, loss=1.2005785703659058
I0216 23:35:31.911155 139578127132416 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.728484570980072, loss=1.2862486839294434
I0216 23:36:47.534197 139578135525120 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.70973801612854, loss=1.2011730670928955
I0216 23:38:06.951716 139578127132416 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.7005087733268738, loss=1.2147252559661865
I0216 23:39:29.007220 139578135525120 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.6013136506080627, loss=1.2005470991134644
I0216 23:40:53.074785 139578135525120 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.6838778257369995, loss=1.2040174007415771
I0216 23:42:08.605852 139578127132416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.5960217714309692, loss=1.181425929069519
I0216 23:43:24.251675 139578135525120 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.5365819931030273, loss=1.1566672325134277
I0216 23:44:39.998526 139578127132416 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.6872671246528625, loss=1.2254259586334229
I0216 23:45:55.618283 139578135525120 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.6327037215232849, loss=1.205772042274475
I0216 23:47:11.202339 139578127132416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.7936496734619141, loss=1.1739636659622192
I0216 23:48:33.909871 139578135525120 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.6048668622970581, loss=1.2041211128234863
I0216 23:49:57.291382 139578127132416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.7861206531524658, loss=1.2000629901885986
I0216 23:51:20.846551 139578135525120 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.691268265247345, loss=1.2273495197296143
I0216 23:52:43.754382 139578127132416 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.7730264663696289, loss=1.2110594511032104
I0216 23:54:10.664935 139578135525120 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.6232178807258606, loss=1.1785948276519775
I0216 23:55:26.317447 139578127132416 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.6964097023010254, loss=1.1512001752853394
I0216 23:56:33.203894 139688679413568 spec.py:321] Evaluating on the training split.
I0216 23:57:28.739717 139688679413568 spec.py:333] Evaluating on the validation split.
I0216 23:58:20.420407 139688679413568 spec.py:349] Evaluating on the test split.
I0216 23:58:46.849207 139688679413568 submission_runner.py:408] Time since start: 53636.60s, 	Step: 61990, 	{'train/ctc_loss': Array(0.16777992, dtype=float32), 'train/wer': 0.06161805731676388, 'validation/ctc_loss': Array(0.42649555, dtype=float32), 'validation/wer': 0.13053090937177173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2465062, dtype=float32), 'test/wer': 0.08285093331708407, 'test/num_examples': 2472, 'score': 49006.85757493973, 'total_duration': 53636.60034799576, 'accumulated_submission_time': 49006.85757493973, 'accumulated_eval_time': 4625.361068725586, 'accumulated_logging_time': 1.8530685901641846}
I0216 23:58:46.889535 139578135525120 logging_writer.py:48] [61990] accumulated_eval_time=4625.361069, accumulated_logging_time=1.853069, accumulated_submission_time=49006.857575, global_step=61990, preemption_count=0, score=49006.857575, test/ctc_loss=0.2465061992406845, test/num_examples=2472, test/wer=0.082851, total_duration=53636.600348, train/ctc_loss=0.16777992248535156, train/wer=0.061618, validation/ctc_loss=0.4264955520629883, validation/num_examples=5348, validation/wer=0.130531
I0216 23:58:55.237820 139578127132416 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.6090993285179138, loss=1.1497018337249756
I0217 00:00:10.726231 139578135525120 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.6358119249343872, loss=1.172257423400879
I0217 00:01:26.213769 139578127132416 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.6072890162467957, loss=1.1471202373504639
I0217 00:02:41.781509 139578135525120 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.668140172958374, loss=1.142254114151001
I0217 00:03:57.470561 139578127132416 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.7000225186347961, loss=1.1818066835403442
I0217 00:05:17.210901 139578135525120 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.6491731405258179, loss=1.144344687461853
I0217 00:06:40.755934 139578127132416 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.7613388895988464, loss=1.1521400213241577
I0217 00:08:04.322283 139578135525120 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.8317457437515259, loss=1.1526329517364502
I0217 00:09:27.777980 139578127132416 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.7067693471908569, loss=1.1570948362350464
I0217 00:10:49.206873 139578135525120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.6045711636543274, loss=1.1626838445663452
I0217 00:12:04.735838 139578127132416 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.6620412468910217, loss=1.1639978885650635
I0217 00:13:20.389359 139578135525120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.6867229342460632, loss=1.1602215766906738
I0217 00:14:36.042505 139578127132416 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.6899155378341675, loss=1.0976712703704834
I0217 00:15:51.636613 139578135525120 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.7461027503013611, loss=1.147862434387207
I0217 00:17:15.695644 139578127132416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.9591876864433289, loss=1.1089630126953125
I0217 00:18:38.816663 139578135525120 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.7776757478713989, loss=1.1982839107513428
I0217 00:20:02.590222 139578127132416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.7248619198799133, loss=1.147335171699524
I0217 00:21:27.485299 139578135525120 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.6165832877159119, loss=1.188496470451355
I0217 00:22:47.346634 139688679413568 spec.py:321] Evaluating on the training split.
I0217 00:23:41.586456 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 00:24:33.235288 139688679413568 spec.py:349] Evaluating on the test split.
I0217 00:24:59.332333 139688679413568 submission_runner.py:408] Time since start: 55209.08s, 	Step: 63797, 	{'train/ctc_loss': Array(0.16459136, dtype=float32), 'train/wer': 0.06262819784275842, 'validation/ctc_loss': Array(0.41153353, dtype=float32), 'validation/wer': 0.12463191635208588, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23887035, dtype=float32), 'test/wer': 0.0810838258891394, 'test/num_examples': 2472, 'score': 50447.22711467743, 'total_duration': 55209.0825612545, 'accumulated_submission_time': 50447.22711467743, 'accumulated_eval_time': 4757.340369462967, 'accumulated_logging_time': 1.9127659797668457}
I0217 00:24:59.376790 139578135525120 logging_writer.py:48] [63797] accumulated_eval_time=4757.340369, accumulated_logging_time=1.912766, accumulated_submission_time=50447.227115, global_step=63797, preemption_count=0, score=50447.227115, test/ctc_loss=0.23887035250663757, test/num_examples=2472, test/wer=0.081084, total_duration=55209.082561, train/ctc_loss=0.16459135711193085, train/wer=0.062628, validation/ctc_loss=0.41153353452682495, validation/num_examples=5348, validation/wer=0.124632
I0217 00:25:02.512542 139578127132416 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.6173135638237, loss=1.1280617713928223
I0217 00:26:21.365179 139578135525120 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.7092584371566772, loss=1.1318284273147583
I0217 00:27:37.047306 139578127132416 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.7568588256835938, loss=1.1611062288284302
I0217 00:28:52.381504 139578135525120 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.63423752784729, loss=1.1270039081573486
I0217 00:30:08.327841 139578127132416 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.7442114353179932, loss=1.087044596672058
I0217 00:31:23.813612 139578135525120 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.8172056078910828, loss=1.1345188617706299
I0217 00:32:43.967607 139578127132416 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.7056772112846375, loss=1.133961796760559
I0217 00:34:07.503234 139578135525120 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.6023361086845398, loss=1.1672606468200684
I0217 00:35:30.792769 139578127132416 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.7120663523674011, loss=1.176330327987671
I0217 00:36:54.686823 139578135525120 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.7543056011199951, loss=1.1765483617782593
I0217 00:38:18.206991 139578127132416 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.6850058436393738, loss=1.1586501598358154
I0217 00:39:43.190174 139578135525120 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.6538276672363281, loss=1.136771559715271
I0217 00:40:58.718766 139578127132416 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.7676572799682617, loss=1.1260566711425781
I0217 00:42:14.555709 139578135525120 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.6982564926147461, loss=1.1258410215377808
I0217 00:43:30.187721 139578127132416 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.6601723432540894, loss=1.0707168579101562
I0217 00:44:45.899996 139578135525120 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.6995899677276611, loss=1.165774941444397
I0217 00:46:01.601515 139578127132416 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.7542378306388855, loss=1.114202857017517
I0217 00:47:23.221073 139578135525120 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.7302373647689819, loss=1.1260608434677124
I0217 00:48:45.999688 139578127132416 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.6988202333450317, loss=1.1135348081588745
I0217 00:48:59.337585 139688679413568 spec.py:321] Evaluating on the training split.
I0217 00:49:53.886668 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 00:50:45.747285 139688679413568 spec.py:349] Evaluating on the test split.
I0217 00:51:11.849207 139688679413568 submission_runner.py:408] Time since start: 56781.60s, 	Step: 65617, 	{'train/ctc_loss': Array(0.14450066, dtype=float32), 'train/wer': 0.05295414225041664, 'validation/ctc_loss': Array(0.3941793, dtype=float32), 'validation/wer': 0.11850121165895904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22475353, dtype=float32), 'test/wer': 0.07551845307009526, 'test/num_examples': 2472, 'score': 51887.10120534897, 'total_duration': 56781.600541591644, 'accumulated_submission_time': 51887.10120534897, 'accumulated_eval_time': 4889.8466360569, 'accumulated_logging_time': 1.9750051498413086}
I0217 00:51:11.892779 139578135525120 logging_writer.py:48] [65617] accumulated_eval_time=4889.846636, accumulated_logging_time=1.975005, accumulated_submission_time=51887.101205, global_step=65617, preemption_count=0, score=51887.101205, test/ctc_loss=0.22475352883338928, test/num_examples=2472, test/wer=0.075518, total_duration=56781.600542, train/ctc_loss=0.14450065791606903, train/wer=0.052954, validation/ctc_loss=0.3941793143749237, validation/num_examples=5348, validation/wer=0.118501
I0217 00:52:15.207378 139578127132416 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.639357328414917, loss=1.1273512840270996
I0217 00:53:30.753483 139578135525120 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.7099270224571228, loss=1.0965521335601807
I0217 00:54:46.267221 139578127132416 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.6905478835105896, loss=1.1074938774108887
I0217 00:56:05.206977 139578135525120 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.6673728823661804, loss=1.1073254346847534
I0217 00:57:20.674044 139578127132416 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.7314462065696716, loss=1.1133633852005005
I0217 00:58:36.138433 139578135525120 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.7616949081420898, loss=1.1520414352416992
I0217 00:59:51.914046 139578127132416 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.8181207776069641, loss=1.128516674041748
I0217 01:01:07.413126 139578135525120 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.8136509656906128, loss=1.101357340812683
I0217 01:02:29.065218 139578127132416 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.8637109398841858, loss=1.07529616355896
I0217 01:03:52.554444 139578135525120 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.7440340518951416, loss=1.0901926755905151
I0217 01:05:15.918239 139578127132416 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.7726772427558899, loss=1.0975874662399292
I0217 01:06:39.777723 139578135525120 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.7573274374008179, loss=1.138883352279663
I0217 01:08:03.529857 139578127132416 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.7995585799217224, loss=1.112475872039795
I0217 01:09:25.914925 139578135525120 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.8011154532432556, loss=1.1084446907043457
I0217 01:10:41.402019 139578127132416 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.957662045955658, loss=1.0945782661437988
I0217 01:11:56.886471 139578135525120 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.7028526663780212, loss=1.110193133354187
I0217 01:13:12.379189 139578127132416 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.7104508280754089, loss=1.0656794309616089
I0217 01:14:27.914196 139578135525120 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.8265012502670288, loss=1.1270843744277954
I0217 01:15:12.319592 139688679413568 spec.py:321] Evaluating on the training split.
I0217 01:16:07.602287 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 01:16:58.722185 139688679413568 spec.py:349] Evaluating on the test split.
I0217 01:17:24.887641 139688679413568 submission_runner.py:408] Time since start: 58354.64s, 	Step: 67460, 	{'train/ctc_loss': Array(0.14300667, dtype=float32), 'train/wer': 0.054331527332953676, 'validation/ctc_loss': Array(0.38965482, dtype=float32), 'validation/wer': 0.11681164737345164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21985038, dtype=float32), 'test/wer': 0.07267483192167855, 'test/num_examples': 2472, 'score': 53327.44103908539, 'total_duration': 58354.63788485527, 'accumulated_submission_time': 53327.44103908539, 'accumulated_eval_time': 5022.4082589149475, 'accumulated_logging_time': 2.0354931354522705}
I0217 01:17:24.941622 139578135525120 logging_writer.py:48] [67460] accumulated_eval_time=5022.408259, accumulated_logging_time=2.035493, accumulated_submission_time=53327.441039, global_step=67460, preemption_count=0, score=53327.441039, test/ctc_loss=0.21985037624835968, test/num_examples=2472, test/wer=0.072675, total_duration=58354.637885, train/ctc_loss=0.14300666749477386, train/wer=0.054332, validation/ctc_loss=0.38965481519699097, validation/num_examples=5348, validation/wer=0.116812
I0217 01:17:55.935922 139578127132416 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.7315819263458252, loss=1.117443323135376
I0217 01:19:11.824181 139578135525120 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.7402415871620178, loss=1.0900646448135376
I0217 01:20:27.645799 139578127132416 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.8044973015785217, loss=1.0917314291000366
I0217 01:21:43.491692 139578135525120 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.7442979216575623, loss=1.1427421569824219
I0217 01:23:01.781919 139578127132416 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.7348203659057617, loss=1.0833863019943237
I0217 01:24:27.265584 139578135525120 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.8029623031616211, loss=1.0307180881500244
I0217 01:25:42.970048 139578127132416 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.7780841588973999, loss=1.098676323890686
I0217 01:26:58.410709 139578135525120 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.9871705174446106, loss=1.0699505805969238
I0217 01:28:13.856856 139578127132416 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.7239964604377747, loss=1.0653716325759888
I0217 01:29:29.409136 139578135525120 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.7594760656356812, loss=1.040100336074829
I0217 01:30:45.005736 139578127132416 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.7308200001716614, loss=1.0209628343582153
I0217 01:32:08.819412 139578135525120 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.6597005128860474, loss=1.042049765586853
I0217 01:33:32.303912 139578127132416 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.7426170110702515, loss=1.0517085790634155
I0217 01:34:56.902307 139578135525120 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.7862021923065186, loss=1.0544577836990356
I0217 01:36:19.436781 139578127132416 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.7393884062767029, loss=1.0542758703231812
I0217 01:37:41.992974 139578135525120 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.8115779161453247, loss=1.0749578475952148
I0217 01:39:01.270145 139578135525120 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.7956187129020691, loss=1.0836156606674194
I0217 01:40:16.885024 139578127132416 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.7734958529472351, loss=1.0528122186660767
I0217 01:41:25.317248 139688679413568 spec.py:321] Evaluating on the training split.
I0217 01:42:19.332669 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 01:43:10.742568 139688679413568 spec.py:349] Evaluating on the test split.
I0217 01:43:36.700037 139688679413568 submission_runner.py:408] Time since start: 59926.45s, 	Step: 69292, 	{'train/ctc_loss': Array(0.15330945, dtype=float32), 'train/wer': 0.05312893297938722, 'validation/ctc_loss': Array(0.3706188, dtype=float32), 'validation/wer': 0.11068094268032479, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20976654, dtype=float32), 'test/wer': 0.06954684865842016, 'test/num_examples': 2472, 'score': 54767.72738194466, 'total_duration': 59926.451124191284, 'accumulated_submission_time': 54767.72738194466, 'accumulated_eval_time': 5153.785512447357, 'accumulated_logging_time': 2.1095762252807617}
I0217 01:43:36.744204 139578135525120 logging_writer.py:48] [69292] accumulated_eval_time=5153.785512, accumulated_logging_time=2.109576, accumulated_submission_time=54767.727382, global_step=69292, preemption_count=0, score=54767.727382, test/ctc_loss=0.20976653695106506, test/num_examples=2472, test/wer=0.069547, total_duration=59926.451124, train/ctc_loss=0.15330944955348969, train/wer=0.053129, validation/ctc_loss=0.3706187903881073, validation/num_examples=5348, validation/wer=0.110681
I0217 01:43:43.622038 139578127132416 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.683197021484375, loss=1.0957846641540527
I0217 01:44:59.177244 139578135525120 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.8663153648376465, loss=1.0599173307418823
I0217 01:46:14.630767 139578127132416 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.7712868452072144, loss=1.029889702796936
I0217 01:47:30.280568 139578135525120 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.7882758975028992, loss=1.0611668825149536
I0217 01:48:45.882459 139578127132416 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.8977627158164978, loss=1.037476897239685
I0217 01:50:05.811847 139578135525120 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.7480784058570862, loss=1.0142518281936646
I0217 01:51:28.200313 139578127132416 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.9024469256401062, loss=1.0310860872268677
I0217 01:52:51.194706 139578135525120 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.7268849015235901, loss=1.0653293132781982
I0217 01:54:12.563170 139578135525120 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.9410686492919922, loss=1.0381540060043335
I0217 01:55:28.103976 139578127132416 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.6719468235969543, loss=1.0047674179077148
I0217 01:56:43.478767 139578135525120 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.708000898361206, loss=0.9951775074005127
I0217 01:57:58.992745 139578127132416 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.9824987649917603, loss=1.0044976472854614
I0217 01:59:14.618756 139578135525120 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.7580349445343018, loss=1.0022927522659302
I0217 02:00:32.799170 139578127132416 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.003631353378296, loss=1.0354042053222656
I0217 02:01:55.733735 139578135525120 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.7693166136741638, loss=1.0782041549682617
I0217 02:03:19.332024 139578127132416 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.760999321937561, loss=1.0799360275268555
I0217 02:04:42.316123 139578135525120 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.719631552696228, loss=1.0629996061325073
I0217 02:06:05.975633 139578127132416 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.9355573654174805, loss=1.0694674253463745
I0217 02:07:29.974877 139578135525120 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.3394235372543335, loss=1.0497632026672363
I0217 02:07:37.220775 139688679413568 spec.py:321] Evaluating on the training split.
I0217 02:08:32.797571 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 02:09:24.238787 139688679413568 spec.py:349] Evaluating on the test split.
I0217 02:09:51.055057 139688679413568 submission_runner.py:408] Time since start: 61500.81s, 	Step: 71111, 	{'train/ctc_loss': Array(0.10428537, dtype=float32), 'train/wer': 0.03912346944557485, 'validation/ctc_loss': Array(0.36181018, dtype=float32), 'validation/wer': 0.10780385606843218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2021894, dtype=float32), 'test/wer': 0.06700790120447667, 'test/num_examples': 2472, 'score': 56208.11917066574, 'total_duration': 61500.805604457855, 'accumulated_submission_time': 56208.11917066574, 'accumulated_eval_time': 5287.613696575165, 'accumulated_logging_time': 2.1699845790863037}
I0217 02:09:51.101619 139578135525120 logging_writer.py:48] [71111] accumulated_eval_time=5287.613697, accumulated_logging_time=2.169985, accumulated_submission_time=56208.119171, global_step=71111, preemption_count=0, score=56208.119171, test/ctc_loss=0.2021894007921219, test/num_examples=2472, test/wer=0.067008, total_duration=61500.805604, train/ctc_loss=0.10428537428379059, train/wer=0.039123, validation/ctc_loss=0.36181017756462097, validation/num_examples=5348, validation/wer=0.107804
I0217 02:10:58.891002 139578127132416 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.7411040663719177, loss=1.0880354642868042
I0217 02:12:14.431348 139578135525120 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.8416504859924316, loss=0.9993875622749329
I0217 02:13:29.877694 139578127132416 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.078179121017456, loss=1.0363173484802246
I0217 02:14:45.485399 139578135525120 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2089200019836426, loss=1.0274803638458252
I0217 02:16:01.159382 139578127132416 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.7721624374389648, loss=0.9710913300514221
I0217 02:17:16.718548 139578135525120 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.7282131910324097, loss=1.0128668546676636
I0217 02:18:37.238844 139578127132416 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.8818753361701965, loss=0.9794723987579346
I0217 02:19:59.586655 139578135525120 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.727707028388977, loss=0.9846788048744202
I0217 02:21:22.133097 139578127132416 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.7170913219451904, loss=1.0888571739196777
I0217 02:22:48.433607 139578135525120 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.8427116870880127, loss=1.0180436372756958
I0217 02:24:04.286931 139578127132416 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.7403945922851562, loss=1.0419414043426514
I0217 02:25:20.136748 139578135525120 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.911335289478302, loss=1.0236164331436157
I0217 02:26:35.828150 139578127132416 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.7569746971130371, loss=0.9809884428977966
I0217 02:27:51.403919 139578135525120 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.7749072313308716, loss=1.0042625665664673
I0217 02:29:06.968893 139578127132416 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.7159640192985535, loss=1.02142333984375
I0217 02:30:27.353979 139578135525120 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.7946299314498901, loss=1.0044783353805542
I0217 02:31:50.812603 139578127132416 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.6904888153076172, loss=1.031417727470398
I0217 02:33:12.941516 139578135525120 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.8648653030395508, loss=1.052093505859375
I0217 02:33:51.533110 139688679413568 spec.py:321] Evaluating on the training split.
I0217 02:34:48.248840 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 02:35:40.648818 139688679413568 spec.py:349] Evaluating on the test split.
I0217 02:36:06.750059 139688679413568 submission_runner.py:408] Time since start: 63076.50s, 	Step: 72948, 	{'train/ctc_loss': Array(0.10251857, dtype=float32), 'train/wer': 0.03766584625353379, 'validation/ctc_loss': Array(0.34724754, dtype=float32), 'validation/wer': 0.10400957741583557, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19403726, dtype=float32), 'test/wer': 0.06556577905063676, 'test/num_examples': 2472, 'score': 57648.46272945404, 'total_duration': 63076.50105428696, 'accumulated_submission_time': 57648.46272945404, 'accumulated_eval_time': 5422.824957847595, 'accumulated_logging_time': 2.234661340713501}
I0217 02:36:06.795536 139578135525120 logging_writer.py:48] [72948] accumulated_eval_time=5422.824958, accumulated_logging_time=2.234661, accumulated_submission_time=57648.462729, global_step=72948, preemption_count=0, score=57648.462729, test/ctc_loss=0.19403725862503052, test/num_examples=2472, test/wer=0.065566, total_duration=63076.501054, train/ctc_loss=0.10251856595277786, train/wer=0.037666, validation/ctc_loss=0.34724754095077515, validation/num_examples=5348, validation/wer=0.104010
I0217 02:36:46.740930 139578127132416 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.7641886472702026, loss=0.981669545173645
I0217 02:38:02.366932 139578135525120 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.8349472284317017, loss=0.9622831344604492
I0217 02:39:21.339990 139578135525120 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.7823725342750549, loss=1.0327675342559814
I0217 02:40:37.340238 139578127132416 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.812296450138092, loss=0.9983842372894287
I0217 02:41:53.055516 139578135525120 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.7170281410217285, loss=0.952210009098053
I0217 02:43:08.776383 139578127132416 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.806386411190033, loss=1.0095738172531128
I0217 02:44:24.513243 139578135525120 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.7999925017356873, loss=0.9728885293006897
I0217 02:45:42.204019 139578127132416 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.8141772747039795, loss=0.9742017984390259
I0217 02:47:06.158362 139578135525120 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.7447915077209473, loss=0.9595935940742493
I0217 02:48:29.441835 139578127132416 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.8326202034950256, loss=0.9963786005973816
I0217 02:49:52.546420 139578135525120 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.8478107452392578, loss=1.0139065980911255
I0217 02:51:16.311114 139578127132416 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.8844851851463318, loss=1.02671217918396
I0217 02:52:39.446257 139578135525120 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.7113835215568542, loss=0.9951885938644409
I0217 02:53:55.238328 139578127132416 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.7175496816635132, loss=0.9408626556396484
I0217 02:55:11.114047 139578135525120 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.8188502788543701, loss=0.9328069686889648
I0217 02:56:27.175426 139578127132416 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.8681963086128235, loss=1.0181841850280762
I0217 02:57:42.894327 139578135525120 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.7116163969039917, loss=0.9735840559005737
I0217 02:58:58.643985 139578127132416 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.7670401930809021, loss=1.0458050966262817
I0217 03:00:06.771022 139688679413568 spec.py:321] Evaluating on the training split.
I0217 03:01:01.308503 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 03:01:53.749480 139688679413568 spec.py:349] Evaluating on the test split.
I0217 03:02:19.998223 139688679413568 submission_runner.py:408] Time since start: 64649.75s, 	Step: 74785, 	{'train/ctc_loss': Array(0.12474066, dtype=float32), 'train/wer': 0.04743712542076024, 'validation/ctc_loss': Array(0.34358847, dtype=float32), 'validation/wer': 0.10281240043639032, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18991898, dtype=float32), 'test/wer': 0.06357524424674507, 'test/num_examples': 2472, 'score': 59088.35154867172, 'total_duration': 64649.74883103371, 'accumulated_submission_time': 59088.35154867172, 'accumulated_eval_time': 5556.046092987061, 'accumulated_logging_time': 2.296834707260132}
I0217 03:02:20.043706 139578135525120 logging_writer.py:48] [74785] accumulated_eval_time=5556.046093, accumulated_logging_time=2.296835, accumulated_submission_time=59088.351549, global_step=74785, preemption_count=0, score=59088.351549, test/ctc_loss=0.18991898000240326, test/num_examples=2472, test/wer=0.063575, total_duration=64649.748831, train/ctc_loss=0.12474066019058228, train/wer=0.047437, validation/ctc_loss=0.3435884714126587, validation/num_examples=5348, validation/wer=0.102812
I0217 03:02:32.204860 139578127132416 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.87007737159729, loss=0.9970149397850037
I0217 03:03:47.607657 139578135525120 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.8860276341438293, loss=0.999487042427063
I0217 03:05:03.125507 139578127132416 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.8572869896888733, loss=0.9974543452262878
I0217 03:06:18.713800 139578135525120 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.7553262114524841, loss=1.0071302652359009
I0217 03:07:38.428783 139578135525120 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.7830407619476318, loss=1.052388072013855
I0217 03:08:53.917174 139578127132416 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.8524128794670105, loss=0.9994330406188965
I0217 03:10:09.425841 139578135525120 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.9494779109954834, loss=1.0117552280426025
I0217 03:11:24.993144 139578127132416 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.0912913084030151, loss=0.9986900687217712
I0217 03:12:40.487295 139578135525120 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.1668230295181274, loss=1.0198912620544434
I0217 03:13:56.205741 139578127132416 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.1780937910079956, loss=0.9636843800544739
I0217 03:15:16.992936 139578135525120 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.7782613635063171, loss=1.0064340829849243
I0217 03:16:39.798560 139578127132416 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.8065996170043945, loss=1.0393928289413452
I0217 03:18:02.940459 139578135525120 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.8078252673149109, loss=0.9639727473258972
I0217 03:19:25.888091 139578127132416 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.862385630607605, loss=0.9481706619262695
I0217 03:20:49.503786 139578135525120 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.8972234725952148, loss=0.9976035356521606
I0217 03:22:09.965377 139578135525120 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.80734783411026, loss=0.9760605096817017
I0217 03:23:25.593444 139578127132416 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.9250523447990417, loss=0.9392374753952026
I0217 03:24:41.084664 139578135525120 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.1404030323028564, loss=0.9753232598304749
I0217 03:25:56.627755 139578127132416 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.7544350028038025, loss=1.0031235218048096
I0217 03:26:20.454194 139688679413568 spec.py:321] Evaluating on the training split.
I0217 03:27:14.294159 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 03:28:06.685504 139688679413568 spec.py:349] Evaluating on the test split.
I0217 03:28:32.788125 139688679413568 submission_runner.py:408] Time since start: 66222.54s, 	Step: 76633, 	{'train/ctc_loss': Array(0.12209161, dtype=float32), 'train/wer': 0.04441446214856531, 'validation/ctc_loss': Array(0.33922857, dtype=float32), 'validation/wer': 0.10118076406924317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18748328, dtype=float32), 'test/wer': 0.062112810513273616, 'test/num_examples': 2472, 'score': 60528.67248296738, 'total_duration': 66222.53850531578, 'accumulated_submission_time': 60528.67248296738, 'accumulated_eval_time': 5688.373752355576, 'accumulated_logging_time': 2.361799716949463}
I0217 03:28:32.834739 139578135525120 logging_writer.py:48] [76633] accumulated_eval_time=5688.373752, accumulated_logging_time=2.361800, accumulated_submission_time=60528.672483, global_step=76633, preemption_count=0, score=60528.672483, test/ctc_loss=0.1874832808971405, test/num_examples=2472, test/wer=0.062113, total_duration=66222.538505, train/ctc_loss=0.1220916137099266, train/wer=0.044414, validation/ctc_loss=0.3392285704612732, validation/num_examples=5348, validation/wer=0.101181
I0217 03:29:24.132781 139578127132416 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.8481122255325317, loss=0.9653984904289246
I0217 03:30:39.678634 139578135525120 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.9412932395935059, loss=0.9668369889259338
I0217 03:31:55.653573 139578127132416 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.786648690700531, loss=0.9757044911384583
I0217 03:33:11.372782 139578135525120 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.790086567401886, loss=0.941539466381073
I0217 03:34:30.558284 139578127132416 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.788112461566925, loss=1.001539707183838
I0217 03:35:52.779153 139578135525120 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.909273087978363, loss=0.9738317728042603
I0217 03:37:15.538033 139578135525120 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.8357036709785461, loss=0.9762518405914307
I0217 03:37:32.640468 139578127132416 logging_writer.py:48] [77324] global_step=77324, preemption_count=0, score=61068.414375
I0217 03:37:33.504024 139688679413568 checkpoints.py:490] Saving checkpoint at step: 77324
I0217 03:37:35.106713 139688679413568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_4/checkpoint_77324
I0217 03:37:35.140967 139688679413568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_4/checkpoint_77324.
I0217 03:37:38.691733 139688679413568 submission_runner.py:583] Tuning trial 4/5
I0217 03:37:38.691990 139688679413568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0217 03:37:38.719336 139688679413568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.807152, dtype=float32), 'train/wer': 0.9418356167355484, 'validation/ctc_loss': Array(30.141817, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 35.08036541938782, 'total_duration': 150.66368436813354, 'accumulated_submission_time': 35.08036541938782, 'accumulated_eval_time': 115.58325505256653, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1774, {'train/ctc_loss': Array(3.0434783, dtype=float32), 'train/wer': 0.6561460888525483, 'validation/ctc_loss': Array(2.9196923, dtype=float32), 'validation/wer': 0.6208521196790794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5539382, dtype=float32), 'test/wer': 0.5610870757418804, 'test/num_examples': 2472, 'score': 1474.9875185489655, 'total_duration': 1715.042043685913, 'accumulated_submission_time': 1474.9875185489655, 'accumulated_eval_time': 239.9583158493042, 'accumulated_logging_time': 0.027961015701293945, 'global_step': 1774, 'preemption_count': 0}), (3603, {'train/ctc_loss': Array(1.1530465, dtype=float32), 'train/wer': 0.35323060101821163, 'validation/ctc_loss': Array(1.2137563, dtype=float32), 'validation/wer': 0.34704615889628004, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9011528, dtype=float32), 'test/wer': 0.2833262242804623, 'test/num_examples': 2472, 'score': 2915.676682949066, 'total_duration': 3286.6037788391113, 'accumulated_submission_time': 2915.676682949066, 'accumulated_eval_time': 370.70670461654663, 'accumulated_logging_time': 0.07816886901855469, 'global_step': 3603, 'preemption_count': 0}), (5426, {'train/ctc_loss': Array(1.018678, dtype=float32), 'train/wer': 0.31768502774297475, 'validation/ctc_loss': Array(1.0831897, dtype=float32), 'validation/wer': 0.3157071550633828, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7714357, dtype=float32), 'test/wer': 0.2490605894420409, 'test/num_examples': 2472, 'score': 4356.196928501129, 'total_duration': 4858.320121049881, 'accumulated_submission_time': 4356.196928501129, 'accumulated_eval_time': 501.77516174316406, 'accumulated_logging_time': 0.13093829154968262, 'global_step': 5426, 'preemption_count': 0}), (7221, {'train/ctc_loss': Array(0.879369, dtype=float32), 'train/wer': 0.284893824790215, 'validation/ctc_loss': Array(0.98021287, dtype=float32), 'validation/wer': 0.2919180899234386, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6883703, dtype=float32), 'test/wer': 0.22862714033270368, 'test/num_examples': 2472, 'score': 5796.309594631195, 'total_duration': 6430.357679367065, 'accumulated_submission_time': 5796.309594631195, 'accumulated_eval_time': 633.5774214267731, 'accumulated_logging_time': 0.18009305000305176, 'global_step': 7221, 'preemption_count': 0}), (9054, {'train/ctc_loss': Array(0.9016759, dtype=float32), 'train/wer': 0.29293339521930845, 'validation/ctc_loss': Array(0.95180404, dtype=float32), 'validation/wer': 0.282427565965417, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6508851, dtype=float32), 'test/wer': 0.21485588934251418, 'test/num_examples': 2472, 'score': 7236.248072147369, 'total_duration': 8000.426544189453, 'accumulated_submission_time': 7236.248072147369, 'accumulated_eval_time': 763.5807249546051, 'accumulated_logging_time': 0.23284149169921875, 'global_step': 9054, 'preemption_count': 0}), (10891, {'train/ctc_loss': Array(0.8089695, dtype=float32), 'train/wer': 0.2691599672493282, 'validation/ctc_loss': Array(0.95127535, dtype=float32), 'validation/wer': 0.2845902082508665, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.65417844, dtype=float32), 'test/wer': 0.21981191477261186, 'test/num_examples': 2472, 'score': 8676.789829492569, 'total_duration': 9574.090694189072, 'accumulated_submission_time': 8676.789829492569, 'accumulated_eval_time': 896.5786190032959, 'accumulated_logging_time': 0.2828640937805176, 'global_step': 10891, 'preemption_count': 0}), (12730, {'train/ctc_loss': Array(0.8335272, dtype=float32), 'train/wer': 0.2669985010888317, 'validation/ctc_loss': Array(0.91189045, dtype=float32), 'validation/wer': 0.2689013970282978, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6159002, dtype=float32), 'test/wer': 0.20378607844332053, 'test/num_examples': 2472, 'score': 10117.304208755493, 'total_duration': 11145.531847715378, 'accumulated_submission_time': 10117.304208755493, 'accumulated_eval_time': 1027.3806738853455, 'accumulated_logging_time': 0.3326706886291504, 'global_step': 12730, 'preemption_count': 0}), (14542, {'train/ctc_loss': Array(0.81453586, dtype=float32), 'train/wer': 0.2609976846979583, 'validation/ctc_loss': Array(0.8834435, dtype=float32), 'validation/wer': 0.26353340992691426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6003142, dtype=float32), 'test/wer': 0.19864724879653892, 'test/num_examples': 2472, 'score': 11557.22633099556, 'total_duration': 12716.850360155106, 'accumulated_submission_time': 11557.22633099556, 'accumulated_eval_time': 1158.6492466926575, 'accumulated_logging_time': 0.3859102725982666, 'global_step': 14542, 'preemption_count': 0}), (16361, {'train/ctc_loss': Array(0.7476235, dtype=float32), 'train/wer': 0.24288179939316654, 'validation/ctc_loss': Array(0.8414512, dtype=float32), 'validation/wer': 0.250461009683617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5641267, dtype=float32), 'test/wer': 0.18853208214002803, 'test/num_examples': 2472, 'score': 12997.759685993195, 'total_duration': 14289.888085842133, 'accumulated_submission_time': 12997.759685993195, 'accumulated_eval_time': 1291.026449918747, 'accumulated_logging_time': 0.43868160247802734, 'global_step': 16361, 'preemption_count': 0}), (18186, {'train/ctc_loss': Array(0.6964414, dtype=float32), 'train/wer': 0.22979803678212796, 'validation/ctc_loss': Array(0.83934134, dtype=float32), 'validation/wer': 0.24964036417351343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5645912, dtype=float32), 'test/wer': 0.18782117685292385, 'test/num_examples': 2472, 'score': 14437.954718589783, 'total_duration': 15863.708154201508, 'accumulated_submission_time': 14437.954718589783, 'accumulated_eval_time': 1424.5227282047272, 'accumulated_logging_time': 0.4945368766784668, 'global_step': 18186, 'preemption_count': 0}), (20022, {'train/ctc_loss': Array(0.6533358, dtype=float32), 'train/wer': 0.21529764532533727, 'validation/ctc_loss': Array(0.8104945, dtype=float32), 'validation/wer': 0.24206146152138022, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54033, dtype=float32), 'test/wer': 0.17943249446509454, 'test/num_examples': 2472, 'score': 15878.121164798737, 'total_duration': 17434.440548181534, 'accumulated_submission_time': 15878.121164798737, 'accumulated_eval_time': 1554.9639530181885, 'accumulated_logging_time': 0.5451233386993408, 'global_step': 20022, 'preemption_count': 0}), (21846, {'train/ctc_loss': Array(0.7111166, dtype=float32), 'train/wer': 0.23846792097414393, 'validation/ctc_loss': Array(0.8012005, dtype=float32), 'validation/wer': 0.23961883429718953, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52948534, dtype=float32), 'test/wer': 0.1805090081855666, 'test/num_examples': 2472, 'score': 17318.338774442673, 'total_duration': 19006.164157390594, 'accumulated_submission_time': 17318.338774442673, 'accumulated_eval_time': 1686.3452100753784, 'accumulated_logging_time': 0.5957059860229492, 'global_step': 21846, 'preemption_count': 0}), (23656, {'train/ctc_loss': Array(0.6636916, dtype=float32), 'train/wer': 0.21857476503706377, 'validation/ctc_loss': Array(0.7818518, dtype=float32), 'validation/wer': 0.2352549311140504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5160569, dtype=float32), 'test/wer': 0.17411086060162898, 'test/num_examples': 2472, 'score': 18758.5142223835, 'total_duration': 20579.324452877045, 'accumulated_submission_time': 18758.5142223835, 'accumulated_eval_time': 1819.2048013210297, 'accumulated_logging_time': 0.6472318172454834, 'global_step': 23656, 'preemption_count': 0}), (25469, {'train/ctc_loss': Array(0.6760904, dtype=float32), 'train/wer': 0.21828787155469956, 'validation/ctc_loss': Array(0.7688105, dtype=float32), 'validation/wer': 0.22999314519632738, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50244045, dtype=float32), 'test/wer': 0.16795645197326997, 'test/num_examples': 2472, 'score': 20199.02674293518, 'total_duration': 22152.642586946487, 'accumulated_submission_time': 20199.02674293518, 'accumulated_eval_time': 1951.8814685344696, 'accumulated_logging_time': 0.7016665935516357, 'global_step': 25469, 'preemption_count': 0}), (27307, {'train/ctc_loss': Array(0.5880113, dtype=float32), 'train/wer': 0.1996346712314533, 'validation/ctc_loss': Array(0.7525327, dtype=float32), 'validation/wer': 0.22395898703380093, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49013776, dtype=float32), 'test/wer': 0.16436130237848598, 'test/num_examples': 2472, 'score': 21639.47647380829, 'total_duration': 23726.517678260803, 'accumulated_submission_time': 21639.47647380829, 'accumulated_eval_time': 2085.1744623184204, 'accumulated_logging_time': 0.7583460807800293, 'global_step': 27307, 'preemption_count': 0}), (29139, {'train/ctc_loss': Array(0.5951339, dtype=float32), 'train/wer': 0.20340781530110189, 'validation/ctc_loss': Array(0.74104714, dtype=float32), 'validation/wer': 0.22305144964615697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47743583, dtype=float32), 'test/wer': 0.1630613612820669, 'test/num_examples': 2472, 'score': 23079.745376586914, 'total_duration': 25297.29909467697, 'accumulated_submission_time': 23079.745376586914, 'accumulated_eval_time': 2215.55966258049, 'accumulated_logging_time': 0.8103816509246826, 'global_step': 29139, 'preemption_count': 0}), (30948, {'train/ctc_loss': Array(0.5952117, dtype=float32), 'train/wer': 0.20037932075385728, 'validation/ctc_loss': Array(0.70316494, dtype=float32), 'validation/wer': 0.21365747221873582, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46406707, dtype=float32), 'test/wer': 0.15609448946844595, 'test/num_examples': 2472, 'score': 24520.142600536346, 'total_duration': 26867.569379091263, 'accumulated_submission_time': 24520.142600536346, 'accumulated_eval_time': 2345.2983088493347, 'accumulated_logging_time': 0.8707478046417236, 'global_step': 30948, 'preemption_count': 0}), (32794, {'train/ctc_loss': Array(0.42188603, dtype=float32), 'train/wer': 0.14991927881854528, 'validation/ctc_loss': Array(0.6963656, dtype=float32), 'validation/wer': 0.20946735279067746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45321754, dtype=float32), 'test/wer': 0.1542461357219751, 'test/num_examples': 2472, 'score': 25960.58037161827, 'total_duration': 28449.918375730515, 'accumulated_submission_time': 25960.58037161827, 'accumulated_eval_time': 2487.0760929584503, 'accumulated_logging_time': 0.9291160106658936, 'global_step': 32794, 'preemption_count': 0}), (34626, {'train/ctc_loss': Array(0.3637514, dtype=float32), 'train/wer': 0.1303431418243586, 'validation/ctc_loss': Array(0.67736506, dtype=float32), 'validation/wer': 0.20642613707676416, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42954934, dtype=float32), 'test/wer': 0.14575589543598805, 'test/num_examples': 2472, 'score': 27400.81459593773, 'total_duration': 30025.020079612732, 'accumulated_submission_time': 27400.81459593773, 'accumulated_eval_time': 2621.817850112915, 'accumulated_logging_time': 0.9812886714935303, 'global_step': 34626, 'preemption_count': 0}), (36470, {'train/ctc_loss': Array(0.3567816, dtype=float32), 'train/wer': 0.12728226377488133, 'validation/ctc_loss': Array(0.66085625, dtype=float32), 'validation/wer': 0.20038232426117766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41625735, dtype=float32), 'test/wer': 0.1412467247577844, 'test/num_examples': 2472, 'score': 28840.77037382126, 'total_duration': 31597.29881477356, 'accumulated_submission_time': 28840.77037382126, 'accumulated_eval_time': 2754.01162815094, 'accumulated_logging_time': 1.035853385925293, 'global_step': 36470, 'preemption_count': 0}), (38286, {'train/ctc_loss': Array(0.3339406, dtype=float32), 'train/wer': 0.12178276727282134, 'validation/ctc_loss': Array(0.637265, dtype=float32), 'validation/wer': 0.19318960773144617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40459725, dtype=float32), 'test/wer': 0.1378546909593159, 'test/num_examples': 2472, 'score': 30281.36931490898, 'total_duration': 33170.72305393219, 'accumulated_submission_time': 30281.36931490898, 'accumulated_eval_time': 2886.7089653015137, 'accumulated_logging_time': 1.089693307876587, 'global_step': 38286, 'preemption_count': 0}), (40096, {'train/ctc_loss': Array(0.3399412, dtype=float32), 'train/wer': 0.12416167131580509, 'validation/ctc_loss': Array(0.62865573, dtype=float32), 'validation/wer': 0.19244619944582292, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39331612, dtype=float32), 'test/wer': 0.13308146974590213, 'test/num_examples': 2472, 'score': 31721.717066049576, 'total_duration': 34746.4487221241, 'accumulated_submission_time': 31721.717066049576, 'accumulated_eval_time': 3021.955439567566, 'accumulated_logging_time': 1.148374080657959, 'global_step': 40096, 'preemption_count': 0}), (41912, {'train/ctc_loss': Array(0.3111691, dtype=float32), 'train/wer': 0.11212839858547488, 'validation/ctc_loss': Array(0.60674435, dtype=float32), 'validation/wer': 0.18461627581412862, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3796361, dtype=float32), 'test/wer': 0.1285316759084354, 'test/num_examples': 2472, 'score': 33162.12484502792, 'total_duration': 36322.11321473122, 'accumulated_submission_time': 33162.12484502792, 'accumulated_eval_time': 3157.081080198288, 'accumulated_logging_time': 1.2041044235229492, 'global_step': 41912, 'preemption_count': 0}), (43754, {'train/ctc_loss': Array(0.3599174, dtype=float32), 'train/wer': 0.12500478243997354, 'validation/ctc_loss': Array(0.59408325, dtype=float32), 'validation/wer': 0.1794317271208859, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36802, dtype=float32), 'test/wer': 0.12361627363760079, 'test/num_examples': 2472, 'score': 34602.23737287521, 'total_duration': 37896.17702317238, 'accumulated_submission_time': 34602.23737287521, 'accumulated_eval_time': 3290.895377635956, 'accumulated_logging_time': 1.264575481414795, 'global_step': 43754, 'preemption_count': 0}), (45578, {'train/ctc_loss': Array(0.29631695, dtype=float32), 'train/wer': 0.10954912157708044, 'validation/ctc_loss': Array(0.5799549, dtype=float32), 'validation/wer': 0.17433407030518358, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35863748, dtype=float32), 'test/wer': 0.12239757885970792, 'test/num_examples': 2472, 'score': 36042.2100276947, 'total_duration': 39469.1763818264, 'accumulated_submission_time': 36042.2100276947, 'accumulated_eval_time': 3423.7875752449036, 'accumulated_logging_time': 1.323909044265747, 'global_step': 45578, 'preemption_count': 0}), (47381, {'train/ctc_loss': Array(0.28037357, dtype=float32), 'train/wer': 0.10386011495524543, 'validation/ctc_loss': Array(0.56814826, dtype=float32), 'validation/wer': 0.17082943124438824, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3452232, dtype=float32), 'test/wer': 0.1170962565758739, 'test/num_examples': 2472, 'score': 37485.10423183441, 'total_duration': 41045.56417584419, 'accumulated_submission_time': 37485.10423183441, 'accumulated_eval_time': 3557.1513023376465, 'accumulated_logging_time': 1.3801522254943848, 'global_step': 47381, 'preemption_count': 0}), (49209, {'train/ctc_loss': Array(0.2589628, dtype=float32), 'train/wer': 0.09479179338541363, 'validation/ctc_loss': Array(0.55032986, dtype=float32), 'validation/wer': 0.16804889116309604, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33619353, dtype=float32), 'test/wer': 0.11274957853472264, 'test/num_examples': 2472, 'score': 38925.508009433746, 'total_duration': 42621.10450196266, 'accumulated_submission_time': 38925.508009433746, 'accumulated_eval_time': 3692.1489946842194, 'accumulated_logging_time': 1.4441523551940918, 'global_step': 49209, 'preemption_count': 0}), (51041, {'train/ctc_loss': Array(0.24425602, dtype=float32), 'train/wer': 0.09251374987281033, 'validation/ctc_loss': Array(0.53289413, dtype=float32), 'validation/wer': 0.16152234569450746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32483763, dtype=float32), 'test/wer': 0.10919505209920176, 'test/num_examples': 2472, 'score': 40365.552735090256, 'total_duration': 44194.29576420784, 'accumulated_submission_time': 40365.552735090256, 'accumulated_eval_time': 3825.167426109314, 'accumulated_logging_time': 1.4977757930755615, 'global_step': 51041, 'preemption_count': 0}), (52875, {'train/ctc_loss': Array(0.25342596, dtype=float32), 'train/wer': 0.09347378746361591, 'validation/ctc_loss': Array(0.51060355, dtype=float32), 'validation/wer': 0.15436824777701613, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31024233, dtype=float32), 'test/wer': 0.10484837405805049, 'test/num_examples': 2472, 'score': 41805.859117269516, 'total_duration': 45769.149518728256, 'accumulated_submission_time': 41805.859117269516, 'accumulated_eval_time': 3959.5775384902954, 'accumulated_logging_time': 1.5582191944122314, 'global_step': 52875, 'preemption_count': 0}), (54678, {'train/ctc_loss': Array(0.23427062, dtype=float32), 'train/wer': 0.08568783792312709, 'validation/ctc_loss': Array(0.4877075, dtype=float32), 'validation/wer': 0.148855440879732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29198828, dtype=float32), 'test/wer': 0.09842991489448134, 'test/num_examples': 2472, 'score': 43246.37415957451, 'total_duration': 47342.83117246628, 'accumulated_submission_time': 43246.37415957451, 'accumulated_eval_time': 4092.6102225780487, 'accumulated_logging_time': 1.617872714996338, 'global_step': 54678, 'preemption_count': 0}), (56498, {'train/ctc_loss': Array(0.22292952, dtype=float32), 'train/wer': 0.08287230030942161, 'validation/ctc_loss': Array(0.46993592, dtype=float32), 'validation/wer': 0.143149540921247, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28381628, dtype=float32), 'test/wer': 0.09558629374606463, 'test/num_examples': 2472, 'score': 44686.488366127014, 'total_duration': 48917.04101586342, 'accumulated_submission_time': 44686.488366127014, 'accumulated_eval_time': 4226.572660923004, 'accumulated_logging_time': 1.6772680282592773, 'global_step': 56498, 'preemption_count': 0}), (58324, {'train/ctc_loss': Array(0.2015512, dtype=float32), 'train/wer': 0.07597622419593453, 'validation/ctc_loss': Array(0.45896965, dtype=float32), 'validation/wer': 0.1386118539830271, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27153856, dtype=float32), 'test/wer': 0.09111774622712408, 'test/num_examples': 2472, 'score': 46126.97168445587, 'total_duration': 50490.77713441849, 'accumulated_submission_time': 46126.97168445587, 'accumulated_eval_time': 4359.694350004196, 'accumulated_logging_time': 1.7328250408172607, 'global_step': 58324, 'preemption_count': 0}), (60165, {'train/ctc_loss': Array(0.18237127, dtype=float32), 'train/wer': 0.06818781602643274, 'validation/ctc_loss': Array(0.4402338, dtype=float32), 'validation/wer': 0.13266458769804107, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26202387, dtype=float32), 'test/wer': 0.08662888712855199, 'test/num_examples': 2472, 'score': 47566.9754588604, 'total_duration': 52062.943118810654, 'accumulated_submission_time': 47566.9754588604, 'accumulated_eval_time': 4491.721256494522, 'accumulated_logging_time': 1.7927451133728027, 'global_step': 60165, 'preemption_count': 0}), (61990, {'train/ctc_loss': Array(0.16777992, dtype=float32), 'train/wer': 0.06161805731676388, 'validation/ctc_loss': Array(0.42649555, dtype=float32), 'validation/wer': 0.13053090937177173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2465062, dtype=float32), 'test/wer': 0.08285093331708407, 'test/num_examples': 2472, 'score': 49006.85757493973, 'total_duration': 53636.60034799576, 'accumulated_submission_time': 49006.85757493973, 'accumulated_eval_time': 4625.361068725586, 'accumulated_logging_time': 1.8530685901641846, 'global_step': 61990, 'preemption_count': 0}), (63797, {'train/ctc_loss': Array(0.16459136, dtype=float32), 'train/wer': 0.06262819784275842, 'validation/ctc_loss': Array(0.41153353, dtype=float32), 'validation/wer': 0.12463191635208588, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23887035, dtype=float32), 'test/wer': 0.0810838258891394, 'test/num_examples': 2472, 'score': 50447.22711467743, 'total_duration': 55209.0825612545, 'accumulated_submission_time': 50447.22711467743, 'accumulated_eval_time': 4757.340369462967, 'accumulated_logging_time': 1.9127659797668457, 'global_step': 63797, 'preemption_count': 0}), (65617, {'train/ctc_loss': Array(0.14450066, dtype=float32), 'train/wer': 0.05295414225041664, 'validation/ctc_loss': Array(0.3941793, dtype=float32), 'validation/wer': 0.11850121165895904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22475353, dtype=float32), 'test/wer': 0.07551845307009526, 'test/num_examples': 2472, 'score': 51887.10120534897, 'total_duration': 56781.600541591644, 'accumulated_submission_time': 51887.10120534897, 'accumulated_eval_time': 4889.8466360569, 'accumulated_logging_time': 1.9750051498413086, 'global_step': 65617, 'preemption_count': 0}), (67460, {'train/ctc_loss': Array(0.14300667, dtype=float32), 'train/wer': 0.054331527332953676, 'validation/ctc_loss': Array(0.38965482, dtype=float32), 'validation/wer': 0.11681164737345164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21985038, dtype=float32), 'test/wer': 0.07267483192167855, 'test/num_examples': 2472, 'score': 53327.44103908539, 'total_duration': 58354.63788485527, 'accumulated_submission_time': 53327.44103908539, 'accumulated_eval_time': 5022.4082589149475, 'accumulated_logging_time': 2.0354931354522705, 'global_step': 67460, 'preemption_count': 0}), (69292, {'train/ctc_loss': Array(0.15330945, dtype=float32), 'train/wer': 0.05312893297938722, 'validation/ctc_loss': Array(0.3706188, dtype=float32), 'validation/wer': 0.11068094268032479, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20976654, dtype=float32), 'test/wer': 0.06954684865842016, 'test/num_examples': 2472, 'score': 54767.72738194466, 'total_duration': 59926.451124191284, 'accumulated_submission_time': 54767.72738194466, 'accumulated_eval_time': 5153.785512447357, 'accumulated_logging_time': 2.1095762252807617, 'global_step': 69292, 'preemption_count': 0}), (71111, {'train/ctc_loss': Array(0.10428537, dtype=float32), 'train/wer': 0.03912346944557485, 'validation/ctc_loss': Array(0.36181018, dtype=float32), 'validation/wer': 0.10780385606843218, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2021894, dtype=float32), 'test/wer': 0.06700790120447667, 'test/num_examples': 2472, 'score': 56208.11917066574, 'total_duration': 61500.805604457855, 'accumulated_submission_time': 56208.11917066574, 'accumulated_eval_time': 5287.613696575165, 'accumulated_logging_time': 2.1699845790863037, 'global_step': 71111, 'preemption_count': 0}), (72948, {'train/ctc_loss': Array(0.10251857, dtype=float32), 'train/wer': 0.03766584625353379, 'validation/ctc_loss': Array(0.34724754, dtype=float32), 'validation/wer': 0.10400957741583557, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19403726, dtype=float32), 'test/wer': 0.06556577905063676, 'test/num_examples': 2472, 'score': 57648.46272945404, 'total_duration': 63076.50105428696, 'accumulated_submission_time': 57648.46272945404, 'accumulated_eval_time': 5422.824957847595, 'accumulated_logging_time': 2.234661340713501, 'global_step': 72948, 'preemption_count': 0}), (74785, {'train/ctc_loss': Array(0.12474066, dtype=float32), 'train/wer': 0.04743712542076024, 'validation/ctc_loss': Array(0.34358847, dtype=float32), 'validation/wer': 0.10281240043639032, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18991898, dtype=float32), 'test/wer': 0.06357524424674507, 'test/num_examples': 2472, 'score': 59088.35154867172, 'total_duration': 64649.74883103371, 'accumulated_submission_time': 59088.35154867172, 'accumulated_eval_time': 5556.046092987061, 'accumulated_logging_time': 2.296834707260132, 'global_step': 74785, 'preemption_count': 0}), (76633, {'train/ctc_loss': Array(0.12209161, dtype=float32), 'train/wer': 0.04441446214856531, 'validation/ctc_loss': Array(0.33922857, dtype=float32), 'validation/wer': 0.10118076406924317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18748328, dtype=float32), 'test/wer': 0.062112810513273616, 'test/num_examples': 2472, 'score': 60528.67248296738, 'total_duration': 66222.53850531578, 'accumulated_submission_time': 60528.67248296738, 'accumulated_eval_time': 5688.373752355576, 'accumulated_logging_time': 2.361799716949463, 'global_step': 76633, 'preemption_count': 0})], 'global_step': 77324}
I0217 03:37:38.719621 139688679413568 submission_runner.py:586] Timing: 61068.41437482834
I0217 03:37:38.719707 139688679413568 submission_runner.py:588] Total number of evals: 43
I0217 03:37:38.719791 139688679413568 submission_runner.py:589] ====================
I0217 03:37:38.719844 139688679413568 submission_runner.py:542] Using RNG seed 1618809895
I0217 03:37:38.722197 139688679413568 submission_runner.py:551] --- Tuning run 5/5 ---
I0217 03:37:38.722340 139688679413568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_5.
I0217 03:37:38.723876 139688679413568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_5/hparams.json.
I0217 03:37:38.725232 139688679413568 submission_runner.py:206] Initializing dataset.
I0217 03:37:38.725543 139688679413568 submission_runner.py:213] Initializing model.
I0217 03:37:42.585784 139688679413568 submission_runner.py:255] Initializing optimizer.
I0217 03:37:43.148409 139688679413568 submission_runner.py:262] Initializing metrics bundle.
I0217 03:37:43.148643 139688679413568 submission_runner.py:280] Initializing checkpoint and logger.
I0217 03:37:43.153249 139688679413568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_5 with prefix checkpoint_
I0217 03:37:43.153409 139688679413568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_5/meta_data_0.json.
I0217 03:37:43.153687 139688679413568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 03:37:43.153798 139688679413568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 03:37:43.808821 139688679413568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 03:37:44.393504 139688679413568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_5/flags_0.json.
I0217 03:37:44.413016 139688679413568 submission_runner.py:314] Starting training loop.
I0217 03:37:44.416212 139688679413568 input_pipeline.py:20] Loading split = train-clean-100
I0217 03:37:44.461702 139688679413568 input_pipeline.py:20] Loading split = train-clean-360
I0217 03:37:44.977424 139688679413568 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0217 03:38:20.561148 139518441629440 logging_writer.py:48] [0] global_step=0, grad_norm=53.161041259765625, loss=31.063493728637695
I0217 03:38:20.586296 139688679413568 spec.py:321] Evaluating on the training split.
I0217 03:39:02.935441 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 03:39:51.990044 139688679413568 spec.py:349] Evaluating on the test split.
I0217 03:40:17.181911 139688679413568 submission_runner.py:408] Time since start: 152.77s, 	Step: 1, 	{'train/ctc_loss': Array(31.451271, dtype=float32), 'train/wer': 0.9408900814105828, 'validation/ctc_loss': Array(30.14182, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 36.173173904418945, 'total_duration': 152.76622414588928, 'accumulated_submission_time': 36.173173904418945, 'accumulated_eval_time': 116.59295606613159, 'accumulated_logging_time': 0}
I0217 03:40:17.196988 139578135525120 logging_writer.py:48] [1] accumulated_eval_time=116.592956, accumulated_logging_time=0, accumulated_submission_time=36.173174, global_step=1, preemption_count=0, score=36.173174, test/ctc_loss=30.20883560180664, test/num_examples=2472, test/wer=0.908598, total_duration=152.766224, train/ctc_loss=31.451271057128906, train/wer=0.940890, validation/ctc_loss=30.141820907592773, validation/num_examples=5348, validation/wer=0.904371
I0217 03:42:00.665396 139518458414848 logging_writer.py:48] [100] global_step=100, grad_norm=0.7267283797264099, loss=5.980681896209717
I0217 03:43:17.332408 139518466807552 logging_writer.py:48] [200] global_step=200, grad_norm=0.5947843194007874, loss=5.854781627655029
I0217 03:44:33.959579 139518458414848 logging_writer.py:48] [300] global_step=300, grad_norm=0.32863864302635193, loss=5.825822830200195
I0217 03:45:50.668970 139518466807552 logging_writer.py:48] [400] global_step=400, grad_norm=2.3709545135498047, loss=5.80350399017334
I0217 03:47:07.342940 139518458414848 logging_writer.py:48] [500] global_step=500, grad_norm=0.5479737520217896, loss=5.801893711090088
I0217 03:48:24.097772 139518466807552 logging_writer.py:48] [600] global_step=600, grad_norm=6.995307922363281, loss=5.848921775817871
I0217 03:49:40.821128 139518458414848 logging_writer.py:48] [700] global_step=700, grad_norm=2.026808977127075, loss=5.695526123046875
I0217 03:51:01.253628 139518466807552 logging_writer.py:48] [800] global_step=800, grad_norm=1.1992017030715942, loss=5.522448539733887
I0217 03:52:24.557954 139518458414848 logging_writer.py:48] [900] global_step=900, grad_norm=2.621089220046997, loss=5.398271083831787
I0217 03:53:47.747826 139518466807552 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1969976425170898, loss=4.412747859954834
I0217 03:55:09.367016 139578135525120 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.8614468574523926, loss=3.832489013671875
I0217 03:56:25.459312 139578127132416 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.5104018449783325, loss=3.4152660369873047
I0217 03:57:41.857856 139578135525120 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.887365460395813, loss=3.188199281692505
I0217 03:58:57.920371 139578127132416 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.8103376030921936, loss=2.9331891536712646
I0217 04:00:13.972968 139578135525120 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8884040713310242, loss=2.9069535732269287
I0217 04:01:34.498082 139578127132416 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6563759446144104, loss=2.7261972427368164
I0217 04:02:58.290925 139578135525120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8686619997024536, loss=2.6306865215301514
I0217 04:04:17.605529 139688679413568 spec.py:321] Evaluating on the training split.
I0217 04:05:06.466131 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 04:05:56.951628 139688679413568 spec.py:349] Evaluating on the test split.
I0217 04:06:23.392449 139688679413568 submission_runner.py:408] Time since start: 1718.97s, 	Step: 1796, 	{'train/ctc_loss': Array(3.44108, dtype=float32), 'train/wer': 0.6439037536543498, 'validation/ctc_loss': Array(3.3643177, dtype=float32), 'validation/wer': 0.6250132751479576, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.0440004, dtype=float32), 'test/wer': 0.5801799605955356, 'test/num_examples': 2472, 'score': 1476.5014803409576, 'total_duration': 1718.9732348918915, 'accumulated_submission_time': 1476.5014803409576, 'accumulated_eval_time': 242.3737189769745, 'accumulated_logging_time': 0.0271456241607666}
I0217 04:06:23.427401 139578135525120 logging_writer.py:48] [1796] accumulated_eval_time=242.373719, accumulated_logging_time=0.027146, accumulated_submission_time=1476.501480, global_step=1796, preemption_count=0, score=1476.501480, test/ctc_loss=3.0440003871917725, test/num_examples=2472, test/wer=0.580180, total_duration=1718.973235, train/ctc_loss=3.441080093383789, train/wer=0.643904, validation/ctc_loss=3.3643176555633545, validation/num_examples=5348, validation/wer=0.625013
I0217 04:06:27.319012 139578127132416 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8763107657432556, loss=2.4582996368408203
I0217 04:07:43.228744 139578135525120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.569728672504425, loss=2.374159574508667
I0217 04:08:59.331490 139578127132416 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.631635308265686, loss=2.3607819080352783
I0217 04:10:19.006187 139578135525120 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.5369949340820312, loss=2.2425460815429688
I0217 04:11:34.978113 139578127132416 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7059006094932556, loss=2.2834863662719727
I0217 04:12:50.975105 139578135525120 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5904657244682312, loss=2.1566457748413086
I0217 04:14:06.966293 139578127132416 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.6574526429176331, loss=2.114140033721924
I0217 04:15:23.299172 139578135525120 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.1165895462036133, loss=2.1067886352539062
I0217 04:16:42.878900 139578127132416 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.4542427659034729, loss=2.022799253463745
I0217 04:18:06.354488 139578135525120 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7425968647003174, loss=2.0832815170288086
I0217 04:19:30.776668 139578127132416 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5178593993186951, loss=1.9939625263214111
I0217 04:20:54.610310 139578135525120 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5020456910133362, loss=1.9657444953918457
I0217 04:22:18.422060 139578127132416 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6524366140365601, loss=1.9758954048156738
I0217 04:23:45.594113 139578135525120 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7732942700386047, loss=1.9718873500823975
I0217 04:25:01.511714 139578127132416 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6545962691307068, loss=1.901837706565857
I0217 04:26:17.335839 139578135525120 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5960453748703003, loss=1.853576421737671
I0217 04:27:33.180676 139578127132416 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.6046929359436035, loss=1.8912444114685059
I0217 04:28:49.109122 139578135525120 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6250332593917847, loss=1.911263108253479
I0217 04:30:05.026247 139578127132416 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8037757277488708, loss=1.873149037361145
I0217 04:30:23.615174 139688679413568 spec.py:321] Evaluating on the training split.
I0217 04:31:16.790576 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 04:32:08.330532 139688679413568 spec.py:349] Evaluating on the test split.
I0217 04:32:34.528409 139688679413568 submission_runner.py:408] Time since start: 3290.11s, 	Step: 3624, 	{'train/ctc_loss': Array(0.90955204, dtype=float32), 'train/wer': 0.28858346674681107, 'validation/ctc_loss': Array(0.95395464, dtype=float32), 'validation/wer': 0.2790677467005223, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6575261, dtype=float32), 'test/wer': 0.21471370828509334, 'test/num_examples': 2472, 'score': 2916.602714776993, 'total_duration': 3290.107299566269, 'accumulated_submission_time': 2916.602714776993, 'accumulated_eval_time': 373.27892112731934, 'accumulated_logging_time': 0.07884597778320312}
I0217 04:32:34.567976 139578135525120 logging_writer.py:48] [3624] accumulated_eval_time=373.278921, accumulated_logging_time=0.078846, accumulated_submission_time=2916.602715, global_step=3624, preemption_count=0, score=2916.602715, test/ctc_loss=0.6575260758399963, test/num_examples=2472, test/wer=0.214714, total_duration=3290.107300, train/ctc_loss=0.9095520377159119, train/wer=0.288583, validation/ctc_loss=0.9539546370506287, validation/num_examples=5348, validation/wer=0.279068
I0217 04:33:33.179818 139578127132416 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7528683543205261, loss=1.8467127084732056
I0217 04:34:49.027184 139578135525120 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7594918012619019, loss=1.8513565063476562
I0217 04:36:04.974797 139578127132416 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5434944033622742, loss=1.816329836845398
I0217 04:37:20.793985 139578135525120 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5396772027015686, loss=1.8474373817443848
I0217 04:38:42.452728 139578127132416 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5988913178443909, loss=1.7893530130386353
I0217 04:40:03.310086 139578135525120 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.45726701617240906, loss=1.768996000289917
I0217 04:41:19.220493 139578127132416 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8676367998123169, loss=1.7654424905776978
I0217 04:42:35.105382 139578135525120 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6828073263168335, loss=1.7492541074752808
I0217 04:43:51.034653 139578127132416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5041840076446533, loss=1.7744958400726318
I0217 04:45:06.934578 139578135525120 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.48563215136528015, loss=1.7123069763183594
I0217 04:46:27.169009 139578127132416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.4935551583766937, loss=1.7359241247177124
I0217 04:47:50.776007 139578135525120 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5569483041763306, loss=1.7618011236190796
I0217 04:49:13.450144 139578127132416 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5650903582572937, loss=1.7225180864334106
I0217 04:50:37.060821 139578135525120 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.611563503742218, loss=1.7067350149154663
I0217 04:52:00.147229 139578127132416 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.4457643926143646, loss=1.6920088529586792
I0217 04:53:23.340404 139578135525120 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.4910210967063904, loss=1.6917599439620972
I0217 04:54:39.199909 139578127132416 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.4571266174316406, loss=1.6780072450637817
I0217 04:55:55.104772 139578135525120 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5341835021972656, loss=1.6540523767471313
I0217 04:56:34.917715 139688679413568 spec.py:321] Evaluating on the training split.
I0217 04:57:28.502037 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 04:58:19.844264 139688679413568 spec.py:349] Evaluating on the test split.
I0217 04:58:46.039094 139688679413568 submission_runner.py:408] Time since start: 4861.62s, 	Step: 5454, 	{'train/ctc_loss': Array(0.55690676, dtype=float32), 'train/wer': 0.18963967600441323, 'validation/ctc_loss': Array(0.74646324, dtype=float32), 'validation/wer': 0.22499203491122546, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48497862, dtype=float32), 'test/wer': 0.1616598622874901, 'test/num_examples': 2472, 'score': 4356.862602472305, 'total_duration': 4861.620371341705, 'accumulated_submission_time': 4356.862602472305, 'accumulated_eval_time': 504.3946657180786, 'accumulated_logging_time': 0.1390526294708252}
I0217 04:58:46.070357 139578135525120 logging_writer.py:48] [5454] accumulated_eval_time=504.394666, accumulated_logging_time=0.139053, accumulated_submission_time=4356.862602, global_step=5454, preemption_count=0, score=4356.862602, test/ctc_loss=0.4849786162376404, test/num_examples=2472, test/wer=0.161660, total_duration=4861.620371, train/ctc_loss=0.5569067597389221, train/wer=0.189640, validation/ctc_loss=0.7464632391929626, validation/num_examples=5348, validation/wer=0.224992
I0217 04:59:21.696401 139578127132416 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.48504775762557983, loss=1.6685466766357422
I0217 05:00:37.583212 139578135525120 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5560781359672546, loss=1.6654363870620728
I0217 05:01:53.628247 139578127132416 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5486021637916565, loss=1.717286467552185
I0217 05:03:09.456253 139578135525120 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.41057106852531433, loss=1.6465892791748047
I0217 05:04:27.617743 139578127132416 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8532158732414246, loss=1.6792505979537964
I0217 05:05:50.179370 139578135525120 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5102273225784302, loss=1.665811538696289
I0217 05:07:13.070350 139578127132416 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5469112992286682, loss=1.673990249633789
I0217 05:08:38.835491 139578135525120 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.45062878727912903, loss=1.6543960571289062
I0217 05:09:54.666970 139578127132416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.528817355632782, loss=1.6551085710525513
I0217 05:11:10.448938 139578135525120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4229535758495331, loss=1.639975666999817
I0217 05:12:26.379683 139578127132416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5030524134635925, loss=1.656831979751587
I0217 05:13:42.264463 139578135525120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6516423225402832, loss=1.662130355834961
I0217 05:14:59.473735 139578127132416 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5269377827644348, loss=1.6389758586883545
I0217 05:16:23.509365 139578135525120 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5140814781188965, loss=1.5849299430847168
I0217 05:17:46.548399 139578127132416 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4638923406600952, loss=1.550492763519287
I0217 05:19:10.992069 139578135525120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4790419936180115, loss=1.597638726234436
I0217 05:20:35.777367 139578127132416 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.46973752975463867, loss=1.606314778327942
I0217 05:22:00.685431 139578135525120 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5762747526168823, loss=1.587912917137146
I0217 05:22:46.221199 139688679413568 spec.py:321] Evaluating on the training split.
I0217 05:23:40.042801 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 05:24:31.846889 139688679413568 spec.py:349] Evaluating on the test split.
I0217 05:24:58.156949 139688679413568 submission_runner.py:408] Time since start: 6433.74s, 	Step: 7256, 	{'train/ctc_loss': Array(0.55555606, dtype=float32), 'train/wer': 0.18881049824992593, 'validation/ctc_loss': Array(0.6697497, dtype=float32), 'validation/wer': 0.20144433609778234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42860916, dtype=float32), 'test/wer': 0.1437044258932017, 'test/num_examples': 2472, 'score': 5796.924175024033, 'total_duration': 6433.738090276718, 'accumulated_submission_time': 5796.924175024033, 'accumulated_eval_time': 636.3247225284576, 'accumulated_logging_time': 0.19132184982299805}
I0217 05:24:58.194240 139578135525120 logging_writer.py:48] [7256] accumulated_eval_time=636.324723, accumulated_logging_time=0.191322, accumulated_submission_time=5796.924175, global_step=7256, preemption_count=0, score=5796.924175, test/ctc_loss=0.428609162569046, test/num_examples=2472, test/wer=0.143704, total_duration=6433.738090, train/ctc_loss=0.555556058883667, train/wer=0.188810, validation/ctc_loss=0.6697496771812439, validation/num_examples=5348, validation/wer=0.201444
I0217 05:25:32.385621 139578127132416 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7459041476249695, loss=1.623306393623352
I0217 05:26:48.402083 139578135525120 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5736936330795288, loss=1.5599918365478516
I0217 05:28:04.392104 139578127132416 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.490532249212265, loss=1.5823312997817993
I0217 05:29:20.399675 139578135525120 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5307257175445557, loss=1.5956462621688843
I0217 05:30:36.465711 139578127132416 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.47913965582847595, loss=1.5685358047485352
I0217 05:31:52.532101 139578135525120 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.49431148171424866, loss=1.635115623474121
I0217 05:33:09.338926 139578127132416 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4376315474510193, loss=1.602207064628601
I0217 05:34:33.192115 139578135525120 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5118085741996765, loss=1.60383141040802
I0217 05:35:56.624211 139578127132416 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5128285884857178, loss=1.527932047843933
I0217 05:37:20.671970 139578135525120 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.49368149042129517, loss=1.5703567266464233
I0217 05:38:42.287665 139578135525120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.504402756690979, loss=1.541802167892456
I0217 05:39:58.183925 139578127132416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5996246337890625, loss=1.50705885887146
I0217 05:41:14.265630 139578135525120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5601611733436584, loss=1.5914644002914429
I0217 05:42:30.203333 139578127132416 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.49699535965919495, loss=1.560202956199646
I0217 05:43:46.125428 139578135525120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5009486079216003, loss=1.51652193069458
I0217 05:45:05.725035 139578127132416 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5489937663078308, loss=1.483317494392395
I0217 05:46:28.787428 139578135525120 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.568204939365387, loss=1.5241179466247559
I0217 05:47:52.589583 139578127132416 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.37851426005363464, loss=1.5366849899291992
I0217 05:48:58.675679 139688679413568 spec.py:321] Evaluating on the training split.
I0217 05:49:53.674216 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 05:50:44.904664 139688679413568 spec.py:349] Evaluating on the test split.
I0217 05:51:11.352225 139688679413568 submission_runner.py:408] Time since start: 8006.93s, 	Step: 9080, 	{'train/ctc_loss': Array(0.45142564, dtype=float32), 'train/wer': 0.15607634180452967, 'validation/ctc_loss': Array(0.624928, dtype=float32), 'validation/wer': 0.18842986377284532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39310047, dtype=float32), 'test/wer': 0.1300956675400646, 'test/num_examples': 2472, 'score': 7237.313529729843, 'total_duration': 8006.933594942093, 'accumulated_submission_time': 7237.313529729843, 'accumulated_eval_time': 768.995701789856, 'accumulated_logging_time': 0.25190162658691406}
I0217 05:51:11.388215 139578135525120 logging_writer.py:48] [9080] accumulated_eval_time=768.995702, accumulated_logging_time=0.251902, accumulated_submission_time=7237.313530, global_step=9080, preemption_count=0, score=7237.313530, test/ctc_loss=0.39310047030448914, test/num_examples=2472, test/wer=0.130096, total_duration=8006.933595, train/ctc_loss=0.4514256417751312, train/wer=0.156076, validation/ctc_loss=0.6249279975891113, validation/num_examples=5348, validation/wer=0.188430
I0217 05:51:27.355308 139578127132416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5758800506591797, loss=1.5429580211639404
I0217 05:52:43.192660 139578135525120 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5154502987861633, loss=1.5644947290420532
I0217 05:54:02.718668 139578135525120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5244033932685852, loss=1.4812039136886597
I0217 05:55:18.602998 139578127132416 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5544118881225586, loss=1.5390809774398804
I0217 05:56:34.638927 139578135525120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.49015262722969055, loss=1.447264313697815
I0217 05:57:50.635333 139578127132416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.4260779321193695, loss=1.5248603820800781
I0217 05:59:06.921136 139578135525120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.45456671714782715, loss=1.549236536026001
I0217 06:00:25.138408 139578127132416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5169601440429688, loss=1.4978229999542236
I0217 06:01:48.344175 139578135525120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.4559405446052551, loss=1.5413321256637573
I0217 06:03:12.047936 139578127132416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.505956768989563, loss=1.4900416135787964
I0217 06:04:34.936834 139578135525120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5678579211235046, loss=1.4841265678405762
I0217 06:05:58.285353 139578127132416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.49268639087677, loss=1.5045746564865112
I0217 06:07:25.126008 139578135525120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5211786031723022, loss=1.5488624572753906
I0217 06:08:40.968305 139578127132416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.4266051650047302, loss=1.4657955169677734
I0217 06:09:56.786467 139578135525120 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5224470496177673, loss=1.556634783744812
I0217 06:11:12.614155 139578127132416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5028610825538635, loss=1.4605273008346558
I0217 06:12:28.590793 139578135525120 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.524038553237915, loss=1.5149638652801514
I0217 06:13:44.440722 139578127132416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5248078107833862, loss=1.5174875259399414
I0217 06:15:07.123458 139578135525120 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.40590664744377136, loss=1.4864832162857056
I0217 06:15:11.890271 139688679413568 spec.py:321] Evaluating on the training split.
I0217 06:16:06.641387 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 06:16:58.498893 139688679413568 spec.py:349] Evaluating on the test split.
I0217 06:17:24.723656 139688679413568 submission_runner.py:408] Time since start: 9580.30s, 	Step: 10907, 	{'train/ctc_loss': Array(0.43169728, dtype=float32), 'train/wer': 0.15281203283448377, 'validation/ctc_loss': Array(0.59886086, dtype=float32), 'validation/wer': 0.18009789818202881, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37033734, dtype=float32), 'test/wer': 0.12507870737107224, 'test/num_examples': 2472, 'score': 8677.727895021439, 'total_duration': 9580.304859876633, 'accumulated_submission_time': 8677.727895021439, 'accumulated_eval_time': 901.8233368396759, 'accumulated_logging_time': 0.3069281578063965}
I0217 06:17:24.758341 139578135525120 logging_writer.py:48] [10907] accumulated_eval_time=901.823337, accumulated_logging_time=0.306928, accumulated_submission_time=8677.727895, global_step=10907, preemption_count=0, score=8677.727895, test/ctc_loss=0.3703373372554779, test/num_examples=2472, test/wer=0.125079, total_duration=9580.304860, train/ctc_loss=0.431697279214859, train/wer=0.152812, validation/ctc_loss=0.5988608598709106, validation/num_examples=5348, validation/wer=0.180098
I0217 06:18:35.886486 139578127132416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.42018258571624756, loss=1.5070109367370605
I0217 06:19:51.726318 139578135525120 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.4297756552696228, loss=1.4535222053527832
I0217 06:21:07.644383 139578127132416 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.49669918417930603, loss=1.4630345106124878
I0217 06:22:24.065829 139578135525120 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.47400999069213867, loss=1.4821254014968872
I0217 06:23:45.614868 139578135525120 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5001190304756165, loss=1.4501326084136963
I0217 06:25:01.410063 139578127132416 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.502733051776886, loss=1.5007805824279785
I0217 06:26:17.243306 139578135525120 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.45324400067329407, loss=1.4001598358154297
I0217 06:27:33.139955 139578127132416 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.4572279155254364, loss=1.4952353239059448
I0217 06:28:49.096749 139578135525120 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5333573222160339, loss=1.4878454208374023
I0217 06:30:12.263160 139578127132416 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.43728718161582947, loss=1.5233268737792969
I0217 06:31:35.317458 139578135525120 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.45472627878189087, loss=1.4364895820617676
I0217 06:32:59.114672 139578127132416 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3857380151748657, loss=1.4406620264053345
I0217 06:34:23.197993 139578135525120 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.45308831334114075, loss=1.446874976158142
I0217 06:35:45.979861 139578127132416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5824262499809265, loss=1.4171353578567505
I0217 06:37:10.465913 139578135525120 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4151439368724823, loss=1.447210431098938
I0217 06:38:26.511654 139578127132416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.4748339354991913, loss=1.4425573348999023
I0217 06:39:42.626345 139578135525120 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.4271029531955719, loss=1.4944336414337158
I0217 06:40:58.692200 139578127132416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.5031752586364746, loss=1.4658702611923218
I0217 06:41:25.064051 139688679413568 spec.py:321] Evaluating on the training split.
I0217 06:42:19.287484 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 06:43:11.459599 139688679413568 spec.py:349] Evaluating on the test split.
I0217 06:43:37.941098 139688679413568 submission_runner.py:408] Time since start: 11153.52s, 	Step: 12736, 	{'train/ctc_loss': Array(0.4072509, dtype=float32), 'train/wer': 0.14037704918032787, 'validation/ctc_loss': Array(0.5681042, dtype=float32), 'validation/wer': 0.17247072226459542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35084808, dtype=float32), 'test/wer': 0.11780716186297809, 'test/num_examples': 2472, 'score': 10117.946796894073, 'total_duration': 11153.522399902344, 'accumulated_submission_time': 10117.946796894073, 'accumulated_eval_time': 1034.6947605609894, 'accumulated_logging_time': 0.3596227169036865}
I0217 06:43:37.974705 139578135525120 logging_writer.py:48] [12736] accumulated_eval_time=1034.694761, accumulated_logging_time=0.359623, accumulated_submission_time=10117.946797, global_step=12736, preemption_count=0, score=10117.946797, test/ctc_loss=0.35084807872772217, test/num_examples=2472, test/wer=0.117807, total_duration=11153.522400, train/ctc_loss=0.40725091099739075, train/wer=0.140377, validation/ctc_loss=0.5681042075157166, validation/num_examples=5348, validation/wer=0.172471
I0217 06:44:27.429611 139578127132416 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5080636739730835, loss=1.3924490213394165
I0217 06:45:43.350227 139578135525120 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5767767429351807, loss=1.4803725481033325
I0217 06:46:59.515737 139578127132416 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7577732801437378, loss=1.4869016408920288
I0217 06:48:15.581827 139578135525120 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4155593514442444, loss=1.490078091621399
I0217 06:49:35.689457 139578127132416 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.43647682666778564, loss=1.4518883228302002
I0217 06:50:59.700942 139578135525120 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5356385111808777, loss=1.4827998876571655
I0217 06:52:26.420143 139578135525120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5924099087715149, loss=1.3929671049118042
I0217 06:53:42.253341 139578127132416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.4259341359138489, loss=1.4721816778182983
I0217 06:54:58.159789 139578135525120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5658188462257385, loss=1.3809651136398315
I0217 06:56:14.106296 139578127132416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5630529522895813, loss=1.4714438915252686
I0217 06:57:30.061401 139578135525120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.562469482421875, loss=1.446834683418274
I0217 06:58:45.937029 139578127132416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.40565726161003113, loss=1.466659665107727
I0217 07:00:09.259908 139578135525120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6045007705688477, loss=1.4959990978240967
I0217 07:01:33.041001 139578127132416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5390821695327759, loss=1.4621891975402832
I0217 07:02:56.930880 139578135525120 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.4122275412082672, loss=1.39057457447052
I0217 07:04:20.250766 139578127132416 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.4589014947414398, loss=1.3872803449630737
I0217 07:05:44.651335 139578135525120 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5374596118927002, loss=1.5051493644714355
I0217 07:07:05.538695 139578135525120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.522487461566925, loss=1.4401003122329712
I0217 07:07:38.543229 139688679413568 spec.py:321] Evaluating on the training split.
I0217 07:08:31.633375 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 07:09:24.250689 139688679413568 spec.py:349] Evaluating on the test split.
I0217 07:09:50.820482 139688679413568 submission_runner.py:408] Time since start: 12726.40s, 	Step: 14545, 	{'train/ctc_loss': Array(0.39576054, dtype=float32), 'train/wer': 0.1385089016958059, 'validation/ctc_loss': Array(0.55048543, dtype=float32), 'validation/wer': 0.1663013989592284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33382007, dtype=float32), 'test/wer': 0.1137042227774054, 'test/num_examples': 2472, 'score': 11558.43038058281, 'total_duration': 12726.402488231659, 'accumulated_submission_time': 11558.43038058281, 'accumulated_eval_time': 1166.9670646190643, 'accumulated_logging_time': 0.4112875461578369}
I0217 07:09:50.858167 139578135525120 logging_writer.py:48] [14545] accumulated_eval_time=1166.967065, accumulated_logging_time=0.411288, accumulated_submission_time=11558.430381, global_step=14545, preemption_count=0, score=11558.430381, test/ctc_loss=0.33382007479667664, test/num_examples=2472, test/wer=0.113704, total_duration=12726.402488, train/ctc_loss=0.39576053619384766, train/wer=0.138509, validation/ctc_loss=0.5504854321479797, validation/num_examples=5348, validation/wer=0.166301
I0217 07:10:33.254235 139578127132416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5983821749687195, loss=1.3812685012817383
I0217 07:11:49.012679 139578135525120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.46599850058555603, loss=1.4052460193634033
I0217 07:13:04.900078 139578127132416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.4599730372428894, loss=1.4104504585266113
I0217 07:14:20.811021 139578135525120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.5511453151702881, loss=1.3967527151107788
I0217 07:15:36.621398 139578127132416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4645593762397766, loss=1.4378784894943237
I0217 07:16:53.562049 139578135525120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.5492016673088074, loss=1.4010183811187744
I0217 07:18:17.195168 139578127132416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.44457775354385376, loss=1.4003386497497559
I0217 07:19:41.119235 139578135525120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.520205020904541, loss=1.3621697425842285
I0217 07:21:05.321398 139578127132416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.47902029752731323, loss=1.4531910419464111
I0217 07:22:28.232179 139578135525120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.4861804246902466, loss=1.4098865985870361
I0217 07:23:44.072902 139578127132416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.499787300825119, loss=1.427161693572998
I0217 07:25:00.141785 139578135525120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5588613748550415, loss=1.4469980001449585
I0217 07:26:16.028923 139578127132416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.45171424746513367, loss=1.387351155281067
I0217 07:27:31.971999 139578135525120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.44001007080078125, loss=1.4569194316864014
I0217 07:28:50.506936 139578127132416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.44419020414352417, loss=1.3964890241622925
I0217 07:30:15.626826 139578135525120 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.48312681913375854, loss=1.399661898612976
I0217 07:31:40.695601 139578127132416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.45750048756599426, loss=1.4341329336166382
I0217 07:33:04.365226 139578135525120 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.48673775792121887, loss=1.3477442264556885
I0217 07:33:51.098359 139688679413568 spec.py:321] Evaluating on the training split.
I0217 07:34:45.230714 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 07:35:37.333578 139688679413568 spec.py:349] Evaluating on the test split.
I0217 07:36:03.788623 139688679413568 submission_runner.py:408] Time since start: 14299.37s, 	Step: 16358, 	{'train/ctc_loss': Array(0.40191302, dtype=float32), 'train/wer': 0.13663466005885086, 'validation/ctc_loss': Array(0.5395458, dtype=float32), 'validation/wer': 0.16157061895980768, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33373216, dtype=float32), 'test/wer': 0.10952003737330653, 'test/num_examples': 2472, 'score': 12998.585268974304, 'total_duration': 14299.370248556137, 'accumulated_submission_time': 12998.585268974304, 'accumulated_eval_time': 1299.6520047187805, 'accumulated_logging_time': 0.46649909019470215}
I0217 07:36:03.825258 139578135525120 logging_writer.py:48] [16358] accumulated_eval_time=1299.652005, accumulated_logging_time=0.466499, accumulated_submission_time=12998.585269, global_step=16358, preemption_count=0, score=12998.585269, test/ctc_loss=0.33373215794563293, test/num_examples=2472, test/wer=0.109520, total_duration=14299.370249, train/ctc_loss=0.40191301703453064, train/wer=0.136635, validation/ctc_loss=0.5395457744598389, validation/num_examples=5348, validation/wer=0.161571
I0217 07:36:36.366414 139578127132416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5462976694107056, loss=1.4928473234176636
I0217 07:37:55.889836 139578135525120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.517340362071991, loss=1.4285552501678467
I0217 07:39:11.698945 139578127132416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.4883778989315033, loss=1.3856937885284424
I0217 07:40:27.639237 139578135525120 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.43078258633613586, loss=1.360520839691162
I0217 07:41:43.571232 139578127132416 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.514224112033844, loss=1.4443095922470093
I0217 07:42:59.635463 139578135525120 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.43853119015693665, loss=1.3815016746520996
I0217 07:44:20.102812 139578127132416 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.4610100984573364, loss=1.3703339099884033
I0217 07:45:44.245034 139578135525120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6091124415397644, loss=1.4150190353393555
I0217 07:47:09.400906 139578127132416 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.4307534992694855, loss=1.4214539527893066
I0217 07:48:33.190087 139578135525120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.4723435640335083, loss=1.3624621629714966
I0217 07:49:57.093702 139578127132416 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6084953546524048, loss=1.343313217163086
I0217 07:51:21.245270 139578135525120 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5422924757003784, loss=1.4106618165969849
I0217 07:52:41.006796 139578135525120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.46071189641952515, loss=1.3886125087738037
I0217 07:53:56.891681 139578127132416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.4833223819732666, loss=1.3274240493774414
I0217 07:55:12.784222 139578135525120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5336803793907166, loss=1.3384432792663574
I0217 07:56:28.833581 139578127132416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.5880802273750305, loss=1.3730950355529785
I0217 07:57:44.886008 139578135525120 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.45493045449256897, loss=1.3622291088104248
I0217 07:59:06.899824 139578127132416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5397216081619263, loss=1.3234121799468994
I0217 08:00:03.896443 139688679413568 spec.py:321] Evaluating on the training split.
I0217 08:00:58.136478 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 08:01:50.402595 139688679413568 spec.py:349] Evaluating on the test split.
I0217 08:02:17.159686 139688679413568 submission_runner.py:408] Time since start: 15872.74s, 	Step: 18169, 	{'train/ctc_loss': Array(0.37821412, dtype=float32), 'train/wer': 0.13356724368248055, 'validation/ctc_loss': Array(0.52589005, dtype=float32), 'validation/wer': 0.15801770663371212, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31530783, dtype=float32), 'test/wer': 0.1063311193711535, 'test/num_examples': 2472, 'score': 14438.571347236633, 'total_duration': 15872.740940332413, 'accumulated_submission_time': 14438.571347236633, 'accumulated_eval_time': 1432.9095618724823, 'accumulated_logging_time': 0.5199224948883057}
I0217 08:02:17.196544 139578135525120 logging_writer.py:48] [18169] accumulated_eval_time=1432.909562, accumulated_logging_time=0.519922, accumulated_submission_time=14438.571347, global_step=18169, preemption_count=0, score=14438.571347, test/ctc_loss=0.3153078258037567, test/num_examples=2472, test/wer=0.106331, total_duration=15872.740940, train/ctc_loss=0.37821412086486816, train/wer=0.133567, validation/ctc_loss=0.525890052318573, validation/num_examples=5348, validation/wer=0.158018
I0217 08:02:41.498991 139578127132416 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5546759366989136, loss=1.4197015762329102
I0217 08:03:57.306543 139578135525120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.4899771213531494, loss=1.3223692178726196
I0217 08:05:13.177055 139578127132416 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.5670439004898071, loss=1.442193865776062
I0217 08:06:29.103648 139578135525120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.46485117077827454, loss=1.362802267074585
I0217 08:07:48.556419 139578135525120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5015659928321838, loss=1.355491280555725
I0217 08:09:04.456107 139578127132416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.45921504497528076, loss=1.4085062742233276
I0217 08:10:20.315037 139578135525120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.47581130266189575, loss=1.4014990329742432
I0217 08:11:36.110015 139578127132416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.5580688118934631, loss=1.3991315364837646
I0217 08:12:51.940944 139578135525120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5206257104873657, loss=1.3595894575119019
I0217 08:14:11.966449 139578127132416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.4418726861476898, loss=1.330605387687683
I0217 08:15:35.982497 139578135525120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.4968073070049286, loss=1.3567421436309814
I0217 08:17:01.192655 139578127132416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5035638809204102, loss=1.3903534412384033
I0217 08:18:25.939996 139578135525120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5056565403938293, loss=1.3851120471954346
I0217 08:19:49.481092 139578127132416 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5134639739990234, loss=1.3748027086257935
I0217 08:21:13.694683 139578135525120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.4688400328159332, loss=1.3701022863388062
I0217 08:22:29.518385 139578127132416 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.5405343770980835, loss=1.3264875411987305
I0217 08:23:45.415540 139578135525120 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.626524806022644, loss=1.3544161319732666
I0217 08:25:01.285936 139578127132416 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.44168829917907715, loss=1.4219539165496826
I0217 08:26:17.158522 139578135525120 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.46808886528015137, loss=1.3473831415176392
I0217 08:26:17.167976 139688679413568 spec.py:321] Evaluating on the training split.
I0217 08:27:08.943580 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 08:28:00.514323 139688679413568 spec.py:349] Evaluating on the test split.
I0217 08:28:26.655499 139688679413568 submission_runner.py:408] Time since start: 17442.24s, 	Step: 20001, 	{'train/ctc_loss': Array(0.37082225, dtype=float32), 'train/wer': 0.1336668088612551, 'validation/ctc_loss': Array(0.50519156, dtype=float32), 'validation/wer': 0.15093119128764107, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30063546, dtype=float32), 'test/wer': 0.10192350659110759, 'test/num_examples': 2472, 'score': 15878.452617168427, 'total_duration': 17442.23735189438, 'accumulated_submission_time': 15878.452617168427, 'accumulated_eval_time': 1562.391996383667, 'accumulated_logging_time': 0.5783975124359131}
I0217 08:28:26.693884 139578135525120 logging_writer.py:48] [20001] accumulated_eval_time=1562.391996, accumulated_logging_time=0.578398, accumulated_submission_time=15878.452617, global_step=20001, preemption_count=0, score=15878.452617, test/ctc_loss=0.3006354570388794, test/num_examples=2472, test/wer=0.101924, total_duration=17442.237352, train/ctc_loss=0.3708222508430481, train/wer=0.133667, validation/ctc_loss=0.5051915645599365, validation/num_examples=5348, validation/wer=0.150931
I0217 08:29:42.363507 139578127132416 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.5724031329154968, loss=1.3618282079696655
I0217 08:30:58.113066 139578135525120 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.5247665643692017, loss=1.4200170040130615
I0217 08:32:13.974364 139578127132416 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.4898918867111206, loss=1.4051430225372314
I0217 08:33:31.234825 139578135525120 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.5408933162689209, loss=1.339015007019043
I0217 08:34:54.573237 139578127132416 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5314914584159851, loss=1.3413711786270142
I0217 08:36:21.676636 139578135525120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.5344970226287842, loss=1.383276343345642
I0217 08:37:37.650896 139578127132416 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.4958173632621765, loss=1.3414980173110962
I0217 08:38:53.609575 139578135525120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.45986154675483704, loss=1.3600101470947266
I0217 08:40:09.626547 139578127132416 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.4125746786594391, loss=1.2190446853637695
I0217 08:41:25.619500 139578135525120 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.48395147919654846, loss=1.3263990879058838
I0217 08:42:41.645488 139578127132416 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.48209133744239807, loss=1.3149529695510864
I0217 08:44:04.365614 139578135525120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.5425631403923035, loss=1.3723393678665161
I0217 08:45:27.439636 139578127132416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6818296909332275, loss=1.415306806564331
I0217 08:46:51.106778 139578135525120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.4678744971752167, loss=1.336182951927185
I0217 08:48:15.208356 139578127132416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5087437033653259, loss=1.3055806159973145
I0217 08:49:38.775579 139578135525120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5855973362922668, loss=1.3563834428787231
I0217 08:51:00.657959 139578135525120 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.4529743790626526, loss=1.3567947149276733
I0217 08:52:16.885386 139578127132416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.5550005435943604, loss=1.31305992603302
I0217 08:52:27.250329 139688679413568 spec.py:321] Evaluating on the training split.
I0217 08:53:21.615596 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 08:54:14.094618 139688679413568 spec.py:349] Evaluating on the test split.
I0217 08:54:40.240237 139688679413568 submission_runner.py:408] Time since start: 19015.82s, 	Step: 21815, 	{'train/ctc_loss': Array(0.31139514, dtype=float32), 'train/wer': 0.10986010097395268, 'validation/ctc_loss': Array(0.49194026, dtype=float32), 'validation/wer': 0.14698243818608378, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29525468, dtype=float32), 'test/wer': 0.09749558223143014, 'test/num_examples': 2472, 'score': 17318.92253255844, 'total_duration': 19015.821897506714, 'accumulated_submission_time': 17318.92253255844, 'accumulated_eval_time': 1695.3766107559204, 'accumulated_logging_time': 0.6333692073822021}
I0217 08:54:40.274769 139578135525120 logging_writer.py:48] [21815] accumulated_eval_time=1695.376611, accumulated_logging_time=0.633369, accumulated_submission_time=17318.922533, global_step=21815, preemption_count=0, score=17318.922533, test/ctc_loss=0.2952546775341034, test/num_examples=2472, test/wer=0.097496, total_duration=19015.821898, train/ctc_loss=0.31139513850212097, train/wer=0.109860, validation/ctc_loss=0.4919402599334717, validation/num_examples=5348, validation/wer=0.146982
I0217 08:55:45.506090 139578127132416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5047571063041687, loss=1.32438063621521
I0217 08:57:01.356064 139578135525120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5525560975074768, loss=1.3033292293548584
I0217 08:58:17.316551 139578127132416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.49654248356819153, loss=1.3397107124328613
I0217 08:59:33.306368 139578135525120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.43716961145401, loss=1.33602774143219
I0217 09:00:50.745578 139578127132416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.5082929134368896, loss=1.3473600149154663
I0217 09:02:15.926930 139578135525120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.5407613515853882, loss=1.3116225004196167
I0217 09:03:40.296195 139578127132416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5595855116844177, loss=1.3938612937927246
I0217 09:05:04.040022 139578135525120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.4791193902492523, loss=1.318837285041809
I0217 09:06:27.509691 139578135525120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.516789436340332, loss=1.3189270496368408
I0217 09:07:43.469523 139578127132416 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6295564770698547, loss=1.283362627029419
I0217 09:08:59.752188 139578135525120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.4784683287143707, loss=1.3196179866790771
I0217 09:10:15.690381 139578127132416 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6154768466949463, loss=1.2891530990600586
I0217 09:11:31.663925 139578135525120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.5155072212219238, loss=1.291587471961975
I0217 09:12:50.642159 139578127132416 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.51655113697052, loss=1.3122332096099854
I0217 09:14:15.984756 139578135525120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.689633309841156, loss=1.284764051437378
I0217 09:15:40.046873 139578127132416 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.5620434284210205, loss=1.3242594003677368
I0217 09:17:04.361674 139578135525120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.4913627803325653, loss=1.3001171350479126
I0217 09:18:29.529820 139578127132416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.5214306712150574, loss=1.3027249574661255
I0217 09:18:40.971655 139688679413568 spec.py:321] Evaluating on the training split.
I0217 09:19:34.297144 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 09:20:25.781542 139688679413568 spec.py:349] Evaluating on the test split.
I0217 09:20:51.975184 139688679413568 submission_runner.py:408] Time since start: 20587.56s, 	Step: 23615, 	{'train/ctc_loss': Array(0.32840356, dtype=float32), 'train/wer': 0.11590025321034546, 'validation/ctc_loss': Array(0.48064968, dtype=float32), 'validation/wer': 0.14498392500265503, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28810266, dtype=float32), 'test/wer': 0.09631751061280036, 'test/num_examples': 2472, 'score': 18759.53474497795, 'total_duration': 20587.556434631348, 'accumulated_submission_time': 18759.53474497795, 'accumulated_eval_time': 1826.3744299411774, 'accumulated_logging_time': 0.6851708889007568}
I0217 09:20:52.011046 139578135525120 logging_writer.py:48] [23615] accumulated_eval_time=1826.374430, accumulated_logging_time=0.685171, accumulated_submission_time=18759.534745, global_step=23615, preemption_count=0, score=18759.534745, test/ctc_loss=0.2881026566028595, test/num_examples=2472, test/wer=0.096318, total_duration=20587.556435, train/ctc_loss=0.3284035623073578, train/wer=0.115900, validation/ctc_loss=0.4806496798992157, validation/num_examples=5348, validation/wer=0.144984
I0217 09:22:00.768432 139578135525120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.47528380155563354, loss=1.3370387554168701
I0217 09:23:16.507499 139578127132416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5272461771965027, loss=1.3056163787841797
I0217 09:24:32.341423 139578135525120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.4243006110191345, loss=1.2818152904510498
I0217 09:25:48.173352 139578127132416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.46776261925697327, loss=1.2943233251571655
I0217 09:27:04.141925 139578135525120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5025302767753601, loss=1.2886680364608765
I0217 09:28:23.995205 139578127132416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.49791398644447327, loss=1.2855528593063354
I0217 09:29:49.629031 139578135525120 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.420853853225708, loss=1.3008513450622559
I0217 09:31:13.200429 139578127132416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4366459846496582, loss=1.2983143329620361
I0217 09:32:37.591306 139578135525120 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6347497701644897, loss=1.3004610538482666
I0217 09:34:00.968724 139578127132416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.548599362373352, loss=1.3356176614761353
I0217 09:35:24.598087 139578135525120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.5508482456207275, loss=1.3038564920425415
I0217 09:36:45.380119 139578135525120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.4346109628677368, loss=1.2377955913543701
I0217 09:38:01.421073 139578127132416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.45436882972717285, loss=1.2913004159927368
I0217 09:39:17.378185 139578135525120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5664312243461609, loss=1.3099900484085083
I0217 09:40:33.409697 139578127132416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.526929497718811, loss=1.2972570657730103
I0217 09:41:49.450094 139578135525120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.567211925983429, loss=1.2277207374572754
I0217 09:43:12.115049 139578127132416 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5204917192459106, loss=1.2929811477661133
I0217 09:44:36.411268 139578135525120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.5686520338058472, loss=1.3015722036361694
I0217 09:44:52.240724 139688679413568 spec.py:321] Evaluating on the training split.
I0217 09:45:45.806383 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 09:46:37.513786 139688679413568 spec.py:349] Evaluating on the test split.
I0217 09:47:04.252260 139688679413568 submission_runner.py:408] Time since start: 22159.83s, 	Step: 25420, 	{'train/ctc_loss': Array(0.3194967, dtype=float32), 'train/wer': 0.11300510415073389, 'validation/ctc_loss': Array(0.47430897, dtype=float32), 'validation/wer': 0.14272473618660514, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27983314, dtype=float32), 'test/wer': 0.09483476529969735, 'test/num_examples': 2472, 'score': 20199.681203603745, 'total_duration': 22159.8330514431, 'accumulated_submission_time': 20199.681203603745, 'accumulated_eval_time': 1958.3797991275787, 'accumulated_logging_time': 0.7363095283508301}
I0217 09:47:04.290447 139578135525120 logging_writer.py:48] [25420] accumulated_eval_time=1958.379799, accumulated_logging_time=0.736310, accumulated_submission_time=20199.681204, global_step=25420, preemption_count=0, score=20199.681204, test/ctc_loss=0.2798331379890442, test/num_examples=2472, test/wer=0.094835, total_duration=22159.833051, train/ctc_loss=0.31949669122695923, train/wer=0.113005, validation/ctc_loss=0.47430896759033203, validation/num_examples=5348, validation/wer=0.142725
I0217 09:48:05.543207 139578127132416 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5975488424301147, loss=1.3081248998641968
I0217 09:49:21.429308 139578135525120 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.43123573064804077, loss=1.3030333518981934
I0217 09:50:37.233618 139578127132416 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.45100292563438416, loss=1.3149187564849854
I0217 09:51:56.557578 139578135525120 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.4944330155849457, loss=1.2987642288208008
I0217 09:53:12.396429 139578127132416 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.5950162410736084, loss=1.3252159357070923
I0217 09:54:28.160332 139578135525120 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5755105018615723, loss=1.3548262119293213
I0217 09:55:43.970387 139578127132416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.43659090995788574, loss=1.2997167110443115
I0217 09:56:59.803920 139578135525120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.5720526576042175, loss=1.2910850048065186
I0217 09:58:20.717237 139578127132416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5621713399887085, loss=1.2608755826950073
I0217 09:59:44.737272 139578135525120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.5338309407234192, loss=1.2406095266342163
I0217 10:01:09.153727 139578127132416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.48797884583473206, loss=1.303907036781311
I0217 10:02:32.411059 139578135525120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.49816620349884033, loss=1.3212107419967651
I0217 10:03:56.775104 139578127132416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.5619423389434814, loss=1.318206548690796
I0217 10:05:22.265204 139578135525120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5060958862304688, loss=1.2826488018035889
I0217 10:06:38.037961 139578127132416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.540614128112793, loss=1.3193395137786865
I0217 10:07:53.925836 139578135525120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5204592943191528, loss=1.32728111743927
I0217 10:09:09.718500 139578127132416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.49259841442108154, loss=1.255155324935913
I0217 10:10:25.623092 139578135525120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.49590927362442017, loss=1.2591605186462402
I0217 10:11:04.693430 139688679413568 spec.py:321] Evaluating on the training split.
I0217 10:11:59.616782 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 10:12:51.922864 139688679413568 spec.py:349] Evaluating on the test split.
I0217 10:13:18.269310 139688679413568 submission_runner.py:408] Time since start: 23733.85s, 	Step: 27253, 	{'train/ctc_loss': Array(0.30807894, dtype=float32), 'train/wer': 0.10906005250612388, 'validation/ctc_loss': Array(0.4601984, dtype=float32), 'validation/wer': 0.1377139712484432, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2673006, dtype=float32), 'test/wer': 0.08878191456949606, 'test/num_examples': 2472, 'score': 21639.995112657547, 'total_duration': 23733.84988975525, 'accumulated_submission_time': 21639.995112657547, 'accumulated_eval_time': 2091.949378967285, 'accumulated_logging_time': 0.7944025993347168}
I0217 10:13:18.307861 139578135525120 logging_writer.py:48] [27253] accumulated_eval_time=2091.949379, accumulated_logging_time=0.794403, accumulated_submission_time=21639.995113, global_step=27253, preemption_count=0, score=21639.995113, test/ctc_loss=0.2673006057739258, test/num_examples=2472, test/wer=0.088782, total_duration=23733.849890, train/ctc_loss=0.30807894468307495, train/wer=0.109060, validation/ctc_loss=0.46019840240478516, validation/num_examples=5348, validation/wer=0.137714
I0217 10:13:54.665548 139578127132416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5449382662773132, loss=1.3086460828781128
I0217 10:15:10.551971 139578135525120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.4380268454551697, loss=1.2865742444992065
I0217 10:16:26.470831 139578127132416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5624945163726807, loss=1.3236472606658936
I0217 10:17:42.315971 139578135525120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.5505221486091614, loss=1.2924102544784546
I0217 10:19:05.005185 139578127132416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6926687955856323, loss=1.2820183038711548
I0217 10:20:30.109359 139578135525120 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6231818199157715, loss=1.2571526765823364
I0217 10:21:49.708706 139578135525120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.5248144268989563, loss=1.297702431678772
I0217 10:23:05.460190 139578127132416 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5063937902450562, loss=1.2470163106918335
I0217 10:24:21.281655 139578135525120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.5015370845794678, loss=1.2811405658721924
I0217 10:25:37.145850 139578127132416 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.5479875802993774, loss=1.3387668132781982
I0217 10:26:53.039345 139578135525120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.49611803889274597, loss=1.3075999021530151
I0217 10:28:17.530576 139578127132416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.4544332027435303, loss=1.3058274984359741
I0217 10:29:41.515508 139578135525120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6423600316047668, loss=1.3618793487548828
I0217 10:31:05.897418 139578127132416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.5696598887443542, loss=1.302080512046814
I0217 10:32:30.608607 139578135525120 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.4747554659843445, loss=1.2487571239471436
I0217 10:33:54.435638 139578127132416 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.4772351384162903, loss=1.2526718378067017
I0217 10:35:17.390562 139578135525120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.5217503309249878, loss=1.265755295753479
I0217 10:36:33.447882 139578127132416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6339527368545532, loss=1.3040286302566528
I0217 10:37:18.779146 139688679413568 spec.py:321] Evaluating on the training split.
I0217 10:38:12.012537 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 10:39:04.445825 139688679413568 spec.py:349] Evaluating on the test split.
I0217 10:39:31.028662 139688679413568 submission_runner.py:408] Time since start: 25306.61s, 	Step: 29061, 	{'train/ctc_loss': Array(0.2758949, dtype=float32), 'train/wer': 0.10113498175922173, 'validation/ctc_loss': Array(0.45608607, dtype=float32), 'validation/wer': 0.13516514284059203, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26807737, dtype=float32), 'test/wer': 0.0894725082769687, 'test/num_examples': 2472, 'score': 23080.382625341415, 'total_duration': 25306.609971761703, 'accumulated_submission_time': 23080.382625341415, 'accumulated_eval_time': 2224.193256378174, 'accumulated_logging_time': 0.8488309383392334}
I0217 10:39:31.066824 139578135525120 logging_writer.py:48] [29061] accumulated_eval_time=2224.193256, accumulated_logging_time=0.848831, accumulated_submission_time=23080.382625, global_step=29061, preemption_count=0, score=23080.382625, test/ctc_loss=0.26807737350463867, test/num_examples=2472, test/wer=0.089473, total_duration=25306.609972, train/ctc_loss=0.2758949100971222, train/wer=0.101135, validation/ctc_loss=0.45608606934547424, validation/num_examples=5348, validation/wer=0.135165
I0217 10:40:01.414028 139578127132416 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5156370997428894, loss=1.2302334308624268
I0217 10:41:17.204604 139578135525120 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.4344025254249573, loss=1.3033620119094849
I0217 10:42:33.043818 139578127132416 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.46752291917800903, loss=1.248503565788269
I0217 10:43:48.953346 139578135525120 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.49965083599090576, loss=1.2633577585220337
I0217 10:45:05.789350 139578127132416 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.5121715664863586, loss=1.2436790466308594
I0217 10:46:30.470813 139578135525120 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.5777661800384521, loss=1.2607735395431519
I0217 10:47:55.637192 139578127132416 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.522740364074707, loss=1.2933157682418823
I0217 10:49:20.131567 139578135525120 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.5013891458511353, loss=1.2760323286056519
I0217 10:50:46.303252 139578135525120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.6162340641021729, loss=1.2240898609161377
I0217 10:52:02.043084 139578127132416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.4791276752948761, loss=1.2689788341522217
I0217 10:53:18.121927 139578135525120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.44002488255500793, loss=1.3122036457061768
I0217 10:54:33.957484 139578127132416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.7586847543716431, loss=1.2698391675949097
I0217 10:55:49.825969 139578135525120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.47629275918006897, loss=1.262058138847351
I0217 10:57:09.302328 139578127132416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.4839209318161011, loss=1.3167139291763306
I0217 10:58:33.801829 139578135525120 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.5786135792732239, loss=1.2618212699890137
I0217 10:59:57.528420 139578127132416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.5477712750434875, loss=1.2395597696304321
I0217 11:01:20.579822 139578135525120 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.48970359563827515, loss=1.3870371580123901
I0217 11:02:44.453712 139578127132416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.5767715573310852, loss=1.2650219202041626
I0217 11:03:31.719714 139688679413568 spec.py:321] Evaluating on the training split.
I0217 11:04:24.899441 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 11:05:17.081777 139688679413568 spec.py:349] Evaluating on the test split.
I0217 11:05:43.879518 139688679413568 submission_runner.py:408] Time since start: 26879.46s, 	Step: 30857, 	{'train/ctc_loss': Array(0.24288544, dtype=float32), 'train/wer': 0.08868910009418615, 'validation/ctc_loss': Array(0.44085503, dtype=float32), 'validation/wer': 0.13189221545323768, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25917184, dtype=float32), 'test/wer': 0.08567424288586924, 'test/num_examples': 2472, 'score': 24520.94918370247, 'total_duration': 26879.460943460464, 'accumulated_submission_time': 24520.94918370247, 'accumulated_eval_time': 2356.3475427627563, 'accumulated_logging_time': 0.9049539566040039}
I0217 11:05:43.915595 139578135525120 logging_writer.py:48] [30857] accumulated_eval_time=2356.347543, accumulated_logging_time=0.904954, accumulated_submission_time=24520.949184, global_step=30857, preemption_count=0, score=24520.949184, test/ctc_loss=0.25917184352874756, test/num_examples=2472, test/wer=0.085674, total_duration=26879.460943, train/ctc_loss=0.24288544058799744, train/wer=0.088689, validation/ctc_loss=0.4408550262451172, validation/num_examples=5348, validation/wer=0.131892
I0217 11:06:20.889258 139578135525120 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.5959841012954712, loss=1.2817492485046387
I0217 11:07:36.682191 139578127132416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.43443363904953003, loss=1.1752625703811646
I0217 11:08:52.544038 139578135525120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.7344099879264832, loss=1.2791388034820557
I0217 11:10:08.467627 139578127132416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.40739601850509644, loss=1.2183359861373901
I0217 11:11:24.632668 139578135525120 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.5284409523010254, loss=1.2932848930358887
I0217 11:12:43.856097 139578127132416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.48487815260887146, loss=1.2311877012252808
I0217 11:14:09.250203 139578135525120 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.5356289744377136, loss=1.2349711656570435
I0217 11:15:33.789591 139578127132416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.48367181420326233, loss=1.2577732801437378
I0217 11:16:57.873584 139578135525120 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.4698260724544525, loss=1.2419763803482056
I0217 11:18:23.119158 139578127132416 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.4671449661254883, loss=1.2593178749084473
I0217 11:19:46.640977 139578135525120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.6510392427444458, loss=1.3190230131149292
I0217 11:21:08.073631 139578135525120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.5835742950439453, loss=1.3128305673599243
I0217 11:22:23.919796 139578127132416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6149219870567322, loss=1.2671583890914917
I0217 11:23:39.780586 139578135525120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.5234460234642029, loss=1.1840497255325317
I0217 11:24:55.661097 139578127132416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5296379327774048, loss=1.23666250705719
I0217 11:26:11.456635 139578135525120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.46102556586265564, loss=1.27565598487854
I0217 11:27:32.626273 139578127132416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.49551257491111755, loss=1.2114975452423096
I0217 11:28:56.230406 139578135525120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.525708019733429, loss=1.2044869661331177
I0217 11:29:44.524925 139688679413568 spec.py:321] Evaluating on the training split.
I0217 11:30:37.920755 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 11:31:30.108933 139688679413568 spec.py:349] Evaluating on the test split.
I0217 11:31:56.443813 139688679413568 submission_runner.py:408] Time since start: 28452.03s, 	Step: 32659, 	{'train/ctc_loss': Array(0.27391443, dtype=float32), 'train/wer': 0.09894852346449923, 'validation/ctc_loss': Array(0.43026587, dtype=float32), 'validation/wer': 0.12937235100456665, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25159425, dtype=float32), 'test/wer': 0.08392744703755611, 'test/num_examples': 2472, 'score': 25961.47345638275, 'total_duration': 28452.025376319885, 'accumulated_submission_time': 25961.47345638275, 'accumulated_eval_time': 2488.2610454559326, 'accumulated_logging_time': 0.9572756290435791}
I0217 11:31:56.491348 139578135525120 logging_writer.py:48] [32659] accumulated_eval_time=2488.261045, accumulated_logging_time=0.957276, accumulated_submission_time=25961.473456, global_step=32659, preemption_count=0, score=25961.473456, test/ctc_loss=0.2515942454338074, test/num_examples=2472, test/wer=0.083927, total_duration=28452.025376, train/ctc_loss=0.2739144265651703, train/wer=0.098949, validation/ctc_loss=0.430265873670578, validation/num_examples=5348, validation/wer=0.129372
I0217 11:32:28.327266 139578127132416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.5800530314445496, loss=1.2470771074295044
I0217 11:33:44.280397 139578135525120 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.4763343036174774, loss=1.2405000925064087
I0217 11:35:00.194561 139578127132416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.4961070120334625, loss=1.2443143129348755
I0217 11:36:19.613128 139578135525120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5314006805419922, loss=1.244677186012268
I0217 11:37:35.446783 139578127132416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.5329327583312988, loss=1.2341431379318237
I0217 11:38:51.352119 139578135525120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.5318218469619751, loss=1.235060453414917
I0217 11:40:07.212705 139578127132416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.4847412407398224, loss=1.2224878072738647
I0217 11:41:23.074945 139578135525120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.5816342234611511, loss=1.2669745683670044
I0217 11:42:41.461071 139578127132416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.566429615020752, loss=1.181553602218628
I0217 11:44:06.185727 139578135525120 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.5426903367042542, loss=1.2218751907348633
I0217 11:45:29.700861 139578127132416 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.5119836926460266, loss=1.2073785066604614
I0217 11:46:53.225479 139578135525120 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.5198613405227661, loss=1.2018870115280151
I0217 11:48:16.813552 139578127132416 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5519292950630188, loss=1.268206238746643
I0217 11:49:43.126439 139578135525120 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5056847333908081, loss=1.2591400146484375
I0217 11:50:58.932697 139578127132416 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.5712617039680481, loss=1.192894458770752
I0217 11:52:14.742732 139578135525120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6562688946723938, loss=1.232303500175476
I0217 11:53:30.614058 139578127132416 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.482288658618927, loss=1.2069306373596191
I0217 11:54:46.519324 139578135525120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.5377681851387024, loss=1.204599380493164
I0217 11:55:56.817680 139688679413568 spec.py:321] Evaluating on the training split.
I0217 11:56:50.618443 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 11:57:42.562909 139688679413568 spec.py:349] Evaluating on the test split.
I0217 11:58:08.862105 139688679413568 submission_runner.py:408] Time since start: 30024.44s, 	Step: 34494, 	{'train/ctc_loss': Array(0.25215727, dtype=float32), 'train/wer': 0.09286672642144937, 'validation/ctc_loss': Array(0.4266692, dtype=float32), 'validation/wer': 0.12781795186189984, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24414307, dtype=float32), 'test/wer': 0.08283062173745252, 'test/num_examples': 2472, 'score': 27401.712621688843, 'total_duration': 30024.442529678345, 'accumulated_submission_time': 27401.712621688843, 'accumulated_eval_time': 2620.2989869117737, 'accumulated_logging_time': 1.022853136062622}
I0217 11:58:08.901289 139578135525120 logging_writer.py:48] [34494] accumulated_eval_time=2620.298987, accumulated_logging_time=1.022853, accumulated_submission_time=27401.712622, global_step=34494, preemption_count=0, score=27401.712622, test/ctc_loss=0.2441430687904358, test/num_examples=2472, test/wer=0.082831, total_duration=30024.442530, train/ctc_loss=0.2521572709083557, train/wer=0.092867, validation/ctc_loss=0.4266692101955414, validation/num_examples=5348, validation/wer=0.127818
I0217 11:58:14.303152 139578127132416 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.598308801651001, loss=1.2583796977996826
I0217 11:59:30.059216 139578135525120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.5461884140968323, loss=1.1697518825531006
I0217 12:00:45.966637 139578127132416 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.5320553779602051, loss=1.1841191053390503
I0217 12:02:01.722823 139578135525120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.5998250842094421, loss=1.254802942276001
I0217 12:03:20.294478 139578127132416 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.5162684917449951, loss=1.2431693077087402
I0217 12:04:44.930425 139578135525120 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.5244351625442505, loss=1.236757755279541
I0217 12:06:05.296877 139578135525120 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.4903736710548401, loss=1.1654378175735474
I0217 12:07:21.113980 139578127132416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.4797697365283966, loss=1.2286044359207153
I0217 12:08:36.989032 139578135525120 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.47680357098579407, loss=1.2380192279815674
I0217 12:09:52.767802 139578127132416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.5029927492141724, loss=1.180022954940796
I0217 12:11:08.605306 139578135525120 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.4952959418296814, loss=1.2169950008392334
I0217 12:12:31.607026 139578127132416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.5924392938613892, loss=1.2414565086364746
I0217 12:13:55.251929 139578135525120 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.4828560948371887, loss=1.248945713043213
I0217 12:15:19.203258 139578127132416 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.5514246225357056, loss=1.2137093544006348
I0217 12:16:42.797297 139578135525120 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.584746778011322, loss=1.2430603504180908
I0217 12:18:07.607270 139578127132416 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6368086338043213, loss=1.216956377029419
I0217 12:19:31.783915 139578135525120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.542055070400238, loss=1.1855251789093018
I0217 12:20:47.713011 139578127132416 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.47793084383010864, loss=1.1649062633514404
I0217 12:22:03.736351 139578135525120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.4864577353000641, loss=1.2232226133346558
I0217 12:22:09.507661 139688679413568 spec.py:321] Evaluating on the training split.
I0217 12:23:02.641172 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 12:23:54.226695 139688679413568 spec.py:349] Evaluating on the test split.
I0217 12:24:20.306648 139688679413568 submission_runner.py:408] Time since start: 31595.89s, 	Step: 36309, 	{'train/ctc_loss': Array(0.26872647, dtype=float32), 'train/wer': 0.09206013506330743, 'validation/ctc_loss': Array(0.41945672, dtype=float32), 'validation/wer': 0.124429168637825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24109034, dtype=float32), 'test/wer': 0.08019011638535128, 'test/num_examples': 2472, 'score': 28842.233000040054, 'total_duration': 31595.887142896652, 'accumulated_submission_time': 28842.233000040054, 'accumulated_eval_time': 2751.0915093421936, 'accumulated_logging_time': 1.0802249908447266}
I0217 12:24:20.343980 139578135525120 logging_writer.py:48] [36309] accumulated_eval_time=2751.091509, accumulated_logging_time=1.080225, accumulated_submission_time=28842.233000, global_step=36309, preemption_count=0, score=28842.233000, test/ctc_loss=0.2410903424024582, test/num_examples=2472, test/wer=0.080190, total_duration=31595.887143, train/ctc_loss=0.2687264680862427, train/wer=0.092060, validation/ctc_loss=0.41945672035217285, validation/num_examples=5348, validation/wer=0.124429
I0217 12:25:30.139865 139578127132416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.5135630965232849, loss=1.1846213340759277
I0217 12:26:45.999523 139578135525120 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.5946385860443115, loss=1.1920968294143677
I0217 12:28:01.905819 139578127132416 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.540247917175293, loss=1.1788352727890015
I0217 12:29:17.711341 139578135525120 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.46695515513420105, loss=1.2514313459396362
I0217 12:30:41.190522 139578127132416 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6368115544319153, loss=1.2376044988632202
I0217 12:32:03.610666 139578135525120 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.5167131423950195, loss=1.1998332738876343
I0217 12:33:27.463089 139578127132416 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.8652707934379578, loss=1.191465973854065
I0217 12:34:53.770299 139578135525120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.4897429049015045, loss=1.1978282928466797
I0217 12:36:09.506235 139578127132416 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.49080556631088257, loss=1.1495410203933716
I0217 12:37:25.541776 139578135525120 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.5181613564491272, loss=1.2096199989318848
I0217 12:38:41.376182 139578127132416 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6148675084114075, loss=1.1758533716201782
I0217 12:39:57.244489 139578135525120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.5060881972312927, loss=1.226586937904358
I0217 12:41:16.017548 139578127132416 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.5124387145042419, loss=1.1864452362060547
I0217 12:42:39.879348 139578135525120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.4930115044116974, loss=1.1985315084457397
I0217 12:44:04.411644 139578127132416 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6276282668113708, loss=1.255409598350525
I0217 12:45:28.935927 139578135525120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6078837513923645, loss=1.1520041227340698
I0217 12:46:53.098160 139578127132416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.488509863615036, loss=1.159125566482544
I0217 12:48:16.452516 139578135525120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.5446634292602539, loss=1.251490831375122
I0217 12:48:20.733682 139688679413568 spec.py:321] Evaluating on the training split.
I0217 12:49:13.547841 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 12:50:04.583966 139688679413568 spec.py:349] Evaluating on the test split.
I0217 12:50:30.902169 139688679413568 submission_runner.py:408] Time since start: 33166.48s, 	Step: 38107, 	{'train/ctc_loss': Array(0.21433543, dtype=float32), 'train/wer': 0.07941517525051416, 'validation/ctc_loss': Array(0.40897462, dtype=float32), 'validation/wer': 0.12077005512806897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23581171, dtype=float32), 'test/wer': 0.0775699226128816, 'test/num_examples': 2472, 'score': 30282.535950660706, 'total_duration': 33166.483761548996, 'accumulated_submission_time': 30282.535950660706, 'accumulated_eval_time': 2881.2546286582947, 'accumulated_logging_time': 1.1364960670471191}
I0217 12:50:30.939264 139578135525120 logging_writer.py:48] [38107] accumulated_eval_time=2881.254629, accumulated_logging_time=1.136496, accumulated_submission_time=30282.535951, global_step=38107, preemption_count=0, score=30282.535951, test/ctc_loss=0.23581171035766602, test/num_examples=2472, test/wer=0.077570, total_duration=33166.483762, train/ctc_loss=0.21433542668819427, train/wer=0.079415, validation/ctc_loss=0.40897461771965027, validation/num_examples=5348, validation/wer=0.120770
I0217 12:51:45.780207 139578135525120 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.5235051512718201, loss=1.1457427740097046
I0217 12:53:01.600865 139578127132416 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.5418875813484192, loss=1.2584229707717896
I0217 12:54:17.548803 139578135525120 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6165533065795898, loss=1.1940052509307861
I0217 12:55:33.667541 139578127132416 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7172096967697144, loss=1.1788616180419922
I0217 12:56:49.607435 139578135525120 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.46806636452674866, loss=1.1783716678619385
I0217 12:58:12.537826 139578127132416 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.5495634078979492, loss=1.1717687845230103
I0217 12:59:35.976731 139578135525120 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.6153435111045837, loss=1.1358237266540527
I0217 13:01:00.801744 139578127132416 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.620308518409729, loss=1.1986374855041504
I0217 13:02:25.223127 139578135525120 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.5348926782608032, loss=1.2143464088439941
I0217 13:03:48.566984 139578127132416 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6418597102165222, loss=1.2353501319885254
I0217 13:05:10.594685 139578135525120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.5845744609832764, loss=1.1722557544708252
I0217 13:06:26.455622 139578127132416 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.5315561890602112, loss=1.1663185358047485
I0217 13:07:42.396784 139578135525120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.5281386971473694, loss=1.1930091381072998
I0217 13:08:58.366050 139578127132416 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.5375326871871948, loss=1.1786179542541504
I0217 13:10:14.341195 139578135525120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.6137164235115051, loss=1.1571310758590698
I0217 13:11:35.483341 139578127132416 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.6925607919692993, loss=1.1765996217727661
I0217 13:13:00.258941 139578135525120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.4849419891834259, loss=1.1646337509155273
I0217 13:14:24.213744 139578127132416 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.5429470539093018, loss=1.151126742362976
I0217 13:14:31.270797 139688679413568 spec.py:321] Evaluating on the training split.
I0217 13:15:25.475748 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 13:16:17.382854 139688679413568 spec.py:349] Evaluating on the test split.
I0217 13:16:43.739472 139688679413568 submission_runner.py:408] Time since start: 34739.32s, 	Step: 39910, 	{'train/ctc_loss': Array(0.19636467, dtype=float32), 'train/wer': 0.07304349756373686, 'validation/ctc_loss': Array(0.40372702, dtype=float32), 'validation/wer': 0.11818260810797764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2262993, dtype=float32), 'test/wer': 0.07432006987183393, 'test/num_examples': 2472, 'score': 31722.782329320908, 'total_duration': 34739.318687200546, 'accumulated_submission_time': 31722.782329320908, 'accumulated_eval_time': 3013.71555685997, 'accumulated_logging_time': 1.1902563571929932}
I0217 13:16:43.781624 139578135525120 logging_writer.py:48] [39910] accumulated_eval_time=3013.715557, accumulated_logging_time=1.190256, accumulated_submission_time=31722.782329, global_step=39910, preemption_count=0, score=31722.782329, test/ctc_loss=0.22629930078983307, test/num_examples=2472, test/wer=0.074320, total_duration=34739.318687, train/ctc_loss=0.19636467099189758, train/wer=0.073043, validation/ctc_loss=0.4037270247936249, validation/num_examples=5348, validation/wer=0.118183
I0217 13:17:52.832736 139578127132416 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.5873673558235168, loss=1.1433738470077515
I0217 13:19:08.765082 139578135525120 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.6411404609680176, loss=1.209756851196289
I0217 13:20:28.213355 139578135525120 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.5980905890464783, loss=1.1740773916244507
I0217 13:21:43.993094 139578127132416 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.5801392197608948, loss=1.1996495723724365
I0217 13:22:59.965230 139578135525120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.564404308795929, loss=1.1160746812820435
I0217 13:24:15.887784 139578127132416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7118626832962036, loss=1.148680567741394
I0217 13:25:31.803623 139578135525120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6655263304710388, loss=1.132719874382019
I0217 13:26:47.726919 139578127132416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.48570719361305237, loss=1.1571203470230103
I0217 13:28:11.151185 139578135525120 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.585376501083374, loss=1.2140822410583496
I0217 13:29:34.379176 139578127132416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.5520669221878052, loss=1.1618083715438843
I0217 13:30:58.521284 139578135525120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.47925713658332825, loss=1.169169545173645
I0217 13:32:22.845838 139578127132416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5349485278129578, loss=1.1604537963867188
I0217 13:33:49.873852 139578135525120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.5339899659156799, loss=1.1715307235717773
I0217 13:35:05.815866 139578127132416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.5737050771713257, loss=1.1650851964950562
I0217 13:36:21.713205 139578135525120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.5883568525314331, loss=1.2279518842697144
I0217 13:37:37.687389 139578127132416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6389907002449036, loss=1.1613426208496094
I0217 13:38:53.612354 139578135525120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.5302875638008118, loss=1.142281413078308
I0217 13:40:09.571314 139578127132416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7418814897537231, loss=1.1864806413650513
I0217 13:40:44.169293 139688679413568 spec.py:321] Evaluating on the training split.
I0217 13:41:36.834007 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 13:42:28.179784 139688679413568 spec.py:349] Evaluating on the test split.
I0217 13:42:54.833483 139688679413568 submission_runner.py:408] Time since start: 36310.41s, 	Step: 41747, 	{'train/ctc_loss': Array(0.21572936, dtype=float32), 'train/wer': 0.07966783665782039, 'validation/ctc_loss': Array(0.395749, dtype=float32), 'validation/wer': 0.11628064145514931, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21935742, dtype=float32), 'test/wer': 0.07165925294010116, 'test/num_examples': 2472, 'score': 33163.08148312569, 'total_duration': 36310.41373419762, 'accumulated_submission_time': 33163.08148312569, 'accumulated_eval_time': 3144.3730943202972, 'accumulated_logging_time': 1.2504262924194336}
I0217 13:42:54.878640 139578135525120 logging_writer.py:48] [41747] accumulated_eval_time=3144.373094, accumulated_logging_time=1.250426, accumulated_submission_time=33163.081483, global_step=41747, preemption_count=0, score=33163.081483, test/ctc_loss=0.2193574160337448, test/num_examples=2472, test/wer=0.071659, total_duration=36310.413734, train/ctc_loss=0.21572935581207275, train/wer=0.079668, validation/ctc_loss=0.3957490026950836, validation/num_examples=5348, validation/wer=0.116281
I0217 13:43:35.780717 139578127132416 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.6535465717315674, loss=1.1892337799072266
I0217 13:44:51.502113 139578135525120 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.571891188621521, loss=1.1273808479309082
I0217 13:46:07.234560 139578127132416 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.5576782822608948, loss=1.1551674604415894
I0217 13:47:23.349755 139578135525120 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.7601202130317688, loss=1.1479319334030151
I0217 13:48:47.940080 139578127132416 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.7016607522964478, loss=1.205509901046753
I0217 13:50:09.554945 139578135525120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.5050914883613586, loss=1.1403025388717651
I0217 13:51:25.422951 139578127132416 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.5984331965446472, loss=1.166133999824524
I0217 13:52:41.322764 139578135525120 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.5924785733222961, loss=1.1378886699676514
I0217 13:53:57.248693 139578127132416 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.49878203868865967, loss=1.1244734525680542
I0217 13:55:13.267031 139578135525120 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.48591333627700806, loss=1.1870253086090088
I0217 13:56:29.161787 139578127132416 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.587327241897583, loss=1.1441010236740112
I0217 13:57:48.451514 139578135525120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.5322867035865784, loss=1.1482805013656616
I0217 13:59:13.040803 139578127132416 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.5298933386802673, loss=1.1820359230041504
I0217 14:00:36.873269 139578135525120 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.6921399831771851, loss=1.1536206007003784
I0217 14:02:00.612431 139578127132416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.5654476284980774, loss=1.115536093711853
I0217 14:03:24.577617 139578135525120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.5862684845924377, loss=1.1226223707199097
I0217 14:04:40.348024 139578127132416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.4936179518699646, loss=1.1084344387054443
I0217 14:05:56.280203 139578135525120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.5564396977424622, loss=1.1177748441696167
I0217 14:06:55.020681 139688679413568 spec.py:321] Evaluating on the training split.
I0217 14:07:56.879585 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 14:08:49.136221 139688679413568 spec.py:349] Evaluating on the test split.
I0217 14:09:15.597158 139688679413568 submission_runner.py:408] Time since start: 37891.18s, 	Step: 43579, 	{'train/ctc_loss': Array(0.1478013, dtype=float32), 'train/wer': 0.0551468587783352, 'validation/ctc_loss': Array(0.38760498, dtype=float32), 'validation/wer': 0.11348079206773705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21774586, dtype=float32), 'test/wer': 0.07263420876241546, 'test/num_examples': 2472, 'score': 34603.13644838333, 'total_duration': 37891.17783498764, 'accumulated_submission_time': 34603.13644838333, 'accumulated_eval_time': 3284.943300962448, 'accumulated_logging_time': 1.312373399734497}
I0217 14:09:15.637552 139578135525120 logging_writer.py:48] [43579] accumulated_eval_time=3284.943301, accumulated_logging_time=1.312373, accumulated_submission_time=34603.136448, global_step=43579, preemption_count=0, score=34603.136448, test/ctc_loss=0.2177458554506302, test/num_examples=2472, test/wer=0.072634, total_duration=37891.177835, train/ctc_loss=0.14780129492282867, train/wer=0.055147, validation/ctc_loss=0.3876049816608429, validation/num_examples=5348, validation/wer=0.113481
I0217 14:09:32.353039 139578127132416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.5722181797027588, loss=1.0621886253356934
I0217 14:10:48.205022 139578135525120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.49926742911338806, loss=1.0827325582504272
I0217 14:12:04.031237 139578127132416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.4565226435661316, loss=1.08524489402771
I0217 14:13:19.781361 139578135525120 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.5469090938568115, loss=1.169942021369934
I0217 14:14:35.578839 139578127132416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.5576090216636658, loss=1.1205801963806152
I0217 14:15:57.185874 139578135525120 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.5762176513671875, loss=1.1488754749298096
I0217 14:17:20.769080 139578127132416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.5320826768875122, loss=1.1449490785598755
I0217 14:18:47.023429 139578135525120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.5965889692306519, loss=1.1006730794906616
I0217 14:20:02.933594 139578127132416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.49296700954437256, loss=1.125722050666809
I0217 14:21:19.075893 139578135525120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.6209067106246948, loss=1.1386767625808716
I0217 14:22:35.086598 139578127132416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.638249933719635, loss=1.1505286693572998
I0217 14:23:50.889432 139578135525120 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.5188897848129272, loss=1.1630803346633911
I0217 14:25:06.848175 139578127132416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.4927102327346802, loss=1.0514707565307617
I0217 14:26:28.606087 139578135525120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.5270320177078247, loss=1.1819289922714233
I0217 14:27:51.357230 139578127132416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.5463258028030396, loss=1.109634280204773
I0217 14:29:15.686572 139578135525120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.555452823638916, loss=1.164624810218811
I0217 14:30:39.950703 139578127132416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.5911754369735718, loss=1.1126196384429932
I0217 14:32:03.223719 139578135525120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.5555738210678101, loss=1.1260403394699097
I0217 14:33:15.996339 139688679413568 spec.py:321] Evaluating on the training split.
I0217 14:34:11.395211 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 14:35:03.875374 139688679413568 spec.py:349] Evaluating on the test split.
I0217 14:35:30.266027 139688679413568 submission_runner.py:408] Time since start: 39465.85s, 	Step: 45391, 	{'train/ctc_loss': Array(0.12686214, dtype=float32), 'train/wer': 0.047560816963704955, 'validation/ctc_loss': Array(0.37629935, dtype=float32), 'validation/wer': 0.11128918582310744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21235178, dtype=float32), 'test/wer': 0.07042024658257673, 'test/num_examples': 2472, 'score': 36043.40504741669, 'total_duration': 39465.84740304947, 'accumulated_submission_time': 36043.40504741669, 'accumulated_eval_time': 3419.2076370716095, 'accumulated_logging_time': 1.3747718334197998}
I0217 14:35:30.307027 139578135525120 logging_writer.py:48] [45391] accumulated_eval_time=3419.207637, accumulated_logging_time=1.374772, accumulated_submission_time=36043.405047, global_step=45391, preemption_count=0, score=36043.405047, test/ctc_loss=0.21235178411006927, test/num_examples=2472, test/wer=0.070420, total_duration=39465.847403, train/ctc_loss=0.12686213850975037, train/wer=0.047561, validation/ctc_loss=0.37629935145378113, validation/num_examples=5348, validation/wer=0.111289
I0217 14:35:37.970345 139578127132416 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.651025116443634, loss=1.1200158596038818
I0217 14:36:53.795859 139578135525120 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.482212096452713, loss=1.1194934844970703
I0217 14:38:09.679653 139578127132416 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.5569794178009033, loss=1.1090654134750366
I0217 14:39:25.726736 139578135525120 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.5881863832473755, loss=1.1548388004302979
I0217 14:40:41.607414 139578127132416 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.585782527923584, loss=1.1234676837921143
I0217 14:41:57.483698 139578135525120 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.5521770119667053, loss=1.082607626914978
I0217 14:43:14.185940 139578127132416 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6248965859413147, loss=1.1601964235305786
I0217 14:44:37.771913 139578135525120 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.5779098868370056, loss=1.0983752012252808
I0217 14:46:01.850864 139578127132416 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.596764326095581, loss=1.0960105657577515
I0217 14:47:25.582743 139578135525120 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.5550046563148499, loss=1.1346936225891113
I0217 14:48:48.407058 139578135525120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.582953691482544, loss=1.1426950693130493
I0217 14:50:04.317847 139578127132416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6794893741607666, loss=1.093126654624939
I0217 14:51:20.214999 139578135525120 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.6833388209342957, loss=1.0888843536376953
I0217 14:52:36.262487 139578127132416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.5708667039871216, loss=1.0782033205032349
I0217 14:53:52.206209 139578135525120 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.6906957030296326, loss=1.1702207326889038
I0217 14:55:10.577033 139578127132416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.6224761009216309, loss=1.0900447368621826
I0217 14:56:34.828076 139578135525120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.6115111708641052, loss=1.1415587663650513
I0217 14:57:59.294985 139578127132416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.7146157026290894, loss=1.080020785331726
I0217 14:59:23.063992 139578135525120 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.5670396685600281, loss=1.1008193492889404
I0217 14:59:31.014618 139688679413568 spec.py:321] Evaluating on the training split.
I0217 15:00:27.215106 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 15:01:19.735766 139688679413568 spec.py:349] Evaluating on the test split.
I0217 15:01:46.598042 139688679413568 submission_runner.py:408] Time since start: 41042.18s, 	Step: 47211, 	{'train/ctc_loss': Array(0.1242431, dtype=float32), 'train/wer': 0.0480329439796949, 'validation/ctc_loss': Array(0.3734636, dtype=float32), 'validation/wer': 0.10818038753777383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20668186, dtype=float32), 'test/wer': 0.06869376231389515, 'test/num_examples': 2472, 'score': 37484.02665233612, 'total_duration': 41042.178755521774, 'accumulated_submission_time': 37484.02665233612, 'accumulated_eval_time': 3554.784821510315, 'accumulated_logging_time': 1.4327740669250488}
I0217 15:01:46.640242 139578135525120 logging_writer.py:48] [47211] accumulated_eval_time=3554.784822, accumulated_logging_time=1.432774, accumulated_submission_time=37484.026652, global_step=47211, preemption_count=0, score=37484.026652, test/ctc_loss=0.20668186247348785, test/num_examples=2472, test/wer=0.068694, total_duration=41042.178756, train/ctc_loss=0.1242431029677391, train/wer=0.048033, validation/ctc_loss=0.37346360087394714, validation/num_examples=5348, validation/wer=0.108180
I0217 15:02:54.707445 139578127132416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.5640987753868103, loss=1.0971789360046387
I0217 15:04:14.493962 139578135525120 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.5010685324668884, loss=1.1175755262374878
I0217 15:05:30.540176 139578127132416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.5342956781387329, loss=1.1619566679000854
I0217 15:06:46.514505 139578135525120 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.5506643652915955, loss=1.1028386354446411
I0217 15:08:02.589112 139578127132416 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.5901451706886292, loss=1.083254098892212
I0217 15:09:18.598970 139578135525120 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.5293154716491699, loss=1.0835697650909424
I0217 15:10:40.394987 139578127132416 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6119897365570068, loss=1.1407524347305298
I0217 15:12:04.620620 139578135525120 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.6218817234039307, loss=1.0968968868255615
I0217 15:13:29.125032 139578127132416 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.584724485874176, loss=1.0942517518997192
I0217 15:14:52.703303 139578135525120 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6515476107597351, loss=1.0888004302978516
I0217 15:16:16.288368 139578127132416 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.5576157569885254, loss=1.125441074371338
I0217 15:17:39.988034 139578135525120 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7100645899772644, loss=1.1044864654541016
I0217 15:18:59.799908 139578135525120 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.561042070388794, loss=1.1180657148361206
I0217 15:20:15.895207 139578127132416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.5310185551643372, loss=1.0982352495193481
I0217 15:21:31.831716 139578135525120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.5807228088378906, loss=1.0446903705596924
I0217 15:22:47.793333 139578127132416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.7438472509384155, loss=1.0752686262130737
I0217 15:24:03.848610 139578135525120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.5288451313972473, loss=1.0957880020141602
I0217 15:25:27.262183 139578127132416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.55168616771698, loss=1.0942546129226685
I0217 15:25:47.190315 139688679413568 spec.py:321] Evaluating on the training split.
I0217 15:26:43.294337 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 15:27:35.023768 139688679413568 spec.py:349] Evaluating on the test split.
I0217 15:28:01.454172 139688679413568 submission_runner.py:408] Time since start: 42617.04s, 	Step: 49025, 	{'train/ctc_loss': Array(0.12451019, dtype=float32), 'train/wer': 0.04706751197491119, 'validation/ctc_loss': Array(0.36440134, dtype=float32), 'validation/wer': 0.10629773019106559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20458083, dtype=float32), 'test/wer': 0.06708914752300286, 'test/num_examples': 2472, 'score': 38924.491559267044, 'total_duration': 42617.03544545174, 'accumulated_submission_time': 38924.491559267044, 'accumulated_eval_time': 3689.0430438518524, 'accumulated_logging_time': 1.4914908409118652}
I0217 15:28:01.495015 139578135525120 logging_writer.py:48] [49025] accumulated_eval_time=3689.043044, accumulated_logging_time=1.491491, accumulated_submission_time=38924.491559, global_step=49025, preemption_count=0, score=38924.491559, test/ctc_loss=0.20458082854747772, test/num_examples=2472, test/wer=0.067089, total_duration=42617.035445, train/ctc_loss=0.12451019138097763, train/wer=0.047068, validation/ctc_loss=0.36440134048461914, validation/num_examples=5348, validation/wer=0.106298
I0217 15:28:59.096853 139578127132416 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.6253622770309448, loss=1.0974007844924927
I0217 15:30:14.897724 139578135525120 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.5452110767364502, loss=1.0924626588821411
I0217 15:31:30.969315 139578127132416 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.5204394459724426, loss=1.0866458415985107
I0217 15:32:46.660941 139578135525120 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.6489649415016174, loss=1.081831455230713
I0217 15:34:07.151234 139578135525120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.6052124500274658, loss=1.0356353521347046
I0217 15:35:22.860162 139578127132416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6343790888786316, loss=1.0970152616500854
I0217 15:36:38.676202 139578135525120 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.5840280652046204, loss=1.0848060846328735
I0217 15:37:54.562836 139578127132416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.5736133456230164, loss=1.0432496070861816
I0217 15:39:10.491856 139578135525120 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.5341042876243591, loss=1.0739467144012451
I0217 15:40:30.858207 139578127132416 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.6589285731315613, loss=1.0909706354141235
I0217 15:41:55.741422 139578135525120 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.5956148505210876, loss=1.0870527029037476
I0217 15:43:19.788300 139578127132416 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.5545997023582458, loss=1.0810632705688477
I0217 15:44:43.257022 139578135525120 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.5905966758728027, loss=1.0532745122909546
I0217 15:46:07.397494 139578127132416 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.5663695931434631, loss=1.0953679084777832
I0217 15:47:32.583966 139578135525120 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.5285264849662781, loss=1.0529507398605347
I0217 15:48:48.368295 139578127132416 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6040153503417969, loss=1.0766938924789429
I0217 15:50:04.364603 139578135525120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8886404633522034, loss=1.0931459665298462
I0217 15:51:20.230187 139578127132416 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.5254339575767517, loss=1.0318940877914429
I0217 15:52:01.628906 139688679413568 spec.py:321] Evaluating on the training split.
I0217 15:52:55.464930 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 15:53:46.864740 139688679413568 spec.py:349] Evaluating on the test split.
I0217 15:54:13.195032 139688679413568 submission_runner.py:408] Time since start: 44188.78s, 	Step: 50856, 	{'train/ctc_loss': Array(0.11252448, dtype=float32), 'train/wer': 0.043614796043756794, 'validation/ctc_loss': Array(0.35983658, dtype=float32), 'validation/wer': 0.10531295557894127, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1984191, dtype=float32), 'test/wer': 0.06585014116547844, 'test/num_examples': 2472, 'score': 40364.53780937195, 'total_duration': 44188.775580883026, 'accumulated_submission_time': 40364.53780937195, 'accumulated_eval_time': 3820.602771282196, 'accumulated_logging_time': 1.5500528812408447}
I0217 15:54:13.234266 139578135525120 logging_writer.py:48] [50856] accumulated_eval_time=3820.602771, accumulated_logging_time=1.550053, accumulated_submission_time=40364.537809, global_step=50856, preemption_count=0, score=40364.537809, test/ctc_loss=0.19841909408569336, test/num_examples=2472, test/wer=0.065850, total_duration=44188.775581, train/ctc_loss=0.11252447962760925, train/wer=0.043615, validation/ctc_loss=0.3598365783691406, validation/num_examples=5348, validation/wer=0.105313
I0217 15:54:47.453375 139578127132416 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.5355162620544434, loss=1.048874020576477
I0217 15:56:03.371347 139578135525120 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.5642274022102356, loss=1.0690470933914185
I0217 15:57:19.311984 139578127132416 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.5323924422264099, loss=1.1027613878250122
I0217 15:58:35.296349 139578135525120 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.602213978767395, loss=1.1235921382904053
I0217 15:59:55.376568 139578127132416 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.7102248072624207, loss=1.0828561782836914
I0217 16:01:19.407173 139578135525120 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.6020522713661194, loss=1.092512845993042
I0217 16:02:46.304316 139578135525120 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.6770331263542175, loss=1.058815598487854
I0217 16:04:02.202440 139578127132416 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.7088538408279419, loss=1.0554403066635132
I0217 16:05:18.080273 139578135525120 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.6218950748443604, loss=1.0996440649032593
I0217 16:06:34.257510 139578127132416 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.735012412071228, loss=1.0509636402130127
I0217 16:07:50.137799 139578135525120 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.6445478796958923, loss=1.0727208852767944
I0217 16:09:06.054952 139578127132416 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.604233980178833, loss=1.0454356670379639
I0217 16:10:29.077181 139578135525120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.606495201587677, loss=1.0463215112686157
I0217 16:11:52.550037 139578127132416 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.605426013469696, loss=0.9893454313278198
I0217 16:13:17.068259 139578135525120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.522003173828125, loss=1.0623444318771362
I0217 16:14:40.258596 139578127132416 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.6528822779655457, loss=1.070818305015564
I0217 16:16:03.672751 139578135525120 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.5417214035987854, loss=1.0762156248092651
I0217 16:17:25.392590 139578135525120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6439670324325562, loss=1.0402889251708984
I0217 16:18:13.535471 139688679413568 spec.py:321] Evaluating on the training split.
I0217 16:19:09.245868 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 16:20:01.771423 139688679413568 spec.py:349] Evaluating on the test split.
I0217 16:20:27.802444 139688679413568 submission_runner.py:408] Time since start: 45763.38s, 	Step: 52665, 	{'train/ctc_loss': Array(0.10154488, dtype=float32), 'train/wer': 0.0393358401807406, 'validation/ctc_loss': Array(0.34599328, dtype=float32), 'validation/wer': 0.10092974308968207, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19205548, dtype=float32), 'test/wer': 0.06300652001706172, 'test/num_examples': 2472, 'score': 41804.75467848778, 'total_duration': 45763.384612083435, 'accumulated_submission_time': 41804.75467848778, 'accumulated_eval_time': 3954.864999771118, 'accumulated_logging_time': 1.6051020622253418}
I0217 16:20:27.839884 139578135525120 logging_writer.py:48] [52665] accumulated_eval_time=3954.865000, accumulated_logging_time=1.605102, accumulated_submission_time=41804.754678, global_step=52665, preemption_count=0, score=41804.754678, test/ctc_loss=0.19205547869205475, test/num_examples=2472, test/wer=0.063007, total_duration=45763.384612, train/ctc_loss=0.10154487937688828, train/wer=0.039336, validation/ctc_loss=0.3459932804107666, validation/num_examples=5348, validation/wer=0.100930
I0217 16:20:55.112614 139578127132416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.5391070246696472, loss=1.0832356214523315
I0217 16:22:11.051453 139578135525120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.5624016523361206, loss=1.0369479656219482
I0217 16:23:26.964704 139578127132416 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.5943089723587036, loss=1.0607655048370361
I0217 16:24:43.077178 139578135525120 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.5860028862953186, loss=1.0726170539855957
I0217 16:25:58.954790 139578127132416 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.6636731624603271, loss=1.085189700126648
I0217 16:27:15.946908 139578135525120 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.7217512726783752, loss=1.0869938135147095
I0217 16:28:40.467732 139578127132416 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.5446537733078003, loss=1.0850404500961304
I0217 16:30:05.682567 139578135525120 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.6127607822418213, loss=1.0482633113861084
I0217 16:31:29.678397 139578127132416 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.5235029458999634, loss=1.0023925304412842
I0217 16:32:54.181156 139578135525120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.577939510345459, loss=1.0330593585968018
I0217 16:34:10.049162 139578127132416 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.5503522753715515, loss=1.0364476442337036
I0217 16:35:26.080803 139578135525120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.5755447149276733, loss=1.0078966617584229
I0217 16:36:41.919252 139578127132416 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.70843505859375, loss=1.0151523351669312
I0217 16:37:57.954466 139578135525120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.6388468742370605, loss=1.008660078048706
I0217 16:39:17.271798 139578127132416 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.7118611335754395, loss=1.022549033164978
I0217 16:40:41.562036 139578135525120 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.7204797863960266, loss=1.031939148902893
I0217 16:42:06.018870 139578127132416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.4980201721191406, loss=1.0429906845092773
I0217 16:43:29.325899 139578135525120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.5955080986022949, loss=0.9940517544746399
I0217 16:44:28.180907 139688679413568 spec.py:321] Evaluating on the training split.
I0217 16:45:22.684326 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 16:46:14.437371 139688679413568 spec.py:349] Evaluating on the test split.
I0217 16:46:41.117124 139688679413568 submission_runner.py:408] Time since start: 47336.70s, 	Step: 54472, 	{'train/ctc_loss': Array(0.1160069, dtype=float32), 'train/wer': 0.04412523748338989, 'validation/ctc_loss': Array(0.3398731, dtype=float32), 'validation/wer': 0.10018633480405882, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18795243, dtype=float32), 'test/wer': 0.06219405683179981, 'test/num_examples': 2472, 'score': 43245.01002693176, 'total_duration': 47336.69872045517, 'accumulated_submission_time': 43245.01002693176, 'accumulated_eval_time': 4087.795857667923, 'accumulated_logging_time': 1.659766435623169}
I0217 16:46:41.161852 139578135525120 logging_writer.py:48] [54472] accumulated_eval_time=4087.795858, accumulated_logging_time=1.659766, accumulated_submission_time=43245.010027, global_step=54472, preemption_count=0, score=43245.010027, test/ctc_loss=0.1879524290561676, test/num_examples=2472, test/wer=0.062194, total_duration=47336.698720, train/ctc_loss=0.11600689589977264, train/wer=0.044125, validation/ctc_loss=0.3398731052875519, validation/num_examples=5348, validation/wer=0.100186
I0217 16:47:03.219027 139578127132416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.6099982261657715, loss=1.0834295749664307
I0217 16:48:22.540304 139578135525120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.6157613396644592, loss=0.9882208108901978
I0217 16:49:38.316040 139578127132416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.6572960615158081, loss=1.0506855249404907
I0217 16:50:54.158313 139578135525120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.6067833304405212, loss=1.0348358154296875
I0217 16:52:10.041857 139578127132416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.6254554390907288, loss=1.0131562948226929
I0217 16:53:25.906244 139578135525120 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.6281011700630188, loss=1.0336949825286865
I0217 16:54:43.820652 139578127132416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.6569781303405762, loss=0.9950699210166931
I0217 16:56:08.206583 139578135525120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.6366746425628662, loss=1.021566390991211
I0217 16:57:33.498499 139578127132416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.6360647678375244, loss=1.0389124155044556
I0217 16:58:57.491031 139578135525120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.6393767595291138, loss=1.022222638130188
I0217 17:00:21.901608 139578127132416 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.7352381944656372, loss=1.072119116783142
I0217 17:01:46.145547 139578135525120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.6924561262130737, loss=1.0514297485351562
I0217 17:03:06.544435 139578135525120 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.7585145235061646, loss=1.0345922708511353
I0217 17:04:22.398484 139578127132416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.6188052296638489, loss=1.047181248664856
I0217 17:05:38.223076 139578135525120 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.7108914852142334, loss=1.0598644018173218
I0217 17:06:54.087541 139578127132416 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.6299309730529785, loss=1.0359928607940674
I0217 17:08:09.956785 139578135525120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.6049312949180603, loss=0.9681507349014282
I0217 17:09:30.712601 139578127132416 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.6073002815246582, loss=1.0388821363449097
I0217 17:10:41.298916 139688679413568 spec.py:321] Evaluating on the training split.
I0217 17:11:36.980955 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 17:12:28.999143 139688679413568 spec.py:349] Evaluating on the test split.
I0217 17:12:55.306891 139688679413568 submission_runner.py:408] Time since start: 48910.89s, 	Step: 56285, 	{'train/ctc_loss': Array(0.10221567, dtype=float32), 'train/wer': 0.03673829051718295, 'validation/ctc_loss': Array(0.3412774, dtype=float32), 'validation/wer': 0.09750234125336706, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1826138, dtype=float32), 'test/wer': 0.058761399874068206, 'test/num_examples': 2472, 'score': 44685.059356451035, 'total_duration': 48910.888063669205, 'accumulated_submission_time': 44685.059356451035, 'accumulated_eval_time': 4221.798095703125, 'accumulated_logging_time': 1.7223410606384277}
I0217 17:12:55.345639 139578135525120 logging_writer.py:48] [56285] accumulated_eval_time=4221.798096, accumulated_logging_time=1.722341, accumulated_submission_time=44685.059356, global_step=56285, preemption_count=0, score=44685.059356, test/ctc_loss=0.182613804936409, test/num_examples=2472, test/wer=0.058761, total_duration=48910.888064, train/ctc_loss=0.10221567004919052, train/wer=0.036738, validation/ctc_loss=0.3412773907184601, validation/num_examples=5348, validation/wer=0.097502
I0217 17:13:07.509759 139578127132416 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.7458672523498535, loss=1.0478554964065552
I0217 17:14:23.221332 139578135525120 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.5678143501281738, loss=1.0360057353973389
I0217 17:15:39.148767 139578127132416 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.6805601119995117, loss=1.0464277267456055
I0217 17:16:55.353583 139578135525120 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.6672350168228149, loss=1.042062759399414
I0217 17:18:14.764322 139578135525120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.6796938180923462, loss=1.0029228925704956
I0217 17:19:30.615411 139578127132416 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.6638902425765991, loss=0.9799044132232666
I0217 17:20:46.577572 139578135525120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7079634070396423, loss=1.0148630142211914
I0217 17:22:02.416846 139578127132416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.5541242361068726, loss=1.022045612335205
I0217 17:23:18.385212 139578135525120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.6492611169815063, loss=1.0111699104309082
I0217 17:24:37.786865 139578127132416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.652844250202179, loss=0.9790264964103699
I0217 17:26:01.831433 139578135525120 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.7460411190986633, loss=1.023500680923462
I0217 17:27:26.037435 139578127132416 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.5388744473457336, loss=0.9392023682594299
I0217 17:28:49.285307 139578135525120 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.6336337327957153, loss=1.0233638286590576
I0217 17:30:14.616609 139578127132416 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.7363585829734802, loss=1.0377750396728516
I0217 17:31:40.862419 139578135525120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.6058363914489746, loss=0.9798493385314941
I0217 17:32:57.115759 139578127132416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.618764340877533, loss=0.999239981174469
I0217 17:34:13.125504 139578135525120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.5884580612182617, loss=0.9833586812019348
I0217 17:35:29.264303 139578127132416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.727938711643219, loss=1.0001623630523682
I0217 17:36:45.254189 139578135525120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.6387104392051697, loss=1.0098304748535156
I0217 17:36:55.596509 139688679413568 spec.py:321] Evaluating on the training split.
I0217 17:37:50.858518 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 17:38:43.716618 139688679413568 spec.py:349] Evaluating on the test split.
I0217 17:39:10.160096 139688679413568 submission_runner.py:408] Time since start: 50485.74s, 	Step: 58115, 	{'train/ctc_loss': Array(0.09541807, dtype=float32), 'train/wer': 0.03673023652248029, 'validation/ctc_loss': Array(0.3292542, dtype=float32), 'validation/wer': 0.09447078019251379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18172288, dtype=float32), 'test/wer': 0.05920825462596226, 'test/num_examples': 2472, 'score': 46125.22264838219, 'total_duration': 50485.74221920967, 'accumulated_submission_time': 46125.22264838219, 'accumulated_eval_time': 4356.356876373291, 'accumulated_logging_time': 1.7787425518035889}
I0217 17:39:10.203277 139578135525120 logging_writer.py:48] [58115] accumulated_eval_time=4356.356876, accumulated_logging_time=1.778743, accumulated_submission_time=46125.222648, global_step=58115, preemption_count=0, score=46125.222648, test/ctc_loss=0.18172287940979004, test/num_examples=2472, test/wer=0.059208, total_duration=50485.742219, train/ctc_loss=0.0954180657863617, train/wer=0.036730, validation/ctc_loss=0.3292542099952698, validation/num_examples=5348, validation/wer=0.094471
I0217 17:40:15.342805 139578127132416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.5840317606925964, loss=0.9928554892539978
I0217 17:41:31.289312 139578135525120 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.5581775307655334, loss=1.0593606233596802
I0217 17:42:47.249213 139578127132416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.7911718487739563, loss=0.9751648306846619
I0217 17:44:03.651977 139578135525120 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.7140864133834839, loss=0.9969865083694458
I0217 17:45:27.460187 139578127132416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.5953547358512878, loss=0.9532219767570496
I0217 17:46:51.129620 139578135525120 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.7142655849456787, loss=1.0136157274246216
I0217 17:48:10.885752 139578135525120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.8753844499588013, loss=1.017799973487854
I0217 17:49:26.668847 139578127132416 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.6512238383293152, loss=0.9684645533561707
I0217 17:50:42.918480 139578135525120 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.8412113189697266, loss=0.9758582711219788
I0217 17:51:58.918761 139578127132416 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.6334215402603149, loss=0.9693028926849365
I0217 17:53:14.812201 139578135525120 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.567016065120697, loss=0.998008668422699
I0217 17:54:35.735795 139578127132416 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.5527940988540649, loss=0.9681370854377747
I0217 17:55:59.582259 139578135525120 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.6845722198486328, loss=0.9804622530937195
I0217 17:57:23.522917 139578127132416 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.7945708632469177, loss=1.0160623788833618
I0217 17:58:47.088458 139578135525120 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.7922183871269226, loss=1.0037455558776855
I0217 18:00:10.791006 139578127132416 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.7858403921127319, loss=0.9593889117240906
I0217 18:01:33.109921 139578135525120 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.7174199223518372, loss=0.9862157106399536
I0217 18:02:49.097517 139578127132416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.5838882923126221, loss=0.9789091944694519
I0217 18:03:10.806058 139688679413568 spec.py:321] Evaluating on the training split.
I0217 18:04:05.463287 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 18:04:57.603851 139688679413568 spec.py:349] Evaluating on the test split.
I0217 18:05:24.003855 139688679413568 submission_runner.py:408] Time since start: 52059.59s, 	Step: 59930, 	{'train/ctc_loss': Array(0.0784812, dtype=float32), 'train/wer': 0.030712803793716655, 'validation/ctc_loss': Array(0.32586622, dtype=float32), 'validation/wer': 0.09347635092732942, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17646027, dtype=float32), 'test/wer': 0.05675055349054496, 'test/num_examples': 2472, 'score': 47565.73729014397, 'total_duration': 52059.585463523865, 'accumulated_submission_time': 47565.73729014397, 'accumulated_eval_time': 4489.549370765686, 'accumulated_logging_time': 1.8408918380737305}
I0217 18:05:24.044454 139578135525120 logging_writer.py:48] [59930] accumulated_eval_time=4489.549371, accumulated_logging_time=1.840892, accumulated_submission_time=47565.737290, global_step=59930, preemption_count=0, score=47565.737290, test/ctc_loss=0.17646026611328125, test/num_examples=2472, test/wer=0.056751, total_duration=52059.585464, train/ctc_loss=0.07848119735717773, train/wer=0.030713, validation/ctc_loss=0.3258662223815918, validation/num_examples=5348, validation/wer=0.093476
I0217 18:06:17.915218 139578127132416 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.7215864658355713, loss=0.9873248338699341
I0217 18:07:33.828012 139578135525120 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.6684737205505371, loss=1.0181397199630737
I0217 18:08:50.044665 139578127132416 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.5991593599319458, loss=0.9871981739997864
I0217 18:10:06.187743 139578135525120 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.5635998249053955, loss=1.0020289421081543
I0217 18:11:22.173688 139578127132416 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.6888447999954224, loss=1.025128960609436
I0217 18:12:45.123307 139578135525120 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.921357274055481, loss=0.9949474930763245
I0217 18:14:09.528513 139578127132416 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.6024476885795593, loss=0.9907301068305969
I0217 18:15:33.225630 139578135525120 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.7733378410339355, loss=0.9867023825645447
I0217 18:16:58.358312 139578135525120 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.7862025499343872, loss=1.015264868736267
I0217 18:18:14.271057 139578127132416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.5969351530075073, loss=1.0001964569091797
I0217 18:19:30.295757 139578135525120 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.6993312239646912, loss=1.0021430253982544
I0217 18:20:46.277479 139578127132416 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.7597166895866394, loss=1.0323210954666138
I0217 18:22:02.245819 139578135525120 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.6079025268554688, loss=1.0067138671875
I0217 18:23:19.902422 139578127132416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.645142674446106, loss=0.9629002809524536
I0217 18:24:44.066949 139578135525120 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.6588970422744751, loss=1.0139507055282593
I0217 18:26:08.316742 139578127132416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.6749081015586853, loss=0.9636829495429993
I0217 18:27:32.722164 139578135525120 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.6007143259048462, loss=0.9738883972167969
I0217 18:28:56.381769 139578127132416 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.5588691234588623, loss=0.9459400773048401
I0217 18:29:24.964566 139688679413568 spec.py:321] Evaluating on the training split.
I0217 18:30:20.267880 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 18:31:12.446058 139688679413568 spec.py:349] Evaluating on the test split.
I0217 18:31:38.963738 139688679413568 submission_runner.py:408] Time since start: 53634.55s, 	Step: 61736, 	{'train/ctc_loss': Array(0.07178165, dtype=float32), 'train/wer': 0.02875196469112674, 'validation/ctc_loss': Array(0.31728697, dtype=float32), 'validation/wer': 0.09083097598887784, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17256908, dtype=float32), 'test/wer': 0.05555217029228363, 'test/num_examples': 2472, 'score': 49006.57438802719, 'total_duration': 53634.54512619972, 'accumulated_submission_time': 49006.57438802719, 'accumulated_eval_time': 4623.5429792404175, 'accumulated_logging_time': 1.8968467712402344}
I0217 18:31:39.009480 139578135525120 logging_writer.py:48] [61736] accumulated_eval_time=4623.542979, accumulated_logging_time=1.896847, accumulated_submission_time=49006.574388, global_step=61736, preemption_count=0, score=49006.574388, test/ctc_loss=0.17256908118724823, test/num_examples=2472, test/wer=0.055552, total_duration=53634.545126, train/ctc_loss=0.07178165018558502, train/wer=0.028752, validation/ctc_loss=0.31728696823120117, validation/num_examples=5348, validation/wer=0.090831
I0217 18:32:31.805599 139578135525120 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.755777895450592, loss=0.9805225133895874
I0217 18:33:47.574229 139578127132416 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.6748026013374329, loss=0.9414805769920349
I0217 18:35:03.591952 139578135525120 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.6466839909553528, loss=0.9461662173271179
I0217 18:36:19.614096 139578127132416 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.7742704153060913, loss=0.9804427623748779
I0217 18:37:35.529339 139578135525120 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.6230067610740662, loss=0.9810114502906799
I0217 18:38:52.371611 139578127132416 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.6630244851112366, loss=0.9920971989631653
I0217 18:40:15.683224 139578135525120 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.8790204524993896, loss=0.9693895578384399
I0217 18:41:39.178007 139578127132416 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.6452882885932922, loss=0.9709046483039856
I0217 18:43:04.107884 139578135525120 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.6479305624961853, loss=0.945479154586792
I0217 18:44:27.860522 139578127132416 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.6354185342788696, loss=0.9625742435455322
I0217 18:45:51.560530 139578135525120 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.5798584818840027, loss=0.9572721123695374
I0217 18:47:13.687750 139578135525120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.5759013891220093, loss=0.9746856689453125
I0217 18:48:29.522988 139578127132416 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.6924978494644165, loss=0.9587748646736145
I0217 18:49:45.371764 139578135525120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.6765907406806946, loss=0.9564641118049622
I0217 18:51:01.262636 139578127132416 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.5957149267196655, loss=0.9706360697746277
I0217 18:52:17.012146 139578135525120 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.9628462195396423, loss=0.9746069312095642
I0217 18:53:38.790024 139578127132416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.6224033236503601, loss=0.9263619184494019
I0217 18:55:03.594697 139578135525120 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.6148020029067993, loss=0.9605297446250916
I0217 18:55:39.198727 139688679413568 spec.py:321] Evaluating on the training split.
I0217 18:56:34.088941 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 18:57:26.181639 139688679413568 spec.py:349] Evaluating on the test split.
I0217 18:57:53.122730 139688679413568 submission_runner.py:408] Time since start: 55208.70s, 	Step: 63544, 	{'train/ctc_loss': Array(0.07886387, dtype=float32), 'train/wer': 0.030922318552310122, 'validation/ctc_loss': Array(0.3153456, dtype=float32), 'validation/wer': 0.08900624656052984, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16620629, dtype=float32), 'test/wer': 0.0538866207624967, 'test/num_examples': 2472, 'score': 50446.679966926575, 'total_duration': 55208.704122543335, 'accumulated_submission_time': 50446.679966926575, 'accumulated_eval_time': 4757.461458683014, 'accumulated_logging_time': 1.9577386379241943}
I0217 18:57:53.167186 139578135525120 logging_writer.py:48] [63544] accumulated_eval_time=4757.461459, accumulated_logging_time=1.957739, accumulated_submission_time=50446.679967, global_step=63544, preemption_count=0, score=50446.679967, test/ctc_loss=0.16620628535747528, test/num_examples=2472, test/wer=0.053887, total_duration=55208.704123, train/ctc_loss=0.07886387407779694, train/wer=0.030922, validation/ctc_loss=0.3153455853462219, validation/num_examples=5348, validation/wer=0.089006
I0217 18:58:36.386955 139578127132416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.5437336564064026, loss=0.9849429726600647
I0217 18:59:52.285462 139578135525120 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.7003052830696106, loss=0.9824891686439514
I0217 19:01:08.544839 139578127132416 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.8941670060157776, loss=0.9210879802703857
I0217 19:02:27.867843 139578135525120 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.6779457330703735, loss=0.9595786333084106
I0217 19:03:43.614891 139578127132416 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.6349754929542542, loss=0.977540910243988
I0217 19:04:59.628358 139578135525120 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.6278430223464966, loss=0.9372320771217346
I0217 19:06:15.589842 139578127132416 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.709026038646698, loss=0.9869728684425354
I0217 19:07:31.430927 139578135525120 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.7507207989692688, loss=0.9302047491073608
I0217 19:08:49.804629 139578127132416 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.9105488657951355, loss=0.9223721623420715
I0217 19:10:14.546860 139578135525120 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.0847008228302002, loss=0.9731509685516357
I0217 19:11:38.686689 139578127132416 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.6216387748718262, loss=0.9725750684738159
I0217 19:13:01.688700 139578135525120 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.7630993723869324, loss=0.9819626808166504
I0217 19:14:25.148125 139578127132416 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.534317672252655, loss=0.9697809219360352
I0217 19:15:51.960853 139578135525120 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.7172954082489014, loss=0.9276635646820068
I0217 19:17:08.350911 139578127132416 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.6734306812286377, loss=0.9407933354377747
I0217 19:18:24.319457 139578135525120 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.854843258857727, loss=0.9497004151344299
I0217 19:19:40.381157 139578127132416 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.6356683969497681, loss=0.9373413324356079
I0217 19:20:56.478690 139578135525120 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.7652900815010071, loss=0.937627911567688
I0217 19:21:53.854621 139688679413568 spec.py:321] Evaluating on the training split.
I0217 19:22:49.370087 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 19:23:41.049952 139688679413568 spec.py:349] Evaluating on the test split.
I0217 19:24:08.261668 139688679413568 submission_runner.py:408] Time since start: 56783.84s, 	Step: 65377, 	{'train/ctc_loss': Array(0.07221077, dtype=float32), 'train/wer': 0.028075026739362952, 'validation/ctc_loss': Array(0.3099662, dtype=float32), 'validation/wer': 0.08895797329522964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16686207, dtype=float32), 'test/wer': 0.05354132390876038, 'test/num_examples': 2472, 'score': 51887.27757143974, 'total_duration': 56783.84284877777, 'accumulated_submission_time': 51887.27757143974, 'accumulated_eval_time': 4891.862742900848, 'accumulated_logging_time': 2.0222373008728027}
I0217 19:24:08.301869 139578135525120 logging_writer.py:48] [65377] accumulated_eval_time=4891.862743, accumulated_logging_time=2.022237, accumulated_submission_time=51887.277571, global_step=65377, preemption_count=0, score=51887.277571, test/ctc_loss=0.16686207056045532, test/num_examples=2472, test/wer=0.053541, total_duration=56783.842849, train/ctc_loss=0.07221077382564545, train/wer=0.028075, validation/ctc_loss=0.30996620655059814, validation/num_examples=5348, validation/wer=0.088958
I0217 19:24:26.509534 139578127132416 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.6434076428413391, loss=0.9563217163085938
I0217 19:25:42.200783 139578135525120 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.6753209829330444, loss=0.9181692004203796
I0217 19:26:57.973368 139578127132416 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.6584787368774414, loss=0.9237395524978638
I0217 19:28:13.873739 139578135525120 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.6963821649551392, loss=0.9416375756263733
I0217 19:29:35.308449 139578127132416 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.704389214515686, loss=0.9439945816993713
I0217 19:30:59.775657 139578135525120 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.662777841091156, loss=0.9588668942451477
I0217 19:32:20.568737 139578135525120 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.6778771877288818, loss=0.9486508965492249
I0217 19:33:36.620064 139578127132416 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.694179356098175, loss=0.9777787327766418
I0217 19:34:52.790512 139578135525120 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.8239708542823792, loss=0.9156741499900818
I0217 19:36:08.692359 139578127132416 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.6192349791526794, loss=0.9415149092674255
I0217 19:37:24.707266 139578135525120 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.9388864636421204, loss=0.9554965496063232
I0217 19:38:44.678019 139578127132416 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.737749457359314, loss=0.8727042078971863
I0217 19:40:08.082460 139578135525120 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.6888962388038635, loss=0.8998036980628967
I0217 19:41:31.235823 139578127132416 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.8015396595001221, loss=0.9213812351226807
I0217 19:42:54.898974 139578135525120 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.7611711621284485, loss=0.9323991537094116
I0217 19:44:18.888217 139578127132416 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.66517573595047, loss=0.9732887148857117
I0217 19:45:42.797113 139578135525120 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.7234645485877991, loss=0.9565092921257019
I0217 19:46:58.696460 139578127132416 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.2743990421295166, loss=0.9511656165122986
I0217 19:48:08.815752 139688679413568 spec.py:321] Evaluating on the training split.
I0217 19:49:04.096101 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 19:49:55.538542 139688679413568 spec.py:349] Evaluating on the test split.
I0217 19:50:22.136828 139688679413568 submission_runner.py:408] Time since start: 58357.72s, 	Step: 67194, 	{'train/ctc_loss': Array(0.06876019, dtype=float32), 'train/wer': 0.026767290890285293, 'validation/ctc_loss': Array(0.30606192, dtype=float32), 'validation/wer': 0.08766424978518397, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16483718, dtype=float32), 'test/wer': 0.05238356386976215, 'test/num_examples': 2472, 'score': 53327.70025777817, 'total_duration': 58357.71854352951, 'accumulated_submission_time': 53327.70025777817, 'accumulated_eval_time': 5025.178674221039, 'accumulated_logging_time': 2.0835318565368652}
I0217 19:50:22.178655 139578135525120 logging_writer.py:48] [67194] accumulated_eval_time=5025.178674, accumulated_logging_time=2.083532, accumulated_submission_time=53327.700258, global_step=67194, preemption_count=0, score=53327.700258, test/ctc_loss=0.16483718156814575, test/num_examples=2472, test/wer=0.052384, total_duration=58357.718544, train/ctc_loss=0.06876018643379211, train/wer=0.026767, validation/ctc_loss=0.30606192350387573, validation/num_examples=5348, validation/wer=0.087664
I0217 19:50:27.556264 139578127132416 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.7201205492019653, loss=0.9304152131080627
I0217 19:51:43.329199 139578135525120 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.7081412076950073, loss=0.8822392821311951
I0217 19:52:59.425592 139578127132416 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.6681836247444153, loss=0.9423684477806091
I0217 19:54:15.325972 139578135525120 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.6836705207824707, loss=0.9842285513877869
I0217 19:55:31.064684 139578127132416 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.7519203424453735, loss=0.9360547065734863
I0217 19:56:51.926293 139578135525120 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.6275909543037415, loss=0.9166041016578674
I0217 19:58:15.872960 139578127132416 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.6295666098594666, loss=0.9373802542686462
I0217 19:59:39.296490 139578135525120 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.7116404175758362, loss=0.9087738394737244
I0217 20:01:06.032335 139578135525120 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.6417971253395081, loss=0.9372863173484802
I0217 20:02:21.828603 139578127132416 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.6707053780555725, loss=0.9117065072059631
I0217 20:03:37.555872 139578135525120 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.6396331191062927, loss=0.9133607745170593
I0217 20:04:53.423674 139578127132416 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.6406322121620178, loss=0.8838812708854675
I0217 20:06:09.352579 139578135525120 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.7036107778549194, loss=0.9467315077781677
I0217 20:07:25.770391 139578127132416 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.6616190671920776, loss=0.8603754639625549
I0217 20:08:49.091610 139578135525120 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.7658847570419312, loss=0.9170615673065186
I0217 20:10:13.752944 139578127132416 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.7071895599365234, loss=0.8934952616691589
I0217 20:11:38.260736 139578135525120 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.664991557598114, loss=0.897478461265564
I0217 20:13:02.274923 139578127132416 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.6599510312080383, loss=0.898116409778595
I0217 20:14:22.883243 139688679413568 spec.py:321] Evaluating on the training split.
I0217 20:15:17.616187 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 20:16:09.457940 139688679413568 spec.py:349] Evaluating on the test split.
I0217 20:16:35.587664 139688679413568 submission_runner.py:408] Time since start: 59931.17s, 	Step: 68998, 	{'train/ctc_loss': Array(0.06089285, dtype=float32), 'train/wer': 0.023473511397360612, 'validation/ctc_loss': Array(0.30153775, dtype=float32), 'validation/wer': 0.08621605182617763, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1602182, dtype=float32), 'test/wer': 0.051185180671500824, 'test/num_examples': 2472, 'score': 54768.31900382042, 'total_duration': 59931.1682472229, 'accumulated_submission_time': 54768.31900382042, 'accumulated_eval_time': 5157.87672996521, 'accumulated_logging_time': 2.142979860305786}
I0217 20:16:35.634435 139578135525120 logging_writer.py:48] [68998] accumulated_eval_time=5157.876730, accumulated_logging_time=2.142980, accumulated_submission_time=54768.319004, global_step=68998, preemption_count=0, score=54768.319004, test/ctc_loss=0.16021819412708282, test/num_examples=2472, test/wer=0.051185, total_duration=59931.168247, train/ctc_loss=0.060892846435308456, train/wer=0.023474, validation/ctc_loss=0.30153775215148926, validation/num_examples=5348, validation/wer=0.086216
I0217 20:16:38.006819 139578127132416 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.6828255653381348, loss=0.9314824342727661
I0217 20:17:57.436867 139578135525120 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.6611111164093018, loss=0.9117060899734497
I0217 20:19:13.504168 139578127132416 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.6023896336555481, loss=0.9593215584754944
I0217 20:20:29.594894 139578135525120 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.6947464942932129, loss=0.9475986361503601
I0217 20:21:45.614737 139578127132416 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.8182080388069153, loss=0.9111263155937195
I0217 20:23:01.644041 139578135525120 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.6495479941368103, loss=0.9117233157157898
I0217 20:24:24.529614 139578127132416 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.5904985070228577, loss=0.8954684734344482
I0217 20:25:47.903364 139578135525120 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.7578811049461365, loss=0.8963209390640259
I0217 20:27:11.936170 139578127132416 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.6242141723632812, loss=0.8678451776504517
I0217 20:28:34.980063 139578135525120 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.8095839619636536, loss=0.8924598097801208
I0217 20:29:59.089648 139578127132416 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.6833848357200623, loss=0.9148274660110474
I0217 20:31:21.271723 139578135525120 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.7313811182975769, loss=0.894812822341919
I0217 20:32:37.166859 139578127132416 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.7369309663772583, loss=0.8715965747833252
I0217 20:33:53.083741 139578135525120 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.6676285862922668, loss=0.904120683670044
I0217 20:35:08.932403 139578127132416 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.6118239164352417, loss=0.9119367599487305
I0217 20:36:24.884004 139578135525120 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.7336369156837463, loss=0.883167564868927
I0217 20:37:43.920459 139578127132416 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.5757803320884705, loss=0.9049887657165527
I0217 20:39:07.124959 139578135525120 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.5762383341789246, loss=0.9048078656196594
I0217 20:40:30.291636 139578127132416 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.6705805659294128, loss=0.923325777053833
I0217 20:40:35.833576 139688679413568 spec.py:321] Evaluating on the training split.
I0217 20:41:31.218490 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 20:42:24.210829 139688679413568 spec.py:349] Evaluating on the test split.
I0217 20:42:50.371837 139688679413568 submission_runner.py:408] Time since start: 61505.95s, 	Step: 70808, 	{'train/ctc_loss': Array(0.05510921, dtype=float32), 'train/wer': 0.02111032384445463, 'validation/ctc_loss': Array(0.29850617, dtype=float32), 'validation/wer': 0.08411133745908841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15855892, dtype=float32), 'test/wer': 0.05027115958808116, 'test/num_examples': 2472, 'score': 56208.43280529976, 'total_duration': 61505.95262885094, 'accumulated_submission_time': 56208.43280529976, 'accumulated_eval_time': 5292.408869028091, 'accumulated_logging_time': 2.2067055702209473}
I0217 20:42:50.422922 139578135525120 logging_writer.py:48] [70808] accumulated_eval_time=5292.408869, accumulated_logging_time=2.206706, accumulated_submission_time=56208.432805, global_step=70808, preemption_count=0, score=56208.432805, test/ctc_loss=0.1585589200258255, test/num_examples=2472, test/wer=0.050271, total_duration=61505.952629, train/ctc_loss=0.055109210312366486, train/wer=0.021110, validation/ctc_loss=0.2985061705112457, validation/num_examples=5348, validation/wer=0.084111
I0217 20:44:00.917122 139578127132416 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.8045196533203125, loss=0.9136450886726379
I0217 20:45:17.107465 139578135525120 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.6258044242858887, loss=0.9117113351821899
I0217 20:46:36.454397 139578135525120 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.7780140042304993, loss=0.8866034150123596
I0217 20:47:52.404003 139578127132416 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.8883164525032043, loss=0.9192584753036499
I0217 20:49:08.306278 139578135525120 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.8081096410751343, loss=0.8851985931396484
I0217 20:50:24.181904 139578127132416 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.6277066469192505, loss=0.8897743225097656
I0217 20:51:40.144360 139578135525120 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.6083424687385559, loss=0.8949423432350159
I0217 20:52:58.369930 139578127132416 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.6496779918670654, loss=0.8622108101844788
I0217 20:54:22.510474 139578135525120 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.7422217726707458, loss=0.907229483127594
I0217 20:55:46.182496 139578127132416 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.7940734028816223, loss=0.8860293030738831
I0217 20:57:10.485838 139578135525120 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.6261519193649292, loss=0.8739318251609802
I0217 20:58:35.077882 139578127132416 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.6622274518013, loss=0.9390777349472046
I0217 21:00:02.351665 139578135525120 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.6547651290893555, loss=0.8897441625595093
I0217 21:01:18.236094 139578127132416 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.6610690355300903, loss=0.8861895799636841
I0217 21:02:34.319537 139578135525120 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.6709451079368591, loss=0.8242285251617432
I0217 21:03:50.360716 139578127132416 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.638049840927124, loss=0.8779848217964172
I0217 21:05:06.338277 139578135525120 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2074183225631714, loss=0.9066633582115173
I0217 21:06:22.302614 139578127132416 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.6578562259674072, loss=0.8687561750411987
I0217 21:06:50.834544 139688679413568 spec.py:321] Evaluating on the training split.
I0217 21:07:47.577769 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 21:08:40.099079 139688679413568 spec.py:349] Evaluating on the test split.
I0217 21:09:06.711038 139688679413568 submission_runner.py:408] Time since start: 63082.29s, 	Step: 72636, 	{'train/ctc_loss': Array(0.05994768, dtype=float32), 'train/wer': 0.022203459216047487, 'validation/ctc_loss': Array(0.29521114, dtype=float32), 'validation/wer': 0.08316518145920426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.158432, dtype=float32), 'test/wer': 0.04976337009729247, 'test/num_examples': 2472, 'score': 57648.75893139839, 'total_duration': 63082.29228138924, 'accumulated_submission_time': 57648.75893139839, 'accumulated_eval_time': 5428.279653549194, 'accumulated_logging_time': 2.27409291267395}
I0217 21:09:06.752196 139578135525120 logging_writer.py:48] [72636] accumulated_eval_time=5428.279654, accumulated_logging_time=2.274093, accumulated_submission_time=57648.758931, global_step=72636, preemption_count=0, score=57648.758931, test/ctc_loss=0.1584320068359375, test/num_examples=2472, test/wer=0.049763, total_duration=63082.292281, train/ctc_loss=0.05994768440723419, train/wer=0.022203, validation/ctc_loss=0.29521113634109497, validation/num_examples=5348, validation/wer=0.083165
I0217 21:09:56.052194 139578127132416 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.7603771686553955, loss=0.884677529335022
I0217 21:11:11.994395 139578135525120 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.8810498714447021, loss=0.921081006526947
I0217 21:12:27.818855 139578127132416 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.8333668112754822, loss=0.8774866461753845
I0217 21:13:44.571296 139578135525120 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.7654715180397034, loss=0.9165863394737244
I0217 21:15:07.770400 139578127132416 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.6791276931762695, loss=0.8650102615356445
I0217 21:16:28.896994 139578135525120 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.7196177244186401, loss=0.9091378450393677
I0217 21:17:44.745218 139578127132416 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.7004623413085938, loss=0.8722578287124634
I0217 21:19:00.605528 139578135525120 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.8153322339057922, loss=0.8701471090316772
I0217 21:20:16.848646 139578127132416 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.7950590252876282, loss=0.8822004199028015
I0217 21:21:32.650336 139578135525120 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.7576285600662231, loss=0.8817020654678345
I0217 21:22:54.051930 139578127132416 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.8183143734931946, loss=0.8593963980674744
I0217 21:24:18.550510 139578135525120 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.6496444344520569, loss=0.8465326428413391
I0217 21:25:42.537474 139578127132416 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.7467053532600403, loss=0.9005987644195557
I0217 21:27:06.777139 139578135525120 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.7084936499595642, loss=0.8919928669929504
I0217 21:28:31.132170 139578127132416 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.5690578818321228, loss=0.8632935285568237
I0217 21:29:54.676984 139578135525120 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.7483808398246765, loss=0.8842021226882935
I0217 21:31:10.605876 139578127132416 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.66628098487854, loss=0.8617130517959595
I0217 21:32:26.552812 139578135525120 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.0608127117156982, loss=0.8639800548553467
I0217 21:33:07.238918 139688679413568 spec.py:321] Evaluating on the training split.
I0217 21:34:00.506345 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 21:34:52.526775 139688679413568 spec.py:349] Evaluating on the test split.
I0217 21:35:18.751464 139688679413568 submission_runner.py:408] Time since start: 64654.33s, 	Step: 74455, 	{'train/ctc_loss': Array(0.05741131, dtype=float32), 'train/wer': 0.021588792614786623, 'validation/ctc_loss': Array(0.29396224, dtype=float32), 'validation/wer': 0.08243142782664105, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15610263, dtype=float32), 'test/wer': 0.04966181219913473, 'test/num_examples': 2472, 'score': 59089.160449266434, 'total_duration': 64654.3326253891, 'accumulated_submission_time': 59089.160449266434, 'accumulated_eval_time': 5559.786453962326, 'accumulated_logging_time': 2.330268144607544}
I0217 21:35:18.791716 139578135525120 logging_writer.py:48] [74455] accumulated_eval_time=5559.786454, accumulated_logging_time=2.330268, accumulated_submission_time=59089.160449, global_step=74455, preemption_count=0, score=59089.160449, test/ctc_loss=0.15610262751579285, test/num_examples=2472, test/wer=0.049662, total_duration=64654.332625, train/ctc_loss=0.057411305606365204, train/wer=0.021589, validation/ctc_loss=0.2939622402191162, validation/num_examples=5348, validation/wer=0.082431
I0217 21:35:53.670723 139578127132416 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.7334269881248474, loss=0.9241275787353516
I0217 21:37:09.868561 139578135525120 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.6187129020690918, loss=0.8119908571243286
I0217 21:38:25.826282 139578127132416 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.662656843662262, loss=0.8753288388252258
I0217 21:39:41.646777 139578135525120 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.6586935520172119, loss=0.8444328904151917
I0217 21:41:00.688905 139578127132416 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.9834206700325012, loss=0.8446435928344727
I0217 21:42:24.700495 139578135525120 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.8067600727081299, loss=0.8704200983047485
I0217 21:43:47.790463 139578127132416 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.7276391386985779, loss=0.8548027873039246
I0217 21:45:14.356113 139578135525120 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.8896435499191284, loss=0.8837123513221741
I0217 21:46:30.259444 139578127132416 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.99354088306427, loss=0.9117661714553833
I0217 21:47:46.188861 139578135525120 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.6407400369644165, loss=0.8571383357048035
I0217 21:49:02.105354 139578127132416 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.6923336386680603, loss=0.9189581274986267
I0217 21:50:18.155815 139578135525120 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.139299750328064, loss=0.9043458104133606
I0217 21:51:34.617289 139578127132416 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.7823330163955688, loss=0.8803332448005676
I0217 21:52:58.350913 139578135525120 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.6594880223274231, loss=0.8810511231422424
I0217 21:54:23.145799 139578127132416 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.715931236743927, loss=0.8865826725959778
I0217 21:55:46.243867 139578135525120 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.7035077810287476, loss=0.8686495423316956
I0217 21:57:09.477750 139578127132416 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.9399231672286987, loss=0.8905172944068909
I0217 21:58:33.347541 139578135525120 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.6931815147399902, loss=0.8945332765579224
I0217 21:59:18.998605 139688679413568 spec.py:321] Evaluating on the training split.
I0217 22:00:14.198732 139688679413568 spec.py:333] Evaluating on the validation split.
I0217 22:01:06.503959 139688679413568 spec.py:349] Evaluating on the test split.
I0217 22:01:32.890450 139688679413568 submission_runner.py:408] Time since start: 66228.47s, 	Step: 76255, 	{'train/ctc_loss': Array(0.05751698, dtype=float32), 'train/wer': 0.021302694306068883, 'validation/ctc_loss': Array(0.29176736, dtype=float32), 'validation/wer': 0.08214178823483978, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15516719, dtype=float32), 'test/wer': 0.0494383848231877, 'test/num_examples': 2472, 'score': 60529.28284430504, 'total_duration': 66228.47186899185, 'accumulated_submission_time': 60529.28284430504, 'accumulated_eval_time': 5693.6730353832245, 'accumulated_logging_time': 2.3865671157836914}
I0217 22:01:32.935363 139578135525120 logging_writer.py:48] [76255] accumulated_eval_time=5693.673035, accumulated_logging_time=2.386567, accumulated_submission_time=60529.282844, global_step=76255, preemption_count=0, score=60529.282844, test/ctc_loss=0.15516719222068787, test/num_examples=2472, test/wer=0.049438, total_duration=66228.471869, train/ctc_loss=0.05751698091626167, train/wer=0.021303, validation/ctc_loss=0.2917673587799072, validation/num_examples=5348, validation/wer=0.082142
I0217 22:02:07.862334 139578127132416 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.7117175459861755, loss=0.8375065922737122
I0217 22:03:23.752593 139578135525120 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.7706827521324158, loss=0.8617458343505859
I0217 22:04:39.592627 139578127132416 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.7418491244316101, loss=0.9233071804046631
I0217 22:05:55.475361 139578135525120 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.6703217029571533, loss=0.8862984776496887
I0217 22:07:11.269854 139578127132416 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.7129223942756653, loss=0.8740200400352478
I0217 22:08:27.042117 139578135525120 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.7411841750144958, loss=0.8943901062011719
I0217 22:09:45.837962 139578127132416 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.6957415342330933, loss=0.8746200799942017
I0217 22:10:31.884911 139578135525120 logging_writer.py:48] [76955] global_step=76955, preemption_count=0, score=61068.168171
I0217 22:10:32.726235 139688679413568 checkpoints.py:490] Saving checkpoint at step: 76955
I0217 22:10:34.329663 139688679413568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_5/checkpoint_76955
I0217 22:10:34.361318 139688679413568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_conformer_jax/trial_5/checkpoint_76955.
I0217 22:10:38.110183 139688679413568 submission_runner.py:583] Tuning trial 5/5
I0217 22:10:38.110450 139688679413568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0217 22:10:38.138367 139688679413568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.451271, dtype=float32), 'train/wer': 0.9408900814105828, 'validation/ctc_loss': Array(30.14182, dtype=float32), 'validation/wer': 0.9043706614402811, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.208836, dtype=float32), 'test/wer': 0.9085978916580343, 'test/num_examples': 2472, 'score': 36.173173904418945, 'total_duration': 152.76622414588928, 'accumulated_submission_time': 36.173173904418945, 'accumulated_eval_time': 116.59295606613159, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1796, {'train/ctc_loss': Array(3.44108, dtype=float32), 'train/wer': 0.6439037536543498, 'validation/ctc_loss': Array(3.3643177, dtype=float32), 'validation/wer': 0.6250132751479576, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.0440004, dtype=float32), 'test/wer': 0.5801799605955356, 'test/num_examples': 2472, 'score': 1476.5014803409576, 'total_duration': 1718.9732348918915, 'accumulated_submission_time': 1476.5014803409576, 'accumulated_eval_time': 242.3737189769745, 'accumulated_logging_time': 0.0271456241607666, 'global_step': 1796, 'preemption_count': 0}), (3624, {'train/ctc_loss': Array(0.90955204, dtype=float32), 'train/wer': 0.28858346674681107, 'validation/ctc_loss': Array(0.95395464, dtype=float32), 'validation/wer': 0.2790677467005223, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6575261, dtype=float32), 'test/wer': 0.21471370828509334, 'test/num_examples': 2472, 'score': 2916.602714776993, 'total_duration': 3290.107299566269, 'accumulated_submission_time': 2916.602714776993, 'accumulated_eval_time': 373.27892112731934, 'accumulated_logging_time': 0.07884597778320312, 'global_step': 3624, 'preemption_count': 0}), (5454, {'train/ctc_loss': Array(0.55690676, dtype=float32), 'train/wer': 0.18963967600441323, 'validation/ctc_loss': Array(0.74646324, dtype=float32), 'validation/wer': 0.22499203491122546, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48497862, dtype=float32), 'test/wer': 0.1616598622874901, 'test/num_examples': 2472, 'score': 4356.862602472305, 'total_duration': 4861.620371341705, 'accumulated_submission_time': 4356.862602472305, 'accumulated_eval_time': 504.3946657180786, 'accumulated_logging_time': 0.1390526294708252, 'global_step': 5454, 'preemption_count': 0}), (7256, {'train/ctc_loss': Array(0.55555606, dtype=float32), 'train/wer': 0.18881049824992593, 'validation/ctc_loss': Array(0.6697497, dtype=float32), 'validation/wer': 0.20144433609778234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42860916, dtype=float32), 'test/wer': 0.1437044258932017, 'test/num_examples': 2472, 'score': 5796.924175024033, 'total_duration': 6433.738090276718, 'accumulated_submission_time': 5796.924175024033, 'accumulated_eval_time': 636.3247225284576, 'accumulated_logging_time': 0.19132184982299805, 'global_step': 7256, 'preemption_count': 0}), (9080, {'train/ctc_loss': Array(0.45142564, dtype=float32), 'train/wer': 0.15607634180452967, 'validation/ctc_loss': Array(0.624928, dtype=float32), 'validation/wer': 0.18842986377284532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39310047, dtype=float32), 'test/wer': 0.1300956675400646, 'test/num_examples': 2472, 'score': 7237.313529729843, 'total_duration': 8006.933594942093, 'accumulated_submission_time': 7237.313529729843, 'accumulated_eval_time': 768.995701789856, 'accumulated_logging_time': 0.25190162658691406, 'global_step': 9080, 'preemption_count': 0}), (10907, {'train/ctc_loss': Array(0.43169728, dtype=float32), 'train/wer': 0.15281203283448377, 'validation/ctc_loss': Array(0.59886086, dtype=float32), 'validation/wer': 0.18009789818202881, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37033734, dtype=float32), 'test/wer': 0.12507870737107224, 'test/num_examples': 2472, 'score': 8677.727895021439, 'total_duration': 9580.304859876633, 'accumulated_submission_time': 8677.727895021439, 'accumulated_eval_time': 901.8233368396759, 'accumulated_logging_time': 0.3069281578063965, 'global_step': 10907, 'preemption_count': 0}), (12736, {'train/ctc_loss': Array(0.4072509, dtype=float32), 'train/wer': 0.14037704918032787, 'validation/ctc_loss': Array(0.5681042, dtype=float32), 'validation/wer': 0.17247072226459542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35084808, dtype=float32), 'test/wer': 0.11780716186297809, 'test/num_examples': 2472, 'score': 10117.946796894073, 'total_duration': 11153.522399902344, 'accumulated_submission_time': 10117.946796894073, 'accumulated_eval_time': 1034.6947605609894, 'accumulated_logging_time': 0.3596227169036865, 'global_step': 12736, 'preemption_count': 0}), (14545, {'train/ctc_loss': Array(0.39576054, dtype=float32), 'train/wer': 0.1385089016958059, 'validation/ctc_loss': Array(0.55048543, dtype=float32), 'validation/wer': 0.1663013989592284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33382007, dtype=float32), 'test/wer': 0.1137042227774054, 'test/num_examples': 2472, 'score': 11558.43038058281, 'total_duration': 12726.402488231659, 'accumulated_submission_time': 11558.43038058281, 'accumulated_eval_time': 1166.9670646190643, 'accumulated_logging_time': 0.4112875461578369, 'global_step': 14545, 'preemption_count': 0}), (16358, {'train/ctc_loss': Array(0.40191302, dtype=float32), 'train/wer': 0.13663466005885086, 'validation/ctc_loss': Array(0.5395458, dtype=float32), 'validation/wer': 0.16157061895980768, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33373216, dtype=float32), 'test/wer': 0.10952003737330653, 'test/num_examples': 2472, 'score': 12998.585268974304, 'total_duration': 14299.370248556137, 'accumulated_submission_time': 12998.585268974304, 'accumulated_eval_time': 1299.6520047187805, 'accumulated_logging_time': 0.46649909019470215, 'global_step': 16358, 'preemption_count': 0}), (18169, {'train/ctc_loss': Array(0.37821412, dtype=float32), 'train/wer': 0.13356724368248055, 'validation/ctc_loss': Array(0.52589005, dtype=float32), 'validation/wer': 0.15801770663371212, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31530783, dtype=float32), 'test/wer': 0.1063311193711535, 'test/num_examples': 2472, 'score': 14438.571347236633, 'total_duration': 15872.740940332413, 'accumulated_submission_time': 14438.571347236633, 'accumulated_eval_time': 1432.9095618724823, 'accumulated_logging_time': 0.5199224948883057, 'global_step': 18169, 'preemption_count': 0}), (20001, {'train/ctc_loss': Array(0.37082225, dtype=float32), 'train/wer': 0.1336668088612551, 'validation/ctc_loss': Array(0.50519156, dtype=float32), 'validation/wer': 0.15093119128764107, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30063546, dtype=float32), 'test/wer': 0.10192350659110759, 'test/num_examples': 2472, 'score': 15878.452617168427, 'total_duration': 17442.23735189438, 'accumulated_submission_time': 15878.452617168427, 'accumulated_eval_time': 1562.391996383667, 'accumulated_logging_time': 0.5783975124359131, 'global_step': 20001, 'preemption_count': 0}), (21815, {'train/ctc_loss': Array(0.31139514, dtype=float32), 'train/wer': 0.10986010097395268, 'validation/ctc_loss': Array(0.49194026, dtype=float32), 'validation/wer': 0.14698243818608378, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29525468, dtype=float32), 'test/wer': 0.09749558223143014, 'test/num_examples': 2472, 'score': 17318.92253255844, 'total_duration': 19015.821897506714, 'accumulated_submission_time': 17318.92253255844, 'accumulated_eval_time': 1695.3766107559204, 'accumulated_logging_time': 0.6333692073822021, 'global_step': 21815, 'preemption_count': 0}), (23615, {'train/ctc_loss': Array(0.32840356, dtype=float32), 'train/wer': 0.11590025321034546, 'validation/ctc_loss': Array(0.48064968, dtype=float32), 'validation/wer': 0.14498392500265503, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28810266, dtype=float32), 'test/wer': 0.09631751061280036, 'test/num_examples': 2472, 'score': 18759.53474497795, 'total_duration': 20587.556434631348, 'accumulated_submission_time': 18759.53474497795, 'accumulated_eval_time': 1826.3744299411774, 'accumulated_logging_time': 0.6851708889007568, 'global_step': 23615, 'preemption_count': 0}), (25420, {'train/ctc_loss': Array(0.3194967, dtype=float32), 'train/wer': 0.11300510415073389, 'validation/ctc_loss': Array(0.47430897, dtype=float32), 'validation/wer': 0.14272473618660514, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27983314, dtype=float32), 'test/wer': 0.09483476529969735, 'test/num_examples': 2472, 'score': 20199.681203603745, 'total_duration': 22159.8330514431, 'accumulated_submission_time': 20199.681203603745, 'accumulated_eval_time': 1958.3797991275787, 'accumulated_logging_time': 0.7363095283508301, 'global_step': 25420, 'preemption_count': 0}), (27253, {'train/ctc_loss': Array(0.30807894, dtype=float32), 'train/wer': 0.10906005250612388, 'validation/ctc_loss': Array(0.4601984, dtype=float32), 'validation/wer': 0.1377139712484432, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2673006, dtype=float32), 'test/wer': 0.08878191456949606, 'test/num_examples': 2472, 'score': 21639.995112657547, 'total_duration': 23733.84988975525, 'accumulated_submission_time': 21639.995112657547, 'accumulated_eval_time': 2091.949378967285, 'accumulated_logging_time': 0.7944025993347168, 'global_step': 27253, 'preemption_count': 0}), (29061, {'train/ctc_loss': Array(0.2758949, dtype=float32), 'train/wer': 0.10113498175922173, 'validation/ctc_loss': Array(0.45608607, dtype=float32), 'validation/wer': 0.13516514284059203, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26807737, dtype=float32), 'test/wer': 0.0894725082769687, 'test/num_examples': 2472, 'score': 23080.382625341415, 'total_duration': 25306.609971761703, 'accumulated_submission_time': 23080.382625341415, 'accumulated_eval_time': 2224.193256378174, 'accumulated_logging_time': 0.8488309383392334, 'global_step': 29061, 'preemption_count': 0}), (30857, {'train/ctc_loss': Array(0.24288544, dtype=float32), 'train/wer': 0.08868910009418615, 'validation/ctc_loss': Array(0.44085503, dtype=float32), 'validation/wer': 0.13189221545323768, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25917184, dtype=float32), 'test/wer': 0.08567424288586924, 'test/num_examples': 2472, 'score': 24520.94918370247, 'total_duration': 26879.460943460464, 'accumulated_submission_time': 24520.94918370247, 'accumulated_eval_time': 2356.3475427627563, 'accumulated_logging_time': 0.9049539566040039, 'global_step': 30857, 'preemption_count': 0}), (32659, {'train/ctc_loss': Array(0.27391443, dtype=float32), 'train/wer': 0.09894852346449923, 'validation/ctc_loss': Array(0.43026587, dtype=float32), 'validation/wer': 0.12937235100456665, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25159425, dtype=float32), 'test/wer': 0.08392744703755611, 'test/num_examples': 2472, 'score': 25961.47345638275, 'total_duration': 28452.025376319885, 'accumulated_submission_time': 25961.47345638275, 'accumulated_eval_time': 2488.2610454559326, 'accumulated_logging_time': 0.9572756290435791, 'global_step': 32659, 'preemption_count': 0}), (34494, {'train/ctc_loss': Array(0.25215727, dtype=float32), 'train/wer': 0.09286672642144937, 'validation/ctc_loss': Array(0.4266692, dtype=float32), 'validation/wer': 0.12781795186189984, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24414307, dtype=float32), 'test/wer': 0.08283062173745252, 'test/num_examples': 2472, 'score': 27401.712621688843, 'total_duration': 30024.442529678345, 'accumulated_submission_time': 27401.712621688843, 'accumulated_eval_time': 2620.2989869117737, 'accumulated_logging_time': 1.022853136062622, 'global_step': 34494, 'preemption_count': 0}), (36309, {'train/ctc_loss': Array(0.26872647, dtype=float32), 'train/wer': 0.09206013506330743, 'validation/ctc_loss': Array(0.41945672, dtype=float32), 'validation/wer': 0.124429168637825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24109034, dtype=float32), 'test/wer': 0.08019011638535128, 'test/num_examples': 2472, 'score': 28842.233000040054, 'total_duration': 31595.887142896652, 'accumulated_submission_time': 28842.233000040054, 'accumulated_eval_time': 2751.0915093421936, 'accumulated_logging_time': 1.0802249908447266, 'global_step': 36309, 'preemption_count': 0}), (38107, {'train/ctc_loss': Array(0.21433543, dtype=float32), 'train/wer': 0.07941517525051416, 'validation/ctc_loss': Array(0.40897462, dtype=float32), 'validation/wer': 0.12077005512806897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23581171, dtype=float32), 'test/wer': 0.0775699226128816, 'test/num_examples': 2472, 'score': 30282.535950660706, 'total_duration': 33166.483761548996, 'accumulated_submission_time': 30282.535950660706, 'accumulated_eval_time': 2881.2546286582947, 'accumulated_logging_time': 1.1364960670471191, 'global_step': 38107, 'preemption_count': 0}), (39910, {'train/ctc_loss': Array(0.19636467, dtype=float32), 'train/wer': 0.07304349756373686, 'validation/ctc_loss': Array(0.40372702, dtype=float32), 'validation/wer': 0.11818260810797764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2262993, dtype=float32), 'test/wer': 0.07432006987183393, 'test/num_examples': 2472, 'score': 31722.782329320908, 'total_duration': 34739.318687200546, 'accumulated_submission_time': 31722.782329320908, 'accumulated_eval_time': 3013.71555685997, 'accumulated_logging_time': 1.1902563571929932, 'global_step': 39910, 'preemption_count': 0}), (41747, {'train/ctc_loss': Array(0.21572936, dtype=float32), 'train/wer': 0.07966783665782039, 'validation/ctc_loss': Array(0.395749, dtype=float32), 'validation/wer': 0.11628064145514931, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21935742, dtype=float32), 'test/wer': 0.07165925294010116, 'test/num_examples': 2472, 'score': 33163.08148312569, 'total_duration': 36310.41373419762, 'accumulated_submission_time': 33163.08148312569, 'accumulated_eval_time': 3144.3730943202972, 'accumulated_logging_time': 1.2504262924194336, 'global_step': 41747, 'preemption_count': 0}), (43579, {'train/ctc_loss': Array(0.1478013, dtype=float32), 'train/wer': 0.0551468587783352, 'validation/ctc_loss': Array(0.38760498, dtype=float32), 'validation/wer': 0.11348079206773705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21774586, dtype=float32), 'test/wer': 0.07263420876241546, 'test/num_examples': 2472, 'score': 34603.13644838333, 'total_duration': 37891.17783498764, 'accumulated_submission_time': 34603.13644838333, 'accumulated_eval_time': 3284.943300962448, 'accumulated_logging_time': 1.312373399734497, 'global_step': 43579, 'preemption_count': 0}), (45391, {'train/ctc_loss': Array(0.12686214, dtype=float32), 'train/wer': 0.047560816963704955, 'validation/ctc_loss': Array(0.37629935, dtype=float32), 'validation/wer': 0.11128918582310744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21235178, dtype=float32), 'test/wer': 0.07042024658257673, 'test/num_examples': 2472, 'score': 36043.40504741669, 'total_duration': 39465.84740304947, 'accumulated_submission_time': 36043.40504741669, 'accumulated_eval_time': 3419.2076370716095, 'accumulated_logging_time': 1.3747718334197998, 'global_step': 45391, 'preemption_count': 0}), (47211, {'train/ctc_loss': Array(0.1242431, dtype=float32), 'train/wer': 0.0480329439796949, 'validation/ctc_loss': Array(0.3734636, dtype=float32), 'validation/wer': 0.10818038753777383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20668186, dtype=float32), 'test/wer': 0.06869376231389515, 'test/num_examples': 2472, 'score': 37484.02665233612, 'total_duration': 41042.178755521774, 'accumulated_submission_time': 37484.02665233612, 'accumulated_eval_time': 3554.784821510315, 'accumulated_logging_time': 1.4327740669250488, 'global_step': 47211, 'preemption_count': 0}), (49025, {'train/ctc_loss': Array(0.12451019, dtype=float32), 'train/wer': 0.04706751197491119, 'validation/ctc_loss': Array(0.36440134, dtype=float32), 'validation/wer': 0.10629773019106559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20458083, dtype=float32), 'test/wer': 0.06708914752300286, 'test/num_examples': 2472, 'score': 38924.491559267044, 'total_duration': 42617.03544545174, 'accumulated_submission_time': 38924.491559267044, 'accumulated_eval_time': 3689.0430438518524, 'accumulated_logging_time': 1.4914908409118652, 'global_step': 49025, 'preemption_count': 0}), (50856, {'train/ctc_loss': Array(0.11252448, dtype=float32), 'train/wer': 0.043614796043756794, 'validation/ctc_loss': Array(0.35983658, dtype=float32), 'validation/wer': 0.10531295557894127, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1984191, dtype=float32), 'test/wer': 0.06585014116547844, 'test/num_examples': 2472, 'score': 40364.53780937195, 'total_duration': 44188.775580883026, 'accumulated_submission_time': 40364.53780937195, 'accumulated_eval_time': 3820.602771282196, 'accumulated_logging_time': 1.5500528812408447, 'global_step': 50856, 'preemption_count': 0}), (52665, {'train/ctc_loss': Array(0.10154488, dtype=float32), 'train/wer': 0.0393358401807406, 'validation/ctc_loss': Array(0.34599328, dtype=float32), 'validation/wer': 0.10092974308968207, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19205548, dtype=float32), 'test/wer': 0.06300652001706172, 'test/num_examples': 2472, 'score': 41804.75467848778, 'total_duration': 45763.384612083435, 'accumulated_submission_time': 41804.75467848778, 'accumulated_eval_time': 3954.864999771118, 'accumulated_logging_time': 1.6051020622253418, 'global_step': 52665, 'preemption_count': 0}), (54472, {'train/ctc_loss': Array(0.1160069, dtype=float32), 'train/wer': 0.04412523748338989, 'validation/ctc_loss': Array(0.3398731, dtype=float32), 'validation/wer': 0.10018633480405882, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18795243, dtype=float32), 'test/wer': 0.06219405683179981, 'test/num_examples': 2472, 'score': 43245.01002693176, 'total_duration': 47336.69872045517, 'accumulated_submission_time': 43245.01002693176, 'accumulated_eval_time': 4087.795857667923, 'accumulated_logging_time': 1.659766435623169, 'global_step': 54472, 'preemption_count': 0}), (56285, {'train/ctc_loss': Array(0.10221567, dtype=float32), 'train/wer': 0.03673829051718295, 'validation/ctc_loss': Array(0.3412774, dtype=float32), 'validation/wer': 0.09750234125336706, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1826138, dtype=float32), 'test/wer': 0.058761399874068206, 'test/num_examples': 2472, 'score': 44685.059356451035, 'total_duration': 48910.888063669205, 'accumulated_submission_time': 44685.059356451035, 'accumulated_eval_time': 4221.798095703125, 'accumulated_logging_time': 1.7223410606384277, 'global_step': 56285, 'preemption_count': 0}), (58115, {'train/ctc_loss': Array(0.09541807, dtype=float32), 'train/wer': 0.03673023652248029, 'validation/ctc_loss': Array(0.3292542, dtype=float32), 'validation/wer': 0.09447078019251379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18172288, dtype=float32), 'test/wer': 0.05920825462596226, 'test/num_examples': 2472, 'score': 46125.22264838219, 'total_duration': 50485.74221920967, 'accumulated_submission_time': 46125.22264838219, 'accumulated_eval_time': 4356.356876373291, 'accumulated_logging_time': 1.7787425518035889, 'global_step': 58115, 'preemption_count': 0}), (59930, {'train/ctc_loss': Array(0.0784812, dtype=float32), 'train/wer': 0.030712803793716655, 'validation/ctc_loss': Array(0.32586622, dtype=float32), 'validation/wer': 0.09347635092732942, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17646027, dtype=float32), 'test/wer': 0.05675055349054496, 'test/num_examples': 2472, 'score': 47565.73729014397, 'total_duration': 52059.585463523865, 'accumulated_submission_time': 47565.73729014397, 'accumulated_eval_time': 4489.549370765686, 'accumulated_logging_time': 1.8408918380737305, 'global_step': 59930, 'preemption_count': 0}), (61736, {'train/ctc_loss': Array(0.07178165, dtype=float32), 'train/wer': 0.02875196469112674, 'validation/ctc_loss': Array(0.31728697, dtype=float32), 'validation/wer': 0.09083097598887784, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17256908, dtype=float32), 'test/wer': 0.05555217029228363, 'test/num_examples': 2472, 'score': 49006.57438802719, 'total_duration': 53634.54512619972, 'accumulated_submission_time': 49006.57438802719, 'accumulated_eval_time': 4623.5429792404175, 'accumulated_logging_time': 1.8968467712402344, 'global_step': 61736, 'preemption_count': 0}), (63544, {'train/ctc_loss': Array(0.07886387, dtype=float32), 'train/wer': 0.030922318552310122, 'validation/ctc_loss': Array(0.3153456, dtype=float32), 'validation/wer': 0.08900624656052984, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16620629, dtype=float32), 'test/wer': 0.0538866207624967, 'test/num_examples': 2472, 'score': 50446.679966926575, 'total_duration': 55208.704122543335, 'accumulated_submission_time': 50446.679966926575, 'accumulated_eval_time': 4757.461458683014, 'accumulated_logging_time': 1.9577386379241943, 'global_step': 63544, 'preemption_count': 0}), (65377, {'train/ctc_loss': Array(0.07221077, dtype=float32), 'train/wer': 0.028075026739362952, 'validation/ctc_loss': Array(0.3099662, dtype=float32), 'validation/wer': 0.08895797329522964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16686207, dtype=float32), 'test/wer': 0.05354132390876038, 'test/num_examples': 2472, 'score': 51887.27757143974, 'total_duration': 56783.84284877777, 'accumulated_submission_time': 51887.27757143974, 'accumulated_eval_time': 4891.862742900848, 'accumulated_logging_time': 2.0222373008728027, 'global_step': 65377, 'preemption_count': 0}), (67194, {'train/ctc_loss': Array(0.06876019, dtype=float32), 'train/wer': 0.026767290890285293, 'validation/ctc_loss': Array(0.30606192, dtype=float32), 'validation/wer': 0.08766424978518397, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16483718, dtype=float32), 'test/wer': 0.05238356386976215, 'test/num_examples': 2472, 'score': 53327.70025777817, 'total_duration': 58357.71854352951, 'accumulated_submission_time': 53327.70025777817, 'accumulated_eval_time': 5025.178674221039, 'accumulated_logging_time': 2.0835318565368652, 'global_step': 67194, 'preemption_count': 0}), (68998, {'train/ctc_loss': Array(0.06089285, dtype=float32), 'train/wer': 0.023473511397360612, 'validation/ctc_loss': Array(0.30153775, dtype=float32), 'validation/wer': 0.08621605182617763, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1602182, dtype=float32), 'test/wer': 0.051185180671500824, 'test/num_examples': 2472, 'score': 54768.31900382042, 'total_duration': 59931.1682472229, 'accumulated_submission_time': 54768.31900382042, 'accumulated_eval_time': 5157.87672996521, 'accumulated_logging_time': 2.142979860305786, 'global_step': 68998, 'preemption_count': 0}), (70808, {'train/ctc_loss': Array(0.05510921, dtype=float32), 'train/wer': 0.02111032384445463, 'validation/ctc_loss': Array(0.29850617, dtype=float32), 'validation/wer': 0.08411133745908841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15855892, dtype=float32), 'test/wer': 0.05027115958808116, 'test/num_examples': 2472, 'score': 56208.43280529976, 'total_duration': 61505.95262885094, 'accumulated_submission_time': 56208.43280529976, 'accumulated_eval_time': 5292.408869028091, 'accumulated_logging_time': 2.2067055702209473, 'global_step': 70808, 'preemption_count': 0}), (72636, {'train/ctc_loss': Array(0.05994768, dtype=float32), 'train/wer': 0.022203459216047487, 'validation/ctc_loss': Array(0.29521114, dtype=float32), 'validation/wer': 0.08316518145920426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.158432, dtype=float32), 'test/wer': 0.04976337009729247, 'test/num_examples': 2472, 'score': 57648.75893139839, 'total_duration': 63082.29228138924, 'accumulated_submission_time': 57648.75893139839, 'accumulated_eval_time': 5428.279653549194, 'accumulated_logging_time': 2.27409291267395, 'global_step': 72636, 'preemption_count': 0}), (74455, {'train/ctc_loss': Array(0.05741131, dtype=float32), 'train/wer': 0.021588792614786623, 'validation/ctc_loss': Array(0.29396224, dtype=float32), 'validation/wer': 0.08243142782664105, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15610263, dtype=float32), 'test/wer': 0.04966181219913473, 'test/num_examples': 2472, 'score': 59089.160449266434, 'total_duration': 64654.3326253891, 'accumulated_submission_time': 59089.160449266434, 'accumulated_eval_time': 5559.786453962326, 'accumulated_logging_time': 2.330268144607544, 'global_step': 74455, 'preemption_count': 0}), (76255, {'train/ctc_loss': Array(0.05751698, dtype=float32), 'train/wer': 0.021302694306068883, 'validation/ctc_loss': Array(0.29176736, dtype=float32), 'validation/wer': 0.08214178823483978, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15516719, dtype=float32), 'test/wer': 0.0494383848231877, 'test/num_examples': 2472, 'score': 60529.28284430504, 'total_duration': 66228.47186899185, 'accumulated_submission_time': 60529.28284430504, 'accumulated_eval_time': 5693.6730353832245, 'accumulated_logging_time': 2.3865671157836914, 'global_step': 76255, 'preemption_count': 0})], 'global_step': 76955}
I0217 22:10:38.138585 139688679413568 submission_runner.py:586] Timing: 61068.16817140579
I0217 22:10:38.138662 139688679413568 submission_runner.py:588] Total number of evals: 43
I0217 22:10:38.138714 139688679413568 submission_runner.py:589] ====================
I0217 22:10:38.210238 139688679413568 submission_runner.py:673] Final librispeech_conformer score: 61068.082364320755
