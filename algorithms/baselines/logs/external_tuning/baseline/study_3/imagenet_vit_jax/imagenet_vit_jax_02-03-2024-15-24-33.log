python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_3 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=1274177056 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_02-03-2024-15-24-33.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0203 15:24:54.696600 140107197974336 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification/study_3/imagenet_vit_jax because --overwrite was set.
I0203 15:24:54.699100 140107197974336 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax.
I0203 15:24:55.693808 140107197974336 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0203 15:24:55.694565 140107197974336 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0203 15:24:55.694712 140107197974336 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0203 15:24:55.695774 140107197974336 submission_runner.py:542] Using RNG seed 1274177056
I0203 15:24:56.814870 140107197974336 submission_runner.py:551] --- Tuning run 1/5 ---
I0203 15:24:56.815131 140107197974336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_1.
I0203 15:24:56.815393 140107197974336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_1/hparams.json.
I0203 15:24:57.002689 140107197974336 submission_runner.py:206] Initializing dataset.
I0203 15:24:57.019201 140107197974336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0203 15:24:57.029998 140107197974336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0203 15:24:57.418907 140107197974336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0203 15:25:05.812769 140107197974336 submission_runner.py:213] Initializing model.
I0203 15:25:14.724698 140107197974336 submission_runner.py:255] Initializing optimizer.
I0203 15:25:15.723397 140107197974336 submission_runner.py:262] Initializing metrics bundle.
I0203 15:25:15.723602 140107197974336 submission_runner.py:280] Initializing checkpoint and logger.
I0203 15:25:15.724769 140107197974336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0203 15:25:15.724909 140107197974336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0203 15:25:16.080728 140107197974336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0203 15:25:16.401238 140107197974336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_1/flags_0.json.
I0203 15:25:16.410691 140107197974336 submission_runner.py:314] Starting training loop.
I0203 15:25:59.216420 139945217873664 logging_writer.py:48] [0] global_step=0, grad_norm=0.335174024105072, loss=6.907756805419922
I0203 15:25:59.234913 140107197974336 spec.py:321] Evaluating on the training split.
I0203 15:25:59.243184 140107197974336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0203 15:25:59.252168 140107197974336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0203 15:25:59.333669 140107197974336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0203 15:26:16.545387 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 15:26:16.555752 140107197974336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0203 15:26:16.572438 140107197974336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0203 15:26:16.638877 140107197974336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0203 15:26:34.031897 140107197974336 spec.py:349] Evaluating on the test split.
I0203 15:26:34.039566 140107197974336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0203 15:26:34.045653 140107197974336 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0203 15:26:34.093855 140107197974336 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0203 15:26:39.631573 140107197974336 submission_runner.py:408] Time since start: 83.22s, 	Step: 1, 	{'train/accuracy': 0.0007617187220603228, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 42.824132204055786, 'total_duration': 83.2208137512207, 'accumulated_submission_time': 42.824132204055786, 'accumulated_eval_time': 40.396591901779175, 'accumulated_logging_time': 0}
I0203 15:26:39.651262 139910195422976 logging_writer.py:48] [1] accumulated_eval_time=40.396592, accumulated_logging_time=0, accumulated_submission_time=42.824132, global_step=1, preemption_count=0, score=42.824132, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=83.220814, train/accuracy=0.000762, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0203 15:27:42.418976 139942822921984 logging_writer.py:48] [100] global_step=100, grad_norm=0.3944686949253082, loss=6.905935764312744
I0203 15:28:26.408530 139942831314688 logging_writer.py:48] [200] global_step=200, grad_norm=0.456752210855484, loss=6.894866943359375
I0203 15:29:12.030888 139942822921984 logging_writer.py:48] [300] global_step=300, grad_norm=0.5040463209152222, loss=6.868990898132324
I0203 15:29:57.505214 139942831314688 logging_writer.py:48] [400] global_step=400, grad_norm=0.6856747269630432, loss=6.848635673522949
I0203 15:30:43.073004 139942822921984 logging_writer.py:48] [500] global_step=500, grad_norm=0.7926198244094849, loss=6.788407325744629
I0203 15:31:28.654872 139942831314688 logging_writer.py:48] [600] global_step=600, grad_norm=0.6570393443107605, loss=6.740931034088135
I0203 15:32:13.905469 139942822921984 logging_writer.py:48] [700] global_step=700, grad_norm=0.7858898043632507, loss=6.717502593994141
I0203 15:32:59.415246 139942831314688 logging_writer.py:48] [800] global_step=800, grad_norm=1.0388301610946655, loss=6.647991180419922
I0203 15:33:39.839377 140107197974336 spec.py:321] Evaluating on the training split.
I0203 15:33:51.715552 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 15:33:59.783770 140107197974336 spec.py:349] Evaluating on the test split.
I0203 15:34:01.432538 140107197974336 submission_runner.py:408] Time since start: 525.02s, 	Step: 890, 	{'train/accuracy': 0.012402343563735485, 'train/loss': 6.457183837890625, 'validation/accuracy': 0.012240000069141388, 'validation/loss': 6.465394973754883, 'validation/num_examples': 50000, 'test/accuracy': 0.010000000707805157, 'test/loss': 6.503350734710693, 'test/num_examples': 10000, 'score': 462.9493408203125, 'total_duration': 525.0217719078064, 'accumulated_submission_time': 462.9493408203125, 'accumulated_eval_time': 61.98973369598389, 'accumulated_logging_time': 0.0296323299407959}
I0203 15:34:01.451924 139910203815680 logging_writer.py:48] [890] accumulated_eval_time=61.989734, accumulated_logging_time=0.029632, accumulated_submission_time=462.949341, global_step=890, preemption_count=0, score=462.949341, test/accuracy=0.010000, test/loss=6.503351, test/num_examples=10000, total_duration=525.021772, train/accuracy=0.012402, train/loss=6.457184, validation/accuracy=0.012240, validation/loss=6.465395, validation/num_examples=50000
I0203 15:34:05.883674 139910212208384 logging_writer.py:48] [900] global_step=900, grad_norm=1.656842589378357, loss=6.63247013092041
I0203 15:34:46.438087 139910203815680 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.607042670249939, loss=6.584073543548584
I0203 15:35:31.849121 139910212208384 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4284865856170654, loss=6.505380630493164
I0203 15:36:17.428490 139910203815680 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1995850801467896, loss=6.475440979003906
I0203 15:37:02.952202 139910212208384 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.15510892868042, loss=6.406258583068848
I0203 15:37:48.064041 139910203815680 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.7579476833343506, loss=6.388918399810791
I0203 15:38:33.542580 139910212208384 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.8377840518951416, loss=6.376266956329346
I0203 15:39:18.986078 139910203815680 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.482202172279358, loss=6.276200294494629
I0203 15:40:04.177070 139910212208384 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.627733826637268, loss=6.277441024780273
I0203 15:40:49.379267 139910203815680 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.6519711017608643, loss=6.369401454925537
I0203 15:41:01.732705 140107197974336 spec.py:321] Evaluating on the training split.
I0203 15:41:13.877832 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 15:41:21.998984 140107197974336 spec.py:349] Evaluating on the test split.
I0203 15:41:23.615708 140107197974336 submission_runner.py:408] Time since start: 967.20s, 	Step: 1829, 	{'train/accuracy': 0.03935546800494194, 'train/loss': 5.864162445068359, 'validation/accuracy': 0.039239998906850815, 'validation/loss': 5.889999866485596, 'validation/num_examples': 50000, 'test/accuracy': 0.03100000135600567, 'test/loss': 5.997738838195801, 'test/num_examples': 10000, 'score': 883.1624338626862, 'total_duration': 967.2049548625946, 'accumulated_submission_time': 883.1624338626862, 'accumulated_eval_time': 83.87272596359253, 'accumulated_logging_time': 0.06127738952636719}
I0203 15:41:23.632511 139910212208384 logging_writer.py:48] [1829] accumulated_eval_time=83.872726, accumulated_logging_time=0.061277, accumulated_submission_time=883.162434, global_step=1829, preemption_count=0, score=883.162434, test/accuracy=0.031000, test/loss=5.997739, test/num_examples=10000, total_duration=967.204955, train/accuracy=0.039355, train/loss=5.864162, validation/accuracy=0.039240, validation/loss=5.890000, validation/num_examples=50000
I0203 15:41:52.009452 139910203815680 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.473271131515503, loss=6.570653438568115
I0203 15:42:36.202050 139910212208384 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.259100914001465, loss=6.214212417602539
I0203 15:43:21.414918 139910203815680 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.2659308910369873, loss=6.463369846343994
I0203 15:44:06.693403 139910212208384 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.6723264455795288, loss=6.142938613891602
I0203 15:44:52.228551 139910203815680 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.768176794052124, loss=6.0857648849487305
I0203 15:45:37.483924 139910212208384 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.2223926782608032, loss=6.567953109741211
I0203 15:46:22.718118 139910203815680 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.3807823657989502, loss=6.634326457977295
I0203 15:47:08.024698 139910212208384 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.6499238014221191, loss=6.015642166137695
I0203 15:47:53.219554 139910203815680 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.6207423210144043, loss=6.01573371887207
I0203 15:48:23.872964 140107197974336 spec.py:321] Evaluating on the training split.
I0203 15:48:35.888410 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 15:48:44.033832 140107197974336 spec.py:349] Evaluating on the test split.
I0203 15:48:45.646054 140107197974336 submission_runner.py:408] Time since start: 1409.24s, 	Step: 2769, 	{'train/accuracy': 0.07162109017372131, 'train/loss': 5.389620780944824, 'validation/accuracy': 0.06551999598741531, 'validation/loss': 5.453647136688232, 'validation/num_examples': 50000, 'test/accuracy': 0.05100000277161598, 'test/loss': 5.634537220001221, 'test/num_examples': 10000, 'score': 1303.3358738422394, 'total_duration': 1409.2352724075317, 'accumulated_submission_time': 1303.3358738422394, 'accumulated_eval_time': 105.64576721191406, 'accumulated_logging_time': 0.09018278121948242}
I0203 15:48:45.662789 139910212208384 logging_writer.py:48] [2769] accumulated_eval_time=105.645767, accumulated_logging_time=0.090183, accumulated_submission_time=1303.335874, global_step=2769, preemption_count=0, score=1303.335874, test/accuracy=0.051000, test/loss=5.634537, test/num_examples=10000, total_duration=1409.235272, train/accuracy=0.071621, train/loss=5.389621, validation/accuracy=0.065520, validation/loss=5.453647, validation/num_examples=50000
I0203 15:48:58.316511 139910203815680 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.4433343410491943, loss=6.592648983001709
I0203 15:49:40.152851 139910212208384 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.4951707124710083, loss=6.048841953277588
I0203 15:50:25.379570 139910203815680 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.141287326812744, loss=5.878790855407715
I0203 15:51:10.817660 139910212208384 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.5835641622543335, loss=5.938509464263916
I0203 15:51:55.989214 139910203815680 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.8471046686172485, loss=6.531930923461914
I0203 15:52:41.158972 139910212208384 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.8957144021987915, loss=5.877353191375732
I0203 15:53:26.323653 139910203815680 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.505744218826294, loss=5.8509745597839355
I0203 15:54:11.525082 139910212208384 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.6337367296218872, loss=5.911099910736084
I0203 15:54:56.852829 139910203815680 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.373559832572937, loss=6.35803747177124
I0203 15:55:41.981184 139910212208384 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.74263596534729, loss=5.825473785400391
I0203 15:55:45.763885 140107197974336 spec.py:321] Evaluating on the training split.
I0203 15:55:57.783044 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 15:56:05.960809 140107197974336 spec.py:349] Evaluating on the test split.
I0203 15:56:07.605756 140107197974336 submission_runner.py:408] Time since start: 1851.19s, 	Step: 3710, 	{'train/accuracy': 0.10431640595197678, 'train/loss': 5.0515570640563965, 'validation/accuracy': 0.09815999865531921, 'validation/loss': 5.091566562652588, 'validation/num_examples': 50000, 'test/accuracy': 0.07480000704526901, 'test/loss': 5.3220343589782715, 'test/num_examples': 10000, 'score': 1723.3727452754974, 'total_duration': 1851.1949937343597, 'accumulated_submission_time': 1723.3727452754974, 'accumulated_eval_time': 127.48762011528015, 'accumulated_logging_time': 0.11726093292236328}
I0203 15:56:07.623291 139910203815680 logging_writer.py:48] [3710] accumulated_eval_time=127.487620, accumulated_logging_time=0.117261, accumulated_submission_time=1723.372745, global_step=3710, preemption_count=0, score=1723.372745, test/accuracy=0.074800, test/loss=5.322034, test/num_examples=10000, total_duration=1851.194994, train/accuracy=0.104316, train/loss=5.051557, validation/accuracy=0.098160, validation/loss=5.091567, validation/num_examples=50000
I0203 15:56:44.184817 139910212208384 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.7297788858413696, loss=5.787086009979248
I0203 15:57:29.836117 139910203815680 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.5352964401245117, loss=5.760799407958984
I0203 15:58:15.222563 139910212208384 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.6476716995239258, loss=5.695587635040283
I0203 15:59:00.461150 139910203815680 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.4095540046691895, loss=5.9928297996521
I0203 15:59:45.847929 139910212208384 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.542863368988037, loss=5.669907569885254
I0203 16:00:31.398892 139910203815680 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.6690813302993774, loss=5.571576118469238
I0203 16:01:17.021444 139910212208384 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.4018689393997192, loss=6.206124782562256
I0203 16:02:02.511320 139910203815680 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.8544065952301025, loss=5.535836696624756
I0203 16:02:47.833202 139910212208384 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.810788631439209, loss=5.492281913757324
I0203 16:03:07.639858 140107197974336 spec.py:321] Evaluating on the training split.
I0203 16:03:19.618148 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 16:03:28.009814 140107197974336 spec.py:349] Evaluating on the test split.
I0203 16:03:29.616040 140107197974336 submission_runner.py:408] Time since start: 2293.21s, 	Step: 4645, 	{'train/accuracy': 0.13974608480930328, 'train/loss': 4.694349765777588, 'validation/accuracy': 0.13157999515533447, 'validation/loss': 4.74459171295166, 'validation/num_examples': 50000, 'test/accuracy': 0.09930000454187393, 'test/loss': 5.037469387054443, 'test/num_examples': 10000, 'score': 2143.325270175934, 'total_duration': 2293.2052896022797, 'accumulated_submission_time': 2143.325270175934, 'accumulated_eval_time': 149.46379971504211, 'accumulated_logging_time': 0.1450655460357666}
I0203 16:03:29.633946 139910203815680 logging_writer.py:48] [4645] accumulated_eval_time=149.463800, accumulated_logging_time=0.145066, accumulated_submission_time=2143.325270, global_step=4645, preemption_count=0, score=2143.325270, test/accuracy=0.099300, test/loss=5.037469, test/num_examples=10000, total_duration=2293.205290, train/accuracy=0.139746, train/loss=4.694350, validation/accuracy=0.131580, validation/loss=4.744592, validation/num_examples=50000
I0203 16:03:51.697579 139910212208384 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.5961668491363525, loss=5.522282600402832
I0203 16:04:35.290678 139910203815680 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.6056711673736572, loss=5.478859901428223
I0203 16:05:20.640745 139910212208384 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.6883291006088257, loss=5.460700988769531
I0203 16:06:07.590380 139910203815680 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.1609838008880615, loss=6.291738510131836
I0203 16:06:53.601512 139910212208384 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.5313161611557007, loss=5.713638782501221
I0203 16:07:38.928274 139910203815680 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.4077980518341064, loss=5.482608318328857
I0203 16:08:24.395330 139910212208384 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.7079567909240723, loss=5.336761951446533
I0203 16:09:09.664874 139910203815680 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.5457431077957153, loss=5.286737442016602
I0203 16:09:55.235579 139910212208384 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.6251786947250366, loss=5.3433027267456055
I0203 16:10:29.944842 140107197974336 spec.py:321] Evaluating on the training split.
I0203 16:10:42.017644 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 16:10:50.195462 140107197974336 spec.py:349] Evaluating on the test split.
I0203 16:10:51.813741 140107197974336 submission_runner.py:408] Time since start: 2735.40s, 	Step: 5578, 	{'train/accuracy': 0.18845702707767487, 'train/loss': 4.301875114440918, 'validation/accuracy': 0.17127999663352966, 'validation/loss': 4.40316104888916, 'validation/num_examples': 50000, 'test/accuracy': 0.12620000541210175, 'test/loss': 4.740323066711426, 'test/num_examples': 10000, 'score': 2563.570514202118, 'total_duration': 2735.4029870033264, 'accumulated_submission_time': 2563.570514202118, 'accumulated_eval_time': 171.33267450332642, 'accumulated_logging_time': 0.17570042610168457}
I0203 16:10:51.831170 139910203815680 logging_writer.py:48] [5578] accumulated_eval_time=171.332675, accumulated_logging_time=0.175700, accumulated_submission_time=2563.570514, global_step=5578, preemption_count=0, score=2563.570514, test/accuracy=0.126200, test/loss=4.740323, test/num_examples=10000, total_duration=2735.402987, train/accuracy=0.188457, train/loss=4.301875, validation/accuracy=0.171280, validation/loss=4.403161, validation/num_examples=50000
I0203 16:11:00.949676 139910212208384 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.583133578300476, loss=5.978959083557129
I0203 16:11:42.312166 139910203815680 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.5547021627426147, loss=5.326568603515625
I0203 16:12:27.690431 139910212208384 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.4589389562606812, loss=5.134391784667969
I0203 16:13:13.151266 139910203815680 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.6681091785430908, loss=5.17531681060791
I0203 16:13:58.470449 139910212208384 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.6376241445541382, loss=5.512843608856201
I0203 16:14:44.072712 139910203815680 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.2395113706588745, loss=6.048192977905273
I0203 16:15:29.512639 139910212208384 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.683396816253662, loss=5.36741828918457
I0203 16:16:16.188154 139910203815680 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.6589488983154297, loss=5.06532621383667
I0203 16:17:02.356889 139910212208384 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.3850476741790771, loss=6.076913356781006
I0203 16:17:48.205250 139910203815680 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.9404573440551758, loss=4.979800701141357
I0203 16:17:52.083211 140107197974336 spec.py:321] Evaluating on the training split.
I0203 16:18:04.336356 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 16:18:15.441726 140107197974336 spec.py:349] Evaluating on the test split.
I0203 16:18:17.053114 140107197974336 submission_runner.py:408] Time since start: 3180.64s, 	Step: 6510, 	{'train/accuracy': 0.22810547053813934, 'train/loss': 3.9725728034973145, 'validation/accuracy': 0.21327999234199524, 'validation/loss': 4.052278518676758, 'validation/num_examples': 50000, 'test/accuracy': 0.16300000250339508, 'test/loss': 4.438085556030273, 'test/num_examples': 10000, 'score': 2983.7556672096252, 'total_duration': 3180.6423647403717, 'accumulated_submission_time': 2983.7556672096252, 'accumulated_eval_time': 196.30260586738586, 'accumulated_logging_time': 0.20665884017944336}
I0203 16:18:17.072027 139910212208384 logging_writer.py:48] [6510] accumulated_eval_time=196.302606, accumulated_logging_time=0.206659, accumulated_submission_time=2983.755667, global_step=6510, preemption_count=0, score=2983.755667, test/accuracy=0.163000, test/loss=4.438086, test/num_examples=10000, total_duration=3180.642365, train/accuracy=0.228105, train/loss=3.972573, validation/accuracy=0.213280, validation/loss=4.052279, validation/num_examples=50000
I0203 16:18:52.943974 139910203815680 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.628037929534912, loss=5.095163345336914
I0203 16:19:38.029612 139910212208384 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.3099026679992676, loss=6.214256763458252
I0203 16:20:23.715932 139910203815680 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.3146779537200928, loss=5.995239734649658
I0203 16:21:09.365302 139910212208384 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.3909510374069214, loss=6.174356460571289
I0203 16:21:54.796324 139910203815680 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.5318607091903687, loss=6.120980262756348
I0203 16:22:39.972426 139910212208384 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.0307562351226807, loss=5.001682281494141
I0203 16:23:25.533041 139910203815680 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.8804467916488647, loss=5.0745344161987305
I0203 16:24:10.947476 139910212208384 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.6320627927780151, loss=4.869516849517822
I0203 16:24:56.521438 139910203815680 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.6361185312271118, loss=4.921401500701904
I0203 16:25:17.215930 140107197974336 spec.py:321] Evaluating on the training split.
I0203 16:25:29.213849 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 16:25:39.592304 140107197974336 spec.py:349] Evaluating on the test split.
I0203 16:25:41.234275 140107197974336 submission_runner.py:408] Time since start: 3624.82s, 	Step: 7447, 	{'train/accuracy': 0.2765820324420929, 'train/loss': 3.669283628463745, 'validation/accuracy': 0.2522999942302704, 'validation/loss': 3.7821102142333984, 'validation/num_examples': 50000, 'test/accuracy': 0.1923000067472458, 'test/loss': 4.209742546081543, 'test/num_examples': 10000, 'score': 3403.835516691208, 'total_duration': 3624.8235108852386, 'accumulated_submission_time': 3403.835516691208, 'accumulated_eval_time': 220.32091236114502, 'accumulated_logging_time': 0.23564529418945312}
I0203 16:25:41.254657 139910212208384 logging_writer.py:48] [7447] accumulated_eval_time=220.320912, accumulated_logging_time=0.235645, accumulated_submission_time=3403.835517, global_step=7447, preemption_count=0, score=3403.835517, test/accuracy=0.192300, test/loss=4.209743, test/num_examples=10000, total_duration=3624.823511, train/accuracy=0.276582, train/loss=3.669284, validation/accuracy=0.252300, validation/loss=3.782110, validation/num_examples=50000
I0203 16:26:02.795348 139910203815680 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.5397186279296875, loss=5.110126495361328
I0203 16:26:45.755251 139910212208384 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.2702484130859375, loss=4.831049919128418
I0203 16:27:31.118681 139910203815680 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.6236684322357178, loss=4.784464359283447
I0203 16:28:16.607732 139910212208384 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.7342822551727295, loss=4.811120986938477
I0203 16:29:02.157504 139910203815680 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.6204873323440552, loss=4.761536598205566
I0203 16:29:47.665246 139910212208384 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.117584705352783, loss=4.744965076446533
I0203 16:30:32.978044 139910203815680 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.4225780963897705, loss=4.984818458557129
I0203 16:31:18.547354 139910212208384 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.2266931533813477, loss=6.097526550292969
I0203 16:32:03.944911 139910203815680 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.6270639896392822, loss=4.639110088348389
I0203 16:32:41.270581 140107197974336 spec.py:321] Evaluating on the training split.
I0203 16:32:53.433529 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 16:33:07.282019 140107197974336 spec.py:349] Evaluating on the test split.
I0203 16:33:08.888025 140107197974336 submission_runner.py:408] Time since start: 4072.48s, 	Step: 8384, 	{'train/accuracy': 0.30943357944488525, 'train/loss': 3.4023940563201904, 'validation/accuracy': 0.2844800055027008, 'validation/loss': 3.535698652267456, 'validation/num_examples': 50000, 'test/accuracy': 0.22380000352859497, 'test/loss': 3.993281841278076, 'test/num_examples': 10000, 'score': 3823.778416156769, 'total_duration': 4072.47727060318, 'accumulated_submission_time': 3823.778416156769, 'accumulated_eval_time': 247.93834471702576, 'accumulated_logging_time': 0.2755928039550781}
I0203 16:33:08.908569 139910212208384 logging_writer.py:48] [8384] accumulated_eval_time=247.938345, accumulated_logging_time=0.275593, accumulated_submission_time=3823.778416, global_step=8384, preemption_count=0, score=3823.778416, test/accuracy=0.223800, test/loss=3.993282, test/num_examples=10000, total_duration=4072.477271, train/accuracy=0.309434, train/loss=3.402394, validation/accuracy=0.284480, validation/loss=3.535699, validation/num_examples=50000
I0203 16:33:15.647646 139910203815680 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.3369449377059937, loss=5.7212419509887695
I0203 16:33:56.711955 139910212208384 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.4889461994171143, loss=4.935298919677734
I0203 16:34:42.357392 139910203815680 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.6684305667877197, loss=4.902266502380371
I0203 16:35:28.010664 139910212208384 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.12144935131073, loss=5.991038799285889
I0203 16:36:13.876526 139910203815680 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.6637275218963623, loss=4.608274459838867
I0203 16:36:58.962337 139910212208384 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.318281650543213, loss=6.187066555023193
I0203 16:37:44.327205 139910203815680 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.299926996231079, loss=5.564833641052246
I0203 16:38:29.798445 139910212208384 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.5803968906402588, loss=4.583279609680176
I0203 16:39:15.226716 139910203815680 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.7751827239990234, loss=4.564652442932129
I0203 16:40:00.631409 139910212208384 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.4913995265960693, loss=4.502351760864258
I0203 16:40:09.003949 140107197974336 spec.py:321] Evaluating on the training split.
I0203 16:40:21.995955 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 16:40:35.530428 140107197974336 spec.py:349] Evaluating on the test split.
I0203 16:40:37.132960 140107197974336 submission_runner.py:408] Time since start: 4520.72s, 	Step: 9320, 	{'train/accuracy': 0.3596288859844208, 'train/loss': 3.166682004928589, 'validation/accuracy': 0.32023999094963074, 'validation/loss': 3.347395896911621, 'validation/num_examples': 50000, 'test/accuracy': 0.2459000051021576, 'test/loss': 3.8278791904449463, 'test/num_examples': 10000, 'score': 4243.810468912125, 'total_duration': 4520.722212314606, 'accumulated_submission_time': 4243.810468912125, 'accumulated_eval_time': 276.0673451423645, 'accumulated_logging_time': 0.3065640926361084}
I0203 16:40:37.151309 139910203815680 logging_writer.py:48] [9320] accumulated_eval_time=276.067345, accumulated_logging_time=0.306564, accumulated_submission_time=4243.810469, global_step=9320, preemption_count=0, score=4243.810469, test/accuracy=0.245900, test/loss=3.827879, test/num_examples=10000, total_duration=4520.722212, train/accuracy=0.359629, train/loss=3.166682, validation/accuracy=0.320240, validation/loss=3.347396, validation/num_examples=50000
I0203 16:41:09.632104 139910212208384 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0412282943725586, loss=5.976205825805664
I0203 16:41:55.001045 139910203815680 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.817284107208252, loss=4.4215989112854
I0203 16:42:40.336884 139910212208384 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.560893177986145, loss=4.462786674499512
I0203 16:43:25.903382 139910203815680 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.4171172380447388, loss=5.3451128005981445
I0203 16:44:11.606797 139910212208384 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.2561455965042114, loss=5.843137264251709
I0203 16:44:57.183020 139910203815680 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.3113762140274048, loss=6.223316192626953
I0203 16:45:43.066631 139910212208384 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.4268922805786133, loss=4.580583572387695
I0203 16:46:28.486085 139910203815680 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.636704921722412, loss=4.4778642654418945
I0203 16:47:14.027513 139910212208384 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.5578266382217407, loss=4.563469409942627
I0203 16:47:37.501009 140107197974336 spec.py:321] Evaluating on the training split.
I0203 16:47:50.003465 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 16:48:07.211943 140107197974336 spec.py:349] Evaluating on the test split.
I0203 16:48:08.826068 140107197974336 submission_runner.py:408] Time since start: 4972.42s, 	Step: 10253, 	{'train/accuracy': 0.36775389313697815, 'train/loss': 3.1120121479034424, 'validation/accuracy': 0.3455999791622162, 'validation/loss': 3.226508617401123, 'validation/num_examples': 50000, 'test/accuracy': 0.26340001821517944, 'test/loss': 3.720574378967285, 'test/num_examples': 10000, 'score': 4664.097786426544, 'total_duration': 4972.415317296982, 'accumulated_submission_time': 4664.097786426544, 'accumulated_eval_time': 307.3923919200897, 'accumulated_logging_time': 0.3343639373779297}
I0203 16:48:08.848081 139910203815680 logging_writer.py:48] [10253] accumulated_eval_time=307.392392, accumulated_logging_time=0.334364, accumulated_submission_time=4664.097786, global_step=10253, preemption_count=0, score=4664.097786, test/accuracy=0.263400, test/loss=3.720574, test/num_examples=10000, total_duration=4972.415317, train/accuracy=0.367754, train/loss=3.112012, validation/accuracy=0.345600, validation/loss=3.226509, validation/num_examples=50000
I0203 16:48:27.723586 139910212208384 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.6402473449707031, loss=4.421335697174072
I0203 16:49:10.631723 139910203815680 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.524930715560913, loss=4.343973159790039
I0203 16:49:56.364480 139910212208384 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.649481177330017, loss=4.4813032150268555
I0203 16:50:42.190370 139910203815680 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.742287516593933, loss=4.437812328338623
I0203 16:51:27.752291 139910212208384 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.3987882137298584, loss=5.234574317932129
I0203 16:52:13.171068 139910203815680 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.255467414855957, loss=5.571012496948242
I0203 16:52:58.562076 139910212208384 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.6371896266937256, loss=4.312868595123291
I0203 16:53:44.842935 139910203815680 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.3421242237091064, loss=5.316067695617676
I0203 16:54:31.019371 139910212208384 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.6047524213790894, loss=4.314514636993408
I0203 16:55:09.139708 140107197974336 spec.py:321] Evaluating on the training split.
I0203 16:55:21.727829 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 16:55:35.776690 140107197974336 spec.py:349] Evaluating on the test split.
I0203 16:55:37.382413 140107197974336 submission_runner.py:408] Time since start: 5420.97s, 	Step: 11185, 	{'train/accuracy': 0.3995117247104645, 'train/loss': 2.8868730068206787, 'validation/accuracy': 0.36805999279022217, 'validation/loss': 3.0476648807525635, 'validation/num_examples': 50000, 'test/accuracy': 0.2808000147342682, 'test/loss': 3.5686235427856445, 'test/num_examples': 10000, 'score': 5084.326451063156, 'total_duration': 5420.9716629981995, 'accumulated_submission_time': 5084.326451063156, 'accumulated_eval_time': 335.6350944042206, 'accumulated_logging_time': 0.3666074275970459}
I0203 16:55:37.402082 139910203815680 logging_writer.py:48] [11185] accumulated_eval_time=335.635094, accumulated_logging_time=0.366607, accumulated_submission_time=5084.326451, global_step=11185, preemption_count=0, score=5084.326451, test/accuracy=0.280800, test/loss=3.568624, test/num_examples=10000, total_duration=5420.971663, train/accuracy=0.399512, train/loss=2.886873, validation/accuracy=0.368060, validation/loss=3.047665, validation/num_examples=50000
I0203 16:55:43.756340 139910212208384 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.432031273841858, loss=4.192986488342285
I0203 16:56:25.274222 139910203815680 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.504456639289856, loss=4.21632194519043
I0203 16:57:10.878686 139910212208384 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.6259528398513794, loss=4.824007034301758
I0203 16:57:56.658027 139910203815680 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.084869384765625, loss=6.0380859375
I0203 16:58:42.317156 139910212208384 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.1300865411758423, loss=5.930670738220215
I0203 16:59:28.137405 139910203815680 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.587306022644043, loss=4.37580680847168
I0203 17:00:13.738440 139910212208384 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.1610994338989258, loss=5.00703763961792
I0203 17:00:59.565403 139910203815680 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.3607810735702515, loss=4.426066875457764
I0203 17:01:45.067250 139910212208384 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4311336278915405, loss=4.12632942199707
I0203 17:02:30.653898 139910203815680 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.226754069328308, loss=5.259184837341309
I0203 17:02:37.669780 140107197974336 spec.py:321] Evaluating on the training split.
I0203 17:02:50.738920 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 17:03:06.482592 140107197974336 spec.py:349] Evaluating on the test split.
I0203 17:03:08.083233 140107197974336 submission_runner.py:408] Time since start: 5871.67s, 	Step: 12117, 	{'train/accuracy': 0.4317578077316284, 'train/loss': 2.685864210128784, 'validation/accuracy': 0.3901999890804291, 'validation/loss': 2.880558729171753, 'validation/num_examples': 50000, 'test/accuracy': 0.2963000237941742, 'test/loss': 3.4384853839874268, 'test/num_examples': 10000, 'score': 5504.530478954315, 'total_duration': 5871.672466278076, 'accumulated_submission_time': 5504.530478954315, 'accumulated_eval_time': 366.0485026836395, 'accumulated_logging_time': 0.3962712287902832}
I0203 17:03:08.102020 139910212208384 logging_writer.py:48] [12117] accumulated_eval_time=366.048503, accumulated_logging_time=0.396271, accumulated_submission_time=5504.530479, global_step=12117, preemption_count=0, score=5504.530479, test/accuracy=0.296300, test/loss=3.438485, test/num_examples=10000, total_duration=5871.672466, train/accuracy=0.431758, train/loss=2.685864, validation/accuracy=0.390200, validation/loss=2.880559, validation/num_examples=50000
I0203 17:03:41.130059 139910203815680 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.7315024137496948, loss=4.154399871826172
I0203 17:04:26.149207 139910212208384 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.445747971534729, loss=4.3670172691345215
I0203 17:05:12.097864 139910203815680 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.5935548543930054, loss=4.192972183227539
I0203 17:05:58.558901 139910212208384 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.4608993530273438, loss=4.362784385681152
I0203 17:06:45.206226 139910203815680 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.0504589080810547, loss=5.827261924743652
I0203 17:07:31.267001 139910212208384 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.6683778762817383, loss=4.142770290374756
I0203 17:08:17.175670 139910203815680 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.5389208793640137, loss=4.165557384490967
I0203 17:09:03.169728 139910212208384 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.6384673118591309, loss=4.198469638824463
I0203 17:09:49.062387 139910203815680 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.2196078300476074, loss=4.819187641143799
I0203 17:10:08.334694 140107197974336 spec.py:321] Evaluating on the training split.
I0203 17:10:21.013216 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 17:10:36.865635 140107197974336 spec.py:349] Evaluating on the test split.
I0203 17:10:38.476802 140107197974336 submission_runner.py:408] Time since start: 6322.07s, 	Step: 13044, 	{'train/accuracy': 0.4320703148841858, 'train/loss': 2.7094104290008545, 'validation/accuracy': 0.4023999869823456, 'validation/loss': 2.8486061096191406, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.408851385116577, 'test/num_examples': 10000, 'score': 5924.697474718094, 'total_duration': 6322.066045045853, 'accumulated_submission_time': 5924.697474718094, 'accumulated_eval_time': 396.19059109687805, 'accumulated_logging_time': 0.429196834564209}
I0203 17:10:38.500330 139910212208384 logging_writer.py:48] [13044] accumulated_eval_time=396.190591, accumulated_logging_time=0.429197, accumulated_submission_time=5924.697475, global_step=13044, preemption_count=0, score=5924.697475, test/accuracy=0.311500, test/loss=3.408851, test/num_examples=10000, total_duration=6322.066045, train/accuracy=0.432070, train/loss=2.709410, validation/accuracy=0.402400, validation/loss=2.848606, validation/num_examples=50000
I0203 17:11:00.968859 139910203815680 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.4046534299850464, loss=4.196391582489014
I0203 17:11:44.448255 139910212208384 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.8616200685501099, loss=4.083909034729004
I0203 17:12:30.076663 139910203815680 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.5325838327407837, loss=4.051358222961426
I0203 17:13:16.076378 139910212208384 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.390177011489868, loss=4.131888389587402
I0203 17:14:02.583631 139910203815680 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.4874743223190308, loss=4.957622051239014
I0203 17:14:48.836374 139910212208384 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.4397324323654175, loss=4.148898124694824
I0203 17:15:35.270175 139910203815680 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.3671141862869263, loss=4.543794631958008
I0203 17:16:22.180148 139910212208384 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.0689243078231812, loss=5.6754584312438965
I0203 17:17:08.735787 139910203815680 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.3583439588546753, loss=4.3096022605896
I0203 17:17:38.703763 140107197974336 spec.py:321] Evaluating on the training split.
I0203 17:17:54.032245 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 17:18:12.877384 140107197974336 spec.py:349] Evaluating on the test split.
I0203 17:18:14.482860 140107197974336 submission_runner.py:408] Time since start: 6778.07s, 	Step: 13967, 	{'train/accuracy': 0.45130857825279236, 'train/loss': 2.5765609741210938, 'validation/accuracy': 0.42010000348091125, 'validation/loss': 2.7298953533172607, 'validation/num_examples': 50000, 'test/accuracy': 0.3273000121116638, 'test/loss': 3.2973392009735107, 'test/num_examples': 10000, 'score': 6344.838493108749, 'total_duration': 6778.072106599808, 'accumulated_submission_time': 6344.838493108749, 'accumulated_eval_time': 431.96968388557434, 'accumulated_logging_time': 0.46321702003479004}
I0203 17:18:14.499705 139910212208384 logging_writer.py:48] [13967] accumulated_eval_time=431.969684, accumulated_logging_time=0.463217, accumulated_submission_time=6344.838493, global_step=13967, preemption_count=0, score=6344.838493, test/accuracy=0.327300, test/loss=3.297339, test/num_examples=10000, total_duration=6778.072107, train/accuracy=0.451309, train/loss=2.576561, validation/accuracy=0.420100, validation/loss=2.729895, validation/num_examples=50000
I0203 17:18:27.846913 139910203815680 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.1182156801223755, loss=5.184870719909668
I0203 17:19:08.409725 139910212208384 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.5316028594970703, loss=4.559301853179932
I0203 17:19:54.287605 139910203815680 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.94317227602005, loss=5.7241082191467285
I0203 17:20:40.336619 139910212208384 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.003010630607605, loss=5.667450904846191
I0203 17:21:26.344454 139910203815680 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0276299715042114, loss=6.024320602416992
I0203 17:22:12.718601 139910212208384 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.6068822145462036, loss=4.089572906494141
I0203 17:22:58.728223 139910203815680 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.519930124282837, loss=4.038116455078125
I0203 17:23:45.102466 139910212208384 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.4418694972991943, loss=4.059835433959961
I0203 17:24:31.304186 139910203815680 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.3633993864059448, loss=4.1674885749816895
I0203 17:25:14.559564 140107197974336 spec.py:321] Evaluating on the training split.
I0203 17:25:30.205475 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 17:25:47.952057 140107197974336 spec.py:349] Evaluating on the test split.
I0203 17:25:49.560863 140107197974336 submission_runner.py:408] Time since start: 7233.15s, 	Step: 14896, 	{'train/accuracy': 0.4676562249660492, 'train/loss': 2.475290536880493, 'validation/accuracy': 0.4269999861717224, 'validation/loss': 2.6804628372192383, 'validation/num_examples': 50000, 'test/accuracy': 0.33420002460479736, 'test/loss': 3.250800371170044, 'test/num_examples': 10000, 'score': 6764.83682847023, 'total_duration': 7233.150120258331, 'accumulated_submission_time': 6764.83682847023, 'accumulated_eval_time': 466.97098088264465, 'accumulated_logging_time': 0.489241361618042}
I0203 17:25:49.581812 139910212208384 logging_writer.py:48] [14896] accumulated_eval_time=466.970981, accumulated_logging_time=0.489241, accumulated_submission_time=6764.836828, global_step=14896, preemption_count=0, score=6764.836828, test/accuracy=0.334200, test/loss=3.250800, test/num_examples=10000, total_duration=7233.150120, train/accuracy=0.467656, train/loss=2.475291, validation/accuracy=0.427000, validation/loss=2.680463, validation/num_examples=50000
I0203 17:25:51.552248 139910203815680 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.5945746898651123, loss=3.8527960777282715
I0203 17:26:31.147230 139910212208384 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.3827906847000122, loss=3.993638753890991
I0203 17:27:16.727206 139910203815680 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.4525716304779053, loss=4.161553859710693
I0203 17:28:02.898909 139910212208384 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.498909592628479, loss=4.015702724456787
I0203 17:28:48.905519 139910203815680 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.4548367261886597, loss=4.101805210113525
I0203 17:29:34.993991 139910212208384 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.171094298362732, loss=5.164604187011719
I0203 17:30:21.189060 139910203815680 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.5339381694793701, loss=4.055019378662109
I0203 17:31:07.036716 139910212208384 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.328176498413086, loss=4.152284622192383
I0203 17:31:52.812544 139910203815680 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.5061122179031372, loss=4.045289993286133
I0203 17:32:38.794455 139910212208384 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.4073306322097778, loss=4.166623592376709
I0203 17:32:49.958458 140107197974336 spec.py:321] Evaluating on the training split.
I0203 17:33:06.519040 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 17:33:26.691654 140107197974336 spec.py:349] Evaluating on the test split.
I0203 17:33:28.285209 140107197974336 submission_runner.py:408] Time since start: 7691.87s, 	Step: 15826, 	{'train/accuracy': 0.46556639671325684, 'train/loss': 2.52592134475708, 'validation/accuracy': 0.435619980096817, 'validation/loss': 2.66223406791687, 'validation/num_examples': 50000, 'test/accuracy': 0.33400002121925354, 'test/loss': 3.236973285675049, 'test/num_examples': 10000, 'score': 7185.151596069336, 'total_duration': 7691.874465227127, 'accumulated_submission_time': 7185.151596069336, 'accumulated_eval_time': 505.29772305488586, 'accumulated_logging_time': 0.520289421081543}
I0203 17:33:28.302910 139910203815680 logging_writer.py:48] [15826] accumulated_eval_time=505.297723, accumulated_logging_time=0.520289, accumulated_submission_time=7185.151596, global_step=15826, preemption_count=0, score=7185.151596, test/accuracy=0.334000, test/loss=3.236973, test/num_examples=10000, total_duration=7691.874465, train/accuracy=0.465566, train/loss=2.525921, validation/accuracy=0.435620, validation/loss=2.662234, validation/num_examples=50000
I0203 17:33:57.803406 139910212208384 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.6428598165512085, loss=4.549970626831055
I0203 17:34:42.014313 139910203815680 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.3101712465286255, loss=3.930363178253174
I0203 17:35:27.999887 139910212208384 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.4226633310317993, loss=3.931915044784546
I0203 17:36:14.033857 139910203815680 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.440106987953186, loss=4.253078937530518
I0203 17:37:00.221993 139910212208384 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.592247486114502, loss=4.604142189025879
I0203 17:37:46.518799 139910203815680 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.6853753328323364, loss=4.019252777099609
I0203 17:38:32.621882 139910212208384 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.0682857036590576, loss=5.682137489318848
I0203 17:39:18.558365 139910203815680 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.3749901056289673, loss=3.882070541381836
I0203 17:40:04.554710 139910212208384 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.3083951473236084, loss=3.9709701538085938
I0203 17:40:28.647936 140107197974336 spec.py:321] Evaluating on the training split.
I0203 17:40:44.330871 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 17:41:06.858217 140107197974336 spec.py:349] Evaluating on the test split.
I0203 17:41:08.451188 140107197974336 submission_runner.py:408] Time since start: 8152.04s, 	Step: 16754, 	{'train/accuracy': 0.47822263836860657, 'train/loss': 2.447401762008667, 'validation/accuracy': 0.4474399983882904, 'validation/loss': 2.5987234115600586, 'validation/num_examples': 50000, 'test/accuracy': 0.3508000075817108, 'test/loss': 3.1780033111572266, 'test/num_examples': 10000, 'score': 7605.435419559479, 'total_duration': 8152.040428161621, 'accumulated_submission_time': 7605.435419559479, 'accumulated_eval_time': 545.1009593009949, 'accumulated_logging_time': 0.5469293594360352}
I0203 17:41:08.467144 139910203815680 logging_writer.py:48] [16754] accumulated_eval_time=545.100959, accumulated_logging_time=0.546929, accumulated_submission_time=7605.435420, global_step=16754, preemption_count=0, score=7605.435420, test/accuracy=0.350800, test/loss=3.178003, test/num_examples=10000, total_duration=8152.040428, train/accuracy=0.478223, train/loss=2.447402, validation/accuracy=0.447440, validation/loss=2.598723, validation/num_examples=50000
I0203 17:41:26.944437 139910212208384 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.1161863803863525, loss=4.8156633377075195
I0203 17:42:10.099975 139910203815680 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.314520001411438, loss=4.094076633453369
I0203 17:42:55.870431 139910212208384 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1518231630325317, loss=4.538728713989258
I0203 17:43:41.536844 139910203815680 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.3088934421539307, loss=4.356841564178467
I0203 17:44:27.329403 139910212208384 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.356614112854004, loss=3.90786075592041
I0203 17:45:13.303431 139910203815680 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.2991808652877808, loss=4.588393688201904
I0203 17:45:59.244120 139910212208384 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.3695495128631592, loss=4.750812530517578
I0203 17:46:45.634696 139910203815680 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.3287022113800049, loss=3.9039766788482666
I0203 17:47:31.717935 139910212208384 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.3669735193252563, loss=3.955312490463257
I0203 17:48:08.839451 140107197974336 spec.py:321] Evaluating on the training split.
I0203 17:48:25.001321 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 17:48:48.335476 140107197974336 spec.py:349] Evaluating on the test split.
I0203 17:48:49.935117 140107197974336 submission_runner.py:408] Time since start: 8613.52s, 	Step: 17682, 	{'train/accuracy': 0.4890234172344208, 'train/loss': 2.3615238666534424, 'validation/accuracy': 0.4515799880027771, 'validation/loss': 2.543088912963867, 'validation/num_examples': 50000, 'test/accuracy': 0.34850001335144043, 'test/loss': 3.1464133262634277, 'test/num_examples': 10000, 'score': 8025.745712041855, 'total_duration': 8613.524369716644, 'accumulated_submission_time': 8025.745712041855, 'accumulated_eval_time': 586.1966207027435, 'accumulated_logging_time': 0.5725915431976318}
I0203 17:48:49.956222 139910203815680 logging_writer.py:48] [17682] accumulated_eval_time=586.196621, accumulated_logging_time=0.572592, accumulated_submission_time=8025.745712, global_step=17682, preemption_count=0, score=8025.745712, test/accuracy=0.348500, test/loss=3.146413, test/num_examples=10000, total_duration=8613.524370, train/accuracy=0.489023, train/loss=2.361524, validation/accuracy=0.451580, validation/loss=2.543089, validation/num_examples=50000
I0203 17:48:57.428815 139910212208384 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.508683681488037, loss=3.8369765281677246
I0203 17:49:39.192446 139910203815680 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.3283826112747192, loss=3.883211135864258
I0203 17:50:25.551494 139910212208384 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.2936092615127563, loss=4.084863185882568
I0203 17:51:11.819734 139910203815680 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.2742944955825806, loss=3.937617063522339
I0203 17:51:58.021670 139910212208384 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.2767796516418457, loss=3.7744076251983643
I0203 17:52:44.238951 139910203815680 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.2504127025604248, loss=4.588736534118652
I0203 17:53:30.511983 139910212208384 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.2174410820007324, loss=4.424421787261963
I0203 17:54:16.594671 139910203815680 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.2805078029632568, loss=3.841825008392334
I0203 17:55:02.604871 139910212208384 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.316970705986023, loss=3.8795034885406494
I0203 17:55:48.482220 139910203815680 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.2507400512695312, loss=3.8224895000457764
I0203 17:55:49.966709 140107197974336 spec.py:321] Evaluating on the training split.
I0203 17:56:04.867778 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 17:56:27.542033 140107197974336 spec.py:349] Evaluating on the test split.
I0203 17:56:29.145438 140107197974336 submission_runner.py:408] Time since start: 9072.73s, 	Step: 18605, 	{'train/accuracy': 0.5197656154632568, 'train/loss': 2.2128489017486572, 'validation/accuracy': 0.4608999788761139, 'validation/loss': 2.4904685020446777, 'validation/num_examples': 50000, 'test/accuracy': 0.3604000210762024, 'test/loss': 3.0848608016967773, 'test/num_examples': 10000, 'score': 8445.693154335022, 'total_duration': 9072.734701156616, 'accumulated_submission_time': 8445.693154335022, 'accumulated_eval_time': 625.3753478527069, 'accumulated_logging_time': 0.6053094863891602}
I0203 17:56:29.164133 139910212208384 logging_writer.py:48] [18605] accumulated_eval_time=625.375348, accumulated_logging_time=0.605309, accumulated_submission_time=8445.693154, global_step=18605, preemption_count=0, score=8445.693154, test/accuracy=0.360400, test/loss=3.084861, test/num_examples=10000, total_duration=9072.734701, train/accuracy=0.519766, train/loss=2.212849, validation/accuracy=0.460900, validation/loss=2.490469, validation/num_examples=50000
I0203 17:57:07.375324 139910203815680 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.3960762023925781, loss=3.8227105140686035
I0203 17:57:53.480962 139910212208384 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.0553770065307617, loss=5.00691556930542
I0203 17:58:39.677653 139910203815680 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.263999581336975, loss=4.173825740814209
I0203 17:59:25.914406 139910212208384 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.413022756576538, loss=3.773005962371826
I0203 18:00:11.862035 139910203815680 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.9263901114463806, loss=5.725637435913086
I0203 18:00:57.836906 139910212208384 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.1895419359207153, loss=3.8688855171203613
I0203 18:01:43.761242 139910203815680 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.3118540048599243, loss=3.8969342708587646
I0203 18:02:29.747331 139910212208384 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.609873652458191, loss=3.918196678161621
I0203 18:03:15.759432 139910203815680 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.9570788741111755, loss=5.858193397521973
I0203 18:03:29.169228 140107197974336 spec.py:321] Evaluating on the training split.
I0203 18:03:43.572687 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 18:04:08.835486 140107197974336 spec.py:349] Evaluating on the test split.
I0203 18:04:10.433289 140107197974336 submission_runner.py:408] Time since start: 9534.02s, 	Step: 19531, 	{'train/accuracy': 0.5014843344688416, 'train/loss': 2.3330583572387695, 'validation/accuracy': 0.4675999879837036, 'validation/loss': 2.486729145050049, 'validation/num_examples': 50000, 'test/accuracy': 0.36400002241134644, 'test/loss': 3.0885519981384277, 'test/num_examples': 10000, 'score': 8865.636800050735, 'total_duration': 9534.022550106049, 'accumulated_submission_time': 8865.636800050735, 'accumulated_eval_time': 666.6394157409668, 'accumulated_logging_time': 0.6336965560913086}
I0203 18:04:12.167252 139910212208384 logging_writer.py:48] [19531] accumulated_eval_time=666.639416, accumulated_logging_time=0.633697, accumulated_submission_time=8865.636800, global_step=19531, preemption_count=0, score=8865.636800, test/accuracy=0.364000, test/loss=3.088552, test/num_examples=10000, total_duration=9534.022550, train/accuracy=0.501484, train/loss=2.333058, validation/accuracy=0.467600, validation/loss=2.486729, validation/num_examples=50000
I0203 18:04:39.685998 139910203815680 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.2897858619689941, loss=4.2918219566345215
I0203 18:05:24.124423 139910212208384 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.202655553817749, loss=3.8844048976898193
I0203 18:06:09.778186 139910203815680 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1585980653762817, loss=4.1222076416015625
I0203 18:06:55.374740 139910212208384 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.3248845338821411, loss=3.86328125
I0203 18:07:41.518004 139910203815680 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.332414984703064, loss=3.7890148162841797
I0203 18:08:28.754981 139910212208384 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.0575376749038696, loss=5.522562503814697
I0203 18:09:15.494174 139910203815680 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.1268694400787354, loss=4.580862045288086
I0203 18:10:02.058765 139910212208384 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.0240886211395264, loss=5.107606887817383
I0203 18:10:47.947319 139910203815680 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.8276649713516235, loss=5.891306400299072
I0203 18:11:10.635182 140107197974336 spec.py:321] Evaluating on the training split.
I0203 18:11:25.425563 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 18:11:48.296104 140107197974336 spec.py:349] Evaluating on the test split.
I0203 18:11:49.900907 140107197974336 submission_runner.py:408] Time since start: 9993.49s, 	Step: 20451, 	{'train/accuracy': 0.5143749713897705, 'train/loss': 2.221330165863037, 'validation/accuracy': 0.4759399890899658, 'validation/loss': 2.4047062397003174, 'validation/num_examples': 50000, 'test/accuracy': 0.37530001997947693, 'test/loss': 3.02260684967041, 'test/num_examples': 10000, 'score': 9284.03717637062, 'total_duration': 9993.490169763565, 'accumulated_submission_time': 9284.03717637062, 'accumulated_eval_time': 705.9051666259766, 'accumulated_logging_time': 2.3828179836273193}
I0203 18:11:49.918406 139910212208384 logging_writer.py:48] [20451] accumulated_eval_time=705.905167, accumulated_logging_time=2.382818, accumulated_submission_time=9284.037176, global_step=20451, preemption_count=0, score=9284.037176, test/accuracy=0.375300, test/loss=3.022607, test/num_examples=10000, total_duration=9993.490170, train/accuracy=0.514375, train/loss=2.221330, validation/accuracy=0.475940, validation/loss=2.404706, validation/num_examples=50000
I0203 18:12:09.554758 139910203815680 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.5463188886642456, loss=3.853426456451416
I0203 18:12:53.236237 139910212208384 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.8530426025390625, loss=5.886279106140137
I0203 18:13:39.358172 139910203815680 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.4110459089279175, loss=3.797335624694824
I0203 18:14:25.603441 139910212208384 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.3141597509384155, loss=3.8187248706817627
I0203 18:15:12.078606 139910203815680 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.0656336545944214, loss=5.8445963859558105
I0203 18:15:57.869293 139910212208384 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0809128284454346, loss=5.2059125900268555
I0203 18:16:43.786741 139910203815680 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.191382884979248, loss=3.957437515258789
I0203 18:17:29.845827 139910212208384 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.985410451889038, loss=3.8240299224853516
I0203 18:18:16.196860 139910203815680 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.2869125604629517, loss=3.766968011856079
I0203 18:18:49.920984 140107197974336 spec.py:321] Evaluating on the training split.
I0203 18:19:04.781070 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 18:19:26.366842 140107197974336 spec.py:349] Evaluating on the test split.
I0203 18:19:27.965422 140107197974336 submission_runner.py:408] Time since start: 10451.55s, 	Step: 21375, 	{'train/accuracy': 0.5318750143051147, 'train/loss': 2.1464452743530273, 'validation/accuracy': 0.48201999068260193, 'validation/loss': 2.3802831172943115, 'validation/num_examples': 50000, 'test/accuracy': 0.37450000643730164, 'test/loss': 3.0088958740234375, 'test/num_examples': 10000, 'score': 9703.975273609161, 'total_duration': 10451.55467915535, 'accumulated_submission_time': 9703.975273609161, 'accumulated_eval_time': 743.9496030807495, 'accumulated_logging_time': 2.413090705871582}
I0203 18:19:27.982600 139910212208384 logging_writer.py:48] [21375] accumulated_eval_time=743.949603, accumulated_logging_time=2.413091, accumulated_submission_time=9703.975274, global_step=21375, preemption_count=0, score=9703.975274, test/accuracy=0.374500, test/loss=3.008896, test/num_examples=10000, total_duration=10451.554679, train/accuracy=0.531875, train/loss=2.146445, validation/accuracy=0.482020, validation/loss=2.380283, validation/num_examples=50000
I0203 18:19:38.192777 139910203815680 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9700244069099426, loss=5.540953636169434
I0203 18:20:20.581355 139910212208384 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.0251662731170654, loss=4.46254301071167
I0203 18:21:06.632994 139910203815680 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.376600742340088, loss=3.8150079250335693
I0203 18:21:52.795682 139910212208384 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.2528785467147827, loss=3.887025833129883
I0203 18:22:38.897383 139910203815680 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.3118475675582886, loss=3.9024083614349365
I0203 18:23:25.079776 139910212208384 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.9918157458305359, loss=4.889382839202881
I0203 18:24:11.300666 139910203815680 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.4093005657196045, loss=3.7706620693206787
I0203 18:24:57.411788 139910212208384 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.1913847923278809, loss=3.88698673248291
I0203 18:25:43.674926 139910203815680 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.272188425064087, loss=3.7209455966949463
I0203 18:26:28.252215 140107197974336 spec.py:321] Evaluating on the training split.
I0203 18:26:39.231856 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 18:27:01.420722 140107197974336 spec.py:349] Evaluating on the test split.
I0203 18:27:03.023019 140107197974336 submission_runner.py:408] Time since start: 10906.61s, 	Step: 22298, 	{'train/accuracy': 0.525390625, 'train/loss': 2.1639983654022217, 'validation/accuracy': 0.49187999963760376, 'validation/loss': 2.3201968669891357, 'validation/num_examples': 50000, 'test/accuracy': 0.3839000165462494, 'test/loss': 2.934317111968994, 'test/num_examples': 10000, 'score': 10124.183888912201, 'total_duration': 10906.612278938293, 'accumulated_submission_time': 10124.183888912201, 'accumulated_eval_time': 778.7204098701477, 'accumulated_logging_time': 2.43961763381958}
I0203 18:27:03.041379 139910212208384 logging_writer.py:48] [22298] accumulated_eval_time=778.720410, accumulated_logging_time=2.439618, accumulated_submission_time=10124.183889, global_step=22298, preemption_count=0, score=10124.183889, test/accuracy=0.383900, test/loss=2.934317, test/num_examples=10000, total_duration=10906.612279, train/accuracy=0.525391, train/loss=2.163998, validation/accuracy=0.491880, validation/loss=2.320197, validation/num_examples=50000
I0203 18:27:04.223785 139910203815680 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.273027777671814, loss=5.7167744636535645
I0203 18:27:44.738517 139910212208384 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.3438138961791992, loss=3.7123019695281982
I0203 18:28:30.539957 139910203815680 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.3016139268875122, loss=3.693650722503662
I0203 18:29:16.968055 139910212208384 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.2812001705169678, loss=3.7106263637542725
I0203 18:30:03.342159 139910203815680 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.3373916149139404, loss=3.8861782550811768
I0203 18:30:49.473062 139910212208384 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.9637715220451355, loss=5.744685649871826
I0203 18:31:35.877296 139910203815680 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.3626289367675781, loss=3.8401076793670654
I0203 18:32:21.824363 139910212208384 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.4668549299240112, loss=3.71311354637146
I0203 18:33:07.762498 139910203815680 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.095708966255188, loss=4.030475616455078
I0203 18:33:53.861786 139910212208384 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.961215615272522, loss=5.737274169921875
I0203 18:34:03.342668 140107197974336 spec.py:321] Evaluating on the training split.
I0203 18:34:14.444819 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 18:34:38.396023 140107197974336 spec.py:349] Evaluating on the test split.
I0203 18:34:39.993710 140107197974336 submission_runner.py:408] Time since start: 11363.58s, 	Step: 23222, 	{'train/accuracy': 0.5257226228713989, 'train/loss': 2.1706438064575195, 'validation/accuracy': 0.49365997314453125, 'validation/loss': 2.339932680130005, 'validation/num_examples': 50000, 'test/accuracy': 0.38930001854896545, 'test/loss': 2.94062876701355, 'test/num_examples': 10000, 'score': 10544.424010038376, 'total_duration': 11363.582971572876, 'accumulated_submission_time': 10544.424010038376, 'accumulated_eval_time': 815.3714473247528, 'accumulated_logging_time': 2.4672508239746094}
I0203 18:34:40.011551 139910203815680 logging_writer.py:48] [23222] accumulated_eval_time=815.371447, accumulated_logging_time=2.467251, accumulated_submission_time=10544.424010, global_step=23222, preemption_count=0, score=10544.424010, test/accuracy=0.389300, test/loss=2.940629, test/num_examples=10000, total_duration=11363.582972, train/accuracy=0.525723, train/loss=2.170644, validation/accuracy=0.493660, validation/loss=2.339933, validation/num_examples=50000
I0203 18:35:11.067568 139910212208384 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.3004149198532104, loss=4.164340019226074
I0203 18:35:56.396504 139910203815680 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.4149588346481323, loss=3.883917808532715
I0203 18:36:42.708404 139910212208384 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.495194673538208, loss=3.720848321914673
I0203 18:37:29.055016 139910203815680 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.1138532161712646, loss=5.762149333953857
I0203 18:38:14.851258 139910212208384 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.3257752656936646, loss=3.9050984382629395
I0203 18:39:00.940649 139910203815680 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.1864479780197144, loss=4.171544075012207
I0203 18:39:47.104667 139910212208384 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.1874988079071045, loss=4.020432472229004
I0203 18:40:32.959882 139910203815680 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.3801149129867554, loss=3.706334352493286
I0203 18:41:19.197097 139910212208384 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.8867916464805603, loss=5.683002471923828
I0203 18:41:40.049667 140107197974336 spec.py:321] Evaluating on the training split.
I0203 18:41:51.534986 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 18:42:14.499405 140107197974336 spec.py:349] Evaluating on the test split.
I0203 18:42:16.093499 140107197974336 submission_runner.py:408] Time since start: 11819.68s, 	Step: 24147, 	{'train/accuracy': 0.5456640720367432, 'train/loss': 2.072561502456665, 'validation/accuracy': 0.5017399787902832, 'validation/loss': 2.2792351245880127, 'validation/num_examples': 50000, 'test/accuracy': 0.39570000767707825, 'test/loss': 2.8866732120513916, 'test/num_examples': 10000, 'score': 10964.400071620941, 'total_duration': 11819.682758808136, 'accumulated_submission_time': 10964.400071620941, 'accumulated_eval_time': 851.4152765274048, 'accumulated_logging_time': 2.4943430423736572}
I0203 18:42:16.111648 139910203815680 logging_writer.py:48] [24147] accumulated_eval_time=851.415277, accumulated_logging_time=2.494343, accumulated_submission_time=10964.400072, global_step=24147, preemption_count=0, score=10964.400072, test/accuracy=0.395700, test/loss=2.886673, test/num_examples=10000, total_duration=11819.682759, train/accuracy=0.545664, train/loss=2.072562, validation/accuracy=0.501740, validation/loss=2.279235, validation/num_examples=50000
I0203 18:42:37.331178 139910212208384 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.4019060134887695, loss=3.722501277923584
I0203 18:43:21.198078 139910203815680 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.4067307710647583, loss=3.6021785736083984
I0203 18:44:07.226332 139910212208384 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.268681287765503, loss=3.737858295440674
I0203 18:44:53.483840 139910203815680 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.4588624238967896, loss=3.643221855163574
I0203 18:45:39.495218 139910212208384 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.3644375801086426, loss=3.5470659732818604
I0203 18:46:25.750764 139910203815680 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.3674981594085693, loss=3.7187442779541016
I0203 18:47:11.907795 139910212208384 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.0913763046264648, loss=5.035136699676514
I0203 18:47:57.814322 139910203815680 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.3162440061569214, loss=3.7169313430786133
I0203 18:48:43.876118 139910212208384 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.3553078174591064, loss=3.943370819091797
I0203 18:49:16.111680 140107197974336 spec.py:321] Evaluating on the training split.
I0203 18:49:27.345322 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 18:49:50.321892 140107197974336 spec.py:349] Evaluating on the test split.
I0203 18:49:51.918559 140107197974336 submission_runner.py:408] Time since start: 12275.51s, 	Step: 25071, 	{'train/accuracy': 0.5464843511581421, 'train/loss': 2.0741374492645264, 'validation/accuracy': 0.5124599933624268, 'validation/loss': 2.23947811126709, 'validation/num_examples': 50000, 'test/accuracy': 0.40380001068115234, 'test/loss': 2.85197377204895, 'test/num_examples': 10000, 'score': 11384.33907365799, 'total_duration': 12275.507807016373, 'accumulated_submission_time': 11384.33907365799, 'accumulated_eval_time': 887.2221512794495, 'accumulated_logging_time': 2.5220894813537598}
I0203 18:49:51.941025 139910203815680 logging_writer.py:48] [25071] accumulated_eval_time=887.222151, accumulated_logging_time=2.522089, accumulated_submission_time=11384.339074, global_step=25071, preemption_count=0, score=11384.339074, test/accuracy=0.403800, test/loss=2.851974, test/num_examples=10000, total_duration=12275.507807, train/accuracy=0.546484, train/loss=2.074137, validation/accuracy=0.512460, validation/loss=2.239478, validation/num_examples=50000
I0203 18:50:03.731641 139910212208384 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8890925645828247, loss=5.489066123962402
I0203 18:50:46.002133 139910203815680 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.222408413887024, loss=3.6789467334747314
I0203 18:51:32.443945 139910212208384 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.526370882987976, loss=4.100057125091553
I0203 18:52:21.135885 139910203815680 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.5517234802246094, loss=3.7532825469970703
I0203 18:53:07.948124 139910212208384 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.2392204999923706, loss=4.04514741897583
I0203 18:53:54.651019 139910203815680 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.1050329208374023, loss=4.432631492614746
I0203 18:54:41.078557 139910212208384 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.4098892211914062, loss=3.6923177242279053
I0203 18:55:27.333425 139910203815680 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.214504361152649, loss=4.70741605758667
I0203 18:56:13.724600 139910212208384 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.1927906274795532, loss=4.5259175300598145
I0203 18:56:52.189030 140107197974336 spec.py:321] Evaluating on the training split.
I0203 18:57:03.568347 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 18:57:27.407845 140107197974336 spec.py:349] Evaluating on the test split.
I0203 18:57:29.013448 140107197974336 submission_runner.py:408] Time since start: 12732.60s, 	Step: 25985, 	{'train/accuracy': 0.5458202958106995, 'train/loss': 2.1079187393188477, 'validation/accuracy': 0.5095399618148804, 'validation/loss': 2.2755002975463867, 'validation/num_examples': 50000, 'test/accuracy': 0.40570002794265747, 'test/loss': 2.873729705810547, 'test/num_examples': 10000, 'score': 11804.526224374771, 'total_duration': 12732.602709293365, 'accumulated_submission_time': 11804.526224374771, 'accumulated_eval_time': 924.0465886592865, 'accumulated_logging_time': 2.5542385578155518}
I0203 18:57:29.033824 139910203815680 logging_writer.py:48] [25985] accumulated_eval_time=924.046589, accumulated_logging_time=2.554239, accumulated_submission_time=11804.526224, global_step=25985, preemption_count=0, score=11804.526224, test/accuracy=0.405700, test/loss=2.873730, test/num_examples=10000, total_duration=12732.602709, train/accuracy=0.545820, train/loss=2.107919, validation/accuracy=0.509540, validation/loss=2.275500, validation/num_examples=50000
I0203 18:57:35.333706 139910212208384 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.5670244693756104, loss=3.6207563877105713
I0203 18:58:16.772367 139910203815680 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.2758924961090088, loss=3.655306339263916
I0203 18:59:02.978692 139910212208384 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.9581121802330017, loss=5.396819114685059
I0203 18:59:49.020989 139910203815680 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.3333784341812134, loss=3.759164810180664
I0203 19:00:35.464027 139910212208384 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.3666620254516602, loss=3.7763404846191406
I0203 19:01:21.545436 139910203815680 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.256293773651123, loss=3.700427770614624
I0203 19:02:07.571857 139910212208384 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.1764121055603027, loss=4.019774436950684
I0203 19:02:53.764764 139910203815680 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.4847193956375122, loss=3.500824451446533
I0203 19:03:39.898838 139910212208384 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.2965315580368042, loss=3.7677783966064453
I0203 19:04:26.038936 139910203815680 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.4469770193099976, loss=4.096432685852051
I0203 19:04:29.030479 140107197974336 spec.py:321] Evaluating on the training split.
I0203 19:04:39.555381 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 19:05:03.215962 140107197974336 spec.py:349] Evaluating on the test split.
I0203 19:05:04.814015 140107197974336 submission_runner.py:408] Time since start: 13188.40s, 	Step: 26908, 	{'train/accuracy': 0.554394543170929, 'train/loss': 2.04179310798645, 'validation/accuracy': 0.5141599774360657, 'validation/loss': 2.2237548828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4059000313282013, 'test/loss': 2.8440353870391846, 'test/num_examples': 10000, 'score': 12224.458804368973, 'total_duration': 13188.403272867203, 'accumulated_submission_time': 12224.458804368973, 'accumulated_eval_time': 959.8301196098328, 'accumulated_logging_time': 2.5867295265197754}
I0203 19:05:04.833252 139910212208384 logging_writer.py:48] [26908] accumulated_eval_time=959.830120, accumulated_logging_time=2.586730, accumulated_submission_time=12224.458804, global_step=26908, preemption_count=0, score=12224.458804, test/accuracy=0.405900, test/loss=2.844035, test/num_examples=10000, total_duration=13188.403273, train/accuracy=0.554395, train/loss=2.041793, validation/accuracy=0.514160, validation/loss=2.223755, validation/num_examples=50000
I0203 19:05:42.014712 139910203815680 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.4398328065872192, loss=3.5893123149871826
I0203 19:06:28.006289 139910212208384 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.3160321712493896, loss=3.5527470111846924
I0203 19:07:14.470637 139910203815680 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.1201562881469727, loss=4.897960662841797
I0203 19:08:00.613895 139910212208384 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.2629448175430298, loss=3.9194483757019043
I0203 19:08:46.821778 139910203815680 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.6254360675811768, loss=3.6753768920898438
I0203 19:09:33.049483 139910212208384 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.2849855422973633, loss=4.085931777954102
I0203 19:10:19.476779 139910203815680 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.4049140214920044, loss=4.038839340209961
I0203 19:11:05.371402 139910212208384 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.1753872632980347, loss=4.852980613708496
I0203 19:11:51.397948 139910203815680 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.3155726194381714, loss=3.7421815395355225
I0203 19:12:05.028090 140107197974336 spec.py:321] Evaluating on the training split.
I0203 19:12:15.842034 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 19:12:39.116305 140107197974336 spec.py:349] Evaluating on the test split.
I0203 19:12:40.703979 140107197974336 submission_runner.py:408] Time since start: 13644.29s, 	Step: 27831, 	{'train/accuracy': 0.5831835865974426, 'train/loss': 1.9377491474151611, 'validation/accuracy': 0.5192999839782715, 'validation/loss': 2.2157299518585205, 'validation/num_examples': 50000, 'test/accuracy': 0.4076000154018402, 'test/loss': 2.831859588623047, 'test/num_examples': 10000, 'score': 12644.591686487198, 'total_duration': 13644.293235778809, 'accumulated_submission_time': 12644.591686487198, 'accumulated_eval_time': 995.5060038566589, 'accumulated_logging_time': 2.6159276962280273}
I0203 19:12:40.724992 139910212208384 logging_writer.py:48] [27831] accumulated_eval_time=995.506004, accumulated_logging_time=2.615928, accumulated_submission_time=12644.591686, global_step=27831, preemption_count=0, score=12644.591686, test/accuracy=0.407600, test/loss=2.831860, test/num_examples=10000, total_duration=13644.293236, train/accuracy=0.583184, train/loss=1.937749, validation/accuracy=0.519300, validation/loss=2.215730, validation/num_examples=50000
I0203 19:13:08.497237 139910203815680 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.3756054639816284, loss=3.6777565479278564
I0203 19:13:53.359647 139910212208384 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.2786368131637573, loss=3.5440409183502197
I0203 19:14:39.837393 139910203815680 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.403957724571228, loss=3.6485633850097656
I0203 19:15:26.209020 139910212208384 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.4870383739471436, loss=3.556885004043579
I0203 19:16:12.334250 139910203815680 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.337833046913147, loss=3.5229592323303223
I0203 19:16:58.546853 139910212208384 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.108331322669983, loss=5.4800262451171875
I0203 19:17:44.823383 139910203815680 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.4861562252044678, loss=3.5563902854919434
I0203 19:18:30.785996 139910212208384 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.025044322013855, loss=5.435132026672363
I0203 19:19:16.965134 139910203815680 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.029839277267456, loss=5.719093322753906
I0203 19:19:41.123691 140107197974336 spec.py:321] Evaluating on the training split.
I0203 19:19:51.571772 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 19:20:16.082507 140107197974336 spec.py:349] Evaluating on the test split.
I0203 19:20:17.688476 140107197974336 submission_runner.py:408] Time since start: 14101.28s, 	Step: 28754, 	{'train/accuracy': 0.56103515625, 'train/loss': 2.0178725719451904, 'validation/accuracy': 0.5253599882125854, 'validation/loss': 2.184187412261963, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.8013460636138916, 'test/num_examples': 10000, 'score': 13064.928541898727, 'total_duration': 14101.277722358704, 'accumulated_submission_time': 13064.928541898727, 'accumulated_eval_time': 1032.0707716941833, 'accumulated_logging_time': 2.6466317176818848}
I0203 19:20:17.713896 139910212208384 logging_writer.py:48] [28754] accumulated_eval_time=1032.070772, accumulated_logging_time=2.646632, accumulated_submission_time=13064.928542, global_step=28754, preemption_count=0, score=13064.928542, test/accuracy=0.415800, test/loss=2.801346, test/num_examples=10000, total_duration=14101.277722, train/accuracy=0.561035, train/loss=2.017873, validation/accuracy=0.525360, validation/loss=2.184187, validation/num_examples=50000
I0203 19:20:36.179478 139910203815680 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.1668798923492432, loss=4.911312103271484
I0203 19:21:19.615652 139910212208384 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.3465816974639893, loss=3.5132319927215576
I0203 19:22:05.744107 139910203815680 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.215061068534851, loss=4.034041881561279
I0203 19:22:51.885410 139910212208384 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.1361371278762817, loss=4.251889705657959
I0203 19:23:37.934797 139910203815680 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.9797318577766418, loss=5.395199775695801
I0203 19:24:24.182636 139910212208384 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.5603950023651123, loss=3.6275975704193115
I0203 19:25:10.429944 139910203815680 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.3960607051849365, loss=3.782716989517212
I0203 19:25:56.335730 139910212208384 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.2119089365005493, loss=4.905440807342529
I0203 19:26:42.255448 139910203815680 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.40137779712677, loss=3.577552318572998
I0203 19:27:17.861981 140107197974336 spec.py:321] Evaluating on the training split.
I0203 19:27:28.678183 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 19:27:51.977890 140107197974336 spec.py:349] Evaluating on the test split.
I0203 19:27:53.577729 140107197974336 submission_runner.py:408] Time since start: 14557.17s, 	Step: 29679, 	{'train/accuracy': 0.5685741901397705, 'train/loss': 1.9686657190322876, 'validation/accuracy': 0.5270999670028687, 'validation/loss': 2.1590123176574707, 'validation/num_examples': 50000, 'test/accuracy': 0.4166000187397003, 'test/loss': 2.762173652648926, 'test/num_examples': 10000, 'score': 13485.01470541954, 'total_duration': 14557.166967391968, 'accumulated_submission_time': 13485.01470541954, 'accumulated_eval_time': 1067.7865002155304, 'accumulated_logging_time': 2.6822452545166016}
I0203 19:27:53.603265 139910212208384 logging_writer.py:48] [29679] accumulated_eval_time=1067.786500, accumulated_logging_time=2.682245, accumulated_submission_time=13485.014705, global_step=29679, preemption_count=0, score=13485.014705, test/accuracy=0.416600, test/loss=2.762174, test/num_examples=10000, total_duration=14557.166967, train/accuracy=0.568574, train/loss=1.968666, validation/accuracy=0.527100, validation/loss=2.159012, validation/num_examples=50000
I0203 19:28:02.255553 139910203815680 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.3277779817581177, loss=3.5568649768829346
I0203 19:28:44.073304 139910212208384 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.5184783935546875, loss=3.5657806396484375
I0203 19:29:30.182470 139910203815680 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.0568287372589111, loss=4.871772289276123
I0203 19:30:16.698119 139910212208384 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.3712444305419922, loss=3.9366888999938965
I0203 19:31:03.240962 139910203815680 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.227729082107544, loss=3.619485378265381
I0203 19:31:49.174431 139910212208384 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.1565526723861694, loss=4.112338066101074
I0203 19:32:35.437716 139910203815680 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.0209578275680542, loss=5.534213066101074
I0203 19:33:21.711677 139910212208384 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.5733399391174316, loss=3.5154547691345215
I0203 19:34:07.820409 139910203815680 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0926966667175293, loss=5.632367134094238
I0203 19:34:54.016752 139910212208384 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.3784843683242798, loss=3.5687296390533447
I0203 19:34:54.031404 140107197974336 spec.py:321] Evaluating on the training split.
I0203 19:35:04.960805 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 19:35:28.348430 140107197974336 spec.py:349] Evaluating on the test split.
I0203 19:35:29.948074 140107197974336 submission_runner.py:408] Time since start: 15013.54s, 	Step: 30601, 	{'train/accuracy': 0.5899999737739563, 'train/loss': 1.8909555673599243, 'validation/accuracy': 0.5362399816513062, 'validation/loss': 2.1361539363861084, 'validation/num_examples': 50000, 'test/accuracy': 0.4199000298976898, 'test/loss': 2.7619855403900146, 'test/num_examples': 10000, 'score': 13905.378865480423, 'total_duration': 15013.537324428558, 'accumulated_submission_time': 13905.378865480423, 'accumulated_eval_time': 1103.7031581401825, 'accumulated_logging_time': 2.7200143337249756}
I0203 19:35:29.970031 139910203815680 logging_writer.py:48] [30601] accumulated_eval_time=1103.703158, accumulated_logging_time=2.720014, accumulated_submission_time=13905.378865, global_step=30601, preemption_count=0, score=13905.378865, test/accuracy=0.419900, test/loss=2.761986, test/num_examples=10000, total_duration=15013.537324, train/accuracy=0.590000, train/loss=1.890956, validation/accuracy=0.536240, validation/loss=2.136154, validation/num_examples=50000
I0203 19:36:10.401237 139910212208384 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.0695726871490479, loss=4.93152379989624
I0203 19:36:56.379003 139910203815680 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.404938817024231, loss=3.6087098121643066
I0203 19:37:42.736101 139910212208384 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.2057663202285767, loss=4.11271333694458
I0203 19:38:28.834895 139910203815680 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.1170183420181274, loss=5.33721399307251
I0203 19:39:14.920242 139910212208384 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.3824284076690674, loss=3.8939208984375
I0203 19:40:01.037755 139910203815680 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.471457839012146, loss=3.42105770111084
I0203 19:40:47.021066 139910212208384 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.3671866655349731, loss=5.737710952758789
I0203 19:41:33.087933 139910203815680 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.264356255531311, loss=3.960017442703247
I0203 19:42:19.460723 139910212208384 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.4424386024475098, loss=3.751204490661621
I0203 19:42:30.235534 140107197974336 spec.py:321] Evaluating on the training split.
I0203 19:42:41.072691 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 19:43:04.815273 140107197974336 spec.py:349] Evaluating on the test split.
I0203 19:43:06.417810 140107197974336 submission_runner.py:408] Time since start: 15470.01s, 	Step: 31525, 	{'train/accuracy': 0.5753320455551147, 'train/loss': 1.9117754697799683, 'validation/accuracy': 0.5366199612617493, 'validation/loss': 2.086362838745117, 'validation/num_examples': 50000, 'test/accuracy': 0.42250001430511475, 'test/loss': 2.7142233848571777, 'test/num_examples': 10000, 'score': 14325.58296585083, 'total_duration': 15470.007066965103, 'accumulated_submission_time': 14325.58296585083, 'accumulated_eval_time': 1139.8854219913483, 'accumulated_logging_time': 2.7515509128570557}
I0203 19:43:06.439553 139910203815680 logging_writer.py:48] [31525] accumulated_eval_time=1139.885422, accumulated_logging_time=2.751551, accumulated_submission_time=14325.582966, global_step=31525, preemption_count=0, score=14325.582966, test/accuracy=0.422500, test/loss=2.714223, test/num_examples=10000, total_duration=15470.007067, train/accuracy=0.575332, train/loss=1.911775, validation/accuracy=0.536620, validation/loss=2.086363, validation/num_examples=50000
I0203 19:43:36.315032 139910212208384 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.3454238176345825, loss=3.4853742122650146
I0203 19:44:22.154255 139910203815680 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.1051530838012695, loss=4.887310981750488
I0203 19:45:08.714285 139910212208384 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.4165289402008057, loss=3.8478591442108154
I0203 19:45:54.705645 139910203815680 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.1910775899887085, loss=4.01847505569458
I0203 19:46:40.911426 139910212208384 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.3158435821533203, loss=3.607900381088257
I0203 19:47:26.961211 139910203815680 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.079415202140808, loss=4.2830586433410645
I0203 19:48:13.180664 139910212208384 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.3727474212646484, loss=3.4773550033569336
I0203 19:48:59.339613 139910203815680 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.3982442617416382, loss=3.4776558876037598
I0203 19:49:45.316350 139910212208384 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.3964663743972778, loss=4.143858432769775
I0203 19:50:06.473775 140107197974336 spec.py:321] Evaluating on the training split.
I0203 19:50:17.195979 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 19:50:40.705424 140107197974336 spec.py:349] Evaluating on the test split.
I0203 19:50:42.300508 140107197974336 submission_runner.py:408] Time since start: 15925.89s, 	Step: 32447, 	{'train/accuracy': 0.5731250047683716, 'train/loss': 1.9334790706634521, 'validation/accuracy': 0.5350800156593323, 'validation/loss': 2.114149808883667, 'validation/num_examples': 50000, 'test/accuracy': 0.41780000925064087, 'test/loss': 2.760493278503418, 'test/num_examples': 10000, 'score': 14745.557694911957, 'total_duration': 15925.88976097107, 'accumulated_submission_time': 14745.557694911957, 'accumulated_eval_time': 1175.7121279239655, 'accumulated_logging_time': 2.7820372581481934}
I0203 19:50:42.324472 139910203815680 logging_writer.py:48] [32447] accumulated_eval_time=1175.712128, accumulated_logging_time=2.782037, accumulated_submission_time=14745.557695, global_step=32447, preemption_count=0, score=14745.557695, test/accuracy=0.417800, test/loss=2.760493, test/num_examples=10000, total_duration=15925.889761, train/accuracy=0.573125, train/loss=1.933479, validation/accuracy=0.535080, validation/loss=2.114150, validation/num_examples=50000
I0203 19:51:03.546919 139910212208384 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.3601714372634888, loss=3.42073917388916
I0203 19:51:47.527138 139910203815680 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.3676140308380127, loss=3.9328794479370117
I0203 19:52:34.128155 139910212208384 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.0906699895858765, loss=4.455211639404297
I0203 19:53:21.012432 139910203815680 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0329123735427856, loss=5.494849681854248
I0203 19:54:07.312839 139910212208384 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.1184029579162598, loss=5.075011730194092
I0203 19:54:53.660237 139910203815680 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.419884443283081, loss=3.510993242263794
I0203 19:55:40.039563 139910212208384 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.4455902576446533, loss=3.6862192153930664
I0203 19:56:26.069597 139910203815680 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.2920194864273071, loss=3.9315569400787354
I0203 19:57:12.557116 139910212208384 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.398297667503357, loss=5.752285480499268
I0203 19:57:42.613533 140107197974336 spec.py:321] Evaluating on the training split.
I0203 19:57:53.127039 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 19:58:16.375585 140107197974336 spec.py:349] Evaluating on the test split.
I0203 19:58:17.991753 140107197974336 submission_runner.py:408] Time since start: 16381.58s, 	Step: 33367, 	{'train/accuracy': 0.5793749690055847, 'train/loss': 1.8931059837341309, 'validation/accuracy': 0.5353599786758423, 'validation/loss': 2.1022086143493652, 'validation/num_examples': 50000, 'test/accuracy': 0.4253000319004059, 'test/loss': 2.7308449745178223, 'test/num_examples': 10000, 'score': 15165.785947084427, 'total_duration': 16381.58100271225, 'accumulated_submission_time': 15165.785947084427, 'accumulated_eval_time': 1211.0903453826904, 'accumulated_logging_time': 2.8150620460510254}
I0203 19:58:18.018068 139910203815680 logging_writer.py:48] [33367] accumulated_eval_time=1211.090345, accumulated_logging_time=2.815062, accumulated_submission_time=15165.785947, global_step=33367, preemption_count=0, score=15165.785947, test/accuracy=0.425300, test/loss=2.730845, test/num_examples=10000, total_duration=16381.581003, train/accuracy=0.579375, train/loss=1.893106, validation/accuracy=0.535360, validation/loss=2.102209, validation/num_examples=50000
I0203 19:58:31.406905 139910212208384 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.9309151768684387, loss=5.130053520202637
I0203 19:59:13.871111 139910203815680 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.6194119453430176, loss=3.609269380569458
I0203 20:00:00.339071 139910212208384 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.0752638578414917, loss=5.4312872886657715
I0203 20:00:46.814121 139910203815680 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.430439829826355, loss=3.516751766204834
I0203 20:01:33.290559 139910212208384 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.2063761949539185, loss=4.2936601638793945
I0203 20:02:19.327560 139910203815680 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.107674241065979, loss=4.873196601867676
I0203 20:03:05.334330 139910212208384 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.0304502248764038, loss=5.028011798858643
I0203 20:03:51.347886 139910203815680 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.4619781970977783, loss=3.5218889713287354
I0203 20:04:37.930310 139910212208384 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.2156122922897339, loss=3.9186441898345947
I0203 20:05:18.430350 140107197974336 spec.py:321] Evaluating on the training split.
I0203 20:05:29.550956 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 20:05:53.656094 140107197974336 spec.py:349] Evaluating on the test split.
I0203 20:05:55.255531 140107197974336 submission_runner.py:408] Time since start: 16838.84s, 	Step: 34289, 	{'train/accuracy': 0.5872460603713989, 'train/loss': 1.8730449676513672, 'validation/accuracy': 0.5454199910163879, 'validation/loss': 2.0598084926605225, 'validation/num_examples': 50000, 'test/accuracy': 0.43480002880096436, 'test/loss': 2.68808913230896, 'test/num_examples': 10000, 'score': 15586.13660979271, 'total_duration': 16838.84478354454, 'accumulated_submission_time': 15586.13660979271, 'accumulated_eval_time': 1247.915519475937, 'accumulated_logging_time': 2.8516042232513428}
I0203 20:05:55.277059 139910203815680 logging_writer.py:48] [34289] accumulated_eval_time=1247.915519, accumulated_logging_time=2.851604, accumulated_submission_time=15586.136610, global_step=34289, preemption_count=0, score=15586.136610, test/accuracy=0.434800, test/loss=2.688089, test/num_examples=10000, total_duration=16838.844784, train/accuracy=0.587246, train/loss=1.873045, validation/accuracy=0.545420, validation/loss=2.059808, validation/num_examples=50000
I0203 20:06:00.003502 139910212208384 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.3369195461273193, loss=3.5309879779815674
I0203 20:06:41.436391 139910203815680 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.5489534139633179, loss=3.48555064201355
I0203 20:07:27.559178 139910212208384 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.3804795742034912, loss=3.479433298110962
I0203 20:08:13.903428 139910203815680 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.1618634462356567, loss=4.767331123352051
I0203 20:09:00.128652 139910212208384 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.0942336320877075, loss=5.691152095794678
I0203 20:09:46.321519 139910203815680 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.5097761154174805, loss=3.607717514038086
I0203 20:10:32.549175 139910212208384 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.0854023694992065, loss=4.321488857269287
I0203 20:11:18.377462 139910203815680 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.1843855381011963, loss=4.81703519821167
I0203 20:12:04.511264 139910212208384 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.3299055099487305, loss=3.5306568145751953
I0203 20:12:50.576317 139910203815680 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.3294377326965332, loss=3.5573363304138184
I0203 20:12:55.360664 140107197974336 spec.py:321] Evaluating on the training split.
I0203 20:13:05.850259 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 20:13:29.574662 140107197974336 spec.py:349] Evaluating on the test split.
I0203 20:13:31.177314 140107197974336 submission_runner.py:408] Time since start: 17294.77s, 	Step: 35212, 	{'train/accuracy': 0.583691418170929, 'train/loss': 1.8888550996780396, 'validation/accuracy': 0.5408999919891357, 'validation/loss': 2.083228588104248, 'validation/num_examples': 50000, 'test/accuracy': 0.4269000291824341, 'test/loss': 2.722228527069092, 'test/num_examples': 10000, 'score': 16006.159181833267, 'total_duration': 17294.766576051712, 'accumulated_submission_time': 16006.159181833267, 'accumulated_eval_time': 1283.7321796417236, 'accumulated_logging_time': 2.881927490234375}
I0203 20:13:31.200517 139910212208384 logging_writer.py:48] [35212] accumulated_eval_time=1283.732180, accumulated_logging_time=2.881927, accumulated_submission_time=16006.159182, global_step=35212, preemption_count=0, score=16006.159182, test/accuracy=0.426900, test/loss=2.722229, test/num_examples=10000, total_duration=17294.766576, train/accuracy=0.583691, train/loss=1.888855, validation/accuracy=0.540900, validation/loss=2.083229, validation/num_examples=50000
I0203 20:14:06.739829 139910203815680 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.2240127325057983, loss=3.9169528484344482
I0203 20:14:52.749601 139910212208384 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.2570306062698364, loss=3.890549659729004
I0203 20:15:39.175891 139910203815680 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.3129814863204956, loss=4.391792297363281
I0203 20:16:25.777512 139910212208384 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.1593761444091797, loss=4.78841495513916
I0203 20:17:11.969605 139910203815680 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.4127804040908813, loss=3.4944887161254883
I0203 20:17:58.104141 139910212208384 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.4675512313842773, loss=3.626821517944336
I0203 20:18:44.437136 139910203815680 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.2037278413772583, loss=4.517479419708252
I0203 20:19:30.520645 139910212208384 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.4826875925064087, loss=3.530717134475708
I0203 20:20:17.461105 139910203815680 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.126395344734192, loss=5.633070468902588
I0203 20:20:31.360018 140107197974336 spec.py:321] Evaluating on the training split.
I0203 20:20:42.123628 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 20:21:05.604206 140107197974336 spec.py:349] Evaluating on the test split.
I0203 20:21:07.202088 140107197974336 submission_runner.py:408] Time since start: 17750.79s, 	Step: 36132, 	{'train/accuracy': 0.5895312428474426, 'train/loss': 1.8599177598953247, 'validation/accuracy': 0.5429800152778625, 'validation/loss': 2.068998336791992, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.674314498901367, 'test/num_examples': 10000, 'score': 16426.25867486, 'total_duration': 17750.791348934174, 'accumulated_submission_time': 16426.25867486, 'accumulated_eval_time': 1319.5742392539978, 'accumulated_logging_time': 2.9135375022888184}
I0203 20:21:07.225849 139910212208384 logging_writer.py:48] [36132] accumulated_eval_time=1319.574239, accumulated_logging_time=2.913538, accumulated_submission_time=16426.258675, global_step=36132, preemption_count=0, score=16426.258675, test/accuracy=0.435300, test/loss=2.674314, test/num_examples=10000, total_duration=17750.791349, train/accuracy=0.589531, train/loss=1.859918, validation/accuracy=0.542980, validation/loss=2.068998, validation/num_examples=50000
I0203 20:21:34.356474 139910203815680 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.4578980207443237, loss=3.635406970977783
I0203 20:22:19.405370 139910212208384 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.592987060546875, loss=3.432103157043457
I0203 20:23:05.806420 139910203815680 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.4246891736984253, loss=3.5705885887145996
I0203 20:23:52.320957 139910212208384 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1468498706817627, loss=4.2456440925598145
I0203 20:24:38.871738 139910203815680 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.423401951789856, loss=3.5063672065734863
I0203 20:25:25.093087 139910212208384 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.3003010749816895, loss=3.869933843612671
I0203 20:26:11.438972 139910203815680 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.3448123931884766, loss=3.5667784214019775
I0203 20:26:57.460361 139910212208384 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.3665300607681274, loss=3.4442789554595947
I0203 20:27:43.716480 139910203815680 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.124239206314087, loss=5.26533842086792
I0203 20:28:07.540729 140107197974336 spec.py:321] Evaluating on the training split.
I0203 20:28:18.092214 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 20:28:42.059897 140107197974336 spec.py:349] Evaluating on the test split.
I0203 20:28:43.661584 140107197974336 submission_runner.py:408] Time since start: 18207.25s, 	Step: 37053, 	{'train/accuracy': 0.5921093821525574, 'train/loss': 1.889419674873352, 'validation/accuracy': 0.5410400032997131, 'validation/loss': 2.1071622371673584, 'validation/num_examples': 50000, 'test/accuracy': 0.4245000183582306, 'test/loss': 2.7397515773773193, 'test/num_examples': 10000, 'score': 16846.513379335403, 'total_duration': 18207.25081396103, 'accumulated_submission_time': 16846.513379335403, 'accumulated_eval_time': 1355.6950623989105, 'accumulated_logging_time': 2.946035861968994}
I0203 20:28:43.684520 139910212208384 logging_writer.py:48] [37053] accumulated_eval_time=1355.695062, accumulated_logging_time=2.946036, accumulated_submission_time=16846.513379, global_step=37053, preemption_count=0, score=16846.513379, test/accuracy=0.424500, test/loss=2.739752, test/num_examples=10000, total_duration=18207.250814, train/accuracy=0.592109, train/loss=1.889420, validation/accuracy=0.541040, validation/loss=2.107162, validation/num_examples=50000
I0203 20:29:02.580024 139910203815680 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.0825920104980469, loss=5.035337448120117
I0203 20:29:46.296723 139910212208384 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.3837692737579346, loss=3.7161853313446045
I0203 20:30:32.899432 139910203815680 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.4477277994155884, loss=3.402523994445801
I0203 20:31:19.428886 139910212208384 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.404510498046875, loss=3.4523911476135254
I0203 20:32:05.429262 139910203815680 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.1767240762710571, loss=4.039736270904541
I0203 20:32:51.618730 139910212208384 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.442133903503418, loss=3.455064296722412
I0203 20:33:37.938308 139910203815680 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.4899967908859253, loss=3.510134220123291
I0203 20:34:24.158163 139910212208384 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.1311298608779907, loss=5.280431270599365
I0203 20:35:10.419375 139910203815680 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.3806334733963013, loss=3.445667266845703
I0203 20:35:43.865864 140107197974336 spec.py:321] Evaluating on the training split.
I0203 20:35:54.476416 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 20:36:18.209633 140107197974336 spec.py:349] Evaluating on the test split.
I0203 20:36:19.817348 140107197974336 submission_runner.py:408] Time since start: 18663.41s, 	Step: 37974, 	{'train/accuracy': 0.5863280892372131, 'train/loss': 1.8857553005218506, 'validation/accuracy': 0.5454800128936768, 'validation/loss': 2.069470167160034, 'validation/num_examples': 50000, 'test/accuracy': 0.4247000217437744, 'test/loss': 2.710883617401123, 'test/num_examples': 10000, 'score': 17266.63311100006, 'total_duration': 18663.406596660614, 'accumulated_submission_time': 17266.63311100006, 'accumulated_eval_time': 1391.6465280056, 'accumulated_logging_time': 2.9790091514587402}
I0203 20:36:19.841284 139910212208384 logging_writer.py:48] [37974] accumulated_eval_time=1391.646528, accumulated_logging_time=2.979009, accumulated_submission_time=17266.633111, global_step=37974, preemption_count=0, score=17266.633111, test/accuracy=0.424700, test/loss=2.710884, test/num_examples=10000, total_duration=18663.406597, train/accuracy=0.586328, train/loss=1.885755, validation/accuracy=0.545480, validation/loss=2.069470, validation/num_examples=50000
I0203 20:36:30.462445 139910203815680 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.5205817222595215, loss=3.4553678035736084
I0203 20:37:12.784363 139910212208384 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.4879117012023926, loss=3.383159637451172
I0203 20:37:58.751126 139910203815680 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.3774967193603516, loss=3.440581798553467
I0203 20:38:45.175289 139910212208384 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.0613751411437988, loss=5.1441450119018555
I0203 20:39:31.307410 139910203815680 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.3409826755523682, loss=3.6086058616638184
I0203 20:40:17.648169 139910212208384 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.4765853881835938, loss=3.4581940174102783
I0203 20:41:03.895423 139910203815680 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.6381360292434692, loss=3.71293044090271
I0203 20:41:49.903441 139910212208384 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.3163316249847412, loss=3.8283276557922363
I0203 20:42:36.085353 139910203815680 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.39817214012146, loss=3.5430681705474854
I0203 20:43:20.192192 140107197974336 spec.py:321] Evaluating on the training split.
I0203 20:43:30.726425 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 20:43:55.132176 140107197974336 spec.py:349] Evaluating on the test split.
I0203 20:43:56.737669 140107197974336 submission_runner.py:408] Time since start: 19120.33s, 	Step: 38897, 	{'train/accuracy': 0.5941210985183716, 'train/loss': 1.8830095529556274, 'validation/accuracy': 0.5507400035858154, 'validation/loss': 2.086865186691284, 'validation/num_examples': 50000, 'test/accuracy': 0.43410003185272217, 'test/loss': 2.703827381134033, 'test/num_examples': 10000, 'score': 17686.921609401703, 'total_duration': 19120.326916456223, 'accumulated_submission_time': 17686.921609401703, 'accumulated_eval_time': 1428.1919829845428, 'accumulated_logging_time': 3.0127615928649902}
I0203 20:43:56.762444 139910212208384 logging_writer.py:48] [38897] accumulated_eval_time=1428.191983, accumulated_logging_time=3.012762, accumulated_submission_time=17686.921609, global_step=38897, preemption_count=0, score=17686.921609, test/accuracy=0.434100, test/loss=2.703827, test/num_examples=10000, total_duration=19120.326916, train/accuracy=0.594121, train/loss=1.883010, validation/accuracy=0.550740, validation/loss=2.086865, validation/num_examples=50000
I0203 20:43:58.337936 139910203815680 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.100929617881775, loss=5.5035600662231445
I0203 20:44:38.911333 139910212208384 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.100908875465393, loss=5.0310797691345215
I0203 20:45:25.180192 139910203815680 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.4421314001083374, loss=3.607009172439575
I0203 20:46:11.291776 139910212208384 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.4308667182922363, loss=3.87031888961792
I0203 20:46:57.633729 139910203815680 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.516183853149414, loss=3.4412455558776855
I0203 20:47:43.436893 139910212208384 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.1856271028518677, loss=4.157055854797363
I0203 20:48:29.878113 139910203815680 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.5185821056365967, loss=3.544435977935791
I0203 20:49:16.120829 139910212208384 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.631479263305664, loss=3.3545215129852295
I0203 20:50:02.137042 139910203815680 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.4988676309585571, loss=3.3439245223999023
I0203 20:50:48.580583 139910212208384 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.3686134815216064, loss=4.032050132751465
I0203 20:50:57.102741 140107197974336 spec.py:321] Evaluating on the training split.
I0203 20:51:07.801016 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 20:51:31.663529 140107197974336 spec.py:349] Evaluating on the test split.
I0203 20:51:33.267796 140107197974336 submission_runner.py:408] Time since start: 19576.86s, 	Step: 39821, 	{'train/accuracy': 0.6173437237739563, 'train/loss': 1.7155250310897827, 'validation/accuracy': 0.555620014667511, 'validation/loss': 2.0076487064361572, 'validation/num_examples': 50000, 'test/accuracy': 0.4416000247001648, 'test/loss': 2.6214611530303955, 'test/num_examples': 10000, 'score': 18107.200429201126, 'total_duration': 19576.857031822205, 'accumulated_submission_time': 18107.200429201126, 'accumulated_eval_time': 1464.3569984436035, 'accumulated_logging_time': 3.0470962524414062}
I0203 20:51:33.289242 139910203815680 logging_writer.py:48] [39821] accumulated_eval_time=1464.356998, accumulated_logging_time=3.047096, accumulated_submission_time=18107.200429, global_step=39821, preemption_count=0, score=18107.200429, test/accuracy=0.441600, test/loss=2.621461, test/num_examples=10000, total_duration=19576.857032, train/accuracy=0.617344, train/loss=1.715525, validation/accuracy=0.555620, validation/loss=2.007649, validation/num_examples=50000
I0203 20:52:04.837807 139910212208384 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.4990593194961548, loss=3.4236695766448975
I0203 20:52:50.546769 139910203815680 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.4752612113952637, loss=3.621370792388916
I0203 20:53:36.837761 139910212208384 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.457172155380249, loss=3.469479560852051
I0203 20:54:23.298743 139910203815680 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.3779159784317017, loss=3.443162202835083
I0203 20:55:09.651552 139910212208384 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.4096673727035522, loss=3.604015827178955
I0203 20:55:55.745309 139910203815680 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.5702767372131348, loss=3.3755016326904297
I0203 20:56:42.129911 139910212208384 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.3360391855239868, loss=3.9318315982818604
I0203 20:57:28.299535 139910203815680 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.9838946461677551, loss=5.535660743713379
I0203 20:58:14.545409 139910212208384 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.5535391569137573, loss=3.480140209197998
I0203 20:58:33.582109 140107197974336 spec.py:321] Evaluating on the training split.
I0203 20:58:44.144635 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 20:59:08.295373 140107197974336 spec.py:349] Evaluating on the test split.
I0203 20:59:09.895618 140107197974336 submission_runner.py:408] Time since start: 20033.48s, 	Step: 40743, 	{'train/accuracy': 0.5999609231948853, 'train/loss': 1.8218204975128174, 'validation/accuracy': 0.5581799745559692, 'validation/loss': 1.9893065690994263, 'validation/num_examples': 50000, 'test/accuracy': 0.445000022649765, 'test/loss': 2.613339900970459, 'test/num_examples': 10000, 'score': 18527.431859254837, 'total_duration': 20033.484882116318, 'accumulated_submission_time': 18527.431859254837, 'accumulated_eval_time': 1500.6704943180084, 'accumulated_logging_time': 3.0773818492889404}
I0203 20:59:09.926948 139910203815680 logging_writer.py:48] [40743] accumulated_eval_time=1500.670494, accumulated_logging_time=3.077382, accumulated_submission_time=18527.431859, global_step=40743, preemption_count=0, score=18527.431859, test/accuracy=0.445000, test/loss=2.613340, test/num_examples=10000, total_duration=20033.484882, train/accuracy=0.599961, train/loss=1.821820, validation/accuracy=0.558180, validation/loss=1.989307, validation/num_examples=50000
I0203 20:59:32.843334 139910212208384 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.125760793685913, loss=5.323977470397949
I0203 21:00:16.896995 139910203815680 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.269681692123413, loss=4.03842306137085
I0203 21:01:03.075773 139910212208384 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.284251093864441, loss=4.422150611877441
I0203 21:01:49.333735 139910203815680 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1422940492630005, loss=5.343772888183594
I0203 21:02:35.308600 139910212208384 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.4294801950454712, loss=3.443784713745117
I0203 21:03:21.512940 139910203815680 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.4296820163726807, loss=3.4557719230651855
I0203 21:04:07.558518 139910212208384 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.1734929084777832, loss=5.555881023406982
I0203 21:04:53.804709 139910203815680 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.30271577835083, loss=4.150754451751709
I0203 21:05:39.968751 139910212208384 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.6415635347366333, loss=3.478536605834961
I0203 21:06:10.222929 140107197974336 spec.py:321] Evaluating on the training split.
I0203 21:06:20.814935 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 21:06:44.047667 140107197974336 spec.py:349] Evaluating on the test split.
I0203 21:06:45.647342 140107197974336 submission_runner.py:408] Time since start: 20489.24s, 	Step: 41667, 	{'train/accuracy': 0.5997461080551147, 'train/loss': 1.8272331953048706, 'validation/accuracy': 0.5560599565505981, 'validation/loss': 2.0224764347076416, 'validation/num_examples': 50000, 'test/accuracy': 0.4398000240325928, 'test/loss': 2.6552605628967285, 'test/num_examples': 10000, 'score': 18947.666902065277, 'total_duration': 20489.236602544785, 'accumulated_submission_time': 18947.666902065277, 'accumulated_eval_time': 1536.0949032306671, 'accumulated_logging_time': 3.118054151535034}
I0203 21:06:45.669514 139910203815680 logging_writer.py:48] [41667] accumulated_eval_time=1536.094903, accumulated_logging_time=3.118054, accumulated_submission_time=18947.666902, global_step=41667, preemption_count=0, score=18947.666902, test/accuracy=0.439800, test/loss=2.655261, test/num_examples=10000, total_duration=20489.236603, train/accuracy=0.599746, train/loss=1.827233, validation/accuracy=0.556060, validation/loss=2.022476, validation/num_examples=50000
I0203 21:06:59.039877 139910212208384 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.0863890647888184, loss=4.431377410888672
I0203 21:07:41.594723 139910203815680 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.913262128829956, loss=3.487046003341675
I0203 21:08:27.702507 139910212208384 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.725817322731018, loss=3.6542375087738037
I0203 21:09:14.022509 139910203815680 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.4724482297897339, loss=3.423689126968384
I0203 21:10:00.002129 139910212208384 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.4465280771255493, loss=4.194356918334961
I0203 21:10:45.902127 139910203815680 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.592348337173462, loss=3.5103514194488525
I0203 21:11:32.093890 139910212208384 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1484932899475098, loss=4.681760311126709
I0203 21:12:18.224609 139910203815680 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.158592700958252, loss=5.3076629638671875
I0203 21:13:04.544261 139910212208384 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.434482216835022, loss=4.779258728027344
I0203 21:13:45.887215 140107197974336 spec.py:321] Evaluating on the training split.
I0203 21:13:56.677853 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 21:14:21.348090 140107197974336 spec.py:349] Evaluating on the test split.
I0203 21:14:22.962381 140107197974336 submission_runner.py:408] Time since start: 20946.55s, 	Step: 42591, 	{'train/accuracy': 0.6067578196525574, 'train/loss': 1.7815989255905151, 'validation/accuracy': 0.5556600093841553, 'validation/loss': 2.0265753269195557, 'validation/num_examples': 50000, 'test/accuracy': 0.4319000244140625, 'test/loss': 2.671928882598877, 'test/num_examples': 10000, 'score': 19367.82270050049, 'total_duration': 20946.551624774933, 'accumulated_submission_time': 19367.82270050049, 'accumulated_eval_time': 1573.170075416565, 'accumulated_logging_time': 3.149902820587158}
I0203 21:14:22.988776 139910203815680 logging_writer.py:48] [42591] accumulated_eval_time=1573.170075, accumulated_logging_time=3.149903, accumulated_submission_time=19367.822701, global_step=42591, preemption_count=0, score=19367.822701, test/accuracy=0.431900, test/loss=2.671929, test/num_examples=10000, total_duration=20946.551625, train/accuracy=0.606758, train/loss=1.781599, validation/accuracy=0.555660, validation/loss=2.026575, validation/num_examples=50000
I0203 21:14:26.921428 139910212208384 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.3421242237091064, loss=3.479252815246582
I0203 21:15:08.029646 139910203815680 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.42146635055542, loss=3.747584819793701
I0203 21:15:53.781745 139910212208384 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.3839964866638184, loss=3.392195224761963
I0203 21:16:40.125691 139910203815680 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.1638017892837524, loss=5.372479438781738
I0203 21:17:26.562639 139910212208384 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.3271160125732422, loss=5.558589935302734
I0203 21:18:12.501037 139910203815680 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.3831019401550293, loss=4.0560126304626465
I0203 21:18:58.782196 139910212208384 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.4696965217590332, loss=4.0767741203308105
I0203 21:19:44.807302 139910203815680 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.3762168884277344, loss=4.939570903778076
I0203 21:20:30.670732 139910212208384 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.3714826107025146, loss=3.4077670574188232
I0203 21:21:16.886514 139910203815680 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.7108007669448853, loss=3.308365821838379
I0203 21:21:23.159178 140107197974336 spec.py:321] Evaluating on the training split.
I0203 21:21:34.023036 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 21:21:57.640088 140107197974336 spec.py:349] Evaluating on the test split.
I0203 21:21:59.241273 140107197974336 submission_runner.py:408] Time since start: 21402.83s, 	Step: 43515, 	{'train/accuracy': 0.6055273413658142, 'train/loss': 1.7664735317230225, 'validation/accuracy': 0.5644599795341492, 'validation/loss': 1.9504902362823486, 'validation/num_examples': 50000, 'test/accuracy': 0.45210000872612, 'test/loss': 2.5576162338256836, 'test/num_examples': 10000, 'score': 19787.93092918396, 'total_duration': 21402.830537080765, 'accumulated_submission_time': 19787.93092918396, 'accumulated_eval_time': 1609.2521879673004, 'accumulated_logging_time': 3.1867854595184326}
I0203 21:21:59.262234 139910212208384 logging_writer.py:48] [43515] accumulated_eval_time=1609.252188, accumulated_logging_time=3.186785, accumulated_submission_time=19787.930929, global_step=43515, preemption_count=0, score=19787.930929, test/accuracy=0.452100, test/loss=2.557616, test/num_examples=10000, total_duration=21402.830537, train/accuracy=0.605527, train/loss=1.766474, validation/accuracy=0.564460, validation/loss=1.950490, validation/num_examples=50000
I0203 21:22:33.407093 139910203815680 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.4323927164077759, loss=3.3313937187194824
I0203 21:23:19.512938 139910212208384 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.6076933145523071, loss=3.412367582321167
I0203 21:24:05.903716 139910203815680 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.3794093132019043, loss=3.3416545391082764
I0203 21:24:52.385041 139910212208384 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.0707762241363525, loss=5.40375280380249
I0203 21:25:38.426198 139910203815680 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.4398348331451416, loss=3.3974151611328125
I0203 21:26:24.654912 139910212208384 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.423539638519287, loss=3.6988179683685303
I0203 21:27:10.975741 139910203815680 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.3981664180755615, loss=3.3636350631713867
I0203 21:27:56.852962 139910212208384 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.5296785831451416, loss=3.517035484313965
I0203 21:28:43.556249 139910203815680 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.4135915040969849, loss=3.447031259536743
I0203 21:28:59.311992 140107197974336 spec.py:321] Evaluating on the training split.
I0203 21:29:09.978999 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 21:29:33.114255 140107197974336 spec.py:349] Evaluating on the test split.
I0203 21:29:34.715263 140107197974336 submission_runner.py:408] Time since start: 21858.30s, 	Step: 44436, 	{'train/accuracy': 0.6005859375, 'train/loss': 1.8105329275131226, 'validation/accuracy': 0.5595600008964539, 'validation/loss': 1.997233271598816, 'validation/num_examples': 50000, 'test/accuracy': 0.4448000192642212, 'test/loss': 2.628516912460327, 'test/num_examples': 10000, 'score': 20207.914827108383, 'total_duration': 21858.304526090622, 'accumulated_submission_time': 20207.914827108383, 'accumulated_eval_time': 1644.6554489135742, 'accumulated_logging_time': 3.2182297706604004}
I0203 21:29:34.736521 139910212208384 logging_writer.py:48] [44436] accumulated_eval_time=1644.655449, accumulated_logging_time=3.218230, accumulated_submission_time=20207.914827, global_step=44436, preemption_count=0, score=20207.914827, test/accuracy=0.444800, test/loss=2.628517, test/num_examples=10000, total_duration=21858.304526, train/accuracy=0.600586, train/loss=1.810533, validation/accuracy=0.559560, validation/loss=1.997233, validation/num_examples=50000
I0203 21:30:00.286470 139910203815680 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.3971619606018066, loss=4.92728853225708
I0203 21:30:45.185402 139910212208384 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.2638590335845947, loss=4.63029146194458
I0203 21:31:31.560175 139910203815680 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.4979766607284546, loss=3.4411239624023438
I0203 21:32:18.000353 139910212208384 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.4237664937973022, loss=3.5880861282348633
I0203 21:33:04.121495 139910203815680 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.5266664028167725, loss=3.4234471321105957
I0203 21:33:50.299589 139910212208384 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.6804178953170776, loss=3.5295937061309814
I0203 21:34:37.176610 139910203815680 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.099255919456482, loss=4.83254337310791
I0203 21:35:23.541374 139910212208384 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.5489312410354614, loss=3.4421374797821045
I0203 21:36:10.179986 139910203815680 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.5284737348556519, loss=3.3868446350097656
I0203 21:36:34.793611 140107197974336 spec.py:321] Evaluating on the training split.
I0203 21:36:45.634596 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 21:37:10.025688 140107197974336 spec.py:349] Evaluating on the test split.
I0203 21:37:11.634397 140107197974336 submission_runner.py:408] Time since start: 22315.22s, 	Step: 45355, 	{'train/accuracy': 0.6087499856948853, 'train/loss': 1.792636513710022, 'validation/accuracy': 0.5613799691200256, 'validation/loss': 2.0202178955078125, 'validation/num_examples': 50000, 'test/accuracy': 0.44360002875328064, 'test/loss': 2.6396775245666504, 'test/num_examples': 10000, 'score': 20627.911128520966, 'total_duration': 22315.223653554916, 'accumulated_submission_time': 20627.911128520966, 'accumulated_eval_time': 1681.4962322711945, 'accumulated_logging_time': 3.2489402294158936}
I0203 21:37:11.657091 139910212208384 logging_writer.py:48] [45355] accumulated_eval_time=1681.496232, accumulated_logging_time=3.248940, accumulated_submission_time=20627.911129, global_step=45355, preemption_count=0, score=20627.911129, test/accuracy=0.443600, test/loss=2.639678, test/num_examples=10000, total_duration=22315.223654, train/accuracy=0.608750, train/loss=1.792637, validation/accuracy=0.561380, validation/loss=2.020218, validation/num_examples=50000
I0203 21:37:29.734428 139910203815680 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.3217664957046509, loss=4.168898105621338
I0203 21:38:13.456420 139910212208384 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.1615017652511597, loss=4.402327537536621
I0203 21:38:59.916556 139910203815680 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.4985346794128418, loss=3.263559341430664
I0203 21:39:46.565215 139910212208384 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.4633634090423584, loss=3.481166362762451
I0203 21:40:32.888027 139910203815680 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.5245286226272583, loss=3.4126996994018555
I0203 21:41:19.162093 139910212208384 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.3134511709213257, loss=5.368496417999268
I0203 21:42:05.611088 139910203815680 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.3844496011734009, loss=3.618170738220215
I0203 21:42:51.615333 139910212208384 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.4401743412017822, loss=3.45772385597229
I0203 21:43:37.678186 139910203815680 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.2994484901428223, loss=4.0816192626953125
I0203 21:44:11.650113 140107197974336 spec.py:321] Evaluating on the training split.
I0203 21:44:22.490724 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 21:44:45.927871 140107197974336 spec.py:349] Evaluating on the test split.
I0203 21:44:47.529822 140107197974336 submission_runner.py:408] Time since start: 22771.12s, 	Step: 46275, 	{'train/accuracy': 0.6075195074081421, 'train/loss': 1.7662358283996582, 'validation/accuracy': 0.5708999633789062, 'validation/loss': 1.937433123588562, 'validation/num_examples': 50000, 'test/accuracy': 0.4536000192165375, 'test/loss': 2.589318037033081, 'test/num_examples': 10000, 'score': 21047.840060949326, 'total_duration': 22771.119074821472, 'accumulated_submission_time': 21047.840060949326, 'accumulated_eval_time': 1717.3759191036224, 'accumulated_logging_time': 3.283371686935425}
I0203 21:44:47.551917 139910212208384 logging_writer.py:48] [46275] accumulated_eval_time=1717.375919, accumulated_logging_time=3.283372, accumulated_submission_time=21047.840061, global_step=46275, preemption_count=0, score=21047.840061, test/accuracy=0.453600, test/loss=2.589318, test/num_examples=10000, total_duration=22771.119075, train/accuracy=0.607520, train/loss=1.766236, validation/accuracy=0.570900, validation/loss=1.937433, validation/num_examples=50000
I0203 21:44:57.786614 139910203815680 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.3798044919967651, loss=3.585521697998047
I0203 21:45:40.372616 139910212208384 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.620306372642517, loss=3.6066277027130127
I0203 21:46:26.549260 139910203815680 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.4515409469604492, loss=3.3002333641052246
I0203 21:47:13.094645 139910212208384 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.7336163520812988, loss=3.4215247631073
I0203 21:47:59.413372 139910203815680 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.3704626560211182, loss=4.088947772979736
I0203 21:48:45.802550 139910212208384 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.3706780672073364, loss=3.4263217449188232
I0203 21:49:32.038326 139910203815680 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.2802655696868896, loss=4.010286808013916
I0203 21:50:18.372547 139910212208384 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.1374050378799438, loss=4.839507102966309
I0203 21:51:04.737610 139910203815680 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.3775101900100708, loss=4.269268035888672
I0203 21:51:47.851726 140107197974336 spec.py:321] Evaluating on the training split.
I0203 21:51:58.624838 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 21:52:22.600072 140107197974336 spec.py:349] Evaluating on the test split.
I0203 21:52:24.208039 140107197974336 submission_runner.py:408] Time since start: 23227.80s, 	Step: 47195, 	{'train/accuracy': 0.6070312261581421, 'train/loss': 1.791407585144043, 'validation/accuracy': 0.5654999613761902, 'validation/loss': 1.9881013631820679, 'validation/num_examples': 50000, 'test/accuracy': 0.4508000314235687, 'test/loss': 2.6037206649780273, 'test/num_examples': 10000, 'score': 21468.07667350769, 'total_duration': 23227.797281980515, 'accumulated_submission_time': 21468.07667350769, 'accumulated_eval_time': 1753.7322096824646, 'accumulated_logging_time': 3.31660795211792}
I0203 21:52:24.236592 139910212208384 logging_writer.py:48] [47195] accumulated_eval_time=1753.732210, accumulated_logging_time=3.316608, accumulated_submission_time=21468.076674, global_step=47195, preemption_count=0, score=21468.076674, test/accuracy=0.450800, test/loss=2.603721, test/num_examples=10000, total_duration=23227.797282, train/accuracy=0.607031, train/loss=1.791408, validation/accuracy=0.565500, validation/loss=1.988101, validation/num_examples=50000
I0203 21:52:26.608714 139910203815680 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.519048810005188, loss=3.345690965652466
I0203 21:53:07.581264 139910212208384 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.464967966079712, loss=3.430380344390869
I0203 21:53:53.577234 139910203815680 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.3744542598724365, loss=3.3253111839294434
I0203 21:54:40.097714 139910212208384 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.5693137645721436, loss=3.391918182373047
I0203 21:55:26.824058 139910203815680 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.4422330856323242, loss=3.9078128337860107
I0203 21:56:12.952736 139910212208384 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2906839847564697, loss=4.219933986663818
I0203 21:56:59.002633 139910203815680 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.484442114830017, loss=3.470271110534668
I0203 21:57:45.133558 139910212208384 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.5625672340393066, loss=3.4222676753997803
I0203 21:58:31.207983 139910203815680 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.4071910381317139, loss=3.675173282623291
I0203 21:59:17.330782 139910212208384 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.4987297058105469, loss=3.3751463890075684
I0203 21:59:24.427242 140107197974336 spec.py:321] Evaluating on the training split.
I0203 21:59:34.988877 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 21:59:58.909726 140107197974336 spec.py:349] Evaluating on the test split.
I0203 22:00:00.509027 140107197974336 submission_runner.py:408] Time since start: 23684.10s, 	Step: 48117, 	{'train/accuracy': 0.6176171898841858, 'train/loss': 1.6974457502365112, 'validation/accuracy': 0.572219967842102, 'validation/loss': 1.9044924974441528, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.536510705947876, 'test/num_examples': 10000, 'score': 21888.20467019081, 'total_duration': 23684.09828400612, 'accumulated_submission_time': 21888.20467019081, 'accumulated_eval_time': 1789.8139972686768, 'accumulated_logging_time': 3.355715274810791}
I0203 22:00:00.534832 139910203815680 logging_writer.py:48] [48117] accumulated_eval_time=1789.813997, accumulated_logging_time=3.355715, accumulated_submission_time=21888.204670, global_step=48117, preemption_count=0, score=21888.204670, test/accuracy=0.460100, test/loss=2.536511, test/num_examples=10000, total_duration=23684.098284, train/accuracy=0.617617, train/loss=1.697446, validation/accuracy=0.572220, validation/loss=1.904492, validation/num_examples=50000
I0203 22:00:33.805612 139910212208384 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.5656911134719849, loss=3.479160785675049
I0203 22:01:19.971815 139910203815680 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.5155539512634277, loss=3.3740389347076416
I0203 22:02:06.117300 139910212208384 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.4192769527435303, loss=3.402165412902832
I0203 22:02:52.362502 139910203815680 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.4002689123153687, loss=5.216675281524658
I0203 22:03:38.743741 139910212208384 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.272567629814148, loss=4.011325836181641
I0203 22:04:24.799120 139910203815680 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.2667880058288574, loss=4.959113121032715
I0203 22:05:11.437796 139910212208384 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.6064039468765259, loss=3.3966565132141113
I0203 22:05:57.247401 139910203815680 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.3832427263259888, loss=3.38759446144104
I0203 22:06:43.423276 139910212208384 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.0683610439300537, loss=5.550053119659424
I0203 22:07:00.569518 140107197974336 spec.py:321] Evaluating on the training split.
I0203 22:07:11.580985 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 22:07:34.789082 140107197974336 spec.py:349] Evaluating on the test split.
I0203 22:07:36.388117 140107197974336 submission_runner.py:408] Time since start: 24139.98s, 	Step: 49039, 	{'train/accuracy': 0.6376562118530273, 'train/loss': 1.6303468942642212, 'validation/accuracy': 0.5713199973106384, 'validation/loss': 1.924487590789795, 'validation/num_examples': 50000, 'test/accuracy': 0.45650002360343933, 'test/loss': 2.5707848072052, 'test/num_examples': 10000, 'score': 22308.17884039879, 'total_duration': 24139.977380990982, 'accumulated_submission_time': 22308.17884039879, 'accumulated_eval_time': 1825.6325912475586, 'accumulated_logging_time': 3.390817880630493}
I0203 22:07:36.412371 139910203815680 logging_writer.py:48] [49039] accumulated_eval_time=1825.632591, accumulated_logging_time=3.390818, accumulated_submission_time=22308.178840, global_step=49039, preemption_count=0, score=22308.178840, test/accuracy=0.456500, test/loss=2.570785, test/num_examples=10000, total_duration=24139.977381, train/accuracy=0.637656, train/loss=1.630347, validation/accuracy=0.571320, validation/loss=1.924488, validation/num_examples=50000
I0203 22:08:00.805493 139910212208384 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.4861564636230469, loss=3.3785605430603027
I0203 22:08:45.102373 139910203815680 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.3923732042312622, loss=3.3943748474121094
I0203 22:09:31.505119 139910212208384 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.2509419918060303, loss=4.385342597961426
I0203 22:10:18.086165 139910203815680 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.436689853668213, loss=3.449018955230713
I0203 22:11:04.224287 139910212208384 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7158534526824951, loss=3.444697380065918
I0203 22:11:50.605608 139910203815680 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.524897575378418, loss=3.5026893615722656
I0203 22:12:36.802791 139910212208384 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.4159440994262695, loss=3.3118152618408203
I0203 22:13:23.096513 139910203815680 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.600450038909912, loss=3.5073299407958984
I0203 22:14:09.339428 139910212208384 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.4248242378234863, loss=5.494286060333252
I0203 22:14:36.526075 140107197974336 spec.py:321] Evaluating on the training split.
I0203 22:14:47.331089 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 22:15:11.639626 140107197974336 spec.py:349] Evaluating on the test split.
I0203 22:15:13.231647 140107197974336 submission_runner.py:408] Time since start: 24596.82s, 	Step: 49960, 	{'train/accuracy': 0.6035937070846558, 'train/loss': 1.750400185585022, 'validation/accuracy': 0.5683599710464478, 'validation/loss': 1.9232509136199951, 'validation/num_examples': 50000, 'test/accuracy': 0.4570000171661377, 'test/loss': 2.5590219497680664, 'test/num_examples': 10000, 'score': 22728.231050014496, 'total_duration': 24596.820907592773, 'accumulated_submission_time': 22728.231050014496, 'accumulated_eval_time': 1862.3381762504578, 'accumulated_logging_time': 3.424906015396118}
I0203 22:15:13.254026 139910203815680 logging_writer.py:48] [49960] accumulated_eval_time=1862.338176, accumulated_logging_time=3.424906, accumulated_submission_time=22728.231050, global_step=49960, preemption_count=0, score=22728.231050, test/accuracy=0.457000, test/loss=2.559022, test/num_examples=10000, total_duration=24596.820908, train/accuracy=0.603594, train/loss=1.750400, validation/accuracy=0.568360, validation/loss=1.923251, validation/num_examples=50000
I0203 22:15:29.374743 139910212208384 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1655423641204834, loss=5.43113899230957
I0203 22:16:12.811427 139910203815680 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.4562838077545166, loss=3.384551525115967
I0203 22:16:58.801192 139910212208384 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.5363825559616089, loss=3.4090211391448975
I0203 22:17:45.079010 139910203815680 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.4909336566925049, loss=3.3522300720214844
I0203 22:18:31.377429 139910212208384 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.178041934967041, loss=5.143324375152588
I0203 22:19:17.843089 139910203815680 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.5057317018508911, loss=3.3171768188476562
I0203 22:20:04.373067 139910212208384 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.5378122329711914, loss=3.354641914367676
I0203 22:20:50.327939 139910203815680 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.312893033027649, loss=4.004997730255127
I0203 22:21:36.500521 139910212208384 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.2160917520523071, loss=4.554238319396973
I0203 22:22:13.233191 140107197974336 spec.py:321] Evaluating on the training split.
I0203 22:22:24.096580 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 22:22:45.227587 140107197974336 spec.py:349] Evaluating on the test split.
I0203 22:22:46.815188 140107197974336 submission_runner.py:408] Time since start: 25050.40s, 	Step: 50881, 	{'train/accuracy': 0.6149609088897705, 'train/loss': 1.7614407539367676, 'validation/accuracy': 0.5702599883079529, 'validation/loss': 1.953475832939148, 'validation/num_examples': 50000, 'test/accuracy': 0.4577000141143799, 'test/loss': 2.5869696140289307, 'test/num_examples': 10000, 'score': 23148.14708518982, 'total_duration': 25050.40443754196, 'accumulated_submission_time': 23148.14708518982, 'accumulated_eval_time': 1895.92019033432, 'accumulated_logging_time': 3.456855535507202}
I0203 22:22:46.841917 139910203815680 logging_writer.py:48] [50881] accumulated_eval_time=1895.920190, accumulated_logging_time=3.456856, accumulated_submission_time=23148.147085, global_step=50881, preemption_count=0, score=23148.147085, test/accuracy=0.457700, test/loss=2.586970, test/num_examples=10000, total_duration=25050.404438, train/accuracy=0.614961, train/loss=1.761441, validation/accuracy=0.570260, validation/loss=1.953476, validation/num_examples=50000
I0203 22:22:54.721993 139910212208384 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.5308548212051392, loss=3.466132164001465
I0203 22:23:36.605625 139910203815680 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.6373341083526611, loss=3.3593697547912598
I0203 22:24:22.659121 139910212208384 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.2417120933532715, loss=5.60135555267334
I0203 22:25:09.310234 139910203815680 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.4471430778503418, loss=3.3670308589935303
I0203 22:25:55.819251 139910212208384 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.5334184169769287, loss=3.506978988647461
I0203 22:26:41.839445 139910203815680 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.6283389329910278, loss=3.3848583698272705
I0203 22:27:28.192866 139910212208384 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.3894816637039185, loss=3.471555709838867
I0203 22:28:14.418178 139910203815680 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.4771666526794434, loss=3.74359393119812
I0203 22:29:00.396878 139910212208384 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.595349907875061, loss=3.5124197006225586
I0203 22:29:46.763496 139910203815680 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.2290246486663818, loss=5.600327491760254
I0203 22:29:46.893334 140107197974336 spec.py:321] Evaluating on the training split.
I0203 22:29:57.522204 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 22:30:21.955691 140107197974336 spec.py:349] Evaluating on the test split.
I0203 22:30:23.566437 140107197974336 submission_runner.py:408] Time since start: 25507.16s, 	Step: 51802, 	{'train/accuracy': 0.6318554282188416, 'train/loss': 1.6667355298995972, 'validation/accuracy': 0.5773599743843079, 'validation/loss': 1.9151332378387451, 'validation/num_examples': 50000, 'test/accuracy': 0.45660001039505005, 'test/loss': 2.557199239730835, 'test/num_examples': 10000, 'score': 23568.13712787628, 'total_duration': 25507.155699014664, 'accumulated_submission_time': 23568.13712787628, 'accumulated_eval_time': 1932.593270778656, 'accumulated_logging_time': 3.4931063652038574}
I0203 22:30:23.589677 139910212208384 logging_writer.py:48] [51802] accumulated_eval_time=1932.593271, accumulated_logging_time=3.493106, accumulated_submission_time=23568.137128, global_step=51802, preemption_count=0, score=23568.137128, test/accuracy=0.456600, test/loss=2.557199, test/num_examples=10000, total_duration=25507.155699, train/accuracy=0.631855, train/loss=1.666736, validation/accuracy=0.577360, validation/loss=1.915133, validation/num_examples=50000
I0203 22:31:03.498249 139910203815680 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.4859869480133057, loss=3.6918158531188965
I0203 22:31:49.256884 139910212208384 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.3811347484588623, loss=3.937835931777954
I0203 22:32:35.636641 139910203815680 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.4972039461135864, loss=3.2799410820007324
I0203 22:33:21.871610 139910212208384 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.7476112842559814, loss=3.399646759033203
I0203 22:34:08.062313 139910203815680 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.4294133186340332, loss=4.27717924118042
I0203 22:34:54.563573 139910212208384 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.40817391872406, loss=3.3522706031799316
I0203 22:35:40.745778 139910203815680 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.4651228189468384, loss=3.231250047683716
I0203 22:36:26.939293 139910212208384 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.186015009880066, loss=5.403709411621094
I0203 22:37:13.299385 139910203815680 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.386386513710022, loss=5.4835357666015625
I0203 22:37:23.627938 140107197974336 spec.py:321] Evaluating on the training split.
I0203 22:37:34.245532 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 22:37:59.137283 140107197974336 spec.py:349] Evaluating on the test split.
I0203 22:38:00.730203 140107197974336 submission_runner.py:408] Time since start: 25964.32s, 	Step: 52724, 	{'train/accuracy': 0.6173242330551147, 'train/loss': 1.7071141004562378, 'validation/accuracy': 0.578220009803772, 'validation/loss': 1.8849382400512695, 'validation/num_examples': 50000, 'test/accuracy': 0.45740002393722534, 'test/loss': 2.535064697265625, 'test/num_examples': 10000, 'score': 23988.11474442482, 'total_duration': 25964.319465875626, 'accumulated_submission_time': 23988.11474442482, 'accumulated_eval_time': 1969.6955354213715, 'accumulated_logging_time': 3.5249931812286377}
I0203 22:38:00.755378 139910212208384 logging_writer.py:48] [52724] accumulated_eval_time=1969.695535, accumulated_logging_time=3.524993, accumulated_submission_time=23988.114744, global_step=52724, preemption_count=0, score=23988.114744, test/accuracy=0.457400, test/loss=2.535065, test/num_examples=10000, total_duration=25964.319466, train/accuracy=0.617324, train/loss=1.707114, validation/accuracy=0.578220, validation/loss=1.884938, validation/num_examples=50000
I0203 22:38:31.031978 139910203815680 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.4261775016784668, loss=3.4312686920166016
I0203 22:39:16.498915 139910212208384 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.3989031314849854, loss=3.8810272216796875
I0203 22:40:02.841822 139910203815680 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.5781121253967285, loss=3.3541457653045654
I0203 22:40:49.241028 139910212208384 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.5389554500579834, loss=3.444932699203491
I0203 22:41:35.477489 139910203815680 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.6573748588562012, loss=3.389725923538208
I0203 22:42:21.820816 139910212208384 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.5893598794937134, loss=3.3019258975982666
I0203 22:43:08.224404 139910203815680 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.5769270658493042, loss=3.336696147918701
I0203 22:43:54.448062 139910212208384 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.4939743280410767, loss=3.2405712604522705
I0203 22:44:41.173102 139910203815680 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.6152950525283813, loss=3.418222427368164
I0203 22:45:01.230459 140107197974336 spec.py:321] Evaluating on the training split.
I0203 22:45:11.995041 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 22:45:35.372936 140107197974336 spec.py:349] Evaluating on the test split.
I0203 22:45:36.968951 140107197974336 submission_runner.py:408] Time since start: 26420.56s, 	Step: 53645, 	{'train/accuracy': 0.6204687356948853, 'train/loss': 1.695976972579956, 'validation/accuracy': 0.5777400135993958, 'validation/loss': 1.8915741443634033, 'validation/num_examples': 50000, 'test/accuracy': 0.4635000228881836, 'test/loss': 2.5295896530151367, 'test/num_examples': 10000, 'score': 24408.527057886124, 'total_duration': 26420.55820798874, 'accumulated_submission_time': 24408.527057886124, 'accumulated_eval_time': 2005.4340209960938, 'accumulated_logging_time': 3.5617029666900635}
I0203 22:45:36.992093 139910212208384 logging_writer.py:48] [53645] accumulated_eval_time=2005.434021, accumulated_logging_time=3.561703, accumulated_submission_time=24408.527058, global_step=53645, preemption_count=0, score=24408.527058, test/accuracy=0.463500, test/loss=2.529590, test/num_examples=10000, total_duration=26420.558208, train/accuracy=0.620469, train/loss=1.695977, validation/accuracy=0.577740, validation/loss=1.891574, validation/num_examples=50000
I0203 22:45:59.033191 139910203815680 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.468961238861084, loss=3.375401496887207
I0203 22:46:43.421209 139910212208384 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.4488359689712524, loss=3.3799281120300293
I0203 22:47:29.557662 139910203815680 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.6800512075424194, loss=3.4260873794555664
I0203 22:48:15.978178 139910212208384 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.3986451625823975, loss=4.105771541595459
I0203 22:49:02.360868 139910203815680 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.3255795240402222, loss=4.195016384124756
I0203 22:49:48.373792 139910212208384 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.6741646528244019, loss=3.4149410724639893
I0203 22:50:34.548444 139910203815680 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.397854208946228, loss=3.666470766067505
I0203 22:51:20.751942 139910212208384 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.6604851484298706, loss=3.4877448081970215
I0203 22:52:06.798326 139910203815680 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.5444139242172241, loss=3.2403135299682617
I0203 22:52:37.274982 140107197974336 spec.py:321] Evaluating on the training split.
I0203 22:52:47.968441 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 22:53:13.258174 140107197974336 spec.py:349] Evaluating on the test split.
I0203 22:53:14.857767 140107197974336 submission_runner.py:408] Time since start: 26878.45s, 	Step: 54568, 	{'train/accuracy': 0.6335741877555847, 'train/loss': 1.6409857273101807, 'validation/accuracy': 0.5818600058555603, 'validation/loss': 1.8811854124069214, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.53002667427063, 'test/num_examples': 10000, 'score': 24828.74754881859, 'total_duration': 26878.447025060654, 'accumulated_submission_time': 24828.74754881859, 'accumulated_eval_time': 2043.0168118476868, 'accumulated_logging_time': 3.5952277183532715}
I0203 22:53:14.880992 139910212208384 logging_writer.py:48] [54568] accumulated_eval_time=2043.016812, accumulated_logging_time=3.595228, accumulated_submission_time=24828.747549, global_step=54568, preemption_count=0, score=24828.747549, test/accuracy=0.461100, test/loss=2.530027, test/num_examples=10000, total_duration=26878.447025, train/accuracy=0.633574, train/loss=1.640986, validation/accuracy=0.581860, validation/loss=1.881185, validation/num_examples=50000
I0203 22:53:27.848899 139910203815680 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.4251068830490112, loss=4.0355353355407715
I0203 22:54:10.365280 139910212208384 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.5852594375610352, loss=3.303588390350342
I0203 22:54:56.647979 139910203815680 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.5432440042495728, loss=3.418048620223999
I0203 22:55:43.264151 139910212208384 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.221421480178833, loss=4.299320220947266
I0203 22:56:29.419842 139910203815680 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.6385382413864136, loss=3.3526110649108887
I0203 22:57:15.702315 139910212208384 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.3060044050216675, loss=5.3332624435424805
I0203 22:58:01.772130 139910203815680 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.4961665868759155, loss=3.924619674682617
I0203 22:58:47.741381 139910212208384 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.1575946807861328, loss=5.362773895263672
I0203 22:59:33.657177 139910203815680 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.5155296325683594, loss=3.366424560546875
I0203 23:00:15.058342 140107197974336 spec.py:321] Evaluating on the training split.
I0203 23:00:25.736817 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 23:00:50.215147 140107197974336 spec.py:349] Evaluating on the test split.
I0203 23:00:51.822615 140107197974336 submission_runner.py:408] Time since start: 27335.41s, 	Step: 55491, 	{'train/accuracy': 0.6231836080551147, 'train/loss': 1.6743615865707397, 'validation/accuracy': 0.5826799869537354, 'validation/loss': 1.8473048210144043, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.4718806743621826, 'test/num_examples': 10000, 'score': 25248.86332678795, 'total_duration': 27335.411857128143, 'accumulated_submission_time': 25248.86332678795, 'accumulated_eval_time': 2079.7810554504395, 'accumulated_logging_time': 3.627671003341675}
I0203 23:00:51.850301 139910212208384 logging_writer.py:48] [55491] accumulated_eval_time=2079.781055, accumulated_logging_time=3.627671, accumulated_submission_time=25248.863327, global_step=55491, preemption_count=0, score=25248.863327, test/accuracy=0.464800, test/loss=2.471881, test/num_examples=10000, total_duration=27335.411857, train/accuracy=0.623184, train/loss=1.674362, validation/accuracy=0.582680, validation/loss=1.847305, validation/num_examples=50000
I0203 23:00:55.782793 139910203815680 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.4372631311416626, loss=4.232882976531982
I0203 23:01:36.675043 139910212208384 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.3616212606430054, loss=3.733384132385254
I0203 23:02:22.696660 139910203815680 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.8117409944534302, loss=3.4518508911132812
I0203 23:03:09.260009 139910212208384 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.6046290397644043, loss=3.270442485809326
I0203 23:03:55.561367 139910203815680 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.8117096424102783, loss=3.3498594760894775
I0203 23:04:41.895794 139910212208384 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.407164216041565, loss=3.9336721897125244
I0203 23:05:28.127990 139910203815680 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.4793071746826172, loss=3.5421791076660156
I0203 23:06:14.399693 139910212208384 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.38996160030365, loss=3.605395793914795
I0203 23:07:00.632609 139910203815680 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.5506184101104736, loss=3.453646183013916
I0203 23:07:46.865784 139910212208384 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.62357759475708, loss=3.2926506996154785
I0203 23:07:52.069367 140107197974336 spec.py:321] Evaluating on the training split.
I0203 23:08:02.972960 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 23:08:28.089746 140107197974336 spec.py:349] Evaluating on the test split.
I0203 23:08:29.685937 140107197974336 submission_runner.py:408] Time since start: 27793.28s, 	Step: 56413, 	{'train/accuracy': 0.6209765672683716, 'train/loss': 1.711039662361145, 'validation/accuracy': 0.5796599984169006, 'validation/loss': 1.9055297374725342, 'validation/num_examples': 50000, 'test/accuracy': 0.465800017118454, 'test/loss': 2.5380899906158447, 'test/num_examples': 10000, 'score': 25669.019901752472, 'total_duration': 27793.275195598602, 'accumulated_submission_time': 25669.019901752472, 'accumulated_eval_time': 2117.397604942322, 'accumulated_logging_time': 3.666386127471924}
I0203 23:08:29.712113 139910203815680 logging_writer.py:48] [56413] accumulated_eval_time=2117.397605, accumulated_logging_time=3.666386, accumulated_submission_time=25669.019902, global_step=56413, preemption_count=0, score=25669.019902, test/accuracy=0.465800, test/loss=2.538090, test/num_examples=10000, total_duration=27793.275196, train/accuracy=0.620977, train/loss=1.711040, validation/accuracy=0.579660, validation/loss=1.905530, validation/num_examples=50000
I0203 23:09:04.705543 139910212208384 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.4471814632415771, loss=3.278282642364502
I0203 23:09:50.602968 139910203815680 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.2596276998519897, loss=4.9427947998046875
I0203 23:10:36.903445 139910212208384 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.5084257125854492, loss=3.6936140060424805
I0203 23:11:23.528296 139910203815680 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.3892207145690918, loss=3.7626302242279053
I0203 23:12:09.638658 139910212208384 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.4003994464874268, loss=5.355010032653809
I0203 23:12:55.607428 139910203815680 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.5789138078689575, loss=3.313473701477051
I0203 23:13:41.648026 139910212208384 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.316025972366333, loss=4.476963043212891
I0203 23:14:27.831512 139910203815680 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.6348323822021484, loss=3.2706665992736816
I0203 23:15:14.086840 139910212208384 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.6623005867004395, loss=3.491428852081299
I0203 23:15:29.864309 140107197974336 spec.py:321] Evaluating on the training split.
I0203 23:15:40.835884 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 23:16:05.097280 140107197974336 spec.py:349] Evaluating on the test split.
I0203 23:16:06.700721 140107197974336 submission_runner.py:408] Time since start: 28250.29s, 	Step: 57336, 	{'train/accuracy': 0.6293359398841858, 'train/loss': 1.6536279916763306, 'validation/accuracy': 0.5833799839019775, 'validation/loss': 1.8656693696975708, 'validation/num_examples': 50000, 'test/accuracy': 0.46150001883506775, 'test/loss': 2.495638608932495, 'test/num_examples': 10000, 'score': 26089.111981153488, 'total_duration': 28250.289984464645, 'accumulated_submission_time': 26089.111981153488, 'accumulated_eval_time': 2154.234006166458, 'accumulated_logging_time': 3.701314687728882}
I0203 23:16:06.723771 139910203815680 logging_writer.py:48] [57336] accumulated_eval_time=2154.234006, accumulated_logging_time=3.701315, accumulated_submission_time=26089.111981, global_step=57336, preemption_count=0, score=26089.111981, test/accuracy=0.461500, test/loss=2.495639, test/num_examples=10000, total_duration=28250.289984, train/accuracy=0.629336, train/loss=1.653628, validation/accuracy=0.583380, validation/loss=1.865669, validation/num_examples=50000
I0203 23:16:32.282201 139910212208384 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.6021324396133423, loss=3.2666122913360596
I0203 23:17:17.042399 139910203815680 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.504830002784729, loss=3.4038143157958984
I0203 23:18:03.207406 139910212208384 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.3932133913040161, loss=5.239692211151123
I0203 23:18:49.587513 139910203815680 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.7543751001358032, loss=3.364474296569824
I0203 23:19:35.795442 139910212208384 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.6091880798339844, loss=3.3446288108825684
I0203 23:20:22.121545 139910203815680 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.2218244075775146, loss=4.725054740905762
I0203 23:21:08.462650 139910212208384 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.2981538772583008, loss=4.569894313812256
I0203 23:21:54.582964 139910203815680 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.697428584098816, loss=3.4165611267089844
I0203 23:22:40.674878 139910212208384 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.626983404159546, loss=3.3423831462860107
I0203 23:23:06.952835 140107197974336 spec.py:321] Evaluating on the training split.
I0203 23:23:17.890842 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 23:23:42.837820 140107197974336 spec.py:349] Evaluating on the test split.
I0203 23:23:44.433497 140107197974336 submission_runner.py:408] Time since start: 28708.02s, 	Step: 58258, 	{'train/accuracy': 0.6524804830551147, 'train/loss': 1.5615637302398682, 'validation/accuracy': 0.5855000019073486, 'validation/loss': 1.8619376420974731, 'validation/num_examples': 50000, 'test/accuracy': 0.4659000337123871, 'test/loss': 2.4890189170837402, 'test/num_examples': 10000, 'score': 26509.280297517776, 'total_duration': 28708.022760629654, 'accumulated_submission_time': 26509.280297517776, 'accumulated_eval_time': 2191.7146582603455, 'accumulated_logging_time': 3.7332868576049805}
I0203 23:23:44.458544 139910203815680 logging_writer.py:48] [58258] accumulated_eval_time=2191.714658, accumulated_logging_time=3.733287, accumulated_submission_time=26509.280298, global_step=58258, preemption_count=0, score=26509.280298, test/accuracy=0.465900, test/loss=2.489019, test/num_examples=10000, total_duration=28708.022761, train/accuracy=0.652480, train/loss=1.561564, validation/accuracy=0.585500, validation/loss=1.861938, validation/num_examples=50000
I0203 23:24:01.380537 139910212208384 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.6324834823608398, loss=3.2624690532684326
I0203 23:24:44.560505 139910203815680 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.307508111000061, loss=4.446303844451904
I0203 23:25:31.006309 139910212208384 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.4995841979980469, loss=3.302043914794922
I0203 23:26:17.773909 139910203815680 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.7786800861358643, loss=3.372744083404541
I0203 23:27:03.944212 139910212208384 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.3212782144546509, loss=4.429656982421875
I0203 23:27:50.379221 139910203815680 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.360588788986206, loss=3.6369268894195557
I0203 23:28:36.749610 139910212208384 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.7527563571929932, loss=3.2786340713500977
I0203 23:29:23.052877 139910203815680 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.3377528190612793, loss=4.430800437927246
I0203 23:30:09.408229 139910212208384 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.3988198041915894, loss=4.924517631530762
I0203 23:30:44.529601 140107197974336 spec.py:321] Evaluating on the training split.
I0203 23:30:55.208655 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 23:31:19.536426 140107197974336 spec.py:349] Evaluating on the test split.
I0203 23:31:21.125559 140107197974336 submission_runner.py:408] Time since start: 29164.71s, 	Step: 59178, 	{'train/accuracy': 0.6221289038658142, 'train/loss': 1.7175687551498413, 'validation/accuracy': 0.5812399983406067, 'validation/loss': 1.9062336683273315, 'validation/num_examples': 50000, 'test/accuracy': 0.4636000096797943, 'test/loss': 2.526654005050659, 'test/num_examples': 10000, 'score': 26929.290602445602, 'total_duration': 29164.714790582657, 'accumulated_submission_time': 26929.290602445602, 'accumulated_eval_time': 2228.310579776764, 'accumulated_logging_time': 3.7675628662109375}
I0203 23:31:21.152884 139910203815680 logging_writer.py:48] [59178] accumulated_eval_time=2228.310580, accumulated_logging_time=3.767563, accumulated_submission_time=26929.290602, global_step=59178, preemption_count=0, score=26929.290602, test/accuracy=0.463600, test/loss=2.526654, test/num_examples=10000, total_duration=29164.714791, train/accuracy=0.622129, train/loss=1.717569, validation/accuracy=0.581240, validation/loss=1.906234, validation/num_examples=50000
I0203 23:31:30.195385 139910212208384 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.7121660709381104, loss=3.2046046257019043
I0203 23:32:12.204387 139910203815680 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.4674265384674072, loss=5.565888404846191
I0203 23:32:58.354600 139910212208384 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.621709942817688, loss=3.1476244926452637
I0203 23:33:44.999506 139910203815680 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.5859111547470093, loss=3.3109476566314697
I0203 23:34:31.699297 139910212208384 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.2029541730880737, loss=5.362868309020996
I0203 23:35:17.872433 139910203815680 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.5539933443069458, loss=3.2407641410827637
I0203 23:36:04.346124 139910212208384 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.4218955039978027, loss=3.6968014240264893
I0203 23:36:50.732784 139910203815680 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.2764623165130615, loss=4.476865768432617
I0203 23:37:37.004015 139910212208384 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.631252408027649, loss=3.410097599029541
I0203 23:38:21.222335 140107197974336 spec.py:321] Evaluating on the training split.
I0203 23:38:31.916065 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 23:38:56.838021 140107197974336 spec.py:349] Evaluating on the test split.
I0203 23:38:58.430594 140107197974336 submission_runner.py:408] Time since start: 29622.02s, 	Step: 60097, 	{'train/accuracy': 0.6240624785423279, 'train/loss': 1.7262558937072754, 'validation/accuracy': 0.5816799998283386, 'validation/loss': 1.931992769241333, 'validation/num_examples': 50000, 'test/accuracy': 0.4661000072956085, 'test/loss': 2.5786662101745605, 'test/num_examples': 10000, 'score': 27349.29820728302, 'total_duration': 29622.019852399826, 'accumulated_submission_time': 27349.29820728302, 'accumulated_eval_time': 2265.5188434123993, 'accumulated_logging_time': 3.8040237426757812}
I0203 23:38:58.458081 139910203815680 logging_writer.py:48] [60097] accumulated_eval_time=2265.518843, accumulated_logging_time=3.804024, accumulated_submission_time=27349.298207, global_step=60097, preemption_count=0, score=27349.298207, test/accuracy=0.466100, test/loss=2.578666, test/num_examples=10000, total_duration=29622.019852, train/accuracy=0.624062, train/loss=1.726256, validation/accuracy=0.581680, validation/loss=1.931993, validation/num_examples=50000
I0203 23:39:00.033427 139910212208384 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.4978837966918945, loss=3.2586255073547363
I0203 23:39:40.559132 139910203815680 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.617641568183899, loss=3.3231585025787354
I0203 23:40:26.723882 139910212208384 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.5389238595962524, loss=3.1972391605377197
I0203 23:41:13.256496 139910203815680 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.2147912979125977, loss=4.717793941497803
I0203 23:41:59.482317 139910212208384 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.5669399499893188, loss=3.1436657905578613
I0203 23:42:45.772193 139910203815680 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.269014835357666, loss=5.442551612854004
I0203 23:43:32.098321 139910212208384 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.2250020503997803, loss=5.187043190002441
I0203 23:44:18.140682 139910203815680 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.359994649887085, loss=4.454540252685547
I0203 23:45:04.747127 139910212208384 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.5548036098480225, loss=3.3840854167938232
I0203 23:45:50.837846 139910203815680 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.5656849145889282, loss=3.2291715145111084
I0203 23:45:58.856057 140107197974336 spec.py:321] Evaluating on the training split.
I0203 23:46:09.835491 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 23:46:33.115349 140107197974336 spec.py:349] Evaluating on the test split.
I0203 23:46:34.721431 140107197974336 submission_runner.py:408] Time since start: 30078.31s, 	Step: 61019, 	{'train/accuracy': 0.6532421708106995, 'train/loss': 1.5591392517089844, 'validation/accuracy': 0.5931999683380127, 'validation/loss': 1.8360151052474976, 'validation/num_examples': 50000, 'test/accuracy': 0.4717000126838684, 'test/loss': 2.467651128768921, 'test/num_examples': 10000, 'score': 27769.632704496384, 'total_duration': 30078.310692310333, 'accumulated_submission_time': 27769.632704496384, 'accumulated_eval_time': 2301.384221792221, 'accumulated_logging_time': 3.8424084186553955}
I0203 23:46:34.749638 139910212208384 logging_writer.py:48] [61019] accumulated_eval_time=2301.384222, accumulated_logging_time=3.842408, accumulated_submission_time=27769.632704, global_step=61019, preemption_count=0, score=27769.632704, test/accuracy=0.471700, test/loss=2.467651, test/num_examples=10000, total_duration=30078.310692, train/accuracy=0.653242, train/loss=1.559139, validation/accuracy=0.593200, validation/loss=1.836015, validation/num_examples=50000
I0203 23:47:07.222610 139910203815680 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.5800845623016357, loss=3.2322335243225098
I0203 23:47:53.077439 139910212208384 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.5039130449295044, loss=3.2816338539123535
I0203 23:48:39.485710 139910203815680 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.5610147714614868, loss=3.2513983249664307
I0203 23:49:25.735906 139910212208384 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.4864966869354248, loss=5.410632133483887
I0203 23:50:12.106317 139910203815680 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.1429319381713867, loss=5.183804035186768
I0203 23:50:58.222043 139910212208384 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.6711068153381348, loss=3.288891315460205
I0203 23:51:44.457700 139910203815680 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.3929613828659058, loss=5.336630821228027
I0203 23:52:30.661301 139910212208384 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.407663106918335, loss=3.5832109451293945
I0203 23:53:17.036031 139910203815680 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.4994075298309326, loss=4.452352523803711
I0203 23:53:35.022856 140107197974336 spec.py:321] Evaluating on the training split.
I0203 23:53:45.597311 140107197974336 spec.py:333] Evaluating on the validation split.
I0203 23:54:11.518305 140107197974336 spec.py:349] Evaluating on the test split.
I0203 23:54:13.122422 140107197974336 submission_runner.py:408] Time since start: 30536.71s, 	Step: 61941, 	{'train/accuracy': 0.6307030916213989, 'train/loss': 1.6259307861328125, 'validation/accuracy': 0.5897600054740906, 'validation/loss': 1.8175615072250366, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.4553451538085938, 'test/num_examples': 10000, 'score': 28189.844376802444, 'total_duration': 30536.711680173874, 'accumulated_submission_time': 28189.844376802444, 'accumulated_eval_time': 2339.4837741851807, 'accumulated_logging_time': 3.8803889751434326}
I0203 23:54:13.150110 139910212208384 logging_writer.py:48] [61941] accumulated_eval_time=2339.483774, accumulated_logging_time=3.880389, accumulated_submission_time=28189.844377, global_step=61941, preemption_count=0, score=28189.844377, test/accuracy=0.472000, test/loss=2.455345, test/num_examples=10000, total_duration=30536.711680, train/accuracy=0.630703, train/loss=1.625931, validation/accuracy=0.589760, validation/loss=1.817562, validation/num_examples=50000
I0203 23:54:36.744400 139910203815680 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.5948399305343628, loss=3.2382044792175293
I0203 23:55:21.207281 139910212208384 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.6228539943695068, loss=3.242494583129883
I0203 23:56:07.519906 139910203815680 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.3358615636825562, loss=3.6631529331207275
I0203 23:56:54.085562 139910212208384 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.3159520626068115, loss=4.561370849609375
I0203 23:57:40.295355 139910203815680 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.3760969638824463, loss=3.8038246631622314
I0203 23:58:26.726837 139910212208384 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.5195332765579224, loss=4.163168907165527
I0203 23:59:13.255220 139910203815680 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.5256834030151367, loss=3.234301805496216
I0203 23:59:59.228362 139910212208384 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.25785231590271, loss=5.413675785064697
I0204 00:00:45.726373 139910203815680 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.4124305248260498, loss=3.7205281257629395
I0204 00:01:13.251969 140107197974336 spec.py:321] Evaluating on the training split.
I0204 00:01:24.140374 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 00:01:47.195710 140107197974336 spec.py:349] Evaluating on the test split.
I0204 00:01:48.790384 140107197974336 submission_runner.py:408] Time since start: 30992.38s, 	Step: 62861, 	{'train/accuracy': 0.6349999904632568, 'train/loss': 1.6504743099212646, 'validation/accuracy': 0.5909799933433533, 'validation/loss': 1.858465313911438, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.47857403755188, 'test/num_examples': 10000, 'score': 28609.88588285446, 'total_duration': 30992.379618406296, 'accumulated_submission_time': 28609.88588285446, 'accumulated_eval_time': 2375.022164583206, 'accumulated_logging_time': 3.916743040084839}
I0204 00:01:48.817685 139910212208384 logging_writer.py:48] [62861] accumulated_eval_time=2375.022165, accumulated_logging_time=3.916743, accumulated_submission_time=28609.885883, global_step=62861, preemption_count=0, score=28609.885883, test/accuracy=0.468700, test/loss=2.478574, test/num_examples=10000, total_duration=30992.379618, train/accuracy=0.635000, train/loss=1.650474, validation/accuracy=0.590980, validation/loss=1.858465, validation/num_examples=50000
I0204 00:02:04.542552 139910203815680 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.6510767936706543, loss=3.2137887477874756
I0204 00:02:47.306039 139910212208384 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.9804136753082275, loss=3.3771533966064453
I0204 00:03:33.495710 139910203815680 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.5563764572143555, loss=3.2819371223449707
I0204 00:04:19.691067 139910212208384 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.5690481662750244, loss=3.251041889190674
I0204 00:05:05.802065 139910203815680 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.5365808010101318, loss=3.914106845855713
I0204 00:05:52.053906 139910212208384 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.2799077033996582, loss=5.336432456970215
I0204 00:06:38.106180 139910203815680 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.2889282703399658, loss=4.492837905883789
I0204 00:07:24.415842 139910212208384 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.5505027770996094, loss=3.217888832092285
I0204 00:08:11.032730 139910203815680 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.3801828622817993, loss=4.181285858154297
I0204 00:08:48.885352 140107197974336 spec.py:321] Evaluating on the training split.
I0204 00:08:59.206775 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 00:09:24.621969 140107197974336 spec.py:349] Evaluating on the test split.
I0204 00:09:26.222569 140107197974336 submission_runner.py:408] Time since start: 31449.81s, 	Step: 63784, 	{'train/accuracy': 0.6424999833106995, 'train/loss': 1.608486294746399, 'validation/accuracy': 0.5922999978065491, 'validation/loss': 1.8405706882476807, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.484283208847046, 'test/num_examples': 10000, 'score': 29029.89266204834, 'total_duration': 31449.811811208725, 'accumulated_submission_time': 29029.89266204834, 'accumulated_eval_time': 2412.3593595027924, 'accumulated_logging_time': 3.953082323074341}
I0204 00:09:26.248204 139910212208384 logging_writer.py:48] [63784] accumulated_eval_time=2412.359360, accumulated_logging_time=3.953082, accumulated_submission_time=29029.892662, global_step=63784, preemption_count=0, score=29029.892662, test/accuracy=0.469500, test/loss=2.484283, test/num_examples=10000, total_duration=31449.811811, train/accuracy=0.642500, train/loss=1.608486, validation/accuracy=0.592300, validation/loss=1.840571, validation/num_examples=50000
I0204 00:09:32.939296 139910203815680 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.7297133207321167, loss=3.2111263275146484
I0204 00:10:14.525259 139910212208384 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.171913981437683, loss=4.75954532623291
I0204 00:11:00.694957 139910203815680 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.5805977582931519, loss=3.2025704383850098
I0204 00:11:47.385803 139910212208384 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.5995088815689087, loss=3.42537522315979
I0204 00:12:33.787673 139910203815680 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.6356102228164673, loss=3.191336154937744
I0204 00:13:20.173387 139910212208384 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.8369452953338623, loss=3.3171675205230713
I0204 00:14:06.608344 139910203815680 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.77396559715271, loss=3.197915554046631
I0204 00:14:52.760270 139910212208384 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.7061214447021484, loss=3.370201349258423
I0204 00:15:38.991809 139910203815680 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.6139042377471924, loss=3.253511428833008
I0204 00:16:25.224871 139910212208384 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.592230200767517, loss=3.263099193572998
I0204 00:16:26.331316 140107197974336 spec.py:321] Evaluating on the training split.
I0204 00:16:36.945265 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 00:17:01.687002 140107197974336 spec.py:349] Evaluating on the test split.
I0204 00:17:03.288302 140107197974336 submission_runner.py:408] Time since start: 31906.88s, 	Step: 64704, 	{'train/accuracy': 0.6398632526397705, 'train/loss': 1.6252983808517456, 'validation/accuracy': 0.5931400060653687, 'validation/loss': 1.8280532360076904, 'validation/num_examples': 50000, 'test/accuracy': 0.4734000265598297, 'test/loss': 2.459376335144043, 'test/num_examples': 10000, 'score': 29449.915155172348, 'total_duration': 31906.87753367424, 'accumulated_submission_time': 29449.915155172348, 'accumulated_eval_time': 2449.3163084983826, 'accumulated_logging_time': 3.9879281520843506}
I0204 00:17:03.321045 139910203815680 logging_writer.py:48] [64704] accumulated_eval_time=2449.316308, accumulated_logging_time=3.987928, accumulated_submission_time=29449.915155, global_step=64704, preemption_count=0, score=29449.915155, test/accuracy=0.473400, test/loss=2.459376, test/num_examples=10000, total_duration=31906.877534, train/accuracy=0.639863, train/loss=1.625298, validation/accuracy=0.593140, validation/loss=1.828053, validation/num_examples=50000
I0204 00:17:42.145555 139910212208384 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.5977146625518799, loss=3.1674880981445312
I0204 00:18:28.057373 139910203815680 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.314267635345459, loss=3.9814372062683105
I0204 00:19:14.513792 139910212208384 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.2185801267623901, loss=5.490313529968262
I0204 00:20:01.405260 139910203815680 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.1963902711868286, loss=4.6594133377075195
I0204 00:20:47.559102 139910212208384 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.3936716318130493, loss=4.8385539054870605
I0204 00:21:33.858641 139910203815680 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.4589762687683105, loss=3.592470169067383
I0204 00:22:20.042350 139910212208384 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.7031235694885254, loss=3.3163020610809326
I0204 00:23:06.178762 139910203815680 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.5061860084533691, loss=3.3207805156707764
I0204 00:23:52.541355 139910212208384 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.4650856256484985, loss=3.596280574798584
I0204 00:24:03.655805 140107197974336 spec.py:321] Evaluating on the training split.
I0204 00:24:14.774160 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 00:24:41.055611 140107197974336 spec.py:349] Evaluating on the test split.
I0204 00:24:42.653173 140107197974336 submission_runner.py:408] Time since start: 32366.24s, 	Step: 65625, 	{'train/accuracy': 0.6363476514816284, 'train/loss': 1.6080312728881836, 'validation/accuracy': 0.596019983291626, 'validation/loss': 1.798905611038208, 'validation/num_examples': 50000, 'test/accuracy': 0.47280001640319824, 'test/loss': 2.4222564697265625, 'test/num_examples': 10000, 'score': 29870.18850684166, 'total_duration': 32366.242438316345, 'accumulated_submission_time': 29870.18850684166, 'accumulated_eval_time': 2488.3136699199677, 'accumulated_logging_time': 4.030225992202759}
I0204 00:24:42.680850 139910203815680 logging_writer.py:48] [65625] accumulated_eval_time=2488.313670, accumulated_logging_time=4.030226, accumulated_submission_time=29870.188507, global_step=65625, preemption_count=0, score=29870.188507, test/accuracy=0.472800, test/loss=2.422256, test/num_examples=10000, total_duration=32366.242438, train/accuracy=0.636348, train/loss=1.608031, validation/accuracy=0.596020, validation/loss=1.798906, validation/num_examples=50000
I0204 00:25:12.534886 139910212208384 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.5155494213104248, loss=3.43996262550354
I0204 00:25:58.149539 139910203815680 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.5722267627716064, loss=3.423377513885498
I0204 00:26:44.171312 139910212208384 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.685066819190979, loss=3.233487367630005
I0204 00:27:30.721201 139910203815680 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.6155951023101807, loss=3.3336877822875977
I0204 00:28:16.805925 139910212208384 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.5785759687423706, loss=3.3401951789855957
I0204 00:29:02.946024 139910203815680 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.3171976804733276, loss=4.233202934265137
I0204 00:29:49.140727 139910212208384 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.59409499168396, loss=3.367208957672119
I0204 00:30:35.236635 139910203815680 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.42559814453125, loss=4.789700508117676
I0204 00:31:21.578942 139910212208384 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.2926247119903564, loss=5.364484786987305
I0204 00:31:42.892035 140107197974336 spec.py:321] Evaluating on the training split.
I0204 00:31:53.587871 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 00:32:19.070936 140107197974336 spec.py:349] Evaluating on the test split.
I0204 00:32:20.671831 140107197974336 submission_runner.py:408] Time since start: 32824.26s, 	Step: 66548, 	{'train/accuracy': 0.6463476419448853, 'train/loss': 1.5539718866348267, 'validation/accuracy': 0.593779981136322, 'validation/loss': 1.7889726161956787, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.425429582595825, 'test/num_examples': 10000, 'score': 30290.33807373047, 'total_duration': 32824.26109623909, 'accumulated_submission_time': 30290.33807373047, 'accumulated_eval_time': 2526.093469142914, 'accumulated_logging_time': 4.068289279937744}
I0204 00:32:20.696311 139910203815680 logging_writer.py:48] [66548] accumulated_eval_time=2526.093469, accumulated_logging_time=4.068289, accumulated_submission_time=30290.338074, global_step=66548, preemption_count=0, score=30290.338074, test/accuracy=0.475700, test/loss=2.425430, test/num_examples=10000, total_duration=32824.261096, train/accuracy=0.646348, train/loss=1.553972, validation/accuracy=0.593780, validation/loss=1.788973, validation/num_examples=50000
I0204 00:32:41.530195 139910212208384 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.9462143182754517, loss=3.290663957595825
I0204 00:33:25.288373 139910203815680 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.5336941480636597, loss=5.379254341125488
I0204 00:34:11.777245 139910212208384 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.592865228652954, loss=3.1918604373931885
I0204 00:34:58.121337 139910203815680 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.4819412231445312, loss=3.506986618041992
I0204 00:35:44.286899 139910212208384 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.4657782316207886, loss=3.5534701347351074
I0204 00:36:30.516737 139910203815680 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.4406712055206299, loss=4.646687030792236
I0204 00:37:16.694131 139910212208384 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.2394816875457764, loss=5.455593109130859
I0204 00:38:02.942372 139910203815680 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.8076074123382568, loss=3.1974990367889404
I0204 00:38:49.237159 139910212208384 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.747515082359314, loss=3.2134170532226562
I0204 00:39:21.062036 140107197974336 spec.py:321] Evaluating on the training split.
I0204 00:39:31.755724 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 00:39:56.658674 140107197974336 spec.py:349] Evaluating on the test split.
I0204 00:39:58.259915 140107197974336 submission_runner.py:408] Time since start: 33281.85s, 	Step: 67470, 	{'train/accuracy': 0.6481249928474426, 'train/loss': 1.55096435546875, 'validation/accuracy': 0.5956199765205383, 'validation/loss': 1.7877963781356812, 'validation/num_examples': 50000, 'test/accuracy': 0.47870001196861267, 'test/loss': 2.4377787113189697, 'test/num_examples': 10000, 'score': 30710.64385533333, 'total_duration': 33281.84917807579, 'accumulated_submission_time': 30710.64385533333, 'accumulated_eval_time': 2563.291362285614, 'accumulated_logging_time': 4.101492404937744}
I0204 00:39:58.287316 139910203815680 logging_writer.py:48] [67470] accumulated_eval_time=2563.291362, accumulated_logging_time=4.101492, accumulated_submission_time=30710.643855, global_step=67470, preemption_count=0, score=30710.643855, test/accuracy=0.478700, test/loss=2.437779, test/num_examples=10000, total_duration=33281.849178, train/accuracy=0.648125, train/loss=1.550964, validation/accuracy=0.595620, validation/loss=1.787796, validation/num_examples=50000
I0204 00:40:10.453551 139910212208384 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.756292700767517, loss=3.134119987487793
I0204 00:40:52.886849 139910203815680 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.3404431343078613, loss=4.31247615814209
I0204 00:41:38.937399 139910212208384 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.2520077228546143, loss=4.070229530334473
I0204 00:42:25.634109 139910203815680 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.2270267009735107, loss=4.776549339294434
I0204 00:43:12.079404 139910212208384 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.7168614864349365, loss=3.296933174133301
I0204 00:43:57.950513 139910203815680 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.5113167762756348, loss=3.8235671520233154
I0204 00:44:44.601264 139910212208384 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.7862117290496826, loss=3.167605400085449
I0204 00:45:30.764708 139910203815680 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.6838093996047974, loss=4.065616607666016
I0204 00:46:16.874471 139910212208384 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.5850862264633179, loss=3.2227158546447754
I0204 00:46:58.285079 140107197974336 spec.py:321] Evaluating on the training split.
I0204 00:47:09.097110 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 00:47:33.511199 140107197974336 spec.py:349] Evaluating on the test split.
I0204 00:47:35.110582 140107197974336 submission_runner.py:408] Time since start: 33738.70s, 	Step: 68391, 	{'train/accuracy': 0.6390038728713989, 'train/loss': 1.6182537078857422, 'validation/accuracy': 0.5971599817276001, 'validation/loss': 1.8191072940826416, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.4577841758728027, 'test/num_examples': 10000, 'score': 31130.579701900482, 'total_duration': 33738.699846982956, 'accumulated_submission_time': 31130.579701900482, 'accumulated_eval_time': 2600.11688375473, 'accumulated_logging_time': 4.13883113861084}
I0204 00:47:35.135552 139910203815680 logging_writer.py:48] [68391] accumulated_eval_time=2600.116884, accumulated_logging_time=4.138831, accumulated_submission_time=31130.579702, global_step=68391, preemption_count=0, score=31130.579702, test/accuracy=0.475400, test/loss=2.457784, test/num_examples=10000, total_duration=33738.699847, train/accuracy=0.639004, train/loss=1.618254, validation/accuracy=0.597160, validation/loss=1.819107, validation/num_examples=50000
I0204 00:47:39.064876 139910212208384 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.835878849029541, loss=3.309082508087158
I0204 00:48:20.319821 139910203815680 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.7613664865493774, loss=3.5221030712127686
I0204 00:49:06.975432 139910212208384 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.614366054534912, loss=3.7076034545898438
I0204 00:49:53.399054 139910203815680 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.2640111446380615, loss=4.869635581970215
I0204 00:50:40.138478 139910212208384 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.574469804763794, loss=3.474562883377075
I0204 00:51:26.115933 139910203815680 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.2933741807937622, loss=5.346308708190918
I0204 00:52:12.521630 139910212208384 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.6852929592132568, loss=3.3095571994781494
I0204 00:52:58.673479 139910203815680 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.4763239622116089, loss=4.109457015991211
I0204 00:53:44.888436 139910212208384 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.7972478866577148, loss=3.2035105228424072
I0204 00:54:31.242154 139910203815680 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.727642297744751, loss=3.24485445022583
I0204 00:54:35.159207 140107197974336 spec.py:321] Evaluating on the training split.
I0204 00:54:46.225953 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 00:55:10.946976 140107197974336 spec.py:349] Evaluating on the test split.
I0204 00:55:12.550166 140107197974336 submission_runner.py:408] Time since start: 34196.14s, 	Step: 69310, 	{'train/accuracy': 0.6440038681030273, 'train/loss': 1.572296142578125, 'validation/accuracy': 0.5978800058364868, 'validation/loss': 1.7804210186004639, 'validation/num_examples': 50000, 'test/accuracy': 0.4824000298976898, 'test/loss': 2.4211783409118652, 'test/num_examples': 10000, 'score': 31550.541101932526, 'total_duration': 34196.13939833641, 'accumulated_submission_time': 31550.541101932526, 'accumulated_eval_time': 2637.507829427719, 'accumulated_logging_time': 4.173320293426514}
I0204 00:55:12.581680 139910212208384 logging_writer.py:48] [69310] accumulated_eval_time=2637.507829, accumulated_logging_time=4.173320, accumulated_submission_time=31550.541102, global_step=69310, preemption_count=0, score=31550.541102, test/accuracy=0.482400, test/loss=2.421178, test/num_examples=10000, total_duration=34196.139398, train/accuracy=0.644004, train/loss=1.572296, validation/accuracy=0.597880, validation/loss=1.780421, validation/num_examples=50000
I0204 00:55:48.738399 139910203815680 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.6606684923171997, loss=3.3414249420166016
I0204 00:56:34.558408 139910212208384 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.8084843158721924, loss=3.204326629638672
I0204 00:57:20.858036 139910203815680 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.3040214776992798, loss=5.376007080078125
I0204 00:58:07.295644 139910212208384 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.5838172435760498, loss=3.3063454627990723
I0204 00:58:53.354486 139910203815680 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.499870777130127, loss=3.0509705543518066
I0204 00:59:39.713624 139910212208384 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.4692096710205078, loss=3.3613035678863525
I0204 01:00:25.858819 139910203815680 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.5214500427246094, loss=3.150808811187744
I0204 01:01:11.879338 139910212208384 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.4512255191802979, loss=3.436522960662842
I0204 01:01:57.748343 139910203815680 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.480393409729004, loss=3.7335808277130127
I0204 01:02:12.765967 140107197974336 spec.py:321] Evaluating on the training split.
I0204 01:02:23.503314 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 01:02:46.374640 140107197974336 spec.py:349] Evaluating on the test split.
I0204 01:02:47.976864 140107197974336 submission_runner.py:408] Time since start: 34651.57s, 	Step: 70234, 	{'train/accuracy': 0.6702734231948853, 'train/loss': 1.4641485214233398, 'validation/accuracy': 0.6001200079917908, 'validation/loss': 1.7625842094421387, 'validation/num_examples': 50000, 'test/accuracy': 0.48190003633499146, 'test/loss': 2.420220375061035, 'test/num_examples': 10000, 'score': 31970.663256645203, 'total_duration': 34651.5661213398, 'accumulated_submission_time': 31970.663256645203, 'accumulated_eval_time': 2672.718720436096, 'accumulated_logging_time': 4.21485447883606}
I0204 01:02:48.002408 139910212208384 logging_writer.py:48] [70234] accumulated_eval_time=2672.718720, accumulated_logging_time=4.214854, accumulated_submission_time=31970.663257, global_step=70234, preemption_count=0, score=31970.663257, test/accuracy=0.481900, test/loss=2.420220, test/num_examples=10000, total_duration=34651.566121, train/accuracy=0.670273, train/loss=1.464149, validation/accuracy=0.600120, validation/loss=1.762584, validation/num_examples=50000
I0204 01:03:14.333323 139910203815680 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.3348321914672852, loss=5.006993770599365
I0204 01:03:59.157096 139910212208384 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.5903961658477783, loss=3.2512223720550537
I0204 01:04:45.633413 139910203815680 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.2924612760543823, loss=5.366670608520508
I0204 01:05:32.562460 139910212208384 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.6122137308120728, loss=3.5453152656555176
I0204 01:06:18.758826 139910203815680 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.5429478883743286, loss=3.8474462032318115
I0204 01:07:05.112281 139910212208384 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.6298426389694214, loss=3.2444944381713867
I0204 01:07:51.350311 139910203815680 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.5457894802093506, loss=3.39316725730896
I0204 01:08:37.745494 139910212208384 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.364622712135315, loss=3.8388495445251465
I0204 01:09:24.074861 139910203815680 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.2894917726516724, loss=4.8257904052734375
I0204 01:09:48.235791 140107197974336 spec.py:321] Evaluating on the training split.
I0204 01:09:58.967402 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 01:10:24.281152 140107197974336 spec.py:349] Evaluating on the test split.
I0204 01:10:25.888614 140107197974336 submission_runner.py:408] Time since start: 35109.48s, 	Step: 71154, 	{'train/accuracy': 0.6441210508346558, 'train/loss': 1.5635147094726562, 'validation/accuracy': 0.6025399565696716, 'validation/loss': 1.7569859027862549, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.3740594387054443, 'test/num_examples': 10000, 'score': 32390.83561515808, 'total_duration': 35109.47786331177, 'accumulated_submission_time': 32390.83561515808, 'accumulated_eval_time': 2710.3715307712555, 'accumulated_logging_time': 4.249778985977173}
I0204 01:10:25.919736 139910212208384 logging_writer.py:48] [71154] accumulated_eval_time=2710.371531, accumulated_logging_time=4.249779, accumulated_submission_time=32390.835615, global_step=71154, preemption_count=0, score=32390.835615, test/accuracy=0.490700, test/loss=2.374059, test/num_examples=10000, total_duration=35109.477863, train/accuracy=0.644121, train/loss=1.563515, validation/accuracy=0.602540, validation/loss=1.756986, validation/num_examples=50000
I0204 01:10:44.410477 139910203815680 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.6642447710037231, loss=3.2200450897216797
I0204 01:11:28.152175 139910212208384 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.68262779712677, loss=3.3415350914001465
I0204 01:12:14.058293 139910203815680 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.6646572351455688, loss=3.383216381072998
I0204 01:13:00.300925 139910212208384 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.6091347932815552, loss=3.3325119018554688
I0204 01:13:46.461627 139910203815680 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.4551937580108643, loss=5.482180595397949
I0204 01:14:32.627585 139910212208384 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.5858776569366455, loss=3.4212305545806885
I0204 01:15:18.795985 139910203815680 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.5229527950286865, loss=3.148819923400879
I0204 01:16:05.101067 139910212208384 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.6046569347381592, loss=3.390080451965332
I0204 01:16:50.791639 139910203815680 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.6858752965927124, loss=3.254213333129883
I0204 01:17:26.059223 140107197974336 spec.py:321] Evaluating on the training split.
I0204 01:17:37.440550 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 01:18:02.610664 140107197974336 spec.py:349] Evaluating on the test split.
I0204 01:18:04.211763 140107197974336 submission_runner.py:408] Time since start: 35567.80s, 	Step: 72077, 	{'train/accuracy': 0.6434960961341858, 'train/loss': 1.6113227605819702, 'validation/accuracy': 0.6014800071716309, 'validation/loss': 1.8047555685043335, 'validation/num_examples': 50000, 'test/accuracy': 0.48020002245903015, 'test/loss': 2.438359498977661, 'test/num_examples': 10000, 'score': 32810.90993022919, 'total_duration': 35567.80102777481, 'accumulated_submission_time': 32810.90993022919, 'accumulated_eval_time': 2748.524088859558, 'accumulated_logging_time': 4.293323755264282}
I0204 01:18:04.241973 139910212208384 logging_writer.py:48] [72077] accumulated_eval_time=2748.524089, accumulated_logging_time=4.293324, accumulated_submission_time=32810.909930, global_step=72077, preemption_count=0, score=32810.909930, test/accuracy=0.480200, test/loss=2.438359, test/num_examples=10000, total_duration=35567.801028, train/accuracy=0.643496, train/loss=1.611323, validation/accuracy=0.601480, validation/loss=1.804756, validation/num_examples=50000
I0204 01:18:13.880022 139910203815680 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.6013333797454834, loss=3.249717950820923
I0204 01:18:55.912494 139910212208384 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.7116074562072754, loss=3.179567813873291
I0204 01:19:42.090009 139910203815680 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.4899917840957642, loss=3.6865553855895996
I0204 01:20:29.063048 139910212208384 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.2743651866912842, loss=4.954829216003418
I0204 01:21:15.356226 139910203815680 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.8231943845748901, loss=3.151923179626465
I0204 01:22:01.563817 139910212208384 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.7606003284454346, loss=3.317201614379883
I0204 01:22:48.130419 139910203815680 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.7853976488113403, loss=3.1405067443847656
I0204 01:23:34.421879 139910212208384 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.6541181802749634, loss=3.6034884452819824
I0204 01:24:20.734687 139910203815680 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.6471935510635376, loss=3.076345920562744
I0204 01:25:04.637912 140107197974336 spec.py:321] Evaluating on the training split.
I0204 01:25:15.065157 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 01:25:41.575679 140107197974336 spec.py:349] Evaluating on the test split.
I0204 01:25:43.175349 140107197974336 submission_runner.py:408] Time since start: 36026.76s, 	Step: 72996, 	{'train/accuracy': 0.6573632955551147, 'train/loss': 1.5504130125045776, 'validation/accuracy': 0.5985599756240845, 'validation/loss': 1.8023332357406616, 'validation/num_examples': 50000, 'test/accuracy': 0.48250001668930054, 'test/loss': 2.439271926879883, 'test/num_examples': 10000, 'score': 33231.244406461716, 'total_duration': 36026.76460838318, 'accumulated_submission_time': 33231.244406461716, 'accumulated_eval_time': 2787.061530351639, 'accumulated_logging_time': 4.332884788513184}
I0204 01:25:43.201541 139910212208384 logging_writer.py:48] [72996] accumulated_eval_time=2787.061530, accumulated_logging_time=4.332885, accumulated_submission_time=33231.244406, global_step=72996, preemption_count=0, score=33231.244406, test/accuracy=0.482500, test/loss=2.439272, test/num_examples=10000, total_duration=36026.764608, train/accuracy=0.657363, train/loss=1.550413, validation/accuracy=0.598560, validation/loss=1.802333, validation/num_examples=50000
I0204 01:25:45.177555 139910203815680 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.537386417388916, loss=3.347928762435913
I0204 01:26:25.917713 139910212208384 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.2890491485595703, loss=3.993774890899658
I0204 01:27:11.806794 139910203815680 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.33620285987854, loss=4.785377502441406
I0204 01:27:57.821374 139910212208384 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.3661361932754517, loss=5.290762424468994
I0204 01:28:44.089607 139910203815680 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.6594998836517334, loss=3.1541314125061035
I0204 01:29:30.477877 139910212208384 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.9256092309951782, loss=3.123931407928467
I0204 01:30:16.742333 139910203815680 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.752852439880371, loss=3.3490896224975586
I0204 01:31:02.738814 139910212208384 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.5284279584884644, loss=3.419442653656006
I0204 01:31:48.775907 139910203815680 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.7089852094650269, loss=3.0871591567993164
I0204 01:32:35.189454 139910212208384 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.6489906311035156, loss=3.147301435470581
I0204 01:32:43.610472 140107197974336 spec.py:321] Evaluating on the training split.
I0204 01:32:54.230560 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 01:33:19.559165 140107197974336 spec.py:349] Evaluating on the test split.
I0204 01:33:21.141269 140107197974336 submission_runner.py:408] Time since start: 36484.73s, 	Step: 73920, 	{'train/accuracy': 0.6471093893051147, 'train/loss': 1.6129900217056274, 'validation/accuracy': 0.6095799803733826, 'validation/loss': 1.7827093601226807, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.417206048965454, 'test/num_examples': 10000, 'score': 33651.5921421051, 'total_duration': 36484.73053359985, 'accumulated_submission_time': 33651.5921421051, 'accumulated_eval_time': 2824.5923268795013, 'accumulated_logging_time': 4.368396282196045}
I0204 01:33:21.170628 139910203815680 logging_writer.py:48] [73920] accumulated_eval_time=2824.592327, accumulated_logging_time=4.368396, accumulated_submission_time=33651.592142, global_step=73920, preemption_count=0, score=33651.592142, test/accuracy=0.486700, test/loss=2.417206, test/num_examples=10000, total_duration=36484.730534, train/accuracy=0.647109, train/loss=1.612990, validation/accuracy=0.609580, validation/loss=1.782709, validation/num_examples=50000
I0204 01:33:53.027681 139910212208384 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.5825915336608887, loss=3.2072389125823975
I0204 01:34:38.856152 139910203815680 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.597182035446167, loss=3.3221616744995117
I0204 01:35:25.411037 139910212208384 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.4390664100646973, loss=4.188665390014648
I0204 01:36:12.032045 139910203815680 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.6223706007003784, loss=3.6086654663085938
I0204 01:36:58.130895 139910212208384 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.5480228662490845, loss=3.4744129180908203
I0204 01:37:44.511283 139910203815680 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.699771761894226, loss=3.3063485622406006
I0204 01:38:30.859182 139910212208384 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.9226855039596558, loss=3.2664103507995605
I0204 01:39:16.965225 139910203815680 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.872280478477478, loss=3.199105978012085
I0204 01:40:03.234611 139910212208384 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.5990548133850098, loss=3.917180061340332
I0204 01:40:21.445823 140107197974336 spec.py:321] Evaluating on the training split.
I0204 01:40:32.283310 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 01:40:56.639463 140107197974336 spec.py:349] Evaluating on the test split.
I0204 01:40:58.242008 140107197974336 submission_runner.py:408] Time since start: 36941.83s, 	Step: 74841, 	{'train/accuracy': 0.6474413871765137, 'train/loss': 1.5477566719055176, 'validation/accuracy': 0.606440007686615, 'validation/loss': 1.7396602630615234, 'validation/num_examples': 50000, 'test/accuracy': 0.48570001125335693, 'test/loss': 2.386080503463745, 'test/num_examples': 10000, 'score': 34071.80634212494, 'total_duration': 36941.8312625885, 'accumulated_submission_time': 34071.80634212494, 'accumulated_eval_time': 2861.388499736786, 'accumulated_logging_time': 4.406642913818359}
I0204 01:40:58.274509 139910203815680 logging_writer.py:48] [74841] accumulated_eval_time=2861.388500, accumulated_logging_time=4.406643, accumulated_submission_time=34071.806342, global_step=74841, preemption_count=0, score=34071.806342, test/accuracy=0.485700, test/loss=2.386081, test/num_examples=10000, total_duration=36941.831263, train/accuracy=0.647441, train/loss=1.547757, validation/accuracy=0.606440, validation/loss=1.739660, validation/num_examples=50000
I0204 01:41:21.841933 139910212208384 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.7780420780181885, loss=3.168088912963867
I0204 01:42:06.335566 139910203815680 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.9102965593338013, loss=3.17668080329895
I0204 01:42:52.525820 139910212208384 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.6869592666625977, loss=3.561737537384033
I0204 01:43:38.803579 139910203815680 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.7009385824203491, loss=3.369922161102295
I0204 01:44:25.144746 139910212208384 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.4541056156158447, loss=3.938389778137207
I0204 01:45:11.327921 139910203815680 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.4366806745529175, loss=5.208251476287842
I0204 01:45:57.415668 139910212208384 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.4773491621017456, loss=3.465158462524414
I0204 01:46:43.581206 139910203815680 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.4351035356521606, loss=4.673739433288574
I0204 01:47:29.678705 139910212208384 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.7442764043807983, loss=3.170008897781372
I0204 01:47:58.532390 140107197974336 spec.py:321] Evaluating on the training split.
I0204 01:48:09.471681 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 01:48:33.942627 140107197974336 spec.py:349] Evaluating on the test split.
I0204 01:48:35.540841 140107197974336 submission_runner.py:408] Time since start: 37399.13s, 	Step: 75764, 	{'train/accuracy': 0.6582812070846558, 'train/loss': 1.5243412256240845, 'validation/accuracy': 0.6102199554443359, 'validation/loss': 1.744834065437317, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.3967623710632324, 'test/num_examples': 10000, 'score': 34492.00156569481, 'total_duration': 37399.13010430336, 'accumulated_submission_time': 34492.00156569481, 'accumulated_eval_time': 2898.396971464157, 'accumulated_logging_time': 4.448941230773926}
I0204 01:48:35.567583 139910203815680 logging_writer.py:48] [75764] accumulated_eval_time=2898.396971, accumulated_logging_time=4.448941, accumulated_submission_time=34492.001566, global_step=75764, preemption_count=0, score=34492.001566, test/accuracy=0.482100, test/loss=2.396762, test/num_examples=10000, total_duration=37399.130104, train/accuracy=0.658281, train/loss=1.524341, validation/accuracy=0.610220, validation/loss=1.744834, validation/num_examples=50000
I0204 01:48:50.100602 139910212208384 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.6919763088226318, loss=3.0967583656311035
I0204 01:49:32.807829 139910203815680 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.5624325275421143, loss=4.092466831207275
I0204 01:50:19.009911 139910212208384 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.5275945663452148, loss=3.255079746246338
I0204 01:51:05.607672 139910203815680 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.7596986293792725, loss=3.137706995010376
I0204 01:51:51.485299 139910212208384 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.7171062231063843, loss=3.1870245933532715
I0204 01:52:37.762078 139910203815680 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.522853970527649, loss=4.7363715171813965
I0204 01:53:24.250716 139910212208384 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.813065528869629, loss=3.185910701751709
I0204 01:54:10.337549 139910203815680 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.7536380290985107, loss=3.1471147537231445
I0204 01:54:56.604304 139910212208384 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.4775733947753906, loss=3.8863399028778076
I0204 01:55:35.850364 140107197974336 spec.py:321] Evaluating on the training split.
I0204 01:55:46.684505 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 01:56:11.167770 140107197974336 spec.py:349] Evaluating on the test split.
I0204 01:56:12.760409 140107197974336 submission_runner.py:408] Time since start: 37856.35s, 	Step: 76687, 	{'train/accuracy': 0.6499804258346558, 'train/loss': 1.5522480010986328, 'validation/accuracy': 0.6068199872970581, 'validation/loss': 1.7552516460418701, 'validation/num_examples': 50000, 'test/accuracy': 0.4863000214099884, 'test/loss': 2.3870294094085693, 'test/num_examples': 10000, 'score': 34912.220725774765, 'total_duration': 37856.34967160225, 'accumulated_submission_time': 34912.220725774765, 'accumulated_eval_time': 2935.307032585144, 'accumulated_logging_time': 4.486681699752808}
I0204 01:56:12.787711 139910203815680 logging_writer.py:48] [76687] accumulated_eval_time=2935.307033, accumulated_logging_time=4.486682, accumulated_submission_time=34912.220726, global_step=76687, preemption_count=0, score=34912.220726, test/accuracy=0.486300, test/loss=2.387029, test/num_examples=10000, total_duration=37856.349672, train/accuracy=0.649980, train/loss=1.552248, validation/accuracy=0.606820, validation/loss=1.755252, validation/num_examples=50000
I0204 01:56:18.301851 139910212208384 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.962345004081726, loss=3.1998543739318848
I0204 01:56:59.448694 139910203815680 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.5452135801315308, loss=3.552736282348633
I0204 01:57:45.532965 139910212208384 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.5867717266082764, loss=4.511093616485596
I0204 01:58:31.852760 139910203815680 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.632257103919983, loss=3.280392646789551
I0204 01:59:17.773659 139910212208384 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.6099861860275269, loss=3.5182511806488037
I0204 02:00:03.998072 139910203815680 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.8024576902389526, loss=3.203031063079834
I0204 02:00:50.249359 139910212208384 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.8108153343200684, loss=3.4237570762634277
I0204 02:01:36.428419 139910203815680 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.4599266052246094, loss=5.310245513916016
I0204 02:02:22.610550 139910212208384 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.7583341598510742, loss=3.257702112197876
I0204 02:03:08.696242 139910203815680 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.6334737539291382, loss=3.4057552814483643
I0204 02:03:13.065744 140107197974336 spec.py:321] Evaluating on the training split.
I0204 02:03:23.900631 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 02:03:47.946828 140107197974336 spec.py:349] Evaluating on the test split.
I0204 02:03:49.538579 140107197974336 submission_runner.py:408] Time since start: 38313.13s, 	Step: 77611, 	{'train/accuracy': 0.655078113079071, 'train/loss': 1.556774377822876, 'validation/accuracy': 0.6091799736022949, 'validation/loss': 1.7530089616775513, 'validation/num_examples': 50000, 'test/accuracy': 0.4870000183582306, 'test/loss': 2.3812808990478516, 'test/num_examples': 10000, 'score': 35332.43723273277, 'total_duration': 38313.127844810486, 'accumulated_submission_time': 35332.43723273277, 'accumulated_eval_time': 2971.7798516750336, 'accumulated_logging_time': 4.523826599121094}
I0204 02:03:49.566998 139910212208384 logging_writer.py:48] [77611] accumulated_eval_time=2971.779852, accumulated_logging_time=4.523827, accumulated_submission_time=35332.437233, global_step=77611, preemption_count=0, score=35332.437233, test/accuracy=0.487000, test/loss=2.381281, test/num_examples=10000, total_duration=38313.127845, train/accuracy=0.655078, train/loss=1.556774, validation/accuracy=0.609180, validation/loss=1.753009, validation/num_examples=50000
I0204 02:04:25.502065 139910203815680 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.5587830543518066, loss=3.3329741954803467
I0204 02:05:11.529493 139910212208384 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.8773528337478638, loss=4.609006404876709
I0204 02:05:57.989371 139910203815680 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.556607723236084, loss=5.234619140625
I0204 02:06:44.140795 139910212208384 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.8803633451461792, loss=3.06575083732605
I0204 02:07:30.148817 139910203815680 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.3882720470428467, loss=4.111945152282715
I0204 02:08:16.613845 139910212208384 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.6841849088668823, loss=3.2166361808776855
I0204 02:09:02.970826 139910203815680 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.6297187805175781, loss=3.0630362033843994
I0204 02:09:49.034443 139910212208384 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.7861911058425903, loss=3.1602749824523926
I0204 02:10:35.415002 139910203815680 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.806059718132019, loss=3.122051239013672
I0204 02:10:49.924124 140107197974336 spec.py:321] Evaluating on the training split.
I0204 02:11:00.801357 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 02:11:26.845340 140107197974336 spec.py:349] Evaluating on the test split.
I0204 02:11:28.436320 140107197974336 submission_runner.py:408] Time since start: 38772.03s, 	Step: 78533, 	{'train/accuracy': 0.6649804711341858, 'train/loss': 1.5041078329086304, 'validation/accuracy': 0.6135199666023254, 'validation/loss': 1.7273281812667847, 'validation/num_examples': 50000, 'test/accuracy': 0.4961000382900238, 'test/loss': 2.3592467308044434, 'test/num_examples': 10000, 'score': 35752.73362803459, 'total_duration': 38772.0255856514, 'accumulated_submission_time': 35752.73362803459, 'accumulated_eval_time': 3010.292057991028, 'accumulated_logging_time': 4.561509370803833}
I0204 02:11:28.464172 139910212208384 logging_writer.py:48] [78533] accumulated_eval_time=3010.292058, accumulated_logging_time=4.561509, accumulated_submission_time=35752.733628, global_step=78533, preemption_count=0, score=35752.733628, test/accuracy=0.496100, test/loss=2.359247, test/num_examples=10000, total_duration=38772.025586, train/accuracy=0.664980, train/loss=1.504108, validation/accuracy=0.613520, validation/loss=1.727328, validation/num_examples=50000
I0204 02:11:55.315394 139910203815680 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.6915783882141113, loss=3.0854997634887695
I0204 02:12:40.066841 139910212208384 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.6928284168243408, loss=3.1915974617004395
I0204 02:13:26.349589 139910203815680 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.7903578281402588, loss=3.1212735176086426
I0204 02:14:13.185641 139910212208384 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.5602777004241943, loss=5.292223930358887
I0204 02:14:59.452595 139910203815680 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.405638575553894, loss=5.419692039489746
I0204 02:15:45.773196 139910212208384 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.7285833358764648, loss=3.206868886947632
I0204 02:16:32.003529 139910203815680 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.7188385725021362, loss=3.2802958488464355
I0204 02:17:18.230571 139910212208384 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.4117826223373413, loss=4.813357353210449
I0204 02:18:04.468993 139910203815680 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.8341466188430786, loss=3.113110065460205
I0204 02:18:28.674627 140107197974336 spec.py:321] Evaluating on the training split.
I0204 02:18:39.150417 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 02:19:04.213686 140107197974336 spec.py:349] Evaluating on the test split.
I0204 02:19:05.815118 140107197974336 submission_runner.py:408] Time since start: 39229.40s, 	Step: 79454, 	{'train/accuracy': 0.6874414086341858, 'train/loss': 1.3758230209350586, 'validation/accuracy': 0.6157599687576294, 'validation/loss': 1.6964789628982544, 'validation/num_examples': 50000, 'test/accuracy': 0.48740002512931824, 'test/loss': 2.3509433269500732, 'test/num_examples': 10000, 'score': 36172.88255786896, 'total_duration': 39229.4043803215, 'accumulated_submission_time': 36172.88255786896, 'accumulated_eval_time': 3047.432544708252, 'accumulated_logging_time': 4.599277496337891}
I0204 02:19:05.847566 139910212208384 logging_writer.py:48] [79454] accumulated_eval_time=3047.432545, accumulated_logging_time=4.599277, accumulated_submission_time=36172.882558, global_step=79454, preemption_count=0, score=36172.882558, test/accuracy=0.487400, test/loss=2.350943, test/num_examples=10000, total_duration=39229.404380, train/accuracy=0.687441, train/loss=1.375823, validation/accuracy=0.615760, validation/loss=1.696479, validation/num_examples=50000
I0204 02:19:24.322814 139910203815680 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.502176284790039, loss=5.386443138122559
I0204 02:20:07.634793 139910212208384 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.2846589088439941, loss=5.038034439086914
I0204 02:20:53.556159 139910203815680 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.6235417127609253, loss=5.040042877197266
I0204 02:21:39.926462 139910212208384 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.7215394973754883, loss=3.3418750762939453
I0204 02:22:26.231352 139910203815680 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.6823328733444214, loss=3.2747585773468018
I0204 02:23:12.206877 139910212208384 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.659034252166748, loss=3.229113817214966
I0204 02:23:58.549377 139910203815680 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.3494906425476074, loss=4.100263595581055
I0204 02:24:44.835500 139910212208384 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.7965127229690552, loss=3.1568498611450195
I0204 02:25:30.818823 139910203815680 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.7235053777694702, loss=3.1561360359191895
I0204 02:26:06.132448 140107197974336 spec.py:321] Evaluating on the training split.
I0204 02:26:17.223491 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 02:26:42.107718 140107197974336 spec.py:349] Evaluating on the test split.
I0204 02:26:43.707207 140107197974336 submission_runner.py:408] Time since start: 39687.30s, 	Step: 80378, 	{'train/accuracy': 0.6550585627555847, 'train/loss': 1.5762782096862793, 'validation/accuracy': 0.6106799840927124, 'validation/loss': 1.7810405492782593, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.4137213230133057, 'test/num_examples': 10000, 'score': 36593.105981349945, 'total_duration': 39687.296456336975, 'accumulated_submission_time': 36593.105981349945, 'accumulated_eval_time': 3085.007307291031, 'accumulated_logging_time': 4.6409912109375}
I0204 02:26:43.734834 139910212208384 logging_writer.py:48] [80378] accumulated_eval_time=3085.007307, accumulated_logging_time=4.640991, accumulated_submission_time=36593.105981, global_step=80378, preemption_count=0, score=36593.105981, test/accuracy=0.488000, test/loss=2.413721, test/num_examples=10000, total_duration=39687.296456, train/accuracy=0.655059, train/loss=1.576278, validation/accuracy=0.610680, validation/loss=1.781041, validation/num_examples=50000
I0204 02:26:52.775340 139910203815680 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.0204577445983887, loss=3.5234553813934326
I0204 02:27:34.504187 139910212208384 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.5041460990905762, loss=4.404248237609863
I0204 02:28:20.761599 139910203815680 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.8146864175796509, loss=3.1523406505584717
I0204 02:29:07.274947 139910212208384 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.6037898063659668, loss=3.102739095687866
I0204 02:29:53.411349 139910203815680 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.5148122310638428, loss=4.839106559753418
I0204 02:30:39.741760 139910212208384 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.8406141996383667, loss=3.2320525646209717
I0204 02:31:26.113900 139910203815680 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.6042500734329224, loss=3.4492545127868652
I0204 02:32:12.277994 139910212208384 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.7480703592300415, loss=3.554478645324707
I0204 02:32:58.463510 139910203815680 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.3795032501220703, loss=4.728766441345215
I0204 02:33:44.016467 140107197974336 spec.py:321] Evaluating on the training split.
I0204 02:33:54.465668 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 02:34:20.615233 140107197974336 spec.py:349] Evaluating on the test split.
I0204 02:34:22.226525 140107197974336 submission_runner.py:408] Time since start: 40145.82s, 	Step: 81300, 	{'train/accuracy': 0.6625585556030273, 'train/loss': 1.5070688724517822, 'validation/accuracy': 0.6144799590110779, 'validation/loss': 1.7260377407073975, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.3620481491088867, 'test/num_examples': 10000, 'score': 37013.32633161545, 'total_duration': 40145.815786361694, 'accumulated_submission_time': 37013.32633161545, 'accumulated_eval_time': 3123.217358827591, 'accumulated_logging_time': 4.677295207977295}
I0204 02:34:22.258110 139910212208384 logging_writer.py:48] [81300] accumulated_eval_time=3123.217359, accumulated_logging_time=4.677295, accumulated_submission_time=37013.326332, global_step=81300, preemption_count=0, score=37013.326332, test/accuracy=0.493900, test/loss=2.362048, test/num_examples=10000, total_duration=40145.815786, train/accuracy=0.662559, train/loss=1.507069, validation/accuracy=0.614480, validation/loss=1.726038, validation/num_examples=50000
I0204 02:34:22.663413 139910203815680 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.009190320968628, loss=3.168109893798828
I0204 02:35:02.976605 139910212208384 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.8577827215194702, loss=3.108311891555786
I0204 02:35:48.993166 139910203815680 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.682029128074646, loss=3.6753578186035156
I0204 02:36:35.547002 139910212208384 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.853868007659912, loss=3.1755332946777344
I0204 02:37:22.292158 139910203815680 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.4223965406417847, loss=3.8941853046417236
I0204 02:38:08.683447 139910212208384 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.8337304592132568, loss=3.0758931636810303
I0204 02:38:55.041458 139910203815680 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.441706895828247, loss=4.972566604614258
I0204 02:39:41.251443 139910212208384 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.8188347816467285, loss=3.1564769744873047
I0204 02:40:27.414815 139910203815680 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.6532142162322998, loss=3.1813275814056396
I0204 02:41:13.879493 139910212208384 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.6641619205474854, loss=3.1462292671203613
I0204 02:41:22.639282 140107197974336 spec.py:321] Evaluating on the training split.
I0204 02:41:33.158770 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 02:41:58.614141 140107197974336 spec.py:349] Evaluating on the test split.
I0204 02:42:00.221941 140107197974336 submission_runner.py:408] Time since start: 40603.81s, 	Step: 82221, 	{'train/accuracy': 0.6760546565055847, 'train/loss': 1.452250361442566, 'validation/accuracy': 0.6148200035095215, 'validation/loss': 1.7269800901412964, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.362778425216675, 'test/num_examples': 10000, 'score': 37433.646663188934, 'total_duration': 40603.81119298935, 'accumulated_submission_time': 37433.646663188934, 'accumulated_eval_time': 3160.800005197525, 'accumulated_logging_time': 4.718563795089722}
I0204 02:42:00.252309 139910203815680 logging_writer.py:48] [82221] accumulated_eval_time=3160.800005, accumulated_logging_time=4.718564, accumulated_submission_time=37433.646663, global_step=82221, preemption_count=0, score=37433.646663, test/accuracy=0.492900, test/loss=2.362778, test/num_examples=10000, total_duration=40603.811193, train/accuracy=0.676055, train/loss=1.452250, validation/accuracy=0.614820, validation/loss=1.726980, validation/num_examples=50000
I0204 02:42:31.703452 139910212208384 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.5239932537078857, loss=4.102939605712891
I0204 02:43:17.806681 139910203815680 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.9726845026016235, loss=3.2454631328582764
I0204 02:44:04.167902 139910212208384 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.4247549772262573, loss=4.5104780197143555
I0204 02:44:51.218877 139910203815680 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.8107787370681763, loss=3.134643077850342
I0204 02:45:37.402041 139910212208384 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.7726534605026245, loss=3.6807241439819336
I0204 02:46:23.780755 139910203815680 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.4542746543884277, loss=5.267021656036377
I0204 02:47:10.218285 139910212208384 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.642642855644226, loss=3.4338817596435547
I0204 02:47:56.373541 139910203815680 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.3811767101287842, loss=4.435949802398682
I0204 02:48:42.350208 139910212208384 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.7641000747680664, loss=3.2002110481262207
I0204 02:49:00.657779 140107197974336 spec.py:321] Evaluating on the training split.
I0204 02:49:11.457629 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 02:49:37.578645 140107197974336 spec.py:349] Evaluating on the test split.
I0204 02:49:39.176927 140107197974336 submission_runner.py:408] Time since start: 41062.77s, 	Step: 83141, 	{'train/accuracy': 0.6602343320846558, 'train/loss': 1.4971674680709839, 'validation/accuracy': 0.6192399859428406, 'validation/loss': 1.6920735836029053, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.319068193435669, 'test/num_examples': 10000, 'score': 37853.990706920624, 'total_duration': 41062.76618885994, 'accumulated_submission_time': 37853.990706920624, 'accumulated_eval_time': 3199.3191499710083, 'accumulated_logging_time': 4.758185863494873}
I0204 02:49:39.205227 139910203815680 logging_writer.py:48] [83141] accumulated_eval_time=3199.319150, accumulated_logging_time=4.758186, accumulated_submission_time=37853.990707, global_step=83141, preemption_count=0, score=37853.990707, test/accuracy=0.502100, test/loss=2.319068, test/num_examples=10000, total_duration=41062.766189, train/accuracy=0.660234, train/loss=1.497167, validation/accuracy=0.619240, validation/loss=1.692074, validation/num_examples=50000
I0204 02:50:02.765777 139910212208384 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.6782751083374023, loss=3.4226717948913574
I0204 02:50:47.106003 139910203815680 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.7781133651733398, loss=3.0249199867248535
I0204 02:51:33.444754 139910212208384 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.7328240871429443, loss=3.2461376190185547
I0204 02:52:19.950430 139910203815680 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.49368417263031, loss=4.241143703460693
I0204 02:53:06.130951 139910212208384 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.9890320301055908, loss=3.1513609886169434
I0204 02:53:52.687205 139910203815680 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.879209280014038, loss=3.2173614501953125
I0204 02:54:39.320828 139910212208384 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.488349199295044, loss=4.86891508102417
I0204 02:55:25.724266 139910203815680 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.6541584730148315, loss=4.003683090209961
I0204 02:56:11.921263 139910212208384 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.7158665657043457, loss=3.348606586456299
I0204 02:56:39.399251 140107197974336 spec.py:321] Evaluating on the training split.
I0204 02:56:49.935191 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 02:57:16.144022 140107197974336 spec.py:349] Evaluating on the test split.
I0204 02:57:17.743137 140107197974336 submission_runner.py:408] Time since start: 41521.33s, 	Step: 84061, 	{'train/accuracy': 0.6661523580551147, 'train/loss': 1.5092904567718506, 'validation/accuracy': 0.6180399656295776, 'validation/loss': 1.7301098108291626, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.355710029602051, 'test/num_examples': 10000, 'score': 38274.122478723526, 'total_duration': 41521.33239722252, 'accumulated_submission_time': 38274.122478723526, 'accumulated_eval_time': 3237.6630806922913, 'accumulated_logging_time': 4.796828985214233}
I0204 02:57:17.775614 139910203815680 logging_writer.py:48] [84061] accumulated_eval_time=3237.663081, accumulated_logging_time=4.796829, accumulated_submission_time=38274.122479, global_step=84061, preemption_count=0, score=38274.122479, test/accuracy=0.498800, test/loss=2.355710, test/num_examples=10000, total_duration=41521.332397, train/accuracy=0.666152, train/loss=1.509290, validation/accuracy=0.618040, validation/loss=1.730110, validation/num_examples=50000
I0204 02:57:33.500982 139910212208384 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.7054919004440308, loss=5.243081569671631
I0204 02:58:16.602928 139910203815680 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.8739638328552246, loss=3.1129422187805176
I0204 02:59:02.901578 139910212208384 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.728637456893921, loss=5.409862995147705
I0204 02:59:49.269856 139910203815680 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.892943263053894, loss=3.749804973602295
I0204 03:00:35.205670 139910212208384 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.4115540981292725, loss=4.715339183807373
I0204 03:01:21.555085 139910203815680 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.74627685546875, loss=3.3317220211029053
I0204 03:02:08.000787 139910212208384 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.676090955734253, loss=4.992337226867676
I0204 03:02:54.236126 139910203815680 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.0372474193573, loss=3.2895936965942383
I0204 03:03:40.527464 139910212208384 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.7996504306793213, loss=3.291890859603882
I0204 03:04:17.969889 140107197974336 spec.py:321] Evaluating on the training split.
I0204 03:04:28.808630 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 03:04:53.958697 140107197974336 spec.py:349] Evaluating on the test split.
I0204 03:04:55.563100 140107197974336 submission_runner.py:408] Time since start: 41979.15s, 	Step: 84983, 	{'train/accuracy': 0.6742187142372131, 'train/loss': 1.436706781387329, 'validation/accuracy': 0.6194199919700623, 'validation/loss': 1.680558204650879, 'validation/num_examples': 50000, 'test/accuracy': 0.5003000497817993, 'test/loss': 2.3204102516174316, 'test/num_examples': 10000, 'score': 38694.25512552261, 'total_duration': 41979.15235233307, 'accumulated_submission_time': 38694.25512552261, 'accumulated_eval_time': 3275.2562849521637, 'accumulated_logging_time': 4.83923602104187}
I0204 03:04:55.593754 139910203815680 logging_writer.py:48] [84983] accumulated_eval_time=3275.256285, accumulated_logging_time=4.839236, accumulated_submission_time=38694.255126, global_step=84983, preemption_count=0, score=38694.255126, test/accuracy=0.500300, test/loss=2.320410, test/num_examples=10000, total_duration=41979.152352, train/accuracy=0.674219, train/loss=1.436707, validation/accuracy=0.619420, validation/loss=1.680558, validation/num_examples=50000
I0204 03:05:02.677333 139910212208384 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.7037787437438965, loss=3.1371817588806152
I0204 03:05:44.368388 139910203815680 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.6823982000350952, loss=3.142801284790039
I0204 03:06:30.422852 139910212208384 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.6072094440460205, loss=3.6800315380096436
I0204 03:07:16.648920 139910203815680 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.4241819381713867, loss=4.7753071784973145
I0204 03:08:03.030469 139910212208384 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.7369134426116943, loss=3.68115234375
I0204 03:08:48.982562 139910203815680 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.7537297010421753, loss=3.121312379837036
I0204 03:09:35.298136 139910212208384 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.7127652168273926, loss=2.974221706390381
I0204 03:10:21.624461 139910203815680 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.005810260772705, loss=3.137932062149048
I0204 03:11:07.810071 139910212208384 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.6682294607162476, loss=3.7058982849121094
I0204 03:11:54.077243 139910203815680 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.4733142852783203, loss=4.320018768310547
I0204 03:11:55.643622 140107197974336 spec.py:321] Evaluating on the training split.
I0204 03:12:06.459258 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 03:12:31.851062 140107197974336 spec.py:349] Evaluating on the test split.
I0204 03:12:33.455830 140107197974336 submission_runner.py:408] Time since start: 42437.05s, 	Step: 85905, 	{'train/accuracy': 0.657031238079071, 'train/loss': 1.5600054264068604, 'validation/accuracy': 0.6131199598312378, 'validation/loss': 1.742652177810669, 'validation/num_examples': 50000, 'test/accuracy': 0.4909000098705292, 'test/loss': 2.3830223083496094, 'test/num_examples': 10000, 'score': 39114.24275946617, 'total_duration': 42437.045094013214, 'accumulated_submission_time': 39114.24275946617, 'accumulated_eval_time': 3313.068485021591, 'accumulated_logging_time': 4.880053997039795}
I0204 03:12:33.486740 139910212208384 logging_writer.py:48] [85905] accumulated_eval_time=3313.068485, accumulated_logging_time=4.880054, accumulated_submission_time=39114.242759, global_step=85905, preemption_count=0, score=39114.242759, test/accuracy=0.490900, test/loss=2.383022, test/num_examples=10000, total_duration=42437.045094, train/accuracy=0.657031, train/loss=1.560005, validation/accuracy=0.613120, validation/loss=1.742652, validation/num_examples=50000
I0204 03:13:12.045120 139910203815680 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.7537474632263184, loss=3.206418752670288
I0204 03:13:57.820271 139910212208384 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.866241693496704, loss=3.047278642654419
I0204 03:14:44.476361 139910203815680 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.5499403476715088, loss=3.8679869174957275
I0204 03:15:30.819541 139910212208384 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.5598629713058472, loss=4.890562534332275
I0204 03:16:17.241352 139910203815680 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.7781116962432861, loss=3.1290738582611084
I0204 03:17:03.474517 139910212208384 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.6259405612945557, loss=3.7782063484191895
I0204 03:17:49.628281 139910203815680 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.9316941499710083, loss=3.09346604347229
I0204 03:18:35.987544 139910212208384 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.5456314086914062, loss=3.8457794189453125
I0204 03:19:22.270733 139910203815680 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.9941552877426147, loss=3.138742208480835
I0204 03:19:33.523269 140107197974336 spec.py:321] Evaluating on the training split.
I0204 03:19:44.159893 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 03:20:10.296826 140107197974336 spec.py:349] Evaluating on the test split.
I0204 03:20:11.895054 140107197974336 submission_runner.py:408] Time since start: 42895.48s, 	Step: 86826, 	{'train/accuracy': 0.6669530868530273, 'train/loss': 1.5276052951812744, 'validation/accuracy': 0.6200399994850159, 'validation/loss': 1.735192894935608, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.3761723041534424, 'test/num_examples': 10000, 'score': 39534.21739983559, 'total_duration': 42895.48432254791, 'accumulated_submission_time': 39534.21739983559, 'accumulated_eval_time': 3351.4402787685394, 'accumulated_logging_time': 4.920087099075317}
I0204 03:20:11.925813 139910212208384 logging_writer.py:48] [86826] accumulated_eval_time=3351.440279, accumulated_logging_time=4.920087, accumulated_submission_time=39534.217400, global_step=86826, preemption_count=0, score=39534.217400, test/accuracy=0.497100, test/loss=2.376172, test/num_examples=10000, total_duration=42895.484323, train/accuracy=0.666953, train/loss=1.527605, validation/accuracy=0.620040, validation/loss=1.735193, validation/num_examples=50000
I0204 03:20:41.390103 139910203815680 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.8177587985992432, loss=3.217135429382324
I0204 03:21:26.617806 139910212208384 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.8086127042770386, loss=3.071697950363159
I0204 03:22:12.765885 139910203815680 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.869025468826294, loss=3.0348808765411377
I0204 03:22:58.875540 139910212208384 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.7335253953933716, loss=3.216630697250366
I0204 03:23:45.015830 139910203815680 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.8067600727081299, loss=3.1552937030792236
I0204 03:24:31.472226 139910212208384 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.7121351957321167, loss=3.2705416679382324
I0204 03:25:17.805975 139910203815680 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.0799598693847656, loss=3.0865674018859863
I0204 03:26:04.199268 139910212208384 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.898344874382019, loss=3.099419593811035
I0204 03:26:49.923760 139910203815680 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.9562677145004272, loss=3.2591376304626465
I0204 03:27:11.995852 140107197974336 spec.py:321] Evaluating on the training split.
I0204 03:27:22.663029 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 03:27:46.323891 140107197974336 spec.py:349] Evaluating on the test split.
I0204 03:27:47.924231 140107197974336 submission_runner.py:408] Time since start: 43351.51s, 	Step: 87749, 	{'train/accuracy': 0.6812499761581421, 'train/loss': 1.3998364210128784, 'validation/accuracy': 0.6264599561691284, 'validation/loss': 1.6430705785751343, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.299318552017212, 'test/num_examples': 10000, 'score': 39954.22675919533, 'total_duration': 43351.513491392136, 'accumulated_submission_time': 39954.22675919533, 'accumulated_eval_time': 3387.36865234375, 'accumulated_logging_time': 4.959564685821533}
I0204 03:27:47.954191 139910212208384 logging_writer.py:48] [87749] accumulated_eval_time=3387.368652, accumulated_logging_time=4.959565, accumulated_submission_time=39954.226759, global_step=87749, preemption_count=0, score=39954.226759, test/accuracy=0.502000, test/loss=2.299319, test/num_examples=10000, total_duration=43351.513491, train/accuracy=0.681250, train/loss=1.399836, validation/accuracy=0.626460, validation/loss=1.643071, validation/num_examples=50000
I0204 03:28:08.408205 139910203815680 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.8647561073303223, loss=3.126866340637207
I0204 03:28:52.164602 139910212208384 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.9205597639083862, loss=3.848121166229248
I0204 03:29:38.413154 139910203815680 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.6703643798828125, loss=3.2933835983276367
I0204 03:30:24.956213 139910212208384 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.7790801525115967, loss=3.0663552284240723
I0204 03:31:11.237199 139910203815680 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.5288244485855103, loss=4.272183418273926
I0204 03:31:57.375707 139910212208384 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.683933973312378, loss=3.9484119415283203
I0204 03:32:43.987582 139910203815680 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.7649391889572144, loss=3.1921117305755615
I0204 03:33:30.184964 139910212208384 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.7531142234802246, loss=5.174347877502441
I0204 03:34:16.321934 139910203815680 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.9022377729415894, loss=3.091254234313965
I0204 03:34:48.171434 140107197974336 spec.py:321] Evaluating on the training split.
I0204 03:34:58.935385 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 03:35:25.291444 140107197974336 spec.py:349] Evaluating on the test split.
I0204 03:35:26.887869 140107197974336 submission_runner.py:408] Time since start: 43810.48s, 	Step: 88670, 	{'train/accuracy': 0.6852734088897705, 'train/loss': 1.405772089958191, 'validation/accuracy': 0.6255399584770203, 'validation/loss': 1.6802127361297607, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.3180766105651855, 'test/num_examples': 10000, 'score': 40374.383286714554, 'total_duration': 43810.477133750916, 'accumulated_submission_time': 40374.383286714554, 'accumulated_eval_time': 3426.085087776184, 'accumulated_logging_time': 4.998172760009766}
I0204 03:35:26.915769 139910212208384 logging_writer.py:48] [88670] accumulated_eval_time=3426.085088, accumulated_logging_time=4.998173, accumulated_submission_time=40374.383287, global_step=88670, preemption_count=0, score=40374.383287, test/accuracy=0.508200, test/loss=2.318077, test/num_examples=10000, total_duration=43810.477134, train/accuracy=0.685273, train/loss=1.405772, validation/accuracy=0.625540, validation/loss=1.680213, validation/num_examples=50000
I0204 03:35:39.098476 139910203815680 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.0944254398345947, loss=3.16810941696167
I0204 03:36:21.565021 139910212208384 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.761491060256958, loss=2.915271282196045
I0204 03:37:07.692235 139910203815680 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.71924889087677, loss=3.254514455795288
I0204 03:37:53.865640 139910212208384 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.8387670516967773, loss=3.4162020683288574
I0204 03:38:40.629690 139910203815680 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.5513588190078735, loss=3.508798360824585
I0204 03:39:26.590023 139910212208384 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.1387720108032227, loss=3.1667232513427734
I0204 03:40:12.952902 139910203815680 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.9217759370803833, loss=3.0133590698242188
I0204 03:40:59.274848 139910212208384 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.7269978523254395, loss=3.4359045028686523
I0204 03:41:45.285958 139910203815680 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.9177331924438477, loss=3.0944600105285645
I0204 03:42:27.182386 140107197974336 spec.py:321] Evaluating on the training split.
I0204 03:42:37.931304 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 03:43:02.934491 140107197974336 spec.py:349] Evaluating on the test split.
I0204 03:43:04.535994 140107197974336 submission_runner.py:408] Time since start: 44268.13s, 	Step: 89592, 	{'train/accuracy': 0.6747655868530273, 'train/loss': 1.4603726863861084, 'validation/accuracy': 0.6269800066947937, 'validation/loss': 1.6675292253494263, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.291046142578125, 'test/num_examples': 10000, 'score': 40794.58645391464, 'total_duration': 44268.12525868416, 'accumulated_submission_time': 40794.58645391464, 'accumulated_eval_time': 3463.438698530197, 'accumulated_logging_time': 5.036881446838379}
I0204 03:43:04.564068 139910212208384 logging_writer.py:48] [89592] accumulated_eval_time=3463.438699, accumulated_logging_time=5.036881, accumulated_submission_time=40794.586454, global_step=89592, preemption_count=0, score=40794.586454, test/accuracy=0.510300, test/loss=2.291046, test/num_examples=10000, total_duration=44268.125259, train/accuracy=0.674766, train/loss=1.460373, validation/accuracy=0.626980, validation/loss=1.667529, validation/num_examples=50000
I0204 03:43:08.100627 139910203815680 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.618038535118103, loss=5.3436360359191895
I0204 03:43:48.873392 139910212208384 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.8935383558273315, loss=3.152575731277466
I0204 03:44:34.966109 139910203815680 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.9288594722747803, loss=3.5369136333465576
I0204 03:45:21.422693 139910212208384 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.687150001525879, loss=3.2666943073272705
I0204 03:46:07.823383 139910203815680 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.8580942153930664, loss=3.2822442054748535
I0204 03:46:53.975346 139910212208384 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.7689402103424072, loss=3.040201187133789
I0204 03:47:40.158233 139910203815680 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.8677783012390137, loss=3.0096242427825928
I0204 03:48:26.440041 139910212208384 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.726637601852417, loss=3.0696465969085693
I0204 03:49:12.757003 139910203815680 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.828966498374939, loss=3.1387650966644287
I0204 03:49:58.722554 139910212208384 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.8634350299835205, loss=3.079638719558716
I0204 03:50:04.985945 140107197974336 spec.py:321] Evaluating on the training split.
I0204 03:50:15.633616 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 03:50:41.380170 140107197974336 spec.py:349] Evaluating on the test split.
I0204 03:50:42.984784 140107197974336 submission_runner.py:408] Time since start: 44726.57s, 	Step: 90515, 	{'train/accuracy': 0.6829491853713989, 'train/loss': 1.4198222160339355, 'validation/accuracy': 0.6295199990272522, 'validation/loss': 1.6494418382644653, 'validation/num_examples': 50000, 'test/accuracy': 0.5087000131607056, 'test/loss': 2.2928144931793213, 'test/num_examples': 10000, 'score': 41214.94577026367, 'total_duration': 44726.574046611786, 'accumulated_submission_time': 41214.94577026367, 'accumulated_eval_time': 3501.4375364780426, 'accumulated_logging_time': 5.075735569000244}
I0204 03:50:43.014164 139910203815680 logging_writer.py:48] [90515] accumulated_eval_time=3501.437536, accumulated_logging_time=5.075736, accumulated_submission_time=41214.945770, global_step=90515, preemption_count=0, score=41214.945770, test/accuracy=0.508700, test/loss=2.292814, test/num_examples=10000, total_duration=44726.574047, train/accuracy=0.682949, train/loss=1.419822, validation/accuracy=0.629520, validation/loss=1.649442, validation/num_examples=50000
I0204 03:51:17.064858 139910212208384 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.9374171495437622, loss=3.0818262100219727
I0204 03:52:03.246309 139910203815680 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.7462124824523926, loss=3.0241551399230957
I0204 03:52:49.916690 139910212208384 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.5914241075515747, loss=4.50423002243042
I0204 03:53:36.646071 139910203815680 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.9565279483795166, loss=3.073186159133911
I0204 03:54:22.763873 139910212208384 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.6093779802322388, loss=3.702620506286621
I0204 03:55:09.423129 139910203815680 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.4907255172729492, loss=4.68058967590332
I0204 03:55:55.683272 139910212208384 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.6202472448349, loss=4.5987725257873535
I0204 03:56:41.929871 139910203815680 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.8444774150848389, loss=3.0092859268188477
I0204 03:57:28.467763 139910212208384 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.6379272937774658, loss=3.65218186378479
I0204 03:57:43.346505 140107197974336 spec.py:321] Evaluating on the training split.
I0204 03:57:54.240408 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 03:58:21.712727 140107197974336 spec.py:349] Evaluating on the test split.
I0204 03:58:23.313880 140107197974336 submission_runner.py:408] Time since start: 45186.90s, 	Step: 91434, 	{'train/accuracy': 0.6955859065055847, 'train/loss': 1.3533570766448975, 'validation/accuracy': 0.6298800110816956, 'validation/loss': 1.656938910484314, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.301645517349243, 'test/num_examples': 10000, 'score': 41635.21865081787, 'total_duration': 45186.903146505356, 'accumulated_submission_time': 41635.21865081787, 'accumulated_eval_time': 3541.404905796051, 'accumulated_logging_time': 5.113611698150635}
I0204 03:58:23.342570 139910203815680 logging_writer.py:48] [91434] accumulated_eval_time=3541.404906, accumulated_logging_time=5.113612, accumulated_submission_time=41635.218651, global_step=91434, preemption_count=0, score=41635.218651, test/accuracy=0.506500, test/loss=2.301646, test/num_examples=10000, total_duration=45186.903147, train/accuracy=0.695586, train/loss=1.353357, validation/accuracy=0.629880, validation/loss=1.656939, validation/num_examples=50000
I0204 03:58:49.664495 139910212208384 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.9600845575332642, loss=3.2421939373016357
I0204 03:59:34.518836 139910203815680 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.0002739429473877, loss=3.041159152984619
I0204 04:00:21.096148 139910212208384 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.7796093225479126, loss=3.05574893951416
I0204 04:01:07.625298 139910203815680 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.6977174282073975, loss=4.748424053192139
I0204 04:01:53.776211 139910212208384 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.7732083797454834, loss=3.8202595710754395
I0204 04:02:40.004073 139910203815680 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.98653244972229, loss=3.071307420730591
I0204 04:03:26.431473 139910212208384 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.6703213453292847, loss=3.7490687370300293
I0204 04:04:12.744604 139910203815680 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.691540241241455, loss=4.056880950927734
I0204 04:04:59.439085 139910212208384 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.6523277759552002, loss=3.259134292602539
I0204 04:05:23.423094 140107197974336 spec.py:321] Evaluating on the training split.
I0204 04:05:34.221513 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 04:05:59.196387 140107197974336 spec.py:349] Evaluating on the test split.
I0204 04:06:00.796063 140107197974336 submission_runner.py:408] Time since start: 45644.39s, 	Step: 92353, 	{'train/accuracy': 0.6741992235183716, 'train/loss': 1.4724483489990234, 'validation/accuracy': 0.6269999742507935, 'validation/loss': 1.6782883405685425, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.322950601577759, 'test/num_examples': 10000, 'score': 42055.23688745499, 'total_duration': 45644.38531947136, 'accumulated_submission_time': 42055.23688745499, 'accumulated_eval_time': 3578.7778816223145, 'accumulated_logging_time': 5.15111517906189}
I0204 04:06:00.826930 139910203815680 logging_writer.py:48] [92353] accumulated_eval_time=3578.777882, accumulated_logging_time=5.151115, accumulated_submission_time=42055.236887, global_step=92353, preemption_count=0, score=42055.236887, test/accuracy=0.502800, test/loss=2.322951, test/num_examples=10000, total_duration=45644.385319, train/accuracy=0.674199, train/loss=1.472448, validation/accuracy=0.627000, validation/loss=1.678288, validation/num_examples=50000
I0204 04:06:19.681044 139910212208384 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.8957557678222656, loss=3.2431280612945557
I0204 04:07:03.281117 139910203815680 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.9495558738708496, loss=3.039799690246582
I0204 04:07:49.090447 139910212208384 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.835354208946228, loss=3.0509135723114014
I0204 04:08:35.257511 139910203815680 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.8815158605575562, loss=3.0906765460968018
I0204 04:09:21.628782 139910212208384 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.6756815910339355, loss=4.826508522033691
I0204 04:10:07.592651 139910203815680 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.6536297798156738, loss=3.935793399810791
I0204 04:10:53.814480 139910212208384 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.0601048469543457, loss=3.0993149280548096
I0204 04:11:40.169235 139910203815680 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.7123420238494873, loss=3.0932226181030273
I0204 04:12:26.245759 139910212208384 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.6471806764602661, loss=5.192337989807129
I0204 04:13:01.249520 140107197974336 spec.py:321] Evaluating on the training split.
I0204 04:13:12.171555 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 04:13:38.420558 140107197974336 spec.py:349] Evaluating on the test split.
I0204 04:13:40.021455 140107197974336 submission_runner.py:408] Time since start: 46103.61s, 	Step: 93277, 	{'train/accuracy': 0.681933581829071, 'train/loss': 1.4165855646133423, 'validation/accuracy': 0.6343399882316589, 'validation/loss': 1.6316964626312256, 'validation/num_examples': 50000, 'test/accuracy': 0.5100000500679016, 'test/loss': 2.268878698348999, 'test/num_examples': 10000, 'score': 42475.596818208694, 'total_duration': 46103.61071944237, 'accumulated_submission_time': 42475.596818208694, 'accumulated_eval_time': 3617.5498201847076, 'accumulated_logging_time': 5.1927220821380615}
I0204 04:13:40.053963 139910203815680 logging_writer.py:48] [93277] accumulated_eval_time=3617.549820, accumulated_logging_time=5.192722, accumulated_submission_time=42475.596818, global_step=93277, preemption_count=0, score=42475.596818, test/accuracy=0.510000, test/loss=2.268879, test/num_examples=10000, total_duration=46103.610719, train/accuracy=0.681934, train/loss=1.416586, validation/accuracy=0.634340, validation/loss=1.631696, validation/num_examples=50000
I0204 04:13:49.489481 139910212208384 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.7987563610076904, loss=4.689470291137695
I0204 04:14:31.780401 139910203815680 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.7385042905807495, loss=3.612060785293579
I0204 04:15:17.861460 139910212208384 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.7639604806900024, loss=4.176331520080566
I0204 04:16:04.288428 139910203815680 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.4988734722137451, loss=5.105922222137451
I0204 04:16:50.327282 139910212208384 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.7899646759033203, loss=2.962293863296509
I0204 04:17:36.670504 139910203815680 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.8240110874176025, loss=3.088982582092285
I0204 04:18:23.214476 139910212208384 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.7850184440612793, loss=3.179072380065918
I0204 04:19:09.418178 139910203815680 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.7643225193023682, loss=3.0665652751922607
I0204 04:19:55.608442 139910212208384 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.2212588787078857, loss=4.254849433898926
I0204 04:20:40.157269 140107197974336 spec.py:321] Evaluating on the training split.
I0204 04:20:50.928216 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 04:21:16.331356 140107197974336 spec.py:349] Evaluating on the test split.
I0204 04:21:17.938062 140107197974336 submission_runner.py:408] Time since start: 46561.53s, 	Step: 94198, 	{'train/accuracy': 0.6891406178474426, 'train/loss': 1.4098796844482422, 'validation/accuracy': 0.633080005645752, 'validation/loss': 1.6661624908447266, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.2933971881866455, 'test/num_examples': 10000, 'score': 42895.638721227646, 'total_duration': 46561.52732515335, 'accumulated_submission_time': 42895.638721227646, 'accumulated_eval_time': 3655.3306124210358, 'accumulated_logging_time': 5.234953880310059}
I0204 04:21:17.970951 139910203815680 logging_writer.py:48] [94198] accumulated_eval_time=3655.330612, accumulated_logging_time=5.234954, accumulated_submission_time=42895.638721, global_step=94198, preemption_count=0, score=42895.638721, test/accuracy=0.510800, test/loss=2.293397, test/num_examples=10000, total_duration=46561.527325, train/accuracy=0.689141, train/loss=1.409880, validation/accuracy=0.633080, validation/loss=1.666162, validation/num_examples=50000
I0204 04:21:19.153320 139910212208384 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.6371546983718872, loss=5.270230293273926
I0204 04:21:59.674718 139910203815680 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.7446576356887817, loss=3.0823919773101807
I0204 04:22:46.053725 139910212208384 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.6850626468658447, loss=4.5845866203308105
I0204 04:23:32.838951 139910203815680 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.743640661239624, loss=3.034848928451538
I0204 04:24:19.680778 139910212208384 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.819558024406433, loss=3.295287847518921
I0204 04:25:05.953275 139910203815680 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.738213300704956, loss=5.01951265335083
I0204 04:25:52.525782 139910212208384 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.8066850900650024, loss=5.093832969665527
I0204 04:26:38.949142 139910203815680 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.1033108234405518, loss=2.9874370098114014
I0204 04:27:25.401108 139910212208384 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.0233054161071777, loss=5.058890342712402
I0204 04:28:11.859276 139910203815680 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.0622715950012207, loss=3.1224613189697266
I0204 04:28:17.990193 140107197974336 spec.py:321] Evaluating on the training split.
I0204 04:28:28.495604 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 04:28:53.840933 140107197974336 spec.py:349] Evaluating on the test split.
I0204 04:28:55.432203 140107197974336 submission_runner.py:408] Time since start: 47019.02s, 	Step: 95115, 	{'train/accuracy': 0.6744726300239563, 'train/loss': 1.4238649606704712, 'validation/accuracy': 0.6321200132369995, 'validation/loss': 1.6284260749816895, 'validation/num_examples': 50000, 'test/accuracy': 0.5033000111579895, 'test/loss': 2.2706520557403564, 'test/num_examples': 10000, 'score': 43315.5960958004, 'total_duration': 47019.021444797516, 'accumulated_submission_time': 43315.5960958004, 'accumulated_eval_time': 3692.7725965976715, 'accumulated_logging_time': 5.277697801589966}
I0204 04:28:55.464646 139910212208384 logging_writer.py:48] [95115] accumulated_eval_time=3692.772597, accumulated_logging_time=5.277698, accumulated_submission_time=43315.596096, global_step=95115, preemption_count=0, score=43315.596096, test/accuracy=0.503300, test/loss=2.270652, test/num_examples=10000, total_duration=47019.021445, train/accuracy=0.674473, train/loss=1.423865, validation/accuracy=0.632120, validation/loss=1.628426, validation/num_examples=50000
I0204 04:29:29.429358 139910203815680 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.4780460596084595, loss=3.8074920177459717
I0204 04:30:15.480080 139910212208384 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.712309718132019, loss=3.2762269973754883
I0204 04:31:01.781643 139910203815680 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.9964572191238403, loss=3.010587215423584
I0204 04:31:48.105452 139910212208384 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.9171597957611084, loss=3.0034940242767334
I0204 04:32:33.990839 139910203815680 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.9571480751037598, loss=4.965953350067139
I0204 04:33:20.327128 139910212208384 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.8619945049285889, loss=3.054121255874634
I0204 04:34:06.584472 139910203815680 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.8788485527038574, loss=3.0345616340637207
I0204 04:34:52.803699 139910212208384 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.8042505979537964, loss=3.826171636581421
I0204 04:35:38.853147 139910203815680 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.160658359527588, loss=3.1147942543029785
I0204 04:35:55.548196 140107197974336 spec.py:321] Evaluating on the training split.
I0204 04:36:06.420212 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 04:36:33.240835 140107197974336 spec.py:349] Evaluating on the test split.
I0204 04:36:34.838196 140107197974336 submission_runner.py:408] Time since start: 47478.43s, 	Step: 96038, 	{'train/accuracy': 0.6746679544448853, 'train/loss': 1.4650830030441284, 'validation/accuracy': 0.6334800124168396, 'validation/loss': 1.658732295036316, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.325078248977661, 'test/num_examples': 10000, 'score': 43735.618525505066, 'total_duration': 47478.4274597168, 'accumulated_submission_time': 43735.618525505066, 'accumulated_eval_time': 3732.062595129013, 'accumulated_logging_time': 5.319170951843262}
I0204 04:36:34.869946 139910212208384 logging_writer.py:48] [96038] accumulated_eval_time=3732.062595, accumulated_logging_time=5.319171, accumulated_submission_time=43735.618526, global_step=96038, preemption_count=0, score=43735.618526, test/accuracy=0.501800, test/loss=2.325078, test/num_examples=10000, total_duration=47478.427460, train/accuracy=0.674668, train/loss=1.465083, validation/accuracy=0.633480, validation/loss=1.658732, validation/num_examples=50000
I0204 04:36:59.610798 139910203815680 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.648959994316101, loss=4.698269844055176
I0204 04:37:44.118282 139910212208384 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.2175261974334717, loss=3.0761380195617676
I0204 04:38:29.996946 139910203815680 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.828263282775879, loss=3.0583839416503906
I0204 04:39:16.548223 139910212208384 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.7260278463363647, loss=3.0100436210632324
I0204 04:40:02.723792 139910203815680 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.6812481880187988, loss=4.9653401374816895
I0204 04:40:48.861338 139910212208384 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.763496994972229, loss=3.4150285720825195
I0204 04:41:35.291037 139910203815680 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.6709730625152588, loss=3.790783405303955
I0204 04:42:21.111353 139910212208384 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.8116915225982666, loss=3.8213443756103516
I0204 04:43:07.498450 139910203815680 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.1341514587402344, loss=3.1053128242492676
I0204 04:43:34.949169 140107197974336 spec.py:321] Evaluating on the training split.
I0204 04:43:45.698192 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 04:44:10.662913 140107197974336 spec.py:349] Evaluating on the test split.
I0204 04:44:12.258531 140107197974336 submission_runner.py:408] Time since start: 47935.85s, 	Step: 96961, 	{'train/accuracy': 0.6908984184265137, 'train/loss': 1.3704414367675781, 'validation/accuracy': 0.6380000114440918, 'validation/loss': 1.6115221977233887, 'validation/num_examples': 50000, 'test/accuracy': 0.5160000324249268, 'test/loss': 2.247906446456909, 'test/num_examples': 10000, 'score': 44155.63729739189, 'total_duration': 47935.84779524803, 'accumulated_submission_time': 44155.63729739189, 'accumulated_eval_time': 3769.371959209442, 'accumulated_logging_time': 5.359732389450073}
I0204 04:44:12.287863 139910212208384 logging_writer.py:48] [96961] accumulated_eval_time=3769.371959, accumulated_logging_time=5.359732, accumulated_submission_time=44155.637297, global_step=96961, preemption_count=0, score=44155.637297, test/accuracy=0.516000, test/loss=2.247906, test/num_examples=10000, total_duration=47935.847795, train/accuracy=0.690898, train/loss=1.370441, validation/accuracy=0.638000, validation/loss=1.611522, validation/num_examples=50000
I0204 04:44:28.019441 139910203815680 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.6248865127563477, loss=4.981130599975586
I0204 04:45:11.237052 139910212208384 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.9884620904922485, loss=3.335096836090088
I0204 04:45:57.297413 139910203815680 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.7097123861312866, loss=3.2722413539886475
I0204 04:46:43.817741 139910212208384 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.734433889389038, loss=3.281273365020752
I0204 04:47:30.005685 139910203815680 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.0461575984954834, loss=3.1978683471679688
I0204 04:48:16.112014 139910212208384 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.3335673809051514, loss=4.992630958557129
I0204 04:49:02.542395 139910203815680 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.77540922164917, loss=5.217703819274902
I0204 04:49:48.536242 139910212208384 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.8070545196533203, loss=4.020187854766846
I0204 04:50:34.726003 139910203815680 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.9420251846313477, loss=3.5635766983032227
I0204 04:51:12.619395 140107197974336 spec.py:321] Evaluating on the training split.
I0204 04:51:23.381629 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 04:51:46.182495 140107197974336 spec.py:349] Evaluating on the test split.
I0204 04:51:47.779771 140107197974336 submission_runner.py:408] Time since start: 48391.37s, 	Step: 97883, 	{'train/accuracy': 0.6875585913658142, 'train/loss': 1.3868331909179688, 'validation/accuracy': 0.6341399550437927, 'validation/loss': 1.6226903200149536, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.252286434173584, 'test/num_examples': 10000, 'score': 44575.90721774101, 'total_duration': 48391.369034051895, 'accumulated_submission_time': 44575.90721774101, 'accumulated_eval_time': 3804.532338619232, 'accumulated_logging_time': 5.398097276687622}
I0204 04:51:47.810611 139910212208384 logging_writer.py:48] [97883] accumulated_eval_time=3804.532339, accumulated_logging_time=5.398097, accumulated_submission_time=44575.907218, global_step=97883, preemption_count=0, score=44575.907218, test/accuracy=0.509200, test/loss=2.252286, test/num_examples=10000, total_duration=48391.369034, train/accuracy=0.687559, train/loss=1.386833, validation/accuracy=0.634140, validation/loss=1.622690, validation/num_examples=50000
I0204 04:51:54.892020 139910203815680 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.0289604663848877, loss=3.0677685737609863
I0204 04:52:36.339239 139910212208384 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.8962205648422241, loss=3.236858367919922
I0204 04:53:22.500082 139910203815680 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.7198251485824585, loss=4.958470344543457
I0204 04:54:08.639463 139910212208384 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.973372459411621, loss=3.037898540496826
I0204 04:54:54.878102 139910203815680 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.306663990020752, loss=3.0142855644226074
I0204 04:55:41.041087 139910212208384 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.2623939514160156, loss=3.047102689743042
I0204 04:56:27.553788 139910203815680 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.8561639785766602, loss=3.300708532333374
I0204 04:57:13.859408 139910212208384 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.8600294589996338, loss=4.601558208465576
I0204 04:57:59.952535 139910203815680 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.9286201000213623, loss=3.0223357677459717
I0204 04:58:45.900833 139910212208384 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.757102608680725, loss=3.399229049682617
I0204 04:58:47.949524 140107197974336 spec.py:321] Evaluating on the training split.
I0204 04:58:58.432461 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 04:59:24.502355 140107197974336 spec.py:349] Evaluating on the test split.
I0204 04:59:26.103063 140107197974336 submission_runner.py:408] Time since start: 48849.69s, 	Step: 98806, 	{'train/accuracy': 0.6914257407188416, 'train/loss': 1.358039379119873, 'validation/accuracy': 0.6440399885177612, 'validation/loss': 1.5768593549728394, 'validation/num_examples': 50000, 'test/accuracy': 0.5145000219345093, 'test/loss': 2.221693754196167, 'test/num_examples': 10000, 'score': 44995.98552107811, 'total_duration': 48849.69229388237, 'accumulated_submission_time': 44995.98552107811, 'accumulated_eval_time': 3842.685833454132, 'accumulated_logging_time': 5.437853097915649}
I0204 04:59:26.138807 139910203815680 logging_writer.py:48] [98806] accumulated_eval_time=3842.685833, accumulated_logging_time=5.437853, accumulated_submission_time=44995.985521, global_step=98806, preemption_count=0, score=44995.985521, test/accuracy=0.514500, test/loss=2.221694, test/num_examples=10000, total_duration=48849.692294, train/accuracy=0.691426, train/loss=1.358039, validation/accuracy=0.644040, validation/loss=1.576859, validation/num_examples=50000
I0204 05:00:04.540999 139910212208384 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.01965594291687, loss=3.0805273056030273
I0204 05:00:50.777044 139910203815680 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.9674274921417236, loss=3.0326600074768066
I0204 05:01:37.530523 139910212208384 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.600258231163025, loss=5.18298864364624
I0204 05:02:23.858348 139910203815680 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.082213878631592, loss=3.0360593795776367
I0204 05:03:10.090585 139910212208384 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.0093228816986084, loss=3.0251450538635254
I0204 05:03:56.511068 139910203815680 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.8046809434890747, loss=3.3802318572998047
I0204 05:04:42.897235 139910212208384 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.9790409803390503, loss=2.991347312927246
I0204 05:05:29.425173 139910203815680 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.8830037117004395, loss=3.0143136978149414
I0204 05:06:15.779179 139910212208384 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.8979392051696777, loss=3.0625741481781006
I0204 05:06:26.576753 140107197974336 spec.py:321] Evaluating on the training split.
I0204 05:06:37.442869 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 05:07:03.748450 140107197974336 spec.py:349] Evaluating on the test split.
I0204 05:07:05.346426 140107197974336 submission_runner.py:408] Time since start: 49308.94s, 	Step: 99725, 	{'train/accuracy': 0.6920703053474426, 'train/loss': 1.3738312721252441, 'validation/accuracy': 0.6429199576377869, 'validation/loss': 1.6061257123947144, 'validation/num_examples': 50000, 'test/accuracy': 0.5164000391960144, 'test/loss': 2.2455978393554688, 'test/num_examples': 10000, 'score': 45416.36224722862, 'total_duration': 49308.9356777668, 'accumulated_submission_time': 45416.36224722862, 'accumulated_eval_time': 3881.4554891586304, 'accumulated_logging_time': 5.483229875564575}
I0204 05:07:05.382306 139910203815680 logging_writer.py:48] [99725] accumulated_eval_time=3881.455489, accumulated_logging_time=5.483230, accumulated_submission_time=45416.362247, global_step=99725, preemption_count=0, score=45416.362247, test/accuracy=0.516400, test/loss=2.245598, test/num_examples=10000, total_duration=49308.935678, train/accuracy=0.692070, train/loss=1.373831, validation/accuracy=0.642920, validation/loss=1.606126, validation/num_examples=50000
I0204 05:07:35.254384 139910212208384 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.7787615060806274, loss=2.962066888809204
I0204 05:08:21.184842 139910203815680 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.1466176509857178, loss=2.9425547122955322
I0204 05:09:07.510456 139910212208384 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.7124159336090088, loss=4.498050689697266
I0204 05:09:53.936640 139910203815680 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.261770248413086, loss=3.0295214653015137
I0204 05:10:40.249469 139910212208384 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.7876516580581665, loss=2.923217535018921
I0204 05:11:26.863192 139910203815680 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.0508813858032227, loss=3.106478452682495
I0204 05:12:13.155310 139910212208384 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.2492876052856445, loss=3.0946097373962402
I0204 05:12:59.439508 139910203815680 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.8314545154571533, loss=3.722724437713623
I0204 05:13:45.846623 139910212208384 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.1402342319488525, loss=2.9912502765655518
I0204 05:14:05.600561 140107197974336 spec.py:321] Evaluating on the training split.
I0204 05:14:16.306979 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 05:14:42.517267 140107197974336 spec.py:349] Evaluating on the test split.
I0204 05:14:44.120323 140107197974336 submission_runner.py:408] Time since start: 49767.71s, 	Step: 100644, 	{'train/accuracy': 0.7134960889816284, 'train/loss': 1.2532609701156616, 'validation/accuracy': 0.6430999636650085, 'validation/loss': 1.5673333406448364, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.2153022289276123, 'test/num_examples': 10000, 'score': 45836.51930832863, 'total_duration': 49767.70958662033, 'accumulated_submission_time': 45836.51930832863, 'accumulated_eval_time': 3919.9752383232117, 'accumulated_logging_time': 5.5283708572387695}
I0204 05:14:44.153190 139910203815680 logging_writer.py:48] [100644] accumulated_eval_time=3919.975238, accumulated_logging_time=5.528371, accumulated_submission_time=45836.519308, global_step=100644, preemption_count=0, score=45836.519308, test/accuracy=0.522600, test/loss=2.215302, test/num_examples=10000, total_duration=49767.709587, train/accuracy=0.713496, train/loss=1.253261, validation/accuracy=0.643100, validation/loss=1.567333, validation/num_examples=50000
I0204 05:15:06.546549 139910212208384 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.7660155296325684, loss=3.1178019046783447
I0204 05:15:50.520312 139910203815680 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.7031137943267822, loss=5.112918853759766
I0204 05:16:36.779860 139910212208384 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.7530927658081055, loss=3.0942320823669434
I0204 05:17:23.037738 139910203815680 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.9502339363098145, loss=2.926943302154541
I0204 05:18:08.999230 139910212208384 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.662346363067627, loss=3.520200252532959
I0204 05:18:54.948957 139910203815680 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.7793755531311035, loss=3.2254202365875244
I0204 05:19:41.149056 139910212208384 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.542782187461853, loss=4.399805545806885
I0204 05:20:27.484518 139910203815680 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.034882068634033, loss=3.001864433288574
I0204 05:21:13.653666 139910212208384 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.0639448165893555, loss=3.06923246383667
I0204 05:21:44.557454 140107197974336 spec.py:321] Evaluating on the training split.
I0204 05:21:55.119011 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 05:22:21.018805 140107197974336 spec.py:349] Evaluating on the test split.
I0204 05:22:22.613224 140107197974336 submission_runner.py:408] Time since start: 50226.20s, 	Step: 101569, 	{'train/accuracy': 0.6882616877555847, 'train/loss': 1.3756904602050781, 'validation/accuracy': 0.6430999636650085, 'validation/loss': 1.592112421989441, 'validation/num_examples': 50000, 'test/accuracy': 0.5160000324249268, 'test/loss': 2.235506534576416, 'test/num_examples': 10000, 'score': 46256.86202788353, 'total_duration': 50226.20248699188, 'accumulated_submission_time': 46256.86202788353, 'accumulated_eval_time': 3958.030996799469, 'accumulated_logging_time': 5.570519685745239}
I0204 05:22:22.647798 139910203815680 logging_writer.py:48] [101569] accumulated_eval_time=3958.030997, accumulated_logging_time=5.570520, accumulated_submission_time=46256.862028, global_step=101569, preemption_count=0, score=46256.862028, test/accuracy=0.516000, test/loss=2.235507, test/num_examples=10000, total_duration=50226.202487, train/accuracy=0.688262, train/loss=1.375690, validation/accuracy=0.643100, validation/loss=1.592112, validation/num_examples=50000
I0204 05:22:35.216082 139910212208384 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.1511666774749756, loss=3.0589704513549805
I0204 05:23:17.494010 139910203815680 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.1121346950531006, loss=2.9643349647521973
I0204 05:24:03.616922 139910212208384 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.871229648590088, loss=3.1588780879974365
I0204 05:24:50.143918 139910203815680 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.8445382118225098, loss=3.093830108642578
I0204 05:25:36.306336 139910212208384 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.1476216316223145, loss=2.9696590900421143
I0204 05:26:22.858052 139910203815680 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.7422641515731812, loss=4.40562629699707
I0204 05:27:09.114848 139910212208384 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.0396511554718018, loss=3.605041742324829
I0204 05:27:55.213179 139910203815680 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.1320791244506836, loss=2.9972333908081055
I0204 05:28:41.309677 139910212208384 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.6307379007339478, loss=4.355355262756348
I0204 05:29:22.670803 140107197974336 spec.py:321] Evaluating on the training split.
I0204 05:29:33.335102 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 05:29:58.010893 140107197974336 spec.py:349] Evaluating on the test split.
I0204 05:29:59.604542 140107197974336 submission_runner.py:408] Time since start: 50683.19s, 	Step: 102491, 	{'train/accuracy': 0.695605456829071, 'train/loss': 1.3783432245254517, 'validation/accuracy': 0.6459800004959106, 'validation/loss': 1.5988671779632568, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.2385599613189697, 'test/num_examples': 10000, 'score': 46676.82009387016, 'total_duration': 50683.19379091263, 'accumulated_submission_time': 46676.82009387016, 'accumulated_eval_time': 3994.964736223221, 'accumulated_logging_time': 5.617609024047852}
I0204 05:29:59.639141 139910203815680 logging_writer.py:48] [102491] accumulated_eval_time=3994.964736, accumulated_logging_time=5.617609, accumulated_submission_time=46676.820094, global_step=102491, preemption_count=0, score=46676.820094, test/accuracy=0.518300, test/loss=2.238560, test/num_examples=10000, total_duration=50683.193791, train/accuracy=0.695605, train/loss=1.378343, validation/accuracy=0.645980, validation/loss=1.598867, validation/num_examples=50000
I0204 05:30:03.574243 139910212208384 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.9766165018081665, loss=3.1281018257141113
I0204 05:30:44.781513 139910203815680 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.0724081993103027, loss=3.022692918777466
I0204 05:31:31.209797 139910212208384 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.3041718006134033, loss=3.725985050201416
I0204 05:32:17.456558 139910203815680 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.3055412769317627, loss=2.991354465484619
I0204 05:33:03.892610 139910212208384 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.967882513999939, loss=3.065357208251953
I0204 05:33:49.776168 139910203815680 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.942292332649231, loss=2.9416301250457764
I0204 05:34:36.282025 139910212208384 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.117894411087036, loss=3.0156409740448
I0204 05:35:22.413279 139910203815680 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.6104111671447754, loss=4.207927227020264
I0204 05:36:08.557862 139910212208384 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.7199255228042603, loss=4.5265913009643555
I0204 05:36:54.823410 139910203815680 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.371131658554077, loss=4.299199104309082
I0204 05:37:00.060526 140107197974336 spec.py:321] Evaluating on the training split.
I0204 05:37:10.902484 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 05:37:38.731035 140107197974336 spec.py:349] Evaluating on the test split.
I0204 05:37:40.318762 140107197974336 submission_runner.py:408] Time since start: 51143.91s, 	Step: 103413, 	{'train/accuracy': 0.7080858945846558, 'train/loss': 1.311424970626831, 'validation/accuracy': 0.6429799795150757, 'validation/loss': 1.5880955457687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5222000479698181, 'test/loss': 2.2103865146636963, 'test/num_examples': 10000, 'score': 47097.179240942, 'total_duration': 51143.90802812576, 'accumulated_submission_time': 47097.179240942, 'accumulated_eval_time': 4035.222962141037, 'accumulated_logging_time': 5.66266655921936}
I0204 05:37:40.349542 139910212208384 logging_writer.py:48] [103413] accumulated_eval_time=4035.222962, accumulated_logging_time=5.662667, accumulated_submission_time=47097.179241, global_step=103413, preemption_count=0, score=47097.179241, test/accuracy=0.522200, test/loss=2.210387, test/num_examples=10000, total_duration=51143.908028, train/accuracy=0.708086, train/loss=1.311425, validation/accuracy=0.642980, validation/loss=1.588096, validation/num_examples=50000
I0204 05:38:15.632084 139910203815680 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.086475372314453, loss=4.415757179260254
I0204 05:39:01.647327 139910212208384 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.020148515701294, loss=3.0483264923095703
I0204 05:39:48.110133 139910203815680 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.0384881496429443, loss=3.3363730907440186
I0204 05:40:34.583224 139910212208384 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.8877490758895874, loss=3.0056233406066895
I0204 05:41:21.032002 139910203815680 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.8538318872451782, loss=3.6296463012695312
I0204 05:42:07.205477 139910212208384 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.9480453729629517, loss=2.980329990386963
I0204 05:42:53.582786 139910203815680 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.9380840063095093, loss=3.2510805130004883
I0204 05:43:39.704281 139910212208384 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.0383591651916504, loss=2.8883962631225586
I0204 05:44:26.044064 139910203815680 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.804702877998352, loss=5.0711774826049805
I0204 05:44:40.605388 140107197974336 spec.py:321] Evaluating on the training split.
I0204 05:44:51.579668 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 05:45:19.631681 140107197974336 spec.py:349] Evaluating on the test split.
I0204 05:45:21.231402 140107197974336 submission_runner.py:408] Time since start: 51604.82s, 	Step: 104333, 	{'train/accuracy': 0.6900585889816284, 'train/loss': 1.4127657413482666, 'validation/accuracy': 0.6438199877738953, 'validation/loss': 1.6250956058502197, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.272473096847534, 'test/num_examples': 10000, 'score': 47517.37286019325, 'total_duration': 51604.820660829544, 'accumulated_submission_time': 47517.37286019325, 'accumulated_eval_time': 4075.8489694595337, 'accumulated_logging_time': 5.703973770141602}
I0204 05:45:21.261527 139910212208384 logging_writer.py:48] [104333] accumulated_eval_time=4075.848969, accumulated_logging_time=5.703974, accumulated_submission_time=47517.372860, global_step=104333, preemption_count=0, score=47517.372860, test/accuracy=0.520800, test/loss=2.272473, test/num_examples=10000, total_duration=51604.820661, train/accuracy=0.690059, train/loss=1.412766, validation/accuracy=0.643820, validation/loss=1.625096, validation/num_examples=50000
I0204 05:45:47.981009 139910203815680 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.0545241832733154, loss=3.067671537399292
I0204 05:46:33.016890 139910212208384 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.9679152965545654, loss=3.01454496383667
I0204 05:47:19.344633 139910203815680 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.0640671253204346, loss=2.973660707473755
I0204 05:48:05.809551 139910212208384 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.037957191467285, loss=2.964008331298828
I0204 05:48:51.765166 139910203815680 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.959888219833374, loss=3.1277358531951904
I0204 05:49:38.068667 139910212208384 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.159785032272339, loss=2.927058696746826
I0204 05:50:24.367473 139910203815680 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.564950942993164, loss=4.114322185516357
I0204 05:51:10.991911 139910212208384 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.9877594709396362, loss=3.0474061965942383
I0204 05:51:57.156044 139910203815680 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.1579298973083496, loss=3.037386417388916
I0204 05:52:21.800979 140107197974336 spec.py:321] Evaluating on the training split.
I0204 05:52:32.414565 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 05:52:58.571417 140107197974336 spec.py:349] Evaluating on the test split.
I0204 05:53:00.169323 140107197974336 submission_runner.py:408] Time since start: 52063.76s, 	Step: 105255, 	{'train/accuracy': 0.7038280963897705, 'train/loss': 1.3086973428726196, 'validation/accuracy': 0.6522600054740906, 'validation/loss': 1.5347038507461548, 'validation/num_examples': 50000, 'test/accuracy': 0.5284000039100647, 'test/loss': 2.170280933380127, 'test/num_examples': 10000, 'score': 47937.850818157196, 'total_duration': 52063.75856423378, 'accumulated_submission_time': 47937.850818157196, 'accumulated_eval_time': 4114.217289924622, 'accumulated_logging_time': 5.7432332038879395}
I0204 05:53:00.207728 139910212208384 logging_writer.py:48] [105255] accumulated_eval_time=4114.217290, accumulated_logging_time=5.743233, accumulated_submission_time=47937.850818, global_step=105255, preemption_count=0, score=47937.850818, test/accuracy=0.528400, test/loss=2.170281, test/num_examples=10000, total_duration=52063.758564, train/accuracy=0.703828, train/loss=1.308697, validation/accuracy=0.652260, validation/loss=1.534704, validation/num_examples=50000
I0204 05:53:18.289005 139910203815680 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.362429618835449, loss=2.9920835494995117
I0204 05:54:01.458088 139910212208384 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.0633139610290527, loss=2.9571189880371094
I0204 05:54:47.674847 139910203815680 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.9356266260147095, loss=3.1634323596954346
I0204 05:55:34.103962 139910212208384 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.9277244806289673, loss=2.9166581630706787
I0204 05:56:20.442017 139910203815680 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.1117422580718994, loss=2.9599413871765137
I0204 05:57:06.720206 139910212208384 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.147881507873535, loss=2.895244598388672
I0204 05:57:52.865468 139910203815680 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.0798683166503906, loss=2.9676530361175537
I0204 05:58:38.997248 139910212208384 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.857946515083313, loss=3.700150966644287
I0204 05:59:25.294797 139910203815680 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.0940778255462646, loss=3.078789234161377
I0204 06:00:00.504570 140107197974336 spec.py:321] Evaluating on the training split.
I0204 06:00:11.490717 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 06:00:38.311736 140107197974336 spec.py:349] Evaluating on the test split.
I0204 06:00:39.906626 140107197974336 submission_runner.py:408] Time since start: 52523.50s, 	Step: 106178, 	{'train/accuracy': 0.695117175579071, 'train/loss': 1.3671385049819946, 'validation/accuracy': 0.638480007648468, 'validation/loss': 1.6232553720474243, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.258666753768921, 'test/num_examples': 10000, 'score': 48358.08609056473, 'total_duration': 52523.495888233185, 'accumulated_submission_time': 48358.08609056473, 'accumulated_eval_time': 4153.619336605072, 'accumulated_logging_time': 5.791689872741699}
I0204 06:00:39.937549 139910212208384 logging_writer.py:48] [106178] accumulated_eval_time=4153.619337, accumulated_logging_time=5.791690, accumulated_submission_time=48358.086091, global_step=106178, preemption_count=0, score=48358.086091, test/accuracy=0.517900, test/loss=2.258667, test/num_examples=10000, total_duration=52523.495888, train/accuracy=0.695117, train/loss=1.367139, validation/accuracy=0.638480, validation/loss=1.623255, validation/num_examples=50000
I0204 06:00:48.978590 139910203815680 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.279299736022949, loss=3.033287763595581
I0204 06:01:30.847426 139910212208384 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.749287486076355, loss=4.957713603973389
I0204 06:02:17.240223 139910203815680 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.0610427856445312, loss=2.9166388511657715
I0204 06:03:03.691895 139910212208384 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.428694486618042, loss=2.949756622314453
I0204 06:03:50.287849 139910203815680 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.7226171493530273, loss=3.0285441875457764
I0204 06:04:36.502656 139910212208384 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.0263583660125732, loss=2.841885805130005
I0204 06:05:22.753525 139910203815680 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.5814168453216553, loss=4.8621649742126465
I0204 06:06:09.038204 139910212208384 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.3603286743164062, loss=3.1523380279541016
I0204 06:06:54.945762 139910203815680 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.9507473707199097, loss=3.2242612838745117
I0204 06:07:39.942468 140107197974336 spec.py:321] Evaluating on the training split.
I0204 06:07:50.936354 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 06:08:18.285491 140107197974336 spec.py:349] Evaluating on the test split.
I0204 06:08:19.886157 140107197974336 submission_runner.py:408] Time since start: 52983.48s, 	Step: 107099, 	{'train/accuracy': 0.7011132836341858, 'train/loss': 1.3185851573944092, 'validation/accuracy': 0.6495000123977661, 'validation/loss': 1.5532654523849487, 'validation/num_examples': 50000, 'test/accuracy': 0.5246000289916992, 'test/loss': 2.1825060844421387, 'test/num_examples': 10000, 'score': 48778.02906394005, 'total_duration': 52983.47541809082, 'accumulated_submission_time': 48778.02906394005, 'accumulated_eval_time': 4193.563049793243, 'accumulated_logging_time': 5.832031965255737}
I0204 06:08:19.917524 139910212208384 logging_writer.py:48] [107099] accumulated_eval_time=4193.563050, accumulated_logging_time=5.832032, accumulated_submission_time=48778.029064, global_step=107099, preemption_count=0, score=48778.029064, test/accuracy=0.524600, test/loss=2.182506, test/num_examples=10000, total_duration=52983.475418, train/accuracy=0.701113, train/loss=1.318585, validation/accuracy=0.649500, validation/loss=1.553265, validation/num_examples=50000
I0204 06:08:20.706195 139910203815680 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.8236767053604126, loss=4.660984516143799
I0204 06:09:01.295661 139910212208384 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.790158987045288, loss=3.3863906860351562
I0204 06:09:47.135374 139910203815680 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.222205400466919, loss=3.2569055557250977
I0204 06:10:33.554401 139910212208384 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.1016769409179688, loss=2.985215902328491
I0204 06:11:19.948101 139910203815680 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.8502459526062012, loss=4.515248775482178
I0204 06:12:06.141344 139910212208384 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.06758451461792, loss=2.88444447517395
I0204 06:12:52.347789 139910203815680 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.986557126045227, loss=2.8046908378601074
I0204 06:13:38.504561 139910212208384 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.9554303884506226, loss=3.1520819664001465
I0204 06:14:24.559661 139910203815680 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.9176902770996094, loss=2.921788454055786
I0204 06:15:10.822263 139910212208384 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.9311720132827759, loss=5.087704658508301
I0204 06:15:20.112155 140107197974336 spec.py:321] Evaluating on the training split.
I0204 06:15:30.668742 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 06:15:56.148945 140107197974336 spec.py:349] Evaluating on the test split.
I0204 06:15:57.744791 140107197974336 submission_runner.py:408] Time since start: 53441.33s, 	Step: 108022, 	{'train/accuracy': 0.7089062333106995, 'train/loss': 1.2863578796386719, 'validation/accuracy': 0.6575999855995178, 'validation/loss': 1.5152268409729004, 'validation/num_examples': 50000, 'test/accuracy': 0.5370000004768372, 'test/loss': 2.137031316757202, 'test/num_examples': 10000, 'score': 49198.16246008873, 'total_duration': 53441.33406162262, 'accumulated_submission_time': 49198.16246008873, 'accumulated_eval_time': 4231.1956782341, 'accumulated_logging_time': 5.872812747955322}
I0204 06:15:57.776574 139910203815680 logging_writer.py:48] [108022] accumulated_eval_time=4231.195678, accumulated_logging_time=5.872813, accumulated_submission_time=49198.162460, global_step=108022, preemption_count=0, score=49198.162460, test/accuracy=0.537000, test/loss=2.137031, test/num_examples=10000, total_duration=53441.334062, train/accuracy=0.708906, train/loss=1.286358, validation/accuracy=0.657600, validation/loss=1.515227, validation/num_examples=50000
I0204 06:16:28.848057 139910212208384 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.4650015830993652, loss=3.002091884613037
I0204 06:17:14.311013 139910203815680 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.9840211868286133, loss=4.3073225021362305
I0204 06:18:00.363712 139910212208384 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.9682561159133911, loss=2.984622001647949
I0204 06:18:46.564676 139910203815680 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.9241241216659546, loss=4.870298385620117
I0204 06:19:32.616952 139910212208384 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.8353368043899536, loss=3.618184804916382
I0204 06:20:18.539827 139910203815680 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.049311637878418, loss=3.0519185066223145
I0204 06:21:04.670015 139910212208384 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.447451114654541, loss=2.9569005966186523
I0204 06:21:50.584195 139910203815680 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.8207389116287231, loss=4.072873592376709
I0204 06:22:36.860515 139910212208384 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.1465353965759277, loss=2.956571578979492
I0204 06:22:58.048079 140107197974336 spec.py:321] Evaluating on the training split.
I0204 06:23:09.126935 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 06:23:35.869567 140107197974336 spec.py:349] Evaluating on the test split.
I0204 06:23:37.478965 140107197974336 submission_runner.py:408] Time since start: 53901.07s, 	Step: 108948, 	{'train/accuracy': 0.7115429639816284, 'train/loss': 1.2524975538253784, 'validation/accuracy': 0.6578199863433838, 'validation/loss': 1.5066378116607666, 'validation/num_examples': 50000, 'test/accuracy': 0.5306000113487244, 'test/loss': 2.1434850692749023, 'test/num_examples': 10000, 'score': 49618.37132978439, 'total_duration': 53901.068217754364, 'accumulated_submission_time': 49618.37132978439, 'accumulated_eval_time': 4270.6265552043915, 'accumulated_logging_time': 5.915415287017822}
I0204 06:23:37.516816 139910203815680 logging_writer.py:48] [108948] accumulated_eval_time=4270.626555, accumulated_logging_time=5.915415, accumulated_submission_time=49618.371330, global_step=108948, preemption_count=0, score=49618.371330, test/accuracy=0.530600, test/loss=2.143485, test/num_examples=10000, total_duration=53901.068218, train/accuracy=0.711543, train/loss=1.252498, validation/accuracy=0.657820, validation/loss=1.506638, validation/num_examples=50000
I0204 06:23:58.342424 139910212208384 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.0757651329040527, loss=2.9941952228546143
I0204 06:24:42.181406 139910212208384 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.0238051414489746, loss=5.1098313331604
I0204 06:25:28.448198 139910203815680 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.787019968032837, loss=4.219657897949219
I0204 06:26:14.573865 139910212208384 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.2280642986297607, loss=3.0063743591308594
I0204 06:27:00.435935 139910203815680 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.127922296524048, loss=2.8665499687194824
I0204 06:27:46.711210 139910212208384 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.1796154975891113, loss=2.9070701599121094
I0204 06:28:32.972836 139910203815680 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.8359830379486084, loss=3.983945846557617
I0204 06:29:19.216836 139910212208384 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.230461359024048, loss=3.1705212593078613
I0204 06:30:05.616205 139910203815680 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.7533961534500122, loss=4.610226631164551
I0204 06:30:37.487551 140107197974336 spec.py:321] Evaluating on the training split.
I0204 06:30:48.436641 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 06:31:15.418364 140107197974336 spec.py:349] Evaluating on the test split.
I0204 06:31:17.011284 140107197974336 submission_runner.py:408] Time since start: 54360.60s, 	Step: 109871, 	{'train/accuracy': 0.7302343845367432, 'train/loss': 1.2425791025161743, 'validation/accuracy': 0.6523799896240234, 'validation/loss': 1.5649343729019165, 'validation/num_examples': 50000, 'test/accuracy': 0.5297999978065491, 'test/loss': 2.186164140701294, 'test/num_examples': 10000, 'score': 50038.27680063248, 'total_duration': 54360.60054755211, 'accumulated_submission_time': 50038.27680063248, 'accumulated_eval_time': 4310.15029501915, 'accumulated_logging_time': 5.9627158641815186}
I0204 06:31:17.045563 139910212208384 logging_writer.py:48] [109871] accumulated_eval_time=4310.150295, accumulated_logging_time=5.962716, accumulated_submission_time=50038.276801, global_step=109871, preemption_count=0, score=50038.276801, test/accuracy=0.529800, test/loss=2.186164, test/num_examples=10000, total_duration=54360.600548, train/accuracy=0.730234, train/loss=1.242579, validation/accuracy=0.652380, validation/loss=1.564934, validation/num_examples=50000
I0204 06:31:29.052405 139910203815680 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.1539039611816406, loss=2.886909246444702
I0204 06:32:11.471587 139910212208384 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.4002859592437744, loss=2.9851231575012207
I0204 06:32:57.656783 139910203815680 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.0788865089416504, loss=2.9941461086273193
I0204 06:33:43.973434 139910212208384 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.9490922689437866, loss=4.760348796844482
I0204 06:34:30.512256 139910203815680 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.8517135381698608, loss=3.717297315597534
I0204 06:35:16.797364 139910212208384 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.9820640087127686, loss=4.586409091949463
I0204 06:36:03.662322 139910203815680 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.243025779724121, loss=2.968965768814087
I0204 06:36:49.980733 139910212208384 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.8221923112869263, loss=4.557038307189941
I0204 06:37:36.298058 139910203815680 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.7604498863220215, loss=3.4561429023742676
I0204 06:38:17.301496 140107197974336 spec.py:321] Evaluating on the training split.
I0204 06:38:28.215571 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 06:38:55.212948 140107197974336 spec.py:349] Evaluating on the test split.
I0204 06:38:56.817712 140107197974336 submission_runner.py:408] Time since start: 54820.41s, 	Step: 110790, 	{'train/accuracy': 0.7073437571525574, 'train/loss': 1.2899636030197144, 'validation/accuracy': 0.6584799885749817, 'validation/loss': 1.5123716592788696, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.155395746231079, 'test/num_examples': 10000, 'score': 50458.470808029175, 'total_duration': 54820.40697169304, 'accumulated_submission_time': 50458.470808029175, 'accumulated_eval_time': 4349.666513442993, 'accumulated_logging_time': 6.006732702255249}
I0204 06:38:56.853645 139910212208384 logging_writer.py:48] [110790] accumulated_eval_time=4349.666513, accumulated_logging_time=6.006733, accumulated_submission_time=50458.470808, global_step=110790, preemption_count=0, score=50458.470808, test/accuracy=0.532900, test/loss=2.155396, test/num_examples=10000, total_duration=54820.406972, train/accuracy=0.707344, train/loss=1.289964, validation/accuracy=0.658480, validation/loss=1.512372, validation/num_examples=50000
I0204 06:39:01.167309 139910203815680 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.0363528728485107, loss=4.627377986907959
I0204 06:39:42.226929 139910212208384 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.8533284664154053, loss=4.371953964233398
I0204 06:40:28.341305 139910203815680 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.0853095054626465, loss=2.9580271244049072
I0204 06:41:14.555567 139910212208384 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.0559909343719482, loss=3.593912124633789
I0204 06:42:00.797542 139910203815680 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.0655887126922607, loss=3.5610570907592773
I0204 06:42:46.879614 139910212208384 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.8497244119644165, loss=3.6864113807678223
I0204 06:43:33.241297 139910203815680 logging_writer.py:48] [111400] global_step=111400, grad_norm=3.687249183654785, loss=5.0200653076171875
I0204 06:44:19.416694 139910212208384 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.1893091201782227, loss=2.9504685401916504
I0204 06:45:05.799293 139910203815680 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.9704408645629883, loss=3.5930306911468506
I0204 06:45:51.975343 139910212208384 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.0895533561706543, loss=2.980297088623047
I0204 06:45:57.163106 140107197974336 spec.py:321] Evaluating on the training split.
I0204 06:46:07.890434 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 06:46:32.622015 140107197974336 spec.py:349] Evaluating on the test split.
I0204 06:46:34.228533 140107197974336 submission_runner.py:408] Time since start: 55277.82s, 	Step: 111713, 	{'train/accuracy': 0.7128515243530273, 'train/loss': 1.2536503076553345, 'validation/accuracy': 0.6627599596977234, 'validation/loss': 1.4864120483398438, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.1200437545776367, 'test/num_examples': 10000, 'score': 50878.71767401695, 'total_duration': 55277.81778669357, 'accumulated_submission_time': 50878.71767401695, 'accumulated_eval_time': 4386.731926679611, 'accumulated_logging_time': 6.0523576736450195}
I0204 06:46:34.260025 139910203815680 logging_writer.py:48] [111713] accumulated_eval_time=4386.731927, accumulated_logging_time=6.052358, accumulated_submission_time=50878.717674, global_step=111713, preemption_count=0, score=50878.717674, test/accuracy=0.536400, test/loss=2.120044, test/num_examples=10000, total_duration=55277.817787, train/accuracy=0.712852, train/loss=1.253650, validation/accuracy=0.662760, validation/loss=1.486412, validation/num_examples=50000
I0204 06:47:09.382109 139910212208384 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.9904764890670776, loss=4.309345245361328
I0204 06:47:54.821306 139910203815680 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.042900800704956, loss=2.9156947135925293
I0204 06:48:41.197923 139910212208384 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.306968927383423, loss=2.848903179168701
I0204 06:49:27.707993 139910203815680 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.2360122203826904, loss=2.915736436843872
I0204 06:50:13.683625 139910212208384 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.2654311656951904, loss=5.05386209487915
I0204 06:50:59.793184 139910203815680 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.0354130268096924, loss=3.7921693325042725
I0204 06:51:46.081902 139910212208384 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.0936615467071533, loss=3.0308234691619873
I0204 06:52:32.333788 139910203815680 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.3596177101135254, loss=2.9021787643432617
I0204 06:53:19.077772 139910212208384 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.8965182304382324, loss=3.3997104167938232
I0204 06:53:34.536163 140107197974336 spec.py:321] Evaluating on the training split.
I0204 06:53:45.239809 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 06:54:10.952035 140107197974336 spec.py:349] Evaluating on the test split.
I0204 06:54:12.543845 140107197974336 submission_runner.py:408] Time since start: 55736.13s, 	Step: 112635, 	{'train/accuracy': 0.7286913990974426, 'train/loss': 1.2007211446762085, 'validation/accuracy': 0.6609399914741516, 'validation/loss': 1.5029429197311401, 'validation/num_examples': 50000, 'test/accuracy': 0.5369000434875488, 'test/loss': 2.1409804821014404, 'test/num_examples': 10000, 'score': 51298.70504426956, 'total_duration': 55736.1331076622, 'accumulated_submission_time': 51298.70504426956, 'accumulated_eval_time': 4424.739602088928, 'accumulated_logging_time': 6.320374011993408}
I0204 06:54:12.575185 139910203815680 logging_writer.py:48] [112635] accumulated_eval_time=4424.739602, accumulated_logging_time=6.320374, accumulated_submission_time=51298.705044, global_step=112635, preemption_count=0, score=51298.705044, test/accuracy=0.536900, test/loss=2.140980, test/num_examples=10000, total_duration=55736.133108, train/accuracy=0.728691, train/loss=1.200721, validation/accuracy=0.660940, validation/loss=1.502943, validation/num_examples=50000
I0204 06:54:38.494668 139910212208384 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.281451463699341, loss=2.9026174545288086
I0204 06:55:23.268955 139910203815680 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.203462600708008, loss=4.761196613311768
I0204 06:56:09.796197 139910212208384 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.1343297958374023, loss=2.8277792930603027
I0204 06:56:56.250074 139910203815680 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.198082685470581, loss=3.009848117828369
I0204 06:57:42.310983 139910212208384 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.677445411682129, loss=2.865593194961548
I0204 06:58:28.586523 139910203815680 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.1889240741729736, loss=2.9902281761169434
I0204 06:59:14.873094 139910212208384 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.070464849472046, loss=2.911691427230835
I0204 07:00:00.961416 139910203815680 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.2793962955474854, loss=2.7668814659118652
I0204 07:00:47.341575 139910212208384 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.0778918266296387, loss=2.870103597640991
I0204 07:01:12.832094 140107197974336 spec.py:321] Evaluating on the training split.
I0204 07:01:23.910907 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 07:01:46.283261 140107197974336 spec.py:349] Evaluating on the test split.
I0204 07:01:47.883227 140107197974336 submission_runner.py:408] Time since start: 56191.47s, 	Step: 113557, 	{'train/accuracy': 0.7140820026397705, 'train/loss': 1.3028429746627808, 'validation/accuracy': 0.6602199673652649, 'validation/loss': 1.5292065143585205, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.165210485458374, 'test/num_examples': 10000, 'score': 51718.90090465546, 'total_duration': 56191.47248148918, 'accumulated_submission_time': 51718.90090465546, 'accumulated_eval_time': 4459.790710687637, 'accumulated_logging_time': 6.36035680770874}
I0204 07:01:47.915660 139910203815680 logging_writer.py:48] [113557] accumulated_eval_time=4459.790711, accumulated_logging_time=6.360357, accumulated_submission_time=51718.900905, global_step=113557, preemption_count=0, score=51718.900905, test/accuracy=0.532800, test/loss=2.165210, test/num_examples=10000, total_duration=56191.472481, train/accuracy=0.714082, train/loss=1.302843, validation/accuracy=0.660220, validation/loss=1.529207, validation/num_examples=50000
I0204 07:02:05.210888 139910212208384 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.0600008964538574, loss=2.937471389770508
I0204 07:02:48.383524 139910203815680 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.3105876445770264, loss=2.9052436351776123
I0204 07:03:34.671666 139910212208384 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.047978639602661, loss=3.424617290496826
I0204 07:04:21.359013 139910203815680 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.21478533744812, loss=2.909605026245117
I0204 07:05:07.976055 139910212208384 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.228374481201172, loss=4.615859031677246
I0204 07:05:54.201071 139910203815680 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.060256242752075, loss=4.9442853927612305
I0204 07:06:40.322030 139910212208384 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.9604207277297974, loss=4.164029121398926
I0204 07:07:26.407184 139910203815680 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.072636365890503, loss=2.954925298690796
I0204 07:08:12.511996 139910212208384 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.198255777359009, loss=4.898987770080566
I0204 07:08:48.179465 140107197974336 spec.py:321] Evaluating on the training split.
I0204 07:08:58.799338 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 07:09:24.931013 140107197974336 spec.py:349] Evaluating on the test split.
I0204 07:09:26.523456 140107197974336 submission_runner.py:408] Time since start: 56650.11s, 	Step: 114479, 	{'train/accuracy': 0.7202343344688416, 'train/loss': 1.2131118774414062, 'validation/accuracy': 0.6656999588012695, 'validation/loss': 1.4627410173416138, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.1023590564727783, 'test/num_examples': 10000, 'score': 52139.10345339775, 'total_duration': 56650.11271905899, 'accumulated_submission_time': 52139.10345339775, 'accumulated_eval_time': 4498.1347053050995, 'accumulated_logging_time': 6.402165174484253}
I0204 07:09:26.559409 139910203815680 logging_writer.py:48] [114479] accumulated_eval_time=4498.134705, accumulated_logging_time=6.402165, accumulated_submission_time=52139.103453, global_step=114479, preemption_count=0, score=52139.103453, test/accuracy=0.538800, test/loss=2.102359, test/num_examples=10000, total_duration=56650.112719, train/accuracy=0.720234, train/loss=1.213112, validation/accuracy=0.665700, validation/loss=1.462741, validation/num_examples=50000
I0204 07:09:35.219693 139910212208384 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.1241633892059326, loss=2.8965346813201904
I0204 07:10:17.017604 139910203815680 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.405453681945801, loss=2.931234836578369
I0204 07:11:02.984490 139910212208384 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.3152787685394287, loss=2.9607009887695312
I0204 07:11:49.034459 139910203815680 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.3587594032287598, loss=2.9628641605377197
I0204 07:12:35.390561 139910212208384 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.208521842956543, loss=2.8755929470062256
I0204 07:13:21.600222 139910203815680 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.0181190967559814, loss=4.377232551574707
I0204 07:14:07.740254 139910212208384 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.211451292037964, loss=2.8651463985443115
I0204 07:14:54.020891 139910203815680 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.098390579223633, loss=4.5274763107299805
I0204 07:15:40.105778 139910212208384 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.1203908920288086, loss=3.0092086791992188
I0204 07:16:26.124016 139910203815680 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.1783432960510254, loss=3.569647789001465
I0204 07:16:26.677730 140107197974336 spec.py:321] Evaluating on the training split.
I0204 07:16:37.377074 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 07:17:03.874029 140107197974336 spec.py:349] Evaluating on the test split.
I0204 07:17:05.467350 140107197974336 submission_runner.py:408] Time since start: 57109.06s, 	Step: 115403, 	{'train/accuracy': 0.7263085842132568, 'train/loss': 1.1966229677200317, 'validation/accuracy': 0.6647199988365173, 'validation/loss': 1.4690905809402466, 'validation/num_examples': 50000, 'test/accuracy': 0.539400041103363, 'test/loss': 2.100294589996338, 'test/num_examples': 10000, 'score': 52559.15979242325, 'total_duration': 57109.056616306305, 'accumulated_submission_time': 52559.15979242325, 'accumulated_eval_time': 4536.924311637878, 'accumulated_logging_time': 6.447792291641235}
I0204 07:17:05.502593 139910212208384 logging_writer.py:48] [115403] accumulated_eval_time=4536.924312, accumulated_logging_time=6.447792, accumulated_submission_time=52559.159792, global_step=115403, preemption_count=0, score=52559.159792, test/accuracy=0.539400, test/loss=2.100295, test/num_examples=10000, total_duration=57109.056616, train/accuracy=0.726309, train/loss=1.196623, validation/accuracy=0.664720, validation/loss=1.469091, validation/num_examples=50000
I0204 07:17:44.891561 139910203815680 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.214834213256836, loss=3.266502857208252
I0204 07:18:30.630112 139910212208384 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.9713221788406372, loss=3.972769021987915
I0204 07:19:16.960490 139910203815680 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.172727584838867, loss=2.7687463760375977
I0204 07:20:03.511420 139910212208384 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.782203197479248, loss=3.831615924835205
I0204 07:20:49.467958 139910203815680 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.7473082542419434, loss=3.978577136993408
I0204 07:21:35.639561 139910212208384 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.185318946838379, loss=2.88858699798584
I0204 07:22:21.840381 139910203815680 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.8310737609863281, loss=3.947279453277588
I0204 07:23:08.156610 139910212208384 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.2151432037353516, loss=2.8347327709198
I0204 07:23:54.230552 139910203815680 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.438279628753662, loss=2.8713715076446533
I0204 07:24:05.909631 140107197974336 spec.py:321] Evaluating on the training split.
I0204 07:24:16.178994 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 07:24:43.817820 140107197974336 spec.py:349] Evaluating on the test split.
I0204 07:24:45.417423 140107197974336 submission_runner.py:408] Time since start: 57569.01s, 	Step: 116327, 	{'train/accuracy': 0.7205468416213989, 'train/loss': 1.2346270084381104, 'validation/accuracy': 0.6696799993515015, 'validation/loss': 1.4559895992279053, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.084333658218384, 'test/num_examples': 10000, 'score': 52979.50097155571, 'total_duration': 57569.00668978691, 'accumulated_submission_time': 52979.50097155571, 'accumulated_eval_time': 4576.432106971741, 'accumulated_logging_time': 6.493849992752075}
I0204 07:24:45.454234 139910212208384 logging_writer.py:48] [116327] accumulated_eval_time=4576.432107, accumulated_logging_time=6.493850, accumulated_submission_time=52979.500972, global_step=116327, preemption_count=0, score=52979.500972, test/accuracy=0.548200, test/loss=2.084334, test/num_examples=10000, total_duration=57569.006690, train/accuracy=0.720547, train/loss=1.234627, validation/accuracy=0.669680, validation/loss=1.455990, validation/num_examples=50000
I0204 07:25:14.705503 139910203815680 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.438579559326172, loss=2.8891713619232178
I0204 07:25:59.707490 139910212208384 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.3791472911834717, loss=2.9384217262268066
I0204 07:26:46.178483 139910203815680 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.2264347076416016, loss=3.7294111251831055
I0204 07:27:32.753581 139910212208384 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.231745958328247, loss=2.87797212600708
I0204 07:28:18.727512 139910203815680 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.254664659500122, loss=2.803424119949341
I0204 07:29:05.014913 139910212208384 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.1467669010162354, loss=2.8465147018432617
I0204 07:29:51.172452 139910203815680 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.956154704093933, loss=3.8072316646575928
I0204 07:30:37.227653 139910212208384 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.3409042358398438, loss=4.683492660522461
I0204 07:31:23.264275 139910203815680 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.1135177612304688, loss=4.70854377746582
I0204 07:31:45.696006 140107197974336 spec.py:321] Evaluating on the training split.
I0204 07:31:56.602061 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 07:32:21.486358 140107197974336 spec.py:349] Evaluating on the test split.
I0204 07:32:23.093798 140107197974336 submission_runner.py:408] Time since start: 58026.68s, 	Step: 117250, 	{'train/accuracy': 0.7180468440055847, 'train/loss': 1.26100754737854, 'validation/accuracy': 0.6637200117111206, 'validation/loss': 1.4970401525497437, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.131301164627075, 'test/num_examples': 10000, 'score': 53399.68098139763, 'total_duration': 58026.68306350708, 'accumulated_submission_time': 53399.68098139763, 'accumulated_eval_time': 4613.82989192009, 'accumulated_logging_time': 6.539278507232666}
I0204 07:32:23.126811 139910212208384 logging_writer.py:48] [117250] accumulated_eval_time=4613.829892, accumulated_logging_time=6.539279, accumulated_submission_time=53399.680981, global_step=117250, preemption_count=0, score=53399.680981, test/accuracy=0.539900, test/loss=2.131301, test/num_examples=10000, total_duration=58026.683064, train/accuracy=0.718047, train/loss=1.261008, validation/accuracy=0.663720, validation/loss=1.497040, validation/num_examples=50000
I0204 07:32:43.193617 139910203815680 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.3102920055389404, loss=2.981729030609131
I0204 07:33:27.132088 139910212208384 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.223796844482422, loss=2.9225687980651855
I0204 07:34:13.356653 139910203815680 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.5007996559143066, loss=2.864611864089966
I0204 07:35:00.111261 139910212208384 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.4729385375976562, loss=4.955682754516602
I0204 07:35:46.166708 139910203815680 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.1841399669647217, loss=2.842148780822754
I0204 07:36:32.253949 139910212208384 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.209947347640991, loss=2.795539617538452
I0204 07:37:18.333441 139910203815680 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.261554718017578, loss=4.851666450500488
I0204 07:38:04.397335 139910212208384 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.3022727966308594, loss=2.8579893112182617
I0204 07:38:50.504689 139910203815680 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.2153677940368652, loss=2.9414560794830322
I0204 07:39:23.510027 140107197974336 spec.py:321] Evaluating on the training split.
I0204 07:39:34.202989 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 07:40:00.888920 140107197974336 spec.py:349] Evaluating on the test split.
I0204 07:40:02.502341 140107197974336 submission_runner.py:408] Time since start: 58486.09s, 	Step: 118173, 	{'train/accuracy': 0.7331640720367432, 'train/loss': 1.1802427768707275, 'validation/accuracy': 0.6723799705505371, 'validation/loss': 1.4435547590255737, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 2.0697150230407715, 'test/num_examples': 10000, 'score': 53819.9982714653, 'total_duration': 58486.09160208702, 'accumulated_submission_time': 53819.9982714653, 'accumulated_eval_time': 4652.822212696075, 'accumulated_logging_time': 6.585542917251587}
I0204 07:40:02.538738 139910212208384 logging_writer.py:48] [118173] accumulated_eval_time=4652.822213, accumulated_logging_time=6.585543, accumulated_submission_time=53819.998271, global_step=118173, preemption_count=0, score=53819.998271, test/accuracy=0.549400, test/loss=2.069715, test/num_examples=10000, total_duration=58486.091602, train/accuracy=0.733164, train/loss=1.180243, validation/accuracy=0.672380, validation/loss=1.443555, validation/num_examples=50000
I0204 07:40:13.551491 139910203815680 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.6711618900299072, loss=5.163141250610352
I0204 07:40:55.930703 139910212208384 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.9792803525924683, loss=4.602782726287842
I0204 07:41:41.988922 139910203815680 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.3283843994140625, loss=2.8169116973876953
I0204 07:42:28.038213 139910212208384 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.111990451812744, loss=2.9794905185699463
I0204 07:43:14.224357 139910203815680 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.378061056137085, loss=2.7418434619903564
I0204 07:44:00.282186 139910212208384 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.1165823936462402, loss=3.007833957672119
I0204 07:44:46.628379 139910203815680 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.2976081371307373, loss=4.802229404449463
I0204 07:45:33.035408 139910212208384 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.363133668899536, loss=3.256859302520752
I0204 07:46:19.241187 139910203815680 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.0084989070892334, loss=3.8948049545288086
I0204 07:47:02.683953 140107197974336 spec.py:321] Evaluating on the training split.
I0204 07:47:13.355011 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 07:47:40.456448 140107197974336 spec.py:349] Evaluating on the test split.
I0204 07:47:42.051918 140107197974336 submission_runner.py:408] Time since start: 58945.64s, 	Step: 119096, 	{'train/accuracy': 0.7335156202316284, 'train/loss': 1.199196219444275, 'validation/accuracy': 0.6699999570846558, 'validation/loss': 1.4819027185440063, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.1308164596557617, 'test/num_examples': 10000, 'score': 54240.08188533783, 'total_duration': 58945.64118242264, 'accumulated_submission_time': 54240.08188533783, 'accumulated_eval_time': 4692.190171718597, 'accumulated_logging_time': 6.631305694580078}
I0204 07:47:42.089197 139910212208384 logging_writer.py:48] [119096] accumulated_eval_time=4692.190172, accumulated_logging_time=6.631306, accumulated_submission_time=54240.081885, global_step=119096, preemption_count=0, score=54240.081885, test/accuracy=0.542100, test/loss=2.130816, test/num_examples=10000, total_duration=58945.641182, train/accuracy=0.733516, train/loss=1.199196, validation/accuracy=0.670000, validation/loss=1.481903, validation/num_examples=50000
I0204 07:47:44.060583 139910203815680 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.3554019927978516, loss=2.7876667976379395
I0204 07:48:24.901632 139910212208384 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.2891159057617188, loss=2.9621856212615967
I0204 07:49:11.036409 139910203815680 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.6390628814697266, loss=3.11625075340271
I0204 07:49:57.375463 139910212208384 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.1148030757904053, loss=3.301422119140625
I0204 07:50:43.845969 139910203815680 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.280791759490967, loss=2.806337833404541
I0204 07:51:30.069792 139910212208384 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.093871831893921, loss=3.4461669921875
I0204 07:52:16.232908 139910203815680 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.1694486141204834, loss=4.490887641906738
I0204 07:53:02.318175 139910212208384 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.2405459880828857, loss=2.7025704383850098
I0204 07:53:48.293778 139910203815680 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.1599762439727783, loss=2.885817289352417
I0204 07:54:34.693318 139910212208384 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.4178614616394043, loss=2.7091786861419678
I0204 07:54:42.161386 140107197974336 spec.py:321] Evaluating on the training split.
I0204 07:54:52.959486 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 07:55:18.859829 140107197974336 spec.py:349] Evaluating on the test split.
I0204 07:55:20.447691 140107197974336 submission_runner.py:408] Time since start: 59404.04s, 	Step: 120018, 	{'train/accuracy': 0.7240429520606995, 'train/loss': 1.2205166816711426, 'validation/accuracy': 0.6732999682426453, 'validation/loss': 1.4396311044692993, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.077719211578369, 'test/num_examples': 10000, 'score': 54660.09229564667, 'total_duration': 59404.036954164505, 'accumulated_submission_time': 54660.09229564667, 'accumulated_eval_time': 4730.476469278336, 'accumulated_logging_time': 6.678221940994263}
I0204 07:55:20.483594 139910203815680 logging_writer.py:48] [120018] accumulated_eval_time=4730.476469, accumulated_logging_time=6.678222, accumulated_submission_time=54660.092296, global_step=120018, preemption_count=0, score=54660.092296, test/accuracy=0.549700, test/loss=2.077719, test/num_examples=10000, total_duration=59404.036954, train/accuracy=0.724043, train/loss=1.220517, validation/accuracy=0.673300, validation/loss=1.439631, validation/num_examples=50000
I0204 07:55:53.080797 139910212208384 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.0143747329711914, loss=3.733872175216675
I0204 07:56:38.639676 139910203815680 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.442511558532715, loss=2.9041919708251953
I0204 07:57:24.701101 139910212208384 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.12166690826416, loss=4.323111534118652
I0204 07:58:10.946759 139910203815680 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.9380650520324707, loss=3.533318042755127
I0204 07:58:56.793139 139910212208384 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.2753491401672363, loss=2.798964023590088
I0204 07:59:43.057436 139910203815680 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.9605021476745605, loss=3.5913846492767334
I0204 08:00:29.180049 139910212208384 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.246270179748535, loss=2.8173656463623047
I0204 08:01:15.087982 139910203815680 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.1583213806152344, loss=3.7269368171691895
I0204 08:02:01.099653 139910212208384 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.845651388168335, loss=4.975055694580078
I0204 08:02:20.661657 140107197974336 spec.py:321] Evaluating on the training split.
I0204 08:02:31.407875 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 08:02:59.012605 140107197974336 spec.py:349] Evaluating on the test split.
I0204 08:03:00.616111 140107197974336 submission_runner.py:408] Time since start: 59864.21s, 	Step: 120944, 	{'train/accuracy': 0.73388671875, 'train/loss': 1.1822478771209717, 'validation/accuracy': 0.676099956035614, 'validation/loss': 1.439404010772705, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.0614137649536133, 'test/num_examples': 10000, 'score': 55080.207591056824, 'total_duration': 59864.2053706646, 'accumulated_submission_time': 55080.207591056824, 'accumulated_eval_time': 4770.4309067726135, 'accumulated_logging_time': 6.724869251251221}
I0204 08:03:00.652520 139910203815680 logging_writer.py:48] [120944] accumulated_eval_time=4770.430907, accumulated_logging_time=6.724869, accumulated_submission_time=55080.207591, global_step=120944, preemption_count=0, score=55080.207591, test/accuracy=0.552500, test/loss=2.061414, test/num_examples=10000, total_duration=59864.205371, train/accuracy=0.733887, train/loss=1.182248, validation/accuracy=0.676100, validation/loss=1.439404, validation/num_examples=50000
I0204 08:03:23.038837 139910212208384 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.354820966720581, loss=2.827085494995117
I0204 08:04:07.177189 139910203815680 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.497494697570801, loss=2.88979434967041
I0204 08:04:53.562848 139910212208384 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.4059770107269287, loss=2.864830732345581
I0204 08:05:40.025939 139910203815680 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.0570733547210693, loss=3.544696092605591
I0204 08:06:26.397828 139910212208384 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.445647716522217, loss=2.8915600776672363
I0204 08:07:12.551698 139910203815680 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.437615156173706, loss=2.9166789054870605
I0204 08:07:58.579803 139910212208384 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.3090410232543945, loss=3.970886707305908
I0204 08:08:44.572998 139910203815680 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.233138084411621, loss=2.8025386333465576
I0204 08:09:30.711990 139910212208384 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.547131061553955, loss=2.8541202545166016
I0204 08:10:00.709747 140107197974336 spec.py:321] Evaluating on the training split.
I0204 08:10:11.558602 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 08:10:38.849913 140107197974336 spec.py:349] Evaluating on the test split.
I0204 08:10:40.452487 140107197974336 submission_runner.py:408] Time since start: 60324.04s, 	Step: 121867, 	{'train/accuracy': 0.7496874928474426, 'train/loss': 1.114646315574646, 'validation/accuracy': 0.6793599724769592, 'validation/loss': 1.4229241609573364, 'validation/num_examples': 50000, 'test/accuracy': 0.5576000213623047, 'test/loss': 2.0427939891815186, 'test/num_examples': 10000, 'score': 55500.20226883888, 'total_duration': 60324.041732788086, 'accumulated_submission_time': 55500.20226883888, 'accumulated_eval_time': 4810.173624038696, 'accumulated_logging_time': 6.771934986114502}
I0204 08:10:40.493614 139910203815680 logging_writer.py:48] [121867] accumulated_eval_time=4810.173624, accumulated_logging_time=6.771935, accumulated_submission_time=55500.202269, global_step=121867, preemption_count=0, score=55500.202269, test/accuracy=0.557600, test/loss=2.042794, test/num_examples=10000, total_duration=60324.041733, train/accuracy=0.749687, train/loss=1.114646, validation/accuracy=0.679360, validation/loss=1.422924, validation/num_examples=50000
I0204 08:10:53.861871 139910212208384 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.268601417541504, loss=4.359475612640381
I0204 08:11:36.613694 139910203815680 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.271381378173828, loss=4.922791957855225
I0204 08:12:23.034028 139910212208384 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.299238920211792, loss=2.7955002784729004
I0204 08:13:09.189124 139910203815680 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.2089297771453857, loss=3.219130516052246
I0204 08:13:55.273442 139910212208384 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.4121408462524414, loss=2.827610731124878
I0204 08:14:41.508589 139910203815680 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.332376480102539, loss=3.1687397956848145
I0204 08:15:27.532217 139910212208384 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.3941853046417236, loss=2.836054801940918
I0204 08:16:13.607559 139910203815680 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.312734842300415, loss=2.8684945106506348
I0204 08:16:59.655945 139910212208384 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.6585984230041504, loss=2.7527124881744385
I0204 08:17:40.867931 140107197974336 spec.py:321] Evaluating on the training split.
I0204 08:17:51.584098 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 08:18:19.070765 140107197974336 spec.py:349] Evaluating on the test split.
I0204 08:18:20.667198 140107197974336 submission_runner.py:408] Time since start: 60784.26s, 	Step: 122791, 	{'train/accuracy': 0.7269140481948853, 'train/loss': 1.2267969846725464, 'validation/accuracy': 0.6785999536514282, 'validation/loss': 1.4419078826904297, 'validation/num_examples': 50000, 'test/accuracy': 0.5555000305175781, 'test/loss': 2.0610427856445312, 'test/num_examples': 10000, 'score': 55920.51446223259, 'total_duration': 60784.256432294846, 'accumulated_submission_time': 55920.51446223259, 'accumulated_eval_time': 4849.972870588303, 'accumulated_logging_time': 6.823628664016724}
I0204 08:18:20.704502 139910203815680 logging_writer.py:48] [122791] accumulated_eval_time=4849.972871, accumulated_logging_time=6.823629, accumulated_submission_time=55920.514462, global_step=122791, preemption_count=0, score=55920.514462, test/accuracy=0.555500, test/loss=2.061043, test/num_examples=10000, total_duration=60784.256432, train/accuracy=0.726914, train/loss=1.226797, validation/accuracy=0.678600, validation/loss=1.441908, validation/num_examples=50000
I0204 08:18:24.636388 139910212208384 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.423271894454956, loss=2.9183602333068848
I0204 08:19:05.703705 139910203815680 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.3431053161621094, loss=3.2741925716400146
I0204 08:19:51.798147 139910212208384 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.6138384342193604, loss=3.235738515853882
I0204 08:20:38.191365 139910203815680 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.6098105907440186, loss=3.1650829315185547
I0204 08:21:24.441571 139910212208384 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.3722352981567383, loss=2.7524499893188477
I0204 08:22:10.703176 139910203815680 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.4506585597991943, loss=2.7984213829040527
I0204 08:22:56.947802 139910212208384 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.355022430419922, loss=2.784970760345459
I0204 08:23:42.600638 139910203815680 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.5103416442871094, loss=2.814242362976074
I0204 08:24:28.600779 139910212208384 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.1098756790161133, loss=2.9686543941497803
I0204 08:25:14.905384 139910203815680 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.241015911102295, loss=2.6590657234191895
I0204 08:25:20.979712 140107197974336 spec.py:321] Evaluating on the training split.
I0204 08:25:31.604132 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 08:25:56.537447 140107197974336 spec.py:349] Evaluating on the test split.
I0204 08:25:58.142111 140107197974336 submission_runner.py:408] Time since start: 61241.73s, 	Step: 123715, 	{'train/accuracy': 0.7386718392372131, 'train/loss': 1.1407707929611206, 'validation/accuracy': 0.6824399828910828, 'validation/loss': 1.3840407133102417, 'validation/num_examples': 50000, 'test/accuracy': 0.560699999332428, 'test/loss': 2.0077319145202637, 'test/num_examples': 10000, 'score': 56340.726959228516, 'total_duration': 61241.7313709259, 'accumulated_submission_time': 56340.726959228516, 'accumulated_eval_time': 4887.135262012482, 'accumulated_logging_time': 6.871109485626221}
I0204 08:25:58.177115 139910212208384 logging_writer.py:48] [123715] accumulated_eval_time=4887.135262, accumulated_logging_time=6.871109, accumulated_submission_time=56340.726959, global_step=123715, preemption_count=0, score=56340.726959, test/accuracy=0.560700, test/loss=2.007732, test/num_examples=10000, total_duration=61241.731371, train/accuracy=0.738672, train/loss=1.140771, validation/accuracy=0.682440, validation/loss=1.384041, validation/num_examples=50000
I0204 08:26:32.131363 139910203815680 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.513657808303833, loss=2.838379383087158
I0204 08:27:17.876501 139910212208384 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.3707382678985596, loss=2.999713897705078
I0204 08:28:03.920397 139910203815680 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.436695098876953, loss=2.836977958679199
I0204 08:28:50.222234 139910212208384 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.51206111907959, loss=2.871835231781006
I0204 08:29:36.257334 139910203815680 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.5513296127319336, loss=2.8818864822387695
I0204 08:30:22.369726 139910212208384 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.220867156982422, loss=3.111027479171753
I0204 08:31:08.507294 139910203815680 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.5292561054229736, loss=2.868561267852783
I0204 08:31:54.421970 139910212208384 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.2607955932617188, loss=3.1000161170959473
I0204 08:32:40.564769 139910203815680 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.3395838737487793, loss=4.9228973388671875
I0204 08:32:58.265833 140107197974336 spec.py:321] Evaluating on the training split.
I0204 08:33:09.127543 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 08:33:36.136851 140107197974336 spec.py:349] Evaluating on the test split.
I0204 08:33:37.723089 140107197974336 submission_runner.py:408] Time since start: 61701.31s, 	Step: 124640, 	{'train/accuracy': 0.7439648509025574, 'train/loss': 1.139055848121643, 'validation/accuracy': 0.6798799633979797, 'validation/loss': 1.4270832538604736, 'validation/num_examples': 50000, 'test/accuracy': 0.5530000329017639, 'test/loss': 2.0573394298553467, 'test/num_examples': 10000, 'score': 56760.75370979309, 'total_duration': 61701.31235575676, 'accumulated_submission_time': 56760.75370979309, 'accumulated_eval_time': 4926.5925216674805, 'accumulated_logging_time': 6.9160919189453125}
I0204 08:33:37.758518 139910212208384 logging_writer.py:48] [124640] accumulated_eval_time=4926.592522, accumulated_logging_time=6.916092, accumulated_submission_time=56760.753710, global_step=124640, preemption_count=0, score=56760.753710, test/accuracy=0.553000, test/loss=2.057339, test/num_examples=10000, total_duration=61701.312356, train/accuracy=0.743965, train/loss=1.139056, validation/accuracy=0.679880, validation/loss=1.427083, validation/num_examples=50000
I0204 08:34:01.735571 139910203815680 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.5302908420562744, loss=3.1080667972564697
I0204 08:34:46.162694 139910212208384 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.622896194458008, loss=2.800062656402588
I0204 08:35:32.191130 139910203815680 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.170572519302368, loss=3.5744898319244385
I0204 08:36:18.536142 139910212208384 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.431199789047241, loss=2.7403063774108887
I0204 08:37:04.627434 139910203815680 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.5310873985290527, loss=2.6481845378875732
I0204 08:37:50.825240 139910212208384 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.893836259841919, loss=2.9113993644714355
I0204 08:38:37.324526 139910203815680 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.596963405609131, loss=2.813671827316284
I0204 08:39:23.633214 139910212208384 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.454810857772827, loss=4.7508649826049805
I0204 08:40:09.798125 139910203815680 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.928297996520996, loss=2.8091158866882324
I0204 08:40:38.046149 140107197974336 spec.py:321] Evaluating on the training split.
I0204 08:40:48.970664 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 08:41:16.142078 140107197974336 spec.py:349] Evaluating on the test split.
I0204 08:41:17.728867 140107197974336 submission_runner.py:408] Time since start: 62161.32s, 	Step: 125563, 	{'train/accuracy': 0.7366601228713989, 'train/loss': 1.1655316352844238, 'validation/accuracy': 0.6827999949455261, 'validation/loss': 1.4155220985412598, 'validation/num_examples': 50000, 'test/accuracy': 0.5559000372886658, 'test/loss': 2.040470600128174, 'test/num_examples': 10000, 'score': 57180.97926735878, 'total_duration': 62161.31812500954, 'accumulated_submission_time': 57180.97926735878, 'accumulated_eval_time': 4966.275221347809, 'accumulated_logging_time': 6.962021350860596}
I0204 08:41:17.766129 139910212208384 logging_writer.py:48] [125563] accumulated_eval_time=4966.275221, accumulated_logging_time=6.962021, accumulated_submission_time=57180.979267, global_step=125563, preemption_count=0, score=57180.979267, test/accuracy=0.555900, test/loss=2.040471, test/num_examples=10000, total_duration=62161.318125, train/accuracy=0.736660, train/loss=1.165532, validation/accuracy=0.682800, validation/loss=1.415522, validation/num_examples=50000
I0204 08:41:32.704396 139910203815680 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.354146957397461, loss=3.50834321975708
I0204 08:42:15.657157 139910212208384 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.4209787845611572, loss=2.775827646255493
I0204 08:43:01.849214 139910203815680 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.5404603481292725, loss=2.86255145072937
I0204 08:43:48.407222 139910212208384 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.695345640182495, loss=3.5073490142822266
I0204 08:44:34.631051 139910203815680 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.8226571083068848, loss=4.015557765960693
I0204 08:45:20.640773 139910212208384 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.6073057651519775, loss=2.8023414611816406
I0204 08:46:07.074598 139910203815680 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.7197606563568115, loss=2.7215137481689453
I0204 08:46:53.199127 139910212208384 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.795042037963867, loss=4.663294315338135
I0204 08:47:39.523288 139910203815680 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.374246835708618, loss=3.2477893829345703
I0204 08:48:18.280978 140107197974336 spec.py:321] Evaluating on the training split.
I0204 08:48:28.925330 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 08:48:53.963702 140107197974336 spec.py:349] Evaluating on the test split.
I0204 08:48:55.569352 140107197974336 submission_runner.py:408] Time since start: 62619.16s, 	Step: 126485, 	{'train/accuracy': 0.7461132407188416, 'train/loss': 1.1315586566925049, 'validation/accuracy': 0.6882199645042419, 'validation/loss': 1.3758423328399658, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9890079498291016, 'test/num_examples': 10000, 'score': 57601.43358683586, 'total_duration': 62619.15861034393, 'accumulated_submission_time': 57601.43358683586, 'accumulated_eval_time': 5003.563591241837, 'accumulated_logging_time': 7.007826805114746}
I0204 08:48:55.604669 139910212208384 logging_writer.py:48] [126485] accumulated_eval_time=5003.563591, accumulated_logging_time=7.007827, accumulated_submission_time=57601.433587, global_step=126485, preemption_count=0, score=57601.433587, test/accuracy=0.566100, test/loss=1.989008, test/num_examples=10000, total_duration=62619.158610, train/accuracy=0.746113, train/loss=1.131559, validation/accuracy=0.688220, validation/loss=1.375842, validation/num_examples=50000
I0204 08:49:01.907972 139910203815680 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.0919179916381836, loss=4.964688301086426
I0204 08:49:43.120669 139910212208384 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.131528854370117, loss=4.038924217224121
I0204 08:50:29.266018 139910203815680 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.409702777862549, loss=2.855750560760498
I0204 08:51:15.568581 139910212208384 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.6982884407043457, loss=2.9268980026245117
I0204 08:52:01.705543 139910203815680 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.4320032596588135, loss=2.7470240592956543
I0204 08:52:47.656491 139910212208384 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.34464955329895, loss=3.030365467071533
I0204 08:53:33.885351 139910203815680 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.2579667568206787, loss=3.84730863571167
I0204 08:54:20.123671 139910212208384 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.4650912284851074, loss=4.721117973327637
I0204 08:55:06.306320 139910203815680 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.5678300857543945, loss=2.685131072998047
I0204 08:55:52.484882 139910212208384 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.3767685890197754, loss=3.000869035720825
I0204 08:55:55.985396 140107197974336 spec.py:321] Evaluating on the training split.
I0204 08:56:06.720283 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 08:56:31.929509 140107197974336 spec.py:349] Evaluating on the test split.
I0204 08:56:33.528692 140107197974336 submission_runner.py:408] Time since start: 63077.12s, 	Step: 127409, 	{'train/accuracy': 0.7524218559265137, 'train/loss': 1.1364513635635376, 'validation/accuracy': 0.6879400014877319, 'validation/loss': 1.4114627838134766, 'validation/num_examples': 50000, 'test/accuracy': 0.5671000480651855, 'test/loss': 2.030573844909668, 'test/num_examples': 10000, 'score': 58021.75379371643, 'total_duration': 63077.11794781685, 'accumulated_submission_time': 58021.75379371643, 'accumulated_eval_time': 5041.106873750687, 'accumulated_logging_time': 7.051853656768799}
I0204 08:56:33.566336 139910203815680 logging_writer.py:48] [127409] accumulated_eval_time=5041.106874, accumulated_logging_time=7.051854, accumulated_submission_time=58021.753794, global_step=127409, preemption_count=0, score=58021.753794, test/accuracy=0.567100, test/loss=2.030574, test/num_examples=10000, total_duration=63077.117948, train/accuracy=0.752422, train/loss=1.136451, validation/accuracy=0.687940, validation/loss=1.411463, validation/num_examples=50000
I0204 08:57:10.243435 139910212208384 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.4332668781280518, loss=4.356164455413818
I0204 08:57:56.102426 139910203815680 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.2722744941711426, loss=2.711702585220337
I0204 08:58:42.407017 139910212208384 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.3268144130706787, loss=3.2227182388305664
I0204 08:59:29.039733 139910203815680 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.290447950363159, loss=3.0092122554779053
I0204 09:00:15.120102 139910212208384 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.641818046569824, loss=2.7663283348083496
I0204 09:01:00.920247 139910203815680 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.4205667972564697, loss=4.136240005493164
I0204 09:01:46.871482 139910212208384 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.4180500507354736, loss=3.6867055892944336
I0204 09:02:32.827311 139910203815680 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.7045865058898926, loss=4.728704929351807
I0204 09:03:18.830241 139910212208384 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.547701597213745, loss=4.959385395050049
I0204 09:03:33.873512 140107197974336 spec.py:321] Evaluating on the training split.
I0204 09:03:44.600348 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 09:04:10.222062 140107197974336 spec.py:349] Evaluating on the test split.
I0204 09:04:11.822869 140107197974336 submission_runner.py:408] Time since start: 63535.41s, 	Step: 128334, 	{'train/accuracy': 0.74964839220047, 'train/loss': 1.1114648580551147, 'validation/accuracy': 0.689799964427948, 'validation/loss': 1.371135950088501, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.993343472480774, 'test/num_examples': 10000, 'score': 58441.99978637695, 'total_duration': 63535.41212558746, 'accumulated_submission_time': 58441.99978637695, 'accumulated_eval_time': 5079.05620598793, 'accumulated_logging_time': 7.098832368850708}
I0204 09:04:11.860882 139910203815680 logging_writer.py:48] [128334] accumulated_eval_time=5079.056206, accumulated_logging_time=7.098832, accumulated_submission_time=58441.999786, global_step=128334, preemption_count=0, score=58441.999786, test/accuracy=0.567800, test/loss=1.993343, test/num_examples=10000, total_duration=63535.412126, train/accuracy=0.749648, train/loss=1.111465, validation/accuracy=0.689800, validation/loss=1.371136, validation/num_examples=50000
I0204 09:04:38.175137 139910212208384 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.5085082054138184, loss=2.9268972873687744
I0204 09:05:22.813421 139910203815680 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.5041263103485107, loss=2.686706066131592
I0204 09:06:09.053847 139910212208384 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.473712682723999, loss=3.1586806774139404
I0204 09:06:55.530372 139910203815680 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.551205635070801, loss=3.115542411804199
I0204 09:07:41.553444 139910212208384 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.7900116443634033, loss=2.816782236099243
I0204 09:08:27.729206 139910203815680 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.6261844635009766, loss=2.71711802482605
I0204 09:09:14.112367 139910212208384 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.5085387229919434, loss=2.614793539047241
I0204 09:09:59.766669 139910203815680 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.5660269260406494, loss=4.805854320526123
I0204 09:10:45.801013 139910212208384 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.724442720413208, loss=2.998270034790039
I0204 09:11:11.866147 140107197974336 spec.py:321] Evaluating on the training split.
I0204 09:11:22.770775 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 09:11:47.226577 140107197974336 spec.py:349] Evaluating on the test split.
I0204 09:11:48.820498 140107197974336 submission_runner.py:408] Time since start: 63992.41s, 	Step: 129258, 	{'train/accuracy': 0.7498828172683716, 'train/loss': 1.1026932001113892, 'validation/accuracy': 0.6937400102615356, 'validation/loss': 1.3566606044769287, 'validation/num_examples': 50000, 'test/accuracy': 0.5713000297546387, 'test/loss': 1.9661833047866821, 'test/num_examples': 10000, 'score': 58861.942873716354, 'total_duration': 63992.40976333618, 'accumulated_submission_time': 58861.942873716354, 'accumulated_eval_time': 5116.010575294495, 'accumulated_logging_time': 7.14700722694397}
I0204 09:11:48.857688 139910203815680 logging_writer.py:48] [129258] accumulated_eval_time=5116.010575, accumulated_logging_time=7.147007, accumulated_submission_time=58861.942874, global_step=129258, preemption_count=0, score=58861.942874, test/accuracy=0.571300, test/loss=1.966183, test/num_examples=10000, total_duration=63992.409763, train/accuracy=0.749883, train/loss=1.102693, validation/accuracy=0.693740, validation/loss=1.356661, validation/num_examples=50000
I0204 09:12:05.748929 139910212208384 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.322949171066284, loss=3.8440184593200684
I0204 09:12:49.038018 139910203815680 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.732240915298462, loss=2.760293960571289
I0204 09:13:34.896186 139910212208384 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.423532247543335, loss=4.76737117767334
I0204 09:14:21.257959 139910203815680 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.799269199371338, loss=2.7964320182800293
I0204 09:15:07.416702 139910212208384 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.4777443408966064, loss=2.759315013885498
I0204 09:15:53.121331 139910203815680 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.811134099960327, loss=2.8685405254364014
I0204 09:16:38.988599 139910212208384 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.00104022026062, loss=2.743459701538086
I0204 09:17:24.712195 139910203815680 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.5563766956329346, loss=2.855942487716675
I0204 09:18:11.010824 139910212208384 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.4902737140655518, loss=3.3205769062042236
I0204 09:18:49.173795 140107197974336 spec.py:321] Evaluating on the training split.
I0204 09:18:59.835111 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 09:19:27.048120 140107197974336 spec.py:349] Evaluating on the test split.
I0204 09:19:28.637616 140107197974336 submission_runner.py:408] Time since start: 64452.23s, 	Step: 130184, 	{'train/accuracy': 0.75355464220047, 'train/loss': 1.101863980293274, 'validation/accuracy': 0.6942200064659119, 'validation/loss': 1.3657922744750977, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.9783955812454224, 'test/num_examples': 10000, 'score': 59282.19617843628, 'total_duration': 64452.22687602043, 'accumulated_submission_time': 59282.19617843628, 'accumulated_eval_time': 5155.474381446838, 'accumulated_logging_time': 7.194597005844116}
I0204 09:19:28.672015 139910203815680 logging_writer.py:48] [130184] accumulated_eval_time=5155.474381, accumulated_logging_time=7.194597, accumulated_submission_time=59282.196178, global_step=130184, preemption_count=0, score=59282.196178, test/accuracy=0.569900, test/loss=1.978396, test/num_examples=10000, total_duration=64452.226876, train/accuracy=0.753555, train/loss=1.101864, validation/accuracy=0.694220, validation/loss=1.365792, validation/num_examples=50000
I0204 09:19:35.356078 139910212208384 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.4760279655456543, loss=3.073866844177246
I0204 09:20:17.127048 139910203815680 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.703946590423584, loss=4.935481548309326
I0204 09:21:03.486706 139910212208384 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.739062786102295, loss=2.856706142425537
I0204 09:21:49.983918 139910203815680 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.7420806884765625, loss=4.815455436706543
I0204 09:22:36.309988 139910212208384 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.3904902935028076, loss=4.2193193435668945
I0204 09:23:22.486544 139910203815680 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.913870096206665, loss=2.7544431686401367
I0204 09:24:08.897955 139910212208384 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.475980043411255, loss=4.017023086547852
I0204 09:24:54.984997 139910203815680 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.690128803253174, loss=2.649108648300171
I0204 09:25:41.460835 139910212208384 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.468172788619995, loss=3.417264461517334
I0204 09:26:27.938429 139910203815680 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.646637439727783, loss=2.7271082401275635
I0204 09:26:28.997175 140107197974336 spec.py:321] Evaluating on the training split.
I0204 09:26:39.681295 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 09:27:09.056245 140107197974336 spec.py:349] Evaluating on the test split.
I0204 09:27:10.662264 140107197974336 submission_runner.py:408] Time since start: 64914.25s, 	Step: 131104, 	{'train/accuracy': 0.7650976181030273, 'train/loss': 1.0323588848114014, 'validation/accuracy': 0.6943999528884888, 'validation/loss': 1.344438910484314, 'validation/num_examples': 50000, 'test/accuracy': 0.5657000541687012, 'test/loss': 1.97388756275177, 'test/num_examples': 10000, 'score': 59702.45468664169, 'total_duration': 64914.25153064728, 'accumulated_submission_time': 59702.45468664169, 'accumulated_eval_time': 5197.139461517334, 'accumulated_logging_time': 7.241278171539307}
I0204 09:27:10.697932 139910212208384 logging_writer.py:48] [131104] accumulated_eval_time=5197.139462, accumulated_logging_time=7.241278, accumulated_submission_time=59702.454687, global_step=131104, preemption_count=0, score=59702.454687, test/accuracy=0.565700, test/loss=1.973888, test/num_examples=10000, total_duration=64914.251531, train/accuracy=0.765098, train/loss=1.032359, validation/accuracy=0.694400, validation/loss=1.344439, validation/num_examples=50000
I0204 09:27:49.891277 139910203815680 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.7721340656280518, loss=2.784331798553467
I0204 09:28:35.925837 139910212208384 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.8626766204833984, loss=2.6930222511291504
I0204 09:29:22.844521 139910203815680 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.647874355316162, loss=2.8573906421661377
I0204 09:30:09.548736 139910212208384 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.6767494678497314, loss=2.7486579418182373
I0204 09:30:55.576355 139910203815680 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.567502737045288, loss=4.636715412139893
I0204 09:31:41.634076 139910212208384 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.488111972808838, loss=4.714795112609863
I0204 09:32:28.012142 139910203815680 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.442854881286621, loss=3.745042085647583
I0204 09:33:13.890385 139910212208384 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.7438814640045166, loss=3.329252243041992
I0204 09:33:59.798672 139910203815680 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.5225486755371094, loss=4.226536750793457
I0204 09:34:10.708073 140107197974336 spec.py:321] Evaluating on the training split.
I0204 09:34:21.247239 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 09:34:48.480421 140107197974336 spec.py:349] Evaluating on the test split.
I0204 09:34:50.074305 140107197974336 submission_runner.py:408] Time since start: 65373.66s, 	Step: 132025, 	{'train/accuracy': 0.7515429258346558, 'train/loss': 1.0836161375045776, 'validation/accuracy': 0.6946199536323547, 'validation/loss': 1.3420709371566772, 'validation/num_examples': 50000, 'test/accuracy': 0.5679000020027161, 'test/loss': 1.9602190256118774, 'test/num_examples': 10000, 'score': 60122.40482163429, 'total_duration': 65373.663570165634, 'accumulated_submission_time': 60122.40482163429, 'accumulated_eval_time': 5236.505722999573, 'accumulated_logging_time': 7.285628318786621}
I0204 09:34:50.111106 139910212208384 logging_writer.py:48] [132025] accumulated_eval_time=5236.505723, accumulated_logging_time=7.285628, accumulated_submission_time=60122.404822, global_step=132025, preemption_count=0, score=60122.404822, test/accuracy=0.567900, test/loss=1.960219, test/num_examples=10000, total_duration=65373.663570, train/accuracy=0.751543, train/loss=1.083616, validation/accuracy=0.694620, validation/loss=1.342071, validation/num_examples=50000
I0204 09:35:19.964662 139910203815680 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.6721765995025635, loss=2.6968908309936523
I0204 09:36:05.185956 139910212208384 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.6213772296905518, loss=3.0383260250091553
I0204 09:36:51.063519 139910203815680 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.4961891174316406, loss=4.211145877838135
I0204 09:37:37.453896 139910212208384 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.000666856765747, loss=2.795203685760498
I0204 09:38:23.453140 139910203815680 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.5054855346679688, loss=2.6977968215942383
I0204 09:39:09.458663 139910212208384 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.5517919063568115, loss=4.017948150634766
I0204 09:39:55.811568 139910203815680 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.802553415298462, loss=2.8732080459594727
I0204 09:40:41.579820 139910212208384 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.7908735275268555, loss=2.8098154067993164
I0204 09:41:27.600849 139910203815680 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.0636725425720215, loss=4.635334491729736
I0204 09:41:50.541193 140107197974336 spec.py:321] Evaluating on the training split.
I0204 09:42:00.995972 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 09:42:26.700557 140107197974336 spec.py:349] Evaluating on the test split.
I0204 09:42:28.300110 140107197974336 submission_runner.py:408] Time since start: 65831.89s, 	Step: 132951, 	{'train/accuracy': 0.758593738079071, 'train/loss': 1.0445502996444702, 'validation/accuracy': 0.6947999596595764, 'validation/loss': 1.3174066543579102, 'validation/num_examples': 50000, 'test/accuracy': 0.5760000348091125, 'test/loss': 1.9236680269241333, 'test/num_examples': 10000, 'score': 60542.771542072296, 'total_duration': 65831.88937497139, 'accumulated_submission_time': 60542.771542072296, 'accumulated_eval_time': 5274.264638900757, 'accumulated_logging_time': 7.332998752593994}
I0204 09:42:28.337514 139910212208384 logging_writer.py:48] [132951] accumulated_eval_time=5274.264639, accumulated_logging_time=7.332999, accumulated_submission_time=60542.771542, global_step=132951, preemption_count=0, score=60542.771542, test/accuracy=0.576000, test/loss=1.923668, test/num_examples=10000, total_duration=65831.889375, train/accuracy=0.758594, train/loss=1.044550, validation/accuracy=0.694800, validation/loss=1.317407, validation/num_examples=50000
I0204 09:42:47.998769 139910203815680 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.7474260330200195, loss=2.7498650550842285
I0204 09:43:31.452562 139910212208384 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.8001315593719482, loss=2.7145347595214844
I0204 09:44:17.415025 139910203815680 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.5825958251953125, loss=2.6433181762695312
I0204 09:45:03.986588 139910212208384 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.868591547012329, loss=2.6930418014526367
I0204 09:45:49.769054 139910203815680 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.804199457168579, loss=2.804287910461426
I0204 09:46:35.858367 139910212208384 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.03841495513916, loss=2.6894283294677734
I0204 09:47:22.217953 139910203815680 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.691108226776123, loss=2.9700660705566406
I0204 09:48:08.046982 139910212208384 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.808117151260376, loss=2.678230047225952
I0204 09:48:53.986678 139910203815680 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.953888416290283, loss=3.852719783782959
I0204 09:49:28.487437 140107197974336 spec.py:321] Evaluating on the training split.
I0204 09:49:39.296293 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 09:50:06.778231 140107197974336 spec.py:349] Evaluating on the test split.
I0204 09:50:08.390298 140107197974336 submission_runner.py:408] Time since start: 66291.98s, 	Step: 133876, 	{'train/accuracy': 0.7695898413658142, 'train/loss': 1.018075704574585, 'validation/accuracy': 0.6988999843597412, 'validation/loss': 1.3271454572677612, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9531786441802979, 'test/num_examples': 10000, 'score': 60962.859546899796, 'total_duration': 66291.97956442833, 'accumulated_submission_time': 60962.859546899796, 'accumulated_eval_time': 5314.167495965958, 'accumulated_logging_time': 7.379750490188599}
I0204 09:50:08.432812 139910212208384 logging_writer.py:48] [133876] accumulated_eval_time=5314.167496, accumulated_logging_time=7.379750, accumulated_submission_time=60962.859547, global_step=133876, preemption_count=0, score=60962.859547, test/accuracy=0.571100, test/loss=1.953179, test/num_examples=10000, total_duration=66291.979564, train/accuracy=0.769590, train/loss=1.018076, validation/accuracy=0.698900, validation/loss=1.327145, validation/num_examples=50000
I0204 09:50:18.265415 139910203815680 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.5969371795654297, loss=2.9386751651763916
I0204 09:51:00.048230 139910212208384 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.7830638885498047, loss=2.6873626708984375
I0204 09:51:46.149978 139910203815680 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.812039375305176, loss=2.7533633708953857
I0204 09:52:32.514520 139910212208384 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.739304542541504, loss=3.0496699810028076
I0204 09:53:18.740260 139910203815680 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.633453607559204, loss=2.8652865886688232
I0204 09:54:05.114052 139910212208384 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.933833599090576, loss=2.5953621864318848
I0204 09:54:51.345657 139910203815680 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.6458425521850586, loss=2.585937261581421
I0204 09:55:37.635963 139910212208384 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.867102861404419, loss=2.7135250568389893
I0204 09:56:23.954403 139910203815680 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.6228017807006836, loss=4.043986797332764
I0204 09:57:08.438294 140107197974336 spec.py:321] Evaluating on the training split.
I0204 09:57:19.277905 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 09:57:46.015817 140107197974336 spec.py:349] Evaluating on the test split.
I0204 09:57:47.620425 140107197974336 submission_runner.py:408] Time since start: 66751.21s, 	Step: 134798, 	{'train/accuracy': 0.7604882717132568, 'train/loss': 1.0434051752090454, 'validation/accuracy': 0.6985399723052979, 'validation/loss': 1.303519606590271, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9330246448516846, 'test/num_examples': 10000, 'score': 61382.80419540405, 'total_duration': 66751.209690094, 'accumulated_submission_time': 61382.80419540405, 'accumulated_eval_time': 5353.349631071091, 'accumulated_logging_time': 7.431820392608643}
I0204 09:57:47.655657 139910212208384 logging_writer.py:48] [134798] accumulated_eval_time=5353.349631, accumulated_logging_time=7.431820, accumulated_submission_time=61382.804195, global_step=134798, preemption_count=0, score=61382.804195, test/accuracy=0.570700, test/loss=1.933025, test/num_examples=10000, total_duration=66751.209690, train/accuracy=0.760488, train/loss=1.043405, validation/accuracy=0.698540, validation/loss=1.303520, validation/num_examples=50000
I0204 09:57:48.833901 139910203815680 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.916043758392334, loss=2.667137622833252
I0204 09:58:29.541174 139910212208384 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.9140403270721436, loss=2.932677745819092
I0204 09:59:15.537121 139910203815680 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.6674225330352783, loss=3.3105995655059814
I0204 10:00:01.599636 139910212208384 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.713538885116577, loss=2.9558558464050293
I0204 10:00:48.229546 139910203815680 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.8370840549468994, loss=4.2431254386901855
I0204 10:01:34.322574 139910212208384 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.844278335571289, loss=2.656996726989746
I0204 10:02:20.478777 139910203815680 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.9174020290374756, loss=2.6353073120117188
I0204 10:03:06.770099 139910212208384 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.1246023178100586, loss=4.6937479972839355
I0204 10:03:52.899619 139910203815680 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.8195605278015137, loss=2.6609456539154053
I0204 10:04:39.232197 139910212208384 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.1385180950164795, loss=4.556305885314941
I0204 10:04:48.054960 140107197974336 spec.py:321] Evaluating on the training split.
I0204 10:04:58.876942 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 10:05:24.809359 140107197974336 spec.py:349] Evaluating on the test split.
I0204 10:05:26.412667 140107197974336 submission_runner.py:408] Time since start: 67210.00s, 	Step: 135721, 	{'train/accuracy': 0.7647656202316284, 'train/loss': 1.0401690006256104, 'validation/accuracy': 0.6980400085449219, 'validation/loss': 1.3200068473815918, 'validation/num_examples': 50000, 'test/accuracy': 0.5758000016212463, 'test/loss': 1.9241626262664795, 'test/num_examples': 10000, 'score': 61803.14104223251, 'total_duration': 67210.00192761421, 'accumulated_submission_time': 61803.14104223251, 'accumulated_eval_time': 5391.70734000206, 'accumulated_logging_time': 7.477010011672974}
I0204 10:05:26.448812 139910203815680 logging_writer.py:48] [135721] accumulated_eval_time=5391.707340, accumulated_logging_time=7.477010, accumulated_submission_time=61803.141042, global_step=135721, preemption_count=0, score=61803.141042, test/accuracy=0.575800, test/loss=1.924163, test/num_examples=10000, total_duration=67210.001928, train/accuracy=0.764766, train/loss=1.040169, validation/accuracy=0.698040, validation/loss=1.320007, validation/num_examples=50000
I0204 10:05:57.874770 139910212208384 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.807689666748047, loss=2.958754301071167
I0204 10:06:43.364502 139910203815680 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.0607683658599854, loss=2.6340157985687256
I0204 10:07:29.750292 139910212208384 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.6471781730651855, loss=3.161956787109375
I0204 10:08:16.143373 139910203815680 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.633373498916626, loss=2.759099006652832
I0204 10:09:02.079091 139910212208384 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.764941692352295, loss=3.996819496154785
I0204 10:09:48.251723 139910203815680 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.8850674629211426, loss=4.646352767944336
I0204 10:10:34.344758 139910212208384 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.098524570465088, loss=4.698240756988525
I0204 10:11:20.544386 139910203815680 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.662679672241211, loss=2.6431167125701904
I0204 10:12:07.055506 139910212208384 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.639700174331665, loss=3.198660373687744
I0204 10:12:26.423145 140107197974336 spec.py:321] Evaluating on the training split.
I0204 10:12:37.074165 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 10:13:04.776131 140107197974336 spec.py:349] Evaluating on the test split.
I0204 10:13:06.378867 140107197974336 submission_runner.py:408] Time since start: 67669.97s, 	Step: 136644, 	{'train/accuracy': 0.7710546851158142, 'train/loss': 1.0173821449279785, 'validation/accuracy': 0.7045800089836121, 'validation/loss': 1.311528205871582, 'validation/num_examples': 50000, 'test/accuracy': 0.5790000557899475, 'test/loss': 1.9301297664642334, 'test/num_examples': 10000, 'score': 62223.05319976807, 'total_duration': 67669.96811890602, 'accumulated_submission_time': 62223.05319976807, 'accumulated_eval_time': 5431.663053035736, 'accumulated_logging_time': 7.522252082824707}
I0204 10:13:06.422661 139910203815680 logging_writer.py:48] [136644] accumulated_eval_time=5431.663053, accumulated_logging_time=7.522252, accumulated_submission_time=62223.053200, global_step=136644, preemption_count=0, score=62223.053200, test/accuracy=0.579000, test/loss=1.930130, test/num_examples=10000, total_duration=67669.968119, train/accuracy=0.771055, train/loss=1.017382, validation/accuracy=0.704580, validation/loss=1.311528, validation/num_examples=50000
I0204 10:13:28.804390 139910212208384 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.8108930587768555, loss=2.8727073669433594
I0204 10:14:12.754134 139910203815680 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.7845425605773926, loss=3.8096158504486084
I0204 10:14:59.016222 139910212208384 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.8976755142211914, loss=2.5517067909240723
I0204 10:15:45.487409 139910203815680 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.7902841567993164, loss=4.307830333709717
I0204 10:16:31.537456 139910212208384 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.8462047576904297, loss=2.548152446746826
I0204 10:17:17.664850 139910203815680 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.7298946380615234, loss=2.7008707523345947
I0204 10:18:03.920612 139910212208384 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.31498646736145, loss=4.756173133850098
I0204 10:18:49.645813 139910203815680 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.005417823791504, loss=2.693981647491455
I0204 10:19:36.048740 139910212208384 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.1861419677734375, loss=3.812648057937622
I0204 10:20:06.561798 140107197974336 spec.py:321] Evaluating on the training split.
I0204 10:20:17.157960 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 10:20:44.048060 140107197974336 spec.py:349] Evaluating on the test split.
I0204 10:20:45.649795 140107197974336 submission_runner.py:408] Time since start: 68129.24s, 	Step: 137568, 	{'train/accuracy': 0.7683789134025574, 'train/loss': 1.041908621788025, 'validation/accuracy': 0.7035399675369263, 'validation/loss': 1.3146579265594482, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9376602172851562, 'test/num_examples': 10000, 'score': 62643.12946271896, 'total_duration': 68129.23905944824, 'accumulated_submission_time': 62643.12946271896, 'accumulated_eval_time': 5470.751053571701, 'accumulated_logging_time': 7.575902938842773}
I0204 10:20:45.685850 139910203815680 logging_writer.py:48] [137568] accumulated_eval_time=5470.751054, accumulated_logging_time=7.575903, accumulated_submission_time=62643.129463, global_step=137568, preemption_count=0, score=62643.129463, test/accuracy=0.575100, test/loss=1.937660, test/num_examples=10000, total_duration=68129.239059, train/accuracy=0.768379, train/loss=1.041909, validation/accuracy=0.703540, validation/loss=1.314658, validation/num_examples=50000
I0204 10:20:58.644279 139910212208384 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.4897520542144775, loss=2.954113721847534
I0204 10:21:41.454599 139910203815680 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.884246349334717, loss=2.548710346221924
I0204 10:22:27.514053 139910212208384 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.8842930793762207, loss=2.7326972484588623
I0204 10:23:13.783173 139910203815680 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.2839810848236084, loss=3.1340537071228027
I0204 10:24:00.038323 139910212208384 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.594527244567871, loss=3.571960926055908
I0204 10:24:46.500756 139910203815680 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.9073660373687744, loss=4.086333274841309
I0204 10:25:32.869133 139910212208384 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.8301520347595215, loss=2.739665985107422
I0204 10:26:19.026351 139910203815680 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.21378755569458, loss=2.7918896675109863
I0204 10:27:05.303502 139910212208384 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.0249857902526855, loss=2.6089181900024414
I0204 10:27:45.719279 140107197974336 spec.py:321] Evaluating on the training split.
I0204 10:27:56.550586 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 10:28:24.231599 140107197974336 spec.py:349] Evaluating on the test split.
I0204 10:28:25.830322 140107197974336 submission_runner.py:408] Time since start: 68589.42s, 	Step: 138489, 	{'train/accuracy': 0.7689452767372131, 'train/loss': 1.025770902633667, 'validation/accuracy': 0.7057200074195862, 'validation/loss': 1.2924665212631226, 'validation/num_examples': 50000, 'test/accuracy': 0.5884000062942505, 'test/loss': 1.8851429224014282, 'test/num_examples': 10000, 'score': 63063.101344347, 'total_duration': 68589.41958975792, 'accumulated_submission_time': 63063.101344347, 'accumulated_eval_time': 5510.862103939056, 'accumulated_logging_time': 7.621830224990845}
I0204 10:28:25.869089 139910203815680 logging_writer.py:48] [138489] accumulated_eval_time=5510.862104, accumulated_logging_time=7.621830, accumulated_submission_time=63063.101344, global_step=138489, preemption_count=0, score=63063.101344, test/accuracy=0.588400, test/loss=1.885143, test/num_examples=10000, total_duration=68589.419590, train/accuracy=0.768945, train/loss=1.025771, validation/accuracy=0.705720, validation/loss=1.292467, validation/num_examples=50000
I0204 10:28:30.591772 139910212208384 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.704395055770874, loss=4.053814888000488
I0204 10:29:11.746114 139910203815680 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.2570061683654785, loss=4.333290100097656
I0204 10:29:57.687671 139910212208384 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.94063663482666, loss=2.7066633701324463
I0204 10:30:43.907465 139910203815680 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.9740872383117676, loss=3.221069812774658
I0204 10:31:30.327041 139910212208384 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.9047389030456543, loss=4.13520622253418
I0204 10:32:16.412794 139910203815680 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.862518310546875, loss=3.1474037170410156
I0204 10:33:02.561001 139910212208384 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.0048704147338867, loss=2.6120638847351074
I0204 10:33:48.341634 139910203815680 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.0137455463409424, loss=2.6603615283966064
I0204 10:34:34.753623 139910212208384 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.465575695037842, loss=3.039595603942871
I0204 10:35:20.725894 139910203815680 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.9815824031829834, loss=2.7091281414031982
I0204 10:35:25.945138 140107197974336 spec.py:321] Evaluating on the training split.
I0204 10:35:36.756848 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 10:36:03.286220 140107197974336 spec.py:349] Evaluating on the test split.
I0204 10:36:04.883694 140107197974336 submission_runner.py:408] Time since start: 69048.47s, 	Step: 139413, 	{'train/accuracy': 0.7711523175239563, 'train/loss': 1.000161051750183, 'validation/accuracy': 0.7072599530220032, 'validation/loss': 1.2786537408828735, 'validation/num_examples': 50000, 'test/accuracy': 0.5861999988555908, 'test/loss': 1.8938807249069214, 'test/num_examples': 10000, 'score': 63483.114550590515, 'total_duration': 69048.47295331955, 'accumulated_submission_time': 63483.114550590515, 'accumulated_eval_time': 5549.8006637096405, 'accumulated_logging_time': 7.670376300811768}
I0204 10:36:04.921648 139910212208384 logging_writer.py:48] [139413] accumulated_eval_time=5549.800664, accumulated_logging_time=7.670376, accumulated_submission_time=63483.114551, global_step=139413, preemption_count=0, score=63483.114551, test/accuracy=0.586200, test/loss=1.893881, test/num_examples=10000, total_duration=69048.472953, train/accuracy=0.771152, train/loss=1.000161, validation/accuracy=0.707260, validation/loss=1.278654, validation/num_examples=50000
I0204 10:36:39.580242 139910203815680 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.8771743774414062, loss=2.6178946495056152
I0204 10:37:25.642194 139910212208384 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.2349092960357666, loss=3.7563765048980713
I0204 10:38:11.695115 139910203815680 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.0857608318328857, loss=2.6440744400024414
I0204 10:38:58.139081 139910212208384 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.8573622703552246, loss=2.638432025909424
I0204 10:39:44.170893 139910203815680 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.0146853923797607, loss=3.3265578746795654
I0204 10:40:30.478818 139910212208384 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.5466859340667725, loss=2.5294156074523926
I0204 10:41:16.584522 139910203815680 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.396681785583496, loss=2.5546889305114746
I0204 10:42:02.995012 139910212208384 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.5512547492980957, loss=4.6413655281066895
I0204 10:42:48.990103 139910203815680 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.777369976043701, loss=2.5752217769622803
I0204 10:43:05.034571 140107197974336 spec.py:321] Evaluating on the training split.
I0204 10:43:15.710145 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 10:43:43.025953 140107197974336 spec.py:349] Evaluating on the test split.
I0204 10:43:44.612994 140107197974336 submission_runner.py:408] Time since start: 69508.20s, 	Step: 140336, 	{'train/accuracy': 0.7856835722923279, 'train/loss': 0.9613086581230164, 'validation/accuracy': 0.7094999551773071, 'validation/loss': 1.2880979776382446, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.8893526792526245, 'test/num_examples': 10000, 'score': 63903.165801763535, 'total_duration': 69508.20226073265, 'accumulated_submission_time': 63903.165801763535, 'accumulated_eval_time': 5589.37908911705, 'accumulated_logging_time': 7.717724561691284}
I0204 10:43:44.648493 139910212208384 logging_writer.py:48] [140336] accumulated_eval_time=5589.379089, accumulated_logging_time=7.717725, accumulated_submission_time=63903.165802, global_step=140336, preemption_count=0, score=63903.165802, test/accuracy=0.590900, test/loss=1.889353, test/num_examples=10000, total_duration=69508.202261, train/accuracy=0.785684, train/loss=0.961309, validation/accuracy=0.709500, validation/loss=1.288098, validation/num_examples=50000
I0204 10:44:10.169997 139910203815680 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.2745602130889893, loss=3.490657329559326
I0204 10:44:55.064574 139910212208384 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.0738348960876465, loss=2.687147617340088
I0204 10:45:41.219390 139910203815680 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.0289573669433594, loss=3.3115458488464355
I0204 10:46:27.684913 139910212208384 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.886761426925659, loss=3.459080219268799
I0204 10:47:13.812788 139910203815680 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.127164363861084, loss=2.657066822052002
I0204 10:47:59.791320 139910212208384 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.8316540718078613, loss=3.180962324142456
I0204 10:48:45.758588 139910203815680 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.0346603393554688, loss=2.6770687103271484
I0204 10:49:31.756206 139910212208384 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.180393934249878, loss=3.021592140197754
I0204 10:50:18.008912 139910203815680 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.012718677520752, loss=3.763303279876709
I0204 10:50:44.771371 140107197974336 spec.py:321] Evaluating on the training split.
I0204 10:50:55.536406 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 10:51:24.792270 140107197974336 spec.py:349] Evaluating on the test split.
I0204 10:51:26.389499 140107197974336 submission_runner.py:408] Time since start: 69969.98s, 	Step: 141260, 	{'train/accuracy': 0.7727343440055847, 'train/loss': 1.010166049003601, 'validation/accuracy': 0.7104399800300598, 'validation/loss': 1.2714622020721436, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8784040212631226, 'test/num_examples': 10000, 'score': 64323.22742891312, 'total_duration': 69969.97873592377, 'accumulated_submission_time': 64323.22742891312, 'accumulated_eval_time': 5630.997187137604, 'accumulated_logging_time': 7.762471675872803}
I0204 10:51:26.425210 139910212208384 logging_writer.py:48] [141260] accumulated_eval_time=5630.997187, accumulated_logging_time=7.762472, accumulated_submission_time=64323.227429, global_step=141260, preemption_count=0, score=64323.227429, test/accuracy=0.587800, test/loss=1.878404, test/num_examples=10000, total_duration=69969.978736, train/accuracy=0.772734, train/loss=1.010166, validation/accuracy=0.710440, validation/loss=1.271462, validation/num_examples=50000
I0204 10:51:42.537127 139910203815680 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.2843244075775146, loss=2.713834524154663
I0204 10:52:25.840994 139910212208384 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.5457639694213867, loss=4.198953628540039
I0204 10:53:11.681551 139910203815680 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.1773741245269775, loss=2.691405773162842
I0204 10:53:57.613629 139910212208384 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.4603335857391357, loss=4.519434928894043
I0204 10:54:43.931064 139910203815680 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.114694833755493, loss=4.394697666168213
I0204 10:55:29.857755 139910212208384 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.1281769275665283, loss=2.5850629806518555
I0204 10:56:15.881322 139910203815680 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.7251954078674316, loss=2.6437466144561768
I0204 10:57:01.762445 139910212208384 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.6418838500976562, loss=2.620983123779297
I0204 10:57:48.045289 139910203815680 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.651158571243286, loss=4.318502426147461
I0204 10:58:26.502207 140107197974336 spec.py:321] Evaluating on the training split.
I0204 10:58:37.351799 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 10:59:03.396839 140107197974336 spec.py:349] Evaluating on the test split.
I0204 10:59:04.993020 140107197974336 submission_runner.py:408] Time since start: 70428.58s, 	Step: 142185, 	{'train/accuracy': 0.7766211032867432, 'train/loss': 0.9818845391273499, 'validation/accuracy': 0.7114999890327454, 'validation/loss': 1.2591363191604614, 'validation/num_examples': 50000, 'test/accuracy': 0.5889000296592712, 'test/loss': 1.8770174980163574, 'test/num_examples': 10000, 'score': 64743.2424428463, 'total_duration': 70428.5822839737, 'accumulated_submission_time': 64743.2424428463, 'accumulated_eval_time': 5669.488003015518, 'accumulated_logging_time': 7.8078320026397705}
I0204 10:59:05.030410 139910212208384 logging_writer.py:48] [142185] accumulated_eval_time=5669.488003, accumulated_logging_time=7.807832, accumulated_submission_time=64743.242443, global_step=142185, preemption_count=0, score=64743.242443, test/accuracy=0.588900, test/loss=1.877017, test/num_examples=10000, total_duration=70428.582284, train/accuracy=0.776621, train/loss=0.981885, validation/accuracy=0.711500, validation/loss=1.259136, validation/num_examples=50000
I0204 10:59:11.309886 139910203815680 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.3507399559020996, loss=2.5713186264038086
I0204 10:59:52.576122 139910212208384 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.510044813156128, loss=2.6055562496185303
I0204 11:00:38.841561 139910203815680 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.1314175128936768, loss=3.011154890060425
I0204 11:01:25.050395 139910212208384 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.148561477661133, loss=2.6328837871551514
I0204 11:02:11.321055 139910203815680 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.5045807361602783, loss=2.9459047317504883
I0204 11:02:57.615514 139910212208384 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.204448938369751, loss=3.055511951446533
I0204 11:03:43.739356 139910203815680 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.953118324279785, loss=3.5589599609375
I0204 11:04:30.048634 139910212208384 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.103226900100708, loss=2.510819911956787
I0204 11:05:16.097586 139910203815680 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.084275960922241, loss=2.6040711402893066
I0204 11:06:02.177263 139910212208384 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.468886137008667, loss=2.589512348175049
I0204 11:06:05.304213 140107197974336 spec.py:321] Evaluating on the training split.
I0204 11:06:15.996496 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 11:06:42.076948 140107197974336 spec.py:349] Evaluating on the test split.
I0204 11:06:43.671347 140107197974336 submission_runner.py:408] Time since start: 70887.26s, 	Step: 143108, 	{'train/accuracy': 0.7864062190055847, 'train/loss': 0.9273659586906433, 'validation/accuracy': 0.7152000069618225, 'validation/loss': 1.233481764793396, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.8319133520126343, 'test/num_examples': 10000, 'score': 65163.4554746151, 'total_duration': 70887.26061153412, 'accumulated_submission_time': 65163.4554746151, 'accumulated_eval_time': 5707.855123996735, 'accumulated_logging_time': 7.854290246963501}
I0204 11:06:43.712244 139910203815680 logging_writer.py:48] [143108] accumulated_eval_time=5707.855124, accumulated_logging_time=7.854290, accumulated_submission_time=65163.455475, global_step=143108, preemption_count=0, score=65163.455475, test/accuracy=0.593200, test/loss=1.831913, test/num_examples=10000, total_duration=70887.260612, train/accuracy=0.786406, train/loss=0.927366, validation/accuracy=0.715200, validation/loss=1.233482, validation/num_examples=50000
I0204 11:07:21.075174 139910212208384 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.5616347789764404, loss=2.635974645614624
I0204 11:08:07.170734 139910203815680 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.2612264156341553, loss=3.0624310970306396
I0204 11:08:53.432512 139910212208384 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.8205792903900146, loss=3.0656373500823975
I0204 11:09:39.827250 139910203815680 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.5123496055603027, loss=2.604965925216675
I0204 11:10:25.842340 139910212208384 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.27451491355896, loss=2.629816770553589
I0204 11:11:11.931769 139910203815680 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.2092537879943848, loss=2.637211799621582
I0204 11:11:57.991793 139910212208384 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.002876043319702, loss=2.9798436164855957
I0204 11:12:44.344237 139910203815680 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.161024570465088, loss=3.9316093921661377
I0204 11:13:30.791495 139910212208384 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.027106523513794, loss=4.1071929931640625
I0204 11:13:43.898468 140107197974336 spec.py:321] Evaluating on the training split.
I0204 11:13:54.598788 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 11:14:21.338622 140107197974336 spec.py:349] Evaluating on the test split.
I0204 11:14:22.935272 140107197974336 submission_runner.py:408] Time since start: 71346.52s, 	Step: 144030, 	{'train/accuracy': 0.7824804782867432, 'train/loss': 0.9607452154159546, 'validation/accuracy': 0.7184999585151672, 'validation/loss': 1.2336714267730713, 'validation/num_examples': 50000, 'test/accuracy': 0.593500018119812, 'test/loss': 1.8458307981491089, 'test/num_examples': 10000, 'score': 65583.5791592598, 'total_duration': 71346.5245103836, 'accumulated_submission_time': 65583.5791592598, 'accumulated_eval_time': 5746.891888856888, 'accumulated_logging_time': 7.905426979064941}
I0204 11:14:22.977061 139910203815680 logging_writer.py:48] [144030] accumulated_eval_time=5746.891889, accumulated_logging_time=7.905427, accumulated_submission_time=65583.579159, global_step=144030, preemption_count=0, score=65583.579159, test/accuracy=0.593500, test/loss=1.845831, test/num_examples=10000, total_duration=71346.524510, train/accuracy=0.782480, train/loss=0.960745, validation/accuracy=0.718500, validation/loss=1.233671, validation/num_examples=50000
I0204 11:14:50.856874 139910212208384 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.2681374549865723, loss=3.2399661540985107
I0204 11:15:36.118348 139910203815680 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.4239096641540527, loss=2.623011589050293
I0204 11:16:22.506741 139910212208384 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.057382345199585, loss=2.54219388961792
I0204 11:17:09.105779 139910203815680 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.908435344696045, loss=4.189119338989258
I0204 11:17:55.115326 139910212208384 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.558511972427368, loss=2.5721640586853027
I0204 11:18:41.400337 139910203815680 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.7162556648254395, loss=3.7542850971221924
I0204 11:19:27.774518 139910212208384 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.162414789199829, loss=3.3982460498809814
I0204 11:20:14.263782 139910203815680 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.678211212158203, loss=4.664361953735352
I0204 11:21:00.545370 139910212208384 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.4421191215515137, loss=2.5530877113342285
I0204 11:21:23.331084 140107197974336 spec.py:321] Evaluating on the training split.
I0204 11:21:34.018642 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 11:22:01.542879 140107197974336 spec.py:349] Evaluating on the test split.
I0204 11:22:03.131276 140107197974336 submission_runner.py:408] Time since start: 71806.72s, 	Step: 144951, 	{'train/accuracy': 0.7843359112739563, 'train/loss': 0.9414471983909607, 'validation/accuracy': 0.7197799682617188, 'validation/loss': 1.2216196060180664, 'validation/num_examples': 50000, 'test/accuracy': 0.5945000052452087, 'test/loss': 1.8343600034713745, 'test/num_examples': 10000, 'score': 66003.87094473839, 'total_duration': 71806.72054195404, 'accumulated_submission_time': 66003.87094473839, 'accumulated_eval_time': 5786.692119598389, 'accumulated_logging_time': 7.9578657150268555}
I0204 11:22:03.169097 139910203815680 logging_writer.py:48] [144951] accumulated_eval_time=5786.692120, accumulated_logging_time=7.957866, accumulated_submission_time=66003.870945, global_step=144951, preemption_count=0, score=66003.870945, test/accuracy=0.594500, test/loss=1.834360, test/num_examples=10000, total_duration=71806.720542, train/accuracy=0.784336, train/loss=0.941447, validation/accuracy=0.719780, validation/loss=1.221620, validation/num_examples=50000
I0204 11:22:22.816445 139910212208384 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.4394068717956543, loss=4.666163444519043
I0204 11:23:06.572802 139910203815680 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.3275375366210938, loss=2.569713830947876
I0204 11:23:52.863740 139910212208384 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.498295307159424, loss=2.5587475299835205
I0204 11:24:39.304582 139910203815680 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.2347187995910645, loss=2.7790417671203613
I0204 11:25:25.563417 139910212208384 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.2468934059143066, loss=2.5433835983276367
I0204 11:26:11.811348 139910203815680 logging_writer.py:48] [145500] global_step=145500, grad_norm=16.97615623474121, loss=3.3755133152008057
I0204 11:26:58.040658 139910212208384 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.567598581314087, loss=3.8357832431793213
I0204 11:27:44.126771 139910203815680 logging_writer.py:48] [145700] global_step=145700, grad_norm=4.139480113983154, loss=4.679107666015625
I0204 11:28:30.410973 139910212208384 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.5115256309509277, loss=2.5984578132629395
I0204 11:29:03.314493 140107197974336 spec.py:321] Evaluating on the training split.
I0204 11:29:14.087953 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 11:29:40.479785 140107197974336 spec.py:349] Evaluating on the test split.
I0204 11:29:42.088750 140107197974336 submission_runner.py:408] Time since start: 72265.68s, 	Step: 145873, 	{'train/accuracy': 0.7897070050239563, 'train/loss': 0.930291473865509, 'validation/accuracy': 0.7209399938583374, 'validation/loss': 1.2243276834487915, 'validation/num_examples': 50000, 'test/accuracy': 0.5939000248908997, 'test/loss': 1.8272168636322021, 'test/num_examples': 10000, 'score': 66423.95629882812, 'total_duration': 72265.67800307274, 'accumulated_submission_time': 66423.95629882812, 'accumulated_eval_time': 5825.466367721558, 'accumulated_logging_time': 8.004544019699097}
I0204 11:29:42.129105 139910203815680 logging_writer.py:48] [145873] accumulated_eval_time=5825.466368, accumulated_logging_time=8.004544, accumulated_submission_time=66423.956299, global_step=145873, preemption_count=0, score=66423.956299, test/accuracy=0.593900, test/loss=1.827217, test/num_examples=10000, total_duration=72265.678003, train/accuracy=0.789707, train/loss=0.930291, validation/accuracy=0.720940, validation/loss=1.224328, validation/num_examples=50000
I0204 11:29:53.139731 139910212208384 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.273341178894043, loss=2.6670331954956055
I0204 11:30:35.354593 139910203815680 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.4623782634735107, loss=2.723381519317627
I0204 11:31:21.257582 139910212208384 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.5567688941955566, loss=2.9308009147644043
I0204 11:32:07.495923 139910203815680 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.98068904876709, loss=3.5335440635681152
I0204 11:32:53.443091 139910212208384 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.1344895362854004, loss=2.8582334518432617
I0204 11:33:39.638423 139910203815680 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.432796001434326, loss=3.813849449157715
I0204 11:34:26.081784 139910212208384 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.5741751194000244, loss=4.3997039794921875
I0204 11:35:12.602454 139910203815680 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.2750978469848633, loss=2.4491748809814453
I0204 11:35:58.590814 139910212208384 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.3292720317840576, loss=2.5226950645446777
I0204 11:36:42.128386 140107197974336 spec.py:321] Evaluating on the training split.
I0204 11:36:52.714325 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 11:37:21.199408 140107197974336 spec.py:349] Evaluating on the test split.
I0204 11:37:22.794476 140107197974336 submission_runner.py:408] Time since start: 72726.38s, 	Step: 146796, 	{'train/accuracy': 0.7828710675239563, 'train/loss': 0.94913649559021, 'validation/accuracy': 0.7192999720573425, 'validation/loss': 1.2191599607467651, 'validation/num_examples': 50000, 'test/accuracy': 0.5915000438690186, 'test/loss': 1.8373186588287354, 'test/num_examples': 10000, 'score': 66843.89471817017, 'total_duration': 72726.38371825218, 'accumulated_submission_time': 66843.89471817017, 'accumulated_eval_time': 5866.13242316246, 'accumulated_logging_time': 8.054444074630737}
I0204 11:37:22.836969 139910203815680 logging_writer.py:48] [146796] accumulated_eval_time=5866.132423, accumulated_logging_time=8.054444, accumulated_submission_time=66843.894718, global_step=146796, preemption_count=0, score=66843.894718, test/accuracy=0.591500, test/loss=1.837319, test/num_examples=10000, total_duration=72726.383718, train/accuracy=0.782871, train/loss=0.949136, validation/accuracy=0.719300, validation/loss=1.219160, validation/num_examples=50000
I0204 11:37:24.803339 139910212208384 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.2215917110443115, loss=2.575913190841675
I0204 11:38:05.755312 139910203815680 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.1960017681121826, loss=2.5126535892486572
I0204 11:38:51.719248 139910212208384 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.923788070678711, loss=3.4774105548858643
I0204 11:39:38.322441 139910203815680 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.6987416744232178, loss=4.629934787750244
I0204 11:40:24.733061 139910212208384 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.2257792949676514, loss=3.409083604812622
I0204 11:41:10.915624 139910203815680 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.2240753173828125, loss=2.634343147277832
I0204 11:41:57.374326 139910212208384 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.0484700202941895, loss=2.679732322692871
I0204 11:42:43.662123 139910203815680 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.5052242279052734, loss=2.563302516937256
I0204 11:43:29.835880 139910212208384 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.4986023902893066, loss=2.639145851135254
I0204 11:44:16.292527 139910203815680 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.726275682449341, loss=2.603400707244873
I0204 11:44:22.910328 140107197974336 spec.py:321] Evaluating on the training split.
I0204 11:44:33.945192 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 11:45:02.348649 140107197974336 spec.py:349] Evaluating on the test split.
I0204 11:45:03.947612 140107197974336 submission_runner.py:408] Time since start: 73187.54s, 	Step: 147716, 	{'train/accuracy': 0.7864453196525574, 'train/loss': 0.94319087266922, 'validation/accuracy': 0.7235599756240845, 'validation/loss': 1.2195154428482056, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.8374955654144287, 'test/num_examples': 10000, 'score': 67263.90523648262, 'total_duration': 73187.53686141968, 'accumulated_submission_time': 67263.90523648262, 'accumulated_eval_time': 5907.169671535492, 'accumulated_logging_time': 8.107472896575928}
I0204 11:45:03.986485 139910212208384 logging_writer.py:48] [147716] accumulated_eval_time=5907.169672, accumulated_logging_time=8.107473, accumulated_submission_time=67263.905236, global_step=147716, preemption_count=0, score=67263.905236, test/accuracy=0.596300, test/loss=1.837496, test/num_examples=10000, total_duration=73187.536861, train/accuracy=0.786445, train/loss=0.943191, validation/accuracy=0.723560, validation/loss=1.219515, validation/num_examples=50000
I0204 11:45:37.493648 139910203815680 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.416881561279297, loss=2.5021910667419434
I0204 11:46:23.192475 139910212208384 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.2679624557495117, loss=2.7066564559936523
I0204 11:47:09.663015 139910203815680 logging_writer.py:48] [148000] global_step=148000, grad_norm=4.0865302085876465, loss=4.513907432556152
I0204 11:47:56.014095 139910212208384 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.348944664001465, loss=2.510507106781006
I0204 11:48:41.987485 139910203815680 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.361572742462158, loss=2.4165282249450684
I0204 11:49:28.344283 139910212208384 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.9406046867370605, loss=3.830033302307129
I0204 11:50:14.640189 139910203815680 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.242246627807617, loss=2.7178544998168945
I0204 11:51:00.519494 139910212208384 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.525959014892578, loss=4.5615434646606445
I0204 11:51:46.738725 139910203815680 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.795742988586426, loss=4.5897135734558105
I0204 11:52:04.272052 140107197974336 spec.py:321] Evaluating on the training split.
I0204 11:52:15.105434 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 11:52:44.100657 140107197974336 spec.py:349] Evaluating on the test split.
I0204 11:52:45.700094 140107197974336 submission_runner.py:408] Time since start: 73649.29s, 	Step: 148639, 	{'train/accuracy': 0.7929491996765137, 'train/loss': 0.936110258102417, 'validation/accuracy': 0.7234199643135071, 'validation/loss': 1.2337615489959717, 'validation/num_examples': 50000, 'test/accuracy': 0.6018000245094299, 'test/loss': 1.8382083177566528, 'test/num_examples': 10000, 'score': 67684.12996077538, 'total_duration': 73649.2893576622, 'accumulated_submission_time': 67684.12996077538, 'accumulated_eval_time': 5948.59770822525, 'accumulated_logging_time': 8.155084133148193}
I0204 11:52:45.737232 139910212208384 logging_writer.py:48] [148639] accumulated_eval_time=5948.597708, accumulated_logging_time=8.155084, accumulated_submission_time=67684.129961, global_step=148639, preemption_count=0, score=67684.129961, test/accuracy=0.601800, test/loss=1.838208, test/num_examples=10000, total_duration=73649.289358, train/accuracy=0.792949, train/loss=0.936110, validation/accuracy=0.723420, validation/loss=1.233762, validation/num_examples=50000
I0204 11:53:10.088858 139910203815680 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.337742328643799, loss=3.191009759902954
I0204 11:53:54.570694 139910212208384 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.6938588619232178, loss=2.6253015995025635
I0204 11:54:41.188687 139910203815680 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.4567811489105225, loss=2.994945526123047
I0204 11:55:27.796046 139910212208384 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.5096397399902344, loss=2.6848645210266113
I0204 11:56:13.935478 139910203815680 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.1196775436401367, loss=3.5566694736480713
I0204 11:57:00.021589 139910212208384 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.745530128479004, loss=2.525322914123535
I0204 11:57:46.497475 139910203815680 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.4963467121124268, loss=2.479681968688965
I0204 11:58:32.859456 139910212208384 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.4701061248779297, loss=3.9106571674346924
I0204 11:59:19.130542 139910203815680 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.371556282043457, loss=2.466226100921631
I0204 11:59:45.959462 140107197974336 spec.py:321] Evaluating on the training split.
I0204 11:59:56.994269 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 12:00:26.146894 140107197974336 spec.py:349] Evaluating on the test split.
I0204 12:00:27.752157 140107197974336 submission_runner.py:408] Time since start: 74111.34s, 	Step: 149560, 	{'train/accuracy': 0.8044726252555847, 'train/loss': 0.8590974807739258, 'validation/accuracy': 0.725600004196167, 'validation/loss': 1.1935111284255981, 'validation/num_examples': 50000, 'test/accuracy': 0.601900041103363, 'test/loss': 1.7990987300872803, 'test/num_examples': 10000, 'score': 68104.28977417946, 'total_duration': 74111.3414068222, 'accumulated_submission_time': 68104.28977417946, 'accumulated_eval_time': 5990.390378952026, 'accumulated_logging_time': 8.203400373458862}
I0204 12:00:27.799337 139910212208384 logging_writer.py:48] [149560] accumulated_eval_time=5990.390379, accumulated_logging_time=8.203400, accumulated_submission_time=68104.289774, global_step=149560, preemption_count=0, score=68104.289774, test/accuracy=0.601900, test/loss=1.799099, test/num_examples=10000, total_duration=74111.341407, train/accuracy=0.804473, train/loss=0.859097, validation/accuracy=0.725600, validation/loss=1.193511, validation/num_examples=50000
I0204 12:00:43.900047 139910203815680 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.576117992401123, loss=3.4809436798095703
I0204 12:01:27.102586 139910212208384 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.1734650135040283, loss=3.6741743087768555
I0204 12:02:13.284950 139910203815680 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.3073017597198486, loss=3.888667583465576
I0204 12:02:59.670361 139910212208384 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.6877925395965576, loss=4.397016525268555
I0204 12:03:45.999058 139910203815680 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.647722005844116, loss=3.057586669921875
I0204 12:04:32.053314 139910212208384 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.510006904602051, loss=2.539022445678711
I0204 12:05:18.554774 139910203815680 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.5087966918945312, loss=2.5716190338134766
I0204 12:06:05.031710 139910212208384 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.086505889892578, loss=4.304239749908447
I0204 12:06:51.111522 139910203815680 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.8449926376342773, loss=2.521632432937622
I0204 12:07:27.866209 140107197974336 spec.py:321] Evaluating on the training split.
I0204 12:07:38.736718 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 12:08:04.761387 140107197974336 spec.py:349] Evaluating on the test split.
I0204 12:08:06.354515 140107197974336 submission_runner.py:408] Time since start: 74569.94s, 	Step: 150481, 	{'train/accuracy': 0.7959179282188416, 'train/loss': 0.9110642075538635, 'validation/accuracy': 0.7285799980163574, 'validation/loss': 1.19636070728302, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.7990301847457886, 'test/num_examples': 10000, 'score': 68524.2951323986, 'total_duration': 74569.94378137589, 'accumulated_submission_time': 68524.2951323986, 'accumulated_eval_time': 6028.878688812256, 'accumulated_logging_time': 8.260981321334839}
I0204 12:08:06.391982 139910212208384 logging_writer.py:48] [150481] accumulated_eval_time=6028.878689, accumulated_logging_time=8.260981, accumulated_submission_time=68524.295132, global_step=150481, preemption_count=0, score=68524.295132, test/accuracy=0.603800, test/loss=1.799030, test/num_examples=10000, total_duration=74569.943781, train/accuracy=0.795918, train/loss=0.911064, validation/accuracy=0.728580, validation/loss=1.196361, validation/num_examples=50000
I0204 12:08:14.256743 139910203815680 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.781700611114502, loss=2.7386603355407715
I0204 12:08:55.757883 139910212208384 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.883026123046875, loss=2.52789306640625
I0204 12:09:41.633241 139910203815680 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.307695388793945, loss=2.629702568054199
I0204 12:10:27.955410 139910212208384 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.6450212001800537, loss=4.027121067047119
I0204 12:11:14.082306 139910203815680 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.734628677368164, loss=2.586411476135254
I0204 12:12:00.143260 139910212208384 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.5716428756713867, loss=3.6606526374816895
I0204 12:12:46.599686 139910203815680 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.5580615997314453, loss=2.4842898845672607
I0204 12:13:32.959983 139910212208384 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.675210952758789, loss=2.562957286834717
I0204 12:14:19.330830 139910203815680 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.1948442459106445, loss=4.485323905944824
I0204 12:15:06.259454 139910212208384 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.497720718383789, loss=2.468146324157715
I0204 12:15:06.367309 140107197974336 spec.py:321] Evaluating on the training split.
I0204 12:15:16.826233 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 12:15:44.480768 140107197974336 spec.py:349] Evaluating on the test split.
I0204 12:15:46.081016 140107197974336 submission_runner.py:408] Time since start: 75029.67s, 	Step: 151402, 	{'train/accuracy': 0.7992382645606995, 'train/loss': 0.8694639205932617, 'validation/accuracy': 0.7313199639320374, 'validation/loss': 1.1657003164291382, 'validation/num_examples': 50000, 'test/accuracy': 0.6051000356674194, 'test/loss': 1.7615711688995361, 'test/num_examples': 10000, 'score': 68944.20941090584, 'total_duration': 75029.67028093338, 'accumulated_submission_time': 68944.20941090584, 'accumulated_eval_time': 6068.592380285263, 'accumulated_logging_time': 8.307986736297607}
I0204 12:15:46.121705 139910203815680 logging_writer.py:48] [151402] accumulated_eval_time=6068.592380, accumulated_logging_time=8.307987, accumulated_submission_time=68944.209411, global_step=151402, preemption_count=0, score=68944.209411, test/accuracy=0.605100, test/loss=1.761571, test/num_examples=10000, total_duration=75029.670281, train/accuracy=0.799238, train/loss=0.869464, validation/accuracy=0.731320, validation/loss=1.165700, validation/num_examples=50000
I0204 12:16:26.254541 139910212208384 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.5434226989746094, loss=2.591353416442871
I0204 12:17:12.197097 139910203815680 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.406172513961792, loss=2.4353907108306885
I0204 12:17:58.508903 139910212208384 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.1656341552734375, loss=4.481307029724121
I0204 12:18:44.858060 139910203815680 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.4318618774414062, loss=3.666869640350342
I0204 12:19:31.171543 139910212208384 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.361538410186768, loss=4.4092607498168945
I0204 12:20:17.623926 139910203815680 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.7067301273345947, loss=3.7601888179779053
I0204 12:21:03.830268 139910212208384 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.562112808227539, loss=3.319478988647461
I0204 12:21:49.925417 139910203815680 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.8575525283813477, loss=2.458712577819824
I0204 12:22:36.298761 139910212208384 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.4191372394561768, loss=3.408076524734497
I0204 12:22:46.134545 140107197974336 spec.py:321] Evaluating on the training split.
I0204 12:22:56.808027 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 12:23:25.509922 140107197974336 spec.py:349] Evaluating on the test split.
I0204 12:23:27.111662 140107197974336 submission_runner.py:408] Time since start: 75490.70s, 	Step: 152323, 	{'train/accuracy': 0.8080468773841858, 'train/loss': 0.8454297184944153, 'validation/accuracy': 0.7318399548530579, 'validation/loss': 1.1674693822860718, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.7557659149169922, 'test/num_examples': 10000, 'score': 69364.16141724586, 'total_duration': 75490.700922966, 'accumulated_submission_time': 69364.16141724586, 'accumulated_eval_time': 6109.569490194321, 'accumulated_logging_time': 8.357463836669922}
I0204 12:23:27.149548 139910203815680 logging_writer.py:48] [152323] accumulated_eval_time=6109.569490, accumulated_logging_time=8.357464, accumulated_submission_time=69364.161417, global_step=152323, preemption_count=0, score=69364.161417, test/accuracy=0.609700, test/loss=1.755766, test/num_examples=10000, total_duration=75490.700923, train/accuracy=0.808047, train/loss=0.845430, validation/accuracy=0.731840, validation/loss=1.167469, validation/num_examples=50000
I0204 12:23:57.786641 139910212208384 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.651123046875, loss=2.461793899536133
I0204 12:24:43.252655 139910203815680 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.6844892501831055, loss=2.5967302322387695
I0204 12:25:29.605994 139910212208384 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.687438488006592, loss=2.5249991416931152
I0204 12:26:16.210418 139910203815680 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.82843017578125, loss=3.9863052368164062
I0204 12:27:02.266699 139910212208384 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.7282426357269287, loss=3.1669909954071045
I0204 12:27:48.395930 139910203815680 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.4867634773254395, loss=3.0323054790496826
I0204 12:28:34.553010 139910212208384 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.097878456115723, loss=2.515333414077759
I0204 12:29:20.791057 139910203815680 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.350246906280518, loss=2.5232763290405273
I0204 12:30:06.922407 139910212208384 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.824251174926758, loss=2.5797982215881348
I0204 12:30:27.359995 140107197974336 spec.py:321] Evaluating on the training split.
I0204 12:30:38.120147 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 12:31:04.875966 140107197974336 spec.py:349] Evaluating on the test split.
I0204 12:31:06.473753 140107197974336 submission_runner.py:408] Time since start: 75950.06s, 	Step: 153246, 	{'train/accuracy': 0.802539050579071, 'train/loss': 0.863842248916626, 'validation/accuracy': 0.7321000099182129, 'validation/loss': 1.1585302352905273, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.7566510438919067, 'test/num_examples': 10000, 'score': 69784.3099322319, 'total_duration': 75950.06301856041, 'accumulated_submission_time': 69784.3099322319, 'accumulated_eval_time': 6148.683254241943, 'accumulated_logging_time': 8.404613018035889}
I0204 12:31:06.515341 139910203815680 logging_writer.py:48] [153246] accumulated_eval_time=6148.683254, accumulated_logging_time=8.404613, accumulated_submission_time=69784.309932, global_step=153246, preemption_count=0, score=69784.309932, test/accuracy=0.614300, test/loss=1.756651, test/num_examples=10000, total_duration=75950.063019, train/accuracy=0.802539, train/loss=0.863842, validation/accuracy=0.732100, validation/loss=1.158530, validation/num_examples=50000
I0204 12:31:28.125091 139910212208384 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.319852828979492, loss=4.164669036865234
I0204 12:32:12.085797 139910203815680 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.556941032409668, loss=3.2073404788970947
I0204 12:32:58.109619 139910212208384 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.623169183731079, loss=3.2735960483551025
I0204 12:33:44.600275 139910203815680 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.886551856994629, loss=2.8093245029449463
I0204 12:34:31.073811 139910212208384 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.093762397766113, loss=2.6289515495300293
I0204 12:35:17.189216 139910203815680 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.207446575164795, loss=3.9602155685424805
I0204 12:36:03.758116 139910212208384 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.756599187850952, loss=3.3000071048736572
I0204 12:36:49.831926 139910203815680 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.5440878868103027, loss=3.026780843734741
I0204 12:37:36.200309 139910212208384 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.778162717819214, loss=2.9884002208709717
I0204 12:38:06.570262 140107197974336 spec.py:321] Evaluating on the training split.
I0204 12:38:17.444541 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 12:38:43.144813 140107197974336 spec.py:349] Evaluating on the test split.
I0204 12:38:44.741308 140107197974336 submission_runner.py:408] Time since start: 76408.33s, 	Step: 154167, 	{'train/accuracy': 0.80726557970047, 'train/loss': 0.8443677425384521, 'validation/accuracy': 0.7341799736022949, 'validation/loss': 1.159018874168396, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.755558729171753, 'test/num_examples': 10000, 'score': 70204.3026239872, 'total_duration': 76408.33057045937, 'accumulated_submission_time': 70204.3026239872, 'accumulated_eval_time': 6186.854301929474, 'accumulated_logging_time': 8.456630945205688}
I0204 12:38:44.781008 139910203815680 logging_writer.py:48] [154167] accumulated_eval_time=6186.854302, accumulated_logging_time=8.456631, accumulated_submission_time=70204.302624, global_step=154167, preemption_count=0, score=70204.302624, test/accuracy=0.614300, test/loss=1.755559, test/num_examples=10000, total_duration=76408.330570, train/accuracy=0.807266, train/loss=0.844368, validation/accuracy=0.734180, validation/loss=1.159019, validation/num_examples=50000
I0204 12:38:58.244758 139910212208384 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.967041254043579, loss=2.4377260208129883
I0204 12:39:40.906764 139910203815680 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.180658340454102, loss=4.517824172973633
I0204 12:40:27.107855 139910212208384 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.031118392944336, loss=2.430669069290161
I0204 12:41:13.770091 139910203815680 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.891477346420288, loss=2.4157536029815674
I0204 12:41:59.818846 139910212208384 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.2639265060424805, loss=2.5490033626556396
I0204 12:42:45.794197 139910203815680 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.101655960083008, loss=2.4557981491088867
I0204 12:43:31.934211 139910212208384 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.8869383335113525, loss=2.523026943206787
I0204 12:44:18.066334 139910203815680 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.766147613525391, loss=4.518679618835449
I0204 12:45:04.508837 139910212208384 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.851186752319336, loss=2.6156210899353027
I0204 12:45:44.902820 140107197974336 spec.py:321] Evaluating on the training split.
I0204 12:45:55.570024 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 12:46:21.104740 140107197974336 spec.py:349] Evaluating on the test split.
I0204 12:46:22.704144 140107197974336 submission_runner.py:408] Time since start: 76866.29s, 	Step: 155089, 	{'train/accuracy': 0.8079296946525574, 'train/loss': 0.8516631722450256, 'validation/accuracy': 0.7345199584960938, 'validation/loss': 1.1670846939086914, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.779624342918396, 'test/num_examples': 10000, 'score': 70624.36257171631, 'total_duration': 76866.29339528084, 'accumulated_submission_time': 70624.36257171631, 'accumulated_eval_time': 6224.655606746674, 'accumulated_logging_time': 8.505312204360962}
I0204 12:46:22.746086 139910203815680 logging_writer.py:48] [155089] accumulated_eval_time=6224.655607, accumulated_logging_time=8.505312, accumulated_submission_time=70624.362572, global_step=155089, preemption_count=0, score=70624.362572, test/accuracy=0.610500, test/loss=1.779624, test/num_examples=10000, total_duration=76866.293395, train/accuracy=0.807930, train/loss=0.851663, validation/accuracy=0.734520, validation/loss=1.167085, validation/num_examples=50000
I0204 12:46:27.717437 139910212208384 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.206554889678955, loss=2.3642494678497314
I0204 12:47:09.032768 139910203815680 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.914468765258789, loss=3.679598331451416
I0204 12:47:54.783555 139910212208384 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.871647357940674, loss=2.527724266052246
I0204 12:48:41.173211 139910203815680 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.1329827308654785, loss=2.4674806594848633
I0204 12:49:27.749137 139910212208384 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.9300224781036377, loss=2.4042162895202637
I0204 12:50:14.236824 139910203815680 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.247539043426514, loss=2.434760093688965
I0204 12:51:00.430731 139910212208384 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.980225086212158, loss=2.4443106651306152
I0204 12:51:46.611837 139910203815680 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.9834203720092773, loss=2.355336904525757
I0204 12:52:32.829769 139910212208384 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.8296289443969727, loss=3.180058717727661
I0204 12:53:19.220641 139910203815680 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.8702073097229004, loss=3.8981668949127197
I0204 12:53:22.966578 140107197974336 spec.py:321] Evaluating on the training split.
I0204 12:53:33.644176 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 12:54:01.467195 140107197974336 spec.py:349] Evaluating on the test split.
I0204 12:54:03.065823 140107197974336 submission_runner.py:408] Time since start: 77326.66s, 	Step: 156010, 	{'train/accuracy': 0.8057616949081421, 'train/loss': 0.8517761826515198, 'validation/accuracy': 0.7357400059700012, 'validation/loss': 1.147667646408081, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.7380512952804565, 'test/num_examples': 10000, 'score': 71044.26182794571, 'total_duration': 77326.6550860405, 'accumulated_submission_time': 71044.26182794571, 'accumulated_eval_time': 6264.754841089249, 'accumulated_logging_time': 8.815968751907349}
I0204 12:54:03.108561 139910212208384 logging_writer.py:48] [156010] accumulated_eval_time=6264.754841, accumulated_logging_time=8.815969, accumulated_submission_time=71044.261828, global_step=156010, preemption_count=0, score=71044.261828, test/accuracy=0.613500, test/loss=1.738051, test/num_examples=10000, total_duration=77326.655086, train/accuracy=0.805762, train/loss=0.851776, validation/accuracy=0.735740, validation/loss=1.147668, validation/num_examples=50000
I0204 12:54:39.472259 139910203815680 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.878343343734741, loss=2.6918303966522217
I0204 12:55:25.500652 139910212208384 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.067638397216797, loss=3.052269458770752
I0204 12:56:12.037851 139910203815680 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.8382909297943115, loss=2.318488359451294
I0204 12:56:58.564477 139910212208384 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.944591760635376, loss=2.441985845565796
I0204 12:57:44.500509 139910203815680 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.714684247970581, loss=2.519486427307129
I0204 12:58:30.626734 139910212208384 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.238439559936523, loss=2.3939032554626465
I0204 12:59:16.893581 139910203815680 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.80282735824585, loss=4.498111724853516
I0204 13:00:02.713152 139910212208384 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.2042694091796875, loss=3.5705173015594482
I0204 13:00:48.876536 139910203815680 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.6863677501678467, loss=2.3371939659118652
I0204 13:01:03.278517 140107197974336 spec.py:321] Evaluating on the training split.
I0204 13:01:14.158091 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 13:01:41.257243 140107197974336 spec.py:349] Evaluating on the test split.
I0204 13:01:42.850930 140107197974336 submission_runner.py:408] Time since start: 77786.44s, 	Step: 156933, 	{'train/accuracy': 0.8101562261581421, 'train/loss': 0.8456878662109375, 'validation/accuracy': 0.7382000088691711, 'validation/loss': 1.148350715637207, 'validation/num_examples': 50000, 'test/accuracy': 0.6139000058174133, 'test/loss': 1.7471004724502563, 'test/num_examples': 10000, 'score': 71464.36837172508, 'total_duration': 77786.44018650055, 'accumulated_submission_time': 71464.36837172508, 'accumulated_eval_time': 6304.327245473862, 'accumulated_logging_time': 8.869405031204224}
I0204 13:01:42.891946 139910212208384 logging_writer.py:48] [156933] accumulated_eval_time=6304.327245, accumulated_logging_time=8.869405, accumulated_submission_time=71464.368372, global_step=156933, preemption_count=0, score=71464.368372, test/accuracy=0.613900, test/loss=1.747100, test/num_examples=10000, total_duration=77786.440187, train/accuracy=0.810156, train/loss=0.845688, validation/accuracy=0.738200, validation/loss=1.148351, validation/num_examples=50000
I0204 13:02:09.631321 139910203815680 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.094797134399414, loss=2.409364938735962
I0204 13:02:54.434275 139910212208384 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.8405401706695557, loss=3.0972249507904053
I0204 13:03:40.608191 139910203815680 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.949942111968994, loss=3.3404877185821533
I0204 13:04:26.969357 139910212208384 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.128429412841797, loss=2.44460129737854
I0204 13:05:13.176766 139910203815680 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.029702663421631, loss=2.326362371444702
I0204 13:05:59.332302 139910212208384 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.8629941940307617, loss=2.3469622135162354
I0204 13:06:45.934040 139910203815680 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.457912445068359, loss=2.812072992324829
I0204 13:07:32.546074 139910212208384 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.148536205291748, loss=2.4019484519958496
I0204 13:08:18.741751 139910203815680 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.310487270355225, loss=2.443789482116699
I0204 13:08:42.924694 140107197974336 spec.py:321] Evaluating on the training split.
I0204 13:08:53.752267 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 13:09:22.182490 140107197974336 spec.py:349] Evaluating on the test split.
I0204 13:09:23.782069 140107197974336 submission_runner.py:408] Time since start: 78247.37s, 	Step: 157854, 	{'train/accuracy': 0.8182226419448853, 'train/loss': 0.813230574131012, 'validation/accuracy': 0.7407799959182739, 'validation/loss': 1.141973853111267, 'validation/num_examples': 50000, 'test/accuracy': 0.6210000514984131, 'test/loss': 1.7272372245788574, 'test/num_examples': 10000, 'score': 71884.33867025375, 'total_duration': 78247.37133145332, 'accumulated_submission_time': 71884.33867025375, 'accumulated_eval_time': 6345.1846034526825, 'accumulated_logging_time': 8.920923233032227}
I0204 13:09:23.821949 139910212208384 logging_writer.py:48] [157854] accumulated_eval_time=6345.184603, accumulated_logging_time=8.920923, accumulated_submission_time=71884.338670, global_step=157854, preemption_count=0, score=71884.338670, test/accuracy=0.621000, test/loss=1.727237, test/num_examples=10000, total_duration=78247.371331, train/accuracy=0.818223, train/loss=0.813231, validation/accuracy=0.740780, validation/loss=1.141974, validation/num_examples=50000
I0204 13:09:42.288328 139910203815680 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.573128700256348, loss=4.432934761047363
I0204 13:10:26.175961 139910212208384 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.593959331512451, loss=3.093471050262451
I0204 13:11:12.476726 139910203815680 logging_writer.py:48] [158100] global_step=158100, grad_norm=3.9081709384918213, loss=2.5719892978668213
I0204 13:11:58.678800 139910212208384 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.31017541885376, loss=3.493957996368408
I0204 13:12:44.983960 139910203815680 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.9975175857543945, loss=3.4468424320220947
I0204 13:13:31.410043 139910212208384 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.9723689556121826, loss=2.2706925868988037
I0204 13:14:17.749703 139910203815680 logging_writer.py:48] [158500] global_step=158500, grad_norm=5.109065532684326, loss=4.431994915008545
I0204 13:15:03.579032 139910212208384 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.277055740356445, loss=2.4514896869659424
I0204 13:15:50.193892 139910203815680 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.438182830810547, loss=2.395162343978882
I0204 13:16:24.046424 140107197974336 spec.py:321] Evaluating on the training split.
I0204 13:16:34.810551 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 13:17:03.262082 140107197974336 spec.py:349] Evaluating on the test split.
I0204 13:17:04.859780 140107197974336 submission_runner.py:408] Time since start: 78708.45s, 	Step: 158776, 	{'train/accuracy': 0.8186913728713989, 'train/loss': 0.811683177947998, 'validation/accuracy': 0.7415399551391602, 'validation/loss': 1.1460314989089966, 'validation/num_examples': 50000, 'test/accuracy': 0.6224000453948975, 'test/loss': 1.7298548221588135, 'test/num_examples': 10000, 'score': 72304.50023531914, 'total_duration': 78708.44903898239, 'accumulated_submission_time': 72304.50023531914, 'accumulated_eval_time': 6385.997955322266, 'accumulated_logging_time': 8.971666812896729}
I0204 13:17:04.904211 139910212208384 logging_writer.py:48] [158776] accumulated_eval_time=6385.997955, accumulated_logging_time=8.971667, accumulated_submission_time=72304.500235, global_step=158776, preemption_count=0, score=72304.500235, test/accuracy=0.622400, test/loss=1.729855, test/num_examples=10000, total_duration=78708.449039, train/accuracy=0.818691, train/loss=0.811683, validation/accuracy=0.741540, validation/loss=1.146031, validation/num_examples=50000
I0204 13:17:14.740811 139910203815680 logging_writer.py:48] [158800] global_step=158800, grad_norm=5.237658500671387, loss=2.3782851696014404
I0204 13:17:56.786306 139910212208384 logging_writer.py:48] [158900] global_step=158900, grad_norm=3.9993674755096436, loss=3.1745269298553467
I0204 13:18:42.557160 139910203815680 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.282779693603516, loss=2.4331014156341553
I0204 13:19:28.696921 139910212208384 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.149342060089111, loss=2.389059543609619
I0204 13:20:15.164408 139910203815680 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.000803470611572, loss=2.914271831512451
I0204 13:21:01.207594 139910212208384 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.351467609405518, loss=2.9399850368499756
I0204 13:21:47.269085 139910203815680 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.186159133911133, loss=2.4195966720581055
I0204 13:22:33.402388 139910212208384 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.205341339111328, loss=2.2857487201690674
I0204 13:23:19.459980 139910203815680 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.296654224395752, loss=2.403900623321533
I0204 13:24:04.938171 140107197974336 spec.py:321] Evaluating on the training split.
I0204 13:24:15.412688 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 13:24:44.859380 140107197974336 spec.py:349] Evaluating on the test split.
I0204 13:24:46.449343 140107197974336 submission_runner.py:408] Time since start: 79170.04s, 	Step: 159700, 	{'train/accuracy': 0.8161132335662842, 'train/loss': 0.8156614303588867, 'validation/accuracy': 0.7422999739646912, 'validation/loss': 1.1298848390579224, 'validation/num_examples': 50000, 'test/accuracy': 0.6160000562667847, 'test/loss': 1.725846529006958, 'test/num_examples': 10000, 'score': 72724.4717707634, 'total_duration': 79170.03859901428, 'accumulated_submission_time': 72724.4717707634, 'accumulated_eval_time': 6427.509117841721, 'accumulated_logging_time': 9.025804042816162}
I0204 13:24:46.488772 139910212208384 logging_writer.py:48] [159700] accumulated_eval_time=6427.509118, accumulated_logging_time=9.025804, accumulated_submission_time=72724.471771, global_step=159700, preemption_count=0, score=72724.471771, test/accuracy=0.616000, test/loss=1.725847, test/num_examples=10000, total_duration=79170.038599, train/accuracy=0.816113, train/loss=0.815661, validation/accuracy=0.742300, validation/loss=1.129885, validation/num_examples=50000
I0204 13:24:46.894667 139910203815680 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.175771236419678, loss=2.552185535430908
I0204 13:25:27.608466 139910212208384 logging_writer.py:48] [159800] global_step=159800, grad_norm=5.4273552894592285, loss=4.434654712677002
I0204 13:26:13.547845 139910203815680 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.0013041496276855, loss=2.371852159500122
I0204 13:27:00.319463 139910212208384 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.3794732093811035, loss=3.7427659034729004
I0204 13:27:46.919397 139910203815680 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.778670310974121, loss=4.262555122375488
I0204 13:28:33.368233 139910212208384 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.454649925231934, loss=2.4368042945861816
I0204 13:29:19.433745 139910203815680 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.483552932739258, loss=2.4427428245544434
I0204 13:30:05.611648 139910212208384 logging_writer.py:48] [160400] global_step=160400, grad_norm=3.804903507232666, loss=2.822723865509033
I0204 13:30:51.661077 139910203815680 logging_writer.py:48] [160500] global_step=160500, grad_norm=3.9541385173797607, loss=2.2336316108703613
I0204 13:31:38.027457 139910212208384 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.1195068359375, loss=2.3987226486206055
I0204 13:31:46.930237 140107197974336 spec.py:321] Evaluating on the training split.
I0204 13:31:57.852777 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 13:32:26.541007 140107197974336 spec.py:349] Evaluating on the test split.
I0204 13:32:28.136019 140107197974336 submission_runner.py:408] Time since start: 79631.73s, 	Step: 160621, 	{'train/accuracy': 0.8175585865974426, 'train/loss': 0.804296612739563, 'validation/accuracy': 0.7449600100517273, 'validation/loss': 1.1215741634368896, 'validation/num_examples': 50000, 'test/accuracy': 0.624500036239624, 'test/loss': 1.7089358568191528, 'test/num_examples': 10000, 'score': 73144.85194015503, 'total_duration': 79631.72527456284, 'accumulated_submission_time': 73144.85194015503, 'accumulated_eval_time': 6468.714903116226, 'accumulated_logging_time': 9.074469327926636}
I0204 13:32:28.175431 139910203815680 logging_writer.py:48] [160621] accumulated_eval_time=6468.714903, accumulated_logging_time=9.074469, accumulated_submission_time=73144.851940, global_step=160621, preemption_count=0, score=73144.851940, test/accuracy=0.624500, test/loss=1.708936, test/num_examples=10000, total_duration=79631.725275, train/accuracy=0.817559, train/loss=0.804297, validation/accuracy=0.744960, validation/loss=1.121574, validation/num_examples=50000
I0204 13:32:59.766745 139910212208384 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.204630374908447, loss=2.337423086166382
I0204 13:33:45.540510 139910203815680 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.718175888061523, loss=2.8341100215911865
I0204 13:34:31.889880 139910212208384 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.322226524353027, loss=2.8665552139282227
I0204 13:35:18.560386 139910203815680 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.301466464996338, loss=2.622037410736084
I0204 13:36:04.778465 139910212208384 logging_writer.py:48] [161100] global_step=161100, grad_norm=6.752314567565918, loss=2.514191150665283
I0204 13:36:51.018008 139910203815680 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.336521148681641, loss=2.382823944091797
I0204 13:37:37.273806 139910212208384 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.520977020263672, loss=2.3632590770721436
I0204 13:38:23.483821 139910203815680 logging_writer.py:48] [161400] global_step=161400, grad_norm=5.349088191986084, loss=4.251814365386963
I0204 13:39:09.576663 139910212208384 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.194965362548828, loss=3.080585479736328
I0204 13:39:28.236991 140107197974336 spec.py:321] Evaluating on the training split.
I0204 13:39:38.859228 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 13:40:07.020849 140107197974336 spec.py:349] Evaluating on the test split.
I0204 13:40:08.615589 140107197974336 submission_runner.py:408] Time since start: 80092.20s, 	Step: 161542, 	{'train/accuracy': 0.8251757621765137, 'train/loss': 0.7653958201408386, 'validation/accuracy': 0.7464799880981445, 'validation/loss': 1.1053043603897095, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.6808910369873047, 'test/num_examples': 10000, 'score': 73564.85105657578, 'total_duration': 80092.20484352112, 'accumulated_submission_time': 73564.85105657578, 'accumulated_eval_time': 6509.093505382538, 'accumulated_logging_time': 9.123837947845459}
I0204 13:40:08.663402 139910203815680 logging_writer.py:48] [161542] accumulated_eval_time=6509.093505, accumulated_logging_time=9.123838, accumulated_submission_time=73564.851057, global_step=161542, preemption_count=0, score=73564.851057, test/accuracy=0.632200, test/loss=1.680891, test/num_examples=10000, total_duration=80092.204844, train/accuracy=0.825176, train/loss=0.765396, validation/accuracy=0.746480, validation/loss=1.105304, validation/num_examples=50000
I0204 13:40:31.855168 139910212208384 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.276269435882568, loss=2.9710705280303955
I0204 13:41:16.304381 139910203815680 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.387500286102295, loss=2.294891357421875
I0204 13:42:02.440058 139910212208384 logging_writer.py:48] [161800] global_step=161800, grad_norm=5.4687886238098145, loss=4.331545352935791
I0204 13:42:48.719624 139910203815680 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.536881923675537, loss=3.2450413703918457
I0204 13:43:34.971302 139910212208384 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.445141792297363, loss=2.621436834335327
I0204 13:44:21.042486 139910203815680 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.702438831329346, loss=2.369147300720215
I0204 13:45:07.456263 139910212208384 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.378311634063721, loss=3.0739142894744873
I0204 13:45:53.583512 139910203815680 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.437230587005615, loss=2.439716339111328
I0204 13:46:39.762017 139910212208384 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.164853096008301, loss=3.281007766723633
I0204 13:47:08.699822 140107197974336 spec.py:321] Evaluating on the training split.
I0204 13:47:19.281785 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 13:47:47.896821 140107197974336 spec.py:349] Evaluating on the test split.
I0204 13:47:49.494406 140107197974336 submission_runner.py:408] Time since start: 80553.08s, 	Step: 162464, 	{'train/accuracy': 0.8218163847923279, 'train/loss': 0.7823770642280579, 'validation/accuracy': 0.7467399835586548, 'validation/loss': 1.1013247966766357, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.6946297883987427, 'test/num_examples': 10000, 'score': 73984.82580113411, 'total_duration': 80553.0836699009, 'accumulated_submission_time': 73984.82580113411, 'accumulated_eval_time': 6549.888410568237, 'accumulated_logging_time': 9.1816246509552}
I0204 13:47:49.537948 139910203815680 logging_writer.py:48] [162464] accumulated_eval_time=6549.888411, accumulated_logging_time=9.181625, accumulated_submission_time=73984.825801, global_step=162464, preemption_count=0, score=73984.825801, test/accuracy=0.626500, test/loss=1.694630, test/num_examples=10000, total_duration=80553.083670, train/accuracy=0.821816, train/loss=0.782377, validation/accuracy=0.746740, validation/loss=1.101325, validation/num_examples=50000
I0204 13:48:04.072757 139910212208384 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.424397945404053, loss=2.4167072772979736
I0204 13:48:47.012305 139910203815680 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.420291423797607, loss=2.1824331283569336
I0204 13:49:32.882981 139910212208384 logging_writer.py:48] [162700] global_step=162700, grad_norm=5.0285115242004395, loss=3.758568286895752
I0204 13:50:19.417024 139910203815680 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.326557159423828, loss=2.7206614017486572
I0204 13:51:05.913274 139910212208384 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.564401626586914, loss=2.3405566215515137
I0204 13:51:51.967724 139910203815680 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.599389553070068, loss=2.6012072563171387
I0204 13:52:38.177167 139910212208384 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.165217876434326, loss=2.3281774520874023
I0204 13:53:24.424535 139910203815680 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.836243629455566, loss=2.3654263019561768
I0204 13:54:10.590374 139910212208384 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.667290687561035, loss=2.352652072906494
I0204 13:54:49.625456 140107197974336 spec.py:321] Evaluating on the training split.
I0204 13:55:00.215546 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 13:55:26.791335 140107197974336 spec.py:349] Evaluating on the test split.
I0204 13:55:28.389635 140107197974336 submission_runner.py:408] Time since start: 81011.98s, 	Step: 163386, 	{'train/accuracy': 0.8213866949081421, 'train/loss': 0.7875846028327942, 'validation/accuracy': 0.7483199834823608, 'validation/loss': 1.1053085327148438, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.7001694440841675, 'test/num_examples': 10000, 'score': 74404.8524620533, 'total_duration': 81011.978900671, 'accumulated_submission_time': 74404.8524620533, 'accumulated_eval_time': 6588.652596235275, 'accumulated_logging_time': 9.234551668167114}
I0204 13:55:28.429166 139910203815680 logging_writer.py:48] [163386] accumulated_eval_time=6588.652596, accumulated_logging_time=9.234552, accumulated_submission_time=74404.852462, global_step=163386, preemption_count=0, score=74404.852462, test/accuracy=0.622300, test/loss=1.700169, test/num_examples=10000, total_duration=81011.978901, train/accuracy=0.821387, train/loss=0.787585, validation/accuracy=0.748320, validation/loss=1.105309, validation/num_examples=50000
I0204 13:55:34.322443 139910212208384 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.663241386413574, loss=2.343377113342285
I0204 13:56:15.894487 139910203815680 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.830636501312256, loss=2.44108247756958
I0204 13:57:01.891885 139910212208384 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.469943523406982, loss=2.5921578407287598
I0204 13:57:48.256285 139910203815680 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.651498794555664, loss=2.425528049468994
I0204 13:58:34.691430 139910212208384 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.9209699630737305, loss=3.697699546813965
I0204 13:59:21.252856 139910203815680 logging_writer.py:48] [163900] global_step=163900, grad_norm=5.162258625030518, loss=2.413332223892212
I0204 14:00:07.726686 139910212208384 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.6253581047058105, loss=2.345813035964966
I0204 14:00:53.746015 139910203815680 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.631308078765869, loss=2.3778128623962402
I0204 14:01:39.892196 139910212208384 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.807590007781982, loss=2.316418409347534
I0204 14:02:26.099397 139910203815680 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.63161039352417, loss=2.3052172660827637
I0204 14:02:28.549914 140107197974336 spec.py:321] Evaluating on the training split.
I0204 14:02:39.521203 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 14:03:07.845909 140107197974336 spec.py:349] Evaluating on the test split.
I0204 14:03:09.450020 140107197974336 submission_runner.py:408] Time since start: 81473.04s, 	Step: 164307, 	{'train/accuracy': 0.8302733898162842, 'train/loss': 0.7504153251647949, 'validation/accuracy': 0.7503199577331543, 'validation/loss': 1.0849838256835938, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.6703184843063354, 'test/num_examples': 10000, 'score': 74824.91179394722, 'total_duration': 81473.03928542137, 'accumulated_submission_time': 74824.91179394722, 'accumulated_eval_time': 6629.552686929703, 'accumulated_logging_time': 9.28411316871643}
I0204 14:03:09.493711 139910212208384 logging_writer.py:48] [164307] accumulated_eval_time=6629.552687, accumulated_logging_time=9.284113, accumulated_submission_time=74824.911794, global_step=164307, preemption_count=0, score=74824.911794, test/accuracy=0.632200, test/loss=1.670318, test/num_examples=10000, total_duration=81473.039285, train/accuracy=0.830273, train/loss=0.750415, validation/accuracy=0.750320, validation/loss=1.084984, validation/num_examples=50000
I0204 14:03:47.332482 139910203815680 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.593603134155273, loss=2.3524322509765625
I0204 14:04:33.500323 139910212208384 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.9125657081604, loss=2.3356425762176514
I0204 14:05:19.885258 139910203815680 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.648672103881836, loss=2.275529146194458
I0204 14:06:06.602696 139910212208384 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.737031936645508, loss=2.49849271774292
I0204 14:06:52.652766 139910203815680 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.546198844909668, loss=2.334944486618042
I0204 14:07:39.235583 139910212208384 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.6679158210754395, loss=3.345017910003662
I0204 14:08:25.752399 139910203815680 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.820326328277588, loss=2.3230674266815186
I0204 14:09:11.840825 139910212208384 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.778056621551514, loss=2.286090135574341
I0204 14:09:58.046556 139910203815680 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.7289862632751465, loss=3.1816015243530273
I0204 14:10:09.820560 140107197974336 spec.py:321] Evaluating on the training split.
I0204 14:10:20.272816 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 14:10:47.727951 140107197974336 spec.py:349] Evaluating on the test split.
I0204 14:10:49.313700 140107197974336 submission_runner.py:408] Time since start: 81932.90s, 	Step: 165227, 	{'train/accuracy': 0.8239843845367432, 'train/loss': 0.7693715691566467, 'validation/accuracy': 0.7511799931526184, 'validation/loss': 1.0776283740997314, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.6670862436294556, 'test/num_examples': 10000, 'score': 75245.17643213272, 'total_duration': 81932.90296435356, 'accumulated_submission_time': 75245.17643213272, 'accumulated_eval_time': 6669.045813798904, 'accumulated_logging_time': 9.33809781074524}
I0204 14:10:49.357166 139910212208384 logging_writer.py:48] [165227] accumulated_eval_time=6669.045814, accumulated_logging_time=9.338098, accumulated_submission_time=75245.176432, global_step=165227, preemption_count=0, score=75245.176432, test/accuracy=0.628300, test/loss=1.667086, test/num_examples=10000, total_duration=81932.902964, train/accuracy=0.823984, train/loss=0.769372, validation/accuracy=0.751180, validation/loss=1.077628, validation/num_examples=50000
I0204 14:11:18.460339 139910203815680 logging_writer.py:48] [165300] global_step=165300, grad_norm=5.450346946716309, loss=4.10910701751709
I0204 14:12:03.814322 139910212208384 logging_writer.py:48] [165400] global_step=165400, grad_norm=5.0257978439331055, loss=2.3077144622802734
I0204 14:12:50.180981 139910203815680 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.75205659866333, loss=2.336603879928589
I0204 14:13:36.532769 139910212208384 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.940525531768799, loss=2.382988214492798
I0204 14:14:22.669417 139910203815680 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.846949100494385, loss=2.4796979427337646
I0204 14:15:09.050173 139910212208384 logging_writer.py:48] [165800] global_step=165800, grad_norm=5.024971008300781, loss=2.2591679096221924
I0204 14:15:55.133381 139910203815680 logging_writer.py:48] [165900] global_step=165900, grad_norm=5.288705348968506, loss=2.368380069732666
I0204 14:16:41.078849 139910212208384 logging_writer.py:48] [166000] global_step=166000, grad_norm=5.085882186889648, loss=2.6095364093780518
I0204 14:17:27.062536 139910203815680 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.6767144203186035, loss=2.729905843734741
I0204 14:17:49.336060 140107197974336 spec.py:321] Evaluating on the training split.
I0204 14:18:00.070504 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 14:18:27.670584 140107197974336 spec.py:349] Evaluating on the test split.
I0204 14:18:29.272506 140107197974336 submission_runner.py:408] Time since start: 82392.86s, 	Step: 166150, 	{'train/accuracy': 0.8310937285423279, 'train/loss': 0.7499398589134216, 'validation/accuracy': 0.7531399726867676, 'validation/loss': 1.070204496383667, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.6644469499588013, 'test/num_examples': 10000, 'score': 75665.09493494034, 'total_duration': 82392.86175775528, 'accumulated_submission_time': 75665.09493494034, 'accumulated_eval_time': 6708.982246637344, 'accumulated_logging_time': 9.390437126159668}
I0204 14:18:29.317669 139910212208384 logging_writer.py:48] [166150] accumulated_eval_time=6708.982247, accumulated_logging_time=9.390437, accumulated_submission_time=75665.094935, global_step=166150, preemption_count=0, score=75665.094935, test/accuracy=0.630700, test/loss=1.664447, test/num_examples=10000, total_duration=82392.861758, train/accuracy=0.831094, train/loss=0.749940, validation/accuracy=0.753140, validation/loss=1.070204, validation/num_examples=50000
I0204 14:18:49.357480 139910203815680 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.962783336639404, loss=2.2377727031707764
I0204 14:19:33.243560 139910212208384 logging_writer.py:48] [166300] global_step=166300, grad_norm=5.013967037200928, loss=2.306633234024048
I0204 14:20:19.525598 139910203815680 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.581167697906494, loss=2.2684519290924072
I0204 14:21:06.154129 139910212208384 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.762453079223633, loss=3.813584804534912
I0204 14:21:52.519455 139910203815680 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.7381367683410645, loss=2.2741787433624268
I0204 14:22:38.638854 139910212208384 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.65768575668335, loss=2.883676052093506
I0204 14:23:24.964317 139910203815680 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.636631488800049, loss=2.3178012371063232
I0204 14:24:11.260708 139910212208384 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.588520050048828, loss=2.280134677886963
I0204 14:24:57.476262 139910203815680 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.973586082458496, loss=3.231257438659668
I0204 14:25:29.571506 140107197974336 spec.py:321] Evaluating on the training split.
I0204 14:25:40.495642 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 14:26:08.053128 140107197974336 spec.py:349] Evaluating on the test split.
I0204 14:26:09.653728 140107197974336 submission_runner.py:408] Time since start: 82853.24s, 	Step: 167071, 	{'train/accuracy': 0.83056640625, 'train/loss': 0.7435762882232666, 'validation/accuracy': 0.7534199953079224, 'validation/loss': 1.0754247903823853, 'validation/num_examples': 50000, 'test/accuracy': 0.6333000063896179, 'test/loss': 1.6693150997161865, 'test/num_examples': 10000, 'score': 76085.28699398041, 'total_duration': 82853.24296784401, 'accumulated_submission_time': 76085.28699398041, 'accumulated_eval_time': 6749.0644516944885, 'accumulated_logging_time': 9.445293426513672}
I0204 14:26:09.695585 139910212208384 logging_writer.py:48] [167071] accumulated_eval_time=6749.064452, accumulated_logging_time=9.445293, accumulated_submission_time=76085.286994, global_step=167071, preemption_count=0, score=76085.286994, test/accuracy=0.633300, test/loss=1.669315, test/num_examples=10000, total_duration=82853.242968, train/accuracy=0.830566, train/loss=0.743576, validation/accuracy=0.753420, validation/loss=1.075425, validation/num_examples=50000
I0204 14:26:21.497665 139910203815680 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.035059928894043, loss=3.769265651702881
I0204 14:27:03.896407 139910212208384 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.850314140319824, loss=2.6804158687591553
I0204 14:27:50.137997 139910203815680 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.164863109588623, loss=3.8261942863464355
I0204 14:28:36.616293 139910212208384 logging_writer.py:48] [167400] global_step=167400, grad_norm=5.217106342315674, loss=2.1963295936584473
I0204 14:29:23.071816 139910203815680 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.040536403656006, loss=2.215662717819214
I0204 14:30:09.265046 139910212208384 logging_writer.py:48] [167600] global_step=167600, grad_norm=5.739555358886719, loss=4.206011772155762
I0204 14:30:55.490969 139910203815680 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.898370742797852, loss=2.3979973793029785
I0204 14:31:41.707813 139910212208384 logging_writer.py:48] [167800] global_step=167800, grad_norm=5.192909240722656, loss=3.311389446258545
I0204 14:32:27.901066 139910203815680 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.191898822784424, loss=2.270616054534912
I0204 14:33:09.730445 140107197974336 spec.py:321] Evaluating on the training split.
I0204 14:33:20.438354 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 14:33:45.935621 140107197974336 spec.py:349] Evaluating on the test split.
I0204 14:33:47.540247 140107197974336 submission_runner.py:408] Time since start: 83311.13s, 	Step: 167992, 	{'train/accuracy': 0.8321484327316284, 'train/loss': 0.7515702843666077, 'validation/accuracy': 0.7541399598121643, 'validation/loss': 1.0826858282089233, 'validation/num_examples': 50000, 'test/accuracy': 0.634600043296814, 'test/loss': 1.6695160865783691, 'test/num_examples': 10000, 'score': 76505.26083564758, 'total_duration': 83311.12951374054, 'accumulated_submission_time': 76505.26083564758, 'accumulated_eval_time': 6786.874273777008, 'accumulated_logging_time': 9.495783567428589}
I0204 14:33:47.582080 139910212208384 logging_writer.py:48] [167992] accumulated_eval_time=6786.874274, accumulated_logging_time=9.495784, accumulated_submission_time=76505.260836, global_step=167992, preemption_count=0, score=76505.260836, test/accuracy=0.634600, test/loss=1.669516, test/num_examples=10000, total_duration=83311.129514, train/accuracy=0.832148, train/loss=0.751570, validation/accuracy=0.754140, validation/loss=1.082686, validation/num_examples=50000
I0204 14:33:51.124473 139910203815680 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.288217067718506, loss=2.3315930366516113
I0204 14:34:32.264312 139910212208384 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.530125617980957, loss=2.331967353820801
I0204 14:35:18.646069 139910203815680 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.830024242401123, loss=2.410799264907837
I0204 14:36:04.926433 139910212208384 logging_writer.py:48] [168300] global_step=168300, grad_norm=5.487421989440918, loss=2.305508613586426
I0204 14:36:51.112483 139910203815680 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.730133056640625, loss=2.5635273456573486
I0204 14:37:37.307609 139910212208384 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.957779407501221, loss=2.2446553707122803
I0204 14:38:23.557948 139910203815680 logging_writer.py:48] [168600] global_step=168600, grad_norm=5.108630657196045, loss=3.0857038497924805
I0204 14:39:09.934195 139910212208384 logging_writer.py:48] [168700] global_step=168700, grad_norm=5.076508522033691, loss=2.251347303390503
I0204 14:39:56.136111 139910203815680 logging_writer.py:48] [168800] global_step=168800, grad_norm=5.466431140899658, loss=2.452455997467041
I0204 14:40:42.473945 139910212208384 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.801931858062744, loss=2.2304325103759766
I0204 14:40:47.803280 140107197974336 spec.py:321] Evaluating on the training split.
I0204 14:40:58.686766 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 14:41:27.048184 140107197974336 spec.py:349] Evaluating on the test split.
I0204 14:41:28.646001 140107197974336 submission_runner.py:408] Time since start: 83772.24s, 	Step: 168913, 	{'train/accuracy': 0.8340038657188416, 'train/loss': 0.7366316318511963, 'validation/accuracy': 0.7570599913597107, 'validation/loss': 1.0599473714828491, 'validation/num_examples': 50000, 'test/accuracy': 0.638700008392334, 'test/loss': 1.6415356397628784, 'test/num_examples': 10000, 'score': 76925.41933321953, 'total_duration': 83772.23526740074, 'accumulated_submission_time': 76925.41933321953, 'accumulated_eval_time': 6827.716981649399, 'accumulated_logging_time': 9.548727989196777}
I0204 14:41:28.688713 139910203815680 logging_writer.py:48] [168913] accumulated_eval_time=6827.716982, accumulated_logging_time=9.548728, accumulated_submission_time=76925.419333, global_step=168913, preemption_count=0, score=76925.419333, test/accuracy=0.638700, test/loss=1.641536, test/num_examples=10000, total_duration=83772.235267, train/accuracy=0.834004, train/loss=0.736632, validation/accuracy=0.757060, validation/loss=1.059947, validation/num_examples=50000
I0204 14:42:03.755115 139910212208384 logging_writer.py:48] [169000] global_step=169000, grad_norm=5.601577281951904, loss=2.2924773693084717
I0204 14:42:49.746139 139910203815680 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.993780612945557, loss=3.291722536087036
I0204 14:43:36.141103 139910212208384 logging_writer.py:48] [169200] global_step=169200, grad_norm=5.433426380157471, loss=2.2656917572021484
I0204 14:44:22.543386 139910203815680 logging_writer.py:48] [169300] global_step=169300, grad_norm=5.9615797996521, loss=2.536038398742676
I0204 14:45:08.821137 139910212208384 logging_writer.py:48] [169400] global_step=169400, grad_norm=5.379062652587891, loss=2.2751760482788086
I0204 14:45:54.990207 139910203815680 logging_writer.py:48] [169500] global_step=169500, grad_norm=5.7040934562683105, loss=3.4755072593688965
I0204 14:46:41.214265 139910212208384 logging_writer.py:48] [169600] global_step=169600, grad_norm=5.7732768058776855, loss=3.5418968200683594
I0204 14:47:27.408146 139910203815680 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.949899196624756, loss=2.2747068405151367
I0204 14:48:13.701544 139910212208384 logging_writer.py:48] [169800] global_step=169800, grad_norm=5.203391075134277, loss=2.2491698265075684
I0204 14:48:28.702593 140107197974336 spec.py:321] Evaluating on the training split.
I0204 14:48:39.655515 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 14:49:06.568338 140107197974336 spec.py:349] Evaluating on the test split.
I0204 14:49:08.174902 140107197974336 submission_runner.py:408] Time since start: 84231.76s, 	Step: 169834, 	{'train/accuracy': 0.8338086009025574, 'train/loss': 0.7275217771530151, 'validation/accuracy': 0.7583400011062622, 'validation/loss': 1.0552046298980713, 'validation/num_examples': 50000, 'test/accuracy': 0.6380000114440918, 'test/loss': 1.6476746797561646, 'test/num_examples': 10000, 'score': 77345.37213397026, 'total_duration': 84231.7641518116, 'accumulated_submission_time': 77345.37213397026, 'accumulated_eval_time': 6867.189259767532, 'accumulated_logging_time': 9.600689172744751}
I0204 14:49:08.224329 139910203815680 logging_writer.py:48] [169834] accumulated_eval_time=6867.189260, accumulated_logging_time=9.600689, accumulated_submission_time=77345.372134, global_step=169834, preemption_count=0, score=77345.372134, test/accuracy=0.638000, test/loss=1.647675, test/num_examples=10000, total_duration=84231.764152, train/accuracy=0.833809, train/loss=0.727522, validation/accuracy=0.758340, validation/loss=1.055205, validation/num_examples=50000
I0204 14:49:34.566244 139910212208384 logging_writer.py:48] [169900] global_step=169900, grad_norm=5.532174587249756, loss=3.4971377849578857
I0204 14:50:19.415510 139910203815680 logging_writer.py:48] [170000] global_step=170000, grad_norm=5.207411289215088, loss=2.4494426250457764
I0204 14:51:05.867485 139910212208384 logging_writer.py:48] [170100] global_step=170100, grad_norm=5.751033782958984, loss=2.2101330757141113
I0204 14:51:52.559373 139910203815680 logging_writer.py:48] [170200] global_step=170200, grad_norm=5.543152809143066, loss=2.4672224521636963
I0204 14:52:03.133875 139910212208384 logging_writer.py:48] [170224] global_step=170224, preemption_count=0, score=77520.194962
I0204 14:52:03.829945 140107197974336 checkpoints.py:490] Saving checkpoint at step: 170224
I0204 14:52:05.127566 140107197974336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_1/checkpoint_170224
I0204 14:52:05.151012 140107197974336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_1/checkpoint_170224.
I0204 14:52:06.108095 140107197974336 submission_runner.py:583] Tuning trial 1/5
I0204 14:52:06.108368 140107197974336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0204 14:52:06.117613 140107197974336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007617187220603228, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 42.824132204055786, 'total_duration': 83.2208137512207, 'accumulated_submission_time': 42.824132204055786, 'accumulated_eval_time': 40.396591901779175, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (890, {'train/accuracy': 0.012402343563735485, 'train/loss': 6.457183837890625, 'validation/accuracy': 0.012240000069141388, 'validation/loss': 6.465394973754883, 'validation/num_examples': 50000, 'test/accuracy': 0.010000000707805157, 'test/loss': 6.503350734710693, 'test/num_examples': 10000, 'score': 462.9493408203125, 'total_duration': 525.0217719078064, 'accumulated_submission_time': 462.9493408203125, 'accumulated_eval_time': 61.98973369598389, 'accumulated_logging_time': 0.0296323299407959, 'global_step': 890, 'preemption_count': 0}), (1829, {'train/accuracy': 0.03935546800494194, 'train/loss': 5.864162445068359, 'validation/accuracy': 0.039239998906850815, 'validation/loss': 5.889999866485596, 'validation/num_examples': 50000, 'test/accuracy': 0.03100000135600567, 'test/loss': 5.997738838195801, 'test/num_examples': 10000, 'score': 883.1624338626862, 'total_duration': 967.2049548625946, 'accumulated_submission_time': 883.1624338626862, 'accumulated_eval_time': 83.87272596359253, 'accumulated_logging_time': 0.06127738952636719, 'global_step': 1829, 'preemption_count': 0}), (2769, {'train/accuracy': 0.07162109017372131, 'train/loss': 5.389620780944824, 'validation/accuracy': 0.06551999598741531, 'validation/loss': 5.453647136688232, 'validation/num_examples': 50000, 'test/accuracy': 0.05100000277161598, 'test/loss': 5.634537220001221, 'test/num_examples': 10000, 'score': 1303.3358738422394, 'total_duration': 1409.2352724075317, 'accumulated_submission_time': 1303.3358738422394, 'accumulated_eval_time': 105.64576721191406, 'accumulated_logging_time': 0.09018278121948242, 'global_step': 2769, 'preemption_count': 0}), (3710, {'train/accuracy': 0.10431640595197678, 'train/loss': 5.0515570640563965, 'validation/accuracy': 0.09815999865531921, 'validation/loss': 5.091566562652588, 'validation/num_examples': 50000, 'test/accuracy': 0.07480000704526901, 'test/loss': 5.3220343589782715, 'test/num_examples': 10000, 'score': 1723.3727452754974, 'total_duration': 1851.1949937343597, 'accumulated_submission_time': 1723.3727452754974, 'accumulated_eval_time': 127.48762011528015, 'accumulated_logging_time': 0.11726093292236328, 'global_step': 3710, 'preemption_count': 0}), (4645, {'train/accuracy': 0.13974608480930328, 'train/loss': 4.694349765777588, 'validation/accuracy': 0.13157999515533447, 'validation/loss': 4.74459171295166, 'validation/num_examples': 50000, 'test/accuracy': 0.09930000454187393, 'test/loss': 5.037469387054443, 'test/num_examples': 10000, 'score': 2143.325270175934, 'total_duration': 2293.2052896022797, 'accumulated_submission_time': 2143.325270175934, 'accumulated_eval_time': 149.46379971504211, 'accumulated_logging_time': 0.1450655460357666, 'global_step': 4645, 'preemption_count': 0}), (5578, {'train/accuracy': 0.18845702707767487, 'train/loss': 4.301875114440918, 'validation/accuracy': 0.17127999663352966, 'validation/loss': 4.40316104888916, 'validation/num_examples': 50000, 'test/accuracy': 0.12620000541210175, 'test/loss': 4.740323066711426, 'test/num_examples': 10000, 'score': 2563.570514202118, 'total_duration': 2735.4029870033264, 'accumulated_submission_time': 2563.570514202118, 'accumulated_eval_time': 171.33267450332642, 'accumulated_logging_time': 0.17570042610168457, 'global_step': 5578, 'preemption_count': 0}), (6510, {'train/accuracy': 0.22810547053813934, 'train/loss': 3.9725728034973145, 'validation/accuracy': 0.21327999234199524, 'validation/loss': 4.052278518676758, 'validation/num_examples': 50000, 'test/accuracy': 0.16300000250339508, 'test/loss': 4.438085556030273, 'test/num_examples': 10000, 'score': 2983.7556672096252, 'total_duration': 3180.6423647403717, 'accumulated_submission_time': 2983.7556672096252, 'accumulated_eval_time': 196.30260586738586, 'accumulated_logging_time': 0.20665884017944336, 'global_step': 6510, 'preemption_count': 0}), (7447, {'train/accuracy': 0.2765820324420929, 'train/loss': 3.669283628463745, 'validation/accuracy': 0.2522999942302704, 'validation/loss': 3.7821102142333984, 'validation/num_examples': 50000, 'test/accuracy': 0.1923000067472458, 'test/loss': 4.209742546081543, 'test/num_examples': 10000, 'score': 3403.835516691208, 'total_duration': 3624.8235108852386, 'accumulated_submission_time': 3403.835516691208, 'accumulated_eval_time': 220.32091236114502, 'accumulated_logging_time': 0.23564529418945312, 'global_step': 7447, 'preemption_count': 0}), (8384, {'train/accuracy': 0.30943357944488525, 'train/loss': 3.4023940563201904, 'validation/accuracy': 0.2844800055027008, 'validation/loss': 3.535698652267456, 'validation/num_examples': 50000, 'test/accuracy': 0.22380000352859497, 'test/loss': 3.993281841278076, 'test/num_examples': 10000, 'score': 3823.778416156769, 'total_duration': 4072.47727060318, 'accumulated_submission_time': 3823.778416156769, 'accumulated_eval_time': 247.93834471702576, 'accumulated_logging_time': 0.2755928039550781, 'global_step': 8384, 'preemption_count': 0}), (9320, {'train/accuracy': 0.3596288859844208, 'train/loss': 3.166682004928589, 'validation/accuracy': 0.32023999094963074, 'validation/loss': 3.347395896911621, 'validation/num_examples': 50000, 'test/accuracy': 0.2459000051021576, 'test/loss': 3.8278791904449463, 'test/num_examples': 10000, 'score': 4243.810468912125, 'total_duration': 4520.722212314606, 'accumulated_submission_time': 4243.810468912125, 'accumulated_eval_time': 276.0673451423645, 'accumulated_logging_time': 0.3065640926361084, 'global_step': 9320, 'preemption_count': 0}), (10253, {'train/accuracy': 0.36775389313697815, 'train/loss': 3.1120121479034424, 'validation/accuracy': 0.3455999791622162, 'validation/loss': 3.226508617401123, 'validation/num_examples': 50000, 'test/accuracy': 0.26340001821517944, 'test/loss': 3.720574378967285, 'test/num_examples': 10000, 'score': 4664.097786426544, 'total_duration': 4972.415317296982, 'accumulated_submission_time': 4664.097786426544, 'accumulated_eval_time': 307.3923919200897, 'accumulated_logging_time': 0.3343639373779297, 'global_step': 10253, 'preemption_count': 0}), (11185, {'train/accuracy': 0.3995117247104645, 'train/loss': 2.8868730068206787, 'validation/accuracy': 0.36805999279022217, 'validation/loss': 3.0476648807525635, 'validation/num_examples': 50000, 'test/accuracy': 0.2808000147342682, 'test/loss': 3.5686235427856445, 'test/num_examples': 10000, 'score': 5084.326451063156, 'total_duration': 5420.9716629981995, 'accumulated_submission_time': 5084.326451063156, 'accumulated_eval_time': 335.6350944042206, 'accumulated_logging_time': 0.3666074275970459, 'global_step': 11185, 'preemption_count': 0}), (12117, {'train/accuracy': 0.4317578077316284, 'train/loss': 2.685864210128784, 'validation/accuracy': 0.3901999890804291, 'validation/loss': 2.880558729171753, 'validation/num_examples': 50000, 'test/accuracy': 0.2963000237941742, 'test/loss': 3.4384853839874268, 'test/num_examples': 10000, 'score': 5504.530478954315, 'total_duration': 5871.672466278076, 'accumulated_submission_time': 5504.530478954315, 'accumulated_eval_time': 366.0485026836395, 'accumulated_logging_time': 0.3962712287902832, 'global_step': 12117, 'preemption_count': 0}), (13044, {'train/accuracy': 0.4320703148841858, 'train/loss': 2.7094104290008545, 'validation/accuracy': 0.4023999869823456, 'validation/loss': 2.8486061096191406, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.408851385116577, 'test/num_examples': 10000, 'score': 5924.697474718094, 'total_duration': 6322.066045045853, 'accumulated_submission_time': 5924.697474718094, 'accumulated_eval_time': 396.19059109687805, 'accumulated_logging_time': 0.429196834564209, 'global_step': 13044, 'preemption_count': 0}), (13967, {'train/accuracy': 0.45130857825279236, 'train/loss': 2.5765609741210938, 'validation/accuracy': 0.42010000348091125, 'validation/loss': 2.7298953533172607, 'validation/num_examples': 50000, 'test/accuracy': 0.3273000121116638, 'test/loss': 3.2973392009735107, 'test/num_examples': 10000, 'score': 6344.838493108749, 'total_duration': 6778.072106599808, 'accumulated_submission_time': 6344.838493108749, 'accumulated_eval_time': 431.96968388557434, 'accumulated_logging_time': 0.46321702003479004, 'global_step': 13967, 'preemption_count': 0}), (14896, {'train/accuracy': 0.4676562249660492, 'train/loss': 2.475290536880493, 'validation/accuracy': 0.4269999861717224, 'validation/loss': 2.6804628372192383, 'validation/num_examples': 50000, 'test/accuracy': 0.33420002460479736, 'test/loss': 3.250800371170044, 'test/num_examples': 10000, 'score': 6764.83682847023, 'total_duration': 7233.150120258331, 'accumulated_submission_time': 6764.83682847023, 'accumulated_eval_time': 466.97098088264465, 'accumulated_logging_time': 0.489241361618042, 'global_step': 14896, 'preemption_count': 0}), (15826, {'train/accuracy': 0.46556639671325684, 'train/loss': 2.52592134475708, 'validation/accuracy': 0.435619980096817, 'validation/loss': 2.66223406791687, 'validation/num_examples': 50000, 'test/accuracy': 0.33400002121925354, 'test/loss': 3.236973285675049, 'test/num_examples': 10000, 'score': 7185.151596069336, 'total_duration': 7691.874465227127, 'accumulated_submission_time': 7185.151596069336, 'accumulated_eval_time': 505.29772305488586, 'accumulated_logging_time': 0.520289421081543, 'global_step': 15826, 'preemption_count': 0}), (16754, {'train/accuracy': 0.47822263836860657, 'train/loss': 2.447401762008667, 'validation/accuracy': 0.4474399983882904, 'validation/loss': 2.5987234115600586, 'validation/num_examples': 50000, 'test/accuracy': 0.3508000075817108, 'test/loss': 3.1780033111572266, 'test/num_examples': 10000, 'score': 7605.435419559479, 'total_duration': 8152.040428161621, 'accumulated_submission_time': 7605.435419559479, 'accumulated_eval_time': 545.1009593009949, 'accumulated_logging_time': 0.5469293594360352, 'global_step': 16754, 'preemption_count': 0}), (17682, {'train/accuracy': 0.4890234172344208, 'train/loss': 2.3615238666534424, 'validation/accuracy': 0.4515799880027771, 'validation/loss': 2.543088912963867, 'validation/num_examples': 50000, 'test/accuracy': 0.34850001335144043, 'test/loss': 3.1464133262634277, 'test/num_examples': 10000, 'score': 8025.745712041855, 'total_duration': 8613.524369716644, 'accumulated_submission_time': 8025.745712041855, 'accumulated_eval_time': 586.1966207027435, 'accumulated_logging_time': 0.5725915431976318, 'global_step': 17682, 'preemption_count': 0}), (18605, {'train/accuracy': 0.5197656154632568, 'train/loss': 2.2128489017486572, 'validation/accuracy': 0.4608999788761139, 'validation/loss': 2.4904685020446777, 'validation/num_examples': 50000, 'test/accuracy': 0.3604000210762024, 'test/loss': 3.0848608016967773, 'test/num_examples': 10000, 'score': 8445.693154335022, 'total_duration': 9072.734701156616, 'accumulated_submission_time': 8445.693154335022, 'accumulated_eval_time': 625.3753478527069, 'accumulated_logging_time': 0.6053094863891602, 'global_step': 18605, 'preemption_count': 0}), (19531, {'train/accuracy': 0.5014843344688416, 'train/loss': 2.3330583572387695, 'validation/accuracy': 0.4675999879837036, 'validation/loss': 2.486729145050049, 'validation/num_examples': 50000, 'test/accuracy': 0.36400002241134644, 'test/loss': 3.0885519981384277, 'test/num_examples': 10000, 'score': 8865.636800050735, 'total_duration': 9534.022550106049, 'accumulated_submission_time': 8865.636800050735, 'accumulated_eval_time': 666.6394157409668, 'accumulated_logging_time': 0.6336965560913086, 'global_step': 19531, 'preemption_count': 0}), (20451, {'train/accuracy': 0.5143749713897705, 'train/loss': 2.221330165863037, 'validation/accuracy': 0.4759399890899658, 'validation/loss': 2.4047062397003174, 'validation/num_examples': 50000, 'test/accuracy': 0.37530001997947693, 'test/loss': 3.02260684967041, 'test/num_examples': 10000, 'score': 9284.03717637062, 'total_duration': 9993.490169763565, 'accumulated_submission_time': 9284.03717637062, 'accumulated_eval_time': 705.9051666259766, 'accumulated_logging_time': 2.3828179836273193, 'global_step': 20451, 'preemption_count': 0}), (21375, {'train/accuracy': 0.5318750143051147, 'train/loss': 2.1464452743530273, 'validation/accuracy': 0.48201999068260193, 'validation/loss': 2.3802831172943115, 'validation/num_examples': 50000, 'test/accuracy': 0.37450000643730164, 'test/loss': 3.0088958740234375, 'test/num_examples': 10000, 'score': 9703.975273609161, 'total_duration': 10451.55467915535, 'accumulated_submission_time': 9703.975273609161, 'accumulated_eval_time': 743.9496030807495, 'accumulated_logging_time': 2.413090705871582, 'global_step': 21375, 'preemption_count': 0}), (22298, {'train/accuracy': 0.525390625, 'train/loss': 2.1639983654022217, 'validation/accuracy': 0.49187999963760376, 'validation/loss': 2.3201968669891357, 'validation/num_examples': 50000, 'test/accuracy': 0.3839000165462494, 'test/loss': 2.934317111968994, 'test/num_examples': 10000, 'score': 10124.183888912201, 'total_duration': 10906.612278938293, 'accumulated_submission_time': 10124.183888912201, 'accumulated_eval_time': 778.7204098701477, 'accumulated_logging_time': 2.43961763381958, 'global_step': 22298, 'preemption_count': 0}), (23222, {'train/accuracy': 0.5257226228713989, 'train/loss': 2.1706438064575195, 'validation/accuracy': 0.49365997314453125, 'validation/loss': 2.339932680130005, 'validation/num_examples': 50000, 'test/accuracy': 0.38930001854896545, 'test/loss': 2.94062876701355, 'test/num_examples': 10000, 'score': 10544.424010038376, 'total_duration': 11363.582971572876, 'accumulated_submission_time': 10544.424010038376, 'accumulated_eval_time': 815.3714473247528, 'accumulated_logging_time': 2.4672508239746094, 'global_step': 23222, 'preemption_count': 0}), (24147, {'train/accuracy': 0.5456640720367432, 'train/loss': 2.072561502456665, 'validation/accuracy': 0.5017399787902832, 'validation/loss': 2.2792351245880127, 'validation/num_examples': 50000, 'test/accuracy': 0.39570000767707825, 'test/loss': 2.8866732120513916, 'test/num_examples': 10000, 'score': 10964.400071620941, 'total_duration': 11819.682758808136, 'accumulated_submission_time': 10964.400071620941, 'accumulated_eval_time': 851.4152765274048, 'accumulated_logging_time': 2.4943430423736572, 'global_step': 24147, 'preemption_count': 0}), (25071, {'train/accuracy': 0.5464843511581421, 'train/loss': 2.0741374492645264, 'validation/accuracy': 0.5124599933624268, 'validation/loss': 2.23947811126709, 'validation/num_examples': 50000, 'test/accuracy': 0.40380001068115234, 'test/loss': 2.85197377204895, 'test/num_examples': 10000, 'score': 11384.33907365799, 'total_duration': 12275.507807016373, 'accumulated_submission_time': 11384.33907365799, 'accumulated_eval_time': 887.2221512794495, 'accumulated_logging_time': 2.5220894813537598, 'global_step': 25071, 'preemption_count': 0}), (25985, {'train/accuracy': 0.5458202958106995, 'train/loss': 2.1079187393188477, 'validation/accuracy': 0.5095399618148804, 'validation/loss': 2.2755002975463867, 'validation/num_examples': 50000, 'test/accuracy': 0.40570002794265747, 'test/loss': 2.873729705810547, 'test/num_examples': 10000, 'score': 11804.526224374771, 'total_duration': 12732.602709293365, 'accumulated_submission_time': 11804.526224374771, 'accumulated_eval_time': 924.0465886592865, 'accumulated_logging_time': 2.5542385578155518, 'global_step': 25985, 'preemption_count': 0}), (26908, {'train/accuracy': 0.554394543170929, 'train/loss': 2.04179310798645, 'validation/accuracy': 0.5141599774360657, 'validation/loss': 2.2237548828125, 'validation/num_examples': 50000, 'test/accuracy': 0.4059000313282013, 'test/loss': 2.8440353870391846, 'test/num_examples': 10000, 'score': 12224.458804368973, 'total_duration': 13188.403272867203, 'accumulated_submission_time': 12224.458804368973, 'accumulated_eval_time': 959.8301196098328, 'accumulated_logging_time': 2.5867295265197754, 'global_step': 26908, 'preemption_count': 0}), (27831, {'train/accuracy': 0.5831835865974426, 'train/loss': 1.9377491474151611, 'validation/accuracy': 0.5192999839782715, 'validation/loss': 2.2157299518585205, 'validation/num_examples': 50000, 'test/accuracy': 0.4076000154018402, 'test/loss': 2.831859588623047, 'test/num_examples': 10000, 'score': 12644.591686487198, 'total_duration': 13644.293235778809, 'accumulated_submission_time': 12644.591686487198, 'accumulated_eval_time': 995.5060038566589, 'accumulated_logging_time': 2.6159276962280273, 'global_step': 27831, 'preemption_count': 0}), (28754, {'train/accuracy': 0.56103515625, 'train/loss': 2.0178725719451904, 'validation/accuracy': 0.5253599882125854, 'validation/loss': 2.184187412261963, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.8013460636138916, 'test/num_examples': 10000, 'score': 13064.928541898727, 'total_duration': 14101.277722358704, 'accumulated_submission_time': 13064.928541898727, 'accumulated_eval_time': 1032.0707716941833, 'accumulated_logging_time': 2.6466317176818848, 'global_step': 28754, 'preemption_count': 0}), (29679, {'train/accuracy': 0.5685741901397705, 'train/loss': 1.9686657190322876, 'validation/accuracy': 0.5270999670028687, 'validation/loss': 2.1590123176574707, 'validation/num_examples': 50000, 'test/accuracy': 0.4166000187397003, 'test/loss': 2.762173652648926, 'test/num_examples': 10000, 'score': 13485.01470541954, 'total_duration': 14557.166967391968, 'accumulated_submission_time': 13485.01470541954, 'accumulated_eval_time': 1067.7865002155304, 'accumulated_logging_time': 2.6822452545166016, 'global_step': 29679, 'preemption_count': 0}), (30601, {'train/accuracy': 0.5899999737739563, 'train/loss': 1.8909555673599243, 'validation/accuracy': 0.5362399816513062, 'validation/loss': 2.1361539363861084, 'validation/num_examples': 50000, 'test/accuracy': 0.4199000298976898, 'test/loss': 2.7619855403900146, 'test/num_examples': 10000, 'score': 13905.378865480423, 'total_duration': 15013.537324428558, 'accumulated_submission_time': 13905.378865480423, 'accumulated_eval_time': 1103.7031581401825, 'accumulated_logging_time': 2.7200143337249756, 'global_step': 30601, 'preemption_count': 0}), (31525, {'train/accuracy': 0.5753320455551147, 'train/loss': 1.9117754697799683, 'validation/accuracy': 0.5366199612617493, 'validation/loss': 2.086362838745117, 'validation/num_examples': 50000, 'test/accuracy': 0.42250001430511475, 'test/loss': 2.7142233848571777, 'test/num_examples': 10000, 'score': 14325.58296585083, 'total_duration': 15470.007066965103, 'accumulated_submission_time': 14325.58296585083, 'accumulated_eval_time': 1139.8854219913483, 'accumulated_logging_time': 2.7515509128570557, 'global_step': 31525, 'preemption_count': 0}), (32447, {'train/accuracy': 0.5731250047683716, 'train/loss': 1.9334790706634521, 'validation/accuracy': 0.5350800156593323, 'validation/loss': 2.114149808883667, 'validation/num_examples': 50000, 'test/accuracy': 0.41780000925064087, 'test/loss': 2.760493278503418, 'test/num_examples': 10000, 'score': 14745.557694911957, 'total_duration': 15925.88976097107, 'accumulated_submission_time': 14745.557694911957, 'accumulated_eval_time': 1175.7121279239655, 'accumulated_logging_time': 2.7820372581481934, 'global_step': 32447, 'preemption_count': 0}), (33367, {'train/accuracy': 0.5793749690055847, 'train/loss': 1.8931059837341309, 'validation/accuracy': 0.5353599786758423, 'validation/loss': 2.1022086143493652, 'validation/num_examples': 50000, 'test/accuracy': 0.4253000319004059, 'test/loss': 2.7308449745178223, 'test/num_examples': 10000, 'score': 15165.785947084427, 'total_duration': 16381.58100271225, 'accumulated_submission_time': 15165.785947084427, 'accumulated_eval_time': 1211.0903453826904, 'accumulated_logging_time': 2.8150620460510254, 'global_step': 33367, 'preemption_count': 0}), (34289, {'train/accuracy': 0.5872460603713989, 'train/loss': 1.8730449676513672, 'validation/accuracy': 0.5454199910163879, 'validation/loss': 2.0598084926605225, 'validation/num_examples': 50000, 'test/accuracy': 0.43480002880096436, 'test/loss': 2.68808913230896, 'test/num_examples': 10000, 'score': 15586.13660979271, 'total_duration': 16838.84478354454, 'accumulated_submission_time': 15586.13660979271, 'accumulated_eval_time': 1247.915519475937, 'accumulated_logging_time': 2.8516042232513428, 'global_step': 34289, 'preemption_count': 0}), (35212, {'train/accuracy': 0.583691418170929, 'train/loss': 1.8888550996780396, 'validation/accuracy': 0.5408999919891357, 'validation/loss': 2.083228588104248, 'validation/num_examples': 50000, 'test/accuracy': 0.4269000291824341, 'test/loss': 2.722228527069092, 'test/num_examples': 10000, 'score': 16006.159181833267, 'total_duration': 17294.766576051712, 'accumulated_submission_time': 16006.159181833267, 'accumulated_eval_time': 1283.7321796417236, 'accumulated_logging_time': 2.881927490234375, 'global_step': 35212, 'preemption_count': 0}), (36132, {'train/accuracy': 0.5895312428474426, 'train/loss': 1.8599177598953247, 'validation/accuracy': 0.5429800152778625, 'validation/loss': 2.068998336791992, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.674314498901367, 'test/num_examples': 10000, 'score': 16426.25867486, 'total_duration': 17750.791348934174, 'accumulated_submission_time': 16426.25867486, 'accumulated_eval_time': 1319.5742392539978, 'accumulated_logging_time': 2.9135375022888184, 'global_step': 36132, 'preemption_count': 0}), (37053, {'train/accuracy': 0.5921093821525574, 'train/loss': 1.889419674873352, 'validation/accuracy': 0.5410400032997131, 'validation/loss': 2.1071622371673584, 'validation/num_examples': 50000, 'test/accuracy': 0.4245000183582306, 'test/loss': 2.7397515773773193, 'test/num_examples': 10000, 'score': 16846.513379335403, 'total_duration': 18207.25081396103, 'accumulated_submission_time': 16846.513379335403, 'accumulated_eval_time': 1355.6950623989105, 'accumulated_logging_time': 2.946035861968994, 'global_step': 37053, 'preemption_count': 0}), (37974, {'train/accuracy': 0.5863280892372131, 'train/loss': 1.8857553005218506, 'validation/accuracy': 0.5454800128936768, 'validation/loss': 2.069470167160034, 'validation/num_examples': 50000, 'test/accuracy': 0.4247000217437744, 'test/loss': 2.710883617401123, 'test/num_examples': 10000, 'score': 17266.63311100006, 'total_duration': 18663.406596660614, 'accumulated_submission_time': 17266.63311100006, 'accumulated_eval_time': 1391.6465280056, 'accumulated_logging_time': 2.9790091514587402, 'global_step': 37974, 'preemption_count': 0}), (38897, {'train/accuracy': 0.5941210985183716, 'train/loss': 1.8830095529556274, 'validation/accuracy': 0.5507400035858154, 'validation/loss': 2.086865186691284, 'validation/num_examples': 50000, 'test/accuracy': 0.43410003185272217, 'test/loss': 2.703827381134033, 'test/num_examples': 10000, 'score': 17686.921609401703, 'total_duration': 19120.326916456223, 'accumulated_submission_time': 17686.921609401703, 'accumulated_eval_time': 1428.1919829845428, 'accumulated_logging_time': 3.0127615928649902, 'global_step': 38897, 'preemption_count': 0}), (39821, {'train/accuracy': 0.6173437237739563, 'train/loss': 1.7155250310897827, 'validation/accuracy': 0.555620014667511, 'validation/loss': 2.0076487064361572, 'validation/num_examples': 50000, 'test/accuracy': 0.4416000247001648, 'test/loss': 2.6214611530303955, 'test/num_examples': 10000, 'score': 18107.200429201126, 'total_duration': 19576.857031822205, 'accumulated_submission_time': 18107.200429201126, 'accumulated_eval_time': 1464.3569984436035, 'accumulated_logging_time': 3.0470962524414062, 'global_step': 39821, 'preemption_count': 0}), (40743, {'train/accuracy': 0.5999609231948853, 'train/loss': 1.8218204975128174, 'validation/accuracy': 0.5581799745559692, 'validation/loss': 1.9893065690994263, 'validation/num_examples': 50000, 'test/accuracy': 0.445000022649765, 'test/loss': 2.613339900970459, 'test/num_examples': 10000, 'score': 18527.431859254837, 'total_duration': 20033.484882116318, 'accumulated_submission_time': 18527.431859254837, 'accumulated_eval_time': 1500.6704943180084, 'accumulated_logging_time': 3.0773818492889404, 'global_step': 40743, 'preemption_count': 0}), (41667, {'train/accuracy': 0.5997461080551147, 'train/loss': 1.8272331953048706, 'validation/accuracy': 0.5560599565505981, 'validation/loss': 2.0224764347076416, 'validation/num_examples': 50000, 'test/accuracy': 0.4398000240325928, 'test/loss': 2.6552605628967285, 'test/num_examples': 10000, 'score': 18947.666902065277, 'total_duration': 20489.236602544785, 'accumulated_submission_time': 18947.666902065277, 'accumulated_eval_time': 1536.0949032306671, 'accumulated_logging_time': 3.118054151535034, 'global_step': 41667, 'preemption_count': 0}), (42591, {'train/accuracy': 0.6067578196525574, 'train/loss': 1.7815989255905151, 'validation/accuracy': 0.5556600093841553, 'validation/loss': 2.0265753269195557, 'validation/num_examples': 50000, 'test/accuracy': 0.4319000244140625, 'test/loss': 2.671928882598877, 'test/num_examples': 10000, 'score': 19367.82270050049, 'total_duration': 20946.551624774933, 'accumulated_submission_time': 19367.82270050049, 'accumulated_eval_time': 1573.170075416565, 'accumulated_logging_time': 3.149902820587158, 'global_step': 42591, 'preemption_count': 0}), (43515, {'train/accuracy': 0.6055273413658142, 'train/loss': 1.7664735317230225, 'validation/accuracy': 0.5644599795341492, 'validation/loss': 1.9504902362823486, 'validation/num_examples': 50000, 'test/accuracy': 0.45210000872612, 'test/loss': 2.5576162338256836, 'test/num_examples': 10000, 'score': 19787.93092918396, 'total_duration': 21402.830537080765, 'accumulated_submission_time': 19787.93092918396, 'accumulated_eval_time': 1609.2521879673004, 'accumulated_logging_time': 3.1867854595184326, 'global_step': 43515, 'preemption_count': 0}), (44436, {'train/accuracy': 0.6005859375, 'train/loss': 1.8105329275131226, 'validation/accuracy': 0.5595600008964539, 'validation/loss': 1.997233271598816, 'validation/num_examples': 50000, 'test/accuracy': 0.4448000192642212, 'test/loss': 2.628516912460327, 'test/num_examples': 10000, 'score': 20207.914827108383, 'total_duration': 21858.304526090622, 'accumulated_submission_time': 20207.914827108383, 'accumulated_eval_time': 1644.6554489135742, 'accumulated_logging_time': 3.2182297706604004, 'global_step': 44436, 'preemption_count': 0}), (45355, {'train/accuracy': 0.6087499856948853, 'train/loss': 1.792636513710022, 'validation/accuracy': 0.5613799691200256, 'validation/loss': 2.0202178955078125, 'validation/num_examples': 50000, 'test/accuracy': 0.44360002875328064, 'test/loss': 2.6396775245666504, 'test/num_examples': 10000, 'score': 20627.911128520966, 'total_duration': 22315.223653554916, 'accumulated_submission_time': 20627.911128520966, 'accumulated_eval_time': 1681.4962322711945, 'accumulated_logging_time': 3.2489402294158936, 'global_step': 45355, 'preemption_count': 0}), (46275, {'train/accuracy': 0.6075195074081421, 'train/loss': 1.7662358283996582, 'validation/accuracy': 0.5708999633789062, 'validation/loss': 1.937433123588562, 'validation/num_examples': 50000, 'test/accuracy': 0.4536000192165375, 'test/loss': 2.589318037033081, 'test/num_examples': 10000, 'score': 21047.840060949326, 'total_duration': 22771.119074821472, 'accumulated_submission_time': 21047.840060949326, 'accumulated_eval_time': 1717.3759191036224, 'accumulated_logging_time': 3.283371686935425, 'global_step': 46275, 'preemption_count': 0}), (47195, {'train/accuracy': 0.6070312261581421, 'train/loss': 1.791407585144043, 'validation/accuracy': 0.5654999613761902, 'validation/loss': 1.9881013631820679, 'validation/num_examples': 50000, 'test/accuracy': 0.4508000314235687, 'test/loss': 2.6037206649780273, 'test/num_examples': 10000, 'score': 21468.07667350769, 'total_duration': 23227.797281980515, 'accumulated_submission_time': 21468.07667350769, 'accumulated_eval_time': 1753.7322096824646, 'accumulated_logging_time': 3.31660795211792, 'global_step': 47195, 'preemption_count': 0}), (48117, {'train/accuracy': 0.6176171898841858, 'train/loss': 1.6974457502365112, 'validation/accuracy': 0.572219967842102, 'validation/loss': 1.9044924974441528, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.536510705947876, 'test/num_examples': 10000, 'score': 21888.20467019081, 'total_duration': 23684.09828400612, 'accumulated_submission_time': 21888.20467019081, 'accumulated_eval_time': 1789.8139972686768, 'accumulated_logging_time': 3.355715274810791, 'global_step': 48117, 'preemption_count': 0}), (49039, {'train/accuracy': 0.6376562118530273, 'train/loss': 1.6303468942642212, 'validation/accuracy': 0.5713199973106384, 'validation/loss': 1.924487590789795, 'validation/num_examples': 50000, 'test/accuracy': 0.45650002360343933, 'test/loss': 2.5707848072052, 'test/num_examples': 10000, 'score': 22308.17884039879, 'total_duration': 24139.977380990982, 'accumulated_submission_time': 22308.17884039879, 'accumulated_eval_time': 1825.6325912475586, 'accumulated_logging_time': 3.390817880630493, 'global_step': 49039, 'preemption_count': 0}), (49960, {'train/accuracy': 0.6035937070846558, 'train/loss': 1.750400185585022, 'validation/accuracy': 0.5683599710464478, 'validation/loss': 1.9232509136199951, 'validation/num_examples': 50000, 'test/accuracy': 0.4570000171661377, 'test/loss': 2.5590219497680664, 'test/num_examples': 10000, 'score': 22728.231050014496, 'total_duration': 24596.820907592773, 'accumulated_submission_time': 22728.231050014496, 'accumulated_eval_time': 1862.3381762504578, 'accumulated_logging_time': 3.424906015396118, 'global_step': 49960, 'preemption_count': 0}), (50881, {'train/accuracy': 0.6149609088897705, 'train/loss': 1.7614407539367676, 'validation/accuracy': 0.5702599883079529, 'validation/loss': 1.953475832939148, 'validation/num_examples': 50000, 'test/accuracy': 0.4577000141143799, 'test/loss': 2.5869696140289307, 'test/num_examples': 10000, 'score': 23148.14708518982, 'total_duration': 25050.40443754196, 'accumulated_submission_time': 23148.14708518982, 'accumulated_eval_time': 1895.92019033432, 'accumulated_logging_time': 3.456855535507202, 'global_step': 50881, 'preemption_count': 0}), (51802, {'train/accuracy': 0.6318554282188416, 'train/loss': 1.6667355298995972, 'validation/accuracy': 0.5773599743843079, 'validation/loss': 1.9151332378387451, 'validation/num_examples': 50000, 'test/accuracy': 0.45660001039505005, 'test/loss': 2.557199239730835, 'test/num_examples': 10000, 'score': 23568.13712787628, 'total_duration': 25507.155699014664, 'accumulated_submission_time': 23568.13712787628, 'accumulated_eval_time': 1932.593270778656, 'accumulated_logging_time': 3.4931063652038574, 'global_step': 51802, 'preemption_count': 0}), (52724, {'train/accuracy': 0.6173242330551147, 'train/loss': 1.7071141004562378, 'validation/accuracy': 0.578220009803772, 'validation/loss': 1.8849382400512695, 'validation/num_examples': 50000, 'test/accuracy': 0.45740002393722534, 'test/loss': 2.535064697265625, 'test/num_examples': 10000, 'score': 23988.11474442482, 'total_duration': 25964.319465875626, 'accumulated_submission_time': 23988.11474442482, 'accumulated_eval_time': 1969.6955354213715, 'accumulated_logging_time': 3.5249931812286377, 'global_step': 52724, 'preemption_count': 0}), (53645, {'train/accuracy': 0.6204687356948853, 'train/loss': 1.695976972579956, 'validation/accuracy': 0.5777400135993958, 'validation/loss': 1.8915741443634033, 'validation/num_examples': 50000, 'test/accuracy': 0.4635000228881836, 'test/loss': 2.5295896530151367, 'test/num_examples': 10000, 'score': 24408.527057886124, 'total_duration': 26420.55820798874, 'accumulated_submission_time': 24408.527057886124, 'accumulated_eval_time': 2005.4340209960938, 'accumulated_logging_time': 3.5617029666900635, 'global_step': 53645, 'preemption_count': 0}), (54568, {'train/accuracy': 0.6335741877555847, 'train/loss': 1.6409857273101807, 'validation/accuracy': 0.5818600058555603, 'validation/loss': 1.8811854124069214, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.53002667427063, 'test/num_examples': 10000, 'score': 24828.74754881859, 'total_duration': 26878.447025060654, 'accumulated_submission_time': 24828.74754881859, 'accumulated_eval_time': 2043.0168118476868, 'accumulated_logging_time': 3.5952277183532715, 'global_step': 54568, 'preemption_count': 0}), (55491, {'train/accuracy': 0.6231836080551147, 'train/loss': 1.6743615865707397, 'validation/accuracy': 0.5826799869537354, 'validation/loss': 1.8473048210144043, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.4718806743621826, 'test/num_examples': 10000, 'score': 25248.86332678795, 'total_duration': 27335.411857128143, 'accumulated_submission_time': 25248.86332678795, 'accumulated_eval_time': 2079.7810554504395, 'accumulated_logging_time': 3.627671003341675, 'global_step': 55491, 'preemption_count': 0}), (56413, {'train/accuracy': 0.6209765672683716, 'train/loss': 1.711039662361145, 'validation/accuracy': 0.5796599984169006, 'validation/loss': 1.9055297374725342, 'validation/num_examples': 50000, 'test/accuracy': 0.465800017118454, 'test/loss': 2.5380899906158447, 'test/num_examples': 10000, 'score': 25669.019901752472, 'total_duration': 27793.275195598602, 'accumulated_submission_time': 25669.019901752472, 'accumulated_eval_time': 2117.397604942322, 'accumulated_logging_time': 3.666386127471924, 'global_step': 56413, 'preemption_count': 0}), (57336, {'train/accuracy': 0.6293359398841858, 'train/loss': 1.6536279916763306, 'validation/accuracy': 0.5833799839019775, 'validation/loss': 1.8656693696975708, 'validation/num_examples': 50000, 'test/accuracy': 0.46150001883506775, 'test/loss': 2.495638608932495, 'test/num_examples': 10000, 'score': 26089.111981153488, 'total_duration': 28250.289984464645, 'accumulated_submission_time': 26089.111981153488, 'accumulated_eval_time': 2154.234006166458, 'accumulated_logging_time': 3.701314687728882, 'global_step': 57336, 'preemption_count': 0}), (58258, {'train/accuracy': 0.6524804830551147, 'train/loss': 1.5615637302398682, 'validation/accuracy': 0.5855000019073486, 'validation/loss': 1.8619376420974731, 'validation/num_examples': 50000, 'test/accuracy': 0.4659000337123871, 'test/loss': 2.4890189170837402, 'test/num_examples': 10000, 'score': 26509.280297517776, 'total_duration': 28708.022760629654, 'accumulated_submission_time': 26509.280297517776, 'accumulated_eval_time': 2191.7146582603455, 'accumulated_logging_time': 3.7332868576049805, 'global_step': 58258, 'preemption_count': 0}), (59178, {'train/accuracy': 0.6221289038658142, 'train/loss': 1.7175687551498413, 'validation/accuracy': 0.5812399983406067, 'validation/loss': 1.9062336683273315, 'validation/num_examples': 50000, 'test/accuracy': 0.4636000096797943, 'test/loss': 2.526654005050659, 'test/num_examples': 10000, 'score': 26929.290602445602, 'total_duration': 29164.714790582657, 'accumulated_submission_time': 26929.290602445602, 'accumulated_eval_time': 2228.310579776764, 'accumulated_logging_time': 3.7675628662109375, 'global_step': 59178, 'preemption_count': 0}), (60097, {'train/accuracy': 0.6240624785423279, 'train/loss': 1.7262558937072754, 'validation/accuracy': 0.5816799998283386, 'validation/loss': 1.931992769241333, 'validation/num_examples': 50000, 'test/accuracy': 0.4661000072956085, 'test/loss': 2.5786662101745605, 'test/num_examples': 10000, 'score': 27349.29820728302, 'total_duration': 29622.019852399826, 'accumulated_submission_time': 27349.29820728302, 'accumulated_eval_time': 2265.5188434123993, 'accumulated_logging_time': 3.8040237426757812, 'global_step': 60097, 'preemption_count': 0}), (61019, {'train/accuracy': 0.6532421708106995, 'train/loss': 1.5591392517089844, 'validation/accuracy': 0.5931999683380127, 'validation/loss': 1.8360151052474976, 'validation/num_examples': 50000, 'test/accuracy': 0.4717000126838684, 'test/loss': 2.467651128768921, 'test/num_examples': 10000, 'score': 27769.632704496384, 'total_duration': 30078.310692310333, 'accumulated_submission_time': 27769.632704496384, 'accumulated_eval_time': 2301.384221792221, 'accumulated_logging_time': 3.8424084186553955, 'global_step': 61019, 'preemption_count': 0}), (61941, {'train/accuracy': 0.6307030916213989, 'train/loss': 1.6259307861328125, 'validation/accuracy': 0.5897600054740906, 'validation/loss': 1.8175615072250366, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.4553451538085938, 'test/num_examples': 10000, 'score': 28189.844376802444, 'total_duration': 30536.711680173874, 'accumulated_submission_time': 28189.844376802444, 'accumulated_eval_time': 2339.4837741851807, 'accumulated_logging_time': 3.8803889751434326, 'global_step': 61941, 'preemption_count': 0}), (62861, {'train/accuracy': 0.6349999904632568, 'train/loss': 1.6504743099212646, 'validation/accuracy': 0.5909799933433533, 'validation/loss': 1.858465313911438, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.47857403755188, 'test/num_examples': 10000, 'score': 28609.88588285446, 'total_duration': 30992.379618406296, 'accumulated_submission_time': 28609.88588285446, 'accumulated_eval_time': 2375.022164583206, 'accumulated_logging_time': 3.916743040084839, 'global_step': 62861, 'preemption_count': 0}), (63784, {'train/accuracy': 0.6424999833106995, 'train/loss': 1.608486294746399, 'validation/accuracy': 0.5922999978065491, 'validation/loss': 1.8405706882476807, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.484283208847046, 'test/num_examples': 10000, 'score': 29029.89266204834, 'total_duration': 31449.811811208725, 'accumulated_submission_time': 29029.89266204834, 'accumulated_eval_time': 2412.3593595027924, 'accumulated_logging_time': 3.953082323074341, 'global_step': 63784, 'preemption_count': 0}), (64704, {'train/accuracy': 0.6398632526397705, 'train/loss': 1.6252983808517456, 'validation/accuracy': 0.5931400060653687, 'validation/loss': 1.8280532360076904, 'validation/num_examples': 50000, 'test/accuracy': 0.4734000265598297, 'test/loss': 2.459376335144043, 'test/num_examples': 10000, 'score': 29449.915155172348, 'total_duration': 31906.87753367424, 'accumulated_submission_time': 29449.915155172348, 'accumulated_eval_time': 2449.3163084983826, 'accumulated_logging_time': 3.9879281520843506, 'global_step': 64704, 'preemption_count': 0}), (65625, {'train/accuracy': 0.6363476514816284, 'train/loss': 1.6080312728881836, 'validation/accuracy': 0.596019983291626, 'validation/loss': 1.798905611038208, 'validation/num_examples': 50000, 'test/accuracy': 0.47280001640319824, 'test/loss': 2.4222564697265625, 'test/num_examples': 10000, 'score': 29870.18850684166, 'total_duration': 32366.242438316345, 'accumulated_submission_time': 29870.18850684166, 'accumulated_eval_time': 2488.3136699199677, 'accumulated_logging_time': 4.030225992202759, 'global_step': 65625, 'preemption_count': 0}), (66548, {'train/accuracy': 0.6463476419448853, 'train/loss': 1.5539718866348267, 'validation/accuracy': 0.593779981136322, 'validation/loss': 1.7889726161956787, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.425429582595825, 'test/num_examples': 10000, 'score': 30290.33807373047, 'total_duration': 32824.26109623909, 'accumulated_submission_time': 30290.33807373047, 'accumulated_eval_time': 2526.093469142914, 'accumulated_logging_time': 4.068289279937744, 'global_step': 66548, 'preemption_count': 0}), (67470, {'train/accuracy': 0.6481249928474426, 'train/loss': 1.55096435546875, 'validation/accuracy': 0.5956199765205383, 'validation/loss': 1.7877963781356812, 'validation/num_examples': 50000, 'test/accuracy': 0.47870001196861267, 'test/loss': 2.4377787113189697, 'test/num_examples': 10000, 'score': 30710.64385533333, 'total_duration': 33281.84917807579, 'accumulated_submission_time': 30710.64385533333, 'accumulated_eval_time': 2563.291362285614, 'accumulated_logging_time': 4.101492404937744, 'global_step': 67470, 'preemption_count': 0}), (68391, {'train/accuracy': 0.6390038728713989, 'train/loss': 1.6182537078857422, 'validation/accuracy': 0.5971599817276001, 'validation/loss': 1.8191072940826416, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.4577841758728027, 'test/num_examples': 10000, 'score': 31130.579701900482, 'total_duration': 33738.699846982956, 'accumulated_submission_time': 31130.579701900482, 'accumulated_eval_time': 2600.11688375473, 'accumulated_logging_time': 4.13883113861084, 'global_step': 68391, 'preemption_count': 0}), (69310, {'train/accuracy': 0.6440038681030273, 'train/loss': 1.572296142578125, 'validation/accuracy': 0.5978800058364868, 'validation/loss': 1.7804210186004639, 'validation/num_examples': 50000, 'test/accuracy': 0.4824000298976898, 'test/loss': 2.4211783409118652, 'test/num_examples': 10000, 'score': 31550.541101932526, 'total_duration': 34196.13939833641, 'accumulated_submission_time': 31550.541101932526, 'accumulated_eval_time': 2637.507829427719, 'accumulated_logging_time': 4.173320293426514, 'global_step': 69310, 'preemption_count': 0}), (70234, {'train/accuracy': 0.6702734231948853, 'train/loss': 1.4641485214233398, 'validation/accuracy': 0.6001200079917908, 'validation/loss': 1.7625842094421387, 'validation/num_examples': 50000, 'test/accuracy': 0.48190003633499146, 'test/loss': 2.420220375061035, 'test/num_examples': 10000, 'score': 31970.663256645203, 'total_duration': 34651.5661213398, 'accumulated_submission_time': 31970.663256645203, 'accumulated_eval_time': 2672.718720436096, 'accumulated_logging_time': 4.21485447883606, 'global_step': 70234, 'preemption_count': 0}), (71154, {'train/accuracy': 0.6441210508346558, 'train/loss': 1.5635147094726562, 'validation/accuracy': 0.6025399565696716, 'validation/loss': 1.7569859027862549, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.3740594387054443, 'test/num_examples': 10000, 'score': 32390.83561515808, 'total_duration': 35109.47786331177, 'accumulated_submission_time': 32390.83561515808, 'accumulated_eval_time': 2710.3715307712555, 'accumulated_logging_time': 4.249778985977173, 'global_step': 71154, 'preemption_count': 0}), (72077, {'train/accuracy': 0.6434960961341858, 'train/loss': 1.6113227605819702, 'validation/accuracy': 0.6014800071716309, 'validation/loss': 1.8047555685043335, 'validation/num_examples': 50000, 'test/accuracy': 0.48020002245903015, 'test/loss': 2.438359498977661, 'test/num_examples': 10000, 'score': 32810.90993022919, 'total_duration': 35567.80102777481, 'accumulated_submission_time': 32810.90993022919, 'accumulated_eval_time': 2748.524088859558, 'accumulated_logging_time': 4.293323755264282, 'global_step': 72077, 'preemption_count': 0}), (72996, {'train/accuracy': 0.6573632955551147, 'train/loss': 1.5504130125045776, 'validation/accuracy': 0.5985599756240845, 'validation/loss': 1.8023332357406616, 'validation/num_examples': 50000, 'test/accuracy': 0.48250001668930054, 'test/loss': 2.439271926879883, 'test/num_examples': 10000, 'score': 33231.244406461716, 'total_duration': 36026.76460838318, 'accumulated_submission_time': 33231.244406461716, 'accumulated_eval_time': 2787.061530351639, 'accumulated_logging_time': 4.332884788513184, 'global_step': 72996, 'preemption_count': 0}), (73920, {'train/accuracy': 0.6471093893051147, 'train/loss': 1.6129900217056274, 'validation/accuracy': 0.6095799803733826, 'validation/loss': 1.7827093601226807, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.417206048965454, 'test/num_examples': 10000, 'score': 33651.5921421051, 'total_duration': 36484.73053359985, 'accumulated_submission_time': 33651.5921421051, 'accumulated_eval_time': 2824.5923268795013, 'accumulated_logging_time': 4.368396282196045, 'global_step': 73920, 'preemption_count': 0}), (74841, {'train/accuracy': 0.6474413871765137, 'train/loss': 1.5477566719055176, 'validation/accuracy': 0.606440007686615, 'validation/loss': 1.7396602630615234, 'validation/num_examples': 50000, 'test/accuracy': 0.48570001125335693, 'test/loss': 2.386080503463745, 'test/num_examples': 10000, 'score': 34071.80634212494, 'total_duration': 36941.8312625885, 'accumulated_submission_time': 34071.80634212494, 'accumulated_eval_time': 2861.388499736786, 'accumulated_logging_time': 4.406642913818359, 'global_step': 74841, 'preemption_count': 0}), (75764, {'train/accuracy': 0.6582812070846558, 'train/loss': 1.5243412256240845, 'validation/accuracy': 0.6102199554443359, 'validation/loss': 1.744834065437317, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.3967623710632324, 'test/num_examples': 10000, 'score': 34492.00156569481, 'total_duration': 37399.13010430336, 'accumulated_submission_time': 34492.00156569481, 'accumulated_eval_time': 2898.396971464157, 'accumulated_logging_time': 4.448941230773926, 'global_step': 75764, 'preemption_count': 0}), (76687, {'train/accuracy': 0.6499804258346558, 'train/loss': 1.5522480010986328, 'validation/accuracy': 0.6068199872970581, 'validation/loss': 1.7552516460418701, 'validation/num_examples': 50000, 'test/accuracy': 0.4863000214099884, 'test/loss': 2.3870294094085693, 'test/num_examples': 10000, 'score': 34912.220725774765, 'total_duration': 37856.34967160225, 'accumulated_submission_time': 34912.220725774765, 'accumulated_eval_time': 2935.307032585144, 'accumulated_logging_time': 4.486681699752808, 'global_step': 76687, 'preemption_count': 0}), (77611, {'train/accuracy': 0.655078113079071, 'train/loss': 1.556774377822876, 'validation/accuracy': 0.6091799736022949, 'validation/loss': 1.7530089616775513, 'validation/num_examples': 50000, 'test/accuracy': 0.4870000183582306, 'test/loss': 2.3812808990478516, 'test/num_examples': 10000, 'score': 35332.43723273277, 'total_duration': 38313.127844810486, 'accumulated_submission_time': 35332.43723273277, 'accumulated_eval_time': 2971.7798516750336, 'accumulated_logging_time': 4.523826599121094, 'global_step': 77611, 'preemption_count': 0}), (78533, {'train/accuracy': 0.6649804711341858, 'train/loss': 1.5041078329086304, 'validation/accuracy': 0.6135199666023254, 'validation/loss': 1.7273281812667847, 'validation/num_examples': 50000, 'test/accuracy': 0.4961000382900238, 'test/loss': 2.3592467308044434, 'test/num_examples': 10000, 'score': 35752.73362803459, 'total_duration': 38772.0255856514, 'accumulated_submission_time': 35752.73362803459, 'accumulated_eval_time': 3010.292057991028, 'accumulated_logging_time': 4.561509370803833, 'global_step': 78533, 'preemption_count': 0}), (79454, {'train/accuracy': 0.6874414086341858, 'train/loss': 1.3758230209350586, 'validation/accuracy': 0.6157599687576294, 'validation/loss': 1.6964789628982544, 'validation/num_examples': 50000, 'test/accuracy': 0.48740002512931824, 'test/loss': 2.3509433269500732, 'test/num_examples': 10000, 'score': 36172.88255786896, 'total_duration': 39229.4043803215, 'accumulated_submission_time': 36172.88255786896, 'accumulated_eval_time': 3047.432544708252, 'accumulated_logging_time': 4.599277496337891, 'global_step': 79454, 'preemption_count': 0}), (80378, {'train/accuracy': 0.6550585627555847, 'train/loss': 1.5762782096862793, 'validation/accuracy': 0.6106799840927124, 'validation/loss': 1.7810405492782593, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.4137213230133057, 'test/num_examples': 10000, 'score': 36593.105981349945, 'total_duration': 39687.296456336975, 'accumulated_submission_time': 36593.105981349945, 'accumulated_eval_time': 3085.007307291031, 'accumulated_logging_time': 4.6409912109375, 'global_step': 80378, 'preemption_count': 0}), (81300, {'train/accuracy': 0.6625585556030273, 'train/loss': 1.5070688724517822, 'validation/accuracy': 0.6144799590110779, 'validation/loss': 1.7260377407073975, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.3620481491088867, 'test/num_examples': 10000, 'score': 37013.32633161545, 'total_duration': 40145.815786361694, 'accumulated_submission_time': 37013.32633161545, 'accumulated_eval_time': 3123.217358827591, 'accumulated_logging_time': 4.677295207977295, 'global_step': 81300, 'preemption_count': 0}), (82221, {'train/accuracy': 0.6760546565055847, 'train/loss': 1.452250361442566, 'validation/accuracy': 0.6148200035095215, 'validation/loss': 1.7269800901412964, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.362778425216675, 'test/num_examples': 10000, 'score': 37433.646663188934, 'total_duration': 40603.81119298935, 'accumulated_submission_time': 37433.646663188934, 'accumulated_eval_time': 3160.800005197525, 'accumulated_logging_time': 4.718563795089722, 'global_step': 82221, 'preemption_count': 0}), (83141, {'train/accuracy': 0.6602343320846558, 'train/loss': 1.4971674680709839, 'validation/accuracy': 0.6192399859428406, 'validation/loss': 1.6920735836029053, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.319068193435669, 'test/num_examples': 10000, 'score': 37853.990706920624, 'total_duration': 41062.76618885994, 'accumulated_submission_time': 37853.990706920624, 'accumulated_eval_time': 3199.3191499710083, 'accumulated_logging_time': 4.758185863494873, 'global_step': 83141, 'preemption_count': 0}), (84061, {'train/accuracy': 0.6661523580551147, 'train/loss': 1.5092904567718506, 'validation/accuracy': 0.6180399656295776, 'validation/loss': 1.7301098108291626, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.355710029602051, 'test/num_examples': 10000, 'score': 38274.122478723526, 'total_duration': 41521.33239722252, 'accumulated_submission_time': 38274.122478723526, 'accumulated_eval_time': 3237.6630806922913, 'accumulated_logging_time': 4.796828985214233, 'global_step': 84061, 'preemption_count': 0}), (84983, {'train/accuracy': 0.6742187142372131, 'train/loss': 1.436706781387329, 'validation/accuracy': 0.6194199919700623, 'validation/loss': 1.680558204650879, 'validation/num_examples': 50000, 'test/accuracy': 0.5003000497817993, 'test/loss': 2.3204102516174316, 'test/num_examples': 10000, 'score': 38694.25512552261, 'total_duration': 41979.15235233307, 'accumulated_submission_time': 38694.25512552261, 'accumulated_eval_time': 3275.2562849521637, 'accumulated_logging_time': 4.83923602104187, 'global_step': 84983, 'preemption_count': 0}), (85905, {'train/accuracy': 0.657031238079071, 'train/loss': 1.5600054264068604, 'validation/accuracy': 0.6131199598312378, 'validation/loss': 1.742652177810669, 'validation/num_examples': 50000, 'test/accuracy': 0.4909000098705292, 'test/loss': 2.3830223083496094, 'test/num_examples': 10000, 'score': 39114.24275946617, 'total_duration': 42437.045094013214, 'accumulated_submission_time': 39114.24275946617, 'accumulated_eval_time': 3313.068485021591, 'accumulated_logging_time': 4.880053997039795, 'global_step': 85905, 'preemption_count': 0}), (86826, {'train/accuracy': 0.6669530868530273, 'train/loss': 1.5276052951812744, 'validation/accuracy': 0.6200399994850159, 'validation/loss': 1.735192894935608, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.3761723041534424, 'test/num_examples': 10000, 'score': 39534.21739983559, 'total_duration': 42895.48432254791, 'accumulated_submission_time': 39534.21739983559, 'accumulated_eval_time': 3351.4402787685394, 'accumulated_logging_time': 4.920087099075317, 'global_step': 86826, 'preemption_count': 0}), (87749, {'train/accuracy': 0.6812499761581421, 'train/loss': 1.3998364210128784, 'validation/accuracy': 0.6264599561691284, 'validation/loss': 1.6430705785751343, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.299318552017212, 'test/num_examples': 10000, 'score': 39954.22675919533, 'total_duration': 43351.513491392136, 'accumulated_submission_time': 39954.22675919533, 'accumulated_eval_time': 3387.36865234375, 'accumulated_logging_time': 4.959564685821533, 'global_step': 87749, 'preemption_count': 0}), (88670, {'train/accuracy': 0.6852734088897705, 'train/loss': 1.405772089958191, 'validation/accuracy': 0.6255399584770203, 'validation/loss': 1.6802127361297607, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.3180766105651855, 'test/num_examples': 10000, 'score': 40374.383286714554, 'total_duration': 43810.477133750916, 'accumulated_submission_time': 40374.383286714554, 'accumulated_eval_time': 3426.085087776184, 'accumulated_logging_time': 4.998172760009766, 'global_step': 88670, 'preemption_count': 0}), (89592, {'train/accuracy': 0.6747655868530273, 'train/loss': 1.4603726863861084, 'validation/accuracy': 0.6269800066947937, 'validation/loss': 1.6675292253494263, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.291046142578125, 'test/num_examples': 10000, 'score': 40794.58645391464, 'total_duration': 44268.12525868416, 'accumulated_submission_time': 40794.58645391464, 'accumulated_eval_time': 3463.438698530197, 'accumulated_logging_time': 5.036881446838379, 'global_step': 89592, 'preemption_count': 0}), (90515, {'train/accuracy': 0.6829491853713989, 'train/loss': 1.4198222160339355, 'validation/accuracy': 0.6295199990272522, 'validation/loss': 1.6494418382644653, 'validation/num_examples': 50000, 'test/accuracy': 0.5087000131607056, 'test/loss': 2.2928144931793213, 'test/num_examples': 10000, 'score': 41214.94577026367, 'total_duration': 44726.574046611786, 'accumulated_submission_time': 41214.94577026367, 'accumulated_eval_time': 3501.4375364780426, 'accumulated_logging_time': 5.075735569000244, 'global_step': 90515, 'preemption_count': 0}), (91434, {'train/accuracy': 0.6955859065055847, 'train/loss': 1.3533570766448975, 'validation/accuracy': 0.6298800110816956, 'validation/loss': 1.656938910484314, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.301645517349243, 'test/num_examples': 10000, 'score': 41635.21865081787, 'total_duration': 45186.903146505356, 'accumulated_submission_time': 41635.21865081787, 'accumulated_eval_time': 3541.404905796051, 'accumulated_logging_time': 5.113611698150635, 'global_step': 91434, 'preemption_count': 0}), (92353, {'train/accuracy': 0.6741992235183716, 'train/loss': 1.4724483489990234, 'validation/accuracy': 0.6269999742507935, 'validation/loss': 1.6782883405685425, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.322950601577759, 'test/num_examples': 10000, 'score': 42055.23688745499, 'total_duration': 45644.38531947136, 'accumulated_submission_time': 42055.23688745499, 'accumulated_eval_time': 3578.7778816223145, 'accumulated_logging_time': 5.15111517906189, 'global_step': 92353, 'preemption_count': 0}), (93277, {'train/accuracy': 0.681933581829071, 'train/loss': 1.4165855646133423, 'validation/accuracy': 0.6343399882316589, 'validation/loss': 1.6316964626312256, 'validation/num_examples': 50000, 'test/accuracy': 0.5100000500679016, 'test/loss': 2.268878698348999, 'test/num_examples': 10000, 'score': 42475.596818208694, 'total_duration': 46103.61071944237, 'accumulated_submission_time': 42475.596818208694, 'accumulated_eval_time': 3617.5498201847076, 'accumulated_logging_time': 5.1927220821380615, 'global_step': 93277, 'preemption_count': 0}), (94198, {'train/accuracy': 0.6891406178474426, 'train/loss': 1.4098796844482422, 'validation/accuracy': 0.633080005645752, 'validation/loss': 1.6661624908447266, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.2933971881866455, 'test/num_examples': 10000, 'score': 42895.638721227646, 'total_duration': 46561.52732515335, 'accumulated_submission_time': 42895.638721227646, 'accumulated_eval_time': 3655.3306124210358, 'accumulated_logging_time': 5.234953880310059, 'global_step': 94198, 'preemption_count': 0}), (95115, {'train/accuracy': 0.6744726300239563, 'train/loss': 1.4238649606704712, 'validation/accuracy': 0.6321200132369995, 'validation/loss': 1.6284260749816895, 'validation/num_examples': 50000, 'test/accuracy': 0.5033000111579895, 'test/loss': 2.2706520557403564, 'test/num_examples': 10000, 'score': 43315.5960958004, 'total_duration': 47019.021444797516, 'accumulated_submission_time': 43315.5960958004, 'accumulated_eval_time': 3692.7725965976715, 'accumulated_logging_time': 5.277697801589966, 'global_step': 95115, 'preemption_count': 0}), (96038, {'train/accuracy': 0.6746679544448853, 'train/loss': 1.4650830030441284, 'validation/accuracy': 0.6334800124168396, 'validation/loss': 1.658732295036316, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.325078248977661, 'test/num_examples': 10000, 'score': 43735.618525505066, 'total_duration': 47478.4274597168, 'accumulated_submission_time': 43735.618525505066, 'accumulated_eval_time': 3732.062595129013, 'accumulated_logging_time': 5.319170951843262, 'global_step': 96038, 'preemption_count': 0}), (96961, {'train/accuracy': 0.6908984184265137, 'train/loss': 1.3704414367675781, 'validation/accuracy': 0.6380000114440918, 'validation/loss': 1.6115221977233887, 'validation/num_examples': 50000, 'test/accuracy': 0.5160000324249268, 'test/loss': 2.247906446456909, 'test/num_examples': 10000, 'score': 44155.63729739189, 'total_duration': 47935.84779524803, 'accumulated_submission_time': 44155.63729739189, 'accumulated_eval_time': 3769.371959209442, 'accumulated_logging_time': 5.359732389450073, 'global_step': 96961, 'preemption_count': 0}), (97883, {'train/accuracy': 0.6875585913658142, 'train/loss': 1.3868331909179688, 'validation/accuracy': 0.6341399550437927, 'validation/loss': 1.6226903200149536, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.252286434173584, 'test/num_examples': 10000, 'score': 44575.90721774101, 'total_duration': 48391.369034051895, 'accumulated_submission_time': 44575.90721774101, 'accumulated_eval_time': 3804.532338619232, 'accumulated_logging_time': 5.398097276687622, 'global_step': 97883, 'preemption_count': 0}), (98806, {'train/accuracy': 0.6914257407188416, 'train/loss': 1.358039379119873, 'validation/accuracy': 0.6440399885177612, 'validation/loss': 1.5768593549728394, 'validation/num_examples': 50000, 'test/accuracy': 0.5145000219345093, 'test/loss': 2.221693754196167, 'test/num_examples': 10000, 'score': 44995.98552107811, 'total_duration': 48849.69229388237, 'accumulated_submission_time': 44995.98552107811, 'accumulated_eval_time': 3842.685833454132, 'accumulated_logging_time': 5.437853097915649, 'global_step': 98806, 'preemption_count': 0}), (99725, {'train/accuracy': 0.6920703053474426, 'train/loss': 1.3738312721252441, 'validation/accuracy': 0.6429199576377869, 'validation/loss': 1.6061257123947144, 'validation/num_examples': 50000, 'test/accuracy': 0.5164000391960144, 'test/loss': 2.2455978393554688, 'test/num_examples': 10000, 'score': 45416.36224722862, 'total_duration': 49308.9356777668, 'accumulated_submission_time': 45416.36224722862, 'accumulated_eval_time': 3881.4554891586304, 'accumulated_logging_time': 5.483229875564575, 'global_step': 99725, 'preemption_count': 0}), (100644, {'train/accuracy': 0.7134960889816284, 'train/loss': 1.2532609701156616, 'validation/accuracy': 0.6430999636650085, 'validation/loss': 1.5673333406448364, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.2153022289276123, 'test/num_examples': 10000, 'score': 45836.51930832863, 'total_duration': 49767.70958662033, 'accumulated_submission_time': 45836.51930832863, 'accumulated_eval_time': 3919.9752383232117, 'accumulated_logging_time': 5.5283708572387695, 'global_step': 100644, 'preemption_count': 0}), (101569, {'train/accuracy': 0.6882616877555847, 'train/loss': 1.3756904602050781, 'validation/accuracy': 0.6430999636650085, 'validation/loss': 1.592112421989441, 'validation/num_examples': 50000, 'test/accuracy': 0.5160000324249268, 'test/loss': 2.235506534576416, 'test/num_examples': 10000, 'score': 46256.86202788353, 'total_duration': 50226.20248699188, 'accumulated_submission_time': 46256.86202788353, 'accumulated_eval_time': 3958.030996799469, 'accumulated_logging_time': 5.570519685745239, 'global_step': 101569, 'preemption_count': 0}), (102491, {'train/accuracy': 0.695605456829071, 'train/loss': 1.3783432245254517, 'validation/accuracy': 0.6459800004959106, 'validation/loss': 1.5988671779632568, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.2385599613189697, 'test/num_examples': 10000, 'score': 46676.82009387016, 'total_duration': 50683.19379091263, 'accumulated_submission_time': 46676.82009387016, 'accumulated_eval_time': 3994.964736223221, 'accumulated_logging_time': 5.617609024047852, 'global_step': 102491, 'preemption_count': 0}), (103413, {'train/accuracy': 0.7080858945846558, 'train/loss': 1.311424970626831, 'validation/accuracy': 0.6429799795150757, 'validation/loss': 1.5880955457687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5222000479698181, 'test/loss': 2.2103865146636963, 'test/num_examples': 10000, 'score': 47097.179240942, 'total_duration': 51143.90802812576, 'accumulated_submission_time': 47097.179240942, 'accumulated_eval_time': 4035.222962141037, 'accumulated_logging_time': 5.66266655921936, 'global_step': 103413, 'preemption_count': 0}), (104333, {'train/accuracy': 0.6900585889816284, 'train/loss': 1.4127657413482666, 'validation/accuracy': 0.6438199877738953, 'validation/loss': 1.6250956058502197, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.272473096847534, 'test/num_examples': 10000, 'score': 47517.37286019325, 'total_duration': 51604.820660829544, 'accumulated_submission_time': 47517.37286019325, 'accumulated_eval_time': 4075.8489694595337, 'accumulated_logging_time': 5.703973770141602, 'global_step': 104333, 'preemption_count': 0}), (105255, {'train/accuracy': 0.7038280963897705, 'train/loss': 1.3086973428726196, 'validation/accuracy': 0.6522600054740906, 'validation/loss': 1.5347038507461548, 'validation/num_examples': 50000, 'test/accuracy': 0.5284000039100647, 'test/loss': 2.170280933380127, 'test/num_examples': 10000, 'score': 47937.850818157196, 'total_duration': 52063.75856423378, 'accumulated_submission_time': 47937.850818157196, 'accumulated_eval_time': 4114.217289924622, 'accumulated_logging_time': 5.7432332038879395, 'global_step': 105255, 'preemption_count': 0}), (106178, {'train/accuracy': 0.695117175579071, 'train/loss': 1.3671385049819946, 'validation/accuracy': 0.638480007648468, 'validation/loss': 1.6232553720474243, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.258666753768921, 'test/num_examples': 10000, 'score': 48358.08609056473, 'total_duration': 52523.495888233185, 'accumulated_submission_time': 48358.08609056473, 'accumulated_eval_time': 4153.619336605072, 'accumulated_logging_time': 5.791689872741699, 'global_step': 106178, 'preemption_count': 0}), (107099, {'train/accuracy': 0.7011132836341858, 'train/loss': 1.3185851573944092, 'validation/accuracy': 0.6495000123977661, 'validation/loss': 1.5532654523849487, 'validation/num_examples': 50000, 'test/accuracy': 0.5246000289916992, 'test/loss': 2.1825060844421387, 'test/num_examples': 10000, 'score': 48778.02906394005, 'total_duration': 52983.47541809082, 'accumulated_submission_time': 48778.02906394005, 'accumulated_eval_time': 4193.563049793243, 'accumulated_logging_time': 5.832031965255737, 'global_step': 107099, 'preemption_count': 0}), (108022, {'train/accuracy': 0.7089062333106995, 'train/loss': 1.2863578796386719, 'validation/accuracy': 0.6575999855995178, 'validation/loss': 1.5152268409729004, 'validation/num_examples': 50000, 'test/accuracy': 0.5370000004768372, 'test/loss': 2.137031316757202, 'test/num_examples': 10000, 'score': 49198.16246008873, 'total_duration': 53441.33406162262, 'accumulated_submission_time': 49198.16246008873, 'accumulated_eval_time': 4231.1956782341, 'accumulated_logging_time': 5.872812747955322, 'global_step': 108022, 'preemption_count': 0}), (108948, {'train/accuracy': 0.7115429639816284, 'train/loss': 1.2524975538253784, 'validation/accuracy': 0.6578199863433838, 'validation/loss': 1.5066378116607666, 'validation/num_examples': 50000, 'test/accuracy': 0.5306000113487244, 'test/loss': 2.1434850692749023, 'test/num_examples': 10000, 'score': 49618.37132978439, 'total_duration': 53901.068217754364, 'accumulated_submission_time': 49618.37132978439, 'accumulated_eval_time': 4270.6265552043915, 'accumulated_logging_time': 5.915415287017822, 'global_step': 108948, 'preemption_count': 0}), (109871, {'train/accuracy': 0.7302343845367432, 'train/loss': 1.2425791025161743, 'validation/accuracy': 0.6523799896240234, 'validation/loss': 1.5649343729019165, 'validation/num_examples': 50000, 'test/accuracy': 0.5297999978065491, 'test/loss': 2.186164140701294, 'test/num_examples': 10000, 'score': 50038.27680063248, 'total_duration': 54360.60054755211, 'accumulated_submission_time': 50038.27680063248, 'accumulated_eval_time': 4310.15029501915, 'accumulated_logging_time': 5.9627158641815186, 'global_step': 109871, 'preemption_count': 0}), (110790, {'train/accuracy': 0.7073437571525574, 'train/loss': 1.2899636030197144, 'validation/accuracy': 0.6584799885749817, 'validation/loss': 1.5123716592788696, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.155395746231079, 'test/num_examples': 10000, 'score': 50458.470808029175, 'total_duration': 54820.40697169304, 'accumulated_submission_time': 50458.470808029175, 'accumulated_eval_time': 4349.666513442993, 'accumulated_logging_time': 6.006732702255249, 'global_step': 110790, 'preemption_count': 0}), (111713, {'train/accuracy': 0.7128515243530273, 'train/loss': 1.2536503076553345, 'validation/accuracy': 0.6627599596977234, 'validation/loss': 1.4864120483398438, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.1200437545776367, 'test/num_examples': 10000, 'score': 50878.71767401695, 'total_duration': 55277.81778669357, 'accumulated_submission_time': 50878.71767401695, 'accumulated_eval_time': 4386.731926679611, 'accumulated_logging_time': 6.0523576736450195, 'global_step': 111713, 'preemption_count': 0}), (112635, {'train/accuracy': 0.7286913990974426, 'train/loss': 1.2007211446762085, 'validation/accuracy': 0.6609399914741516, 'validation/loss': 1.5029429197311401, 'validation/num_examples': 50000, 'test/accuracy': 0.5369000434875488, 'test/loss': 2.1409804821014404, 'test/num_examples': 10000, 'score': 51298.70504426956, 'total_duration': 55736.1331076622, 'accumulated_submission_time': 51298.70504426956, 'accumulated_eval_time': 4424.739602088928, 'accumulated_logging_time': 6.320374011993408, 'global_step': 112635, 'preemption_count': 0}), (113557, {'train/accuracy': 0.7140820026397705, 'train/loss': 1.3028429746627808, 'validation/accuracy': 0.6602199673652649, 'validation/loss': 1.5292065143585205, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.165210485458374, 'test/num_examples': 10000, 'score': 51718.90090465546, 'total_duration': 56191.47248148918, 'accumulated_submission_time': 51718.90090465546, 'accumulated_eval_time': 4459.790710687637, 'accumulated_logging_time': 6.36035680770874, 'global_step': 113557, 'preemption_count': 0}), (114479, {'train/accuracy': 0.7202343344688416, 'train/loss': 1.2131118774414062, 'validation/accuracy': 0.6656999588012695, 'validation/loss': 1.4627410173416138, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.1023590564727783, 'test/num_examples': 10000, 'score': 52139.10345339775, 'total_duration': 56650.11271905899, 'accumulated_submission_time': 52139.10345339775, 'accumulated_eval_time': 4498.1347053050995, 'accumulated_logging_time': 6.402165174484253, 'global_step': 114479, 'preemption_count': 0}), (115403, {'train/accuracy': 0.7263085842132568, 'train/loss': 1.1966229677200317, 'validation/accuracy': 0.6647199988365173, 'validation/loss': 1.4690905809402466, 'validation/num_examples': 50000, 'test/accuracy': 0.539400041103363, 'test/loss': 2.100294589996338, 'test/num_examples': 10000, 'score': 52559.15979242325, 'total_duration': 57109.056616306305, 'accumulated_submission_time': 52559.15979242325, 'accumulated_eval_time': 4536.924311637878, 'accumulated_logging_time': 6.447792291641235, 'global_step': 115403, 'preemption_count': 0}), (116327, {'train/accuracy': 0.7205468416213989, 'train/loss': 1.2346270084381104, 'validation/accuracy': 0.6696799993515015, 'validation/loss': 1.4559895992279053, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.084333658218384, 'test/num_examples': 10000, 'score': 52979.50097155571, 'total_duration': 57569.00668978691, 'accumulated_submission_time': 52979.50097155571, 'accumulated_eval_time': 4576.432106971741, 'accumulated_logging_time': 6.493849992752075, 'global_step': 116327, 'preemption_count': 0}), (117250, {'train/accuracy': 0.7180468440055847, 'train/loss': 1.26100754737854, 'validation/accuracy': 0.6637200117111206, 'validation/loss': 1.4970401525497437, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.131301164627075, 'test/num_examples': 10000, 'score': 53399.68098139763, 'total_duration': 58026.68306350708, 'accumulated_submission_time': 53399.68098139763, 'accumulated_eval_time': 4613.82989192009, 'accumulated_logging_time': 6.539278507232666, 'global_step': 117250, 'preemption_count': 0}), (118173, {'train/accuracy': 0.7331640720367432, 'train/loss': 1.1802427768707275, 'validation/accuracy': 0.6723799705505371, 'validation/loss': 1.4435547590255737, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 2.0697150230407715, 'test/num_examples': 10000, 'score': 53819.9982714653, 'total_duration': 58486.09160208702, 'accumulated_submission_time': 53819.9982714653, 'accumulated_eval_time': 4652.822212696075, 'accumulated_logging_time': 6.585542917251587, 'global_step': 118173, 'preemption_count': 0}), (119096, {'train/accuracy': 0.7335156202316284, 'train/loss': 1.199196219444275, 'validation/accuracy': 0.6699999570846558, 'validation/loss': 1.4819027185440063, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.1308164596557617, 'test/num_examples': 10000, 'score': 54240.08188533783, 'total_duration': 58945.64118242264, 'accumulated_submission_time': 54240.08188533783, 'accumulated_eval_time': 4692.190171718597, 'accumulated_logging_time': 6.631305694580078, 'global_step': 119096, 'preemption_count': 0}), (120018, {'train/accuracy': 0.7240429520606995, 'train/loss': 1.2205166816711426, 'validation/accuracy': 0.6732999682426453, 'validation/loss': 1.4396311044692993, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.077719211578369, 'test/num_examples': 10000, 'score': 54660.09229564667, 'total_duration': 59404.036954164505, 'accumulated_submission_time': 54660.09229564667, 'accumulated_eval_time': 4730.476469278336, 'accumulated_logging_time': 6.678221940994263, 'global_step': 120018, 'preemption_count': 0}), (120944, {'train/accuracy': 0.73388671875, 'train/loss': 1.1822478771209717, 'validation/accuracy': 0.676099956035614, 'validation/loss': 1.439404010772705, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.0614137649536133, 'test/num_examples': 10000, 'score': 55080.207591056824, 'total_duration': 59864.2053706646, 'accumulated_submission_time': 55080.207591056824, 'accumulated_eval_time': 4770.4309067726135, 'accumulated_logging_time': 6.724869251251221, 'global_step': 120944, 'preemption_count': 0}), (121867, {'train/accuracy': 0.7496874928474426, 'train/loss': 1.114646315574646, 'validation/accuracy': 0.6793599724769592, 'validation/loss': 1.4229241609573364, 'validation/num_examples': 50000, 'test/accuracy': 0.5576000213623047, 'test/loss': 2.0427939891815186, 'test/num_examples': 10000, 'score': 55500.20226883888, 'total_duration': 60324.041732788086, 'accumulated_submission_time': 55500.20226883888, 'accumulated_eval_time': 4810.173624038696, 'accumulated_logging_time': 6.771934986114502, 'global_step': 121867, 'preemption_count': 0}), (122791, {'train/accuracy': 0.7269140481948853, 'train/loss': 1.2267969846725464, 'validation/accuracy': 0.6785999536514282, 'validation/loss': 1.4419078826904297, 'validation/num_examples': 50000, 'test/accuracy': 0.5555000305175781, 'test/loss': 2.0610427856445312, 'test/num_examples': 10000, 'score': 55920.51446223259, 'total_duration': 60784.256432294846, 'accumulated_submission_time': 55920.51446223259, 'accumulated_eval_time': 4849.972870588303, 'accumulated_logging_time': 6.823628664016724, 'global_step': 122791, 'preemption_count': 0}), (123715, {'train/accuracy': 0.7386718392372131, 'train/loss': 1.1407707929611206, 'validation/accuracy': 0.6824399828910828, 'validation/loss': 1.3840407133102417, 'validation/num_examples': 50000, 'test/accuracy': 0.560699999332428, 'test/loss': 2.0077319145202637, 'test/num_examples': 10000, 'score': 56340.726959228516, 'total_duration': 61241.7313709259, 'accumulated_submission_time': 56340.726959228516, 'accumulated_eval_time': 4887.135262012482, 'accumulated_logging_time': 6.871109485626221, 'global_step': 123715, 'preemption_count': 0}), (124640, {'train/accuracy': 0.7439648509025574, 'train/loss': 1.139055848121643, 'validation/accuracy': 0.6798799633979797, 'validation/loss': 1.4270832538604736, 'validation/num_examples': 50000, 'test/accuracy': 0.5530000329017639, 'test/loss': 2.0573394298553467, 'test/num_examples': 10000, 'score': 56760.75370979309, 'total_duration': 61701.31235575676, 'accumulated_submission_time': 56760.75370979309, 'accumulated_eval_time': 4926.5925216674805, 'accumulated_logging_time': 6.9160919189453125, 'global_step': 124640, 'preemption_count': 0}), (125563, {'train/accuracy': 0.7366601228713989, 'train/loss': 1.1655316352844238, 'validation/accuracy': 0.6827999949455261, 'validation/loss': 1.4155220985412598, 'validation/num_examples': 50000, 'test/accuracy': 0.5559000372886658, 'test/loss': 2.040470600128174, 'test/num_examples': 10000, 'score': 57180.97926735878, 'total_duration': 62161.31812500954, 'accumulated_submission_time': 57180.97926735878, 'accumulated_eval_time': 4966.275221347809, 'accumulated_logging_time': 6.962021350860596, 'global_step': 125563, 'preemption_count': 0}), (126485, {'train/accuracy': 0.7461132407188416, 'train/loss': 1.1315586566925049, 'validation/accuracy': 0.6882199645042419, 'validation/loss': 1.3758423328399658, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9890079498291016, 'test/num_examples': 10000, 'score': 57601.43358683586, 'total_duration': 62619.15861034393, 'accumulated_submission_time': 57601.43358683586, 'accumulated_eval_time': 5003.563591241837, 'accumulated_logging_time': 7.007826805114746, 'global_step': 126485, 'preemption_count': 0}), (127409, {'train/accuracy': 0.7524218559265137, 'train/loss': 1.1364513635635376, 'validation/accuracy': 0.6879400014877319, 'validation/loss': 1.4114627838134766, 'validation/num_examples': 50000, 'test/accuracy': 0.5671000480651855, 'test/loss': 2.030573844909668, 'test/num_examples': 10000, 'score': 58021.75379371643, 'total_duration': 63077.11794781685, 'accumulated_submission_time': 58021.75379371643, 'accumulated_eval_time': 5041.106873750687, 'accumulated_logging_time': 7.051853656768799, 'global_step': 127409, 'preemption_count': 0}), (128334, {'train/accuracy': 0.74964839220047, 'train/loss': 1.1114648580551147, 'validation/accuracy': 0.689799964427948, 'validation/loss': 1.371135950088501, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.993343472480774, 'test/num_examples': 10000, 'score': 58441.99978637695, 'total_duration': 63535.41212558746, 'accumulated_submission_time': 58441.99978637695, 'accumulated_eval_time': 5079.05620598793, 'accumulated_logging_time': 7.098832368850708, 'global_step': 128334, 'preemption_count': 0}), (129258, {'train/accuracy': 0.7498828172683716, 'train/loss': 1.1026932001113892, 'validation/accuracy': 0.6937400102615356, 'validation/loss': 1.3566606044769287, 'validation/num_examples': 50000, 'test/accuracy': 0.5713000297546387, 'test/loss': 1.9661833047866821, 'test/num_examples': 10000, 'score': 58861.942873716354, 'total_duration': 63992.40976333618, 'accumulated_submission_time': 58861.942873716354, 'accumulated_eval_time': 5116.010575294495, 'accumulated_logging_time': 7.14700722694397, 'global_step': 129258, 'preemption_count': 0}), (130184, {'train/accuracy': 0.75355464220047, 'train/loss': 1.101863980293274, 'validation/accuracy': 0.6942200064659119, 'validation/loss': 1.3657922744750977, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.9783955812454224, 'test/num_examples': 10000, 'score': 59282.19617843628, 'total_duration': 64452.22687602043, 'accumulated_submission_time': 59282.19617843628, 'accumulated_eval_time': 5155.474381446838, 'accumulated_logging_time': 7.194597005844116, 'global_step': 130184, 'preemption_count': 0}), (131104, {'train/accuracy': 0.7650976181030273, 'train/loss': 1.0323588848114014, 'validation/accuracy': 0.6943999528884888, 'validation/loss': 1.344438910484314, 'validation/num_examples': 50000, 'test/accuracy': 0.5657000541687012, 'test/loss': 1.97388756275177, 'test/num_examples': 10000, 'score': 59702.45468664169, 'total_duration': 64914.25153064728, 'accumulated_submission_time': 59702.45468664169, 'accumulated_eval_time': 5197.139461517334, 'accumulated_logging_time': 7.241278171539307, 'global_step': 131104, 'preemption_count': 0}), (132025, {'train/accuracy': 0.7515429258346558, 'train/loss': 1.0836161375045776, 'validation/accuracy': 0.6946199536323547, 'validation/loss': 1.3420709371566772, 'validation/num_examples': 50000, 'test/accuracy': 0.5679000020027161, 'test/loss': 1.9602190256118774, 'test/num_examples': 10000, 'score': 60122.40482163429, 'total_duration': 65373.663570165634, 'accumulated_submission_time': 60122.40482163429, 'accumulated_eval_time': 5236.505722999573, 'accumulated_logging_time': 7.285628318786621, 'global_step': 132025, 'preemption_count': 0}), (132951, {'train/accuracy': 0.758593738079071, 'train/loss': 1.0445502996444702, 'validation/accuracy': 0.6947999596595764, 'validation/loss': 1.3174066543579102, 'validation/num_examples': 50000, 'test/accuracy': 0.5760000348091125, 'test/loss': 1.9236680269241333, 'test/num_examples': 10000, 'score': 60542.771542072296, 'total_duration': 65831.88937497139, 'accumulated_submission_time': 60542.771542072296, 'accumulated_eval_time': 5274.264638900757, 'accumulated_logging_time': 7.332998752593994, 'global_step': 132951, 'preemption_count': 0}), (133876, {'train/accuracy': 0.7695898413658142, 'train/loss': 1.018075704574585, 'validation/accuracy': 0.6988999843597412, 'validation/loss': 1.3271454572677612, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9531786441802979, 'test/num_examples': 10000, 'score': 60962.859546899796, 'total_duration': 66291.97956442833, 'accumulated_submission_time': 60962.859546899796, 'accumulated_eval_time': 5314.167495965958, 'accumulated_logging_time': 7.379750490188599, 'global_step': 133876, 'preemption_count': 0}), (134798, {'train/accuracy': 0.7604882717132568, 'train/loss': 1.0434051752090454, 'validation/accuracy': 0.6985399723052979, 'validation/loss': 1.303519606590271, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9330246448516846, 'test/num_examples': 10000, 'score': 61382.80419540405, 'total_duration': 66751.209690094, 'accumulated_submission_time': 61382.80419540405, 'accumulated_eval_time': 5353.349631071091, 'accumulated_logging_time': 7.431820392608643, 'global_step': 134798, 'preemption_count': 0}), (135721, {'train/accuracy': 0.7647656202316284, 'train/loss': 1.0401690006256104, 'validation/accuracy': 0.6980400085449219, 'validation/loss': 1.3200068473815918, 'validation/num_examples': 50000, 'test/accuracy': 0.5758000016212463, 'test/loss': 1.9241626262664795, 'test/num_examples': 10000, 'score': 61803.14104223251, 'total_duration': 67210.00192761421, 'accumulated_submission_time': 61803.14104223251, 'accumulated_eval_time': 5391.70734000206, 'accumulated_logging_time': 7.477010011672974, 'global_step': 135721, 'preemption_count': 0}), (136644, {'train/accuracy': 0.7710546851158142, 'train/loss': 1.0173821449279785, 'validation/accuracy': 0.7045800089836121, 'validation/loss': 1.311528205871582, 'validation/num_examples': 50000, 'test/accuracy': 0.5790000557899475, 'test/loss': 1.9301297664642334, 'test/num_examples': 10000, 'score': 62223.05319976807, 'total_duration': 67669.96811890602, 'accumulated_submission_time': 62223.05319976807, 'accumulated_eval_time': 5431.663053035736, 'accumulated_logging_time': 7.522252082824707, 'global_step': 136644, 'preemption_count': 0}), (137568, {'train/accuracy': 0.7683789134025574, 'train/loss': 1.041908621788025, 'validation/accuracy': 0.7035399675369263, 'validation/loss': 1.3146579265594482, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9376602172851562, 'test/num_examples': 10000, 'score': 62643.12946271896, 'total_duration': 68129.23905944824, 'accumulated_submission_time': 62643.12946271896, 'accumulated_eval_time': 5470.751053571701, 'accumulated_logging_time': 7.575902938842773, 'global_step': 137568, 'preemption_count': 0}), (138489, {'train/accuracy': 0.7689452767372131, 'train/loss': 1.025770902633667, 'validation/accuracy': 0.7057200074195862, 'validation/loss': 1.2924665212631226, 'validation/num_examples': 50000, 'test/accuracy': 0.5884000062942505, 'test/loss': 1.8851429224014282, 'test/num_examples': 10000, 'score': 63063.101344347, 'total_duration': 68589.41958975792, 'accumulated_submission_time': 63063.101344347, 'accumulated_eval_time': 5510.862103939056, 'accumulated_logging_time': 7.621830224990845, 'global_step': 138489, 'preemption_count': 0}), (139413, {'train/accuracy': 0.7711523175239563, 'train/loss': 1.000161051750183, 'validation/accuracy': 0.7072599530220032, 'validation/loss': 1.2786537408828735, 'validation/num_examples': 50000, 'test/accuracy': 0.5861999988555908, 'test/loss': 1.8938807249069214, 'test/num_examples': 10000, 'score': 63483.114550590515, 'total_duration': 69048.47295331955, 'accumulated_submission_time': 63483.114550590515, 'accumulated_eval_time': 5549.8006637096405, 'accumulated_logging_time': 7.670376300811768, 'global_step': 139413, 'preemption_count': 0}), (140336, {'train/accuracy': 0.7856835722923279, 'train/loss': 0.9613086581230164, 'validation/accuracy': 0.7094999551773071, 'validation/loss': 1.2880979776382446, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.8893526792526245, 'test/num_examples': 10000, 'score': 63903.165801763535, 'total_duration': 69508.20226073265, 'accumulated_submission_time': 63903.165801763535, 'accumulated_eval_time': 5589.37908911705, 'accumulated_logging_time': 7.717724561691284, 'global_step': 140336, 'preemption_count': 0}), (141260, {'train/accuracy': 0.7727343440055847, 'train/loss': 1.010166049003601, 'validation/accuracy': 0.7104399800300598, 'validation/loss': 1.2714622020721436, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8784040212631226, 'test/num_examples': 10000, 'score': 64323.22742891312, 'total_duration': 69969.97873592377, 'accumulated_submission_time': 64323.22742891312, 'accumulated_eval_time': 5630.997187137604, 'accumulated_logging_time': 7.762471675872803, 'global_step': 141260, 'preemption_count': 0}), (142185, {'train/accuracy': 0.7766211032867432, 'train/loss': 0.9818845391273499, 'validation/accuracy': 0.7114999890327454, 'validation/loss': 1.2591363191604614, 'validation/num_examples': 50000, 'test/accuracy': 0.5889000296592712, 'test/loss': 1.8770174980163574, 'test/num_examples': 10000, 'score': 64743.2424428463, 'total_duration': 70428.5822839737, 'accumulated_submission_time': 64743.2424428463, 'accumulated_eval_time': 5669.488003015518, 'accumulated_logging_time': 7.8078320026397705, 'global_step': 142185, 'preemption_count': 0}), (143108, {'train/accuracy': 0.7864062190055847, 'train/loss': 0.9273659586906433, 'validation/accuracy': 0.7152000069618225, 'validation/loss': 1.233481764793396, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.8319133520126343, 'test/num_examples': 10000, 'score': 65163.4554746151, 'total_duration': 70887.26061153412, 'accumulated_submission_time': 65163.4554746151, 'accumulated_eval_time': 5707.855123996735, 'accumulated_logging_time': 7.854290246963501, 'global_step': 143108, 'preemption_count': 0}), (144030, {'train/accuracy': 0.7824804782867432, 'train/loss': 0.9607452154159546, 'validation/accuracy': 0.7184999585151672, 'validation/loss': 1.2336714267730713, 'validation/num_examples': 50000, 'test/accuracy': 0.593500018119812, 'test/loss': 1.8458307981491089, 'test/num_examples': 10000, 'score': 65583.5791592598, 'total_duration': 71346.5245103836, 'accumulated_submission_time': 65583.5791592598, 'accumulated_eval_time': 5746.891888856888, 'accumulated_logging_time': 7.905426979064941, 'global_step': 144030, 'preemption_count': 0}), (144951, {'train/accuracy': 0.7843359112739563, 'train/loss': 0.9414471983909607, 'validation/accuracy': 0.7197799682617188, 'validation/loss': 1.2216196060180664, 'validation/num_examples': 50000, 'test/accuracy': 0.5945000052452087, 'test/loss': 1.8343600034713745, 'test/num_examples': 10000, 'score': 66003.87094473839, 'total_duration': 71806.72054195404, 'accumulated_submission_time': 66003.87094473839, 'accumulated_eval_time': 5786.692119598389, 'accumulated_logging_time': 7.9578657150268555, 'global_step': 144951, 'preemption_count': 0}), (145873, {'train/accuracy': 0.7897070050239563, 'train/loss': 0.930291473865509, 'validation/accuracy': 0.7209399938583374, 'validation/loss': 1.2243276834487915, 'validation/num_examples': 50000, 'test/accuracy': 0.5939000248908997, 'test/loss': 1.8272168636322021, 'test/num_examples': 10000, 'score': 66423.95629882812, 'total_duration': 72265.67800307274, 'accumulated_submission_time': 66423.95629882812, 'accumulated_eval_time': 5825.466367721558, 'accumulated_logging_time': 8.004544019699097, 'global_step': 145873, 'preemption_count': 0}), (146796, {'train/accuracy': 0.7828710675239563, 'train/loss': 0.94913649559021, 'validation/accuracy': 0.7192999720573425, 'validation/loss': 1.2191599607467651, 'validation/num_examples': 50000, 'test/accuracy': 0.5915000438690186, 'test/loss': 1.8373186588287354, 'test/num_examples': 10000, 'score': 66843.89471817017, 'total_duration': 72726.38371825218, 'accumulated_submission_time': 66843.89471817017, 'accumulated_eval_time': 5866.13242316246, 'accumulated_logging_time': 8.054444074630737, 'global_step': 146796, 'preemption_count': 0}), (147716, {'train/accuracy': 0.7864453196525574, 'train/loss': 0.94319087266922, 'validation/accuracy': 0.7235599756240845, 'validation/loss': 1.2195154428482056, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.8374955654144287, 'test/num_examples': 10000, 'score': 67263.90523648262, 'total_duration': 73187.53686141968, 'accumulated_submission_time': 67263.90523648262, 'accumulated_eval_time': 5907.169671535492, 'accumulated_logging_time': 8.107472896575928, 'global_step': 147716, 'preemption_count': 0}), (148639, {'train/accuracy': 0.7929491996765137, 'train/loss': 0.936110258102417, 'validation/accuracy': 0.7234199643135071, 'validation/loss': 1.2337615489959717, 'validation/num_examples': 50000, 'test/accuracy': 0.6018000245094299, 'test/loss': 1.8382083177566528, 'test/num_examples': 10000, 'score': 67684.12996077538, 'total_duration': 73649.2893576622, 'accumulated_submission_time': 67684.12996077538, 'accumulated_eval_time': 5948.59770822525, 'accumulated_logging_time': 8.155084133148193, 'global_step': 148639, 'preemption_count': 0}), (149560, {'train/accuracy': 0.8044726252555847, 'train/loss': 0.8590974807739258, 'validation/accuracy': 0.725600004196167, 'validation/loss': 1.1935111284255981, 'validation/num_examples': 50000, 'test/accuracy': 0.601900041103363, 'test/loss': 1.7990987300872803, 'test/num_examples': 10000, 'score': 68104.28977417946, 'total_duration': 74111.3414068222, 'accumulated_submission_time': 68104.28977417946, 'accumulated_eval_time': 5990.390378952026, 'accumulated_logging_time': 8.203400373458862, 'global_step': 149560, 'preemption_count': 0}), (150481, {'train/accuracy': 0.7959179282188416, 'train/loss': 0.9110642075538635, 'validation/accuracy': 0.7285799980163574, 'validation/loss': 1.19636070728302, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.7990301847457886, 'test/num_examples': 10000, 'score': 68524.2951323986, 'total_duration': 74569.94378137589, 'accumulated_submission_time': 68524.2951323986, 'accumulated_eval_time': 6028.878688812256, 'accumulated_logging_time': 8.260981321334839, 'global_step': 150481, 'preemption_count': 0}), (151402, {'train/accuracy': 0.7992382645606995, 'train/loss': 0.8694639205932617, 'validation/accuracy': 0.7313199639320374, 'validation/loss': 1.1657003164291382, 'validation/num_examples': 50000, 'test/accuracy': 0.6051000356674194, 'test/loss': 1.7615711688995361, 'test/num_examples': 10000, 'score': 68944.20941090584, 'total_duration': 75029.67028093338, 'accumulated_submission_time': 68944.20941090584, 'accumulated_eval_time': 6068.592380285263, 'accumulated_logging_time': 8.307986736297607, 'global_step': 151402, 'preemption_count': 0}), (152323, {'train/accuracy': 0.8080468773841858, 'train/loss': 0.8454297184944153, 'validation/accuracy': 0.7318399548530579, 'validation/loss': 1.1674693822860718, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.7557659149169922, 'test/num_examples': 10000, 'score': 69364.16141724586, 'total_duration': 75490.700922966, 'accumulated_submission_time': 69364.16141724586, 'accumulated_eval_time': 6109.569490194321, 'accumulated_logging_time': 8.357463836669922, 'global_step': 152323, 'preemption_count': 0}), (153246, {'train/accuracy': 0.802539050579071, 'train/loss': 0.863842248916626, 'validation/accuracy': 0.7321000099182129, 'validation/loss': 1.1585302352905273, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.7566510438919067, 'test/num_examples': 10000, 'score': 69784.3099322319, 'total_duration': 75950.06301856041, 'accumulated_submission_time': 69784.3099322319, 'accumulated_eval_time': 6148.683254241943, 'accumulated_logging_time': 8.404613018035889, 'global_step': 153246, 'preemption_count': 0}), (154167, {'train/accuracy': 0.80726557970047, 'train/loss': 0.8443677425384521, 'validation/accuracy': 0.7341799736022949, 'validation/loss': 1.159018874168396, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.755558729171753, 'test/num_examples': 10000, 'score': 70204.3026239872, 'total_duration': 76408.33057045937, 'accumulated_submission_time': 70204.3026239872, 'accumulated_eval_time': 6186.854301929474, 'accumulated_logging_time': 8.456630945205688, 'global_step': 154167, 'preemption_count': 0}), (155089, {'train/accuracy': 0.8079296946525574, 'train/loss': 0.8516631722450256, 'validation/accuracy': 0.7345199584960938, 'validation/loss': 1.1670846939086914, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.779624342918396, 'test/num_examples': 10000, 'score': 70624.36257171631, 'total_duration': 76866.29339528084, 'accumulated_submission_time': 70624.36257171631, 'accumulated_eval_time': 6224.655606746674, 'accumulated_logging_time': 8.505312204360962, 'global_step': 155089, 'preemption_count': 0}), (156010, {'train/accuracy': 0.8057616949081421, 'train/loss': 0.8517761826515198, 'validation/accuracy': 0.7357400059700012, 'validation/loss': 1.147667646408081, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.7380512952804565, 'test/num_examples': 10000, 'score': 71044.26182794571, 'total_duration': 77326.6550860405, 'accumulated_submission_time': 71044.26182794571, 'accumulated_eval_time': 6264.754841089249, 'accumulated_logging_time': 8.815968751907349, 'global_step': 156010, 'preemption_count': 0}), (156933, {'train/accuracy': 0.8101562261581421, 'train/loss': 0.8456878662109375, 'validation/accuracy': 0.7382000088691711, 'validation/loss': 1.148350715637207, 'validation/num_examples': 50000, 'test/accuracy': 0.6139000058174133, 'test/loss': 1.7471004724502563, 'test/num_examples': 10000, 'score': 71464.36837172508, 'total_duration': 77786.44018650055, 'accumulated_submission_time': 71464.36837172508, 'accumulated_eval_time': 6304.327245473862, 'accumulated_logging_time': 8.869405031204224, 'global_step': 156933, 'preemption_count': 0}), (157854, {'train/accuracy': 0.8182226419448853, 'train/loss': 0.813230574131012, 'validation/accuracy': 0.7407799959182739, 'validation/loss': 1.141973853111267, 'validation/num_examples': 50000, 'test/accuracy': 0.6210000514984131, 'test/loss': 1.7272372245788574, 'test/num_examples': 10000, 'score': 71884.33867025375, 'total_duration': 78247.37133145332, 'accumulated_submission_time': 71884.33867025375, 'accumulated_eval_time': 6345.1846034526825, 'accumulated_logging_time': 8.920923233032227, 'global_step': 157854, 'preemption_count': 0}), (158776, {'train/accuracy': 0.8186913728713989, 'train/loss': 0.811683177947998, 'validation/accuracy': 0.7415399551391602, 'validation/loss': 1.1460314989089966, 'validation/num_examples': 50000, 'test/accuracy': 0.6224000453948975, 'test/loss': 1.7298548221588135, 'test/num_examples': 10000, 'score': 72304.50023531914, 'total_duration': 78708.44903898239, 'accumulated_submission_time': 72304.50023531914, 'accumulated_eval_time': 6385.997955322266, 'accumulated_logging_time': 8.971666812896729, 'global_step': 158776, 'preemption_count': 0}), (159700, {'train/accuracy': 0.8161132335662842, 'train/loss': 0.8156614303588867, 'validation/accuracy': 0.7422999739646912, 'validation/loss': 1.1298848390579224, 'validation/num_examples': 50000, 'test/accuracy': 0.6160000562667847, 'test/loss': 1.725846529006958, 'test/num_examples': 10000, 'score': 72724.4717707634, 'total_duration': 79170.03859901428, 'accumulated_submission_time': 72724.4717707634, 'accumulated_eval_time': 6427.509117841721, 'accumulated_logging_time': 9.025804042816162, 'global_step': 159700, 'preemption_count': 0}), (160621, {'train/accuracy': 0.8175585865974426, 'train/loss': 0.804296612739563, 'validation/accuracy': 0.7449600100517273, 'validation/loss': 1.1215741634368896, 'validation/num_examples': 50000, 'test/accuracy': 0.624500036239624, 'test/loss': 1.7089358568191528, 'test/num_examples': 10000, 'score': 73144.85194015503, 'total_duration': 79631.72527456284, 'accumulated_submission_time': 73144.85194015503, 'accumulated_eval_time': 6468.714903116226, 'accumulated_logging_time': 9.074469327926636, 'global_step': 160621, 'preemption_count': 0}), (161542, {'train/accuracy': 0.8251757621765137, 'train/loss': 0.7653958201408386, 'validation/accuracy': 0.7464799880981445, 'validation/loss': 1.1053043603897095, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.6808910369873047, 'test/num_examples': 10000, 'score': 73564.85105657578, 'total_duration': 80092.20484352112, 'accumulated_submission_time': 73564.85105657578, 'accumulated_eval_time': 6509.093505382538, 'accumulated_logging_time': 9.123837947845459, 'global_step': 161542, 'preemption_count': 0}), (162464, {'train/accuracy': 0.8218163847923279, 'train/loss': 0.7823770642280579, 'validation/accuracy': 0.7467399835586548, 'validation/loss': 1.1013247966766357, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.6946297883987427, 'test/num_examples': 10000, 'score': 73984.82580113411, 'total_duration': 80553.0836699009, 'accumulated_submission_time': 73984.82580113411, 'accumulated_eval_time': 6549.888410568237, 'accumulated_logging_time': 9.1816246509552, 'global_step': 162464, 'preemption_count': 0}), (163386, {'train/accuracy': 0.8213866949081421, 'train/loss': 0.7875846028327942, 'validation/accuracy': 0.7483199834823608, 'validation/loss': 1.1053085327148438, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.7001694440841675, 'test/num_examples': 10000, 'score': 74404.8524620533, 'total_duration': 81011.978900671, 'accumulated_submission_time': 74404.8524620533, 'accumulated_eval_time': 6588.652596235275, 'accumulated_logging_time': 9.234551668167114, 'global_step': 163386, 'preemption_count': 0}), (164307, {'train/accuracy': 0.8302733898162842, 'train/loss': 0.7504153251647949, 'validation/accuracy': 0.7503199577331543, 'validation/loss': 1.0849838256835938, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.6703184843063354, 'test/num_examples': 10000, 'score': 74824.91179394722, 'total_duration': 81473.03928542137, 'accumulated_submission_time': 74824.91179394722, 'accumulated_eval_time': 6629.552686929703, 'accumulated_logging_time': 9.28411316871643, 'global_step': 164307, 'preemption_count': 0}), (165227, {'train/accuracy': 0.8239843845367432, 'train/loss': 0.7693715691566467, 'validation/accuracy': 0.7511799931526184, 'validation/loss': 1.0776283740997314, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.6670862436294556, 'test/num_examples': 10000, 'score': 75245.17643213272, 'total_duration': 81932.90296435356, 'accumulated_submission_time': 75245.17643213272, 'accumulated_eval_time': 6669.045813798904, 'accumulated_logging_time': 9.33809781074524, 'global_step': 165227, 'preemption_count': 0}), (166150, {'train/accuracy': 0.8310937285423279, 'train/loss': 0.7499398589134216, 'validation/accuracy': 0.7531399726867676, 'validation/loss': 1.070204496383667, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.6644469499588013, 'test/num_examples': 10000, 'score': 75665.09493494034, 'total_duration': 82392.86175775528, 'accumulated_submission_time': 75665.09493494034, 'accumulated_eval_time': 6708.982246637344, 'accumulated_logging_time': 9.390437126159668, 'global_step': 166150, 'preemption_count': 0}), (167071, {'train/accuracy': 0.83056640625, 'train/loss': 0.7435762882232666, 'validation/accuracy': 0.7534199953079224, 'validation/loss': 1.0754247903823853, 'validation/num_examples': 50000, 'test/accuracy': 0.6333000063896179, 'test/loss': 1.6693150997161865, 'test/num_examples': 10000, 'score': 76085.28699398041, 'total_duration': 82853.24296784401, 'accumulated_submission_time': 76085.28699398041, 'accumulated_eval_time': 6749.0644516944885, 'accumulated_logging_time': 9.445293426513672, 'global_step': 167071, 'preemption_count': 0}), (167992, {'train/accuracy': 0.8321484327316284, 'train/loss': 0.7515702843666077, 'validation/accuracy': 0.7541399598121643, 'validation/loss': 1.0826858282089233, 'validation/num_examples': 50000, 'test/accuracy': 0.634600043296814, 'test/loss': 1.6695160865783691, 'test/num_examples': 10000, 'score': 76505.26083564758, 'total_duration': 83311.12951374054, 'accumulated_submission_time': 76505.26083564758, 'accumulated_eval_time': 6786.874273777008, 'accumulated_logging_time': 9.495783567428589, 'global_step': 167992, 'preemption_count': 0}), (168913, {'train/accuracy': 0.8340038657188416, 'train/loss': 0.7366316318511963, 'validation/accuracy': 0.7570599913597107, 'validation/loss': 1.0599473714828491, 'validation/num_examples': 50000, 'test/accuracy': 0.638700008392334, 'test/loss': 1.6415356397628784, 'test/num_examples': 10000, 'score': 76925.41933321953, 'total_duration': 83772.23526740074, 'accumulated_submission_time': 76925.41933321953, 'accumulated_eval_time': 6827.716981649399, 'accumulated_logging_time': 9.548727989196777, 'global_step': 168913, 'preemption_count': 0}), (169834, {'train/accuracy': 0.8338086009025574, 'train/loss': 0.7275217771530151, 'validation/accuracy': 0.7583400011062622, 'validation/loss': 1.0552046298980713, 'validation/num_examples': 50000, 'test/accuracy': 0.6380000114440918, 'test/loss': 1.6476746797561646, 'test/num_examples': 10000, 'score': 77345.37213397026, 'total_duration': 84231.7641518116, 'accumulated_submission_time': 77345.37213397026, 'accumulated_eval_time': 6867.189259767532, 'accumulated_logging_time': 9.600689172744751, 'global_step': 169834, 'preemption_count': 0})], 'global_step': 170224}
I0204 14:52:06.118518 140107197974336 submission_runner.py:586] Timing: 77520.19496154785
I0204 14:52:06.118600 140107197974336 submission_runner.py:588] Total number of evals: 185
I0204 14:52:06.118649 140107197974336 submission_runner.py:589] ====================
I0204 14:52:06.118694 140107197974336 submission_runner.py:542] Using RNG seed 1274177056
I0204 14:52:06.120067 140107197974336 submission_runner.py:551] --- Tuning run 2/5 ---
I0204 14:52:06.120184 140107197974336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_2.
I0204 14:52:06.124943 140107197974336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_2/hparams.json.
I0204 14:52:06.126671 140107197974336 submission_runner.py:206] Initializing dataset.
I0204 14:52:06.137792 140107197974336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0204 14:52:06.148121 140107197974336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0204 14:52:06.350979 140107197974336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0204 14:52:10.880960 140107197974336 submission_runner.py:213] Initializing model.
I0204 14:52:17.182025 140107197974336 submission_runner.py:255] Initializing optimizer.
I0204 14:52:17.650631 140107197974336 submission_runner.py:262] Initializing metrics bundle.
I0204 14:52:17.650801 140107197974336 submission_runner.py:280] Initializing checkpoint and logger.
I0204 14:52:17.750316 140107197974336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_2 with prefix checkpoint_
I0204 14:52:17.750453 140107197974336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0204 14:52:34.203511 140107197974336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0204 14:52:50.241473 140107197974336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_2/flags_0.json.
I0204 14:52:50.253377 140107197974336 submission_runner.py:314] Starting training loop.
I0204 14:53:23.732861 139946372675328 logging_writer.py:48] [0] global_step=0, grad_norm=0.2981005311012268, loss=6.9077534675598145
I0204 14:53:23.743291 140107197974336 spec.py:321] Evaluating on the training split.
I0204 14:53:32.026671 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 14:53:47.551778 140107197974336 spec.py:349] Evaluating on the test split.
I0204 14:53:49.152048 140107197974336 submission_runner.py:408] Time since start: 58.90s, 	Step: 1, 	{'train/accuracy': 0.0008203124743886292, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 33.48979616165161, 'total_duration': 58.898629665374756, 'accumulated_submission_time': 33.48979616165161, 'accumulated_eval_time': 25.40871500968933, 'accumulated_logging_time': 0}
I0204 14:53:49.160604 139946381068032 logging_writer.py:48] [1] accumulated_eval_time=25.408715, accumulated_logging_time=0, accumulated_submission_time=33.489796, global_step=1, preemption_count=0, score=33.489796, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=58.898630, train/accuracy=0.000820, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0204 14:54:57.206419 139946414638848 logging_writer.py:48] [100] global_step=100, grad_norm=0.3882684111595154, loss=6.9038615226745605
I0204 14:55:43.200357 139946397853440 logging_writer.py:48] [200] global_step=200, grad_norm=0.4314170479774475, loss=6.882570743560791
I0204 14:56:30.827542 139946414638848 logging_writer.py:48] [300] global_step=300, grad_norm=0.5014576315879822, loss=6.8551836013793945
I0204 14:57:18.368592 139946397853440 logging_writer.py:48] [400] global_step=400, grad_norm=0.5497487187385559, loss=6.846945762634277
I0204 14:58:05.360157 139946414638848 logging_writer.py:48] [500] global_step=500, grad_norm=0.5792925953865051, loss=6.786308288574219
I0204 14:58:51.732788 139946397853440 logging_writer.py:48] [600] global_step=600, grad_norm=0.4908987283706665, loss=6.741988182067871
I0204 14:59:38.297924 139946414638848 logging_writer.py:48] [700] global_step=700, grad_norm=1.03419828414917, loss=6.709144592285156
I0204 15:00:24.923777 139946397853440 logging_writer.py:48] [800] global_step=800, grad_norm=0.8235628604888916, loss=6.6419548988342285
I0204 15:00:49.237761 140107197974336 spec.py:321] Evaluating on the training split.
I0204 15:01:00.470691 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 15:01:26.380625 140107197974336 spec.py:349] Evaluating on the test split.
I0204 15:01:27.969930 140107197974336 submission_runner.py:408] Time since start: 517.72s, 	Step: 854, 	{'train/accuracy': 0.014042968861758709, 'train/loss': 6.451799392700195, 'validation/accuracy': 0.013439999893307686, 'validation/loss': 6.465761661529541, 'validation/num_examples': 50000, 'test/accuracy': 0.010100000537931919, 'test/loss': 6.498650550842285, 'test/num_examples': 10000, 'score': 453.5089168548584, 'total_duration': 517.7165124416351, 'accumulated_submission_time': 453.5089168548584, 'accumulated_eval_time': 64.14088726043701, 'accumulated_logging_time': 0.017528057098388672}
I0204 15:01:27.986944 139946414638848 logging_writer.py:48] [854] accumulated_eval_time=64.140887, accumulated_logging_time=0.017528, accumulated_submission_time=453.508917, global_step=854, preemption_count=0, score=453.508917, test/accuracy=0.010100, test/loss=6.498651, test/num_examples=10000, total_duration=517.716512, train/accuracy=0.014043, train/loss=6.451799, validation/accuracy=0.013440, validation/loss=6.465762, validation/num_examples=50000
I0204 15:01:46.409980 139946397853440 logging_writer.py:48] [900] global_step=900, grad_norm=1.3741965293884277, loss=6.6504807472229
I0204 15:02:30.427232 139946414638848 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9647620916366577, loss=6.600851058959961
I0204 15:03:16.883227 139946397853440 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0621063709259033, loss=6.531242370605469
I0204 15:04:03.454161 139946414638848 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9986406564712524, loss=6.517337799072266
I0204 15:04:49.977059 139946397853440 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8765630722045898, loss=6.445308208465576
I0204 15:05:36.852822 139946414638848 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.397495985031128, loss=6.496276378631592
I0204 15:06:23.445983 139946397853440 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.6469560861587524, loss=6.440392971038818
I0204 15:07:10.237964 139946414638848 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.2138859033584595, loss=6.352611064910889
I0204 15:07:57.157828 139946397853440 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.244513750076294, loss=6.343469619750977
I0204 15:08:28.252452 140107197974336 spec.py:321] Evaluating on the training split.
I0204 15:08:39.214648 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 15:09:06.404813 140107197974336 spec.py:349] Evaluating on the test split.
I0204 15:09:08.015185 140107197974336 submission_runner.py:408] Time since start: 977.76s, 	Step: 1768, 	{'train/accuracy': 0.04525390639901161, 'train/loss': 5.853660583496094, 'validation/accuracy': 0.04465999826788902, 'validation/loss': 5.874964714050293, 'validation/num_examples': 50000, 'test/accuracy': 0.03519999980926514, 'test/loss': 5.988783836364746, 'test/num_examples': 10000, 'score': 873.7088196277618, 'total_duration': 977.7617542743683, 'accumulated_submission_time': 873.7088196277618, 'accumulated_eval_time': 103.90360403060913, 'accumulated_logging_time': 0.047904253005981445}
I0204 15:09:08.030421 139946414638848 logging_writer.py:48] [1768] accumulated_eval_time=103.903604, accumulated_logging_time=0.047904, accumulated_submission_time=873.708820, global_step=1768, preemption_count=0, score=873.708820, test/accuracy=0.035200, test/loss=5.988784, test/num_examples=10000, total_duration=977.761754, train/accuracy=0.045254, train/loss=5.853661, validation/accuracy=0.044660, validation/loss=5.874965, validation/num_examples=50000
I0204 15:09:20.967707 139946397853440 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.379875659942627, loss=6.445387840270996
I0204 15:10:04.209753 139946414638848 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9422666430473328, loss=6.602548122406006
I0204 15:10:50.684234 139946397853440 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0879851579666138, loss=6.3060150146484375
I0204 15:11:37.057197 139946414638848 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.02070152759552, loss=6.541909694671631
I0204 15:12:23.167154 139946397853440 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.2395490407943726, loss=6.260061264038086
I0204 15:13:09.830117 139946414638848 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1418899297714233, loss=6.214788436889648
I0204 15:13:55.885068 139946397853440 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8032820820808411, loss=6.615158557891846
I0204 15:14:42.538239 139946414638848 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9041208624839783, loss=6.700382709503174
I0204 15:15:28.771703 139946397853440 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.0359760522842407, loss=6.1449480056762695
I0204 15:16:08.294151 140107197974336 spec.py:321] Evaluating on the training split.
I0204 15:16:19.387565 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 15:16:48.292816 140107197974336 spec.py:349] Evaluating on the test split.
I0204 15:16:49.893555 140107197974336 submission_runner.py:408] Time since start: 1439.64s, 	Step: 2686, 	{'train/accuracy': 0.07212890684604645, 'train/loss': 5.425541400909424, 'validation/accuracy': 0.06647999584674835, 'validation/loss': 5.466614246368408, 'validation/num_examples': 50000, 'test/accuracy': 0.05380000174045563, 'test/loss': 5.632424831390381, 'test/num_examples': 10000, 'score': 1293.9095661640167, 'total_duration': 1439.6401374340057, 'accumulated_submission_time': 1293.9095661640167, 'accumulated_eval_time': 145.50300693511963, 'accumulated_logging_time': 0.07345223426818848}
I0204 15:16:49.912585 139946414638848 logging_writer.py:48] [2686] accumulated_eval_time=145.503007, accumulated_logging_time=0.073452, accumulated_submission_time=1293.909566, global_step=2686, preemption_count=0, score=1293.909566, test/accuracy=0.053800, test/loss=5.632425, test/num_examples=10000, total_duration=1439.640137, train/accuracy=0.072129, train/loss=5.425541, validation/accuracy=0.066480, validation/loss=5.466614, validation/num_examples=50000
I0204 15:16:55.814387 139946397853440 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.272392749786377, loss=6.168303966522217
I0204 15:17:38.160928 139946414638848 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.4190099239349365, loss=6.661293983459473
I0204 15:18:24.575431 139946397853440 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.1092543601989746, loss=6.17989444732666
I0204 15:19:11.359510 139946414638848 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.2474486827850342, loss=6.037815093994141
I0204 15:19:57.648572 139946397853440 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9549809098243713, loss=6.066676616668701
I0204 15:20:44.127960 139946414638848 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9823570847511292, loss=6.61678409576416
I0204 15:21:30.595065 139946397853440 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0545716285705566, loss=5.992592811584473
I0204 15:22:17.038681 139946414638848 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.0906589031219482, loss=5.9971537590026855
I0204 15:23:03.524788 139946397853440 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.5065795183181763, loss=6.080501079559326
I0204 15:23:49.975116 139946414638848 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9156093597412109, loss=6.434338092803955
I0204 15:23:49.988412 140107197974336 spec.py:321] Evaluating on the training split.
I0204 15:24:01.234586 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 15:24:26.876352 140107197974336 spec.py:349] Evaluating on the test split.
I0204 15:24:28.480788 140107197974336 submission_runner.py:408] Time since start: 1898.23s, 	Step: 3601, 	{'train/accuracy': 0.11101562529802322, 'train/loss': 5.073092937469482, 'validation/accuracy': 0.09957999736070633, 'validation/loss': 5.1325178146362305, 'validation/num_examples': 50000, 'test/accuracy': 0.07490000128746033, 'test/loss': 5.360351085662842, 'test/num_examples': 10000, 'score': 1713.9229459762573, 'total_duration': 1898.2273745536804, 'accumulated_submission_time': 1713.9229459762573, 'accumulated_eval_time': 183.99536895751953, 'accumulated_logging_time': 0.10294008255004883}
I0204 15:24:28.495210 139946397853440 logging_writer.py:48] [3601] accumulated_eval_time=183.995369, accumulated_logging_time=0.102940, accumulated_submission_time=1713.922946, global_step=3601, preemption_count=0, score=1713.922946, test/accuracy=0.074900, test/loss=5.360351, test/num_examples=10000, total_duration=1898.227375, train/accuracy=0.111016, train/loss=5.073093, validation/accuracy=0.099580, validation/loss=5.132518, validation/num_examples=50000
I0204 15:25:09.528014 139946414638848 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.1028262376785278, loss=5.943991661071777
I0204 15:25:55.327576 139946397853440 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9389449954032898, loss=5.92557954788208
I0204 15:26:41.746459 139946414638848 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.0749115943908691, loss=5.937495708465576
I0204 15:27:28.195141 139946397853440 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.1781091690063477, loss=5.850253105163574
I0204 15:28:14.611112 139946414638848 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.3215715885162354, loss=6.157834053039551
I0204 15:29:00.939528 139946397853440 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.1683114767074585, loss=5.817494869232178
I0204 15:29:47.335495 139946414638848 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.017001986503601, loss=5.707332611083984
I0204 15:30:33.873079 139946397853440 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.939493715763092, loss=6.2891950607299805
I0204 15:31:20.223936 139946414638848 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.4003841876983643, loss=5.762710094451904
I0204 15:31:28.628814 140107197974336 spec.py:321] Evaluating on the training split.
I0204 15:31:39.716218 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 15:32:06.953251 140107197974336 spec.py:349] Evaluating on the test split.
I0204 15:32:08.552494 140107197974336 submission_runner.py:408] Time since start: 2358.30s, 	Step: 4520, 	{'train/accuracy': 0.15251952409744263, 'train/loss': 4.659289360046387, 'validation/accuracy': 0.13673999905586243, 'validation/loss': 4.747751712799072, 'validation/num_examples': 50000, 'test/accuracy': 0.10580000281333923, 'test/loss': 5.029491424560547, 'test/num_examples': 10000, 'score': 2133.9952919483185, 'total_duration': 2358.2990391254425, 'accumulated_submission_time': 2133.9952919483185, 'accumulated_eval_time': 223.91900992393494, 'accumulated_logging_time': 0.12615704536437988}
I0204 15:32:08.568229 139946397853440 logging_writer.py:48] [4520] accumulated_eval_time=223.919010, accumulated_logging_time=0.126157, accumulated_submission_time=2133.995292, global_step=4520, preemption_count=0, score=2133.995292, test/accuracy=0.105800, test/loss=5.029491, test/num_examples=10000, total_duration=2358.299039, train/accuracy=0.152520, train/loss=4.659289, validation/accuracy=0.136740, validation/loss=4.747752, validation/num_examples=50000
I0204 15:32:40.944272 139946414638848 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.127079725265503, loss=5.71083927154541
I0204 15:33:26.951511 139946397853440 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.2714450359344482, loss=5.713713645935059
I0204 15:34:13.619815 139946414638848 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.2455286979675293, loss=5.65052604675293
I0204 15:35:00.100737 139946397853440 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.5265990495681763, loss=5.667340278625488
I0204 15:35:46.736716 139946414638848 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8643478155136108, loss=6.385778903961182
I0204 15:36:32.955120 139946397853440 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.0899842977523804, loss=5.8602681159973145
I0204 15:37:19.959316 139946414638848 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.9982950687408447, loss=5.638861179351807
I0204 15:38:06.091990 139946397853440 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.2565957307815552, loss=5.569721698760986
I0204 15:38:52.500107 139946414638848 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.0377719402313232, loss=5.49612283706665
I0204 15:39:08.919219 140107197974336 spec.py:321] Evaluating on the training split.
I0204 15:39:19.786528 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 15:39:48.311826 140107197974336 spec.py:349] Evaluating on the test split.
I0204 15:39:49.902688 140107197974336 submission_runner.py:408] Time since start: 2819.65s, 	Step: 5437, 	{'train/accuracy': 0.2096288949251175, 'train/loss': 4.283182621002197, 'validation/accuracy': 0.1894799917936325, 'validation/loss': 4.372105121612549, 'validation/num_examples': 50000, 'test/accuracy': 0.14220000803470612, 'test/loss': 4.704031467437744, 'test/num_examples': 10000, 'score': 2554.283395767212, 'total_duration': 2819.649272918701, 'accumulated_submission_time': 2554.283395767212, 'accumulated_eval_time': 264.90250039100647, 'accumulated_logging_time': 0.1519322395324707}
I0204 15:39:49.917464 139946397853440 logging_writer.py:48] [5437] accumulated_eval_time=264.902500, accumulated_logging_time=0.151932, accumulated_submission_time=2554.283396, global_step=5437, preemption_count=0, score=2554.283396, test/accuracy=0.142200, test/loss=4.704031, test/num_examples=10000, total_duration=2819.649273, train/accuracy=0.209629, train/loss=4.283183, validation/accuracy=0.189480, validation/loss=4.372105, validation/num_examples=50000
I0204 15:40:15.087053 139946414638848 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.0925384759902954, loss=5.552734375
I0204 15:41:00.401111 139946397853440 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.965347409248352, loss=6.074388027191162
I0204 15:41:46.890942 139946414638848 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.225728988647461, loss=5.55303955078125
I0204 15:42:33.227487 139946397853440 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.2751431465148926, loss=5.430728912353516
I0204 15:43:19.505279 139946414638848 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.1552623510360718, loss=5.403110027313232
I0204 15:44:05.923207 139946397853440 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.744166374206543, loss=5.821639060974121
I0204 15:44:51.932083 139946414638848 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.9618680477142334, loss=6.174363136291504
I0204 15:45:38.050167 139946397853440 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.0943318605422974, loss=5.526156425476074
I0204 15:46:24.694957 139946414638848 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.2766863107681274, loss=5.318653106689453
I0204 15:46:50.325935 140107197974336 spec.py:321] Evaluating on the training split.
I0204 15:47:01.040840 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 15:47:30.291863 140107197974336 spec.py:349] Evaluating on the test split.
I0204 15:47:31.894351 140107197974336 submission_runner.py:408] Time since start: 3281.64s, 	Step: 6357, 	{'train/accuracy': 0.2499413937330246, 'train/loss': 3.9393117427825928, 'validation/accuracy': 0.23253999650478363, 'validation/loss': 4.036503314971924, 'validation/num_examples': 50000, 'test/accuracy': 0.18070000410079956, 'test/loss': 4.413839817047119, 'test/num_examples': 10000, 'score': 2974.6288928985596, 'total_duration': 3281.6409327983856, 'accumulated_submission_time': 2974.6288928985596, 'accumulated_eval_time': 306.4709143638611, 'accumulated_logging_time': 0.176743745803833}
I0204 15:47:31.909623 139946397853440 logging_writer.py:48] [6357] accumulated_eval_time=306.470914, accumulated_logging_time=0.176744, accumulated_submission_time=2974.628893, global_step=6357, preemption_count=0, score=2974.628893, test/accuracy=0.180700, test/loss=4.413840, test/num_examples=10000, total_duration=3281.640933, train/accuracy=0.249941, train/loss=3.939312, validation/accuracy=0.232540, validation/loss=4.036503, validation/num_examples=50000
I0204 15:47:49.205842 139946414638848 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.9807742834091187, loss=6.1643781661987305
I0204 15:48:33.407851 139946397853440 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.3454123735427856, loss=5.3035993576049805
I0204 15:49:19.987921 139946414638848 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.156375527381897, loss=5.326164722442627
I0204 15:50:06.450983 139946397853440 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8355434536933899, loss=6.3335490226745605
I0204 15:50:52.619848 139946414638848 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9208683371543884, loss=6.10267972946167
I0204 15:51:38.859658 139946397853440 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9928728938102722, loss=6.308443069458008
I0204 15:52:25.194848 139946414638848 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8877133131027222, loss=6.217819690704346
I0204 15:53:11.314902 139946397853440 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.0296388864517212, loss=5.22963285446167
I0204 15:53:57.601768 139946414638848 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.2890886068344116, loss=5.348952770233154
I0204 15:54:32.078880 140107197974336 spec.py:321] Evaluating on the training split.
I0204 15:54:42.797926 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 15:55:11.555123 140107197974336 spec.py:349] Evaluating on the test split.
I0204 15:55:13.157976 140107197974336 submission_runner.py:408] Time since start: 3742.90s, 	Step: 7275, 	{'train/accuracy': 0.29560545086860657, 'train/loss': 3.652733087539673, 'validation/accuracy': 0.26211997866630554, 'validation/loss': 3.811382293701172, 'validation/num_examples': 50000, 'test/accuracy': 0.20170001685619354, 'test/loss': 4.236976146697998, 'test/num_examples': 10000, 'score': 3394.7360577583313, 'total_duration': 3742.9045355319977, 'accumulated_submission_time': 3394.7360577583313, 'accumulated_eval_time': 347.54998540878296, 'accumulated_logging_time': 0.2015523910522461}
I0204 15:55:13.177885 139946397853440 logging_writer.py:48] [7275] accumulated_eval_time=347.549985, accumulated_logging_time=0.201552, accumulated_submission_time=3394.736058, global_step=7275, preemption_count=0, score=3394.736058, test/accuracy=0.201700, test/loss=4.236976, test/num_examples=10000, total_duration=3742.904536, train/accuracy=0.295605, train/loss=3.652733, validation/accuracy=0.262120, validation/loss=3.811382, validation/num_examples=50000
I0204 15:55:23.400838 139946414638848 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.1682238578796387, loss=5.190185546875
I0204 15:56:06.240023 139946397853440 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0083478689193726, loss=5.190142631530762
I0204 15:56:52.499493 139946414638848 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.164089560508728, loss=5.378965854644775
I0204 15:57:39.149016 139946397853440 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.032890796661377, loss=5.103152275085449
I0204 15:58:25.563816 139946414638848 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.2695708274841309, loss=5.163188457489014
I0204 15:59:11.993283 139946397853440 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.1454404592514038, loss=5.08405876159668
I0204 15:59:58.328458 139946414638848 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.1248626708984375, loss=5.092221736907959
I0204 16:00:44.888603 139946397853440 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.2466713190078735, loss=5.079218864440918
I0204 16:01:31.258416 139946414638848 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0291179418563843, loss=5.239053249359131
I0204 16:02:13.281098 140107197974336 spec.py:321] Evaluating on the training split.
I0204 16:02:24.039521 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 16:02:51.721333 140107197974336 spec.py:349] Evaluating on the test split.
I0204 16:02:53.339263 140107197974336 submission_runner.py:408] Time since start: 4203.09s, 	Step: 8192, 	{'train/accuracy': 0.3202148377895355, 'train/loss': 3.4828038215637207, 'validation/accuracy': 0.29877999424934387, 'validation/loss': 3.5942399501800537, 'validation/num_examples': 50000, 'test/accuracy': 0.2297000139951706, 'test/loss': 4.045166015625, 'test/num_examples': 10000, 'score': 3814.776090860367, 'total_duration': 4203.085846185684, 'accumulated_submission_time': 3814.776090860367, 'accumulated_eval_time': 387.60815477371216, 'accumulated_logging_time': 0.23152995109558105}
I0204 16:02:53.355897 139946397853440 logging_writer.py:48] [8192] accumulated_eval_time=387.608155, accumulated_logging_time=0.231530, accumulated_submission_time=3814.776091, global_step=8192, preemption_count=0, score=3814.776091, test/accuracy=0.229700, test/loss=4.045166, test/num_examples=10000, total_duration=4203.085846, train/accuracy=0.320215, train/loss=3.482804, validation/accuracy=0.298780, validation/loss=3.594240, validation/num_examples=50000
I0204 16:02:56.900935 139946414638848 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7676448822021484, loss=6.231049537658691
I0204 16:03:38.790254 139946397853440 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.1623997688293457, loss=4.989909648895264
I0204 16:04:24.844781 139946414638848 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.9385587573051453, loss=5.87800407409668
I0204 16:05:11.552029 139946397853440 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9591208696365356, loss=5.2001824378967285
I0204 16:05:57.668799 139946414638848 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.0274537801742554, loss=5.16886043548584
I0204 16:06:44.070139 139946397853440 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7443971633911133, loss=6.134138107299805
I0204 16:07:30.004565 139946414638848 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.2472965717315674, loss=4.997823238372803
I0204 16:08:16.239137 139946397853440 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7524980902671814, loss=6.283217430114746
I0204 16:09:02.504194 139946414638848 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.739209771156311, loss=5.693047523498535
I0204 16:09:48.746917 139946397853440 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9782246351242065, loss=4.911993503570557
I0204 16:09:53.579977 140107197974336 spec.py:321] Evaluating on the training split.
I0204 16:10:04.748483 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 16:10:31.166198 140107197974336 spec.py:349] Evaluating on the test split.
I0204 16:10:32.760765 140107197974336 submission_runner.py:408] Time since start: 4662.51s, 	Step: 9112, 	{'train/accuracy': 0.36253905296325684, 'train/loss': 3.197558879852295, 'validation/accuracy': 0.3323200047016144, 'validation/loss': 3.329472780227661, 'validation/num_examples': 50000, 'test/accuracy': 0.2621000111103058, 'test/loss': 3.810883045196533, 'test/num_examples': 10000, 'score': 4234.9379251003265, 'total_duration': 4662.507352590561, 'accumulated_submission_time': 4234.9379251003265, 'accumulated_eval_time': 426.7889401912689, 'accumulated_logging_time': 0.2578392028808594}
I0204 16:10:32.776582 139946414638848 logging_writer.py:48] [9112] accumulated_eval_time=426.788940, accumulated_logging_time=0.257839, accumulated_submission_time=4234.937925, global_step=9112, preemption_count=0, score=4234.937925, test/accuracy=0.262100, test/loss=3.810883, test/num_examples=10000, total_duration=4662.507353, train/accuracy=0.362539, train/loss=3.197559, validation/accuracy=0.332320, validation/loss=3.329473, validation/num_examples=50000
I0204 16:11:08.937493 139946397853440 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9899533987045288, loss=4.863905429840088
I0204 16:11:54.977289 139946414638848 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9462272524833679, loss=4.8452467918396
I0204 16:12:41.718204 139946397853440 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6613162755966187, loss=6.104942798614502
I0204 16:13:28.070297 139946414638848 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.4357434511184692, loss=4.819489002227783
I0204 16:14:14.398659 139946397853440 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.2027082443237305, loss=4.819012641906738
I0204 16:15:00.912461 139946414638848 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.8698272109031677, loss=5.587406158447266
I0204 16:15:47.093734 139946397853440 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7499259114265442, loss=5.957968711853027
I0204 16:16:33.389922 139946414638848 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.761195182800293, loss=6.313544273376465
I0204 16:17:19.705076 139946397853440 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.0621507167816162, loss=4.9246039390563965
I0204 16:17:33.097545 140107197974336 spec.py:321] Evaluating on the training split.
I0204 16:17:43.788443 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 16:18:11.976720 140107197974336 spec.py:349] Evaluating on the test split.
I0204 16:18:13.577807 140107197974336 submission_runner.py:408] Time since start: 5123.32s, 	Step: 10031, 	{'train/accuracy': 0.3985546827316284, 'train/loss': 2.9937057495117188, 'validation/accuracy': 0.36319997906684875, 'validation/loss': 3.172254800796509, 'validation/num_examples': 50000, 'test/accuracy': 0.2775000035762787, 'test/loss': 3.6873080730438232, 'test/num_examples': 10000, 'score': 4655.196612596512, 'total_duration': 5123.324376821518, 'accumulated_submission_time': 4655.196612596512, 'accumulated_eval_time': 467.26917695999146, 'accumulated_logging_time': 0.28348612785339355}
I0204 16:18:13.594186 139946414638848 logging_writer.py:48] [10031] accumulated_eval_time=467.269177, accumulated_logging_time=0.283486, accumulated_submission_time=4655.196613, global_step=10031, preemption_count=0, score=4655.196613, test/accuracy=0.277500, test/loss=3.687308, test/num_examples=10000, total_duration=5123.324377, train/accuracy=0.398555, train/loss=2.993706, validation/accuracy=0.363200, validation/loss=3.172255, validation/num_examples=50000
I0204 16:18:41.124667 139946397853440 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.0335360765457153, loss=4.810349464416504
I0204 16:19:27.048880 139946414638848 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9659044146537781, loss=4.889227390289307
I0204 16:20:13.195093 139946397853440 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.0270349979400635, loss=4.768249988555908
I0204 16:20:59.421944 139946414638848 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9233378767967224, loss=4.623208999633789
I0204 16:21:45.632811 139946397853440 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.1734142303466797, loss=4.810196876525879
I0204 16:22:31.832125 139946414638848 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.9696658849716187, loss=4.764826774597168
I0204 16:23:17.976716 139946397853440 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.747485876083374, loss=5.395315170288086
I0204 16:24:04.324404 139946414638848 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7023927569389343, loss=5.720280647277832
I0204 16:24:50.511758 139946397853440 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.0015861988067627, loss=4.611828327178955
I0204 16:25:13.971213 140107197974336 spec.py:321] Evaluating on the training split.
I0204 16:25:24.834704 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 16:25:50.235662 140107197974336 spec.py:349] Evaluating on the test split.
I0204 16:25:51.867536 140107197974336 submission_runner.py:408] Time since start: 5581.61s, 	Step: 10952, 	{'train/accuracy': 0.4247460961341858, 'train/loss': 2.851715087890625, 'validation/accuracy': 0.3962799906730652, 'validation/loss': 2.9970977306365967, 'validation/num_examples': 50000, 'test/accuracy': 0.3062000274658203, 'test/loss': 3.53668212890625, 'test/num_examples': 10000, 'score': 5075.5094628334045, 'total_duration': 5581.6141130924225, 'accumulated_submission_time': 5075.5094628334045, 'accumulated_eval_time': 505.16552233695984, 'accumulated_logging_time': 0.30973124504089355}
I0204 16:25:51.883664 139946414638848 logging_writer.py:48] [10952] accumulated_eval_time=505.165522, accumulated_logging_time=0.309731, accumulated_submission_time=5075.509463, global_step=10952, preemption_count=0, score=5075.509463, test/accuracy=0.306200, test/loss=3.536682, test/num_examples=10000, total_duration=5581.614113, train/accuracy=0.424746, train/loss=2.851715, validation/accuracy=0.396280, validation/loss=2.997098, validation/num_examples=50000
I0204 16:26:11.167261 139946397853440 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7821112275123596, loss=5.456620216369629
I0204 16:26:55.543192 139946414638848 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9397390484809875, loss=4.669131755828857
I0204 16:27:42.320583 139946397853440 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9446306228637695, loss=4.559505462646484
I0204 16:28:28.344915 139946414638848 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.0361945629119873, loss=4.5206804275512695
I0204 16:29:14.408117 139946397853440 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7992525696754456, loss=5.0283894538879395
I0204 16:30:00.456458 139946414638848 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7303968667984009, loss=6.129275798797607
I0204 16:30:46.790212 139946397853440 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6600913405418396, loss=6.030238151550293
I0204 16:31:33.039572 139946414638848 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.9406418800354004, loss=4.708939552307129
I0204 16:32:19.279243 139946397853440 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7230916619300842, loss=5.187085151672363
I0204 16:32:51.872796 140107197974336 spec.py:321] Evaluating on the training split.
I0204 16:33:02.790702 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 16:33:28.751331 140107197974336 spec.py:349] Evaluating on the test split.
I0204 16:33:30.357190 140107197974336 submission_runner.py:408] Time since start: 6040.10s, 	Step: 11872, 	{'train/accuracy': 0.4528906047344208, 'train/loss': 2.6884093284606934, 'validation/accuracy': 0.4200599789619446, 'validation/loss': 2.8471667766571045, 'validation/num_examples': 50000, 'test/accuracy': 0.3208000063896179, 'test/loss': 3.4061996936798096, 'test/num_examples': 10000, 'score': 5495.436726093292, 'total_duration': 6040.103754281998, 'accumulated_submission_time': 5495.436726093292, 'accumulated_eval_time': 543.6498990058899, 'accumulated_logging_time': 0.33508753776550293}
I0204 16:33:30.373999 139946414638848 logging_writer.py:48] [11872] accumulated_eval_time=543.649899, accumulated_logging_time=0.335088, accumulated_submission_time=5495.436726, global_step=11872, preemption_count=0, score=5495.436726, test/accuracy=0.320800, test/loss=3.406200, test/num_examples=10000, total_duration=6040.103754, train/accuracy=0.452891, train/loss=2.688409, validation/accuracy=0.420060, validation/loss=2.847167, validation/num_examples=50000
I0204 16:33:41.791448 139946397853440 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9022448658943176, loss=4.739918231964111
I0204 16:34:24.917379 139946414638848 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9576345682144165, loss=4.424324989318848
I0204 16:35:11.373630 139946397853440 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7644829750061035, loss=5.3867011070251465
I0204 16:35:57.553382 139946414638848 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.0814296007156372, loss=4.4255452156066895
I0204 16:36:43.952415 139946397853440 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9779670834541321, loss=4.629333972930908
I0204 16:37:30.216278 139946414638848 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.0897234678268433, loss=4.547860145568848
I0204 16:38:16.780349 139946397853440 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8813820481300354, loss=4.678475856781006
I0204 16:39:03.175978 139946414638848 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6385999917984009, loss=5.897002696990967
I0204 16:39:49.510474 139946397853440 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.039807915687561, loss=4.432814598083496
I0204 16:40:30.745998 140107197974336 spec.py:321] Evaluating on the training split.
I0204 16:40:41.706495 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 16:41:10.422897 140107197974336 spec.py:349] Evaluating on the test split.
I0204 16:41:12.028004 140107197974336 submission_runner.py:408] Time since start: 6501.77s, 	Step: 12791, 	{'train/accuracy': 0.4826757609844208, 'train/loss': 2.5154643058776855, 'validation/accuracy': 0.4415600001811981, 'validation/loss': 2.720947742462158, 'validation/num_examples': 50000, 'test/accuracy': 0.34060001373291016, 'test/loss': 3.2832746505737305, 'test/num_examples': 10000, 'score': 5915.742879390717, 'total_duration': 6501.774571418762, 'accumulated_submission_time': 5915.742879390717, 'accumulated_eval_time': 584.9318788051605, 'accumulated_logging_time': 0.3650226593017578}
I0204 16:41:12.047040 139946414638848 logging_writer.py:48] [12791] accumulated_eval_time=584.931879, accumulated_logging_time=0.365023, accumulated_submission_time=5915.742879, global_step=12791, preemption_count=0, score=5915.742879, test/accuracy=0.340600, test/loss=3.283275, test/num_examples=10000, total_duration=6501.774571, train/accuracy=0.482676, train/loss=2.515464, validation/accuracy=0.441560, validation/loss=2.720948, validation/num_examples=50000
I0204 16:41:15.982758 139946397853440 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.0822654962539673, loss=4.496013641357422
I0204 16:41:57.770656 139946414638848 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9168245196342468, loss=4.494054794311523
I0204 16:42:44.035549 139946397853440 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.769496738910675, loss=5.0279622077941895
I0204 16:43:30.651009 139946414638848 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9761629700660706, loss=4.440267562866211
I0204 16:44:16.946917 139946397853440 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.9146490693092346, loss=4.417425632476807
I0204 16:45:03.484723 139946414638848 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.8822135925292969, loss=4.3508100509643555
I0204 16:45:49.605937 139946397853440 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0463008880615234, loss=4.432855129241943
I0204 16:46:35.889310 139946414638848 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7708602547645569, loss=5.097184181213379
I0204 16:47:22.167645 139946397853440 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8604004383087158, loss=4.4102959632873535
I0204 16:48:08.710150 139946414638848 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9187656044960022, loss=4.752109527587891
I0204 16:48:12.029645 140107197974336 spec.py:321] Evaluating on the training split.
I0204 16:48:23.017378 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 16:48:47.117389 140107197974336 spec.py:349] Evaluating on the test split.
I0204 16:48:48.724959 140107197974336 submission_runner.py:408] Time since start: 6958.47s, 	Step: 13709, 	{'train/accuracy': 0.5004491806030273, 'train/loss': 2.3911969661712646, 'validation/accuracy': 0.46215999126434326, 'validation/loss': 2.5576767921447754, 'validation/num_examples': 50000, 'test/accuracy': 0.3562000095844269, 'test/loss': 3.150334119796753, 'test/num_examples': 10000, 'score': 6335.661201000214, 'total_duration': 6958.471544742584, 'accumulated_submission_time': 6335.661201000214, 'accumulated_eval_time': 621.6272025108337, 'accumulated_logging_time': 0.394378662109375}
I0204 16:48:48.744395 139946397853440 logging_writer.py:48] [13709] accumulated_eval_time=621.627203, accumulated_logging_time=0.394379, accumulated_submission_time=6335.661201, global_step=13709, preemption_count=0, score=6335.661201, test/accuracy=0.356200, test/loss=3.150334, test/num_examples=10000, total_duration=6958.471545, train/accuracy=0.500449, train/loss=2.391197, validation/accuracy=0.462160, validation/loss=2.557677, validation/num_examples=50000
I0204 16:49:26.019522 139946414638848 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6866791844367981, loss=5.745504379272461
I0204 16:50:12.036415 139946397853440 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.9184479117393494, loss=4.503612518310547
I0204 16:50:58.366497 139946414638848 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6892620325088501, loss=5.312691688537598
I0204 16:51:44.729011 139946397853440 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.9133674502372742, loss=4.802227973937988
I0204 16:52:30.935309 139946414638848 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6300830245018005, loss=5.827702045440674
I0204 16:53:17.408603 139946397853440 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.8000842928886414, loss=5.735480785369873
I0204 16:54:03.716309 139946414638848 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7162349224090576, loss=6.06493616104126
I0204 16:54:50.048110 139946397853440 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.0292084217071533, loss=4.3553924560546875
I0204 16:55:36.142201 139946414638848 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.907819926738739, loss=4.31956148147583
I0204 16:55:49.116842 140107197974336 spec.py:321] Evaluating on the training split.
I0204 16:56:00.086971 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 16:56:26.107816 140107197974336 spec.py:349] Evaluating on the test split.
I0204 16:56:27.704764 140107197974336 submission_runner.py:408] Time since start: 7417.45s, 	Step: 14630, 	{'train/accuracy': 0.5197460651397705, 'train/loss': 2.3464345932006836, 'validation/accuracy': 0.4825599789619446, 'validation/loss': 2.5191760063171387, 'validation/num_examples': 50000, 'test/accuracy': 0.3742000162601471, 'test/loss': 3.108792543411255, 'test/num_examples': 10000, 'score': 6755.971422195435, 'total_duration': 7417.451339006424, 'accumulated_submission_time': 6755.971422195435, 'accumulated_eval_time': 660.215106010437, 'accumulated_logging_time': 0.42304444313049316}
I0204 16:56:27.722561 139946397853440 logging_writer.py:48] [14630] accumulated_eval_time=660.215106, accumulated_logging_time=0.423044, accumulated_submission_time=6755.971422, global_step=14630, preemption_count=0, score=6755.971422, test/accuracy=0.374200, test/loss=3.108793, test/num_examples=10000, total_duration=7417.451339, train/accuracy=0.519746, train/loss=2.346435, validation/accuracy=0.482560, validation/loss=2.519176, validation/num_examples=50000
I0204 16:56:55.647768 139946414638848 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9454624652862549, loss=4.334895133972168
I0204 16:57:41.397556 139946397853440 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.9129423499107361, loss=4.3932671546936035
I0204 16:58:27.889571 139946414638848 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.0412230491638184, loss=4.184620380401611
I0204 16:59:14.532536 139946397853440 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9674032926559448, loss=4.281477928161621
I0204 17:00:00.281602 139946414638848 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9891731142997742, loss=4.428173065185547
I0204 17:00:46.771063 139946397853440 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.93799889087677, loss=4.287624835968018
I0204 17:01:33.164541 139946414638848 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.965379536151886, loss=4.355806827545166
I0204 17:02:19.542329 139946397853440 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.7538807392120361, loss=5.27823543548584
I0204 17:03:05.746629 139946414638848 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9514689445495605, loss=4.298213958740234
I0204 17:03:28.138586 140107197974336 spec.py:321] Evaluating on the training split.
I0204 17:03:38.972469 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 17:04:05.133285 140107197974336 spec.py:349] Evaluating on the test split.
I0204 17:04:06.730379 140107197974336 submission_runner.py:408] Time since start: 7876.48s, 	Step: 15550, 	{'train/accuracy': 0.53466796875, 'train/loss': 2.2342047691345215, 'validation/accuracy': 0.4933599829673767, 'validation/loss': 2.4432485103607178, 'validation/num_examples': 50000, 'test/accuracy': 0.38270002603530884, 'test/loss': 3.0271334648132324, 'test/num_examples': 10000, 'score': 7176.325411558151, 'total_duration': 7876.476963996887, 'accumulated_submission_time': 7176.325411558151, 'accumulated_eval_time': 698.8068988323212, 'accumulated_logging_time': 0.4497511386871338}
I0204 17:04:06.746816 139946397853440 logging_writer.py:48] [15550] accumulated_eval_time=698.806899, accumulated_logging_time=0.449751, accumulated_submission_time=7176.325412, global_step=15550, preemption_count=0, score=7176.325412, test/accuracy=0.382700, test/loss=3.027133, test/num_examples=10000, total_duration=7876.476964, train/accuracy=0.534668, train/loss=2.234205, validation/accuracy=0.493360, validation/loss=2.443249, validation/num_examples=50000
I0204 17:04:26.794308 139946414638848 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.9138330817222595, loss=4.429789066314697
I0204 17:05:11.894278 139946397853440 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.8591358065605164, loss=4.290968894958496
I0204 17:05:58.251572 139946414638848 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9256324172019958, loss=4.380017280578613
I0204 17:06:44.613866 139946397853440 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.9182228446006775, loss=4.7264275550842285
I0204 17:07:31.063677 139946414638848 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9415963292121887, loss=4.262963771820068
I0204 17:08:17.383532 139946397853440 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.8970569372177124, loss=4.206817150115967
I0204 17:09:03.767199 139946414638848 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.8742858171463013, loss=4.422719955444336
I0204 17:09:50.239029 139946397853440 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.8377951383590698, loss=4.752381801605225
I0204 17:10:36.441201 139946414638848 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.9732570052146912, loss=4.227940559387207
I0204 17:11:06.801868 140107197974336 spec.py:321] Evaluating on the training split.
I0204 17:11:17.774731 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 17:11:44.983235 140107197974336 spec.py:349] Evaluating on the test split.
I0204 17:11:46.576917 140107197974336 submission_runner.py:408] Time since start: 8336.32s, 	Step: 16467, 	{'train/accuracy': 0.5671288967132568, 'train/loss': 2.1125993728637695, 'validation/accuracy': 0.505299985408783, 'validation/loss': 2.3933229446411133, 'validation/num_examples': 50000, 'test/accuracy': 0.40060001611709595, 'test/loss': 2.9778337478637695, 'test/num_examples': 10000, 'score': 7596.317331790924, 'total_duration': 8336.32350063324, 'accumulated_submission_time': 7596.317331790924, 'accumulated_eval_time': 738.5819492340088, 'accumulated_logging_time': 0.4763305187225342}
I0204 17:11:46.596819 139946397853440 logging_writer.py:48] [16467] accumulated_eval_time=738.581949, accumulated_logging_time=0.476331, accumulated_submission_time=7596.317332, global_step=16467, preemption_count=0, score=7596.317332, test/accuracy=0.400600, test/loss=2.977834, test/num_examples=10000, total_duration=8336.323501, train/accuracy=0.567129, train/loss=2.112599, validation/accuracy=0.505300, validation/loss=2.393323, validation/num_examples=50000
I0204 17:11:59.978888 139946414638848 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.72792649269104, loss=5.6935858726501465
I0204 17:12:43.228405 139946397853440 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.8631748557090759, loss=4.150975704193115
I0204 17:13:29.829342 139946414638848 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.1495399475097656, loss=4.177161693572998
I0204 17:14:16.253880 139946397853440 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.7783952355384827, loss=4.948234558105469
I0204 17:15:02.598695 139946414638848 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.8811724185943604, loss=4.304376602172852
I0204 17:15:49.012701 139946397853440 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.8279791474342346, loss=4.709851264953613
I0204 17:16:35.030345 139946414638848 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.8735715746879578, loss=4.545064926147461
I0204 17:17:21.169274 139946397853440 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9101714491844177, loss=4.17746639251709
I0204 17:18:07.231891 139946414638848 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.8171706795692444, loss=4.685865879058838
I0204 17:18:46.579759 140107197974336 spec.py:321] Evaluating on the training split.
I0204 17:18:57.251962 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 17:19:24.928520 140107197974336 spec.py:349] Evaluating on the test split.
I0204 17:19:26.533479 140107197974336 submission_runner.py:408] Time since start: 8796.28s, 	Step: 17387, 	{'train/accuracy': 0.5574023127555847, 'train/loss': 2.110726833343506, 'validation/accuracy': 0.5231800079345703, 'validation/loss': 2.286231279373169, 'validation/num_examples': 50000, 'test/accuracy': 0.40880000591278076, 'test/loss': 2.8916168212890625, 'test/num_examples': 10000, 'score': 8016.236355781555, 'total_duration': 8796.280049562454, 'accumulated_submission_time': 8016.236355781555, 'accumulated_eval_time': 778.5356502532959, 'accumulated_logging_time': 0.5067436695098877}
I0204 17:19:26.554017 139946397853440 logging_writer.py:48] [17387] accumulated_eval_time=778.535650, accumulated_logging_time=0.506744, accumulated_submission_time=8016.236356, global_step=17387, preemption_count=0, score=8016.236356, test/accuracy=0.408800, test/loss=2.891617, test/num_examples=10000, total_duration=8796.280050, train/accuracy=0.557402, train/loss=2.110727, validation/accuracy=0.523180, validation/loss=2.286231, validation/num_examples=50000
I0204 17:19:32.064547 139946414638848 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7344579100608826, loss=4.842474937438965
I0204 17:20:14.668107 139946397853440 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8120201230049133, loss=4.09527587890625
I0204 17:21:00.324951 139946414638848 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8695241808891296, loss=4.151898384094238
I0204 17:21:46.902663 139946397853440 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9514004588127136, loss=4.131653785705566
I0204 17:22:33.515228 139946414638848 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.9531369805335999, loss=4.101313591003418
I0204 17:23:19.729859 139946397853440 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.9056322574615479, loss=4.233397483825684
I0204 17:24:06.159472 139946414638848 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.9524357914924622, loss=4.1902995109558105
I0204 17:24:52.226454 139946397853440 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9559846520423889, loss=4.013527870178223
I0204 17:25:38.528882 139946414638848 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8249605894088745, loss=4.7213311195373535
I0204 17:26:24.880733 139946397853440 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.8857458233833313, loss=4.61279296875
I0204 17:26:26.806131 140107197974336 spec.py:321] Evaluating on the training split.
I0204 17:26:37.429299 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 17:27:06.602947 140107197974336 spec.py:349] Evaluating on the test split.
I0204 17:27:08.201118 140107197974336 submission_runner.py:408] Time since start: 9257.95s, 	Step: 18306, 	{'train/accuracy': 0.5698437094688416, 'train/loss': 2.086137056350708, 'validation/accuracy': 0.5271199941635132, 'validation/loss': 2.2758429050445557, 'validation/num_examples': 50000, 'test/accuracy': 0.4140000343322754, 'test/loss': 2.887305736541748, 'test/num_examples': 10000, 'score': 8436.425481557846, 'total_duration': 9257.947703361511, 'accumulated_submission_time': 8436.425481557846, 'accumulated_eval_time': 819.9306211471558, 'accumulated_logging_time': 0.5377237796783447}
I0204 17:27:08.218554 139946414638848 logging_writer.py:48] [18306] accumulated_eval_time=819.930621, accumulated_logging_time=0.537724, accumulated_submission_time=8436.425482, global_step=18306, preemption_count=0, score=8436.425482, test/accuracy=0.414000, test/loss=2.887306, test/num_examples=10000, total_duration=9257.947703, train/accuracy=0.569844, train/loss=2.086137, validation/accuracy=0.527120, validation/loss=2.275843, validation/num_examples=50000
I0204 17:27:47.270977 139946397853440 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8988286852836609, loss=4.068427085876465
I0204 17:28:33.436737 139946414638848 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.9265351295471191, loss=4.061212539672852
I0204 17:29:19.922310 139946397853440 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.9172108769416809, loss=4.0727105140686035
I0204 17:30:06.048490 139946414638848 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9169551730155945, loss=4.0863752365112305
I0204 17:30:52.461976 139946397853440 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7270970940589905, loss=5.11782169342041
I0204 17:31:38.584554 139946414638848 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.8481261730194092, loss=4.327803134918213
I0204 17:32:24.817589 139946397853440 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8834139108657837, loss=4.009994983673096
I0204 17:33:13.821334 139946414638848 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6735661029815674, loss=5.730391502380371
I0204 17:34:00.863415 139946397853440 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9279007911682129, loss=4.127366065979004
I0204 17:34:08.559029 140107197974336 spec.py:321] Evaluating on the training split.
I0204 17:34:19.378612 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 17:34:46.389058 140107197974336 spec.py:349] Evaluating on the test split.
I0204 17:34:47.987652 140107197974336 submission_runner.py:408] Time since start: 9717.73s, 	Step: 19218, 	{'train/accuracy': 0.5857617259025574, 'train/loss': 2.0329670906066895, 'validation/accuracy': 0.5317599773406982, 'validation/loss': 2.2811155319213867, 'validation/num_examples': 50000, 'test/accuracy': 0.417900025844574, 'test/loss': 2.88185453414917, 'test/num_examples': 10000, 'score': 8856.704848766327, 'total_duration': 9717.73423075676, 'accumulated_submission_time': 8856.704848766327, 'accumulated_eval_time': 859.3592464923859, 'accumulated_logging_time': 0.5637521743774414}
I0204 17:34:48.005406 139946414638848 logging_writer.py:48] [19218] accumulated_eval_time=859.359246, accumulated_logging_time=0.563752, accumulated_submission_time=8856.704849, global_step=19218, preemption_count=0, score=8856.704849, test/accuracy=0.417900, test/loss=2.881855, test/num_examples=10000, total_duration=9717.734231, train/accuracy=0.585762, train/loss=2.032967, validation/accuracy=0.531760, validation/loss=2.281116, validation/num_examples=50000
I0204 17:35:21.596634 139946397853440 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0838689804077148, loss=4.146416664123535
I0204 17:36:07.635092 139946414638848 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9761689305305481, loss=4.151033878326416
I0204 17:36:54.016456 139946397853440 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7101120948791504, loss=5.816921710968018
I0204 17:37:40.141332 139946414638848 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7962188720703125, loss=4.405978202819824
I0204 17:38:26.678720 139946397853440 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.911186158657074, loss=4.069024085998535
I0204 17:39:13.473674 139946414638848 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8560085296630859, loss=4.316399097442627
I0204 17:39:59.571852 139946397853440 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.025541067123413, loss=4.08535099029541
I0204 17:40:46.332955 139946414638848 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.91312575340271, loss=4.024131774902344
I0204 17:41:33.020698 139946397853440 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.7107822895050049, loss=5.4873857498168945
I0204 17:41:48.469334 140107197974336 spec.py:321] Evaluating on the training split.
I0204 17:41:59.347733 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 17:42:28.045083 140107197974336 spec.py:349] Evaluating on the test split.
I0204 17:42:29.640443 140107197974336 submission_runner.py:408] Time since start: 10179.39s, 	Step: 20135, 	{'train/accuracy': 0.5826562643051147, 'train/loss': 2.0578503608703613, 'validation/accuracy': 0.5440599918365479, 'validation/loss': 2.237067222595215, 'validation/num_examples': 50000, 'test/accuracy': 0.42160001397132874, 'test/loss': 2.8544681072235107, 'test/num_examples': 10000, 'score': 9277.106140851974, 'total_duration': 10179.387027740479, 'accumulated_submission_time': 9277.106140851974, 'accumulated_eval_time': 900.5303463935852, 'accumulated_logging_time': 0.5916616916656494}
I0204 17:42:29.658000 139946414638848 logging_writer.py:48] [20135] accumulated_eval_time=900.530346, accumulated_logging_time=0.591662, accumulated_submission_time=9277.106141, global_step=20135, preemption_count=0, score=9277.106141, test/accuracy=0.421600, test/loss=2.854468, test/num_examples=10000, total_duration=10179.387028, train/accuracy=0.582656, train/loss=2.057850, validation/accuracy=0.544060, validation/loss=2.237067, validation/num_examples=50000
I0204 17:42:55.600110 139946397853440 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8598210215568542, loss=4.6788434982299805
I0204 17:43:41.305214 139946414638848 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7218258380889893, loss=5.15062141418457
I0204 17:44:28.028997 139946397853440 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6494491696357727, loss=5.87618350982666
I0204 17:45:14.359959 139946414638848 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9822863340377808, loss=3.977210760116577
I0204 17:46:00.715661 139946397853440 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6673912405967712, loss=5.84627103805542
I0204 17:46:47.109543 139946414638848 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.9889964461326599, loss=4.03101110458374
I0204 17:47:33.506191 139946397853440 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.9014213681221008, loss=4.074954032897949
I0204 17:48:19.828059 139946414638848 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7204198241233826, loss=5.82786750793457
I0204 17:49:06.344995 139946397853440 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8697034120559692, loss=5.250256538391113
I0204 17:49:30.007254 140107197974336 spec.py:321] Evaluating on the training split.
I0204 17:49:40.828005 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 17:50:04.658279 140107197974336 spec.py:349] Evaluating on the test split.
I0204 17:50:06.253931 140107197974336 submission_runner.py:408] Time since start: 10636.00s, 	Step: 21053, 	{'train/accuracy': 0.5937890410423279, 'train/loss': 1.9904780387878418, 'validation/accuracy': 0.5496399998664856, 'validation/loss': 2.194326162338257, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.793226718902588, 'test/num_examples': 10000, 'score': 9697.392251729965, 'total_duration': 10636.000517368317, 'accumulated_submission_time': 9697.392251729965, 'accumulated_eval_time': 936.777051448822, 'accumulated_logging_time': 0.619286298751831}
I0204 17:50:06.275322 139946414638848 logging_writer.py:48] [21053] accumulated_eval_time=936.777051, accumulated_logging_time=0.619286, accumulated_submission_time=9697.392252, global_step=21053, preemption_count=0, score=9697.392252, test/accuracy=0.439400, test/loss=2.793227, test/num_examples=10000, total_duration=10636.000517, train/accuracy=0.593789, train/loss=1.990478, validation/accuracy=0.549640, validation/loss=2.194326, validation/num_examples=50000
I0204 17:50:25.450649 139946397853440 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.8911721110343933, loss=4.176395893096924
I0204 17:51:09.815662 139946414638848 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.9302141666412354, loss=4.033198356628418
I0204 17:51:56.249038 139946397853440 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.0501656532287598, loss=4.04330587387085
I0204 17:52:42.461982 139946414638848 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.717315137386322, loss=5.490774154663086
I0204 17:53:28.753542 139946397853440 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7901691794395447, loss=4.635182857513428
I0204 17:54:15.226627 139946414638848 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.1022576093673706, loss=4.046629905700684
I0204 17:55:01.294119 139946397853440 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.9613609313964844, loss=4.074681282043457
I0204 17:55:47.601527 139946414638848 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.9241045117378235, loss=4.130860328674316
I0204 17:56:33.966876 139946397853440 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7858906984329224, loss=4.962491035461426
I0204 17:57:06.633279 140107197974336 spec.py:321] Evaluating on the training split.
I0204 17:57:17.556107 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 17:57:44.035228 140107197974336 spec.py:349] Evaluating on the test split.
I0204 17:57:45.635630 140107197974336 submission_runner.py:408] Time since start: 11095.38s, 	Step: 21972, 	{'train/accuracy': 0.613964855670929, 'train/loss': 1.8714021444320679, 'validation/accuracy': 0.5616599917411804, 'validation/loss': 2.118140935897827, 'validation/num_examples': 50000, 'test/accuracy': 0.4426000118255615, 'test/loss': 2.743307590484619, 'test/num_examples': 10000, 'score': 10117.686156272888, 'total_duration': 11095.38220667839, 'accumulated_submission_time': 10117.686156272888, 'accumulated_eval_time': 975.7793915271759, 'accumulated_logging_time': 0.6520423889160156}
I0204 17:57:45.657003 139946414638848 logging_writer.py:48] [21972] accumulated_eval_time=975.779392, accumulated_logging_time=0.652042, accumulated_submission_time=10117.686156, global_step=21972, preemption_count=0, score=10117.686156, test/accuracy=0.442600, test/loss=2.743308, test/num_examples=10000, total_duration=11095.382207, train/accuracy=0.613965, train/loss=1.871402, validation/accuracy=0.561660, validation/loss=2.118141, validation/num_examples=50000
I0204 17:57:57.079097 139946397853440 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.9484530687332153, loss=3.9400634765625
I0204 17:58:40.333739 139946414638848 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.8881126642227173, loss=4.118296146392822
I0204 17:59:26.804159 139946397853440 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9965488314628601, loss=3.9685380458831787
I0204 18:00:13.302399 139946414638848 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.8497167825698853, loss=5.726692199707031
I0204 18:00:59.318773 139946397853440 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.982090413570404, loss=3.940540313720703
I0204 18:01:45.772377 139946414638848 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.9700982570648193, loss=3.9046590328216553
I0204 18:02:32.032928 139946397853440 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.9413425922393799, loss=4.005405426025391
I0204 18:03:18.287637 139946414638848 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9462408423423767, loss=4.129817962646484
I0204 18:04:04.468468 139946397853440 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.7699689269065857, loss=5.7348408699035645
I0204 18:04:45.817559 140107197974336 spec.py:321] Evaluating on the training split.
I0204 18:04:56.423461 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 18:05:24.700620 140107197974336 spec.py:349] Evaluating on the test split.
I0204 18:05:26.311416 140107197974336 submission_runner.py:408] Time since start: 11556.06s, 	Step: 22891, 	{'train/accuracy': 0.6078515648841858, 'train/loss': 1.876462459564209, 'validation/accuracy': 0.5680199861526489, 'validation/loss': 2.0680747032165527, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.677503824234009, 'test/num_examples': 10000, 'score': 10537.782514810562, 'total_duration': 11556.057990550995, 'accumulated_submission_time': 10537.782514810562, 'accumulated_eval_time': 1016.273241519928, 'accumulated_logging_time': 0.684720516204834}
I0204 18:05:26.330418 139946414638848 logging_writer.py:48] [22891] accumulated_eval_time=1016.273242, accumulated_logging_time=0.684721, accumulated_submission_time=10537.782515, global_step=22891, preemption_count=0, score=10537.782515, test/accuracy=0.450200, test/loss=2.677504, test/num_examples=10000, total_duration=11556.057991, train/accuracy=0.607852, train/loss=1.876462, validation/accuracy=0.568020, validation/loss=2.068075, validation/num_examples=50000
I0204 18:05:30.264434 139946397853440 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.9348247647285461, loss=4.017758846282959
I0204 18:06:12.475496 139946414638848 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.0273107290267944, loss=3.963595390319824
I0204 18:06:58.968086 139946397853440 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.9214510321617126, loss=4.230978965759277
I0204 18:07:45.663980 139946414638848 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.7610170245170593, loss=5.705741882324219
I0204 18:08:31.827451 139946397853440 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8702215552330017, loss=4.258831024169922
I0204 18:09:18.319048 139946414638848 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.9487513899803162, loss=4.028946399688721
I0204 18:10:04.771371 139946397853440 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.009541630744934, loss=3.9460976123809814
I0204 18:10:51.293764 139946414638848 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.8454769253730774, loss=5.713335990905762
I0204 18:11:37.661181 139946397853440 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.9605956077575684, loss=4.02300500869751
I0204 18:12:24.280789 139946414638848 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.883394181728363, loss=4.306746006011963
I0204 18:12:26.720561 140107197974336 spec.py:321] Evaluating on the training split.
I0204 18:12:37.373990 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 18:13:06.681740 140107197974336 spec.py:349] Evaluating on the test split.
I0204 18:13:08.277832 140107197974336 submission_runner.py:408] Time since start: 12018.02s, 	Step: 23807, 	{'train/accuracy': 0.6229491829872131, 'train/loss': 1.8563594818115234, 'validation/accuracy': 0.5753600001335144, 'validation/loss': 2.064666986465454, 'validation/num_examples': 50000, 'test/accuracy': 0.4628000259399414, 'test/loss': 2.6657001972198486, 'test/num_examples': 10000, 'score': 10958.110257148743, 'total_duration': 12018.024418115616, 'accumulated_submission_time': 10958.110257148743, 'accumulated_eval_time': 1057.8305151462555, 'accumulated_logging_time': 0.7141125202178955}
I0204 18:13:08.298701 139946397853440 logging_writer.py:48] [23807] accumulated_eval_time=1057.830515, accumulated_logging_time=0.714113, accumulated_submission_time=10958.110257, global_step=23807, preemption_count=0, score=10958.110257, test/accuracy=0.462800, test/loss=2.665700, test/num_examples=10000, total_duration=12018.024418, train/accuracy=0.622949, train/loss=1.856359, validation/accuracy=0.575360, validation/loss=2.064667, validation/num_examples=50000
I0204 18:13:46.820647 139946414638848 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.9194429516792297, loss=4.191624641418457
I0204 18:14:33.170161 139946397853440 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9207388758659363, loss=3.9084763526916504
I0204 18:15:19.778070 139946414638848 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7615169882774353, loss=5.654176235198975
I0204 18:16:06.063826 139946397853440 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.9210358262062073, loss=3.978773593902588
I0204 18:16:52.349081 139946414638848 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.934575080871582, loss=3.831698179244995
I0204 18:17:38.944476 139946397853440 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.9057825803756714, loss=3.931802749633789
I0204 18:18:25.257509 139946414638848 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0434070825576782, loss=3.8945200443267822
I0204 18:19:11.687659 139946397853440 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.027937412261963, loss=3.7887649536132812
I0204 18:19:57.962600 139946414638848 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.9733277559280396, loss=3.9514107704162598
I0204 18:20:08.311538 140107197974336 spec.py:321] Evaluating on the training split.
I0204 18:20:19.018872 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 18:20:48.109829 140107197974336 spec.py:349] Evaluating on the test split.
I0204 18:20:49.716247 140107197974336 submission_runner.py:408] Time since start: 12479.46s, 	Step: 24724, 	{'train/accuracy': 0.6285351514816284, 'train/loss': 1.7818245887756348, 'validation/accuracy': 0.5766000151634216, 'validation/loss': 2.0169196128845215, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.632568836212158, 'test/num_examples': 10000, 'score': 11378.051944494247, 'total_duration': 12479.462833404541, 'accumulated_submission_time': 11378.051944494247, 'accumulated_eval_time': 1099.2352216243744, 'accumulated_logging_time': 0.7455592155456543}
I0204 18:20:49.735142 139946397853440 logging_writer.py:48] [24724] accumulated_eval_time=1099.235222, accumulated_logging_time=0.745559, accumulated_submission_time=11378.051944, global_step=24724, preemption_count=0, score=11378.051944, test/accuracy=0.460100, test/loss=2.632569, test/num_examples=10000, total_duration=12479.462833, train/accuracy=0.628535, train/loss=1.781825, validation/accuracy=0.576600, validation/loss=2.016920, validation/num_examples=50000
I0204 18:21:20.562361 139946414638848 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.8546165227890015, loss=5.0465497970581055
I0204 18:22:06.702100 139946397853440 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.9959923028945923, loss=3.927506446838379
I0204 18:22:53.047040 139946414638848 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8935412764549255, loss=4.117335319519043
I0204 18:23:39.388080 139946397853440 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.7023649215698242, loss=5.462409973144531
I0204 18:24:25.654804 139946414638848 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.9724478125572205, loss=3.954535961151123
I0204 18:25:12.498836 139946397853440 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.8984044194221497, loss=4.271022319793701
I0204 18:25:58.765919 139946414638848 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0032248497009277, loss=3.913973331451416
I0204 18:26:45.071497 139946397853440 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8517816066741943, loss=4.206404209136963
I0204 18:27:31.333774 139946414638848 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8082479238510132, loss=4.568658351898193
I0204 18:27:50.009676 140107197974336 spec.py:321] Evaluating on the training split.
I0204 18:28:00.952433 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 18:28:29.773726 140107197974336 spec.py:349] Evaluating on the test split.
I0204 18:28:31.398620 140107197974336 submission_runner.py:408] Time since start: 12941.15s, 	Step: 25642, 	{'train/accuracy': 0.6348242163658142, 'train/loss': 1.7657935619354248, 'validation/accuracy': 0.5819199681282043, 'validation/loss': 1.9933316707611084, 'validation/num_examples': 50000, 'test/accuracy': 0.46080002188682556, 'test/loss': 2.615546226501465, 'test/num_examples': 10000, 'score': 11798.264919519424, 'total_duration': 12941.145199537277, 'accumulated_submission_time': 11798.264919519424, 'accumulated_eval_time': 1140.624148607254, 'accumulated_logging_time': 0.7733578681945801}
I0204 18:28:31.417390 139946397853440 logging_writer.py:48] [25642] accumulated_eval_time=1140.624149, accumulated_logging_time=0.773358, accumulated_submission_time=11798.264920, global_step=25642, preemption_count=0, score=11798.264920, test/accuracy=0.460800, test/loss=2.615546, test/num_examples=10000, total_duration=12941.145200, train/accuracy=0.634824, train/loss=1.765794, validation/accuracy=0.581920, validation/loss=1.993332, validation/num_examples=50000
I0204 18:28:54.609208 139946414638848 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.9162611365318298, loss=3.8480422496795654
I0204 18:29:39.932461 139946397853440 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.8407334089279175, loss=4.815497398376465
I0204 18:30:26.432648 139946414638848 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.8632255792617798, loss=4.662093162536621
I0204 18:31:12.769258 139946397853440 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.9148763418197632, loss=3.841684579849243
I0204 18:31:58.787740 139946414638848 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.0100857019424438, loss=3.9149134159088135
I0204 18:32:45.096531 139946397853440 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8061770796775818, loss=5.388970851898193
I0204 18:33:31.401925 139946414638848 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.9026892185211182, loss=4.011157035827637
I0204 18:34:17.544492 139946397853440 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.9246925115585327, loss=3.951111316680908
I0204 18:35:03.709363 139946414638848 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.8447231650352478, loss=3.9465949535369873
I0204 18:35:31.626508 140107197974336 spec.py:321] Evaluating on the training split.
I0204 18:35:42.361572 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 18:36:05.176585 140107197974336 spec.py:349] Evaluating on the test split.
I0204 18:36:06.772252 140107197974336 submission_runner.py:408] Time since start: 13396.52s, 	Step: 26562, 	{'train/accuracy': 0.6374804377555847, 'train/loss': 1.7287397384643555, 'validation/accuracy': 0.5893999934196472, 'validation/loss': 1.9390709400177002, 'validation/num_examples': 50000, 'test/accuracy': 0.47370001673698425, 'test/loss': 2.5475146770477295, 'test/num_examples': 10000, 'score': 12218.406804323196, 'total_duration': 13396.518834590912, 'accumulated_submission_time': 12218.406804323196, 'accumulated_eval_time': 1175.7698872089386, 'accumulated_logging_time': 0.8034751415252686}
I0204 18:36:06.791006 139946397853440 logging_writer.py:48] [26562] accumulated_eval_time=1175.769887, accumulated_logging_time=0.803475, accumulated_submission_time=12218.406804, global_step=26562, preemption_count=0, score=12218.406804, test/accuracy=0.473700, test/loss=2.547515, test/num_examples=10000, total_duration=13396.518835, train/accuracy=0.637480, train/loss=1.728740, validation/accuracy=0.589400, validation/loss=1.939071, validation/num_examples=50000
I0204 18:36:22.135409 139946414638848 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.8791989088058472, loss=4.223456859588623
I0204 18:37:05.925328 139946397853440 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.9881357550621033, loss=3.734919309616089
I0204 18:37:52.597485 139946414638848 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.9528931379318237, loss=3.963423252105713
I0204 18:38:39.099293 139946397853440 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.9514995813369751, loss=4.2845611572265625
I0204 18:39:25.773708 139946414638848 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.073259711265564, loss=3.8391518592834473
I0204 18:40:12.339956 139946397853440 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.9113037586212158, loss=3.7680625915527344
I0204 18:40:58.574824 139946414638848 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8062008023262024, loss=4.951329231262207
I0204 18:41:45.264338 139946397853440 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.9342178106307983, loss=4.096865653991699
I0204 18:42:31.821027 139946414638848 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.0503196716308594, loss=3.8772363662719727
I0204 18:43:07.157477 140107197974336 spec.py:321] Evaluating on the training split.
I0204 18:43:17.997263 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 18:43:41.538381 140107197974336 spec.py:349] Evaluating on the test split.
I0204 18:43:43.134325 140107197974336 submission_runner.py:408] Time since start: 13852.88s, 	Step: 27477, 	{'train/accuracy': 0.6408398151397705, 'train/loss': 1.7599726915359497, 'validation/accuracy': 0.5889399647712708, 'validation/loss': 1.9790270328521729, 'validation/num_examples': 50000, 'test/accuracy': 0.4715000092983246, 'test/loss': 2.6012699604034424, 'test/num_examples': 10000, 'score': 12638.708625555038, 'total_duration': 13852.880910873413, 'accumulated_submission_time': 12638.708625555038, 'accumulated_eval_time': 1211.7467403411865, 'accumulated_logging_time': 0.8337299823760986}
I0204 18:43:43.154339 139946397853440 logging_writer.py:48] [27477] accumulated_eval_time=1211.746740, accumulated_logging_time=0.833730, accumulated_submission_time=12638.708626, global_step=27477, preemption_count=0, score=12638.708626, test/accuracy=0.471500, test/loss=2.601270, test/num_examples=10000, total_duration=13852.880911, train/accuracy=0.640840, train/loss=1.759973, validation/accuracy=0.588940, validation/loss=1.979027, validation/num_examples=50000
I0204 18:43:52.597315 139946414638848 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0027445554733276, loss=4.280584335327148
I0204 18:44:35.599812 139946397853440 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.9033756256103516, loss=4.173919677734375
I0204 18:45:21.504775 139946414638848 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8727087378501892, loss=4.917638778686523
I0204 18:46:08.202194 139946397853440 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.955165684223175, loss=3.970088481903076
I0204 18:46:54.584762 139946414638848 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.0027555227279663, loss=3.87640380859375
I0204 18:47:40.930515 139946397853440 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.0098297595977783, loss=3.777927875518799
I0204 18:48:27.442299 139946414638848 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.046954870223999, loss=3.918728828430176
I0204 18:49:13.830612 139946397853440 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.022233486175537, loss=3.7528023719787598
I0204 18:50:00.098803 139946414638848 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.0529441833496094, loss=3.7649145126342773
I0204 18:50:43.516352 140107197974336 spec.py:321] Evaluating on the training split.
I0204 18:50:54.192750 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 18:51:20.190030 140107197974336 spec.py:349] Evaluating on the test split.
I0204 18:51:21.794177 140107197974336 submission_runner.py:408] Time since start: 14311.54s, 	Step: 28395, 	{'train/accuracy': 0.6732617020606995, 'train/loss': 1.6018322706222534, 'validation/accuracy': 0.5973399877548218, 'validation/loss': 1.9325991868972778, 'validation/num_examples': 50000, 'test/accuracy': 0.4756000339984894, 'test/loss': 2.5660927295684814, 'test/num_examples': 10000, 'score': 13059.00931596756, 'total_duration': 14311.540762901306, 'accumulated_submission_time': 13059.00931596756, 'accumulated_eval_time': 1250.0245730876923, 'accumulated_logging_time': 0.862741231918335}
I0204 18:51:21.816565 139946397853440 logging_writer.py:48] [28395] accumulated_eval_time=1250.024573, accumulated_logging_time=0.862741, accumulated_submission_time=13059.009316, global_step=28395, preemption_count=0, score=13059.009316, test/accuracy=0.475600, test/loss=2.566093, test/num_examples=10000, total_duration=14311.540763, train/accuracy=0.673262, train/loss=1.601832, validation/accuracy=0.597340, validation/loss=1.932599, validation/num_examples=50000
I0204 18:51:24.182136 139946414638848 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.7702968120574951, loss=5.362602233886719
I0204 18:52:06.142984 139946397853440 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.9933792948722839, loss=3.7481162548065186
I0204 18:52:52.210772 139946414638848 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.7971304655075073, loss=5.3884477615356445
I0204 18:53:38.903282 139946397853440 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.8120527863502502, loss=5.617920875549316
I0204 18:54:25.129460 139946414638848 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.810502827167511, loss=4.9263787269592285
I0204 18:55:11.431401 139946397853440 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.9295718669891357, loss=3.7201716899871826
I0204 18:55:57.890468 139946414638848 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8652319312095642, loss=4.178104877471924
I0204 18:56:44.432504 139946397853440 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.8353897929191589, loss=4.42571496963501
I0204 18:57:30.842328 139946414638848 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.7475294470787048, loss=5.318571090698242
I0204 18:58:17.257005 139946397853440 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.0456656217575073, loss=3.828867197036743
I0204 18:58:22.085760 140107197974336 spec.py:321] Evaluating on the training split.
I0204 18:58:32.924830 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 18:59:00.681751 140107197974336 spec.py:349] Evaluating on the test split.
I0204 18:59:02.296033 140107197974336 submission_runner.py:408] Time since start: 14772.04s, 	Step: 29312, 	{'train/accuracy': 0.6476367115974426, 'train/loss': 1.7281827926635742, 'validation/accuracy': 0.5992599725723267, 'validation/loss': 1.9460116624832153, 'validation/num_examples': 50000, 'test/accuracy': 0.47680002450942993, 'test/loss': 2.5661449432373047, 'test/num_examples': 10000, 'score': 13479.215354442596, 'total_duration': 14772.042618513107, 'accumulated_submission_time': 13479.215354442596, 'accumulated_eval_time': 1290.234845161438, 'accumulated_logging_time': 0.8960211277008057}
I0204 18:59:02.318598 139946414638848 logging_writer.py:48] [29312] accumulated_eval_time=1290.234845, accumulated_logging_time=0.896021, accumulated_submission_time=13479.215354, global_step=29312, preemption_count=0, score=13479.215354, test/accuracy=0.476800, test/loss=2.566145, test/num_examples=10000, total_duration=14772.042619, train/accuracy=0.647637, train/loss=1.728183, validation/accuracy=0.599260, validation/loss=1.946012, validation/num_examples=50000
I0204 18:59:38.560025 139946397853440 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.9046202898025513, loss=4.026676654815674
I0204 19:00:24.834072 139946414638848 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8795574903488159, loss=4.950677871704102
I0204 19:01:11.196778 139946397853440 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.0321153402328491, loss=3.758876323699951
I0204 19:01:57.411709 139946414638848 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.9181044101715088, loss=3.8270368576049805
I0204 19:02:44.039627 139946397853440 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.054211974143982, loss=3.8061017990112305
I0204 19:03:30.426235 139946414638848 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.8610804080963135, loss=4.8813629150390625
I0204 19:04:16.665875 139946397853440 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.0498875379562378, loss=4.098057746887207
I0204 19:05:03.046181 139946414638848 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.9435807466506958, loss=3.871762275695801
I0204 19:05:49.433835 139946397853440 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.8470659255981445, loss=4.27679967880249
I0204 19:06:02.605044 140107197974336 spec.py:321] Evaluating on the training split.
I0204 19:06:13.348365 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 19:06:42.483449 140107197974336 spec.py:349] Evaluating on the test split.
I0204 19:06:44.086164 140107197974336 submission_runner.py:408] Time since start: 15233.83s, 	Step: 30230, 	{'train/accuracy': 0.6563280820846558, 'train/loss': 1.6538163423538208, 'validation/accuracy': 0.6067399978637695, 'validation/loss': 1.8779222965240479, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.5124006271362305, 'test/num_examples': 10000, 'score': 13899.440378904343, 'total_duration': 15233.832745790482, 'accumulated_submission_time': 13899.440378904343, 'accumulated_eval_time': 1331.7159614562988, 'accumulated_logging_time': 0.9272348880767822}
I0204 19:06:44.105570 139946414638848 logging_writer.py:48] [30230] accumulated_eval_time=1331.715961, accumulated_logging_time=0.927235, accumulated_submission_time=13899.440379, global_step=30230, preemption_count=0, score=13899.440379, test/accuracy=0.482100, test/loss=2.512401, test/num_examples=10000, total_duration=15233.832746, train/accuracy=0.656328, train/loss=1.653816, validation/accuracy=0.606740, validation/loss=1.877922, validation/num_examples=50000
I0204 19:07:12.372652 139946397853440 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.8096650838851929, loss=5.482560157775879
I0204 19:07:58.431690 139946414638848 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.0943118333816528, loss=3.7175824642181396
I0204 19:08:44.936117 139946397853440 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.8213421106338501, loss=5.541971683502197
I0204 19:09:31.143570 139946414638848 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.982822060585022, loss=3.7915215492248535
I0204 19:10:17.502468 139946397853440 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.8267972469329834, loss=4.966925144195557
I0204 19:11:03.741587 139946414638848 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.026212453842163, loss=3.7943546772003174
I0204 19:11:50.070102 139946397853440 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.8860698342323303, loss=4.233175754547119
I0204 19:12:36.347944 139946414638848 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8021997809410095, loss=5.21657657623291
I0204 19:13:22.893103 139946397853440 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.8661434650421143, loss=4.094508171081543
I0204 19:13:44.341971 140107197974336 spec.py:321] Evaluating on the training split.
I0204 19:13:55.120450 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 19:14:17.817255 140107197974336 spec.py:349] Evaluating on the test split.
I0204 19:14:19.418994 140107197974336 submission_runner.py:408] Time since start: 15689.17s, 	Step: 31148, 	{'train/accuracy': 0.6665819883346558, 'train/loss': 1.621629238128662, 'validation/accuracy': 0.6076200008392334, 'validation/loss': 1.8831592798233032, 'validation/num_examples': 50000, 'test/accuracy': 0.48590001463890076, 'test/loss': 2.506326675415039, 'test/num_examples': 10000, 'score': 14319.611972808838, 'total_duration': 15689.165579557419, 'accumulated_submission_time': 14319.611972808838, 'accumulated_eval_time': 1366.7929692268372, 'accumulated_logging_time': 0.9575145244598389}
I0204 19:14:19.441950 139946414638848 logging_writer.py:48] [31148] accumulated_eval_time=1366.792969, accumulated_logging_time=0.957515, accumulated_submission_time=14319.611973, global_step=31148, preemption_count=0, score=14319.611973, test/accuracy=0.485900, test/loss=2.506327, test/num_examples=10000, total_duration=15689.165580, train/accuracy=0.666582, train/loss=1.621629, validation/accuracy=0.607620, validation/loss=1.883159, validation/num_examples=50000
I0204 19:14:40.267668 139946397853440 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9532480835914612, loss=3.6636159420013428
I0204 19:15:25.112984 139946414638848 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.9100232720375061, loss=5.666293144226074
I0204 19:16:11.438500 139946397853440 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.9671873450279236, loss=4.1806488037109375
I0204 19:16:58.266725 139946414638848 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.8473483920097351, loss=3.9288370609283447
I0204 19:17:44.799408 139946397853440 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.921107828617096, loss=3.739459991455078
I0204 19:18:31.199340 139946414638848 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.766068160533905, loss=4.883616924285889
I0204 19:19:17.696651 139946397853440 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.0101120471954346, loss=4.055395603179932
I0204 19:20:04.141850 139946414638848 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.9020923972129822, loss=4.146230697631836
I0204 19:20:50.552608 139946397853440 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.121747374534607, loss=3.8111166954040527
I0204 19:21:19.520548 140107197974336 spec.py:321] Evaluating on the training split.
I0204 19:21:30.278279 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 19:21:57.355289 140107197974336 spec.py:349] Evaluating on the test split.
I0204 19:21:58.962955 140107197974336 submission_runner.py:408] Time since start: 16148.71s, 	Step: 32064, 	{'train/accuracy': 0.6660351157188416, 'train/loss': 1.5995913743972778, 'validation/accuracy': 0.616919994354248, 'validation/loss': 1.8113443851470947, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.442761182785034, 'test/num_examples': 10000, 'score': 14739.628532409668, 'total_duration': 16148.70953464508, 'accumulated_submission_time': 14739.628532409668, 'accumulated_eval_time': 1406.235392332077, 'accumulated_logging_time': 0.9896261692047119}
I0204 19:21:58.987380 139946414638848 logging_writer.py:48] [32064] accumulated_eval_time=1406.235392, accumulated_logging_time=0.989626, accumulated_submission_time=14739.628532, global_step=32064, preemption_count=0, score=14739.628532, test/accuracy=0.493700, test/loss=2.442761, test/num_examples=10000, total_duration=16148.709535, train/accuracy=0.666035, train/loss=1.599591, validation/accuracy=0.616920, validation/loss=1.811344, validation/num_examples=50000
I0204 19:22:13.545855 139946397853440 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.8566395044326782, loss=4.373824119567871
I0204 19:22:57.584191 139946414638848 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.1754119396209717, loss=3.7744333744049072
I0204 19:23:44.502326 139946397853440 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.9965883493423462, loss=3.6874170303344727
I0204 19:24:31.472450 139946414638848 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.8976451754570007, loss=4.28688383102417
I0204 19:25:18.102884 139946397853440 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.9847797751426697, loss=3.658010959625244
I0204 19:26:04.361335 139946414638848 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.8693651556968689, loss=4.087643623352051
I0204 19:26:50.582394 139946397853440 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.8592028617858887, loss=4.546348571777344
I0204 19:27:37.065916 139946414638848 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.8100280165672302, loss=5.380838394165039
I0204 19:28:23.549731 139946397853440 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.8347291946411133, loss=5.035106658935547
I0204 19:28:59.248882 140107197974336 spec.py:321] Evaluating on the training split.
I0204 19:29:10.495227 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 19:29:40.085181 140107197974336 spec.py:349] Evaluating on the test split.
I0204 19:29:41.678901 140107197974336 submission_runner.py:408] Time since start: 16611.43s, 	Step: 32979, 	{'train/accuracy': 0.66064453125, 'train/loss': 1.6689999103546143, 'validation/accuracy': 0.6136199831962585, 'validation/loss': 1.892295241355896, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.53031587600708, 'test/num_examples': 10000, 'score': 15159.826711416245, 'total_duration': 16611.425479650497, 'accumulated_submission_time': 15159.826711416245, 'accumulated_eval_time': 1448.665411233902, 'accumulated_logging_time': 1.0246226787567139}
I0204 19:29:41.701305 139946414638848 logging_writer.py:48] [32979] accumulated_eval_time=1448.665411, accumulated_logging_time=1.024623, accumulated_submission_time=15159.826711, global_step=32979, preemption_count=0, score=15159.826711, test/accuracy=0.489300, test/loss=2.530316, test/num_examples=10000, total_duration=16611.425480, train/accuracy=0.660645, train/loss=1.669000, validation/accuracy=0.613620, validation/loss=1.892295, validation/num_examples=50000
I0204 19:29:50.353969 139946397853440 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.060730218887329, loss=3.712489604949951
I0204 19:30:33.070773 139946414638848 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.9783141016960144, loss=3.8479738235473633
I0204 19:31:19.377454 139946397853440 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.9525159597396851, loss=4.085875511169434
I0204 19:32:05.839112 139946414638848 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.8946035504341125, loss=5.615786552429199
I0204 19:32:51.847756 139946397853440 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.8137566447257996, loss=5.0732035636901855
I0204 19:33:38.300171 139946414638848 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.0356130599975586, loss=3.7242095470428467
I0204 19:34:24.634257 139946397853440 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.8433891534805298, loss=5.387619972229004
I0204 19:35:11.154531 139946414638848 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.9971429109573364, loss=3.7444279193878174
I0204 19:35:57.298159 139946397853440 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.8660892844200134, loss=4.419195652008057
I0204 19:36:41.796635 140107197974336 spec.py:321] Evaluating on the training split.
I0204 19:36:52.833498 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 19:37:22.462090 140107197974336 spec.py:349] Evaluating on the test split.
I0204 19:37:24.055402 140107197974336 submission_runner.py:408] Time since start: 17073.80s, 	Step: 33898, 	{'train/accuracy': 0.6800194978713989, 'train/loss': 1.5640877485275269, 'validation/accuracy': 0.6166399717330933, 'validation/loss': 1.8367356061935425, 'validation/num_examples': 50000, 'test/accuracy': 0.49890002608299255, 'test/loss': 2.4557673931121826, 'test/num_examples': 10000, 'score': 15579.860214233398, 'total_duration': 17073.8019824028, 'accumulated_submission_time': 15579.860214233398, 'accumulated_eval_time': 1490.9241652488708, 'accumulated_logging_time': 1.0565321445465088}
I0204 19:37:24.075886 139946414638848 logging_writer.py:48] [33898] accumulated_eval_time=1490.924165, accumulated_logging_time=1.056532, accumulated_submission_time=15579.860214, global_step=33898, preemption_count=0, score=15579.860214, test/accuracy=0.498900, test/loss=2.455767, test/num_examples=10000, total_duration=17073.801982, train/accuracy=0.680019, train/loss=1.564088, validation/accuracy=0.616640, validation/loss=1.836736, validation/num_examples=50000
I0204 19:37:25.266355 139946397853440 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.8631338477134705, loss=4.846551418304443
I0204 19:38:06.847399 139946414638848 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.8233643770217896, loss=4.956205368041992
I0204 19:38:52.910658 139946397853440 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.99609375, loss=3.7009191513061523
I0204 19:39:39.630620 139946414638848 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.9204039573669434, loss=4.083098888397217
I0204 19:40:26.004509 139946397853440 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.9952670335769653, loss=3.716535806655884
I0204 19:41:12.460244 139946414638848 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.0496981143951416, loss=3.7476813793182373
I0204 19:41:58.693839 139946397853440 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.9491825699806213, loss=3.73016619682312
I0204 19:42:45.092936 139946414638848 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.8578791618347168, loss=4.79854154586792
I0204 19:43:31.533452 139946397853440 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.8267660140991211, loss=5.564194679260254
I0204 19:44:18.134966 139946414638848 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.9710894823074341, loss=3.7842438220977783
I0204 19:44:24.279350 140107197974336 spec.py:321] Evaluating on the training split.
I0204 19:44:35.181004 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 19:45:01.978638 140107197974336 spec.py:349] Evaluating on the test split.
I0204 19:45:03.583689 140107197974336 submission_runner.py:408] Time since start: 17533.33s, 	Step: 34815, 	{'train/accuracy': 0.6681445240974426, 'train/loss': 1.6435627937316895, 'validation/accuracy': 0.6217399835586548, 'validation/loss': 1.8544306755065918, 'validation/num_examples': 50000, 'test/accuracy': 0.498600035905838, 'test/loss': 2.475801467895508, 'test/num_examples': 10000, 'score': 16000.001391410828, 'total_duration': 17533.330276489258, 'accumulated_submission_time': 16000.001391410828, 'accumulated_eval_time': 1530.2285268306732, 'accumulated_logging_time': 1.0866823196411133}
I0204 19:45:03.603811 139946397853440 logging_writer.py:48] [34815] accumulated_eval_time=1530.228527, accumulated_logging_time=1.086682, accumulated_submission_time=16000.001391, global_step=34815, preemption_count=0, score=16000.001391, test/accuracy=0.498600, test/loss=2.475801, test/num_examples=10000, total_duration=17533.330276, train/accuracy=0.668145, train/loss=1.643563, validation/accuracy=0.621740, validation/loss=1.854431, validation/num_examples=50000
I0204 19:45:38.507045 139946414638848 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.922031044960022, loss=4.42508602142334
I0204 19:46:24.544395 139946397853440 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.8131948709487915, loss=4.795373439788818
I0204 19:47:10.968389 139946414638848 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.9256093502044678, loss=3.724799394607544
I0204 19:47:57.602769 139946397853440 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.975946307182312, loss=3.787018299102783
I0204 19:48:43.869044 139946414638848 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.9134741425514221, loss=4.050504684448242
I0204 19:49:30.375036 139946397853440 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.971544623374939, loss=4.00423002243042
I0204 19:50:16.556315 139946414638848 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.9875375032424927, loss=4.508748531341553
I0204 19:51:02.946049 139946397853440 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.8803153038024902, loss=4.8032660484313965
I0204 19:51:49.141483 139946414638848 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.037199854850769, loss=3.6604785919189453
I0204 19:52:03.817106 140107197974336 spec.py:321] Evaluating on the training split.
I0204 19:52:15.787531 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 19:52:44.667726 140107197974336 spec.py:349] Evaluating on the test split.
I0204 19:52:46.278542 140107197974336 submission_runner.py:408] Time since start: 17996.03s, 	Step: 35733, 	{'train/accuracy': 0.6791015267372131, 'train/loss': 1.499345064163208, 'validation/accuracy': 0.6260600090026855, 'validation/loss': 1.74126398563385, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.3599939346313477, 'test/num_examples': 10000, 'score': 16420.151757001877, 'total_duration': 17996.025110006332, 'accumulated_submission_time': 16420.151757001877, 'accumulated_eval_time': 1572.68993806839, 'accumulated_logging_time': 1.116673469543457}
I0204 19:52:46.303675 139946397853440 logging_writer.py:48] [35733] accumulated_eval_time=1572.689938, accumulated_logging_time=1.116673, accumulated_submission_time=16420.151757, global_step=35733, preemption_count=0, score=16420.151757, test/accuracy=0.506900, test/loss=2.359994, test/num_examples=10000, total_duration=17996.025110, train/accuracy=0.679102, train/loss=1.499345, validation/accuracy=0.626060, validation/loss=1.741264, validation/num_examples=50000
I0204 19:53:13.056644 139946414638848 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.9932925701141357, loss=3.821394443511963
I0204 19:53:58.910366 139946397853440 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.9598251581192017, loss=4.569183349609375
I0204 19:54:45.698987 139946414638848 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.0135248899459839, loss=3.777541399002075
I0204 19:55:32.357687 139946397853440 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.8823952674865723, loss=5.4769287109375
I0204 19:56:18.833364 139946414638848 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.9807696342468262, loss=3.7943978309631348
I0204 19:57:05.093459 139946397853440 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.0017551183700562, loss=3.589014768600464
I0204 19:57:51.354081 139946414638848 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.055725336074829, loss=3.7255921363830566
I0204 19:58:37.581859 139946397853440 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.883151650428772, loss=4.330042362213135
I0204 19:59:23.929401 139946414638848 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.9588124752044678, loss=3.634533405303955
I0204 19:59:46.639322 140107197974336 spec.py:321] Evaluating on the training split.
I0204 19:59:57.439103 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 20:00:25.520270 140107197974336 spec.py:349] Evaluating on the test split.
I0204 20:00:27.118106 140107197974336 submission_runner.py:408] Time since start: 18456.86s, 	Step: 36651, 	{'train/accuracy': 0.6833788752555847, 'train/loss': 1.5450021028518677, 'validation/accuracy': 0.6283800005912781, 'validation/loss': 1.795507788658142, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.42209792137146, 'test/num_examples': 10000, 'score': 16840.42435503006, 'total_duration': 18456.86468553543, 'accumulated_submission_time': 16840.42435503006, 'accumulated_eval_time': 1613.168706893921, 'accumulated_logging_time': 1.1520779132843018}
I0204 20:00:27.139195 139946397853440 logging_writer.py:48] [36651] accumulated_eval_time=1613.168707, accumulated_logging_time=1.152078, accumulated_submission_time=16840.424355, global_step=36651, preemption_count=0, score=16840.424355, test/accuracy=0.504300, test/loss=2.422098, test/num_examples=10000, total_duration=18456.864686, train/accuracy=0.683379, train/loss=1.545002, validation/accuracy=0.628380, validation/loss=1.795508, validation/num_examples=50000
I0204 20:00:47.099751 139946414638848 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.9367724657058716, loss=3.991175651550293
I0204 20:01:31.441970 139946397853440 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.0379496812820435, loss=3.7913546562194824
I0204 20:02:17.703046 139946414638848 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.0251762866973877, loss=3.696413516998291
I0204 20:03:03.984066 139946397853440 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.9225960969924927, loss=5.150618076324463
I0204 20:03:50.151543 139946414638848 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.8900138139724731, loss=4.929272651672363
I0204 20:04:36.749413 139946397853440 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.0372908115386963, loss=3.948456048965454
I0204 20:05:23.258472 139946414638848 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.967807412147522, loss=3.592467784881592
I0204 20:06:09.548928 139946397853440 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.0160211324691772, loss=3.6868131160736084
I0204 20:06:55.728285 139946414638848 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.9875508546829224, loss=4.165945529937744
I0204 20:07:27.371626 140107197974336 spec.py:321] Evaluating on the training split.
I0204 20:07:37.714463 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 20:08:08.388058 140107197974336 spec.py:349] Evaluating on the test split.
I0204 20:08:09.978326 140107197974336 submission_runner.py:408] Time since start: 18919.72s, 	Step: 37570, 	{'train/accuracy': 0.6815234422683716, 'train/loss': 1.5546667575836182, 'validation/accuracy': 0.6279199719429016, 'validation/loss': 1.7888097763061523, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.409806251525879, 'test/num_examples': 10000, 'score': 17260.590923786163, 'total_duration': 18919.72491168976, 'accumulated_submission_time': 17260.590923786163, 'accumulated_eval_time': 1655.7754156589508, 'accumulated_logging_time': 1.1850988864898682}
I0204 20:08:10.002197 139946397853440 logging_writer.py:48] [37570] accumulated_eval_time=1655.775416, accumulated_logging_time=1.185099, accumulated_submission_time=17260.590924, global_step=37570, preemption_count=0, score=17260.590924, test/accuracy=0.503400, test/loss=2.409806, test/num_examples=10000, total_duration=18919.724912, train/accuracy=0.681523, train/loss=1.554667, validation/accuracy=0.627920, validation/loss=1.788810, validation/num_examples=50000
I0204 20:08:22.182298 139946414638848 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.9950062036514282, loss=3.677225351333618
I0204 20:09:05.352380 139946397853440 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.0190757513046265, loss=3.6638360023498535
I0204 20:09:51.526943 139946414638848 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.0188732147216797, loss=5.234878063201904
I0204 20:10:37.933132 139946397853440 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.9611807465553284, loss=3.6764209270477295
I0204 20:11:24.236546 139946414638848 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0300266742706299, loss=3.6813931465148926
I0204 20:12:10.567053 139946397853440 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.0948431491851807, loss=3.575024366378784
I0204 20:12:56.457834 139946414638848 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.0726704597473145, loss=3.6284549236297607
I0204 20:13:42.728975 139946397853440 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.8335264921188354, loss=5.032861709594727
I0204 20:14:29.087733 139946414638848 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.9313027858734131, loss=3.7866811752319336
I0204 20:15:10.384897 140107197974336 spec.py:321] Evaluating on the training split.
I0204 20:15:20.938605 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 20:15:50.833963 140107197974336 spec.py:349] Evaluating on the test split.
I0204 20:15:52.423364 140107197974336 submission_runner.py:408] Time since start: 19382.17s, 	Step: 38491, 	{'train/accuracy': 0.6863867044448853, 'train/loss': 1.5237195491790771, 'validation/accuracy': 0.634660005569458, 'validation/loss': 1.7491233348846436, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.3639705181121826, 'test/num_examples': 10000, 'score': 17680.91156888008, 'total_duration': 19382.169947624207, 'accumulated_submission_time': 17680.91156888008, 'accumulated_eval_time': 1697.8138728141785, 'accumulated_logging_time': 1.2178847789764404}
I0204 20:15:52.450099 139946397853440 logging_writer.py:48] [38491] accumulated_eval_time=1697.813873, accumulated_logging_time=1.217885, accumulated_submission_time=17680.911569, global_step=38491, preemption_count=0, score=17680.911569, test/accuracy=0.510800, test/loss=2.363971, test/num_examples=10000, total_duration=19382.169948, train/accuracy=0.686387, train/loss=1.523720, validation/accuracy=0.634660, validation/loss=1.749123, validation/num_examples=50000
I0204 20:15:56.394404 139946414638848 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.0212210416793823, loss=3.614779233932495
I0204 20:16:38.337428 139946397853440 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.0080527067184448, loss=3.861464500427246
I0204 20:17:24.452576 139946414638848 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.9117763042449951, loss=4.012221813201904
I0204 20:18:11.026529 139946397853440 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.0127931833267212, loss=3.7739577293395996
I0204 20:18:57.243273 139946414638848 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.9000943303108215, loss=5.402388572692871
I0204 20:19:43.521193 139946397853440 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.9341455698013306, loss=4.9551496505737305
I0204 20:20:29.919392 139946414638848 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.0640830993652344, loss=3.770031452178955
I0204 20:21:16.146622 139946397853440 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.0234075784683228, loss=3.9796066284179688
I0204 20:22:02.651703 139946414638848 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.037807822227478, loss=3.6960866451263428
I0204 20:22:48.807500 139946397853440 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9145320057868958, loss=4.267885208129883
I0204 20:22:52.800551 140107197974336 spec.py:321] Evaluating on the training split.
I0204 20:23:03.501981 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 20:23:31.349952 140107197974336 spec.py:349] Evaluating on the test split.
I0204 20:23:32.946304 140107197974336 submission_runner.py:408] Time since start: 19842.69s, 	Step: 39410, 	{'train/accuracy': 0.6941015720367432, 'train/loss': 1.4616892337799072, 'validation/accuracy': 0.6366599798202515, 'validation/loss': 1.711472749710083, 'validation/num_examples': 50000, 'test/accuracy': 0.5131000280380249, 'test/loss': 2.3418853282928467, 'test/num_examples': 10000, 'score': 18101.19814991951, 'total_duration': 19842.69286584854, 'accumulated_submission_time': 18101.19814991951, 'accumulated_eval_time': 1737.9596025943756, 'accumulated_logging_time': 1.2555735111236572}
I0204 20:23:32.967564 139946414638848 logging_writer.py:48] [39410] accumulated_eval_time=1737.959603, accumulated_logging_time=1.255574, accumulated_submission_time=18101.198150, global_step=39410, preemption_count=0, score=18101.198150, test/accuracy=0.513100, test/loss=2.341885, test/num_examples=10000, total_duration=19842.692866, train/accuracy=0.694102, train/loss=1.461689, validation/accuracy=0.636660, validation/loss=1.711473, validation/num_examples=50000
I0204 20:24:10.136090 139946397853440 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.0919005870819092, loss=3.764209508895874
I0204 20:24:56.475260 139946414638848 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.0135524272918701, loss=3.5764241218566895
I0204 20:25:43.087355 139946397853440 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.02133309841156, loss=3.5653553009033203
I0204 20:26:29.574984 139946414638848 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.9288753271102905, loss=4.13112735748291
I0204 20:27:15.629513 139946397853440 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.0614614486694336, loss=3.6366305351257324
I0204 20:28:01.810384 139946414638848 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9632009267807007, loss=3.843259811401367
I0204 20:28:47.865676 139946397853440 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.0322154760360718, loss=3.6269938945770264
I0204 20:29:34.233557 139946414638848 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.0631388425827026, loss=3.6409108638763428
I0204 20:30:20.639552 139946397853440 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.9869614243507385, loss=3.7906055450439453
I0204 20:30:33.193696 140107197974336 spec.py:321] Evaluating on the training split.
I0204 20:30:43.912819 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 20:31:14.554266 140107197974336 spec.py:349] Evaluating on the test split.
I0204 20:31:16.151717 140107197974336 submission_runner.py:408] Time since start: 20305.90s, 	Step: 40329, 	{'train/accuracy': 0.7090820074081421, 'train/loss': 1.4305404424667358, 'validation/accuracy': 0.6382399797439575, 'validation/loss': 1.7424010038375854, 'validation/num_examples': 50000, 'test/accuracy': 0.5110000371932983, 'test/loss': 2.36631178855896, 'test/num_examples': 10000, 'score': 18521.3622546196, 'total_duration': 20305.8983001709, 'accumulated_submission_time': 18521.3622546196, 'accumulated_eval_time': 1780.9176177978516, 'accumulated_logging_time': 1.2859671115875244}
I0204 20:31:16.172536 139946414638848 logging_writer.py:48] [40329] accumulated_eval_time=1780.917618, accumulated_logging_time=1.285967, accumulated_submission_time=18521.362255, global_step=40329, preemption_count=0, score=18521.362255, test/accuracy=0.511000, test/loss=2.366312, test/num_examples=10000, total_duration=20305.898300, train/accuracy=0.709082, train/loss=1.430540, validation/accuracy=0.638240, validation/loss=1.742401, validation/num_examples=50000
I0204 20:31:44.477314 139946397853440 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.0293456315994263, loss=3.6102068424224854
I0204 20:32:30.797441 139946414638848 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.8933165669441223, loss=4.08241605758667
I0204 20:33:17.470112 139946397853440 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.8534270524978638, loss=5.409061908721924
I0204 20:34:03.769133 139946414638848 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.0320836305618286, loss=3.6770570278167725
I0204 20:34:50.349602 139946397853440 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.8972370624542236, loss=5.182521820068359
I0204 20:35:36.730355 139946414638848 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.9405622482299805, loss=4.1313090324401855
I0204 20:36:22.906688 139946397853440 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.883414626121521, loss=4.457676887512207
I0204 20:37:09.528165 139946414638848 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.0106297731399536, loss=5.257336139678955
I0204 20:37:55.793410 139946397853440 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.0398743152618408, loss=3.6349878311157227
I0204 20:38:16.375082 140107197974336 spec.py:321] Evaluating on the training split.
I0204 20:38:27.194302 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 20:38:49.040981 140107197974336 spec.py:349] Evaluating on the test split.
I0204 20:38:50.643258 140107197974336 submission_runner.py:408] Time since start: 20760.39s, 	Step: 41246, 	{'train/accuracy': 0.6861523389816284, 'train/loss': 1.5130900144577026, 'validation/accuracy': 0.6380400061607361, 'validation/loss': 1.7185349464416504, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.3481850624084473, 'test/num_examples': 10000, 'score': 18941.50358247757, 'total_duration': 20760.38983654976, 'accumulated_submission_time': 18941.50358247757, 'accumulated_eval_time': 1815.185781955719, 'accumulated_logging_time': 1.3155813217163086}
I0204 20:38:50.664346 139946414638848 logging_writer.py:48] [41246] accumulated_eval_time=1815.185782, accumulated_logging_time=1.315581, accumulated_submission_time=18941.503582, global_step=41246, preemption_count=0, score=18941.503582, test/accuracy=0.511400, test/loss=2.348185, test/num_examples=10000, total_duration=20760.389837, train/accuracy=0.686152, train/loss=1.513090, validation/accuracy=0.638040, validation/loss=1.718535, validation/num_examples=50000
I0204 20:39:12.319338 139946397853440 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.0281574726104736, loss=3.6287312507629395
I0204 20:39:56.836482 139946414638848 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.9226201772689819, loss=5.363483905792236
I0204 20:40:42.963119 139946397853440 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9618579149246216, loss=4.286771297454834
I0204 20:41:29.554284 139946414638848 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.1039409637451172, loss=3.6661224365234375
I0204 20:42:15.987529 139946397853440 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.8557583689689636, loss=4.484462261199951
I0204 20:43:02.363801 139946414638848 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.0778096914291382, loss=3.69588041305542
I0204 20:43:48.480351 139946397853440 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.0905876159667969, loss=3.8229336738586426
I0204 20:44:35.134071 139946414638848 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.0438376665115356, loss=3.625215530395508
I0204 20:45:21.518004 139946397853440 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.991950511932373, loss=4.257033824920654
I0204 20:45:50.724084 140107197974336 spec.py:321] Evaluating on the training split.
I0204 20:46:01.532892 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 20:46:31.242115 140107197974336 spec.py:349] Evaluating on the test split.
I0204 20:46:32.839048 140107197974336 submission_runner.py:408] Time since start: 21222.59s, 	Step: 42165, 	{'train/accuracy': 0.6976562142372131, 'train/loss': 1.452872633934021, 'validation/accuracy': 0.6402599811553955, 'validation/loss': 1.7018929719924927, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.3191921710968018, 'test/num_examples': 10000, 'score': 19361.500798225403, 'total_duration': 21222.585634231567, 'accumulated_submission_time': 19361.500798225403, 'accumulated_eval_time': 1857.3007380962372, 'accumulated_logging_time': 1.3462746143341064}
I0204 20:46:32.862734 139946414638848 logging_writer.py:48] [42165] accumulated_eval_time=1857.300738, accumulated_logging_time=1.346275, accumulated_submission_time=19361.500798, global_step=42165, preemption_count=0, score=19361.500798, test/accuracy=0.516200, test/loss=2.319192, test/num_examples=10000, total_duration=21222.585634, train/accuracy=0.697656, train/loss=1.452873, validation/accuracy=0.640260, validation/loss=1.701893, validation/num_examples=50000
I0204 20:46:47.036228 139946397853440 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.0861139297485352, loss=3.6813087463378906
I0204 20:47:30.829882 139946414638848 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.9712358117103577, loss=4.674005508422852
I0204 20:48:17.356102 139946397853440 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.970207691192627, loss=5.2249979972839355
I0204 20:49:03.948686 139946414638848 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.010502576828003, loss=4.729123592376709
I0204 20:49:49.785778 139946397853440 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.0179522037506104, loss=3.6183390617370605
I0204 20:50:36.154416 139946414638848 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.1121785640716553, loss=3.918612241744995
I0204 20:51:22.452160 139946397853440 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.0363850593566895, loss=3.602940320968628
I0204 20:52:08.950108 139946414638848 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.9118252396583557, loss=5.193017482757568
I0204 20:52:55.323985 139946397853440 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.8725377321243286, loss=5.415993690490723
I0204 20:53:33.273687 140107197974336 spec.py:321] Evaluating on the training split.
I0204 20:53:44.110106 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 20:54:09.738512 140107197974336 spec.py:349] Evaluating on the test split.
I0204 20:54:11.347813 140107197974336 submission_runner.py:408] Time since start: 21681.09s, 	Step: 43084, 	{'train/accuracy': 0.7085351347923279, 'train/loss': 1.3984525203704834, 'validation/accuracy': 0.6442199945449829, 'validation/loss': 1.6737351417541504, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.2900736331939697, 'test/num_examples': 10000, 'score': 19781.848207473755, 'total_duration': 21681.09438085556, 'accumulated_submission_time': 19781.848207473755, 'accumulated_eval_time': 1895.374864578247, 'accumulated_logging_time': 1.380638599395752}
I0204 20:54:11.375547 139946414638848 logging_writer.py:48] [43084] accumulated_eval_time=1895.374865, accumulated_logging_time=1.380639, accumulated_submission_time=19781.848207, global_step=43084, preemption_count=0, score=19781.848207, test/accuracy=0.521500, test/loss=2.290074, test/num_examples=10000, total_duration=21681.094381, train/accuracy=0.708535, train/loss=1.398453, validation/accuracy=0.644220, validation/loss=1.673735, validation/num_examples=50000
I0204 20:54:18.080775 139946397853440 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.9082445502281189, loss=4.187046527862549
I0204 20:55:00.275027 139946414638848 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.0262982845306396, loss=4.224984169006348
I0204 20:55:46.558046 139946397853440 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.0179744958877563, loss=4.896377086639404
I0204 20:56:33.174762 139946414638848 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.0156925916671753, loss=3.5820693969726562
I0204 20:57:19.556805 139946397853440 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.1052930355072021, loss=3.587769031524658
I0204 20:58:06.285384 139946414638848 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.0179511308670044, loss=3.5447702407836914
I0204 20:58:52.912933 139946397853440 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.0386663675308228, loss=3.57600736618042
I0204 20:59:39.579361 139946414638848 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.9932992458343506, loss=3.577622890472412
I0204 21:00:25.822965 139946397853440 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.9466109871864319, loss=5.224964141845703
I0204 21:01:11.428952 140107197974336 spec.py:321] Evaluating on the training split.
I0204 21:01:22.270576 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 21:01:49.526736 140107197974336 spec.py:349] Evaluating on the test split.
I0204 21:01:51.129857 140107197974336 submission_runner.py:408] Time since start: 22140.88s, 	Step: 44000, 	{'train/accuracy': 0.6931054592132568, 'train/loss': 1.5006791353225708, 'validation/accuracy': 0.6433599591255188, 'validation/loss': 1.717563271522522, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.348926305770874, 'test/num_examples': 10000, 'score': 20201.839075803757, 'total_duration': 22140.8764231205, 'accumulated_submission_time': 20201.839075803757, 'accumulated_eval_time': 1935.0757467746735, 'accumulated_logging_time': 1.41862154006958}
I0204 21:01:51.158041 139946414638848 logging_writer.py:48] [44000] accumulated_eval_time=1935.075747, accumulated_logging_time=1.418622, accumulated_submission_time=20201.839076, global_step=44000, preemption_count=0, score=20201.839076, test/accuracy=0.519900, test/loss=2.348926, test/num_examples=10000, total_duration=22140.876423, train/accuracy=0.693105, train/loss=1.500679, validation/accuracy=0.643360, validation/loss=1.717563, validation/num_examples=50000
I0204 21:01:51.553406 139946397853440 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1352295875549316, loss=3.609576940536499
I0204 21:02:32.916982 139946414638848 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.9895462989807129, loss=3.764941930770874
I0204 21:03:19.217414 139946397853440 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.0326979160308838, loss=3.570932388305664
I0204 21:04:05.916590 139946414638848 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.2410063743591309, loss=3.679823637008667
I0204 21:04:52.262343 139946397853440 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.9738036394119263, loss=3.6660337448120117
I0204 21:05:38.697940 139946414638848 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.0743496417999268, loss=4.905708312988281
I0204 21:06:25.071308 139946397853440 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.028358817100525, loss=4.661501407623291
I0204 21:07:11.771251 139946414638848 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.0716240406036377, loss=3.651716470718384
I0204 21:07:58.667364 139946397853440 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.9360664486885071, loss=3.812464475631714
I0204 21:08:44.951409 139946414638848 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.0945266485214233, loss=3.656853199005127
I0204 21:08:51.197424 140107197974336 spec.py:321] Evaluating on the training split.
I0204 21:09:02.108826 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 21:09:31.708647 140107197974336 spec.py:349] Evaluating on the test split.
I0204 21:09:33.315218 140107197974336 submission_runner.py:408] Time since start: 22603.06s, 	Step: 44915, 	{'train/accuracy': 0.7015234231948853, 'train/loss': 1.447856068611145, 'validation/accuracy': 0.6456999778747559, 'validation/loss': 1.6839470863342285, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.2985410690307617, 'test/num_examples': 10000, 'score': 20621.815400123596, 'total_duration': 22603.06175518036, 'accumulated_submission_time': 20621.815400123596, 'accumulated_eval_time': 1977.193481206894, 'accumulated_logging_time': 1.4574127197265625}
I0204 21:09:33.338701 139946397853440 logging_writer.py:48] [44915] accumulated_eval_time=1977.193481, accumulated_logging_time=1.457413, accumulated_submission_time=20621.815400, global_step=44915, preemption_count=0, score=20621.815400, test/accuracy=0.525800, test/loss=2.298541, test/num_examples=10000, total_duration=22603.061755, train/accuracy=0.701523, train/loss=1.447856, validation/accuracy=0.645700, validation/loss=1.683947, validation/num_examples=50000
I0204 21:10:08.457427 139946414638848 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.1315455436706543, loss=3.688408374786377
I0204 21:10:54.356481 139946397853440 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.9067874550819397, loss=4.7870683670043945
I0204 21:11:40.461323 139946414638848 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.0793451070785522, loss=3.648469924926758
I0204 21:12:27.229613 139946397853440 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.0081359148025513, loss=3.624843120574951
I0204 21:13:13.284707 139946414638848 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.8918159008026123, loss=4.290957927703857
I0204 21:13:59.439259 139946397853440 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.9667162895202637, loss=4.431195259094238
I0204 21:14:45.798788 139946414638848 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.2344872951507568, loss=3.5072057247161865
I0204 21:15:32.156297 139946397853440 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.0009281635284424, loss=3.618683099746704
I0204 21:16:18.388559 139946414638848 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.0376126766204834, loss=3.5903160572052
I0204 21:16:33.728333 140107197974336 spec.py:321] Evaluating on the training split.
I0204 21:16:44.572469 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 21:17:11.391758 140107197974336 spec.py:349] Evaluating on the test split.
I0204 21:17:12.988291 140107197974336 submission_runner.py:408] Time since start: 23062.73s, 	Step: 45835, 	{'train/accuracy': 0.7068163752555847, 'train/loss': 1.426822304725647, 'validation/accuracy': 0.6466799974441528, 'validation/loss': 1.6960935592651367, 'validation/num_examples': 50000, 'test/accuracy': 0.5192000269889832, 'test/loss': 2.3293352127075195, 'test/num_examples': 10000, 'score': 21042.14222931862, 'total_duration': 23062.734843730927, 'accumulated_submission_time': 21042.14222931862, 'accumulated_eval_time': 2016.4534318447113, 'accumulated_logging_time': 1.4908981323242188}
I0204 21:17:13.011246 139946397853440 logging_writer.py:48] [45835] accumulated_eval_time=2016.453432, accumulated_logging_time=1.490898, accumulated_submission_time=21042.142229, global_step=45835, preemption_count=0, score=21042.142229, test/accuracy=0.519200, test/loss=2.329335, test/num_examples=10000, total_duration=23062.734844, train/accuracy=0.706816, train/loss=1.426822, validation/accuracy=0.646680, validation/loss=1.696094, validation/num_examples=50000
I0204 21:17:38.970598 139946414638848 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.9012858271598816, loss=5.180381774902344
I0204 21:18:24.631093 139946397853440 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9972615838050842, loss=3.7290711402893066
I0204 21:19:11.247968 139946414638848 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.030632734298706, loss=3.684044122695923
I0204 21:19:57.189694 139946397853440 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.022868275642395, loss=4.160524368286133
I0204 21:20:43.776505 139946414638848 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.0094096660614014, loss=3.7442121505737305
I0204 21:21:29.876722 139946397853440 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.0632562637329102, loss=3.7606277465820312
I0204 21:22:16.057586 139946414638848 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.0477718114852905, loss=3.526128053665161
I0204 21:23:02.283669 139946397853440 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.2162463665008545, loss=3.56758975982666
I0204 21:23:48.565763 139946414638848 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.9887678623199463, loss=4.164053916931152
I0204 21:24:13.358344 140107197974336 spec.py:321] Evaluating on the training split.
I0204 21:24:24.154505 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 21:24:48.871719 140107197974336 spec.py:349] Evaluating on the test split.
I0204 21:24:50.475983 140107197974336 submission_runner.py:408] Time since start: 23520.22s, 	Step: 46755, 	{'train/accuracy': 0.706250011920929, 'train/loss': 1.3982168436050415, 'validation/accuracy': 0.6541199684143066, 'validation/loss': 1.6292552947998047, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.2454986572265625, 'test/num_examples': 10000, 'score': 21462.424863100052, 'total_duration': 23520.222547769547, 'accumulated_submission_time': 21462.424863100052, 'accumulated_eval_time': 2053.571048259735, 'accumulated_logging_time': 1.5256400108337402}
I0204 21:24:50.500804 139946397853440 logging_writer.py:48] [46755] accumulated_eval_time=2053.571048, accumulated_logging_time=1.525640, accumulated_submission_time=21462.424863, global_step=46755, preemption_count=0, score=21462.424863, test/accuracy=0.531700, test/loss=2.245499, test/num_examples=10000, total_duration=23520.222548, train/accuracy=0.706250, train/loss=1.398217, validation/accuracy=0.654120, validation/loss=1.629255, validation/num_examples=50000
I0204 21:25:08.603939 139946414638848 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.0089024305343628, loss=3.578136444091797
I0204 21:25:52.759686 139946397853440 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.006701946258545, loss=4.103662967681885
I0204 21:26:39.283301 139946414638848 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.9129489064216614, loss=4.829139232635498
I0204 21:27:25.572658 139946397853440 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.9370304942131042, loss=4.307980060577393
I0204 21:28:12.027221 139946414638848 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.0942938327789307, loss=3.5300586223602295
I0204 21:28:58.528501 139946397853440 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.9920619130134583, loss=3.638317346572876
I0204 21:29:44.810696 139946414638848 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.021760106086731, loss=3.5245614051818848
I0204 21:30:31.099074 139946397853440 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.0554333925247192, loss=3.530190944671631
I0204 21:31:17.592290 139946414638848 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.9823994040489197, loss=3.9879884719848633
I0204 21:31:50.831343 140107197974336 spec.py:321] Evaluating on the training split.
I0204 21:32:01.543354 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 21:32:30.767413 140107197974336 spec.py:349] Evaluating on the test split.
I0204 21:32:32.369896 140107197974336 submission_runner.py:408] Time since start: 23982.12s, 	Step: 47674, 	{'train/accuracy': 0.7119921445846558, 'train/loss': 1.3972750902175903, 'validation/accuracy': 0.6566799879074097, 'validation/loss': 1.6393754482269287, 'validation/num_examples': 50000, 'test/accuracy': 0.5311000347137451, 'test/loss': 2.2582650184631348, 'test/num_examples': 10000, 'score': 21882.692858219147, 'total_duration': 23982.116462945938, 'accumulated_submission_time': 21882.692858219147, 'accumulated_eval_time': 2095.10959148407, 'accumulated_logging_time': 1.5598258972167969}
I0204 21:32:32.397333 139946397853440 logging_writer.py:48] [47674] accumulated_eval_time=2095.109591, accumulated_logging_time=1.559826, accumulated_submission_time=21882.692858, global_step=47674, preemption_count=0, score=21882.692858, test/accuracy=0.531100, test/loss=2.258265, test/num_examples=10000, total_duration=23982.116463, train/accuracy=0.711992, train/loss=1.397275, validation/accuracy=0.656680, validation/loss=1.639375, validation/num_examples=50000
I0204 21:32:43.034422 139946414638848 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.9520664215087891, loss=4.269885063171387
I0204 21:33:25.740292 139946397853440 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.029373049736023, loss=3.6365647315979004
I0204 21:34:12.190500 139946414638848 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.039796233177185, loss=3.5847713947296143
I0204 21:34:58.575875 139946397853440 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9489725232124329, loss=3.826505184173584
I0204 21:35:44.889811 139946414638848 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.1080617904663086, loss=3.6179652214050293
I0204 21:36:31.251953 139946397853440 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.005954384803772, loss=3.669820547103882
I0204 21:37:17.508057 139946414638848 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.1768794059753418, loss=3.5727319717407227
I0204 21:38:03.668247 139946397853440 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.0368249416351318, loss=3.602576971054077
I0204 21:38:49.623646 139946414638848 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0237658023834229, loss=5.080407619476318
I0204 21:39:33.292290 140107197974336 spec.py:321] Evaluating on the training split.
I0204 21:39:43.931392 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 21:40:13.264036 140107197974336 spec.py:349] Evaluating on the test split.
I0204 21:40:14.878925 140107197974336 submission_runner.py:408] Time since start: 24444.63s, 	Step: 48595, 	{'train/accuracy': 0.7161718606948853, 'train/loss': 1.3505080938339233, 'validation/accuracy': 0.6545000076293945, 'validation/loss': 1.6139466762542725, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.2515869140625, 'test/num_examples': 10000, 'score': 22303.524026870728, 'total_duration': 24444.625487089157, 'accumulated_submission_time': 22303.524026870728, 'accumulated_eval_time': 2136.696215391159, 'accumulated_logging_time': 1.5978095531463623}
I0204 21:40:14.904219 139946397853440 logging_writer.py:48] [48595] accumulated_eval_time=2136.696215, accumulated_logging_time=1.597810, accumulated_submission_time=22303.524027, global_step=48595, preemption_count=0, score=22303.524027, test/accuracy=0.532300, test/loss=2.251587, test/num_examples=10000, total_duration=24444.625487, train/accuracy=0.716172, train/loss=1.350508, validation/accuracy=0.654500, validation/loss=1.613947, validation/num_examples=50000
I0204 21:40:17.267879 139946414638848 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0311797857284546, loss=4.110810279846191
I0204 21:40:58.808038 139946397853440 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.9637624025344849, loss=4.854989051818848
I0204 21:41:45.221795 139946414638848 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.1940760612487793, loss=3.5790417194366455
I0204 21:42:31.709996 139946397853440 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.0901203155517578, loss=3.6142220497131348
I0204 21:43:18.373898 139946414638848 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9646557569503784, loss=5.313847064971924
I0204 21:44:04.762017 139946397853440 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.057634949684143, loss=3.594682216644287
I0204 21:44:51.233828 139946414638848 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.1532233953475952, loss=3.582549810409546
I0204 21:45:37.490813 139946397853440 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9099892377853394, loss=4.430482387542725
I0204 21:46:23.827466 139946414638848 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.9980964064598083, loss=3.6128382682800293
I0204 21:47:10.096231 139946397853440 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0537912845611572, loss=3.574497699737549
I0204 21:47:15.313955 140107197974336 spec.py:321] Evaluating on the training split.
I0204 21:47:26.010928 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 21:47:50.090457 140107197974336 spec.py:349] Evaluating on the test split.
I0204 21:47:51.691606 140107197974336 submission_runner.py:408] Time since start: 24901.44s, 	Step: 49513, 	{'train/accuracy': 0.7284765243530273, 'train/loss': 1.366170048713684, 'validation/accuracy': 0.6598399877548218, 'validation/loss': 1.6686944961547852, 'validation/num_examples': 50000, 'test/accuracy': 0.5327000021934509, 'test/loss': 2.3007359504699707, 'test/num_examples': 10000, 'score': 22723.87028694153, 'total_duration': 24901.43813586235, 'accumulated_submission_time': 22723.87028694153, 'accumulated_eval_time': 2173.0738096237183, 'accumulated_logging_time': 1.6330816745758057}
I0204 21:47:51.718695 139946414638848 logging_writer.py:48] [49513] accumulated_eval_time=2173.073810, accumulated_logging_time=1.633082, accumulated_submission_time=22723.870287, global_step=49513, preemption_count=0, score=22723.870287, test/accuracy=0.532700, test/loss=2.300736, test/num_examples=10000, total_duration=24901.438136, train/accuracy=0.728477, train/loss=1.366170, validation/accuracy=0.659840, validation/loss=1.668694, validation/num_examples=50000
I0204 21:48:27.332459 139946397853440 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.9744920134544373, loss=3.724806547164917
I0204 21:49:13.575182 139946414638848 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.1895469427108765, loss=3.515381097793579
I0204 21:50:00.176383 139946397853440 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.1027411222457886, loss=3.6792361736297607
I0204 21:50:46.515887 139946414638848 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.0662596225738525, loss=5.318019866943359
I0204 21:51:33.073097 139946397853440 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.0695533752441406, loss=5.246701717376709
I0204 21:52:19.545810 139946414638848 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.084430456161499, loss=3.595034122467041
I0204 21:53:05.732150 139946397853440 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.0197116136550903, loss=3.615178108215332
I0204 21:53:52.049183 139946414638848 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.0676913261413574, loss=3.563521146774292
I0204 21:54:38.686826 139946397853440 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.9170225858688354, loss=4.984734535217285
I0204 21:54:51.763772 140107197974336 spec.py:321] Evaluating on the training split.
I0204 21:55:02.537678 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 21:55:32.132241 140107197974336 spec.py:349] Evaluating on the test split.
I0204 21:55:33.730682 140107197974336 submission_runner.py:408] Time since start: 25363.48s, 	Step: 50430, 	{'train/accuracy': 0.7109179496765137, 'train/loss': 1.3661249876022339, 'validation/accuracy': 0.6569799780845642, 'validation/loss': 1.603185772895813, 'validation/num_examples': 50000, 'test/accuracy': 0.5389000177383423, 'test/loss': 2.222001552581787, 'test/num_examples': 10000, 'score': 23143.849014759064, 'total_duration': 25363.477248191833, 'accumulated_submission_time': 23143.849014759064, 'accumulated_eval_time': 2215.0406877994537, 'accumulated_logging_time': 1.6706516742706299}
I0204 21:55:33.754651 139946414638848 logging_writer.py:48] [50430] accumulated_eval_time=2215.040688, accumulated_logging_time=1.670652, accumulated_submission_time=23143.849015, global_step=50430, preemption_count=0, score=23143.849015, test/accuracy=0.538900, test/loss=2.222002, test/num_examples=10000, total_duration=25363.477248, train/accuracy=0.710918, train/loss=1.366125, validation/accuracy=0.656980, validation/loss=1.603186, validation/num_examples=50000
I0204 21:56:01.965277 139946397853440 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.031062126159668, loss=3.543135166168213
I0204 21:56:48.232405 139946414638848 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.1520135402679443, loss=3.5239977836608887
I0204 21:57:34.908514 139946397853440 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.953332245349884, loss=4.104645729064941
I0204 21:58:21.158900 139946414638848 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.888368546962738, loss=4.549981117248535
I0204 21:59:07.560405 139946397853440 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.1046991348266602, loss=3.637887716293335
I0204 21:59:53.954316 139946414638848 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1229586601257324, loss=3.6001553535461426
I0204 22:00:40.334417 139946397853440 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.9993392825126648, loss=5.381174087524414
I0204 22:01:26.713946 139946414638848 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.1057837009429932, loss=3.5216152667999268
I0204 22:02:13.190193 139946397853440 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.0908955335617065, loss=3.6416571140289307
I0204 22:02:33.796038 140107197974336 spec.py:321] Evaluating on the training split.
I0204 22:02:44.370584 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 22:03:14.551404 140107197974336 spec.py:349] Evaluating on the test split.
I0204 22:03:16.155766 140107197974336 submission_runner.py:408] Time since start: 25825.90s, 	Step: 51346, 	{'train/accuracy': 0.7173827886581421, 'train/loss': 1.383202314376831, 'validation/accuracy': 0.6582199931144714, 'validation/loss': 1.6434626579284668, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.27443790435791, 'test/num_examples': 10000, 'score': 23563.827913999557, 'total_duration': 25825.90231370926, 'accumulated_submission_time': 23563.827913999557, 'accumulated_eval_time': 2257.4003636837006, 'accumulated_logging_time': 1.7045204639434814}
I0204 22:03:16.186570 139946414638848 logging_writer.py:48] [51346] accumulated_eval_time=2257.400364, accumulated_logging_time=1.704520, accumulated_submission_time=23563.827914, global_step=51346, preemption_count=0, score=23563.827914, test/accuracy=0.532900, test/loss=2.274438, test/num_examples=10000, total_duration=25825.902314, train/accuracy=0.717383, train/loss=1.383202, validation/accuracy=0.658220, validation/loss=1.643463, validation/num_examples=50000
I0204 22:03:37.817476 139946397853440 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.0764471292495728, loss=3.5122158527374268
I0204 22:04:22.786989 139946414638848 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.0266835689544678, loss=3.640657663345337
I0204 22:05:09.507220 139946397853440 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.027622938156128, loss=3.8338263034820557
I0204 22:05:55.629016 139946414638848 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.0921854972839355, loss=3.6355879306793213
I0204 22:06:42.054363 139946397853440 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.0114011764526367, loss=5.425829887390137
I0204 22:07:28.518741 139946414638848 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.089657187461853, loss=3.866664409637451
I0204 22:08:14.808584 139946397853440 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.0248585939407349, loss=4.084231853485107
I0204 22:09:00.902670 139946414638848 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.0758817195892334, loss=3.4582395553588867
I0204 22:09:47.079310 139946397853440 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.0974010229110718, loss=3.5247347354888916
I0204 22:10:16.550678 140107197974336 spec.py:321] Evaluating on the training split.
I0204 22:10:27.411861 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 22:10:53.700410 140107197974336 spec.py:349] Evaluating on the test split.
I0204 22:10:55.328806 140107197974336 submission_runner.py:408] Time since start: 26285.08s, 	Step: 52265, 	{'train/accuracy': 0.7373827695846558, 'train/loss': 1.3297600746154785, 'validation/accuracy': 0.6584999561309814, 'validation/loss': 1.656172513961792, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.2770166397094727, 'test/num_examples': 10000, 'score': 23984.125440120697, 'total_duration': 26285.075368642807, 'accumulated_submission_time': 23984.125440120697, 'accumulated_eval_time': 2296.1784982681274, 'accumulated_logging_time': 1.7483222484588623}
I0204 22:10:55.359396 139946414638848 logging_writer.py:48] [52265] accumulated_eval_time=2296.178498, accumulated_logging_time=1.748322, accumulated_submission_time=23984.125440, global_step=52265, preemption_count=0, score=23984.125440, test/accuracy=0.534300, test/loss=2.277017, test/num_examples=10000, total_duration=26285.075369, train/accuracy=0.737383, train/loss=1.329760, validation/accuracy=0.658500, validation/loss=1.656173, validation/num_examples=50000
I0204 22:11:09.701304 139946397853440 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.9522940516471863, loss=4.277356147766113
I0204 22:11:53.415290 139946414638848 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.0748567581176758, loss=3.5829954147338867
I0204 22:12:39.887845 139946397853440 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.0340931415557861, loss=3.5000219345092773
I0204 22:13:26.593439 139946414638848 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.1048625707626343, loss=5.231530666351318
I0204 22:14:12.930146 139946397853440 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.061750054359436, loss=5.286289215087891
I0204 22:14:59.452448 139946414638848 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.0638549327850342, loss=3.6152970790863037
I0204 22:15:45.880424 139946397853440 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.913783848285675, loss=3.9962575435638428
I0204 22:16:32.460353 139946414638848 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.1688063144683838, loss=3.546199321746826
I0204 22:17:18.980863 139946397853440 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.1048463582992554, loss=3.5794200897216797
I0204 22:17:55.479388 140107197974336 spec.py:321] Evaluating on the training split.
I0204 22:18:06.344808 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 22:18:36.227557 140107197974336 spec.py:349] Evaluating on the test split.
I0204 22:18:37.824415 140107197974336 submission_runner.py:408] Time since start: 26747.57s, 	Step: 53181, 	{'train/accuracy': 0.7142968773841858, 'train/loss': 1.3528928756713867, 'validation/accuracy': 0.663599967956543, 'validation/loss': 1.5829827785491943, 'validation/num_examples': 50000, 'test/accuracy': 0.5390000343322754, 'test/loss': 2.2040934562683105, 'test/num_examples': 10000, 'score': 24404.181349277496, 'total_duration': 26747.570974826813, 'accumulated_submission_time': 24404.181349277496, 'accumulated_eval_time': 2338.523509502411, 'accumulated_logging_time': 1.7898108959197998}
I0204 22:18:37.853058 139946414638848 logging_writer.py:48] [53181] accumulated_eval_time=2338.523510, accumulated_logging_time=1.789811, accumulated_submission_time=24404.181349, global_step=53181, preemption_count=0, score=24404.181349, test/accuracy=0.539000, test/loss=2.204093, test/num_examples=10000, total_duration=26747.570975, train/accuracy=0.714297, train/loss=1.352893, validation/accuracy=0.663600, validation/loss=1.582983, validation/num_examples=50000
I0204 22:18:45.725149 139946397853440 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2223247289657593, loss=3.582322359085083
I0204 22:19:28.760686 139946414638848 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.104796051979065, loss=3.522434949874878
I0204 22:20:15.272986 139946397853440 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.0661661624908447, loss=3.492003917694092
I0204 22:21:01.911439 139946414638848 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0251052379608154, loss=3.5385706424713135
I0204 22:21:48.099907 139946397853440 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.031544804573059, loss=3.6006064414978027
I0204 22:22:34.755103 139946414638848 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.1082038879394531, loss=3.4741902351379395
I0204 22:23:21.433299 139946397853440 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.1228083372116089, loss=3.5534934997558594
I0204 22:24:08.038736 139946414638848 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.0723687410354614, loss=3.5985023975372314
I0204 22:24:54.707576 139946397853440 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0193899869918823, loss=4.165265083312988
I0204 22:25:38.012368 140107197974336 spec.py:321] Evaluating on the training split.
I0204 22:25:48.885059 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 22:26:14.198749 140107197974336 spec.py:349] Evaluating on the test split.
I0204 22:26:15.790265 140107197974336 submission_runner.py:408] Time since start: 27205.54s, 	Step: 54095, 	{'train/accuracy': 0.7222851514816284, 'train/loss': 1.373044729232788, 'validation/accuracy': 0.6613799929618835, 'validation/loss': 1.6264369487762451, 'validation/num_examples': 50000, 'test/accuracy': 0.5357000231742859, 'test/loss': 2.2335379123687744, 'test/num_examples': 10000, 'score': 24824.275985479355, 'total_duration': 27205.53682422638, 'accumulated_submission_time': 24824.275985479355, 'accumulated_eval_time': 2376.301381111145, 'accumulated_logging_time': 1.831099271774292}
I0204 22:26:15.816338 139946414638848 logging_writer.py:48] [54095] accumulated_eval_time=2376.301381, accumulated_logging_time=1.831099, accumulated_submission_time=24824.275985, global_step=54095, preemption_count=0, score=24824.275985, test/accuracy=0.535700, test/loss=2.233538, test/num_examples=10000, total_duration=27205.536824, train/accuracy=0.722285, train/loss=1.373045, validation/accuracy=0.661380, validation/loss=1.626437, validation/num_examples=50000
I0204 22:26:18.182235 139946397853440 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.9068815112113953, loss=4.276988983154297
I0204 22:26:59.753250 139946414638848 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.121216893196106, loss=3.593114137649536
I0204 22:27:45.868284 139946397853440 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.0523872375488281, loss=3.8142127990722656
I0204 22:28:32.483477 139946414638848 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.1796069145202637, loss=3.6616244316101074
I0204 22:29:18.744071 139946397853440 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.0606539249420166, loss=3.4369900226593018
I0204 22:30:05.181954 139946414638848 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.001381516456604, loss=4.138448238372803
I0204 22:30:51.478988 139946397853440 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.1463568210601807, loss=3.508490562438965
I0204 22:31:37.958650 139946414638848 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.0908037424087524, loss=3.5834920406341553
I0204 22:32:24.602207 139946397853440 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.030307412147522, loss=4.298413276672363
I0204 22:33:10.891196 139946414638848 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.0939663648605347, loss=3.5162363052368164
I0204 22:33:16.125521 140107197974336 spec.py:321] Evaluating on the training split.
I0204 22:33:26.894317 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 22:33:52.916626 140107197974336 spec.py:349] Evaluating on the test split.
I0204 22:33:54.524074 140107197974336 submission_runner.py:408] Time since start: 27664.27s, 	Step: 55013, 	{'train/accuracy': 0.7419335842132568, 'train/loss': 1.2533196210861206, 'validation/accuracy': 0.6665599942207336, 'validation/loss': 1.5780009031295776, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.186769962310791, 'test/num_examples': 10000, 'score': 25244.52186369896, 'total_duration': 27664.27063536644, 'accumulated_submission_time': 25244.52186369896, 'accumulated_eval_time': 2414.6999106407166, 'accumulated_logging_time': 1.867379903793335}
I0204 22:33:54.551470 139946397853440 logging_writer.py:48] [55013] accumulated_eval_time=2414.699911, accumulated_logging_time=1.867380, accumulated_submission_time=25244.521864, global_step=55013, preemption_count=0, score=25244.521864, test/accuracy=0.547900, test/loss=2.186770, test/num_examples=10000, total_duration=27664.270635, train/accuracy=0.741934, train/loss=1.253320, validation/accuracy=0.666560, validation/loss=1.578001, validation/num_examples=50000
I0204 22:34:30.592398 139946414638848 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0862305164337158, loss=5.142129898071289
I0204 22:35:16.505463 139946397853440 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.0387898683547974, loss=3.975292205810547
I0204 22:36:02.953353 139946414638848 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.1049588918685913, loss=5.162675380706787
I0204 22:36:49.298070 139946397853440 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.0739747285842896, loss=3.5252957344055176
I0204 22:37:35.631720 139946414638848 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.017696738243103, loss=4.2586236000061035
I0204 22:38:22.291712 139946397853440 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.988755464553833, loss=3.8489935398101807
I0204 22:39:08.877165 139946414638848 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.2529430389404297, loss=3.625462770462036
I0204 22:39:55.103185 139946397853440 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.1268795728683472, loss=3.4791901111602783
I0204 22:40:41.495904 139946414638848 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.2076115608215332, loss=3.5192501544952393
I0204 22:40:54.999613 140107197974336 spec.py:321] Evaluating on the training split.
I0204 22:41:05.798413 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 22:41:35.749996 140107197974336 spec.py:349] Evaluating on the test split.
I0204 22:41:37.352065 140107197974336 submission_runner.py:408] Time since start: 28127.10s, 	Step: 55931, 	{'train/accuracy': 0.72083979845047, 'train/loss': 1.3554744720458984, 'validation/accuracy': 0.6647799611091614, 'validation/loss': 1.584027886390686, 'validation/num_examples': 50000, 'test/accuracy': 0.5414000153541565, 'test/loss': 2.191251754760742, 'test/num_examples': 10000, 'score': 25664.90718483925, 'total_duration': 28127.09863138199, 'accumulated_submission_time': 25664.90718483925, 'accumulated_eval_time': 2457.0523324012756, 'accumulated_logging_time': 1.9043676853179932}
I0204 22:41:37.380739 139946397853440 logging_writer.py:48] [55931] accumulated_eval_time=2457.052332, accumulated_logging_time=1.904368, accumulated_submission_time=25664.907185, global_step=55931, preemption_count=0, score=25664.907185, test/accuracy=0.541400, test/loss=2.191252, test/num_examples=10000, total_duration=28127.098631, train/accuracy=0.720840, train/loss=1.355474, validation/accuracy=0.664780, validation/loss=1.584028, validation/num_examples=50000
I0204 22:42:05.112379 139946414638848 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.9718602895736694, loss=4.025609970092773
I0204 22:42:51.172122 139946397853440 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0425831079483032, loss=3.6850714683532715
I0204 22:43:38.079701 139946414638848 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.05463445186615, loss=3.7557308673858643
I0204 22:44:25.040975 139946397853440 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.102659821510315, loss=3.613109827041626
I0204 22:45:11.292581 139946414638848 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2182666063308716, loss=3.5026955604553223
I0204 22:45:57.583609 139946397853440 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.1297876834869385, loss=3.5134449005126953
I0204 22:46:44.221871 139946414638848 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.9466138482093811, loss=4.757073402404785
I0204 22:47:30.579098 139946397853440 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.0477126836776733, loss=3.8165156841278076
I0204 22:48:17.059461 139946414638848 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.1039365530014038, loss=3.9663729667663574
I0204 22:48:37.778187 140107197974336 spec.py:321] Evaluating on the training split.
I0204 22:48:48.573374 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 22:49:18.399884 140107197974336 spec.py:349] Evaluating on the test split.
I0204 22:49:19.988020 140107197974336 submission_runner.py:408] Time since start: 28589.73s, 	Step: 56846, 	{'train/accuracy': 0.7316796779632568, 'train/loss': 1.2941749095916748, 'validation/accuracy': 0.6717199683189392, 'validation/loss': 1.55128014087677, 'validation/num_examples': 50000, 'test/accuracy': 0.5484000444412231, 'test/loss': 2.1710853576660156, 'test/num_examples': 10000, 'score': 26085.24145579338, 'total_duration': 28589.734586954117, 'accumulated_submission_time': 26085.24145579338, 'accumulated_eval_time': 2499.262162208557, 'accumulated_logging_time': 1.9429833889007568}
I0204 22:49:20.012069 139946397853440 logging_writer.py:48] [56846] accumulated_eval_time=2499.262162, accumulated_logging_time=1.942983, accumulated_submission_time=26085.241456, global_step=56846, preemption_count=0, score=26085.241456, test/accuracy=0.548400, test/loss=2.171085, test/num_examples=10000, total_duration=28589.734587, train/accuracy=0.731680, train/loss=1.294175, validation/accuracy=0.671720, validation/loss=1.551280, validation/num_examples=50000
I0204 22:49:41.631957 139946414638848 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.119667410850525, loss=5.131345272064209
I0204 22:50:26.910795 139946397853440 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.045809030532837, loss=3.5040736198425293
I0204 22:51:13.338751 139946414638848 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.0098947286605835, loss=4.449220180511475
I0204 22:51:59.855974 139946397853440 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.07171630859375, loss=3.5196571350097656
I0204 22:52:46.478149 139946414638848 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.1562343835830688, loss=3.6047780513763428
I0204 22:53:32.946943 139946397853440 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.0702277421951294, loss=3.448561906814575
I0204 22:54:19.212103 139946414638848 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.0140044689178467, loss=3.5538134574890137
I0204 22:55:06.037063 139946397853440 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.084614634513855, loss=5.0862579345703125
I0204 22:55:52.364963 139946414638848 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.2263089418411255, loss=3.5060338973999023
I0204 22:56:20.424858 140107197974336 spec.py:321] Evaluating on the training split.
I0204 22:56:31.193488 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 22:56:58.524583 140107197974336 spec.py:349] Evaluating on the test split.
I0204 22:57:00.121063 140107197974336 submission_runner.py:408] Time since start: 29049.87s, 	Step: 57762, 	{'train/accuracy': 0.7390429377555847, 'train/loss': 1.2824914455413818, 'validation/accuracy': 0.6715999841690063, 'validation/loss': 1.5749740600585938, 'validation/num_examples': 50000, 'test/accuracy': 0.5490000247955322, 'test/loss': 2.189195394515991, 'test/num_examples': 10000, 'score': 26505.591643333435, 'total_duration': 29049.86763215065, 'accumulated_submission_time': 26505.591643333435, 'accumulated_eval_time': 2538.9583842754364, 'accumulated_logging_time': 1.9764173030853271}
I0204 22:57:00.146392 139946397853440 logging_writer.py:48] [57762] accumulated_eval_time=2538.958384, accumulated_logging_time=1.976417, accumulated_submission_time=26505.591643, global_step=57762, preemption_count=0, score=26505.591643, test/accuracy=0.549000, test/loss=2.189195, test/num_examples=10000, total_duration=29049.867632, train/accuracy=0.739043, train/loss=1.282491, validation/accuracy=0.671600, validation/loss=1.574974, validation/num_examples=50000
I0204 22:57:15.468471 139946414638848 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.1012810468673706, loss=3.5246176719665527
I0204 22:57:59.495136 139946397853440 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.0138920545578003, loss=4.6424689292907715
I0204 22:58:46.246259 139946414638848 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1071851253509521, loss=4.548835277557373
I0204 22:59:32.927497 139946397853440 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.096917986869812, loss=3.5594429969787598
I0204 23:00:19.097811 139946414638848 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.2724697589874268, loss=3.5875449180603027
I0204 23:01:05.886397 139946397853440 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.159674048423767, loss=3.4703714847564697
I0204 23:01:52.184374 139946414638848 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.9862304329872131, loss=4.415075778961182
I0204 23:02:38.680310 139946397853440 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.0595300197601318, loss=3.4924018383026123
I0204 23:03:25.061342 139946414638848 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.182249903678894, loss=3.5600101947784424
I0204 23:04:00.488569 140107197974336 spec.py:321] Evaluating on the training split.
I0204 23:04:12.254876 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 23:04:43.078526 140107197974336 spec.py:349] Evaluating on the test split.
I0204 23:04:44.682321 140107197974336 submission_runner.py:408] Time since start: 29514.43s, 	Step: 58678, 	{'train/accuracy': 0.7281835675239563, 'train/loss': 1.3011599779129028, 'validation/accuracy': 0.6716399788856506, 'validation/loss': 1.5460153818130493, 'validation/num_examples': 50000, 'test/accuracy': 0.5476000308990479, 'test/loss': 2.1589043140411377, 'test/num_examples': 10000, 'score': 26925.87080693245, 'total_duration': 29514.428884983063, 'accumulated_submission_time': 26925.87080693245, 'accumulated_eval_time': 2583.1521196365356, 'accumulated_logging_time': 2.011991500854492}
I0204 23:04:44.713051 139946397853440 logging_writer.py:48] [58678] accumulated_eval_time=2583.152120, accumulated_logging_time=2.011992, accumulated_submission_time=26925.870807, global_step=58678, preemption_count=0, score=26925.870807, test/accuracy=0.547600, test/loss=2.158904, test/num_examples=10000, total_duration=29514.428885, train/accuracy=0.728184, train/loss=1.301160, validation/accuracy=0.671640, validation/loss=1.546015, validation/num_examples=50000
I0204 23:04:53.740151 139946414638848 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.9876223206520081, loss=4.411721706390381
I0204 23:05:36.841169 139946397853440 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.9874157309532166, loss=3.7388007640838623
I0204 23:06:22.556391 139946414638848 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.1758238077163696, loss=3.4506001472473145
I0204 23:07:09.048373 139946397853440 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.0425710678100586, loss=4.432233810424805
I0204 23:07:55.316547 139946414638848 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.9923077821731567, loss=4.837575435638428
I0204 23:08:41.670954 139946397853440 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.0398160219192505, loss=3.379429340362549
I0204 23:09:28.008440 139946414638848 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.0668057203292847, loss=5.334343433380127
I0204 23:10:14.404040 139946397853440 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.2594562768936157, loss=3.340128183364868
I0204 23:11:00.788561 139946414638848 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.0772342681884766, loss=3.4684221744537354
I0204 23:11:44.925230 140107197974336 spec.py:321] Evaluating on the training split.
I0204 23:11:55.626334 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 23:12:20.915925 140107197974336 spec.py:349] Evaluating on the test split.
I0204 23:12:22.511409 140107197974336 submission_runner.py:408] Time since start: 29972.26s, 	Step: 59597, 	{'train/accuracy': 0.7227734327316284, 'train/loss': 1.3543765544891357, 'validation/accuracy': 0.6686399579048157, 'validation/loss': 1.5894092321395874, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.1943323612213135, 'test/num_examples': 10000, 'score': 27346.019050359726, 'total_duration': 29972.257979154587, 'accumulated_submission_time': 27346.019050359726, 'accumulated_eval_time': 2620.7382864952087, 'accumulated_logging_time': 2.0540719032287598}
I0204 23:12:22.543198 139946397853440 logging_writer.py:48] [59597] accumulated_eval_time=2620.738286, accumulated_logging_time=2.054072, accumulated_submission_time=27346.019050, global_step=59597, preemption_count=0, score=27346.019050, test/accuracy=0.543300, test/loss=2.194332, test/num_examples=10000, total_duration=29972.257979, train/accuracy=0.722773, train/loss=1.354377, validation/accuracy=0.668640, validation/loss=1.589409, validation/num_examples=50000
I0204 23:12:24.123800 139946414638848 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.0175331830978394, loss=5.110125541687012
I0204 23:13:05.580031 139946397853440 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.0731531381607056, loss=3.41544246673584
I0204 23:13:51.787572 139946414638848 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.0548131465911865, loss=3.8634021282196045
I0204 23:14:39.025496 139946397853440 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.0164003372192383, loss=4.484619140625
I0204 23:15:25.434736 139946414638848 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.096814751625061, loss=3.499490737915039
I0204 23:16:12.146459 139946397853440 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.1461023092269897, loss=3.4976234436035156
I0204 23:16:58.449352 139946414638848 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.1886531114578247, loss=3.485154628753662
I0204 23:17:44.948512 139946397853440 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.106563687324524, loss=3.4602785110473633
I0204 23:18:31.360506 139946414638848 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.0209300518035889, loss=4.667904853820801
I0204 23:19:17.942755 139946397853440 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.0759731531143188, loss=3.390432834625244
I0204 23:19:22.793433 140107197974336 spec.py:321] Evaluating on the training split.
I0204 23:19:33.924398 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 23:20:04.053827 140107197974336 spec.py:349] Evaluating on the test split.
I0204 23:20:05.649277 140107197974336 submission_runner.py:408] Time since start: 30435.40s, 	Step: 60512, 	{'train/accuracy': 0.7395703196525574, 'train/loss': 1.2908554077148438, 'validation/accuracy': 0.6735599637031555, 'validation/loss': 1.5688538551330566, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.1810519695281982, 'test/num_examples': 10000, 'score': 27766.20599770546, 'total_duration': 30435.395832777023, 'accumulated_submission_time': 27766.20599770546, 'accumulated_eval_time': 2663.5940973758698, 'accumulated_logging_time': 2.0963516235351562}
I0204 23:20:05.674092 139946414638848 logging_writer.py:48] [60512] accumulated_eval_time=2663.594097, accumulated_logging_time=2.096352, accumulated_submission_time=27766.205998, global_step=60512, preemption_count=0, score=27766.205998, test/accuracy=0.552800, test/loss=2.181052, test/num_examples=10000, total_duration=30435.395833, train/accuracy=0.739570, train/loss=1.290855, validation/accuracy=0.673560, validation/loss=1.568854, validation/num_examples=50000
I0204 23:20:41.900266 139946397853440 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.1103465557098389, loss=5.183107376098633
I0204 23:21:27.836125 139946414638848 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.0535067319869995, loss=4.989440441131592
I0204 23:22:14.489040 139946397853440 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.96450275182724, loss=4.420243740081787
I0204 23:23:00.734147 139946414638848 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0680752992630005, loss=3.5149052143096924
I0204 23:23:47.229077 139946397853440 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.3616830110549927, loss=3.466829538345337
I0204 23:24:33.804113 139946414638848 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.1222693920135498, loss=3.346829414367676
I0204 23:25:20.376040 139946397853440 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.053230881690979, loss=3.3861284255981445
I0204 23:26:07.037516 139946414638848 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.1126503944396973, loss=3.4132442474365234
I0204 23:26:52.990168 139946397853440 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.167996883392334, loss=5.182326316833496
I0204 23:27:05.820944 140107197974336 spec.py:321] Evaluating on the training split.
I0204 23:27:16.477475 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 23:27:46.124353 140107197974336 spec.py:349] Evaluating on the test split.
I0204 23:27:47.721689 140107197974336 submission_runner.py:408] Time since start: 30897.47s, 	Step: 61429, 	{'train/accuracy': 0.7494531273841858, 'train/loss': 1.2534152269363403, 'validation/accuracy': 0.6738799810409546, 'validation/loss': 1.5649051666259766, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.1733086109161377, 'test/num_examples': 10000, 'score': 28186.290335416794, 'total_duration': 30897.46825361252, 'accumulated_submission_time': 28186.290335416794, 'accumulated_eval_time': 2705.494818210602, 'accumulated_logging_time': 2.1315011978149414}
I0204 23:27:47.746710 139946414638848 logging_writer.py:48] [61429] accumulated_eval_time=2705.494818, accumulated_logging_time=2.131501, accumulated_submission_time=28186.290335, global_step=61429, preemption_count=0, score=28186.290335, test/accuracy=0.551700, test/loss=2.173309, test/num_examples=10000, total_duration=30897.468254, train/accuracy=0.749453, train/loss=1.253415, validation/accuracy=0.673880, validation/loss=1.564905, validation/num_examples=50000
I0204 23:28:16.039757 139946397853440 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.097855806350708, loss=5.008476734161377
I0204 23:29:01.976332 139946414638848 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.23236083984375, loss=3.400174617767334
I0204 23:29:48.556196 139946397853440 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.086411476135254, loss=5.105683326721191
I0204 23:30:34.929490 139946414638848 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.0967984199523926, loss=3.739346504211426
I0204 23:31:21.417433 139946397853440 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.996429979801178, loss=4.44629430770874
I0204 23:32:07.642270 139946414638848 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.1539350748062134, loss=3.426920175552368
I0204 23:32:53.846310 139946397853440 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.1314308643341064, loss=3.4150185585021973
I0204 23:33:40.252216 139946414638848 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.0035686492919922, loss=3.7736117839813232
I0204 23:34:28.067423 139946397853440 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.9996284246444702, loss=4.483370304107666
I0204 23:34:47.766547 140107197974336 spec.py:321] Evaluating on the training split.
I0204 23:34:58.384335 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 23:35:27.110090 140107197974336 spec.py:349] Evaluating on the test split.
I0204 23:35:28.707585 140107197974336 submission_runner.py:408] Time since start: 31358.45s, 	Step: 62343, 	{'train/accuracy': 0.7285351157188416, 'train/loss': 1.318561315536499, 'validation/accuracy': 0.6755599975585938, 'validation/loss': 1.5545648336410522, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.1707515716552734, 'test/num_examples': 10000, 'score': 28606.247513771057, 'total_duration': 31358.454117536545, 'accumulated_submission_time': 28606.247513771057, 'accumulated_eval_time': 2746.435801267624, 'accumulated_logging_time': 2.166461229324341}
I0204 23:35:28.736052 139946414638848 logging_writer.py:48] [62343] accumulated_eval_time=2746.435801, accumulated_logging_time=2.166461, accumulated_submission_time=28606.247514, global_step=62343, preemption_count=0, score=28606.247514, test/accuracy=0.550300, test/loss=2.170752, test/num_examples=10000, total_duration=31358.454118, train/accuracy=0.728535, train/loss=1.318561, validation/accuracy=0.675560, validation/loss=1.554565, validation/num_examples=50000
I0204 23:35:51.529243 139946397853440 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.088194489479065, loss=3.8718652725219727
I0204 23:36:36.949177 139946414638848 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.043343424797058, loss=4.192465782165527
I0204 23:37:23.533185 139946397853440 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.1611875295639038, loss=3.3824427127838135
I0204 23:38:10.194391 139946414638848 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.0922489166259766, loss=5.119744300842285
I0204 23:38:56.553448 139946397853440 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.1243066787719727, loss=3.8485989570617676
I0204 23:39:43.139469 139946414638848 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.1772761344909668, loss=3.4630672931671143
I0204 23:40:29.605968 139946397853440 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.1190431118011475, loss=3.516646146774292
I0204 23:41:16.015843 139946414638848 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.055466890335083, loss=3.488233804702759
I0204 23:42:02.188427 139946397853440 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.2121083736419678, loss=3.4448156356811523
I0204 23:42:28.761379 140107197974336 spec.py:321] Evaluating on the training split.
I0204 23:42:39.298399 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 23:43:05.415657 140107197974336 spec.py:349] Evaluating on the test split.
I0204 23:43:07.014316 140107197974336 submission_runner.py:408] Time since start: 31816.76s, 	Step: 63259, 	{'train/accuracy': 0.7363085746765137, 'train/loss': 1.2871750593185425, 'validation/accuracy': 0.6755399703979492, 'validation/loss': 1.5438107252120972, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 2.1767959594726562, 'test/num_examples': 10000, 'score': 29026.210033893585, 'total_duration': 31816.76087665558, 'accumulated_submission_time': 29026.210033893585, 'accumulated_eval_time': 2784.688705921173, 'accumulated_logging_time': 2.2054715156555176}
I0204 23:43:07.040662 139946414638848 logging_writer.py:48] [63259] accumulated_eval_time=2784.688706, accumulated_logging_time=2.205472, accumulated_submission_time=29026.210034, global_step=63259, preemption_count=0, score=29026.210034, test/accuracy=0.552900, test/loss=2.176796, test/num_examples=10000, total_duration=31816.760877, train/accuracy=0.736309, train/loss=1.287175, validation/accuracy=0.675540, validation/loss=1.543811, validation/num_examples=50000
I0204 23:43:23.566411 139946397853440 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0763956308364868, loss=4.013747215270996
I0204 23:44:07.492775 139946414638848 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.1386140584945679, loss=5.131222248077393
I0204 23:44:54.083858 139946397853440 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.0117557048797607, loss=4.448530197143555
I0204 23:45:40.752012 139946414638848 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.1284655332565308, loss=3.4244065284729004
I0204 23:46:27.039247 139946397853440 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.0123982429504395, loss=4.2099928855896
I0204 23:47:13.870057 139946414638848 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.1778391599655151, loss=3.3879153728485107
I0204 23:47:59.976274 139946397853440 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.1368577480316162, loss=4.658529758453369
I0204 23:48:46.486498 139946414638848 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1145869493484497, loss=3.3589179515838623
I0204 23:49:32.995050 139946397853440 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.1153548955917358, loss=3.607240676879883
I0204 23:50:07.041248 140107197974336 spec.py:321] Evaluating on the training split.
I0204 23:50:17.792976 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 23:50:45.011998 140107197974336 spec.py:349] Evaluating on the test split.
I0204 23:50:46.603266 140107197974336 submission_runner.py:408] Time since start: 32276.35s, 	Step: 64175, 	{'train/accuracy': 0.752734363079071, 'train/loss': 1.2191165685653687, 'validation/accuracy': 0.6774399876594543, 'validation/loss': 1.541650414466858, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 2.1525299549102783, 'test/num_examples': 10000, 'score': 29446.1415143013, 'total_duration': 32276.34983420372, 'accumulated_submission_time': 29446.1415143013, 'accumulated_eval_time': 2824.250700235367, 'accumulated_logging_time': 2.2430827617645264}
I0204 23:50:46.629740 139946414638848 logging_writer.py:48] [64175] accumulated_eval_time=2824.250700, accumulated_logging_time=2.243083, accumulated_submission_time=29446.141514, global_step=64175, preemption_count=0, score=29446.141514, test/accuracy=0.559000, test/loss=2.152530, test/num_examples=10000, total_duration=32276.349834, train/accuracy=0.752734, train/loss=1.219117, validation/accuracy=0.677440, validation/loss=1.541650, validation/num_examples=50000
I0204 23:50:56.870179 139946397853440 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.1731032133102417, loss=3.3960602283477783
I0204 23:51:39.865467 139946414638848 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.2036323547363281, loss=3.4573893547058105
I0204 23:52:26.093781 139946397853440 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.1633249521255493, loss=3.369431257247925
I0204 23:53:12.693020 139946414638848 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.1346142292022705, loss=3.5012435913085938
I0204 23:53:58.847594 139946397853440 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.186087727546692, loss=3.529003381729126
I0204 23:54:45.579453 139946414638848 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.1913795471191406, loss=3.4543099403381348
I0204 23:55:31.949624 139946397853440 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.071536898612976, loss=3.407648801803589
I0204 23:56:18.567998 139946414638848 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.051669716835022, loss=4.052196502685547
I0204 23:57:04.964757 139946397853440 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.1623387336730957, loss=5.258745193481445
I0204 23:57:46.657695 140107197974336 spec.py:321] Evaluating on the training split.
I0204 23:57:57.392646 140107197974336 spec.py:333] Evaluating on the validation split.
I0204 23:58:22.418992 140107197974336 spec.py:349] Evaluating on the test split.
I0204 23:58:24.011081 140107197974336 submission_runner.py:408] Time since start: 32733.76s, 	Step: 65091, 	{'train/accuracy': 0.7371875047683716, 'train/loss': 1.289478063583374, 'validation/accuracy': 0.6808599829673767, 'validation/loss': 1.5334938764572144, 'validation/num_examples': 50000, 'test/accuracy': 0.5505000352859497, 'test/loss': 2.155724048614502, 'test/num_examples': 10000, 'score': 29866.106069087982, 'total_duration': 32733.75764989853, 'accumulated_submission_time': 29866.106069087982, 'accumulated_eval_time': 2861.604071855545, 'accumulated_logging_time': 2.2804622650146484}
I0204 23:58:24.036538 139946414638848 logging_writer.py:48] [65091] accumulated_eval_time=2861.604072, accumulated_logging_time=2.280462, accumulated_submission_time=29866.106069, global_step=65091, preemption_count=0, score=29866.106069, test/accuracy=0.550500, test/loss=2.155724, test/num_examples=10000, total_duration=32733.757650, train/accuracy=0.737188, train/loss=1.289478, validation/accuracy=0.680860, validation/loss=1.533494, validation/num_examples=50000
I0204 23:58:27.973957 139946397853440 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.0086759328842163, loss=4.591845512390137
I0204 23:59:09.757898 139946414638848 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.0369648933410645, loss=4.696915626525879
I0204 23:59:55.930552 139946397853440 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.0245158672332764, loss=3.7140414714813232
I0205 00:00:42.664325 139946414638848 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.1787408590316772, loss=3.5144155025482178
I0205 00:01:29.062118 139946397853440 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.1695729494094849, loss=3.498598098754883
I0205 00:02:15.493695 139946414638848 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.0688194036483765, loss=3.734327793121338
I0205 00:03:01.844351 139946397853440 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.1560636758804321, loss=3.6028671264648438
I0205 00:03:48.171683 139946414638848 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.1068079471588135, loss=3.5793447494506836
I0205 00:04:34.688304 139946397853440 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.1407544612884521, loss=3.4077086448669434
I0205 00:05:21.115031 139946414638848 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.114957332611084, loss=3.5686235427856445
I0205 00:05:24.260341 140107197974336 spec.py:321] Evaluating on the training split.
I0205 00:05:35.037692 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 00:06:03.248835 140107197974336 spec.py:349] Evaluating on the test split.
I0205 00:06:04.847292 140107197974336 submission_runner.py:408] Time since start: 33194.59s, 	Step: 66008, 	{'train/accuracy': 0.7390820384025574, 'train/loss': 1.268944501876831, 'validation/accuracy': 0.6768999695777893, 'validation/loss': 1.5356301069259644, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.1502127647399902, 'test/num_examples': 10000, 'score': 30286.265585184097, 'total_duration': 33194.593849658966, 'accumulated_submission_time': 30286.265585184097, 'accumulated_eval_time': 2902.1909971237183, 'accumulated_logging_time': 2.317537307739258}
I0205 00:06:04.874790 139946397853440 logging_writer.py:48] [66008] accumulated_eval_time=2902.190997, accumulated_logging_time=2.317537, accumulated_submission_time=30286.265585, global_step=66008, preemption_count=0, score=30286.265585, test/accuracy=0.553900, test/loss=2.150213, test/num_examples=10000, total_duration=33194.593850, train/accuracy=0.739082, train/loss=1.268945, validation/accuracy=0.676900, validation/loss=1.535630, validation/num_examples=50000
I0205 00:06:42.919498 139946414638848 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.1290056705474854, loss=3.4908242225646973
I0205 00:07:29.029635 139946397853440 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.052786111831665, loss=4.240365982055664
I0205 00:08:15.645531 139946414638848 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.1351977586746216, loss=3.5491373538970947
I0205 00:09:01.756898 139946397853440 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0309175252914429, loss=4.639071941375732
I0205 00:09:48.054556 139946414638848 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.1356126070022583, loss=5.134891986846924
I0205 00:10:34.555190 139946397853440 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.232336401939392, loss=3.457864284515381
I0205 00:11:20.770090 139946414638848 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.3719760179519653, loss=5.146758079528809
I0205 00:12:07.225574 139946397853440 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.1416566371917725, loss=3.4263930320739746
I0205 00:12:53.126714 139946414638848 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.0699323415756226, loss=3.723872423171997
I0205 00:13:04.950384 140107197974336 spec.py:321] Evaluating on the training split.
I0205 00:13:15.637702 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 00:13:44.816268 140107197974336 spec.py:349] Evaluating on the test split.
I0205 00:13:46.433850 140107197974336 submission_runner.py:408] Time since start: 33656.18s, 	Step: 66927, 	{'train/accuracy': 0.7473242282867432, 'train/loss': 1.2253973484039307, 'validation/accuracy': 0.6799399852752686, 'validation/loss': 1.5155017375946045, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.161947011947632, 'test/num_examples': 10000, 'score': 30706.27566933632, 'total_duration': 33656.18039536476, 'accumulated_submission_time': 30706.27566933632, 'accumulated_eval_time': 2943.6744248867035, 'accumulated_logging_time': 2.357588052749634}
I0205 00:13:46.465035 139946397853440 logging_writer.py:48] [66927] accumulated_eval_time=2943.674425, accumulated_logging_time=2.357588, accumulated_submission_time=30706.275669, global_step=66927, preemption_count=0, score=30706.275669, test/accuracy=0.553500, test/loss=2.161947, test/num_examples=10000, total_duration=33656.180395, train/accuracy=0.747324, train/loss=1.225397, validation/accuracy=0.679940, validation/loss=1.515502, validation/num_examples=50000
I0205 00:14:15.909801 139946414638848 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.030439019203186, loss=3.7147128582000732
I0205 00:15:01.892044 139946397853440 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.0752463340759277, loss=4.558998107910156
I0205 00:15:48.584104 139946414638848 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.0907646417617798, loss=5.1633429527282715
I0205 00:16:35.135823 139946397853440 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1752512454986572, loss=3.442089557647705
I0205 00:17:21.463850 139946414638848 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.2319225072860718, loss=3.35280704498291
I0205 00:18:08.012793 139946397853440 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.14901864528656, loss=3.401522636413574
I0205 00:18:54.477282 139946414638848 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.0775808095932007, loss=4.280699729919434
I0205 00:19:40.889703 139946397853440 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.077492117881775, loss=4.125125408172607
I0205 00:20:27.443792 139946414638848 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.0677608251571655, loss=4.663115501403809
I0205 00:20:46.880658 140107197974336 spec.py:321] Evaluating on the training split.
I0205 00:20:57.536684 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 00:21:27.247797 140107197974336 spec.py:349] Evaluating on the test split.
I0205 00:21:28.851800 140107197974336 submission_runner.py:408] Time since start: 34118.60s, 	Step: 67844, 	{'train/accuracy': 0.738964855670929, 'train/loss': 1.255308747291565, 'validation/accuracy': 0.6803999543190002, 'validation/loss': 1.512239933013916, 'validation/num_examples': 50000, 'test/accuracy': 0.5626000165939331, 'test/loss': 2.1146597862243652, 'test/num_examples': 10000, 'score': 31126.626373529434, 'total_duration': 34118.59836268425, 'accumulated_submission_time': 31126.626373529434, 'accumulated_eval_time': 2985.6455442905426, 'accumulated_logging_time': 2.4006874561309814}
I0205 00:21:28.879790 139946397853440 logging_writer.py:48] [67844] accumulated_eval_time=2985.645544, accumulated_logging_time=2.400687, accumulated_submission_time=31126.626374, global_step=67844, preemption_count=0, score=31126.626374, test/accuracy=0.562600, test/loss=2.114660, test/num_examples=10000, total_duration=34118.598363, train/accuracy=0.738965, train/loss=1.255309, validation/accuracy=0.680400, validation/loss=1.512240, validation/num_examples=50000
I0205 00:21:51.274470 139946414638848 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.221634864807129, loss=3.4654247760772705
I0205 00:22:36.738166 139946397853440 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1742092370986938, loss=3.9074740409851074
I0205 00:23:23.269028 139946414638848 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.0862640142440796, loss=3.3577544689178467
I0205 00:24:09.855024 139946397853440 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.9750105738639832, loss=4.049611568450928
I0205 00:24:56.413051 139946414638848 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.228559136390686, loss=3.417604923248291
I0205 00:25:42.747082 139946397853440 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.1936663389205933, loss=3.4930598735809326
I0205 00:26:28.976689 139946414638848 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.1607917547225952, loss=3.634112596511841
I0205 00:27:15.379768 139946397853440 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.0600857734680176, loss=3.780792474746704
I0205 00:28:01.526519 139946414638848 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.1141376495361328, loss=4.74375057220459
I0205 00:28:28.992020 140107197974336 spec.py:321] Evaluating on the training split.
I0205 00:28:39.518880 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 00:29:07.411179 140107197974336 spec.py:349] Evaluating on the test split.
I0205 00:29:09.009851 140107197974336 submission_runner.py:408] Time since start: 34578.76s, 	Step: 68761, 	{'train/accuracy': 0.7493945360183716, 'train/loss': 1.2292940616607666, 'validation/accuracy': 0.6869199872016907, 'validation/loss': 1.5031391382217407, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.1279420852661133, 'test/num_examples': 10000, 'score': 31546.674610376358, 'total_duration': 34578.75641846657, 'accumulated_submission_time': 31546.674610376358, 'accumulated_eval_time': 3025.6633553504944, 'accumulated_logging_time': 2.43937611579895}
I0205 00:29:09.035326 139946397853440 logging_writer.py:48] [68761] accumulated_eval_time=3025.663355, accumulated_logging_time=2.439376, accumulated_submission_time=31546.674610, global_step=68761, preemption_count=0, score=31546.674610, test/accuracy=0.557400, test/loss=2.127942, test/num_examples=10000, total_duration=34578.756418, train/accuracy=0.749395, train/loss=1.229294, validation/accuracy=0.686920, validation/loss=1.503139, validation/num_examples=50000
I0205 00:29:24.782054 139946414638848 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.1135408878326416, loss=3.6747539043426514
I0205 00:30:08.469016 139946397853440 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.0659818649291992, loss=5.082845211029053
I0205 00:30:54.497213 139946414638848 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1851032972335815, loss=3.4608426094055176
I0205 00:31:41.222784 139946397853440 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.0733767747879028, loss=4.145840167999268
I0205 00:32:27.707820 139946414638848 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.2532756328582764, loss=3.389604091644287
I0205 00:33:14.235919 139946397853440 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.0603426694869995, loss=3.4342164993286133
I0205 00:34:00.609714 139946414638848 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.1531492471694946, loss=3.4700942039489746
I0205 00:34:47.244359 139946397853440 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.1522101163864136, loss=3.4574739933013916
I0205 00:35:33.855340 139946414638848 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1586809158325195, loss=5.087449073791504
I0205 00:36:09.337131 140107197974336 spec.py:321] Evaluating on the training split.
I0205 00:36:20.022641 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 00:36:49.200513 140107197974336 spec.py:349] Evaluating on the test split.
I0205 00:36:50.794336 140107197974336 submission_runner.py:408] Time since start: 35040.54s, 	Step: 69678, 	{'train/accuracy': 0.7514257431030273, 'train/loss': 1.3177834749221802, 'validation/accuracy': 0.6843000054359436, 'validation/loss': 1.611196517944336, 'validation/num_examples': 50000, 'test/accuracy': 0.5576000213623047, 'test/loss': 2.2235536575317383, 'test/num_examples': 10000, 'score': 31966.91418480873, 'total_duration': 35040.54090499878, 'accumulated_submission_time': 31966.91418480873, 'accumulated_eval_time': 3067.120540380478, 'accumulated_logging_time': 2.4745750427246094}
I0205 00:36:50.828917 139946397853440 logging_writer.py:48] [69678] accumulated_eval_time=3067.120540, accumulated_logging_time=2.474575, accumulated_submission_time=31966.914185, global_step=69678, preemption_count=0, score=31966.914185, test/accuracy=0.557600, test/loss=2.223554, test/num_examples=10000, total_duration=35040.540905, train/accuracy=0.751426, train/loss=1.317783, validation/accuracy=0.684300, validation/loss=1.611197, validation/num_examples=50000
I0205 00:36:59.871821 139946414638848 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.1143670082092285, loss=3.41654372215271
I0205 00:37:42.875954 139946397853440 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.1505181789398193, loss=3.3285202980041504
I0205 00:38:29.320450 139946414638848 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.129623532295227, loss=3.4851512908935547
I0205 00:39:16.005882 139946397853440 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1271954774856567, loss=3.369612693786621
I0205 00:40:02.271842 139946414638848 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.1109691858291626, loss=3.6086158752441406
I0205 00:40:48.641276 139946397853440 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.0665024518966675, loss=3.8240489959716797
I0205 00:41:34.982090 139946414638848 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1059088706970215, loss=4.8214111328125
I0205 00:42:21.434837 139946397853440 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.0992481708526611, loss=3.455193281173706
I0205 00:43:07.886559 139946414638848 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1300058364868164, loss=5.1258368492126465
I0205 00:43:51.087903 140107197974336 spec.py:321] Evaluating on the training split.
I0205 00:44:02.028957 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 00:44:29.721043 140107197974336 spec.py:349] Evaluating on the test split.
I0205 00:44:31.319435 140107197974336 submission_runner.py:408] Time since start: 35501.07s, 	Step: 70595, 	{'train/accuracy': 0.7463085651397705, 'train/loss': 1.2566375732421875, 'validation/accuracy': 0.6869800090789795, 'validation/loss': 1.5039660930633545, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.119065761566162, 'test/num_examples': 10000, 'score': 32387.108137845993, 'total_duration': 35501.06599497795, 'accumulated_submission_time': 32387.108137845993, 'accumulated_eval_time': 3107.352053165436, 'accumulated_logging_time': 2.5219199657440186}
I0205 00:44:31.350666 139946397853440 logging_writer.py:48] [70595] accumulated_eval_time=3107.352053, accumulated_logging_time=2.521920, accumulated_submission_time=32387.108138, global_step=70595, preemption_count=0, score=32387.108138, test/accuracy=0.558600, test/loss=2.119066, test/num_examples=10000, total_duration=35501.065995, train/accuracy=0.746309, train/loss=1.256638, validation/accuracy=0.686980, validation/loss=1.503966, validation/num_examples=50000
I0205 00:44:33.714978 139946414638848 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.0327093601226807, loss=3.668246030807495
I0205 00:45:15.350405 139946397853440 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.0628114938735962, loss=3.9353256225585938
I0205 00:46:01.761689 139946414638848 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.1917465925216675, loss=3.430790662765503
I0205 00:46:48.116287 139946397853440 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.1546763181686401, loss=3.496792793273926
I0205 00:47:34.549325 139946414638848 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.0698007345199585, loss=3.95449161529541
I0205 00:48:21.100274 139946397853440 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.198836326599121, loss=4.747984886169434
I0205 00:49:07.534298 139946414638848 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.160448431968689, loss=3.3794636726379395
I0205 00:49:53.970384 139946397853440 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.1739366054534912, loss=3.462095022201538
I0205 00:50:40.362269 139946414638848 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.2462586164474487, loss=3.531113386154175
I0205 00:51:26.949410 139946397853440 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2463879585266113, loss=3.4704971313476562
I0205 00:51:31.777398 140107197974336 spec.py:321] Evaluating on the training split.
I0205 00:51:42.468460 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 00:52:12.082562 140107197974336 spec.py:349] Evaluating on the test split.
I0205 00:52:13.680817 140107197974336 submission_runner.py:408] Time since start: 35963.43s, 	Step: 71512, 	{'train/accuracy': 0.7489648461341858, 'train/loss': 1.1991535425186157, 'validation/accuracy': 0.6866599917411804, 'validation/loss': 1.4654242992401123, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 2.0869626998901367, 'test/num_examples': 10000, 'score': 32807.47200012207, 'total_duration': 35963.427382707596, 'accumulated_submission_time': 32807.47200012207, 'accumulated_eval_time': 3149.2554540634155, 'accumulated_logging_time': 2.5634772777557373}
I0205 00:52:13.707444 139946414638848 logging_writer.py:48] [71512] accumulated_eval_time=3149.255454, accumulated_logging_time=2.563477, accumulated_submission_time=32807.472000, global_step=71512, preemption_count=0, score=32807.472000, test/accuracy=0.561300, test/loss=2.086963, test/num_examples=10000, total_duration=35963.427383, train/accuracy=0.748965, train/loss=1.199154, validation/accuracy=0.686660, validation/loss=1.465424, validation/num_examples=50000
I0205 00:52:50.150673 139946397853440 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.2653100490570068, loss=5.160118103027344
I0205 00:53:36.347303 139946414638848 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.0950063467025757, loss=3.581313371658325
I0205 00:54:23.081795 139946397853440 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.2582902908325195, loss=3.3568480014801025
I0205 00:55:09.748727 139946414638848 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.1781502962112427, loss=3.548450231552124
I0205 00:55:56.247100 139946397853440 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.1850755214691162, loss=3.393672227859497
I0205 00:56:42.586806 139946414638848 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.2567870616912842, loss=3.446960687637329
I0205 00:57:29.111502 139946397853440 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.29079270362854, loss=3.3883116245269775
I0205 00:58:15.618315 139946414638848 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.1433779001235962, loss=3.846796989440918
I0205 00:59:02.154980 139946397853440 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.1352083683013916, loss=4.782505989074707
I0205 00:59:13.878820 140107197974336 spec.py:321] Evaluating on the training split.
I0205 00:59:24.434845 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 00:59:52.157126 140107197974336 spec.py:349] Evaluating on the test split.
I0205 00:59:53.766286 140107197974336 submission_runner.py:408] Time since start: 36423.51s, 	Step: 72427, 	{'train/accuracy': 0.7582226395606995, 'train/loss': 1.187957525253296, 'validation/accuracy': 0.6908599734306335, 'validation/loss': 1.4750837087631226, 'validation/num_examples': 50000, 'test/accuracy': 0.5659000277519226, 'test/loss': 2.090136766433716, 'test/num_examples': 10000, 'score': 33227.58208632469, 'total_duration': 36423.512846946716, 'accumulated_submission_time': 33227.58208632469, 'accumulated_eval_time': 3189.1428916454315, 'accumulated_logging_time': 2.59970760345459}
I0205 00:59:53.793576 139946414638848 logging_writer.py:48] [72427] accumulated_eval_time=3189.142892, accumulated_logging_time=2.599708, accumulated_submission_time=33227.582086, global_step=72427, preemption_count=0, score=33227.582086, test/accuracy=0.565900, test/loss=2.090137, test/num_examples=10000, total_duration=36423.512847, train/accuracy=0.758223, train/loss=1.187958, validation/accuracy=0.690860, validation/loss=1.475084, validation/num_examples=50000
I0205 01:00:23.117493 139946397853440 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.164589285850525, loss=3.3660309314727783
I0205 01:01:09.496219 139946414638848 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.1599180698394775, loss=3.4976022243499756
I0205 01:01:55.863209 139946397853440 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.3161085844039917, loss=3.3135414123535156
I0205 01:02:42.624594 139946414638848 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.2046775817871094, loss=3.7554125785827637
I0205 01:03:28.903999 139946397853440 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.201155424118042, loss=3.236210823059082
I0205 01:04:15.047431 139946414638848 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.114381194114685, loss=3.540562868118286
I0205 01:05:01.511669 139946397853440 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.0423243045806885, loss=4.062601089477539
I0205 01:05:47.706707 139946414638848 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.1307963132858276, loss=4.653871536254883
I0205 01:06:33.982908 139946397853440 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.2012715339660645, loss=5.0564374923706055
I0205 01:06:54.032056 140107197974336 spec.py:321] Evaluating on the training split.
I0205 01:07:04.967206 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 01:07:36.501597 140107197974336 spec.py:349] Evaluating on the test split.
I0205 01:07:38.091903 140107197974336 submission_runner.py:408] Time since start: 36887.84s, 	Step: 73345, 	{'train/accuracy': 0.7671679258346558, 'train/loss': 1.1537946462631226, 'validation/accuracy': 0.6909599900245667, 'validation/loss': 1.4814988374710083, 'validation/num_examples': 50000, 'test/accuracy': 0.5666000247001648, 'test/loss': 2.1033482551574707, 'test/num_examples': 10000, 'score': 33647.756412267685, 'total_duration': 36887.83846831322, 'accumulated_submission_time': 33647.756412267685, 'accumulated_eval_time': 3233.202719926834, 'accumulated_logging_time': 2.637953042984009}
I0205 01:07:38.120460 139946414638848 logging_writer.py:48] [73345] accumulated_eval_time=3233.202720, accumulated_logging_time=2.637953, accumulated_submission_time=33647.756412, global_step=73345, preemption_count=0, score=33647.756412, test/accuracy=0.566600, test/loss=2.103348, test/num_examples=10000, total_duration=36887.838468, train/accuracy=0.767168, train/loss=1.153795, validation/accuracy=0.690960, validation/loss=1.481499, validation/num_examples=50000
I0205 01:08:00.129235 139946397853440 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.1461607217788696, loss=3.3517301082611084
I0205 01:08:45.294079 139946414638848 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.3018238544464111, loss=3.326549768447876
I0205 01:09:31.814026 139946397853440 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.2919007539749146, loss=3.5870578289031982
I0205 01:10:18.118298 139946414638848 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.0941669940948486, loss=3.5823073387145996
I0205 01:11:04.784095 139946397853440 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.1901599168777466, loss=3.257178544998169
I0205 01:11:51.142914 139946414638848 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.305108904838562, loss=3.3080646991729736
I0205 01:12:37.549580 139946397853440 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.1342023611068726, loss=3.395469903945923
I0205 01:13:24.009929 139946414638848 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.187926173210144, loss=3.5442745685577393
I0205 01:14:10.403074 139946397853440 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.1321570873260498, loss=4.192453861236572
I0205 01:14:38.445755 140107197974336 spec.py:321] Evaluating on the training split.
I0205 01:14:49.324304 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 01:15:18.925455 140107197974336 spec.py:349] Evaluating on the test split.
I0205 01:15:20.519210 140107197974336 submission_runner.py:408] Time since start: 37350.27s, 	Step: 74262, 	{'train/accuracy': 0.750195324420929, 'train/loss': 1.2365492582321167, 'validation/accuracy': 0.6885799765586853, 'validation/loss': 1.5010799169540405, 'validation/num_examples': 50000, 'test/accuracy': 0.5663000345230103, 'test/loss': 2.115422248840332, 'test/num_examples': 10000, 'score': 34068.01615142822, 'total_duration': 37350.265760183334, 'accumulated_submission_time': 34068.01615142822, 'accumulated_eval_time': 3275.276128768921, 'accumulated_logging_time': 2.678273916244507}
I0205 01:15:20.550216 139946414638848 logging_writer.py:48] [74262] accumulated_eval_time=3275.276129, accumulated_logging_time=2.678274, accumulated_submission_time=34068.016151, global_step=74262, preemption_count=0, score=34068.016151, test/accuracy=0.566300, test/loss=2.115422, test/num_examples=10000, total_duration=37350.265760, train/accuracy=0.750195, train/loss=1.236549, validation/accuracy=0.688580, validation/loss=1.501080, validation/num_examples=50000
I0205 01:15:35.862245 139946397853440 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.1833953857421875, loss=3.7225265502929688
I0205 01:16:19.795711 139946414638848 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.0849636793136597, loss=3.6100680828094482
I0205 01:17:06.422836 139946397853440 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.1648284196853638, loss=3.4616928100585938
I0205 01:17:52.815688 139946414638848 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.130112648010254, loss=3.4835762977600098
I0205 01:18:39.328064 139946397853440 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.3237872123718262, loss=3.3912510871887207
I0205 01:19:25.606164 139946414638848 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.1339550018310547, loss=3.9823408126831055
I0205 01:20:12.177024 139946397853440 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.2947242259979248, loss=3.40159273147583
I0205 01:20:58.467392 139946414638848 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.2894247770309448, loss=3.3644485473632812
I0205 01:21:45.135699 139946397853440 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.354149341583252, loss=3.718985080718994
I0205 01:22:20.678704 140107197974336 spec.py:321] Evaluating on the training split.
I0205 01:22:31.474688 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 01:23:03.367885 140107197974336 spec.py:349] Evaluating on the test split.
I0205 01:23:04.968855 140107197974336 submission_runner.py:408] Time since start: 37814.72s, 	Step: 75178, 	{'train/accuracy': 0.7569140195846558, 'train/loss': 1.1788876056671143, 'validation/accuracy': 0.6946199536323547, 'validation/loss': 1.4552501440048218, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.0848517417907715, 'test/num_examples': 10000, 'score': 34488.080597639084, 'total_duration': 37814.71540975571, 'accumulated_submission_time': 34488.080597639084, 'accumulated_eval_time': 3319.566258430481, 'accumulated_logging_time': 2.720487594604492}
I0205 01:23:04.997245 139946414638848 logging_writer.py:48] [75178] accumulated_eval_time=3319.566258, accumulated_logging_time=2.720488, accumulated_submission_time=34488.080598, global_step=75178, preemption_count=0, score=34488.080598, test/accuracy=0.564000, test/loss=2.084852, test/num_examples=10000, total_duration=37814.715410, train/accuracy=0.756914, train/loss=1.178888, validation/accuracy=0.694620, validation/loss=1.455250, validation/num_examples=50000
I0205 01:23:14.063598 139946397853440 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.2463253736495972, loss=3.5278546810150146
I0205 01:23:56.682083 139946414638848 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.0661662817001343, loss=4.01947021484375
I0205 01:24:43.207103 139946397853440 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.244920253753662, loss=4.958911418914795
I0205 01:25:29.652750 139946414638848 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.0912067890167236, loss=3.635723829269409
I0205 01:26:15.975060 139946397853440 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.2218858003616333, loss=4.574376583099365
I0205 01:27:02.639194 139946414638848 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.1701937913894653, loss=3.353264093399048
I0205 01:27:48.767873 139946397853440 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.2509112358093262, loss=3.2990074157714844
I0205 01:28:35.376621 139946414638848 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.1206117868423462, loss=4.143206596374512
I0205 01:29:22.275172 139946397853440 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.1708894968032837, loss=3.392409086227417
I0205 01:30:05.310157 140107197974336 spec.py:321] Evaluating on the training split.
I0205 01:30:16.114740 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 01:30:44.456461 140107197974336 spec.py:349] Evaluating on the test split.
I0205 01:30:46.058956 140107197974336 submission_runner.py:408] Time since start: 38275.81s, 	Step: 76094, 	{'train/accuracy': 0.7713086009025574, 'train/loss': 1.1538770198822021, 'validation/accuracy': 0.6943599581718445, 'validation/loss': 1.489396095275879, 'validation/num_examples': 50000, 'test/accuracy': 0.5666000247001648, 'test/loss': 2.119791030883789, 'test/num_examples': 10000, 'score': 34908.33102440834, 'total_duration': 38275.80552268028, 'accumulated_submission_time': 34908.33102440834, 'accumulated_eval_time': 3360.3150522708893, 'accumulated_logging_time': 2.7591450214385986}
I0205 01:30:46.095337 139946414638848 logging_writer.py:48] [76094] accumulated_eval_time=3360.315052, accumulated_logging_time=2.759145, accumulated_submission_time=34908.331024, global_step=76094, preemption_count=0, score=34908.331024, test/accuracy=0.566600, test/loss=2.119791, test/num_examples=10000, total_duration=38275.805523, train/accuracy=0.771309, train/loss=1.153877, validation/accuracy=0.694360, validation/loss=1.489396, validation/num_examples=50000
I0205 01:30:48.846335 139946397853440 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.2450536489486694, loss=3.3137383460998535
I0205 01:31:30.909079 139946414638848 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.2913283109664917, loss=3.3460381031036377
I0205 01:32:17.242511 139946397853440 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.1193718910217285, loss=4.624139785766602
I0205 01:33:03.813125 139946414638848 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.2203285694122314, loss=3.3864169120788574
I0205 01:33:50.317903 139946397853440 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.2483330965042114, loss=3.35135555267334
I0205 01:34:36.938194 139946414638848 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.0663810968399048, loss=3.964193344116211
I0205 01:35:23.491839 139946397853440 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.2161362171173096, loss=3.3280563354492188
I0205 01:36:10.036578 139946414638848 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.1576426029205322, loss=3.738966941833496
I0205 01:36:56.430480 139946397853440 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.1687451601028442, loss=4.4807891845703125
I0205 01:37:43.088073 139946414638848 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.1960498094558716, loss=3.4155399799346924
I0205 01:37:46.466250 140107197974336 spec.py:321] Evaluating on the training split.
I0205 01:37:57.442023 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 01:38:29.951211 140107197974336 spec.py:349] Evaluating on the test split.
I0205 01:38:31.552270 140107197974336 submission_runner.py:408] Time since start: 38741.30s, 	Step: 77009, 	{'train/accuracy': 0.7552343606948853, 'train/loss': 1.1852645874023438, 'validation/accuracy': 0.6930199861526489, 'validation/loss': 1.4465210437774658, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.0639443397521973, 'test/num_examples': 10000, 'score': 35328.639944553375, 'total_duration': 38741.29879665375, 'accumulated_submission_time': 35328.639944553375, 'accumulated_eval_time': 3405.4010264873505, 'accumulated_logging_time': 2.8056118488311768}
I0205 01:38:31.584468 139946397853440 logging_writer.py:48] [77009] accumulated_eval_time=3405.401026, accumulated_logging_time=2.805612, accumulated_submission_time=35328.639945, global_step=77009, preemption_count=0, score=35328.639945, test/accuracy=0.569800, test/loss=2.063944, test/num_examples=10000, total_duration=38741.298797, train/accuracy=0.755234, train/loss=1.185265, validation/accuracy=0.693020, validation/loss=1.446521, validation/num_examples=50000
I0205 01:39:09.560492 139946414638848 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.1256455183029175, loss=3.691744327545166
I0205 01:39:55.658118 139946397853440 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.2607327699661255, loss=3.3600122928619385
I0205 01:40:42.465988 139946414638848 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.2829383611679077, loss=3.582521677017212
I0205 01:41:28.852172 139946397853440 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.3012776374816895, loss=5.062294006347656
I0205 01:42:15.745905 139946414638848 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.2226642370224, loss=3.4197614192962646
I0205 01:43:02.363642 139946397853440 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.190464735031128, loss=3.575516939163208
I0205 01:43:48.618221 139946414638848 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.396470308303833, loss=3.513262987136841
I0205 01:44:35.235578 139946397853440 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.1858147382736206, loss=4.503629684448242
I0205 01:45:21.971362 139946414638848 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.322874665260315, loss=4.989533424377441
I0205 01:45:31.838268 140107197974336 spec.py:321] Evaluating on the training split.
I0205 01:45:42.774142 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 01:46:13.438277 140107197974336 spec.py:349] Evaluating on the test split.
I0205 01:46:15.026368 140107197974336 submission_runner.py:408] Time since start: 39204.77s, 	Step: 77923, 	{'train/accuracy': 0.7587695121765137, 'train/loss': 1.2047119140625, 'validation/accuracy': 0.6947199702262878, 'validation/loss': 1.4846562147140503, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 2.111642599105835, 'test/num_examples': 10000, 'score': 35748.82993221283, 'total_duration': 39204.77293586731, 'accumulated_submission_time': 35748.82993221283, 'accumulated_eval_time': 3448.5891098976135, 'accumulated_logging_time': 2.8494107723236084}
I0205 01:46:15.054118 139946397853440 logging_writer.py:48] [77923] accumulated_eval_time=3448.589110, accumulated_logging_time=2.849411, accumulated_submission_time=35748.829932, global_step=77923, preemption_count=0, score=35748.829932, test/accuracy=0.568400, test/loss=2.111643, test/num_examples=10000, total_duration=39204.772936, train/accuracy=0.758770, train/loss=1.204712, validation/accuracy=0.694720, validation/loss=1.484656, validation/num_examples=50000
I0205 01:46:46.391789 139946414638848 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.3440009355545044, loss=3.225904941558838
I0205 01:47:32.493466 139946397853440 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.2119938135147095, loss=4.139968395233154
I0205 01:48:18.921644 139946414638848 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.2241932153701782, loss=3.334238052368164
I0205 01:49:05.439028 139946397853440 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.150757074356079, loss=3.277181386947632
I0205 01:49:51.701270 139946414638848 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.1483343839645386, loss=3.3279335498809814
I0205 01:50:38.095393 139946397853440 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.1764180660247803, loss=3.3284194469451904
I0205 01:51:24.654573 139946414638848 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.2824435234069824, loss=3.3309261798858643
I0205 01:52:11.106409 139946397853440 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.250770092010498, loss=3.405304431915283
I0205 01:52:57.662201 139946414638848 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.233870267868042, loss=3.31557035446167
I0205 01:53:15.247343 140107197974336 spec.py:321] Evaluating on the training split.
I0205 01:53:25.917076 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 01:53:54.488165 140107197974336 spec.py:349] Evaluating on the test split.
I0205 01:53:56.105956 140107197974336 submission_runner.py:408] Time since start: 39665.85s, 	Step: 78839, 	{'train/accuracy': 0.7691015601158142, 'train/loss': 1.1444746255874634, 'validation/accuracy': 0.6953999996185303, 'validation/loss': 1.4568653106689453, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.074385166168213, 'test/num_examples': 10000, 'score': 36168.960582733154, 'total_duration': 39665.85251927376, 'accumulated_submission_time': 36168.960582733154, 'accumulated_eval_time': 3489.4477009773254, 'accumulated_logging_time': 2.8870458602905273}
I0205 01:53:56.133998 139946397853440 logging_writer.py:48] [78839] accumulated_eval_time=3489.447701, accumulated_logging_time=2.887046, accumulated_submission_time=36168.960583, global_step=78839, preemption_count=0, score=36168.960583, test/accuracy=0.569800, test/loss=2.074385, test/num_examples=10000, total_duration=39665.852519, train/accuracy=0.769102, train/loss=1.144475, validation/accuracy=0.695400, validation/loss=1.456865, validation/num_examples=50000
I0205 01:54:20.509013 139946414638848 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.3872665166854858, loss=5.046927452087402
I0205 01:55:05.821065 139946397853440 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.3144954442977905, loss=5.104269504547119
I0205 01:55:52.177276 139946414638848 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.2598371505737305, loss=3.3887321949005127
I0205 01:56:38.616023 139946397853440 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.2667255401611328, loss=3.515795946121216
I0205 01:57:24.908542 139946414638848 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.1053173542022705, loss=4.664041519165039
I0205 01:58:11.486836 139946397853440 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.1746647357940674, loss=3.3450803756713867
I0205 01:58:57.765752 139946414638848 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.3083666563034058, loss=5.098548412322998
I0205 01:59:44.092149 139946397853440 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.2324329614639282, loss=4.851230621337891
I0205 02:00:30.582758 139946414638848 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.3109862804412842, loss=4.808946132659912
I0205 02:00:56.212380 140107197974336 spec.py:321] Evaluating on the training split.
I0205 02:01:07.048199 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 02:01:38.215848 140107197974336 spec.py:349] Evaluating on the test split.
I0205 02:01:39.814220 140107197974336 submission_runner.py:408] Time since start: 40129.56s, 	Step: 79757, 	{'train/accuracy': 0.7608398199081421, 'train/loss': 1.156747817993164, 'validation/accuracy': 0.6993399858474731, 'validation/loss': 1.4243495464324951, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 2.030494213104248, 'test/num_examples': 10000, 'score': 36588.97576904297, 'total_duration': 40129.560782432556, 'accumulated_submission_time': 36588.97576904297, 'accumulated_eval_time': 3533.049519300461, 'accumulated_logging_time': 2.9246163368225098}
I0205 02:01:39.847128 139946397853440 logging_writer.py:48] [79757] accumulated_eval_time=3533.049519, accumulated_logging_time=2.924616, accumulated_submission_time=36588.975769, global_step=79757, preemption_count=0, score=36588.975769, test/accuracy=0.578100, test/loss=2.030494, test/num_examples=10000, total_duration=40129.560782, train/accuracy=0.760840, train/loss=1.156748, validation/accuracy=0.699340, validation/loss=1.424350, validation/num_examples=50000
I0205 02:01:57.147432 139946414638848 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.2077791690826416, loss=3.5278050899505615
I0205 02:02:41.620190 139946397853440 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.2050471305847168, loss=3.418365716934204
I0205 02:03:28.277288 139946414638848 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.1538914442062378, loss=3.3893682956695557
I0205 02:04:14.883611 139946397853440 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.1015387773513794, loss=4.115630626678467
I0205 02:05:01.118123 139946414638848 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.351815938949585, loss=3.344531297683716
I0205 02:05:47.407211 139946397853440 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.3248075246810913, loss=3.331387996673584
I0205 02:06:33.672960 139946414638848 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.171005129814148, loss=3.6517419815063477
I0205 02:07:20.004412 139946397853440 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.2210034132003784, loss=4.380684852600098
I0205 02:08:06.562997 139946414638848 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.2150962352752686, loss=3.329531192779541
I0205 02:08:40.237500 140107197974336 spec.py:321] Evaluating on the training split.
I0205 02:08:50.937119 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 02:09:22.468217 140107197974336 spec.py:349] Evaluating on the test split.
I0205 02:09:24.066761 140107197974336 submission_runner.py:408] Time since start: 40593.81s, 	Step: 80674, 	{'train/accuracy': 0.7592187523841858, 'train/loss': 1.2048238515853882, 'validation/accuracy': 0.6990999579429626, 'validation/loss': 1.4711554050445557, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 2.0904242992401123, 'test/num_examples': 10000, 'score': 37009.30336403847, 'total_duration': 40593.813328027725, 'accumulated_submission_time': 37009.30336403847, 'accumulated_eval_time': 3576.878761768341, 'accumulated_logging_time': 2.968376398086548}
I0205 02:09:24.097726 139946397853440 logging_writer.py:48] [80674] accumulated_eval_time=3576.878762, accumulated_logging_time=2.968376, accumulated_submission_time=37009.303364, global_step=80674, preemption_count=0, score=37009.303364, test/accuracy=0.570600, test/loss=2.090424, test/num_examples=10000, total_duration=40593.813328, train/accuracy=0.759219, train/loss=1.204824, validation/accuracy=0.699100, validation/loss=1.471155, validation/num_examples=50000
I0205 02:09:34.710303 139946414638848 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.2649108171463013, loss=3.3426387310028076
I0205 02:10:18.022302 139946397853440 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.193522334098816, loss=4.678801536560059
I0205 02:11:04.507172 139946414638848 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.2045550346374512, loss=3.3943185806274414
I0205 02:11:51.066262 139946397853440 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.1691596508026123, loss=3.6016671657562256
I0205 02:12:37.484639 139946414638848 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.2111704349517822, loss=3.6725308895111084
I0205 02:13:24.276576 139946397853440 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.20878005027771, loss=4.56008768081665
I0205 02:14:10.699305 139946414638848 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.4174203872680664, loss=3.3479673862457275
I0205 02:14:57.419170 139946397853440 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.3319276571273804, loss=3.319373846054077
I0205 02:15:44.059623 139946414638848 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.180436134338379, loss=3.7807717323303223
I0205 02:16:24.176890 140107197974336 spec.py:321] Evaluating on the training split.
I0205 02:16:35.829199 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 02:17:07.463815 140107197974336 spec.py:349] Evaluating on the test split.
I0205 02:17:09.068711 140107197974336 submission_runner.py:408] Time since start: 41058.82s, 	Step: 81588, 	{'train/accuracy': 0.7721484303474426, 'train/loss': 1.1226656436920166, 'validation/accuracy': 0.7008000016212463, 'validation/loss': 1.4333691596984863, 'validation/num_examples': 50000, 'test/accuracy': 0.574400007724762, 'test/loss': 2.0534827709198, 'test/num_examples': 10000, 'score': 37429.31781625748, 'total_duration': 41058.81526851654, 'accumulated_submission_time': 37429.31781625748, 'accumulated_eval_time': 3621.7705612182617, 'accumulated_logging_time': 3.010806083679199}
I0205 02:17:09.104107 139946397853440 logging_writer.py:48] [81588] accumulated_eval_time=3621.770561, accumulated_logging_time=3.010806, accumulated_submission_time=37429.317816, global_step=81588, preemption_count=0, score=37429.317816, test/accuracy=0.574400, test/loss=2.053483, test/num_examples=10000, total_duration=41058.815269, train/accuracy=0.772148, train/loss=1.122666, validation/accuracy=0.700800, validation/loss=1.433369, validation/num_examples=50000
I0205 02:17:14.222456 139946414638848 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.2890872955322266, loss=3.357339382171631
I0205 02:17:56.705953 139946397853440 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.2012155055999756, loss=3.954949378967285
I0205 02:18:43.077326 139946414638848 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.2871520519256592, loss=3.270034074783325
I0205 02:19:29.816921 139946397853440 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.228857159614563, loss=4.75478982925415
I0205 02:20:16.034066 139946414638848 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.243397831916809, loss=3.2594969272613525
I0205 02:21:02.354392 139946397853440 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.2462176084518433, loss=3.3986473083496094
I0205 02:21:48.518650 139946414638848 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.319116234779358, loss=3.4227123260498047
I0205 02:22:34.956739 139946397853440 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.1214052438735962, loss=4.085489273071289
I0205 02:23:21.515648 139946414638848 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.3047146797180176, loss=3.3849594593048096
I0205 02:24:07.912471 139946397853440 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.1644446849822998, loss=4.390857696533203
I0205 02:24:09.442134 140107197974336 spec.py:321] Evaluating on the training split.
I0205 02:24:20.194123 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 02:24:51.313718 140107197974336 spec.py:349] Evaluating on the test split.
I0205 02:24:52.908889 140107197974336 submission_runner.py:408] Time since start: 41522.66s, 	Step: 82505, 	{'train/accuracy': 0.7578710913658142, 'train/loss': 1.1451164484024048, 'validation/accuracy': 0.6981199979782104, 'validation/loss': 1.403847336769104, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 2.033210515975952, 'test/num_examples': 10000, 'score': 37849.59191274643, 'total_duration': 41522.65545606613, 'accumulated_submission_time': 37849.59191274643, 'accumulated_eval_time': 3665.23730635643, 'accumulated_logging_time': 3.0572826862335205}
I0205 02:24:52.937217 139946414638848 logging_writer.py:48] [82505] accumulated_eval_time=3665.237306, accumulated_logging_time=3.057283, accumulated_submission_time=37849.591913, global_step=82505, preemption_count=0, score=37849.591913, test/accuracy=0.573100, test/loss=2.033211, test/num_examples=10000, total_duration=41522.655456, train/accuracy=0.757871, train/loss=1.145116, validation/accuracy=0.698120, validation/loss=1.403847, validation/num_examples=50000
I0205 02:25:32.696163 139946397853440 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.264406442642212, loss=3.326321840286255
I0205 02:26:18.667627 139946414638848 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.2080217599868774, loss=3.740936040878296
I0205 02:27:05.069350 139946397853440 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.357566237449646, loss=5.003232479095459
I0205 02:27:51.641945 139946414638848 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.2307524681091309, loss=3.6399521827697754
I0205 02:28:37.881688 139946397853440 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.1764553785324097, loss=4.364293575286865
I0205 02:29:24.200561 139946414638848 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.2107212543487549, loss=3.3903000354766846
I0205 02:30:10.749570 139946397853440 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.2612241506576538, loss=3.5779170989990234
I0205 02:30:57.204381 139946414638848 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.2083176374435425, loss=3.2358078956604004
I0205 02:31:43.504599 139946397853440 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.1936918497085571, loss=3.4194860458374023
I0205 02:31:53.013667 140107197974336 spec.py:321] Evaluating on the training split.
I0205 02:32:03.854749 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 02:32:35.655801 140107197974336 spec.py:349] Evaluating on the test split.
I0205 02:32:37.260368 140107197974336 submission_runner.py:408] Time since start: 41987.01s, 	Step: 83422, 	{'train/accuracy': 0.7671093344688416, 'train/loss': 1.1266734600067139, 'validation/accuracy': 0.702459990978241, 'validation/loss': 1.4112627506256104, 'validation/num_examples': 50000, 'test/accuracy': 0.5782000422477722, 'test/loss': 2.0119259357452393, 'test/num_examples': 10000, 'score': 38269.605160713196, 'total_duration': 41987.00690460205, 'accumulated_submission_time': 38269.605160713196, 'accumulated_eval_time': 3709.483957052231, 'accumulated_logging_time': 3.096383810043335}
I0205 02:32:37.291841 139946414638848 logging_writer.py:48] [83422] accumulated_eval_time=3709.483957, accumulated_logging_time=3.096384, accumulated_submission_time=38269.605161, global_step=83422, preemption_count=0, score=38269.605161, test/accuracy=0.578200, test/loss=2.011926, test/num_examples=10000, total_duration=41987.006905, train/accuracy=0.767109, train/loss=1.126673, validation/accuracy=0.702460, validation/loss=1.411263, validation/num_examples=50000
I0205 02:33:09.310957 139946397853440 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.1533617973327637, loss=4.219571590423584
I0205 02:33:55.253056 139946414638848 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.270359992980957, loss=3.3047242164611816
I0205 02:34:42.176141 139946397853440 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.3091514110565186, loss=3.3824193477630615
I0205 02:35:28.759643 139946414638848 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.2728590965270996, loss=4.673802852630615
I0205 02:36:15.488297 139946397853440 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.1385066509246826, loss=4.065319538116455
I0205 02:37:01.976908 139946414638848 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.1416292190551758, loss=3.509387493133545
I0205 02:37:48.484082 139946397853440 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.417253017425537, loss=5.001893997192383
I0205 02:38:34.893021 139946414638848 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.2745410203933716, loss=3.317392587661743
I0205 02:39:21.205954 139946397853440 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.4112628698349, loss=5.0996599197387695
I0205 02:39:37.611309 140107197974336 spec.py:321] Evaluating on the training split.
I0205 02:39:48.659143 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 02:40:14.296423 140107197974336 spec.py:349] Evaluating on the test split.
I0205 02:40:15.906820 140107197974336 submission_runner.py:408] Time since start: 42445.65s, 	Step: 84337, 	{'train/accuracy': 0.7741601467132568, 'train/loss': 1.1334631443023682, 'validation/accuracy': 0.7036600112915039, 'validation/loss': 1.4320693016052246, 'validation/num_examples': 50000, 'test/accuracy': 0.5787000060081482, 'test/loss': 2.0397210121154785, 'test/num_examples': 10000, 'score': 38689.862147808075, 'total_duration': 42445.65338349342, 'accumulated_submission_time': 38689.862147808075, 'accumulated_eval_time': 3747.7794547080994, 'accumulated_logging_time': 3.1379082202911377}
I0205 02:40:15.936190 139946414638848 logging_writer.py:48] [84337] accumulated_eval_time=3747.779455, accumulated_logging_time=3.137908, accumulated_submission_time=38689.862148, global_step=84337, preemption_count=0, score=38689.862148, test/accuracy=0.578700, test/loss=2.039721, test/num_examples=10000, total_duration=42445.653383, train/accuracy=0.774160, train/loss=1.133463, validation/accuracy=0.703660, validation/loss=1.432069, validation/num_examples=50000
I0205 02:40:41.133723 139946397853440 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.1873465776443481, loss=3.8766705989837646
I0205 02:41:26.851383 139946414638848 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.1988636255264282, loss=4.599885940551758
I0205 02:42:13.432807 139946397853440 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.2277371883392334, loss=3.4676623344421387
I0205 02:42:59.911348 139946414638848 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.2371639013290405, loss=4.742018222808838
I0205 02:43:46.556156 139946397853440 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.3003731966018677, loss=3.3481266498565674
I0205 02:44:33.269675 139946414638848 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.2227725982666016, loss=3.4735729694366455
I0205 02:45:19.554954 139946397853440 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.284020185470581, loss=3.3648509979248047
I0205 02:46:06.170348 139946414638848 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.2504527568817139, loss=3.3315560817718506
I0205 02:46:52.341843 139946397853440 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.170516014099121, loss=3.832866907119751
I0205 02:47:16.257504 140107197974336 spec.py:321] Evaluating on the training split.
I0205 02:47:27.209812 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 02:47:54.093625 140107197974336 spec.py:349] Evaluating on the test split.
I0205 02:47:55.694346 140107197974336 submission_runner.py:408] Time since start: 42905.44s, 	Step: 85253, 	{'train/accuracy': 0.7835546731948853, 'train/loss': 1.083608627319336, 'validation/accuracy': 0.7017199993133545, 'validation/loss': 1.4271472692489624, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 2.0556511878967285, 'test/num_examples': 10000, 'score': 39110.120413541794, 'total_duration': 42905.44090247154, 'accumulated_submission_time': 39110.120413541794, 'accumulated_eval_time': 3787.2162635326385, 'accumulated_logging_time': 3.178086042404175}
I0205 02:47:55.724876 139946414638848 logging_writer.py:48] [85253] accumulated_eval_time=3787.216264, accumulated_logging_time=3.178086, accumulated_submission_time=39110.120414, global_step=85253, preemption_count=0, score=39110.120414, test/accuracy=0.577900, test/loss=2.055651, test/num_examples=10000, total_duration=42905.440902, train/accuracy=0.783555, train/loss=1.083609, validation/accuracy=0.701720, validation/loss=1.427147, validation/num_examples=50000
I0205 02:48:14.597018 139946397853440 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.2743386030197144, loss=4.5977396965026855
I0205 02:48:59.264495 139946414638848 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.175292730331421, loss=3.7456655502319336
I0205 02:49:45.738669 139946397853440 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.5501102209091187, loss=3.310394525527954
I0205 02:50:32.107170 139946414638848 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.2616788148880005, loss=3.1835083961486816
I0205 02:51:18.720018 139946397853440 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.1984916925430298, loss=3.3199403285980225
I0205 02:52:05.253868 139946414638848 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.197172999382019, loss=3.855856418609619
I0205 02:52:51.609540 139946397853440 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.3209388256072998, loss=4.311929225921631
I0205 02:53:38.169107 139946414638848 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.2808343172073364, loss=3.408785343170166
I0205 02:54:24.758285 139946397853440 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.2265385389328003, loss=3.285080909729004
I0205 02:54:55.761047 140107197974336 spec.py:321] Evaluating on the training split.
I0205 02:55:06.547346 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 02:55:35.877813 140107197974336 spec.py:349] Evaluating on the test split.
I0205 02:55:37.473296 140107197974336 submission_runner.py:408] Time since start: 43367.22s, 	Step: 86168, 	{'train/accuracy': 0.7661913633346558, 'train/loss': 1.1293996572494507, 'validation/accuracy': 0.7059599757194519, 'validation/loss': 1.391916036605835, 'validation/num_examples': 50000, 'test/accuracy': 0.5843999981880188, 'test/loss': 2.0091748237609863, 'test/num_examples': 10000, 'score': 39530.094547748566, 'total_duration': 43367.21985912323, 'accumulated_submission_time': 39530.094547748566, 'accumulated_eval_time': 3828.9285061359406, 'accumulated_logging_time': 3.2188732624053955}
I0205 02:55:37.507226 139946414638848 logging_writer.py:48] [86168] accumulated_eval_time=3828.928506, accumulated_logging_time=3.218873, accumulated_submission_time=39530.094548, global_step=86168, preemption_count=0, score=39530.094548, test/accuracy=0.584400, test/loss=2.009175, test/num_examples=10000, total_duration=43367.219859, train/accuracy=0.766191, train/loss=1.129400, validation/accuracy=0.705960, validation/loss=1.391916, validation/num_examples=50000
I0205 02:55:50.468861 139946397853440 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.2297378778457642, loss=3.861537218093872
I0205 02:56:34.039835 139946414638848 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.22511887550354, loss=4.688441276550293
I0205 02:57:20.292199 139946397853440 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.29110848903656, loss=3.3491857051849365
I0205 02:58:06.745676 139946414638848 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.1694577932357788, loss=3.8950352668762207
I0205 02:58:52.927091 139946397853440 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.2387564182281494, loss=3.260838031768799
I0205 02:59:39.308873 139946414638848 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.1166303157806396, loss=3.883960723876953
I0205 03:00:25.611000 139946397853440 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.2808538675308228, loss=3.3158178329467773
I0205 03:01:12.110197 139946414638848 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.5021504163742065, loss=3.418731451034546
I0205 03:01:58.393420 139946397853440 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.3114047050476074, loss=3.247990369796753
I0205 03:02:37.737670 140107197974336 spec.py:321] Evaluating on the training split.
I0205 03:02:48.124484 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 03:03:19.251178 140107197974336 spec.py:349] Evaluating on the test split.
I0205 03:03:20.845197 140107197974336 submission_runner.py:408] Time since start: 43830.59s, 	Step: 87086, 	{'train/accuracy': 0.7721288800239563, 'train/loss': 1.1214532852172852, 'validation/accuracy': 0.7034800052642822, 'validation/loss': 1.4084711074829102, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 2.038839817047119, 'test/num_examples': 10000, 'score': 39950.2617855072, 'total_duration': 43830.59176373482, 'accumulated_submission_time': 39950.2617855072, 'accumulated_eval_time': 3872.0360209941864, 'accumulated_logging_time': 3.2629506587982178}
I0205 03:03:20.877832 139946414638848 logging_writer.py:48] [87086] accumulated_eval_time=3872.036021, accumulated_logging_time=3.262951, accumulated_submission_time=39950.261786, global_step=87086, preemption_count=0, score=39950.261786, test/accuracy=0.580000, test/loss=2.038840, test/num_examples=10000, total_duration=43830.591764, train/accuracy=0.772129, train/loss=1.121453, validation/accuracy=0.703480, validation/loss=1.408471, validation/num_examples=50000
I0205 03:03:26.778813 139946397853440 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.2236135005950928, loss=3.270704746246338
I0205 03:04:09.184348 139946414638848 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.1904652118682861, loss=3.4063353538513184
I0205 03:04:55.550002 139946397853440 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.3379000425338745, loss=3.303821086883545
I0205 03:05:42.446512 139946414638848 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.18790602684021, loss=3.4216790199279785
I0205 03:06:28.589336 139946397853440 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.3330239057540894, loss=3.286799192428589
I0205 03:07:15.495347 139946414638848 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.267736792564392, loss=3.272183895111084
I0205 03:08:01.683662 139946397853440 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.3054434061050415, loss=3.4413058757781982
I0205 03:08:47.976567 139946414638848 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.255547285079956, loss=3.2905125617980957
I0205 03:09:34.224603 139946397853440 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.2462953329086304, loss=3.914548635482788
I0205 03:10:20.710787 139946414638848 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.1971396207809448, loss=3.493328332901001
I0205 03:10:20.901916 140107197974336 spec.py:321] Evaluating on the training split.
I0205 03:10:31.868403 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 03:11:02.879672 140107197974336 spec.py:349] Evaluating on the test split.
I0205 03:11:04.484415 140107197974336 submission_runner.py:408] Time since start: 44294.23s, 	Step: 88002, 	{'train/accuracy': 0.7907617092132568, 'train/loss': 1.0512465238571167, 'validation/accuracy': 0.708620011806488, 'validation/loss': 1.3993642330169678, 'validation/num_examples': 50000, 'test/accuracy': 0.5897000432014465, 'test/loss': 2.0038678646087646, 'test/num_examples': 10000, 'score': 40370.222638845444, 'total_duration': 44294.23095417023, 'accumulated_submission_time': 40370.222638845444, 'accumulated_eval_time': 3915.6184599399567, 'accumulated_logging_time': 3.306154489517212}
I0205 03:11:04.521026 139946397853440 logging_writer.py:48] [88002] accumulated_eval_time=3915.618460, accumulated_logging_time=3.306154, accumulated_submission_time=40370.222639, global_step=88002, preemption_count=0, score=40370.222639, test/accuracy=0.589700, test/loss=2.003868, test/num_examples=10000, total_duration=44294.230954, train/accuracy=0.790762, train/loss=1.051247, validation/accuracy=0.708620, validation/loss=1.399364, validation/num_examples=50000
I0205 03:11:45.675185 139946414638848 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.3461421728134155, loss=3.3404345512390137
I0205 03:12:32.132887 139946397853440 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.1560808420181274, loss=4.259830951690674
I0205 03:13:18.929481 139946414638848 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.1978209018707275, loss=4.022769927978516
I0205 03:14:05.235689 139946397853440 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.2342280149459839, loss=3.3439998626708984
I0205 03:14:52.181765 139946414638848 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.3206487894058228, loss=4.854894161224365
I0205 03:15:38.701114 139946397853440 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.2887924909591675, loss=3.2863144874572754
I0205 03:16:25.373167 139946414638848 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.4429693222045898, loss=3.3836510181427
I0205 03:17:11.905960 139946397853440 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.3656306266784668, loss=3.135510206222534
I0205 03:17:58.524466 139946414638848 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.2761926651000977, loss=3.4775567054748535
I0205 03:18:04.645462 140107197974336 spec.py:321] Evaluating on the training split.
I0205 03:18:15.198077 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 03:18:43.086584 140107197974336 spec.py:349] Evaluating on the test split.
I0205 03:18:44.684327 140107197974336 submission_runner.py:408] Time since start: 44754.43s, 	Step: 88915, 	{'train/accuracy': 0.7695898413658142, 'train/loss': 1.1286250352859497, 'validation/accuracy': 0.7078799605369568, 'validation/loss': 1.400683045387268, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.0153114795684814, 'test/num_examples': 10000, 'score': 40790.28323984146, 'total_duration': 44754.43089079857, 'accumulated_submission_time': 40790.28323984146, 'accumulated_eval_time': 3955.6573054790497, 'accumulated_logging_time': 3.3536322116851807}
I0205 03:18:44.714493 139946397853440 logging_writer.py:48] [88915] accumulated_eval_time=3955.657305, accumulated_logging_time=3.353632, accumulated_submission_time=40790.283240, global_step=88915, preemption_count=0, score=40790.283240, test/accuracy=0.583500, test/loss=2.015311, test/num_examples=10000, total_duration=44754.430891, train/accuracy=0.769590, train/loss=1.128625, validation/accuracy=0.707880, validation/loss=1.400683, validation/num_examples=50000
I0205 03:19:19.335378 139946414638848 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.316659688949585, loss=3.531080961227417
I0205 03:20:05.727172 139946397853440 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.2371041774749756, loss=3.618567943572998
I0205 03:20:52.278581 139946414638848 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.2203342914581299, loss=3.31628155708313
I0205 03:21:38.549404 139946397853440 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.4667738676071167, loss=3.2864203453063965
I0205 03:22:25.026539 139946414638848 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.2340270280838013, loss=3.5469167232513428
I0205 03:23:11.322410 139946397853440 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.4705736637115479, loss=3.2754898071289062
I0205 03:23:57.685754 139946414638848 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.44015634059906, loss=5.0462646484375
I0205 03:24:44.617710 139946397853440 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.3401700258255005, loss=3.3512723445892334
I0205 03:25:31.192470 139946414638848 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.270554780960083, loss=3.678516149520874
I0205 03:25:44.762749 140107197974336 spec.py:321] Evaluating on the training split.
I0205 03:25:55.467818 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 03:26:26.535399 140107197974336 spec.py:349] Evaluating on the test split.
I0205 03:26:28.133468 140107197974336 submission_runner.py:408] Time since start: 45217.88s, 	Step: 89831, 	{'train/accuracy': 0.7773827910423279, 'train/loss': 1.0906800031661987, 'validation/accuracy': 0.7084000110626221, 'validation/loss': 1.3841772079467773, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 1.9896676540374756, 'test/num_examples': 10000, 'score': 41210.26914906502, 'total_duration': 45217.8800368309, 'accumulated_submission_time': 41210.26914906502, 'accumulated_eval_time': 3999.0280084609985, 'accumulated_logging_time': 3.3931477069854736}
I0205 03:26:28.163532 139946397853440 logging_writer.py:48] [89831] accumulated_eval_time=3999.028008, accumulated_logging_time=3.393148, accumulated_submission_time=41210.269149, global_step=89831, preemption_count=0, score=41210.269149, test/accuracy=0.585100, test/loss=1.989668, test/num_examples=10000, total_duration=45217.880037, train/accuracy=0.777383, train/loss=1.090680, validation/accuracy=0.708400, validation/loss=1.384177, validation/num_examples=50000
I0205 03:26:55.959449 139946414638848 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.208206057548523, loss=3.4607956409454346
I0205 03:27:42.138745 139946397853440 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.237437129020691, loss=3.4381930828094482
I0205 03:28:28.798836 139946414638848 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.2889689207077026, loss=3.2929582595825195
I0205 03:29:15.573601 139946397853440 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.3416962623596191, loss=3.267923593521118
I0205 03:30:02.056413 139946414638848 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.2949177026748657, loss=3.2542192935943604
I0205 03:30:48.594156 139946397853440 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.2099428176879883, loss=3.338563919067383
I0205 03:31:35.074155 139946414638848 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.5285913944244385, loss=3.312822103500366
I0205 03:32:21.699140 139946397853440 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.3948643207550049, loss=3.3109757900238037
I0205 03:33:08.177231 139946414638848 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.2544639110565186, loss=3.2375316619873047
I0205 03:33:28.384424 140107197974336 spec.py:321] Evaluating on the training split.
I0205 03:33:39.165627 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 03:34:10.696157 140107197974336 spec.py:349] Evaluating on the test split.
I0205 03:34:12.297054 140107197974336 submission_runner.py:408] Time since start: 45682.04s, 	Step: 90745, 	{'train/accuracy': 0.7867773175239563, 'train/loss': 1.062552809715271, 'validation/accuracy': 0.7113800048828125, 'validation/loss': 1.3798199892044067, 'validation/num_examples': 50000, 'test/accuracy': 0.589400053024292, 'test/loss': 1.9841927289962769, 'test/num_examples': 10000, 'score': 41630.42470932007, 'total_duration': 45682.04362034798, 'accumulated_submission_time': 41630.42470932007, 'accumulated_eval_time': 4042.9406147003174, 'accumulated_logging_time': 3.434882879257202}
I0205 03:34:12.327358 139946397853440 logging_writer.py:48] [90745] accumulated_eval_time=4042.940615, accumulated_logging_time=3.434883, accumulated_submission_time=41630.424709, global_step=90745, preemption_count=0, score=41630.424709, test/accuracy=0.589400, test/loss=1.984193, test/num_examples=10000, total_duration=45682.043620, train/accuracy=0.786777, train/loss=1.062553, validation/accuracy=0.711380, validation/loss=1.379820, validation/num_examples=50000
I0205 03:34:34.310893 139946414638848 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.6230167150497437, loss=4.425719738006592
I0205 03:35:19.292659 139946397853440 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.3545860052108765, loss=3.3155202865600586
I0205 03:36:05.898002 139946414638848 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.2468940019607544, loss=3.8311100006103516
I0205 03:36:52.213543 139946397853440 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.3084609508514404, loss=4.555080890655518
I0205 03:37:38.734900 139946414638848 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.2856786251068115, loss=4.471681118011475
I0205 03:38:25.313945 139946397853440 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.2244758605957031, loss=3.2396717071533203
I0205 03:39:12.004569 139946414638848 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.3120312690734863, loss=3.7926266193389893
I0205 03:39:58.290459 139946397853440 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.3027667999267578, loss=3.3710992336273193
I0205 03:40:44.925332 139946414638848 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.3673006296157837, loss=3.2413980960845947
I0205 03:41:12.647487 140107197974336 spec.py:321] Evaluating on the training split.
I0205 03:41:23.573975 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 03:41:50.122559 140107197974336 spec.py:349] Evaluating on the test split.
I0205 03:41:51.717027 140107197974336 submission_runner.py:408] Time since start: 46141.46s, 	Step: 91661, 	{'train/accuracy': 0.7730078101158142, 'train/loss': 1.0981395244598389, 'validation/accuracy': 0.7106599807739258, 'validation/loss': 1.366332769393921, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.9762824773788452, 'test/num_examples': 10000, 'score': 42050.68173789978, 'total_duration': 46141.46359419823, 'accumulated_submission_time': 42050.68173789978, 'accumulated_eval_time': 4082.0101313591003, 'accumulated_logging_time': 3.4753620624542236}
I0205 03:41:51.747770 139946397853440 logging_writer.py:48] [91661] accumulated_eval_time=4082.010131, accumulated_logging_time=3.475362, accumulated_submission_time=42050.681738, global_step=91661, preemption_count=0, score=42050.681738, test/accuracy=0.584800, test/loss=1.976282, test/num_examples=10000, total_duration=46141.463594, train/accuracy=0.773008, train/loss=1.098140, validation/accuracy=0.710660, validation/loss=1.366333, validation/num_examples=50000
I0205 03:42:07.486305 139946414638848 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.3122196197509766, loss=3.279452085494995
I0205 03:42:51.602210 139946397853440 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.3412457704544067, loss=4.608062744140625
I0205 03:43:38.620310 139946414638848 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.324476718902588, loss=3.9170925617218018
I0205 03:44:24.994368 139946397853440 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.33885657787323, loss=3.2536420822143555
I0205 03:45:11.735137 139946414638848 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.2199779748916626, loss=3.8319668769836426
I0205 03:45:58.208858 139946397853440 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.2118213176727295, loss=4.076240062713623
I0205 03:46:44.769530 139946414638848 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.2920879125595093, loss=3.4116220474243164
I0205 03:47:31.328569 139946397853440 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.329999566078186, loss=3.510244369506836
I0205 03:48:17.873826 139946414638848 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.2974315881729126, loss=3.2462453842163086
I0205 03:48:51.923477 140107197974336 spec.py:321] Evaluating on the training split.
I0205 03:49:02.717472 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 03:49:34.686245 140107197974336 spec.py:349] Evaluating on the test split.
I0205 03:49:36.282397 140107197974336 submission_runner.py:408] Time since start: 46606.03s, 	Step: 92575, 	{'train/accuracy': 0.779980480670929, 'train/loss': 1.0799758434295654, 'validation/accuracy': 0.708620011806488, 'validation/loss': 1.3764957189559937, 'validation/num_examples': 50000, 'test/accuracy': 0.5852000117301941, 'test/loss': 1.9942631721496582, 'test/num_examples': 10000, 'score': 42470.79413843155, 'total_duration': 46606.02896428108, 'accumulated_submission_time': 42470.79413843155, 'accumulated_eval_time': 4126.3690321445465, 'accumulated_logging_time': 3.515968084335327}
I0205 03:49:36.313697 139946397853440 logging_writer.py:48] [92575] accumulated_eval_time=4126.369032, accumulated_logging_time=3.515968, accumulated_submission_time=42470.794138, global_step=92575, preemption_count=0, score=42470.794138, test/accuracy=0.585200, test/loss=1.994263, test/num_examples=10000, total_duration=46606.028964, train/accuracy=0.779980, train/loss=1.079976, validation/accuracy=0.708620, validation/loss=1.376496, validation/num_examples=50000
I0205 03:49:46.538629 139946414638848 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.4332160949707031, loss=3.2833311557769775
I0205 03:50:29.522443 139946397853440 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.386644959449768, loss=3.3232438564300537
I0205 03:51:15.684853 139946414638848 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.3622251749038696, loss=4.654031753540039
I0205 03:52:01.983685 139946397853440 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.2430881261825562, loss=3.992185592651367
I0205 03:52:48.205327 139946414638848 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.3214682340621948, loss=3.2366862297058105
I0205 03:53:34.635496 139946397853440 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.3035889863967896, loss=3.2856976985931396
I0205 03:54:21.104914 139946414638848 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.4286683797836304, loss=4.883076190948486
I0205 03:55:07.535257 139946397853440 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.3652220964431763, loss=4.512263298034668
I0205 03:55:53.712335 139946414638848 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.2186673879623413, loss=3.7026681900024414
I0205 03:56:36.374385 140107197974336 spec.py:321] Evaluating on the training split.
I0205 03:56:47.230154 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 03:57:17.316607 140107197974336 spec.py:349] Evaluating on the test split.
I0205 03:57:18.921186 140107197974336 submission_runner.py:408] Time since start: 47068.67s, 	Step: 93494, 	{'train/accuracy': 0.7886718511581421, 'train/loss': 1.0608985424041748, 'validation/accuracy': 0.7127999663352966, 'validation/loss': 1.3815165758132935, 'validation/num_examples': 50000, 'test/accuracy': 0.5883000493049622, 'test/loss': 1.989989161491394, 'test/num_examples': 10000, 'score': 42890.790291547775, 'total_duration': 47068.66774916649, 'accumulated_submission_time': 42890.790291547775, 'accumulated_eval_time': 4168.915802717209, 'accumulated_logging_time': 3.5591742992401123}
I0205 03:57:18.956535 139946397853440 logging_writer.py:48] [93494] accumulated_eval_time=4168.915803, accumulated_logging_time=3.559174, accumulated_submission_time=42890.790292, global_step=93494, preemption_count=0, score=42890.790292, test/accuracy=0.588300, test/loss=1.989989, test/num_examples=10000, total_duration=47068.667749, train/accuracy=0.788672, train/loss=1.060899, validation/accuracy=0.712800, validation/loss=1.381517, validation/num_examples=50000
I0205 03:57:21.711846 139946414638848 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.2948341369628906, loss=4.172919750213623
I0205 03:58:03.751940 139946397853440 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.3759326934814453, loss=4.816288948059082
I0205 03:58:50.112007 139946414638848 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.2855232954025269, loss=3.250204086303711
I0205 03:59:36.896611 139946397853440 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.3070327043533325, loss=3.291536569595337
I0205 04:00:23.187138 139946414638848 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.2923640012741089, loss=3.36649489402771
I0205 04:01:09.787877 139946397853440 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.3438935279846191, loss=3.279262065887451
I0205 04:01:56.185389 139946414638848 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.3002057075500488, loss=4.243023872375488
I0205 04:02:42.660516 139946397853440 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.4256914854049683, loss=4.916528701782227
I0205 04:03:28.912934 139946414638848 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.3574997186660767, loss=3.314272880554199
I0205 04:04:15.573014 139946397853440 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.2688740491867065, loss=4.50396728515625
I0205 04:04:19.040870 140107197974336 spec.py:321] Evaluating on the training split.
I0205 04:04:30.017418 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 04:05:01.896952 140107197974336 spec.py:349] Evaluating on the test split.
I0205 04:05:03.497728 140107197974336 submission_runner.py:408] Time since start: 47533.24s, 	Step: 94409, 	{'train/accuracy': 0.7827734351158142, 'train/loss': 1.0911414623260498, 'validation/accuracy': 0.7120400071144104, 'validation/loss': 1.3809247016906738, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.9795022010803223, 'test/num_examples': 10000, 'score': 43310.811838150024, 'total_duration': 47533.24429869652, 'accumulated_submission_time': 43310.811838150024, 'accumulated_eval_time': 4213.372656822205, 'accumulated_logging_time': 3.6043410301208496}
I0205 04:05:03.531645 139946414638848 logging_writer.py:48] [94409] accumulated_eval_time=4213.372657, accumulated_logging_time=3.604341, accumulated_submission_time=43310.811838, global_step=94409, preemption_count=0, score=43310.811838, test/accuracy=0.593200, test/loss=1.979502, test/num_examples=10000, total_duration=47533.244299, train/accuracy=0.782773, train/loss=1.091141, validation/accuracy=0.712040, validation/loss=1.380925, validation/num_examples=50000
I0205 04:05:41.115081 139946397853440 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.315255880355835, loss=3.206651449203491
I0205 04:06:27.297134 139946414638848 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.293859601020813, loss=3.504342555999756
I0205 04:07:13.490497 139946397853440 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.5342622995376587, loss=4.772583484649658
I0205 04:07:59.623751 139946414638848 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.547031283378601, loss=4.764410018920898
I0205 04:08:46.248786 139946397853440 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.298183560371399, loss=3.1554641723632812
I0205 04:09:32.387445 139946414638848 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.4164243936538696, loss=4.6597580909729
I0205 04:10:18.891832 139946397853440 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.4026062488555908, loss=3.3026037216186523
I0205 04:11:04.945400 139946414638848 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.1364223957061768, loss=3.8886027336120605
I0205 04:11:51.495736 139946397853440 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.3559309244155884, loss=3.4469127655029297
I0205 04:12:04.209262 140107197974336 spec.py:321] Evaluating on the training split.
I0205 04:12:15.056962 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 04:12:45.540091 140107197974336 spec.py:349] Evaluating on the test split.
I0205 04:12:47.135585 140107197974336 submission_runner.py:408] Time since start: 47996.88s, 	Step: 95328, 	{'train/accuracy': 0.7830663919448853, 'train/loss': 1.0953134298324585, 'validation/accuracy': 0.7137599587440491, 'validation/loss': 1.3859901428222656, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.9971227645874023, 'test/num_examples': 10000, 'score': 43731.426703214645, 'total_duration': 47996.88214635849, 'accumulated_submission_time': 43731.426703214645, 'accumulated_eval_time': 4256.298967838287, 'accumulated_logging_time': 3.647977590560913}
I0205 04:12:47.166384 139946414638848 logging_writer.py:48] [95328] accumulated_eval_time=4256.298968, accumulated_logging_time=3.647978, accumulated_submission_time=43731.426703, global_step=95328, preemption_count=0, score=43731.426703, test/accuracy=0.591900, test/loss=1.997123, test/num_examples=10000, total_duration=47996.882146, train/accuracy=0.783066, train/loss=1.095313, validation/accuracy=0.713760, validation/loss=1.385990, validation/num_examples=50000
I0205 04:13:16.205896 139946397853440 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.4535813331604004, loss=3.16837739944458
I0205 04:14:02.352864 139946414638848 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.4121880531311035, loss=3.2262330055236816
I0205 04:14:49.052577 139946397853440 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.506130576133728, loss=4.758635520935059
I0205 04:15:35.293621 139946414638848 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.3092231750488281, loss=3.2355875968933105
I0205 04:16:21.864121 139946397853440 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.3976210355758667, loss=3.2236359119415283
I0205 04:17:08.277353 139946414638848 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.2972053289413452, loss=3.876331329345703
I0205 04:17:54.732176 139946397853440 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.3665868043899536, loss=3.2958626747131348
I0205 04:18:41.299982 139946414638848 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.3344392776489258, loss=4.491410732269287
I0205 04:19:28.028241 139946397853440 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.4960126876831055, loss=3.2486696243286133
I0205 04:19:47.517774 140107197974336 spec.py:321] Evaluating on the training split.
I0205 04:19:58.522093 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 04:20:30.739426 140107197974336 spec.py:349] Evaluating on the test split.
I0205 04:20:32.329957 140107197974336 submission_runner.py:408] Time since start: 48462.08s, 	Step: 96244, 	{'train/accuracy': 0.7925195097923279, 'train/loss': 1.027241826057434, 'validation/accuracy': 0.7157999873161316, 'validation/loss': 1.3453587293624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.94822096824646, 'test/num_examples': 10000, 'score': 44151.71507334709, 'total_duration': 48462.07651758194, 'accumulated_submission_time': 44151.71507334709, 'accumulated_eval_time': 4301.111127138138, 'accumulated_logging_time': 3.689197540283203}
I0205 04:20:32.360662 139946414638848 logging_writer.py:48] [96244] accumulated_eval_time=4301.111127, accumulated_logging_time=3.689198, accumulated_submission_time=44151.715073, global_step=96244, preemption_count=0, score=44151.715073, test/accuracy=0.587100, test/loss=1.948221, test/num_examples=10000, total_duration=48462.076518, train/accuracy=0.792520, train/loss=1.027242, validation/accuracy=0.715800, validation/loss=1.345359, validation/num_examples=50000
I0205 04:20:54.769171 139946397853440 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.3435653448104858, loss=3.233248233795166
I0205 04:21:40.170328 139946414638848 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.3031467199325562, loss=3.2391517162323
I0205 04:22:26.504642 139946397853440 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.5694572925567627, loss=4.745184421539307
I0205 04:23:13.291054 139946414638848 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.3584545850753784, loss=3.553297519683838
I0205 04:23:59.443614 139946397853440 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.3667868375778198, loss=3.8575093746185303
I0205 04:24:46.337636 139946414638848 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.3863259553909302, loss=3.921976089477539
I0205 04:25:32.728099 139946397853440 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.4637858867645264, loss=3.266819477081299
I0205 04:26:19.262779 139946414638848 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.534803867340088, loss=4.74307918548584
I0205 04:27:05.774687 139946397853440 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.2990472316741943, loss=3.507841110229492
I0205 04:27:32.677236 140107197974336 spec.py:321] Evaluating on the training split.
I0205 04:27:43.390582 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 04:28:11.051348 140107197974336 spec.py:349] Evaluating on the test split.
I0205 04:28:12.653269 140107197974336 submission_runner.py:408] Time since start: 48922.40s, 	Step: 97160, 	{'train/accuracy': 0.8024218678474426, 'train/loss': 1.0138721466064453, 'validation/accuracy': 0.7195599675178528, 'validation/loss': 1.3655524253845215, 'validation/num_examples': 50000, 'test/accuracy': 0.5987000465393066, 'test/loss': 1.9719301462173462, 'test/num_examples': 10000, 'score': 44571.969373226166, 'total_duration': 48922.39983391762, 'accumulated_submission_time': 44571.969373226166, 'accumulated_eval_time': 4341.087158918381, 'accumulated_logging_time': 3.729381799697876}
I0205 04:28:12.687882 139946414638848 logging_writer.py:48] [97160] accumulated_eval_time=4341.087159, accumulated_logging_time=3.729382, accumulated_submission_time=44571.969373, global_step=97160, preemption_count=0, score=44571.969373, test/accuracy=0.598700, test/loss=1.971930, test/num_examples=10000, total_duration=48922.399834, train/accuracy=0.802422, train/loss=1.013872, validation/accuracy=0.719560, validation/loss=1.365552, validation/num_examples=50000
I0205 04:28:28.826546 139946397853440 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.4596285820007324, loss=3.4780290126800537
I0205 04:29:13.446322 139946414638848 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.5101498365402222, loss=3.449352979660034
I0205 04:30:00.056433 139946397853440 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.2729145288467407, loss=3.274317502975464
I0205 04:30:46.545299 139946414638848 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.5398650169372559, loss=4.73259162902832
I0205 04:31:33.416410 139946397853440 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.5073872804641724, loss=4.836944103240967
I0205 04:32:20.230589 139946414638848 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.238914966583252, loss=4.006514549255371
I0205 04:33:06.992536 139946397853440 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.3406063318252563, loss=3.6216397285461426
I0205 04:33:53.409860 139946414638848 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.410290002822876, loss=3.2549352645874023
I0205 04:34:39.949661 139946397853440 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.3442413806915283, loss=3.4031124114990234
I0205 04:35:12.857413 140107197974336 spec.py:321] Evaluating on the training split.
I0205 04:35:23.729815 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 04:35:53.522325 140107197974336 spec.py:349] Evaluating on the test split.
I0205 04:35:55.117883 140107197974336 submission_runner.py:408] Time since start: 49384.86s, 	Step: 98072, 	{'train/accuracy': 0.7826171517372131, 'train/loss': 1.069865107536316, 'validation/accuracy': 0.7172399759292603, 'validation/loss': 1.3597993850708008, 'validation/num_examples': 50000, 'test/accuracy': 0.5934000015258789, 'test/loss': 1.9582252502441406, 'test/num_examples': 10000, 'score': 44992.07482409477, 'total_duration': 49384.86444759369, 'accumulated_submission_time': 44992.07482409477, 'accumulated_eval_time': 4383.347653388977, 'accumulated_logging_time': 3.7755613327026367}
I0205 04:35:55.154801 139946414638848 logging_writer.py:48] [98072] accumulated_eval_time=4383.347653, accumulated_logging_time=3.775561, accumulated_submission_time=44992.074824, global_step=98072, preemption_count=0, score=44992.074824, test/accuracy=0.593400, test/loss=1.958225, test/num_examples=10000, total_duration=49384.864448, train/accuracy=0.782617, train/loss=1.069865, validation/accuracy=0.717240, validation/loss=1.359799, validation/num_examples=50000
I0205 04:36:06.548902 139946397853440 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.5091651678085327, loss=4.697132110595703
I0205 04:36:49.843991 139946414638848 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.3364311456680298, loss=3.22882342338562
I0205 04:37:36.231939 139946397853440 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.4619946479797363, loss=3.243842124938965
I0205 04:38:22.873185 139946414638848 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.418350338935852, loss=3.3030145168304443
I0205 04:39:09.209213 139946397853440 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.3017387390136719, loss=3.4499356746673584
I0205 04:39:55.500892 139946414638848 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.440863013267517, loss=4.521369934082031
I0205 04:40:42.169043 139946397853440 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.4122745990753174, loss=3.258435010910034
I0205 04:41:28.619214 139946414638848 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.378806471824646, loss=3.550259590148926
I0205 04:42:15.665026 139946397853440 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.543014645576477, loss=3.2869200706481934
I0205 04:42:55.374503 140107197974336 spec.py:321] Evaluating on the training split.
I0205 04:43:06.241306 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 04:43:39.022157 140107197974336 spec.py:349] Evaluating on the test split.
I0205 04:43:40.615422 140107197974336 submission_runner.py:408] Time since start: 49850.36s, 	Step: 98987, 	{'train/accuracy': 0.7899804711341858, 'train/loss': 1.0631240606307983, 'validation/accuracy': 0.7159000039100647, 'validation/loss': 1.3712981939315796, 'validation/num_examples': 50000, 'test/accuracy': 0.5945000052452087, 'test/loss': 1.9744479656219482, 'test/num_examples': 10000, 'score': 45412.23049378395, 'total_duration': 49850.361988306046, 'accumulated_submission_time': 45412.23049378395, 'accumulated_eval_time': 4428.588559150696, 'accumulated_logging_time': 3.823927640914917}
I0205 04:43:40.648655 139946414638848 logging_writer.py:48] [98987] accumulated_eval_time=4428.588559, accumulated_logging_time=3.823928, accumulated_submission_time=45412.230494, global_step=98987, preemption_count=0, score=45412.230494, test/accuracy=0.594500, test/loss=1.974448, test/num_examples=10000, total_duration=49850.361988, train/accuracy=0.789980, train/loss=1.063124, validation/accuracy=0.715900, validation/loss=1.371298, validation/num_examples=50000
I0205 04:43:46.156243 139946397853440 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.3658068180084229, loss=3.2289226055145264
I0205 04:44:28.493573 139946414638848 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.382260799407959, loss=4.868127822875977
I0205 04:45:14.655039 139946397853440 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.345511555671692, loss=3.2162702083587646
I0205 04:46:01.196898 139946414638848 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.3788623809814453, loss=3.2390129566192627
I0205 04:46:47.638190 139946397853440 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.3530323505401611, loss=3.549670934677124
I0205 04:47:34.121546 139946414638848 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.3813916444778442, loss=3.2053024768829346
I0205 04:48:20.476205 139946397853440 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.3436704874038696, loss=3.2704453468322754
I0205 04:49:06.953384 139946414638848 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.2358314990997314, loss=3.251711130142212
I0205 04:49:53.218184 139946397853440 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.3319377899169922, loss=3.218322277069092
I0205 04:50:39.859906 139946414638848 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.2856484651565552, loss=3.154754400253296
I0205 04:50:40.922296 140107197974336 spec.py:321] Evaluating on the training split.
I0205 04:50:51.938892 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 04:51:23.057480 140107197974336 spec.py:349] Evaluating on the test split.
I0205 04:51:24.662431 140107197974336 submission_runner.py:408] Time since start: 50314.41s, 	Step: 99904, 	{'train/accuracy': 0.8057421445846558, 'train/loss': 0.96631920337677, 'validation/accuracy': 0.7188799977302551, 'validation/loss': 1.3333569765090942, 'validation/num_examples': 50000, 'test/accuracy': 0.5960000157356262, 'test/loss': 1.9273792505264282, 'test/num_examples': 10000, 'score': 45832.44064116478, 'total_duration': 50314.4089922905, 'accumulated_submission_time': 45832.44064116478, 'accumulated_eval_time': 4472.3286554813385, 'accumulated_logging_time': 3.8672292232513428}
I0205 04:51:24.699667 139946397853440 logging_writer.py:48] [99904] accumulated_eval_time=4472.328655, accumulated_logging_time=3.867229, accumulated_submission_time=45832.440641, global_step=99904, preemption_count=0, score=45832.440641, test/accuracy=0.596000, test/loss=1.927379, test/num_examples=10000, total_duration=50314.408992, train/accuracy=0.805742, train/loss=0.966319, validation/accuracy=0.718880, validation/loss=1.333357, validation/num_examples=50000
I0205 04:52:04.824478 139946414638848 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.4204431772232056, loss=4.401854515075684
I0205 04:52:51.043999 139946397853440 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.4214987754821777, loss=3.275143623352051
I0205 04:53:37.452655 139946414638848 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.2953587770462036, loss=3.1391351222991943
I0205 04:54:23.955661 139946397853440 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.3454744815826416, loss=3.302569627761841
I0205 04:55:10.396463 139946414638848 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.386862874031067, loss=3.253774642944336
I0205 04:55:56.598263 139946397853440 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.2907754182815552, loss=3.767517328262329
I0205 04:56:42.606254 139946414638848 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.6375385522842407, loss=3.1862502098083496
I0205 04:57:28.965946 139946397853440 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.3729771375656128, loss=3.2862343788146973
I0205 04:58:15.104895 139946414638848 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.5056426525115967, loss=4.789196968078613
I0205 04:58:25.107116 140107197974336 spec.py:321] Evaluating on the training split.
I0205 04:58:35.741159 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 04:59:01.923268 140107197974336 spec.py:349] Evaluating on the test split.
I0205 04:59:03.523288 140107197974336 submission_runner.py:408] Time since start: 50773.27s, 	Step: 100823, 	{'train/accuracy': 0.7916210889816284, 'train/loss': 1.0372837781906128, 'validation/accuracy': 0.723039984703064, 'validation/loss': 1.3263189792633057, 'validation/num_examples': 50000, 'test/accuracy': 0.5957000255584717, 'test/loss': 1.943742036819458, 'test/num_examples': 10000, 'score': 46252.78383398056, 'total_duration': 50773.26985836029, 'accumulated_submission_time': 46252.78383398056, 'accumulated_eval_time': 4510.744811296463, 'accumulated_logging_time': 3.916255474090576}
I0205 04:59:03.553597 139946397853440 logging_writer.py:48] [100823] accumulated_eval_time=4510.744811, accumulated_logging_time=3.916255, accumulated_submission_time=46252.783834, global_step=100823, preemption_count=0, score=46252.783834, test/accuracy=0.595700, test/loss=1.943742, test/num_examples=10000, total_duration=50773.269858, train/accuracy=0.791621, train/loss=1.037284, validation/accuracy=0.723040, validation/loss=1.326319, validation/num_examples=50000
I0205 04:59:34.761311 139946414638848 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.3714706897735596, loss=3.2577056884765625
I0205 05:00:20.937625 139946397853440 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.4497603178024292, loss=3.1670360565185547
I0205 05:01:07.614784 139946414638848 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.288213849067688, loss=3.646973133087158
I0205 05:01:53.778582 139946397853440 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.2860275506973267, loss=3.5050573348999023
I0205 05:02:40.205746 139946414638848 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.3547896146774292, loss=4.354764938354492
I0205 05:03:26.777598 139946397853440 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.3831106424331665, loss=3.2485458850860596
I0205 05:04:13.059661 139946414638848 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.4796977043151855, loss=3.262371063232422
I0205 05:04:59.377164 139946397853440 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.5078729391098022, loss=3.2732717990875244
I0205 05:05:45.885888 139946414638848 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.4950191974639893, loss=3.2148656845092773
I0205 05:06:03.685631 140107197974336 spec.py:321] Evaluating on the training split.
I0205 05:06:14.565842 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 05:06:44.023591 140107197974336 spec.py:349] Evaluating on the test split.
I0205 05:06:45.621075 140107197974336 submission_runner.py:408] Time since start: 51235.37s, 	Step: 101740, 	{'train/accuracy': 0.7941210865974426, 'train/loss': 1.010243535041809, 'validation/accuracy': 0.7223399877548218, 'validation/loss': 1.3201320171356201, 'validation/num_examples': 50000, 'test/accuracy': 0.5976999998092651, 'test/loss': 1.9327346086502075, 'test/num_examples': 10000, 'score': 46672.85423207283, 'total_duration': 51235.3676404953, 'accumulated_submission_time': 46672.85423207283, 'accumulated_eval_time': 4552.680232286453, 'accumulated_logging_time': 3.956141948699951}
I0205 05:06:45.656205 139946397853440 logging_writer.py:48] [101740] accumulated_eval_time=4552.680232, accumulated_logging_time=3.956142, accumulated_submission_time=46672.854232, global_step=101740, preemption_count=0, score=46672.854232, test/accuracy=0.597700, test/loss=1.932735, test/num_examples=10000, total_duration=51235.367640, train/accuracy=0.794121, train/loss=1.010244, validation/accuracy=0.722340, validation/loss=1.320132, validation/num_examples=50000
I0205 05:07:09.623893 139946414638848 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.3081305027008057, loss=3.376554012298584
I0205 05:07:55.193348 139946397853440 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.459663987159729, loss=3.3255043029785156
I0205 05:08:41.769977 139946414638848 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.6134767532348633, loss=3.244858980178833
I0205 05:09:28.282463 139946397853440 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.3007662296295166, loss=4.374795436859131
I0205 05:10:14.628530 139946414638848 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.3647713661193848, loss=3.7207834720611572
I0205 05:11:00.878063 139946397853440 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.483115792274475, loss=3.216576099395752
I0205 05:11:47.264842 139946414638848 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.3248400688171387, loss=4.241879463195801
I0205 05:12:33.812398 139946397853440 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.512870192527771, loss=3.2999582290649414
I0205 05:13:20.275542 139946414638848 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.4193600416183472, loss=3.2235918045043945
I0205 05:13:45.888969 140107197974336 spec.py:321] Evaluating on the training split.
I0205 05:13:56.501304 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 05:14:24.318726 140107197974336 spec.py:349] Evaluating on the test split.
I0205 05:14:25.934028 140107197974336 submission_runner.py:408] Time since start: 51695.68s, 	Step: 102657, 	{'train/accuracy': 0.8057616949081421, 'train/loss': 0.9902685880661011, 'validation/accuracy': 0.7222999930381775, 'validation/loss': 1.3363498449325562, 'validation/num_examples': 50000, 'test/accuracy': 0.6000000238418579, 'test/loss': 1.952873945236206, 'test/num_examples': 10000, 'score': 47093.024424791336, 'total_duration': 51695.68058466911, 'accumulated_submission_time': 47093.024424791336, 'accumulated_eval_time': 4592.725254058838, 'accumulated_logging_time': 4.001856803894043}
I0205 05:14:25.966225 139946397853440 logging_writer.py:48] [102657] accumulated_eval_time=4592.725254, accumulated_logging_time=4.001857, accumulated_submission_time=47093.024425, global_step=102657, preemption_count=0, score=47093.024425, test/accuracy=0.600000, test/loss=1.952874, test/num_examples=10000, total_duration=51695.680585, train/accuracy=0.805762, train/loss=0.990269, validation/accuracy=0.722300, validation/loss=1.336350, validation/num_examples=50000
I0205 05:14:43.242137 139946414638848 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.3508604764938354, loss=3.7199690341949463
I0205 05:15:27.142019 139946397853440 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.4659888744354248, loss=3.216573715209961
I0205 05:16:13.876588 139946414638848 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.4053795337677002, loss=3.2588071823120117
I0205 05:17:00.221691 139946397853440 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.3848563432693481, loss=3.1933960914611816
I0205 05:17:46.679537 139946414638848 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.3583035469055176, loss=3.205747365951538
I0205 05:18:33.110331 139946397853440 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.3943250179290771, loss=4.155400276184082
I0205 05:19:19.500451 139946414638848 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.477888584136963, loss=4.369176864624023
I0205 05:20:06.007250 139946397853440 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.4467798471450806, loss=4.217105388641357
I0205 05:20:52.219750 139946414638848 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.530311942100525, loss=4.2962541580200195
I0205 05:21:26.004811 140107197974336 spec.py:321] Evaluating on the training split.
I0205 05:21:36.901294 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 05:22:07.732092 140107197974336 spec.py:349] Evaluating on the test split.
I0205 05:22:09.335081 140107197974336 submission_runner.py:408] Time since start: 52159.08s, 	Step: 103574, 	{'train/accuracy': 0.79505854845047, 'train/loss': 1.0035017728805542, 'validation/accuracy': 0.7264399528503418, 'validation/loss': 1.2918392419815063, 'validation/num_examples': 50000, 'test/accuracy': 0.603600025177002, 'test/loss': 1.899206280708313, 'test/num_examples': 10000, 'score': 47512.99956417084, 'total_duration': 52159.081652879715, 'accumulated_submission_time': 47512.99956417084, 'accumulated_eval_time': 4636.055516242981, 'accumulated_logging_time': 4.0445640087127686}
I0205 05:22:09.373154 139946397853440 logging_writer.py:48] [103574] accumulated_eval_time=4636.055516, accumulated_logging_time=4.044564, accumulated_submission_time=47512.999564, global_step=103574, preemption_count=0, score=47512.999564, test/accuracy=0.603600, test/loss=1.899206, test/num_examples=10000, total_duration=52159.081653, train/accuracy=0.795059, train/loss=1.003502, validation/accuracy=0.726440, validation/loss=1.291839, validation/num_examples=50000
I0205 05:22:19.983817 139946414638848 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.3414620161056519, loss=3.1944611072540283
I0205 05:23:03.209550 139946397853440 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.3910388946533203, loss=3.499979019165039
I0205 05:23:49.412220 139946414638848 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.4481292963027954, loss=3.217055320739746
I0205 05:24:36.469980 139946397853440 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.4084216356277466, loss=3.724569082260132
I0205 05:25:22.692576 139946414638848 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.4425122737884521, loss=3.155749797821045
I0205 05:26:09.200575 139946397853440 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.3980721235275269, loss=3.435652256011963
I0205 05:26:55.243670 139946414638848 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.3477227687835693, loss=3.1687426567077637
I0205 05:27:41.705908 139946397853440 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.5207370519638062, loss=4.82413387298584
I0205 05:28:27.995537 139946414638848 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.3868086338043213, loss=3.2616829872131348
I0205 05:29:09.672922 140107197974336 spec.py:321] Evaluating on the training split.
I0205 05:29:21.086648 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 05:29:47.502419 140107197974336 spec.py:349] Evaluating on the test split.
I0205 05:29:49.099751 140107197974336 submission_runner.py:408] Time since start: 52618.85s, 	Step: 104491, 	{'train/accuracy': 0.7984960675239563, 'train/loss': 1.0110760927200317, 'validation/accuracy': 0.7244200110435486, 'validation/loss': 1.3226091861724854, 'validation/num_examples': 50000, 'test/accuracy': 0.5982000231742859, 'test/loss': 1.9376882314682007, 'test/num_examples': 10000, 'score': 47933.23684620857, 'total_duration': 52618.84631681442, 'accumulated_submission_time': 47933.23684620857, 'accumulated_eval_time': 4675.482330560684, 'accumulated_logging_time': 4.093322277069092}
I0205 05:29:49.136265 139946397853440 logging_writer.py:48] [104491] accumulated_eval_time=4675.482331, accumulated_logging_time=4.093322, accumulated_submission_time=47933.236846, global_step=104491, preemption_count=0, score=47933.236846, test/accuracy=0.598200, test/loss=1.937688, test/num_examples=10000, total_duration=52618.846317, train/accuracy=0.798496, train/loss=1.011076, validation/accuracy=0.724420, validation/loss=1.322609, validation/num_examples=50000
I0205 05:29:53.067297 139946414638848 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.3817296028137207, loss=3.2217674255371094
I0205 05:30:35.122727 139946397853440 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.4898136854171753, loss=3.2346372604370117
I0205 05:31:21.406269 139946414638848 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.4301427602767944, loss=3.2132272720336914
I0205 05:32:08.166959 139946397853440 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.449038028717041, loss=3.3521013259887695
I0205 05:32:54.326612 139946414638848 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.573108434677124, loss=3.162968397140503
I0205 05:33:40.857130 139946397853440 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.4034709930419922, loss=4.129334926605225
I0205 05:34:27.454771 139946414638848 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.5091948509216309, loss=3.254115581512451
I0205 05:35:13.992164 139946397853440 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.5243781805038452, loss=3.212146282196045
I0205 05:36:00.345664 139946414638848 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.5164790153503418, loss=3.132201671600342
I0205 05:36:47.844511 139946397853440 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.4656505584716797, loss=3.137874126434326
I0205 05:36:49.488239 140107197974336 spec.py:321] Evaluating on the training split.
I0205 05:37:00.206674 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 05:37:30.682982 140107197974336 spec.py:349] Evaluating on the test split.
I0205 05:37:32.289425 140107197974336 submission_runner.py:408] Time since start: 53082.04s, 	Step: 105405, 	{'train/accuracy': 0.8055859208106995, 'train/loss': 0.946385383605957, 'validation/accuracy': 0.7270799875259399, 'validation/loss': 1.2798376083374023, 'validation/num_examples': 50000, 'test/accuracy': 0.607200026512146, 'test/loss': 1.8782799243927002, 'test/num_examples': 10000, 'score': 48353.525584459305, 'total_duration': 53082.0359852314, 'accumulated_submission_time': 48353.525584459305, 'accumulated_eval_time': 4718.283473730087, 'accumulated_logging_time': 4.140423059463501}
I0205 05:37:32.323249 139946414638848 logging_writer.py:48] [105405] accumulated_eval_time=4718.283474, accumulated_logging_time=4.140423, accumulated_submission_time=48353.525584, global_step=105405, preemption_count=0, score=48353.525584, test/accuracy=0.607200, test/loss=1.878280, test/num_examples=10000, total_duration=53082.035985, train/accuracy=0.805586, train/loss=0.946385, validation/accuracy=0.727080, validation/loss=1.279838, validation/num_examples=50000
I0205 05:38:12.043911 139946397853440 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.3761813640594482, loss=3.3607864379882812
I0205 05:38:58.235526 139946414638848 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.4936456680297852, loss=3.182178258895874
I0205 05:39:44.782666 139946397853440 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.376512050628662, loss=3.123344659805298
I0205 05:40:31.022804 139946414638848 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.3971045017242432, loss=3.132183313369751
I0205 05:41:17.601652 139946397853440 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.5204201936721802, loss=3.195504665374756
I0205 05:42:04.115615 139946414638848 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.3400640487670898, loss=3.8292417526245117
I0205 05:42:50.534690 139946397853440 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.60802161693573, loss=3.2896199226379395
I0205 05:43:37.004100 139946414638848 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.3689544200897217, loss=3.192798137664795
I0205 05:44:23.244029 139946397853440 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.651504635810852, loss=4.688352584838867
I0205 05:44:32.437783 140107197974336 spec.py:321] Evaluating on the training split.
I0205 05:44:42.926050 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 05:45:11.550508 140107197974336 spec.py:349] Evaluating on the test split.
I0205 05:45:13.154071 140107197974336 submission_runner.py:408] Time since start: 53542.90s, 	Step: 106321, 	{'train/accuracy': 0.7962890267372131, 'train/loss': 1.0095607042312622, 'validation/accuracy': 0.726639986038208, 'validation/loss': 1.3089427947998047, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.9131704568862915, 'test/num_examples': 10000, 'score': 48773.57774686813, 'total_duration': 53542.90063166618, 'accumulated_submission_time': 48773.57774686813, 'accumulated_eval_time': 4758.99973154068, 'accumulated_logging_time': 4.183903694152832}
I0205 05:45:13.192332 139946414638848 logging_writer.py:48] [106321] accumulated_eval_time=4758.999732, accumulated_logging_time=4.183904, accumulated_submission_time=48773.577747, global_step=106321, preemption_count=0, score=48773.577747, test/accuracy=0.603500, test/loss=1.913170, test/num_examples=10000, total_duration=53542.900632, train/accuracy=0.796289, train/loss=1.009561, validation/accuracy=0.726640, validation/loss=1.308943, validation/num_examples=50000
I0205 05:45:45.405516 139946397853440 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.4462788105010986, loss=3.1427738666534424
I0205 05:46:31.214814 139946414638848 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.5319159030914307, loss=3.1582489013671875
I0205 05:47:17.481865 139946397853440 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.3978512287139893, loss=3.255443572998047
I0205 05:48:03.898384 139946414638848 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.3949557542800903, loss=3.16672945022583
I0205 05:48:49.953978 139946397853440 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.7578896284103394, loss=4.605345249176025
I0205 05:49:36.275373 139946414638848 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.3446180820465088, loss=3.327949047088623
I0205 05:50:22.590653 139946397853440 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.4091789722442627, loss=3.4153664112091064
I0205 05:51:09.081058 139946414638848 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.6717349290847778, loss=4.4995574951171875
I0205 05:51:55.366041 139946397853440 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.3811113834381104, loss=3.566563844680786
I0205 05:52:13.170374 140107197974336 spec.py:321] Evaluating on the training split.
I0205 05:52:23.816647 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 05:52:50.834288 140107197974336 spec.py:349] Evaluating on the test split.
I0205 05:52:52.438041 140107197974336 submission_runner.py:408] Time since start: 54002.18s, 	Step: 107240, 	{'train/accuracy': 0.79638671875, 'train/loss': 1.0163118839263916, 'validation/accuracy': 0.7265799641609192, 'validation/loss': 1.3119384050369263, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.9311659336090088, 'test/num_examples': 10000, 'score': 49193.492560863495, 'total_duration': 54002.184608221054, 'accumulated_submission_time': 49193.492560863495, 'accumulated_eval_time': 4798.267370700836, 'accumulated_logging_time': 4.23191499710083}
I0205 05:52:52.470450 139946414638848 logging_writer.py:48] [107240] accumulated_eval_time=4798.267371, accumulated_logging_time=4.231915, accumulated_submission_time=49193.492561, global_step=107240, preemption_count=0, score=49193.492561, test/accuracy=0.599500, test/loss=1.931166, test/num_examples=10000, total_duration=54002.184608, train/accuracy=0.796387, train/loss=1.016312, validation/accuracy=0.726580, validation/loss=1.311938, validation/num_examples=50000
I0205 05:53:16.433674 139946397853440 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.500002145767212, loss=3.4365854263305664
I0205 05:54:01.867726 139946414638848 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.4815117120742798, loss=3.164666175842285
I0205 05:54:48.598060 139946397853440 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.4756760597229004, loss=4.371393203735352
I0205 05:55:35.368828 139946414638848 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.5276955366134644, loss=3.157360076904297
I0205 05:56:21.270919 139946397853440 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.4561785459518433, loss=3.0988450050354004
I0205 05:57:07.753896 139946414638848 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.4396984577178955, loss=3.3455727100372314
I0205 05:57:54.111298 139946397853440 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.5443501472473145, loss=3.214210033416748
I0205 05:58:40.648532 139946414638848 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.6421706676483154, loss=4.834963798522949
I0205 05:59:26.884745 139946397853440 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.6445069313049316, loss=3.1636857986450195
I0205 05:59:52.782137 140107197974336 spec.py:321] Evaluating on the training split.
I0205 06:00:03.458099 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 06:00:33.424658 140107197974336 spec.py:349] Evaluating on the test split.
I0205 06:00:35.028083 140107197974336 submission_runner.py:408] Time since start: 54464.77s, 	Step: 108158, 	{'train/accuracy': 0.8068554401397705, 'train/loss': 0.9952368140220642, 'validation/accuracy': 0.7298399806022644, 'validation/loss': 1.3251501321792603, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.9325958490371704, 'test/num_examples': 10000, 'score': 49613.74026465416, 'total_duration': 54464.774629831314, 'accumulated_submission_time': 49613.74026465416, 'accumulated_eval_time': 4840.51326918602, 'accumulated_logging_time': 4.2742462158203125}
I0205 06:00:35.071575 139946414638848 logging_writer.py:48] [108158] accumulated_eval_time=4840.513269, accumulated_logging_time=4.274246, accumulated_submission_time=49613.740265, global_step=108158, preemption_count=0, score=49613.740265, test/accuracy=0.604300, test/loss=1.932596, test/num_examples=10000, total_duration=54464.774630, train/accuracy=0.806855, train/loss=0.995237, validation/accuracy=0.729840, validation/loss=1.325150, validation/num_examples=50000
I0205 06:00:51.981785 139946397853440 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.3863860368728638, loss=4.153435230255127
I0205 06:01:36.305244 139946414638848 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.4357659816741943, loss=3.1822187900543213
I0205 06:02:22.895283 139946397853440 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.639885425567627, loss=4.642495632171631
I0205 06:03:09.611968 139946414638848 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.3634027242660522, loss=3.7182271480560303
I0205 06:03:55.808907 139946397853440 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.4607197046279907, loss=3.231862783432007
I0205 06:04:42.896071 139946414638848 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.4965882301330566, loss=3.1753337383270264
I0205 06:05:29.248543 139946397853440 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.435687780380249, loss=4.111603260040283
I0205 06:06:15.888092 139946414638848 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.625959038734436, loss=3.2176451683044434
I0205 06:07:02.240953 139946397853440 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.5976506471633911, loss=3.191418409347534
I0205 06:07:35.162373 140107197974336 spec.py:321] Evaluating on the training split.
I0205 06:07:45.653104 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 06:08:14.279755 140107197974336 spec.py:349] Evaluating on the test split.
I0205 06:08:15.877219 140107197974336 submission_runner.py:408] Time since start: 54925.62s, 	Step: 109073, 	{'train/accuracy': 0.8169335722923279, 'train/loss': 0.9389804601669312, 'validation/accuracy': 0.7301599979400635, 'validation/loss': 1.2972155809402466, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.8995212316513062, 'test/num_examples': 10000, 'score': 50033.76714491844, 'total_duration': 54925.62378549576, 'accumulated_submission_time': 50033.76714491844, 'accumulated_eval_time': 4881.228107690811, 'accumulated_logging_time': 4.3293633460998535}
I0205 06:08:15.915300 139946414638848 logging_writer.py:48] [109073] accumulated_eval_time=4881.228108, accumulated_logging_time=4.329363, accumulated_submission_time=50033.767145, global_step=109073, preemption_count=0, score=50033.767145, test/accuracy=0.608900, test/loss=1.899521, test/num_examples=10000, total_duration=54925.623785, train/accuracy=0.816934, train/loss=0.938980, validation/accuracy=0.730160, validation/loss=1.297216, validation/num_examples=50000
I0205 06:08:26.928504 139946397853440 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.6269183158874512, loss=4.827636241912842
I0205 06:09:10.001684 139946414638848 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.5255935192108154, loss=4.207601547241211
I0205 06:09:56.114101 139946397853440 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.6617050170898438, loss=3.1757516860961914
I0205 06:10:42.857303 139946414638848 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.395736575126648, loss=3.132503032684326
I0205 06:11:29.333733 139946397853440 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.4310581684112549, loss=3.18284010887146
I0205 06:12:15.724946 139946414638848 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.409812331199646, loss=4.035686016082764
I0205 06:13:01.936978 139946397853440 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.4831533432006836, loss=3.357189893722534
I0205 06:13:48.277616 139946414638848 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.7662118673324585, loss=4.437015056610107
I0205 06:14:34.820229 139946397853440 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.4471875429153442, loss=3.1370656490325928
I0205 06:15:16.092214 140107197974336 spec.py:321] Evaluating on the training split.
I0205 06:15:27.076396 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 06:15:51.962927 140107197974336 spec.py:349] Evaluating on the test split.
I0205 06:15:53.570056 140107197974336 submission_runner.py:408] Time since start: 55383.32s, 	Step: 109990, 	{'train/accuracy': 0.8023046851158142, 'train/loss': 0.9991682767868042, 'validation/accuracy': 0.730239987373352, 'validation/loss': 1.3013273477554321, 'validation/num_examples': 50000, 'test/accuracy': 0.6098000407218933, 'test/loss': 1.8990883827209473, 'test/num_examples': 10000, 'score': 50453.88138437271, 'total_duration': 55383.31662654877, 'accumulated_submission_time': 50453.88138437271, 'accumulated_eval_time': 4918.705953121185, 'accumulated_logging_time': 4.377705097198486}
I0205 06:15:53.604428 139946414638848 logging_writer.py:48] [109990] accumulated_eval_time=4918.705953, accumulated_logging_time=4.377705, accumulated_submission_time=50453.881384, global_step=109990, preemption_count=0, score=50453.881384, test/accuracy=0.609800, test/loss=1.899088, test/num_examples=10000, total_duration=55383.316627, train/accuracy=0.802305, train/loss=0.999168, validation/accuracy=0.730240, validation/loss=1.301327, validation/num_examples=50000
I0205 06:15:57.925316 139946397853440 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.5266120433807373, loss=3.1804616451263428
I0205 06:16:40.125311 139946414638848 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.6043634414672852, loss=3.21270751953125
I0205 06:17:26.116988 139946397853440 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.452681541442871, loss=4.533611297607422
I0205 06:18:12.540246 139946414638848 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.3659844398498535, loss=3.790616989135742
I0205 06:18:59.063004 139946397853440 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.5688042640686035, loss=4.452786445617676
I0205 06:19:45.591884 139946414638848 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.539414405822754, loss=3.2099289894104004
I0205 06:20:32.140797 139946397853440 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.4994527101516724, loss=4.389199256896973
I0205 06:21:18.567898 139946414638848 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.4382638931274414, loss=3.57817006111145
I0205 06:22:05.057556 139946397853440 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.6193760633468628, loss=4.446652412414551
I0205 06:22:51.224604 139946414638848 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.487640380859375, loss=4.284101486206055
I0205 06:22:53.766687 140107197974336 spec.py:321] Evaluating on the training split.
I0205 06:23:04.604413 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 06:23:34.133457 140107197974336 spec.py:349] Evaluating on the test split.
I0205 06:23:35.733702 140107197974336 submission_runner.py:408] Time since start: 55845.48s, 	Step: 110907, 	{'train/accuracy': 0.8082812428474426, 'train/loss': 0.9439259171485901, 'validation/accuracy': 0.7319799661636353, 'validation/loss': 1.2687028646469116, 'validation/num_examples': 50000, 'test/accuracy': 0.6110000014305115, 'test/loss': 1.8786910772323608, 'test/num_examples': 10000, 'score': 50873.97991299629, 'total_duration': 55845.48027086258, 'accumulated_submission_time': 50873.97991299629, 'accumulated_eval_time': 4960.672949314117, 'accumulated_logging_time': 4.423073053359985}
I0205 06:23:35.769913 139946397853440 logging_writer.py:48] [110907] accumulated_eval_time=4960.672949, accumulated_logging_time=4.423073, accumulated_submission_time=50873.979913, global_step=110907, preemption_count=0, score=50873.979913, test/accuracy=0.611000, test/loss=1.878691, test/num_examples=10000, total_duration=55845.480271, train/accuracy=0.808281, train/loss=0.943926, validation/accuracy=0.731980, validation/loss=1.268703, validation/num_examples=50000
I0205 06:24:14.465790 139946414638848 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.4363441467285156, loss=3.2039220333099365
I0205 06:25:00.760425 139946397853440 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.5447322130203247, loss=3.737776517868042
I0205 06:25:47.290120 139946414638848 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.3820279836654663, loss=3.6997663974761963
I0205 06:26:33.698928 139946397853440 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.4247019290924072, loss=3.7473111152648926
I0205 06:27:20.733932 139946414638848 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.6792207956314087, loss=4.696410179138184
I0205 06:28:07.784590 139946397853440 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.4749776124954224, loss=3.1742777824401855
I0205 06:28:54.256258 139946414638848 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.4513412714004517, loss=3.6930227279663086
I0205 06:29:40.836687 139946397853440 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.3917961120605469, loss=3.250608205795288
I0205 06:30:27.669352 139946414638848 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.529741883277893, loss=4.286577224731445
I0205 06:30:36.015374 140107197974336 spec.py:321] Evaluating on the training split.
I0205 06:30:46.926959 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 06:31:13.623720 140107197974336 spec.py:349] Evaluating on the test split.
I0205 06:31:15.224511 140107197974336 submission_runner.py:408] Time since start: 56304.97s, 	Step: 111820, 	{'train/accuracy': 0.8192773461341858, 'train/loss': 0.9103418588638306, 'validation/accuracy': 0.7331399917602539, 'validation/loss': 1.2813720703125, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.898208737373352, 'test/num_examples': 10000, 'score': 51294.16231417656, 'total_duration': 56304.97105741501, 'accumulated_submission_time': 51294.16231417656, 'accumulated_eval_time': 4999.882031202316, 'accumulated_logging_time': 4.470271587371826}
I0205 06:31:15.264701 139946397853440 logging_writer.py:48] [111820] accumulated_eval_time=4999.882031, accumulated_logging_time=4.470272, accumulated_submission_time=51294.162314, global_step=111820, preemption_count=0, score=51294.162314, test/accuracy=0.613800, test/loss=1.898209, test/num_examples=10000, total_duration=56304.971057, train/accuracy=0.819277, train/loss=0.910342, validation/accuracy=0.733140, validation/loss=1.281372, validation/num_examples=50000
I0205 06:31:47.706164 139946414638848 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.436536431312561, loss=3.137908697128296
I0205 06:32:33.991866 139946397853440 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.5365008115768433, loss=3.1131792068481445
I0205 06:33:20.712486 139946414638848 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.5706865787506104, loss=3.1710045337677
I0205 06:34:07.254736 139946397853440 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.7362405061721802, loss=4.802865982055664
I0205 06:34:53.771386 139946414638848 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.4892432689666748, loss=3.855757713317871
I0205 06:35:40.133583 139946397853440 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.4757996797561646, loss=3.2515830993652344
I0205 06:36:26.597946 139946414638848 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.5383071899414062, loss=3.172943592071533
I0205 06:37:13.481729 139946397853440 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.48257315158844, loss=3.545405387878418
I0205 06:37:59.994669 139946414638848 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.4850503206253052, loss=3.145768642425537
I0205 06:38:15.480951 140107197974336 spec.py:321] Evaluating on the training split.
I0205 06:38:26.339747 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 06:38:54.055003 140107197974336 spec.py:349] Evaluating on the test split.
I0205 06:38:55.644847 140107197974336 submission_runner.py:408] Time since start: 56765.39s, 	Step: 112735, 	{'train/accuracy': 0.8069726228713989, 'train/loss': 0.9850320816040039, 'validation/accuracy': 0.7335000038146973, 'validation/loss': 1.2869131565093994, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.8878358602523804, 'test/num_examples': 10000, 'score': 51714.31570267677, 'total_duration': 56765.39139795303, 'accumulated_submission_time': 51714.31570267677, 'accumulated_eval_time': 5040.045891523361, 'accumulated_logging_time': 4.520854949951172}
I0205 06:38:55.678174 139946397853440 logging_writer.py:48] [112735] accumulated_eval_time=5040.045892, accumulated_logging_time=4.520855, accumulated_submission_time=51714.315703, global_step=112735, preemption_count=0, score=51714.315703, test/accuracy=0.616700, test/loss=1.887836, test/num_examples=10000, total_duration=56765.391398, train/accuracy=0.806973, train/loss=0.985032, validation/accuracy=0.733500, validation/loss=1.286913, validation/num_examples=50000
I0205 06:39:21.880177 139946414638848 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.5643280744552612, loss=4.573829650878906
I0205 06:40:07.840762 139946397853440 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.5025099515914917, loss=3.1120400428771973
I0205 06:40:54.232566 139946414638848 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.5619871616363525, loss=3.225552797317505
I0205 06:41:40.592285 139946397853440 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.4171525239944458, loss=3.089491844177246
I0205 06:42:27.045676 139946414638848 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.5032235383987427, loss=3.228627920150757
I0205 06:43:13.490180 139946397853440 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.619543194770813, loss=3.147207498550415
I0205 06:43:59.775408 139946414638848 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.5431231260299683, loss=3.0715227127075195
I0205 06:44:46.238740 139946397853440 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.6283347606658936, loss=3.109917402267456
I0205 06:45:32.736726 139946414638848 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.839640498161316, loss=3.170410394668579
I0205 06:45:55.920632 140107197974336 spec.py:321] Evaluating on the training split.
I0205 06:46:06.819243 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 06:46:33.413612 140107197974336 spec.py:349] Evaluating on the test split.
I0205 06:46:35.005742 140107197974336 submission_runner.py:408] Time since start: 57224.75s, 	Step: 113652, 	{'train/accuracy': 0.810351550579071, 'train/loss': 0.9742421507835388, 'validation/accuracy': 0.7343800067901611, 'validation/loss': 1.293757438659668, 'validation/num_examples': 50000, 'test/accuracy': 0.6139000058174133, 'test/loss': 1.887000322341919, 'test/num_examples': 10000, 'score': 52134.493783950806, 'total_duration': 57224.7523059845, 'accumulated_submission_time': 52134.493783950806, 'accumulated_eval_time': 5079.130994319916, 'accumulated_logging_time': 4.565948486328125}
I0205 06:46:35.041521 139946397853440 logging_writer.py:48] [113652] accumulated_eval_time=5079.130994, accumulated_logging_time=4.565948, accumulated_submission_time=52134.493784, global_step=113652, preemption_count=0, score=52134.493784, test/accuracy=0.613900, test/loss=1.887000, test/num_examples=10000, total_duration=57224.752306, train/accuracy=0.810352, train/loss=0.974242, validation/accuracy=0.734380, validation/loss=1.293757, validation/num_examples=50000
I0205 06:46:54.449923 139946414638848 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.5177701711654663, loss=3.1611547470092773
I0205 06:47:39.011854 139946397853440 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.6531182527542114, loss=3.5132522583007812
I0205 06:48:25.495347 139946414638848 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.5167096853256226, loss=3.1649813652038574
I0205 06:49:11.866652 139946397853440 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.7029166221618652, loss=4.510452747344971
I0205 06:49:58.205159 139946414638848 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.8132586479187012, loss=4.65081787109375
I0205 06:50:44.549458 139946397853440 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.519562005996704, loss=4.124167442321777
I0205 06:51:30.885407 139946414638848 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.6347801685333252, loss=3.2141945362091064
I0205 06:52:17.451830 139946397853440 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.7720613479614258, loss=4.649519920349121
I0205 06:53:03.757778 139946414638848 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.5937342643737793, loss=3.1133477687835693
I0205 06:53:35.385200 140107197974336 spec.py:321] Evaluating on the training split.
I0205 06:53:46.083796 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 06:54:18.539661 140107197974336 spec.py:349] Evaluating on the test split.
I0205 06:54:20.144449 140107197974336 submission_runner.py:408] Time since start: 57689.89s, 	Step: 114570, 	{'train/accuracy': 0.8231640458106995, 'train/loss': 0.8864479660987854, 'validation/accuracy': 0.7347399592399597, 'validation/loss': 1.2553902864456177, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.8648344278335571, 'test/num_examples': 10000, 'score': 52554.772089481354, 'total_duration': 57689.89100813866, 'accumulated_submission_time': 52554.772089481354, 'accumulated_eval_time': 5123.890210866928, 'accumulated_logging_time': 4.613940000534058}
I0205 06:54:20.182624 139946397853440 logging_writer.py:48] [114570] accumulated_eval_time=5123.890211, accumulated_logging_time=4.613940, accumulated_submission_time=52554.772089, global_step=114570, preemption_count=0, score=52554.772089, test/accuracy=0.609600, test/loss=1.864834, test/num_examples=10000, total_duration=57689.891008, train/accuracy=0.823164, train/loss=0.886448, validation/accuracy=0.734740, validation/loss=1.255390, validation/num_examples=50000
I0205 06:54:32.357216 139946414638848 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.5095031261444092, loss=3.058326005935669
I0205 06:55:15.723003 139946397853440 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.5673967599868774, loss=3.183166027069092
I0205 06:56:02.122311 139946414638848 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.5472649335861206, loss=3.129364013671875
I0205 06:56:48.747263 139946397853440 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.524397373199463, loss=3.1273508071899414
I0205 06:57:34.978818 139946414638848 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.5726960897445679, loss=4.247208595275879
I0205 06:58:21.779670 139946397853440 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.647710919380188, loss=3.1523020267486572
I0205 06:59:08.269356 139946414638848 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.6236329078674316, loss=4.417576789855957
I0205 06:59:54.830961 139946397853440 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.5510205030441284, loss=3.21705961227417
I0205 07:00:41.054946 139946414638848 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.504267930984497, loss=3.6829071044921875
I0205 07:01:20.341385 140107197974336 spec.py:321] Evaluating on the training split.
I0205 07:01:30.999516 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 07:02:00.699206 140107197974336 spec.py:349] Evaluating on the test split.
I0205 07:02:02.309629 140107197974336 submission_runner.py:408] Time since start: 58152.06s, 	Step: 115486, 	{'train/accuracy': 0.8078905940055847, 'train/loss': 0.9823316931724548, 'validation/accuracy': 0.7350599765777588, 'validation/loss': 1.3003755807876587, 'validation/num_examples': 50000, 'test/accuracy': 0.6133000254631042, 'test/loss': 1.8954529762268066, 'test/num_examples': 10000, 'score': 52974.86817359924, 'total_duration': 58152.056190013885, 'accumulated_submission_time': 52974.86817359924, 'accumulated_eval_time': 5165.858438968658, 'accumulated_logging_time': 4.662526845932007}
I0205 07:02:02.350344 139946397853440 logging_writer.py:48] [115486] accumulated_eval_time=5165.858439, accumulated_logging_time=4.662527, accumulated_submission_time=52974.868174, global_step=115486, preemption_count=0, score=52974.868174, test/accuracy=0.613300, test/loss=1.895453, test/num_examples=10000, total_duration=58152.056190, train/accuracy=0.807891, train/loss=0.982332, validation/accuracy=0.735060, validation/loss=1.300376, validation/num_examples=50000
I0205 07:02:08.253520 139946414638848 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.5395766496658325, loss=3.453141689300537
I0205 07:02:50.543994 139946397853440 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.519423007965088, loss=4.009910583496094
I0205 07:03:36.958097 139946414638848 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.628909945487976, loss=3.0702755451202393
I0205 07:04:23.597156 139946397853440 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.4582843780517578, loss=3.9085021018981934
I0205 07:05:10.091830 139946414638848 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.521005392074585, loss=4.003437042236328
I0205 07:05:56.463176 139946397853440 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.6884653568267822, loss=3.113612413406372
I0205 07:06:42.705934 139946414638848 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.5175234079360962, loss=4.010399341583252
I0205 07:07:29.142825 139946397853440 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.6748850345611572, loss=3.129978656768799
I0205 07:08:15.554462 139946414638848 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.5626775026321411, loss=3.1161386966705322
I0205 07:09:02.118611 139946397853440 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.585509181022644, loss=3.150134801864624
I0205 07:09:02.729678 140107197974336 spec.py:321] Evaluating on the training split.
I0205 07:09:13.291614 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 07:09:44.378724 140107197974336 spec.py:349] Evaluating on the test split.
I0205 07:09:45.968193 140107197974336 submission_runner.py:408] Time since start: 58615.71s, 	Step: 116403, 	{'train/accuracy': 0.8154101371765137, 'train/loss': 0.9408923983573914, 'validation/accuracy': 0.738099992275238, 'validation/loss': 1.263514518737793, 'validation/num_examples': 50000, 'test/accuracy': 0.612500011920929, 'test/loss': 1.8793542385101318, 'test/num_examples': 10000, 'score': 53395.18107008934, 'total_duration': 58615.714760541916, 'accumulated_submission_time': 53395.18107008934, 'accumulated_eval_time': 5209.096930503845, 'accumulated_logging_time': 4.717260360717773}
I0205 07:09:46.006113 139946414638848 logging_writer.py:48] [116403] accumulated_eval_time=5209.096931, accumulated_logging_time=4.717260, accumulated_submission_time=53395.181070, global_step=116403, preemption_count=0, score=53395.181070, test/accuracy=0.612500, test/loss=1.879354, test/num_examples=10000, total_duration=58615.714761, train/accuracy=0.815410, train/loss=0.940892, validation/accuracy=0.738100, validation/loss=1.263515, validation/num_examples=50000
I0205 07:10:26.478716 139946397853440 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.5811388492584229, loss=3.145974636077881
I0205 07:11:12.716457 139946414638848 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.4552785158157349, loss=3.7885820865631104
I0205 07:11:59.064859 139946397853440 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.619575023651123, loss=3.1488804817199707
I0205 07:12:45.286644 139946414638848 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.5403132438659668, loss=3.0650792121887207
I0205 07:13:31.578873 139946397853440 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.625870943069458, loss=3.1286654472351074
I0205 07:14:17.905734 139946414638848 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.5243295431137085, loss=3.8596575260162354
I0205 07:15:04.546297 139946397853440 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.7528562545776367, loss=4.491145610809326
I0205 07:15:50.867290 139946414638848 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.7691946029663086, loss=4.479726791381836
I0205 07:16:37.144716 139946397853440 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.6326416730880737, loss=3.2189273834228516
I0205 07:16:45.997893 140107197974336 spec.py:321] Evaluating on the training split.
I0205 07:16:56.835437 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 07:17:25.134873 140107197974336 spec.py:349] Evaluating on the test split.
I0205 07:17:26.737036 140107197974336 submission_runner.py:408] Time since start: 59076.48s, 	Step: 117321, 	{'train/accuracy': 0.8239257335662842, 'train/loss': 0.8793371915817261, 'validation/accuracy': 0.7392399907112122, 'validation/loss': 1.2323999404907227, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.8169059753417969, 'test/num_examples': 10000, 'score': 53815.10822582245, 'total_duration': 59076.4835767746, 'accumulated_submission_time': 53815.10822582245, 'accumulated_eval_time': 5249.836025476456, 'accumulated_logging_time': 4.766160011291504}
I0205 07:17:26.780465 139946414638848 logging_writer.py:48] [117321] accumulated_eval_time=5249.836025, accumulated_logging_time=4.766160, accumulated_submission_time=53815.108226, global_step=117321, preemption_count=0, score=53815.108226, test/accuracy=0.623200, test/loss=1.816906, test/num_examples=10000, total_duration=59076.483577, train/accuracy=0.823926, train/loss=0.879337, validation/accuracy=0.739240, validation/loss=1.232400, validation/num_examples=50000
I0205 07:17:58.921803 139946397853440 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.736938238143921, loss=3.1737821102142334
I0205 07:18:45.096780 139946414638848 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.727675437927246, loss=3.131434917449951
I0205 07:19:34.185082 139946397853440 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.8539093732833862, loss=4.689804553985596
I0205 07:20:38.053766 139946414638848 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.5600842237472534, loss=3.0724735260009766
I0205 07:21:24.239297 139946397853440 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.5768605470657349, loss=3.1558542251586914
I0205 07:22:10.803632 139946414638848 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.7111316919326782, loss=4.599474906921387
I0205 07:22:56.914283 139946397853440 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.5766175985336304, loss=3.145524501800537
I0205 07:23:43.609959 139946414638848 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.6226086616516113, loss=3.234515428543091
I0205 07:24:27.130472 140107197974336 spec.py:321] Evaluating on the training split.
I0205 07:24:37.837846 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 07:25:08.163885 140107197974336 spec.py:349] Evaluating on the test split.
I0205 07:25:09.759889 140107197974336 submission_runner.py:408] Time since start: 59539.51s, 	Step: 118195, 	{'train/accuracy': 0.8167187571525574, 'train/loss': 0.927771270275116, 'validation/accuracy': 0.7392599582672119, 'validation/loss': 1.2517048120498657, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.852914571762085, 'test/num_examples': 10000, 'score': 54235.39735746384, 'total_duration': 59539.506457567215, 'accumulated_submission_time': 54235.39735746384, 'accumulated_eval_time': 5292.465433597565, 'accumulated_logging_time': 4.820432662963867}
I0205 07:25:09.794590 139946397853440 logging_writer.py:48] [118195] accumulated_eval_time=5292.465434, accumulated_logging_time=4.820433, accumulated_submission_time=54235.397357, global_step=118195, preemption_count=0, score=54235.397357, test/accuracy=0.617300, test/loss=1.852915, test/num_examples=10000, total_duration=59539.506458, train/accuracy=0.816719, train/loss=0.927771, validation/accuracy=0.739260, validation/loss=1.251705, validation/num_examples=50000
I0205 07:25:12.162351 139946414638848 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.042717218399048, loss=4.861974239349365
I0205 07:25:53.892362 139946397853440 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.6542131900787354, loss=4.486687660217285
I0205 07:26:40.190357 139946414638848 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.5804082155227661, loss=3.0563907623291016
I0205 07:27:26.749876 139946397853440 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.4959962368011475, loss=3.2168707847595215
I0205 07:28:13.095553 139946414638848 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.6472526788711548, loss=3.069183349609375
I0205 07:28:59.419299 139946397853440 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.6099544763565063, loss=3.297126054763794
I0205 07:29:45.731286 139946414638848 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.817139744758606, loss=4.581282138824463
I0205 07:30:32.486680 139946397853440 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.5782729387283325, loss=3.4791955947875977
I0205 07:31:18.977170 139946414638848 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.6397863626480103, loss=3.952570676803589
I0205 07:32:05.558530 139946397853440 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.7333142757415771, loss=3.0692591667175293
I0205 07:32:09.797599 140107197974336 spec.py:321] Evaluating on the training split.
I0205 07:32:20.779545 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 07:32:47.923598 140107197974336 spec.py:349] Evaluating on the test split.
I0205 07:32:49.526904 140107197974336 submission_runner.py:408] Time since start: 59999.27s, 	Step: 119111, 	{'train/accuracy': 0.8149804472923279, 'train/loss': 0.9616518020629883, 'validation/accuracy': 0.7415800094604492, 'validation/loss': 1.2812042236328125, 'validation/num_examples': 50000, 'test/accuracy': 0.6168000102043152, 'test/loss': 1.890485405921936, 'test/num_examples': 10000, 'score': 54655.33577847481, 'total_duration': 59999.27346301079, 'accumulated_submission_time': 54655.33577847481, 'accumulated_eval_time': 5332.194699764252, 'accumulated_logging_time': 4.867582082748413}
I0205 07:32:49.561284 139946414638848 logging_writer.py:48] [119111] accumulated_eval_time=5332.194700, accumulated_logging_time=4.867582, accumulated_submission_time=54655.335778, global_step=119111, preemption_count=0, score=54655.335778, test/accuracy=0.616800, test/loss=1.890485, test/num_examples=10000, total_duration=59999.273463, train/accuracy=0.814980, train/loss=0.961652, validation/accuracy=0.741580, validation/loss=1.281204, validation/num_examples=50000
I0205 07:33:26.230519 139946397853440 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.6614412069320679, loss=3.2077248096466064
I0205 07:34:12.721220 139946414638848 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.593636155128479, loss=3.2888383865356445
I0205 07:34:59.592260 139946397853440 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.5516549348831177, loss=3.4195404052734375
I0205 07:35:46.227981 139946414638848 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.7745720148086548, loss=3.0810723304748535
I0205 07:36:32.811088 139946397853440 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.5051167011260986, loss=3.589348554611206
I0205 07:37:19.377943 139946414638848 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.627022624015808, loss=4.372250556945801
I0205 07:38:05.984691 139946397853440 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.6259204149246216, loss=3.0497212409973145
I0205 07:38:52.488744 139946414638848 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.7694616317749023, loss=3.1721599102020264
I0205 07:39:39.212870 139946397853440 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.5650285482406616, loss=3.0197296142578125
I0205 07:39:49.554910 140107197974336 spec.py:321] Evaluating on the training split.
I0205 07:40:00.915186 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 07:40:31.278672 140107197974336 spec.py:349] Evaluating on the test split.
I0205 07:40:32.872295 140107197974336 submission_runner.py:408] Time since start: 60462.62s, 	Step: 120024, 	{'train/accuracy': 0.8238866925239563, 'train/loss': 0.8949841856956482, 'validation/accuracy': 0.7396199703216553, 'validation/loss': 1.2500559091567993, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.8545596599578857, 'test/num_examples': 10000, 'score': 55075.26651906967, 'total_duration': 60462.61881303787, 'accumulated_submission_time': 55075.26651906967, 'accumulated_eval_time': 5375.512019634247, 'accumulated_logging_time': 4.913069009780884}
I0205 07:40:32.910783 139946414638848 logging_writer.py:48] [120024] accumulated_eval_time=5375.512020, accumulated_logging_time=4.913069, accumulated_submission_time=55075.266519, global_step=120024, preemption_count=0, score=55075.266519, test/accuracy=0.617300, test/loss=1.854560, test/num_examples=10000, total_duration=60462.618813, train/accuracy=0.823887, train/loss=0.894984, validation/accuracy=0.739620, validation/loss=1.250056, validation/num_examples=50000
I0205 07:41:04.114797 139946397853440 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.553010106086731, loss=3.8317599296569824
I0205 07:41:49.864628 139946414638848 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.5820791721343994, loss=3.0742132663726807
I0205 07:42:36.244352 139946397853440 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.589592456817627, loss=4.187152862548828
I0205 07:43:22.949607 139946414638848 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.5595886707305908, loss=3.6936492919921875
I0205 07:44:09.255331 139946397853440 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.6778508424758911, loss=3.0710458755493164
I0205 07:44:55.841822 139946414638848 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.6394556760787964, loss=3.740872383117676
I0205 07:45:42.262887 139946397853440 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.5755761861801147, loss=3.093183755874634
I0205 07:46:28.976210 139946414638848 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.685794711112976, loss=3.790010452270508
I0205 07:47:15.505739 139946397853440 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.866111397743225, loss=4.712045669555664
I0205 07:47:33.252062 140107197974336 spec.py:321] Evaluating on the training split.
I0205 07:47:44.013559 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 07:48:11.091434 140107197974336 spec.py:349] Evaluating on the test split.
I0205 07:48:12.690144 140107197974336 submission_runner.py:408] Time since start: 60922.44s, 	Step: 120940, 	{'train/accuracy': 0.8238281011581421, 'train/loss': 0.8898305296897888, 'validation/accuracy': 0.7408599853515625, 'validation/loss': 1.232032060623169, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.833022952079773, 'test/num_examples': 10000, 'score': 55495.54484438896, 'total_duration': 60922.43671345711, 'accumulated_submission_time': 55495.54484438896, 'accumulated_eval_time': 5414.950091123581, 'accumulated_logging_time': 4.961091995239258}
I0205 07:48:12.724848 139946414638848 logging_writer.py:48] [120940] accumulated_eval_time=5414.950091, accumulated_logging_time=4.961092, accumulated_submission_time=55495.544844, global_step=120940, preemption_count=0, score=55495.544844, test/accuracy=0.623000, test/loss=1.833023, test/num_examples=10000, total_duration=60922.436713, train/accuracy=0.823828, train/loss=0.889831, validation/accuracy=0.740860, validation/loss=1.232032, validation/num_examples=50000
I0205 07:48:36.711367 139946397853440 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.7637126445770264, loss=3.12271785736084
I0205 07:49:22.054955 139946414638848 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.7017658948898315, loss=3.17903470993042
I0205 07:50:08.823428 139946397853440 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.7193882465362549, loss=3.150287628173828
I0205 07:50:55.285760 139946414638848 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.642316222190857, loss=3.7189583778381348
I0205 07:51:41.952318 139946397853440 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.7259695529937744, loss=3.119143486022949
I0205 07:52:28.340498 139946414638848 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.7244092226028442, loss=3.1502797603607178
I0205 07:53:14.556382 139946397853440 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.6236783266067505, loss=3.9722023010253906
I0205 07:54:01.085857 139946414638848 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.5636348724365234, loss=3.0523927211761475
I0205 07:54:47.737317 139946397853440 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.679965853691101, loss=3.116774559020996
I0205 07:55:12.953466 140107197974336 spec.py:321] Evaluating on the training split.
I0205 07:55:23.535809 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 07:55:53.441976 140107197974336 spec.py:349] Evaluating on the test split.
I0205 07:55:55.040908 140107197974336 submission_runner.py:408] Time since start: 61384.79s, 	Step: 121856, 	{'train/accuracy': 0.8225390315055847, 'train/loss': 0.8875148296356201, 'validation/accuracy': 0.744879961013794, 'validation/loss': 1.2177189588546753, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.8269776105880737, 'test/num_examples': 10000, 'score': 55915.70909833908, 'total_duration': 61384.78747153282, 'accumulated_submission_time': 55915.70909833908, 'accumulated_eval_time': 5457.037507534027, 'accumulated_logging_time': 5.007095813751221}
I0205 07:55:55.076939 139946414638848 logging_writer.py:48] [121856] accumulated_eval_time=5457.037508, accumulated_logging_time=5.007096, accumulated_submission_time=55915.709098, global_step=121856, preemption_count=0, score=55915.709098, test/accuracy=0.618600, test/loss=1.826978, test/num_examples=10000, total_duration=61384.787472, train/accuracy=0.822539, train/loss=0.887515, validation/accuracy=0.744880, validation/loss=1.217719, validation/num_examples=50000
I0205 07:56:12.767710 139946397853440 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.7461615800857544, loss=4.296030521392822
I0205 07:56:56.921552 139946414638848 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.9908908605575562, loss=4.684297561645508
I0205 07:57:43.561694 139946397853440 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.6586613655090332, loss=3.0720107555389404
I0205 07:58:30.005881 139946414638848 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.571386456489563, loss=3.4516327381134033
I0205 07:59:16.591884 139946397853440 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.6463239192962646, loss=3.096439838409424
I0205 08:00:02.925898 139946414638848 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.4680968523025513, loss=3.3293182849884033
I0205 08:00:49.089131 139946397853440 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.7128245830535889, loss=3.1495447158813477
I0205 08:01:35.676103 139946414638848 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.685173511505127, loss=3.1299290657043457
I0205 08:02:22.301141 139946397853440 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.6541173458099365, loss=3.0424106121063232
I0205 08:02:55.464374 140107197974336 spec.py:321] Evaluating on the training split.
I0205 08:03:06.173578 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 08:03:38.675865 140107197974336 spec.py:349] Evaluating on the test split.
I0205 08:03:40.269421 140107197974336 submission_runner.py:408] Time since start: 61850.02s, 	Step: 122773, 	{'train/accuracy': 0.82582026720047, 'train/loss': 0.8763144016265869, 'validation/accuracy': 0.7443599700927734, 'validation/loss': 1.2213648557662964, 'validation/num_examples': 50000, 'test/accuracy': 0.6287000179290771, 'test/loss': 1.8050850629806519, 'test/num_examples': 10000, 'score': 56336.03149437904, 'total_duration': 61850.01596212387, 'accumulated_submission_time': 56336.03149437904, 'accumulated_eval_time': 5501.842509508133, 'accumulated_logging_time': 5.055784463882446}
I0205 08:03:40.314768 139946414638848 logging_writer.py:48] [122773] accumulated_eval_time=5501.842510, accumulated_logging_time=5.055784, accumulated_submission_time=56336.031494, global_step=122773, preemption_count=0, score=56336.031494, test/accuracy=0.628700, test/loss=1.805085, test/num_examples=10000, total_duration=61850.015962, train/accuracy=0.825820, train/loss=0.876314, validation/accuracy=0.744360, validation/loss=1.221365, validation/num_examples=50000
I0205 08:03:51.327587 139946397853440 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.7082525491714478, loss=3.1914467811584473
I0205 08:04:34.551181 139946414638848 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.5623087882995605, loss=3.493889331817627
I0205 08:05:21.011193 139946397853440 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.7314763069152832, loss=3.430891275405884
I0205 08:06:07.555086 139946414638848 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.4865062236785889, loss=3.3806004524230957
I0205 08:06:53.778230 139946397853440 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.6758906841278076, loss=3.071528434753418
I0205 08:07:40.460910 139946414638848 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.591408371925354, loss=3.091702461242676
I0205 08:08:26.999885 139946397853440 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.7875096797943115, loss=3.0474534034729004
I0205 08:09:13.334749 139946414638848 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.6717935800552368, loss=3.0972793102264404
I0205 08:09:59.563731 139946397853440 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.6091200113296509, loss=3.232851505279541
I0205 08:10:40.702644 140107197974336 spec.py:321] Evaluating on the training split.
I0205 08:10:51.296597 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 08:11:18.273267 140107197974336 spec.py:349] Evaluating on the test split.
I0205 08:11:19.877765 140107197974336 submission_runner.py:408] Time since start: 62309.62s, 	Step: 123690, 	{'train/accuracy': 0.8439843654632568, 'train/loss': 0.8367078900337219, 'validation/accuracy': 0.744159996509552, 'validation/loss': 1.2481415271759033, 'validation/num_examples': 50000, 'test/accuracy': 0.6236000061035156, 'test/loss': 1.8550283908843994, 'test/num_examples': 10000, 'score': 56756.35433626175, 'total_duration': 62309.62432742119, 'accumulated_submission_time': 56756.35433626175, 'accumulated_eval_time': 5541.017600536346, 'accumulated_logging_time': 5.112732887268066}
I0205 08:11:19.915443 139946414638848 logging_writer.py:48] [123690] accumulated_eval_time=5541.017601, accumulated_logging_time=5.112733, accumulated_submission_time=56756.354336, global_step=123690, preemption_count=0, score=56756.354336, test/accuracy=0.623600, test/loss=1.855028, test/num_examples=10000, total_duration=62309.624327, train/accuracy=0.843984, train/loss=0.836708, validation/accuracy=0.744160, validation/loss=1.248142, validation/num_examples=50000
I0205 08:11:24.248120 139946397853440 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.5183240175247192, loss=2.9749343395233154
I0205 08:12:06.481653 139946414638848 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.820155143737793, loss=3.081845760345459
I0205 08:12:52.986865 139946397853440 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.7958295345306396, loss=3.2563352584838867
I0205 08:13:39.213633 139946414638848 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.5826886892318726, loss=3.0935633182525635
I0205 08:14:25.642134 139946397853440 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.7867549657821655, loss=3.1020541191101074
I0205 08:15:12.052652 139946414638848 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.7314887046813965, loss=3.0981101989746094
I0205 08:15:58.683261 139946397853440 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.5379277467727661, loss=3.3371741771698
I0205 08:16:45.140260 139946414638848 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.7152196168899536, loss=3.1137237548828125
I0205 08:17:31.708780 139946397853440 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.63462233543396, loss=3.333080530166626
I0205 08:18:18.276564 139946414638848 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.9604061841964722, loss=4.647733211517334
I0205 08:18:20.275231 140107197974336 spec.py:321] Evaluating on the training split.
I0205 08:18:30.967944 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 08:18:59.803824 140107197974336 spec.py:349] Evaluating on the test split.
I0205 08:19:01.413427 140107197974336 submission_runner.py:408] Time since start: 62771.16s, 	Step: 124606, 	{'train/accuracy': 0.8265038728713989, 'train/loss': 0.8778988718986511, 'validation/accuracy': 0.7480599880218506, 'validation/loss': 1.2074445486068726, 'validation/num_examples': 50000, 'test/accuracy': 0.6204000115394592, 'test/loss': 1.806689739227295, 'test/num_examples': 10000, 'score': 57176.65181708336, 'total_duration': 62771.15999698639, 'accumulated_submission_time': 57176.65181708336, 'accumulated_eval_time': 5582.155804157257, 'accumulated_logging_time': 5.160711050033569}
I0205 08:19:01.447701 139946397853440 logging_writer.py:48] [124606] accumulated_eval_time=5582.155804, accumulated_logging_time=5.160711, accumulated_submission_time=57176.651817, global_step=124606, preemption_count=0, score=57176.651817, test/accuracy=0.620400, test/loss=1.806690, test/num_examples=10000, total_duration=62771.159997, train/accuracy=0.826504, train/loss=0.877899, validation/accuracy=0.748060, validation/loss=1.207445, validation/num_examples=50000
I0205 08:19:40.502484 139946414638848 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.7582036256790161, loss=3.2861058712005615
I0205 08:20:26.777514 139946397853440 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.6469645500183105, loss=3.017051935195923
I0205 08:21:13.378858 139946414638848 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.6593763828277588, loss=3.731058359146118
I0205 08:21:59.501626 139946397853440 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.7102928161621094, loss=3.1073431968688965
I0205 08:22:45.914065 139946414638848 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.575074553489685, loss=2.971038341522217
I0205 08:23:32.107337 139946397853440 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.630204200744629, loss=3.1631362438201904
I0205 08:24:18.295289 139946414638848 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.5942578315734863, loss=3.0391178131103516
I0205 08:25:04.778794 139946397853440 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.7883878946304321, loss=4.52293586730957
I0205 08:25:50.905888 139946414638848 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.715972661972046, loss=3.081434726715088
I0205 08:26:01.692299 140107197974336 spec.py:321] Evaluating on the training split.
I0205 08:26:12.583896 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 08:26:44.356702 140107197974336 spec.py:349] Evaluating on the test split.
I0205 08:26:45.958719 140107197974336 submission_runner.py:408] Time since start: 63235.71s, 	Step: 125525, 	{'train/accuracy': 0.8316210508346558, 'train/loss': 0.8772971630096436, 'validation/accuracy': 0.7468799948692322, 'validation/loss': 1.236533522605896, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.8307219743728638, 'test/num_examples': 10000, 'score': 57596.83524441719, 'total_duration': 63235.70528793335, 'accumulated_submission_time': 57596.83524441719, 'accumulated_eval_time': 5626.422222137451, 'accumulated_logging_time': 5.2043843269348145}
I0205 08:26:45.996802 139946397853440 logging_writer.py:48] [125525] accumulated_eval_time=5626.422222, accumulated_logging_time=5.204384, accumulated_submission_time=57596.835244, global_step=125525, preemption_count=0, score=57596.835244, test/accuracy=0.623200, test/loss=1.830722, test/num_examples=10000, total_duration=63235.705288, train/accuracy=0.831621, train/loss=0.877297, validation/accuracy=0.746880, validation/loss=1.236534, validation/num_examples=50000
I0205 08:27:16.386120 139946414638848 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.6204619407653809, loss=3.6565024852752686
I0205 08:28:02.623801 139946397853440 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.7113856077194214, loss=3.084481954574585
I0205 08:28:49.152422 139946414638848 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.6821041107177734, loss=3.143138885498047
I0205 08:29:35.363925 139946397853440 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.7198814153671265, loss=3.622652530670166
I0205 08:30:21.719211 139946414638848 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.7356648445129395, loss=4.003878116607666
I0205 08:31:08.108099 139946397853440 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.906724452972412, loss=3.047149896621704
I0205 08:31:54.688372 139946414638848 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.677065372467041, loss=2.9950180053710938
I0205 08:32:41.131222 139946397853440 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.751130223274231, loss=4.461933612823486
I0205 08:33:27.628435 139946414638848 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.580830454826355, loss=3.412181854248047
I0205 08:33:46.104305 140107197974336 spec.py:321] Evaluating on the training split.
I0205 08:33:56.666226 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 08:34:28.827276 140107197974336 spec.py:349] Evaluating on the test split.
I0205 08:34:30.422992 140107197974336 submission_runner.py:408] Time since start: 63700.17s, 	Step: 126442, 	{'train/accuracy': 0.84095698595047, 'train/loss': 0.8439698815345764, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.2318177223205566, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8440407514572144, 'test/num_examples': 10000, 'score': 58016.88030552864, 'total_duration': 63700.169562101364, 'accumulated_submission_time': 58016.88030552864, 'accumulated_eval_time': 5670.740884780884, 'accumulated_logging_time': 5.2526023387908936}
I0205 08:34:30.460313 139946397853440 logging_writer.py:48] [126442] accumulated_eval_time=5670.740885, accumulated_logging_time=5.252602, accumulated_submission_time=58016.880306, global_step=126442, preemption_count=0, score=58016.880306, test/accuracy=0.627100, test/loss=1.844041, test/num_examples=10000, total_duration=63700.169562, train/accuracy=0.840957, train/loss=0.843970, validation/accuracy=0.747060, validation/loss=1.231818, validation/num_examples=50000
I0205 08:34:53.650071 139946414638848 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.9681459665298462, loss=4.677769184112549
I0205 08:35:38.762479 139946397853440 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.7776710987091064, loss=4.105634689331055
I0205 08:36:25.455820 139946414638848 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.6068745851516724, loss=3.155252456665039
I0205 08:37:11.910992 139946397853440 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.6331417560577393, loss=3.1595983505249023
I0205 08:37:58.246918 139946414638848 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.692130446434021, loss=3.013918876647949
I0205 08:38:44.655451 139946397853440 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.610089898109436, loss=3.252854347229004
I0205 08:39:31.306221 139946414638848 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.623617172241211, loss=3.943279266357422
I0205 08:40:17.678096 139946397853440 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.9388840198516846, loss=4.5355634689331055
I0205 08:41:04.166740 139946414638848 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.7501850128173828, loss=2.992096185684204
I0205 08:41:30.751152 140107197974336 spec.py:321] Evaluating on the training split.
I0205 08:41:42.577317 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 08:42:07.760735 140107197974336 spec.py:349] Evaluating on the test split.
I0205 08:42:09.355467 140107197974336 submission_runner.py:408] Time since start: 64159.10s, 	Step: 127359, 	{'train/accuracy': 0.8307226300239563, 'train/loss': 0.8531256914138794, 'validation/accuracy': 0.7465199828147888, 'validation/loss': 1.2010209560394287, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.790854811668396, 'test/num_examples': 10000, 'score': 58437.10874581337, 'total_duration': 64159.10203433037, 'accumulated_submission_time': 58437.10874581337, 'accumulated_eval_time': 5709.3451907634735, 'accumulated_logging_time': 5.299185037612915}
I0205 08:42:09.390742 139946397853440 logging_writer.py:48] [127359] accumulated_eval_time=5709.345191, accumulated_logging_time=5.299185, accumulated_submission_time=58437.108746, global_step=127359, preemption_count=0, score=58437.108746, test/accuracy=0.627900, test/loss=1.790855, test/num_examples=10000, total_duration=64159.102034, train/accuracy=0.830723, train/loss=0.853126, validation/accuracy=0.746520, validation/loss=1.201021, validation/num_examples=50000
I0205 08:42:25.908829 139946414638848 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.7134132385253906, loss=3.3256077766418457
I0205 08:43:10.090847 139946397853440 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.9507309198379517, loss=4.282931327819824
I0205 08:43:56.638258 139946414638848 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.6418992280960083, loss=3.0492942333221436
I0205 08:44:43.199562 139946397853440 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.7336411476135254, loss=3.4104232788085938
I0205 08:45:29.614711 139946414638848 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.686029314994812, loss=3.211979389190674
I0205 08:46:16.221649 139946397853440 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.7523525953292847, loss=3.0329630374908447
I0205 08:47:02.551874 139946414638848 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.9892412424087524, loss=4.176102161407471
I0205 08:47:48.889953 139946397853440 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.7010136842727661, loss=3.7771081924438477
I0205 08:48:35.539809 139946414638848 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.9973701238632202, loss=4.477629661560059
I0205 08:49:09.491334 140107197974336 spec.py:321] Evaluating on the training split.
I0205 08:49:20.142066 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 08:49:50.431324 140107197974336 spec.py:349] Evaluating on the test split.
I0205 08:49:52.029687 140107197974336 submission_runner.py:408] Time since start: 64621.78s, 	Step: 128275, 	{'train/accuracy': 0.8322070240974426, 'train/loss': 0.8553465604782104, 'validation/accuracy': 0.7501800060272217, 'validation/loss': 1.1990872621536255, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.7960751056671143, 'test/num_examples': 10000, 'score': 58857.14763689041, 'total_duration': 64621.77625489235, 'accumulated_submission_time': 58857.14763689041, 'accumulated_eval_time': 5751.883533239365, 'accumulated_logging_time': 5.343764781951904}
I0205 08:49:52.070353 139946397853440 logging_writer.py:48] [128275] accumulated_eval_time=5751.883533, accumulated_logging_time=5.343765, accumulated_submission_time=58857.147637, global_step=128275, preemption_count=0, score=58857.147637, test/accuracy=0.630400, test/loss=1.796075, test/num_examples=10000, total_duration=64621.776255, train/accuracy=0.832207, train/loss=0.855347, validation/accuracy=0.750180, validation/loss=1.199087, validation/num_examples=50000
I0205 08:50:02.301065 139946414638848 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.0665910243988037, loss=4.653328895568848
I0205 08:50:45.410326 139946397853440 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.734925627708435, loss=3.2084386348724365
I0205 08:51:31.789706 139946414638848 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.7428478002548218, loss=2.9849019050598145
I0205 08:52:18.745320 139946397853440 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.6714355945587158, loss=3.395677089691162
I0205 08:53:05.340543 139946414638848 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.7514816522598267, loss=3.3724348545074463
I0205 08:53:51.926419 139946397853440 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.759448766708374, loss=3.0781612396240234
I0205 08:54:38.624837 139946414638848 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.709120750427246, loss=3.0021166801452637
I0205 08:55:25.025783 139946397853440 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.8008813858032227, loss=2.990107536315918
I0205 08:56:11.653734 139946414638848 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.0109188556671143, loss=4.588351249694824
I0205 08:56:52.289285 140107197974336 spec.py:321] Evaluating on the training split.
I0205 08:57:03.317307 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 08:57:34.807473 140107197974336 spec.py:349] Evaluating on the test split.
I0205 08:57:36.418494 140107197974336 submission_runner.py:408] Time since start: 65086.17s, 	Step: 129189, 	{'train/accuracy': 0.8402343392372131, 'train/loss': 0.8571125864982605, 'validation/accuracy': 0.7488999962806702, 'validation/loss': 1.232455849647522, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.829653024673462, 'test/num_examples': 10000, 'score': 59277.302680015564, 'total_duration': 65086.165039777756, 'accumulated_submission_time': 59277.302680015564, 'accumulated_eval_time': 5796.012695074081, 'accumulated_logging_time': 5.3958094120025635}
I0205 08:57:36.460698 139946397853440 logging_writer.py:48] [129189] accumulated_eval_time=5796.012695, accumulated_logging_time=5.395809, accumulated_submission_time=59277.302680, global_step=129189, preemption_count=0, score=59277.302680, test/accuracy=0.627900, test/loss=1.829653, test/num_examples=10000, total_duration=65086.165040, train/accuracy=0.840234, train/loss=0.857113, validation/accuracy=0.748900, validation/loss=1.232456, validation/num_examples=50000
I0205 08:57:41.186402 139946414638848 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.804228663444519, loss=3.2321157455444336
I0205 08:58:23.442619 139946397853440 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.9323610067367554, loss=3.9391579627990723
I0205 08:59:09.663819 139946414638848 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.7467387914657593, loss=3.0018129348754883
I0205 08:59:56.321788 139946397853440 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.164249897003174, loss=4.560457229614258
I0205 09:00:42.465953 139946414638848 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.8824422359466553, loss=3.0731313228607178
I0205 09:01:28.932461 139946397853440 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.8297619819641113, loss=3.0857505798339844
I0205 09:02:15.217163 139946414638848 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.8193458318710327, loss=3.092635154724121
I0205 09:03:01.509206 139946397853440 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.7171059846878052, loss=2.986276149749756
I0205 09:03:48.164420 139946414638848 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.7029290199279785, loss=3.1085681915283203
I0205 09:04:34.677230 139946397853440 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.6741974353790283, loss=3.5279855728149414
I0205 09:04:36.856488 140107197974336 spec.py:321] Evaluating on the training split.
I0205 09:04:47.319217 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 09:05:13.458548 140107197974336 spec.py:349] Evaluating on the test split.
I0205 09:05:15.063646 140107197974336 submission_runner.py:408] Time since start: 65544.81s, 	Step: 130106, 	{'train/accuracy': 0.83314448595047, 'train/loss': 0.8533020615577698, 'validation/accuracy': 0.7516599893569946, 'validation/loss': 1.195619821548462, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.786401629447937, 'test/num_examples': 10000, 'score': 59697.6349568367, 'total_duration': 65544.8101940155, 'accumulated_submission_time': 59697.6349568367, 'accumulated_eval_time': 5834.219810962677, 'accumulated_logging_time': 5.449113845825195}
I0205 09:05:15.105611 139946414638848 logging_writer.py:48] [130106] accumulated_eval_time=5834.219811, accumulated_logging_time=5.449114, accumulated_submission_time=59697.634957, global_step=130106, preemption_count=0, score=59697.634957, test/accuracy=0.630600, test/loss=1.786402, test/num_examples=10000, total_duration=65544.810194, train/accuracy=0.833144, train/loss=0.853302, validation/accuracy=0.751660, validation/loss=1.195620, validation/num_examples=50000
I0205 09:05:53.958032 139946397853440 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.7371022701263428, loss=3.3101863861083984
I0205 09:06:39.850187 139946414638848 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.161015748977661, loss=4.6592864990234375
I0205 09:07:26.855131 139946397853440 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.8319382667541504, loss=3.1138198375701904
I0205 09:08:13.484264 139946414638848 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.106363296508789, loss=4.631998062133789
I0205 09:08:59.817918 139946397853440 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.8953478336334229, loss=4.175272464752197
I0205 09:09:46.419460 139946414638848 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.7708485126495361, loss=2.998952627182007
I0205 09:10:32.688548 139946397853440 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.912562608718872, loss=4.037024974822998
I0205 09:11:19.238122 139946414638848 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.7236454486846924, loss=2.955109119415283
I0205 09:12:05.804359 139946397853440 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.6524403095245361, loss=3.5665321350097656
I0205 09:12:15.173912 140107197974336 spec.py:321] Evaluating on the training split.
I0205 09:12:26.274538 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 09:12:54.796590 140107197974336 spec.py:349] Evaluating on the test split.
I0205 09:12:56.391654 140107197974336 submission_runner.py:408] Time since start: 66006.14s, 	Step: 131022, 	{'train/accuracy': 0.8349804282188416, 'train/loss': 0.847000241279602, 'validation/accuracy': 0.7511000037193298, 'validation/loss': 1.1966373920440674, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.7943549156188965, 'test/num_examples': 10000, 'score': 60117.64026284218, 'total_duration': 66006.13822078705, 'accumulated_submission_time': 60117.64026284218, 'accumulated_eval_time': 5875.437610626221, 'accumulated_logging_time': 5.5017664432525635}
I0205 09:12:56.426800 139946414638848 logging_writer.py:48] [131022] accumulated_eval_time=5875.437611, accumulated_logging_time=5.501766, accumulated_submission_time=60117.640263, global_step=131022, preemption_count=0, score=60117.640263, test/accuracy=0.629900, test/loss=1.794355, test/num_examples=10000, total_duration=66006.138221, train/accuracy=0.834980, train/loss=0.847000, validation/accuracy=0.751100, validation/loss=1.196637, validation/num_examples=50000
I0205 09:13:28.147020 139946397853440 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.91476571559906, loss=2.9906084537506104
I0205 09:14:14.515850 139946414638848 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.876966118812561, loss=3.083226442337036
I0205 09:15:01.047196 139946397853440 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.1269009113311768, loss=3.0401437282562256
I0205 09:15:47.739722 139946414638848 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.765079140663147, loss=3.1623244285583496
I0205 09:16:34.092718 139946397853440 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.935853362083435, loss=3.0711557865142822
I0205 09:17:20.572450 139946414638848 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.1091690063476562, loss=4.4771857261657715
I0205 09:18:06.915865 139946397853440 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.8983252048492432, loss=4.503093719482422
I0205 09:18:53.237320 139946414638848 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.742735505104065, loss=3.854515790939331
I0205 09:19:39.579255 139946397853440 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.7432347536087036, loss=3.463134527206421
I0205 09:19:56.410772 140107197974336 spec.py:321] Evaluating on the training split.
I0205 09:20:07.404257 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 09:20:38.791291 140107197974336 spec.py:349] Evaluating on the test split.
I0205 09:20:40.383452 140107197974336 submission_runner.py:408] Time since start: 66470.13s, 	Step: 131938, 	{'train/accuracy': 0.841113269329071, 'train/loss': 0.8369109630584717, 'validation/accuracy': 0.7544800043106079, 'validation/loss': 1.2058559656143188, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.801892876625061, 'test/num_examples': 10000, 'score': 60537.56083083153, 'total_duration': 66470.13001537323, 'accumulated_submission_time': 60537.56083083153, 'accumulated_eval_time': 5919.410274267197, 'accumulated_logging_time': 5.547304630279541}
I0205 09:20:40.423006 139946414638848 logging_writer.py:48] [131938] accumulated_eval_time=5919.410274, accumulated_logging_time=5.547305, accumulated_submission_time=60537.560831, global_step=131938, preemption_count=0, score=60537.560831, test/accuracy=0.631700, test/loss=1.801893, test/num_examples=10000, total_duration=66470.130015, train/accuracy=0.841113, train/loss=0.836911, validation/accuracy=0.754480, validation/loss=1.205856, validation/num_examples=50000
I0205 09:21:05.224800 139946397853440 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.8116751909255981, loss=4.173666000366211
I0205 09:21:50.916164 139946414638848 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.766417384147644, loss=2.993940830230713
I0205 09:22:37.285302 139946397853440 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.773431420326233, loss=3.3101868629455566
I0205 09:23:23.654841 139946414638848 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.9532997608184814, loss=4.0993194580078125
I0205 09:24:10.130712 139946397853440 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.920625925064087, loss=3.074232578277588
I0205 09:24:56.613922 139946414638848 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.9452182054519653, loss=3.0104799270629883
I0205 09:25:43.031897 139946397853440 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.739994764328003, loss=4.002246856689453
I0205 09:26:29.536253 139946414638848 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.752295970916748, loss=3.153012275695801
I0205 09:27:15.866132 139946397853440 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.883960247039795, loss=3.122152805328369
I0205 09:27:40.604292 140107197974336 spec.py:321] Evaluating on the training split.
I0205 09:27:51.295419 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 09:28:22.591956 140107197974336 spec.py:349] Evaluating on the test split.
I0205 09:28:24.186889 140107197974336 submission_runner.py:408] Time since start: 66933.93s, 	Step: 132855, 	{'train/accuracy': 0.8413476347923279, 'train/loss': 0.8087218403816223, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.1676216125488281, 'validation/num_examples': 50000, 'test/accuracy': 0.6373000144958496, 'test/loss': 1.7519946098327637, 'test/num_examples': 10000, 'score': 60957.67953944206, 'total_duration': 66933.93345880508, 'accumulated_submission_time': 60957.67953944206, 'accumulated_eval_time': 5962.992874383926, 'accumulated_logging_time': 5.59683632850647}
I0205 09:28:24.226393 139946414638848 logging_writer.py:48] [132855] accumulated_eval_time=5962.992874, accumulated_logging_time=5.596836, accumulated_submission_time=60957.679539, global_step=132855, preemption_count=0, score=60957.679539, test/accuracy=0.637300, test/loss=1.751995, test/num_examples=10000, total_duration=66933.933459, train/accuracy=0.841348, train/loss=0.808722, validation/accuracy=0.756060, validation/loss=1.167622, validation/num_examples=50000
I0205 09:28:42.318617 139946397853440 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.163365364074707, loss=4.463100910186768
I0205 09:29:26.866501 139946414638848 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.661420464515686, loss=3.0390191078186035
I0205 09:30:13.669627 139946397853440 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.763629674911499, loss=2.9835915565490723
I0205 09:31:00.334074 139946414638848 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.7875269651412964, loss=3.013645648956299
I0205 09:31:46.774858 139946397853440 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.873189926147461, loss=3.027181625366211
I0205 09:32:33.289085 139946414638848 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.8075878620147705, loss=3.12636661529541
I0205 09:33:19.745450 139946397853440 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.7917258739471436, loss=2.9270260334014893
I0205 09:34:06.417304 139946414638848 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.8171271085739136, loss=3.2119553089141846
I0205 09:34:52.984970 139946397853440 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.9168179035186768, loss=3.0090131759643555
I0205 09:35:24.227659 140107197974336 spec.py:321] Evaluating on the training split.
I0205 09:35:34.931475 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 09:36:07.291867 140107197974336 spec.py:349] Evaluating on the test split.
I0205 09:36:08.890023 140107197974336 submission_runner.py:408] Time since start: 67398.64s, 	Step: 133769, 	{'train/accuracy': 0.8415429592132568, 'train/loss': 0.8192632794380188, 'validation/accuracy': 0.7564199566841125, 'validation/loss': 1.1783759593963623, 'validation/num_examples': 50000, 'test/accuracy': 0.6380000114440918, 'test/loss': 1.7589350938796997, 'test/num_examples': 10000, 'score': 61377.61789727211, 'total_duration': 67398.6365814209, 'accumulated_submission_time': 61377.61789727211, 'accumulated_eval_time': 6007.655216932297, 'accumulated_logging_time': 5.646831274032593}
I0205 09:36:08.932550 139946414638848 logging_writer.py:48] [133769] accumulated_eval_time=6007.655217, accumulated_logging_time=5.646831, accumulated_submission_time=61377.617897, global_step=133769, preemption_count=0, score=61377.617897, test/accuracy=0.638000, test/loss=1.758935, test/num_examples=10000, total_duration=67398.636581, train/accuracy=0.841543, train/loss=0.819263, validation/accuracy=0.756420, validation/loss=1.178376, validation/num_examples=50000
I0205 09:36:21.634360 139946397853440 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.877120018005371, loss=3.9099678993225098
I0205 09:37:05.177222 139946414638848 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.832871437072754, loss=3.226571559906006
I0205 09:37:50.978757 139946397853440 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.8887273073196411, loss=2.996736764907837
I0205 09:38:37.542492 139946414638848 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.7748650312423706, loss=3.027174949645996
I0205 09:39:23.865954 139946397853440 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.7671587467193604, loss=3.3017165660858154
I0205 09:40:10.239544 139946414638848 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.9005461931228638, loss=3.1265430450439453
I0205 09:40:56.422647 139946397853440 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.8310835361480713, loss=2.911309003829956
I0205 09:41:42.789755 139946414638848 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.8972784280776978, loss=2.96405029296875
I0205 09:42:29.046977 139946397853440 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.756796956062317, loss=3.035567045211792
I0205 09:43:09.090657 140107197974336 spec.py:321] Evaluating on the training split.
I0205 09:43:19.892166 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 09:43:47.098167 140107197974336 spec.py:349] Evaluating on the test split.
I0205 09:43:48.691092 140107197974336 submission_runner.py:408] Time since start: 67858.44s, 	Step: 134688, 	{'train/accuracy': 0.8436328172683716, 'train/loss': 0.8358486890792847, 'validation/accuracy': 0.7567600011825562, 'validation/loss': 1.1952334642410278, 'validation/num_examples': 50000, 'test/accuracy': 0.6371000409126282, 'test/loss': 1.7937754392623901, 'test/num_examples': 10000, 'score': 61797.7135746479, 'total_duration': 67858.43765830994, 'accumulated_submission_time': 61797.7135746479, 'accumulated_eval_time': 6047.2556438446045, 'accumulated_logging_time': 5.699421167373657}
I0205 09:43:48.728413 139946414638848 logging_writer.py:48] [134688] accumulated_eval_time=6047.255644, accumulated_logging_time=5.699421, accumulated_submission_time=61797.713575, global_step=134688, preemption_count=0, score=61797.713575, test/accuracy=0.637100, test/loss=1.793775, test/num_examples=10000, total_duration=67858.437658, train/accuracy=0.843633, train/loss=0.835849, validation/accuracy=0.756760, validation/loss=1.195233, validation/num_examples=50000
I0205 09:43:53.851319 139946397853440 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.877149224281311, loss=4.044228553771973
I0205 09:44:36.306715 139946414638848 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.899039626121521, loss=3.017162322998047
I0205 09:45:22.330284 139946397853440 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.8140387535095215, loss=3.1850669384002686
I0205 09:46:09.066666 139946414638848 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.817362904548645, loss=3.539902925491333
I0205 09:46:54.936414 139946397853440 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.8734345436096191, loss=3.176213264465332
I0205 09:47:41.237203 139946414638848 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.9402729272842407, loss=4.211662292480469
I0205 09:48:27.837359 139946397853440 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.7840749025344849, loss=3.001697063446045
I0205 09:49:14.007739 139946414638848 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.869809865951538, loss=2.9930121898651123
I0205 09:50:00.360396 139946397853440 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.436112403869629, loss=4.538277626037598
I0205 09:50:46.586977 139946414638848 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.8609685897827148, loss=2.968644380569458
I0205 09:50:49.084127 140107197974336 spec.py:321] Evaluating on the training split.
I0205 09:50:59.888515 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 09:51:31.925528 140107197974336 spec.py:349] Evaluating on the test split.
I0205 09:51:33.522311 140107197974336 submission_runner.py:408] Time since start: 68323.27s, 	Step: 135607, 	{'train/accuracy': 0.857714831829071, 'train/loss': 0.7488970160484314, 'validation/accuracy': 0.75764000415802, 'validation/loss': 1.1549402475357056, 'validation/num_examples': 50000, 'test/accuracy': 0.6403000354766846, 'test/loss': 1.7427631616592407, 'test/num_examples': 10000, 'score': 62218.0052587986, 'total_duration': 68323.26888012886, 'accumulated_submission_time': 62218.0052587986, 'accumulated_eval_time': 6091.693810224533, 'accumulated_logging_time': 5.74712872505188}
I0205 09:51:33.558099 139946397853440 logging_writer.py:48] [135607] accumulated_eval_time=6091.693810, accumulated_logging_time=5.747129, accumulated_submission_time=62218.005259, global_step=135607, preemption_count=0, score=62218.005259, test/accuracy=0.640300, test/loss=1.742763, test/num_examples=10000, total_duration=68323.268880, train/accuracy=0.857715, train/loss=0.748897, validation/accuracy=0.757640, validation/loss=1.154940, validation/num_examples=50000
I0205 09:52:12.423645 139946414638848 logging_writer.py:48] [135700] global_step=135700, grad_norm=2.0475735664367676, loss=4.37680721282959
I0205 09:52:58.476168 139946397853440 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.9391268491744995, loss=3.271007776260376
I0205 09:53:44.941995 139946414638848 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.7571313381195068, loss=2.9638142585754395
I0205 09:54:31.523715 139946397853440 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.8675510883331299, loss=3.3705766201019287
I0205 09:55:17.888936 139946414638848 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.8576022386550903, loss=3.116351366043091
I0205 09:56:04.160153 139946397853440 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.0294151306152344, loss=4.016957759857178
I0205 09:56:50.319421 139946414638848 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.1563148498535156, loss=4.421371936798096
I0205 09:57:37.054872 139946397853440 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.313255786895752, loss=4.494819164276123
I0205 09:58:23.137265 139946414638848 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.0767557621002197, loss=2.9693422317504883
I0205 09:58:33.961736 140107197974336 spec.py:321] Evaluating on the training split.
I0205 09:58:44.687504 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 09:59:13.155483 140107197974336 spec.py:349] Evaluating on the test split.
I0205 09:59:14.747287 140107197974336 submission_runner.py:408] Time since start: 68784.49s, 	Step: 136525, 	{'train/accuracy': 0.8420116901397705, 'train/loss': 0.8260501623153687, 'validation/accuracy': 0.7569199800491333, 'validation/loss': 1.185738205909729, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.7610141038894653, 'test/num_examples': 10000, 'score': 62638.34627819061, 'total_duration': 68784.49385523796, 'accumulated_submission_time': 62638.34627819061, 'accumulated_eval_time': 6132.479343414307, 'accumulated_logging_time': 5.793379783630371}
I0205 09:59:14.783626 139946397853440 logging_writer.py:48] [136525] accumulated_eval_time=6132.479343, accumulated_logging_time=5.793380, accumulated_submission_time=62638.346278, global_step=136525, preemption_count=0, score=62638.346278, test/accuracy=0.640200, test/loss=1.761014, test/num_examples=10000, total_duration=68784.493855, train/accuracy=0.842012, train/loss=0.826050, validation/accuracy=0.756920, validation/loss=1.185738, validation/num_examples=50000
I0205 09:59:45.175878 139946414638848 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.8465890884399414, loss=3.393707752227783
I0205 10:00:31.242783 139946397853440 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.717040777206421, loss=3.1567018032073975
I0205 10:01:17.692886 139946414638848 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.8502581119537354, loss=3.892205238342285
I0205 10:02:04.176293 139946397853440 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.8391679525375366, loss=2.9179158210754395
I0205 10:02:50.628025 139946414638848 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.1173317432403564, loss=4.2529144287109375
I0205 10:03:36.964418 139946397853440 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.9747817516326904, loss=2.9062540531158447
I0205 10:04:23.124776 139946414638848 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.8230301141738892, loss=3.035715103149414
I0205 10:05:09.719070 139946397853440 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.4714531898498535, loss=4.523529529571533
I0205 10:05:56.159553 139946414638848 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.9068782329559326, loss=2.995365858078003
I0205 10:06:14.856543 140107197974336 spec.py:321] Evaluating on the training split.
I0205 10:06:25.439062 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 10:06:54.906569 140107197974336 spec.py:349] Evaluating on the test split.
I0205 10:06:56.501182 140107197974336 submission_runner.py:408] Time since start: 69246.25s, 	Step: 137441, 	{'train/accuracy': 0.8498241901397705, 'train/loss': 0.7776594161987305, 'validation/accuracy': 0.7576599717140198, 'validation/loss': 1.155328392982483, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.739344596862793, 'test/num_examples': 10000, 'score': 63058.35525393486, 'total_duration': 69246.24775362015, 'accumulated_submission_time': 63058.35525393486, 'accumulated_eval_time': 6174.12397646904, 'accumulated_logging_time': 5.840576648712158}
I0205 10:06:56.536701 139946397853440 logging_writer.py:48] [137441] accumulated_eval_time=6174.123976, accumulated_logging_time=5.840577, accumulated_submission_time=63058.355254, global_step=137441, preemption_count=0, score=63058.355254, test/accuracy=0.640700, test/loss=1.739345, test/num_examples=10000, total_duration=69246.247754, train/accuracy=0.849824, train/loss=0.777659, validation/accuracy=0.757660, validation/loss=1.155328, validation/num_examples=50000
I0205 10:07:20.135918 139946414638848 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.8945523500442505, loss=3.868933916091919
I0205 10:08:05.473700 139946397853440 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.946083426475525, loss=3.2300667762756348
I0205 10:08:52.129118 139946414638848 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.9218188524246216, loss=2.907927989959717
I0205 10:09:39.107501 139946397853440 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.9414749145507812, loss=3.037052631378174
I0205 10:10:25.547703 139946414638848 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.8197945356369019, loss=3.312236785888672
I0205 10:11:12.066729 139946397853440 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.7074682712554932, loss=3.714653253555298
I0205 10:11:58.406031 139946414638848 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.0162341594696045, loss=4.08414363861084
I0205 10:12:44.735557 139946397853440 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.8514890670776367, loss=3.0920372009277344
I0205 10:13:31.085102 139946414638848 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.024040460586548, loss=3.1067352294921875
I0205 10:13:56.708963 140107197974336 spec.py:321] Evaluating on the training split.
I0205 10:14:07.551855 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 10:14:40.033949 140107197974336 spec.py:349] Evaluating on the test split.
I0205 10:14:41.631738 140107197974336 submission_runner.py:408] Time since start: 69711.38s, 	Step: 138357, 	{'train/accuracy': 0.8564062118530273, 'train/loss': 0.7429553270339966, 'validation/accuracy': 0.7604999542236328, 'validation/loss': 1.145262360572815, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.75374174118042, 'test/num_examples': 10000, 'score': 63478.462926864624, 'total_duration': 69711.37830281258, 'accumulated_submission_time': 63478.462926864624, 'accumulated_eval_time': 6219.046733617783, 'accumulated_logging_time': 5.887576580047607}
I0205 10:14:41.668341 139946397853440 logging_writer.py:48] [138357] accumulated_eval_time=6219.046734, accumulated_logging_time=5.887577, accumulated_submission_time=63478.462927, global_step=138357, preemption_count=0, score=63478.462927, test/accuracy=0.638900, test/loss=1.753742, test/num_examples=10000, total_duration=69711.378303, train/accuracy=0.856406, train/loss=0.742955, validation/accuracy=0.760500, validation/loss=1.145262, validation/num_examples=50000
I0205 10:14:58.980591 139946414638848 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.858352780342102, loss=2.8819854259490967
I0205 10:15:43.208027 139946397853440 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.0585877895355225, loss=4.049412727355957
I0205 10:16:29.852040 139946414638848 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.2460179328918457, loss=4.277854919433594
I0205 10:17:16.317794 139946397853440 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.8886823654174805, loss=2.980607032775879
I0205 10:18:02.446061 139946414638848 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.9949208498001099, loss=3.444983720779419
I0205 10:18:49.032103 139946397853440 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.0125107765197754, loss=4.129795551300049
I0205 10:19:35.353343 139946414638848 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.1184964179992676, loss=3.4486043453216553
I0205 10:20:21.594167 139946397853440 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.954816460609436, loss=2.906134605407715
I0205 10:21:07.950490 139946414638848 logging_writer.py:48] [139200] global_step=139200, grad_norm=1.9421582221984863, loss=2.9916510581970215
I0205 10:21:41.821995 140107197974336 spec.py:321] Evaluating on the training split.
I0205 10:21:52.720873 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 10:22:23.486829 140107197974336 spec.py:349] Evaluating on the test split.
I0205 10:22:25.079550 140107197974336 submission_runner.py:408] Time since start: 70174.83s, 	Step: 139275, 	{'train/accuracy': 0.848437488079071, 'train/loss': 0.8022940158843994, 'validation/accuracy': 0.7615199685096741, 'validation/loss': 1.1675846576690674, 'validation/num_examples': 50000, 'test/accuracy': 0.64000004529953, 'test/loss': 1.7675161361694336, 'test/num_examples': 10000, 'score': 63898.55232334137, 'total_duration': 70174.82610559464, 'accumulated_submission_time': 63898.55232334137, 'accumulated_eval_time': 6262.304250955582, 'accumulated_logging_time': 5.935348272323608}
I0205 10:22:25.117546 139946397853440 logging_writer.py:48] [139275] accumulated_eval_time=6262.304251, accumulated_logging_time=5.935348, accumulated_submission_time=63898.552323, global_step=139275, preemption_count=0, score=63898.552323, test/accuracy=0.640000, test/loss=1.767516, test/num_examples=10000, total_duration=70174.826106, train/accuracy=0.848437, train/loss=0.802294, validation/accuracy=0.761520, validation/loss=1.167585, validation/num_examples=50000
I0205 10:22:35.344266 139946414638848 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.9714138507843018, loss=3.3095667362213135
I0205 10:23:18.403912 139946397853440 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.013460159301758, loss=3.027193069458008
I0205 10:24:05.039846 139946414638848 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.026214838027954, loss=2.9742796421051025
I0205 10:24:52.010054 139946397853440 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.031036615371704, loss=3.820023536682129
I0205 10:25:38.376312 139946414638848 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.0262153148651123, loss=2.984273672103882
I0205 10:26:25.092126 139946397853440 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.893953800201416, loss=2.941727876663208
I0205 10:27:11.642997 139946414638848 logging_writer.py:48] [139900] global_step=139900, grad_norm=1.8517086505889893, loss=3.5293538570404053
I0205 10:27:58.107764 139946397853440 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.1126515865325928, loss=2.89272403717041
I0205 10:28:44.554417 139946414638848 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.9576704502105713, loss=2.890048027038574
I0205 10:29:25.154973 140107197974336 spec.py:321] Evaluating on the training split.
I0205 10:29:35.920172 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 10:30:08.032482 140107197974336 spec.py:349] Evaluating on the test split.
I0205 10:30:09.631643 140107197974336 submission_runner.py:408] Time since start: 70639.38s, 	Step: 140189, 	{'train/accuracy': 0.850390613079071, 'train/loss': 0.7843379974365234, 'validation/accuracy': 0.7585200071334839, 'validation/loss': 1.1634849309921265, 'validation/num_examples': 50000, 'test/accuracy': 0.6385000348091125, 'test/loss': 1.748010277748108, 'test/num_examples': 10000, 'score': 64318.52702474594, 'total_duration': 70639.37820744514, 'accumulated_submission_time': 64318.52702474594, 'accumulated_eval_time': 6306.780901193619, 'accumulated_logging_time': 5.983578443527222}
I0205 10:30:09.672333 139946397853440 logging_writer.py:48] [140189] accumulated_eval_time=6306.780901, accumulated_logging_time=5.983578, accumulated_submission_time=64318.527025, global_step=140189, preemption_count=0, score=64318.527025, test/accuracy=0.638500, test/loss=1.748010, test/num_examples=10000, total_duration=70639.378207, train/accuracy=0.850391, train/loss=0.784338, validation/accuracy=0.758520, validation/loss=1.163485, validation/num_examples=50000
I0205 10:30:14.397778 139946414638848 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.7352447509765625, loss=4.453924179077148
I0205 10:30:56.597031 139946397853440 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.7344449758529663, loss=2.931882858276367
I0205 10:31:43.007703 139946414638848 logging_writer.py:48] [140400] global_step=140400, grad_norm=1.9211074113845825, loss=3.633575439453125
I0205 10:32:29.792757 139946397853440 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.0298657417297363, loss=2.9888272285461426
I0205 10:33:16.376558 139946414638848 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.83600914478302, loss=3.5281519889831543
I0205 10:34:03.101518 139946397853440 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.9726767539978027, loss=3.5967397689819336
I0205 10:34:49.612427 139946414638848 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.9586577415466309, loss=2.948164463043213
I0205 10:35:36.114625 139946397853440 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.1410441398620605, loss=3.377927780151367
I0205 10:36:22.225214 139946414638848 logging_writer.py:48] [141000] global_step=141000, grad_norm=1.9796534776687622, loss=2.9774608612060547
I0205 10:37:08.664168 139946397853440 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.852792739868164, loss=3.272092342376709
I0205 10:37:09.788206 140107197974336 spec.py:321] Evaluating on the training split.
I0205 10:37:20.349084 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 10:37:52.453369 140107197974336 spec.py:349] Evaluating on the test split.
I0205 10:37:54.050108 140107197974336 submission_runner.py:408] Time since start: 71103.80s, 	Step: 141104, 	{'train/accuracy': 0.8593944907188416, 'train/loss': 0.7617772221565247, 'validation/accuracy': 0.7626199722290039, 'validation/loss': 1.1537964344024658, 'validation/num_examples': 50000, 'test/accuracy': 0.645300030708313, 'test/loss': 1.7508445978164673, 'test/num_examples': 10000, 'score': 64738.57985305786, 'total_duration': 71103.796667099, 'accumulated_submission_time': 64738.57985305786, 'accumulated_eval_time': 6351.042797088623, 'accumulated_logging_time': 6.035136699676514}
I0205 10:37:54.091996 139946414638848 logging_writer.py:48] [141104] accumulated_eval_time=6351.042797, accumulated_logging_time=6.035137, accumulated_submission_time=64738.579853, global_step=141104, preemption_count=0, score=64738.579853, test/accuracy=0.645300, test/loss=1.750845, test/num_examples=10000, total_duration=71103.796667, train/accuracy=0.859394, train/loss=0.761777, validation/accuracy=0.762620, validation/loss=1.153796, validation/num_examples=50000
I0205 10:38:34.286610 139946397853440 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.144115924835205, loss=3.8956663608551025
I0205 10:39:20.790677 139946414638848 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.042536973953247, loss=3.100879192352295
I0205 10:40:07.758777 139946397853440 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.0881731510162354, loss=4.123261451721191
I0205 10:40:54.393965 139946414638848 logging_writer.py:48] [141500] global_step=141500, grad_norm=1.9394418001174927, loss=3.0013651847839355
I0205 10:41:41.223851 139946397853440 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.2750651836395264, loss=4.400829792022705
I0205 10:42:27.769184 139946414638848 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.2873661518096924, loss=4.259101390838623
I0205 10:43:14.495865 139946397853440 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.940066933631897, loss=2.923475742340088
I0205 10:44:01.258407 139946414638848 logging_writer.py:48] [141900] global_step=141900, grad_norm=1.9818201065063477, loss=2.999762535095215
I0205 10:44:48.006084 139946397853440 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.063103675842285, loss=2.973062038421631
I0205 10:44:54.148712 140107197974336 spec.py:321] Evaluating on the training split.
I0205 10:45:04.952092 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 10:45:37.322030 140107197974336 spec.py:349] Evaluating on the test split.
I0205 10:45:38.929744 140107197974336 submission_runner.py:408] Time since start: 71568.68s, 	Step: 142015, 	{'train/accuracy': 0.8515819907188416, 'train/loss': 0.7848900556564331, 'validation/accuracy': 0.7644199728965759, 'validation/loss': 1.1486810445785522, 'validation/num_examples': 50000, 'test/accuracy': 0.6426000595092773, 'test/loss': 1.7553709745407104, 'test/num_examples': 10000, 'score': 65158.57492208481, 'total_duration': 71568.67630791664, 'accumulated_submission_time': 65158.57492208481, 'accumulated_eval_time': 6395.823813199997, 'accumulated_logging_time': 6.086883783340454}
I0205 10:45:38.967319 139946414638848 logging_writer.py:48] [142015] accumulated_eval_time=6395.823813, accumulated_logging_time=6.086884, accumulated_submission_time=65158.574922, global_step=142015, preemption_count=0, score=65158.574922, test/accuracy=0.642600, test/loss=1.755371, test/num_examples=10000, total_duration=71568.676308, train/accuracy=0.851582, train/loss=0.784890, validation/accuracy=0.764420, validation/loss=1.148681, validation/num_examples=50000
I0205 10:46:14.102783 139946397853440 logging_writer.py:48] [142100] global_step=142100, grad_norm=2.3477888107299805, loss=4.194344997406006
I0205 10:47:00.340824 139946414638848 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.1642143726348877, loss=2.862581729888916
I0205 10:47:47.053371 139946397853440 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.0930588245391846, loss=2.9240288734436035
I0205 10:48:33.715803 139946414638848 logging_writer.py:48] [142400] global_step=142400, grad_norm=1.9096547365188599, loss=3.314441204071045
I0205 10:49:20.227311 139946397853440 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.0369927883148193, loss=2.99442458152771
I0205 10:50:06.634277 139946414638848 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.1120588779449463, loss=3.2478513717651367
I0205 10:50:53.162417 139946397853440 logging_writer.py:48] [142700] global_step=142700, grad_norm=1.8446816205978394, loss=3.3128552436828613
I0205 10:51:39.531740 139946414638848 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.029062509536743, loss=3.7330663204193115
I0205 10:52:26.066887 139946397853440 logging_writer.py:48] [142900] global_step=142900, grad_norm=1.861797571182251, loss=2.861363410949707
I0205 10:52:39.248594 140107197974336 spec.py:321] Evaluating on the training split.
I0205 10:52:50.037826 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 10:53:16.912064 140107197974336 spec.py:349] Evaluating on the test split.
I0205 10:53:18.511006 140107197974336 submission_runner.py:408] Time since start: 72028.26s, 	Step: 142930, 	{'train/accuracy': 0.8580859303474426, 'train/loss': 0.7559224367141724, 'validation/accuracy': 0.7646999955177307, 'validation/loss': 1.1461162567138672, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.7412879467010498, 'test/num_examples': 10000, 'score': 65578.79041981697, 'total_duration': 72028.25753879547, 'accumulated_submission_time': 65578.79041981697, 'accumulated_eval_time': 6435.086161613464, 'accumulated_logging_time': 6.138216018676758}
I0205 10:53:18.555516 139946414638848 logging_writer.py:48] [142930] accumulated_eval_time=6435.086162, accumulated_logging_time=6.138216, accumulated_submission_time=65578.790420, global_step=142930, preemption_count=0, score=65578.790420, test/accuracy=0.645700, test/loss=1.741288, test/num_examples=10000, total_duration=72028.257539, train/accuracy=0.858086, train/loss=0.755922, validation/accuracy=0.764700, validation/loss=1.146116, validation/num_examples=50000
I0205 10:53:46.520039 139946397853440 logging_writer.py:48] [143000] global_step=143000, grad_norm=1.9820810556411743, loss=2.9141438007354736
I0205 10:54:32.715961 139946414638848 logging_writer.py:48] [143100] global_step=143100, grad_norm=1.9986889362335205, loss=2.9325408935546875
I0205 10:55:19.475887 139946397853440 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.0473392009735107, loss=2.9873757362365723
I0205 10:56:06.180045 139946414638848 logging_writer.py:48] [143300] global_step=143300, grad_norm=1.9225735664367676, loss=3.3048171997070312
I0205 10:56:52.545800 139946397853440 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.9789878129959106, loss=3.2742373943328857
I0205 10:57:38.911053 139946414638848 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.023188352584839, loss=2.972116470336914
I0205 10:58:25.518478 139946397853440 logging_writer.py:48] [143600] global_step=143600, grad_norm=2.0791940689086914, loss=2.9765069484710693
I0205 10:59:12.030140 139946414638848 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.100214958190918, loss=2.959207773208618
I0205 10:59:58.470034 139946397853440 logging_writer.py:48] [143800] global_step=143800, grad_norm=1.8323743343353271, loss=3.2475593090057373
I0205 11:00:18.738645 140107197974336 spec.py:321] Evaluating on the training split.
I0205 11:00:29.431105 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 11:00:55.900533 140107197974336 spec.py:349] Evaluating on the test split.
I0205 11:00:57.506536 140107197974336 submission_runner.py:408] Time since start: 72487.25s, 	Step: 143845, 	{'train/accuracy': 0.8574609160423279, 'train/loss': 0.7697817087173462, 'validation/accuracy': 0.7655199766159058, 'validation/loss': 1.1547305583953857, 'validation/num_examples': 50000, 'test/accuracy': 0.645300030708313, 'test/loss': 1.753394603729248, 'test/num_examples': 10000, 'score': 65998.90920972824, 'total_duration': 72487.25309491158, 'accumulated_submission_time': 65998.90920972824, 'accumulated_eval_time': 6473.854023933411, 'accumulated_logging_time': 6.194704294204712}
I0205 11:00:57.544474 139946414638848 logging_writer.py:48] [143845] accumulated_eval_time=6473.854024, accumulated_logging_time=6.194704, accumulated_submission_time=65998.909210, global_step=143845, preemption_count=0, score=65998.909210, test/accuracy=0.645300, test/loss=1.753395, test/num_examples=10000, total_duration=72487.253095, train/accuracy=0.857461, train/loss=0.769782, validation/accuracy=0.765520, validation/loss=1.154731, validation/num_examples=50000
I0205 11:01:19.574755 139946397853440 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.160148859024048, loss=3.9459455013275146
I0205 11:02:04.806340 139946414638848 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.116983652114868, loss=4.113109111785889
I0205 11:02:50.864825 139946397853440 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.0476949214935303, loss=3.5040082931518555
I0205 11:03:37.689276 139946414638848 logging_writer.py:48] [144200] global_step=144200, grad_norm=1.8728582859039307, loss=2.98195743560791
I0205 11:04:24.051272 139946397853440 logging_writer.py:48] [144300] global_step=144300, grad_norm=1.9448522329330444, loss=2.9253387451171875
I0205 11:05:10.779084 139946414638848 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.158027410507202, loss=4.146937370300293
I0205 11:05:57.265538 139946397853440 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.231867790222168, loss=2.8925604820251465
I0205 11:06:43.506862 139946414638848 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.317370891571045, loss=3.823367118835449
I0205 11:07:30.001794 139946397853440 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.045924425125122, loss=3.582334518432617
I0205 11:07:57.914433 140107197974336 spec.py:321] Evaluating on the training split.
I0205 11:08:08.788488 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 11:08:38.297141 140107197974336 spec.py:349] Evaluating on the test split.
I0205 11:08:39.899899 140107197974336 submission_runner.py:408] Time since start: 72949.65s, 	Step: 144762, 	{'train/accuracy': 0.8602929711341858, 'train/loss': 0.742518961429596, 'validation/accuracy': 0.7663799524307251, 'validation/loss': 1.1345235109329224, 'validation/num_examples': 50000, 'test/accuracy': 0.6467000246047974, 'test/loss': 1.7295129299163818, 'test/num_examples': 10000, 'score': 66419.2152094841, 'total_duration': 72949.6464586258, 'accumulated_submission_time': 66419.2152094841, 'accumulated_eval_time': 6515.839464187622, 'accumulated_logging_time': 6.242878437042236}
I0205 11:08:39.943334 139946414638848 logging_writer.py:48] [144762] accumulated_eval_time=6515.839464, accumulated_logging_time=6.242878, accumulated_submission_time=66419.215209, global_step=144762, preemption_count=0, score=66419.215209, test/accuracy=0.646700, test/loss=1.729513, test/num_examples=10000, total_duration=72949.646459, train/accuracy=0.860293, train/loss=0.742519, validation/accuracy=0.766380, validation/loss=1.134524, validation/num_examples=50000
I0205 11:08:55.276390 139946397853440 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.7720844745635986, loss=4.4059834480285645
I0205 11:09:39.353746 139946414638848 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.085261583328247, loss=2.9121148586273193
I0205 11:10:26.219876 139946397853440 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.4474079608917236, loss=4.44333553314209
I0205 11:11:13.326002 139946414638848 logging_writer.py:48] [145100] global_step=145100, grad_norm=1.9155787229537964, loss=2.8483235836029053
I0205 11:12:00.133451 139946397853440 logging_writer.py:48] [145200] global_step=145200, grad_norm=1.8940167427062988, loss=2.9184913635253906
I0205 11:12:46.573073 139946414638848 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.219618558883667, loss=3.1085681915283203
I0205 11:13:32.974264 139946397853440 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.29264760017395, loss=2.951765537261963
I0205 11:14:19.863454 139946414638848 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.829975962638855, loss=3.5578997135162354
I0205 11:15:06.502261 139946397853440 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.1487910747528076, loss=3.9240520000457764
I0205 11:15:40.189890 140107197974336 spec.py:321] Evaluating on the training split.
I0205 11:15:51.230973 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 11:16:22.626032 140107197974336 spec.py:349] Evaluating on the test split.
I0205 11:16:24.227262 140107197974336 submission_runner.py:408] Time since start: 73413.97s, 	Step: 145674, 	{'train/accuracy': 0.8600585460662842, 'train/loss': 0.7430623173713684, 'validation/accuracy': 0.7668799757957458, 'validation/loss': 1.1308563947677612, 'validation/num_examples': 50000, 'test/accuracy': 0.6487000584602356, 'test/loss': 1.7240246534347534, 'test/num_examples': 10000, 'score': 66839.39895510674, 'total_duration': 73413.97380805016, 'accumulated_submission_time': 66839.39895510674, 'accumulated_eval_time': 6559.876788377762, 'accumulated_logging_time': 6.297804832458496}
I0205 11:16:24.269229 139946414638848 logging_writer.py:48] [145674] accumulated_eval_time=6559.876788, accumulated_logging_time=6.297805, accumulated_submission_time=66839.398955, global_step=145674, preemption_count=0, score=66839.398955, test/accuracy=0.648700, test/loss=1.724025, test/num_examples=10000, total_duration=73413.973808, train/accuracy=0.860059, train/loss=0.743062, validation/accuracy=0.766880, validation/loss=1.130856, validation/num_examples=50000
I0205 11:16:34.892204 139946397853440 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.956010341644287, loss=4.476857662200928
I0205 11:17:18.015801 139946414638848 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.1361865997314453, loss=2.9756312370300293
I0205 11:18:04.474650 139946397853440 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.06070876121521, loss=3.0000410079956055
I0205 11:18:51.140789 139946414638848 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.325387716293335, loss=3.0644803047180176
I0205 11:19:37.488296 139946397853440 logging_writer.py:48] [146100] global_step=146100, grad_norm=1.931567668914795, loss=3.1858696937561035
I0205 11:20:24.054771 139946414638848 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.2190353870391846, loss=3.665822982788086
I0205 11:21:10.596142 139946397853440 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.9931626319885254, loss=3.1641149520874023
I0205 11:21:57.532375 139946414638848 logging_writer.py:48] [146400] global_step=146400, grad_norm=2.160234212875366, loss=3.90112566947937
I0205 11:22:44.189349 139946397853440 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.2650158405303955, loss=4.258492469787598
I0205 11:23:24.442912 140107197974336 spec.py:321] Evaluating on the training split.
I0205 11:23:35.062043 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 11:24:05.196783 140107197974336 spec.py:349] Evaluating on the test split.
I0205 11:24:06.795988 140107197974336 submission_runner.py:408] Time since start: 73876.54s, 	Step: 146588, 	{'train/accuracy': 0.864062488079071, 'train/loss': 0.7536942958831787, 'validation/accuracy': 0.7701799869537354, 'validation/loss': 1.145656943321228, 'validation/num_examples': 50000, 'test/accuracy': 0.64410001039505, 'test/loss': 1.748490333557129, 'test/num_examples': 10000, 'score': 67259.50889992714, 'total_duration': 73876.54255223274, 'accumulated_submission_time': 67259.50889992714, 'accumulated_eval_time': 6602.229855775833, 'accumulated_logging_time': 6.350820302963257}
I0205 11:24:06.838610 139946414638848 logging_writer.py:48] [146588] accumulated_eval_time=6602.229856, accumulated_logging_time=6.350820, accumulated_submission_time=67259.508900, global_step=146588, preemption_count=0, score=67259.508900, test/accuracy=0.644100, test/loss=1.748490, test/num_examples=10000, total_duration=73876.542552, train/accuracy=0.864062, train/loss=0.753694, validation/accuracy=0.770180, validation/loss=1.145657, validation/num_examples=50000
I0205 11:24:11.961294 139946397853440 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.1708340644836426, loss=2.854337692260742
I0205 11:24:54.250459 139946414638848 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.063464641571045, loss=2.8890998363494873
I0205 11:25:40.550082 139946397853440 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.1042304039001465, loss=2.9544081687927246
I0205 11:26:27.133731 139946414638848 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.0178468227386475, loss=2.927564859390259
I0205 11:27:13.487660 139946397853440 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.046239137649536, loss=3.7027506828308105
I0205 11:28:00.044725 139946414638848 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.504730701446533, loss=4.456153392791748
I0205 11:28:46.459123 139946397853440 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.093106269836426, loss=3.632930278778076
I0205 11:29:33.127722 139946414638848 logging_writer.py:48] [147300] global_step=147300, grad_norm=1.9976247549057007, loss=2.9939377307891846
I0205 11:30:19.611345 139946397853440 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.1813788414001465, loss=3.021589994430542
I0205 11:31:05.990819 139946414638848 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.159982204437256, loss=2.93652606010437
I0205 11:31:07.056648 140107197974336 spec.py:321] Evaluating on the training split.
I0205 11:31:17.643887 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 11:31:49.868079 140107197974336 spec.py:349] Evaluating on the test split.
I0205 11:31:51.455694 140107197974336 submission_runner.py:408] Time since start: 74341.20s, 	Step: 147504, 	{'train/accuracy': 0.8710546493530273, 'train/loss': 0.708790123462677, 'validation/accuracy': 0.7684599757194519, 'validation/loss': 1.1278865337371826, 'validation/num_examples': 50000, 'test/accuracy': 0.6520000100135803, 'test/loss': 1.7187994718551636, 'test/num_examples': 10000, 'score': 67679.66255092621, 'total_duration': 74341.20226407051, 'accumulated_submission_time': 67679.66255092621, 'accumulated_eval_time': 6646.628881692886, 'accumulated_logging_time': 6.404725551605225}
I0205 11:31:51.497562 139946397853440 logging_writer.py:48] [147504] accumulated_eval_time=6646.628882, accumulated_logging_time=6.404726, accumulated_submission_time=67679.662551, global_step=147504, preemption_count=0, score=67679.662551, test/accuracy=0.652000, test/loss=1.718799, test/num_examples=10000, total_duration=74341.202264, train/accuracy=0.871055, train/loss=0.708790, validation/accuracy=0.768460, validation/loss=1.127887, validation/num_examples=50000
I0205 11:32:31.701261 139946414638848 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.2591941356658936, loss=2.9561500549316406
I0205 11:33:17.793154 139946397853440 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.1987452507019043, loss=2.8491737842559814
I0205 11:34:04.043040 139946414638848 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.082056999206543, loss=2.9289069175720215
I0205 11:34:50.686570 139946397853440 logging_writer.py:48] [147900] global_step=147900, grad_norm=1.9760830402374268, loss=3.061889886856079
I0205 11:35:36.853034 139946414638848 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.896851062774658, loss=4.373296737670898
I0205 11:36:23.422292 139946397853440 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.1191914081573486, loss=2.8906867504119873
I0205 11:37:09.998318 139946414638848 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.1703543663024902, loss=2.8589839935302734
I0205 11:37:56.387614 139946397853440 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.1883859634399414, loss=3.868938446044922
I0205 11:38:42.912939 139946414638848 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.0596909523010254, loss=3.056765079498291
I0205 11:38:51.876351 140107197974336 spec.py:321] Evaluating on the training split.
I0205 11:39:02.646620 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 11:39:34.542404 140107197974336 spec.py:349] Evaluating on the test split.
I0205 11:39:36.138771 140107197974336 submission_runner.py:408] Time since start: 74805.89s, 	Step: 148421, 	{'train/accuracy': 0.8649804592132568, 'train/loss': 0.7262025475502014, 'validation/accuracy': 0.7682799696922302, 'validation/loss': 1.1214993000030518, 'validation/num_examples': 50000, 'test/accuracy': 0.6528000235557556, 'test/loss': 1.7120332717895508, 'test/num_examples': 10000, 'score': 68099.97631287575, 'total_duration': 74805.88531470299, 'accumulated_submission_time': 68099.97631287575, 'accumulated_eval_time': 6690.891248703003, 'accumulated_logging_time': 6.458734512329102}
I0205 11:39:36.182989 139946397853440 logging_writer.py:48] [148421] accumulated_eval_time=6690.891249, accumulated_logging_time=6.458735, accumulated_submission_time=68099.976313, global_step=148421, preemption_count=0, score=68099.976313, test/accuracy=0.652800, test/loss=1.712033, test/num_examples=10000, total_duration=74805.885315, train/accuracy=0.864980, train/loss=0.726203, validation/accuracy=0.768280, validation/loss=1.121499, validation/num_examples=50000
I0205 11:40:08.898917 139946414638848 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.6272709369659424, loss=4.414883613586426
I0205 11:40:55.249425 139946397853440 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.668138027191162, loss=4.4672040939331055
I0205 11:41:42.244013 139946414638848 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.181619644165039, loss=3.421947956085205
I0205 11:42:28.683661 139946397853440 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.1359550952911377, loss=3.007563591003418
I0205 11:43:15.475935 139946414638848 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.5186421871185303, loss=3.2880289554595947
I0205 11:44:02.029008 139946397853440 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.2508280277252197, loss=3.0444672107696533
I0205 11:44:48.531239 139946414638848 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.0731310844421387, loss=3.730145215988159
I0205 11:45:34.683700 139946397853440 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.071810245513916, loss=2.896688222885132
I0205 11:46:20.875937 139946414638848 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.091555118560791, loss=2.833359718322754
I0205 11:46:36.289365 140107197974336 spec.py:321] Evaluating on the training split.
I0205 11:46:47.001085 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 11:47:17.244343 140107197974336 spec.py:349] Evaluating on the test split.
I0205 11:47:18.843558 140107197974336 submission_runner.py:408] Time since start: 75268.59s, 	Step: 149335, 	{'train/accuracy': 0.8691601157188416, 'train/loss': 0.7210755944252014, 'validation/accuracy': 0.7702999711036682, 'validation/loss': 1.124349594116211, 'validation/num_examples': 50000, 'test/accuracy': 0.651900053024292, 'test/loss': 1.711452603340149, 'test/num_examples': 10000, 'score': 68519.64030075073, 'total_duration': 75268.590113163, 'accumulated_submission_time': 68519.64030075073, 'accumulated_eval_time': 6733.445414304733, 'accumulated_logging_time': 6.892820119857788}
I0205 11:47:18.883433 139946397853440 logging_writer.py:48] [149335] accumulated_eval_time=6733.445414, accumulated_logging_time=6.892820, accumulated_submission_time=68519.640301, global_step=149335, preemption_count=0, score=68519.640301, test/accuracy=0.651900, test/loss=1.711453, test/num_examples=10000, total_duration=75268.590113, train/accuracy=0.869160, train/loss=0.721076, validation/accuracy=0.770300, validation/loss=1.124350, validation/num_examples=50000
I0205 11:47:45.052521 139946414638848 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.2827420234680176, loss=3.9422178268432617
I0205 11:48:31.068763 139946397853440 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.1747000217437744, loss=2.8530893325805664
I0205 11:49:17.569577 139946414638848 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.126985549926758, loss=3.6270761489868164
I0205 11:50:04.192691 139946397853440 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.08595871925354, loss=3.770271062850952
I0205 11:50:50.756545 139946414638848 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.36457896232605, loss=3.9437341690063477
I0205 11:51:37.165487 139946397853440 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.4516959190368652, loss=4.2873029708862305
I0205 11:52:23.831244 139946414638848 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.2632391452789307, loss=3.2765140533447266
I0205 11:53:10.337536 139946397853440 logging_writer.py:48] [150100] global_step=150100, grad_norm=1.990998387336731, loss=2.8835251331329346
I0205 11:53:56.797224 139946414638848 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.1649739742279053, loss=2.975198745727539
I0205 11:54:19.199246 140107197974336 spec.py:321] Evaluating on the training split.
I0205 11:54:31.026597 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 11:55:03.565091 140107197974336 spec.py:349] Evaluating on the test split.
I0205 11:55:05.163678 140107197974336 submission_runner.py:408] Time since start: 75734.91s, 	Step: 150250, 	{'train/accuracy': 0.8727734088897705, 'train/loss': 0.6951795816421509, 'validation/accuracy': 0.770859956741333, 'validation/loss': 1.1124303340911865, 'validation/num_examples': 50000, 'test/accuracy': 0.6508000493049622, 'test/loss': 1.714530348777771, 'test/num_examples': 10000, 'score': 68939.89294409752, 'total_duration': 75734.91024708748, 'accumulated_submission_time': 68939.89294409752, 'accumulated_eval_time': 6779.409840583801, 'accumulated_logging_time': 6.94247579574585}
I0205 11:55:05.205940 139946397853440 logging_writer.py:48] [150250] accumulated_eval_time=6779.409841, accumulated_logging_time=6.942476, accumulated_submission_time=68939.892944, global_step=150250, preemption_count=0, score=68939.892944, test/accuracy=0.650800, test/loss=1.714530, test/num_examples=10000, total_duration=75734.910247, train/accuracy=0.872773, train/loss=0.695180, validation/accuracy=0.770860, validation/loss=1.112430, validation/num_examples=50000
I0205 11:55:25.261173 139946414638848 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.7889018058776855, loss=4.211799144744873
I0205 11:56:10.016432 139946397853440 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.300952196121216, loss=2.9145288467407227
I0205 11:56:56.468091 139946414638848 logging_writer.py:48] [150500] global_step=150500, grad_norm=1.9930226802825928, loss=3.052168369293213
I0205 11:57:43.384619 139946397853440 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.1966607570648193, loss=2.9015791416168213
I0205 11:58:29.841571 139946414638848 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.30381441116333, loss=2.971541166305542
I0205 11:59:16.348415 139946397853440 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.308713912963867, loss=4.0775556564331055
I0205 12:00:02.638952 139946414638848 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.210289478302002, loss=2.9778051376342773
I0205 12:00:49.024886 139946397853440 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.3005149364471436, loss=3.7662770748138428
I0205 12:01:35.542523 139946414638848 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.2721307277679443, loss=2.8600869178771973
I0205 12:02:05.301320 140107197974336 spec.py:321] Evaluating on the training split.
I0205 12:02:15.952511 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 12:02:44.030234 140107197974336 spec.py:349] Evaluating on the test split.
I0205 12:02:45.623206 140107197974336 submission_runner.py:408] Time since start: 76195.37s, 	Step: 151165, 	{'train/accuracy': 0.8675585985183716, 'train/loss': 0.7195205688476562, 'validation/accuracy': 0.7733199596405029, 'validation/loss': 1.1224658489227295, 'validation/num_examples': 50000, 'test/accuracy': 0.6515000462532043, 'test/loss': 1.7132558822631836, 'test/num_examples': 10000, 'score': 69359.92496109009, 'total_duration': 76195.36973547935, 'accumulated_submission_time': 69359.92496109009, 'accumulated_eval_time': 6819.731669664383, 'accumulated_logging_time': 6.9958555698394775}
I0205 12:02:45.671710 139946397853440 logging_writer.py:48] [151165] accumulated_eval_time=6819.731670, accumulated_logging_time=6.995856, accumulated_submission_time=69359.924961, global_step=151165, preemption_count=0, score=69359.924961, test/accuracy=0.651500, test/loss=1.713256, test/num_examples=10000, total_duration=76195.369735, train/accuracy=0.867559, train/loss=0.719521, validation/accuracy=0.773320, validation/loss=1.122466, validation/num_examples=50000
I0205 12:02:59.849823 139946414638848 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.316354751586914, loss=2.9331486225128174
I0205 12:03:43.667870 139946397853440 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.8785910606384277, loss=4.365211009979248
I0205 12:04:30.631287 139946414638848 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.121490716934204, loss=2.873831272125244
I0205 12:05:17.432464 139946397853440 logging_writer.py:48] [151500] global_step=151500, grad_norm=1.9979673624038696, loss=3.004624366760254
I0205 12:06:03.982189 139946414638848 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.2829864025115967, loss=2.825955390930176
I0205 12:06:50.294821 139946397853440 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.8265743255615234, loss=4.32647180557251
I0205 12:07:36.739264 139946414638848 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.2130141258239746, loss=3.745979070663452
I0205 12:08:23.295657 139946397853440 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.525782823562622, loss=4.267113208770752
I0205 12:09:09.909101 139946414638848 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.2389161586761475, loss=3.8578784465789795
I0205 12:09:45.711458 140107197974336 spec.py:321] Evaluating on the training split.
I0205 12:09:56.687059 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 12:10:27.663261 140107197974336 spec.py:349] Evaluating on the test split.
I0205 12:10:29.265218 140107197974336 submission_runner.py:408] Time since start: 76659.01s, 	Step: 152079, 	{'train/accuracy': 0.8717772960662842, 'train/loss': 0.6925562620162964, 'validation/accuracy': 0.7725399732589722, 'validation/loss': 1.0992447137832642, 'validation/num_examples': 50000, 'test/accuracy': 0.6582000255584717, 'test/loss': 1.690200686454773, 'test/num_examples': 10000, 'score': 69779.90133500099, 'total_duration': 76659.01178598404, 'accumulated_submission_time': 69779.90133500099, 'accumulated_eval_time': 6863.285403966904, 'accumulated_logging_time': 7.0559470653533936}
I0205 12:10:29.312440 139946397853440 logging_writer.py:48] [152079] accumulated_eval_time=6863.285404, accumulated_logging_time=7.055947, accumulated_submission_time=69779.901335, global_step=152079, preemption_count=0, score=69779.901335, test/accuracy=0.658200, test/loss=1.690201, test/num_examples=10000, total_duration=76659.011786, train/accuracy=0.871777, train/loss=0.692556, validation/accuracy=0.772540, validation/loss=1.099245, validation/num_examples=50000
I0205 12:10:37.958476 139946414638848 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.1850554943084717, loss=3.5311477184295654
I0205 12:11:20.835582 139946397853440 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.188518762588501, loss=2.874940872192383
I0205 12:12:07.189404 139946414638848 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.354936122894287, loss=3.657010078430176
I0205 12:12:53.924821 139946397853440 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.264504909515381, loss=2.867743492126465
I0205 12:13:40.356422 139946414638848 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.210160255432129, loss=2.959684371948242
I0205 12:14:26.833328 139946397853440 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.118170738220215, loss=2.914987087249756
I0205 12:15:13.283191 139946414638848 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.435811996459961, loss=4.072216510772705
I0205 12:15:59.250020 139946397853440 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.2221922874450684, loss=3.431227207183838
I0205 12:16:45.789605 139946414638848 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.2208704948425293, loss=3.330855131149292
I0205 12:17:29.350122 140107197974336 spec.py:321] Evaluating on the training split.
I0205 12:17:40.056040 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 12:18:05.834868 140107197974336 spec.py:349] Evaluating on the test split.
I0205 12:18:07.436030 140107197974336 submission_runner.py:408] Time since start: 77117.18s, 	Step: 152996, 	{'train/accuracy': 0.8743945360183716, 'train/loss': 0.6965427398681641, 'validation/accuracy': 0.7730000019073486, 'validation/loss': 1.1139885187149048, 'validation/num_examples': 50000, 'test/accuracy': 0.6490000486373901, 'test/loss': 1.7110410928726196, 'test/num_examples': 10000, 'score': 70199.87671470642, 'total_duration': 77117.18258023262, 'accumulated_submission_time': 70199.87671470642, 'accumulated_eval_time': 6901.371284723282, 'accumulated_logging_time': 7.113300085067749}
I0205 12:18:07.482532 139946397853440 logging_writer.py:48] [152996] accumulated_eval_time=6901.371285, accumulated_logging_time=7.113300, accumulated_submission_time=70199.876715, global_step=152996, preemption_count=0, score=70199.876715, test/accuracy=0.649000, test/loss=1.711041, test/num_examples=10000, total_duration=77117.182580, train/accuracy=0.874395, train/loss=0.696543, validation/accuracy=0.773000, validation/loss=1.113989, validation/num_examples=50000
I0205 12:18:09.461708 139946414638848 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.2081058025360107, loss=2.8543789386749268
I0205 12:18:51.053717 139946397853440 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.35622501373291, loss=2.9012529850006104
I0205 12:19:37.482046 139946414638848 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.1896867752075195, loss=2.9724645614624023
I0205 12:20:24.505995 139946397853440 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.5503180027008057, loss=4.113774299621582
I0205 12:21:10.986662 139946414638848 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.1194231510162354, loss=3.4797630310058594
I0205 12:21:57.486162 139946397853440 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.156453847885132, loss=3.481654405593872
I0205 12:22:43.882412 139946414638848 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.0993764400482178, loss=3.151746988296509
I0205 12:23:30.668727 139946397853440 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.2502388954162598, loss=2.922130823135376
I0205 12:24:17.257049 139946414638848 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.5008223056793213, loss=3.9889495372772217
I0205 12:25:04.145766 139946397853440 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.2220520973205566, loss=3.532667636871338
I0205 12:25:07.573514 140107197974336 spec.py:321] Evaluating on the training split.
I0205 12:25:18.339306 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 12:25:48.472296 140107197974336 spec.py:349] Evaluating on the test split.
I0205 12:25:50.080468 140107197974336 submission_runner.py:408] Time since start: 77579.83s, 	Step: 153909, 	{'train/accuracy': 0.8716406226158142, 'train/loss': 0.702102541923523, 'validation/accuracy': 0.7744199633598328, 'validation/loss': 1.1024911403656006, 'validation/num_examples': 50000, 'test/accuracy': 0.6552000045776367, 'test/loss': 1.6988980770111084, 'test/num_examples': 10000, 'score': 70619.90300965309, 'total_duration': 77579.82703661919, 'accumulated_submission_time': 70619.90300965309, 'accumulated_eval_time': 6943.878223657608, 'accumulated_logging_time': 7.172998905181885}
I0205 12:25:50.122455 139946414638848 logging_writer.py:48] [153909] accumulated_eval_time=6943.878224, accumulated_logging_time=7.172999, accumulated_submission_time=70619.903010, global_step=153909, preemption_count=0, score=70619.903010, test/accuracy=0.655200, test/loss=1.698898, test/num_examples=10000, total_duration=77579.827037, train/accuracy=0.871641, train/loss=0.702103, validation/accuracy=0.774420, validation/loss=1.102491, validation/num_examples=50000
I0205 12:26:27.884224 139946397853440 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.221364974975586, loss=3.3483521938323975
I0205 12:27:13.719560 139946414638848 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.1319382190704346, loss=3.292215585708618
I0205 12:28:00.170941 139946397853440 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.2446157932281494, loss=2.81518816947937
I0205 12:28:46.563250 139946414638848 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.664537191390991, loss=4.380660533905029
I0205 12:29:33.092746 139946397853440 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.22676157951355, loss=2.861504077911377
I0205 12:30:19.419866 139946414638848 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.360381603240967, loss=2.8560385704040527
I0205 12:31:05.934292 139946397853440 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.247523546218872, loss=2.922928810119629
I0205 12:31:52.112509 139946414638848 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.24117374420166, loss=2.7916386127471924
I0205 12:32:38.543514 139946397853440 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.192793130874634, loss=2.9140312671661377
I0205 12:32:50.221237 140107197974336 spec.py:321] Evaluating on the training split.
I0205 12:33:01.209778 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 12:33:29.129440 140107197974336 spec.py:349] Evaluating on the test split.
I0205 12:33:30.731401 140107197974336 submission_runner.py:408] Time since start: 78040.48s, 	Step: 154827, 	{'train/accuracy': 0.8707226514816284, 'train/loss': 0.7155790328979492, 'validation/accuracy': 0.7752999663352966, 'validation/loss': 1.1152511835098267, 'validation/num_examples': 50000, 'test/accuracy': 0.6602000594139099, 'test/loss': 1.70155668258667, 'test/num_examples': 10000, 'score': 71039.93932843208, 'total_duration': 78040.47797226906, 'accumulated_submission_time': 71039.93932843208, 'accumulated_eval_time': 6984.388374567032, 'accumulated_logging_time': 7.224968194961548}
I0205 12:33:30.776203 139946414638848 logging_writer.py:48] [154827] accumulated_eval_time=6984.388375, accumulated_logging_time=7.224968, accumulated_submission_time=71039.939328, global_step=154827, preemption_count=0, score=71039.939328, test/accuracy=0.660200, test/loss=1.701557, test/num_examples=10000, total_duration=78040.477972, train/accuracy=0.870723, train/loss=0.715579, validation/accuracy=0.775300, validation/loss=1.115251, validation/num_examples=50000
I0205 12:34:00.090829 139946397853440 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.223766565322876, loss=4.340256690979004
I0205 12:34:46.577773 139946414638848 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.2137231826782227, loss=2.999068021774292
I0205 12:35:33.369789 139946397853440 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.1796605587005615, loss=2.853015661239624
I0205 12:36:19.878842 139946414638848 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.302259922027588, loss=3.8199312686920166
I0205 12:37:06.495032 139946397853440 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.27622652053833, loss=2.907621383666992
I0205 12:37:53.005431 139946414638848 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.2392690181732178, loss=2.879502773284912
I0205 12:38:39.546002 139946397853440 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.278531551361084, loss=2.872547149658203
I0205 12:39:26.081060 139946414638848 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.3379995822906494, loss=2.8560874462127686
I0205 12:40:12.502219 139946397853440 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.523329734802246, loss=2.8638670444488525
I0205 12:40:31.179585 140107197974336 spec.py:321] Evaluating on the training split.
I0205 12:40:42.018526 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 12:41:10.919970 140107197974336 spec.py:349] Evaluating on the test split.
I0205 12:41:12.510106 140107197974336 submission_runner.py:408] Time since start: 78502.26s, 	Step: 155742, 	{'train/accuracy': 0.8770117163658142, 'train/loss': 0.6983265280723572, 'validation/accuracy': 0.7755199670791626, 'validation/loss': 1.1166795492172241, 'validation/num_examples': 50000, 'test/accuracy': 0.6535000205039978, 'test/loss': 1.7088316679000854, 'test/num_examples': 10000, 'score': 71460.28168869019, 'total_duration': 78502.2566742897, 'accumulated_submission_time': 71460.28168869019, 'accumulated_eval_time': 7025.718888044357, 'accumulated_logging_time': 7.279221773147583}
I0205 12:41:12.553884 139946414638848 logging_writer.py:48] [155742] accumulated_eval_time=7025.718888, accumulated_logging_time=7.279222, accumulated_submission_time=71460.281689, global_step=155742, preemption_count=0, score=71460.281689, test/accuracy=0.653500, test/loss=1.708832, test/num_examples=10000, total_duration=78502.256674, train/accuracy=0.877012, train/loss=0.698327, validation/accuracy=0.775520, validation/loss=1.116680, validation/num_examples=50000
I0205 12:41:35.768707 139946397853440 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.3135788440704346, loss=2.782130002975464
I0205 12:42:21.156290 139946414638848 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.3583836555480957, loss=3.442962169647217
I0205 12:43:07.775491 139946397853440 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.4556527137756348, loss=3.952059507369995
I0205 12:43:54.129879 139946414638848 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.2140252590179443, loss=3.043362855911255
I0205 12:44:40.973074 139946397853440 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.2947962284088135, loss=3.3549747467041016
I0205 12:45:27.423684 139946414638848 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.22705340385437, loss=2.8181309700012207
I0205 12:46:14.172260 139946397853440 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.381047487258911, loss=2.881786346435547
I0205 12:47:00.483926 139946414638848 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.1517155170440674, loss=2.96803617477417
I0205 12:47:46.825964 139946397853440 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.4333178997039795, loss=2.8349690437316895
I0205 12:48:12.638021 140107197974336 spec.py:321] Evaluating on the training split.
I0205 12:48:23.461203 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 12:48:51.767328 140107197974336 spec.py:349] Evaluating on the test split.
I0205 12:48:53.377044 140107197974336 submission_runner.py:408] Time since start: 78963.12s, 	Step: 156657, 	{'train/accuracy': 0.8760351538658142, 'train/loss': 0.6916414499282837, 'validation/accuracy': 0.7758199572563171, 'validation/loss': 1.1019681692123413, 'validation/num_examples': 50000, 'test/accuracy': 0.657200038433075, 'test/loss': 1.6836694478988647, 'test/num_examples': 10000, 'score': 71880.30394387245, 'total_duration': 78963.12360739708, 'accumulated_submission_time': 71880.30394387245, 'accumulated_eval_time': 7066.457926273346, 'accumulated_logging_time': 7.332519292831421}
I0205 12:48:53.419276 139946414638848 logging_writer.py:48] [156657] accumulated_eval_time=7066.457926, accumulated_logging_time=7.332519, accumulated_submission_time=71880.303944, global_step=156657, preemption_count=0, score=71880.303944, test/accuracy=0.657200, test/loss=1.683669, test/num_examples=10000, total_duration=78963.123607, train/accuracy=0.876035, train/loss=0.691641, validation/accuracy=0.775820, validation/loss=1.101968, validation/num_examples=50000
I0205 12:49:10.714818 139946397853440 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.754117727279663, loss=4.333309650421143
I0205 12:49:54.854216 139946414638848 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.3341221809387207, loss=3.6864736080169678
I0205 12:50:41.283886 139946397853440 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.2691831588745117, loss=2.7752490043640137
I0205 12:51:27.995302 139946414638848 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.3272452354431152, loss=2.847034454345703
I0205 12:52:14.480007 139946397853440 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.2150559425354004, loss=3.351297378540039
I0205 12:53:00.546165 139946414638848 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.209104537963867, loss=3.567105293273926
I0205 12:53:46.819419 139946397853440 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.4263622760772705, loss=2.8427329063415527
I0205 12:54:33.391122 139946414638848 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.44881534576416, loss=2.8330750465393066
I0205 12:55:19.899172 139946397853440 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.2829601764678955, loss=2.799619674682617
I0205 12:55:53.682001 140107197974336 spec.py:321] Evaluating on the training split.
I0205 12:56:04.475729 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 12:56:35.817376 140107197974336 spec.py:349] Evaluating on the test split.
I0205 12:56:37.421303 140107197974336 submission_runner.py:408] Time since start: 79427.17s, 	Step: 157574, 	{'train/accuracy': 0.8784960508346558, 'train/loss': 0.6948211789131165, 'validation/accuracy': 0.7772799730300903, 'validation/loss': 1.1076263189315796, 'validation/num_examples': 50000, 'test/accuracy': 0.6581000089645386, 'test/loss': 1.694753646850586, 'test/num_examples': 10000, 'score': 72300.49660873413, 'total_duration': 79427.16787004471, 'accumulated_submission_time': 72300.49660873413, 'accumulated_eval_time': 7110.1972115039825, 'accumulated_logging_time': 7.3921473026275635}
I0205 12:56:37.462437 139946414638848 logging_writer.py:48] [157574] accumulated_eval_time=7110.197212, accumulated_logging_time=7.392147, accumulated_submission_time=72300.496609, global_step=157574, preemption_count=0, score=72300.496609, test/accuracy=0.658100, test/loss=1.694754, test/num_examples=10000, total_duration=79427.167870, train/accuracy=0.878496, train/loss=0.694821, validation/accuracy=0.777280, validation/loss=1.107626, validation/num_examples=50000
I0205 12:56:48.113838 139946397853440 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.314964532852173, loss=3.157877206802368
I0205 12:57:31.539671 139946414638848 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.3164560794830322, loss=2.7916464805603027
I0205 12:58:17.608365 139946397853440 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.3585739135742188, loss=2.8173470497131348
I0205 12:59:04.096861 139946414638848 logging_writer.py:48] [157900] global_step=157900, grad_norm=3.2079312801361084, loss=4.341676712036133
I0205 12:59:50.309257 139946397853440 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.2878758907318115, loss=3.376390218734741
I0205 13:00:37.029846 139946414638848 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.4944424629211426, loss=2.964716672897339
I0205 13:01:23.591484 139946397853440 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.5529913902282715, loss=3.690911293029785
I0205 13:02:10.261276 139946414638848 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.2525744438171387, loss=3.5960161685943604
I0205 13:02:56.917860 139946397853440 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.3042824268341064, loss=2.764730215072632
I0205 13:03:37.826865 140107197974336 spec.py:321] Evaluating on the training split.
I0205 13:03:48.656556 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 13:04:20.100080 140107197974336 spec.py:349] Evaluating on the test split.
I0205 13:04:21.689595 140107197974336 submission_runner.py:408] Time since start: 79891.44s, 	Step: 158489, 	{'train/accuracy': 0.8775194883346558, 'train/loss': 0.6820436716079712, 'validation/accuracy': 0.776419997215271, 'validation/loss': 1.095947504043579, 'validation/num_examples': 50000, 'test/accuracy': 0.6564000248908997, 'test/loss': 1.6833994388580322, 'test/num_examples': 10000, 'score': 72720.79914164543, 'total_duration': 79891.43614602089, 'accumulated_submission_time': 72720.79914164543, 'accumulated_eval_time': 7154.0599048137665, 'accumulated_logging_time': 7.443153619766235}
I0205 13:04:21.730813 139946414638848 logging_writer.py:48] [158489] accumulated_eval_time=7154.059905, accumulated_logging_time=7.443154, accumulated_submission_time=72720.799142, global_step=158489, preemption_count=0, score=72720.799142, test/accuracy=0.656400, test/loss=1.683399, test/num_examples=10000, total_duration=79891.436146, train/accuracy=0.877519, train/loss=0.682044, validation/accuracy=0.776420, validation/loss=1.095948, validation/num_examples=50000
I0205 13:04:26.456650 139946397853440 logging_writer.py:48] [158500] global_step=158500, grad_norm=3.1229169368743896, loss=4.313107013702393
I0205 13:05:09.014737 139946414638848 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.397695302963257, loss=2.8284623622894287
I0205 13:05:55.145773 139946397853440 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.454721450805664, loss=2.799147605895996
I0205 13:06:41.962863 139946414638848 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.501970052719116, loss=2.8304336071014404
I0205 13:07:28.737642 139946397853440 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.4618475437164307, loss=3.433816432952881
I0205 13:08:15.377464 139946414638848 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.321209192276001, loss=2.849925994873047
I0205 13:09:02.003647 139946397853440 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.288763999938965, loss=2.817117691040039
I0205 13:09:48.387692 139946414638848 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.584627151489258, loss=3.2206766605377197
I0205 13:10:34.916303 139946397853440 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.357779026031494, loss=3.2387685775756836
I0205 13:11:21.477742 139946414638848 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.329463005065918, loss=2.8588552474975586
I0205 13:11:22.044462 140107197974336 spec.py:321] Evaluating on the training split.
I0205 13:11:32.864339 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 13:12:04.134004 140107197974336 spec.py:349] Evaluating on the test split.
I0205 13:12:05.731488 140107197974336 submission_runner.py:408] Time since start: 80355.48s, 	Step: 159403, 	{'train/accuracy': 0.8854882717132568, 'train/loss': 0.6608580350875854, 'validation/accuracy': 0.7779200077056885, 'validation/loss': 1.1026121377944946, 'validation/num_examples': 50000, 'test/accuracy': 0.6584000587463379, 'test/loss': 1.6912035942077637, 'test/num_examples': 10000, 'score': 73141.05099487305, 'total_duration': 80355.4780535698, 'accumulated_submission_time': 73141.05099487305, 'accumulated_eval_time': 7197.746908187866, 'accumulated_logging_time': 7.494960784912109}
I0205 13:12:05.772181 139946397853440 logging_writer.py:48] [159403] accumulated_eval_time=7197.746908, accumulated_logging_time=7.494961, accumulated_submission_time=73141.050995, global_step=159403, preemption_count=0, score=73141.050995, test/accuracy=0.658400, test/loss=1.691204, test/num_examples=10000, total_duration=80355.478054, train/accuracy=0.885488, train/loss=0.660858, validation/accuracy=0.777920, validation/loss=1.102612, validation/num_examples=50000
I0205 13:12:46.387636 139946414638848 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.318887948989868, loss=2.7482495307922363
I0205 13:13:32.568725 139946397853440 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.2580461502075195, loss=2.813951015472412
I0205 13:14:19.371593 139946414638848 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.2987217903137207, loss=2.9814293384552
I0205 13:15:06.009575 139946397853440 logging_writer.py:48] [159800] global_step=159800, grad_norm=3.3017377853393555, loss=4.288636207580566
I0205 13:15:52.537060 139946414638848 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.3651602268218994, loss=2.770419120788574
I0205 13:16:38.965510 139946397853440 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.661970853805542, loss=3.8649582862854004
I0205 13:17:25.492223 139946414638848 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.123042583465576, loss=4.22404146194458
I0205 13:18:11.928185 139946397853440 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.5657460689544678, loss=2.854856252670288
I0205 13:18:58.175976 139946414638848 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.345703601837158, loss=2.8389737606048584
I0205 13:19:05.798617 140107197974336 spec.py:321] Evaluating on the training split.
I0205 13:19:16.542790 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 13:19:49.741145 140107197974336 spec.py:349] Evaluating on the test split.
I0205 13:19:51.336613 140107197974336 submission_runner.py:408] Time since start: 80821.08s, 	Step: 160318, 	{'train/accuracy': 0.8786523342132568, 'train/loss': 0.6797360181808472, 'validation/accuracy': 0.778939962387085, 'validation/loss': 1.0944668054580688, 'validation/num_examples': 50000, 'test/accuracy': 0.6579000353813171, 'test/loss': 1.6871545314788818, 'test/num_examples': 10000, 'score': 73561.01437163353, 'total_duration': 80821.08317613602, 'accumulated_submission_time': 73561.01437163353, 'accumulated_eval_time': 7243.284869670868, 'accumulated_logging_time': 7.547072887420654}
I0205 13:19:51.380514 139946397853440 logging_writer.py:48] [160318] accumulated_eval_time=7243.284870, accumulated_logging_time=7.547073, accumulated_submission_time=73561.014372, global_step=160318, preemption_count=0, score=73561.014372, test/accuracy=0.657900, test/loss=1.687155, test/num_examples=10000, total_duration=80821.083176, train/accuracy=0.878652, train/loss=0.679736, validation/accuracy=0.778940, validation/loss=1.094467, validation/num_examples=50000
I0205 13:20:24.959246 139946414638848 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.3724112510681152, loss=3.1922264099121094
I0205 13:21:11.042961 139946397853440 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.2070181369781494, loss=2.741436719894409
I0205 13:21:57.557771 139946414638848 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.35575270652771, loss=2.8568673133850098
I0205 13:22:43.993176 139946397853440 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.3552517890930176, loss=2.8178019523620605
I0205 13:23:30.227827 139946414638848 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.4685745239257812, loss=3.187727451324463
I0205 13:24:16.732117 139946397853440 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.4126760959625244, loss=3.2027182579040527
I0205 13:25:03.300845 139946414638848 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.371882915496826, loss=3.022243022918701
I0205 13:25:49.525640 139946397853440 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.5698118209838867, loss=2.900301456451416
I0205 13:26:35.831442 139946414638848 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.354210138320923, loss=2.8559300899505615
I0205 13:26:51.364233 140107197974336 spec.py:321] Evaluating on the training split.
I0205 13:27:02.186479 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 13:27:26.906816 140107197974336 spec.py:349] Evaluating on the test split.
I0205 13:27:28.507223 140107197974336 submission_runner.py:408] Time since start: 81278.25s, 	Step: 161235, 	{'train/accuracy': 0.8811132907867432, 'train/loss': 0.6593165993690491, 'validation/accuracy': 0.7783799767494202, 'validation/loss': 1.0817325115203857, 'validation/num_examples': 50000, 'test/accuracy': 0.6635000109672546, 'test/loss': 1.6645426750183105, 'test/num_examples': 10000, 'score': 73980.93570017815, 'total_duration': 81278.25377750397, 'accumulated_submission_time': 73980.93570017815, 'accumulated_eval_time': 7280.42783331871, 'accumulated_logging_time': 7.600719690322876}
I0205 13:27:28.548997 139946397853440 logging_writer.py:48] [161235] accumulated_eval_time=7280.427833, accumulated_logging_time=7.600720, accumulated_submission_time=73980.935700, global_step=161235, preemption_count=0, score=73980.935700, test/accuracy=0.663500, test/loss=1.664543, test/num_examples=10000, total_duration=81278.253778, train/accuracy=0.881113, train/loss=0.659317, validation/accuracy=0.778380, validation/loss=1.081733, validation/num_examples=50000
I0205 13:27:54.529617 139946414638848 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.3090102672576904, loss=2.7949562072753906
I0205 13:28:40.709742 139946397853440 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.993603467941284, loss=4.239401817321777
I0205 13:29:27.109699 139946414638848 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.419917106628418, loss=3.4092912673950195
I0205 13:30:13.838730 139946397853440 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.4641168117523193, loss=3.2642719745635986
I0205 13:31:00.267455 139946414638848 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.474351167678833, loss=2.761709213256836
I0205 13:31:46.662835 139946397853440 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.721569538116455, loss=4.312577724456787
I0205 13:32:33.191648 139946414638848 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.386432409286499, loss=3.4391415119171143
I0205 13:33:19.645170 139946397853440 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.2595162391662598, loss=3.0153555870056152
I0205 13:34:06.249240 139946414638848 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.3736343383789062, loss=2.7786872386932373
I0205 13:34:28.820356 140107197974336 spec.py:321] Evaluating on the training split.
I0205 13:34:39.942622 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 13:35:10.727925 140107197974336 spec.py:349] Evaluating on the test split.
I0205 13:35:12.318700 140107197974336 submission_runner.py:408] Time since start: 81742.07s, 	Step: 162150, 	{'train/accuracy': 0.8886327743530273, 'train/loss': 0.6440286040306091, 'validation/accuracy': 0.7780199646949768, 'validation/loss': 1.0898090600967407, 'validation/num_examples': 50000, 'test/accuracy': 0.6640000343322754, 'test/loss': 1.6774790287017822, 'test/num_examples': 10000, 'score': 74401.1434378624, 'total_duration': 81742.06526184082, 'accumulated_submission_time': 74401.1434378624, 'accumulated_eval_time': 7323.926148414612, 'accumulated_logging_time': 7.653084754943848}
I0205 13:35:12.363899 139946397853440 logging_writer.py:48] [162150] accumulated_eval_time=7323.926148, accumulated_logging_time=7.653085, accumulated_submission_time=74401.143438, global_step=162150, preemption_count=0, score=74401.143438, test/accuracy=0.664000, test/loss=1.677479, test/num_examples=10000, total_duration=81742.065262, train/accuracy=0.888633, train/loss=0.644029, validation/accuracy=0.778020, validation/loss=1.089809, validation/num_examples=50000
I0205 13:35:32.427117 139946414638848 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.4203383922576904, loss=3.4146997928619385
I0205 13:36:17.366855 139946397853440 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.45045804977417, loss=2.8471760749816895
I0205 13:37:03.671653 139946414638848 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.5509510040283203, loss=3.5447587966918945
I0205 13:37:50.053153 139946397853440 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.4657375812530518, loss=2.818732738494873
I0205 13:38:36.337145 139946414638848 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.3094208240509033, loss=2.6713879108428955
I0205 13:39:23.161075 139946397853440 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.632611036300659, loss=3.90431547164917
I0205 13:40:10.012586 139946414638848 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.245581865310669, loss=3.1188554763793945
I0205 13:40:56.223271 139946397853440 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.3453540802001953, loss=2.859102249145508
I0205 13:41:42.701550 139946414638848 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.2195229530334473, loss=3.017935276031494
I0205 13:42:12.540615 140107197974336 spec.py:321] Evaluating on the training split.
I0205 13:42:23.319841 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 13:42:54.805497 140107197974336 spec.py:349] Evaluating on the test split.
I0205 13:42:56.400304 140107197974336 submission_runner.py:408] Time since start: 82206.15s, 	Step: 163066, 	{'train/accuracy': 0.8836523294448853, 'train/loss': 0.6655579805374146, 'validation/accuracy': 0.7795799970626831, 'validation/loss': 1.0824384689331055, 'validation/num_examples': 50000, 'test/accuracy': 0.6609000563621521, 'test/loss': 1.669062614440918, 'test/num_examples': 10000, 'score': 74821.25588536263, 'total_duration': 82206.14687275887, 'accumulated_submission_time': 74821.25588536263, 'accumulated_eval_time': 7367.785813331604, 'accumulated_logging_time': 7.709946155548096}
I0205 13:42:56.445735 139946397853440 logging_writer.py:48] [163066] accumulated_eval_time=7367.785813, accumulated_logging_time=7.709946, accumulated_submission_time=74821.255885, global_step=163066, preemption_count=0, score=74821.255885, test/accuracy=0.660900, test/loss=1.669063, test/num_examples=10000, total_duration=82206.146873, train/accuracy=0.883652, train/loss=0.665558, validation/accuracy=0.779580, validation/loss=1.082438, validation/num_examples=50000
I0205 13:43:10.204488 139946414638848 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.3983960151672363, loss=2.803600549697876
I0205 13:43:53.685248 139946397853440 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.5706050395965576, loss=2.8124144077301025
I0205 13:44:40.599074 139946414638848 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.262787342071533, loss=2.83071231842041
I0205 13:45:27.494891 139946397853440 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.44065523147583, loss=2.813516139984131
I0205 13:46:14.123026 139946414638848 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.4039597511291504, loss=2.8392956256866455
I0205 13:47:00.453967 139946397853440 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.636610746383667, loss=3.0016252994537354
I0205 13:47:46.973671 139946414638848 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.6834185123443604, loss=2.8593337535858154
I0205 13:48:33.519189 139946397853440 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.7327709197998047, loss=3.8037095069885254
I0205 13:49:20.327704 139946414638848 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.490935802459717, loss=2.841643810272217
I0205 13:49:56.606012 140107197974336 spec.py:321] Evaluating on the training split.
I0205 13:50:07.417761 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 13:50:39.531525 140107197974336 spec.py:349] Evaluating on the test split.
I0205 13:50:41.134718 140107197974336 submission_runner.py:408] Time since start: 82670.88s, 	Step: 163980, 	{'train/accuracy': 0.8823828101158142, 'train/loss': 0.6626566648483276, 'validation/accuracy': 0.7808399796485901, 'validation/loss': 1.0874329805374146, 'validation/num_examples': 50000, 'test/accuracy': 0.659500002861023, 'test/loss': 1.6772193908691406, 'test/num_examples': 10000, 'score': 75241.35402417183, 'total_duration': 82670.88124489784, 'accumulated_submission_time': 75241.35402417183, 'accumulated_eval_time': 7412.314453601837, 'accumulated_logging_time': 7.765376091003418}
I0205 13:50:41.183419 139946397853440 logging_writer.py:48] [163980] accumulated_eval_time=7412.314454, accumulated_logging_time=7.765376, accumulated_submission_time=75241.354024, global_step=163980, preemption_count=0, score=75241.354024, test/accuracy=0.659500, test/loss=1.677219, test/num_examples=10000, total_duration=82670.881245, train/accuracy=0.882383, train/loss=0.662657, validation/accuracy=0.780840, validation/loss=1.087433, validation/num_examples=50000
I0205 13:50:49.432477 139946414638848 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.572573184967041, loss=2.8094115257263184
I0205 13:51:31.912390 139946397853440 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.3471505641937256, loss=2.8304567337036133
I0205 13:52:18.486201 139946414638848 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.525984048843384, loss=2.793794870376587
I0205 13:53:05.397470 139946397853440 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.465484619140625, loss=2.7809205055236816
I0205 13:53:51.674432 139946414638848 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.4260270595550537, loss=2.8173329830169678
I0205 13:54:38.415231 139946397853440 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.552234649658203, loss=2.768106460571289
I0205 13:55:24.885879 139946414638848 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.641146421432495, loss=2.767068862915039
I0205 13:56:11.435472 139946397853440 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.313426971435547, loss=2.9270689487457275
I0205 13:56:57.725079 139946414638848 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.482527256011963, loss=2.8412973880767822
I0205 13:57:41.526297 140107197974336 spec.py:321] Evaluating on the training split.
I0205 13:57:52.342611 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 13:58:23.413904 140107197974336 spec.py:349] Evaluating on the test split.
I0205 13:58:25.012392 140107197974336 submission_runner.py:408] Time since start: 83134.76s, 	Step: 164896, 	{'train/accuracy': 0.8887304663658142, 'train/loss': 0.6466950178146362, 'validation/accuracy': 0.7809199690818787, 'validation/loss': 1.0870565176010132, 'validation/num_examples': 50000, 'test/accuracy': 0.6633000373840332, 'test/loss': 1.6752506494522095, 'test/num_examples': 10000, 'score': 75661.63371706009, 'total_duration': 83134.75895619392, 'accumulated_submission_time': 75661.63371706009, 'accumulated_eval_time': 7455.80052113533, 'accumulated_logging_time': 7.824825286865234}
I0205 13:58:25.060034 139946397853440 logging_writer.py:48] [164896] accumulated_eval_time=7455.800521, accumulated_logging_time=7.824825, accumulated_submission_time=75661.633717, global_step=164896, preemption_count=0, score=75661.633717, test/accuracy=0.663300, test/loss=1.675251, test/num_examples=10000, total_duration=83134.758956, train/accuracy=0.888730, train/loss=0.646695, validation/accuracy=0.780920, validation/loss=1.087057, validation/num_examples=50000
I0205 13:58:27.032416 139946414638848 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.5755839347839355, loss=3.603647232055664
I0205 13:59:09.123308 139946397853440 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.5558829307556152, loss=2.784268856048584
I0205 13:59:55.231323 139946414638848 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.6120991706848145, loss=2.7837700843811035
I0205 14:00:42.171560 139946397853440 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.2254068851470947, loss=3.4377732276916504
I0205 14:01:28.791288 139946414638848 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.9450700283050537, loss=4.083883285522461
I0205 14:02:15.164491 139946397853440 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.5654959678649902, loss=2.789931058883667
I0205 14:03:01.579703 139946414638848 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.553638458251953, loss=2.7880136966705322
I0205 14:03:48.057457 139946397853440 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.4865448474884033, loss=2.819579601287842
I0205 14:04:34.683910 139946414638848 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.5182344913482666, loss=2.898306131362915
I0205 14:05:21.154120 139946397853440 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.3461525440216064, loss=2.755502462387085
I0205 14:05:25.469017 140107197974336 spec.py:321] Evaluating on the training split.
I0205 14:05:36.244625 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 14:06:09.112423 140107197974336 spec.py:349] Evaluating on the test split.
I0205 14:06:10.704539 140107197974336 submission_runner.py:408] Time since start: 83600.45s, 	Step: 165811, 	{'train/accuracy': 0.8844726085662842, 'train/loss': 0.6560773849487305, 'validation/accuracy': 0.7813000082969666, 'validation/loss': 1.0782166719436646, 'validation/num_examples': 50000, 'test/accuracy': 0.6655000448226929, 'test/loss': 1.6651965379714966, 'test/num_examples': 10000, 'score': 76081.98018074036, 'total_duration': 83600.45110487938, 'accumulated_submission_time': 76081.98018074036, 'accumulated_eval_time': 7501.036018610001, 'accumulated_logging_time': 7.8832173347473145}
I0205 14:06:10.745948 139946414638848 logging_writer.py:48] [165811] accumulated_eval_time=7501.036019, accumulated_logging_time=7.883217, accumulated_submission_time=76081.980181, global_step=165811, preemption_count=0, score=76081.980181, test/accuracy=0.665500, test/loss=1.665197, test/num_examples=10000, total_duration=83600.451105, train/accuracy=0.884473, train/loss=0.656077, validation/accuracy=0.781300, validation/loss=1.078217, validation/num_examples=50000
I0205 14:06:47.440098 139946397853440 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.704345226287842, loss=2.862332820892334
I0205 14:07:33.531745 139946414638848 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.4696311950683594, loss=2.992769241333008
I0205 14:08:20.237032 139946397853440 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.5255796909332275, loss=3.1099770069122314
I0205 14:09:06.546365 139946414638848 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.523618698120117, loss=2.727463960647583
I0205 14:09:52.779490 139946397853440 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.35329270362854, loss=2.76944637298584
I0205 14:10:39.512093 139946414638848 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.4208457469940186, loss=2.7851192951202393
I0205 14:11:26.314487 139946397853440 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.8258464336395264, loss=3.9440577030181885
I0205 14:12:12.866141 139946414638848 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.513336420059204, loss=2.744900941848755
I0205 14:12:59.249351 139946397853440 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.4149718284606934, loss=3.247981548309326
I0205 14:13:11.142878 140107197974336 spec.py:321] Evaluating on the training split.
I0205 14:13:21.745512 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 14:13:48.445526 140107197974336 spec.py:349] Evaluating on the test split.
I0205 14:13:50.041947 140107197974336 submission_runner.py:408] Time since start: 84059.79s, 	Step: 166727, 	{'train/accuracy': 0.8896484375, 'train/loss': 0.6387453079223633, 'validation/accuracy': 0.7838599681854248, 'validation/loss': 1.0713698863983154, 'validation/num_examples': 50000, 'test/accuracy': 0.6671000123023987, 'test/loss': 1.654746413230896, 'test/num_examples': 10000, 'score': 76502.31396532059, 'total_duration': 84059.78851270676, 'accumulated_submission_time': 76502.31396532059, 'accumulated_eval_time': 7539.935069799423, 'accumulated_logging_time': 7.935632944107056}
I0205 14:13:50.082735 139946414638848 logging_writer.py:48] [166727] accumulated_eval_time=7539.935070, accumulated_logging_time=7.935633, accumulated_submission_time=76502.313965, global_step=166727, preemption_count=0, score=76502.313965, test/accuracy=0.667100, test/loss=1.654746, test/num_examples=10000, total_duration=84059.788513, train/accuracy=0.889648, train/loss=0.638745, validation/accuracy=0.783860, validation/loss=1.071370, validation/num_examples=50000
I0205 14:14:19.631902 139946397853440 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.530587911605835, loss=2.792433500289917
I0205 14:15:05.924364 139946414638848 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.6518020629882812, loss=2.7793548107147217
I0205 14:15:52.194689 139946397853440 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.4520208835601807, loss=3.5313937664031982
I0205 14:16:38.729561 139946414638848 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.704697608947754, loss=3.8706274032592773
I0205 14:17:25.100777 139946397853440 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.464627981185913, loss=3.0601744651794434
I0205 14:18:11.499738 139946414638848 logging_writer.py:48] [167300] global_step=167300, grad_norm=3.199030637741089, loss=3.95711088180542
I0205 14:18:57.863004 139946397853440 logging_writer.py:48] [167400] global_step=167400, grad_norm=2.450456380844116, loss=2.7562780380249023
I0205 14:19:44.182989 139946414638848 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.5496139526367188, loss=2.704845428466797
I0205 14:20:30.512078 139946397853440 logging_writer.py:48] [167600] global_step=167600, grad_norm=3.2699456214904785, loss=4.171050548553467
I0205 14:20:50.385081 140107197974336 spec.py:321] Evaluating on the training split.
I0205 14:21:01.042150 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 14:21:32.886798 140107197974336 spec.py:349] Evaluating on the test split.
I0205 14:21:34.484680 140107197974336 submission_runner.py:408] Time since start: 84524.23s, 	Step: 167644, 	{'train/accuracy': 0.8884179592132568, 'train/loss': 0.649350643157959, 'validation/accuracy': 0.7827999591827393, 'validation/loss': 1.0806797742843628, 'validation/num_examples': 50000, 'test/accuracy': 0.6630000472068787, 'test/loss': 1.6625733375549316, 'test/num_examples': 10000, 'score': 76922.55463337898, 'total_duration': 84524.23124790192, 'accumulated_submission_time': 76922.55463337898, 'accumulated_eval_time': 7584.034651994705, 'accumulated_logging_time': 7.9859490394592285}
I0205 14:21:34.527469 139946414638848 logging_writer.py:48] [167644] accumulated_eval_time=7584.034652, accumulated_logging_time=7.985949, accumulated_submission_time=76922.554633, global_step=167644, preemption_count=0, score=76922.554633, test/accuracy=0.663000, test/loss=1.662573, test/num_examples=10000, total_duration=84524.231248, train/accuracy=0.888418, train/loss=0.649351, validation/accuracy=0.782800, validation/loss=1.080680, validation/num_examples=50000
I0205 14:21:56.927660 139946397853440 logging_writer.py:48] [167700] global_step=167700, grad_norm=2.536020040512085, loss=2.941904067993164
I0205 14:22:41.972662 139946414638848 logging_writer.py:48] [167800] global_step=167800, grad_norm=2.4966893196105957, loss=3.5609185695648193
I0205 14:23:28.242709 139946397853440 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.479310989379883, loss=2.758807420730591
I0205 14:24:15.020527 139946414638848 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.700582981109619, loss=2.8013744354248047
I0205 14:25:01.404989 139946397853440 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.592257261276245, loss=2.776106119155884
I0205 14:25:47.853592 139946414638848 logging_writer.py:48] [168200] global_step=168200, grad_norm=2.4177262783050537, loss=2.8812777996063232
I0205 14:26:34.320297 139946397853440 logging_writer.py:48] [168300] global_step=168300, grad_norm=2.4537501335144043, loss=2.7938735485076904
I0205 14:27:20.775185 139946414638848 logging_writer.py:48] [168400] global_step=168400, grad_norm=2.5043561458587646, loss=3.0052549839019775
I0205 14:28:07.124563 139946397853440 logging_writer.py:48] [168500] global_step=168500, grad_norm=3.005552053451538, loss=2.7139580249786377
I0205 14:28:34.644160 140107197974336 spec.py:321] Evaluating on the training split.
I0205 14:28:45.812692 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 14:29:15.811742 140107197974336 spec.py:349] Evaluating on the test split.
I0205 14:29:17.413985 140107197974336 submission_runner.py:408] Time since start: 84987.16s, 	Step: 168561, 	{'train/accuracy': 0.8898632526397705, 'train/loss': 0.6395523548126221, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 1.0810366868972778, 'validation/num_examples': 50000, 'test/accuracy': 0.666700005531311, 'test/loss': 1.6670899391174316, 'test/num_examples': 10000, 'score': 77342.60869932175, 'total_duration': 84987.16055512428, 'accumulated_submission_time': 77342.60869932175, 'accumulated_eval_time': 7626.804463386536, 'accumulated_logging_time': 8.039025783538818}
I0205 14:29:17.459119 139946414638848 logging_writer.py:48] [168561] accumulated_eval_time=7626.804463, accumulated_logging_time=8.039026, accumulated_submission_time=77342.608699, global_step=168561, preemption_count=0, score=77342.608699, test/accuracy=0.666700, test/loss=1.667090, test/num_examples=10000, total_duration=84987.160555, train/accuracy=0.889863, train/loss=0.639552, validation/accuracy=0.782880, validation/loss=1.081037, validation/num_examples=50000
I0205 14:29:33.196577 139946397853440 logging_writer.py:48] [168600] global_step=168600, grad_norm=2.557056188583374, loss=3.428154468536377
I0205 14:30:17.325694 139946414638848 logging_writer.py:48] [168700] global_step=168700, grad_norm=2.5084962844848633, loss=2.740341901779175
I0205 14:31:03.864118 139946397853440 logging_writer.py:48] [168800] global_step=168800, grad_norm=2.490860939025879, loss=2.873260498046875
I0205 14:31:50.812477 139946414638848 logging_writer.py:48] [168900] global_step=168900, grad_norm=2.3563287258148193, loss=2.7455315589904785
I0205 14:32:15.106592 139946397853440 logging_writer.py:48] [168954] global_step=168954, preemption_count=0, score=77520.176187
I0205 14:32:15.835329 140107197974336 checkpoints.py:490] Saving checkpoint at step: 168954
I0205 14:32:17.161063 140107197974336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_2/checkpoint_168954
I0205 14:32:17.186005 140107197974336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_2/checkpoint_168954.
I0205 14:32:18.100892 140107197974336 submission_runner.py:583] Tuning trial 2/5
I0205 14:32:18.101138 140107197974336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0205 14:32:18.109990 140107197974336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008203124743886292, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 33.48979616165161, 'total_duration': 58.898629665374756, 'accumulated_submission_time': 33.48979616165161, 'accumulated_eval_time': 25.40871500968933, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (854, {'train/accuracy': 0.014042968861758709, 'train/loss': 6.451799392700195, 'validation/accuracy': 0.013439999893307686, 'validation/loss': 6.465761661529541, 'validation/num_examples': 50000, 'test/accuracy': 0.010100000537931919, 'test/loss': 6.498650550842285, 'test/num_examples': 10000, 'score': 453.5089168548584, 'total_duration': 517.7165124416351, 'accumulated_submission_time': 453.5089168548584, 'accumulated_eval_time': 64.14088726043701, 'accumulated_logging_time': 0.017528057098388672, 'global_step': 854, 'preemption_count': 0}), (1768, {'train/accuracy': 0.04525390639901161, 'train/loss': 5.853660583496094, 'validation/accuracy': 0.04465999826788902, 'validation/loss': 5.874964714050293, 'validation/num_examples': 50000, 'test/accuracy': 0.03519999980926514, 'test/loss': 5.988783836364746, 'test/num_examples': 10000, 'score': 873.7088196277618, 'total_duration': 977.7617542743683, 'accumulated_submission_time': 873.7088196277618, 'accumulated_eval_time': 103.90360403060913, 'accumulated_logging_time': 0.047904253005981445, 'global_step': 1768, 'preemption_count': 0}), (2686, {'train/accuracy': 0.07212890684604645, 'train/loss': 5.425541400909424, 'validation/accuracy': 0.06647999584674835, 'validation/loss': 5.466614246368408, 'validation/num_examples': 50000, 'test/accuracy': 0.05380000174045563, 'test/loss': 5.632424831390381, 'test/num_examples': 10000, 'score': 1293.9095661640167, 'total_duration': 1439.6401374340057, 'accumulated_submission_time': 1293.9095661640167, 'accumulated_eval_time': 145.50300693511963, 'accumulated_logging_time': 0.07345223426818848, 'global_step': 2686, 'preemption_count': 0}), (3601, {'train/accuracy': 0.11101562529802322, 'train/loss': 5.073092937469482, 'validation/accuracy': 0.09957999736070633, 'validation/loss': 5.1325178146362305, 'validation/num_examples': 50000, 'test/accuracy': 0.07490000128746033, 'test/loss': 5.360351085662842, 'test/num_examples': 10000, 'score': 1713.9229459762573, 'total_duration': 1898.2273745536804, 'accumulated_submission_time': 1713.9229459762573, 'accumulated_eval_time': 183.99536895751953, 'accumulated_logging_time': 0.10294008255004883, 'global_step': 3601, 'preemption_count': 0}), (4520, {'train/accuracy': 0.15251952409744263, 'train/loss': 4.659289360046387, 'validation/accuracy': 0.13673999905586243, 'validation/loss': 4.747751712799072, 'validation/num_examples': 50000, 'test/accuracy': 0.10580000281333923, 'test/loss': 5.029491424560547, 'test/num_examples': 10000, 'score': 2133.9952919483185, 'total_duration': 2358.2990391254425, 'accumulated_submission_time': 2133.9952919483185, 'accumulated_eval_time': 223.91900992393494, 'accumulated_logging_time': 0.12615704536437988, 'global_step': 4520, 'preemption_count': 0}), (5437, {'train/accuracy': 0.2096288949251175, 'train/loss': 4.283182621002197, 'validation/accuracy': 0.1894799917936325, 'validation/loss': 4.372105121612549, 'validation/num_examples': 50000, 'test/accuracy': 0.14220000803470612, 'test/loss': 4.704031467437744, 'test/num_examples': 10000, 'score': 2554.283395767212, 'total_duration': 2819.649272918701, 'accumulated_submission_time': 2554.283395767212, 'accumulated_eval_time': 264.90250039100647, 'accumulated_logging_time': 0.1519322395324707, 'global_step': 5437, 'preemption_count': 0}), (6357, {'train/accuracy': 0.2499413937330246, 'train/loss': 3.9393117427825928, 'validation/accuracy': 0.23253999650478363, 'validation/loss': 4.036503314971924, 'validation/num_examples': 50000, 'test/accuracy': 0.18070000410079956, 'test/loss': 4.413839817047119, 'test/num_examples': 10000, 'score': 2974.6288928985596, 'total_duration': 3281.6409327983856, 'accumulated_submission_time': 2974.6288928985596, 'accumulated_eval_time': 306.4709143638611, 'accumulated_logging_time': 0.176743745803833, 'global_step': 6357, 'preemption_count': 0}), (7275, {'train/accuracy': 0.29560545086860657, 'train/loss': 3.652733087539673, 'validation/accuracy': 0.26211997866630554, 'validation/loss': 3.811382293701172, 'validation/num_examples': 50000, 'test/accuracy': 0.20170001685619354, 'test/loss': 4.236976146697998, 'test/num_examples': 10000, 'score': 3394.7360577583313, 'total_duration': 3742.9045355319977, 'accumulated_submission_time': 3394.7360577583313, 'accumulated_eval_time': 347.54998540878296, 'accumulated_logging_time': 0.2015523910522461, 'global_step': 7275, 'preemption_count': 0}), (8192, {'train/accuracy': 0.3202148377895355, 'train/loss': 3.4828038215637207, 'validation/accuracy': 0.29877999424934387, 'validation/loss': 3.5942399501800537, 'validation/num_examples': 50000, 'test/accuracy': 0.2297000139951706, 'test/loss': 4.045166015625, 'test/num_examples': 10000, 'score': 3814.776090860367, 'total_duration': 4203.085846185684, 'accumulated_submission_time': 3814.776090860367, 'accumulated_eval_time': 387.60815477371216, 'accumulated_logging_time': 0.23152995109558105, 'global_step': 8192, 'preemption_count': 0}), (9112, {'train/accuracy': 0.36253905296325684, 'train/loss': 3.197558879852295, 'validation/accuracy': 0.3323200047016144, 'validation/loss': 3.329472780227661, 'validation/num_examples': 50000, 'test/accuracy': 0.2621000111103058, 'test/loss': 3.810883045196533, 'test/num_examples': 10000, 'score': 4234.9379251003265, 'total_duration': 4662.507352590561, 'accumulated_submission_time': 4234.9379251003265, 'accumulated_eval_time': 426.7889401912689, 'accumulated_logging_time': 0.2578392028808594, 'global_step': 9112, 'preemption_count': 0}), (10031, {'train/accuracy': 0.3985546827316284, 'train/loss': 2.9937057495117188, 'validation/accuracy': 0.36319997906684875, 'validation/loss': 3.172254800796509, 'validation/num_examples': 50000, 'test/accuracy': 0.2775000035762787, 'test/loss': 3.6873080730438232, 'test/num_examples': 10000, 'score': 4655.196612596512, 'total_duration': 5123.324376821518, 'accumulated_submission_time': 4655.196612596512, 'accumulated_eval_time': 467.26917695999146, 'accumulated_logging_time': 0.28348612785339355, 'global_step': 10031, 'preemption_count': 0}), (10952, {'train/accuracy': 0.4247460961341858, 'train/loss': 2.851715087890625, 'validation/accuracy': 0.3962799906730652, 'validation/loss': 2.9970977306365967, 'validation/num_examples': 50000, 'test/accuracy': 0.3062000274658203, 'test/loss': 3.53668212890625, 'test/num_examples': 10000, 'score': 5075.5094628334045, 'total_duration': 5581.6141130924225, 'accumulated_submission_time': 5075.5094628334045, 'accumulated_eval_time': 505.16552233695984, 'accumulated_logging_time': 0.30973124504089355, 'global_step': 10952, 'preemption_count': 0}), (11872, {'train/accuracy': 0.4528906047344208, 'train/loss': 2.6884093284606934, 'validation/accuracy': 0.4200599789619446, 'validation/loss': 2.8471667766571045, 'validation/num_examples': 50000, 'test/accuracy': 0.3208000063896179, 'test/loss': 3.4061996936798096, 'test/num_examples': 10000, 'score': 5495.436726093292, 'total_duration': 6040.103754281998, 'accumulated_submission_time': 5495.436726093292, 'accumulated_eval_time': 543.6498990058899, 'accumulated_logging_time': 0.33508753776550293, 'global_step': 11872, 'preemption_count': 0}), (12791, {'train/accuracy': 0.4826757609844208, 'train/loss': 2.5154643058776855, 'validation/accuracy': 0.4415600001811981, 'validation/loss': 2.720947742462158, 'validation/num_examples': 50000, 'test/accuracy': 0.34060001373291016, 'test/loss': 3.2832746505737305, 'test/num_examples': 10000, 'score': 5915.742879390717, 'total_duration': 6501.774571418762, 'accumulated_submission_time': 5915.742879390717, 'accumulated_eval_time': 584.9318788051605, 'accumulated_logging_time': 0.3650226593017578, 'global_step': 12791, 'preemption_count': 0}), (13709, {'train/accuracy': 0.5004491806030273, 'train/loss': 2.3911969661712646, 'validation/accuracy': 0.46215999126434326, 'validation/loss': 2.5576767921447754, 'validation/num_examples': 50000, 'test/accuracy': 0.3562000095844269, 'test/loss': 3.150334119796753, 'test/num_examples': 10000, 'score': 6335.661201000214, 'total_duration': 6958.471544742584, 'accumulated_submission_time': 6335.661201000214, 'accumulated_eval_time': 621.6272025108337, 'accumulated_logging_time': 0.394378662109375, 'global_step': 13709, 'preemption_count': 0}), (14630, {'train/accuracy': 0.5197460651397705, 'train/loss': 2.3464345932006836, 'validation/accuracy': 0.4825599789619446, 'validation/loss': 2.5191760063171387, 'validation/num_examples': 50000, 'test/accuracy': 0.3742000162601471, 'test/loss': 3.108792543411255, 'test/num_examples': 10000, 'score': 6755.971422195435, 'total_duration': 7417.451339006424, 'accumulated_submission_time': 6755.971422195435, 'accumulated_eval_time': 660.215106010437, 'accumulated_logging_time': 0.42304444313049316, 'global_step': 14630, 'preemption_count': 0}), (15550, {'train/accuracy': 0.53466796875, 'train/loss': 2.2342047691345215, 'validation/accuracy': 0.4933599829673767, 'validation/loss': 2.4432485103607178, 'validation/num_examples': 50000, 'test/accuracy': 0.38270002603530884, 'test/loss': 3.0271334648132324, 'test/num_examples': 10000, 'score': 7176.325411558151, 'total_duration': 7876.476963996887, 'accumulated_submission_time': 7176.325411558151, 'accumulated_eval_time': 698.8068988323212, 'accumulated_logging_time': 0.4497511386871338, 'global_step': 15550, 'preemption_count': 0}), (16467, {'train/accuracy': 0.5671288967132568, 'train/loss': 2.1125993728637695, 'validation/accuracy': 0.505299985408783, 'validation/loss': 2.3933229446411133, 'validation/num_examples': 50000, 'test/accuracy': 0.40060001611709595, 'test/loss': 2.9778337478637695, 'test/num_examples': 10000, 'score': 7596.317331790924, 'total_duration': 8336.32350063324, 'accumulated_submission_time': 7596.317331790924, 'accumulated_eval_time': 738.5819492340088, 'accumulated_logging_time': 0.4763305187225342, 'global_step': 16467, 'preemption_count': 0}), (17387, {'train/accuracy': 0.5574023127555847, 'train/loss': 2.110726833343506, 'validation/accuracy': 0.5231800079345703, 'validation/loss': 2.286231279373169, 'validation/num_examples': 50000, 'test/accuracy': 0.40880000591278076, 'test/loss': 2.8916168212890625, 'test/num_examples': 10000, 'score': 8016.236355781555, 'total_duration': 8796.280049562454, 'accumulated_submission_time': 8016.236355781555, 'accumulated_eval_time': 778.5356502532959, 'accumulated_logging_time': 0.5067436695098877, 'global_step': 17387, 'preemption_count': 0}), (18306, {'train/accuracy': 0.5698437094688416, 'train/loss': 2.086137056350708, 'validation/accuracy': 0.5271199941635132, 'validation/loss': 2.2758429050445557, 'validation/num_examples': 50000, 'test/accuracy': 0.4140000343322754, 'test/loss': 2.887305736541748, 'test/num_examples': 10000, 'score': 8436.425481557846, 'total_duration': 9257.947703361511, 'accumulated_submission_time': 8436.425481557846, 'accumulated_eval_time': 819.9306211471558, 'accumulated_logging_time': 0.5377237796783447, 'global_step': 18306, 'preemption_count': 0}), (19218, {'train/accuracy': 0.5857617259025574, 'train/loss': 2.0329670906066895, 'validation/accuracy': 0.5317599773406982, 'validation/loss': 2.2811155319213867, 'validation/num_examples': 50000, 'test/accuracy': 0.417900025844574, 'test/loss': 2.88185453414917, 'test/num_examples': 10000, 'score': 8856.704848766327, 'total_duration': 9717.73423075676, 'accumulated_submission_time': 8856.704848766327, 'accumulated_eval_time': 859.3592464923859, 'accumulated_logging_time': 0.5637521743774414, 'global_step': 19218, 'preemption_count': 0}), (20135, {'train/accuracy': 0.5826562643051147, 'train/loss': 2.0578503608703613, 'validation/accuracy': 0.5440599918365479, 'validation/loss': 2.237067222595215, 'validation/num_examples': 50000, 'test/accuracy': 0.42160001397132874, 'test/loss': 2.8544681072235107, 'test/num_examples': 10000, 'score': 9277.106140851974, 'total_duration': 10179.387027740479, 'accumulated_submission_time': 9277.106140851974, 'accumulated_eval_time': 900.5303463935852, 'accumulated_logging_time': 0.5916616916656494, 'global_step': 20135, 'preemption_count': 0}), (21053, {'train/accuracy': 0.5937890410423279, 'train/loss': 1.9904780387878418, 'validation/accuracy': 0.5496399998664856, 'validation/loss': 2.194326162338257, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.793226718902588, 'test/num_examples': 10000, 'score': 9697.392251729965, 'total_duration': 10636.000517368317, 'accumulated_submission_time': 9697.392251729965, 'accumulated_eval_time': 936.777051448822, 'accumulated_logging_time': 0.619286298751831, 'global_step': 21053, 'preemption_count': 0}), (21972, {'train/accuracy': 0.613964855670929, 'train/loss': 1.8714021444320679, 'validation/accuracy': 0.5616599917411804, 'validation/loss': 2.118140935897827, 'validation/num_examples': 50000, 'test/accuracy': 0.4426000118255615, 'test/loss': 2.743307590484619, 'test/num_examples': 10000, 'score': 10117.686156272888, 'total_duration': 11095.38220667839, 'accumulated_submission_time': 10117.686156272888, 'accumulated_eval_time': 975.7793915271759, 'accumulated_logging_time': 0.6520423889160156, 'global_step': 21972, 'preemption_count': 0}), (22891, {'train/accuracy': 0.6078515648841858, 'train/loss': 1.876462459564209, 'validation/accuracy': 0.5680199861526489, 'validation/loss': 2.0680747032165527, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.677503824234009, 'test/num_examples': 10000, 'score': 10537.782514810562, 'total_duration': 11556.057990550995, 'accumulated_submission_time': 10537.782514810562, 'accumulated_eval_time': 1016.273241519928, 'accumulated_logging_time': 0.684720516204834, 'global_step': 22891, 'preemption_count': 0}), (23807, {'train/accuracy': 0.6229491829872131, 'train/loss': 1.8563594818115234, 'validation/accuracy': 0.5753600001335144, 'validation/loss': 2.064666986465454, 'validation/num_examples': 50000, 'test/accuracy': 0.4628000259399414, 'test/loss': 2.6657001972198486, 'test/num_examples': 10000, 'score': 10958.110257148743, 'total_duration': 12018.024418115616, 'accumulated_submission_time': 10958.110257148743, 'accumulated_eval_time': 1057.8305151462555, 'accumulated_logging_time': 0.7141125202178955, 'global_step': 23807, 'preemption_count': 0}), (24724, {'train/accuracy': 0.6285351514816284, 'train/loss': 1.7818245887756348, 'validation/accuracy': 0.5766000151634216, 'validation/loss': 2.0169196128845215, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.632568836212158, 'test/num_examples': 10000, 'score': 11378.051944494247, 'total_duration': 12479.462833404541, 'accumulated_submission_time': 11378.051944494247, 'accumulated_eval_time': 1099.2352216243744, 'accumulated_logging_time': 0.7455592155456543, 'global_step': 24724, 'preemption_count': 0}), (25642, {'train/accuracy': 0.6348242163658142, 'train/loss': 1.7657935619354248, 'validation/accuracy': 0.5819199681282043, 'validation/loss': 1.9933316707611084, 'validation/num_examples': 50000, 'test/accuracy': 0.46080002188682556, 'test/loss': 2.615546226501465, 'test/num_examples': 10000, 'score': 11798.264919519424, 'total_duration': 12941.145199537277, 'accumulated_submission_time': 11798.264919519424, 'accumulated_eval_time': 1140.624148607254, 'accumulated_logging_time': 0.7733578681945801, 'global_step': 25642, 'preemption_count': 0}), (26562, {'train/accuracy': 0.6374804377555847, 'train/loss': 1.7287397384643555, 'validation/accuracy': 0.5893999934196472, 'validation/loss': 1.9390709400177002, 'validation/num_examples': 50000, 'test/accuracy': 0.47370001673698425, 'test/loss': 2.5475146770477295, 'test/num_examples': 10000, 'score': 12218.406804323196, 'total_duration': 13396.518834590912, 'accumulated_submission_time': 12218.406804323196, 'accumulated_eval_time': 1175.7698872089386, 'accumulated_logging_time': 0.8034751415252686, 'global_step': 26562, 'preemption_count': 0}), (27477, {'train/accuracy': 0.6408398151397705, 'train/loss': 1.7599726915359497, 'validation/accuracy': 0.5889399647712708, 'validation/loss': 1.9790270328521729, 'validation/num_examples': 50000, 'test/accuracy': 0.4715000092983246, 'test/loss': 2.6012699604034424, 'test/num_examples': 10000, 'score': 12638.708625555038, 'total_duration': 13852.880910873413, 'accumulated_submission_time': 12638.708625555038, 'accumulated_eval_time': 1211.7467403411865, 'accumulated_logging_time': 0.8337299823760986, 'global_step': 27477, 'preemption_count': 0}), (28395, {'train/accuracy': 0.6732617020606995, 'train/loss': 1.6018322706222534, 'validation/accuracy': 0.5973399877548218, 'validation/loss': 1.9325991868972778, 'validation/num_examples': 50000, 'test/accuracy': 0.4756000339984894, 'test/loss': 2.5660927295684814, 'test/num_examples': 10000, 'score': 13059.00931596756, 'total_duration': 14311.540762901306, 'accumulated_submission_time': 13059.00931596756, 'accumulated_eval_time': 1250.0245730876923, 'accumulated_logging_time': 0.862741231918335, 'global_step': 28395, 'preemption_count': 0}), (29312, {'train/accuracy': 0.6476367115974426, 'train/loss': 1.7281827926635742, 'validation/accuracy': 0.5992599725723267, 'validation/loss': 1.9460116624832153, 'validation/num_examples': 50000, 'test/accuracy': 0.47680002450942993, 'test/loss': 2.5661449432373047, 'test/num_examples': 10000, 'score': 13479.215354442596, 'total_duration': 14772.042618513107, 'accumulated_submission_time': 13479.215354442596, 'accumulated_eval_time': 1290.234845161438, 'accumulated_logging_time': 0.8960211277008057, 'global_step': 29312, 'preemption_count': 0}), (30230, {'train/accuracy': 0.6563280820846558, 'train/loss': 1.6538163423538208, 'validation/accuracy': 0.6067399978637695, 'validation/loss': 1.8779222965240479, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.5124006271362305, 'test/num_examples': 10000, 'score': 13899.440378904343, 'total_duration': 15233.832745790482, 'accumulated_submission_time': 13899.440378904343, 'accumulated_eval_time': 1331.7159614562988, 'accumulated_logging_time': 0.9272348880767822, 'global_step': 30230, 'preemption_count': 0}), (31148, {'train/accuracy': 0.6665819883346558, 'train/loss': 1.621629238128662, 'validation/accuracy': 0.6076200008392334, 'validation/loss': 1.8831592798233032, 'validation/num_examples': 50000, 'test/accuracy': 0.48590001463890076, 'test/loss': 2.506326675415039, 'test/num_examples': 10000, 'score': 14319.611972808838, 'total_duration': 15689.165579557419, 'accumulated_submission_time': 14319.611972808838, 'accumulated_eval_time': 1366.7929692268372, 'accumulated_logging_time': 0.9575145244598389, 'global_step': 31148, 'preemption_count': 0}), (32064, {'train/accuracy': 0.6660351157188416, 'train/loss': 1.5995913743972778, 'validation/accuracy': 0.616919994354248, 'validation/loss': 1.8113443851470947, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.442761182785034, 'test/num_examples': 10000, 'score': 14739.628532409668, 'total_duration': 16148.70953464508, 'accumulated_submission_time': 14739.628532409668, 'accumulated_eval_time': 1406.235392332077, 'accumulated_logging_time': 0.9896261692047119, 'global_step': 32064, 'preemption_count': 0}), (32979, {'train/accuracy': 0.66064453125, 'train/loss': 1.6689999103546143, 'validation/accuracy': 0.6136199831962585, 'validation/loss': 1.892295241355896, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.53031587600708, 'test/num_examples': 10000, 'score': 15159.826711416245, 'total_duration': 16611.425479650497, 'accumulated_submission_time': 15159.826711416245, 'accumulated_eval_time': 1448.665411233902, 'accumulated_logging_time': 1.0246226787567139, 'global_step': 32979, 'preemption_count': 0}), (33898, {'train/accuracy': 0.6800194978713989, 'train/loss': 1.5640877485275269, 'validation/accuracy': 0.6166399717330933, 'validation/loss': 1.8367356061935425, 'validation/num_examples': 50000, 'test/accuracy': 0.49890002608299255, 'test/loss': 2.4557673931121826, 'test/num_examples': 10000, 'score': 15579.860214233398, 'total_duration': 17073.8019824028, 'accumulated_submission_time': 15579.860214233398, 'accumulated_eval_time': 1490.9241652488708, 'accumulated_logging_time': 1.0565321445465088, 'global_step': 33898, 'preemption_count': 0}), (34815, {'train/accuracy': 0.6681445240974426, 'train/loss': 1.6435627937316895, 'validation/accuracy': 0.6217399835586548, 'validation/loss': 1.8544306755065918, 'validation/num_examples': 50000, 'test/accuracy': 0.498600035905838, 'test/loss': 2.475801467895508, 'test/num_examples': 10000, 'score': 16000.001391410828, 'total_duration': 17533.330276489258, 'accumulated_submission_time': 16000.001391410828, 'accumulated_eval_time': 1530.2285268306732, 'accumulated_logging_time': 1.0866823196411133, 'global_step': 34815, 'preemption_count': 0}), (35733, {'train/accuracy': 0.6791015267372131, 'train/loss': 1.499345064163208, 'validation/accuracy': 0.6260600090026855, 'validation/loss': 1.74126398563385, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.3599939346313477, 'test/num_examples': 10000, 'score': 16420.151757001877, 'total_duration': 17996.025110006332, 'accumulated_submission_time': 16420.151757001877, 'accumulated_eval_time': 1572.68993806839, 'accumulated_logging_time': 1.116673469543457, 'global_step': 35733, 'preemption_count': 0}), (36651, {'train/accuracy': 0.6833788752555847, 'train/loss': 1.5450021028518677, 'validation/accuracy': 0.6283800005912781, 'validation/loss': 1.795507788658142, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.42209792137146, 'test/num_examples': 10000, 'score': 16840.42435503006, 'total_duration': 18456.86468553543, 'accumulated_submission_time': 16840.42435503006, 'accumulated_eval_time': 1613.168706893921, 'accumulated_logging_time': 1.1520779132843018, 'global_step': 36651, 'preemption_count': 0}), (37570, {'train/accuracy': 0.6815234422683716, 'train/loss': 1.5546667575836182, 'validation/accuracy': 0.6279199719429016, 'validation/loss': 1.7888097763061523, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.409806251525879, 'test/num_examples': 10000, 'score': 17260.590923786163, 'total_duration': 18919.72491168976, 'accumulated_submission_time': 17260.590923786163, 'accumulated_eval_time': 1655.7754156589508, 'accumulated_logging_time': 1.1850988864898682, 'global_step': 37570, 'preemption_count': 0}), (38491, {'train/accuracy': 0.6863867044448853, 'train/loss': 1.5237195491790771, 'validation/accuracy': 0.634660005569458, 'validation/loss': 1.7491233348846436, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.3639705181121826, 'test/num_examples': 10000, 'score': 17680.91156888008, 'total_duration': 19382.169947624207, 'accumulated_submission_time': 17680.91156888008, 'accumulated_eval_time': 1697.8138728141785, 'accumulated_logging_time': 1.2178847789764404, 'global_step': 38491, 'preemption_count': 0}), (39410, {'train/accuracy': 0.6941015720367432, 'train/loss': 1.4616892337799072, 'validation/accuracy': 0.6366599798202515, 'validation/loss': 1.711472749710083, 'validation/num_examples': 50000, 'test/accuracy': 0.5131000280380249, 'test/loss': 2.3418853282928467, 'test/num_examples': 10000, 'score': 18101.19814991951, 'total_duration': 19842.69286584854, 'accumulated_submission_time': 18101.19814991951, 'accumulated_eval_time': 1737.9596025943756, 'accumulated_logging_time': 1.2555735111236572, 'global_step': 39410, 'preemption_count': 0}), (40329, {'train/accuracy': 0.7090820074081421, 'train/loss': 1.4305404424667358, 'validation/accuracy': 0.6382399797439575, 'validation/loss': 1.7424010038375854, 'validation/num_examples': 50000, 'test/accuracy': 0.5110000371932983, 'test/loss': 2.36631178855896, 'test/num_examples': 10000, 'score': 18521.3622546196, 'total_duration': 20305.8983001709, 'accumulated_submission_time': 18521.3622546196, 'accumulated_eval_time': 1780.9176177978516, 'accumulated_logging_time': 1.2859671115875244, 'global_step': 40329, 'preemption_count': 0}), (41246, {'train/accuracy': 0.6861523389816284, 'train/loss': 1.5130900144577026, 'validation/accuracy': 0.6380400061607361, 'validation/loss': 1.7185349464416504, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.3481850624084473, 'test/num_examples': 10000, 'score': 18941.50358247757, 'total_duration': 20760.38983654976, 'accumulated_submission_time': 18941.50358247757, 'accumulated_eval_time': 1815.185781955719, 'accumulated_logging_time': 1.3155813217163086, 'global_step': 41246, 'preemption_count': 0}), (42165, {'train/accuracy': 0.6976562142372131, 'train/loss': 1.452872633934021, 'validation/accuracy': 0.6402599811553955, 'validation/loss': 1.7018929719924927, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.3191921710968018, 'test/num_examples': 10000, 'score': 19361.500798225403, 'total_duration': 21222.585634231567, 'accumulated_submission_time': 19361.500798225403, 'accumulated_eval_time': 1857.3007380962372, 'accumulated_logging_time': 1.3462746143341064, 'global_step': 42165, 'preemption_count': 0}), (43084, {'train/accuracy': 0.7085351347923279, 'train/loss': 1.3984525203704834, 'validation/accuracy': 0.6442199945449829, 'validation/loss': 1.6737351417541504, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.2900736331939697, 'test/num_examples': 10000, 'score': 19781.848207473755, 'total_duration': 21681.09438085556, 'accumulated_submission_time': 19781.848207473755, 'accumulated_eval_time': 1895.374864578247, 'accumulated_logging_time': 1.380638599395752, 'global_step': 43084, 'preemption_count': 0}), (44000, {'train/accuracy': 0.6931054592132568, 'train/loss': 1.5006791353225708, 'validation/accuracy': 0.6433599591255188, 'validation/loss': 1.717563271522522, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.348926305770874, 'test/num_examples': 10000, 'score': 20201.839075803757, 'total_duration': 22140.8764231205, 'accumulated_submission_time': 20201.839075803757, 'accumulated_eval_time': 1935.0757467746735, 'accumulated_logging_time': 1.41862154006958, 'global_step': 44000, 'preemption_count': 0}), (44915, {'train/accuracy': 0.7015234231948853, 'train/loss': 1.447856068611145, 'validation/accuracy': 0.6456999778747559, 'validation/loss': 1.6839470863342285, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.2985410690307617, 'test/num_examples': 10000, 'score': 20621.815400123596, 'total_duration': 22603.06175518036, 'accumulated_submission_time': 20621.815400123596, 'accumulated_eval_time': 1977.193481206894, 'accumulated_logging_time': 1.4574127197265625, 'global_step': 44915, 'preemption_count': 0}), (45835, {'train/accuracy': 0.7068163752555847, 'train/loss': 1.426822304725647, 'validation/accuracy': 0.6466799974441528, 'validation/loss': 1.6960935592651367, 'validation/num_examples': 50000, 'test/accuracy': 0.5192000269889832, 'test/loss': 2.3293352127075195, 'test/num_examples': 10000, 'score': 21042.14222931862, 'total_duration': 23062.734843730927, 'accumulated_submission_time': 21042.14222931862, 'accumulated_eval_time': 2016.4534318447113, 'accumulated_logging_time': 1.4908981323242188, 'global_step': 45835, 'preemption_count': 0}), (46755, {'train/accuracy': 0.706250011920929, 'train/loss': 1.3982168436050415, 'validation/accuracy': 0.6541199684143066, 'validation/loss': 1.6292552947998047, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.2454986572265625, 'test/num_examples': 10000, 'score': 21462.424863100052, 'total_duration': 23520.222547769547, 'accumulated_submission_time': 21462.424863100052, 'accumulated_eval_time': 2053.571048259735, 'accumulated_logging_time': 1.5256400108337402, 'global_step': 46755, 'preemption_count': 0}), (47674, {'train/accuracy': 0.7119921445846558, 'train/loss': 1.3972750902175903, 'validation/accuracy': 0.6566799879074097, 'validation/loss': 1.6393754482269287, 'validation/num_examples': 50000, 'test/accuracy': 0.5311000347137451, 'test/loss': 2.2582650184631348, 'test/num_examples': 10000, 'score': 21882.692858219147, 'total_duration': 23982.116462945938, 'accumulated_submission_time': 21882.692858219147, 'accumulated_eval_time': 2095.10959148407, 'accumulated_logging_time': 1.5598258972167969, 'global_step': 47674, 'preemption_count': 0}), (48595, {'train/accuracy': 0.7161718606948853, 'train/loss': 1.3505080938339233, 'validation/accuracy': 0.6545000076293945, 'validation/loss': 1.6139466762542725, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.2515869140625, 'test/num_examples': 10000, 'score': 22303.524026870728, 'total_duration': 24444.625487089157, 'accumulated_submission_time': 22303.524026870728, 'accumulated_eval_time': 2136.696215391159, 'accumulated_logging_time': 1.5978095531463623, 'global_step': 48595, 'preemption_count': 0}), (49513, {'train/accuracy': 0.7284765243530273, 'train/loss': 1.366170048713684, 'validation/accuracy': 0.6598399877548218, 'validation/loss': 1.6686944961547852, 'validation/num_examples': 50000, 'test/accuracy': 0.5327000021934509, 'test/loss': 2.3007359504699707, 'test/num_examples': 10000, 'score': 22723.87028694153, 'total_duration': 24901.43813586235, 'accumulated_submission_time': 22723.87028694153, 'accumulated_eval_time': 2173.0738096237183, 'accumulated_logging_time': 1.6330816745758057, 'global_step': 49513, 'preemption_count': 0}), (50430, {'train/accuracy': 0.7109179496765137, 'train/loss': 1.3661249876022339, 'validation/accuracy': 0.6569799780845642, 'validation/loss': 1.603185772895813, 'validation/num_examples': 50000, 'test/accuracy': 0.5389000177383423, 'test/loss': 2.222001552581787, 'test/num_examples': 10000, 'score': 23143.849014759064, 'total_duration': 25363.477248191833, 'accumulated_submission_time': 23143.849014759064, 'accumulated_eval_time': 2215.0406877994537, 'accumulated_logging_time': 1.6706516742706299, 'global_step': 50430, 'preemption_count': 0}), (51346, {'train/accuracy': 0.7173827886581421, 'train/loss': 1.383202314376831, 'validation/accuracy': 0.6582199931144714, 'validation/loss': 1.6434626579284668, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.27443790435791, 'test/num_examples': 10000, 'score': 23563.827913999557, 'total_duration': 25825.90231370926, 'accumulated_submission_time': 23563.827913999557, 'accumulated_eval_time': 2257.4003636837006, 'accumulated_logging_time': 1.7045204639434814, 'global_step': 51346, 'preemption_count': 0}), (52265, {'train/accuracy': 0.7373827695846558, 'train/loss': 1.3297600746154785, 'validation/accuracy': 0.6584999561309814, 'validation/loss': 1.656172513961792, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.2770166397094727, 'test/num_examples': 10000, 'score': 23984.125440120697, 'total_duration': 26285.075368642807, 'accumulated_submission_time': 23984.125440120697, 'accumulated_eval_time': 2296.1784982681274, 'accumulated_logging_time': 1.7483222484588623, 'global_step': 52265, 'preemption_count': 0}), (53181, {'train/accuracy': 0.7142968773841858, 'train/loss': 1.3528928756713867, 'validation/accuracy': 0.663599967956543, 'validation/loss': 1.5829827785491943, 'validation/num_examples': 50000, 'test/accuracy': 0.5390000343322754, 'test/loss': 2.2040934562683105, 'test/num_examples': 10000, 'score': 24404.181349277496, 'total_duration': 26747.570974826813, 'accumulated_submission_time': 24404.181349277496, 'accumulated_eval_time': 2338.523509502411, 'accumulated_logging_time': 1.7898108959197998, 'global_step': 53181, 'preemption_count': 0}), (54095, {'train/accuracy': 0.7222851514816284, 'train/loss': 1.373044729232788, 'validation/accuracy': 0.6613799929618835, 'validation/loss': 1.6264369487762451, 'validation/num_examples': 50000, 'test/accuracy': 0.5357000231742859, 'test/loss': 2.2335379123687744, 'test/num_examples': 10000, 'score': 24824.275985479355, 'total_duration': 27205.53682422638, 'accumulated_submission_time': 24824.275985479355, 'accumulated_eval_time': 2376.301381111145, 'accumulated_logging_time': 1.831099271774292, 'global_step': 54095, 'preemption_count': 0}), (55013, {'train/accuracy': 0.7419335842132568, 'train/loss': 1.2533196210861206, 'validation/accuracy': 0.6665599942207336, 'validation/loss': 1.5780009031295776, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.186769962310791, 'test/num_examples': 10000, 'score': 25244.52186369896, 'total_duration': 27664.27063536644, 'accumulated_submission_time': 25244.52186369896, 'accumulated_eval_time': 2414.6999106407166, 'accumulated_logging_time': 1.867379903793335, 'global_step': 55013, 'preemption_count': 0}), (55931, {'train/accuracy': 0.72083979845047, 'train/loss': 1.3554744720458984, 'validation/accuracy': 0.6647799611091614, 'validation/loss': 1.584027886390686, 'validation/num_examples': 50000, 'test/accuracy': 0.5414000153541565, 'test/loss': 2.191251754760742, 'test/num_examples': 10000, 'score': 25664.90718483925, 'total_duration': 28127.09863138199, 'accumulated_submission_time': 25664.90718483925, 'accumulated_eval_time': 2457.0523324012756, 'accumulated_logging_time': 1.9043676853179932, 'global_step': 55931, 'preemption_count': 0}), (56846, {'train/accuracy': 0.7316796779632568, 'train/loss': 1.2941749095916748, 'validation/accuracy': 0.6717199683189392, 'validation/loss': 1.55128014087677, 'validation/num_examples': 50000, 'test/accuracy': 0.5484000444412231, 'test/loss': 2.1710853576660156, 'test/num_examples': 10000, 'score': 26085.24145579338, 'total_duration': 28589.734586954117, 'accumulated_submission_time': 26085.24145579338, 'accumulated_eval_time': 2499.262162208557, 'accumulated_logging_time': 1.9429833889007568, 'global_step': 56846, 'preemption_count': 0}), (57762, {'train/accuracy': 0.7390429377555847, 'train/loss': 1.2824914455413818, 'validation/accuracy': 0.6715999841690063, 'validation/loss': 1.5749740600585938, 'validation/num_examples': 50000, 'test/accuracy': 0.5490000247955322, 'test/loss': 2.189195394515991, 'test/num_examples': 10000, 'score': 26505.591643333435, 'total_duration': 29049.86763215065, 'accumulated_submission_time': 26505.591643333435, 'accumulated_eval_time': 2538.9583842754364, 'accumulated_logging_time': 1.9764173030853271, 'global_step': 57762, 'preemption_count': 0}), (58678, {'train/accuracy': 0.7281835675239563, 'train/loss': 1.3011599779129028, 'validation/accuracy': 0.6716399788856506, 'validation/loss': 1.5460153818130493, 'validation/num_examples': 50000, 'test/accuracy': 0.5476000308990479, 'test/loss': 2.1589043140411377, 'test/num_examples': 10000, 'score': 26925.87080693245, 'total_duration': 29514.428884983063, 'accumulated_submission_time': 26925.87080693245, 'accumulated_eval_time': 2583.1521196365356, 'accumulated_logging_time': 2.011991500854492, 'global_step': 58678, 'preemption_count': 0}), (59597, {'train/accuracy': 0.7227734327316284, 'train/loss': 1.3543765544891357, 'validation/accuracy': 0.6686399579048157, 'validation/loss': 1.5894092321395874, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.1943323612213135, 'test/num_examples': 10000, 'score': 27346.019050359726, 'total_duration': 29972.257979154587, 'accumulated_submission_time': 27346.019050359726, 'accumulated_eval_time': 2620.7382864952087, 'accumulated_logging_time': 2.0540719032287598, 'global_step': 59597, 'preemption_count': 0}), (60512, {'train/accuracy': 0.7395703196525574, 'train/loss': 1.2908554077148438, 'validation/accuracy': 0.6735599637031555, 'validation/loss': 1.5688538551330566, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.1810519695281982, 'test/num_examples': 10000, 'score': 27766.20599770546, 'total_duration': 30435.395832777023, 'accumulated_submission_time': 27766.20599770546, 'accumulated_eval_time': 2663.5940973758698, 'accumulated_logging_time': 2.0963516235351562, 'global_step': 60512, 'preemption_count': 0}), (61429, {'train/accuracy': 0.7494531273841858, 'train/loss': 1.2534152269363403, 'validation/accuracy': 0.6738799810409546, 'validation/loss': 1.5649051666259766, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.1733086109161377, 'test/num_examples': 10000, 'score': 28186.290335416794, 'total_duration': 30897.46825361252, 'accumulated_submission_time': 28186.290335416794, 'accumulated_eval_time': 2705.494818210602, 'accumulated_logging_time': 2.1315011978149414, 'global_step': 61429, 'preemption_count': 0}), (62343, {'train/accuracy': 0.7285351157188416, 'train/loss': 1.318561315536499, 'validation/accuracy': 0.6755599975585938, 'validation/loss': 1.5545648336410522, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.1707515716552734, 'test/num_examples': 10000, 'score': 28606.247513771057, 'total_duration': 31358.454117536545, 'accumulated_submission_time': 28606.247513771057, 'accumulated_eval_time': 2746.435801267624, 'accumulated_logging_time': 2.166461229324341, 'global_step': 62343, 'preemption_count': 0}), (63259, {'train/accuracy': 0.7363085746765137, 'train/loss': 1.2871750593185425, 'validation/accuracy': 0.6755399703979492, 'validation/loss': 1.5438107252120972, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 2.1767959594726562, 'test/num_examples': 10000, 'score': 29026.210033893585, 'total_duration': 31816.76087665558, 'accumulated_submission_time': 29026.210033893585, 'accumulated_eval_time': 2784.688705921173, 'accumulated_logging_time': 2.2054715156555176, 'global_step': 63259, 'preemption_count': 0}), (64175, {'train/accuracy': 0.752734363079071, 'train/loss': 1.2191165685653687, 'validation/accuracy': 0.6774399876594543, 'validation/loss': 1.541650414466858, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 2.1525299549102783, 'test/num_examples': 10000, 'score': 29446.1415143013, 'total_duration': 32276.34983420372, 'accumulated_submission_time': 29446.1415143013, 'accumulated_eval_time': 2824.250700235367, 'accumulated_logging_time': 2.2430827617645264, 'global_step': 64175, 'preemption_count': 0}), (65091, {'train/accuracy': 0.7371875047683716, 'train/loss': 1.289478063583374, 'validation/accuracy': 0.6808599829673767, 'validation/loss': 1.5334938764572144, 'validation/num_examples': 50000, 'test/accuracy': 0.5505000352859497, 'test/loss': 2.155724048614502, 'test/num_examples': 10000, 'score': 29866.106069087982, 'total_duration': 32733.75764989853, 'accumulated_submission_time': 29866.106069087982, 'accumulated_eval_time': 2861.604071855545, 'accumulated_logging_time': 2.2804622650146484, 'global_step': 65091, 'preemption_count': 0}), (66008, {'train/accuracy': 0.7390820384025574, 'train/loss': 1.268944501876831, 'validation/accuracy': 0.6768999695777893, 'validation/loss': 1.5356301069259644, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.1502127647399902, 'test/num_examples': 10000, 'score': 30286.265585184097, 'total_duration': 33194.593849658966, 'accumulated_submission_time': 30286.265585184097, 'accumulated_eval_time': 2902.1909971237183, 'accumulated_logging_time': 2.317537307739258, 'global_step': 66008, 'preemption_count': 0}), (66927, {'train/accuracy': 0.7473242282867432, 'train/loss': 1.2253973484039307, 'validation/accuracy': 0.6799399852752686, 'validation/loss': 1.5155017375946045, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.161947011947632, 'test/num_examples': 10000, 'score': 30706.27566933632, 'total_duration': 33656.18039536476, 'accumulated_submission_time': 30706.27566933632, 'accumulated_eval_time': 2943.6744248867035, 'accumulated_logging_time': 2.357588052749634, 'global_step': 66927, 'preemption_count': 0}), (67844, {'train/accuracy': 0.738964855670929, 'train/loss': 1.255308747291565, 'validation/accuracy': 0.6803999543190002, 'validation/loss': 1.512239933013916, 'validation/num_examples': 50000, 'test/accuracy': 0.5626000165939331, 'test/loss': 2.1146597862243652, 'test/num_examples': 10000, 'score': 31126.626373529434, 'total_duration': 34118.59836268425, 'accumulated_submission_time': 31126.626373529434, 'accumulated_eval_time': 2985.6455442905426, 'accumulated_logging_time': 2.4006874561309814, 'global_step': 67844, 'preemption_count': 0}), (68761, {'train/accuracy': 0.7493945360183716, 'train/loss': 1.2292940616607666, 'validation/accuracy': 0.6869199872016907, 'validation/loss': 1.5031391382217407, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.1279420852661133, 'test/num_examples': 10000, 'score': 31546.674610376358, 'total_duration': 34578.75641846657, 'accumulated_submission_time': 31546.674610376358, 'accumulated_eval_time': 3025.6633553504944, 'accumulated_logging_time': 2.43937611579895, 'global_step': 68761, 'preemption_count': 0}), (69678, {'train/accuracy': 0.7514257431030273, 'train/loss': 1.3177834749221802, 'validation/accuracy': 0.6843000054359436, 'validation/loss': 1.611196517944336, 'validation/num_examples': 50000, 'test/accuracy': 0.5576000213623047, 'test/loss': 2.2235536575317383, 'test/num_examples': 10000, 'score': 31966.91418480873, 'total_duration': 35040.54090499878, 'accumulated_submission_time': 31966.91418480873, 'accumulated_eval_time': 3067.120540380478, 'accumulated_logging_time': 2.4745750427246094, 'global_step': 69678, 'preemption_count': 0}), (70595, {'train/accuracy': 0.7463085651397705, 'train/loss': 1.2566375732421875, 'validation/accuracy': 0.6869800090789795, 'validation/loss': 1.5039660930633545, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.119065761566162, 'test/num_examples': 10000, 'score': 32387.108137845993, 'total_duration': 35501.06599497795, 'accumulated_submission_time': 32387.108137845993, 'accumulated_eval_time': 3107.352053165436, 'accumulated_logging_time': 2.5219199657440186, 'global_step': 70595, 'preemption_count': 0}), (71512, {'train/accuracy': 0.7489648461341858, 'train/loss': 1.1991535425186157, 'validation/accuracy': 0.6866599917411804, 'validation/loss': 1.4654242992401123, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 2.0869626998901367, 'test/num_examples': 10000, 'score': 32807.47200012207, 'total_duration': 35963.427382707596, 'accumulated_submission_time': 32807.47200012207, 'accumulated_eval_time': 3149.2554540634155, 'accumulated_logging_time': 2.5634772777557373, 'global_step': 71512, 'preemption_count': 0}), (72427, {'train/accuracy': 0.7582226395606995, 'train/loss': 1.187957525253296, 'validation/accuracy': 0.6908599734306335, 'validation/loss': 1.4750837087631226, 'validation/num_examples': 50000, 'test/accuracy': 0.5659000277519226, 'test/loss': 2.090136766433716, 'test/num_examples': 10000, 'score': 33227.58208632469, 'total_duration': 36423.512846946716, 'accumulated_submission_time': 33227.58208632469, 'accumulated_eval_time': 3189.1428916454315, 'accumulated_logging_time': 2.59970760345459, 'global_step': 72427, 'preemption_count': 0}), (73345, {'train/accuracy': 0.7671679258346558, 'train/loss': 1.1537946462631226, 'validation/accuracy': 0.6909599900245667, 'validation/loss': 1.4814988374710083, 'validation/num_examples': 50000, 'test/accuracy': 0.5666000247001648, 'test/loss': 2.1033482551574707, 'test/num_examples': 10000, 'score': 33647.756412267685, 'total_duration': 36887.83846831322, 'accumulated_submission_time': 33647.756412267685, 'accumulated_eval_time': 3233.202719926834, 'accumulated_logging_time': 2.637953042984009, 'global_step': 73345, 'preemption_count': 0}), (74262, {'train/accuracy': 0.750195324420929, 'train/loss': 1.2365492582321167, 'validation/accuracy': 0.6885799765586853, 'validation/loss': 1.5010799169540405, 'validation/num_examples': 50000, 'test/accuracy': 0.5663000345230103, 'test/loss': 2.115422248840332, 'test/num_examples': 10000, 'score': 34068.01615142822, 'total_duration': 37350.265760183334, 'accumulated_submission_time': 34068.01615142822, 'accumulated_eval_time': 3275.276128768921, 'accumulated_logging_time': 2.678273916244507, 'global_step': 74262, 'preemption_count': 0}), (75178, {'train/accuracy': 0.7569140195846558, 'train/loss': 1.1788876056671143, 'validation/accuracy': 0.6946199536323547, 'validation/loss': 1.4552501440048218, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.0848517417907715, 'test/num_examples': 10000, 'score': 34488.080597639084, 'total_duration': 37814.71540975571, 'accumulated_submission_time': 34488.080597639084, 'accumulated_eval_time': 3319.566258430481, 'accumulated_logging_time': 2.720487594604492, 'global_step': 75178, 'preemption_count': 0}), (76094, {'train/accuracy': 0.7713086009025574, 'train/loss': 1.1538770198822021, 'validation/accuracy': 0.6943599581718445, 'validation/loss': 1.489396095275879, 'validation/num_examples': 50000, 'test/accuracy': 0.5666000247001648, 'test/loss': 2.119791030883789, 'test/num_examples': 10000, 'score': 34908.33102440834, 'total_duration': 38275.80552268028, 'accumulated_submission_time': 34908.33102440834, 'accumulated_eval_time': 3360.3150522708893, 'accumulated_logging_time': 2.7591450214385986, 'global_step': 76094, 'preemption_count': 0}), (77009, {'train/accuracy': 0.7552343606948853, 'train/loss': 1.1852645874023438, 'validation/accuracy': 0.6930199861526489, 'validation/loss': 1.4465210437774658, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.0639443397521973, 'test/num_examples': 10000, 'score': 35328.639944553375, 'total_duration': 38741.29879665375, 'accumulated_submission_time': 35328.639944553375, 'accumulated_eval_time': 3405.4010264873505, 'accumulated_logging_time': 2.8056118488311768, 'global_step': 77009, 'preemption_count': 0}), (77923, {'train/accuracy': 0.7587695121765137, 'train/loss': 1.2047119140625, 'validation/accuracy': 0.6947199702262878, 'validation/loss': 1.4846562147140503, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 2.111642599105835, 'test/num_examples': 10000, 'score': 35748.82993221283, 'total_duration': 39204.77293586731, 'accumulated_submission_time': 35748.82993221283, 'accumulated_eval_time': 3448.5891098976135, 'accumulated_logging_time': 2.8494107723236084, 'global_step': 77923, 'preemption_count': 0}), (78839, {'train/accuracy': 0.7691015601158142, 'train/loss': 1.1444746255874634, 'validation/accuracy': 0.6953999996185303, 'validation/loss': 1.4568653106689453, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.074385166168213, 'test/num_examples': 10000, 'score': 36168.960582733154, 'total_duration': 39665.85251927376, 'accumulated_submission_time': 36168.960582733154, 'accumulated_eval_time': 3489.4477009773254, 'accumulated_logging_time': 2.8870458602905273, 'global_step': 78839, 'preemption_count': 0}), (79757, {'train/accuracy': 0.7608398199081421, 'train/loss': 1.156747817993164, 'validation/accuracy': 0.6993399858474731, 'validation/loss': 1.4243495464324951, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 2.030494213104248, 'test/num_examples': 10000, 'score': 36588.97576904297, 'total_duration': 40129.560782432556, 'accumulated_submission_time': 36588.97576904297, 'accumulated_eval_time': 3533.049519300461, 'accumulated_logging_time': 2.9246163368225098, 'global_step': 79757, 'preemption_count': 0}), (80674, {'train/accuracy': 0.7592187523841858, 'train/loss': 1.2048238515853882, 'validation/accuracy': 0.6990999579429626, 'validation/loss': 1.4711554050445557, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 2.0904242992401123, 'test/num_examples': 10000, 'score': 37009.30336403847, 'total_duration': 40593.813328027725, 'accumulated_submission_time': 37009.30336403847, 'accumulated_eval_time': 3576.878761768341, 'accumulated_logging_time': 2.968376398086548, 'global_step': 80674, 'preemption_count': 0}), (81588, {'train/accuracy': 0.7721484303474426, 'train/loss': 1.1226656436920166, 'validation/accuracy': 0.7008000016212463, 'validation/loss': 1.4333691596984863, 'validation/num_examples': 50000, 'test/accuracy': 0.574400007724762, 'test/loss': 2.0534827709198, 'test/num_examples': 10000, 'score': 37429.31781625748, 'total_duration': 41058.81526851654, 'accumulated_submission_time': 37429.31781625748, 'accumulated_eval_time': 3621.7705612182617, 'accumulated_logging_time': 3.010806083679199, 'global_step': 81588, 'preemption_count': 0}), (82505, {'train/accuracy': 0.7578710913658142, 'train/loss': 1.1451164484024048, 'validation/accuracy': 0.6981199979782104, 'validation/loss': 1.403847336769104, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 2.033210515975952, 'test/num_examples': 10000, 'score': 37849.59191274643, 'total_duration': 41522.65545606613, 'accumulated_submission_time': 37849.59191274643, 'accumulated_eval_time': 3665.23730635643, 'accumulated_logging_time': 3.0572826862335205, 'global_step': 82505, 'preemption_count': 0}), (83422, {'train/accuracy': 0.7671093344688416, 'train/loss': 1.1266734600067139, 'validation/accuracy': 0.702459990978241, 'validation/loss': 1.4112627506256104, 'validation/num_examples': 50000, 'test/accuracy': 0.5782000422477722, 'test/loss': 2.0119259357452393, 'test/num_examples': 10000, 'score': 38269.605160713196, 'total_duration': 41987.00690460205, 'accumulated_submission_time': 38269.605160713196, 'accumulated_eval_time': 3709.483957052231, 'accumulated_logging_time': 3.096383810043335, 'global_step': 83422, 'preemption_count': 0}), (84337, {'train/accuracy': 0.7741601467132568, 'train/loss': 1.1334631443023682, 'validation/accuracy': 0.7036600112915039, 'validation/loss': 1.4320693016052246, 'validation/num_examples': 50000, 'test/accuracy': 0.5787000060081482, 'test/loss': 2.0397210121154785, 'test/num_examples': 10000, 'score': 38689.862147808075, 'total_duration': 42445.65338349342, 'accumulated_submission_time': 38689.862147808075, 'accumulated_eval_time': 3747.7794547080994, 'accumulated_logging_time': 3.1379082202911377, 'global_step': 84337, 'preemption_count': 0}), (85253, {'train/accuracy': 0.7835546731948853, 'train/loss': 1.083608627319336, 'validation/accuracy': 0.7017199993133545, 'validation/loss': 1.4271472692489624, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 2.0556511878967285, 'test/num_examples': 10000, 'score': 39110.120413541794, 'total_duration': 42905.44090247154, 'accumulated_submission_time': 39110.120413541794, 'accumulated_eval_time': 3787.2162635326385, 'accumulated_logging_time': 3.178086042404175, 'global_step': 85253, 'preemption_count': 0}), (86168, {'train/accuracy': 0.7661913633346558, 'train/loss': 1.1293996572494507, 'validation/accuracy': 0.7059599757194519, 'validation/loss': 1.391916036605835, 'validation/num_examples': 50000, 'test/accuracy': 0.5843999981880188, 'test/loss': 2.0091748237609863, 'test/num_examples': 10000, 'score': 39530.094547748566, 'total_duration': 43367.21985912323, 'accumulated_submission_time': 39530.094547748566, 'accumulated_eval_time': 3828.9285061359406, 'accumulated_logging_time': 3.2188732624053955, 'global_step': 86168, 'preemption_count': 0}), (87086, {'train/accuracy': 0.7721288800239563, 'train/loss': 1.1214532852172852, 'validation/accuracy': 0.7034800052642822, 'validation/loss': 1.4084711074829102, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 2.038839817047119, 'test/num_examples': 10000, 'score': 39950.2617855072, 'total_duration': 43830.59176373482, 'accumulated_submission_time': 39950.2617855072, 'accumulated_eval_time': 3872.0360209941864, 'accumulated_logging_time': 3.2629506587982178, 'global_step': 87086, 'preemption_count': 0}), (88002, {'train/accuracy': 0.7907617092132568, 'train/loss': 1.0512465238571167, 'validation/accuracy': 0.708620011806488, 'validation/loss': 1.3993642330169678, 'validation/num_examples': 50000, 'test/accuracy': 0.5897000432014465, 'test/loss': 2.0038678646087646, 'test/num_examples': 10000, 'score': 40370.222638845444, 'total_duration': 44294.23095417023, 'accumulated_submission_time': 40370.222638845444, 'accumulated_eval_time': 3915.6184599399567, 'accumulated_logging_time': 3.306154489517212, 'global_step': 88002, 'preemption_count': 0}), (88915, {'train/accuracy': 0.7695898413658142, 'train/loss': 1.1286250352859497, 'validation/accuracy': 0.7078799605369568, 'validation/loss': 1.400683045387268, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.0153114795684814, 'test/num_examples': 10000, 'score': 40790.28323984146, 'total_duration': 44754.43089079857, 'accumulated_submission_time': 40790.28323984146, 'accumulated_eval_time': 3955.6573054790497, 'accumulated_logging_time': 3.3536322116851807, 'global_step': 88915, 'preemption_count': 0}), (89831, {'train/accuracy': 0.7773827910423279, 'train/loss': 1.0906800031661987, 'validation/accuracy': 0.7084000110626221, 'validation/loss': 1.3841772079467773, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 1.9896676540374756, 'test/num_examples': 10000, 'score': 41210.26914906502, 'total_duration': 45217.8800368309, 'accumulated_submission_time': 41210.26914906502, 'accumulated_eval_time': 3999.0280084609985, 'accumulated_logging_time': 3.3931477069854736, 'global_step': 89831, 'preemption_count': 0}), (90745, {'train/accuracy': 0.7867773175239563, 'train/loss': 1.062552809715271, 'validation/accuracy': 0.7113800048828125, 'validation/loss': 1.3798199892044067, 'validation/num_examples': 50000, 'test/accuracy': 0.589400053024292, 'test/loss': 1.9841927289962769, 'test/num_examples': 10000, 'score': 41630.42470932007, 'total_duration': 45682.04362034798, 'accumulated_submission_time': 41630.42470932007, 'accumulated_eval_time': 4042.9406147003174, 'accumulated_logging_time': 3.434882879257202, 'global_step': 90745, 'preemption_count': 0}), (91661, {'train/accuracy': 0.7730078101158142, 'train/loss': 1.0981395244598389, 'validation/accuracy': 0.7106599807739258, 'validation/loss': 1.366332769393921, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.9762824773788452, 'test/num_examples': 10000, 'score': 42050.68173789978, 'total_duration': 46141.46359419823, 'accumulated_submission_time': 42050.68173789978, 'accumulated_eval_time': 4082.0101313591003, 'accumulated_logging_time': 3.4753620624542236, 'global_step': 91661, 'preemption_count': 0}), (92575, {'train/accuracy': 0.779980480670929, 'train/loss': 1.0799758434295654, 'validation/accuracy': 0.708620011806488, 'validation/loss': 1.3764957189559937, 'validation/num_examples': 50000, 'test/accuracy': 0.5852000117301941, 'test/loss': 1.9942631721496582, 'test/num_examples': 10000, 'score': 42470.79413843155, 'total_duration': 46606.02896428108, 'accumulated_submission_time': 42470.79413843155, 'accumulated_eval_time': 4126.3690321445465, 'accumulated_logging_time': 3.515968084335327, 'global_step': 92575, 'preemption_count': 0}), (93494, {'train/accuracy': 0.7886718511581421, 'train/loss': 1.0608985424041748, 'validation/accuracy': 0.7127999663352966, 'validation/loss': 1.3815165758132935, 'validation/num_examples': 50000, 'test/accuracy': 0.5883000493049622, 'test/loss': 1.989989161491394, 'test/num_examples': 10000, 'score': 42890.790291547775, 'total_duration': 47068.66774916649, 'accumulated_submission_time': 42890.790291547775, 'accumulated_eval_time': 4168.915802717209, 'accumulated_logging_time': 3.5591742992401123, 'global_step': 93494, 'preemption_count': 0}), (94409, {'train/accuracy': 0.7827734351158142, 'train/loss': 1.0911414623260498, 'validation/accuracy': 0.7120400071144104, 'validation/loss': 1.3809247016906738, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.9795022010803223, 'test/num_examples': 10000, 'score': 43310.811838150024, 'total_duration': 47533.24429869652, 'accumulated_submission_time': 43310.811838150024, 'accumulated_eval_time': 4213.372656822205, 'accumulated_logging_time': 3.6043410301208496, 'global_step': 94409, 'preemption_count': 0}), (95328, {'train/accuracy': 0.7830663919448853, 'train/loss': 1.0953134298324585, 'validation/accuracy': 0.7137599587440491, 'validation/loss': 1.3859901428222656, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.9971227645874023, 'test/num_examples': 10000, 'score': 43731.426703214645, 'total_duration': 47996.88214635849, 'accumulated_submission_time': 43731.426703214645, 'accumulated_eval_time': 4256.298967838287, 'accumulated_logging_time': 3.647977590560913, 'global_step': 95328, 'preemption_count': 0}), (96244, {'train/accuracy': 0.7925195097923279, 'train/loss': 1.027241826057434, 'validation/accuracy': 0.7157999873161316, 'validation/loss': 1.3453587293624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.94822096824646, 'test/num_examples': 10000, 'score': 44151.71507334709, 'total_duration': 48462.07651758194, 'accumulated_submission_time': 44151.71507334709, 'accumulated_eval_time': 4301.111127138138, 'accumulated_logging_time': 3.689197540283203, 'global_step': 96244, 'preemption_count': 0}), (97160, {'train/accuracy': 0.8024218678474426, 'train/loss': 1.0138721466064453, 'validation/accuracy': 0.7195599675178528, 'validation/loss': 1.3655524253845215, 'validation/num_examples': 50000, 'test/accuracy': 0.5987000465393066, 'test/loss': 1.9719301462173462, 'test/num_examples': 10000, 'score': 44571.969373226166, 'total_duration': 48922.39983391762, 'accumulated_submission_time': 44571.969373226166, 'accumulated_eval_time': 4341.087158918381, 'accumulated_logging_time': 3.729381799697876, 'global_step': 97160, 'preemption_count': 0}), (98072, {'train/accuracy': 0.7826171517372131, 'train/loss': 1.069865107536316, 'validation/accuracy': 0.7172399759292603, 'validation/loss': 1.3597993850708008, 'validation/num_examples': 50000, 'test/accuracy': 0.5934000015258789, 'test/loss': 1.9582252502441406, 'test/num_examples': 10000, 'score': 44992.07482409477, 'total_duration': 49384.86444759369, 'accumulated_submission_time': 44992.07482409477, 'accumulated_eval_time': 4383.347653388977, 'accumulated_logging_time': 3.7755613327026367, 'global_step': 98072, 'preemption_count': 0}), (98987, {'train/accuracy': 0.7899804711341858, 'train/loss': 1.0631240606307983, 'validation/accuracy': 0.7159000039100647, 'validation/loss': 1.3712981939315796, 'validation/num_examples': 50000, 'test/accuracy': 0.5945000052452087, 'test/loss': 1.9744479656219482, 'test/num_examples': 10000, 'score': 45412.23049378395, 'total_duration': 49850.361988306046, 'accumulated_submission_time': 45412.23049378395, 'accumulated_eval_time': 4428.588559150696, 'accumulated_logging_time': 3.823927640914917, 'global_step': 98987, 'preemption_count': 0}), (99904, {'train/accuracy': 0.8057421445846558, 'train/loss': 0.96631920337677, 'validation/accuracy': 0.7188799977302551, 'validation/loss': 1.3333569765090942, 'validation/num_examples': 50000, 'test/accuracy': 0.5960000157356262, 'test/loss': 1.9273792505264282, 'test/num_examples': 10000, 'score': 45832.44064116478, 'total_duration': 50314.4089922905, 'accumulated_submission_time': 45832.44064116478, 'accumulated_eval_time': 4472.3286554813385, 'accumulated_logging_time': 3.8672292232513428, 'global_step': 99904, 'preemption_count': 0}), (100823, {'train/accuracy': 0.7916210889816284, 'train/loss': 1.0372837781906128, 'validation/accuracy': 0.723039984703064, 'validation/loss': 1.3263189792633057, 'validation/num_examples': 50000, 'test/accuracy': 0.5957000255584717, 'test/loss': 1.943742036819458, 'test/num_examples': 10000, 'score': 46252.78383398056, 'total_duration': 50773.26985836029, 'accumulated_submission_time': 46252.78383398056, 'accumulated_eval_time': 4510.744811296463, 'accumulated_logging_time': 3.916255474090576, 'global_step': 100823, 'preemption_count': 0}), (101740, {'train/accuracy': 0.7941210865974426, 'train/loss': 1.010243535041809, 'validation/accuracy': 0.7223399877548218, 'validation/loss': 1.3201320171356201, 'validation/num_examples': 50000, 'test/accuracy': 0.5976999998092651, 'test/loss': 1.9327346086502075, 'test/num_examples': 10000, 'score': 46672.85423207283, 'total_duration': 51235.3676404953, 'accumulated_submission_time': 46672.85423207283, 'accumulated_eval_time': 4552.680232286453, 'accumulated_logging_time': 3.956141948699951, 'global_step': 101740, 'preemption_count': 0}), (102657, {'train/accuracy': 0.8057616949081421, 'train/loss': 0.9902685880661011, 'validation/accuracy': 0.7222999930381775, 'validation/loss': 1.3363498449325562, 'validation/num_examples': 50000, 'test/accuracy': 0.6000000238418579, 'test/loss': 1.952873945236206, 'test/num_examples': 10000, 'score': 47093.024424791336, 'total_duration': 51695.68058466911, 'accumulated_submission_time': 47093.024424791336, 'accumulated_eval_time': 4592.725254058838, 'accumulated_logging_time': 4.001856803894043, 'global_step': 102657, 'preemption_count': 0}), (103574, {'train/accuracy': 0.79505854845047, 'train/loss': 1.0035017728805542, 'validation/accuracy': 0.7264399528503418, 'validation/loss': 1.2918392419815063, 'validation/num_examples': 50000, 'test/accuracy': 0.603600025177002, 'test/loss': 1.899206280708313, 'test/num_examples': 10000, 'score': 47512.99956417084, 'total_duration': 52159.081652879715, 'accumulated_submission_time': 47512.99956417084, 'accumulated_eval_time': 4636.055516242981, 'accumulated_logging_time': 4.0445640087127686, 'global_step': 103574, 'preemption_count': 0}), (104491, {'train/accuracy': 0.7984960675239563, 'train/loss': 1.0110760927200317, 'validation/accuracy': 0.7244200110435486, 'validation/loss': 1.3226091861724854, 'validation/num_examples': 50000, 'test/accuracy': 0.5982000231742859, 'test/loss': 1.9376882314682007, 'test/num_examples': 10000, 'score': 47933.23684620857, 'total_duration': 52618.84631681442, 'accumulated_submission_time': 47933.23684620857, 'accumulated_eval_time': 4675.482330560684, 'accumulated_logging_time': 4.093322277069092, 'global_step': 104491, 'preemption_count': 0}), (105405, {'train/accuracy': 0.8055859208106995, 'train/loss': 0.946385383605957, 'validation/accuracy': 0.7270799875259399, 'validation/loss': 1.2798376083374023, 'validation/num_examples': 50000, 'test/accuracy': 0.607200026512146, 'test/loss': 1.8782799243927002, 'test/num_examples': 10000, 'score': 48353.525584459305, 'total_duration': 53082.0359852314, 'accumulated_submission_time': 48353.525584459305, 'accumulated_eval_time': 4718.283473730087, 'accumulated_logging_time': 4.140423059463501, 'global_step': 105405, 'preemption_count': 0}), (106321, {'train/accuracy': 0.7962890267372131, 'train/loss': 1.0095607042312622, 'validation/accuracy': 0.726639986038208, 'validation/loss': 1.3089427947998047, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.9131704568862915, 'test/num_examples': 10000, 'score': 48773.57774686813, 'total_duration': 53542.90063166618, 'accumulated_submission_time': 48773.57774686813, 'accumulated_eval_time': 4758.99973154068, 'accumulated_logging_time': 4.183903694152832, 'global_step': 106321, 'preemption_count': 0}), (107240, {'train/accuracy': 0.79638671875, 'train/loss': 1.0163118839263916, 'validation/accuracy': 0.7265799641609192, 'validation/loss': 1.3119384050369263, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.9311659336090088, 'test/num_examples': 10000, 'score': 49193.492560863495, 'total_duration': 54002.184608221054, 'accumulated_submission_time': 49193.492560863495, 'accumulated_eval_time': 4798.267370700836, 'accumulated_logging_time': 4.23191499710083, 'global_step': 107240, 'preemption_count': 0}), (108158, {'train/accuracy': 0.8068554401397705, 'train/loss': 0.9952368140220642, 'validation/accuracy': 0.7298399806022644, 'validation/loss': 1.3251501321792603, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.9325958490371704, 'test/num_examples': 10000, 'score': 49613.74026465416, 'total_duration': 54464.774629831314, 'accumulated_submission_time': 49613.74026465416, 'accumulated_eval_time': 4840.51326918602, 'accumulated_logging_time': 4.2742462158203125, 'global_step': 108158, 'preemption_count': 0}), (109073, {'train/accuracy': 0.8169335722923279, 'train/loss': 0.9389804601669312, 'validation/accuracy': 0.7301599979400635, 'validation/loss': 1.2972155809402466, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.8995212316513062, 'test/num_examples': 10000, 'score': 50033.76714491844, 'total_duration': 54925.62378549576, 'accumulated_submission_time': 50033.76714491844, 'accumulated_eval_time': 4881.228107690811, 'accumulated_logging_time': 4.3293633460998535, 'global_step': 109073, 'preemption_count': 0}), (109990, {'train/accuracy': 0.8023046851158142, 'train/loss': 0.9991682767868042, 'validation/accuracy': 0.730239987373352, 'validation/loss': 1.3013273477554321, 'validation/num_examples': 50000, 'test/accuracy': 0.6098000407218933, 'test/loss': 1.8990883827209473, 'test/num_examples': 10000, 'score': 50453.88138437271, 'total_duration': 55383.31662654877, 'accumulated_submission_time': 50453.88138437271, 'accumulated_eval_time': 4918.705953121185, 'accumulated_logging_time': 4.377705097198486, 'global_step': 109990, 'preemption_count': 0}), (110907, {'train/accuracy': 0.8082812428474426, 'train/loss': 0.9439259171485901, 'validation/accuracy': 0.7319799661636353, 'validation/loss': 1.2687028646469116, 'validation/num_examples': 50000, 'test/accuracy': 0.6110000014305115, 'test/loss': 1.8786910772323608, 'test/num_examples': 10000, 'score': 50873.97991299629, 'total_duration': 55845.48027086258, 'accumulated_submission_time': 50873.97991299629, 'accumulated_eval_time': 4960.672949314117, 'accumulated_logging_time': 4.423073053359985, 'global_step': 110907, 'preemption_count': 0}), (111820, {'train/accuracy': 0.8192773461341858, 'train/loss': 0.9103418588638306, 'validation/accuracy': 0.7331399917602539, 'validation/loss': 1.2813720703125, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.898208737373352, 'test/num_examples': 10000, 'score': 51294.16231417656, 'total_duration': 56304.97105741501, 'accumulated_submission_time': 51294.16231417656, 'accumulated_eval_time': 4999.882031202316, 'accumulated_logging_time': 4.470271587371826, 'global_step': 111820, 'preemption_count': 0}), (112735, {'train/accuracy': 0.8069726228713989, 'train/loss': 0.9850320816040039, 'validation/accuracy': 0.7335000038146973, 'validation/loss': 1.2869131565093994, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.8878358602523804, 'test/num_examples': 10000, 'score': 51714.31570267677, 'total_duration': 56765.39139795303, 'accumulated_submission_time': 51714.31570267677, 'accumulated_eval_time': 5040.045891523361, 'accumulated_logging_time': 4.520854949951172, 'global_step': 112735, 'preemption_count': 0}), (113652, {'train/accuracy': 0.810351550579071, 'train/loss': 0.9742421507835388, 'validation/accuracy': 0.7343800067901611, 'validation/loss': 1.293757438659668, 'validation/num_examples': 50000, 'test/accuracy': 0.6139000058174133, 'test/loss': 1.887000322341919, 'test/num_examples': 10000, 'score': 52134.493783950806, 'total_duration': 57224.7523059845, 'accumulated_submission_time': 52134.493783950806, 'accumulated_eval_time': 5079.130994319916, 'accumulated_logging_time': 4.565948486328125, 'global_step': 113652, 'preemption_count': 0}), (114570, {'train/accuracy': 0.8231640458106995, 'train/loss': 0.8864479660987854, 'validation/accuracy': 0.7347399592399597, 'validation/loss': 1.2553902864456177, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.8648344278335571, 'test/num_examples': 10000, 'score': 52554.772089481354, 'total_duration': 57689.89100813866, 'accumulated_submission_time': 52554.772089481354, 'accumulated_eval_time': 5123.890210866928, 'accumulated_logging_time': 4.613940000534058, 'global_step': 114570, 'preemption_count': 0}), (115486, {'train/accuracy': 0.8078905940055847, 'train/loss': 0.9823316931724548, 'validation/accuracy': 0.7350599765777588, 'validation/loss': 1.3003755807876587, 'validation/num_examples': 50000, 'test/accuracy': 0.6133000254631042, 'test/loss': 1.8954529762268066, 'test/num_examples': 10000, 'score': 52974.86817359924, 'total_duration': 58152.056190013885, 'accumulated_submission_time': 52974.86817359924, 'accumulated_eval_time': 5165.858438968658, 'accumulated_logging_time': 4.662526845932007, 'global_step': 115486, 'preemption_count': 0}), (116403, {'train/accuracy': 0.8154101371765137, 'train/loss': 0.9408923983573914, 'validation/accuracy': 0.738099992275238, 'validation/loss': 1.263514518737793, 'validation/num_examples': 50000, 'test/accuracy': 0.612500011920929, 'test/loss': 1.8793542385101318, 'test/num_examples': 10000, 'score': 53395.18107008934, 'total_duration': 58615.714760541916, 'accumulated_submission_time': 53395.18107008934, 'accumulated_eval_time': 5209.096930503845, 'accumulated_logging_time': 4.717260360717773, 'global_step': 116403, 'preemption_count': 0}), (117321, {'train/accuracy': 0.8239257335662842, 'train/loss': 0.8793371915817261, 'validation/accuracy': 0.7392399907112122, 'validation/loss': 1.2323999404907227, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.8169059753417969, 'test/num_examples': 10000, 'score': 53815.10822582245, 'total_duration': 59076.4835767746, 'accumulated_submission_time': 53815.10822582245, 'accumulated_eval_time': 5249.836025476456, 'accumulated_logging_time': 4.766160011291504, 'global_step': 117321, 'preemption_count': 0}), (118195, {'train/accuracy': 0.8167187571525574, 'train/loss': 0.927771270275116, 'validation/accuracy': 0.7392599582672119, 'validation/loss': 1.2517048120498657, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.852914571762085, 'test/num_examples': 10000, 'score': 54235.39735746384, 'total_duration': 59539.506457567215, 'accumulated_submission_time': 54235.39735746384, 'accumulated_eval_time': 5292.465433597565, 'accumulated_logging_time': 4.820432662963867, 'global_step': 118195, 'preemption_count': 0}), (119111, {'train/accuracy': 0.8149804472923279, 'train/loss': 0.9616518020629883, 'validation/accuracy': 0.7415800094604492, 'validation/loss': 1.2812042236328125, 'validation/num_examples': 50000, 'test/accuracy': 0.6168000102043152, 'test/loss': 1.890485405921936, 'test/num_examples': 10000, 'score': 54655.33577847481, 'total_duration': 59999.27346301079, 'accumulated_submission_time': 54655.33577847481, 'accumulated_eval_time': 5332.194699764252, 'accumulated_logging_time': 4.867582082748413, 'global_step': 119111, 'preemption_count': 0}), (120024, {'train/accuracy': 0.8238866925239563, 'train/loss': 0.8949841856956482, 'validation/accuracy': 0.7396199703216553, 'validation/loss': 1.2500559091567993, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.8545596599578857, 'test/num_examples': 10000, 'score': 55075.26651906967, 'total_duration': 60462.61881303787, 'accumulated_submission_time': 55075.26651906967, 'accumulated_eval_time': 5375.512019634247, 'accumulated_logging_time': 4.913069009780884, 'global_step': 120024, 'preemption_count': 0}), (120940, {'train/accuracy': 0.8238281011581421, 'train/loss': 0.8898305296897888, 'validation/accuracy': 0.7408599853515625, 'validation/loss': 1.232032060623169, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.833022952079773, 'test/num_examples': 10000, 'score': 55495.54484438896, 'total_duration': 60922.43671345711, 'accumulated_submission_time': 55495.54484438896, 'accumulated_eval_time': 5414.950091123581, 'accumulated_logging_time': 4.961091995239258, 'global_step': 120940, 'preemption_count': 0}), (121856, {'train/accuracy': 0.8225390315055847, 'train/loss': 0.8875148296356201, 'validation/accuracy': 0.744879961013794, 'validation/loss': 1.2177189588546753, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.8269776105880737, 'test/num_examples': 10000, 'score': 55915.70909833908, 'total_duration': 61384.78747153282, 'accumulated_submission_time': 55915.70909833908, 'accumulated_eval_time': 5457.037507534027, 'accumulated_logging_time': 5.007095813751221, 'global_step': 121856, 'preemption_count': 0}), (122773, {'train/accuracy': 0.82582026720047, 'train/loss': 0.8763144016265869, 'validation/accuracy': 0.7443599700927734, 'validation/loss': 1.2213648557662964, 'validation/num_examples': 50000, 'test/accuracy': 0.6287000179290771, 'test/loss': 1.8050850629806519, 'test/num_examples': 10000, 'score': 56336.03149437904, 'total_duration': 61850.01596212387, 'accumulated_submission_time': 56336.03149437904, 'accumulated_eval_time': 5501.842509508133, 'accumulated_logging_time': 5.055784463882446, 'global_step': 122773, 'preemption_count': 0}), (123690, {'train/accuracy': 0.8439843654632568, 'train/loss': 0.8367078900337219, 'validation/accuracy': 0.744159996509552, 'validation/loss': 1.2481415271759033, 'validation/num_examples': 50000, 'test/accuracy': 0.6236000061035156, 'test/loss': 1.8550283908843994, 'test/num_examples': 10000, 'score': 56756.35433626175, 'total_duration': 62309.62432742119, 'accumulated_submission_time': 56756.35433626175, 'accumulated_eval_time': 5541.017600536346, 'accumulated_logging_time': 5.112732887268066, 'global_step': 123690, 'preemption_count': 0}), (124606, {'train/accuracy': 0.8265038728713989, 'train/loss': 0.8778988718986511, 'validation/accuracy': 0.7480599880218506, 'validation/loss': 1.2074445486068726, 'validation/num_examples': 50000, 'test/accuracy': 0.6204000115394592, 'test/loss': 1.806689739227295, 'test/num_examples': 10000, 'score': 57176.65181708336, 'total_duration': 62771.15999698639, 'accumulated_submission_time': 57176.65181708336, 'accumulated_eval_time': 5582.155804157257, 'accumulated_logging_time': 5.160711050033569, 'global_step': 124606, 'preemption_count': 0}), (125525, {'train/accuracy': 0.8316210508346558, 'train/loss': 0.8772971630096436, 'validation/accuracy': 0.7468799948692322, 'validation/loss': 1.236533522605896, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.8307219743728638, 'test/num_examples': 10000, 'score': 57596.83524441719, 'total_duration': 63235.70528793335, 'accumulated_submission_time': 57596.83524441719, 'accumulated_eval_time': 5626.422222137451, 'accumulated_logging_time': 5.2043843269348145, 'global_step': 125525, 'preemption_count': 0}), (126442, {'train/accuracy': 0.84095698595047, 'train/loss': 0.8439698815345764, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.2318177223205566, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8440407514572144, 'test/num_examples': 10000, 'score': 58016.88030552864, 'total_duration': 63700.169562101364, 'accumulated_submission_time': 58016.88030552864, 'accumulated_eval_time': 5670.740884780884, 'accumulated_logging_time': 5.2526023387908936, 'global_step': 126442, 'preemption_count': 0}), (127359, {'train/accuracy': 0.8307226300239563, 'train/loss': 0.8531256914138794, 'validation/accuracy': 0.7465199828147888, 'validation/loss': 1.2010209560394287, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.790854811668396, 'test/num_examples': 10000, 'score': 58437.10874581337, 'total_duration': 64159.10203433037, 'accumulated_submission_time': 58437.10874581337, 'accumulated_eval_time': 5709.3451907634735, 'accumulated_logging_time': 5.299185037612915, 'global_step': 127359, 'preemption_count': 0}), (128275, {'train/accuracy': 0.8322070240974426, 'train/loss': 0.8553465604782104, 'validation/accuracy': 0.7501800060272217, 'validation/loss': 1.1990872621536255, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.7960751056671143, 'test/num_examples': 10000, 'score': 58857.14763689041, 'total_duration': 64621.77625489235, 'accumulated_submission_time': 58857.14763689041, 'accumulated_eval_time': 5751.883533239365, 'accumulated_logging_time': 5.343764781951904, 'global_step': 128275, 'preemption_count': 0}), (129189, {'train/accuracy': 0.8402343392372131, 'train/loss': 0.8571125864982605, 'validation/accuracy': 0.7488999962806702, 'validation/loss': 1.232455849647522, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.829653024673462, 'test/num_examples': 10000, 'score': 59277.302680015564, 'total_duration': 65086.165039777756, 'accumulated_submission_time': 59277.302680015564, 'accumulated_eval_time': 5796.012695074081, 'accumulated_logging_time': 5.3958094120025635, 'global_step': 129189, 'preemption_count': 0}), (130106, {'train/accuracy': 0.83314448595047, 'train/loss': 0.8533020615577698, 'validation/accuracy': 0.7516599893569946, 'validation/loss': 1.195619821548462, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.786401629447937, 'test/num_examples': 10000, 'score': 59697.6349568367, 'total_duration': 65544.8101940155, 'accumulated_submission_time': 59697.6349568367, 'accumulated_eval_time': 5834.219810962677, 'accumulated_logging_time': 5.449113845825195, 'global_step': 130106, 'preemption_count': 0}), (131022, {'train/accuracy': 0.8349804282188416, 'train/loss': 0.847000241279602, 'validation/accuracy': 0.7511000037193298, 'validation/loss': 1.1966373920440674, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.7943549156188965, 'test/num_examples': 10000, 'score': 60117.64026284218, 'total_duration': 66006.13822078705, 'accumulated_submission_time': 60117.64026284218, 'accumulated_eval_time': 5875.437610626221, 'accumulated_logging_time': 5.5017664432525635, 'global_step': 131022, 'preemption_count': 0}), (131938, {'train/accuracy': 0.841113269329071, 'train/loss': 0.8369109630584717, 'validation/accuracy': 0.7544800043106079, 'validation/loss': 1.2058559656143188, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.801892876625061, 'test/num_examples': 10000, 'score': 60537.56083083153, 'total_duration': 66470.13001537323, 'accumulated_submission_time': 60537.56083083153, 'accumulated_eval_time': 5919.410274267197, 'accumulated_logging_time': 5.547304630279541, 'global_step': 131938, 'preemption_count': 0}), (132855, {'train/accuracy': 0.8413476347923279, 'train/loss': 0.8087218403816223, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.1676216125488281, 'validation/num_examples': 50000, 'test/accuracy': 0.6373000144958496, 'test/loss': 1.7519946098327637, 'test/num_examples': 10000, 'score': 60957.67953944206, 'total_duration': 66933.93345880508, 'accumulated_submission_time': 60957.67953944206, 'accumulated_eval_time': 5962.992874383926, 'accumulated_logging_time': 5.59683632850647, 'global_step': 132855, 'preemption_count': 0}), (133769, {'train/accuracy': 0.8415429592132568, 'train/loss': 0.8192632794380188, 'validation/accuracy': 0.7564199566841125, 'validation/loss': 1.1783759593963623, 'validation/num_examples': 50000, 'test/accuracy': 0.6380000114440918, 'test/loss': 1.7589350938796997, 'test/num_examples': 10000, 'score': 61377.61789727211, 'total_duration': 67398.6365814209, 'accumulated_submission_time': 61377.61789727211, 'accumulated_eval_time': 6007.655216932297, 'accumulated_logging_time': 5.646831274032593, 'global_step': 133769, 'preemption_count': 0}), (134688, {'train/accuracy': 0.8436328172683716, 'train/loss': 0.8358486890792847, 'validation/accuracy': 0.7567600011825562, 'validation/loss': 1.1952334642410278, 'validation/num_examples': 50000, 'test/accuracy': 0.6371000409126282, 'test/loss': 1.7937754392623901, 'test/num_examples': 10000, 'score': 61797.7135746479, 'total_duration': 67858.43765830994, 'accumulated_submission_time': 61797.7135746479, 'accumulated_eval_time': 6047.2556438446045, 'accumulated_logging_time': 5.699421167373657, 'global_step': 134688, 'preemption_count': 0}), (135607, {'train/accuracy': 0.857714831829071, 'train/loss': 0.7488970160484314, 'validation/accuracy': 0.75764000415802, 'validation/loss': 1.1549402475357056, 'validation/num_examples': 50000, 'test/accuracy': 0.6403000354766846, 'test/loss': 1.7427631616592407, 'test/num_examples': 10000, 'score': 62218.0052587986, 'total_duration': 68323.26888012886, 'accumulated_submission_time': 62218.0052587986, 'accumulated_eval_time': 6091.693810224533, 'accumulated_logging_time': 5.74712872505188, 'global_step': 135607, 'preemption_count': 0}), (136525, {'train/accuracy': 0.8420116901397705, 'train/loss': 0.8260501623153687, 'validation/accuracy': 0.7569199800491333, 'validation/loss': 1.185738205909729, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.7610141038894653, 'test/num_examples': 10000, 'score': 62638.34627819061, 'total_duration': 68784.49385523796, 'accumulated_submission_time': 62638.34627819061, 'accumulated_eval_time': 6132.479343414307, 'accumulated_logging_time': 5.793379783630371, 'global_step': 136525, 'preemption_count': 0}), (137441, {'train/accuracy': 0.8498241901397705, 'train/loss': 0.7776594161987305, 'validation/accuracy': 0.7576599717140198, 'validation/loss': 1.155328392982483, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.739344596862793, 'test/num_examples': 10000, 'score': 63058.35525393486, 'total_duration': 69246.24775362015, 'accumulated_submission_time': 63058.35525393486, 'accumulated_eval_time': 6174.12397646904, 'accumulated_logging_time': 5.840576648712158, 'global_step': 137441, 'preemption_count': 0}), (138357, {'train/accuracy': 0.8564062118530273, 'train/loss': 0.7429553270339966, 'validation/accuracy': 0.7604999542236328, 'validation/loss': 1.145262360572815, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.75374174118042, 'test/num_examples': 10000, 'score': 63478.462926864624, 'total_duration': 69711.37830281258, 'accumulated_submission_time': 63478.462926864624, 'accumulated_eval_time': 6219.046733617783, 'accumulated_logging_time': 5.887576580047607, 'global_step': 138357, 'preemption_count': 0}), (139275, {'train/accuracy': 0.848437488079071, 'train/loss': 0.8022940158843994, 'validation/accuracy': 0.7615199685096741, 'validation/loss': 1.1675846576690674, 'validation/num_examples': 50000, 'test/accuracy': 0.64000004529953, 'test/loss': 1.7675161361694336, 'test/num_examples': 10000, 'score': 63898.55232334137, 'total_duration': 70174.82610559464, 'accumulated_submission_time': 63898.55232334137, 'accumulated_eval_time': 6262.304250955582, 'accumulated_logging_time': 5.935348272323608, 'global_step': 139275, 'preemption_count': 0}), (140189, {'train/accuracy': 0.850390613079071, 'train/loss': 0.7843379974365234, 'validation/accuracy': 0.7585200071334839, 'validation/loss': 1.1634849309921265, 'validation/num_examples': 50000, 'test/accuracy': 0.6385000348091125, 'test/loss': 1.748010277748108, 'test/num_examples': 10000, 'score': 64318.52702474594, 'total_duration': 70639.37820744514, 'accumulated_submission_time': 64318.52702474594, 'accumulated_eval_time': 6306.780901193619, 'accumulated_logging_time': 5.983578443527222, 'global_step': 140189, 'preemption_count': 0}), (141104, {'train/accuracy': 0.8593944907188416, 'train/loss': 0.7617772221565247, 'validation/accuracy': 0.7626199722290039, 'validation/loss': 1.1537964344024658, 'validation/num_examples': 50000, 'test/accuracy': 0.645300030708313, 'test/loss': 1.7508445978164673, 'test/num_examples': 10000, 'score': 64738.57985305786, 'total_duration': 71103.796667099, 'accumulated_submission_time': 64738.57985305786, 'accumulated_eval_time': 6351.042797088623, 'accumulated_logging_time': 6.035136699676514, 'global_step': 141104, 'preemption_count': 0}), (142015, {'train/accuracy': 0.8515819907188416, 'train/loss': 0.7848900556564331, 'validation/accuracy': 0.7644199728965759, 'validation/loss': 1.1486810445785522, 'validation/num_examples': 50000, 'test/accuracy': 0.6426000595092773, 'test/loss': 1.7553709745407104, 'test/num_examples': 10000, 'score': 65158.57492208481, 'total_duration': 71568.67630791664, 'accumulated_submission_time': 65158.57492208481, 'accumulated_eval_time': 6395.823813199997, 'accumulated_logging_time': 6.086883783340454, 'global_step': 142015, 'preemption_count': 0}), (142930, {'train/accuracy': 0.8580859303474426, 'train/loss': 0.7559224367141724, 'validation/accuracy': 0.7646999955177307, 'validation/loss': 1.1461162567138672, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.7412879467010498, 'test/num_examples': 10000, 'score': 65578.79041981697, 'total_duration': 72028.25753879547, 'accumulated_submission_time': 65578.79041981697, 'accumulated_eval_time': 6435.086161613464, 'accumulated_logging_time': 6.138216018676758, 'global_step': 142930, 'preemption_count': 0}), (143845, {'train/accuracy': 0.8574609160423279, 'train/loss': 0.7697817087173462, 'validation/accuracy': 0.7655199766159058, 'validation/loss': 1.1547305583953857, 'validation/num_examples': 50000, 'test/accuracy': 0.645300030708313, 'test/loss': 1.753394603729248, 'test/num_examples': 10000, 'score': 65998.90920972824, 'total_duration': 72487.25309491158, 'accumulated_submission_time': 65998.90920972824, 'accumulated_eval_time': 6473.854023933411, 'accumulated_logging_time': 6.194704294204712, 'global_step': 143845, 'preemption_count': 0}), (144762, {'train/accuracy': 0.8602929711341858, 'train/loss': 0.742518961429596, 'validation/accuracy': 0.7663799524307251, 'validation/loss': 1.1345235109329224, 'validation/num_examples': 50000, 'test/accuracy': 0.6467000246047974, 'test/loss': 1.7295129299163818, 'test/num_examples': 10000, 'score': 66419.2152094841, 'total_duration': 72949.6464586258, 'accumulated_submission_time': 66419.2152094841, 'accumulated_eval_time': 6515.839464187622, 'accumulated_logging_time': 6.242878437042236, 'global_step': 144762, 'preemption_count': 0}), (145674, {'train/accuracy': 0.8600585460662842, 'train/loss': 0.7430623173713684, 'validation/accuracy': 0.7668799757957458, 'validation/loss': 1.1308563947677612, 'validation/num_examples': 50000, 'test/accuracy': 0.6487000584602356, 'test/loss': 1.7240246534347534, 'test/num_examples': 10000, 'score': 66839.39895510674, 'total_duration': 73413.97380805016, 'accumulated_submission_time': 66839.39895510674, 'accumulated_eval_time': 6559.876788377762, 'accumulated_logging_time': 6.297804832458496, 'global_step': 145674, 'preemption_count': 0}), (146588, {'train/accuracy': 0.864062488079071, 'train/loss': 0.7536942958831787, 'validation/accuracy': 0.7701799869537354, 'validation/loss': 1.145656943321228, 'validation/num_examples': 50000, 'test/accuracy': 0.64410001039505, 'test/loss': 1.748490333557129, 'test/num_examples': 10000, 'score': 67259.50889992714, 'total_duration': 73876.54255223274, 'accumulated_submission_time': 67259.50889992714, 'accumulated_eval_time': 6602.229855775833, 'accumulated_logging_time': 6.350820302963257, 'global_step': 146588, 'preemption_count': 0}), (147504, {'train/accuracy': 0.8710546493530273, 'train/loss': 0.708790123462677, 'validation/accuracy': 0.7684599757194519, 'validation/loss': 1.1278865337371826, 'validation/num_examples': 50000, 'test/accuracy': 0.6520000100135803, 'test/loss': 1.7187994718551636, 'test/num_examples': 10000, 'score': 67679.66255092621, 'total_duration': 74341.20226407051, 'accumulated_submission_time': 67679.66255092621, 'accumulated_eval_time': 6646.628881692886, 'accumulated_logging_time': 6.404725551605225, 'global_step': 147504, 'preemption_count': 0}), (148421, {'train/accuracy': 0.8649804592132568, 'train/loss': 0.7262025475502014, 'validation/accuracy': 0.7682799696922302, 'validation/loss': 1.1214993000030518, 'validation/num_examples': 50000, 'test/accuracy': 0.6528000235557556, 'test/loss': 1.7120332717895508, 'test/num_examples': 10000, 'score': 68099.97631287575, 'total_duration': 74805.88531470299, 'accumulated_submission_time': 68099.97631287575, 'accumulated_eval_time': 6690.891248703003, 'accumulated_logging_time': 6.458734512329102, 'global_step': 148421, 'preemption_count': 0}), (149335, {'train/accuracy': 0.8691601157188416, 'train/loss': 0.7210755944252014, 'validation/accuracy': 0.7702999711036682, 'validation/loss': 1.124349594116211, 'validation/num_examples': 50000, 'test/accuracy': 0.651900053024292, 'test/loss': 1.711452603340149, 'test/num_examples': 10000, 'score': 68519.64030075073, 'total_duration': 75268.590113163, 'accumulated_submission_time': 68519.64030075073, 'accumulated_eval_time': 6733.445414304733, 'accumulated_logging_time': 6.892820119857788, 'global_step': 149335, 'preemption_count': 0}), (150250, {'train/accuracy': 0.8727734088897705, 'train/loss': 0.6951795816421509, 'validation/accuracy': 0.770859956741333, 'validation/loss': 1.1124303340911865, 'validation/num_examples': 50000, 'test/accuracy': 0.6508000493049622, 'test/loss': 1.714530348777771, 'test/num_examples': 10000, 'score': 68939.89294409752, 'total_duration': 75734.91024708748, 'accumulated_submission_time': 68939.89294409752, 'accumulated_eval_time': 6779.409840583801, 'accumulated_logging_time': 6.94247579574585, 'global_step': 150250, 'preemption_count': 0}), (151165, {'train/accuracy': 0.8675585985183716, 'train/loss': 0.7195205688476562, 'validation/accuracy': 0.7733199596405029, 'validation/loss': 1.1224658489227295, 'validation/num_examples': 50000, 'test/accuracy': 0.6515000462532043, 'test/loss': 1.7132558822631836, 'test/num_examples': 10000, 'score': 69359.92496109009, 'total_duration': 76195.36973547935, 'accumulated_submission_time': 69359.92496109009, 'accumulated_eval_time': 6819.731669664383, 'accumulated_logging_time': 6.9958555698394775, 'global_step': 151165, 'preemption_count': 0}), (152079, {'train/accuracy': 0.8717772960662842, 'train/loss': 0.6925562620162964, 'validation/accuracy': 0.7725399732589722, 'validation/loss': 1.0992447137832642, 'validation/num_examples': 50000, 'test/accuracy': 0.6582000255584717, 'test/loss': 1.690200686454773, 'test/num_examples': 10000, 'score': 69779.90133500099, 'total_duration': 76659.01178598404, 'accumulated_submission_time': 69779.90133500099, 'accumulated_eval_time': 6863.285403966904, 'accumulated_logging_time': 7.0559470653533936, 'global_step': 152079, 'preemption_count': 0}), (152996, {'train/accuracy': 0.8743945360183716, 'train/loss': 0.6965427398681641, 'validation/accuracy': 0.7730000019073486, 'validation/loss': 1.1139885187149048, 'validation/num_examples': 50000, 'test/accuracy': 0.6490000486373901, 'test/loss': 1.7110410928726196, 'test/num_examples': 10000, 'score': 70199.87671470642, 'total_duration': 77117.18258023262, 'accumulated_submission_time': 70199.87671470642, 'accumulated_eval_time': 6901.371284723282, 'accumulated_logging_time': 7.113300085067749, 'global_step': 152996, 'preemption_count': 0}), (153909, {'train/accuracy': 0.8716406226158142, 'train/loss': 0.702102541923523, 'validation/accuracy': 0.7744199633598328, 'validation/loss': 1.1024911403656006, 'validation/num_examples': 50000, 'test/accuracy': 0.6552000045776367, 'test/loss': 1.6988980770111084, 'test/num_examples': 10000, 'score': 70619.90300965309, 'total_duration': 77579.82703661919, 'accumulated_submission_time': 70619.90300965309, 'accumulated_eval_time': 6943.878223657608, 'accumulated_logging_time': 7.172998905181885, 'global_step': 153909, 'preemption_count': 0}), (154827, {'train/accuracy': 0.8707226514816284, 'train/loss': 0.7155790328979492, 'validation/accuracy': 0.7752999663352966, 'validation/loss': 1.1152511835098267, 'validation/num_examples': 50000, 'test/accuracy': 0.6602000594139099, 'test/loss': 1.70155668258667, 'test/num_examples': 10000, 'score': 71039.93932843208, 'total_duration': 78040.47797226906, 'accumulated_submission_time': 71039.93932843208, 'accumulated_eval_time': 6984.388374567032, 'accumulated_logging_time': 7.224968194961548, 'global_step': 154827, 'preemption_count': 0}), (155742, {'train/accuracy': 0.8770117163658142, 'train/loss': 0.6983265280723572, 'validation/accuracy': 0.7755199670791626, 'validation/loss': 1.1166795492172241, 'validation/num_examples': 50000, 'test/accuracy': 0.6535000205039978, 'test/loss': 1.7088316679000854, 'test/num_examples': 10000, 'score': 71460.28168869019, 'total_duration': 78502.2566742897, 'accumulated_submission_time': 71460.28168869019, 'accumulated_eval_time': 7025.718888044357, 'accumulated_logging_time': 7.279221773147583, 'global_step': 155742, 'preemption_count': 0}), (156657, {'train/accuracy': 0.8760351538658142, 'train/loss': 0.6916414499282837, 'validation/accuracy': 0.7758199572563171, 'validation/loss': 1.1019681692123413, 'validation/num_examples': 50000, 'test/accuracy': 0.657200038433075, 'test/loss': 1.6836694478988647, 'test/num_examples': 10000, 'score': 71880.30394387245, 'total_duration': 78963.12360739708, 'accumulated_submission_time': 71880.30394387245, 'accumulated_eval_time': 7066.457926273346, 'accumulated_logging_time': 7.332519292831421, 'global_step': 156657, 'preemption_count': 0}), (157574, {'train/accuracy': 0.8784960508346558, 'train/loss': 0.6948211789131165, 'validation/accuracy': 0.7772799730300903, 'validation/loss': 1.1076263189315796, 'validation/num_examples': 50000, 'test/accuracy': 0.6581000089645386, 'test/loss': 1.694753646850586, 'test/num_examples': 10000, 'score': 72300.49660873413, 'total_duration': 79427.16787004471, 'accumulated_submission_time': 72300.49660873413, 'accumulated_eval_time': 7110.1972115039825, 'accumulated_logging_time': 7.3921473026275635, 'global_step': 157574, 'preemption_count': 0}), (158489, {'train/accuracy': 0.8775194883346558, 'train/loss': 0.6820436716079712, 'validation/accuracy': 0.776419997215271, 'validation/loss': 1.095947504043579, 'validation/num_examples': 50000, 'test/accuracy': 0.6564000248908997, 'test/loss': 1.6833994388580322, 'test/num_examples': 10000, 'score': 72720.79914164543, 'total_duration': 79891.43614602089, 'accumulated_submission_time': 72720.79914164543, 'accumulated_eval_time': 7154.0599048137665, 'accumulated_logging_time': 7.443153619766235, 'global_step': 158489, 'preemption_count': 0}), (159403, {'train/accuracy': 0.8854882717132568, 'train/loss': 0.6608580350875854, 'validation/accuracy': 0.7779200077056885, 'validation/loss': 1.1026121377944946, 'validation/num_examples': 50000, 'test/accuracy': 0.6584000587463379, 'test/loss': 1.6912035942077637, 'test/num_examples': 10000, 'score': 73141.05099487305, 'total_duration': 80355.4780535698, 'accumulated_submission_time': 73141.05099487305, 'accumulated_eval_time': 7197.746908187866, 'accumulated_logging_time': 7.494960784912109, 'global_step': 159403, 'preemption_count': 0}), (160318, {'train/accuracy': 0.8786523342132568, 'train/loss': 0.6797360181808472, 'validation/accuracy': 0.778939962387085, 'validation/loss': 1.0944668054580688, 'validation/num_examples': 50000, 'test/accuracy': 0.6579000353813171, 'test/loss': 1.6871545314788818, 'test/num_examples': 10000, 'score': 73561.01437163353, 'total_duration': 80821.08317613602, 'accumulated_submission_time': 73561.01437163353, 'accumulated_eval_time': 7243.284869670868, 'accumulated_logging_time': 7.547072887420654, 'global_step': 160318, 'preemption_count': 0}), (161235, {'train/accuracy': 0.8811132907867432, 'train/loss': 0.6593165993690491, 'validation/accuracy': 0.7783799767494202, 'validation/loss': 1.0817325115203857, 'validation/num_examples': 50000, 'test/accuracy': 0.6635000109672546, 'test/loss': 1.6645426750183105, 'test/num_examples': 10000, 'score': 73980.93570017815, 'total_duration': 81278.25377750397, 'accumulated_submission_time': 73980.93570017815, 'accumulated_eval_time': 7280.42783331871, 'accumulated_logging_time': 7.600719690322876, 'global_step': 161235, 'preemption_count': 0}), (162150, {'train/accuracy': 0.8886327743530273, 'train/loss': 0.6440286040306091, 'validation/accuracy': 0.7780199646949768, 'validation/loss': 1.0898090600967407, 'validation/num_examples': 50000, 'test/accuracy': 0.6640000343322754, 'test/loss': 1.6774790287017822, 'test/num_examples': 10000, 'score': 74401.1434378624, 'total_duration': 81742.06526184082, 'accumulated_submission_time': 74401.1434378624, 'accumulated_eval_time': 7323.926148414612, 'accumulated_logging_time': 7.653084754943848, 'global_step': 162150, 'preemption_count': 0}), (163066, {'train/accuracy': 0.8836523294448853, 'train/loss': 0.6655579805374146, 'validation/accuracy': 0.7795799970626831, 'validation/loss': 1.0824384689331055, 'validation/num_examples': 50000, 'test/accuracy': 0.6609000563621521, 'test/loss': 1.669062614440918, 'test/num_examples': 10000, 'score': 74821.25588536263, 'total_duration': 82206.14687275887, 'accumulated_submission_time': 74821.25588536263, 'accumulated_eval_time': 7367.785813331604, 'accumulated_logging_time': 7.709946155548096, 'global_step': 163066, 'preemption_count': 0}), (163980, {'train/accuracy': 0.8823828101158142, 'train/loss': 0.6626566648483276, 'validation/accuracy': 0.7808399796485901, 'validation/loss': 1.0874329805374146, 'validation/num_examples': 50000, 'test/accuracy': 0.659500002861023, 'test/loss': 1.6772193908691406, 'test/num_examples': 10000, 'score': 75241.35402417183, 'total_duration': 82670.88124489784, 'accumulated_submission_time': 75241.35402417183, 'accumulated_eval_time': 7412.314453601837, 'accumulated_logging_time': 7.765376091003418, 'global_step': 163980, 'preemption_count': 0}), (164896, {'train/accuracy': 0.8887304663658142, 'train/loss': 0.6466950178146362, 'validation/accuracy': 0.7809199690818787, 'validation/loss': 1.0870565176010132, 'validation/num_examples': 50000, 'test/accuracy': 0.6633000373840332, 'test/loss': 1.6752506494522095, 'test/num_examples': 10000, 'score': 75661.63371706009, 'total_duration': 83134.75895619392, 'accumulated_submission_time': 75661.63371706009, 'accumulated_eval_time': 7455.80052113533, 'accumulated_logging_time': 7.824825286865234, 'global_step': 164896, 'preemption_count': 0}), (165811, {'train/accuracy': 0.8844726085662842, 'train/loss': 0.6560773849487305, 'validation/accuracy': 0.7813000082969666, 'validation/loss': 1.0782166719436646, 'validation/num_examples': 50000, 'test/accuracy': 0.6655000448226929, 'test/loss': 1.6651965379714966, 'test/num_examples': 10000, 'score': 76081.98018074036, 'total_duration': 83600.45110487938, 'accumulated_submission_time': 76081.98018074036, 'accumulated_eval_time': 7501.036018610001, 'accumulated_logging_time': 7.8832173347473145, 'global_step': 165811, 'preemption_count': 0}), (166727, {'train/accuracy': 0.8896484375, 'train/loss': 0.6387453079223633, 'validation/accuracy': 0.7838599681854248, 'validation/loss': 1.0713698863983154, 'validation/num_examples': 50000, 'test/accuracy': 0.6671000123023987, 'test/loss': 1.654746413230896, 'test/num_examples': 10000, 'score': 76502.31396532059, 'total_duration': 84059.78851270676, 'accumulated_submission_time': 76502.31396532059, 'accumulated_eval_time': 7539.935069799423, 'accumulated_logging_time': 7.935632944107056, 'global_step': 166727, 'preemption_count': 0}), (167644, {'train/accuracy': 0.8884179592132568, 'train/loss': 0.649350643157959, 'validation/accuracy': 0.7827999591827393, 'validation/loss': 1.0806797742843628, 'validation/num_examples': 50000, 'test/accuracy': 0.6630000472068787, 'test/loss': 1.6625733375549316, 'test/num_examples': 10000, 'score': 76922.55463337898, 'total_duration': 84524.23124790192, 'accumulated_submission_time': 76922.55463337898, 'accumulated_eval_time': 7584.034651994705, 'accumulated_logging_time': 7.9859490394592285, 'global_step': 167644, 'preemption_count': 0}), (168561, {'train/accuracy': 0.8898632526397705, 'train/loss': 0.6395523548126221, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 1.0810366868972778, 'validation/num_examples': 50000, 'test/accuracy': 0.666700005531311, 'test/loss': 1.6670899391174316, 'test/num_examples': 10000, 'score': 77342.60869932175, 'total_duration': 84987.16055512428, 'accumulated_submission_time': 77342.60869932175, 'accumulated_eval_time': 7626.804463386536, 'accumulated_logging_time': 8.039025783538818, 'global_step': 168561, 'preemption_count': 0})], 'global_step': 168954}
I0205 14:32:18.111104 140107197974336 submission_runner.py:586] Timing: 77520.17618727684
I0205 14:32:18.111189 140107197974336 submission_runner.py:588] Total number of evals: 185
I0205 14:32:18.111233 140107197974336 submission_runner.py:589] ====================
I0205 14:32:18.111279 140107197974336 submission_runner.py:542] Using RNG seed 1274177056
I0205 14:32:18.112668 140107197974336 submission_runner.py:551] --- Tuning run 3/5 ---
I0205 14:32:18.112773 140107197974336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_3.
I0205 14:32:18.116349 140107197974336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_3/hparams.json.
I0205 14:32:18.117126 140107197974336 submission_runner.py:206] Initializing dataset.
I0205 14:32:18.127272 140107197974336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0205 14:32:18.141388 140107197974336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0205 14:32:18.336166 140107197974336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0205 14:32:22.555708 140107197974336 submission_runner.py:213] Initializing model.
I0205 14:32:28.915790 140107197974336 submission_runner.py:255] Initializing optimizer.
I0205 14:32:29.387939 140107197974336 submission_runner.py:262] Initializing metrics bundle.
I0205 14:32:29.388097 140107197974336 submission_runner.py:280] Initializing checkpoint and logger.
I0205 14:32:29.405133 140107197974336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_3 with prefix checkpoint_
I0205 14:32:29.405263 140107197974336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 14:32:45.837280 140107197974336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 14:33:02.099812 140107197974336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_3/flags_0.json.
I0205 14:33:02.104820 140107197974336 submission_runner.py:314] Starting training loop.
I0205 14:33:36.576142 139946372675328 logging_writer.py:48] [0] global_step=0, grad_norm=0.3736867904663086, loss=6.907756328582764
I0205 14:33:36.590429 140107197974336 spec.py:321] Evaluating on the training split.
I0205 14:33:45.011904 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 14:34:05.915011 140107197974336 spec.py:349] Evaluating on the test split.
I0205 14:34:07.511769 140107197974336 submission_runner.py:408] Time since start: 65.41s, 	Step: 1, 	{'train/accuracy': 0.0011328124674037099, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 34.48550629615784, 'total_duration': 65.40688729286194, 'accumulated_submission_time': 34.48550629615784, 'accumulated_eval_time': 30.9212863445282, 'accumulated_logging_time': 0}
I0205 14:34:07.520324 139946381068032 logging_writer.py:48] [1] accumulated_eval_time=30.921286, accumulated_logging_time=0, accumulated_submission_time=34.485506, global_step=1, preemption_count=0, score=34.485506, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=65.406887, train/accuracy=0.001133, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0205 14:35:14.369582 139946414638848 logging_writer.py:48] [100] global_step=100, grad_norm=0.437795490026474, loss=6.905725955963135
I0205 14:36:00.190776 139946397853440 logging_writer.py:48] [200] global_step=200, grad_norm=0.5048342347145081, loss=6.892522811889648
I0205 14:36:47.829078 139946414638848 logging_writer.py:48] [300] global_step=300, grad_norm=0.5578539371490479, loss=6.864783763885498
I0205 14:37:35.014740 139946397853440 logging_writer.py:48] [400] global_step=400, grad_norm=0.6846789717674255, loss=6.836038589477539
I0205 14:38:22.261769 139946414638848 logging_writer.py:48] [500] global_step=500, grad_norm=0.8843151330947876, loss=6.766287803649902
I0205 14:39:09.188839 139946397853440 logging_writer.py:48] [600] global_step=600, grad_norm=0.7923102378845215, loss=6.717329025268555
I0205 14:39:56.396258 139946414638848 logging_writer.py:48] [700] global_step=700, grad_norm=0.8244312405586243, loss=6.6789631843566895
I0205 14:40:43.338459 139946397853440 logging_writer.py:48] [800] global_step=800, grad_norm=1.3026480674743652, loss=6.605895519256592
I0205 14:41:07.635815 140107197974336 spec.py:321] Evaluating on the training split.
I0205 14:41:18.775980 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 14:41:51.829497 140107197974336 spec.py:349] Evaluating on the test split.
I0205 14:41:53.425874 140107197974336 submission_runner.py:408] Time since start: 531.32s, 	Step: 853, 	{'train/accuracy': 0.011249999515712261, 'train/loss': 6.477405071258545, 'validation/accuracy': 0.010859999805688858, 'validation/loss': 6.4868550300598145, 'validation/num_examples': 50000, 'test/accuracy': 0.010500000789761543, 'test/loss': 6.523868083953857, 'test/num_examples': 10000, 'score': 454.543984413147, 'total_duration': 531.3210079669952, 'accumulated_submission_time': 454.543984413147, 'accumulated_eval_time': 76.71136093139648, 'accumulated_logging_time': 0.017364501953125}
I0205 14:41:53.443652 139946414638848 logging_writer.py:48] [853] accumulated_eval_time=76.711361, accumulated_logging_time=0.017365, accumulated_submission_time=454.543984, global_step=853, preemption_count=0, score=454.543984, test/accuracy=0.010500, test/loss=6.523868, test/num_examples=10000, total_duration=531.321008, train/accuracy=0.011250, train/loss=6.477405, validation/accuracy=0.010860, validation/loss=6.486855, validation/num_examples=50000
I0205 14:42:12.297706 139946397853440 logging_writer.py:48] [900] global_step=900, grad_norm=1.5872690677642822, loss=6.577225685119629
I0205 14:42:57.261140 139946414638848 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.8541743755340576, loss=6.521769046783447
I0205 14:43:44.384969 139946397853440 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.0020956993103027, loss=6.425652980804443
I0205 14:44:31.330712 139946414638848 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.6735156774520874, loss=6.390590667724609
I0205 14:45:18.494014 139946397853440 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.4376095533370972, loss=6.2981109619140625
I0205 14:46:05.761257 139946414638848 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.0206539630889893, loss=6.28939151763916
I0205 14:46:52.447308 139946397853440 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.2515695095062256, loss=6.239997863769531
I0205 14:47:39.544407 139946414638848 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.7355350255966187, loss=6.163635730743408
I0205 14:48:26.780256 139946397853440 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.116194009780884, loss=6.148123264312744
I0205 14:48:53.731357 140107197974336 spec.py:321] Evaluating on the training split.
I0205 14:49:04.963930 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 14:49:38.391925 140107197974336 spec.py:349] Evaluating on the test split.
I0205 14:49:40.001059 140107197974336 submission_runner.py:408] Time since start: 997.90s, 	Step: 1759, 	{'train/accuracy': 0.03460937365889549, 'train/loss': 5.880190372467041, 'validation/accuracy': 0.033479999750852585, 'validation/loss': 5.913444995880127, 'validation/num_examples': 50000, 'test/accuracy': 0.027900001034140587, 'test/loss': 6.0221991539001465, 'test/num_examples': 10000, 'score': 874.770247220993, 'total_duration': 997.8961570262909, 'accumulated_submission_time': 874.770247220993, 'accumulated_eval_time': 122.98103356361389, 'accumulated_logging_time': 0.045339345932006836}
I0205 14:49:40.018978 139946414638848 logging_writer.py:48] [1759] accumulated_eval_time=122.981034, accumulated_logging_time=0.045339, accumulated_submission_time=874.770247, global_step=1759, preemption_count=0, score=874.770247, test/accuracy=0.027900, test/loss=6.022199, test/num_examples=10000, total_duration=997.896157, train/accuracy=0.034609, train/loss=5.880190, validation/accuracy=0.033480, validation/loss=5.913445, validation/num_examples=50000
I0205 14:49:56.504377 139946397853440 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.031510591506958, loss=6.270082473754883
I0205 14:50:40.832077 139946414638848 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.828689455986023, loss=6.481170177459717
I0205 14:51:27.519091 139946397853440 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.4351346492767334, loss=6.051638603210449
I0205 14:52:13.992770 139946414638848 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.6264896392822266, loss=6.371042251586914
I0205 14:53:00.689534 139946397853440 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.0051186084747314, loss=5.991173267364502
I0205 14:53:47.234325 139946414638848 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.1235480308532715, loss=5.8998026847839355
I0205 14:54:33.827042 139946397853440 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.3887468576431274, loss=6.478625297546387
I0205 14:55:20.535026 139946414638848 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.5514843463897705, loss=6.585514068603516
I0205 14:56:07.141865 139946397853440 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.068438768386841, loss=5.826122283935547
I0205 14:56:40.282707 140107197974336 spec.py:321] Evaluating on the training split.
I0205 14:56:51.377311 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 14:57:25.371264 140107197974336 spec.py:349] Evaluating on the test split.
I0205 14:57:26.973229 140107197974336 submission_runner.py:408] Time since start: 1464.87s, 	Step: 2673, 	{'train/accuracy': 0.06058593466877937, 'train/loss': 5.4684343338012695, 'validation/accuracy': 0.05729999765753746, 'validation/loss': 5.506952285766602, 'validation/num_examples': 50000, 'test/accuracy': 0.045900002121925354, 'test/loss': 5.680256366729736, 'test/num_examples': 10000, 'score': 1294.968270778656, 'total_duration': 1464.868360042572, 'accumulated_submission_time': 1294.968270778656, 'accumulated_eval_time': 169.67155838012695, 'accumulated_logging_time': 0.07653570175170898}
I0205 14:57:26.993160 139946414638848 logging_writer.py:48] [2673] accumulated_eval_time=169.671558, accumulated_logging_time=0.076536, accumulated_submission_time=1294.968271, global_step=2673, preemption_count=0, score=1294.968271, test/accuracy=0.045900, test/loss=5.680256, test/num_examples=10000, total_duration=1464.868360, train/accuracy=0.060586, train/loss=5.468434, validation/accuracy=0.057300, validation/loss=5.506952, validation/num_examples=50000
I0205 14:57:37.995252 139946397853440 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.8989231586456299, loss=5.8101677894592285
I0205 14:58:21.623410 139946414638848 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.6083346605300903, loss=6.530424118041992
I0205 14:59:08.258131 139946397853440 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.9745596647262573, loss=5.863525867462158
I0205 14:59:54.803762 139946414638848 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.3980581760406494, loss=5.631616115570068
I0205 15:00:41.394733 139946397853440 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.6743580102920532, loss=5.717252731323242
I0205 15:01:27.760246 139946414638848 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.8205616474151611, loss=6.476442337036133
I0205 15:02:14.196629 139946397853440 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.2922298908233643, loss=5.643918991088867
I0205 15:03:00.514977 139946414638848 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.8226300477981567, loss=5.591483116149902
I0205 15:03:46.939222 139946397853440 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.0950052738189697, loss=5.702160835266113
I0205 15:04:27.311502 140107197974336 spec.py:321] Evaluating on the training split.
I0205 15:04:39.113108 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 15:05:14.143880 140107197974336 spec.py:349] Evaluating on the test split.
I0205 15:05:15.742343 140107197974336 submission_runner.py:408] Time since start: 1933.64s, 	Step: 3588, 	{'train/accuracy': 0.0949414074420929, 'train/loss': 5.101712703704834, 'validation/accuracy': 0.08590000122785568, 'validation/loss': 5.135260105133057, 'validation/num_examples': 50000, 'test/accuracy': 0.06810000538825989, 'test/loss': 5.376662731170654, 'test/num_examples': 10000, 'score': 1715.2244164943695, 'total_duration': 1933.6374650001526, 'accumulated_submission_time': 1715.2244164943695, 'accumulated_eval_time': 218.10238456726074, 'accumulated_logging_time': 0.10631585121154785}
I0205 15:05:15.759110 139946414638848 logging_writer.py:48] [3588] accumulated_eval_time=218.102385, accumulated_logging_time=0.106316, accumulated_submission_time=1715.224416, global_step=3588, preemption_count=0, score=1715.224416, test/accuracy=0.068100, test/loss=5.376663, test/num_examples=10000, total_duration=1933.637465, train/accuracy=0.094941, train/loss=5.101713, validation/accuracy=0.085900, validation/loss=5.135260, validation/num_examples=50000
I0205 15:05:20.879198 139946397853440 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.8267549276351929, loss=6.2799577713012695
I0205 15:06:03.295785 139946414638848 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.0360164642333984, loss=5.561869144439697
I0205 15:06:49.228744 139946397853440 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.8618836402893066, loss=5.551044940948486
I0205 15:07:35.664780 139946414638848 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.7180839776992798, loss=5.536627769470215
I0205 15:08:22.025001 139946397853440 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.0195870399475098, loss=5.4198102951049805
I0205 15:09:08.630658 139946414638848 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.0311217308044434, loss=5.773369312286377
I0205 15:09:55.156920 139946397853440 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.8290058374404907, loss=5.3892822265625
I0205 15:10:41.780567 139946414638848 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.7552865743637085, loss=5.265536308288574
I0205 15:11:28.299007 139946397853440 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.900527000427246, loss=6.115322113037109
I0205 15:12:14.743938 139946414638848 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.433905839920044, loss=5.285017490386963
I0205 15:12:15.842808 140107197974336 spec.py:321] Evaluating on the training split.
I0205 15:12:26.802578 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 15:13:00.757595 140107197974336 spec.py:349] Evaluating on the test split.
I0205 15:13:02.360756 140107197974336 submission_runner.py:408] Time since start: 2400.26s, 	Step: 4504, 	{'train/accuracy': 0.13169921934604645, 'train/loss': 4.694127559661865, 'validation/accuracy': 0.12099999934434891, 'validation/loss': 4.762543678283691, 'validation/num_examples': 50000, 'test/accuracy': 0.09380000084638596, 'test/loss': 5.061717510223389, 'test/num_examples': 10000, 'score': 2135.2453899383545, 'total_duration': 2400.255875349045, 'accumulated_submission_time': 2135.2453899383545, 'accumulated_eval_time': 264.62031650543213, 'accumulated_logging_time': 0.13405632972717285}
I0205 15:13:02.378707 139946397853440 logging_writer.py:48] [4504] accumulated_eval_time=264.620317, accumulated_logging_time=0.134056, accumulated_submission_time=2135.245390, global_step=4504, preemption_count=0, score=2135.245390, test/accuracy=0.093800, test/loss=5.061718, test/num_examples=10000, total_duration=2400.255875, train/accuracy=0.131699, train/loss=4.694128, validation/accuracy=0.121000, validation/loss=4.762544, validation/num_examples=50000
I0205 15:13:42.411194 139946414638848 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.059837818145752, loss=5.207843780517578
I0205 15:14:28.952158 139946397853440 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.211211919784546, loss=5.24857759475708
I0205 15:15:15.764624 139946414638848 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.7525635957717896, loss=5.152351379394531
I0205 15:16:02.244315 139946397853440 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.856590986251831, loss=5.176390647888184
I0205 15:16:48.787541 139946414638848 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.507604956626892, loss=6.185041904449463
I0205 15:17:35.564705 139946397853440 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.8977662324905396, loss=5.4671525955200195
I0205 15:18:22.590958 139946414638848 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.002250909805298, loss=5.210631370544434
I0205 15:19:09.376217 139946397853440 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.06968092918396, loss=5.038599014282227
I0205 15:19:55.817621 139946414638848 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.9216468334197998, loss=4.953090667724609
I0205 15:20:02.602364 140107197974336 spec.py:321] Evaluating on the training split.
I0205 15:20:13.700194 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 15:20:47.657710 140107197974336 spec.py:349] Evaluating on the test split.
I0205 15:20:49.252986 140107197974336 submission_runner.py:408] Time since start: 2867.15s, 	Step: 5416, 	{'train/accuracy': 0.1699804663658142, 'train/loss': 4.376136302947998, 'validation/accuracy': 0.15546000003814697, 'validation/loss': 4.456610679626465, 'validation/num_examples': 50000, 'test/accuracy': 0.11700000613927841, 'test/loss': 4.778557300567627, 'test/num_examples': 10000, 'score': 2555.4077939987183, 'total_duration': 2867.1481182575226, 'accumulated_submission_time': 2555.4077939987183, 'accumulated_eval_time': 311.2709410190582, 'accumulated_logging_time': 0.16153669357299805}
I0205 15:20:49.271941 139946397853440 logging_writer.py:48] [5416] accumulated_eval_time=311.270941, accumulated_logging_time=0.161537, accumulated_submission_time=2555.407794, global_step=5416, preemption_count=0, score=2555.407794, test/accuracy=0.117000, test/loss=4.778557, test/num_examples=10000, total_duration=2867.148118, train/accuracy=0.169980, train/loss=4.376136, validation/accuracy=0.155460, validation/loss=4.456611, validation/num_examples=50000
I0205 15:21:23.854288 139946414638848 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.8356330394744873, loss=4.976259708404541
I0205 15:22:10.171734 139946397853440 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.6733167171478271, loss=5.780013561248779
I0205 15:22:56.666433 139946414638848 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.7408509254455566, loss=5.0610880851745605
I0205 15:23:43.081845 139946397853440 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.926172137260437, loss=4.791537284851074
I0205 15:24:29.664532 139946414638848 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.896734595298767, loss=4.843038082122803
I0205 15:25:16.381946 139946397853440 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.17537260055542, loss=5.2702155113220215
I0205 15:26:02.561032 139946414638848 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.3377305269241333, loss=5.919361114501953
I0205 15:26:48.811558 139946397853440 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.976629614830017, loss=5.04345178604126
I0205 15:27:35.253351 139946414638848 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.0367040634155273, loss=4.651523113250732
I0205 15:27:49.280413 140107197974336 spec.py:321] Evaluating on the training split.
I0205 15:28:00.347355 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 15:28:36.435683 140107197974336 spec.py:349] Evaluating on the test split.
I0205 15:28:38.027107 140107197974336 submission_runner.py:408] Time since start: 3335.92s, 	Step: 6332, 	{'train/accuracy': 0.21662108600139618, 'train/loss': 4.001551628112793, 'validation/accuracy': 0.2006799876689911, 'validation/loss': 4.088918209075928, 'validation/num_examples': 50000, 'test/accuracy': 0.1517000049352646, 'test/loss': 4.465686321258545, 'test/num_examples': 10000, 'score': 2975.353636741638, 'total_duration': 3335.9222333431244, 'accumulated_submission_time': 2975.353636741638, 'accumulated_eval_time': 360.0176274776459, 'accumulated_logging_time': 0.19207549095153809}
I0205 15:28:38.043275 139946397853440 logging_writer.py:48] [6332] accumulated_eval_time=360.017627, accumulated_logging_time=0.192075, accumulated_submission_time=2975.353637, global_step=6332, preemption_count=0, score=2975.353637, test/accuracy=0.151700, test/loss=4.465686, test/num_examples=10000, total_duration=3335.922233, train/accuracy=0.216621, train/loss=4.001552, validation/accuracy=0.200680, validation/loss=4.088918, validation/num_examples=50000
I0205 15:29:05.315341 139946414638848 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.5744757652282715, loss=5.897502899169922
I0205 15:29:51.248885 139946397853440 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.278259515762329, loss=4.623733043670654
I0205 15:30:38.148893 139946414638848 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.925927758216858, loss=4.721458435058594
I0205 15:31:24.283910 139946397853440 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.4395233392715454, loss=6.09500789642334
I0205 15:32:10.810574 139946414638848 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.8101694583892822, loss=5.867916584014893
I0205 15:32:57.172450 139946397853440 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.4555730819702148, loss=6.058595657348633
I0205 15:33:43.793170 139946414638848 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.6763811111450195, loss=6.003878116607666
I0205 15:34:29.985627 139946397853440 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.5499024391174316, loss=4.5592217445373535
I0205 15:35:16.741719 139946414638848 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.2225935459136963, loss=4.708713531494141
I0205 15:35:38.159424 140107197974336 spec.py:321] Evaluating on the training split.
I0205 15:35:49.167898 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 15:36:21.401316 140107197974336 spec.py:349] Evaluating on the test split.
I0205 15:36:23.001146 140107197974336 submission_runner.py:408] Time since start: 3800.90s, 	Step: 7248, 	{'train/accuracy': 0.25996091961860657, 'train/loss': 3.7070982456207275, 'validation/accuracy': 0.23823998868465424, 'validation/loss': 3.8109381198883057, 'validation/num_examples': 50000, 'test/accuracy': 0.1785000115633011, 'test/loss': 4.246798515319824, 'test/num_examples': 10000, 'score': 3395.407825946808, 'total_duration': 3800.8962712287903, 'accumulated_submission_time': 3395.407825946808, 'accumulated_eval_time': 404.85935401916504, 'accumulated_logging_time': 0.21831059455871582}
I0205 15:36:23.018874 139946397853440 logging_writer.py:48] [7248] accumulated_eval_time=404.859354, accumulated_logging_time=0.218311, accumulated_submission_time=3395.407826, global_step=7248, preemption_count=0, score=3395.407826, test/accuracy=0.178500, test/loss=4.246799, test/num_examples=10000, total_duration=3800.896271, train/accuracy=0.259961, train/loss=3.707098, validation/accuracy=0.238240, validation/loss=3.810938, validation/num_examples=50000
I0205 15:36:43.837318 139946414638848 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.185082197189331, loss=4.472942352294922
I0205 15:37:28.836278 139946397853440 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.772862434387207, loss=4.576076507568359
I0205 15:38:15.362329 139946414638848 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.8847359418869019, loss=4.744378089904785
I0205 15:39:01.748613 139946397853440 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.4943125247955322, loss=4.427091121673584
I0205 15:39:48.192443 139946414638848 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.9024467468261719, loss=4.354323863983154
I0205 15:40:34.608793 139946397853440 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.7815335988998413, loss=4.3429341316223145
I0205 15:41:21.281136 139946414638848 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.8960832357406616, loss=4.3321990966796875
I0205 15:42:07.736768 139946397853440 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.480424165725708, loss=4.341457843780518
I0205 15:42:53.974925 139946414638848 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.7053375244140625, loss=4.6337127685546875
I0205 15:43:23.371644 140107197974336 spec.py:321] Evaluating on the training split.
I0205 15:43:34.284475 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 15:44:10.306955 140107197974336 spec.py:349] Evaluating on the test split.
I0205 15:44:11.904192 140107197974336 submission_runner.py:408] Time since start: 4269.80s, 	Step: 8165, 	{'train/accuracy': 0.29240232706069946, 'train/loss': 3.420924186706543, 'validation/accuracy': 0.2739599943161011, 'validation/loss': 3.5337398052215576, 'validation/num_examples': 50000, 'test/accuracy': 0.20920000970363617, 'test/loss': 4.015152454376221, 'test/num_examples': 10000, 'score': 3815.6993346214294, 'total_duration': 4269.799302577972, 'accumulated_submission_time': 3815.6993346214294, 'accumulated_eval_time': 453.3918721675873, 'accumulated_logging_time': 0.24537086486816406}
I0205 15:44:11.922941 139946397853440 logging_writer.py:48] [8165] accumulated_eval_time=453.391872, accumulated_logging_time=0.245371, accumulated_submission_time=3815.699335, global_step=8165, preemption_count=0, score=3815.699335, test/accuracy=0.209200, test/loss=4.015152, test/num_examples=10000, total_duration=4269.799303, train/accuracy=0.292402, train/loss=3.420924, validation/accuracy=0.273960, validation/loss=3.533740, validation/num_examples=50000
I0205 15:44:26.074240 139946414638848 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.1568560600280762, loss=5.926083564758301
I0205 15:45:10.163202 139946397853440 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.831473469734192, loss=4.206880569458008
I0205 15:45:56.439448 139946414638848 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.3871911764144897, loss=5.538181781768799
I0205 15:46:42.882744 139946397853440 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.7562612295150757, loss=4.518407344818115
I0205 15:47:29.384087 139946414638848 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.679990291595459, loss=4.5088791847229
I0205 15:48:15.796226 139946397853440 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.3911681175231934, loss=5.860995292663574
I0205 15:49:02.015712 139946414638848 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.8562120199203491, loss=4.124840259552002
I0205 15:49:48.250038 139946397853440 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.8545780181884766, loss=6.067873954772949
I0205 15:50:34.602852 139946414638848 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.7013930082321167, loss=5.347865581512451
I0205 15:51:12.079123 140107197974336 spec.py:321] Evaluating on the training split.
I0205 15:51:23.186842 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 15:51:57.488881 140107197974336 spec.py:349] Evaluating on the test split.
I0205 15:51:59.075395 140107197974336 submission_runner.py:408] Time since start: 4736.97s, 	Step: 9082, 	{'train/accuracy': 0.3405078053474426, 'train/loss': 3.166249990463257, 'validation/accuracy': 0.3056800067424774, 'validation/loss': 3.3514530658721924, 'validation/num_examples': 50000, 'test/accuracy': 0.2371000051498413, 'test/loss': 3.8389713764190674, 'test/num_examples': 10000, 'score': 4235.792031049728, 'total_duration': 4736.970528125763, 'accumulated_submission_time': 4235.792031049728, 'accumulated_eval_time': 500.38815212249756, 'accumulated_logging_time': 0.2754819393157959}
I0205 15:51:59.092022 139946397853440 logging_writer.py:48] [9082] accumulated_eval_time=500.388152, accumulated_logging_time=0.275482, accumulated_submission_time=4235.792031, global_step=9082, preemption_count=0, score=4235.792031, test/accuracy=0.237100, test/loss=3.838971, test/num_examples=10000, total_duration=4736.970528, train/accuracy=0.340508, train/loss=3.166250, validation/accuracy=0.305680, validation/loss=3.351453, validation/num_examples=50000
I0205 15:52:06.561414 139946414638848 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.8023496866226196, loss=4.148770809173584
I0205 15:52:49.461076 139946397853440 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.8084460496902466, loss=4.075645923614502
I0205 15:53:36.117037 139946414638848 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.753383755683899, loss=4.011624336242676
I0205 15:54:22.644178 139946397853440 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.3105919361114502, loss=5.831339359283447
I0205 15:55:09.175689 139946414638848 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.012317180633545, loss=3.9271156787872314
I0205 15:55:55.528026 139946397853440 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.8772125244140625, loss=3.976210117340088
I0205 15:56:41.997949 139946414638848 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.5713361501693726, loss=5.154596328735352
I0205 15:57:28.636878 139946397853440 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.5640184879302979, loss=5.675736427307129
I0205 15:58:15.027488 139946414638848 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.5587897300720215, loss=6.099281311035156
I0205 15:58:59.372606 140107197974336 spec.py:321] Evaluating on the training split.
I0205 15:59:10.675799 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 15:59:47.232175 140107197974336 spec.py:349] Evaluating on the test split.
I0205 15:59:48.824303 140107197974336 submission_runner.py:408] Time since start: 5206.72s, 	Step: 9997, 	{'train/accuracy': 0.35783201456069946, 'train/loss': 3.022150754928589, 'validation/accuracy': 0.3312000036239624, 'validation/loss': 3.1509346961975098, 'validation/num_examples': 50000, 'test/accuracy': 0.2614000141620636, 'test/loss': 3.6755166053771973, 'test/num_examples': 10000, 'score': 4656.01069688797, 'total_duration': 5206.719424247742, 'accumulated_submission_time': 4656.01069688797, 'accumulated_eval_time': 549.8398551940918, 'accumulated_logging_time': 0.3025200366973877}
I0205 15:59:48.846139 139946397853440 logging_writer.py:48] [9997] accumulated_eval_time=549.839855, accumulated_logging_time=0.302520, accumulated_submission_time=4656.010697, global_step=9997, preemption_count=0, score=4656.010697, test/accuracy=0.261400, test/loss=3.675517, test/num_examples=10000, total_duration=5206.719424, train/accuracy=0.357832, train/loss=3.022151, validation/accuracy=0.331200, validation/loss=3.150935, validation/num_examples=50000
I0205 15:59:50.429781 139946414638848 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.8052335977554321, loss=4.101893901824951
I0205 16:00:32.099356 139946397853440 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.894547939300537, loss=4.046877384185791
I0205 16:01:18.218560 139946414638848 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.1821765899658203, loss=4.139572620391846
I0205 16:02:04.811031 139946397853440 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.06227970123291, loss=3.9592361450195312
I0205 16:02:51.384109 139946414638848 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.789263367652893, loss=3.788100242614746
I0205 16:03:37.883397 139946397853440 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.0018327236175537, loss=3.990468740463257
I0205 16:04:24.510062 139946414638848 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.903218150138855, loss=3.9374594688415527
I0205 16:05:11.153868 139946397853440 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.6337242126464844, loss=4.926665306091309
I0205 16:05:57.600064 139946414638848 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.4353563785552979, loss=5.357067584991455
I0205 16:06:44.301522 139946397853440 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.8269749879837036, loss=3.875910758972168
I0205 16:06:49.056703 140107197974336 spec.py:321] Evaluating on the training split.
I0205 16:07:00.066683 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 16:07:37.537683 140107197974336 spec.py:349] Evaluating on the test split.
I0205 16:07:39.128405 140107197974336 submission_runner.py:408] Time since start: 5677.02s, 	Step: 10912, 	{'train/accuracy': 0.38544920086860657, 'train/loss': 2.840325355529785, 'validation/accuracy': 0.3569599986076355, 'validation/loss': 2.9956555366516113, 'validation/num_examples': 50000, 'test/accuracy': 0.27330002188682556, 'test/loss': 3.5355544090270996, 'test/num_examples': 10000, 'score': 5076.158985376358, 'total_duration': 5677.023521661758, 'accumulated_submission_time': 5076.158985376358, 'accumulated_eval_time': 599.9115297794342, 'accumulated_logging_time': 0.3341560363769531}
I0205 16:07:39.148153 139946414638848 logging_writer.py:48] [10912] accumulated_eval_time=599.911530, accumulated_logging_time=0.334156, accumulated_submission_time=5076.158985, global_step=10912, preemption_count=0, score=5076.158985, test/accuracy=0.273300, test/loss=3.535554, test/num_examples=10000, total_duration=5677.023522, train/accuracy=0.385449, train/loss=2.840325, validation/accuracy=0.356960, validation/loss=2.995656, validation/num_examples=50000
I0205 16:08:15.771005 139946397853440 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.4654561281204224, loss=5.052722454071045
I0205 16:09:02.208613 139946414638848 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.8668441772460938, loss=3.7249205112457275
I0205 16:09:49.059720 139946397853440 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.8467655181884766, loss=3.7205090522766113
I0205 16:10:35.097493 139946414638848 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.2185006141662598, loss=3.7295918464660645
I0205 16:11:21.238882 139946397853440 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.5469610691070557, loss=4.437258243560791
I0205 16:12:07.464697 139946414638848 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.3914086818695068, loss=5.899811744689941
I0205 16:12:53.728862 139946397853440 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.3163862228393555, loss=5.7335968017578125
I0205 16:13:40.109215 139946414638848 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.6951663494110107, loss=3.9221997261047363
I0205 16:14:26.345956 139946397853440 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.544185996055603, loss=4.730101108551025
I0205 16:14:39.494865 140107197974336 spec.py:321] Evaluating on the training split.
I0205 16:14:50.425508 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 16:15:25.259089 140107197974336 spec.py:349] Evaluating on the test split.
I0205 16:15:26.860839 140107197974336 submission_runner.py:408] Time since start: 6144.76s, 	Step: 11830, 	{'train/accuracy': 0.4166601598262787, 'train/loss': 2.7132813930511475, 'validation/accuracy': 0.3746599853038788, 'validation/loss': 2.916720151901245, 'validation/num_examples': 50000, 'test/accuracy': 0.2883000075817108, 'test/loss': 3.481478691101074, 'test/num_examples': 10000, 'score': 5496.442660808563, 'total_duration': 6144.755959510803, 'accumulated_submission_time': 5496.442660808563, 'accumulated_eval_time': 647.2774829864502, 'accumulated_logging_time': 0.3645823001861572}
I0205 16:15:26.879959 139946414638848 logging_writer.py:48] [11830] accumulated_eval_time=647.277483, accumulated_logging_time=0.364582, accumulated_submission_time=5496.442661, global_step=11830, preemption_count=0, score=5496.442661, test/accuracy=0.288300, test/loss=3.481479, test/num_examples=10000, total_duration=6144.755960, train/accuracy=0.416660, train/loss=2.713281, validation/accuracy=0.374660, validation/loss=2.916720, validation/num_examples=50000
I0205 16:15:55.056639 139946397853440 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.637465000152588, loss=3.9564342498779297
I0205 16:16:41.109475 139946414638848 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.8301762342453003, loss=3.598619222640991
I0205 16:17:27.837874 139946397853440 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.620882511138916, loss=5.010678768157959
I0205 16:18:14.381696 139946414638848 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.0013234615325928, loss=3.5591835975646973
I0205 16:19:00.939856 139946397853440 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.6134788990020752, loss=3.9099652767181396
I0205 16:19:47.132833 139946414638848 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.8698813915252686, loss=3.71354603767395
I0205 16:20:33.719882 139946397853440 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.7635411024093628, loss=3.9671199321746826
I0205 16:21:20.057708 139946414638848 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.2413960695266724, loss=5.652571678161621
I0205 16:22:06.490518 139946397853440 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.9053078889846802, loss=3.5133070945739746
I0205 16:22:27.128754 140107197974336 spec.py:321] Evaluating on the training split.
I0205 16:22:38.024662 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 16:23:15.101107 140107197974336 spec.py:349] Evaluating on the test split.
I0205 16:23:16.689085 140107197974336 submission_runner.py:408] Time since start: 6614.58s, 	Step: 12746, 	{'train/accuracy': 0.4178515672683716, 'train/loss': 2.683227777481079, 'validation/accuracy': 0.3882399797439575, 'validation/loss': 2.8298232555389404, 'validation/num_examples': 50000, 'test/accuracy': 0.30230000615119934, 'test/loss': 3.4036993980407715, 'test/num_examples': 10000, 'score': 5916.627821445465, 'total_duration': 6614.584210395813, 'accumulated_submission_time': 5916.627821445465, 'accumulated_eval_time': 696.8378114700317, 'accumulated_logging_time': 0.39503002166748047}
I0205 16:23:16.709342 139946414638848 logging_writer.py:48] [12746] accumulated_eval_time=696.837811, accumulated_logging_time=0.395030, accumulated_submission_time=5916.627821, global_step=12746, preemption_count=0, score=5916.627821, test/accuracy=0.302300, test/loss=3.403699, test/num_examples=10000, total_duration=6614.584210, train/accuracy=0.417852, train/loss=2.683228, validation/accuracy=0.388240, validation/loss=2.829823, validation/num_examples=50000
I0205 16:23:38.323936 139946397853440 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.866997241973877, loss=3.6474499702453613
I0205 16:24:23.659795 139946414638848 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.9459559917449951, loss=3.7037477493286133
I0205 16:25:10.292552 139946397853440 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.4521304368972778, loss=4.499009609222412
I0205 16:25:56.686117 139946414638848 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.8592779636383057, loss=3.677915096282959
I0205 16:26:43.079030 139946397853440 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.8072218894958496, loss=3.5807900428771973
I0205 16:27:29.648694 139946414638848 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.646619439125061, loss=3.4889185428619385
I0205 16:28:16.176369 139946397853440 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.8429142236709595, loss=3.5019233226776123
I0205 16:29:02.574872 139946414638848 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.7185614109039307, loss=4.627585411071777
I0205 16:29:51.342459 139946397853440 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.714751124382019, loss=3.6790013313293457
I0205 16:30:17.042463 140107197974336 spec.py:321] Evaluating on the training split.
I0205 16:30:28.214947 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 16:30:58.804164 140107197974336 spec.py:349] Evaluating on the test split.
I0205 16:31:00.400730 140107197974336 submission_runner.py:408] Time since start: 7078.30s, 	Step: 13657, 	{'train/accuracy': 0.4305664002895355, 'train/loss': 2.5879738330841064, 'validation/accuracy': 0.4039999842643738, 'validation/loss': 2.7390167713165283, 'validation/num_examples': 50000, 'test/accuracy': 0.3061000108718872, 'test/loss': 3.3392672538757324, 'test/num_examples': 10000, 'score': 6336.897862434387, 'total_duration': 7078.295861721039, 'accumulated_submission_time': 6336.897862434387, 'accumulated_eval_time': 740.1960797309875, 'accumulated_logging_time': 0.4252433776855469}
I0205 16:31:00.422853 139946414638848 logging_writer.py:48] [13657] accumulated_eval_time=740.196080, accumulated_logging_time=0.425243, accumulated_submission_time=6336.897862, global_step=13657, preemption_count=0, score=6336.897862, test/accuracy=0.306100, test/loss=3.339267, test/num_examples=10000, total_duration=7078.295862, train/accuracy=0.430566, train/loss=2.587974, validation/accuracy=0.404000, validation/loss=2.739017, validation/num_examples=50000
I0205 16:31:17.719825 139946397853440 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.644378900527954, loss=4.109015941619873
I0205 16:32:02.129713 139946414638848 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.1409705877304077, loss=5.50058650970459
I0205 16:32:48.086535 139946397853440 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.6988306045532227, loss=3.81455659866333
I0205 16:33:34.397002 139946414638848 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.291359543800354, loss=4.885404109954834
I0205 16:34:20.685258 139946397853440 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.389520287513733, loss=4.128393650054932
I0205 16:35:07.457597 139946414638848 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.0083363056182861, loss=5.543381690979004
I0205 16:35:54.102259 139946397853440 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.1514064073562622, loss=5.471484661102295
I0205 16:36:40.505278 139946414638848 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.2146787643432617, loss=5.833679676055908
I0205 16:37:27.219796 139946397853440 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.777190923690796, loss=3.4366695880889893
I0205 16:38:00.739768 140107197974336 spec.py:321] Evaluating on the training split.
I0205 16:38:11.779409 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 16:38:49.578779 140107197974336 spec.py:349] Evaluating on the test split.
I0205 16:38:51.180620 140107197974336 submission_runner.py:408] Time since start: 7549.08s, 	Step: 14574, 	{'train/accuracy': 0.4543749988079071, 'train/loss': 2.460162401199341, 'validation/accuracy': 0.4130399823188782, 'validation/loss': 2.6595137119293213, 'validation/num_examples': 50000, 'test/accuracy': 0.31790000200271606, 'test/loss': 3.2573442459106445, 'test/num_examples': 10000, 'score': 6757.152330160141, 'total_duration': 7549.075749397278, 'accumulated_submission_time': 6757.152330160141, 'accumulated_eval_time': 790.636931180954, 'accumulated_logging_time': 0.457782506942749}
I0205 16:38:51.198757 139946414638848 logging_writer.py:48] [14574] accumulated_eval_time=790.636931, accumulated_logging_time=0.457783, accumulated_submission_time=6757.152330, global_step=14574, preemption_count=0, score=6757.152330, test/accuracy=0.317900, test/loss=3.257344, test/num_examples=10000, total_duration=7549.075749, train/accuracy=0.454375, train/loss=2.460162, validation/accuracy=0.413040, validation/loss=2.659514, validation/num_examples=50000
I0205 16:39:01.812746 139946397853440 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.8554120063781738, loss=3.4961724281311035
I0205 16:39:45.305366 139946414638848 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.5993069410324097, loss=3.5221915245056152
I0205 16:40:31.715515 139946397853440 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.6713252067565918, loss=3.5697438716888428
I0205 16:41:18.317450 139946414638848 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.8094373941421509, loss=3.215625047683716
I0205 16:42:04.900872 139946397853440 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.6802788972854614, loss=3.461219549179077
I0205 16:42:50.930861 139946414638848 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.609739065170288, loss=3.670820713043213
I0205 16:43:37.309515 139946397853440 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.5935724973678589, loss=3.4791996479034424
I0205 16:44:23.999948 139946414638848 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.8480312824249268, loss=3.6127233505249023
I0205 16:45:10.740436 139946397853440 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.3594528436660767, loss=4.851919174194336
I0205 16:45:51.391658 140107197974336 spec.py:321] Evaluating on the training split.
I0205 16:46:02.726175 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 16:46:37.933127 140107197974336 spec.py:349] Evaluating on the test split.
I0205 16:46:39.528184 140107197974336 submission_runner.py:408] Time since start: 8017.42s, 	Step: 15489, 	{'train/accuracy': 0.4491015672683716, 'train/loss': 2.4975130558013916, 'validation/accuracy': 0.4224399924278259, 'validation/loss': 2.632542133331299, 'validation/num_examples': 50000, 'test/accuracy': 0.32850000262260437, 'test/loss': 3.233921527862549, 'test/num_examples': 10000, 'score': 7177.283156871796, 'total_duration': 8017.423315048218, 'accumulated_submission_time': 7177.283156871796, 'accumulated_eval_time': 838.7734625339508, 'accumulated_logging_time': 0.48542118072509766}
I0205 16:46:39.546356 139946414638848 logging_writer.py:48] [15489] accumulated_eval_time=838.773463, accumulated_logging_time=0.485421, accumulated_submission_time=7177.283157, global_step=15489, preemption_count=0, score=7177.283157, test/accuracy=0.328500, test/loss=3.233922, test/num_examples=10000, total_duration=8017.423315, train/accuracy=0.449102, train/loss=2.497513, validation/accuracy=0.422440, validation/loss=2.632542, validation/num_examples=50000
I0205 16:46:44.261214 139946397853440 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.7341779470443726, loss=3.4992868900299072
I0205 16:47:26.841942 139946414638848 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.49237060546875, loss=3.657693862915039
I0205 16:48:13.256737 139946397853440 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.7327240705490112, loss=3.5447778701782227
I0205 16:48:59.654333 139946414638848 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.7034088373184204, loss=3.6269962787628174
I0205 16:49:46.108639 139946397853440 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.483283519744873, loss=4.1153564453125
I0205 16:50:32.514533 139946414638848 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.5401214361190796, loss=3.3563506603240967
I0205 16:51:19.032055 139946397853440 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.576285719871521, loss=3.32004451751709
I0205 16:52:06.068437 139946414638848 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.3945651054382324, loss=3.6967763900756836
I0205 16:52:52.336485 139946397853440 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.471807599067688, loss=4.179635047912598
I0205 16:53:38.643704 139946414638848 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.6369285583496094, loss=3.4235315322875977
I0205 16:53:39.700112 140107197974336 spec.py:321] Evaluating on the training split.
I0205 16:53:50.676177 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 16:54:26.554270 140107197974336 spec.py:349] Evaluating on the test split.
I0205 16:54:28.151727 140107197974336 submission_runner.py:408] Time since start: 8486.05s, 	Step: 16404, 	{'train/accuracy': 0.4757421910762787, 'train/loss': 2.3351094722747803, 'validation/accuracy': 0.44132000207901, 'validation/loss': 2.5121448040008545, 'validation/num_examples': 50000, 'test/accuracy': 0.34230002760887146, 'test/loss': 3.1236488819122314, 'test/num_examples': 10000, 'score': 7597.3759133815765, 'total_duration': 8486.046859264374, 'accumulated_submission_time': 7597.3759133815765, 'accumulated_eval_time': 887.2250754833221, 'accumulated_logging_time': 0.5128743648529053}
I0205 16:54:28.169088 139946397853440 logging_writer.py:48] [16404] accumulated_eval_time=887.225075, accumulated_logging_time=0.512874, accumulated_submission_time=7597.375913, global_step=16404, preemption_count=0, score=7597.375913, test/accuracy=0.342300, test/loss=3.123649, test/num_examples=10000, total_duration=8486.046859, train/accuracy=0.475742, train/loss=2.335109, validation/accuracy=0.441320, validation/loss=2.512145, validation/num_examples=50000
I0205 16:55:08.382099 139946414638848 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.1047027111053467, loss=5.482151031494141
I0205 16:55:54.693311 139946397853440 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.5875682830810547, loss=3.2978296279907227
I0205 16:56:41.396447 139946414638848 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.577629804611206, loss=3.3733596801757812
I0205 16:57:27.885121 139946397853440 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.2723042964935303, loss=4.486142158508301
I0205 16:58:14.049869 139946414638848 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.5758354663848877, loss=3.549617290496826
I0205 16:59:00.510132 139946397853440 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.3271839618682861, loss=4.1159443855285645
I0205 16:59:47.277386 139946414638848 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.4938815832138062, loss=3.863921642303467
I0205 17:00:33.762741 139946397853440 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.5084588527679443, loss=3.3696658611297607
I0205 17:01:20.484482 139946414638848 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.4950512647628784, loss=4.2005205154418945
I0205 17:01:28.533138 140107197974336 spec.py:321] Evaluating on the training split.
I0205 17:01:39.590357 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 17:02:13.842100 140107197974336 spec.py:349] Evaluating on the test split.
I0205 17:02:15.432121 140107197974336 submission_runner.py:408] Time since start: 8953.33s, 	Step: 17319, 	{'train/accuracy': 0.4820898473262787, 'train/loss': 2.317319869995117, 'validation/accuracy': 0.44287997484207153, 'validation/loss': 2.516441583633423, 'validation/num_examples': 50000, 'test/accuracy': 0.33890002965927124, 'test/loss': 3.1433703899383545, 'test/num_examples': 10000, 'score': 8017.678372621536, 'total_duration': 8953.327250957489, 'accumulated_submission_time': 8017.678372621536, 'accumulated_eval_time': 934.1244015693665, 'accumulated_logging_time': 0.5392537117004395}
I0205 17:02:15.450571 139946397853440 logging_writer.py:48] [17319] accumulated_eval_time=934.124402, accumulated_logging_time=0.539254, accumulated_submission_time=8017.678373, global_step=17319, preemption_count=0, score=8017.678373, test/accuracy=0.338900, test/loss=3.143370, test/num_examples=10000, total_duration=8953.327251, train/accuracy=0.482090, train/loss=2.317320, validation/accuracy=0.442880, validation/loss=2.516442, validation/num_examples=50000
I0205 17:02:48.905715 139946414638848 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.26638662815094, loss=4.427748203277588
I0205 17:03:35.241468 139946397853440 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.583457112312317, loss=3.3893327713012695
I0205 17:04:21.425274 139946414638848 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.6858375072479248, loss=3.356330156326294
I0205 17:05:08.183109 139946397853440 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.6338154077529907, loss=3.327878475189209
I0205 17:05:54.544827 139946414638848 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.635391354560852, loss=3.233638048171997
I0205 17:06:41.125014 139946397853440 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.7685763835906982, loss=3.483234405517578
I0205 17:07:27.775572 139946414638848 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.6703383922576904, loss=3.3384063243865967
I0205 17:08:14.196836 139946397853440 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.727124810218811, loss=3.2226762771606445
I0205 17:09:00.629617 139946414638848 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.3101695775985718, loss=4.201858043670654
I0205 17:09:15.762982 140107197974336 spec.py:321] Evaluating on the training split.
I0205 17:09:26.573128 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 17:09:58.087920 140107197974336 spec.py:349] Evaluating on the test split.
I0205 17:09:59.688987 140107197974336 submission_runner.py:408] Time since start: 9417.58s, 	Step: 18234, 	{'train/accuracy': 0.4768359363079071, 'train/loss': 2.3421082496643066, 'validation/accuracy': 0.4420199990272522, 'validation/loss': 2.51889705657959, 'validation/num_examples': 50000, 'test/accuracy': 0.343500018119812, 'test/loss': 3.1278634071350098, 'test/num_examples': 10000, 'score': 8437.92775630951, 'total_duration': 9417.584113836288, 'accumulated_submission_time': 8437.92775630951, 'accumulated_eval_time': 978.050395488739, 'accumulated_logging_time': 0.5683753490447998}
I0205 17:09:59.710084 139946397853440 logging_writer.py:48] [18234] accumulated_eval_time=978.050395, accumulated_logging_time=0.568375, accumulated_submission_time=8437.927756, global_step=18234, preemption_count=0, score=8437.927756, test/accuracy=0.343500, test/loss=3.127863, test/num_examples=10000, total_duration=9417.584114, train/accuracy=0.476836, train/loss=2.342108, validation/accuracy=0.442020, validation/loss=2.518897, validation/num_examples=50000
I0205 17:10:26.453775 139946414638848 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.2409943342208862, loss=4.062526702880859
I0205 17:11:12.406758 139946397853440 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.6046026945114136, loss=3.2500128746032715
I0205 17:11:58.979094 139946414638848 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.5943697690963745, loss=3.2516582012176514
I0205 17:12:45.261573 139946397853440 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.4474880695343018, loss=3.222820520401001
I0205 17:13:31.779646 139946414638848 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.6003674268722534, loss=3.266895055770874
I0205 17:14:18.146505 139946397853440 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.1135191917419434, loss=4.7170209884643555
I0205 17:15:04.947716 139946414638848 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.4225715398788452, loss=3.6077301502227783
I0205 17:15:51.465163 139946397853440 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.5800386667251587, loss=3.1552915573120117
I0205 17:16:38.030210 139946414638848 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.0434956550598145, loss=5.52072811126709
I0205 17:16:59.753501 140107197974336 spec.py:321] Evaluating on the training split.
I0205 17:17:10.890279 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 17:17:48.510736 140107197974336 spec.py:349] Evaluating on the test split.
I0205 17:17:50.099581 140107197974336 submission_runner.py:408] Time since start: 9887.99s, 	Step: 19148, 	{'train/accuracy': 0.49177733063697815, 'train/loss': 2.2599568367004395, 'validation/accuracy': 0.4593999981880188, 'validation/loss': 2.4413387775421143, 'validation/num_examples': 50000, 'test/accuracy': 0.35860002040863037, 'test/loss': 3.0742838382720947, 'test/num_examples': 10000, 'score': 8857.909608125687, 'total_duration': 9887.99471116066, 'accumulated_submission_time': 8857.909608125687, 'accumulated_eval_time': 1028.3964698314667, 'accumulated_logging_time': 0.5990426540374756}
I0205 17:17:50.118850 139946397853440 logging_writer.py:48] [19148] accumulated_eval_time=1028.396470, accumulated_logging_time=0.599043, accumulated_submission_time=8857.909608, global_step=19148, preemption_count=0, score=8857.909608, test/accuracy=0.358600, test/loss=3.074284, test/num_examples=10000, total_duration=9887.994711, train/accuracy=0.491777, train/loss=2.259957, validation/accuracy=0.459400, validation/loss=2.441339, validation/num_examples=50000
I0205 17:18:11.176883 139946414638848 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.5227992534637451, loss=3.2852885723114014
I0205 17:18:56.047947 139946397853440 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.6440401077270508, loss=3.404930591583252
I0205 17:19:42.557845 139946414638848 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.5471093654632568, loss=3.3266539573669434
I0205 17:20:28.994670 139946397853440 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.2893619537353516, loss=5.675298690795898
I0205 17:21:15.372119 139946414638848 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.360845685005188, loss=3.7952113151550293
I0205 17:22:01.753720 139946397853440 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.6029276847839355, loss=3.2574803829193115
I0205 17:22:48.275745 139946414638848 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.409365177154541, loss=3.630465030670166
I0205 17:23:34.737603 139946397853440 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.6310943365097046, loss=3.248628854751587
I0205 17:24:21.140321 139946414638848 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.5223604440689087, loss=3.1834404468536377
I0205 17:24:50.128635 140107197974336 spec.py:321] Evaluating on the training split.
I0205 17:25:01.323118 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 17:25:38.330390 140107197974336 spec.py:349] Evaluating on the test split.
I0205 17:25:39.927796 140107197974336 submission_runner.py:408] Time since start: 10357.82s, 	Step: 20064, 	{'train/accuracy': 0.509082019329071, 'train/loss': 2.1851208209991455, 'validation/accuracy': 0.4652799963951111, 'validation/loss': 2.3844094276428223, 'validation/num_examples': 50000, 'test/accuracy': 0.36160001158714294, 'test/loss': 3.0053672790527344, 'test/num_examples': 10000, 'score': 9277.858618497849, 'total_duration': 10357.822923898697, 'accumulated_submission_time': 9277.858618497849, 'accumulated_eval_time': 1078.1956298351288, 'accumulated_logging_time': 0.6277709007263184}
I0205 17:25:39.948006 139946397853440 logging_writer.py:48] [20064] accumulated_eval_time=1078.195630, accumulated_logging_time=0.627771, accumulated_submission_time=9277.858618, global_step=20064, preemption_count=0, score=9277.858618, test/accuracy=0.361600, test/loss=3.005367, test/num_examples=10000, total_duration=10357.822924, train/accuracy=0.509082, train/loss=2.185121, validation/accuracy=0.465280, validation/loss=2.384409, validation/num_examples=50000
I0205 17:25:54.478703 139946414638848 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.0659018754959106, loss=5.292198181152344
I0205 17:26:38.489546 139946397853440 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.3139042854309082, loss=4.172208786010742
I0205 17:27:24.976691 139946414638848 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.33946692943573, loss=4.830275535583496
I0205 17:28:11.701763 139946397853440 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.112295150756836, loss=5.739584922790527
I0205 17:28:58.254886 139946414638848 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.5983911752700806, loss=3.1908187866210938
I0205 17:29:44.775500 139946397853440 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.9149168133735657, loss=5.6794610023498535
I0205 17:30:31.120320 139946414638848 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.5978072881698608, loss=3.2106711864471436
I0205 17:31:17.582353 139946397853440 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.385737657546997, loss=3.177934169769287
I0205 17:32:03.817958 139946414638848 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.1532793045043945, loss=5.661016941070557
I0205 17:32:40.325309 140107197974336 spec.py:321] Evaluating on the training split.
I0205 17:32:51.324416 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 17:33:26.228075 140107197974336 spec.py:349] Evaluating on the test split.
I0205 17:33:27.826520 140107197974336 submission_runner.py:408] Time since start: 10825.72s, 	Step: 20980, 	{'train/accuracy': 0.53369140625, 'train/loss': 2.057878255844116, 'validation/accuracy': 0.4750799834728241, 'validation/loss': 2.334637403488159, 'validation/num_examples': 50000, 'test/accuracy': 0.36410000920295715, 'test/loss': 2.9753594398498535, 'test/num_examples': 10000, 'score': 9698.17512512207, 'total_duration': 10825.721643447876, 'accumulated_submission_time': 9698.17512512207, 'accumulated_eval_time': 1125.6968231201172, 'accumulated_logging_time': 0.6573843955993652}
I0205 17:33:27.850183 139946397853440 logging_writer.py:48] [20980] accumulated_eval_time=1125.696823, accumulated_logging_time=0.657384, accumulated_submission_time=9698.175125, global_step=20980, preemption_count=0, score=9698.175125, test/accuracy=0.364100, test/loss=2.975359, test/num_examples=10000, total_duration=10825.721643, train/accuracy=0.533691, train/loss=2.057878, validation/accuracy=0.475080, validation/loss=2.334637, validation/num_examples=50000
I0205 17:33:36.099778 139946414638848 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0975606441497803, loss=4.9001970291137695
I0205 17:34:19.338325 139946397853440 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.4131819009780884, loss=3.415818452835083
I0205 17:35:05.940631 139946414638848 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.5841872692108154, loss=3.2389605045318604
I0205 17:35:52.195291 139946397853440 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.7567425966262817, loss=3.1235196590423584
I0205 17:36:38.413856 139946414638848 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.1335947513580322, loss=5.339227676391602
I0205 17:37:24.704096 139946397853440 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.3337558507919312, loss=4.134667873382568
I0205 17:38:11.087856 139946414638848 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.6117950677871704, loss=3.2760214805603027
I0205 17:38:57.463131 139946397853440 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.3611079454421997, loss=3.2856292724609375
I0205 17:39:43.973136 139946414638848 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.6178398132324219, loss=3.3778605461120605
I0205 17:40:28.238395 140107197974336 spec.py:321] Evaluating on the training split.
I0205 17:40:39.053677 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 17:41:13.911539 140107197974336 spec.py:349] Evaluating on the test split.
I0205 17:41:15.513633 140107197974336 submission_runner.py:408] Time since start: 11293.41s, 	Step: 21897, 	{'train/accuracy': 0.507519543170929, 'train/loss': 2.1788175106048584, 'validation/accuracy': 0.47655999660491943, 'validation/loss': 2.3385961055755615, 'validation/num_examples': 50000, 'test/accuracy': 0.3733000159263611, 'test/loss': 2.9646759033203125, 'test/num_examples': 10000, 'score': 10118.501426696777, 'total_duration': 11293.408746242523, 'accumulated_submission_time': 10118.501426696777, 'accumulated_eval_time': 1172.9720392227173, 'accumulated_logging_time': 0.6915838718414307}
I0205 17:41:15.540461 139946397853440 logging_writer.py:48] [21897] accumulated_eval_time=1172.972039, accumulated_logging_time=0.691584, accumulated_submission_time=10118.501427, global_step=21897, preemption_count=0, score=10118.501427, test/accuracy=0.373300, test/loss=2.964676, test/num_examples=10000, total_duration=11293.408746, train/accuracy=0.507520, train/loss=2.178818, validation/accuracy=0.476560, validation/loss=2.338596, validation/num_examples=50000
I0205 17:41:17.113529 139946414638848 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.0294839143753052, loss=4.584022045135498
I0205 17:41:59.022942 139946397853440 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.6724209785461426, loss=3.140948534011841
I0205 17:42:45.030741 139946414638848 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.3479236364364624, loss=3.310237407684326
I0205 17:43:31.817129 139946397853440 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.7587167024612427, loss=3.075542688369751
I0205 17:44:18.140116 139946414638848 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.4971762895584106, loss=5.593259811401367
I0205 17:45:05.035698 139946397853440 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.5876593589782715, loss=3.0520172119140625
I0205 17:45:51.334888 139946414638848 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.617533802986145, loss=3.02184796333313
I0205 17:46:37.339695 139946397853440 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.8765379190444946, loss=3.187903642654419
I0205 17:47:23.641853 139946414638848 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.5423181056976318, loss=3.280478000640869
I0205 17:48:10.201109 139946397853440 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.1007071733474731, loss=5.542928695678711
I0205 17:48:15.861269 140107197974336 spec.py:321] Evaluating on the training split.
I0205 17:48:26.656434 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 17:49:03.201801 140107197974336 spec.py:349] Evaluating on the test split.
I0205 17:49:04.803252 140107197974336 submission_runner.py:408] Time since start: 11762.70s, 	Step: 22814, 	{'train/accuracy': 0.5238476395606995, 'train/loss': 2.071101427078247, 'validation/accuracy': 0.4860999882221222, 'validation/loss': 2.2656033039093018, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 2.9053289890289307, 'test/num_examples': 10000, 'score': 10538.751983642578, 'total_duration': 11762.698380231857, 'accumulated_submission_time': 10538.751983642578, 'accumulated_eval_time': 1221.914003610611, 'accumulated_logging_time': 0.729525089263916}
I0205 17:49:04.831279 139946414638848 logging_writer.py:48] [22814] accumulated_eval_time=1221.914004, accumulated_logging_time=0.729525, accumulated_submission_time=10538.751984, global_step=22814, preemption_count=0, score=10538.751984, test/accuracy=0.377500, test/loss=2.905329, test/num_examples=10000, total_duration=11762.698380, train/accuracy=0.523848, train/loss=2.071101, validation/accuracy=0.486100, validation/loss=2.265603, validation/num_examples=50000
I0205 17:49:40.431429 139946397853440 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.5695300102233887, loss=3.220839500427246
I0205 17:50:26.663786 139946414638848 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.586139440536499, loss=3.062591075897217
I0205 17:51:13.434479 139946397853440 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.3737812042236328, loss=3.537059783935547
I0205 17:51:59.455682 139946414638848 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.0467222929000854, loss=5.519360065460205
I0205 17:52:45.905311 139946397853440 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.5795767307281494, loss=3.599698066711426
I0205 17:53:32.476351 139946414638848 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.5801829099655151, loss=3.3063249588012695
I0205 17:54:18.862311 139946397853440 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.4701576232910156, loss=3.071367025375366
I0205 17:55:05.562107 139946414638848 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.4244564771652222, loss=5.628629684448242
I0205 17:55:51.911099 139946397853440 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.5964833498001099, loss=3.262519359588623
I0205 17:56:05.115738 140107197974336 spec.py:321] Evaluating on the training split.
I0205 17:56:15.928428 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 17:56:49.374592 140107197974336 spec.py:349] Evaluating on the test split.
I0205 17:56:50.974874 140107197974336 submission_runner.py:408] Time since start: 12228.87s, 	Step: 23730, 	{'train/accuracy': 0.5507421493530273, 'train/loss': 1.9389535188674927, 'validation/accuracy': 0.49563997983932495, 'validation/loss': 2.2019717693328857, 'validation/num_examples': 50000, 'test/accuracy': 0.3871000111103058, 'test/loss': 2.8293137550354004, 'test/num_examples': 10000, 'score': 10958.97521162033, 'total_duration': 12228.869998455048, 'accumulated_submission_time': 10958.97521162033, 'accumulated_eval_time': 1267.7731275558472, 'accumulated_logging_time': 0.766730546951294}
I0205 17:56:50.995870 139946414638848 logging_writer.py:48] [23730] accumulated_eval_time=1267.773128, accumulated_logging_time=0.766731, accumulated_submission_time=10958.975212, global_step=23730, preemption_count=0, score=10958.975212, test/accuracy=0.387100, test/loss=2.829314, test/num_examples=10000, total_duration=12228.869998, train/accuracy=0.550742, train/loss=1.938954, validation/accuracy=0.495640, validation/loss=2.201972, validation/num_examples=50000
I0205 17:57:19.334701 139946397853440 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.3228579759597778, loss=3.7267658710479736
I0205 17:58:05.073643 139946414638848 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.4451348781585693, loss=3.519432783126831
I0205 17:58:51.096778 139946397853440 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.6332236528396606, loss=3.113203763961792
I0205 17:59:37.595706 139946414638848 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0073258876800537, loss=5.486714839935303
I0205 18:00:24.001190 139946397853440 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.471062421798706, loss=3.1083130836486816
I0205 18:01:10.467946 139946414638848 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.5711573362350464, loss=2.9472575187683105
I0205 18:01:57.140824 139946397853440 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.6594740152359009, loss=3.029726982116699
I0205 18:02:43.352533 139946414638848 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.8491473197937012, loss=2.99092435836792
I0205 18:03:29.977323 139946397853440 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.6470683813095093, loss=2.8922290802001953
I0205 18:03:51.201177 140107197974336 spec.py:321] Evaluating on the training split.
I0205 18:04:02.093197 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 18:04:39.701754 140107197974336 spec.py:349] Evaluating on the test split.
I0205 18:04:41.308393 140107197974336 submission_runner.py:408] Time since start: 12699.20s, 	Step: 24647, 	{'train/accuracy': 0.5287109017372131, 'train/loss': 2.059361219406128, 'validation/accuracy': 0.4931999742984772, 'validation/loss': 2.238321542739868, 'validation/num_examples': 50000, 'test/accuracy': 0.3889000117778778, 'test/loss': 2.873844623565674, 'test/num_examples': 10000, 'score': 11379.114792823792, 'total_duration': 12699.20350074768, 'accumulated_submission_time': 11379.114792823792, 'accumulated_eval_time': 1317.8803231716156, 'accumulated_logging_time': 0.8011837005615234}
I0205 18:04:41.328431 139946414638848 logging_writer.py:48] [24647] accumulated_eval_time=1317.880323, accumulated_logging_time=0.801184, accumulated_submission_time=11379.114793, global_step=24647, preemption_count=0, score=11379.114793, test/accuracy=0.388900, test/loss=2.873845, test/num_examples=10000, total_duration=12699.203501, train/accuracy=0.528711, train/loss=2.059361, validation/accuracy=0.493200, validation/loss=2.238322, validation/num_examples=50000
I0205 18:05:02.534670 139946397853440 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.5900779962539673, loss=3.0912423133850098
I0205 18:05:47.590640 139946414638848 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.2942824363708496, loss=4.723614692687988
I0205 18:06:34.468751 139946397853440 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.5614556074142456, loss=3.078456163406372
I0205 18:07:21.020876 139946414638848 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.5499252080917358, loss=3.388019561767578
I0205 18:08:07.328092 139946397853440 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.0266683101654053, loss=5.300294399261475
I0205 18:08:53.893171 139946414638848 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.607513189315796, loss=3.0736536979675293
I0205 18:09:40.442617 139946397853440 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.6243517398834229, loss=3.582598924636841
I0205 18:10:26.913418 139946414638848 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.618672490119934, loss=3.1462135314941406
I0205 18:11:13.321883 139946397853440 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.3748191595077515, loss=3.5509121417999268
I0205 18:11:41.674491 140107197974336 spec.py:321] Evaluating on the training split.
I0205 18:11:52.216059 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 18:12:28.868569 140107197974336 spec.py:349] Evaluating on the test split.
I0205 18:12:30.470908 140107197974336 submission_runner.py:408] Time since start: 13168.37s, 	Step: 25563, 	{'train/accuracy': 0.5419335961341858, 'train/loss': 1.9885637760162354, 'validation/accuracy': 0.5034799575805664, 'validation/loss': 2.1718690395355225, 'validation/num_examples': 50000, 'test/accuracy': 0.3960000276565552, 'test/loss': 2.7996408939361572, 'test/num_examples': 10000, 'score': 11799.398998975754, 'total_duration': 13168.366040945053, 'accumulated_submission_time': 11799.398998975754, 'accumulated_eval_time': 1366.6767621040344, 'accumulated_logging_time': 0.8305325508117676}
I0205 18:12:30.491129 139946414638848 logging_writer.py:48] [25563] accumulated_eval_time=1366.676762, accumulated_logging_time=0.830533, accumulated_submission_time=11799.398999, global_step=25563, preemption_count=0, score=11799.398999, test/accuracy=0.396000, test/loss=2.799641, test/num_examples=10000, total_duration=13168.366041, train/accuracy=0.541934, train/loss=1.988564, validation/accuracy=0.503480, validation/loss=2.171869, validation/num_examples=50000
I0205 18:12:45.435020 139946397853440 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.256473183631897, loss=4.039371967315674
I0205 18:13:29.386220 139946414638848 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.5696444511413574, loss=3.049741506576538
I0205 18:14:15.961002 139946397853440 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.2610751390457153, loss=4.361425399780273
I0205 18:15:02.645569 139946414638848 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.2766919136047363, loss=4.173348903656006
I0205 18:15:49.227330 139946397853440 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.6513336896896362, loss=3.0189247131347656
I0205 18:16:35.833766 139946414638848 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.613100528717041, loss=2.9538140296936035
I0205 18:17:22.340300 139946397853440 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.0926419496536255, loss=5.1940598487854
I0205 18:18:08.935483 139946414638848 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.551351547241211, loss=3.1507225036621094
I0205 18:18:55.366996 139946397853440 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.860790491104126, loss=3.2271976470947266
I0205 18:19:30.503376 140107197974336 spec.py:321] Evaluating on the training split.
I0205 18:19:42.261453 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 18:20:17.639977 140107197974336 spec.py:349] Evaluating on the test split.
I0205 18:20:19.230644 140107197974336 submission_runner.py:408] Time since start: 13637.13s, 	Step: 26477, 	{'train/accuracy': 0.5632421970367432, 'train/loss': 1.8941307067871094, 'validation/accuracy': 0.5120199918746948, 'validation/loss': 2.144299268722534, 'validation/num_examples': 50000, 'test/accuracy': 0.39740002155303955, 'test/loss': 2.7761967182159424, 'test/num_examples': 10000, 'score': 12219.34901380539, 'total_duration': 13637.125775814056, 'accumulated_submission_time': 12219.34901380539, 'accumulated_eval_time': 1415.4040472507477, 'accumulated_logging_time': 0.8615057468414307}
I0205 18:20:19.250882 139946414638848 logging_writer.py:48] [26477] accumulated_eval_time=1415.404047, accumulated_logging_time=0.861506, accumulated_submission_time=12219.349014, global_step=26477, preemption_count=0, score=12219.349014, test/accuracy=0.397400, test/loss=2.776197, test/num_examples=10000, total_duration=13637.125776, train/accuracy=0.563242, train/loss=1.894131, validation/accuracy=0.512020, validation/loss=2.144299, validation/num_examples=50000
I0205 18:20:28.675574 139946397853440 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.5005545616149902, loss=3.1060280799865723
I0205 18:21:11.815402 139946414638848 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.3372821807861328, loss=3.5712451934814453
I0205 18:21:58.334158 139946397853440 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.5956072807312012, loss=2.8107264041900635
I0205 18:22:44.842841 139946414638848 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.5509624481201172, loss=3.172874927520752
I0205 18:23:31.297097 139946397853440 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.349812626838684, loss=3.67508602142334
I0205 18:24:17.752531 139946414638848 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.6602643728256226, loss=2.9454944133758545
I0205 18:25:04.617982 139946397853440 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.46649169921875, loss=2.844412088394165
I0205 18:25:51.136306 139946414638848 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.2282416820526123, loss=4.568387031555176
I0205 18:26:37.570506 139946397853440 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.4960652589797974, loss=3.392456293106079
I0205 18:27:19.617397 140107197974336 spec.py:321] Evaluating on the training split.
I0205 18:27:30.618846 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 18:28:07.268821 140107197974336 spec.py:349] Evaluating on the test split.
I0205 18:28:08.864345 140107197974336 submission_runner.py:408] Time since start: 14106.76s, 	Step: 27392, 	{'train/accuracy': 0.5482421517372131, 'train/loss': 1.9673892259597778, 'validation/accuracy': 0.5123999714851379, 'validation/loss': 2.13213849067688, 'validation/num_examples': 50000, 'test/accuracy': 0.4012000262737274, 'test/loss': 2.763537645339966, 'test/num_examples': 10000, 'score': 12639.654458522797, 'total_duration': 14106.759474277496, 'accumulated_submission_time': 12639.654458522797, 'accumulated_eval_time': 1464.6509912014008, 'accumulated_logging_time': 0.891242265701294}
I0205 18:28:08.887846 139946414638848 logging_writer.py:48] [27392] accumulated_eval_time=1464.650991, accumulated_logging_time=0.891242, accumulated_submission_time=12639.654459, global_step=27392, preemption_count=0, score=12639.654459, test/accuracy=0.401200, test/loss=2.763538, test/num_examples=10000, total_duration=14106.759474, train/accuracy=0.548242, train/loss=1.967389, validation/accuracy=0.512400, validation/loss=2.132138, validation/num_examples=50000
I0205 18:28:12.431448 139946397853440 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.5712921619415283, loss=3.0384671688079834
I0205 18:28:54.622248 139946414638848 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.4257287979125977, loss=3.573131799697876
I0205 18:29:40.491074 139946397853440 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.465968370437622, loss=3.5359537601470947
I0205 18:30:26.725940 139946414638848 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.2733063697814941, loss=4.501955509185791
I0205 18:31:13.498510 139946397853440 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.6553385257720947, loss=3.140166759490967
I0205 18:31:59.916016 139946414638848 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.5629290342330933, loss=3.004958391189575
I0205 18:32:46.119861 139946397853440 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.592339277267456, loss=2.9148268699645996
I0205 18:33:32.752038 139946414638848 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.5448685884475708, loss=3.0276687145233154
I0205 18:34:19.270250 139946397853440 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.6075444221496582, loss=2.8902082443237305
I0205 18:35:06.034827 139946414638848 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.6664961576461792, loss=2.8501250743865967
I0205 18:35:08.905620 140107197974336 spec.py:321] Evaluating on the training split.
I0205 18:35:19.776515 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 18:35:55.100447 140107197974336 spec.py:349] Evaluating on the test split.
I0205 18:35:56.695800 140107197974336 submission_runner.py:408] Time since start: 14574.59s, 	Step: 28308, 	{'train/accuracy': 0.5547069907188416, 'train/loss': 1.929384708404541, 'validation/accuracy': 0.5189200043678284, 'validation/loss': 2.1071009635925293, 'validation/num_examples': 50000, 'test/accuracy': 0.40060001611709595, 'test/loss': 2.744941473007202, 'test/num_examples': 10000, 'score': 13059.61026597023, 'total_duration': 14574.590921640396, 'accumulated_submission_time': 13059.61026597023, 'accumulated_eval_time': 1512.441159248352, 'accumulated_logging_time': 0.9247598648071289}
I0205 18:35:56.719605 139946397853440 logging_writer.py:48] [28308] accumulated_eval_time=1512.441159, accumulated_logging_time=0.924760, accumulated_submission_time=13059.610266, global_step=28308, preemption_count=0, score=13059.610266, test/accuracy=0.400600, test/loss=2.744941, test/num_examples=10000, total_duration=14574.590922, train/accuracy=0.554707, train/loss=1.929385, validation/accuracy=0.518920, validation/loss=2.107101, validation/num_examples=50000
I0205 18:36:35.439517 139946414638848 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.09160315990448, loss=5.216111183166504
I0205 18:37:21.699363 139946397853440 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.5651298761367798, loss=2.9136087894439697
I0205 18:38:08.329845 139946414638848 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.1941616535186768, loss=5.188648223876953
I0205 18:38:54.573306 139946397853440 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.1393651962280273, loss=5.5508036613464355
I0205 18:39:40.925157 139946414638848 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.3204491138458252, loss=4.572619915008545
I0205 18:40:27.055565 139946397853440 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.6288907527923584, loss=2.889909505844116
I0205 18:41:13.490773 139946414638848 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.4205594062805176, loss=3.4732227325439453
I0205 18:41:59.717140 139946397853440 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.3594902753829956, loss=3.8217649459838867
I0205 18:42:46.372894 139946414638848 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.2007538080215454, loss=5.1123366355896
I0205 18:42:56.714445 140107197974336 spec.py:321] Evaluating on the training split.
I0205 18:43:07.599960 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 18:43:45.852797 140107197974336 spec.py:349] Evaluating on the test split.
I0205 18:43:47.447301 140107197974336 submission_runner.py:408] Time since start: 15045.34s, 	Step: 29224, 	{'train/accuracy': 0.5635741949081421, 'train/loss': 1.887037754058838, 'validation/accuracy': 0.5194000005722046, 'validation/loss': 2.098005533218384, 'validation/num_examples': 50000, 'test/accuracy': 0.4052000045776367, 'test/loss': 2.7502341270446777, 'test/num_examples': 10000, 'score': 13479.542765378952, 'total_duration': 15045.342432498932, 'accumulated_submission_time': 13479.542765378952, 'accumulated_eval_time': 1563.1740138530731, 'accumulated_logging_time': 0.9577534198760986}
I0205 18:43:47.470020 139946397853440 logging_writer.py:48] [29224] accumulated_eval_time=1563.174014, accumulated_logging_time=0.957753, accumulated_submission_time=13479.542765, global_step=29224, preemption_count=0, score=13479.542765, test/accuracy=0.405200, test/loss=2.750234, test/num_examples=10000, total_duration=15045.342432, train/accuracy=0.563574, train/loss=1.887038, validation/accuracy=0.519400, validation/loss=2.098006, validation/num_examples=50000
I0205 18:44:18.442389 139946414638848 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.2522192001342773, loss=2.98296856880188
I0205 18:45:04.873348 139946397853440 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.575650691986084, loss=3.1916322708129883
I0205 18:45:51.147524 139946414638848 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.3087102174758911, loss=4.588079929351807
I0205 18:46:37.629168 139946397853440 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.6204688549041748, loss=2.9161789417266846
I0205 18:47:24.029775 139946414638848 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.5053423643112183, loss=2.9656057357788086
I0205 18:48:10.593730 139946397853440 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.6197731494903564, loss=2.9168612957000732
I0205 18:48:56.735692 139946414638848 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.3272743225097656, loss=4.5209197998046875
I0205 18:49:43.197672 139946397853440 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.55862557888031, loss=3.406616449356079
I0205 18:50:29.507921 139946414638848 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.5506004095077515, loss=3.039261817932129
I0205 18:50:47.707575 140107197974336 spec.py:321] Evaluating on the training split.
I0205 18:50:58.712030 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 18:51:34.769650 140107197974336 spec.py:349] Evaluating on the test split.
I0205 18:51:36.363241 140107197974336 submission_runner.py:408] Time since start: 15514.26s, 	Step: 30141, 	{'train/accuracy': 0.5602734088897705, 'train/loss': 1.8773443698883057, 'validation/accuracy': 0.5280199646949768, 'validation/loss': 2.0621399879455566, 'validation/num_examples': 50000, 'test/accuracy': 0.4148000180721283, 'test/loss': 2.698044538497925, 'test/num_examples': 10000, 'score': 13899.718740701675, 'total_duration': 15514.25835442543, 'accumulated_submission_time': 13899.718740701675, 'accumulated_eval_time': 1611.8296627998352, 'accumulated_logging_time': 0.9903049468994141}
I0205 18:51:36.383662 139946397853440 logging_writer.py:48] [30141] accumulated_eval_time=1611.829663, accumulated_logging_time=0.990305, accumulated_submission_time=13899.718741, global_step=30141, preemption_count=0, score=13899.718741, test/accuracy=0.414800, test/loss=2.698045, test/num_examples=10000, total_duration=15514.258354, train/accuracy=0.560273, train/loss=1.877344, validation/accuracy=0.528020, validation/loss=2.062140, validation/num_examples=50000
I0205 18:51:59.947082 139946414638848 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.383690595626831, loss=3.676115036010742
I0205 18:52:45.483672 139946397853440 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.1342663764953613, loss=5.305671215057373
I0205 18:53:31.983632 139946414638848 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.888346552848816, loss=2.931872606277466
I0205 18:54:18.414665 139946397853440 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0799524784088135, loss=5.402860641479492
I0205 18:55:05.092247 139946414638848 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.7705471515655518, loss=2.8856263160705566
I0205 18:55:51.424384 139946397853440 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.27150559425354, loss=4.671144485473633
I0205 18:56:37.771055 139946414638848 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.5422884225845337, loss=2.98681902885437
I0205 18:57:24.215204 139946397853440 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.5788092613220215, loss=3.6169962882995605
I0205 18:58:10.726194 139946414638848 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.3407117128372192, loss=5.065896987915039
I0205 18:58:36.462245 140107197974336 spec.py:321] Evaluating on the training split.
I0205 18:58:47.266987 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 18:59:23.139824 140107197974336 spec.py:349] Evaluating on the test split.
I0205 18:59:24.740802 140107197974336 submission_runner.py:408] Time since start: 15982.64s, 	Step: 31057, 	{'train/accuracy': 0.5613867044448853, 'train/loss': 1.8950296640396118, 'validation/accuracy': 0.526199996471405, 'validation/loss': 2.0695595741271973, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.691118001937866, 'test/num_examples': 10000, 'score': 14319.735169649124, 'total_duration': 15982.635932683945, 'accumulated_submission_time': 14319.735169649124, 'accumulated_eval_time': 1660.1082208156586, 'accumulated_logging_time': 1.0202922821044922}
I0205 18:59:24.766463 139946397853440 logging_writer.py:48] [31057] accumulated_eval_time=1660.108221, accumulated_logging_time=1.020292, accumulated_submission_time=14319.735170, global_step=31057, preemption_count=0, score=14319.735170, test/accuracy=0.415800, test/loss=2.691118, test/num_examples=10000, total_duration=15982.635933, train/accuracy=0.561387, train/loss=1.895030, validation/accuracy=0.526200, validation/loss=2.069560, validation/num_examples=50000
I0205 18:59:42.056686 139946414638848 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.664427399635315, loss=3.352229595184326
I0205 19:00:26.760886 139946397853440 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.485055685043335, loss=2.650303602218628
I0205 19:01:13.194616 139946414638848 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.6107739210128784, loss=5.494668483734131
I0205 19:02:00.068365 139946397853440 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.5299229621887207, loss=3.44711971282959
I0205 19:02:46.475401 139946414638848 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.4109957218170166, loss=3.16029953956604
I0205 19:03:32.884867 139946397853440 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.6041972637176514, loss=2.8514091968536377
I0205 19:04:19.347930 139946414638848 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.3357290029525757, loss=4.605715274810791
I0205 19:05:06.063767 139946397853440 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.8189054727554321, loss=3.315732479095459
I0205 19:05:52.376263 139946414638848 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.3240031003952026, loss=3.4414079189300537
I0205 19:06:25.032626 140107197974336 spec.py:321] Evaluating on the training split.
I0205 19:06:36.027822 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 19:07:11.978844 140107197974336 spec.py:349] Evaluating on the test split.
I0205 19:07:13.589756 140107197974336 submission_runner.py:408] Time since start: 16451.48s, 	Step: 31972, 	{'train/accuracy': 0.5696093440055847, 'train/loss': 1.8706759214401245, 'validation/accuracy': 0.5287600159645081, 'validation/loss': 2.060586452484131, 'validation/num_examples': 50000, 'test/accuracy': 0.41190001368522644, 'test/loss': 2.7069880962371826, 'test/num_examples': 10000, 'score': 14739.937356710434, 'total_duration': 16451.484882354736, 'accumulated_submission_time': 14739.937356710434, 'accumulated_eval_time': 1708.6653501987457, 'accumulated_logging_time': 1.0565845966339111}
I0205 19:07:13.611287 139946397853440 logging_writer.py:48] [31972] accumulated_eval_time=1708.665350, accumulated_logging_time=1.056585, accumulated_submission_time=14739.937357, global_step=31972, preemption_count=0, score=14739.937357, test/accuracy=0.411900, test/loss=2.706988, test/num_examples=10000, total_duration=16451.484882, train/accuracy=0.569609, train/loss=1.870676, validation/accuracy=0.528760, validation/loss=2.060586, validation/num_examples=50000
I0205 19:07:24.999908 139946414638848 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.8313515186309814, loss=2.955453872680664
I0205 19:08:08.609472 139946397853440 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.1923762559890747, loss=3.813551425933838
I0205 19:08:55.022360 139946414638848 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.7960883378982544, loss=2.800663709640503
I0205 19:09:41.737961 139946397853440 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.5834472179412842, loss=2.806842565536499
I0205 19:10:28.057980 139946414638848 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.4543507099151611, loss=3.6946394443511963
I0205 19:11:14.216650 139946397853440 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.6341192722320557, loss=2.7930617332458496
I0205 19:12:00.268801 139946414638848 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.5765647888183594, loss=3.4384713172912598
I0205 19:12:46.552649 139946397853440 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.3182158470153809, loss=4.0754570960998535
I0205 19:13:33.006415 139946414638848 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.3040465116500854, loss=5.266128063201904
I0205 19:14:13.875781 140107197974336 spec.py:321] Evaluating on the training split.
I0205 19:14:25.050306 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 19:15:01.212299 140107197974336 spec.py:349] Evaluating on the test split.
I0205 19:15:02.825397 140107197974336 submission_runner.py:408] Time since start: 16920.72s, 	Step: 32890, 	{'train/accuracy': 0.5985351204872131, 'train/loss': 1.70049250125885, 'validation/accuracy': 0.5352999567985535, 'validation/loss': 2.0219011306762695, 'validation/num_examples': 50000, 'test/accuracy': 0.41600000858306885, 'test/loss': 2.677694797515869, 'test/num_examples': 10000, 'score': 15160.13657617569, 'total_duration': 16920.72052717209, 'accumulated_submission_time': 15160.13657617569, 'accumulated_eval_time': 1757.6149718761444, 'accumulated_logging_time': 1.0907628536224365}
I0205 19:15:02.847087 139946397853440 logging_writer.py:48] [32890] accumulated_eval_time=1757.614972, accumulated_logging_time=1.090763, accumulated_submission_time=15160.136576, global_step=32890, preemption_count=0, score=15160.136576, test/accuracy=0.416000, test/loss=2.677695, test/num_examples=10000, total_duration=16920.720527, train/accuracy=0.598535, train/loss=1.700493, validation/accuracy=0.535300, validation/loss=2.021901, validation/num_examples=50000
I0205 19:15:07.167602 139946414638848 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.4245426654815674, loss=4.841819763183594
I0205 19:15:49.595527 139946397853440 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.790851354598999, loss=2.8869335651397705
I0205 19:16:35.858524 139946414638848 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.9156309366226196, loss=3.143364429473877
I0205 19:17:22.402985 139946397853440 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.695391058921814, loss=3.4281976222991943
I0205 19:18:08.761298 139946414638848 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.438409686088562, loss=5.560492515563965
I0205 19:18:54.967398 139946397853440 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.1983541250228882, loss=4.863856792449951
I0205 19:19:41.288074 139946414638848 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.6048626899719238, loss=2.8855741024017334
I0205 19:20:27.737949 139946397853440 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.3595144748687744, loss=5.211754322052002
I0205 19:21:14.218719 139946414638848 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.7577875852584839, loss=2.87802791595459
I0205 19:22:00.557533 139946397853440 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.342764973640442, loss=3.8521549701690674
I0205 19:22:03.149967 140107197974336 spec.py:321] Evaluating on the training split.
I0205 19:22:14.069574 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 19:22:50.364704 140107197974336 spec.py:349] Evaluating on the test split.
I0205 19:22:51.959549 140107197974336 submission_runner.py:408] Time since start: 17389.85s, 	Step: 33807, 	{'train/accuracy': 0.567578136920929, 'train/loss': 1.879085898399353, 'validation/accuracy': 0.5329399704933167, 'validation/loss': 2.052635431289673, 'validation/num_examples': 50000, 'test/accuracy': 0.4164000153541565, 'test/loss': 2.689453363418579, 'test/num_examples': 10000, 'score': 15580.376125097275, 'total_duration': 17389.854667186737, 'accumulated_submission_time': 15580.376125097275, 'accumulated_eval_time': 1806.4245445728302, 'accumulated_logging_time': 1.1232614517211914}
I0205 19:22:51.980043 139946414638848 logging_writer.py:48] [33807] accumulated_eval_time=1806.424545, accumulated_logging_time=1.123261, accumulated_submission_time=15580.376125, global_step=33807, preemption_count=0, score=15580.376125, test/accuracy=0.416400, test/loss=2.689453, test/num_examples=10000, total_duration=17389.854667, train/accuracy=0.567578, train/loss=1.879086, validation/accuracy=0.532940, validation/loss=2.052635, validation/num_examples=50000
I0205 19:23:30.822419 139946397853440 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.2614166736602783, loss=4.56472635269165
I0205 19:24:17.084647 139946414638848 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.1631110906600952, loss=4.7037272453308105
I0205 19:25:03.554403 139946397853440 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.6711852550506592, loss=2.864401340484619
I0205 19:25:49.831919 139946414638848 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.4831491708755493, loss=3.3665080070495605
I0205 19:26:36.379813 139946397853440 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.5433177947998047, loss=2.885735273361206
I0205 19:27:22.720160 139946414638848 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.7200686931610107, loss=2.76497220993042
I0205 19:28:09.075100 139946397853440 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.670972466468811, loss=2.84702205657959
I0205 19:28:55.347793 139946414638848 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.395788311958313, loss=4.4728169441223145
I0205 19:29:41.710450 139946397853440 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.25307297706604, loss=5.495248317718506
I0205 19:29:52.109516 140107197974336 spec.py:321] Evaluating on the training split.
I0205 19:30:03.280370 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 19:30:36.901556 140107197974336 spec.py:349] Evaluating on the test split.
I0205 19:30:38.500160 140107197974336 submission_runner.py:408] Time since start: 17856.40s, 	Step: 34724, 	{'train/accuracy': 0.5770702958106995, 'train/loss': 1.8023401498794556, 'validation/accuracy': 0.5371999740600586, 'validation/loss': 1.9966063499450684, 'validation/num_examples': 50000, 'test/accuracy': 0.41260001063346863, 'test/loss': 2.6572988033294678, 'test/num_examples': 10000, 'score': 16000.444960832596, 'total_duration': 17856.395292282104, 'accumulated_submission_time': 16000.444960832596, 'accumulated_eval_time': 1852.8152103424072, 'accumulated_logging_time': 1.1534223556518555}
I0205 19:30:38.521860 139946414638848 logging_writer.py:48] [34724] accumulated_eval_time=1852.815210, accumulated_logging_time=1.153422, accumulated_submission_time=16000.444961, global_step=34724, preemption_count=0, score=16000.444961, test/accuracy=0.412600, test/loss=2.657299, test/num_examples=10000, total_duration=17856.395292, train/accuracy=0.577070, train/loss=1.802340, validation/accuracy=0.537200, validation/loss=1.996606, validation/num_examples=50000
I0205 19:31:09.837086 139946397853440 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.6067551374435425, loss=2.970841646194458
I0205 19:31:55.836421 139946414638848 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.378129243850708, loss=3.927546501159668
I0205 19:32:42.644397 139946397853440 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.4250150918960571, loss=4.492855072021484
I0205 19:33:28.886242 139946414638848 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.5720916986465454, loss=2.922973871231079
I0205 19:34:15.230405 139946397853440 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.6211870908737183, loss=2.899808168411255
I0205 19:35:01.936381 139946414638848 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.48793625831604, loss=3.400228977203369
I0205 19:35:48.245274 139946397853440 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.5287370681762695, loss=3.2831242084503174
I0205 19:36:34.720657 139946414638848 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.3159414529800415, loss=3.991978168487549
I0205 19:37:21.492451 139946397853440 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.6138428449630737, loss=4.457924842834473
I0205 19:37:38.502902 140107197974336 spec.py:321] Evaluating on the training split.
I0205 19:37:49.514756 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 19:38:25.787110 140107197974336 spec.py:349] Evaluating on the test split.
I0205 19:38:27.388094 140107197974336 submission_runner.py:408] Time since start: 18325.28s, 	Step: 35638, 	{'train/accuracy': 0.5990234017372131, 'train/loss': 1.7254287004470825, 'validation/accuracy': 0.5380600094795227, 'validation/loss': 2.003264904022217, 'validation/num_examples': 50000, 'test/accuracy': 0.4256000220775604, 'test/loss': 2.619293212890625, 'test/num_examples': 10000, 'score': 16420.363413095474, 'total_duration': 18325.28321290016, 'accumulated_submission_time': 16420.363413095474, 'accumulated_eval_time': 1901.7003903388977, 'accumulated_logging_time': 1.184575080871582}
I0205 19:38:27.414443 139946414638848 logging_writer.py:48] [35638] accumulated_eval_time=1901.700390, accumulated_logging_time=1.184575, accumulated_submission_time=16420.363413, global_step=35638, preemption_count=0, score=16420.363413, test/accuracy=0.425600, test/loss=2.619293, test/num_examples=10000, total_duration=18325.283213, train/accuracy=0.599023, train/loss=1.725429, validation/accuracy=0.538060, validation/loss=2.003265, validation/num_examples=50000
I0205 19:38:52.162142 139946397853440 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.8457053899765015, loss=2.838571071624756
I0205 19:39:38.193221 139946414638848 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.6471880674362183, loss=3.0021533966064453
I0205 19:40:24.435285 139946397853440 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.3580225706100464, loss=4.107165336608887
I0205 19:41:10.602489 139946414638848 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.823518991470337, loss=2.8936471939086914
I0205 19:41:57.400988 139946397853440 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.389782428741455, loss=5.404323101043701
I0205 19:42:43.993639 139946414638848 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.571492314338684, loss=2.979948043823242
I0205 19:43:30.298811 139946397853440 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.9214448928833008, loss=2.7061452865600586
I0205 19:44:16.732607 139946414638848 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.564998984336853, loss=2.9009339809417725
I0205 19:45:03.444795 139946397853440 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.3701633214950562, loss=3.729506015777588
I0205 19:45:27.743588 140107197974336 spec.py:321] Evaluating on the training split.
I0205 19:45:38.857432 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 19:46:15.352264 140107197974336 spec.py:349] Evaluating on the test split.
I0205 19:46:16.954525 140107197974336 submission_runner.py:408] Time since start: 18794.85s, 	Step: 36554, 	{'train/accuracy': 0.575488269329071, 'train/loss': 1.8254637718200684, 'validation/accuracy': 0.5359799861907959, 'validation/loss': 2.010288715362549, 'validation/num_examples': 50000, 'test/accuracy': 0.4204000234603882, 'test/loss': 2.6431541442871094, 'test/num_examples': 10000, 'score': 16840.63138628006, 'total_duration': 18794.849648475647, 'accumulated_submission_time': 16840.63138628006, 'accumulated_eval_time': 1950.9113097190857, 'accumulated_logging_time': 1.2209351062774658}
I0205 19:46:16.980146 139946414638848 logging_writer.py:48] [36554] accumulated_eval_time=1950.911310, accumulated_logging_time=1.220935, accumulated_submission_time=16840.631386, global_step=36554, preemption_count=0, score=16840.631386, test/accuracy=0.420400, test/loss=2.643154, test/num_examples=10000, total_duration=18794.849648, train/accuracy=0.575488, train/loss=1.825464, validation/accuracy=0.535980, validation/loss=2.010289, validation/num_examples=50000
I0205 19:46:35.432422 139946397853440 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.6312907934188843, loss=2.8130457401275635
I0205 19:47:20.352624 139946414638848 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.4951343536376953, loss=3.261167287826538
I0205 19:48:06.935373 139946397853440 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.731194019317627, loss=2.9665257930755615
I0205 19:48:53.188739 139946414638848 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.7462965250015259, loss=2.844564437866211
I0205 19:49:39.616699 139946397853440 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.4703232049942017, loss=5.02836275100708
I0205 19:50:25.837978 139946414638848 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.2438184022903442, loss=4.687185287475586
I0205 19:51:12.226978 139946397853440 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.66935396194458, loss=3.1900241374969482
I0205 19:51:58.870643 139946414638848 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.661720871925354, loss=2.7439329624176025
I0205 19:52:45.583528 139946397853440 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.7838069200515747, loss=2.8507819175720215
I0205 19:53:16.996159 140107197974336 spec.py:321] Evaluating on the training split.
I0205 19:53:27.771296 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 19:53:57.903777 140107197974336 spec.py:349] Evaluating on the test split.
I0205 19:53:59.510010 140107197974336 submission_runner.py:408] Time since start: 19257.41s, 	Step: 37469, 	{'train/accuracy': 0.5883203148841858, 'train/loss': 1.7518789768218994, 'validation/accuracy': 0.5458799600601196, 'validation/loss': 1.9552130699157715, 'validation/num_examples': 50000, 'test/accuracy': 0.43230003118515015, 'test/loss': 2.6027326583862305, 'test/num_examples': 10000, 'score': 17260.585029363632, 'total_duration': 19257.405125379562, 'accumulated_submission_time': 17260.585029363632, 'accumulated_eval_time': 1993.4251432418823, 'accumulated_logging_time': 1.2563939094543457}
I0205 19:53:59.539471 139946414638848 logging_writer.py:48] [37469] accumulated_eval_time=1993.425143, accumulated_logging_time=1.256394, accumulated_submission_time=17260.585029, global_step=37469, preemption_count=0, score=17260.585029, test/accuracy=0.432300, test/loss=2.602733, test/num_examples=10000, total_duration=19257.405125, train/accuracy=0.588320, train/loss=1.751879, validation/accuracy=0.545880, validation/loss=1.955213, validation/num_examples=50000
I0205 19:54:12.127378 139946397853440 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.5435312986373901, loss=3.5355703830718994
I0205 19:54:56.061670 139946414638848 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.9169894456863403, loss=2.7336320877075195
I0205 19:55:42.072985 139946397853440 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.6152299642562866, loss=2.8612418174743652
I0205 19:56:28.512493 139946414638848 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.520128846168518, loss=5.109019756317139
I0205 19:57:15.174388 139946397853440 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.5485560894012451, loss=2.7920644283294678
I0205 19:58:01.927860 139946414638848 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.7501983642578125, loss=2.765183210372925
I0205 19:58:48.542104 139946397853440 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.8835084438323975, loss=2.726174831390381
I0205 19:59:34.951221 139946414638848 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.695738434791565, loss=2.758155584335327
I0205 20:00:21.440826 139946397853440 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.2528002262115479, loss=4.871035099029541
I0205 20:00:59.856968 140107197974336 spec.py:321] Evaluating on the training split.
I0205 20:01:10.997635 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 20:01:46.385424 140107197974336 spec.py:349] Evaluating on the test split.
I0205 20:01:47.990176 140107197974336 submission_runner.py:408] Time since start: 19725.89s, 	Step: 38384, 	{'train/accuracy': 0.6019335985183716, 'train/loss': 1.7069507837295532, 'validation/accuracy': 0.5504199862480164, 'validation/loss': 1.9425902366638184, 'validation/num_examples': 50000, 'test/accuracy': 0.43460002541542053, 'test/loss': 2.583150625228882, 'test/num_examples': 10000, 'score': 17680.8397500515, 'total_duration': 19725.885306596756, 'accumulated_submission_time': 17680.8397500515, 'accumulated_eval_time': 2041.5583720207214, 'accumulated_logging_time': 1.2973315715789795}
I0205 20:01:48.015973 139946414638848 logging_writer.py:48] [38384] accumulated_eval_time=2041.558372, accumulated_logging_time=1.297332, accumulated_submission_time=17680.839750, global_step=38384, preemption_count=0, score=17680.839750, test/accuracy=0.434600, test/loss=2.583151, test/num_examples=10000, total_duration=19725.885307, train/accuracy=0.601934, train/loss=1.706951, validation/accuracy=0.550420, validation/loss=1.942590, validation/num_examples=50000
I0205 20:01:54.686643 139946397853440 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.6331613063812256, loss=2.9649658203125
I0205 20:02:37.277971 139946414638848 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.5900804996490479, loss=2.78631329536438
I0205 20:03:23.728368 139946397853440 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.7675855159759521, loss=3.1119353771209717
I0205 20:04:10.187342 139946414638848 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.636318564414978, loss=3.318770408630371
I0205 20:04:56.450505 139946397853440 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.7377713918685913, loss=2.833486557006836
I0205 20:05:42.798140 139946414638848 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.3542872667312622, loss=5.316987991333008
I0205 20:06:29.087031 139946397853440 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.236663579940796, loss=4.749709606170654
I0205 20:07:15.589830 139946414638848 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.6161118745803833, loss=2.989025115966797
I0205 20:08:01.995919 139946397853440 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.589105486869812, loss=3.345925807952881
I0205 20:08:48.358777 139946414638848 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.7149157524108887, loss=2.751647710800171
I0205 20:08:48.375586 140107197974336 spec.py:321] Evaluating on the training split.
I0205 20:08:59.413776 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 20:09:33.037735 140107197974336 spec.py:349] Evaluating on the test split.
I0205 20:09:34.632617 140107197974336 submission_runner.py:408] Time since start: 20192.53s, 	Step: 39301, 	{'train/accuracy': 0.5848632454872131, 'train/loss': 1.7843482494354248, 'validation/accuracy': 0.545199990272522, 'validation/loss': 1.9751474857330322, 'validation/num_examples': 50000, 'test/accuracy': 0.4333000183105469, 'test/loss': 2.616333484649658, 'test/num_examples': 10000, 'score': 18101.136819839478, 'total_duration': 20192.52774953842, 'accumulated_submission_time': 18101.136819839478, 'accumulated_eval_time': 2087.8153867721558, 'accumulated_logging_time': 1.333068609237671}
I0205 20:09:34.654709 139946397853440 logging_writer.py:48] [39301] accumulated_eval_time=2087.815387, accumulated_logging_time=1.333069, accumulated_submission_time=18101.136820, global_step=39301, preemption_count=0, score=18101.136820, test/accuracy=0.433300, test/loss=2.616333, test/num_examples=10000, total_duration=20192.527750, train/accuracy=0.584863, train/loss=1.784348, validation/accuracy=0.545200, validation/loss=1.975147, validation/num_examples=50000
I0205 20:10:16.264941 139946414638848 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.3128939867019653, loss=3.729062080383301
I0205 20:11:02.726456 139946397853440 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.72384774684906, loss=2.875927209854126
I0205 20:11:48.983277 139946414638848 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.8951338529586792, loss=2.646263360977173
I0205 20:12:35.253364 139946397853440 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.7848471403121948, loss=2.692246913909912
I0205 20:13:22.008353 139946414638848 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.838935375213623, loss=3.521831750869751
I0205 20:14:08.134831 139946397853440 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.7217719554901123, loss=2.7082252502441406
I0205 20:14:54.715123 139946414638848 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.5965768098831177, loss=3.0792970657348633
I0205 20:15:40.964412 139946397853440 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.717332124710083, loss=2.762258768081665
I0205 20:16:27.185960 139946414638848 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.6085435152053833, loss=2.748067617416382
I0205 20:16:34.711692 140107197974336 spec.py:321] Evaluating on the training split.
I0205 20:16:45.485425 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 20:17:21.571733 140107197974336 spec.py:349] Evaluating on the test split.
I0205 20:17:23.168703 140107197974336 submission_runner.py:408] Time since start: 20661.06s, 	Step: 40218, 	{'train/accuracy': 0.5871874690055847, 'train/loss': 1.7634190320968628, 'validation/accuracy': 0.5512199997901917, 'validation/loss': 1.9427509307861328, 'validation/num_examples': 50000, 'test/accuracy': 0.4305000305175781, 'test/loss': 2.6063010692596436, 'test/num_examples': 10000, 'score': 18521.130932092667, 'total_duration': 20661.063827991486, 'accumulated_submission_time': 18521.130932092667, 'accumulated_eval_time': 2136.2723891735077, 'accumulated_logging_time': 1.364790678024292}
I0205 20:17:23.194364 139946397853440 logging_writer.py:48] [40218] accumulated_eval_time=2136.272389, accumulated_logging_time=1.364791, accumulated_submission_time=18521.130932, global_step=40218, preemption_count=0, score=18521.130932, test/accuracy=0.430500, test/loss=2.606301, test/num_examples=10000, total_duration=20661.063828, train/accuracy=0.587187, train/loss=1.763419, validation/accuracy=0.551220, validation/loss=1.942751, validation/num_examples=50000
I0205 20:17:56.950726 139946414638848 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.6792830228805542, loss=2.92456316947937
I0205 20:18:43.020782 139946397853440 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.8457529544830322, loss=2.671191930770874
I0205 20:19:29.675842 139946414638848 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.4908701181411743, loss=3.4361486434936523
I0205 20:20:16.150991 139946397853440 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.13200843334198, loss=5.322176933288574
I0205 20:21:02.496254 139946414638848 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.8405723571777344, loss=2.814328670501709
I0205 20:21:48.786369 139946397853440 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.3634916543960571, loss=5.088119029998779
I0205 20:22:35.269353 139946414638848 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.6526278257369995, loss=3.5004055500030518
I0205 20:23:21.453881 139946397853440 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.4696722030639648, loss=3.9850168228149414
I0205 20:24:07.758221 139946414638848 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.5721120834350586, loss=5.146905899047852
I0205 20:24:23.224167 140107197974336 spec.py:321] Evaluating on the training split.
I0205 20:24:34.156616 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 20:25:10.482955 140107197974336 spec.py:349] Evaluating on the test split.
I0205 20:25:12.084106 140107197974336 submission_runner.py:408] Time since start: 21129.98s, 	Step: 41135, 	{'train/accuracy': 0.597851574420929, 'train/loss': 1.7207162380218506, 'validation/accuracy': 0.5503999590873718, 'validation/loss': 1.93899405002594, 'validation/num_examples': 50000, 'test/accuracy': 0.4401000142097473, 'test/loss': 2.5727827548980713, 'test/num_examples': 10000, 'score': 18941.098199129105, 'total_duration': 21129.979234695435, 'accumulated_submission_time': 18941.098199129105, 'accumulated_eval_time': 2185.132310628891, 'accumulated_logging_time': 1.3997371196746826}
I0205 20:25:12.106830 139946397853440 logging_writer.py:48] [41135] accumulated_eval_time=2185.132311, accumulated_logging_time=1.399737, accumulated_submission_time=18941.098199, global_step=41135, preemption_count=0, score=18941.098199, test/accuracy=0.440100, test/loss=2.572783, test/num_examples=10000, total_duration=21129.979235, train/accuracy=0.597852, train/loss=1.720716, validation/accuracy=0.550400, validation/loss=1.938994, validation/num_examples=50000
I0205 20:25:38.025578 139946414638848 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.8140531778335571, loss=2.8382201194763184
I0205 20:26:23.945739 139946397853440 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.6291632652282715, loss=2.7613024711608887
I0205 20:27:10.094798 139946414638848 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.4451655149459839, loss=5.38951301574707
I0205 20:27:56.736500 139946397853440 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.4588088989257812, loss=3.7289323806762695
I0205 20:28:43.294636 139946414638848 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.774114727973938, loss=2.7127013206481934
I0205 20:29:30.114837 139946397853440 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.3829385042190552, loss=4.008975028991699
I0205 20:30:16.682105 139946414638848 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.9289641380310059, loss=2.850236415863037
I0205 20:31:03.218149 139946397853440 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.7000608444213867, loss=3.003910541534424
I0205 20:31:49.504631 139946414638848 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.6062003374099731, loss=2.788447856903076
I0205 20:32:12.573915 140107197974336 spec.py:321] Evaluating on the training split.
I0205 20:32:23.448923 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 20:32:56.321164 140107197974336 spec.py:349] Evaluating on the test split.
I0205 20:32:57.922808 140107197974336 submission_runner.py:408] Time since start: 21595.82s, 	Step: 42051, 	{'train/accuracy': 0.5936523079872131, 'train/loss': 1.7368556261062622, 'validation/accuracy': 0.5519399642944336, 'validation/loss': 1.9425498247146606, 'validation/num_examples': 50000, 'test/accuracy': 0.43540000915527344, 'test/loss': 2.5808470249176025, 'test/num_examples': 10000, 'score': 19361.50342464447, 'total_duration': 21595.81792283058, 'accumulated_submission_time': 19361.50342464447, 'accumulated_eval_time': 2230.4811882972717, 'accumulated_logging_time': 1.4327614307403564}
I0205 20:32:57.954780 139946397853440 logging_writer.py:48] [42051] accumulated_eval_time=2230.481188, accumulated_logging_time=1.432761, accumulated_submission_time=19361.503425, global_step=42051, preemption_count=0, score=19361.503425, test/accuracy=0.435400, test/loss=2.580847, test/num_examples=10000, total_duration=21595.817923, train/accuracy=0.593652, train/loss=1.736856, validation/accuracy=0.551940, validation/loss=1.942550, validation/num_examples=50000
I0205 20:33:17.595630 139946414638848 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.5317565202713013, loss=3.686572551727295
I0205 20:34:02.797639 139946397853440 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.7618919610977173, loss=2.882736921310425
I0205 20:34:49.575435 139946414638848 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.4368054866790771, loss=4.365969181060791
I0205 20:35:36.217266 139946397853440 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.5212596654891968, loss=5.098484992980957
I0205 20:36:22.759038 139946414638848 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.4218254089355469, loss=4.381099700927734
I0205 20:37:09.155943 139946397853440 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.6049410104751587, loss=2.761672258377075
I0205 20:37:55.339617 139946414638848 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.4986165761947632, loss=3.18654203414917
I0205 20:38:41.810631 139946397853440 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.6214323043823242, loss=2.736783504486084
I0205 20:39:28.329631 139946414638848 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.267592430114746, loss=5.089407920837402
I0205 20:39:58.217269 140107197974336 spec.py:321] Evaluating on the training split.
I0205 20:40:09.452325 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 20:40:44.791072 140107197974336 spec.py:349] Evaluating on the test split.
I0205 20:40:46.380323 140107197974336 submission_runner.py:408] Time since start: 22064.28s, 	Step: 42966, 	{'train/accuracy': 0.5927538871765137, 'train/loss': 1.7270139455795288, 'validation/accuracy': 0.5557399988174438, 'validation/loss': 1.9057121276855469, 'validation/num_examples': 50000, 'test/accuracy': 0.4368000328540802, 'test/loss': 2.5653188228607178, 'test/num_examples': 10000, 'score': 19781.703052520752, 'total_duration': 22064.275451898575, 'accumulated_submission_time': 19781.703052520752, 'accumulated_eval_time': 2278.644249200821, 'accumulated_logging_time': 1.4757418632507324}
I0205 20:40:46.410774 139946397853440 logging_writer.py:48] [42966] accumulated_eval_time=2278.644249, accumulated_logging_time=1.475742, accumulated_submission_time=19781.703053, global_step=42966, preemption_count=0, score=19781.703053, test/accuracy=0.436800, test/loss=2.565319, test/num_examples=10000, total_duration=22064.275452, train/accuracy=0.592754, train/loss=1.727014, validation/accuracy=0.555740, validation/loss=1.905712, validation/num_examples=50000
I0205 20:41:00.164659 139946414638848 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.7755773067474365, loss=5.382685661315918
I0205 20:41:44.238102 139946397853440 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.5133544206619263, loss=3.530064344406128
I0205 20:42:30.697190 139946414638848 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.7856954336166382, loss=3.596795082092285
I0205 20:43:17.197464 139946397853440 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.4257301092147827, loss=4.633340835571289
I0205 20:44:03.482238 139946414638848 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.5525579452514648, loss=2.6957342624664307
I0205 20:44:50.044912 139946397853440 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.6819565296173096, loss=2.6416101455688477
I0205 20:45:36.730300 139946414638848 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.613976001739502, loss=2.658261299133301
I0205 20:46:23.154452 139946397853440 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.7145490646362305, loss=2.725330114364624
I0205 20:47:09.355533 139946414638848 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.724766492843628, loss=2.6202609539031982
I0205 20:47:46.460296 140107197974336 spec.py:321] Evaluating on the training split.
I0205 20:47:57.411900 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 20:48:33.052852 140107197974336 spec.py:349] Evaluating on the test split.
I0205 20:48:34.642481 140107197974336 submission_runner.py:408] Time since start: 22532.54s, 	Step: 43882, 	{'train/accuracy': 0.6057812571525574, 'train/loss': 1.6457350254058838, 'validation/accuracy': 0.5611599683761597, 'validation/loss': 1.8691457509994507, 'validation/num_examples': 50000, 'test/accuracy': 0.4455000162124634, 'test/loss': 2.518744707107544, 'test/num_examples': 10000, 'score': 20201.69106078148, 'total_duration': 22532.53761100769, 'accumulated_submission_time': 20201.69106078148, 'accumulated_eval_time': 2326.826430082321, 'accumulated_logging_time': 1.5156099796295166}
I0205 20:48:34.669280 139946397853440 logging_writer.py:48] [43882] accumulated_eval_time=2326.826430, accumulated_logging_time=1.515610, accumulated_submission_time=20201.691061, global_step=43882, preemption_count=0, score=20201.691061, test/accuracy=0.445500, test/loss=2.518745, test/num_examples=10000, total_duration=22532.537611, train/accuracy=0.605781, train/loss=1.645735, validation/accuracy=0.561160, validation/loss=1.869146, validation/num_examples=50000
I0205 20:48:42.135684 139946414638848 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.2710803747177124, loss=5.187148571014404
I0205 20:49:24.896562 139946397853440 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.7523581981658936, loss=2.7776589393615723
I0205 20:50:11.305949 139946414638848 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.5967057943344116, loss=3.028193950653076
I0205 20:50:57.759816 139946397853440 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.8351407051086426, loss=2.6651124954223633
I0205 20:51:44.267953 139946414638848 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.785271406173706, loss=2.8181159496307373
I0205 20:52:30.897339 139946397853440 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.812425136566162, loss=2.8524599075317383
I0205 20:53:17.547227 139946414638848 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.4888159036636353, loss=4.577803134918213
I0205 20:54:04.048731 139946397853440 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.4660348892211914, loss=4.281443119049072
I0205 20:54:50.659954 139946414638848 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.9322513341903687, loss=2.776918649673462
I0205 20:55:35.317700 140107197974336 spec.py:321] Evaluating on the training split.
I0205 20:55:46.455685 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 20:56:20.495120 140107197974336 spec.py:349] Evaluating on the test split.
I0205 20:56:22.093872 140107197974336 submission_runner.py:408] Time since start: 22999.99s, 	Step: 44796, 	{'train/accuracy': 0.6296679377555847, 'train/loss': 1.5705506801605225, 'validation/accuracy': 0.5589399933815002, 'validation/loss': 1.8887207508087158, 'validation/num_examples': 50000, 'test/accuracy': 0.44510000944137573, 'test/loss': 2.5309979915618896, 'test/num_examples': 10000, 'score': 20622.27797460556, 'total_duration': 22999.989002227783, 'accumulated_submission_time': 20622.27797460556, 'accumulated_eval_time': 2373.602608203888, 'accumulated_logging_time': 1.5518851280212402}
I0205 20:56:22.120091 139946397853440 logging_writer.py:48] [44796] accumulated_eval_time=2373.602608, accumulated_logging_time=1.551885, accumulated_submission_time=20622.277975, global_step=44796, preemption_count=0, score=20622.277975, test/accuracy=0.445100, test/loss=2.530998, test/num_examples=10000, total_duration=22999.989002, train/accuracy=0.629668, train/loss=1.570551, validation/accuracy=0.558940, validation/loss=1.888721, validation/num_examples=50000
I0205 20:56:24.087274 139946414638848 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.6009845733642578, loss=3.0615804195404053
I0205 20:57:06.508203 139946397853440 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.8234343528747559, loss=2.812814235687256
I0205 20:57:52.979600 139946414638848 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.9586695432662964, loss=2.8638434410095215
I0205 20:58:39.795872 139946397853440 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.5278687477111816, loss=4.539490699768066
I0205 20:59:26.325988 139946414638848 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.6397578716278076, loss=2.8090267181396484
I0205 21:00:13.012445 139946397853440 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.7807996273040771, loss=2.749119281768799
I0205 21:00:59.053437 139946414638848 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.4605357646942139, loss=3.743086099624634
I0205 21:01:45.647490 139946397853440 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.3716936111450195, loss=3.9703316688537598
I0205 21:02:32.029205 139946414638848 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.0238499641418457, loss=2.5668954849243164
I0205 21:03:18.383822 139946397853440 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.7162330150604248, loss=2.7778587341308594
I0205 21:03:22.250154 140107197974336 spec.py:321] Evaluating on the training split.
I0205 21:03:33.077550 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 21:04:07.442656 140107197974336 spec.py:349] Evaluating on the test split.
I0205 21:04:09.045403 140107197974336 submission_runner.py:408] Time since start: 23466.94s, 	Step: 45710, 	{'train/accuracy': 0.6051172018051147, 'train/loss': 1.6690466403961182, 'validation/accuracy': 0.5628399848937988, 'validation/loss': 1.8617463111877441, 'validation/num_examples': 50000, 'test/accuracy': 0.4449000358581543, 'test/loss': 2.515469789505005, 'test/num_examples': 10000, 'score': 21042.347343206406, 'total_duration': 23466.94051671028, 'accumulated_submission_time': 21042.347343206406, 'accumulated_eval_time': 2420.3978378772736, 'accumulated_logging_time': 1.5879981517791748}
I0205 21:04:09.071560 139946414638848 logging_writer.py:48] [45710] accumulated_eval_time=2420.397838, accumulated_logging_time=1.587998, accumulated_submission_time=21042.347343, global_step=45710, preemption_count=0, score=21042.347343, test/accuracy=0.444900, test/loss=2.515470, test/num_examples=10000, total_duration=23466.940517, train/accuracy=0.605117, train/loss=1.669047, validation/accuracy=0.562840, validation/loss=1.861746, validation/num_examples=50000
I0205 21:04:46.579107 139946397853440 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.7167267799377441, loss=2.709411144256592
I0205 21:05:33.076023 139946414638848 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.669108510017395, loss=5.159275054931641
I0205 21:06:19.886311 139946397853440 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.4738045930862427, loss=2.9543747901916504
I0205 21:07:06.710882 139946414638848 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.5957564115524292, loss=2.7752678394317627
I0205 21:07:53.085784 139946397853440 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.4103599786758423, loss=3.625237464904785
I0205 21:08:39.895589 139946414638848 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.6577885150909424, loss=2.983757734298706
I0205 21:09:26.334275 139946397853440 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.738590955734253, loss=2.9706270694732666
I0205 21:10:13.022709 139946414638848 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.6670845746994019, loss=2.7094039916992188
I0205 21:10:59.402251 139946397853440 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.952000379562378, loss=2.72182559967041
I0205 21:11:09.428098 140107197974336 spec.py:321] Evaluating on the training split.
I0205 21:11:20.194583 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 21:11:55.919708 140107197974336 spec.py:349] Evaluating on the test split.
I0205 21:11:57.518709 140107197974336 submission_runner.py:408] Time since start: 23935.41s, 	Step: 46623, 	{'train/accuracy': 0.6066796779632568, 'train/loss': 1.6673763990402222, 'validation/accuracy': 0.5652799606323242, 'validation/loss': 1.8668668270111084, 'validation/num_examples': 50000, 'test/accuracy': 0.44530001282691956, 'test/loss': 2.5359339714050293, 'test/num_examples': 10000, 'score': 21462.64211010933, 'total_duration': 23935.413821458817, 'accumulated_submission_time': 21462.64211010933, 'accumulated_eval_time': 2468.4884293079376, 'accumulated_logging_time': 1.6240994930267334}
I0205 21:11:57.546397 139946414638848 logging_writer.py:48] [46623] accumulated_eval_time=2468.488429, accumulated_logging_time=1.624099, accumulated_submission_time=21462.642110, global_step=46623, preemption_count=0, score=21462.642110, test/accuracy=0.445300, test/loss=2.535934, test/num_examples=10000, total_duration=23935.413821, train/accuracy=0.606680, train/loss=1.667376, validation/accuracy=0.565280, validation/loss=1.866867, validation/num_examples=50000
I0205 21:12:29.021524 139946397853440 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.5902202129364014, loss=3.603449583053589
I0205 21:13:15.042752 139946414638848 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.7186284065246582, loss=2.784529209136963
I0205 21:14:01.604656 139946397853440 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.5480421781539917, loss=3.498210906982422
I0205 21:14:48.213065 139946414638848 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.398105263710022, loss=4.571332931518555
I0205 21:15:34.635987 139946397853440 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.5559172630310059, loss=3.8359272480010986
I0205 21:16:21.061184 139946414638848 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.6474581956863403, loss=2.6175038814544678
I0205 21:17:07.443612 139946397853440 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.5774390697479248, loss=2.7954952716827393
I0205 21:17:53.982957 139946414638848 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.6637061834335327, loss=2.5871798992156982
I0205 21:18:40.385262 139946397853440 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.846554160118103, loss=2.771733045578003
I0205 21:18:57.808673 140107197974336 spec.py:321] Evaluating on the training split.
I0205 21:19:08.820844 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 21:19:42.764825 140107197974336 spec.py:349] Evaluating on the test split.
I0205 21:19:44.362428 140107197974336 submission_runner.py:408] Time since start: 24402.26s, 	Step: 47539, 	{'train/accuracy': 0.626269519329071, 'train/loss': 1.5778939723968506, 'validation/accuracy': 0.5616999864578247, 'validation/loss': 1.8711748123168945, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.509688377380371, 'test/num_examples': 10000, 'score': 21882.842244386673, 'total_duration': 24402.25755739212, 'accumulated_submission_time': 21882.842244386673, 'accumulated_eval_time': 2515.0421693325043, 'accumulated_logging_time': 1.66135573387146}
I0205 21:19:44.384671 139946414638848 logging_writer.py:48] [47539] accumulated_eval_time=2515.042169, accumulated_logging_time=1.661356, accumulated_submission_time=21882.842244, global_step=47539, preemption_count=0, score=21882.842244, test/accuracy=0.447900, test/loss=2.509688, test/num_examples=10000, total_duration=24402.257557, train/accuracy=0.626270, train/loss=1.577894, validation/accuracy=0.561700, validation/loss=1.871175, validation/num_examples=50000
I0205 21:20:08.744445 139946397853440 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.5222233533859253, loss=3.3237850666046143
I0205 21:20:54.351592 139946414638848 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.5571411848068237, loss=3.79298996925354
I0205 21:21:40.844288 139946397853440 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.7473036050796509, loss=2.8360238075256348
I0205 21:22:27.563559 139946414638848 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.760446310043335, loss=2.742973804473877
I0205 21:23:13.904260 139946397853440 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.5496625900268555, loss=3.158451557159424
I0205 21:24:00.380100 139946414638848 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.8553189039230347, loss=2.719559669494629
I0205 21:24:46.959461 139946397853440 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.8029084205627441, loss=2.8886988162994385
I0205 21:25:33.527844 139946414638848 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.893922209739685, loss=2.6651365756988525
I0205 21:26:20.126262 139946397853440 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.746930718421936, loss=2.6888928413391113
I0205 21:26:44.722558 140107197974336 spec.py:321] Evaluating on the training split.
I0205 21:26:55.689058 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 21:27:29.632258 140107197974336 spec.py:349] Evaluating on the test split.
I0205 21:27:31.226539 140107197974336 submission_runner.py:408] Time since start: 24869.12s, 	Step: 48455, 	{'train/accuracy': 0.6093554496765137, 'train/loss': 1.643090009689331, 'validation/accuracy': 0.5676599740982056, 'validation/loss': 1.8277660608291626, 'validation/num_examples': 50000, 'test/accuracy': 0.45430001616477966, 'test/loss': 2.494823455810547, 'test/num_examples': 10000, 'score': 22303.119409799576, 'total_duration': 24869.121667146683, 'accumulated_submission_time': 22303.119409799576, 'accumulated_eval_time': 2561.5461843013763, 'accumulated_logging_time': 1.692392110824585}
I0205 21:27:31.251683 139946414638848 logging_writer.py:48] [48455] accumulated_eval_time=2561.546184, accumulated_logging_time=1.692392, accumulated_submission_time=22303.119410, global_step=48455, preemption_count=0, score=22303.119410, test/accuracy=0.454300, test/loss=2.494823, test/num_examples=10000, total_duration=24869.121667, train/accuracy=0.609355, train/loss=1.643090, validation/accuracy=0.567660, validation/loss=1.827766, validation/num_examples=50000
I0205 21:27:49.322654 139946397853440 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.3513952493667603, loss=4.856320858001709
I0205 21:28:34.179394 139946414638848 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.5080052614212036, loss=3.514441967010498
I0205 21:29:20.712980 139946397853440 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.4875391721725464, loss=4.665172100067139
I0205 21:30:07.263726 139946414638848 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.9661065340042114, loss=2.6943798065185547
I0205 21:30:53.482405 139946397853440 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.7378636598587036, loss=2.722384214401245
I0205 21:31:39.938486 139946414638848 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.2499192953109741, loss=5.348491668701172
I0205 21:32:26.298403 139946397853440 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.8272879123687744, loss=2.703522205352783
I0205 21:33:12.627392 139946414638848 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.767782211303711, loss=2.736940860748291
I0205 21:33:59.206720 139946397853440 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.4241373538970947, loss=4.005353927612305
I0205 21:34:31.494715 140107197974336 spec.py:321] Evaluating on the training split.
I0205 21:34:43.217964 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 21:35:17.902222 140107197974336 spec.py:349] Evaluating on the test split.
I0205 21:35:19.503864 140107197974336 submission_runner.py:408] Time since start: 25337.40s, 	Step: 49371, 	{'train/accuracy': 0.611132800579071, 'train/loss': 1.6391677856445312, 'validation/accuracy': 0.5672399997711182, 'validation/loss': 1.8397936820983887, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.4801652431488037, 'test/num_examples': 10000, 'score': 22723.300478219986, 'total_duration': 25337.39897251129, 'accumulated_submission_time': 22723.300478219986, 'accumulated_eval_time': 2609.5553126335144, 'accumulated_logging_time': 1.7268388271331787}
I0205 21:35:19.536435 139946414638848 logging_writer.py:48] [49371] accumulated_eval_time=2609.555313, accumulated_logging_time=1.726839, accumulated_submission_time=22723.300478, global_step=49371, preemption_count=0, score=22723.300478, test/accuracy=0.447300, test/loss=2.480165, test/num_examples=10000, total_duration=25337.398973, train/accuracy=0.611133, train/loss=1.639168, validation/accuracy=0.567240, validation/loss=1.839794, validation/num_examples=50000
I0205 21:35:31.318932 139946397853440 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.688200831413269, loss=2.8099169731140137
I0205 21:36:14.666339 139946414638848 logging_writer.py:48] [49500] global_step=49500, grad_norm=2.095679759979248, loss=2.729098320007324
I0205 21:37:01.003186 139946397853440 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.7158875465393066, loss=2.888737678527832
I0205 21:37:47.597806 139946414638848 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.8574986457824707, loss=2.6258528232574463
I0205 21:38:34.263193 139946397853440 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.7683212757110596, loss=2.8094122409820557
I0205 21:39:20.924409 139946414638848 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.5258269309997559, loss=5.263038158416748
I0205 21:40:07.324859 139946397853440 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.3090633153915405, loss=5.152163505554199
I0205 21:40:53.653542 139946414638848 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.8391475677490234, loss=2.650238037109375
I0205 21:41:40.070804 139946397853440 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.7136164903640747, loss=2.7651894092559814
I0205 21:42:19.797164 140107197974336 spec.py:321] Evaluating on the training split.
I0205 21:42:30.665679 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 21:43:04.999587 140107197974336 spec.py:349] Evaluating on the test split.
I0205 21:43:06.586591 140107197974336 submission_runner.py:408] Time since start: 25804.48s, 	Step: 50287, 	{'train/accuracy': 0.6187109351158142, 'train/loss': 1.5916036367416382, 'validation/accuracy': 0.5705599784851074, 'validation/loss': 1.8403029441833496, 'validation/num_examples': 50000, 'test/accuracy': 0.4564000070095062, 'test/loss': 2.4771194458007812, 'test/num_examples': 10000, 'score': 23143.497784137726, 'total_duration': 25804.48171377182, 'accumulated_submission_time': 23143.497784137726, 'accumulated_eval_time': 2656.3447353839874, 'accumulated_logging_time': 1.7708237171173096}
I0205 21:43:06.613783 139946414638848 logging_writer.py:48] [50287] accumulated_eval_time=2656.344735, accumulated_logging_time=1.770824, accumulated_submission_time=23143.497784, global_step=50287, preemption_count=0, score=23143.497784, test/accuracy=0.456400, test/loss=2.477119, test/num_examples=10000, total_duration=25804.481714, train/accuracy=0.618711, train/loss=1.591604, validation/accuracy=0.570560, validation/loss=1.840303, validation/num_examples=50000
I0205 21:43:12.113781 139946397853440 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.8474184274673462, loss=2.612197160720825
I0205 21:43:54.575939 139946414638848 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.296659231185913, loss=4.867032527923584
I0205 21:44:41.412787 139946397853440 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.7895393371582031, loss=2.6505141258239746
I0205 21:45:27.976372 139946414638848 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.8098396062850952, loss=2.6613576412200928
I0205 21:46:14.578605 139946397853440 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.4941291809082031, loss=3.5234568119049072
I0205 21:47:00.972666 139946414638848 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.3836696147918701, loss=4.178344249725342
I0205 21:47:47.429034 139946397853440 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.7360308170318604, loss=2.8319711685180664
I0205 21:48:33.888760 139946414638848 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.9616070985794067, loss=2.7507083415985107
I0205 21:49:20.325091 139946397853440 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.435884952545166, loss=5.396437168121338
I0205 21:50:06.731091 139946414638848 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7393300533294678, loss=2.636563539505005
I0205 21:50:06.743128 140107197974336 spec.py:321] Evaluating on the training split.
I0205 21:50:17.551946 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 21:50:52.059785 140107197974336 spec.py:349] Evaluating on the test split.
I0205 21:50:53.655835 140107197974336 submission_runner.py:408] Time since start: 26271.55s, 	Step: 51201, 	{'train/accuracy': 0.6166015267372131, 'train/loss': 1.6008018255233765, 'validation/accuracy': 0.5757399797439575, 'validation/loss': 1.7892749309539795, 'validation/num_examples': 50000, 'test/accuracy': 0.457800030708313, 'test/loss': 2.448241949081421, 'test/num_examples': 10000, 'score': 23563.565361738205, 'total_duration': 26271.550952911377, 'accumulated_submission_time': 23563.565361738205, 'accumulated_eval_time': 2703.2574343681335, 'accumulated_logging_time': 1.8078598976135254}
I0205 21:50:53.686934 139946397853440 logging_writer.py:48] [51201] accumulated_eval_time=2703.257434, accumulated_logging_time=1.807860, accumulated_submission_time=23563.565362, global_step=51201, preemption_count=0, score=23563.565362, test/accuracy=0.457800, test/loss=2.448242, test/num_examples=10000, total_duration=26271.550953, train/accuracy=0.616602, train/loss=1.600802, validation/accuracy=0.575740, validation/loss=1.789275, validation/num_examples=50000
I0205 21:51:35.469344 139946414638848 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.853500247001648, loss=2.8933260440826416
I0205 21:52:21.472100 139946397853440 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.847838044166565, loss=2.62376070022583
I0205 21:53:08.119458 139946414638848 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.7699743509292603, loss=2.810106039047241
I0205 21:53:54.481184 139946397853440 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.5786482095718384, loss=3.1641359329223633
I0205 21:54:41.100203 139946414638848 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.81857430934906, loss=2.8478572368621826
I0205 21:55:27.766687 139946397853440 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.4505558013916016, loss=5.37299108505249
I0205 21:56:14.537315 139946414638848 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.5324496030807495, loss=3.140376567840576
I0205 21:57:00.842189 139946397853440 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.7733211517333984, loss=3.47092866897583
I0205 21:57:47.260471 139946414638848 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.9738713502883911, loss=2.5616633892059326
I0205 21:57:53.974598 140107197974336 spec.py:321] Evaluating on the training split.
I0205 21:58:04.769491 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 21:58:42.122498 140107197974336 spec.py:349] Evaluating on the test split.
I0205 21:58:43.716783 140107197974336 submission_runner.py:408] Time since start: 26741.61s, 	Step: 52116, 	{'train/accuracy': 0.6089453101158142, 'train/loss': 1.667687177658081, 'validation/accuracy': 0.5686599612236023, 'validation/loss': 1.8564475774765015, 'validation/num_examples': 50000, 'test/accuracy': 0.4490000307559967, 'test/loss': 2.522512912750244, 'test/num_examples': 10000, 'score': 23983.790986299515, 'total_duration': 26741.611914396286, 'accumulated_submission_time': 23983.790986299515, 'accumulated_eval_time': 2752.9996156692505, 'accumulated_logging_time': 1.848759651184082}
I0205 21:58:43.740154 139946397853440 logging_writer.py:48] [52116] accumulated_eval_time=2752.999616, accumulated_logging_time=1.848760, accumulated_submission_time=23983.790986, global_step=52116, preemption_count=0, score=23983.790986, test/accuracy=0.449000, test/loss=2.522513, test/num_examples=10000, total_duration=26741.611914, train/accuracy=0.608945, train/loss=1.667687, validation/accuracy=0.568660, validation/loss=1.856448, validation/num_examples=50000
I0205 21:59:18.640813 139946414638848 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.7998743057250977, loss=2.6694750785827637
I0205 22:00:04.879762 139946397853440 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.5195285081863403, loss=3.8120765686035156
I0205 22:00:51.293785 139946414638848 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.8609784841537476, loss=2.672640562057495
I0205 22:01:37.865362 139946397853440 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.744856834411621, loss=2.600084066390991
I0205 22:02:24.240643 139946414638848 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.3196630477905273, loss=5.182705879211426
I0205 22:03:10.455897 139946397853440 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.4229240417480469, loss=5.15010404586792
I0205 22:03:56.988463 139946414638848 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.9885903596878052, loss=2.7577695846557617
I0205 22:04:43.255288 139946397853440 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.4785873889923096, loss=3.2932181358337402
I0205 22:05:29.679730 139946414638848 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.8988616466522217, loss=2.68157696723938
I0205 22:05:43.743186 140107197974336 spec.py:321] Evaluating on the training split.
I0205 22:05:54.451040 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 22:06:27.116044 140107197974336 spec.py:349] Evaluating on the test split.
I0205 22:06:28.712325 140107197974336 submission_runner.py:408] Time since start: 27206.61s, 	Step: 53032, 	{'train/accuracy': 0.6204296946525574, 'train/loss': 1.5840575695037842, 'validation/accuracy': 0.5726199746131897, 'validation/loss': 1.8054448366165161, 'validation/num_examples': 50000, 'test/accuracy': 0.45650002360343933, 'test/loss': 2.4617958068847656, 'test/num_examples': 10000, 'score': 24403.7320561409, 'total_duration': 27206.607456207275, 'accumulated_submission_time': 24403.7320561409, 'accumulated_eval_time': 2797.968763113022, 'accumulated_logging_time': 1.8818917274475098}
I0205 22:06:28.736997 139946397853440 logging_writer.py:48] [53032] accumulated_eval_time=2797.968763, accumulated_logging_time=1.881892, accumulated_submission_time=24403.732056, global_step=53032, preemption_count=0, score=24403.732056, test/accuracy=0.456500, test/loss=2.461796, test/num_examples=10000, total_duration=27206.607456, train/accuracy=0.620430, train/loss=1.584058, validation/accuracy=0.572620, validation/loss=1.805445, validation/num_examples=50000
I0205 22:06:56.263919 139946414638848 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.795283317565918, loss=2.719790458679199
I0205 22:07:42.241030 139946397853440 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.807026743888855, loss=2.7208895683288574
I0205 22:08:28.705121 139946414638848 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.7959848642349243, loss=2.6244211196899414
I0205 22:09:15.126862 139946397853440 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.882851481437683, loss=2.6228952407836914
I0205 22:10:01.435477 139946414638848 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.7590937614440918, loss=2.5712482929229736
I0205 22:10:48.144481 139946397853440 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.7256346940994263, loss=2.7291159629821777
I0205 22:11:34.574826 139946414638848 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.7486343383789062, loss=2.6147713661193848
I0205 22:12:20.858059 139946397853440 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.8394886255264282, loss=2.6777701377868652
I0205 22:13:06.978611 139946414638848 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.729815125465393, loss=2.717190742492676
I0205 22:13:28.887881 140107197974336 spec.py:321] Evaluating on the training split.
I0205 22:13:39.716585 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 22:14:16.788806 140107197974336 spec.py:349] Evaluating on the test split.
I0205 22:14:18.384288 140107197974336 submission_runner.py:408] Time since start: 27676.28s, 	Step: 53949, 	{'train/accuracy': 0.6197265386581421, 'train/loss': 1.6068668365478516, 'validation/accuracy': 0.5780799984931946, 'validation/loss': 1.7958285808563232, 'validation/num_examples': 50000, 'test/accuracy': 0.45580002665519714, 'test/loss': 2.460911512374878, 'test/num_examples': 10000, 'score': 24823.819769382477, 'total_duration': 27676.279418468475, 'accumulated_submission_time': 24823.819769382477, 'accumulated_eval_time': 2847.4651761054993, 'accumulated_logging_time': 1.9169471263885498}
I0205 22:14:18.408873 139946397853440 logging_writer.py:48] [53949] accumulated_eval_time=2847.465176, accumulated_logging_time=1.916947, accumulated_submission_time=24823.819769, global_step=53949, preemption_count=0, score=24823.819769, test/accuracy=0.455800, test/loss=2.460912, test/num_examples=10000, total_duration=27676.279418, train/accuracy=0.619727, train/loss=1.606867, validation/accuracy=0.578080, validation/loss=1.795829, validation/num_examples=50000
I0205 22:14:38.855676 139946414638848 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.7325323820114136, loss=3.6239116191864014
I0205 22:15:23.901978 139946397853440 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.3545387983322144, loss=3.7349328994750977
I0205 22:16:10.715604 139946414638848 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8246263265609741, loss=2.739830493927002
I0205 22:16:57.064259 139946397853440 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.5918728113174438, loss=3.076853036880493
I0205 22:17:43.793517 139946414638848 logging_writer.py:48] [54400] global_step=54400, grad_norm=2.0267562866210938, loss=2.8666763305664062
I0205 22:18:30.204478 139946397853440 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.9078001976013184, loss=2.4751198291778564
I0205 22:19:16.621669 139946414638848 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.622452735900879, loss=3.5490894317626953
I0205 22:20:03.296921 139946397853440 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.8342373371124268, loss=2.6463429927825928
I0205 22:20:49.800113 139946414638848 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.913448452949524, loss=2.6804111003875732
I0205 22:21:18.445660 140107197974336 spec.py:321] Evaluating on the training split.
I0205 22:21:29.220255 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 22:22:00.164788 140107197974336 spec.py:349] Evaluating on the test split.
I0205 22:22:01.762877 140107197974336 submission_runner.py:408] Time since start: 28139.66s, 	Step: 54863, 	{'train/accuracy': 0.6193749904632568, 'train/loss': 1.6178147792816162, 'validation/accuracy': 0.5789799690246582, 'validation/loss': 1.8013064861297607, 'validation/num_examples': 50000, 'test/accuracy': 0.4589000344276428, 'test/loss': 2.458005428314209, 'test/num_examples': 10000, 'score': 25243.794989824295, 'total_duration': 28139.657998085022, 'accumulated_submission_time': 25243.794989824295, 'accumulated_eval_time': 2890.782431602478, 'accumulated_logging_time': 1.9509387016296387}
I0205 22:22:01.792487 139946397853440 logging_writer.py:48] [54863] accumulated_eval_time=2890.782432, accumulated_logging_time=1.950939, accumulated_submission_time=25243.794990, global_step=54863, preemption_count=0, score=25243.794990, test/accuracy=0.458900, test/loss=2.458005, test/num_examples=10000, total_duration=28139.657998, train/accuracy=0.619375, train/loss=1.617815, validation/accuracy=0.578980, validation/loss=1.801306, validation/num_examples=50000
I0205 22:22:16.742790 139946414638848 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.4604182243347168, loss=3.8123607635498047
I0205 22:23:00.929757 139946397853440 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.0142126083374023, loss=2.6566343307495117
I0205 22:23:47.786989 139946414638848 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.6239904165267944, loss=5.0794196128845215
I0205 22:24:34.846383 139946397853440 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.5894625186920166, loss=3.374433994293213
I0205 22:25:21.447880 139946414638848 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.3124468326568604, loss=5.083461761474609
I0205 22:26:08.213780 139946397853440 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8904002904891968, loss=2.6332688331604004
I0205 22:26:54.927346 139946414638848 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.571000576019287, loss=3.7896947860717773
I0205 22:27:41.558768 139946397853440 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.609270691871643, loss=3.127345323562622
I0205 22:28:28.265732 139946414638848 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.6740564107894897, loss=2.710787534713745
I0205 22:29:01.878946 140107197974336 spec.py:321] Evaluating on the training split.
I0205 22:29:12.866653 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 22:29:47.217362 140107197974336 spec.py:349] Evaluating on the test split.
I0205 22:29:48.825094 140107197974336 submission_runner.py:408] Time since start: 28606.72s, 	Step: 55774, 	{'train/accuracy': 0.6284765601158142, 'train/loss': 1.5806477069854736, 'validation/accuracy': 0.5812999606132507, 'validation/loss': 1.804103970527649, 'validation/num_examples': 50000, 'test/accuracy': 0.46310001611709595, 'test/loss': 2.472367525100708, 'test/num_examples': 10000, 'score': 25663.818971157074, 'total_duration': 28606.72021007538, 'accumulated_submission_time': 25663.818971157074, 'accumulated_eval_time': 2937.7285718917847, 'accumulated_logging_time': 1.9914829730987549}
I0205 22:29:48.856587 139946397853440 logging_writer.py:48] [55774] accumulated_eval_time=2937.728572, accumulated_logging_time=1.991483, accumulated_submission_time=25663.818971, global_step=55774, preemption_count=0, score=25663.818971, test/accuracy=0.463100, test/loss=2.472368, test/num_examples=10000, total_duration=28606.720210, train/accuracy=0.628477, train/loss=1.580648, validation/accuracy=0.581300, validation/loss=1.804104, validation/num_examples=50000
I0205 22:29:59.467639 139946414638848 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.6832566261291504, loss=2.5533111095428467
I0205 22:30:43.058957 139946397853440 logging_writer.py:48] [55900] global_step=55900, grad_norm=2.018787145614624, loss=2.6691837310791016
I0205 22:31:29.542180 139946414638848 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.725112795829773, loss=3.47316312789917
I0205 22:32:16.126244 139946397853440 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.7133089303970337, loss=2.94929838180542
I0205 22:33:02.829312 139946414638848 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.5325407981872559, loss=3.058732509613037
I0205 22:33:49.319051 139946397853440 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.9864212274551392, loss=2.7485859394073486
I0205 22:34:35.850620 139946414638848 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.7811359167099, loss=2.5461277961730957
I0205 22:35:22.513453 139946397853440 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.971579909324646, loss=2.5988996028900146
I0205 22:36:09.332403 139946414638848 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.4750505685806274, loss=4.602859020233154
I0205 22:36:49.269707 140107197974336 spec.py:321] Evaluating on the training split.
I0205 22:37:00.167440 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 22:37:35.692362 140107197974336 spec.py:349] Evaluating on the test split.
I0205 22:37:37.291193 140107197974336 submission_runner.py:408] Time since start: 29075.19s, 	Step: 56688, 	{'train/accuracy': 0.6417773365974426, 'train/loss': 1.5043836832046509, 'validation/accuracy': 0.578719973564148, 'validation/loss': 1.7915959358215332, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.447606086730957, 'test/num_examples': 10000, 'score': 26084.16915845871, 'total_duration': 29075.186302661896, 'accumulated_submission_time': 26084.16915845871, 'accumulated_eval_time': 2985.750026702881, 'accumulated_logging_time': 2.033883810043335}
I0205 22:37:37.320141 139946397853440 logging_writer.py:48] [56688] accumulated_eval_time=2985.750027, accumulated_logging_time=2.033884, accumulated_submission_time=26084.169158, global_step=56688, preemption_count=0, score=26084.169158, test/accuracy=0.460600, test/loss=2.447606, test/num_examples=10000, total_duration=29075.186303, train/accuracy=0.641777, train/loss=1.504384, validation/accuracy=0.578720, validation/loss=1.791596, validation/num_examples=50000
I0205 22:37:42.841234 139946414638848 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.7440344095230103, loss=3.104348659515381
I0205 22:38:25.584409 139946397853440 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.725592017173767, loss=3.25138521194458
I0205 22:39:12.352814 139946414638848 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.5674941539764404, loss=5.076536178588867
I0205 22:39:58.982239 139946397853440 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.9355623722076416, loss=2.6760964393615723
I0205 22:40:45.180597 139946414638848 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.4099860191345215, loss=4.0637102127075195
I0205 22:41:32.067666 139946397853440 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.8868873119354248, loss=2.56550931930542
I0205 22:42:18.504739 139946414638848 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.7563703060150146, loss=2.785205125808716
I0205 22:43:05.262339 139946397853440 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.7860630750656128, loss=2.539775848388672
I0205 22:43:51.461496 139946414638848 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8929665088653564, loss=2.6968319416046143
I0205 22:44:37.785770 139946397853440 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.724519968032837, loss=5.018198490142822
I0205 22:44:37.801126 140107197974336 spec.py:321] Evaluating on the training split.
I0205 22:44:48.787243 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 22:45:23.461297 140107197974336 spec.py:349] Evaluating on the test split.
I0205 22:45:25.052822 140107197974336 submission_runner.py:408] Time since start: 29542.95s, 	Step: 57601, 	{'train/accuracy': 0.6244726181030273, 'train/loss': 1.5794726610183716, 'validation/accuracy': 0.5821599960327148, 'validation/loss': 1.773970603942871, 'validation/num_examples': 50000, 'test/accuracy': 0.4669000208377838, 'test/loss': 2.421680450439453, 'test/num_examples': 10000, 'score': 26504.193346261978, 'total_duration': 29542.947952270508, 'accumulated_submission_time': 26504.193346261978, 'accumulated_eval_time': 3033.001730442047, 'accumulated_logging_time': 2.4678242206573486}
I0205 22:45:25.077586 139946414638848 logging_writer.py:48] [57601] accumulated_eval_time=3033.001730, accumulated_logging_time=2.467824, accumulated_submission_time=26504.193346, global_step=57601, preemption_count=0, score=26504.193346, test/accuracy=0.466900, test/loss=2.421680, test/num_examples=10000, total_duration=29542.947952, train/accuracy=0.624473, train/loss=1.579473, validation/accuracy=0.582160, validation/loss=1.773971, validation/num_examples=50000
I0205 22:46:06.524117 139946397853440 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.7704036235809326, loss=2.6509413719177246
I0205 22:46:52.482983 139946414638848 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.9451647996902466, loss=2.6574831008911133
I0205 22:47:39.068555 139946397853440 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.4149627685546875, loss=4.356398582458496
I0205 22:48:25.429532 139946414638848 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.4561964273452759, loss=4.185471057891846
I0205 22:49:11.944683 139946397853440 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.9307371377944946, loss=2.7555336952209473
I0205 22:49:58.069719 139946414638848 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.9135091304779053, loss=2.659104585647583
I0205 22:50:44.748821 139946397853440 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.8893396854400635, loss=2.5279808044433594
I0205 22:51:31.031525 139946414638848 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.5051079988479614, loss=4.045401573181152
I0205 22:52:17.465502 139946397853440 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.8145371675491333, loss=2.6105337142944336
I0205 22:52:25.387819 140107197974336 spec.py:321] Evaluating on the training split.
I0205 22:52:36.451277 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 22:53:11.687161 140107197974336 spec.py:349] Evaluating on the test split.
I0205 22:53:13.290860 140107197974336 submission_runner.py:408] Time since start: 30011.19s, 	Step: 58519, 	{'train/accuracy': 0.6330859065055847, 'train/loss': 1.5414679050445557, 'validation/accuracy': 0.5845400094985962, 'validation/loss': 1.7692793607711792, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.407954692840576, 'test/num_examples': 10000, 'score': 26924.44057393074, 'total_duration': 30011.185983896255, 'accumulated_submission_time': 26924.44057393074, 'accumulated_eval_time': 3080.904748916626, 'accumulated_logging_time': 2.5036418437957764}
I0205 22:53:13.318377 139946414638848 logging_writer.py:48] [58519] accumulated_eval_time=3080.904749, accumulated_logging_time=2.503642, accumulated_submission_time=26924.440574, global_step=58519, preemption_count=0, score=26924.440574, test/accuracy=0.469500, test/loss=2.407955, test/num_examples=10000, total_duration=30011.185984, train/accuracy=0.633086, train/loss=1.541468, validation/accuracy=0.584540, validation/loss=1.769279, validation/num_examples=50000
I0205 22:53:46.752043 139946397853440 logging_writer.py:48] [58600] global_step=58600, grad_norm=2.193401575088501, loss=2.713514566421509
I0205 22:54:33.272746 139946414638848 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.4970839023590088, loss=4.013092517852783
I0205 22:55:19.997619 139946397853440 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.521024227142334, loss=3.0265462398529053
I0205 22:56:06.506146 139946414638848 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.8104709386825562, loss=2.5581536293029785
I0205 22:56:53.040878 139946397853440 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.3744640350341797, loss=4.041094779968262
I0205 22:57:39.807973 139946414638848 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.4832327365875244, loss=4.630602836608887
I0205 22:58:26.366118 139946397853440 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.9594210386276245, loss=2.5082082748413086
I0205 22:59:12.958178 139946414638848 logging_writer.py:48] [59300] global_step=59300, grad_norm=2.032853364944458, loss=5.394003391265869
I0205 22:59:59.659451 139946397853440 logging_writer.py:48] [59400] global_step=59400, grad_norm=2.074359655380249, loss=2.450301170349121
I0205 23:00:13.369712 140107197974336 spec.py:321] Evaluating on the training split.
I0205 23:00:24.307009 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 23:00:59.766662 140107197974336 spec.py:349] Evaluating on the test split.
I0205 23:01:01.358332 140107197974336 submission_runner.py:408] Time since start: 30479.25s, 	Step: 59431, 	{'train/accuracy': 0.6523241996765137, 'train/loss': 1.4520971775054932, 'validation/accuracy': 0.5870400071144104, 'validation/loss': 1.7456282377243042, 'validation/num_examples': 50000, 'test/accuracy': 0.46730002760887146, 'test/loss': 2.3970510959625244, 'test/num_examples': 10000, 'score': 27344.42973256111, 'total_duration': 30479.253449440002, 'accumulated_submission_time': 27344.42973256111, 'accumulated_eval_time': 3128.893353700638, 'accumulated_logging_time': 2.5405941009521484}
I0205 23:01:01.394852 139946414638848 logging_writer.py:48] [59431] accumulated_eval_time=3128.893354, accumulated_logging_time=2.540594, accumulated_submission_time=27344.429733, global_step=59431, preemption_count=0, score=27344.429733, test/accuracy=0.467300, test/loss=2.397051, test/num_examples=10000, total_duration=30479.253449, train/accuracy=0.652324, train/loss=1.452097, validation/accuracy=0.587040, validation/loss=1.745628, validation/num_examples=50000
I0205 23:01:29.390490 139946397853440 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.6534942388534546, loss=2.610989809036255
I0205 23:02:15.551112 139946414638848 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.5999135971069336, loss=5.026327610015869
I0205 23:03:02.161344 139946397853440 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.0195462703704834, loss=2.5466856956481934
I0205 23:03:48.536604 139946414638848 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.5841730833053589, loss=3.1357829570770264
I0205 23:04:35.498293 139946397853440 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.5223326683044434, loss=4.109071254730225
I0205 23:05:21.954906 139946414638848 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.2104504108428955, loss=2.689943790435791
I0205 23:06:08.301199 139946397853440 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.811715841293335, loss=2.570221185684204
I0205 23:06:54.578480 139946414638848 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.0300402641296387, loss=2.5980560779571533
I0205 23:07:40.827823 139946397853440 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.921446681022644, loss=2.5330493450164795
I0205 23:08:01.441817 140107197974336 spec.py:321] Evaluating on the training split.
I0205 23:08:12.498584 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 23:08:48.077461 140107197974336 spec.py:349] Evaluating on the test split.
I0205 23:08:49.683921 140107197974336 submission_runner.py:408] Time since start: 30947.58s, 	Step: 60346, 	{'train/accuracy': 0.6209570169448853, 'train/loss': 1.5848476886749268, 'validation/accuracy': 0.58051997423172, 'validation/loss': 1.7770845890045166, 'validation/num_examples': 50000, 'test/accuracy': 0.4643000364303589, 'test/loss': 2.428161144256592, 'test/num_examples': 10000, 'score': 27764.40898680687, 'total_duration': 30947.579052448273, 'accumulated_submission_time': 27764.40898680687, 'accumulated_eval_time': 3177.135461807251, 'accumulated_logging_time': 2.5926928520202637}
I0205 23:08:49.710758 139946414638848 logging_writer.py:48] [60346] accumulated_eval_time=3177.135462, accumulated_logging_time=2.592693, accumulated_submission_time=27764.408987, global_step=60346, preemption_count=0, score=27764.408987, test/accuracy=0.464300, test/loss=2.428161, test/num_examples=10000, total_duration=30947.579052, train/accuracy=0.620957, train/loss=1.584848, validation/accuracy=0.580520, validation/loss=1.777085, validation/num_examples=50000
I0205 23:09:11.326768 139946397853440 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.4077942371368408, loss=4.393263339996338
I0205 23:09:56.756436 139946414638848 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.8189148902893066, loss=2.4508650302886963
I0205 23:10:43.619456 139946397853440 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.4570165872573853, loss=5.233295440673828
I0205 23:11:30.050148 139946414638848 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.383542776107788, loss=4.931421756744385
I0205 23:12:16.420869 139946397853440 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.5876967906951904, loss=4.094974517822266
I0205 23:13:02.932733 139946414638848 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.9211113452911377, loss=2.7136547565460205
I0205 23:13:49.192445 139946397853440 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.8461053371429443, loss=2.639122724533081
I0205 23:14:36.020549 139946414638848 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.8131003379821777, loss=2.4797065258026123
I0205 23:15:22.528473 139946397853440 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.8914800882339478, loss=2.5353498458862305
I0205 23:15:49.690270 140107197974336 spec.py:321] Evaluating on the training split.
I0205 23:16:00.682829 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 23:16:35.778245 140107197974336 spec.py:349] Evaluating on the test split.
I0205 23:16:37.380646 140107197974336 submission_runner.py:408] Time since start: 31415.28s, 	Step: 61260, 	{'train/accuracy': 0.6322460770606995, 'train/loss': 1.5737278461456299, 'validation/accuracy': 0.5851399898529053, 'validation/loss': 1.7762945890426636, 'validation/num_examples': 50000, 'test/accuracy': 0.4652000367641449, 'test/loss': 2.4296584129333496, 'test/num_examples': 10000, 'score': 28184.326326608658, 'total_duration': 31415.275759458542, 'accumulated_submission_time': 28184.326326608658, 'accumulated_eval_time': 3224.825823068619, 'accumulated_logging_time': 2.629995107650757}
I0205 23:16:37.411259 139946414638848 logging_writer.py:48] [61260] accumulated_eval_time=3224.825823, accumulated_logging_time=2.629995, accumulated_submission_time=28184.326327, global_step=61260, preemption_count=0, score=28184.326327, test/accuracy=0.465200, test/loss=2.429658, test/num_examples=10000, total_duration=31415.275759, train/accuracy=0.632246, train/loss=1.573728, validation/accuracy=0.585140, validation/loss=1.776295, validation/num_examples=50000
I0205 23:16:53.513470 139946397853440 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.7386353015899658, loss=2.473208427429199
I0205 23:17:37.978533 139946414638848 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.8590511083602905, loss=5.181353569030762
I0205 23:18:23.884052 139946397853440 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.51089608669281, loss=4.943620681762695
I0205 23:19:10.729601 139946414638848 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.102996587753296, loss=2.558443069458008
I0205 23:19:57.347324 139946397853440 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.5263817310333252, loss=5.061880111694336
I0205 23:20:43.904419 139946414638848 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.914159893989563, loss=2.9780707359313965
I0205 23:21:30.357212 139946397853440 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.5886324644088745, loss=4.018123149871826
I0205 23:22:16.901224 139946414638848 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.7340210676193237, loss=2.4458351135253906
I0205 23:23:03.568773 139946397853440 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.8017559051513672, loss=2.530243396759033
I0205 23:23:37.468226 140107197974336 spec.py:321] Evaluating on the training split.
I0205 23:23:48.407170 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 23:24:25.299746 140107197974336 spec.py:349] Evaluating on the test split.
I0205 23:24:26.902981 140107197974336 submission_runner.py:408] Time since start: 31884.80s, 	Step: 62175, 	{'train/accuracy': 0.6433984041213989, 'train/loss': 1.473775863647461, 'validation/accuracy': 0.5825200080871582, 'validation/loss': 1.7541874647140503, 'validation/num_examples': 50000, 'test/accuracy': 0.4636000096797943, 'test/loss': 2.4132707118988037, 'test/num_examples': 10000, 'score': 28604.319380521774, 'total_duration': 31884.798108816147, 'accumulated_submission_time': 28604.319380521774, 'accumulated_eval_time': 3274.260570049286, 'accumulated_logging_time': 2.6725854873657227}
I0205 23:24:26.929734 139946414638848 logging_writer.py:48] [62175] accumulated_eval_time=3274.260570, accumulated_logging_time=2.672585, accumulated_submission_time=28604.319381, global_step=62175, preemption_count=0, score=28604.319381, test/accuracy=0.463600, test/loss=2.413271, test/num_examples=10000, total_duration=31884.798109, train/accuracy=0.643398, train/loss=1.473776, validation/accuracy=0.582520, validation/loss=1.754187, validation/num_examples=50000
I0205 23:24:37.146397 139946397853440 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.5964475870132446, loss=3.1034255027770996
I0205 23:25:20.728717 139946414638848 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.4619983434677124, loss=4.185448169708252
I0205 23:26:07.353488 139946397853440 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.500930905342102, loss=3.220733642578125
I0205 23:26:53.469713 139946414638848 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.644716501235962, loss=3.683894395828247
I0205 23:27:39.750462 139946397853440 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.9432904720306396, loss=2.5022337436676025
I0205 23:28:26.229692 139946414638848 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.3558698892593384, loss=5.042980670928955
I0205 23:29:12.986495 139946397853440 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.6624703407287598, loss=3.103239059448242
I0205 23:29:59.499717 139946414638848 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.91586434841156, loss=2.5724966526031494
I0205 23:30:45.981218 139946397853440 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.2767844200134277, loss=2.6788041591644287
I0205 23:31:27.249817 140107197974336 spec.py:321] Evaluating on the training split.
I0205 23:31:37.872453 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 23:32:14.902172 140107197974336 spec.py:349] Evaluating on the test split.
I0205 23:32:16.503392 140107197974336 submission_runner.py:408] Time since start: 32354.40s, 	Step: 63090, 	{'train/accuracy': 0.6314452886581421, 'train/loss': 1.5430465936660767, 'validation/accuracy': 0.5873000025749207, 'validation/loss': 1.7412338256835938, 'validation/num_examples': 50000, 'test/accuracy': 0.4662000238895416, 'test/loss': 2.3992021083831787, 'test/num_examples': 10000, 'score': 29024.578023433685, 'total_duration': 32354.39852118492, 'accumulated_submission_time': 29024.578023433685, 'accumulated_eval_time': 3323.5141406059265, 'accumulated_logging_time': 2.7087838649749756}
I0205 23:32:16.533845 139946414638848 logging_writer.py:48] [63090] accumulated_eval_time=3323.514141, accumulated_logging_time=2.708784, accumulated_submission_time=29024.578023, global_step=63090, preemption_count=0, score=29024.578023, test/accuracy=0.466200, test/loss=2.399202, test/num_examples=10000, total_duration=32354.398521, train/accuracy=0.631445, train/loss=1.543047, validation/accuracy=0.587300, validation/loss=1.741234, validation/num_examples=50000
I0205 23:32:20.856812 139946397853440 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.712552547454834, loss=2.5614709854125977
I0205 23:33:03.200003 139946414638848 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.7799512147903442, loss=2.6070213317871094
I0205 23:33:49.501578 139946397853440 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.7995543479919434, loss=3.4235053062438965
I0205 23:34:36.514916 139946414638848 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.318062424659729, loss=5.042290210723877
I0205 23:35:22.933668 139946397853440 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.5012279748916626, loss=4.102814674377441
I0205 23:36:09.518886 139946414638848 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.807752013206482, loss=2.460904598236084
I0205 23:36:55.781649 139946397853440 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.5735645294189453, loss=3.7792816162109375
I0205 23:37:42.133177 139946414638848 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.9247158765792847, loss=2.4872498512268066
I0205 23:38:28.272528 139946397853440 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.3940057754516602, loss=4.406733512878418
I0205 23:39:14.482721 139946414638848 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.7379016876220703, loss=2.4305171966552734
I0205 23:39:16.506222 140107197974336 spec.py:321] Evaluating on the training split.
I0205 23:39:27.256852 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 23:39:58.248326 140107197974336 spec.py:349] Evaluating on the test split.
I0205 23:39:59.848330 140107197974336 submission_runner.py:408] Time since start: 32817.74s, 	Step: 64006, 	{'train/accuracy': 0.6298632621765137, 'train/loss': 1.5495365858078003, 'validation/accuracy': 0.5903800129890442, 'validation/loss': 1.7368825674057007, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.39388108253479, 'test/num_examples': 10000, 'score': 29444.48857831955, 'total_duration': 32817.74343061447, 'accumulated_submission_time': 29444.48857831955, 'accumulated_eval_time': 3366.8562116622925, 'accumulated_logging_time': 2.7485859394073486}
I0205 23:39:59.874279 139946397853440 logging_writer.py:48] [64006] accumulated_eval_time=3366.856212, accumulated_logging_time=2.748586, accumulated_submission_time=29444.488578, global_step=64006, preemption_count=0, score=29444.488578, test/accuracy=0.468700, test/loss=2.393881, test/num_examples=10000, total_duration=32817.743431, train/accuracy=0.629863, train/loss=1.549537, validation/accuracy=0.590380, validation/loss=1.736883, validation/num_examples=50000
I0205 23:40:39.477588 139946414638848 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.803217887878418, loss=2.8375587463378906
I0205 23:41:25.650267 139946397853440 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.8414360284805298, loss=2.429198741912842
I0205 23:42:12.437188 139946414638848 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.9669232368469238, loss=2.5874133110046387
I0205 23:42:58.550301 139946397853440 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.896769404411316, loss=2.489686965942383
I0205 23:43:45.260960 139946414638848 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.7681217193603516, loss=2.680968761444092
I0205 23:44:31.855894 139946397853440 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.8217147588729858, loss=2.531370162963867
I0205 23:45:18.464199 139946414638848 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.879237413406372, loss=2.544095277786255
I0205 23:46:04.942183 139946397853440 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.8998719453811646, loss=2.4804797172546387
I0205 23:46:51.334700 139946414638848 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.5558253526687622, loss=3.4705817699432373
I0205 23:47:00.289268 140107197974336 spec.py:321] Evaluating on the training split.
I0205 23:47:11.219765 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 23:47:46.026018 140107197974336 spec.py:349] Evaluating on the test split.
I0205 23:47:47.618466 140107197974336 submission_runner.py:408] Time since start: 33285.51s, 	Step: 64921, 	{'train/accuracy': 0.6470116972923279, 'train/loss': 1.4817765951156616, 'validation/accuracy': 0.5940399765968323, 'validation/loss': 1.731095790863037, 'validation/num_examples': 50000, 'test/accuracy': 0.4788000285625458, 'test/loss': 2.3833441734313965, 'test/num_examples': 10000, 'score': 29864.842218399048, 'total_duration': 33285.5135974884, 'accumulated_submission_time': 29864.842218399048, 'accumulated_eval_time': 3414.1854150295258, 'accumulated_logging_time': 2.78342866897583}
I0205 23:47:47.647217 139946397853440 logging_writer.py:48] [64921] accumulated_eval_time=3414.185415, accumulated_logging_time=2.783429, accumulated_submission_time=29864.842218, global_step=64921, preemption_count=0, score=29864.842218, test/accuracy=0.478800, test/loss=2.383344, test/num_examples=10000, total_duration=33285.513597, train/accuracy=0.647012, train/loss=1.481777, validation/accuracy=0.594040, validation/loss=1.731096, validation/num_examples=50000
I0205 23:48:20.228010 139946414638848 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.4855172634124756, loss=5.278850555419922
I0205 23:49:06.086559 139946397853440 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.523340106010437, loss=4.336675643920898
I0205 23:49:52.284122 139946414638848 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.6072252988815308, loss=4.499369144439697
I0205 23:50:38.686982 139946397853440 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.8769930601119995, loss=2.99611234664917
I0205 23:51:24.862549 139946414638848 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.163098096847534, loss=2.612969398498535
I0205 23:52:11.298192 139946397853440 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.0157365798950195, loss=2.6562705039978027
I0205 23:52:57.626368 139946414638848 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.7910010814666748, loss=3.0082778930664062
I0205 23:53:44.158021 139946397853440 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.832613468170166, loss=2.8202052116394043
I0205 23:54:30.786016 139946414638848 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.6456488370895386, loss=2.747189521789551
I0205 23:54:48.032404 140107197974336 spec.py:321] Evaluating on the training split.
I0205 23:54:58.659089 140107197974336 spec.py:333] Evaluating on the validation split.
I0205 23:55:32.268912 140107197974336 spec.py:349] Evaluating on the test split.
I0205 23:55:33.872138 140107197974336 submission_runner.py:408] Time since start: 33751.77s, 	Step: 65839, 	{'train/accuracy': 0.64111328125, 'train/loss': 1.4903844594955444, 'validation/accuracy': 0.5980600118637085, 'validation/loss': 1.6916265487670898, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.3636457920074463, 'test/num_examples': 10000, 'score': 30285.16464781761, 'total_duration': 33751.76726102829, 'accumulated_submission_time': 30285.16464781761, 'accumulated_eval_time': 3460.025162935257, 'accumulated_logging_time': 2.822338342666626}
I0205 23:55:33.903877 139946397853440 logging_writer.py:48] [65839] accumulated_eval_time=3460.025163, accumulated_logging_time=2.822338, accumulated_submission_time=30285.164648, global_step=65839, preemption_count=0, score=30285.164648, test/accuracy=0.476200, test/loss=2.363646, test/num_examples=10000, total_duration=33751.767261, train/accuracy=0.641113, train/loss=1.490384, validation/accuracy=0.598060, validation/loss=1.691627, validation/num_examples=50000
I0205 23:55:58.348664 139946414638848 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.8947463035583496, loss=2.5474653244018555
I0205 23:56:44.490613 139946397853440 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.8864648342132568, loss=2.693810224533081
I0205 23:57:31.043135 139946414638848 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.8000781536102295, loss=2.6301932334899902
I0205 23:58:17.730396 139946397853440 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.693841576576233, loss=3.789515256881714
I0205 23:59:04.196977 139946414638848 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.8421671390533447, loss=2.740931272506714
I0205 23:59:50.625042 139946397853440 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.5175085067749023, loss=4.406614303588867
I0206 00:00:37.418678 139946414638848 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.5593491792678833, loss=5.141563415527344
I0206 00:01:23.944398 139946397853440 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.0150725841522217, loss=2.6241822242736816
I0206 00:02:10.587803 139946414638848 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.7717652320861816, loss=5.09735631942749
I0206 00:02:33.964373 140107197974336 spec.py:321] Evaluating on the training split.
I0206 00:02:44.767383 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 00:03:22.472011 140107197974336 spec.py:349] Evaluating on the test split.
I0206 00:03:24.074215 140107197974336 submission_runner.py:408] Time since start: 34221.97s, 	Step: 66752, 	{'train/accuracy': 0.6405078172683716, 'train/loss': 1.486151099205017, 'validation/accuracy': 0.6005600094795227, 'validation/loss': 1.6860800981521606, 'validation/num_examples': 50000, 'test/accuracy': 0.4765000343322754, 'test/loss': 2.350003242492676, 'test/num_examples': 10000, 'score': 30705.16337299347, 'total_duration': 34221.96933102608, 'accumulated_submission_time': 30705.16337299347, 'accumulated_eval_time': 3510.1349868774414, 'accumulated_logging_time': 2.8638916015625}
I0206 00:03:24.109771 139946397853440 logging_writer.py:48] [66752] accumulated_eval_time=3510.134987, accumulated_logging_time=2.863892, accumulated_submission_time=30705.163373, global_step=66752, preemption_count=0, score=30705.163373, test/accuracy=0.476500, test/loss=2.350003, test/num_examples=10000, total_duration=34221.969331, train/accuracy=0.640508, train/loss=1.486151, validation/accuracy=0.600560, validation/loss=1.686080, validation/num_examples=50000
I0206 00:03:43.354773 139946414638848 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.7844130992889404, loss=2.4692306518554688
I0206 00:04:28.419244 139946397853440 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.6198136806488037, loss=2.9453530311584473
I0206 00:05:15.132205 139946414638848 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.6820402145385742, loss=2.992481231689453
I0206 00:06:01.302696 139946397853440 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.6681126356124878, loss=4.27098274230957
I0206 00:06:47.738638 139946414638848 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.4195561408996582, loss=5.250382900238037
I0206 00:07:34.080282 139946397853440 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.0218231678009033, loss=2.5355818271636963
I0206 00:08:20.665059 139946414638848 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.982336163520813, loss=2.518282175064087
I0206 00:09:07.233766 139946397853440 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.7644532918930054, loss=2.465404510498047
I0206 00:09:53.510220 139946414638848 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.3908963203430176, loss=3.859893560409546
I0206 00:10:24.325275 140107197974336 spec.py:321] Evaluating on the training split.
I0206 00:10:35.196090 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 00:11:10.864456 140107197974336 spec.py:349] Evaluating on the test split.
I0206 00:11:12.473295 140107197974336 submission_runner.py:408] Time since start: 34690.37s, 	Step: 67668, 	{'train/accuracy': 0.6430078148841858, 'train/loss': 1.4980931282043457, 'validation/accuracy': 0.5955199599266052, 'validation/loss': 1.7154990434646606, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.372490167617798, 'test/num_examples': 10000, 'score': 31125.316086292267, 'total_duration': 34690.3684053421, 'accumulated_submission_time': 31125.316086292267, 'accumulated_eval_time': 3558.2829871177673, 'accumulated_logging_time': 2.9099161624908447}
I0206 00:11:12.506378 139946397853440 logging_writer.py:48] [67668] accumulated_eval_time=3558.282987, accumulated_logging_time=2.909916, accumulated_submission_time=31125.316086, global_step=67668, preemption_count=0, score=31125.316086, test/accuracy=0.478000, test/loss=2.372490, test/num_examples=10000, total_duration=34690.368405, train/accuracy=0.643008, train/loss=1.498093, validation/accuracy=0.595520, validation/loss=1.715499, validation/num_examples=50000
I0206 00:11:25.468654 139946414638848 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.6225966215133667, loss=3.627048969268799
I0206 00:12:09.259312 139946397853440 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.449090600013733, loss=4.465179443359375
I0206 00:12:55.844611 139946414638848 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.071314811706543, loss=2.599945068359375
I0206 00:13:42.353026 139946397853440 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.684081792831421, loss=3.266944169998169
I0206 00:14:29.281730 139946414638848 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.8681118488311768, loss=2.442753314971924
I0206 00:15:15.894234 139946397853440 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.4803673028945923, loss=3.525979995727539
I0206 00:16:02.325469 139946414638848 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.8532904386520386, loss=2.4905338287353516
I0206 00:16:49.251368 139946397853440 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.0100080966949463, loss=2.6151580810546875
I0206 00:17:35.682754 139946414638848 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.7561014890670776, loss=2.8633744716644287
I0206 00:18:12.854551 140107197974336 spec.py:321] Evaluating on the training split.
I0206 00:18:24.014402 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 00:18:58.665412 140107197974336 spec.py:349] Evaluating on the test split.
I0206 00:19:00.264067 140107197974336 submission_runner.py:408] Time since start: 35158.16s, 	Step: 68581, 	{'train/accuracy': 0.662304699420929, 'train/loss': 1.3984365463256836, 'validation/accuracy': 0.599399983882904, 'validation/loss': 1.6794729232788086, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.3231451511383057, 'test/num_examples': 10000, 'score': 31545.59892988205, 'total_duration': 35158.15919518471, 'accumulated_submission_time': 31545.59892988205, 'accumulated_eval_time': 3605.6924924850464, 'accumulated_logging_time': 2.954616069793701}
I0206 00:19:00.291346 139946397853440 logging_writer.py:48] [68581] accumulated_eval_time=3605.692492, accumulated_logging_time=2.954616, accumulated_submission_time=31545.598930, global_step=68581, preemption_count=0, score=31545.598930, test/accuracy=0.485200, test/loss=2.323145, test/num_examples=10000, total_duration=35158.159195, train/accuracy=0.662305, train/loss=1.398437, validation/accuracy=0.599400, validation/loss=1.679473, validation/num_examples=50000
I0206 00:19:08.149420 139946414638848 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.7392829656600952, loss=3.131458282470703
I0206 00:19:51.311697 139946397853440 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.6839081048965454, loss=4.548764228820801
I0206 00:20:37.530765 139946414638848 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.9309419393539429, loss=2.860456943511963
I0206 00:21:23.906649 139946397853440 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.6464439630508423, loss=5.051244735717773
I0206 00:22:10.224886 139946414638848 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.122040271759033, loss=2.634385824203491
I0206 00:22:56.467781 139946397853440 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.7397791147232056, loss=3.700594186782837
I0206 00:23:42.999676 139946414638848 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.93077552318573, loss=2.5105180740356445
I0206 00:24:29.343884 139946397853440 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.0476107597351074, loss=2.520965576171875
I0206 00:25:15.785335 139946414638848 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.8502683639526367, loss=2.6263442039489746
I0206 00:26:00.661135 140107197974336 spec.py:321] Evaluating on the training split.
I0206 00:26:11.414165 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 00:26:47.562692 140107197974336 spec.py:349] Evaluating on the test split.
I0206 00:26:49.161858 140107197974336 submission_runner.py:408] Time since start: 35627.06s, 	Step: 69499, 	{'train/accuracy': 0.6392773389816284, 'train/loss': 1.4914463758468628, 'validation/accuracy': 0.6018999814987183, 'validation/loss': 1.6752866506576538, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.3373358249664307, 'test/num_examples': 10000, 'score': 31965.907709360123, 'total_duration': 35627.05697226524, 'accumulated_submission_time': 31965.907709360123, 'accumulated_eval_time': 3654.193194627762, 'accumulated_logging_time': 2.99135160446167}
I0206 00:26:49.194084 139946397853440 logging_writer.py:48] [69499] accumulated_eval_time=3654.193195, accumulated_logging_time=2.991352, accumulated_submission_time=31965.907709, global_step=69499, preemption_count=0, score=31965.907709, test/accuracy=0.485000, test/loss=2.337336, test/num_examples=10000, total_duration=35627.056972, train/accuracy=0.639277, train/loss=1.491446, validation/accuracy=0.601900, validation/loss=1.675287, validation/num_examples=50000
I0206 00:26:49.984115 139946414638848 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.962624430656433, loss=2.471559524536133
I0206 00:27:31.957027 139946397853440 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.7342230081558228, loss=5.071503639221191
I0206 00:28:18.290055 139946414638848 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.7505638599395752, loss=2.5528383255004883
I0206 00:29:04.827312 139946397853440 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.95249342918396, loss=2.3668527603149414
I0206 00:29:50.883263 139946414638848 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.6663343906402588, loss=2.652437448501587
I0206 00:30:37.657180 139946397853440 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.9528206586837769, loss=2.360001564025879
I0206 00:31:23.944010 139946414638848 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.7107627391815186, loss=2.812957286834717
I0206 00:32:10.259041 139946397853440 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.5777475833892822, loss=3.180863857269287
I0206 00:32:56.654019 139946414638848 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.5432451963424683, loss=4.705455780029297
I0206 00:33:43.093280 139946397853440 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.042102575302124, loss=2.4746670722961426
I0206 00:33:49.219875 140107197974336 spec.py:321] Evaluating on the training split.
I0206 00:34:00.296638 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 00:34:37.129920 140107197974336 spec.py:349] Evaluating on the test split.
I0206 00:34:38.723509 140107197974336 submission_runner.py:408] Time since start: 36096.62s, 	Step: 70415, 	{'train/accuracy': 0.647753894329071, 'train/loss': 1.4472213983535767, 'validation/accuracy': 0.5977199673652649, 'validation/loss': 1.679577112197876, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.3356761932373047, 'test/num_examples': 10000, 'score': 32385.870503902435, 'total_duration': 36096.61861395836, 'accumulated_submission_time': 32385.870503902435, 'accumulated_eval_time': 3703.6968002319336, 'accumulated_logging_time': 3.0350594520568848}
I0206 00:34:38.750644 139946414638848 logging_writer.py:48] [70415] accumulated_eval_time=3703.696800, accumulated_logging_time=3.035059, accumulated_submission_time=32385.870504, global_step=70415, preemption_count=0, score=32385.870504, test/accuracy=0.485000, test/loss=2.335676, test/num_examples=10000, total_duration=36096.618614, train/accuracy=0.647754, train/loss=1.447221, validation/accuracy=0.597720, validation/loss=1.679577, validation/num_examples=50000
I0206 00:35:14.075077 139946397853440 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.48704195022583, loss=5.0846662521362305
I0206 00:36:00.241858 139946414638848 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.778586745262146, loss=2.8925881385803223
I0206 00:36:47.081903 139946397853440 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.7471729516983032, loss=3.2607946395874023
I0206 00:37:33.334655 139946414638848 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.0718109607696533, loss=2.4728009700775146
I0206 00:38:19.782894 139946397853440 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.8577662706375122, loss=2.7357888221740723
I0206 00:39:06.164702 139946414638848 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.716320276260376, loss=3.3411571979522705
I0206 00:39:52.386034 139946397853440 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.587815761566162, loss=4.5521559715271
I0206 00:40:38.924828 139946414638848 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.073108196258545, loss=2.4820127487182617
I0206 00:41:25.330528 139946397853440 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.8769010305404663, loss=2.640415668487549
I0206 00:41:38.873695 140107197974336 spec.py:321] Evaluating on the training split.
I0206 00:41:49.863737 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 00:42:26.334939 140107197974336 spec.py:349] Evaluating on the test split.
I0206 00:42:27.935499 140107197974336 submission_runner.py:408] Time since start: 36565.83s, 	Step: 71331, 	{'train/accuracy': 0.6681054830551147, 'train/loss': 1.376840353012085, 'validation/accuracy': 0.5995399951934814, 'validation/loss': 1.685808539390564, 'validation/num_examples': 50000, 'test/accuracy': 0.47360002994537354, 'test/loss': 2.3584470748901367, 'test/num_examples': 10000, 'score': 32805.93040370941, 'total_duration': 36565.83061218262, 'accumulated_submission_time': 32805.93040370941, 'accumulated_eval_time': 3752.7585911750793, 'accumulated_logging_time': 3.072389841079712}
I0206 00:42:27.970254 139946414638848 logging_writer.py:48] [71331] accumulated_eval_time=3752.758591, accumulated_logging_time=3.072390, accumulated_submission_time=32805.930404, global_step=71331, preemption_count=0, score=32805.930404, test/accuracy=0.473600, test/loss=2.358447, test/num_examples=10000, total_duration=36565.830612, train/accuracy=0.668105, train/loss=1.376840, validation/accuracy=0.599540, validation/loss=1.685809, validation/num_examples=50000
I0206 00:42:55.724581 139946397853440 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.8931374549865723, loss=2.665764331817627
I0206 00:43:41.831654 139946414638848 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.7821240425109863, loss=2.600565195083618
I0206 00:44:28.545505 139946397853440 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.6973546743392944, loss=5.183948040008545
I0206 00:45:15.142806 139946414638848 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.826549768447876, loss=2.8009960651397705
I0206 00:46:01.645579 139946397853440 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.9022763967514038, loss=2.4188973903656006
I0206 00:46:48.078944 139946414638848 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.1739418506622314, loss=2.7210795879364014
I0206 00:47:34.485918 139946397853440 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.8142828941345215, loss=2.530094623565674
I0206 00:48:21.170106 139946414638848 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.9269639253616333, loss=2.4842071533203125
I0206 00:49:07.476348 139946397853440 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.001497507095337, loss=2.458892345428467
I0206 00:49:28.160493 140107197974336 spec.py:321] Evaluating on the training split.
I0206 00:49:39.604407 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 00:50:14.380929 140107197974336 spec.py:349] Evaluating on the test split.
I0206 00:50:15.983340 140107197974336 submission_runner.py:408] Time since start: 37033.88s, 	Step: 72246, 	{'train/accuracy': 0.6423632502555847, 'train/loss': 1.4910801649093628, 'validation/accuracy': 0.5996599793434143, 'validation/loss': 1.6887861490249634, 'validation/num_examples': 50000, 'test/accuracy': 0.47780001163482666, 'test/loss': 2.3342955112457275, 'test/num_examples': 10000, 'score': 33226.05816960335, 'total_duration': 37033.87846469879, 'accumulated_submission_time': 33226.05816960335, 'accumulated_eval_time': 3800.5814397335052, 'accumulated_logging_time': 3.1176953315734863}
I0206 00:50:16.016933 139946414638848 logging_writer.py:48] [72246] accumulated_eval_time=3800.581440, accumulated_logging_time=3.117695, accumulated_submission_time=33226.058170, global_step=72246, preemption_count=0, score=33226.058170, test/accuracy=0.477800, test/loss=2.334296, test/num_examples=10000, total_duration=37033.878465, train/accuracy=0.642363, train/loss=1.491080, validation/accuracy=0.599660, validation/loss=1.688786, validation/num_examples=50000
I0206 00:50:37.628695 139946397853440 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.9489670991897583, loss=3.127887725830078
I0206 00:51:23.328348 139946414638848 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.491837501525879, loss=4.615900039672852
I0206 00:52:10.135399 139946397853440 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.0071465969085693, loss=2.3532774448394775
I0206 00:52:56.618105 139946414638848 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.1572372913360596, loss=2.6450812816619873
I0206 00:53:42.849964 139946397853440 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.920033574104309, loss=2.389680862426758
I0206 00:54:29.417310 139946414638848 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.9907200336456299, loss=3.035508632659912
I0206 00:55:15.776329 139946397853440 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.8201872110366821, loss=2.288823366165161
I0206 00:56:02.031761 139946414638848 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.6972960233688354, loss=2.727844715118408
I0206 00:56:48.422224 139946397853440 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.5695464611053467, loss=3.511976480484009
I0206 00:57:16.284868 140107197974336 spec.py:321] Evaluating on the training split.
I0206 00:57:27.151460 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 00:58:00.334110 140107197974336 spec.py:349] Evaluating on the test split.
I0206 00:58:01.938241 140107197974336 submission_runner.py:408] Time since start: 37499.83s, 	Step: 73161, 	{'train/accuracy': 0.6495702862739563, 'train/loss': 1.450307846069336, 'validation/accuracy': 0.604200005531311, 'validation/loss': 1.6661208868026733, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.3093228340148926, 'test/num_examples': 10000, 'score': 33646.26445245743, 'total_duration': 37499.833355903625, 'accumulated_submission_time': 33646.26445245743, 'accumulated_eval_time': 3846.234811067581, 'accumulated_logging_time': 3.1611366271972656}
I0206 00:58:01.974716 139946414638848 logging_writer.py:48] [73161] accumulated_eval_time=3846.234811, accumulated_logging_time=3.161137, accumulated_submission_time=33646.264452, global_step=73161, preemption_count=0, score=33646.264452, test/accuracy=0.484500, test/loss=2.309323, test/num_examples=10000, total_duration=37499.833356, train/accuracy=0.649570, train/loss=1.450308, validation/accuracy=0.604200, validation/loss=1.666121, validation/num_examples=50000
I0206 00:58:17.690824 139946397853440 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.6028754711151123, loss=4.372714996337891
I0206 00:59:02.158100 139946414638848 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.5084081888198853, loss=5.049650192260742
I0206 00:59:48.787558 139946397853440 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.8858234882354736, loss=2.345822334289551
I0206 01:00:35.378089 139946414638848 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.0178780555725098, loss=2.3997979164123535
I0206 01:01:22.042687 139946397853440 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.2341909408569336, loss=2.7633309364318848
I0206 01:02:08.515037 139946414638848 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.778853178024292, loss=2.7472591400146484
I0206 01:02:55.149801 139946397853440 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.773268461227417, loss=2.254364490509033
I0206 01:03:41.494021 139946414638848 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.0005393028259277, loss=2.4057819843292236
I0206 01:04:28.044317 139946397853440 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.9358247518539429, loss=2.5025010108947754
I0206 01:05:02.231388 140107197974336 spec.py:321] Evaluating on the training split.
I0206 01:05:13.424833 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 01:05:52.167912 140107197974336 spec.py:349] Evaluating on the test split.
I0206 01:05:53.770105 140107197974336 submission_runner.py:408] Time since start: 37971.67s, 	Step: 74075, 	{'train/accuracy': 0.662890613079071, 'train/loss': 1.4327744245529175, 'validation/accuracy': 0.6055799722671509, 'validation/loss': 1.702540636062622, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.3480513095855713, 'test/num_examples': 10000, 'score': 34066.458422899246, 'total_duration': 37971.66522574425, 'accumulated_submission_time': 34066.458422899246, 'accumulated_eval_time': 3897.773527622223, 'accumulated_logging_time': 3.2078804969787598}
I0206 01:05:53.801755 139946414638848 logging_writer.py:48] [74075] accumulated_eval_time=3897.773528, accumulated_logging_time=3.207880, accumulated_submission_time=34066.458423, global_step=74075, preemption_count=0, score=34066.458423, test/accuracy=0.488000, test/loss=2.348051, test/num_examples=10000, total_duration=37971.665226, train/accuracy=0.662891, train/loss=1.432774, validation/accuracy=0.605580, validation/loss=1.702541, validation/num_examples=50000
I0206 01:06:04.002097 139946397853440 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.8558597564697266, loss=2.698441982269287
I0206 01:06:47.672923 139946414638848 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.606536865234375, loss=3.74094295501709
I0206 01:07:33.983597 139946397853440 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.932458758354187, loss=3.0733516216278076
I0206 01:08:20.763221 139946414638848 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.6603193283081055, loss=2.8991127014160156
I0206 01:09:07.415469 139946397853440 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.9881970882415771, loss=2.5528078079223633
I0206 01:09:53.801807 139946414638848 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.8030333518981934, loss=2.5739336013793945
I0206 01:10:40.475804 139946397853440 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.0306341648101807, loss=2.559839963912964
I0206 01:11:27.114606 139946414638848 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.776118278503418, loss=3.3868236541748047
I0206 01:12:13.769008 139946397853440 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.820936918258667, loss=2.385410785675049
I0206 01:12:53.961694 140107197974336 spec.py:321] Evaluating on the training split.
I0206 01:13:04.938380 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 01:13:36.971351 140107197974336 spec.py:349] Evaluating on the test split.
I0206 01:13:38.573876 140107197974336 submission_runner.py:408] Time since start: 38436.47s, 	Step: 74988, 	{'train/accuracy': 0.6476953029632568, 'train/loss': 1.45093834400177, 'validation/accuracy': 0.6082199811935425, 'validation/loss': 1.6413438320159912, 'validation/num_examples': 50000, 'test/accuracy': 0.48590001463890076, 'test/loss': 2.3130125999450684, 'test/num_examples': 10000, 'score': 34486.554055690765, 'total_duration': 38436.468970775604, 'accumulated_submission_time': 34486.554055690765, 'accumulated_eval_time': 3942.385674238205, 'accumulated_logging_time': 3.2509477138519287}
I0206 01:13:38.611340 139946414638848 logging_writer.py:48] [74988] accumulated_eval_time=3942.385674, accumulated_logging_time=3.250948, accumulated_submission_time=34486.554056, global_step=74988, preemption_count=0, score=34486.554056, test/accuracy=0.485900, test/loss=2.313013, test/num_examples=10000, total_duration=38436.468971, train/accuracy=0.647695, train/loss=1.450938, validation/accuracy=0.608220, validation/loss=1.641344, validation/num_examples=50000
I0206 01:13:43.717082 139946397853440 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.1134860515594482, loss=2.387727737426758
I0206 01:14:26.553600 139946414638848 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.7636526823043823, loss=3.014073371887207
I0206 01:15:12.711893 139946397853440 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.9779731035232544, loss=2.648259401321411
I0206 01:15:59.766362 139946414638848 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.6365995407104492, loss=3.429255485534668
I0206 01:16:46.438014 139946397853440 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.5398775339126587, loss=4.880454063415527
I0206 01:17:33.063887 139946414638848 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.5752798318862915, loss=2.8660669326782227
I0206 01:18:19.640762 139946397853440 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.6755399703979492, loss=4.329590797424316
I0206 01:19:06.292402 139946414638848 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.8991501331329346, loss=2.361687183380127
I0206 01:19:53.332454 139946397853440 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.8263908624649048, loss=2.371579647064209
I0206 01:20:38.678410 140107197974336 spec.py:321] Evaluating on the training split.
I0206 01:20:49.798882 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 01:21:25.510522 140107197974336 spec.py:349] Evaluating on the test split.
I0206 01:21:27.112264 140107197974336 submission_runner.py:408] Time since start: 38905.01s, 	Step: 75899, 	{'train/accuracy': 0.6452929377555847, 'train/loss': 1.4677882194519043, 'validation/accuracy': 0.6007599830627441, 'validation/loss': 1.683293342590332, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.326888084411621, 'test/num_examples': 10000, 'score': 34906.559433460236, 'total_duration': 38905.00736904144, 'accumulated_submission_time': 34906.559433460236, 'accumulated_eval_time': 3990.819508075714, 'accumulated_logging_time': 3.2981767654418945}
I0206 01:21:27.146122 139946414638848 logging_writer.py:48] [75899] accumulated_eval_time=3990.819508, accumulated_logging_time=3.298177, accumulated_submission_time=34906.559433, global_step=75899, preemption_count=0, score=34906.559433, test/accuracy=0.479200, test/loss=2.326888, test/num_examples=10000, total_duration=38905.007369, train/accuracy=0.645293, train/loss=1.467788, validation/accuracy=0.600760, validation/loss=1.683293, validation/num_examples=50000
I0206 01:21:27.939816 139946397853440 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.8447810411453247, loss=3.6018638610839844
I0206 01:22:09.838492 139946414638848 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.087496757507324, loss=2.5514371395111084
I0206 01:22:55.929488 139946397853440 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.9782195091247559, loss=2.4011027812957764
I0206 01:23:42.707448 139946414638848 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.9345567226409912, loss=2.4716343879699707
I0206 01:24:29.183462 139946397853440 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.5867624282836914, loss=4.3823466300964355
I0206 01:25:15.682542 139946414638848 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.9026048183441162, loss=2.4833931922912598
I0206 01:26:01.843632 139946397853440 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.0322182178497314, loss=2.4511263370513916
I0206 01:26:48.423291 139946414638848 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.6246458292007446, loss=3.3715343475341797
I0206 01:27:35.063561 139946397853440 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.159050464630127, loss=2.4456076622009277
I0206 01:28:21.825803 139946414638848 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.990873098373413, loss=2.97123384475708
I0206 01:28:27.126100 140107197974336 spec.py:321] Evaluating on the training split.
I0206 01:28:37.981711 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 01:29:13.204809 140107197974336 spec.py:349] Evaluating on the test split.
I0206 01:29:14.804308 140107197974336 submission_runner.py:408] Time since start: 39372.70s, 	Step: 76813, 	{'train/accuracy': 0.6568945050239563, 'train/loss': 1.4267654418945312, 'validation/accuracy': 0.6042400002479553, 'validation/loss': 1.6688212156295776, 'validation/num_examples': 50000, 'test/accuracy': 0.48580002784729004, 'test/loss': 2.3179571628570557, 'test/num_examples': 10000, 'score': 35326.47755908966, 'total_duration': 39372.69943475723, 'accumulated_submission_time': 35326.47755908966, 'accumulated_eval_time': 4038.497713804245, 'accumulated_logging_time': 3.342529058456421}
I0206 01:29:14.831567 139946397853440 logging_writer.py:48] [76813] accumulated_eval_time=4038.497714, accumulated_logging_time=3.342529, accumulated_submission_time=35326.477559, global_step=76813, preemption_count=0, score=35326.477559, test/accuracy=0.485800, test/loss=2.317957, test/num_examples=10000, total_duration=39372.699435, train/accuracy=0.656895, train/loss=1.426765, validation/accuracy=0.604240, validation/loss=1.668821, validation/num_examples=50000
I0206 01:29:51.097826 139946414638848 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.759883999824524, loss=4.204023838043213
I0206 01:30:37.354125 139946397853440 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.9325027465820312, loss=2.6604020595550537
I0206 01:31:23.902729 139946414638848 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.7700071334838867, loss=2.9567811489105225
I0206 01:32:10.298993 139946397853440 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.0807271003723145, loss=2.5177154541015625
I0206 01:32:56.732279 139946414638848 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.9706499576568604, loss=2.7457261085510254
I0206 01:33:43.249891 139946397853440 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.8624658584594727, loss=5.050299644470215
I0206 01:34:29.804262 139946414638848 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.9852594137191772, loss=2.504375696182251
I0206 01:35:16.397140 139946397853440 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.8760185241699219, loss=2.784254789352417
I0206 01:36:02.946192 139946414638848 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.8467786312103271, loss=2.6059134006500244
I0206 01:36:15.095922 140107197974336 spec.py:321] Evaluating on the training split.
I0206 01:36:25.949607 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 01:36:58.765152 140107197974336 spec.py:349] Evaluating on the test split.
I0206 01:37:00.364048 140107197974336 submission_runner.py:408] Time since start: 39838.26s, 	Step: 77728, 	{'train/accuracy': 0.6592968702316284, 'train/loss': 1.4043962955474854, 'validation/accuracy': 0.6110000014305115, 'validation/loss': 1.6132807731628418, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.274594783782959, 'test/num_examples': 10000, 'score': 35746.67924427986, 'total_duration': 39838.25917339325, 'accumulated_submission_time': 35746.67924427986, 'accumulated_eval_time': 4083.7658500671387, 'accumulated_logging_time': 3.3802947998046875}
I0206 01:37:00.391769 139946397853440 logging_writer.py:48] [77728] accumulated_eval_time=4083.765850, accumulated_logging_time=3.380295, accumulated_submission_time=35746.679244, global_step=77728, preemption_count=0, score=35746.679244, test/accuracy=0.488100, test/loss=2.274595, test/num_examples=10000, total_duration=39838.259173, train/accuracy=0.659297, train/loss=1.404396, validation/accuracy=0.611000, validation/loss=1.613281, validation/num_examples=50000
I0206 01:37:29.742241 139946414638848 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.8000847101211548, loss=4.199633598327637
I0206 01:38:15.902038 139946397853440 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.8542324304580688, loss=4.936175346374512
I0206 01:39:02.559209 139946414638848 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.0249009132385254, loss=2.33890700340271
I0206 01:39:48.928078 139946397853440 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.7025132179260254, loss=3.612802028656006
I0206 01:40:35.374197 139946414638848 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.9079203605651855, loss=2.3984124660491943
I0206 01:41:21.793446 139946397853440 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.9802643060684204, loss=2.3154022693634033
I0206 01:42:08.069779 139946414638848 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.958899736404419, loss=2.3833107948303223
I0206 01:42:54.293060 139946397853440 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.125351905822754, loss=2.36742901802063
I0206 01:43:40.724114 139946414638848 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.9938517808914185, loss=2.3986828327178955
I0206 01:44:00.529913 140107197974336 spec.py:321] Evaluating on the training split.
I0206 01:44:11.568443 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 01:44:47.886198 140107197974336 spec.py:349] Evaluating on the test split.
I0206 01:44:49.481898 140107197974336 submission_runner.py:408] Time since start: 40307.38s, 	Step: 78644, 	{'train/accuracy': 0.6494140625, 'train/loss': 1.4841097593307495, 'validation/accuracy': 0.6038999557495117, 'validation/loss': 1.6853262186050415, 'validation/num_examples': 50000, 'test/accuracy': 0.4855000376701355, 'test/loss': 2.3392891883850098, 'test/num_examples': 10000, 'score': 36166.756766080856, 'total_duration': 40307.37702679634, 'accumulated_submission_time': 36166.756766080856, 'accumulated_eval_time': 4132.717834472656, 'accumulated_logging_time': 3.4168825149536133}
I0206 01:44:49.513750 139946397853440 logging_writer.py:48] [78644] accumulated_eval_time=4132.717834, accumulated_logging_time=3.416883, accumulated_submission_time=36166.756766, global_step=78644, preemption_count=0, score=36166.756766, test/accuracy=0.485500, test/loss=2.339289, test/num_examples=10000, total_duration=40307.377027, train/accuracy=0.649414, train/loss=1.484110, validation/accuracy=0.603900, validation/loss=1.685326, validation/num_examples=50000
I0206 01:45:11.917276 139946414638848 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.075665235519409, loss=2.473090648651123
I0206 01:45:57.401570 139946397853440 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.144662618637085, loss=2.3759632110595703
I0206 01:46:43.664843 139946414638848 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.6357892751693726, loss=4.950428009033203
I0206 01:47:30.164821 139946397853440 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.6853209733963013, loss=5.069411754608154
I0206 01:48:16.780492 139946414638848 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.0752933025360107, loss=2.4765853881835938
I0206 01:49:03.216424 139946397853440 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.9985549449920654, loss=2.609496593475342
I0206 01:49:49.722983 139946414638848 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.5236283540725708, loss=4.457204818725586
I0206 01:50:36.257799 139946397853440 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.8653559684753418, loss=2.3512420654296875
I0206 01:51:22.783138 139946414638848 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.9233758449554443, loss=5.09960412979126
I0206 01:51:49.674123 140107197974336 spec.py:321] Evaluating on the training split.
I0206 01:52:00.585014 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 01:52:35.100092 140107197974336 spec.py:349] Evaluating on the test split.
I0206 01:52:36.693543 140107197974336 submission_runner.py:408] Time since start: 40774.59s, 	Step: 79559, 	{'train/accuracy': 0.6655663847923279, 'train/loss': 1.3757052421569824, 'validation/accuracy': 0.6121199727058411, 'validation/loss': 1.6102243661880493, 'validation/num_examples': 50000, 'test/accuracy': 0.4878000319004059, 'test/loss': 2.2785677909851074, 'test/num_examples': 10000, 'score': 36586.85572004318, 'total_duration': 40774.5886554718, 'accumulated_submission_time': 36586.85572004318, 'accumulated_eval_time': 4179.737259864807, 'accumulated_logging_time': 3.457533836364746}
I0206 01:52:36.725753 139946397853440 logging_writer.py:48] [79559] accumulated_eval_time=4179.737260, accumulated_logging_time=3.457534, accumulated_submission_time=36586.855720, global_step=79559, preemption_count=0, score=36586.855720, test/accuracy=0.487800, test/loss=2.278568, test/num_examples=10000, total_duration=40774.588655, train/accuracy=0.665566, train/loss=1.375705, validation/accuracy=0.612120, validation/loss=1.610224, validation/num_examples=50000
I0206 01:52:53.214487 139946414638848 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.615478515625, loss=4.698633193969727
I0206 01:53:37.907068 139946397853440 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.7843761444091797, loss=4.691246509552002
I0206 01:54:24.420008 139946414638848 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.977192997932434, loss=2.6837635040283203
I0206 01:55:11.131351 139946397853440 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.0593128204345703, loss=2.5549542903900146
I0206 01:55:57.757094 139946414638848 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.3246517181396484, loss=2.5062246322631836
I0206 01:56:44.532026 139946397853440 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.7717812061309814, loss=3.641686201095581
I0206 01:57:30.732553 139946414638848 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.9040253162384033, loss=2.4385011196136475
I0206 01:58:17.224686 139946397853440 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.123939275741577, loss=2.482825994491577
I0206 01:59:03.454624 139946414638848 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.8196496963500977, loss=2.950730800628662
I0206 01:59:36.994083 140107197974336 spec.py:321] Evaluating on the training split.
I0206 01:59:47.921171 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 02:00:21.442817 140107197974336 spec.py:349] Evaluating on the test split.
I0206 02:00:23.050751 140107197974336 submission_runner.py:408] Time since start: 41240.95s, 	Step: 80474, 	{'train/accuracy': 0.6681054830551147, 'train/loss': 1.3617504835128784, 'validation/accuracy': 0.6156399846076965, 'validation/loss': 1.5994772911071777, 'validation/num_examples': 50000, 'test/accuracy': 0.4944000244140625, 'test/loss': 2.2579493522644043, 'test/num_examples': 10000, 'score': 37007.06138277054, 'total_duration': 41240.94586133957, 'accumulated_submission_time': 37007.06138277054, 'accumulated_eval_time': 4225.793916940689, 'accumulated_logging_time': 3.5003437995910645}
I0206 02:00:23.087823 139946397853440 logging_writer.py:48] [80474] accumulated_eval_time=4225.793917, accumulated_logging_time=3.500344, accumulated_submission_time=37007.061383, global_step=80474, preemption_count=0, score=37007.061383, test/accuracy=0.494400, test/loss=2.257949, test/num_examples=10000, total_duration=41240.945861, train/accuracy=0.668105, train/loss=1.361750, validation/accuracy=0.615640, validation/loss=1.599477, validation/num_examples=50000
I0206 02:00:33.697180 139946414638848 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.7644613981246948, loss=4.082276344299316
I0206 02:01:17.282398 139946397853440 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.1176180839538574, loss=2.4477930068969727
I0206 02:02:04.023198 139946414638848 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.0442960262298584, loss=2.3945133686065674
I0206 02:02:50.380755 139946397853440 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.5707981586456299, loss=4.439651966094971
I0206 02:03:37.046503 139946414638848 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.035794973373413, loss=2.5162110328674316
I0206 02:04:23.524535 139946397853440 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.7949862480163574, loss=2.7866463661193848
I0206 02:05:10.378230 139946414638848 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.7452678680419922, loss=2.90861177444458
I0206 02:05:56.817513 139946397853440 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.5922253131866455, loss=4.331114768981934
I0206 02:06:43.239473 139946414638848 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.336881637573242, loss=2.424534797668457
I0206 02:07:23.200730 140107197974336 spec.py:321] Evaluating on the training split.
I0206 02:07:34.075547 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 02:08:10.436571 140107197974336 spec.py:349] Evaluating on the test split.
I0206 02:08:12.043769 140107197974336 submission_runner.py:408] Time since start: 41709.94s, 	Step: 81388, 	{'train/accuracy': 0.6598241925239563, 'train/loss': 1.4070793390274048, 'validation/accuracy': 0.6183599829673767, 'validation/loss': 1.610404133796692, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.2725863456726074, 'test/num_examples': 10000, 'score': 37427.10963559151, 'total_duration': 41709.938883304596, 'accumulated_submission_time': 37427.10963559151, 'accumulated_eval_time': 4274.636933803558, 'accumulated_logging_time': 3.5497610569000244}
I0206 02:08:12.078545 139946397853440 logging_writer.py:48] [81388] accumulated_eval_time=4274.636934, accumulated_logging_time=3.549761, accumulated_submission_time=37427.109636, global_step=81388, preemption_count=0, score=37427.109636, test/accuracy=0.494900, test/loss=2.272586, test/num_examples=10000, total_duration=41709.938883, train/accuracy=0.659824, train/loss=1.407079, validation/accuracy=0.618360, validation/loss=1.610404, validation/num_examples=50000
I0206 02:08:17.189158 139946414638848 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.0627636909484863, loss=2.307986259460449
I0206 02:08:59.625062 139946397853440 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.7614518404006958, loss=3.153597831726074
I0206 02:09:46.055768 139946414638848 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.057945728302002, loss=2.399731159210205
I0206 02:10:32.764277 139946397853440 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.7853549718856812, loss=3.352024793624878
I0206 02:11:19.301578 139946414638848 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.015241861343384, loss=2.3620359897613525
I0206 02:12:05.948358 139946397853440 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.768869161605835, loss=4.687889575958252
I0206 02:12:52.469025 139946414638848 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.9418458938598633, loss=2.364424467086792
I0206 02:13:38.830059 139946397853440 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.9939762353897095, loss=2.42830228805542
I0206 02:14:25.436417 139946414638848 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.8987561464309692, loss=2.4740653038024902
I0206 02:15:12.305049 139946397853440 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.5841116905212402, loss=3.611967086791992
I0206 02:15:12.318725 140107197974336 spec.py:321] Evaluating on the training split.
I0206 02:15:23.102372 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 02:15:56.981543 140107197974336 spec.py:349] Evaluating on the test split.
I0206 02:15:58.582859 140107197974336 submission_runner.py:408] Time since start: 42176.48s, 	Step: 82301, 	{'train/accuracy': 0.6676562428474426, 'train/loss': 1.3960615396499634, 'validation/accuracy': 0.6139400005340576, 'validation/loss': 1.63629150390625, 'validation/num_examples': 50000, 'test/accuracy': 0.4942000210285187, 'test/loss': 2.2834160327911377, 'test/num_examples': 10000, 'score': 37847.287464141846, 'total_duration': 42176.4779920578, 'accumulated_submission_time': 37847.287464141846, 'accumulated_eval_time': 4320.90106010437, 'accumulated_logging_time': 3.595423936843872}
I0206 02:15:58.614319 139946414638848 logging_writer.py:48] [82301] accumulated_eval_time=4320.901060, accumulated_logging_time=3.595424, accumulated_submission_time=37847.287464, global_step=82301, preemption_count=0, score=37847.287464, test/accuracy=0.494200, test/loss=2.283416, test/num_examples=10000, total_duration=42176.477992, train/accuracy=0.667656, train/loss=1.396062, validation/accuracy=0.613940, validation/loss=1.636292, validation/num_examples=50000
I0206 02:16:40.334047 139946397853440 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.2173564434051514, loss=2.5104286670684814
I0206 02:17:26.697021 139946414638848 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.6618257761001587, loss=4.092658042907715
I0206 02:18:13.329075 139946397853440 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.043214797973633, loss=2.339146614074707
I0206 02:18:59.446120 139946414638848 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.9742075204849243, loss=3.0588150024414062
I0206 02:19:45.985327 139946397853440 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.7182588577270508, loss=4.998974323272705
I0206 02:20:32.592795 139946414638848 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.8587948083877563, loss=2.8447792530059814
I0206 02:21:19.086719 139946397853440 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.6532022953033447, loss=4.048666954040527
I0206 02:22:05.489402 139946414638848 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.112276792526245, loss=2.450484275817871
I0206 02:22:51.937691 139946397853440 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.049778461456299, loss=2.807668685913086
I0206 02:22:59.033692 140107197974336 spec.py:321] Evaluating on the training split.
I0206 02:23:10.077784 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 02:23:44.519659 140107197974336 spec.py:349] Evaluating on the test split.
I0206 02:23:46.120698 140107197974336 submission_runner.py:408] Time since start: 42644.02s, 	Step: 83217, 	{'train/accuracy': 0.685742199420929, 'train/loss': 1.2981655597686768, 'validation/accuracy': 0.6175999641418457, 'validation/loss': 1.6042635440826416, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.265681266784668, 'test/num_examples': 10000, 'score': 38267.64587044716, 'total_duration': 42644.01581954956, 'accumulated_submission_time': 38267.64587044716, 'accumulated_eval_time': 4367.9880521297455, 'accumulated_logging_time': 3.6356606483459473}
I0206 02:23:46.149146 139946414638848 logging_writer.py:48] [83217] accumulated_eval_time=4367.988052, accumulated_logging_time=3.635661, accumulated_submission_time=38267.645870, global_step=83217, preemption_count=0, score=38267.645870, test/accuracy=0.493700, test/loss=2.265681, test/num_examples=10000, total_duration=42644.015820, train/accuracy=0.685742, train/loss=1.298166, validation/accuracy=0.617600, validation/loss=1.604264, validation/num_examples=50000
I0206 02:24:20.692169 139946397853440 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.9716293811798096, loss=2.302252769470215
I0206 02:25:06.983167 139946414638848 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.04172682762146, loss=2.4575493335723877
I0206 02:25:53.335537 139946397853440 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.686765193939209, loss=3.7213525772094727
I0206 02:26:39.762933 139946414638848 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.367413282394409, loss=2.3816916942596436
I0206 02:27:26.300572 139946397853440 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.0568313598632812, loss=2.4510769844055176
I0206 02:28:12.629606 139946414638848 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.6809078454971313, loss=4.539589881896973
I0206 02:28:58.917554 139946397853440 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.89178466796875, loss=3.542546033859253
I0206 02:29:45.339752 139946414638848 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.022768259048462, loss=2.659581184387207
I0206 02:30:31.854363 139946397853440 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.741647720336914, loss=4.961430549621582
I0206 02:30:46.646019 140107197974336 spec.py:321] Evaluating on the training split.
I0206 02:30:57.481865 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 02:31:34.335286 140107197974336 spec.py:349] Evaluating on the test split.
I0206 02:31:35.933758 140107197974336 submission_runner.py:408] Time since start: 43113.83s, 	Step: 84133, 	{'train/accuracy': 0.6615625023841858, 'train/loss': 1.408983588218689, 'validation/accuracy': 0.617680013179779, 'validation/loss': 1.6139475107192993, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.265106439590454, 'test/num_examples': 10000, 'score': 38688.0792453289, 'total_duration': 43113.82888507843, 'accumulated_submission_time': 38688.0792453289, 'accumulated_eval_time': 4417.275773525238, 'accumulated_logging_time': 3.674084186553955}
I0206 02:31:35.964098 139946414638848 logging_writer.py:48] [84133] accumulated_eval_time=4417.275774, accumulated_logging_time=3.674084, accumulated_submission_time=38688.079245, global_step=84133, preemption_count=0, score=38688.079245, test/accuracy=0.499900, test/loss=2.265106, test/num_examples=10000, total_duration=43113.828885, train/accuracy=0.661563, train/loss=1.408984, validation/accuracy=0.617680, validation/loss=1.613948, validation/num_examples=50000
I0206 02:32:03.105216 139946397853440 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.0288290977478027, loss=2.2889318466186523
I0206 02:32:48.935693 139946414638848 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.7966322898864746, loss=5.089014530181885
I0206 02:33:35.377417 139946397853440 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.7810739278793335, loss=3.196336269378662
I0206 02:34:21.644510 139946414638848 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.6783112287521362, loss=4.373768329620361
I0206 02:35:08.301334 139946397853440 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.007521390914917, loss=2.603696584701538
I0206 02:35:54.430158 139946414638848 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.859041452407837, loss=4.645868301391602
I0206 02:36:41.062503 139946397853440 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.062171220779419, loss=2.467822551727295
I0206 02:37:27.787405 139946414638848 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.3639392852783203, loss=2.6608572006225586
I0206 02:38:14.107562 139946397853440 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.7990363836288452, loss=2.3947134017944336
I0206 02:38:36.087292 140107197974336 spec.py:321] Evaluating on the training split.
I0206 02:38:47.325512 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 02:39:23.753359 140107197974336 spec.py:349] Evaluating on the test split.
I0206 02:39:25.346958 140107197974336 submission_runner.py:408] Time since start: 43583.24s, 	Step: 85049, 	{'train/accuracy': 0.6717578172683716, 'train/loss': 1.3764458894729614, 'validation/accuracy': 0.6211400032043457, 'validation/loss': 1.6145461797714233, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.244614601135254, 'test/num_examples': 10000, 'score': 39108.140686035156, 'total_duration': 43583.242089509964, 'accumulated_submission_time': 39108.140686035156, 'accumulated_eval_time': 4466.535435676575, 'accumulated_logging_time': 3.7140793800354004}
I0206 02:39:25.380251 139946414638848 logging_writer.py:48] [85049] accumulated_eval_time=4466.535436, accumulated_logging_time=3.714079, accumulated_submission_time=39108.140686, global_step=85049, preemption_count=0, score=39108.140686, test/accuracy=0.505800, test/loss=2.244615, test/num_examples=10000, total_duration=43583.242090, train/accuracy=0.671758, train/loss=1.376446, validation/accuracy=0.621140, validation/loss=1.614546, validation/num_examples=50000
I0206 02:39:45.799465 139946397853440 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.9307626485824585, loss=2.3903417587280273
I0206 02:40:30.926619 139946414638848 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.9531883001327515, loss=3.1545703411102295
I0206 02:41:17.165537 139946397853440 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.573447346687317, loss=4.32722282409668
I0206 02:42:03.896167 139946414638848 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.9117738008499146, loss=3.076319694519043
I0206 02:42:50.122218 139946397853440 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.116546630859375, loss=2.359621047973633
I0206 02:43:36.680578 139946414638848 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.9714782238006592, loss=2.1505253314971924
I0206 02:44:23.443059 139946397853440 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.0521793365478516, loss=2.3594048023223877
I0206 02:45:10.126454 139946414638848 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.7932859659194946, loss=3.1499216556549072
I0206 02:45:56.474146 139946397853440 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.7182198762893677, loss=3.8641462326049805
I0206 02:46:25.799832 140107197974336 spec.py:321] Evaluating on the training split.
I0206 02:46:36.901125 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 02:47:09.840612 140107197974336 spec.py:349] Evaluating on the test split.
I0206 02:47:11.431485 140107197974336 submission_runner.py:408] Time since start: 44049.33s, 	Step: 85964, 	{'train/accuracy': 0.6827148199081421, 'train/loss': 1.2997052669525146, 'validation/accuracy': 0.6204400062561035, 'validation/loss': 1.5726711750030518, 'validation/num_examples': 50000, 'test/accuracy': 0.5038000345230103, 'test/loss': 2.207498550415039, 'test/num_examples': 10000, 'score': 39528.49926805496, 'total_duration': 44049.326600790024, 'accumulated_submission_time': 39528.49926805496, 'accumulated_eval_time': 4512.167064666748, 'accumulated_logging_time': 3.7566189765930176}
I0206 02:47:11.468480 139946414638848 logging_writer.py:48] [85964] accumulated_eval_time=4512.167065, accumulated_logging_time=3.756619, accumulated_submission_time=39528.499268, global_step=85964, preemption_count=0, score=39528.499268, test/accuracy=0.503800, test/loss=2.207499, test/num_examples=10000, total_duration=44049.326601, train/accuracy=0.682715, train/loss=1.299705, validation/accuracy=0.620440, validation/loss=1.572671, validation/num_examples=50000
I0206 02:47:26.023214 139946397853440 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.547353744506836, loss=2.4790945053100586
I0206 02:48:10.453253 139946414638848 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.127073049545288, loss=2.33270525932312
I0206 02:48:57.105655 139946397853440 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.6047914028167725, loss=3.32028865814209
I0206 02:49:43.807880 139946414638848 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.6436429023742676, loss=4.58474063873291
I0206 02:50:30.131200 139946397853440 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.032451868057251, loss=2.4978270530700684
I0206 02:51:16.732276 139946414638848 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.6701953411102295, loss=3.1917271614074707
I0206 02:52:03.158146 139946397853440 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.3733224868774414, loss=2.2658531665802
I0206 02:52:49.697262 139946414638848 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.6939616203308105, loss=3.302196979522705
I0206 02:53:36.313755 139946397853440 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.093231201171875, loss=2.4360411167144775
I0206 02:54:11.793628 140107197974336 spec.py:321] Evaluating on the training split.
I0206 02:54:22.498134 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 02:54:57.407665 140107197974336 spec.py:349] Evaluating on the test split.
I0206 02:54:59.008502 140107197974336 submission_runner.py:408] Time since start: 44516.90s, 	Step: 86878, 	{'train/accuracy': 0.6692578196525574, 'train/loss': 1.3671681880950928, 'validation/accuracy': 0.6223799586296082, 'validation/loss': 1.5760130882263184, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.202033042907715, 'test/num_examples': 10000, 'score': 39948.76182985306, 'total_duration': 44516.90362381935, 'accumulated_submission_time': 39948.76182985306, 'accumulated_eval_time': 4559.381934642792, 'accumulated_logging_time': 3.8039591312408447}
I0206 02:54:59.038263 139946414638848 logging_writer.py:48] [86878] accumulated_eval_time=4559.381935, accumulated_logging_time=3.803959, accumulated_submission_time=39948.761830, global_step=86878, preemption_count=0, score=39948.761830, test/accuracy=0.503500, test/loss=2.202033, test/num_examples=10000, total_duration=44516.903624, train/accuracy=0.669258, train/loss=1.367168, validation/accuracy=0.622380, validation/loss=1.576013, validation/num_examples=50000
I0206 02:55:08.074750 139946397853440 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.0380232334136963, loss=2.4643781185150146
I0206 02:55:51.283607 139946414638848 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.9805781841278076, loss=2.2500369548797607
I0206 02:56:37.620362 139946397853440 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.0802993774414062, loss=2.343292713165283
I0206 02:57:24.238791 139946414638848 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.9047613143920898, loss=2.5779709815979004
I0206 02:58:10.657747 139946397853440 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.157048463821411, loss=2.4224932193756104
I0206 02:58:57.136518 139946414638848 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.844461441040039, loss=2.562049627304077
I0206 02:59:43.582088 139946397853440 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.985072374343872, loss=2.3440489768981934
I0206 03:00:30.122737 139946414638848 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.2514827251434326, loss=2.3661086559295654
I0206 03:01:16.713072 139946397853440 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.1700167655944824, loss=2.5927059650421143
I0206 03:01:59.368915 140107197974336 spec.py:321] Evaluating on the training split.
I0206 03:02:10.316364 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 03:02:44.984364 140107197974336 spec.py:349] Evaluating on the test split.
I0206 03:02:46.586700 140107197974336 submission_runner.py:408] Time since start: 44984.48s, 	Step: 87793, 	{'train/accuracy': 0.6750195026397705, 'train/loss': 1.3554376363754272, 'validation/accuracy': 0.6269999742507935, 'validation/loss': 1.583235740661621, 'validation/num_examples': 50000, 'test/accuracy': 0.5076000094413757, 'test/loss': 2.2284297943115234, 'test/num_examples': 10000, 'score': 40369.03018307686, 'total_duration': 44984.48181915283, 'accumulated_submission_time': 40369.03018307686, 'accumulated_eval_time': 4606.599714756012, 'accumulated_logging_time': 3.843261957168579}
I0206 03:02:46.624206 139946414638848 logging_writer.py:48] [87793] accumulated_eval_time=4606.599715, accumulated_logging_time=3.843262, accumulated_submission_time=40369.030183, global_step=87793, preemption_count=0, score=40369.030183, test/accuracy=0.507600, test/loss=2.228430, test/num_examples=10000, total_duration=44984.481819, train/accuracy=0.675020, train/loss=1.355438, validation/accuracy=0.627000, validation/loss=1.583236, validation/num_examples=50000
I0206 03:02:49.767381 139946397853440 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.2673258781433105, loss=2.3806910514831543
I0206 03:03:32.058627 139946414638848 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.022862434387207, loss=3.349144697189331
I0206 03:04:18.207788 139946397853440 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.103451728820801, loss=2.643017053604126
I0206 03:05:05.096813 139946414638848 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.3065714836120605, loss=2.365119695663452
I0206 03:05:51.387480 139946397853440 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.5954939126968384, loss=3.8231167793273926
I0206 03:06:37.719930 139946414638848 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.6555871963500977, loss=3.41182541847229
I0206 03:07:24.374176 139946397853440 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.108088493347168, loss=2.3977179527282715
I0206 03:08:10.880474 139946414638848 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.8121291399002075, loss=4.754047393798828
I0206 03:08:57.398387 139946397853440 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.9790658950805664, loss=2.338015079498291
I0206 03:09:44.418679 139946414638848 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.2250659465789795, loss=2.4618358612060547
I0206 03:09:46.783459 140107197974336 spec.py:321] Evaluating on the training split.
I0206 03:09:57.610473 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 03:10:31.249974 140107197974336 spec.py:349] Evaluating on the test split.
I0206 03:10:32.846909 140107197974336 submission_runner.py:408] Time since start: 45450.74s, 	Step: 88707, 	{'train/accuracy': 0.6813281178474426, 'train/loss': 1.3070732355117798, 'validation/accuracy': 0.6232199668884277, 'validation/loss': 1.5689715147018433, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.2088680267333984, 'test/num_examples': 10000, 'score': 40789.127017498016, 'total_duration': 45450.74204015732, 'accumulated_submission_time': 40789.127017498016, 'accumulated_eval_time': 4652.663172245026, 'accumulated_logging_time': 3.8918819427490234}
I0206 03:10:32.876157 139946397853440 logging_writer.py:48] [88707] accumulated_eval_time=4652.663172, accumulated_logging_time=3.891882, accumulated_submission_time=40789.127017, global_step=88707, preemption_count=0, score=40789.127017, test/accuracy=0.501800, test/loss=2.208868, test/num_examples=10000, total_duration=45450.742040, train/accuracy=0.681328, train/loss=1.307073, validation/accuracy=0.623220, validation/loss=1.568972, validation/num_examples=50000
I0206 03:11:12.150795 139946414638848 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.1130266189575195, loss=2.138859510421753
I0206 03:11:57.788316 139946397853440 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.214580774307251, loss=2.6243503093719482
I0206 03:12:44.252298 139946414638848 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.06831431388855, loss=2.6944994926452637
I0206 03:13:31.327410 139946397853440 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.7384719848632812, loss=2.8981785774230957
I0206 03:14:17.827647 139946414638848 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.1051692962646484, loss=2.4438891410827637
I0206 03:15:04.615437 139946397853440 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.1609606742858887, loss=2.2706334590911865
I0206 03:15:51.340329 139946414638848 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.8522005081176758, loss=2.7073824405670166
I0206 03:16:37.833485 139946397853440 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.9766736030578613, loss=2.287238359451294
I0206 03:17:24.330661 139946414638848 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.9390995502471924, loss=5.069154739379883
I0206 03:17:32.902170 140107197974336 spec.py:321] Evaluating on the training split.
I0206 03:17:43.962759 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 03:18:20.921476 140107197974336 spec.py:349] Evaluating on the test split.
I0206 03:18:22.514566 140107197974336 submission_runner.py:408] Time since start: 45920.41s, 	Step: 89620, 	{'train/accuracy': 0.6719921827316284, 'train/loss': 1.3421682119369507, 'validation/accuracy': 0.6292600035667419, 'validation/loss': 1.5393462181091309, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.181424617767334, 'test/num_examples': 10000, 'score': 41209.09097337723, 'total_duration': 45920.40969824791, 'accumulated_submission_time': 41209.09097337723, 'accumulated_eval_time': 4702.275573730469, 'accumulated_logging_time': 3.930466890335083}
I0206 03:18:22.543608 139946397853440 logging_writer.py:48] [89620] accumulated_eval_time=4702.275574, accumulated_logging_time=3.930467, accumulated_submission_time=41209.090973, global_step=89620, preemption_count=0, score=41209.090973, test/accuracy=0.508200, test/loss=2.181425, test/num_examples=10000, total_duration=45920.409698, train/accuracy=0.671992, train/loss=1.342168, validation/accuracy=0.629260, validation/loss=1.539346, validation/num_examples=50000
I0206 03:18:55.449351 139946414638848 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.0597333908081055, loss=2.454847812652588
I0206 03:19:41.574763 139946397853440 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.24722957611084, loss=2.993682384490967
I0206 03:20:28.386346 139946414638848 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.163078784942627, loss=2.56062650680542
I0206 03:21:14.663048 139946397853440 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.9611008167266846, loss=2.647786855697632
I0206 03:22:01.213043 139946414638848 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.9899938106536865, loss=2.253370761871338
I0206 03:22:47.616730 139946397853440 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.0874574184417725, loss=2.3102736473083496
I0206 03:23:34.145246 139946414638848 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.0968308448791504, loss=2.314480781555176
I0206 03:24:21.029149 139946397853440 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.0721001625061035, loss=2.4968185424804688
I0206 03:25:07.532435 139946414638848 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.1174042224884033, loss=2.3238766193389893
I0206 03:25:22.553010 140107197974336 spec.py:321] Evaluating on the training split.
I0206 03:25:33.472925 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 03:26:06.917621 140107197974336 spec.py:349] Evaluating on the test split.
I0206 03:26:08.516861 140107197974336 submission_runner.py:408] Time since start: 46386.41s, 	Step: 90534, 	{'train/accuracy': 0.6768749952316284, 'train/loss': 1.3328397274017334, 'validation/accuracy': 0.6259399652481079, 'validation/loss': 1.5650203227996826, 'validation/num_examples': 50000, 'test/accuracy': 0.504800021648407, 'test/loss': 2.2196996212005615, 'test/num_examples': 10000, 'score': 41629.03953385353, 'total_duration': 46386.41199302673, 'accumulated_submission_time': 41629.03953385353, 'accumulated_eval_time': 4748.239414215088, 'accumulated_logging_time': 3.968465805053711}
I0206 03:26:08.550020 139946397853440 logging_writer.py:48] [90534] accumulated_eval_time=4748.239414, accumulated_logging_time=3.968466, accumulated_submission_time=41629.039534, global_step=90534, preemption_count=0, score=41629.039534, test/accuracy=0.504800, test/loss=2.219700, test/num_examples=10000, total_duration=46386.411993, train/accuracy=0.676875, train/loss=1.332840, validation/accuracy=0.625940, validation/loss=1.565020, validation/num_examples=50000
I0206 03:26:35.308576 139946414638848 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.2980008125305176, loss=2.345715045928955
I0206 03:27:21.400141 139946397853440 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.047236442565918, loss=2.269364356994629
I0206 03:28:08.508538 139946414638848 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.7130951881408691, loss=4.093922138214111
I0206 03:28:54.936596 139946397853440 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.0690102577209473, loss=2.319754123687744
I0206 03:29:41.540304 139946414638848 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.7674552202224731, loss=3.1195366382598877
I0206 03:30:28.007624 139946397853440 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.867060661315918, loss=4.305577754974365
I0206 03:31:14.590231 139946414638848 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.9756590127944946, loss=4.209860324859619
I0206 03:32:01.112493 139946397853440 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.9395637512207031, loss=2.2662148475646973
I0206 03:32:47.903644 139946414638848 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.8921326398849487, loss=3.0314650535583496
I0206 03:33:08.661208 140107197974336 spec.py:321] Evaluating on the training split.
I0206 03:33:19.616544 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 03:33:56.528414 140107197974336 spec.py:349] Evaluating on the test split.
I0206 03:33:58.124121 140107197974336 submission_runner.py:408] Time since start: 46856.02s, 	Step: 91446, 	{'train/accuracy': 0.6907030940055847, 'train/loss': 1.2894749641418457, 'validation/accuracy': 0.6323599815368652, 'validation/loss': 1.5473542213439941, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.1950297355651855, 'test/num_examples': 10000, 'score': 42049.08965468407, 'total_duration': 46856.01924753189, 'accumulated_submission_time': 42049.08965468407, 'accumulated_eval_time': 4797.702326059341, 'accumulated_logging_time': 4.011040687561035}
I0206 03:33:58.157263 139946397853440 logging_writer.py:48] [91446] accumulated_eval_time=4797.702326, accumulated_logging_time=4.011041, accumulated_submission_time=42049.089655, global_step=91446, preemption_count=0, score=42049.089655, test/accuracy=0.511400, test/loss=2.195030, test/num_examples=10000, total_duration=46856.019248, train/accuracy=0.690703, train/loss=1.289475, validation/accuracy=0.632360, validation/loss=1.547354, validation/num_examples=50000
I0206 03:34:19.749630 139946414638848 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.203012704849243, loss=2.5045809745788574
I0206 03:35:05.427613 139946397853440 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.09928560256958, loss=2.286252975463867
I0206 03:35:52.170954 139946414638848 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.018968105316162, loss=2.440934658050537
I0206 03:36:38.901425 139946397853440 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.9311366081237793, loss=4.380452632904053
I0206 03:37:25.605993 139946414638848 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.9333997964859009, loss=3.227217435836792
I0206 03:38:11.995570 139946397853440 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.1298954486846924, loss=2.2426695823669434
I0206 03:38:58.354478 139946414638848 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.0734543800354004, loss=3.15701961517334
I0206 03:39:44.901809 139946397853440 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.7393261194229126, loss=3.4969053268432617
I0206 03:40:31.583641 139946414638848 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.9162623882293701, loss=2.579737901687622
I0206 03:40:58.446898 140107197974336 spec.py:321] Evaluating on the training split.
I0206 03:41:09.420458 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 03:41:45.093699 140107197974336 spec.py:349] Evaluating on the test split.
I0206 03:41:46.690126 140107197974336 submission_runner.py:408] Time since start: 47324.59s, 	Step: 92359, 	{'train/accuracy': 0.6765429377555847, 'train/loss': 1.3410677909851074, 'validation/accuracy': 0.6250799894332886, 'validation/loss': 1.5731840133666992, 'validation/num_examples': 50000, 'test/accuracy': 0.5012000203132629, 'test/loss': 2.218226194381714, 'test/num_examples': 10000, 'score': 42469.318239450455, 'total_duration': 47324.5852496624, 'accumulated_submission_time': 42469.318239450455, 'accumulated_eval_time': 4845.945533275604, 'accumulated_logging_time': 4.053576469421387}
I0206 03:41:46.721581 139946397853440 logging_writer.py:48] [92359] accumulated_eval_time=4845.945533, accumulated_logging_time=4.053576, accumulated_submission_time=42469.318239, global_step=92359, preemption_count=0, score=42469.318239, test/accuracy=0.501200, test/loss=2.218226, test/num_examples=10000, total_duration=47324.585250, train/accuracy=0.676543, train/loss=1.341068, validation/accuracy=0.625080, validation/loss=1.573184, validation/num_examples=50000
I0206 03:42:03.250678 139946414638848 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.197190523147583, loss=2.5935490131378174
I0206 03:42:47.885857 139946397853440 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.305849552154541, loss=2.246652841567993
I0206 03:43:34.178829 139946414638848 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.0135080814361572, loss=2.2927896976470947
I0206 03:44:20.568174 139946397853440 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.1334621906280518, loss=2.3607358932495117
I0206 03:45:07.193950 139946414638848 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.7706325054168701, loss=4.415358066558838
I0206 03:45:53.381341 139946397853440 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.8438036441802979, loss=3.442202091217041
I0206 03:46:39.629861 139946414638848 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.252154588699341, loss=2.241872787475586
I0206 03:47:26.106452 139946397853440 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.0494372844696045, loss=2.4255154132843018
I0206 03:48:12.518310 139946414638848 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.7591731548309326, loss=4.829681396484375
I0206 03:48:47.103686 140107197974336 spec.py:321] Evaluating on the training split.
I0206 03:48:58.072579 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 03:49:33.588570 140107197974336 spec.py:349] Evaluating on the test split.
I0206 03:49:35.188706 140107197974336 submission_runner.py:408] Time since start: 47793.08s, 	Step: 93276, 	{'train/accuracy': 0.68115234375, 'train/loss': 1.303971529006958, 'validation/accuracy': 0.6333000063896179, 'validation/loss': 1.5291578769683838, 'validation/num_examples': 50000, 'test/accuracy': 0.5135000348091125, 'test/loss': 2.1676101684570312, 'test/num_examples': 10000, 'score': 42889.63666677475, 'total_duration': 47793.083832740784, 'accumulated_submission_time': 42889.63666677475, 'accumulated_eval_time': 4894.030555963516, 'accumulated_logging_time': 4.096000909805298}
I0206 03:49:35.223908 139946397853440 logging_writer.py:48] [93276] accumulated_eval_time=4894.030556, accumulated_logging_time=4.096001, accumulated_submission_time=42889.636667, global_step=93276, preemption_count=0, score=42889.636667, test/accuracy=0.513500, test/loss=2.167610, test/num_examples=10000, total_duration=47793.083833, train/accuracy=0.681152, train/loss=1.303972, validation/accuracy=0.633300, validation/loss=1.529158, validation/num_examples=50000
I0206 03:49:45.041463 139946414638848 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.063227415084839, loss=4.259258270263672
I0206 03:50:28.470795 139946397853440 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.872554063796997, loss=2.9626903533935547
I0206 03:51:14.737573 139946414638848 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.829578161239624, loss=3.701899290084839
I0206 03:52:01.304987 139946397853440 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.5990837812423706, loss=4.73561954498291
I0206 03:52:47.707211 139946414638848 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.122556209564209, loss=2.2557082176208496
I0206 03:53:34.333645 139946397853440 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.0590128898620605, loss=2.3032429218292236
I0206 03:54:20.592295 139946414638848 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.9386488199234009, loss=2.4608707427978516
I0206 03:55:07.185401 139946397853440 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.0162482261657715, loss=2.2905995845794678
I0206 03:55:53.604328 139946414638848 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.6938836574554443, loss=3.7364940643310547
I0206 03:56:35.501231 140107197974336 spec.py:321] Evaluating on the training split.
I0206 03:56:46.448416 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 03:57:21.724380 140107197974336 spec.py:349] Evaluating on the test split.
I0206 03:57:23.315718 140107197974336 submission_runner.py:408] Time since start: 48261.21s, 	Step: 94192, 	{'train/accuracy': 0.6882030963897705, 'train/loss': 1.2961716651916504, 'validation/accuracy': 0.6321399807929993, 'validation/loss': 1.5467734336853027, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.1976771354675293, 'test/num_examples': 10000, 'score': 43309.85211658478, 'total_duration': 48261.21060657501, 'accumulated_submission_time': 43309.85211658478, 'accumulated_eval_time': 4941.8448095321655, 'accumulated_logging_time': 4.140573740005493}
I0206 03:57:23.347093 139946397853440 logging_writer.py:48] [94192] accumulated_eval_time=4941.844810, accumulated_logging_time=4.140574, accumulated_submission_time=43309.852117, global_step=94192, preemption_count=0, score=43309.852117, test/accuracy=0.512600, test/loss=2.197677, test/num_examples=10000, total_duration=48261.210607, train/accuracy=0.688203, train/loss=1.296172, validation/accuracy=0.632140, validation/loss=1.546773, validation/num_examples=50000
I0206 03:57:26.890582 139946414638848 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.7317495346069336, loss=4.942071437835693
I0206 03:58:09.288331 139946397853440 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.2673392295837402, loss=2.351978302001953
I0206 03:58:55.378318 139946414638848 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.7713841199874878, loss=4.246284484863281
I0206 03:59:41.699005 139946397853440 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.056154251098633, loss=2.2738401889801025
I0206 04:00:27.976934 139946414638848 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.8645973205566406, loss=2.7044312953948975
I0206 04:01:14.368537 139946397853440 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.832156777381897, loss=4.616607666015625
I0206 04:02:00.611322 139946414638848 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.9939193725585938, loss=4.702017307281494
I0206 04:02:47.013889 139946397853440 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.105989933013916, loss=2.182774543762207
I0206 04:03:33.285108 139946414638848 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.902724266052246, loss=4.521302700042725
I0206 04:04:19.751259 139946397853440 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.3862462043762207, loss=2.3357880115509033
I0206 04:04:23.574974 140107197974336 spec.py:321] Evaluating on the training split.
I0206 04:04:35.299963 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 04:05:12.099608 140107197974336 spec.py:349] Evaluating on the test split.
I0206 04:05:13.701209 140107197974336 submission_runner.py:408] Time since start: 48731.60s, 	Step: 95110, 	{'train/accuracy': 0.7078515291213989, 'train/loss': 1.1914904117584229, 'validation/accuracy': 0.6358000040054321, 'validation/loss': 1.5101193189620972, 'validation/num_examples': 50000, 'test/accuracy': 0.5103999972343445, 'test/loss': 2.1727511882781982, 'test/num_examples': 10000, 'score': 43730.018189907074, 'total_duration': 48731.59634041786, 'accumulated_submission_time': 43730.018189907074, 'accumulated_eval_time': 4991.971055984497, 'accumulated_logging_time': 4.181777000427246}
I0206 04:05:13.731757 139946414638848 logging_writer.py:48] [95110] accumulated_eval_time=4991.971056, accumulated_logging_time=4.181777, accumulated_submission_time=43730.018190, global_step=95110, preemption_count=0, score=43730.018190, test/accuracy=0.510400, test/loss=2.172751, test/num_examples=10000, total_duration=48731.596340, train/accuracy=0.707852, train/loss=1.191490, validation/accuracy=0.635800, validation/loss=1.510119, validation/num_examples=50000
I0206 04:05:51.128728 139946397853440 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.7887015342712402, loss=3.2579498291015625
I0206 04:06:37.262136 139946414638848 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.0796163082122803, loss=2.597553014755249
I0206 04:07:24.010906 139946397853440 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.2915232181549072, loss=2.221242904663086
I0206 04:08:10.540205 139946414638848 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.353944778442383, loss=2.222630500793457
I0206 04:08:56.973725 139946397853440 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.782099962234497, loss=4.590099334716797
I0206 04:09:43.475914 139946414638848 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.022589683532715, loss=2.249174118041992
I0206 04:10:29.913463 139946397853440 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.1052732467651367, loss=2.2254796028137207
I0206 04:11:16.529317 139946414638848 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.9326828718185425, loss=3.2546472549438477
I0206 04:12:02.995842 139946397853440 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.1816000938415527, loss=2.3139724731445312
I0206 04:12:13.881989 140107197974336 spec.py:321] Evaluating on the training split.
I0206 04:12:24.950766 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 04:12:58.295806 140107197974336 spec.py:349] Evaluating on the test split.
I0206 04:12:59.901748 140107197974336 submission_runner.py:408] Time since start: 49197.80s, 	Step: 96025, 	{'train/accuracy': 0.6827734112739563, 'train/loss': 1.2824567556381226, 'validation/accuracy': 0.6331599950790405, 'validation/loss': 1.5004512071609497, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.1757290363311768, 'test/num_examples': 10000, 'score': 44150.107147455215, 'total_duration': 49197.796854019165, 'accumulated_submission_time': 44150.107147455215, 'accumulated_eval_time': 5037.990777015686, 'accumulated_logging_time': 4.222255706787109}
I0206 04:12:59.936196 139946414638848 logging_writer.py:48] [96025] accumulated_eval_time=5037.990777, accumulated_logging_time=4.222256, accumulated_submission_time=44150.107147, global_step=96025, preemption_count=0, score=44150.107147, test/accuracy=0.509400, test/loss=2.175729, test/num_examples=10000, total_duration=49197.796854, train/accuracy=0.682773, train/loss=1.282457, validation/accuracy=0.633160, validation/loss=1.500451, validation/num_examples=50000
I0206 04:13:30.819564 139946397853440 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.791496992111206, loss=4.254918098449707
I0206 04:14:16.971001 139946414638848 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.3354299068450928, loss=2.3745720386505127
I0206 04:15:03.964913 139946397853440 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.1427817344665527, loss=2.2894184589385986
I0206 04:15:50.283098 139946414638848 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.2075109481811523, loss=2.2863407135009766
I0206 04:16:36.956088 139946397853440 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.8851399421691895, loss=4.602139949798584
I0206 04:17:23.574563 139946414638848 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.9663559198379517, loss=2.736940383911133
I0206 04:18:10.091428 139946397853440 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.0319340229034424, loss=3.2414252758026123
I0206 04:18:56.649119 139946414638848 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.292530059814453, loss=3.322641611099243
I0206 04:19:43.242531 139946397853440 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.518310308456421, loss=2.3814618587493896
I0206 04:20:00.155493 140107197974336 spec.py:321] Evaluating on the training split.
I0206 04:20:11.218321 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 04:20:46.326281 140107197974336 spec.py:349] Evaluating on the test split.
I0206 04:20:47.920151 140107197974336 submission_runner.py:408] Time since start: 49665.82s, 	Step: 96938, 	{'train/accuracy': 0.6849414110183716, 'train/loss': 1.297351360321045, 'validation/accuracy': 0.6337800025939941, 'validation/loss': 1.5296707153320312, 'validation/num_examples': 50000, 'test/accuracy': 0.5078999996185303, 'test/loss': 2.185168504714966, 'test/num_examples': 10000, 'score': 44570.26457285881, 'total_duration': 49665.81526470184, 'accumulated_submission_time': 44570.26457285881, 'accumulated_eval_time': 5085.7554042339325, 'accumulated_logging_time': 4.266266822814941}
I0206 04:20:47.959459 139946414638848 logging_writer.py:48] [96938] accumulated_eval_time=5085.755404, accumulated_logging_time=4.266267, accumulated_submission_time=44570.264573, global_step=96938, preemption_count=0, score=44570.264573, test/accuracy=0.507900, test/loss=2.185169, test/num_examples=10000, total_duration=49665.815265, train/accuracy=0.684941, train/loss=1.297351, validation/accuracy=0.633780, validation/loss=1.529671, validation/num_examples=50000
I0206 04:21:12.864266 139946397853440 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.836103916168213, loss=4.583817481994629
I0206 04:21:58.723667 139946414638848 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.1531074047088623, loss=2.7281484603881836
I0206 04:22:45.526667 139946397853440 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.3004794120788574, loss=2.6325392723083496
I0206 04:23:32.012569 139946414638848 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.0120043754577637, loss=2.5162711143493652
I0206 04:24:18.723513 139946397853440 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.5485589504241943, loss=2.4023377895355225
I0206 04:25:05.285284 139946414638848 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.3568034172058105, loss=4.592504501342773
I0206 04:25:51.429203 139946397853440 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.8819029331207275, loss=4.788124084472656
I0206 04:26:37.870602 139946414638848 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.7972148656845093, loss=3.502997875213623
I0206 04:27:24.483048 139946397853440 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.1619157791137695, loss=2.8546855449676514
I0206 04:27:48.395921 140107197974336 spec.py:321] Evaluating on the training split.
I0206 04:27:59.240107 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 04:28:36.511812 140107197974336 spec.py:349] Evaluating on the test split.
I0206 04:28:38.112582 140107197974336 submission_runner.py:408] Time since start: 50136.01s, 	Step: 97853, 	{'train/accuracy': 0.706347644329071, 'train/loss': 1.1973272562026978, 'validation/accuracy': 0.6412000060081482, 'validation/loss': 1.4900233745574951, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.120568037033081, 'test/num_examples': 10000, 'score': 44990.63896560669, 'total_duration': 50136.00769329071, 'accumulated_submission_time': 44990.63896560669, 'accumulated_eval_time': 5135.472050905228, 'accumulated_logging_time': 4.316110610961914}
I0206 04:28:38.150712 139946414638848 logging_writer.py:48] [97853] accumulated_eval_time=5135.472051, accumulated_logging_time=4.316111, accumulated_submission_time=44990.638966, global_step=97853, preemption_count=0, score=44990.638966, test/accuracy=0.521600, test/loss=2.120568, test/num_examples=10000, total_duration=50136.007693, train/accuracy=0.706348, train/loss=1.197327, validation/accuracy=0.641200, validation/loss=1.490023, validation/num_examples=50000
I0206 04:28:57.024091 139946397853440 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.305873394012451, loss=2.322193145751953
I0206 04:29:42.091503 139946414638848 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.0083425045013428, loss=2.5418283939361572
I0206 04:30:28.682862 139946397853440 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.9620577096939087, loss=4.604456424713135
I0206 04:31:14.950092 139946414638848 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.1751492023468018, loss=2.2852540016174316
I0206 04:32:01.163549 139946397853440 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.1951639652252197, loss=2.2398905754089355
I0206 04:32:47.487122 139946414638848 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.343693733215332, loss=2.3605291843414307
I0206 04:33:33.851976 139946397853440 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.9984058141708374, loss=2.666987419128418
I0206 04:34:20.222632 139946414638848 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.854418158531189, loss=4.2716193199157715
I0206 04:35:06.889909 139946397853440 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.2853105068206787, loss=2.236283779144287
I0206 04:35:38.526538 140107197974336 spec.py:321] Evaluating on the training split.
I0206 04:35:49.377962 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 04:36:23.599799 140107197974336 spec.py:349] Evaluating on the test split.
I0206 04:36:25.193384 140107197974336 submission_runner.py:408] Time since start: 50603.09s, 	Step: 98770, 	{'train/accuracy': 0.6881640553474426, 'train/loss': 1.2870441675186157, 'validation/accuracy': 0.6415799856185913, 'validation/loss': 1.4922298192977905, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.161956548690796, 'test/num_examples': 10000, 'score': 45410.94944810867, 'total_duration': 50603.08851194382, 'accumulated_submission_time': 45410.94944810867, 'accumulated_eval_time': 5182.138915061951, 'accumulated_logging_time': 4.366321802139282}
I0206 04:36:25.228665 139946414638848 logging_writer.py:48] [98770] accumulated_eval_time=5182.138915, accumulated_logging_time=4.366322, accumulated_submission_time=45410.949448, global_step=98770, preemption_count=0, score=45410.949448, test/accuracy=0.515900, test/loss=2.161957, test/num_examples=10000, total_duration=50603.088512, train/accuracy=0.688164, train/loss=1.287044, validation/accuracy=0.641580, validation/loss=1.492230, validation/num_examples=50000
I0206 04:36:37.417687 139946397853440 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.0782930850982666, loss=2.7475762367248535
I0206 04:37:21.383864 139946414638848 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.2868525981903076, loss=2.3214824199676514
I0206 04:38:07.617825 139946397853440 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.2988500595092773, loss=2.2455310821533203
I0206 04:38:54.212612 139946414638848 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.8613154888153076, loss=4.840872287750244
I0206 04:39:40.820939 139946397853440 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.280088424682617, loss=2.234029769897461
I0206 04:40:27.292945 139946414638848 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.341212034225464, loss=2.25553035736084
I0206 04:41:13.794150 139946397853440 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.239187479019165, loss=2.7278828620910645
I0206 04:42:00.179349 139946414638848 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.2458972930908203, loss=2.18923282623291
I0206 04:42:46.418143 139946397853440 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.0867855548858643, loss=2.3717875480651855
I0206 04:43:25.466232 140107197974336 spec.py:321] Evaluating on the training split.
I0206 04:43:36.514007 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 04:44:13.492544 140107197974336 spec.py:349] Evaluating on the test split.
I0206 04:44:15.085689 140107197974336 submission_runner.py:408] Time since start: 51072.98s, 	Step: 99686, 	{'train/accuracy': 0.6941796541213989, 'train/loss': 1.2387899160385132, 'validation/accuracy': 0.6456999778747559, 'validation/loss': 1.462598204612732, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.105508327484131, 'test/num_examples': 10000, 'score': 45831.12450551987, 'total_duration': 51072.9808216095, 'accumulated_submission_time': 45831.12450551987, 'accumulated_eval_time': 5231.758380651474, 'accumulated_logging_time': 4.412039041519165}
I0206 04:44:15.119508 139946414638848 logging_writer.py:48] [99686] accumulated_eval_time=5231.758381, accumulated_logging_time=4.412039, accumulated_submission_time=45831.124506, global_step=99686, preemption_count=0, score=45831.124506, test/accuracy=0.525500, test/loss=2.105508, test/num_examples=10000, total_duration=51072.980822, train/accuracy=0.694180, train/loss=1.238790, validation/accuracy=0.645700, validation/loss=1.462598, validation/num_examples=50000
I0206 04:44:21.406442 139946397853440 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.0905802249908447, loss=2.2483279705047607
I0206 04:45:04.236943 139946414638848 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.072892665863037, loss=2.205083131790161
I0206 04:45:51.446703 139946397853440 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.9486051797866821, loss=2.126436233520508
I0206 04:46:38.329128 139946414638848 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.785506248474121, loss=4.060932159423828
I0206 04:47:24.999168 139946397853440 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.490499258041382, loss=2.302330255508423
I0206 04:48:11.719696 139946414638848 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.1561362743377686, loss=2.130629777908325
I0206 04:48:58.509723 139946397853440 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.2591636180877686, loss=2.337672472000122
I0206 04:49:45.211953 139946414638848 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.3224048614501953, loss=2.335036039352417
I0206 04:50:31.968501 139946397853440 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.9371626377105713, loss=3.074495792388916
I0206 04:51:15.156252 140107197974336 spec.py:321] Evaluating on the training split.
I0206 04:51:25.957073 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 04:51:58.974474 140107197974336 spec.py:349] Evaluating on the test split.
I0206 04:52:00.583544 140107197974336 submission_runner.py:408] Time since start: 51538.48s, 	Step: 100594, 	{'train/accuracy': 0.7018945217132568, 'train/loss': 1.217579960823059, 'validation/accuracy': 0.642799973487854, 'validation/loss': 1.4809935092926025, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.107346296310425, 'test/num_examples': 10000, 'score': 46250.72084593773, 'total_duration': 51538.478660821915, 'accumulated_submission_time': 46250.72084593773, 'accumulated_eval_time': 5277.1856644153595, 'accumulated_logging_time': 4.834028720855713}
I0206 04:52:00.623126 139946414638848 logging_writer.py:48] [100594] accumulated_eval_time=5277.185664, accumulated_logging_time=4.834029, accumulated_submission_time=46250.720846, global_step=100594, preemption_count=0, score=46250.720846, test/accuracy=0.517600, test/loss=2.107346, test/num_examples=10000, total_duration=51538.478661, train/accuracy=0.701895, train/loss=1.217580, validation/accuracy=0.642800, validation/loss=1.480994, validation/num_examples=50000
I0206 04:52:03.387985 139946397853440 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.3867714405059814, loss=2.174896717071533
I0206 04:52:45.992689 139946414638848 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.3345282077789307, loss=2.3708763122558594
I0206 04:53:32.353244 139946397853440 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.8527835607528687, loss=4.652047157287598
I0206 04:54:19.265081 139946414638848 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.156022787094116, loss=2.317974328994751
I0206 04:55:05.958121 139946397853440 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.224567174911499, loss=2.103991746902466
I0206 04:55:52.686406 139946414638848 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.01035213470459, loss=2.946002960205078
I0206 04:56:39.037840 139946397853440 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.0728657245635986, loss=2.6553139686584473
I0206 04:57:25.467386 139946414638848 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.9579401016235352, loss=3.9761035442352295
I0206 04:58:11.548357 139946397853440 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.3010878562927246, loss=2.337411642074585
I0206 04:58:57.848610 139946414638848 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.2063393592834473, loss=2.340593099594116
I0206 04:59:00.784975 140107197974336 spec.py:321] Evaluating on the training split.
I0206 04:59:12.004550 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 04:59:49.001927 140107197974336 spec.py:349] Evaluating on the test split.
I0206 04:59:50.598126 140107197974336 submission_runner.py:408] Time since start: 52008.49s, 	Step: 101508, 	{'train/accuracy': 0.6890038847923279, 'train/loss': 1.2767558097839355, 'validation/accuracy': 0.643839955329895, 'validation/loss': 1.4891250133514404, 'validation/num_examples': 50000, 'test/accuracy': 0.5198000073432922, 'test/loss': 2.138503074645996, 'test/num_examples': 10000, 'score': 46670.82105779648, 'total_duration': 52008.493248701096, 'accumulated_submission_time': 46670.82105779648, 'accumulated_eval_time': 5326.998807668686, 'accumulated_logging_time': 4.8833067417144775}
I0206 04:59:50.634318 139946397853440 logging_writer.py:48] [101508] accumulated_eval_time=5326.998808, accumulated_logging_time=4.883307, accumulated_submission_time=46670.821058, global_step=101508, preemption_count=0, score=46670.821058, test/accuracy=0.519800, test/loss=2.138503, test/num_examples=10000, total_duration=52008.493249, train/accuracy=0.689004, train/loss=1.276756, validation/accuracy=0.643840, validation/loss=1.489125, validation/num_examples=50000
I0206 05:00:28.925252 139946414638848 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.2348287105560303, loss=2.3965492248535156
I0206 05:01:15.244847 139946397853440 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.1788859367370605, loss=2.203266143798828
I0206 05:02:01.595956 139946414638848 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.1832659244537354, loss=2.5195298194885254
I0206 05:02:47.875299 139946397853440 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.1101644039154053, loss=2.406248092651367
I0206 05:03:34.358869 139946414638848 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.5744423866271973, loss=2.2514448165893555
I0206 05:04:20.768189 139946397853440 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.9838662147521973, loss=3.9577159881591797
I0206 05:05:07.235346 139946414638848 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.093229055404663, loss=3.067920446395874
I0206 05:05:53.709456 139946397853440 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.414170026779175, loss=2.220456600189209
I0206 05:06:40.263330 139946414638848 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.928354024887085, loss=3.898003101348877
I0206 05:06:51.045126 140107197974336 spec.py:321] Evaluating on the training split.
I0206 05:07:01.956219 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 05:07:38.709279 140107197974336 spec.py:349] Evaluating on the test split.
I0206 05:07:40.305915 140107197974336 submission_runner.py:408] Time since start: 52478.20s, 	Step: 102425, 	{'train/accuracy': 0.7002929449081421, 'train/loss': 1.2077990770339966, 'validation/accuracy': 0.6493200063705444, 'validation/loss': 1.445142149925232, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.082939863204956, 'test/num_examples': 10000, 'score': 47091.16972374916, 'total_duration': 52478.20104813576, 'accumulated_submission_time': 47091.16972374916, 'accumulated_eval_time': 5376.259582996368, 'accumulated_logging_time': 4.928251028060913}
I0206 05:07:40.336817 139946397853440 logging_writer.py:48] [102425] accumulated_eval_time=5376.259583, accumulated_logging_time=4.928251, accumulated_submission_time=47091.169724, global_step=102425, preemption_count=0, score=47091.169724, test/accuracy=0.529700, test/loss=2.082940, test/num_examples=10000, total_duration=52478.201048, train/accuracy=0.700293, train/loss=1.207799, validation/accuracy=0.649320, validation/loss=1.445142, validation/num_examples=50000
I0206 05:08:11.102526 139946414638848 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.3079776763916016, loss=2.4229869842529297
I0206 05:08:56.792743 139946397853440 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.2382450103759766, loss=2.1556529998779297
I0206 05:09:43.299001 139946414638848 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.0153517723083496, loss=3.096020221710205
I0206 05:10:29.784827 139946397853440 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.2193517684936523, loss=2.209303140640259
I0206 05:11:16.201225 139946414638848 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.176924467086792, loss=2.275444984436035
I0206 05:12:02.699599 139946397853440 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.1950364112854004, loss=2.117736577987671
I0206 05:12:49.253396 139946414638848 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.1199817657470703, loss=2.2170300483703613
I0206 05:13:35.596671 139946397853440 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.9479005336761475, loss=3.7314505577087402
I0206 05:14:22.120395 139946414638848 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.9487794637680054, loss=4.086759090423584
I0206 05:14:40.402208 140107197974336 spec.py:321] Evaluating on the training split.
I0206 05:14:51.242560 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 05:15:26.860489 140107197974336 spec.py:349] Evaluating on the test split.
I0206 05:15:28.464668 140107197974336 submission_runner.py:408] Time since start: 52946.36s, 	Step: 103341, 	{'train/accuracy': 0.7048437595367432, 'train/loss': 1.213708758354187, 'validation/accuracy': 0.647599995136261, 'validation/loss': 1.473738670349121, 'validation/num_examples': 50000, 'test/accuracy': 0.5266000032424927, 'test/loss': 2.1160645484924316, 'test/num_examples': 10000, 'score': 47511.17261624336, 'total_duration': 52946.35978603363, 'accumulated_submission_time': 47511.17261624336, 'accumulated_eval_time': 5424.322031259537, 'accumulated_logging_time': 4.9686126708984375}
I0206 05:15:28.506135 139946397853440 logging_writer.py:48] [103341] accumulated_eval_time=5424.322031, accumulated_logging_time=4.968613, accumulated_submission_time=47511.172616, global_step=103341, preemption_count=0, score=47511.172616, test/accuracy=0.526600, test/loss=2.116065, test/num_examples=10000, total_duration=52946.359786, train/accuracy=0.704844, train/loss=1.213709, validation/accuracy=0.647600, validation/loss=1.473739, validation/num_examples=50000
I0206 05:15:52.087036 139946414638848 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.986197590827942, loss=3.8224480152130127
I0206 05:16:37.954907 139946397853440 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.18310284614563, loss=4.005051612854004
I0206 05:17:24.678893 139946414638848 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.2519869804382324, loss=2.229294776916504
I0206 05:18:11.232401 139946397853440 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.228477954864502, loss=2.677638292312622
I0206 05:18:57.514972 139946414638848 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.2041025161743164, loss=2.199779987335205
I0206 05:19:43.959201 139946397853440 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.094804286956787, loss=3.023362636566162
I0206 05:20:30.077554 139946414638848 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.4310553073883057, loss=2.2144618034362793
I0206 05:21:16.428416 139946397853440 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.3155581951141357, loss=2.570559501647949
I0206 05:22:02.888328 139946414638848 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.1932806968688965, loss=2.1851541996002197
I0206 05:22:28.843516 140107197974336 spec.py:321] Evaluating on the training split.
I0206 05:22:39.637365 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 05:23:16.846834 140107197974336 spec.py:349] Evaluating on the test split.
I0206 05:23:18.439451 140107197974336 submission_runner.py:408] Time since start: 53416.33s, 	Step: 104258, 	{'train/accuracy': 0.6996484398841858, 'train/loss': 1.24612295627594, 'validation/accuracy': 0.6481599807739258, 'validation/loss': 1.4764598608016968, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.1275715827941895, 'test/num_examples': 10000, 'score': 47931.445341825485, 'total_duration': 53416.33458185196, 'accumulated_submission_time': 47931.445341825485, 'accumulated_eval_time': 5473.918003559113, 'accumulated_logging_time': 5.021668195724487}
I0206 05:23:18.472203 139946397853440 logging_writer.py:48] [104258] accumulated_eval_time=5473.918004, accumulated_logging_time=5.021668, accumulated_submission_time=47931.445342, global_step=104258, preemption_count=0, score=47931.445342, test/accuracy=0.523800, test/loss=2.127572, test/num_examples=10000, total_duration=53416.334582, train/accuracy=0.699648, train/loss=1.246123, validation/accuracy=0.648160, validation/loss=1.476460, validation/num_examples=50000
I0206 05:23:35.393847 139946414638848 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.4339241981506348, loss=4.757620334625244
I0206 05:24:19.820238 139946397853440 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.372868061065674, loss=2.308864116668701
I0206 05:25:06.847806 139946414638848 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.184523344039917, loss=2.2289650440216064
I0206 05:25:53.368195 139946397853440 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.4586257934570312, loss=2.134490489959717
I0206 05:26:40.096638 139946414638848 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.292515516281128, loss=2.1765012741088867
I0206 05:27:26.758829 139946397853440 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.3005170822143555, loss=2.4431235790252686
I0206 05:28:13.474602 139946414638848 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.669801712036133, loss=2.1712000370025635
I0206 05:28:59.919423 139946397853440 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.900146484375, loss=3.6621055603027344
I0206 05:29:46.532406 139946414638848 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.479721784591675, loss=2.2567949295043945
I0206 05:30:18.481322 140107197974336 spec.py:321] Evaluating on the training split.
I0206 05:30:29.466814 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 05:31:03.286984 140107197974336 spec.py:349] Evaluating on the test split.
I0206 05:31:04.889194 140107197974336 submission_runner.py:408] Time since start: 53882.78s, 	Step: 105170, 	{'train/accuracy': 0.701464831829071, 'train/loss': 1.2353618144989014, 'validation/accuracy': 0.6521399617195129, 'validation/loss': 1.4591549634933472, 'validation/num_examples': 50000, 'test/accuracy': 0.5285000205039978, 'test/loss': 2.1071197986602783, 'test/num_examples': 10000, 'score': 48351.39259338379, 'total_duration': 53882.78431844711, 'accumulated_submission_time': 48351.39259338379, 'accumulated_eval_time': 5520.325870513916, 'accumulated_logging_time': 5.064110040664673}
I0206 05:31:04.922302 139946397853440 logging_writer.py:48] [105170] accumulated_eval_time=5520.325871, accumulated_logging_time=5.064110, accumulated_submission_time=48351.392593, global_step=105170, preemption_count=0, score=48351.392593, test/accuracy=0.528500, test/loss=2.107120, test/num_examples=10000, total_duration=53882.784318, train/accuracy=0.701465, train/loss=1.235362, validation/accuracy=0.652140, validation/loss=1.459155, validation/num_examples=50000
I0206 05:31:17.100304 139946414638848 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.155663013458252, loss=2.2129969596862793
I0206 05:32:00.769973 139946397853440 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.408339262008667, loss=2.169170618057251
I0206 05:32:47.338137 139946414638848 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.251095771789551, loss=2.1491000652313232
I0206 05:33:34.074816 139946397853440 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.510434150695801, loss=2.4071614742279053
I0206 05:34:20.769074 139946414638848 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.4025158882141113, loss=2.096376895904541
I0206 05:35:07.707895 139946397853440 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.294400453567505, loss=2.119917631149292
I0206 05:35:54.194403 139946414638848 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.416158676147461, loss=2.143059492111206
I0206 05:36:40.664262 139946397853440 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.4529573917388916, loss=2.2218384742736816
I0206 05:37:27.517879 139946414638848 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.868448257446289, loss=3.175537586212158
I0206 05:38:05.087845 140107197974336 spec.py:321] Evaluating on the training split.
I0206 05:38:15.954293 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 05:38:50.084167 140107197974336 spec.py:349] Evaluating on the test split.
I0206 05:38:51.674681 140107197974336 submission_runner.py:408] Time since start: 54349.57s, 	Step: 106082, 	{'train/accuracy': 0.708984375, 'train/loss': 1.166994333267212, 'validation/accuracy': 0.6547200083732605, 'validation/loss': 1.4128574132919312, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.0650956630706787, 'test/num_examples': 10000, 'score': 48771.495816230774, 'total_duration': 54349.56981277466, 'accumulated_submission_time': 48771.495816230774, 'accumulated_eval_time': 5566.9127151966095, 'accumulated_logging_time': 5.108115911483765}
I0206 05:38:51.708296 139946397853440 logging_writer.py:48] [106082] accumulated_eval_time=5566.912715, accumulated_logging_time=5.108116, accumulated_submission_time=48771.495816, global_step=106082, preemption_count=0, score=48771.495816, test/accuracy=0.533900, test/loss=2.065096, test/num_examples=10000, total_duration=54349.569813, train/accuracy=0.708984, train/loss=1.166994, validation/accuracy=0.654720, validation/loss=1.412857, validation/num_examples=50000
I0206 05:38:59.172172 139946414638848 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.705660104751587, loss=2.3337438106536865
I0206 05:39:42.335980 139946397853440 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.317548990249634, loss=2.234832763671875
I0206 05:40:28.809480 139946414638848 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.9492542743682861, loss=4.537572860717773
I0206 05:41:15.603030 139946397853440 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.2685577869415283, loss=2.090921401977539
I0206 05:42:01.925378 139946414638848 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.3034286499023438, loss=2.1093647480010986
I0206 05:42:48.748435 139946397853440 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.299664258956909, loss=2.201841115951538
I0206 05:43:35.282155 139946414638848 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.3668696880340576, loss=2.0562148094177246
I0206 05:44:21.770460 139946397853440 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.137171983718872, loss=4.470998287200928
I0206 05:45:08.594918 139946414638848 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.5364837646484375, loss=2.4216508865356445
I0206 05:45:51.709716 140107197974336 spec.py:321] Evaluating on the training split.
I0206 05:46:02.856758 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 05:46:40.115279 140107197974336 spec.py:349] Evaluating on the test split.
I0206 05:46:41.717510 140107197974336 submission_runner.py:408] Time since start: 54819.61s, 	Step: 106995, 	{'train/accuracy': 0.7225390672683716, 'train/loss': 1.131512999534607, 'validation/accuracy': 0.6502199769020081, 'validation/loss': 1.4502038955688477, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.100848436355591, 'test/num_examples': 10000, 'score': 49191.43444299698, 'total_duration': 54819.61263132095, 'accumulated_submission_time': 49191.43444299698, 'accumulated_eval_time': 5616.92050743103, 'accumulated_logging_time': 5.1528544425964355}
I0206 05:46:41.751729 139946397853440 logging_writer.py:48] [106995] accumulated_eval_time=5616.920507, accumulated_logging_time=5.152854, accumulated_submission_time=49191.434443, global_step=106995, preemption_count=0, score=49191.434443, test/accuracy=0.524200, test/loss=2.100848, test/num_examples=10000, total_duration=54819.612631, train/accuracy=0.722539, train/loss=1.131513, validation/accuracy=0.650220, validation/loss=1.450204, validation/num_examples=50000
I0206 05:46:44.106485 139946414638848 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.189148426055908, loss=2.5295305252075195
I0206 05:47:26.523557 139946397853440 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.981555461883545, loss=4.174482345581055
I0206 05:48:12.513324 139946414638848 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.0642549991607666, loss=2.72271728515625
I0206 05:48:59.065743 139946397853440 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.2355329990386963, loss=2.592071056365967
I0206 05:49:45.432180 139946414638848 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.580110549926758, loss=2.2399230003356934
I0206 05:50:32.000958 139946397853440 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.2358100414276123, loss=4.073598861694336
I0206 05:51:18.253649 139946414638848 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.5119171142578125, loss=2.181884765625
I0206 05:52:04.561409 139946397853440 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.4105796813964844, loss=2.024712562561035
I0206 05:52:50.684588 139946414638848 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.315037250518799, loss=2.482038736343384
I0206 05:53:37.368885 139946397853440 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.417479991912842, loss=2.2049148082733154
I0206 05:53:42.139776 140107197974336 spec.py:321] Evaluating on the training split.
I0206 05:53:53.054543 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 05:54:31.993034 140107197974336 spec.py:349] Evaluating on the test split.
I0206 05:54:33.604338 140107197974336 submission_runner.py:408] Time since start: 55291.50s, 	Step: 107912, 	{'train/accuracy': 0.7108789086341858, 'train/loss': 1.1651406288146973, 'validation/accuracy': 0.6574999690055847, 'validation/loss': 1.407038688659668, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.0478579998016357, 'test/num_examples': 10000, 'score': 49611.7597925663, 'total_duration': 55291.499467134476, 'accumulated_submission_time': 49611.7597925663, 'accumulated_eval_time': 5668.385055780411, 'accumulated_logging_time': 5.196820497512817}
I0206 05:54:33.639292 139946414638848 logging_writer.py:48] [107912] accumulated_eval_time=5668.385056, accumulated_logging_time=5.196820, accumulated_submission_time=49611.759793, global_step=107912, preemption_count=0, score=49611.759793, test/accuracy=0.533900, test/loss=2.047858, test/num_examples=10000, total_duration=55291.499467, train/accuracy=0.710879, train/loss=1.165141, validation/accuracy=0.657500, validation/loss=1.407039, validation/num_examples=50000
I0206 05:55:10.392274 139946397853440 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.2662670612335205, loss=4.736387252807617
I0206 05:55:56.478354 139946414638848 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.758833885192871, loss=2.2234280109405518
I0206 05:56:43.440151 139946397853440 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.3290717601776123, loss=3.7390546798706055
I0206 05:57:29.787937 139946414638848 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.2522993087768555, loss=2.210200071334839
I0206 05:58:16.376757 139946397853440 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.128788709640503, loss=4.444676399230957
I0206 05:59:02.724817 139946414638848 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.1943180561065674, loss=3.0862414836883545
I0206 05:59:49.086433 139946397853440 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.624971389770508, loss=2.35369610786438
I0206 06:00:35.659600 139946414638848 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.415046215057373, loss=2.191533088684082
I0206 06:01:22.250488 139946397853440 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.2179434299468994, loss=3.5934526920318604
I0206 06:01:34.052141 140107197974336 spec.py:321] Evaluating on the training split.
I0206 06:01:45.009689 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 06:02:21.888143 140107197974336 spec.py:349] Evaluating on the test split.
I0206 06:02:23.474572 140107197974336 submission_runner.py:408] Time since start: 55761.37s, 	Step: 108827, 	{'train/accuracy': 0.7103906273841858, 'train/loss': 1.18244469165802, 'validation/accuracy': 0.6546799540519714, 'validation/loss': 1.4344879388809204, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.086609125137329, 'test/num_examples': 10000, 'score': 50032.11195850372, 'total_duration': 55761.369703531265, 'accumulated_submission_time': 50032.11195850372, 'accumulated_eval_time': 5717.807471752167, 'accumulated_logging_time': 5.240591287612915}
I0206 06:02:23.511789 139946414638848 logging_writer.py:48] [108827] accumulated_eval_time=5717.807472, accumulated_logging_time=5.240591, accumulated_submission_time=50032.111959, global_step=108827, preemption_count=0, score=50032.111959, test/accuracy=0.529300, test/loss=2.086609, test/num_examples=10000, total_duration=55761.369704, train/accuracy=0.710391, train/loss=1.182445, validation/accuracy=0.654680, validation/loss=1.434488, validation/num_examples=50000
I0206 06:02:53.278185 139946397853440 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.3954732418060303, loss=2.2570648193359375
I0206 06:03:39.609321 139946414638848 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.6577742099761963, loss=2.2778196334838867
I0206 06:04:26.010290 139946397853440 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.1891427040100098, loss=4.7081828117370605
I0206 06:05:12.776355 139946414638848 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.9370841979980469, loss=3.767641544342041
I0206 06:05:59.081511 139946397853440 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.470118999481201, loss=2.267294406890869
I0206 06:06:45.406926 139946414638848 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.38989520072937, loss=2.0701067447662354
I0206 06:07:32.067187 139946397853440 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.319174289703369, loss=2.195873498916626
I0206 06:08:18.647796 139946414638848 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.9158220291137695, loss=3.445199489593506
I0206 06:09:05.223560 139946397853440 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.3854455947875977, loss=2.508877754211426
I0206 06:09:23.753730 140107197974336 spec.py:321] Evaluating on the training split.
I0206 06:09:34.905448 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 06:10:09.948925 140107197974336 spec.py:349] Evaluating on the test split.
I0206 06:10:11.541995 140107197974336 submission_runner.py:408] Time since start: 56229.44s, 	Step: 109742, 	{'train/accuracy': 0.7229882478713989, 'train/loss': 1.1345579624176025, 'validation/accuracy': 0.6586799621582031, 'validation/loss': 1.426134467124939, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.0645999908447266, 'test/num_examples': 10000, 'score': 50452.293276786804, 'total_duration': 56229.43710780144, 'accumulated_submission_time': 50452.293276786804, 'accumulated_eval_time': 5765.595708608627, 'accumulated_logging_time': 5.286548137664795}
I0206 06:10:11.583194 139946414638848 logging_writer.py:48] [109742] accumulated_eval_time=5765.595709, accumulated_logging_time=5.286548, accumulated_submission_time=50452.293277, global_step=109742, preemption_count=0, score=50452.293277, test/accuracy=0.537700, test/loss=2.064600, test/num_examples=10000, total_duration=56229.437108, train/accuracy=0.722988, train/loss=1.134558, validation/accuracy=0.658680, validation/loss=1.426134, validation/num_examples=50000
I0206 06:10:34.755888 139946397853440 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.0654666423797607, loss=4.204120635986328
I0206 06:11:20.795660 139946414638848 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.5248935222625732, loss=2.1750247478485107
I0206 06:12:07.433224 139946397853440 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.473879814147949, loss=2.1118721961975098
I0206 06:12:53.744111 139946414638848 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.4416232109069824, loss=2.244328260421753
I0206 06:13:40.027176 139946397853440 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.3402233123779297, loss=4.359955310821533
I0206 06:14:26.431442 139946414638848 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.1968605518341064, loss=3.146571159362793
I0206 06:15:13.082836 139946397853440 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.033808946609497, loss=4.1332879066467285
I0206 06:15:59.350597 139946414638848 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.405372142791748, loss=2.1457929611206055
I0206 06:16:45.769693 139946397853440 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.947594404220581, loss=4.115545749664307
I0206 06:17:11.872352 140107197974336 spec.py:321] Evaluating on the training split.
I0206 06:17:22.729620 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 06:17:56.094106 140107197974336 spec.py:349] Evaluating on the test split.
I0206 06:17:57.694487 140107197974336 submission_runner.py:408] Time since start: 56695.59s, 	Step: 110658, 	{'train/accuracy': 0.7133398056030273, 'train/loss': 1.1982223987579346, 'validation/accuracy': 0.6570599675178528, 'validation/loss': 1.440184235572815, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.095458984375, 'test/num_examples': 10000, 'score': 50872.51973128319, 'total_duration': 56695.58960843086, 'accumulated_submission_time': 50872.51973128319, 'accumulated_eval_time': 5811.417835235596, 'accumulated_logging_time': 5.338428258895874}
I0206 06:17:57.731869 139946414638848 logging_writer.py:48] [110658] accumulated_eval_time=5811.417835, accumulated_logging_time=5.338428, accumulated_submission_time=50872.519731, global_step=110658, preemption_count=0, score=50872.519731, test/accuracy=0.534100, test/loss=2.095459, test/num_examples=10000, total_duration=56695.589608, train/accuracy=0.713340, train/loss=1.198222, validation/accuracy=0.657060, validation/loss=1.440184, validation/num_examples=50000
I0206 06:18:14.631724 139946397853440 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.265444755554199, loss=2.853127956390381
I0206 06:18:59.306106 139946414638848 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.2374823093414307, loss=4.175243377685547
I0206 06:19:45.629852 139946397853440 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.1459691524505615, loss=3.864717483520508
I0206 06:20:32.287519 139946414638848 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.3012571334838867, loss=2.2386229038238525
I0206 06:21:18.692677 139946397853440 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.1775245666503906, loss=3.0003573894500732
I0206 06:22:05.005217 139946414638848 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.18459415435791, loss=2.9830615520477295
I0206 06:22:51.273221 139946397853440 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.157430410385132, loss=3.0904958248138428
I0206 06:23:37.512835 139946414638848 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.04316782951355, loss=4.611893653869629
I0206 06:24:23.744500 139946397853440 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.6396822929382324, loss=2.1304304599761963
I0206 06:24:57.934498 140107197974336 spec.py:321] Evaluating on the training split.
I0206 06:25:08.932267 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 06:25:44.652637 140107197974336 spec.py:349] Evaluating on the test split.
I0206 06:25:46.255372 140107197974336 submission_runner.py:408] Time since start: 57164.15s, 	Step: 111575, 	{'train/accuracy': 0.7201171517372131, 'train/loss': 1.1232796907424927, 'validation/accuracy': 0.6644399762153625, 'validation/loss': 1.3742200136184692, 'validation/num_examples': 50000, 'test/accuracy': 0.5422000288963318, 'test/loss': 2.019754409790039, 'test/num_examples': 10000, 'score': 51292.659793138504, 'total_duration': 57164.1505010128, 'accumulated_submission_time': 51292.659793138504, 'accumulated_eval_time': 5859.738709926605, 'accumulated_logging_time': 5.3858771324157715}
I0206 06:25:46.289857 139946414638848 logging_writer.py:48] [111575] accumulated_eval_time=5859.738710, accumulated_logging_time=5.385877, accumulated_submission_time=51292.659793, global_step=111575, preemption_count=0, score=51292.659793, test/accuracy=0.542200, test/loss=2.019754, test/num_examples=10000, total_duration=57164.150501, train/accuracy=0.720117, train/loss=1.123280, validation/accuracy=0.664440, validation/loss=1.374220, validation/num_examples=50000
I0206 06:25:56.516689 139946397853440 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.2037365436553955, loss=3.0251193046569824
I0206 06:26:39.747894 139946414638848 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.373274564743042, loss=2.249631643295288
I0206 06:27:26.594735 139946397853440 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.3219962120056152, loss=3.8291430473327637
I0206 06:28:13.073073 139946414638848 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.7055814266204834, loss=2.125978469848633
I0206 06:28:59.624647 139946397853440 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.4732866287231445, loss=2.068356990814209
I0206 06:29:46.131507 139946414638848 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.535532236099243, loss=2.0797348022460938
I0206 06:30:32.440808 139946397853440 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.484924554824829, loss=4.785611629486084
I0206 06:31:18.632791 139946414638848 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.1632020473480225, loss=3.1904401779174805
I0206 06:32:05.240211 139946397853440 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.4587230682373047, loss=2.332676410675049
I0206 06:32:46.491311 140107197974336 spec.py:321] Evaluating on the training split.
I0206 06:32:57.260719 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 06:33:29.561449 140107197974336 spec.py:349] Evaluating on the test split.
I0206 06:33:31.159244 140107197974336 submission_runner.py:408] Time since start: 57629.05s, 	Step: 112491, 	{'train/accuracy': 0.7299218773841858, 'train/loss': 1.0928118228912354, 'validation/accuracy': 0.6615599989891052, 'validation/loss': 1.3830986022949219, 'validation/num_examples': 50000, 'test/accuracy': 0.5380000472068787, 'test/loss': 2.0178375244140625, 'test/num_examples': 10000, 'score': 51712.80060315132, 'total_duration': 57629.054327726364, 'accumulated_submission_time': 51712.80060315132, 'accumulated_eval_time': 5904.406593084335, 'accumulated_logging_time': 5.429534912109375}
I0206 06:33:31.197885 139946414638848 logging_writer.py:48] [112491] accumulated_eval_time=5904.406593, accumulated_logging_time=5.429535, accumulated_submission_time=51712.800603, global_step=112491, preemption_count=0, score=51712.800603, test/accuracy=0.538000, test/loss=2.017838, test/num_examples=10000, total_duration=57629.054328, train/accuracy=0.729922, train/loss=1.092812, validation/accuracy=0.661560, validation/loss=1.383099, validation/num_examples=50000
I0206 06:33:35.149248 139946397853440 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.3691611289978027, loss=2.164808988571167
I0206 06:34:17.580442 139946414638848 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.2753710746765137, loss=2.7705562114715576
I0206 06:35:03.789998 139946397853440 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.6631922721862793, loss=2.1050033569335938
I0206 06:35:50.311510 139946414638848 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.3893909454345703, loss=4.383293151855469
I0206 06:36:36.797259 139946397853440 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.963852882385254, loss=2.09417462348938
I0206 06:37:23.238366 139946414638848 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.655067205429077, loss=2.320005416870117
I0206 06:38:09.643510 139946397853440 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.4755666255950928, loss=2.0714073181152344
I0206 06:38:55.862881 139946414638848 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.319669008255005, loss=2.2673933506011963
I0206 06:39:42.246834 139946397853440 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.559452533721924, loss=2.086801052093506
I0206 06:40:28.872426 139946414638848 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.8272342681884766, loss=1.9392237663269043
I0206 06:40:31.312683 140107197974336 spec.py:321] Evaluating on the training split.
I0206 06:40:42.188735 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 06:41:17.821233 140107197974336 spec.py:349] Evaluating on the test split.
I0206 06:41:19.416570 140107197974336 submission_runner.py:408] Time since start: 58097.31s, 	Step: 113407, 	{'train/accuracy': 0.7118163704872131, 'train/loss': 1.1474217176437378, 'validation/accuracy': 0.6642599701881409, 'validation/loss': 1.3698629140853882, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 2.007035255432129, 'test/num_examples': 10000, 'score': 52132.85252594948, 'total_duration': 58097.311703681946, 'accumulated_submission_time': 52132.85252594948, 'accumulated_eval_time': 5952.5104813575745, 'accumulated_logging_time': 5.479061126708984}
I0206 06:41:19.450343 139946397853440 logging_writer.py:48] [113407] accumulated_eval_time=5952.510481, accumulated_logging_time=5.479061, accumulated_submission_time=52132.852526, global_step=113407, preemption_count=0, score=52132.852526, test/accuracy=0.538400, test/loss=2.007035, test/num_examples=10000, total_duration=58097.311704, train/accuracy=0.711816, train/loss=1.147422, validation/accuracy=0.664260, validation/loss=1.369863, validation/num_examples=50000
I0206 06:41:58.483274 139946414638848 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.5831706523895264, loss=2.0436556339263916
I0206 06:42:44.888774 139946397853440 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.796907901763916, loss=2.2506165504455566
I0206 06:43:31.944490 139946414638848 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.4363937377929688, loss=2.185410976409912
I0206 06:44:18.460935 139946397853440 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.2022547721862793, loss=2.790924072265625
I0206 06:45:05.285717 139946414638848 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.553785562515259, loss=2.122307300567627
I0206 06:45:51.691242 139946397853440 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.253769874572754, loss=4.246185779571533
I0206 06:46:38.478375 139946414638848 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.5585174560546875, loss=4.560576438903809
I0206 06:47:25.036729 139946397853440 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.1170156002044678, loss=3.712097406387329
I0206 06:48:11.498305 139946414638848 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.625091314315796, loss=2.1598258018493652
I0206 06:48:19.535397 140107197974336 spec.py:321] Evaluating on the training split.
I0206 06:48:30.510042 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 06:49:03.386965 140107197974336 spec.py:349] Evaluating on the test split.
I0206 06:49:04.990814 140107197974336 submission_runner.py:408] Time since start: 58562.89s, 	Step: 114319, 	{'train/accuracy': 0.7221288681030273, 'train/loss': 1.117608666419983, 'validation/accuracy': 0.6670199632644653, 'validation/loss': 1.3616474866867065, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.0026955604553223, 'test/num_examples': 10000, 'score': 52552.876588106155, 'total_duration': 58562.885924339294, 'accumulated_submission_time': 52552.876588106155, 'accumulated_eval_time': 5997.965879917145, 'accumulated_logging_time': 5.521757125854492}
I0206 06:49:05.027958 139946397853440 logging_writer.py:48] [114319] accumulated_eval_time=5997.965880, accumulated_logging_time=5.521757, accumulated_submission_time=52552.876588, global_step=114319, preemption_count=0, score=52552.876588, test/accuracy=0.543300, test/loss=2.002696, test/num_examples=10000, total_duration=58562.885924, train/accuracy=0.722129, train/loss=1.117609, validation/accuracy=0.667020, validation/loss=1.361647, validation/num_examples=50000
I0206 06:49:38.551131 139946414638848 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.353409767150879, loss=4.5307159423828125
I0206 06:50:24.481206 139946397853440 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.998040199279785, loss=2.1435203552246094
I0206 06:51:10.858194 139946414638848 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.6812527179718018, loss=2.081571578979492
I0206 06:51:57.175029 139946397853440 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.768873929977417, loss=2.1242690086364746
I0206 06:52:43.432876 139946414638848 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.64920711517334, loss=2.166818141937256
I0206 06:53:30.052879 139946397853440 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.513463258743286, loss=2.105149745941162
I0206 06:54:16.510315 139946414638848 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.4478604793548584, loss=3.948413372039795
I0206 06:55:03.054201 139946397853440 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.4976296424865723, loss=2.1300601959228516
I0206 06:55:49.460952 139946414638848 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.4178128242492676, loss=4.141417503356934
I0206 06:56:05.314450 140107197974336 spec.py:321] Evaluating on the training split.
I0206 06:56:16.347344 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 06:56:53.013816 140107197974336 spec.py:349] Evaluating on the test split.
I0206 06:56:54.619207 140107197974336 submission_runner.py:408] Time since start: 59032.51s, 	Step: 115236, 	{'train/accuracy': 0.7292773127555847, 'train/loss': 1.092387318611145, 'validation/accuracy': 0.6671000123023987, 'validation/loss': 1.3630927801132202, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 2.007647752761841, 'test/num_examples': 10000, 'score': 52973.100821495056, 'total_duration': 59032.51432132721, 'accumulated_submission_time': 52973.100821495056, 'accumulated_eval_time': 6047.270622730255, 'accumulated_logging_time': 5.5683817863464355}
I0206 06:56:54.656715 139946397853440 logging_writer.py:48] [115236] accumulated_eval_time=6047.270623, accumulated_logging_time=5.568382, accumulated_submission_time=52973.100821, global_step=115236, preemption_count=0, score=52973.100821, test/accuracy=0.549400, test/loss=2.007648, test/num_examples=10000, total_duration=59032.514321, train/accuracy=0.729277, train/loss=1.092387, validation/accuracy=0.667100, validation/loss=1.363093, validation/num_examples=50000
I0206 06:57:20.283955 139946414638848 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.664863348007202, loss=2.26611328125
I0206 06:58:06.272645 139946397853440 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.30707049369812, loss=2.9631097316741943
I0206 06:58:52.876840 139946414638848 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.3170182704925537, loss=2.598518133163452
I0206 06:59:39.187246 139946397853440 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.240316390991211, loss=3.4755358695983887
I0206 07:00:25.829473 139946414638848 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.504153251647949, loss=2.003404378890991
I0206 07:01:12.433314 139946397853440 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.1885948181152344, loss=3.2718610763549805
I0206 07:01:59.193696 139946414638848 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.200488805770874, loss=3.5343446731567383
I0206 07:02:45.762177 139946397853440 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.5405216217041016, loss=2.095613718032837
I0206 07:03:32.243916 139946414638848 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.410675525665283, loss=3.4741244316101074
I0206 07:03:54.734324 140107197974336 spec.py:321] Evaluating on the training split.
I0206 07:04:05.704174 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 07:04:41.113797 140107197974336 spec.py:349] Evaluating on the test split.
I0206 07:04:42.711301 140107197974336 submission_runner.py:408] Time since start: 59500.61s, 	Step: 116150, 	{'train/accuracy': 0.7247265577316284, 'train/loss': 1.1091890335083008, 'validation/accuracy': 0.6702199578285217, 'validation/loss': 1.354252815246582, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 1.9925272464752197, 'test/num_examples': 10000, 'score': 53393.11542224884, 'total_duration': 59500.606415987015, 'accumulated_submission_time': 53393.11542224884, 'accumulated_eval_time': 6095.24760055542, 'accumulated_logging_time': 5.615738153457642}
I0206 07:04:42.746730 139946397853440 logging_writer.py:48] [116150] accumulated_eval_time=6095.247601, accumulated_logging_time=5.615738, accumulated_submission_time=53393.115422, global_step=116150, preemption_count=0, score=53393.115422, test/accuracy=0.545200, test/loss=1.992527, test/num_examples=10000, total_duration=59500.606416, train/accuracy=0.724727, train/loss=1.109189, validation/accuracy=0.670220, validation/loss=1.354253, validation/num_examples=50000
I0206 07:05:02.772814 139946414638848 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.523853063583374, loss=2.0722267627716064
I0206 07:05:48.222810 139946397853440 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.3961169719696045, loss=2.0493085384368896
I0206 07:06:34.627177 139946414638848 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.4503962993621826, loss=2.093623399734497
I0206 07:07:21.101577 139946397853440 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.663128614425659, loss=2.1203861236572266
I0206 07:08:07.525970 139946414638848 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.2391622066497803, loss=3.135483980178833
I0206 07:08:53.770118 139946397853440 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.7175004482269287, loss=2.1309404373168945
I0206 07:09:40.292052 139946414638848 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.6646039485931396, loss=1.983008623123169
I0206 07:10:26.619564 139946397853440 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.4451193809509277, loss=2.0148820877075195
I0206 07:11:13.028091 139946414638848 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.218290328979492, loss=3.235360622406006
I0206 07:11:42.801455 140107197974336 spec.py:321] Evaluating on the training split.
I0206 07:11:54.013165 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 07:12:28.092953 140107197974336 spec.py:349] Evaluating on the test split.
I0206 07:12:29.693953 140107197974336 submission_runner.py:408] Time since start: 59967.59s, 	Step: 117066, 	{'train/accuracy': 0.7264843583106995, 'train/loss': 1.096244215965271, 'validation/accuracy': 0.672760009765625, 'validation/loss': 1.340212106704712, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 1.99046790599823, 'test/num_examples': 10000, 'score': 53813.10630583763, 'total_duration': 59967.58908033371, 'accumulated_submission_time': 53813.10630583763, 'accumulated_eval_time': 6142.140088558197, 'accumulated_logging_time': 5.661764621734619}
I0206 07:12:29.733848 139946397853440 logging_writer.py:48] [117066] accumulated_eval_time=6142.140089, accumulated_logging_time=5.661765, accumulated_submission_time=53813.106306, global_step=117066, preemption_count=0, score=53813.106306, test/accuracy=0.550800, test/loss=1.990468, test/num_examples=10000, total_duration=59967.589080, train/accuracy=0.726484, train/loss=1.096244, validation/accuracy=0.672760, validation/loss=1.340212, validation/num_examples=50000
I0206 07:12:43.498583 139946414638848 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.7481689453125, loss=4.286808013916016
I0206 07:13:27.512554 139946397853440 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.5013513565063477, loss=4.25577449798584
I0206 07:14:13.880997 139946414638848 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.534165382385254, loss=2.2150166034698486
I0206 07:15:00.597211 139946397853440 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.606804847717285, loss=2.160309076309204
I0206 07:15:47.120571 139946414638848 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.96077036857605, loss=2.040107250213623
I0206 07:16:33.382873 139946397853440 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.540276527404785, loss=4.572179317474365
I0206 07:17:19.704392 139946414638848 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.492581605911255, loss=1.9998000860214233
I0206 07:18:06.221061 139946397853440 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.6826915740966797, loss=2.0122005939483643
I0206 07:18:52.610137 139946414638848 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.2784368991851807, loss=4.470094203948975
I0206 07:19:29.786544 140107197974336 spec.py:321] Evaluating on the training split.
I0206 07:19:41.323837 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 07:20:18.461078 140107197974336 spec.py:349] Evaluating on the test split.
I0206 07:20:20.064402 140107197974336 submission_runner.py:408] Time since start: 60437.96s, 	Step: 117982, 	{'train/accuracy': 0.7369335889816284, 'train/loss': 1.0518841743469238, 'validation/accuracy': 0.6758599877357483, 'validation/loss': 1.3248143196105957, 'validation/num_examples': 50000, 'test/accuracy': 0.5498000383377075, 'test/loss': 1.9690297842025757, 'test/num_examples': 10000, 'score': 54233.0949075222, 'total_duration': 60437.959518909454, 'accumulated_submission_time': 54233.0949075222, 'accumulated_eval_time': 6192.417934656143, 'accumulated_logging_time': 5.713583707809448}
I0206 07:20:20.103260 139946397853440 logging_writer.py:48] [117982] accumulated_eval_time=6192.417935, accumulated_logging_time=5.713584, accumulated_submission_time=54233.094908, global_step=117982, preemption_count=0, score=54233.094908, test/accuracy=0.549800, test/loss=1.969030, test/num_examples=10000, total_duration=60437.959519, train/accuracy=0.736934, train/loss=1.051884, validation/accuracy=0.675860, validation/loss=1.324814, validation/num_examples=50000
I0206 07:20:27.575752 139946414638848 logging_writer.py:48] [118000] global_step=118000, grad_norm=3.184511661529541, loss=2.0576822757720947
I0206 07:21:10.695868 139946397853440 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.637485980987549, loss=2.1608712673187256
I0206 07:21:57.228425 139946414638848 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.6125621795654297, loss=4.773151874542236
I0206 07:22:43.339877 139946397853440 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.2862863540649414, loss=4.229744911193848
I0206 07:23:29.840517 139946414638848 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.550290822982788, loss=2.0501890182495117
I0206 07:24:16.368762 139946397853440 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.812753438949585, loss=2.2862868309020996
I0206 07:25:03.141233 139946414638848 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.739126205444336, loss=1.9595469236373901
I0206 07:25:49.636049 139946397853440 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.7552127838134766, loss=2.3010170459747314
I0206 07:26:36.171755 139946414638848 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.63718581199646, loss=4.49528694152832
I0206 07:27:20.362919 140107197974336 spec.py:321] Evaluating on the training split.
I0206 07:27:31.759337 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 07:28:06.917556 140107197974336 spec.py:349] Evaluating on the test split.
I0206 07:28:08.522659 140107197974336 submission_runner.py:408] Time since start: 60906.42s, 	Step: 118897, 	{'train/accuracy': 0.7423437237739563, 'train/loss': 1.0445845127105713, 'validation/accuracy': 0.6692599654197693, 'validation/loss': 1.3613452911376953, 'validation/num_examples': 50000, 'test/accuracy': 0.5490000247955322, 'test/loss': 2.0090084075927734, 'test/num_examples': 10000, 'score': 54653.28877854347, 'total_duration': 60906.417788267136, 'accumulated_submission_time': 54653.28877854347, 'accumulated_eval_time': 6240.577670812607, 'accumulated_logging_time': 5.76579213142395}
I0206 07:28:08.558287 139946397853440 logging_writer.py:48] [118897] accumulated_eval_time=6240.577671, accumulated_logging_time=5.765792, accumulated_submission_time=54653.288779, global_step=118897, preemption_count=0, score=54653.288779, test/accuracy=0.549000, test/loss=2.009008, test/num_examples=10000, total_duration=60906.417788, train/accuracy=0.742344, train/loss=1.044585, validation/accuracy=0.669260, validation/loss=1.361345, validation/num_examples=50000
I0206 07:28:10.134110 139946414638848 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.400631904602051, loss=2.591343641281128
I0206 07:28:52.055370 139946397853440 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.249098062515259, loss=3.3560800552368164
I0206 07:29:38.212525 139946414638848 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.6749837398529053, loss=2.0051488876342773
I0206 07:30:24.918802 139946397853440 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.644266366958618, loss=2.2337937355041504
I0206 07:31:11.429937 139946414638848 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.7997829914093018, loss=2.3970155715942383
I0206 07:31:57.961403 139946397853440 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.5126705169677734, loss=2.6377546787261963
I0206 07:32:44.482051 139946414638848 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.6091806888580322, loss=1.9880468845367432
I0206 07:33:30.859573 139946397853440 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.4797534942626953, loss=2.829911231994629
I0206 07:34:17.507539 139946414638848 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.498866558074951, loss=4.0533833503723145
I0206 07:35:04.192593 139946397853440 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.7374253273010254, loss=1.9893293380737305
I0206 07:35:08.961184 140107197974336 spec.py:321] Evaluating on the training split.
I0206 07:35:19.749365 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 07:35:56.019603 140107197974336 spec.py:349] Evaluating on the test split.
I0206 07:35:57.611125 140107197974336 submission_runner.py:408] Time since start: 61375.51s, 	Step: 119812, 	{'train/accuracy': 0.7238671779632568, 'train/loss': 1.152235507965088, 'validation/accuracy': 0.6700599789619446, 'validation/loss': 1.3908920288085938, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.030686378479004, 'test/num_examples': 10000, 'score': 55073.62894105911, 'total_duration': 61375.50624871254, 'accumulated_submission_time': 55073.62894105911, 'accumulated_eval_time': 6289.227605819702, 'accumulated_logging_time': 5.812516689300537}
I0206 07:35:57.645866 139946414638848 logging_writer.py:48] [119812] accumulated_eval_time=6289.227606, accumulated_logging_time=5.812517, accumulated_submission_time=55073.628941, global_step=119812, preemption_count=0, score=55073.628941, test/accuracy=0.548700, test/loss=2.030686, test/num_examples=10000, total_duration=61375.506249, train/accuracy=0.723867, train/loss=1.152236, validation/accuracy=0.670060, validation/loss=1.390892, validation/num_examples=50000
I0206 07:36:34.501711 139946397853440 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.656170129776001, loss=2.0953922271728516
I0206 07:37:21.052895 139946414638848 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.450634002685547, loss=1.9101731777191162
I0206 07:38:07.808417 139946397853440 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.3416903018951416, loss=3.231855869293213
I0206 07:38:54.239569 139946414638848 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.6904702186584473, loss=2.1391806602478027
I0206 07:39:40.667120 139946397853440 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.3821747303009033, loss=3.859333038330078
I0206 07:40:27.123463 139946414638848 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.536712884902954, loss=2.978677272796631
I0206 07:41:13.632018 139946397853440 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.085411787033081, loss=2.0275931358337402
I0206 07:41:59.950141 139946414638848 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.473073959350586, loss=3.0335443019866943
I0206 07:42:46.273271 139946397853440 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.832566261291504, loss=2.0615878105163574
I0206 07:42:57.922369 140107197974336 spec.py:321] Evaluating on the training split.
I0206 07:43:09.109306 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 07:43:45.841582 140107197974336 spec.py:349] Evaluating on the test split.
I0206 07:43:47.447426 140107197974336 submission_runner.py:408] Time since start: 61845.34s, 	Step: 120727, 	{'train/accuracy': 0.7369335889816284, 'train/loss': 1.0716278553009033, 'validation/accuracy': 0.6764400005340576, 'validation/loss': 1.331760287284851, 'validation/num_examples': 50000, 'test/accuracy': 0.5505000352859497, 'test/loss': 1.9711410999298096, 'test/num_examples': 10000, 'score': 55493.843186855316, 'total_duration': 61845.34255743027, 'accumulated_submission_time': 55493.843186855316, 'accumulated_eval_time': 6338.752652645111, 'accumulated_logging_time': 5.857455015182495}
I0206 07:43:47.484700 139946414638848 logging_writer.py:48] [120727] accumulated_eval_time=6338.752653, accumulated_logging_time=5.857455, accumulated_submission_time=55493.843187, global_step=120727, preemption_count=0, score=55493.843187, test/accuracy=0.550500, test/loss=1.971141, test/num_examples=10000, total_duration=61845.342557, train/accuracy=0.736934, train/loss=1.071628, validation/accuracy=0.676440, validation/loss=1.331760, validation/num_examples=50000
I0206 07:44:17.296861 139946397853440 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.378537178039551, loss=3.176981210708618
I0206 07:45:03.545581 139946414638848 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.54050612449646, loss=4.60130500793457
I0206 07:45:49.970855 139946397853440 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.954016923904419, loss=2.0535712242126465
I0206 07:46:36.369743 139946414638848 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.6052653789520264, loss=2.1811578273773193
I0206 07:47:22.885195 139946397853440 logging_writer.py:48] [121200] global_step=121200, grad_norm=3.1257872581481934, loss=2.0931363105773926
I0206 07:48:09.445187 139946414638848 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.758159875869751, loss=3.055454969406128
I0206 07:48:55.648873 139946397853440 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.7775347232818604, loss=2.070084571838379
I0206 07:49:41.871014 139946414638848 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.4736194610595703, loss=2.2048845291137695
I0206 07:50:28.243603 139946397853440 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.4231300354003906, loss=3.485088348388672
I0206 07:50:47.781044 140107197974336 spec.py:321] Evaluating on the training split.
I0206 07:50:58.702437 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 07:51:35.516396 140107197974336 spec.py:349] Evaluating on the test split.
I0206 07:51:37.111033 140107197974336 submission_runner.py:408] Time since start: 62315.01s, 	Step: 121644, 	{'train/accuracy': 0.746386706829071, 'train/loss': 1.0214617252349854, 'validation/accuracy': 0.6764199733734131, 'validation/loss': 1.3321937322616577, 'validation/num_examples': 50000, 'test/accuracy': 0.5493000149726868, 'test/loss': 1.9808764457702637, 'test/num_examples': 10000, 'score': 55914.077816963196, 'total_duration': 62315.006165504456, 'accumulated_submission_time': 55914.077816963196, 'accumulated_eval_time': 6388.082646846771, 'accumulated_logging_time': 5.904085636138916}
I0206 07:51:37.146374 139946414638848 logging_writer.py:48] [121644] accumulated_eval_time=6388.082647, accumulated_logging_time=5.904086, accumulated_submission_time=55914.077817, global_step=121644, preemption_count=0, score=55914.077817, test/accuracy=0.549300, test/loss=1.980876, test/num_examples=10000, total_duration=62315.006166, train/accuracy=0.746387, train/loss=1.021462, validation/accuracy=0.676420, validation/loss=1.332194, validation/num_examples=50000
I0206 07:51:59.779520 139946397853440 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.8581271171569824, loss=1.9951251745224
I0206 07:52:45.245131 139946414638848 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.8128559589385986, loss=2.0878493785858154
I0206 07:53:31.869025 139946397853440 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.7155425548553467, loss=3.946683406829834
I0206 07:54:18.484326 139946414638848 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.571734666824341, loss=4.513306617736816
I0206 07:55:05.219063 139946397853440 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.6980814933776855, loss=1.9864176511764526
I0206 07:55:51.490565 139946414638848 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.4556615352630615, loss=2.514207601547241
I0206 07:56:38.038789 139946397853440 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.758747100830078, loss=2.0744473934173584
I0206 07:57:24.600285 139946414638848 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.509000778198242, loss=2.454526662826538
I0206 07:58:10.871710 139946397853440 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.2128114700317383, loss=2.0597100257873535
I0206 07:58:37.530692 140107197974336 spec.py:321] Evaluating on the training split.
I0206 07:58:48.433986 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 07:59:26.929261 140107197974336 spec.py:349] Evaluating on the test split.
I0206 07:59:28.528637 140107197974336 submission_runner.py:408] Time since start: 62786.42s, 	Step: 122559, 	{'train/accuracy': 0.7341406345367432, 'train/loss': 1.0811148881912231, 'validation/accuracy': 0.6796999573707581, 'validation/loss': 1.328904390335083, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.9604276418685913, 'test/num_examples': 10000, 'score': 56334.39921450615, 'total_duration': 62786.4237511158, 'accumulated_submission_time': 56334.39921450615, 'accumulated_eval_time': 6439.080575942993, 'accumulated_logging_time': 5.9502387046813965}
I0206 07:59:28.572758 139946414638848 logging_writer.py:48] [122559] accumulated_eval_time=6439.080576, accumulated_logging_time=5.950239, accumulated_submission_time=56334.399215, global_step=122559, preemption_count=0, score=56334.399215, test/accuracy=0.555000, test/loss=1.960428, test/num_examples=10000, total_duration=62786.423751, train/accuracy=0.734141, train/loss=1.081115, validation/accuracy=0.679700, validation/loss=1.328904, validation/num_examples=50000
I0206 07:59:45.074528 139946397853440 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.7583820819854736, loss=2.0947062969207764
I0206 08:00:29.700260 139946414638848 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.027587413787842, loss=2.0025031566619873
I0206 08:01:15.822388 139946397853440 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.755093574523926, loss=2.088294267654419
I0206 08:02:02.757941 139946414638848 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.6329398155212402, loss=2.549428939819336
I0206 08:02:49.005254 139946397853440 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.7894415855407715, loss=2.561391592025757
I0206 08:03:35.474159 139946414638848 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.6902644634246826, loss=2.483250617980957
I0206 08:04:22.013652 139946397853440 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.242600917816162, loss=1.9906938076019287
I0206 08:05:08.776004 139946414638848 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.5357820987701416, loss=2.0404481887817383
I0206 08:05:55.435804 139946397853440 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.801553249359131, loss=1.9269154071807861
I0206 08:06:28.576742 140107197974336 spec.py:321] Evaluating on the training split.
I0206 08:06:39.540203 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 08:07:16.269628 140107197974336 spec.py:349] Evaluating on the test split.
I0206 08:07:17.865312 140107197974336 submission_runner.py:408] Time since start: 63255.76s, 	Step: 123473, 	{'train/accuracy': 0.7335742115974426, 'train/loss': 1.078386902809143, 'validation/accuracy': 0.6755599975585938, 'validation/loss': 1.3359389305114746, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 1.9859886169433594, 'test/num_examples': 10000, 'score': 56754.340900182724, 'total_duration': 63255.76044034958, 'accumulated_submission_time': 56754.340900182724, 'accumulated_eval_time': 6488.3691465854645, 'accumulated_logging_time': 6.0046210289001465}
I0206 08:07:17.902590 139946414638848 logging_writer.py:48] [123473] accumulated_eval_time=6488.369147, accumulated_logging_time=6.004621, accumulated_submission_time=56754.340900, global_step=123473, preemption_count=0, score=56754.340900, test/accuracy=0.552300, test/loss=1.985989, test/num_examples=10000, total_duration=63255.760440, train/accuracy=0.733574, train/loss=1.078387, validation/accuracy=0.675560, validation/loss=1.335939, validation/num_examples=50000
I0206 08:07:28.897728 139946397853440 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.6763696670532227, loss=2.018134117126465
I0206 08:08:12.607398 139946414638848 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.741173267364502, loss=2.2271435260772705
I0206 08:08:59.042499 139946397853440 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.601792812347412, loss=1.9013653993606567
I0206 08:09:45.682102 139946414638848 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.660280704498291, loss=2.121945858001709
I0206 08:10:32.106168 139946397853440 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.9325475692749023, loss=2.2107722759246826
I0206 08:11:18.639289 139946414638848 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.9509756565093994, loss=2.070936918258667
I0206 08:12:04.768552 139946397853440 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.8493831157684326, loss=2.0978446006774902
I0206 08:12:51.358217 139946414638848 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.1011037826538086, loss=2.0835940837860107
I0206 08:13:37.680721 139946397853440 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.44313907623291, loss=2.410463809967041
I0206 08:14:18.107275 140107197974336 spec.py:321] Evaluating on the training split.
I0206 08:14:28.960371 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 08:15:01.698309 140107197974336 spec.py:349] Evaluating on the test split.
I0206 08:15:03.297580 140107197974336 submission_runner.py:408] Time since start: 63721.19s, 	Step: 124389, 	{'train/accuracy': 0.75, 'train/loss': 0.994757890701294, 'validation/accuracy': 0.6810799837112427, 'validation/loss': 1.293065071105957, 'validation/num_examples': 50000, 'test/accuracy': 0.5511000156402588, 'test/loss': 1.9417612552642822, 'test/num_examples': 10000, 'score': 57174.481586933136, 'total_duration': 63721.19270992279, 'accumulated_submission_time': 57174.481586933136, 'accumulated_eval_time': 6533.5595326423645, 'accumulated_logging_time': 6.052314043045044}
I0206 08:15:03.337208 139946414638848 logging_writer.py:48] [124389] accumulated_eval_time=6533.559533, accumulated_logging_time=6.052314, accumulated_submission_time=57174.481587, global_step=124389, preemption_count=0, score=57174.481587, test/accuracy=0.551100, test/loss=1.941761, test/num_examples=10000, total_duration=63721.192710, train/accuracy=0.750000, train/loss=0.994758, validation/accuracy=0.681080, validation/loss=1.293065, validation/num_examples=50000
I0206 08:15:08.061229 139946397853440 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.9252543449401855, loss=2.0890257358551025
I0206 08:15:50.621077 139946414638848 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.5683252811431885, loss=2.3863728046417236
I0206 08:16:36.819596 139946397853440 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.7314555644989014, loss=4.499242305755615
I0206 08:17:23.468720 139946414638848 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.8503754138946533, loss=2.370438575744629
I0206 08:18:09.960273 139946397853440 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.9857325553894043, loss=1.982230544090271
I0206 08:18:56.007138 139946414638848 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.5135602951049805, loss=3.0383479595184326
I0206 08:19:42.579321 139946397853440 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.900472640991211, loss=1.9594097137451172
I0206 08:20:29.156796 139946414638848 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.730419635772705, loss=1.9227081537246704
I0206 08:21:15.568757 139946397853440 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.0699923038482666, loss=2.1034350395202637
I0206 08:22:02.106630 139946414638848 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.7064785957336426, loss=1.9878475666046143
I0206 08:22:03.743637 140107197974336 spec.py:321] Evaluating on the training split.
I0206 08:22:14.542633 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 08:22:52.552448 140107197974336 spec.py:349] Evaluating on the test split.
I0206 08:22:54.152533 140107197974336 submission_runner.py:408] Time since start: 64192.05s, 	Step: 125305, 	{'train/accuracy': 0.74378901720047, 'train/loss': 1.0434715747833252, 'validation/accuracy': 0.6869999766349792, 'validation/loss': 1.2920804023742676, 'validation/num_examples': 50000, 'test/accuracy': 0.560200035572052, 'test/loss': 1.9421415328979492, 'test/num_examples': 10000, 'score': 57594.81980538368, 'total_duration': 64192.04764842987, 'accumulated_submission_time': 57594.81980538368, 'accumulated_eval_time': 6583.968408584595, 'accumulated_logging_time': 6.108830213546753}
I0206 08:22:54.188353 139946397853440 logging_writer.py:48] [125305] accumulated_eval_time=6583.968409, accumulated_logging_time=6.108830, accumulated_submission_time=57594.819805, global_step=125305, preemption_count=0, score=57594.819805, test/accuracy=0.560200, test/loss=1.942142, test/num_examples=10000, total_duration=64192.047648, train/accuracy=0.743789, train/loss=1.043472, validation/accuracy=0.687000, validation/loss=1.292080, validation/num_examples=50000
I0206 08:23:33.926111 139946414638848 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.6549293994903564, loss=4.299716949462891
I0206 08:24:20.038727 139946397853440 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.8638391494750977, loss=2.0236246585845947
I0206 08:25:06.843633 139946414638848 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.6636102199554443, loss=2.927976131439209
I0206 08:25:53.104848 139946397853440 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.00974702835083, loss=1.9491040706634521
I0206 08:26:39.637886 139946414638848 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.0847835540771484, loss=2.173325777053833
I0206 08:27:25.960482 139946397853440 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.8836753368377686, loss=2.8693809509277344
I0206 08:28:12.395344 139946414638848 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.31937837600708, loss=3.5131149291992188
I0206 08:28:58.926545 139946397853440 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.210103988647461, loss=1.9804004430770874
I0206 08:29:45.529298 139946414638848 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.936889410018921, loss=1.873924732208252
I0206 08:29:54.535862 140107197974336 spec.py:321] Evaluating on the training split.
I0206 08:30:05.478476 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 08:30:41.647666 140107197974336 spec.py:349] Evaluating on the test split.
I0206 08:30:43.237477 140107197974336 submission_runner.py:408] Time since start: 64661.13s, 	Step: 126221, 	{'train/accuracy': 0.7441992163658142, 'train/loss': 1.0305161476135254, 'validation/accuracy': 0.6879000067710876, 'validation/loss': 1.277883529663086, 'validation/num_examples': 50000, 'test/accuracy': 0.5663000345230103, 'test/loss': 1.9110310077667236, 'test/num_examples': 10000, 'score': 58015.106491565704, 'total_duration': 64661.13260555267, 'accumulated_submission_time': 58015.106491565704, 'accumulated_eval_time': 6632.6700274944305, 'accumulated_logging_time': 6.153582334518433}
I0206 08:30:43.276461 139946397853440 logging_writer.py:48] [126221] accumulated_eval_time=6632.670027, accumulated_logging_time=6.153582, accumulated_submission_time=58015.106492, global_step=126221, preemption_count=0, score=58015.106492, test/accuracy=0.566300, test/loss=1.911031, test/num_examples=10000, total_duration=64661.132606, train/accuracy=0.744199, train/loss=1.030516, validation/accuracy=0.687900, validation/loss=1.277884, validation/num_examples=50000
I0206 08:31:16.035470 139946414638848 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.7774507999420166, loss=4.209407806396484
I0206 08:32:01.885813 139946397853440 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.511992931365967, loss=2.554037570953369
I0206 08:32:48.041229 139946414638848 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.1279375553131104, loss=4.601860046386719
I0206 08:33:34.753104 139946397853440 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.727788209915161, loss=3.653559684753418
I0206 08:34:21.081568 139946414638848 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.8423683643341064, loss=2.072598457336426
I0206 08:35:07.569974 139946397853440 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.880720376968384, loss=2.183546543121338
I0206 08:35:53.729119 139946414638848 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.975069284439087, loss=1.9273264408111572
I0206 08:36:40.299002 139946397853440 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.628058433532715, loss=2.3178606033325195
I0206 08:37:26.568921 139946414638848 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.5271942615509033, loss=3.3829293251037598
I0206 08:37:43.372148 140107197974336 spec.py:321] Evaluating on the training split.
I0206 08:37:54.136608 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 08:38:31.739962 140107197974336 spec.py:349] Evaluating on the test split.
I0206 08:38:33.337311 140107197974336 submission_runner.py:408] Time since start: 65131.23s, 	Step: 127138, 	{'train/accuracy': 0.7535937428474426, 'train/loss': 0.9919482469558716, 'validation/accuracy': 0.6867799758911133, 'validation/loss': 1.2697197198867798, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.8933143615722656, 'test/num_examples': 10000, 'score': 58435.141280412674, 'total_duration': 65131.23244309425, 'accumulated_submission_time': 58435.141280412674, 'accumulated_eval_time': 6682.635187864304, 'accumulated_logging_time': 6.201958656311035}
I0206 08:38:33.373961 139946397853440 logging_writer.py:48] [127138] accumulated_eval_time=6682.635188, accumulated_logging_time=6.201959, accumulated_submission_time=58435.141280, global_step=127138, preemption_count=0, score=58435.141280, test/accuracy=0.562100, test/loss=1.893314, test/num_examples=10000, total_duration=65131.232443, train/accuracy=0.753594, train/loss=0.991948, validation/accuracy=0.686780, validation/loss=1.269720, validation/num_examples=50000
I0206 08:38:58.167212 139946414638848 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.1058716773986816, loss=4.299800872802734
I0206 08:39:44.371897 139946397853440 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.11071515083313, loss=1.9032119512557983
I0206 08:40:31.233015 139946414638848 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.5532138347625732, loss=2.36482572555542
I0206 08:41:17.917021 139946397853440 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.0063207149505615, loss=3.940131664276123
I0206 08:42:04.311764 139946414638848 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.6533703804016113, loss=1.9309680461883545
I0206 08:42:50.528304 139946397853440 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.867516040802002, loss=2.5753908157348633
I0206 08:43:36.978364 139946414638848 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.5323567390441895, loss=2.327887773513794
I0206 08:44:23.309874 139946397853440 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.0632987022399902, loss=1.960464596748352
I0206 08:45:09.871530 139946414638848 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.8456051349639893, loss=3.6439809799194336
I0206 08:45:33.450302 140107197974336 spec.py:321] Evaluating on the training split.
I0206 08:45:44.256904 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 08:46:20.918287 140107197974336 spec.py:349] Evaluating on the test split.
I0206 08:46:22.517037 140107197974336 submission_runner.py:408] Time since start: 65600.41s, 	Step: 128053, 	{'train/accuracy': 0.7463476657867432, 'train/loss': 1.0128285884857178, 'validation/accuracy': 0.6887399554252625, 'validation/loss': 1.2626125812530518, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.9025838375091553, 'test/num_examples': 10000, 'score': 58855.1561756134, 'total_duration': 65600.41216874123, 'accumulated_submission_time': 58855.1561756134, 'accumulated_eval_time': 6731.701937913895, 'accumulated_logging_time': 6.2473344802856445}
I0206 08:46:22.555036 139946397853440 logging_writer.py:48] [128053] accumulated_eval_time=6731.701938, accumulated_logging_time=6.247334, accumulated_submission_time=58855.156176, global_step=128053, preemption_count=0, score=58855.156176, test/accuracy=0.567500, test/loss=1.902584, test/num_examples=10000, total_duration=65600.412169, train/accuracy=0.746348, train/loss=1.012829, validation/accuracy=0.688740, validation/loss=1.262613, validation/num_examples=50000
I0206 08:46:41.415202 139946414638848 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.677522659301758, loss=3.1580753326416016
I0206 08:47:26.547695 139946397853440 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.881086826324463, loss=4.274319648742676
I0206 08:48:12.951872 139946414638848 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.0116939544677734, loss=4.613240718841553
I0206 08:48:59.310665 139946397853440 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.065094470977783, loss=2.170607328414917
I0206 08:49:45.955809 139946414638848 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.1454920768737793, loss=1.8636536598205566
I0206 08:50:32.480553 139946397853440 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.7380619049072266, loss=2.5022053718566895
I0206 08:51:19.085688 139946414638848 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.7895476818084717, loss=2.4414124488830566
I0206 08:52:05.644914 139946397853440 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.0730512142181396, loss=2.0209779739379883
I0206 08:52:51.743505 139946414638848 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.175483226776123, loss=1.8700913190841675
I0206 08:53:22.557553 140107197974336 spec.py:321] Evaluating on the training split.
I0206 08:53:33.755615 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 08:54:11.924240 140107197974336 spec.py:349] Evaluating on the test split.
I0206 08:54:13.518065 140107197974336 submission_runner.py:408] Time since start: 66071.41s, 	Step: 128968, 	{'train/accuracy': 0.7517382502555847, 'train/loss': 0.9961110353469849, 'validation/accuracy': 0.6873199939727783, 'validation/loss': 1.2667717933654785, 'validation/num_examples': 50000, 'test/accuracy': 0.5649000406265259, 'test/loss': 1.8984917402267456, 'test/num_examples': 10000, 'score': 59275.097254276276, 'total_duration': 66071.41319656372, 'accumulated_submission_time': 59275.097254276276, 'accumulated_eval_time': 6782.662457227707, 'accumulated_logging_time': 6.294980049133301}
I0206 08:54:13.559186 139946397853440 logging_writer.py:48] [128968] accumulated_eval_time=6782.662457, accumulated_logging_time=6.294980, accumulated_submission_time=59275.097254, global_step=128968, preemption_count=0, score=59275.097254, test/accuracy=0.564900, test/loss=1.898492, test/num_examples=10000, total_duration=66071.413197, train/accuracy=0.751738, train/loss=0.996111, validation/accuracy=0.687320, validation/loss=1.266772, validation/num_examples=50000
I0206 08:54:26.518000 139946414638848 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.036555051803589, loss=1.8236973285675049
I0206 08:55:10.312456 139946397853440 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.9166927337646484, loss=4.380784511566162
I0206 08:55:56.717701 139946414638848 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.8718788623809814, loss=2.260240077972412
I0206 08:56:43.451690 139946397853440 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.3755080699920654, loss=3.3433995246887207
I0206 08:57:30.011085 139946414638848 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.2056877613067627, loss=1.9567089080810547
I0206 08:58:16.506928 139946397853440 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.8942840099334717, loss=4.376771450042725
I0206 08:59:02.813359 139946414638848 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.8862645626068115, loss=1.965652585029602
I0206 08:59:49.093477 139946397853440 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.0733397006988525, loss=1.9912776947021484
I0206 09:00:35.722138 139946414638848 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.080017328262329, loss=2.067918539047241
I0206 09:01:13.578796 140107197974336 spec.py:321] Evaluating on the training split.
I0206 09:01:24.764701 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 09:01:59.807061 140107197974336 spec.py:349] Evaluating on the test split.
I0206 09:02:01.408734 140107197974336 submission_runner.py:408] Time since start: 66539.30s, 	Step: 129883, 	{'train/accuracy': 0.7596484422683716, 'train/loss': 0.9546266794204712, 'validation/accuracy': 0.693839967250824, 'validation/loss': 1.2397924661636353, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.8531309366226196, 'test/num_examples': 10000, 'score': 59695.053881406784, 'total_duration': 66539.30385136604, 'accumulated_submission_time': 59695.053881406784, 'accumulated_eval_time': 6830.492385864258, 'accumulated_logging_time': 6.34734582901001}
I0206 09:02:01.454903 139946397853440 logging_writer.py:48] [129883] accumulated_eval_time=6830.492386, accumulated_logging_time=6.347346, accumulated_submission_time=59695.053881, global_step=129883, preemption_count=0, score=59695.053881, test/accuracy=0.573200, test/loss=1.853131, test/num_examples=10000, total_duration=66539.303851, train/accuracy=0.759648, train/loss=0.954627, validation/accuracy=0.693840, validation/loss=1.239792, validation/num_examples=50000
I0206 09:02:08.537252 139946414638848 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.2449636459350586, loss=1.9195618629455566
I0206 09:02:51.454330 139946397853440 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.2060985565185547, loss=2.0592689514160156
I0206 09:03:38.033097 139946414638848 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.9320504665374756, loss=2.675422191619873
I0206 09:04:24.694617 139946397853440 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.8886959552764893, loss=2.4245457649230957
I0206 09:05:11.396763 139946414638848 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.123561382293701, loss=4.569893836975098
I0206 09:05:58.175848 139946397853440 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.5894932746887207, loss=2.0412299633026123
I0206 09:06:44.731874 139946414638848 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.909574031829834, loss=4.461763381958008
I0206 09:07:31.142734 139946397853440 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.829519748687744, loss=3.7473936080932617
I0206 09:08:17.801058 139946414638848 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.100944995880127, loss=1.944508671760559
I0206 09:09:01.663566 140107197974336 spec.py:321] Evaluating on the training split.
I0206 09:09:12.868347 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 09:09:48.854833 140107197974336 spec.py:349] Evaluating on the test split.
I0206 09:09:50.448890 140107197974336 submission_runner.py:408] Time since start: 67008.34s, 	Step: 130796, 	{'train/accuracy': 0.764453113079071, 'train/loss': 0.9491795897483826, 'validation/accuracy': 0.691540002822876, 'validation/loss': 1.2591371536254883, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.8973604440689087, 'test/num_examples': 10000, 'score': 60115.20013332367, 'total_duration': 67008.34401488304, 'accumulated_submission_time': 60115.20013332367, 'accumulated_eval_time': 6879.277712106705, 'accumulated_logging_time': 6.404633522033691}
I0206 09:09:50.491320 139946397853440 logging_writer.py:48] [130796] accumulated_eval_time=6879.277712, accumulated_logging_time=6.404634, accumulated_submission_time=60115.200133, global_step=130796, preemption_count=0, score=60115.200133, test/accuracy=0.567500, test/loss=1.897360, test/num_examples=10000, total_duration=67008.344015, train/accuracy=0.764453, train/loss=0.949180, validation/accuracy=0.691540, validation/loss=1.259137, validation/num_examples=50000
I0206 09:09:52.455518 139946414638848 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.828350305557251, loss=3.4907279014587402
I0206 09:10:34.666572 139946397853440 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.053419828414917, loss=1.8421447277069092
I0206 09:11:20.795659 139946414638848 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.8551809787750244, loss=2.798492431640625
I0206 09:12:07.491664 139946397853440 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.100281000137329, loss=1.8823307752609253
I0206 09:12:53.928379 139946414638848 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.523009777069092, loss=2.0357720851898193
I0206 09:13:40.543432 139946397853440 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.1269030570983887, loss=1.8663864135742188
I0206 09:14:27.055828 139946414638848 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.091914415359497, loss=2.1477723121643066
I0206 09:15:13.648554 139946397853440 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.195516347885132, loss=1.9844632148742676
I0206 09:16:00.063687 139946414638848 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.91434907913208, loss=4.255667209625244
I0206 09:16:46.503617 139946397853440 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.098037004470825, loss=4.304922580718994
I0206 09:16:50.763951 140107197974336 spec.py:321] Evaluating on the training split.
I0206 09:17:01.956258 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 09:17:37.952236 140107197974336 spec.py:349] Evaluating on the test split.
I0206 09:17:39.553020 140107197974336 submission_runner.py:408] Time since start: 67477.45s, 	Step: 131711, 	{'train/accuracy': 0.7549218535423279, 'train/loss': 0.9674057364463806, 'validation/accuracy': 0.6934999823570251, 'validation/loss': 1.2365845441818237, 'validation/num_examples': 50000, 'test/accuracy': 0.5690000057220459, 'test/loss': 1.871274471282959, 'test/num_examples': 10000, 'score': 60535.409593105316, 'total_duration': 67477.44813871384, 'accumulated_submission_time': 60535.409593105316, 'accumulated_eval_time': 6928.066775798798, 'accumulated_logging_time': 6.457538366317749}
I0206 09:17:39.594343 139946414638848 logging_writer.py:48] [131711] accumulated_eval_time=6928.066776, accumulated_logging_time=6.457538, accumulated_submission_time=60535.409593, global_step=131711, preemption_count=0, score=60535.409593, test/accuracy=0.569000, test/loss=1.871274, test/num_examples=10000, total_duration=67477.448139, train/accuracy=0.754922, train/loss=0.967406, validation/accuracy=0.693500, validation/loss=1.236585, validation/num_examples=50000
I0206 09:18:17.075329 139946397853440 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.8493881225585938, loss=3.1851229667663574
I0206 09:19:03.206723 139946414638848 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.118100881576538, loss=2.7150049209594727
I0206 09:19:49.716216 139946397853440 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.8868720531463623, loss=3.766939878463745
I0206 09:20:36.412377 139946414638848 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.9486281871795654, loss=1.892214298248291
I0206 09:21:22.859081 139946397853440 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.8533413410186768, loss=2.366840124130249
I0206 09:22:09.311844 139946414638848 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.1989829540252686, loss=3.730215072631836
I0206 09:22:55.719980 139946397853440 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.2753238677978516, loss=1.9848999977111816
I0206 09:23:42.278341 139946414638848 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.019054889678955, loss=1.882312297821045
I0206 09:24:29.022714 139946397853440 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.723613739013672, loss=3.4313502311706543
I0206 09:24:39.700244 140107197974336 spec.py:321] Evaluating on the training split.
I0206 09:24:50.651654 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 09:25:28.693861 140107197974336 spec.py:349] Evaluating on the test split.
I0206 09:25:30.295942 140107197974336 submission_runner.py:408] Time since start: 67948.19s, 	Step: 132625, 	{'train/accuracy': 0.7603515386581421, 'train/loss': 0.9755155444145203, 'validation/accuracy': 0.6967200040817261, 'validation/loss': 1.2530672550201416, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.8836565017700195, 'test/num_examples': 10000, 'score': 60955.45423436165, 'total_duration': 67948.19107365608, 'accumulated_submission_time': 60955.45423436165, 'accumulated_eval_time': 6978.662467956543, 'accumulated_logging_time': 6.508562088012695}
I0206 09:25:30.331973 139946414638848 logging_writer.py:48] [132625] accumulated_eval_time=6978.662468, accumulated_logging_time=6.508562, accumulated_submission_time=60955.454234, global_step=132625, preemption_count=0, score=60955.454234, test/accuracy=0.577000, test/loss=1.883657, test/num_examples=10000, total_duration=67948.191074, train/accuracy=0.760352, train/loss=0.975516, validation/accuracy=0.696720, validation/loss=1.253067, validation/num_examples=50000
I0206 09:26:01.071632 139946397853440 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.158933401107788, loss=2.151559829711914
I0206 09:26:47.020174 139946414638848 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.2263805866241455, loss=2.019167423248291
I0206 09:27:33.726083 139946397853440 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.252046585083008, loss=4.214372158050537
I0206 09:28:20.020068 139946414638848 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.380514144897461, loss=1.9599380493164062
I0206 09:29:06.696721 139946397853440 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.1926631927490234, loss=1.8197327852249146
I0206 09:29:52.928112 139946414638848 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.940480947494507, loss=1.8229550123214722
I0206 09:30:39.380716 139946397853440 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.2232842445373535, loss=1.921069622039795
I0206 09:31:25.899381 139946414638848 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.0556046962738037, loss=2.0305724143981934
I0206 09:32:12.614606 139946397853440 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.384153127670288, loss=1.8024042844772339
I0206 09:32:30.368864 140107197974336 spec.py:321] Evaluating on the training split.
I0206 09:32:41.520129 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 09:33:16.862658 140107197974336 spec.py:349] Evaluating on the test split.
I0206 09:33:18.465224 140107197974336 submission_runner.py:408] Time since start: 68416.36s, 	Step: 133540, 	{'train/accuracy': 0.7734179496765137, 'train/loss': 0.9080670475959778, 'validation/accuracy': 0.6987400054931641, 'validation/loss': 1.2287685871124268, 'validation/num_examples': 50000, 'test/accuracy': 0.5764999985694885, 'test/loss': 1.851581335067749, 'test/num_examples': 10000, 'score': 61375.42639231682, 'total_duration': 68416.36034202576, 'accumulated_submission_time': 61375.42639231682, 'accumulated_eval_time': 7026.758812665939, 'accumulated_logging_time': 6.553529500961304}
I0206 09:33:18.507233 139946414638848 logging_writer.py:48] [133540] accumulated_eval_time=7026.758813, accumulated_logging_time=6.553530, accumulated_submission_time=61375.426392, global_step=133540, preemption_count=0, score=61375.426392, test/accuracy=0.576500, test/loss=1.851581, test/num_examples=10000, total_duration=68416.360342, train/accuracy=0.773418, train/loss=0.908067, validation/accuracy=0.698740, validation/loss=1.228769, validation/num_examples=50000
I0206 09:33:42.550181 139946397853440 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.91892147064209, loss=2.3123955726623535
I0206 09:34:28.896615 139946414638848 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.555457353591919, loss=1.9598597288131714
I0206 09:35:15.781616 139946397853440 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.1898365020751953, loss=3.3672988414764404
I0206 09:36:02.370586 139946414638848 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.193121910095215, loss=2.2866768836975098
I0206 09:36:48.757863 139946397853440 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.462092876434326, loss=1.8543646335601807
I0206 09:37:35.316903 139946414638848 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.099086284637451, loss=1.883257269859314
I0206 09:38:21.878701 139946397853440 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.9032692909240723, loss=2.344560146331787
I0206 09:39:08.459933 139946414638848 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.055112361907959, loss=2.0936615467071533
I0206 09:39:54.969949 139946397853440 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.084261894226074, loss=1.7204113006591797
I0206 09:40:18.858625 140107197974336 spec.py:321] Evaluating on the training split.
I0206 09:40:29.889718 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 09:41:03.818199 140107197974336 spec.py:349] Evaluating on the test split.
I0206 09:41:05.416435 140107197974336 submission_runner.py:408] Time since start: 68883.31s, 	Step: 134453, 	{'train/accuracy': 0.7599413990974426, 'train/loss': 0.9587976336479187, 'validation/accuracy': 0.7029199600219727, 'validation/loss': 1.2122979164123535, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 1.8428810834884644, 'test/num_examples': 10000, 'score': 61795.71336269379, 'total_duration': 68883.31156492233, 'accumulated_submission_time': 61795.71336269379, 'accumulated_eval_time': 7073.3166263103485, 'accumulated_logging_time': 6.606256008148193}
I0206 09:41:05.454086 139946414638848 logging_writer.py:48] [134453] accumulated_eval_time=7073.316626, accumulated_logging_time=6.606256, accumulated_submission_time=61795.713363, global_step=134453, preemption_count=0, score=61795.713363, test/accuracy=0.577900, test/loss=1.842881, test/num_examples=10000, total_duration=68883.311565, train/accuracy=0.759941, train/loss=0.958798, validation/accuracy=0.702920, validation/loss=1.212298, validation/num_examples=50000
I0206 09:41:24.319120 139946397853440 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.100836992263794, loss=1.7745583057403564
I0206 09:42:09.228092 139946414638848 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.1533446311950684, loss=1.9916504621505737
I0206 09:42:55.762846 139946397853440 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.1434755325317383, loss=3.570260524749756
I0206 09:43:42.288059 139946414638848 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.9981231689453125, loss=1.895874261856079
I0206 09:44:29.145244 139946397853440 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.236302375793457, loss=2.1507389545440674
I0206 09:45:15.500144 139946414638848 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.1051549911499023, loss=2.747706890106201
I0206 09:46:01.747825 139946397853440 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.963737726211548, loss=2.193497896194458
I0206 09:46:48.092986 139946414638848 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.088679313659668, loss=3.769805431365967
I0206 09:47:34.455411 139946397853440 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.193876028060913, loss=1.852797269821167
I0206 09:48:05.828526 140107197974336 spec.py:321] Evaluating on the training split.
I0206 09:48:16.572349 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 09:48:54.849018 140107197974336 spec.py:349] Evaluating on the test split.
I0206 09:48:56.454431 140107197974336 submission_runner.py:408] Time since start: 69354.35s, 	Step: 135369, 	{'train/accuracy': 0.7627929449081421, 'train/loss': 0.9346864819526672, 'validation/accuracy': 0.7001399993896484, 'validation/loss': 1.2140921354293823, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.8443801403045654, 'test/num_examples': 10000, 'score': 62216.026047468185, 'total_duration': 69354.34955620766, 'accumulated_submission_time': 62216.026047468185, 'accumulated_eval_time': 7123.942526578903, 'accumulated_logging_time': 6.653573513031006}
I0206 09:48:56.493594 139946414638848 logging_writer.py:48] [135369] accumulated_eval_time=7123.942527, accumulated_logging_time=6.653574, accumulated_submission_time=62216.026047, global_step=135369, preemption_count=0, score=62216.026047, test/accuracy=0.575000, test/loss=1.844380, test/num_examples=10000, total_duration=69354.349556, train/accuracy=0.762793, train/loss=0.934686, validation/accuracy=0.700140, validation/loss=1.214092, validation/num_examples=50000
I0206 09:49:09.059098 139946397853440 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.533979654312134, loss=1.9349188804626465
I0206 09:49:52.661216 139946414638848 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.173466205596924, loss=4.279287338256836
I0206 09:50:39.157506 139946397853440 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.234755277633667, loss=1.831201195716858
I0206 09:51:26.129592 139946414638848 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.7740378379821777, loss=4.085860729217529
I0206 09:52:12.397883 139946397853440 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.2298502922058105, loss=2.2779839038848877
I0206 09:52:58.794420 139946414638848 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.6200451850891113, loss=1.8024187088012695
I0206 09:53:45.341453 139946397853440 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.2171809673309326, loss=2.446995973587036
I0206 09:54:32.025889 139946414638848 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.1472761631011963, loss=1.9972904920578003
I0206 09:55:18.399388 139946397853440 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.1662309169769287, loss=3.4771406650543213
I0206 09:55:56.727224 140107197974336 spec.py:321] Evaluating on the training split.
I0206 09:56:07.595998 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 09:56:44.365355 140107197974336 spec.py:349] Evaluating on the test split.
I0206 09:56:45.972392 140107197974336 submission_runner.py:408] Time since start: 69823.87s, 	Step: 136284, 	{'train/accuracy': 0.7731054425239563, 'train/loss': 0.9033991694450378, 'validation/accuracy': 0.7005999684333801, 'validation/loss': 1.2164199352264404, 'validation/num_examples': 50000, 'test/accuracy': 0.5800999999046326, 'test/loss': 1.845173954963684, 'test/num_examples': 10000, 'score': 62636.19765305519, 'total_duration': 69823.86749219894, 'accumulated_submission_time': 62636.19765305519, 'accumulated_eval_time': 7173.187728404999, 'accumulated_logging_time': 6.702480316162109}
I0206 09:56:46.020201 139946414638848 logging_writer.py:48] [136284] accumulated_eval_time=7173.187728, accumulated_logging_time=6.702480, accumulated_submission_time=62636.197653, global_step=136284, preemption_count=0, score=62636.197653, test/accuracy=0.580100, test/loss=1.845174, test/num_examples=10000, total_duration=69823.867492, train/accuracy=0.773105, train/loss=0.903399, validation/accuracy=0.700600, validation/loss=1.216420, validation/num_examples=50000
I0206 09:56:52.715771 139946397853440 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.4939420223236084, loss=4.208774566650391
I0206 09:57:35.585202 139946414638848 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.4631683826446533, loss=4.321224212646484
I0206 09:58:21.791085 139946397853440 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.6451199054718018, loss=1.8540672063827515
I0206 09:59:08.877091 139946414638848 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.145519256591797, loss=2.504240036010742
I0206 09:59:55.573873 139946397853440 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.0142548084259033, loss=2.1449689865112305
I0206 10:00:42.474533 139946414638848 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.6158316135406494, loss=3.318162441253662
I0206 10:01:29.344005 139946397853440 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.3200089931488037, loss=1.7302662134170532
I0206 10:02:16.045002 139946414638848 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.363760471343994, loss=3.908939838409424
I0206 10:03:02.669846 139946397853440 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.642883777618408, loss=1.7427475452423096
I0206 10:03:46.331321 140107197974336 spec.py:321] Evaluating on the training split.
I0206 10:03:57.322919 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 10:04:34.381983 140107197974336 spec.py:349] Evaluating on the test split.
I0206 10:04:35.979266 140107197974336 submission_runner.py:408] Time since start: 70293.87s, 	Step: 137195, 	{'train/accuracy': 0.7671093344688416, 'train/loss': 0.9361597299575806, 'validation/accuracy': 0.7026599645614624, 'validation/loss': 1.2115135192871094, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.8194403648376465, 'test/num_examples': 10000, 'score': 63056.44626426697, 'total_duration': 70293.874396801, 'accumulated_submission_time': 63056.44626426697, 'accumulated_eval_time': 7222.835695266724, 'accumulated_logging_time': 6.760934591293335}
I0206 10:04:36.016850 139946414638848 logging_writer.py:48] [137195] accumulated_eval_time=7222.835695, accumulated_logging_time=6.760935, accumulated_submission_time=63056.446264, global_step=137195, preemption_count=0, score=63056.446264, test/accuracy=0.583300, test/loss=1.819440, test/num_examples=10000, total_duration=70293.874397, train/accuracy=0.767109, train/loss=0.936160, validation/accuracy=0.702660, validation/loss=1.211514, validation/num_examples=50000
I0206 10:04:38.380398 139946397853440 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.1441032886505127, loss=1.9105520248413086
I0206 10:05:20.742913 139946414638848 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.676734685897827, loss=4.370564937591553
I0206 10:06:07.237716 139946397853440 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.6749911308288574, loss=1.8369407653808594
I0206 10:06:54.237077 139946414638848 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.2590513229370117, loss=3.3323726654052734
I0206 10:07:40.825446 139946397853440 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.2784276008605957, loss=2.2734615802764893
I0206 10:08:27.417060 139946414638848 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.2068982124328613, loss=1.726020336151123
I0206 10:09:14.026046 139946397853440 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.413689136505127, loss=1.9707995653152466
I0206 10:10:00.606758 139946414638848 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.698228120803833, loss=2.507969856262207
I0206 10:10:47.419636 139946397853440 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.91745924949646, loss=3.0520238876342773
I0206 10:11:34.220529 139946414638848 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.1321070194244385, loss=3.552683115005493
I0206 10:11:36.402776 140107197974336 spec.py:321] Evaluating on the training split.
I0206 10:11:47.328349 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 10:12:23.693493 140107197974336 spec.py:349] Evaluating on the test split.
I0206 10:12:25.289290 140107197974336 submission_runner.py:408] Time since start: 70763.18s, 	Step: 138106, 	{'train/accuracy': 0.7684765458106995, 'train/loss': 0.9252225756645203, 'validation/accuracy': 0.7012400031089783, 'validation/loss': 1.2247495651245117, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.8519049882888794, 'test/num_examples': 10000, 'score': 63476.769704818726, 'total_duration': 70763.18441557884, 'accumulated_submission_time': 63476.769704818726, 'accumulated_eval_time': 7271.722208023071, 'accumulated_logging_time': 6.809126853942871}
I0206 10:12:25.329990 139946397853440 logging_writer.py:48] [138106] accumulated_eval_time=7271.722208, accumulated_logging_time=6.809127, accumulated_submission_time=63476.769705, global_step=138106, preemption_count=0, score=63476.769705, test/accuracy=0.582600, test/loss=1.851905, test/num_examples=10000, total_duration=70763.184416, train/accuracy=0.768477, train/loss=0.925223, validation/accuracy=0.701240, validation/loss=1.224750, validation/num_examples=50000
I0206 10:13:05.122024 139946414638848 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.591348648071289, loss=1.9800220727920532
I0206 10:13:51.178511 139946397853440 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.4011926651000977, loss=2.055095672607422
I0206 10:14:38.225276 139946414638848 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.389627456665039, loss=1.7711775302886963
I0206 10:15:24.766436 139946397853440 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.283017635345459, loss=3.51701021194458
I0206 10:16:11.512954 139946414638848 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.411620616912842, loss=3.8936877250671387
I0206 10:16:58.102826 139946397853440 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.349297285079956, loss=1.8953871726989746
I0206 10:17:44.661660 139946414638848 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.410432815551758, loss=2.563983917236328
I0206 10:18:31.129257 139946397853440 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.2092456817626953, loss=3.760159969329834
I0206 10:19:17.389020 139946414638848 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.404914617538452, loss=2.5885045528411865
I0206 10:19:25.386851 140107197974336 spec.py:321] Evaluating on the training split.
I0206 10:19:36.278020 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 10:20:15.155848 140107197974336 spec.py:349] Evaluating on the test split.
I0206 10:20:16.744318 140107197974336 submission_runner.py:408] Time since start: 71234.64s, 	Step: 139019, 	{'train/accuracy': 0.7814257740974426, 'train/loss': 0.8607298135757446, 'validation/accuracy': 0.7080000042915344, 'validation/loss': 1.1757205724716187, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 1.8092293739318848, 'test/num_examples': 10000, 'score': 63896.763201236725, 'total_duration': 71234.63944602013, 'accumulated_submission_time': 63896.763201236725, 'accumulated_eval_time': 7323.079668521881, 'accumulated_logging_time': 6.859033823013306}
I0206 10:20:16.784292 139946397853440 logging_writer.py:48] [139019] accumulated_eval_time=7323.079669, accumulated_logging_time=6.859034, accumulated_submission_time=63896.763201, global_step=139019, preemption_count=0, score=63896.763201, test/accuracy=0.587900, test/loss=1.809229, test/num_examples=10000, total_duration=71234.639446, train/accuracy=0.781426, train/loss=0.860730, validation/accuracy=0.708000, validation/loss=1.175721, validation/num_examples=50000
I0206 10:20:50.092962 139946414638848 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.513915777206421, loss=1.8216378688812256
I0206 10:21:36.335701 139946397853440 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.440558910369873, loss=1.898359775543213
I0206 10:22:22.980618 139946414638848 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.895643711090088, loss=2.4057798385620117
I0206 10:23:09.452360 139946397853440 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.3831424713134766, loss=1.8547585010528564
I0206 10:23:55.880457 139946414638848 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.3679139614105225, loss=1.7733237743377686
I0206 10:24:42.601724 139946397853440 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.2896502017974854, loss=3.2558460235595703
I0206 10:25:29.286726 139946414638848 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.326341152191162, loss=1.8712078332901
I0206 10:26:15.877610 139946397853440 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.784785509109497, loss=1.7813881635665894
I0206 10:27:02.419664 139946414638848 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.5063564777374268, loss=2.7486512660980225
I0206 10:27:17.146392 140107197974336 spec.py:321] Evaluating on the training split.
I0206 10:27:28.080050 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 10:28:02.960619 140107197974336 spec.py:349] Evaluating on the test split.
I0206 10:28:04.564053 140107197974336 submission_runner.py:408] Time since start: 71702.46s, 	Step: 139933, 	{'train/accuracy': 0.7718554735183716, 'train/loss': 0.9126390814781189, 'validation/accuracy': 0.7079600095748901, 'validation/loss': 1.185042142868042, 'validation/num_examples': 50000, 'test/accuracy': 0.5852000117301941, 'test/loss': 1.8195505142211914, 'test/num_examples': 10000, 'score': 64317.06393766403, 'total_duration': 71702.45918250084, 'accumulated_submission_time': 64317.06393766403, 'accumulated_eval_time': 7370.497317314148, 'accumulated_logging_time': 6.90878701210022}
I0206 10:28:04.602817 139946397853440 logging_writer.py:48] [139933] accumulated_eval_time=7370.497317, accumulated_logging_time=6.908787, accumulated_submission_time=64317.063938, global_step=139933, preemption_count=0, score=64317.063938, test/accuracy=0.585200, test/loss=1.819551, test/num_examples=10000, total_duration=71702.459183, train/accuracy=0.771855, train/loss=0.912639, validation/accuracy=0.707960, validation/loss=1.185042, validation/num_examples=50000
I0206 10:28:31.734640 139946414638848 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.5460143089294434, loss=1.684268832206726
I0206 10:29:17.876431 139946397853440 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.8130602836608887, loss=1.6855846643447876
I0206 10:30:04.187988 139946414638848 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.674600124359131, loss=4.165180206298828
I0206 10:30:50.574199 139946397853440 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.2907307147979736, loss=1.7959470748901367
I0206 10:31:36.890965 139946414638848 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.1340365409851074, loss=2.8775734901428223
I0206 10:32:23.593905 139946397853440 logging_writer.py:48] [140500] global_step=140500, grad_norm=4.1522908210754395, loss=1.8430302143096924
I0206 10:33:10.150214 139946414638848 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.4373064041137695, loss=2.6884498596191406
I0206 10:33:56.620501 139946397853440 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.065244197845459, loss=2.883486270904541
I0206 10:34:43.421416 139946414638848 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.381941080093384, loss=1.7616294622421265
I0206 10:35:04.626507 140107197974336 spec.py:321] Evaluating on the training split.
I0206 10:35:16.478249 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 10:35:53.725263 140107197974336 spec.py:349] Evaluating on the test split.
I0206 10:35:55.320621 140107197974336 submission_runner.py:408] Time since start: 72173.22s, 	Step: 140847, 	{'train/accuracy': 0.7747851610183716, 'train/loss': 0.8819167017936707, 'validation/accuracy': 0.7116400003433228, 'validation/loss': 1.1624095439910889, 'validation/num_examples': 50000, 'test/accuracy': 0.5847000479698181, 'test/loss': 1.7930099964141846, 'test/num_examples': 10000, 'score': 64737.026354551315, 'total_duration': 72173.21574926376, 'accumulated_submission_time': 64737.026354551315, 'accumulated_eval_time': 7421.191428661346, 'accumulated_logging_time': 6.957134008407593}
I0206 10:35:55.362981 139946397853440 logging_writer.py:48] [140847] accumulated_eval_time=7421.191429, accumulated_logging_time=6.957134, accumulated_submission_time=64737.026355, global_step=140847, preemption_count=0, score=64737.026355, test/accuracy=0.584700, test/loss=1.793010, test/num_examples=10000, total_duration=72173.215749, train/accuracy=0.774785, train/loss=0.881917, validation/accuracy=0.711640, validation/loss=1.162410, validation/num_examples=50000
I0206 10:36:16.570946 139946414638848 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.3061795234680176, loss=2.5468897819519043
I0206 10:37:02.135599 139946397853440 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.607515335083008, loss=1.8879985809326172
I0206 10:37:48.488419 139946414638848 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.330357074737549, loss=2.3405795097351074
I0206 10:38:35.245896 139946397853440 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.9664363861083984, loss=3.2880403995513916
I0206 10:39:21.681037 139946414638848 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.4625558853149414, loss=1.9875855445861816
I0206 10:40:08.105316 139946397853440 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.7501320838928223, loss=3.70786190032959
I0206 10:40:54.533658 139946414638848 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.6291017532348633, loss=1.8200839757919312
I0206 10:41:41.023746 139946397853440 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.6199259757995605, loss=4.128803730010986
I0206 10:42:27.413940 139946414638848 logging_writer.py:48] [141700] global_step=141700, grad_norm=4.0012946128845215, loss=3.89089298248291
I0206 10:42:55.443753 140107197974336 spec.py:321] Evaluating on the training split.
I0206 10:43:06.482837 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 10:43:46.256794 140107197974336 spec.py:349] Evaluating on the test split.
I0206 10:43:47.858369 140107197974336 submission_runner.py:408] Time since start: 72645.75s, 	Step: 141762, 	{'train/accuracy': 0.77978515625, 'train/loss': 0.8617091178894043, 'validation/accuracy': 0.7116599678993225, 'validation/loss': 1.165801763534546, 'validation/num_examples': 50000, 'test/accuracy': 0.5872000455856323, 'test/loss': 1.7895723581314087, 'test/num_examples': 10000, 'score': 65157.04562306404, 'total_duration': 72645.75348472595, 'accumulated_submission_time': 65157.04562306404, 'accumulated_eval_time': 7473.606050014496, 'accumulated_logging_time': 7.00886607170105}
I0206 10:43:47.903365 139946397853440 logging_writer.py:48] [141762] accumulated_eval_time=7473.606050, accumulated_logging_time=7.008866, accumulated_submission_time=65157.045623, global_step=141762, preemption_count=0, score=65157.045623, test/accuracy=0.587200, test/loss=1.789572, test/num_examples=10000, total_duration=72645.753485, train/accuracy=0.779785, train/loss=0.861709, validation/accuracy=0.711660, validation/loss=1.165802, validation/num_examples=50000
I0206 10:44:03.222919 139946414638848 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.648561716079712, loss=1.7391881942749023
I0206 10:44:47.938439 139946397853440 logging_writer.py:48] [141900] global_step=141900, grad_norm=4.087006092071533, loss=1.8423399925231934
I0206 10:45:34.569385 139946414638848 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.4554078578948975, loss=1.8000004291534424
I0206 10:46:21.179731 139946397853440 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.9271154403686523, loss=3.8478798866271973
I0206 10:47:07.695385 139946414638848 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.856128692626953, loss=1.6893925666809082
I0206 10:47:54.075535 139946397853440 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.413926601409912, loss=1.7273155450820923
I0206 10:48:40.868535 139946414638848 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.2391445636749268, loss=2.3619823455810547
I0206 10:49:27.283282 139946397853440 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.7531850337982178, loss=1.8405193090438843
I0206 10:50:13.965514 139946414638848 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.8022141456604004, loss=2.3028371334075928
I0206 10:50:48.121149 140107197974336 spec.py:321] Evaluating on the training split.
I0206 10:50:59.020354 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 10:51:39.502713 140107197974336 spec.py:349] Evaluating on the test split.
I0206 10:51:41.106839 140107197974336 submission_runner.py:408] Time since start: 73119.00s, 	Step: 142676, 	{'train/accuracy': 0.7821679711341858, 'train/loss': 0.8531965613365173, 'validation/accuracy': 0.7168200016021729, 'validation/loss': 1.1503877639770508, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.76817786693573, 'test/num_examples': 10000, 'score': 65577.20102715492, 'total_duration': 73119.00194859505, 'accumulated_submission_time': 65577.20102715492, 'accumulated_eval_time': 7526.591713428497, 'accumulated_logging_time': 7.064823865890503}
I0206 10:51:41.155314 139946397853440 logging_writer.py:48] [142676] accumulated_eval_time=7526.591713, accumulated_logging_time=7.064824, accumulated_submission_time=65577.201027, global_step=142676, preemption_count=0, score=65577.201027, test/accuracy=0.589800, test/loss=1.768178, test/num_examples=10000, total_duration=73119.001949, train/accuracy=0.782168, train/loss=0.853197, validation/accuracy=0.716820, validation/loss=1.150388, validation/num_examples=50000
I0206 10:51:51.388735 139946414638848 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.9971237182617188, loss=2.418586492538452
I0206 10:52:34.759693 139946397853440 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.3276147842407227, loss=3.0084540843963623
I0206 10:53:21.579012 139946414638848 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.5771608352661133, loss=1.6892218589782715
I0206 10:54:08.517879 139946397853440 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.595339775085449, loss=1.7199987173080444
I0206 10:54:54.768839 139946414638848 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.871367931365967, loss=1.7182499170303345
I0206 10:55:41.499195 139946397853440 logging_writer.py:48] [143200] global_step=143200, grad_norm=4.275400161743164, loss=1.8777782917022705
I0206 10:56:27.886364 139946414638848 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.6557998657226562, loss=2.364389181137085
I0206 10:57:14.437946 139946397853440 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.269914388656616, loss=2.4431750774383545
I0206 10:58:00.816679 139946414638848 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.7971997261047363, loss=1.8158791065216064
I0206 10:58:41.212226 140107197974336 spec.py:321] Evaluating on the training split.
I0206 10:58:52.435441 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 10:59:30.766611 140107197974336 spec.py:349] Evaluating on the test split.
I0206 10:59:32.363193 140107197974336 submission_runner.py:408] Time since start: 73590.26s, 	Step: 143588, 	{'train/accuracy': 0.7820116877555847, 'train/loss': 0.8580734729766846, 'validation/accuracy': 0.7165200114250183, 'validation/loss': 1.1449278593063354, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.7643986940383911, 'test/num_examples': 10000, 'score': 65996.80779743195, 'total_duration': 73590.25830888748, 'accumulated_submission_time': 65996.80779743195, 'accumulated_eval_time': 7577.742676258087, 'accumulated_logging_time': 7.511917352676392}
I0206 10:59:32.401991 139946397853440 logging_writer.py:48] [143588] accumulated_eval_time=7577.742676, accumulated_logging_time=7.511917, accumulated_submission_time=65996.807797, global_step=143588, preemption_count=0, score=65996.807797, test/accuracy=0.591900, test/loss=1.764399, test/num_examples=10000, total_duration=73590.258309, train/accuracy=0.782012, train/loss=0.858073, validation/accuracy=0.716520, validation/loss=1.144928, validation/num_examples=50000
I0206 10:59:37.508642 139946414638848 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.582733392715454, loss=1.8198186159133911
I0206 11:00:20.398147 139946397853440 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.863101005554199, loss=1.748640775680542
I0206 11:01:06.743035 139946414638848 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.353186845779419, loss=2.244899272918701
I0206 11:01:53.256971 139946397853440 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.448223352432251, loss=3.4272286891937256
I0206 11:02:39.737574 139946414638848 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.6757733821868896, loss=3.6746528148651123
I0206 11:03:26.432680 139946397853440 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.860933303833008, loss=2.6774191856384277
I0206 11:04:13.098848 139946414638848 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.9942893981933594, loss=1.8682405948638916
I0206 11:04:59.486128 139946397853440 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.8589184284210205, loss=1.7492200136184692
I0206 11:05:46.089739 139946414638848 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.4805474281311035, loss=3.701298236846924
I0206 11:06:32.584912 139946397853440 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.6802279949188232, loss=1.6669597625732422
I0206 11:06:32.601171 140107197974336 spec.py:321] Evaluating on the training split.
I0206 11:06:43.698829 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 11:07:18.056371 140107197974336 spec.py:349] Evaluating on the test split.
I0206 11:07:19.655970 140107197974336 submission_runner.py:408] Time since start: 74057.55s, 	Step: 144501, 	{'train/accuracy': 0.7851171493530273, 'train/loss': 0.8476221561431885, 'validation/accuracy': 0.7163400053977966, 'validation/loss': 1.144951581954956, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.7644582986831665, 'test/num_examples': 10000, 'score': 66416.94591355324, 'total_duration': 74057.55110120773, 'accumulated_submission_time': 66416.94591355324, 'accumulated_eval_time': 7624.79746389389, 'accumulated_logging_time': 7.559880018234253}
I0206 11:07:19.695135 139946414638848 logging_writer.py:48] [144501] accumulated_eval_time=7624.797464, accumulated_logging_time=7.559880, accumulated_submission_time=66416.945914, global_step=144501, preemption_count=0, score=66416.945914, test/accuracy=0.592000, test/loss=1.764458, test/num_examples=10000, total_duration=74057.551101, train/accuracy=0.785117, train/loss=0.847622, validation/accuracy=0.716340, validation/loss=1.144952, validation/num_examples=50000
I0206 11:08:01.493733 139946397853440 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.821793794631958, loss=3.1603899002075195
I0206 11:08:47.842778 139946414638848 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.915480375289917, loss=2.8518099784851074
I0206 11:09:34.599607 139946397853440 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.855060577392578, loss=4.2947235107421875
I0206 11:10:21.040676 139946414638848 logging_writer.py:48] [144900] global_step=144900, grad_norm=4.2107343673706055, loss=1.7428785562515259
I0206 11:11:07.885819 139946397853440 logging_writer.py:48] [145000] global_step=145000, grad_norm=4.094705104827881, loss=4.200242519378662
I0206 11:11:54.269438 139946414638848 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.6424291133880615, loss=1.6300057172775269
I0206 11:12:40.934649 139946397853440 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.76194429397583, loss=1.7156667709350586
I0206 11:13:27.304326 139946414638848 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.9062185287475586, loss=2.0598526000976562
I0206 11:14:13.921119 139946397853440 logging_writer.py:48] [145400] global_step=145400, grad_norm=4.047143936157227, loss=1.807126522064209
I0206 11:14:20.112060 140107197974336 spec.py:321] Evaluating on the training split.
I0206 11:14:31.252257 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 11:15:02.259604 140107197974336 spec.py:349] Evaluating on the test split.
I0206 11:15:03.850483 140107197974336 submission_runner.py:408] Time since start: 74521.75s, 	Step: 145415, 	{'train/accuracy': 0.7938281297683716, 'train/loss': 0.8047267198562622, 'validation/accuracy': 0.7177599668502808, 'validation/loss': 1.1373757123947144, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.743360996246338, 'test/num_examples': 10000, 'score': 66837.3013036251, 'total_duration': 74521.7456138134, 'accumulated_submission_time': 66837.3013036251, 'accumulated_eval_time': 7668.535885095596, 'accumulated_logging_time': 7.608190298080444}
I0206 11:15:03.888904 139946414638848 logging_writer.py:48] [145415] accumulated_eval_time=7668.535885, accumulated_logging_time=7.608190, accumulated_submission_time=66837.301304, global_step=145415, preemption_count=0, score=66837.301304, test/accuracy=0.602500, test/loss=1.743361, test/num_examples=10000, total_duration=74521.745614, train/accuracy=0.793828, train/loss=0.804727, validation/accuracy=0.717760, validation/loss=1.137376, validation/num_examples=50000
I0206 11:15:39.190529 139946397853440 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.59443998336792, loss=2.7810235023498535
I0206 11:16:25.448445 139946414638848 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.835700273513794, loss=3.362604856491089
I0206 11:17:12.293128 139946397853440 logging_writer.py:48] [145700] global_step=145700, grad_norm=5.534400463104248, loss=4.320508003234863
I0206 11:17:58.655602 139946414638848 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.8732986450195312, loss=1.768584966659546
I0206 11:18:45.235374 139946397853440 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.8333749771118164, loss=1.9164776802062988
I0206 11:19:31.898024 139946414638848 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.815455913543701, loss=1.906272292137146
I0206 11:20:18.559474 139946397853440 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.785583019256592, loss=2.248337984085083
I0206 11:21:05.058923 139946414638848 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.5288290977478027, loss=2.945173501968384
I0206 11:21:51.685281 139946397853440 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.935426712036133, loss=2.1724603176116943
I0206 11:22:04.132718 140107197974336 spec.py:321] Evaluating on the training split.
I0206 11:22:14.907608 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 11:22:51.925303 140107197974336 spec.py:349] Evaluating on the test split.
I0206 11:22:53.519601 140107197974336 submission_runner.py:408] Time since start: 74991.41s, 	Step: 146328, 	{'train/accuracy': 0.7868554592132568, 'train/loss': 0.8384296894073486, 'validation/accuracy': 0.7208999991416931, 'validation/loss': 1.1160486936569214, 'validation/num_examples': 50000, 'test/accuracy': 0.6013000011444092, 'test/loss': 1.727980136871338, 'test/num_examples': 10000, 'score': 67257.48422813416, 'total_duration': 74991.41473031044, 'accumulated_submission_time': 67257.48422813416, 'accumulated_eval_time': 7717.922769069672, 'accumulated_logging_time': 7.655825853347778}
I0206 11:22:53.559980 139946414638848 logging_writer.py:48] [146328] accumulated_eval_time=7717.922769, accumulated_logging_time=7.655826, accumulated_submission_time=67257.484228, global_step=146328, preemption_count=0, score=67257.484228, test/accuracy=0.601300, test/loss=1.727980, test/num_examples=10000, total_duration=74991.414730, train/accuracy=0.786855, train/loss=0.838430, validation/accuracy=0.720900, validation/loss=1.116049, validation/num_examples=50000
I0206 11:23:22.867840 139946397853440 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.9056153297424316, loss=3.26530122756958
I0206 11:24:08.810919 139946414638848 logging_writer.py:48] [146500] global_step=146500, grad_norm=4.339956760406494, loss=3.9659078121185303
I0206 11:24:55.276055 139946397853440 logging_writer.py:48] [146600] global_step=146600, grad_norm=4.039004325866699, loss=1.6415417194366455
I0206 11:25:42.155989 139946414638848 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.785338878631592, loss=1.7421765327453613
I0206 11:26:28.527726 139946397853440 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.0576019287109375, loss=1.7739982604980469
I0206 11:27:15.380894 139946414638848 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.7573225498199463, loss=1.724338412284851
I0206 11:28:01.811561 139946397853440 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.764434337615967, loss=2.939352512359619
I0206 11:28:48.358464 139946414638848 logging_writer.py:48] [147100] global_step=147100, grad_norm=4.1320953369140625, loss=4.183682918548584
I0206 11:29:34.593726 139946397853440 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.8502566814422607, loss=2.860024929046631
I0206 11:29:53.675384 140107197974336 spec.py:321] Evaluating on the training split.
I0206 11:30:04.410332 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 11:30:41.291787 140107197974336 spec.py:349] Evaluating on the test split.
I0206 11:30:42.894581 140107197974336 submission_runner.py:408] Time since start: 75460.79s, 	Step: 147243, 	{'train/accuracy': 0.7939453125, 'train/loss': 0.8069570064544678, 'validation/accuracy': 0.7222599983215332, 'validation/loss': 1.1095448732376099, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.7282333374023438, 'test/num_examples': 10000, 'score': 67677.5356965065, 'total_duration': 75460.78970003128, 'accumulated_submission_time': 67677.5356965065, 'accumulated_eval_time': 7767.141966342926, 'accumulated_logging_time': 7.707298278808594}
I0206 11:30:42.936819 139946414638848 logging_writer.py:48] [147243] accumulated_eval_time=7767.141966, accumulated_logging_time=7.707298, accumulated_submission_time=67677.535697, global_step=147243, preemption_count=0, score=67677.535697, test/accuracy=0.602500, test/loss=1.728233, test/num_examples=10000, total_duration=75460.789700, train/accuracy=0.793945, train/loss=0.806957, validation/accuracy=0.722260, validation/loss=1.109545, validation/num_examples=50000
I0206 11:31:05.729629 139946397853440 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.78078293800354, loss=1.8665249347686768
I0206 11:31:51.449773 139946414638848 logging_writer.py:48] [147400] global_step=147400, grad_norm=4.017764568328857, loss=1.924889087677002
I0206 11:32:38.042587 139946397853440 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.9164819717407227, loss=1.7753437757492065
I0206 11:33:24.581380 139946414638848 logging_writer.py:48] [147600] global_step=147600, grad_norm=4.159061431884766, loss=1.8079487085342407
I0206 11:34:10.896100 139946397853440 logging_writer.py:48] [147700] global_step=147700, grad_norm=4.096660137176514, loss=1.6956573724746704
I0206 11:34:57.558090 139946414638848 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.122467994689941, loss=1.6922262907028198
I0206 11:35:44.140382 139946397853440 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.298793792724609, loss=1.9751029014587402
I0206 11:36:30.762766 139946414638848 logging_writer.py:48] [148000] global_step=148000, grad_norm=4.263918399810791, loss=4.061503887176514
I0206 11:37:17.171923 139946397853440 logging_writer.py:48] [148100] global_step=148100, grad_norm=4.20268440246582, loss=1.6984634399414062
I0206 11:37:43.242615 140107197974336 spec.py:321] Evaluating on the training split.
I0206 11:37:54.186318 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 11:38:32.536849 140107197974336 spec.py:349] Evaluating on the test split.
I0206 11:38:34.141000 140107197974336 submission_runner.py:408] Time since start: 75932.04s, 	Step: 148158, 	{'train/accuracy': 0.7961523532867432, 'train/loss': 0.7881932854652405, 'validation/accuracy': 0.7221599817276001, 'validation/loss': 1.1152422428131104, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.723463773727417, 'test/num_examples': 10000, 'score': 68097.77912926674, 'total_duration': 75932.03611707687, 'accumulated_submission_time': 68097.77912926674, 'accumulated_eval_time': 7818.040345191956, 'accumulated_logging_time': 7.759216070175171}
I0206 11:38:34.186134 139946414638848 logging_writer.py:48] [148158] accumulated_eval_time=7818.040345, accumulated_logging_time=7.759216, accumulated_submission_time=68097.779129, global_step=148158, preemption_count=0, score=68097.779129, test/accuracy=0.600700, test/loss=1.723464, test/num_examples=10000, total_duration=75932.036117, train/accuracy=0.796152, train/loss=0.788193, validation/accuracy=0.722160, validation/loss=1.115242, validation/num_examples=50000
I0206 11:38:51.271464 139946397853440 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.965482711791992, loss=1.6569769382476807
I0206 11:39:35.699099 139946414638848 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.855027675628662, loss=3.303490161895752
I0206 11:40:22.587533 139946397853440 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.8860414028167725, loss=1.927156925201416
I0206 11:41:08.981670 139946414638848 logging_writer.py:48] [148500] global_step=148500, grad_norm=4.480013370513916, loss=4.124825477600098
I0206 11:41:55.333112 139946397853440 logging_writer.py:48] [148600] global_step=148600, grad_norm=4.130090236663818, loss=4.168578147888184
I0206 11:42:42.034056 139946414638848 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.8512837886810303, loss=2.5934598445892334
I0206 11:43:28.443444 139946397853440 logging_writer.py:48] [148800] global_step=148800, grad_norm=4.526534080505371, loss=1.8220040798187256
I0206 11:44:14.922904 139946414638848 logging_writer.py:48] [148900] global_step=148900, grad_norm=4.014699935913086, loss=2.2286136150360107
I0206 11:45:01.373691 139946397853440 logging_writer.py:48] [149000] global_step=149000, grad_norm=4.036621570587158, loss=1.842979073524475
I0206 11:45:34.410573 140107197974336 spec.py:321] Evaluating on the training split.
I0206 11:45:45.036178 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 11:46:22.372016 140107197974336 spec.py:349] Evaluating on the test split.
I0206 11:46:23.977561 140107197974336 submission_runner.py:408] Time since start: 76401.87s, 	Step: 149073, 	{'train/accuracy': 0.7910937070846558, 'train/loss': 0.8174343109130859, 'validation/accuracy': 0.7259599566459656, 'validation/loss': 1.0968220233917236, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.7236626148223877, 'test/num_examples': 10000, 'score': 68517.94238138199, 'total_duration': 76401.87269186974, 'accumulated_submission_time': 68517.94238138199, 'accumulated_eval_time': 7867.6073389053345, 'accumulated_logging_time': 7.8136961460113525}
I0206 11:46:24.018187 139946414638848 logging_writer.py:48] [149073] accumulated_eval_time=7867.607339, accumulated_logging_time=7.813696, accumulated_submission_time=68517.942381, global_step=149073, preemption_count=0, score=68517.942381, test/accuracy=0.601400, test/loss=1.723663, test/num_examples=10000, total_duration=76401.872692, train/accuracy=0.791094, train/loss=0.817434, validation/accuracy=0.725960, validation/loss=1.096822, validation/num_examples=50000
I0206 11:46:35.021611 139946397853440 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.850809097290039, loss=3.010680913925171
I0206 11:47:18.526214 139946414638848 logging_writer.py:48] [149200] global_step=149200, grad_norm=4.495446681976318, loss=1.6996562480926514
I0206 11:48:04.820903 139946397853440 logging_writer.py:48] [149300] global_step=149300, grad_norm=4.060715198516846, loss=1.6482903957366943
I0206 11:48:51.554823 139946414638848 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.802759885787964, loss=3.3463094234466553
I0206 11:49:37.902189 139946397853440 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.019151210784912, loss=1.575479507446289
I0206 11:50:24.468036 139946414638848 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.9987728595733643, loss=2.8530073165893555
I0206 11:51:11.165317 139946397853440 logging_writer.py:48] [149700] global_step=149700, grad_norm=4.070709228515625, loss=3.1033105850219727
I0206 11:51:57.459644 139946414638848 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.194424152374268, loss=3.382922887802124
I0206 11:52:43.868536 139946397853440 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.545849800109863, loss=3.9442551136016846
I0206 11:53:24.131196 140107197974336 spec.py:321] Evaluating on the training split.
I0206 11:53:35.200091 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 11:54:11.529542 140107197974336 spec.py:349] Evaluating on the test split.
I0206 11:54:13.138556 140107197974336 submission_runner.py:408] Time since start: 76871.03s, 	Step: 149988, 	{'train/accuracy': 0.7939257621765137, 'train/loss': 0.8128007054328918, 'validation/accuracy': 0.7231400012969971, 'validation/loss': 1.1107922792434692, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.7529717683792114, 'test/num_examples': 10000, 'score': 68937.99177789688, 'total_duration': 76871.03368258476, 'accumulated_submission_time': 68937.99177789688, 'accumulated_eval_time': 7916.614735364914, 'accumulated_logging_time': 7.865723133087158}
I0206 11:54:13.180605 139946414638848 logging_writer.py:48] [149988] accumulated_eval_time=7916.614735, accumulated_logging_time=7.865723, accumulated_submission_time=68937.991778, global_step=149988, preemption_count=0, score=68937.991778, test/accuracy=0.602400, test/loss=1.752972, test/num_examples=10000, total_duration=76871.033683, train/accuracy=0.793926, train/loss=0.812801, validation/accuracy=0.723140, validation/loss=1.110792, validation/num_examples=50000
I0206 11:54:18.289952 139946397853440 logging_writer.py:48] [150000] global_step=150000, grad_norm=4.370495796203613, loss=2.3795981407165527
I0206 11:55:00.932352 139946414638848 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.420976638793945, loss=1.723204255104065
I0206 11:55:47.131915 139946397853440 logging_writer.py:48] [150200] global_step=150200, grad_norm=4.119096279144287, loss=1.7962692975997925
I0206 11:56:33.946849 139946414638848 logging_writer.py:48] [150300] global_step=150300, grad_norm=5.063682556152344, loss=3.8523926734924316
I0206 11:57:20.845024 139946397853440 logging_writer.py:48] [150400] global_step=150400, grad_norm=4.3708014488220215, loss=1.7483662366867065
I0206 11:58:07.477928 139946414638848 logging_writer.py:48] [150500] global_step=150500, grad_norm=4.226767063140869, loss=1.993933916091919
I0206 11:58:54.027297 139946397853440 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.118039608001709, loss=1.705521583557129
I0206 11:59:40.809375 139946414638848 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.34119987487793, loss=1.7627010345458984
I0206 12:00:27.481450 139946397853440 logging_writer.py:48] [150800] global_step=150800, grad_norm=4.269175052642822, loss=3.559122085571289
I0206 12:01:13.554037 140107197974336 spec.py:321] Evaluating on the training split.
I0206 12:01:24.719081 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 12:01:59.547957 140107197974336 spec.py:349] Evaluating on the test split.
I0206 12:02:01.141087 140107197974336 submission_runner.py:408] Time since start: 77339.04s, 	Step: 150900, 	{'train/accuracy': 0.80189448595047, 'train/loss': 0.7681282758712769, 'validation/accuracy': 0.7280600070953369, 'validation/loss': 1.0890097618103027, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.7106510400772095, 'test/num_examples': 10000, 'score': 69358.29975485802, 'total_duration': 77339.0362174511, 'accumulated_submission_time': 69358.29975485802, 'accumulated_eval_time': 7964.20180106163, 'accumulated_logging_time': 7.918400287628174}
I0206 12:02:01.185916 139946414638848 logging_writer.py:48] [150900] accumulated_eval_time=7964.201801, accumulated_logging_time=7.918400, accumulated_submission_time=69358.299755, global_step=150900, preemption_count=0, score=69358.299755, test/accuracy=0.603300, test/loss=1.710651, test/num_examples=10000, total_duration=77339.036217, train/accuracy=0.801894, train/loss=0.768128, validation/accuracy=0.728060, validation/loss=1.089010, validation/num_examples=50000
I0206 12:02:01.587593 139946397853440 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.972482919692993, loss=1.7979880571365356
I0206 12:02:43.365170 139946414638848 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.996164560317993, loss=3.155829429626465
I0206 12:03:29.687521 139946397853440 logging_writer.py:48] [151100] global_step=151100, grad_norm=4.456341743469238, loss=1.705255389213562
I0206 12:04:16.574708 139946414638848 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.364149570465088, loss=1.7596677541732788
I0206 12:05:03.214986 139946397853440 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.974003314971924, loss=4.042652130126953
I0206 12:05:49.612895 139946414638848 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.211289882659912, loss=1.6271992921829224
I0206 12:06:36.058058 139946397853440 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.8290276527404785, loss=1.8726993799209595
I0206 12:07:22.592931 139946414638848 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.1142072677612305, loss=1.6147738695144653
I0206 12:08:09.091716 139946397853440 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.739555835723877, loss=4.026566028594971
I0206 12:08:55.363078 139946414638848 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.071616172790527, loss=3.099618434906006
I0206 12:09:01.491528 140107197974336 spec.py:321] Evaluating on the training split.
I0206 12:09:12.780078 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 12:09:50.124611 140107197974336 spec.py:349] Evaluating on the test split.
I0206 12:09:51.734771 140107197974336 submission_runner.py:408] Time since start: 77809.63s, 	Step: 151815, 	{'train/accuracy': 0.7974413633346558, 'train/loss': 0.7807225584983826, 'validation/accuracy': 0.7289199829101562, 'validation/loss': 1.0758874416351318, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.681167483329773, 'test/num_examples': 10000, 'score': 69778.54154014587, 'total_duration': 77809.62989974022, 'accumulated_submission_time': 69778.54154014587, 'accumulated_eval_time': 8014.445042133331, 'accumulated_logging_time': 7.975107192993164}
I0206 12:09:51.790680 139946397853440 logging_writer.py:48] [151815] accumulated_eval_time=8014.445042, accumulated_logging_time=7.975107, accumulated_submission_time=69778.541540, global_step=151815, preemption_count=0, score=69778.541540, test/accuracy=0.614300, test/loss=1.681167, test/num_examples=10000, total_duration=77809.629900, train/accuracy=0.797441, train/loss=0.780723, validation/accuracy=0.728920, validation/loss=1.075887, validation/num_examples=50000
I0206 12:10:27.275538 139946414638848 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.718871116638184, loss=3.8980865478515625
I0206 12:11:13.828502 139946397853440 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.675347328186035, loss=3.228750705718994
I0206 12:12:00.271716 139946414638848 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.8252413272857666, loss=2.7400152683258057
I0206 12:12:46.707966 139946397853440 logging_writer.py:48] [152200] global_step=152200, grad_norm=4.241575717926025, loss=1.5972545146942139
I0206 12:13:33.337021 139946414638848 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.822134017944336, loss=2.8084871768951416
I0206 12:14:19.850719 139946397853440 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.419984340667725, loss=1.5801544189453125
I0206 12:15:06.690482 139946414638848 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.331124305725098, loss=1.8082270622253418
I0206 12:15:52.988437 139946397853440 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.3241190910339355, loss=1.7456914186477661
I0206 12:16:39.344377 139946414638848 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.588440895080566, loss=3.522378921508789
I0206 12:16:52.104578 140107197974336 spec.py:321] Evaluating on the training split.
I0206 12:17:03.273160 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 12:17:41.563211 140107197974336 spec.py:349] Evaluating on the test split.
I0206 12:17:43.154969 140107197974336 submission_runner.py:408] Time since start: 78281.05s, 	Step: 152729, 	{'train/accuracy': 0.8052343726158142, 'train/loss': 0.7729941606521606, 'validation/accuracy': 0.7317799925804138, 'validation/loss': 1.0814770460128784, 'validation/num_examples': 50000, 'test/accuracy': 0.6094000339508057, 'test/loss': 1.697064757347107, 'test/num_examples': 10000, 'score': 70198.79391741753, 'total_duration': 78281.05008983612, 'accumulated_submission_time': 70198.79391741753, 'accumulated_eval_time': 8065.495423555374, 'accumulated_logging_time': 8.04086971282959}
I0206 12:17:43.203705 139946397853440 logging_writer.py:48] [152729] accumulated_eval_time=8065.495424, accumulated_logging_time=8.040870, accumulated_submission_time=70198.793917, global_step=152729, preemption_count=0, score=70198.793917, test/accuracy=0.609400, test/loss=1.697065, test/num_examples=10000, total_duration=78281.050090, train/accuracy=0.805234, train/loss=0.772994, validation/accuracy=0.731780, validation/loss=1.081477, validation/num_examples=50000
I0206 12:18:11.931789 139946414638848 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.194720268249512, loss=2.5519537925720215
I0206 12:18:57.880279 139946397853440 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.34135103225708, loss=2.418126106262207
I0206 12:19:44.458530 139946414638848 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.756356716156006, loss=1.6787288188934326
I0206 12:20:30.932769 139946397853440 logging_writer.py:48] [153100] global_step=153100, grad_norm=5.418552398681641, loss=1.6737146377563477
I0206 12:21:17.363021 139946414638848 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.601001262664795, loss=1.8029639720916748
I0206 12:22:03.736860 139946397853440 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.611399173736572, loss=3.666156053543091
I0206 12:22:50.217378 139946414638848 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.162630081176758, loss=2.623121738433838
I0206 12:23:36.705988 139946397853440 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.151379108428955, loss=2.6928598880767822
I0206 12:24:23.190984 139946414638848 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.043491363525391, loss=2.0954179763793945
I0206 12:24:43.334203 140107197974336 spec.py:321] Evaluating on the training split.
I0206 12:24:54.290668 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 12:25:28.296453 140107197974336 spec.py:349] Evaluating on the test split.
I0206 12:25:29.894225 140107197974336 submission_runner.py:408] Time since start: 78747.79s, 	Step: 153644, 	{'train/accuracy': 0.805468738079071, 'train/loss': 0.7481812834739685, 'validation/accuracy': 0.7303999662399292, 'validation/loss': 1.0703809261322021, 'validation/num_examples': 50000, 'test/accuracy': 0.6111000180244446, 'test/loss': 1.6869773864746094, 'test/num_examples': 10000, 'score': 70618.86042189598, 'total_duration': 78747.78933882713, 'accumulated_submission_time': 70618.86042189598, 'accumulated_eval_time': 8112.055419683456, 'accumulated_logging_time': 8.100271940231323}
I0206 12:25:29.940834 139946397853440 logging_writer.py:48] [153644] accumulated_eval_time=8112.055420, accumulated_logging_time=8.100272, accumulated_submission_time=70618.860422, global_step=153644, preemption_count=0, score=70618.860422, test/accuracy=0.611100, test/loss=1.686977, test/num_examples=10000, total_duration=78747.789339, train/accuracy=0.805469, train/loss=0.748181, validation/accuracy=0.730400, validation/loss=1.070381, validation/num_examples=50000
I0206 12:25:52.343976 139946414638848 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.740373611450195, loss=1.7733336687088013
I0206 12:26:38.017240 139946397853440 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.53846549987793, loss=3.4969067573547363
I0206 12:27:24.671474 139946414638848 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.569149017333984, loss=2.7078473567962646
I0206 12:28:11.495062 139946397853440 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.0692243576049805, loss=2.3929922580718994
I0206 12:28:57.680758 139946414638848 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.145137310028076, loss=2.2949090003967285
I0206 12:29:44.124536 139946397853440 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.64422082901001, loss=1.6057122945785522
I0206 12:30:30.843645 139946414638848 logging_writer.py:48] [154300] global_step=154300, grad_norm=5.368321418762207, loss=4.059041500091553
I0206 12:31:17.337910 139946397853440 logging_writer.py:48] [154400] global_step=154400, grad_norm=5.432183742523193, loss=1.631651759147644
I0206 12:32:03.793550 139946414638848 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.518784999847412, loss=1.541319727897644
I0206 12:32:30.039789 140107197974336 spec.py:321] Evaluating on the training split.
I0206 12:32:40.956769 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 12:33:15.952486 140107197974336 spec.py:349] Evaluating on the test split.
I0206 12:33:17.546434 140107197974336 submission_runner.py:408] Time since start: 79215.44s, 	Step: 154558, 	{'train/accuracy': 0.806640625, 'train/loss': 0.7597796320915222, 'validation/accuracy': 0.7336199879646301, 'validation/loss': 1.068161129951477, 'validation/num_examples': 50000, 'test/accuracy': 0.6081000566482544, 'test/loss': 1.683420181274414, 'test/num_examples': 10000, 'score': 71038.89610767365, 'total_duration': 79215.44156551361, 'accumulated_submission_time': 71038.89610767365, 'accumulated_eval_time': 8159.562078952789, 'accumulated_logging_time': 8.1575448513031}
I0206 12:33:17.597421 139946397853440 logging_writer.py:48] [154558] accumulated_eval_time=8159.562079, accumulated_logging_time=8.157545, accumulated_submission_time=71038.896108, global_step=154558, preemption_count=0, score=71038.896108, test/accuracy=0.608100, test/loss=1.683420, test/num_examples=10000, total_duration=79215.441566, train/accuracy=0.806641, train/loss=0.759780, validation/accuracy=0.733620, validation/loss=1.068161, validation/num_examples=50000
I0206 12:33:34.486542 139946414638848 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.956747055053711, loss=1.7663600444793701
I0206 12:34:19.322843 139946397853440 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.6778764724731445, loss=1.5462310314178467
I0206 12:35:06.215317 139946414638848 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.934900760650635, loss=1.6835882663726807
I0206 12:35:52.742483 139946397853440 logging_writer.py:48] [154900] global_step=154900, grad_norm=5.249208927154541, loss=4.034871578216553
I0206 12:36:39.337096 139946414638848 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.336774826049805, loss=1.8868482112884521
I0206 12:37:25.947964 139946397853440 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.615638256072998, loss=1.528272032737732
I0206 12:38:12.372412 139946414638848 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.594363212585449, loss=3.197653293609619
I0206 12:38:58.931257 139946397853440 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.8500752449035645, loss=1.70734703540802
I0206 12:39:45.669566 139946414638848 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.873968601226807, loss=1.6242129802703857
I0206 12:40:17.554315 140107197974336 spec.py:321] Evaluating on the training split.
I0206 12:40:28.359792 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 12:41:02.479062 140107197974336 spec.py:349] Evaluating on the test split.
I0206 12:41:04.087059 140107197974336 submission_runner.py:408] Time since start: 79681.98s, 	Step: 155470, 	{'train/accuracy': 0.8057226538658142, 'train/loss': 0.7690586447715759, 'validation/accuracy': 0.7343399524688721, 'validation/loss': 1.0750396251678467, 'validation/num_examples': 50000, 'test/accuracy': 0.6116000413894653, 'test/loss': 1.6947035789489746, 'test/num_examples': 10000, 'score': 71458.79195690155, 'total_duration': 79681.98218774796, 'accumulated_submission_time': 71458.79195690155, 'accumulated_eval_time': 8206.094826936722, 'accumulated_logging_time': 8.21754264831543}
I0206 12:41:04.127002 139946397853440 logging_writer.py:48] [155470] accumulated_eval_time=8206.094827, accumulated_logging_time=8.217543, accumulated_submission_time=71458.791957, global_step=155470, preemption_count=0, score=71458.791957, test/accuracy=0.611600, test/loss=1.694704, test/num_examples=10000, total_duration=79681.982188, train/accuracy=0.805723, train/loss=0.769059, validation/accuracy=0.734340, validation/loss=1.075040, validation/num_examples=50000
I0206 12:41:16.294210 139946414638848 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.505158424377441, loss=1.5836048126220703
I0206 12:42:00.300570 139946397853440 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.985416412353516, loss=1.6128875017166138
I0206 12:42:46.619429 139946414638848 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.680887222290039, loss=1.676728367805481
I0206 12:43:33.243283 139946397853440 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.474355220794678, loss=1.5418468713760376
I0206 12:44:19.498105 139946414638848 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.851716041564941, loss=2.5336999893188477
I0206 12:45:06.143442 139946397853440 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.719205856323242, loss=3.36991024017334
I0206 12:45:52.517872 139946414638848 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.292449474334717, loss=2.003840446472168
I0206 12:46:39.034963 139946397853440 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.632985591888428, loss=2.395017385482788
I0206 12:47:25.537923 139946414638848 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.428442001342773, loss=1.5385963916778564
I0206 12:48:04.233712 140107197974336 spec.py:321] Evaluating on the training split.
I0206 12:48:15.046573 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 12:48:52.128414 140107197974336 spec.py:349] Evaluating on the test split.
I0206 12:48:53.726411 140107197974336 submission_runner.py:408] Time since start: 80151.62s, 	Step: 156385, 	{'train/accuracy': 0.8109179735183716, 'train/loss': 0.7232632040977478, 'validation/accuracy': 0.7350800037384033, 'validation/loss': 1.0466346740722656, 'validation/num_examples': 50000, 'test/accuracy': 0.6154000163078308, 'test/loss': 1.6492758989334106, 'test/num_examples': 10000, 'score': 71878.8363673687, 'total_duration': 80151.62154054642, 'accumulated_submission_time': 71878.8363673687, 'accumulated_eval_time': 8255.587515115738, 'accumulated_logging_time': 8.26708173751831}
I0206 12:48:53.773226 139946397853440 logging_writer.py:48] [156385] accumulated_eval_time=8255.587515, accumulated_logging_time=8.267082, accumulated_submission_time=71878.836367, global_step=156385, preemption_count=0, score=71878.836367, test/accuracy=0.615400, test/loss=1.649276, test/num_examples=10000, total_duration=80151.621541, train/accuracy=0.810918, train/loss=0.723263, validation/accuracy=0.735080, validation/loss=1.046635, validation/num_examples=50000
I0206 12:49:00.051488 139946414638848 logging_writer.py:48] [156400] global_step=156400, grad_norm=5.0516533851623535, loss=1.6527713537216187
I0206 12:49:42.651324 139946397853440 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.340376377105713, loss=1.7670249938964844
I0206 12:50:28.708914 139946414638848 logging_writer.py:48] [156600] global_step=156600, grad_norm=5.127074241638184, loss=1.5334440469741821
I0206 12:51:15.226486 139946397853440 logging_writer.py:48] [156700] global_step=156700, grad_norm=5.476962566375732, loss=4.04505729675293
I0206 12:52:01.371580 139946414638848 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.772568702697754, loss=3.0062685012817383
I0206 12:52:47.980046 139946397853440 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.6270670890808105, loss=1.4812138080596924
I0206 12:53:34.583386 139946414638848 logging_writer.py:48] [157000] global_step=157000, grad_norm=5.080358028411865, loss=1.611507773399353
I0206 12:54:20.961349 139946397853440 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.797382354736328, loss=2.421354293823242
I0206 12:55:07.536817 139946414638848 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.473964691162109, loss=2.7407519817352295
I0206 12:55:53.693097 139946397853440 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.949353218078613, loss=1.612807035446167
I0206 12:55:53.854569 140107197974336 spec.py:321] Evaluating on the training split.
I0206 12:56:04.719906 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 12:56:38.238602 140107197974336 spec.py:349] Evaluating on the test split.
I0206 12:56:39.838062 140107197974336 submission_runner.py:408] Time since start: 80617.73s, 	Step: 157302, 	{'train/accuracy': 0.8225781321525574, 'train/loss': 0.6812586188316345, 'validation/accuracy': 0.739139974117279, 'validation/loss': 1.0413532257080078, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.6407500505447388, 'test/num_examples': 10000, 'score': 72298.85396432877, 'total_duration': 80617.73319363594, 'accumulated_submission_time': 72298.85396432877, 'accumulated_eval_time': 8301.570994377136, 'accumulated_logging_time': 8.324889659881592}
I0206 12:56:39.882322 139946414638848 logging_writer.py:48] [157302] accumulated_eval_time=8301.570994, accumulated_logging_time=8.324890, accumulated_submission_time=72298.853964, global_step=157302, preemption_count=0, score=72298.853964, test/accuracy=0.616400, test/loss=1.640750, test/num_examples=10000, total_duration=80617.733194, train/accuracy=0.822578, train/loss=0.681259, validation/accuracy=0.739140, validation/loss=1.041353, validation/num_examples=50000
I0206 12:57:21.355053 139946397853440 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.948981285095215, loss=1.5197765827178955
I0206 12:58:07.721989 139946414638848 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.807165622711182, loss=1.5103956460952759
I0206 12:58:54.553191 139946397853440 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.4776482582092285, loss=2.085875988006592
I0206 12:59:40.736475 139946414638848 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.905571937561035, loss=1.4923044443130493
I0206 13:00:27.526572 139946397853440 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.852001667022705, loss=1.5850138664245605
I0206 13:01:14.281993 139946414638848 logging_writer.py:48] [157900] global_step=157900, grad_norm=5.366985321044922, loss=3.948101043701172
I0206 13:02:00.794244 139946397853440 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.799607753753662, loss=2.4633119106292725
I0206 13:02:47.416481 139946414638848 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.784712314605713, loss=1.786524772644043
I0206 13:03:34.050753 139946397853440 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.638245105743408, loss=2.9086129665374756
I0206 13:03:40.240264 140107197974336 spec.py:321] Evaluating on the training split.
I0206 13:03:51.246311 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 13:04:27.429306 140107197974336 spec.py:349] Evaluating on the test split.
I0206 13:04:29.024440 140107197974336 submission_runner.py:408] Time since start: 81086.92s, 	Step: 158215, 	{'train/accuracy': 0.8095898032188416, 'train/loss': 0.7571691870689392, 'validation/accuracy': 0.7379199862480164, 'validation/loss': 1.0595265626907349, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.6860063076019287, 'test/num_examples': 10000, 'score': 72719.15087842941, 'total_duration': 81086.91956949234, 'accumulated_submission_time': 72719.15087842941, 'accumulated_eval_time': 8350.355193376541, 'accumulated_logging_time': 8.37764310836792}
I0206 13:04:29.065419 139946414638848 logging_writer.py:48] [158215] accumulated_eval_time=8350.355193, accumulated_logging_time=8.377643, accumulated_submission_time=72719.150878, global_step=158215, preemption_count=0, score=72719.150878, test/accuracy=0.610800, test/loss=1.686006, test/num_examples=10000, total_duration=81086.919569, train/accuracy=0.809590, train/loss=0.757169, validation/accuracy=0.737920, validation/loss=1.059527, validation/num_examples=50000
I0206 13:05:04.415890 139946397853440 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.839529037475586, loss=2.8138294219970703
I0206 13:05:50.594776 139946414638848 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.695428848266602, loss=1.4709278345108032
I0206 13:06:37.053763 139946397853440 logging_writer.py:48] [158500] global_step=158500, grad_norm=6.03205680847168, loss=4.045566082000732
I0206 13:07:23.588074 139946414638848 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.788139820098877, loss=1.6057941913604736
I0206 13:08:10.271482 139946397853440 logging_writer.py:48] [158700] global_step=158700, grad_norm=5.278650760650635, loss=1.493769884109497
I0206 13:08:56.425622 139946414638848 logging_writer.py:48] [158800] global_step=158800, grad_norm=5.253671169281006, loss=1.47598397731781
I0206 13:09:42.751577 139946397853440 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.923834323883057, loss=2.553971529006958
I0206 13:10:29.113327 139946414638848 logging_writer.py:48] [159000] global_step=159000, grad_norm=5.180175304412842, loss=1.5694727897644043
I0206 13:11:15.642276 139946397853440 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.935128211975098, loss=1.5261590480804443
I0206 13:11:29.310856 140107197974336 spec.py:321] Evaluating on the training split.
I0206 13:11:40.146572 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 13:12:13.555431 140107197974336 spec.py:349] Evaluating on the test split.
I0206 13:12:15.164549 140107197974336 submission_runner.py:408] Time since start: 81553.06s, 	Step: 159131, 	{'train/accuracy': 0.8150585889816284, 'train/loss': 0.7215979695320129, 'validation/accuracy': 0.7387599945068359, 'validation/loss': 1.0488839149475098, 'validation/num_examples': 50000, 'test/accuracy': 0.6171000003814697, 'test/loss': 1.6728469133377075, 'test/num_examples': 10000, 'score': 73139.33359980583, 'total_duration': 81553.05968117714, 'accumulated_submission_time': 73139.33359980583, 'accumulated_eval_time': 8396.208889245987, 'accumulated_logging_time': 8.428681373596191}
I0206 13:12:15.213792 139946414638848 logging_writer.py:48] [159131] accumulated_eval_time=8396.208889, accumulated_logging_time=8.428681, accumulated_submission_time=73139.333600, global_step=159131, preemption_count=0, score=73139.333600, test/accuracy=0.617100, test/loss=1.672847, test/num_examples=10000, total_duration=81553.059681, train/accuracy=0.815059, train/loss=0.721598, validation/accuracy=0.738760, validation/loss=1.048884, validation/num_examples=50000
I0206 13:12:43.214925 139946397853440 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.5852155685424805, loss=2.210399627685547
I0206 13:13:29.325387 139946414638848 logging_writer.py:48] [159300] global_step=159300, grad_norm=5.027681350708008, loss=2.269179344177246
I0206 13:14:16.038703 139946397853440 logging_writer.py:48] [159400] global_step=159400, grad_norm=5.152020454406738, loss=1.5330383777618408
I0206 13:15:02.617388 139946414638848 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.992796897888184, loss=1.4631874561309814
I0206 13:15:48.841734 139946397853440 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.75559663772583, loss=1.494653344154358
I0206 13:16:35.309901 139946414638848 logging_writer.py:48] [159700] global_step=159700, grad_norm=5.608814239501953, loss=1.8479647636413574
I0206 13:17:21.606744 139946397853440 logging_writer.py:48] [159800] global_step=159800, grad_norm=6.301324367523193, loss=3.9271929264068604
I0206 13:18:08.197011 139946414638848 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.695035934448242, loss=1.4894789457321167
I0206 13:18:54.530463 139946397853440 logging_writer.py:48] [160000] global_step=160000, grad_norm=5.4033074378967285, loss=3.2238481044769287
I0206 13:19:15.305077 140107197974336 spec.py:321] Evaluating on the training split.
I0206 13:19:26.258319 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 13:20:03.952478 140107197974336 spec.py:349] Evaluating on the test split.
I0206 13:20:05.554793 140107197974336 submission_runner.py:408] Time since start: 82023.45s, 	Step: 160046, 	{'train/accuracy': 0.8247265219688416, 'train/loss': 0.6886405944824219, 'validation/accuracy': 0.7408199906349182, 'validation/loss': 1.0412501096725464, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.6554361581802368, 'test/num_examples': 10000, 'score': 73559.36250901222, 'total_duration': 82023.44992232323, 'accumulated_submission_time': 73559.36250901222, 'accumulated_eval_time': 8446.458607912064, 'accumulated_logging_time': 8.487337112426758}
I0206 13:20:05.595353 139946414638848 logging_writer.py:48] [160046] accumulated_eval_time=8446.458608, accumulated_logging_time=8.487337, accumulated_submission_time=73559.362509, global_step=160046, preemption_count=0, score=73559.362509, test/accuracy=0.620600, test/loss=1.655436, test/num_examples=10000, total_duration=82023.449922, train/accuracy=0.824727, train/loss=0.688641, validation/accuracy=0.740820, validation/loss=1.041250, validation/num_examples=50000
I0206 13:20:27.208499 139946397853440 logging_writer.py:48] [160100] global_step=160100, grad_norm=5.784505844116211, loss=3.8391783237457275
I0206 13:21:12.763397 139946414638848 logging_writer.py:48] [160200] global_step=160200, grad_norm=5.25966215133667, loss=1.5340546369552612
I0206 13:21:58.876861 139946397853440 logging_writer.py:48] [160300] global_step=160300, grad_norm=5.938533306121826, loss=1.5964720249176025
I0206 13:22:45.877367 139946414638848 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.72793436050415, loss=2.1724867820739746
I0206 13:23:32.213717 139946397853440 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.839757442474365, loss=1.3917757272720337
I0206 13:24:18.925750 139946414638848 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.720094203948975, loss=1.5185914039611816
I0206 13:25:05.451431 139946397853440 logging_writer.py:48] [160700] global_step=160700, grad_norm=5.169382572174072, loss=1.5057181119918823
I0206 13:25:52.038710 139946414638848 logging_writer.py:48] [160800] global_step=160800, grad_norm=5.620215892791748, loss=2.170738935470581
I0206 13:26:38.681012 139946397853440 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.665944576263428, loss=2.156395196914673
I0206 13:27:06.043394 140107197974336 spec.py:321] Evaluating on the training split.
I0206 13:27:16.942632 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 13:27:52.660542 140107197974336 spec.py:349] Evaluating on the test split.
I0206 13:27:54.254697 140107197974336 submission_runner.py:408] Time since start: 82492.15s, 	Step: 160960, 	{'train/accuracy': 0.8200390338897705, 'train/loss': 0.7016811370849609, 'validation/accuracy': 0.744439959526062, 'validation/loss': 1.0223753452301025, 'validation/num_examples': 50000, 'test/accuracy': 0.6252000331878662, 'test/loss': 1.632118582725525, 'test/num_examples': 10000, 'score': 73979.74830436707, 'total_duration': 82492.14982962608, 'accumulated_submission_time': 73979.74830436707, 'accumulated_eval_time': 8494.669929981232, 'accumulated_logging_time': 8.537365913391113}
I0206 13:27:54.298979 139946414638848 logging_writer.py:48] [160960] accumulated_eval_time=8494.669930, accumulated_logging_time=8.537366, accumulated_submission_time=73979.748304, global_step=160960, preemption_count=0, score=73979.748304, test/accuracy=0.625200, test/loss=1.632119, test/num_examples=10000, total_duration=82492.149830, train/accuracy=0.820039, train/loss=0.701681, validation/accuracy=0.744440, validation/loss=1.022375, validation/num_examples=50000
I0206 13:28:10.406506 139946397853440 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.916927814483643, loss=1.834835410118103
I0206 13:28:54.786338 139946414638848 logging_writer.py:48] [161100] global_step=161100, grad_norm=5.532023906707764, loss=1.654598593711853
I0206 13:29:41.401136 139946397853440 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.891474723815918, loss=1.5686581134796143
I0206 13:30:28.260735 139946414638848 logging_writer.py:48] [161300] global_step=161300, grad_norm=5.178445816040039, loss=1.527553915977478
I0206 13:31:14.778707 139946397853440 logging_writer.py:48] [161400] global_step=161400, grad_norm=5.6864166259765625, loss=3.738478660583496
I0206 13:32:01.384039 139946414638848 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.997367858886719, loss=2.459876775741577
I0206 13:32:47.961815 139946397853440 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.596336841583252, loss=2.2283577919006348
I0206 13:33:34.738270 139946414638848 logging_writer.py:48] [161700] global_step=161700, grad_norm=5.443028926849365, loss=1.4407522678375244
I0206 13:34:21.114516 139946397853440 logging_writer.py:48] [161800] global_step=161800, grad_norm=6.239277362823486, loss=3.8864619731903076
I0206 13:34:54.444756 140107197974336 spec.py:321] Evaluating on the training split.
I0206 13:35:05.428661 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 13:35:40.125706 140107197974336 spec.py:349] Evaluating on the test split.
I0206 13:35:41.727313 140107197974336 submission_runner.py:408] Time since start: 82959.62s, 	Step: 161873, 	{'train/accuracy': 0.8234765529632568, 'train/loss': 0.682098388671875, 'validation/accuracy': 0.7447400093078613, 'validation/loss': 1.0194628238677979, 'validation/num_examples': 50000, 'test/accuracy': 0.6238000392913818, 'test/loss': 1.6342037916183472, 'test/num_examples': 10000, 'score': 74399.8327550888, 'total_duration': 82959.62244343758, 'accumulated_submission_time': 74399.8327550888, 'accumulated_eval_time': 8541.95247745514, 'accumulated_logging_time': 8.591475486755371}
I0206 13:35:41.770800 139946414638848 logging_writer.py:48] [161873] accumulated_eval_time=8541.952477, accumulated_logging_time=8.591475, accumulated_submission_time=74399.832755, global_step=161873, preemption_count=0, score=74399.832755, test/accuracy=0.623800, test/loss=1.634204, test/num_examples=10000, total_duration=82959.622443, train/accuracy=0.823477, train/loss=0.682098, validation/accuracy=0.744740, validation/loss=1.019463, validation/num_examples=50000
I0206 13:35:52.775030 139946397853440 logging_writer.py:48] [161900] global_step=161900, grad_norm=5.248267650604248, loss=2.5571906566619873
I0206 13:36:36.700740 139946414638848 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.837347507476807, loss=1.8495018482208252
I0206 13:37:22.946666 139946397853440 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.96741247177124, loss=1.4475698471069336
I0206 13:38:09.694087 139946414638848 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.940322399139404, loss=2.4629509449005127
I0206 13:38:56.084627 139946397853440 logging_writer.py:48] [162300] global_step=162300, grad_norm=5.3506059646606445, loss=1.6478787660598755
I0206 13:39:42.705128 139946414638848 logging_writer.py:48] [162400] global_step=162400, grad_norm=5.000448703765869, loss=2.6936123371124268
I0206 13:40:29.250987 139946397853440 logging_writer.py:48] [162500] global_step=162500, grad_norm=5.460960865020752, loss=1.5915590524673462
I0206 13:41:15.831100 139946414638848 logging_writer.py:48] [162600] global_step=162600, grad_norm=5.197373867034912, loss=1.237559199333191
I0206 13:42:01.843841 139946397853440 logging_writer.py:48] [162700] global_step=162700, grad_norm=5.57489013671875, loss=3.2916769981384277
I0206 13:42:41.764501 140107197974336 spec.py:321] Evaluating on the training split.
I0206 13:42:52.702198 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 13:43:29.531072 140107197974336 spec.py:349] Evaluating on the test split.
I0206 13:43:31.128830 140107197974336 submission_runner.py:408] Time since start: 83429.02s, 	Step: 162788, 	{'train/accuracy': 0.8300195336341858, 'train/loss': 0.6636582612991333, 'validation/accuracy': 0.7465400099754333, 'validation/loss': 1.0095263719558716, 'validation/num_examples': 50000, 'test/accuracy': 0.6231000423431396, 'test/loss': 1.6292407512664795, 'test/num_examples': 10000, 'score': 74819.76510477066, 'total_duration': 83429.02394080162, 'accumulated_submission_time': 74819.76510477066, 'accumulated_eval_time': 8591.316792964935, 'accumulated_logging_time': 8.644585847854614}
I0206 13:43:31.179104 139946414638848 logging_writer.py:48] [162788] accumulated_eval_time=8591.316793, accumulated_logging_time=8.644586, accumulated_submission_time=74819.765105, global_step=162788, preemption_count=0, score=74819.765105, test/accuracy=0.623100, test/loss=1.629241, test/num_examples=10000, total_duration=83429.023941, train/accuracy=0.830020, train/loss=0.663658, validation/accuracy=0.746540, validation/loss=1.009526, validation/num_examples=50000
I0206 13:43:36.290648 139946397853440 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.772520542144775, loss=2.0249648094177246
I0206 13:44:18.834708 139946414638848 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.911681175231934, loss=1.5236173868179321
I0206 13:45:05.240616 139946397853440 logging_writer.py:48] [163000] global_step=163000, grad_norm=5.084396839141846, loss=1.81184720993042
I0206 13:45:51.852493 139946414638848 logging_writer.py:48] [163100] global_step=163100, grad_norm=5.154715538024902, loss=1.4252662658691406
I0206 13:46:38.257213 139946397853440 logging_writer.py:48] [163200] global_step=163200, grad_norm=5.388487815856934, loss=1.4762041568756104
I0206 13:47:24.796219 139946414638848 logging_writer.py:48] [163300] global_step=163300, grad_norm=5.390718460083008, loss=1.483554720878601
I0206 13:48:11.301680 139946397853440 logging_writer.py:48] [163400] global_step=163400, grad_norm=5.423767566680908, loss=1.4804565906524658
I0206 13:48:57.771352 139946414638848 logging_writer.py:48] [163500] global_step=163500, grad_norm=5.608236789703369, loss=1.536693811416626
I0206 13:49:44.262607 139946397853440 logging_writer.py:48] [163600] global_step=163600, grad_norm=5.237855911254883, loss=1.8200416564941406
I0206 13:50:31.060509 139946414638848 logging_writer.py:48] [163700] global_step=163700, grad_norm=5.599106788635254, loss=1.6035871505737305
I0206 13:50:31.262686 140107197974336 spec.py:321] Evaluating on the training split.
I0206 13:50:43.121139 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 13:51:15.026555 140107197974336 spec.py:349] Evaluating on the test split.
I0206 13:51:16.629983 140107197974336 submission_runner.py:408] Time since start: 83894.53s, 	Step: 163702, 	{'train/accuracy': 0.8236327767372131, 'train/loss': 0.6771386861801147, 'validation/accuracy': 0.7473399639129639, 'validation/loss': 1.0083914995193481, 'validation/num_examples': 50000, 'test/accuracy': 0.6289000511169434, 'test/loss': 1.6085121631622314, 'test/num_examples': 10000, 'score': 75239.78667020798, 'total_duration': 83894.52511429787, 'accumulated_submission_time': 75239.78667020798, 'accumulated_eval_time': 8636.684086561203, 'accumulated_logging_time': 8.705162763595581}
I0206 13:51:16.671483 139946397853440 logging_writer.py:48] [163702] accumulated_eval_time=8636.684087, accumulated_logging_time=8.705163, accumulated_submission_time=75239.786670, global_step=163702, preemption_count=0, score=75239.786670, test/accuracy=0.628900, test/loss=1.608512, test/num_examples=10000, total_duration=83894.525114, train/accuracy=0.823633, train/loss=0.677139, validation/accuracy=0.747340, validation/loss=1.008391, validation/num_examples=50000
I0206 13:51:57.742892 139946414638848 logging_writer.py:48] [163800] global_step=163800, grad_norm=5.825021266937256, loss=3.1773428916931152
I0206 13:52:43.920948 139946397853440 logging_writer.py:48] [163900] global_step=163900, grad_norm=6.15103816986084, loss=1.6202783584594727
I0206 13:53:30.432068 139946414638848 logging_writer.py:48] [164000] global_step=164000, grad_norm=5.834813117980957, loss=1.532431960105896
I0206 13:54:17.040794 139946397853440 logging_writer.py:48] [164100] global_step=164100, grad_norm=5.63258171081543, loss=1.5384927988052368
I0206 13:55:03.759966 139946414638848 logging_writer.py:48] [164200] global_step=164200, grad_norm=5.652609825134277, loss=1.4967424869537354
I0206 13:55:50.155317 139946397853440 logging_writer.py:48] [164300] global_step=164300, grad_norm=5.673875331878662, loss=1.4575517177581787
I0206 13:56:36.634584 139946414638848 logging_writer.py:48] [164400] global_step=164400, grad_norm=5.241318225860596, loss=1.5079306364059448
I0206 13:57:23.078432 139946397853440 logging_writer.py:48] [164500] global_step=164500, grad_norm=5.229831218719482, loss=1.4477041959762573
I0206 13:58:09.671119 139946414638848 logging_writer.py:48] [164600] global_step=164600, grad_norm=5.6354804039001465, loss=1.3750898838043213
I0206 13:58:16.717463 140107197974336 spec.py:321] Evaluating on the training split.
I0206 13:58:27.865787 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 13:59:02.741372 140107197974336 spec.py:349] Evaluating on the test split.
I0206 13:59:04.333760 140107197974336 submission_runner.py:408] Time since start: 84362.23s, 	Step: 164617, 	{'train/accuracy': 0.8244921565055847, 'train/loss': 0.6821433305740356, 'validation/accuracy': 0.7472999691963196, 'validation/loss': 1.0059750080108643, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.6200848817825317, 'test/num_examples': 10000, 'score': 75659.76452445984, 'total_duration': 84362.22888803482, 'accumulated_submission_time': 75659.76452445984, 'accumulated_eval_time': 8684.300383806229, 'accumulated_logging_time': 8.755192041397095}
I0206 13:59:04.375992 139946397853440 logging_writer.py:48] [164617] accumulated_eval_time=8684.300384, accumulated_logging_time=8.755192, accumulated_submission_time=75659.764524, global_step=164617, preemption_count=0, score=75659.764524, test/accuracy=0.626500, test/loss=1.620085, test/num_examples=10000, total_duration=84362.228888, train/accuracy=0.824492, train/loss=0.682143, validation/accuracy=0.747300, validation/loss=1.005975, validation/num_examples=50000
I0206 13:59:38.847818 139946414638848 logging_writer.py:48] [164700] global_step=164700, grad_norm=5.765763282775879, loss=1.692591905593872
I0206 14:00:25.123989 139946397853440 logging_writer.py:48] [164800] global_step=164800, grad_norm=6.191189765930176, loss=1.463030219078064
I0206 14:01:12.036937 139946414638848 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.918087005615234, loss=2.751024007797241
I0206 14:01:58.681132 139946397853440 logging_writer.py:48] [165000] global_step=165000, grad_norm=6.1042304039001465, loss=1.4324302673339844
I0206 14:02:45.131864 139946414638848 logging_writer.py:48] [165100] global_step=165100, grad_norm=5.436975479125977, loss=1.525513768196106
I0206 14:03:31.505259 139946397853440 logging_writer.py:48] [165200] global_step=165200, grad_norm=5.234057903289795, loss=2.558032512664795
I0206 14:04:17.941263 139946414638848 logging_writer.py:48] [165300] global_step=165300, grad_norm=7.250857353210449, loss=3.6295828819274902
I0206 14:05:04.611614 139946397853440 logging_writer.py:48] [165400] global_step=165400, grad_norm=5.961100101470947, loss=1.4640018939971924
I0206 14:05:51.061029 139946414638848 logging_writer.py:48] [165500] global_step=165500, grad_norm=5.9237823486328125, loss=1.4704675674438477
I0206 14:06:04.697937 140107197974336 spec.py:321] Evaluating on the training split.
I0206 14:06:15.524302 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 14:06:52.798945 140107197974336 spec.py:349] Evaluating on the test split.
I0206 14:06:54.392452 140107197974336 submission_runner.py:408] Time since start: 84832.29s, 	Step: 165531, 	{'train/accuracy': 0.8278319835662842, 'train/loss': 0.6549848914146423, 'validation/accuracy': 0.7478399872779846, 'validation/loss': 0.9953515529632568, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.6096539497375488, 'test/num_examples': 10000, 'score': 76080.02516317368, 'total_duration': 84832.28757619858, 'accumulated_submission_time': 76080.02516317368, 'accumulated_eval_time': 8733.994886159897, 'accumulated_logging_time': 8.806623458862305}
I0206 14:06:54.438939 139946397853440 logging_writer.py:48] [165531] accumulated_eval_time=8733.994886, accumulated_logging_time=8.806623, accumulated_submission_time=76080.025163, global_step=165531, preemption_count=0, score=76080.025163, test/accuracy=0.628200, test/loss=1.609654, test/num_examples=10000, total_duration=84832.287576, train/accuracy=0.827832, train/loss=0.654985, validation/accuracy=0.747840, validation/loss=0.995352, validation/num_examples=50000
I0206 14:07:22.420067 139946414638848 logging_writer.py:48] [165600] global_step=165600, grad_norm=5.413254737854004, loss=1.4846781492233276
I0206 14:08:08.512324 139946397853440 logging_writer.py:48] [165700] global_step=165700, grad_norm=5.6964111328125, loss=1.629636287689209
I0206 14:08:54.836912 139946414638848 logging_writer.py:48] [165800] global_step=165800, grad_norm=5.549487113952637, loss=1.403509497642517
I0206 14:09:41.328198 139946397853440 logging_writer.py:48] [165900] global_step=165900, grad_norm=6.461187839508057, loss=1.5627927780151367
I0206 14:10:27.847491 139946414638848 logging_writer.py:48] [166000] global_step=166000, grad_norm=6.632375717163086, loss=1.838341474533081
I0206 14:11:14.534154 139946397853440 logging_writer.py:48] [166100] global_step=166100, grad_norm=5.473381042480469, loss=2.0626277923583984
I0206 14:12:00.776336 139946414638848 logging_writer.py:48] [166200] global_step=166200, grad_norm=5.872434616088867, loss=1.4017248153686523
I0206 14:12:47.229393 139946397853440 logging_writer.py:48] [166300] global_step=166300, grad_norm=5.7911376953125, loss=1.441881775856018
I0206 14:13:33.668984 139946414638848 logging_writer.py:48] [166400] global_step=166400, grad_norm=5.5878705978393555, loss=1.4721651077270508
I0206 14:13:54.649945 140107197974336 spec.py:321] Evaluating on the training split.
I0206 14:14:05.745436 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 14:14:38.435436 140107197974336 spec.py:349] Evaluating on the test split.
I0206 14:14:40.029439 140107197974336 submission_runner.py:408] Time since start: 85297.92s, 	Step: 166447, 	{'train/accuracy': 0.8307226300239563, 'train/loss': 0.6519699096679688, 'validation/accuracy': 0.7495200037956238, 'validation/loss': 0.9923651218414307, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.6084636449813843, 'test/num_examples': 10000, 'score': 76500.17499065399, 'total_duration': 85297.9245531559, 'accumulated_submission_time': 76500.17499065399, 'accumulated_eval_time': 8779.374361515045, 'accumulated_logging_time': 8.862721681594849}
I0206 14:14:40.082137 139946397853440 logging_writer.py:48] [166447] accumulated_eval_time=8779.374362, accumulated_logging_time=8.862722, accumulated_submission_time=76500.174991, global_step=166447, preemption_count=0, score=76500.174991, test/accuracy=0.631800, test/loss=1.608464, test/num_examples=10000, total_duration=85297.924553, train/accuracy=0.830723, train/loss=0.651970, validation/accuracy=0.749520, validation/loss=0.992365, validation/num_examples=50000
I0206 14:15:01.300127 139946414638848 logging_writer.py:48] [166500] global_step=166500, grad_norm=5.586864948272705, loss=3.2793047428131104
I0206 14:15:46.835473 139946397853440 logging_writer.py:48] [166600] global_step=166600, grad_norm=6.00663423538208, loss=1.4552217721939087
I0206 14:16:33.595076 139946414638848 logging_writer.py:48] [166700] global_step=166700, grad_norm=5.98164176940918, loss=2.154442071914673
I0206 14:17:20.323481 139946397853440 logging_writer.py:48] [166800] global_step=166800, grad_norm=6.335328578948975, loss=1.4501316547393799
I0206 14:18:06.883784 139946414638848 logging_writer.py:48] [166900] global_step=166900, grad_norm=5.530137538909912, loss=1.4219622611999512
I0206 14:18:53.317766 139946397853440 logging_writer.py:48] [167000] global_step=167000, grad_norm=5.421854496002197, loss=2.659203052520752
I0206 14:19:40.020852 139946414638848 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.946432590484619, loss=3.18926739692688
I0206 14:20:26.634846 139946397853440 logging_writer.py:48] [167200] global_step=167200, grad_norm=5.940501689910889, loss=1.9284274578094482
I0206 14:21:13.329168 139946414638848 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.752539157867432, loss=3.3494608402252197
I0206 14:21:40.130100 140107197974336 spec.py:321] Evaluating on the training split.
I0206 14:21:50.969131 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 14:22:28.111494 140107197974336 spec.py:349] Evaluating on the test split.
I0206 14:22:29.707682 140107197974336 submission_runner.py:408] Time since start: 85767.60s, 	Step: 167359, 	{'train/accuracy': 0.8291015625, 'train/loss': 0.6523779034614563, 'validation/accuracy': 0.7506399750709534, 'validation/loss': 0.9864511489868164, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.5886270999908447, 'test/num_examples': 10000, 'score': 76920.16060471535, 'total_duration': 85767.60281062126, 'accumulated_submission_time': 76920.16060471535, 'accumulated_eval_time': 8828.95196390152, 'accumulated_logging_time': 8.925573348999023}
I0206 14:22:29.752377 139946397853440 logging_writer.py:48] [167359] accumulated_eval_time=8828.951964, accumulated_logging_time=8.925573, accumulated_submission_time=76920.160605, global_step=167359, preemption_count=0, score=76920.160605, test/accuracy=0.629700, test/loss=1.588627, test/num_examples=10000, total_duration=85767.602811, train/accuracy=0.829102, train/loss=0.652378, validation/accuracy=0.750640, validation/loss=0.986451, validation/num_examples=50000
I0206 14:22:46.250067 139946414638848 logging_writer.py:48] [167400] global_step=167400, grad_norm=5.594123363494873, loss=1.362959623336792
I0206 14:23:30.980032 139946397853440 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.527276515960693, loss=1.3642812967300415
I0206 14:24:17.535904 139946414638848 logging_writer.py:48] [167600] global_step=167600, grad_norm=6.566524982452393, loss=3.792185068130493
I0206 14:25:04.284275 139946397853440 logging_writer.py:48] [167700] global_step=167700, grad_norm=5.8834662437438965, loss=1.5930979251861572
I0206 14:25:50.577941 139946414638848 logging_writer.py:48] [167800] global_step=167800, grad_norm=5.663102149963379, loss=2.7373430728912354
I0206 14:26:37.324796 139946397853440 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.7982707023620605, loss=1.353971004486084
I0206 14:27:23.977844 139946414638848 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.72273588180542, loss=1.4845609664916992
I0206 14:28:10.420792 139946397853440 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.792393207550049, loss=1.4376589059829712
I0206 14:28:57.106084 139946414638848 logging_writer.py:48] [168200] global_step=168200, grad_norm=5.502488613128662, loss=1.6083238124847412
I0206 14:29:30.136715 140107197974336 spec.py:321] Evaluating on the training split.
I0206 14:29:40.833191 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 14:30:17.486939 140107197974336 spec.py:349] Evaluating on the test split.
I0206 14:30:19.085722 140107197974336 submission_runner.py:408] Time since start: 86236.98s, 	Step: 168272, 	{'train/accuracy': 0.8313866853713989, 'train/loss': 0.637546181678772, 'validation/accuracy': 0.753600001335144, 'validation/loss': 0.9787457585334778, 'validation/num_examples': 50000, 'test/accuracy': 0.6333000063896179, 'test/loss': 1.587046504020691, 'test/num_examples': 10000, 'score': 77340.48335027695, 'total_duration': 86236.98085284233, 'accumulated_submission_time': 77340.48335027695, 'accumulated_eval_time': 8877.900985002518, 'accumulated_logging_time': 8.980156898498535}
I0206 14:30:19.131752 139946397853440 logging_writer.py:48] [168272] accumulated_eval_time=8877.900985, accumulated_logging_time=8.980157, accumulated_submission_time=77340.483350, global_step=168272, preemption_count=0, score=77340.483350, test/accuracy=0.633300, test/loss=1.587047, test/num_examples=10000, total_duration=86236.980853, train/accuracy=0.831387, train/loss=0.637546, validation/accuracy=0.753600, validation/loss=0.978746, validation/num_examples=50000
I0206 14:30:30.527104 139946414638848 logging_writer.py:48] [168300] global_step=168300, grad_norm=5.714118480682373, loss=1.4701815843582153
I0206 14:31:14.091222 139946397853440 logging_writer.py:48] [168400] global_step=168400, grad_norm=5.2232232093811035, loss=1.846879243850708
I0206 14:32:00.559474 139946414638848 logging_writer.py:48] [168500] global_step=168500, grad_norm=5.8917236328125, loss=1.3719984292984009
I0206 14:32:47.277638 139946397853440 logging_writer.py:48] [168600] global_step=168600, grad_norm=5.534240245819092, loss=2.457838773727417
I0206 14:33:19.093096 139946414638848 logging_writer.py:48] [168670] global_step=168670, preemption_count=0, score=77520.355408
I0206 14:33:19.731824 140107197974336 checkpoints.py:490] Saving checkpoint at step: 168670
I0206 14:33:21.055508 140107197974336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_3/checkpoint_168670
I0206 14:33:21.076429 140107197974336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_3/checkpoint_168670.
I0206 14:33:22.054160 140107197974336 submission_runner.py:583] Tuning trial 3/5
I0206 14:33:22.054436 140107197974336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0206 14:33:22.064201 140107197974336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011328124674037099, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 34.48550629615784, 'total_duration': 65.40688729286194, 'accumulated_submission_time': 34.48550629615784, 'accumulated_eval_time': 30.9212863445282, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (853, {'train/accuracy': 0.011249999515712261, 'train/loss': 6.477405071258545, 'validation/accuracy': 0.010859999805688858, 'validation/loss': 6.4868550300598145, 'validation/num_examples': 50000, 'test/accuracy': 0.010500000789761543, 'test/loss': 6.523868083953857, 'test/num_examples': 10000, 'score': 454.543984413147, 'total_duration': 531.3210079669952, 'accumulated_submission_time': 454.543984413147, 'accumulated_eval_time': 76.71136093139648, 'accumulated_logging_time': 0.017364501953125, 'global_step': 853, 'preemption_count': 0}), (1759, {'train/accuracy': 0.03460937365889549, 'train/loss': 5.880190372467041, 'validation/accuracy': 0.033479999750852585, 'validation/loss': 5.913444995880127, 'validation/num_examples': 50000, 'test/accuracy': 0.027900001034140587, 'test/loss': 6.0221991539001465, 'test/num_examples': 10000, 'score': 874.770247220993, 'total_duration': 997.8961570262909, 'accumulated_submission_time': 874.770247220993, 'accumulated_eval_time': 122.98103356361389, 'accumulated_logging_time': 0.045339345932006836, 'global_step': 1759, 'preemption_count': 0}), (2673, {'train/accuracy': 0.06058593466877937, 'train/loss': 5.4684343338012695, 'validation/accuracy': 0.05729999765753746, 'validation/loss': 5.506952285766602, 'validation/num_examples': 50000, 'test/accuracy': 0.045900002121925354, 'test/loss': 5.680256366729736, 'test/num_examples': 10000, 'score': 1294.968270778656, 'total_duration': 1464.868360042572, 'accumulated_submission_time': 1294.968270778656, 'accumulated_eval_time': 169.67155838012695, 'accumulated_logging_time': 0.07653570175170898, 'global_step': 2673, 'preemption_count': 0}), (3588, {'train/accuracy': 0.0949414074420929, 'train/loss': 5.101712703704834, 'validation/accuracy': 0.08590000122785568, 'validation/loss': 5.135260105133057, 'validation/num_examples': 50000, 'test/accuracy': 0.06810000538825989, 'test/loss': 5.376662731170654, 'test/num_examples': 10000, 'score': 1715.2244164943695, 'total_duration': 1933.6374650001526, 'accumulated_submission_time': 1715.2244164943695, 'accumulated_eval_time': 218.10238456726074, 'accumulated_logging_time': 0.10631585121154785, 'global_step': 3588, 'preemption_count': 0}), (4504, {'train/accuracy': 0.13169921934604645, 'train/loss': 4.694127559661865, 'validation/accuracy': 0.12099999934434891, 'validation/loss': 4.762543678283691, 'validation/num_examples': 50000, 'test/accuracy': 0.09380000084638596, 'test/loss': 5.061717510223389, 'test/num_examples': 10000, 'score': 2135.2453899383545, 'total_duration': 2400.255875349045, 'accumulated_submission_time': 2135.2453899383545, 'accumulated_eval_time': 264.62031650543213, 'accumulated_logging_time': 0.13405632972717285, 'global_step': 4504, 'preemption_count': 0}), (5416, {'train/accuracy': 0.1699804663658142, 'train/loss': 4.376136302947998, 'validation/accuracy': 0.15546000003814697, 'validation/loss': 4.456610679626465, 'validation/num_examples': 50000, 'test/accuracy': 0.11700000613927841, 'test/loss': 4.778557300567627, 'test/num_examples': 10000, 'score': 2555.4077939987183, 'total_duration': 2867.1481182575226, 'accumulated_submission_time': 2555.4077939987183, 'accumulated_eval_time': 311.2709410190582, 'accumulated_logging_time': 0.16153669357299805, 'global_step': 5416, 'preemption_count': 0}), (6332, {'train/accuracy': 0.21662108600139618, 'train/loss': 4.001551628112793, 'validation/accuracy': 0.2006799876689911, 'validation/loss': 4.088918209075928, 'validation/num_examples': 50000, 'test/accuracy': 0.1517000049352646, 'test/loss': 4.465686321258545, 'test/num_examples': 10000, 'score': 2975.353636741638, 'total_duration': 3335.9222333431244, 'accumulated_submission_time': 2975.353636741638, 'accumulated_eval_time': 360.0176274776459, 'accumulated_logging_time': 0.19207549095153809, 'global_step': 6332, 'preemption_count': 0}), (7248, {'train/accuracy': 0.25996091961860657, 'train/loss': 3.7070982456207275, 'validation/accuracy': 0.23823998868465424, 'validation/loss': 3.8109381198883057, 'validation/num_examples': 50000, 'test/accuracy': 0.1785000115633011, 'test/loss': 4.246798515319824, 'test/num_examples': 10000, 'score': 3395.407825946808, 'total_duration': 3800.8962712287903, 'accumulated_submission_time': 3395.407825946808, 'accumulated_eval_time': 404.85935401916504, 'accumulated_logging_time': 0.21831059455871582, 'global_step': 7248, 'preemption_count': 0}), (8165, {'train/accuracy': 0.29240232706069946, 'train/loss': 3.420924186706543, 'validation/accuracy': 0.2739599943161011, 'validation/loss': 3.5337398052215576, 'validation/num_examples': 50000, 'test/accuracy': 0.20920000970363617, 'test/loss': 4.015152454376221, 'test/num_examples': 10000, 'score': 3815.6993346214294, 'total_duration': 4269.799302577972, 'accumulated_submission_time': 3815.6993346214294, 'accumulated_eval_time': 453.3918721675873, 'accumulated_logging_time': 0.24537086486816406, 'global_step': 8165, 'preemption_count': 0}), (9082, {'train/accuracy': 0.3405078053474426, 'train/loss': 3.166249990463257, 'validation/accuracy': 0.3056800067424774, 'validation/loss': 3.3514530658721924, 'validation/num_examples': 50000, 'test/accuracy': 0.2371000051498413, 'test/loss': 3.8389713764190674, 'test/num_examples': 10000, 'score': 4235.792031049728, 'total_duration': 4736.970528125763, 'accumulated_submission_time': 4235.792031049728, 'accumulated_eval_time': 500.38815212249756, 'accumulated_logging_time': 0.2754819393157959, 'global_step': 9082, 'preemption_count': 0}), (9997, {'train/accuracy': 0.35783201456069946, 'train/loss': 3.022150754928589, 'validation/accuracy': 0.3312000036239624, 'validation/loss': 3.1509346961975098, 'validation/num_examples': 50000, 'test/accuracy': 0.2614000141620636, 'test/loss': 3.6755166053771973, 'test/num_examples': 10000, 'score': 4656.01069688797, 'total_duration': 5206.719424247742, 'accumulated_submission_time': 4656.01069688797, 'accumulated_eval_time': 549.8398551940918, 'accumulated_logging_time': 0.3025200366973877, 'global_step': 9997, 'preemption_count': 0}), (10912, {'train/accuracy': 0.38544920086860657, 'train/loss': 2.840325355529785, 'validation/accuracy': 0.3569599986076355, 'validation/loss': 2.9956555366516113, 'validation/num_examples': 50000, 'test/accuracy': 0.27330002188682556, 'test/loss': 3.5355544090270996, 'test/num_examples': 10000, 'score': 5076.158985376358, 'total_duration': 5677.023521661758, 'accumulated_submission_time': 5076.158985376358, 'accumulated_eval_time': 599.9115297794342, 'accumulated_logging_time': 0.3341560363769531, 'global_step': 10912, 'preemption_count': 0}), (11830, {'train/accuracy': 0.4166601598262787, 'train/loss': 2.7132813930511475, 'validation/accuracy': 0.3746599853038788, 'validation/loss': 2.916720151901245, 'validation/num_examples': 50000, 'test/accuracy': 0.2883000075817108, 'test/loss': 3.481478691101074, 'test/num_examples': 10000, 'score': 5496.442660808563, 'total_duration': 6144.755959510803, 'accumulated_submission_time': 5496.442660808563, 'accumulated_eval_time': 647.2774829864502, 'accumulated_logging_time': 0.3645823001861572, 'global_step': 11830, 'preemption_count': 0}), (12746, {'train/accuracy': 0.4178515672683716, 'train/loss': 2.683227777481079, 'validation/accuracy': 0.3882399797439575, 'validation/loss': 2.8298232555389404, 'validation/num_examples': 50000, 'test/accuracy': 0.30230000615119934, 'test/loss': 3.4036993980407715, 'test/num_examples': 10000, 'score': 5916.627821445465, 'total_duration': 6614.584210395813, 'accumulated_submission_time': 5916.627821445465, 'accumulated_eval_time': 696.8378114700317, 'accumulated_logging_time': 0.39503002166748047, 'global_step': 12746, 'preemption_count': 0}), (13657, {'train/accuracy': 0.4305664002895355, 'train/loss': 2.5879738330841064, 'validation/accuracy': 0.4039999842643738, 'validation/loss': 2.7390167713165283, 'validation/num_examples': 50000, 'test/accuracy': 0.3061000108718872, 'test/loss': 3.3392672538757324, 'test/num_examples': 10000, 'score': 6336.897862434387, 'total_duration': 7078.295861721039, 'accumulated_submission_time': 6336.897862434387, 'accumulated_eval_time': 740.1960797309875, 'accumulated_logging_time': 0.4252433776855469, 'global_step': 13657, 'preemption_count': 0}), (14574, {'train/accuracy': 0.4543749988079071, 'train/loss': 2.460162401199341, 'validation/accuracy': 0.4130399823188782, 'validation/loss': 2.6595137119293213, 'validation/num_examples': 50000, 'test/accuracy': 0.31790000200271606, 'test/loss': 3.2573442459106445, 'test/num_examples': 10000, 'score': 6757.152330160141, 'total_duration': 7549.075749397278, 'accumulated_submission_time': 6757.152330160141, 'accumulated_eval_time': 790.636931180954, 'accumulated_logging_time': 0.457782506942749, 'global_step': 14574, 'preemption_count': 0}), (15489, {'train/accuracy': 0.4491015672683716, 'train/loss': 2.4975130558013916, 'validation/accuracy': 0.4224399924278259, 'validation/loss': 2.632542133331299, 'validation/num_examples': 50000, 'test/accuracy': 0.32850000262260437, 'test/loss': 3.233921527862549, 'test/num_examples': 10000, 'score': 7177.283156871796, 'total_duration': 8017.423315048218, 'accumulated_submission_time': 7177.283156871796, 'accumulated_eval_time': 838.7734625339508, 'accumulated_logging_time': 0.48542118072509766, 'global_step': 15489, 'preemption_count': 0}), (16404, {'train/accuracy': 0.4757421910762787, 'train/loss': 2.3351094722747803, 'validation/accuracy': 0.44132000207901, 'validation/loss': 2.5121448040008545, 'validation/num_examples': 50000, 'test/accuracy': 0.34230002760887146, 'test/loss': 3.1236488819122314, 'test/num_examples': 10000, 'score': 7597.3759133815765, 'total_duration': 8486.046859264374, 'accumulated_submission_time': 7597.3759133815765, 'accumulated_eval_time': 887.2250754833221, 'accumulated_logging_time': 0.5128743648529053, 'global_step': 16404, 'preemption_count': 0}), (17319, {'train/accuracy': 0.4820898473262787, 'train/loss': 2.317319869995117, 'validation/accuracy': 0.44287997484207153, 'validation/loss': 2.516441583633423, 'validation/num_examples': 50000, 'test/accuracy': 0.33890002965927124, 'test/loss': 3.1433703899383545, 'test/num_examples': 10000, 'score': 8017.678372621536, 'total_duration': 8953.327250957489, 'accumulated_submission_time': 8017.678372621536, 'accumulated_eval_time': 934.1244015693665, 'accumulated_logging_time': 0.5392537117004395, 'global_step': 17319, 'preemption_count': 0}), (18234, {'train/accuracy': 0.4768359363079071, 'train/loss': 2.3421082496643066, 'validation/accuracy': 0.4420199990272522, 'validation/loss': 2.51889705657959, 'validation/num_examples': 50000, 'test/accuracy': 0.343500018119812, 'test/loss': 3.1278634071350098, 'test/num_examples': 10000, 'score': 8437.92775630951, 'total_duration': 9417.584113836288, 'accumulated_submission_time': 8437.92775630951, 'accumulated_eval_time': 978.050395488739, 'accumulated_logging_time': 0.5683753490447998, 'global_step': 18234, 'preemption_count': 0}), (19148, {'train/accuracy': 0.49177733063697815, 'train/loss': 2.2599568367004395, 'validation/accuracy': 0.4593999981880188, 'validation/loss': 2.4413387775421143, 'validation/num_examples': 50000, 'test/accuracy': 0.35860002040863037, 'test/loss': 3.0742838382720947, 'test/num_examples': 10000, 'score': 8857.909608125687, 'total_duration': 9887.99471116066, 'accumulated_submission_time': 8857.909608125687, 'accumulated_eval_time': 1028.3964698314667, 'accumulated_logging_time': 0.5990426540374756, 'global_step': 19148, 'preemption_count': 0}), (20064, {'train/accuracy': 0.509082019329071, 'train/loss': 2.1851208209991455, 'validation/accuracy': 0.4652799963951111, 'validation/loss': 2.3844094276428223, 'validation/num_examples': 50000, 'test/accuracy': 0.36160001158714294, 'test/loss': 3.0053672790527344, 'test/num_examples': 10000, 'score': 9277.858618497849, 'total_duration': 10357.822923898697, 'accumulated_submission_time': 9277.858618497849, 'accumulated_eval_time': 1078.1956298351288, 'accumulated_logging_time': 0.6277709007263184, 'global_step': 20064, 'preemption_count': 0}), (20980, {'train/accuracy': 0.53369140625, 'train/loss': 2.057878255844116, 'validation/accuracy': 0.4750799834728241, 'validation/loss': 2.334637403488159, 'validation/num_examples': 50000, 'test/accuracy': 0.36410000920295715, 'test/loss': 2.9753594398498535, 'test/num_examples': 10000, 'score': 9698.17512512207, 'total_duration': 10825.721643447876, 'accumulated_submission_time': 9698.17512512207, 'accumulated_eval_time': 1125.6968231201172, 'accumulated_logging_time': 0.6573843955993652, 'global_step': 20980, 'preemption_count': 0}), (21897, {'train/accuracy': 0.507519543170929, 'train/loss': 2.1788175106048584, 'validation/accuracy': 0.47655999660491943, 'validation/loss': 2.3385961055755615, 'validation/num_examples': 50000, 'test/accuracy': 0.3733000159263611, 'test/loss': 2.9646759033203125, 'test/num_examples': 10000, 'score': 10118.501426696777, 'total_duration': 11293.408746242523, 'accumulated_submission_time': 10118.501426696777, 'accumulated_eval_time': 1172.9720392227173, 'accumulated_logging_time': 0.6915838718414307, 'global_step': 21897, 'preemption_count': 0}), (22814, {'train/accuracy': 0.5238476395606995, 'train/loss': 2.071101427078247, 'validation/accuracy': 0.4860999882221222, 'validation/loss': 2.2656033039093018, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 2.9053289890289307, 'test/num_examples': 10000, 'score': 10538.751983642578, 'total_duration': 11762.698380231857, 'accumulated_submission_time': 10538.751983642578, 'accumulated_eval_time': 1221.914003610611, 'accumulated_logging_time': 0.729525089263916, 'global_step': 22814, 'preemption_count': 0}), (23730, {'train/accuracy': 0.5507421493530273, 'train/loss': 1.9389535188674927, 'validation/accuracy': 0.49563997983932495, 'validation/loss': 2.2019717693328857, 'validation/num_examples': 50000, 'test/accuracy': 0.3871000111103058, 'test/loss': 2.8293137550354004, 'test/num_examples': 10000, 'score': 10958.97521162033, 'total_duration': 12228.869998455048, 'accumulated_submission_time': 10958.97521162033, 'accumulated_eval_time': 1267.7731275558472, 'accumulated_logging_time': 0.766730546951294, 'global_step': 23730, 'preemption_count': 0}), (24647, {'train/accuracy': 0.5287109017372131, 'train/loss': 2.059361219406128, 'validation/accuracy': 0.4931999742984772, 'validation/loss': 2.238321542739868, 'validation/num_examples': 50000, 'test/accuracy': 0.3889000117778778, 'test/loss': 2.873844623565674, 'test/num_examples': 10000, 'score': 11379.114792823792, 'total_duration': 12699.20350074768, 'accumulated_submission_time': 11379.114792823792, 'accumulated_eval_time': 1317.8803231716156, 'accumulated_logging_time': 0.8011837005615234, 'global_step': 24647, 'preemption_count': 0}), (25563, {'train/accuracy': 0.5419335961341858, 'train/loss': 1.9885637760162354, 'validation/accuracy': 0.5034799575805664, 'validation/loss': 2.1718690395355225, 'validation/num_examples': 50000, 'test/accuracy': 0.3960000276565552, 'test/loss': 2.7996408939361572, 'test/num_examples': 10000, 'score': 11799.398998975754, 'total_duration': 13168.366040945053, 'accumulated_submission_time': 11799.398998975754, 'accumulated_eval_time': 1366.6767621040344, 'accumulated_logging_time': 0.8305325508117676, 'global_step': 25563, 'preemption_count': 0}), (26477, {'train/accuracy': 0.5632421970367432, 'train/loss': 1.8941307067871094, 'validation/accuracy': 0.5120199918746948, 'validation/loss': 2.144299268722534, 'validation/num_examples': 50000, 'test/accuracy': 0.39740002155303955, 'test/loss': 2.7761967182159424, 'test/num_examples': 10000, 'score': 12219.34901380539, 'total_duration': 13637.125775814056, 'accumulated_submission_time': 12219.34901380539, 'accumulated_eval_time': 1415.4040472507477, 'accumulated_logging_time': 0.8615057468414307, 'global_step': 26477, 'preemption_count': 0}), (27392, {'train/accuracy': 0.5482421517372131, 'train/loss': 1.9673892259597778, 'validation/accuracy': 0.5123999714851379, 'validation/loss': 2.13213849067688, 'validation/num_examples': 50000, 'test/accuracy': 0.4012000262737274, 'test/loss': 2.763537645339966, 'test/num_examples': 10000, 'score': 12639.654458522797, 'total_duration': 14106.759474277496, 'accumulated_submission_time': 12639.654458522797, 'accumulated_eval_time': 1464.6509912014008, 'accumulated_logging_time': 0.891242265701294, 'global_step': 27392, 'preemption_count': 0}), (28308, {'train/accuracy': 0.5547069907188416, 'train/loss': 1.929384708404541, 'validation/accuracy': 0.5189200043678284, 'validation/loss': 2.1071009635925293, 'validation/num_examples': 50000, 'test/accuracy': 0.40060001611709595, 'test/loss': 2.744941473007202, 'test/num_examples': 10000, 'score': 13059.61026597023, 'total_duration': 14574.590921640396, 'accumulated_submission_time': 13059.61026597023, 'accumulated_eval_time': 1512.441159248352, 'accumulated_logging_time': 0.9247598648071289, 'global_step': 28308, 'preemption_count': 0}), (29224, {'train/accuracy': 0.5635741949081421, 'train/loss': 1.887037754058838, 'validation/accuracy': 0.5194000005722046, 'validation/loss': 2.098005533218384, 'validation/num_examples': 50000, 'test/accuracy': 0.4052000045776367, 'test/loss': 2.7502341270446777, 'test/num_examples': 10000, 'score': 13479.542765378952, 'total_duration': 15045.342432498932, 'accumulated_submission_time': 13479.542765378952, 'accumulated_eval_time': 1563.1740138530731, 'accumulated_logging_time': 0.9577534198760986, 'global_step': 29224, 'preemption_count': 0}), (30141, {'train/accuracy': 0.5602734088897705, 'train/loss': 1.8773443698883057, 'validation/accuracy': 0.5280199646949768, 'validation/loss': 2.0621399879455566, 'validation/num_examples': 50000, 'test/accuracy': 0.4148000180721283, 'test/loss': 2.698044538497925, 'test/num_examples': 10000, 'score': 13899.718740701675, 'total_duration': 15514.25835442543, 'accumulated_submission_time': 13899.718740701675, 'accumulated_eval_time': 1611.8296627998352, 'accumulated_logging_time': 0.9903049468994141, 'global_step': 30141, 'preemption_count': 0}), (31057, {'train/accuracy': 0.5613867044448853, 'train/loss': 1.8950296640396118, 'validation/accuracy': 0.526199996471405, 'validation/loss': 2.0695595741271973, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.691118001937866, 'test/num_examples': 10000, 'score': 14319.735169649124, 'total_duration': 15982.635932683945, 'accumulated_submission_time': 14319.735169649124, 'accumulated_eval_time': 1660.1082208156586, 'accumulated_logging_time': 1.0202922821044922, 'global_step': 31057, 'preemption_count': 0}), (31972, {'train/accuracy': 0.5696093440055847, 'train/loss': 1.8706759214401245, 'validation/accuracy': 0.5287600159645081, 'validation/loss': 2.060586452484131, 'validation/num_examples': 50000, 'test/accuracy': 0.41190001368522644, 'test/loss': 2.7069880962371826, 'test/num_examples': 10000, 'score': 14739.937356710434, 'total_duration': 16451.484882354736, 'accumulated_submission_time': 14739.937356710434, 'accumulated_eval_time': 1708.6653501987457, 'accumulated_logging_time': 1.0565845966339111, 'global_step': 31972, 'preemption_count': 0}), (32890, {'train/accuracy': 0.5985351204872131, 'train/loss': 1.70049250125885, 'validation/accuracy': 0.5352999567985535, 'validation/loss': 2.0219011306762695, 'validation/num_examples': 50000, 'test/accuracy': 0.41600000858306885, 'test/loss': 2.677694797515869, 'test/num_examples': 10000, 'score': 15160.13657617569, 'total_duration': 16920.72052717209, 'accumulated_submission_time': 15160.13657617569, 'accumulated_eval_time': 1757.6149718761444, 'accumulated_logging_time': 1.0907628536224365, 'global_step': 32890, 'preemption_count': 0}), (33807, {'train/accuracy': 0.567578136920929, 'train/loss': 1.879085898399353, 'validation/accuracy': 0.5329399704933167, 'validation/loss': 2.052635431289673, 'validation/num_examples': 50000, 'test/accuracy': 0.4164000153541565, 'test/loss': 2.689453363418579, 'test/num_examples': 10000, 'score': 15580.376125097275, 'total_duration': 17389.854667186737, 'accumulated_submission_time': 15580.376125097275, 'accumulated_eval_time': 1806.4245445728302, 'accumulated_logging_time': 1.1232614517211914, 'global_step': 33807, 'preemption_count': 0}), (34724, {'train/accuracy': 0.5770702958106995, 'train/loss': 1.8023401498794556, 'validation/accuracy': 0.5371999740600586, 'validation/loss': 1.9966063499450684, 'validation/num_examples': 50000, 'test/accuracy': 0.41260001063346863, 'test/loss': 2.6572988033294678, 'test/num_examples': 10000, 'score': 16000.444960832596, 'total_duration': 17856.395292282104, 'accumulated_submission_time': 16000.444960832596, 'accumulated_eval_time': 1852.8152103424072, 'accumulated_logging_time': 1.1534223556518555, 'global_step': 34724, 'preemption_count': 0}), (35638, {'train/accuracy': 0.5990234017372131, 'train/loss': 1.7254287004470825, 'validation/accuracy': 0.5380600094795227, 'validation/loss': 2.003264904022217, 'validation/num_examples': 50000, 'test/accuracy': 0.4256000220775604, 'test/loss': 2.619293212890625, 'test/num_examples': 10000, 'score': 16420.363413095474, 'total_duration': 18325.28321290016, 'accumulated_submission_time': 16420.363413095474, 'accumulated_eval_time': 1901.7003903388977, 'accumulated_logging_time': 1.184575080871582, 'global_step': 35638, 'preemption_count': 0}), (36554, {'train/accuracy': 0.575488269329071, 'train/loss': 1.8254637718200684, 'validation/accuracy': 0.5359799861907959, 'validation/loss': 2.010288715362549, 'validation/num_examples': 50000, 'test/accuracy': 0.4204000234603882, 'test/loss': 2.6431541442871094, 'test/num_examples': 10000, 'score': 16840.63138628006, 'total_duration': 18794.849648475647, 'accumulated_submission_time': 16840.63138628006, 'accumulated_eval_time': 1950.9113097190857, 'accumulated_logging_time': 1.2209351062774658, 'global_step': 36554, 'preemption_count': 0}), (37469, {'train/accuracy': 0.5883203148841858, 'train/loss': 1.7518789768218994, 'validation/accuracy': 0.5458799600601196, 'validation/loss': 1.9552130699157715, 'validation/num_examples': 50000, 'test/accuracy': 0.43230003118515015, 'test/loss': 2.6027326583862305, 'test/num_examples': 10000, 'score': 17260.585029363632, 'total_duration': 19257.405125379562, 'accumulated_submission_time': 17260.585029363632, 'accumulated_eval_time': 1993.4251432418823, 'accumulated_logging_time': 1.2563939094543457, 'global_step': 37469, 'preemption_count': 0}), (38384, {'train/accuracy': 0.6019335985183716, 'train/loss': 1.7069507837295532, 'validation/accuracy': 0.5504199862480164, 'validation/loss': 1.9425902366638184, 'validation/num_examples': 50000, 'test/accuracy': 0.43460002541542053, 'test/loss': 2.583150625228882, 'test/num_examples': 10000, 'score': 17680.8397500515, 'total_duration': 19725.885306596756, 'accumulated_submission_time': 17680.8397500515, 'accumulated_eval_time': 2041.5583720207214, 'accumulated_logging_time': 1.2973315715789795, 'global_step': 38384, 'preemption_count': 0}), (39301, {'train/accuracy': 0.5848632454872131, 'train/loss': 1.7843482494354248, 'validation/accuracy': 0.545199990272522, 'validation/loss': 1.9751474857330322, 'validation/num_examples': 50000, 'test/accuracy': 0.4333000183105469, 'test/loss': 2.616333484649658, 'test/num_examples': 10000, 'score': 18101.136819839478, 'total_duration': 20192.52774953842, 'accumulated_submission_time': 18101.136819839478, 'accumulated_eval_time': 2087.8153867721558, 'accumulated_logging_time': 1.333068609237671, 'global_step': 39301, 'preemption_count': 0}), (40218, {'train/accuracy': 0.5871874690055847, 'train/loss': 1.7634190320968628, 'validation/accuracy': 0.5512199997901917, 'validation/loss': 1.9427509307861328, 'validation/num_examples': 50000, 'test/accuracy': 0.4305000305175781, 'test/loss': 2.6063010692596436, 'test/num_examples': 10000, 'score': 18521.130932092667, 'total_duration': 20661.063827991486, 'accumulated_submission_time': 18521.130932092667, 'accumulated_eval_time': 2136.2723891735077, 'accumulated_logging_time': 1.364790678024292, 'global_step': 40218, 'preemption_count': 0}), (41135, {'train/accuracy': 0.597851574420929, 'train/loss': 1.7207162380218506, 'validation/accuracy': 0.5503999590873718, 'validation/loss': 1.93899405002594, 'validation/num_examples': 50000, 'test/accuracy': 0.4401000142097473, 'test/loss': 2.5727827548980713, 'test/num_examples': 10000, 'score': 18941.098199129105, 'total_duration': 21129.979234695435, 'accumulated_submission_time': 18941.098199129105, 'accumulated_eval_time': 2185.132310628891, 'accumulated_logging_time': 1.3997371196746826, 'global_step': 41135, 'preemption_count': 0}), (42051, {'train/accuracy': 0.5936523079872131, 'train/loss': 1.7368556261062622, 'validation/accuracy': 0.5519399642944336, 'validation/loss': 1.9425498247146606, 'validation/num_examples': 50000, 'test/accuracy': 0.43540000915527344, 'test/loss': 2.5808470249176025, 'test/num_examples': 10000, 'score': 19361.50342464447, 'total_duration': 21595.81792283058, 'accumulated_submission_time': 19361.50342464447, 'accumulated_eval_time': 2230.4811882972717, 'accumulated_logging_time': 1.4327614307403564, 'global_step': 42051, 'preemption_count': 0}), (42966, {'train/accuracy': 0.5927538871765137, 'train/loss': 1.7270139455795288, 'validation/accuracy': 0.5557399988174438, 'validation/loss': 1.9057121276855469, 'validation/num_examples': 50000, 'test/accuracy': 0.4368000328540802, 'test/loss': 2.5653188228607178, 'test/num_examples': 10000, 'score': 19781.703052520752, 'total_duration': 22064.275451898575, 'accumulated_submission_time': 19781.703052520752, 'accumulated_eval_time': 2278.644249200821, 'accumulated_logging_time': 1.4757418632507324, 'global_step': 42966, 'preemption_count': 0}), (43882, {'train/accuracy': 0.6057812571525574, 'train/loss': 1.6457350254058838, 'validation/accuracy': 0.5611599683761597, 'validation/loss': 1.8691457509994507, 'validation/num_examples': 50000, 'test/accuracy': 0.4455000162124634, 'test/loss': 2.518744707107544, 'test/num_examples': 10000, 'score': 20201.69106078148, 'total_duration': 22532.53761100769, 'accumulated_submission_time': 20201.69106078148, 'accumulated_eval_time': 2326.826430082321, 'accumulated_logging_time': 1.5156099796295166, 'global_step': 43882, 'preemption_count': 0}), (44796, {'train/accuracy': 0.6296679377555847, 'train/loss': 1.5705506801605225, 'validation/accuracy': 0.5589399933815002, 'validation/loss': 1.8887207508087158, 'validation/num_examples': 50000, 'test/accuracy': 0.44510000944137573, 'test/loss': 2.5309979915618896, 'test/num_examples': 10000, 'score': 20622.27797460556, 'total_duration': 22999.989002227783, 'accumulated_submission_time': 20622.27797460556, 'accumulated_eval_time': 2373.602608203888, 'accumulated_logging_time': 1.5518851280212402, 'global_step': 44796, 'preemption_count': 0}), (45710, {'train/accuracy': 0.6051172018051147, 'train/loss': 1.6690466403961182, 'validation/accuracy': 0.5628399848937988, 'validation/loss': 1.8617463111877441, 'validation/num_examples': 50000, 'test/accuracy': 0.4449000358581543, 'test/loss': 2.515469789505005, 'test/num_examples': 10000, 'score': 21042.347343206406, 'total_duration': 23466.94051671028, 'accumulated_submission_time': 21042.347343206406, 'accumulated_eval_time': 2420.3978378772736, 'accumulated_logging_time': 1.5879981517791748, 'global_step': 45710, 'preemption_count': 0}), (46623, {'train/accuracy': 0.6066796779632568, 'train/loss': 1.6673763990402222, 'validation/accuracy': 0.5652799606323242, 'validation/loss': 1.8668668270111084, 'validation/num_examples': 50000, 'test/accuracy': 0.44530001282691956, 'test/loss': 2.5359339714050293, 'test/num_examples': 10000, 'score': 21462.64211010933, 'total_duration': 23935.413821458817, 'accumulated_submission_time': 21462.64211010933, 'accumulated_eval_time': 2468.4884293079376, 'accumulated_logging_time': 1.6240994930267334, 'global_step': 46623, 'preemption_count': 0}), (47539, {'train/accuracy': 0.626269519329071, 'train/loss': 1.5778939723968506, 'validation/accuracy': 0.5616999864578247, 'validation/loss': 1.8711748123168945, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.509688377380371, 'test/num_examples': 10000, 'score': 21882.842244386673, 'total_duration': 24402.25755739212, 'accumulated_submission_time': 21882.842244386673, 'accumulated_eval_time': 2515.0421693325043, 'accumulated_logging_time': 1.66135573387146, 'global_step': 47539, 'preemption_count': 0}), (48455, {'train/accuracy': 0.6093554496765137, 'train/loss': 1.643090009689331, 'validation/accuracy': 0.5676599740982056, 'validation/loss': 1.8277660608291626, 'validation/num_examples': 50000, 'test/accuracy': 0.45430001616477966, 'test/loss': 2.494823455810547, 'test/num_examples': 10000, 'score': 22303.119409799576, 'total_duration': 24869.121667146683, 'accumulated_submission_time': 22303.119409799576, 'accumulated_eval_time': 2561.5461843013763, 'accumulated_logging_time': 1.692392110824585, 'global_step': 48455, 'preemption_count': 0}), (49371, {'train/accuracy': 0.611132800579071, 'train/loss': 1.6391677856445312, 'validation/accuracy': 0.5672399997711182, 'validation/loss': 1.8397936820983887, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.4801652431488037, 'test/num_examples': 10000, 'score': 22723.300478219986, 'total_duration': 25337.39897251129, 'accumulated_submission_time': 22723.300478219986, 'accumulated_eval_time': 2609.5553126335144, 'accumulated_logging_time': 1.7268388271331787, 'global_step': 49371, 'preemption_count': 0}), (50287, {'train/accuracy': 0.6187109351158142, 'train/loss': 1.5916036367416382, 'validation/accuracy': 0.5705599784851074, 'validation/loss': 1.8403029441833496, 'validation/num_examples': 50000, 'test/accuracy': 0.4564000070095062, 'test/loss': 2.4771194458007812, 'test/num_examples': 10000, 'score': 23143.497784137726, 'total_duration': 25804.48171377182, 'accumulated_submission_time': 23143.497784137726, 'accumulated_eval_time': 2656.3447353839874, 'accumulated_logging_time': 1.7708237171173096, 'global_step': 50287, 'preemption_count': 0}), (51201, {'train/accuracy': 0.6166015267372131, 'train/loss': 1.6008018255233765, 'validation/accuracy': 0.5757399797439575, 'validation/loss': 1.7892749309539795, 'validation/num_examples': 50000, 'test/accuracy': 0.457800030708313, 'test/loss': 2.448241949081421, 'test/num_examples': 10000, 'score': 23563.565361738205, 'total_duration': 26271.550952911377, 'accumulated_submission_time': 23563.565361738205, 'accumulated_eval_time': 2703.2574343681335, 'accumulated_logging_time': 1.8078598976135254, 'global_step': 51201, 'preemption_count': 0}), (52116, {'train/accuracy': 0.6089453101158142, 'train/loss': 1.667687177658081, 'validation/accuracy': 0.5686599612236023, 'validation/loss': 1.8564475774765015, 'validation/num_examples': 50000, 'test/accuracy': 0.4490000307559967, 'test/loss': 2.522512912750244, 'test/num_examples': 10000, 'score': 23983.790986299515, 'total_duration': 26741.611914396286, 'accumulated_submission_time': 23983.790986299515, 'accumulated_eval_time': 2752.9996156692505, 'accumulated_logging_time': 1.848759651184082, 'global_step': 52116, 'preemption_count': 0}), (53032, {'train/accuracy': 0.6204296946525574, 'train/loss': 1.5840575695037842, 'validation/accuracy': 0.5726199746131897, 'validation/loss': 1.8054448366165161, 'validation/num_examples': 50000, 'test/accuracy': 0.45650002360343933, 'test/loss': 2.4617958068847656, 'test/num_examples': 10000, 'score': 24403.7320561409, 'total_duration': 27206.607456207275, 'accumulated_submission_time': 24403.7320561409, 'accumulated_eval_time': 2797.968763113022, 'accumulated_logging_time': 1.8818917274475098, 'global_step': 53032, 'preemption_count': 0}), (53949, {'train/accuracy': 0.6197265386581421, 'train/loss': 1.6068668365478516, 'validation/accuracy': 0.5780799984931946, 'validation/loss': 1.7958285808563232, 'validation/num_examples': 50000, 'test/accuracy': 0.45580002665519714, 'test/loss': 2.460911512374878, 'test/num_examples': 10000, 'score': 24823.819769382477, 'total_duration': 27676.279418468475, 'accumulated_submission_time': 24823.819769382477, 'accumulated_eval_time': 2847.4651761054993, 'accumulated_logging_time': 1.9169471263885498, 'global_step': 53949, 'preemption_count': 0}), (54863, {'train/accuracy': 0.6193749904632568, 'train/loss': 1.6178147792816162, 'validation/accuracy': 0.5789799690246582, 'validation/loss': 1.8013064861297607, 'validation/num_examples': 50000, 'test/accuracy': 0.4589000344276428, 'test/loss': 2.458005428314209, 'test/num_examples': 10000, 'score': 25243.794989824295, 'total_duration': 28139.657998085022, 'accumulated_submission_time': 25243.794989824295, 'accumulated_eval_time': 2890.782431602478, 'accumulated_logging_time': 1.9509387016296387, 'global_step': 54863, 'preemption_count': 0}), (55774, {'train/accuracy': 0.6284765601158142, 'train/loss': 1.5806477069854736, 'validation/accuracy': 0.5812999606132507, 'validation/loss': 1.804103970527649, 'validation/num_examples': 50000, 'test/accuracy': 0.46310001611709595, 'test/loss': 2.472367525100708, 'test/num_examples': 10000, 'score': 25663.818971157074, 'total_duration': 28606.72021007538, 'accumulated_submission_time': 25663.818971157074, 'accumulated_eval_time': 2937.7285718917847, 'accumulated_logging_time': 1.9914829730987549, 'global_step': 55774, 'preemption_count': 0}), (56688, {'train/accuracy': 0.6417773365974426, 'train/loss': 1.5043836832046509, 'validation/accuracy': 0.578719973564148, 'validation/loss': 1.7915959358215332, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.447606086730957, 'test/num_examples': 10000, 'score': 26084.16915845871, 'total_duration': 29075.186302661896, 'accumulated_submission_time': 26084.16915845871, 'accumulated_eval_time': 2985.750026702881, 'accumulated_logging_time': 2.033883810043335, 'global_step': 56688, 'preemption_count': 0}), (57601, {'train/accuracy': 0.6244726181030273, 'train/loss': 1.5794726610183716, 'validation/accuracy': 0.5821599960327148, 'validation/loss': 1.773970603942871, 'validation/num_examples': 50000, 'test/accuracy': 0.4669000208377838, 'test/loss': 2.421680450439453, 'test/num_examples': 10000, 'score': 26504.193346261978, 'total_duration': 29542.947952270508, 'accumulated_submission_time': 26504.193346261978, 'accumulated_eval_time': 3033.001730442047, 'accumulated_logging_time': 2.4678242206573486, 'global_step': 57601, 'preemption_count': 0}), (58519, {'train/accuracy': 0.6330859065055847, 'train/loss': 1.5414679050445557, 'validation/accuracy': 0.5845400094985962, 'validation/loss': 1.7692793607711792, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.407954692840576, 'test/num_examples': 10000, 'score': 26924.44057393074, 'total_duration': 30011.185983896255, 'accumulated_submission_time': 26924.44057393074, 'accumulated_eval_time': 3080.904748916626, 'accumulated_logging_time': 2.5036418437957764, 'global_step': 58519, 'preemption_count': 0}), (59431, {'train/accuracy': 0.6523241996765137, 'train/loss': 1.4520971775054932, 'validation/accuracy': 0.5870400071144104, 'validation/loss': 1.7456282377243042, 'validation/num_examples': 50000, 'test/accuracy': 0.46730002760887146, 'test/loss': 2.3970510959625244, 'test/num_examples': 10000, 'score': 27344.42973256111, 'total_duration': 30479.253449440002, 'accumulated_submission_time': 27344.42973256111, 'accumulated_eval_time': 3128.893353700638, 'accumulated_logging_time': 2.5405941009521484, 'global_step': 59431, 'preemption_count': 0}), (60346, {'train/accuracy': 0.6209570169448853, 'train/loss': 1.5848476886749268, 'validation/accuracy': 0.58051997423172, 'validation/loss': 1.7770845890045166, 'validation/num_examples': 50000, 'test/accuracy': 0.4643000364303589, 'test/loss': 2.428161144256592, 'test/num_examples': 10000, 'score': 27764.40898680687, 'total_duration': 30947.579052448273, 'accumulated_submission_time': 27764.40898680687, 'accumulated_eval_time': 3177.135461807251, 'accumulated_logging_time': 2.5926928520202637, 'global_step': 60346, 'preemption_count': 0}), (61260, {'train/accuracy': 0.6322460770606995, 'train/loss': 1.5737278461456299, 'validation/accuracy': 0.5851399898529053, 'validation/loss': 1.7762945890426636, 'validation/num_examples': 50000, 'test/accuracy': 0.4652000367641449, 'test/loss': 2.4296584129333496, 'test/num_examples': 10000, 'score': 28184.326326608658, 'total_duration': 31415.275759458542, 'accumulated_submission_time': 28184.326326608658, 'accumulated_eval_time': 3224.825823068619, 'accumulated_logging_time': 2.629995107650757, 'global_step': 61260, 'preemption_count': 0}), (62175, {'train/accuracy': 0.6433984041213989, 'train/loss': 1.473775863647461, 'validation/accuracy': 0.5825200080871582, 'validation/loss': 1.7541874647140503, 'validation/num_examples': 50000, 'test/accuracy': 0.4636000096797943, 'test/loss': 2.4132707118988037, 'test/num_examples': 10000, 'score': 28604.319380521774, 'total_duration': 31884.798108816147, 'accumulated_submission_time': 28604.319380521774, 'accumulated_eval_time': 3274.260570049286, 'accumulated_logging_time': 2.6725854873657227, 'global_step': 62175, 'preemption_count': 0}), (63090, {'train/accuracy': 0.6314452886581421, 'train/loss': 1.5430465936660767, 'validation/accuracy': 0.5873000025749207, 'validation/loss': 1.7412338256835938, 'validation/num_examples': 50000, 'test/accuracy': 0.4662000238895416, 'test/loss': 2.3992021083831787, 'test/num_examples': 10000, 'score': 29024.578023433685, 'total_duration': 32354.39852118492, 'accumulated_submission_time': 29024.578023433685, 'accumulated_eval_time': 3323.5141406059265, 'accumulated_logging_time': 2.7087838649749756, 'global_step': 63090, 'preemption_count': 0}), (64006, {'train/accuracy': 0.6298632621765137, 'train/loss': 1.5495365858078003, 'validation/accuracy': 0.5903800129890442, 'validation/loss': 1.7368825674057007, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.39388108253479, 'test/num_examples': 10000, 'score': 29444.48857831955, 'total_duration': 32817.74343061447, 'accumulated_submission_time': 29444.48857831955, 'accumulated_eval_time': 3366.8562116622925, 'accumulated_logging_time': 2.7485859394073486, 'global_step': 64006, 'preemption_count': 0}), (64921, {'train/accuracy': 0.6470116972923279, 'train/loss': 1.4817765951156616, 'validation/accuracy': 0.5940399765968323, 'validation/loss': 1.731095790863037, 'validation/num_examples': 50000, 'test/accuracy': 0.4788000285625458, 'test/loss': 2.3833441734313965, 'test/num_examples': 10000, 'score': 29864.842218399048, 'total_duration': 33285.5135974884, 'accumulated_submission_time': 29864.842218399048, 'accumulated_eval_time': 3414.1854150295258, 'accumulated_logging_time': 2.78342866897583, 'global_step': 64921, 'preemption_count': 0}), (65839, {'train/accuracy': 0.64111328125, 'train/loss': 1.4903844594955444, 'validation/accuracy': 0.5980600118637085, 'validation/loss': 1.6916265487670898, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.3636457920074463, 'test/num_examples': 10000, 'score': 30285.16464781761, 'total_duration': 33751.76726102829, 'accumulated_submission_time': 30285.16464781761, 'accumulated_eval_time': 3460.025162935257, 'accumulated_logging_time': 2.822338342666626, 'global_step': 65839, 'preemption_count': 0}), (66752, {'train/accuracy': 0.6405078172683716, 'train/loss': 1.486151099205017, 'validation/accuracy': 0.6005600094795227, 'validation/loss': 1.6860800981521606, 'validation/num_examples': 50000, 'test/accuracy': 0.4765000343322754, 'test/loss': 2.350003242492676, 'test/num_examples': 10000, 'score': 30705.16337299347, 'total_duration': 34221.96933102608, 'accumulated_submission_time': 30705.16337299347, 'accumulated_eval_time': 3510.1349868774414, 'accumulated_logging_time': 2.8638916015625, 'global_step': 66752, 'preemption_count': 0}), (67668, {'train/accuracy': 0.6430078148841858, 'train/loss': 1.4980931282043457, 'validation/accuracy': 0.5955199599266052, 'validation/loss': 1.7154990434646606, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.372490167617798, 'test/num_examples': 10000, 'score': 31125.316086292267, 'total_duration': 34690.3684053421, 'accumulated_submission_time': 31125.316086292267, 'accumulated_eval_time': 3558.2829871177673, 'accumulated_logging_time': 2.9099161624908447, 'global_step': 67668, 'preemption_count': 0}), (68581, {'train/accuracy': 0.662304699420929, 'train/loss': 1.3984365463256836, 'validation/accuracy': 0.599399983882904, 'validation/loss': 1.6794729232788086, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.3231451511383057, 'test/num_examples': 10000, 'score': 31545.59892988205, 'total_duration': 35158.15919518471, 'accumulated_submission_time': 31545.59892988205, 'accumulated_eval_time': 3605.6924924850464, 'accumulated_logging_time': 2.954616069793701, 'global_step': 68581, 'preemption_count': 0}), (69499, {'train/accuracy': 0.6392773389816284, 'train/loss': 1.4914463758468628, 'validation/accuracy': 0.6018999814987183, 'validation/loss': 1.6752866506576538, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.3373358249664307, 'test/num_examples': 10000, 'score': 31965.907709360123, 'total_duration': 35627.05697226524, 'accumulated_submission_time': 31965.907709360123, 'accumulated_eval_time': 3654.193194627762, 'accumulated_logging_time': 2.99135160446167, 'global_step': 69499, 'preemption_count': 0}), (70415, {'train/accuracy': 0.647753894329071, 'train/loss': 1.4472213983535767, 'validation/accuracy': 0.5977199673652649, 'validation/loss': 1.679577112197876, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.3356761932373047, 'test/num_examples': 10000, 'score': 32385.870503902435, 'total_duration': 36096.61861395836, 'accumulated_submission_time': 32385.870503902435, 'accumulated_eval_time': 3703.6968002319336, 'accumulated_logging_time': 3.0350594520568848, 'global_step': 70415, 'preemption_count': 0}), (71331, {'train/accuracy': 0.6681054830551147, 'train/loss': 1.376840353012085, 'validation/accuracy': 0.5995399951934814, 'validation/loss': 1.685808539390564, 'validation/num_examples': 50000, 'test/accuracy': 0.47360002994537354, 'test/loss': 2.3584470748901367, 'test/num_examples': 10000, 'score': 32805.93040370941, 'total_duration': 36565.83061218262, 'accumulated_submission_time': 32805.93040370941, 'accumulated_eval_time': 3752.7585911750793, 'accumulated_logging_time': 3.072389841079712, 'global_step': 71331, 'preemption_count': 0}), (72246, {'train/accuracy': 0.6423632502555847, 'train/loss': 1.4910801649093628, 'validation/accuracy': 0.5996599793434143, 'validation/loss': 1.6887861490249634, 'validation/num_examples': 50000, 'test/accuracy': 0.47780001163482666, 'test/loss': 2.3342955112457275, 'test/num_examples': 10000, 'score': 33226.05816960335, 'total_duration': 37033.87846469879, 'accumulated_submission_time': 33226.05816960335, 'accumulated_eval_time': 3800.5814397335052, 'accumulated_logging_time': 3.1176953315734863, 'global_step': 72246, 'preemption_count': 0}), (73161, {'train/accuracy': 0.6495702862739563, 'train/loss': 1.450307846069336, 'validation/accuracy': 0.604200005531311, 'validation/loss': 1.6661208868026733, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.3093228340148926, 'test/num_examples': 10000, 'score': 33646.26445245743, 'total_duration': 37499.833355903625, 'accumulated_submission_time': 33646.26445245743, 'accumulated_eval_time': 3846.234811067581, 'accumulated_logging_time': 3.1611366271972656, 'global_step': 73161, 'preemption_count': 0}), (74075, {'train/accuracy': 0.662890613079071, 'train/loss': 1.4327744245529175, 'validation/accuracy': 0.6055799722671509, 'validation/loss': 1.702540636062622, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.3480513095855713, 'test/num_examples': 10000, 'score': 34066.458422899246, 'total_duration': 37971.66522574425, 'accumulated_submission_time': 34066.458422899246, 'accumulated_eval_time': 3897.773527622223, 'accumulated_logging_time': 3.2078804969787598, 'global_step': 74075, 'preemption_count': 0}), (74988, {'train/accuracy': 0.6476953029632568, 'train/loss': 1.45093834400177, 'validation/accuracy': 0.6082199811935425, 'validation/loss': 1.6413438320159912, 'validation/num_examples': 50000, 'test/accuracy': 0.48590001463890076, 'test/loss': 2.3130125999450684, 'test/num_examples': 10000, 'score': 34486.554055690765, 'total_duration': 38436.468970775604, 'accumulated_submission_time': 34486.554055690765, 'accumulated_eval_time': 3942.385674238205, 'accumulated_logging_time': 3.2509477138519287, 'global_step': 74988, 'preemption_count': 0}), (75899, {'train/accuracy': 0.6452929377555847, 'train/loss': 1.4677882194519043, 'validation/accuracy': 0.6007599830627441, 'validation/loss': 1.683293342590332, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.326888084411621, 'test/num_examples': 10000, 'score': 34906.559433460236, 'total_duration': 38905.00736904144, 'accumulated_submission_time': 34906.559433460236, 'accumulated_eval_time': 3990.819508075714, 'accumulated_logging_time': 3.2981767654418945, 'global_step': 75899, 'preemption_count': 0}), (76813, {'train/accuracy': 0.6568945050239563, 'train/loss': 1.4267654418945312, 'validation/accuracy': 0.6042400002479553, 'validation/loss': 1.6688212156295776, 'validation/num_examples': 50000, 'test/accuracy': 0.48580002784729004, 'test/loss': 2.3179571628570557, 'test/num_examples': 10000, 'score': 35326.47755908966, 'total_duration': 39372.69943475723, 'accumulated_submission_time': 35326.47755908966, 'accumulated_eval_time': 4038.497713804245, 'accumulated_logging_time': 3.342529058456421, 'global_step': 76813, 'preemption_count': 0}), (77728, {'train/accuracy': 0.6592968702316284, 'train/loss': 1.4043962955474854, 'validation/accuracy': 0.6110000014305115, 'validation/loss': 1.6132807731628418, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.274594783782959, 'test/num_examples': 10000, 'score': 35746.67924427986, 'total_duration': 39838.25917339325, 'accumulated_submission_time': 35746.67924427986, 'accumulated_eval_time': 4083.7658500671387, 'accumulated_logging_time': 3.3802947998046875, 'global_step': 77728, 'preemption_count': 0}), (78644, {'train/accuracy': 0.6494140625, 'train/loss': 1.4841097593307495, 'validation/accuracy': 0.6038999557495117, 'validation/loss': 1.6853262186050415, 'validation/num_examples': 50000, 'test/accuracy': 0.4855000376701355, 'test/loss': 2.3392891883850098, 'test/num_examples': 10000, 'score': 36166.756766080856, 'total_duration': 40307.37702679634, 'accumulated_submission_time': 36166.756766080856, 'accumulated_eval_time': 4132.717834472656, 'accumulated_logging_time': 3.4168825149536133, 'global_step': 78644, 'preemption_count': 0}), (79559, {'train/accuracy': 0.6655663847923279, 'train/loss': 1.3757052421569824, 'validation/accuracy': 0.6121199727058411, 'validation/loss': 1.6102243661880493, 'validation/num_examples': 50000, 'test/accuracy': 0.4878000319004059, 'test/loss': 2.2785677909851074, 'test/num_examples': 10000, 'score': 36586.85572004318, 'total_duration': 40774.5886554718, 'accumulated_submission_time': 36586.85572004318, 'accumulated_eval_time': 4179.737259864807, 'accumulated_logging_time': 3.457533836364746, 'global_step': 79559, 'preemption_count': 0}), (80474, {'train/accuracy': 0.6681054830551147, 'train/loss': 1.3617504835128784, 'validation/accuracy': 0.6156399846076965, 'validation/loss': 1.5994772911071777, 'validation/num_examples': 50000, 'test/accuracy': 0.4944000244140625, 'test/loss': 2.2579493522644043, 'test/num_examples': 10000, 'score': 37007.06138277054, 'total_duration': 41240.94586133957, 'accumulated_submission_time': 37007.06138277054, 'accumulated_eval_time': 4225.793916940689, 'accumulated_logging_time': 3.5003437995910645, 'global_step': 80474, 'preemption_count': 0}), (81388, {'train/accuracy': 0.6598241925239563, 'train/loss': 1.4070793390274048, 'validation/accuracy': 0.6183599829673767, 'validation/loss': 1.610404133796692, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.2725863456726074, 'test/num_examples': 10000, 'score': 37427.10963559151, 'total_duration': 41709.938883304596, 'accumulated_submission_time': 37427.10963559151, 'accumulated_eval_time': 4274.636933803558, 'accumulated_logging_time': 3.5497610569000244, 'global_step': 81388, 'preemption_count': 0}), (82301, {'train/accuracy': 0.6676562428474426, 'train/loss': 1.3960615396499634, 'validation/accuracy': 0.6139400005340576, 'validation/loss': 1.63629150390625, 'validation/num_examples': 50000, 'test/accuracy': 0.4942000210285187, 'test/loss': 2.2834160327911377, 'test/num_examples': 10000, 'score': 37847.287464141846, 'total_duration': 42176.4779920578, 'accumulated_submission_time': 37847.287464141846, 'accumulated_eval_time': 4320.90106010437, 'accumulated_logging_time': 3.595423936843872, 'global_step': 82301, 'preemption_count': 0}), (83217, {'train/accuracy': 0.685742199420929, 'train/loss': 1.2981655597686768, 'validation/accuracy': 0.6175999641418457, 'validation/loss': 1.6042635440826416, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.265681266784668, 'test/num_examples': 10000, 'score': 38267.64587044716, 'total_duration': 42644.01581954956, 'accumulated_submission_time': 38267.64587044716, 'accumulated_eval_time': 4367.9880521297455, 'accumulated_logging_time': 3.6356606483459473, 'global_step': 83217, 'preemption_count': 0}), (84133, {'train/accuracy': 0.6615625023841858, 'train/loss': 1.408983588218689, 'validation/accuracy': 0.617680013179779, 'validation/loss': 1.6139475107192993, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.265106439590454, 'test/num_examples': 10000, 'score': 38688.0792453289, 'total_duration': 43113.82888507843, 'accumulated_submission_time': 38688.0792453289, 'accumulated_eval_time': 4417.275773525238, 'accumulated_logging_time': 3.674084186553955, 'global_step': 84133, 'preemption_count': 0}), (85049, {'train/accuracy': 0.6717578172683716, 'train/loss': 1.3764458894729614, 'validation/accuracy': 0.6211400032043457, 'validation/loss': 1.6145461797714233, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.244614601135254, 'test/num_examples': 10000, 'score': 39108.140686035156, 'total_duration': 43583.242089509964, 'accumulated_submission_time': 39108.140686035156, 'accumulated_eval_time': 4466.535435676575, 'accumulated_logging_time': 3.7140793800354004, 'global_step': 85049, 'preemption_count': 0}), (85964, {'train/accuracy': 0.6827148199081421, 'train/loss': 1.2997052669525146, 'validation/accuracy': 0.6204400062561035, 'validation/loss': 1.5726711750030518, 'validation/num_examples': 50000, 'test/accuracy': 0.5038000345230103, 'test/loss': 2.207498550415039, 'test/num_examples': 10000, 'score': 39528.49926805496, 'total_duration': 44049.326600790024, 'accumulated_submission_time': 39528.49926805496, 'accumulated_eval_time': 4512.167064666748, 'accumulated_logging_time': 3.7566189765930176, 'global_step': 85964, 'preemption_count': 0}), (86878, {'train/accuracy': 0.6692578196525574, 'train/loss': 1.3671681880950928, 'validation/accuracy': 0.6223799586296082, 'validation/loss': 1.5760130882263184, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.202033042907715, 'test/num_examples': 10000, 'score': 39948.76182985306, 'total_duration': 44516.90362381935, 'accumulated_submission_time': 39948.76182985306, 'accumulated_eval_time': 4559.381934642792, 'accumulated_logging_time': 3.8039591312408447, 'global_step': 86878, 'preemption_count': 0}), (87793, {'train/accuracy': 0.6750195026397705, 'train/loss': 1.3554376363754272, 'validation/accuracy': 0.6269999742507935, 'validation/loss': 1.583235740661621, 'validation/num_examples': 50000, 'test/accuracy': 0.5076000094413757, 'test/loss': 2.2284297943115234, 'test/num_examples': 10000, 'score': 40369.03018307686, 'total_duration': 44984.48181915283, 'accumulated_submission_time': 40369.03018307686, 'accumulated_eval_time': 4606.599714756012, 'accumulated_logging_time': 3.843261957168579, 'global_step': 87793, 'preemption_count': 0}), (88707, {'train/accuracy': 0.6813281178474426, 'train/loss': 1.3070732355117798, 'validation/accuracy': 0.6232199668884277, 'validation/loss': 1.5689715147018433, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.2088680267333984, 'test/num_examples': 10000, 'score': 40789.127017498016, 'total_duration': 45450.74204015732, 'accumulated_submission_time': 40789.127017498016, 'accumulated_eval_time': 4652.663172245026, 'accumulated_logging_time': 3.8918819427490234, 'global_step': 88707, 'preemption_count': 0}), (89620, {'train/accuracy': 0.6719921827316284, 'train/loss': 1.3421682119369507, 'validation/accuracy': 0.6292600035667419, 'validation/loss': 1.5393462181091309, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.181424617767334, 'test/num_examples': 10000, 'score': 41209.09097337723, 'total_duration': 45920.40969824791, 'accumulated_submission_time': 41209.09097337723, 'accumulated_eval_time': 4702.275573730469, 'accumulated_logging_time': 3.930466890335083, 'global_step': 89620, 'preemption_count': 0}), (90534, {'train/accuracy': 0.6768749952316284, 'train/loss': 1.3328397274017334, 'validation/accuracy': 0.6259399652481079, 'validation/loss': 1.5650203227996826, 'validation/num_examples': 50000, 'test/accuracy': 0.504800021648407, 'test/loss': 2.2196996212005615, 'test/num_examples': 10000, 'score': 41629.03953385353, 'total_duration': 46386.41199302673, 'accumulated_submission_time': 41629.03953385353, 'accumulated_eval_time': 4748.239414215088, 'accumulated_logging_time': 3.968465805053711, 'global_step': 90534, 'preemption_count': 0}), (91446, {'train/accuracy': 0.6907030940055847, 'train/loss': 1.2894749641418457, 'validation/accuracy': 0.6323599815368652, 'validation/loss': 1.5473542213439941, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.1950297355651855, 'test/num_examples': 10000, 'score': 42049.08965468407, 'total_duration': 46856.01924753189, 'accumulated_submission_time': 42049.08965468407, 'accumulated_eval_time': 4797.702326059341, 'accumulated_logging_time': 4.011040687561035, 'global_step': 91446, 'preemption_count': 0}), (92359, {'train/accuracy': 0.6765429377555847, 'train/loss': 1.3410677909851074, 'validation/accuracy': 0.6250799894332886, 'validation/loss': 1.5731840133666992, 'validation/num_examples': 50000, 'test/accuracy': 0.5012000203132629, 'test/loss': 2.218226194381714, 'test/num_examples': 10000, 'score': 42469.318239450455, 'total_duration': 47324.5852496624, 'accumulated_submission_time': 42469.318239450455, 'accumulated_eval_time': 4845.945533275604, 'accumulated_logging_time': 4.053576469421387, 'global_step': 92359, 'preemption_count': 0}), (93276, {'train/accuracy': 0.68115234375, 'train/loss': 1.303971529006958, 'validation/accuracy': 0.6333000063896179, 'validation/loss': 1.5291578769683838, 'validation/num_examples': 50000, 'test/accuracy': 0.5135000348091125, 'test/loss': 2.1676101684570312, 'test/num_examples': 10000, 'score': 42889.63666677475, 'total_duration': 47793.083832740784, 'accumulated_submission_time': 42889.63666677475, 'accumulated_eval_time': 4894.030555963516, 'accumulated_logging_time': 4.096000909805298, 'global_step': 93276, 'preemption_count': 0}), (94192, {'train/accuracy': 0.6882030963897705, 'train/loss': 1.2961716651916504, 'validation/accuracy': 0.6321399807929993, 'validation/loss': 1.5467734336853027, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.1976771354675293, 'test/num_examples': 10000, 'score': 43309.85211658478, 'total_duration': 48261.21060657501, 'accumulated_submission_time': 43309.85211658478, 'accumulated_eval_time': 4941.8448095321655, 'accumulated_logging_time': 4.140573740005493, 'global_step': 94192, 'preemption_count': 0}), (95110, {'train/accuracy': 0.7078515291213989, 'train/loss': 1.1914904117584229, 'validation/accuracy': 0.6358000040054321, 'validation/loss': 1.5101193189620972, 'validation/num_examples': 50000, 'test/accuracy': 0.5103999972343445, 'test/loss': 2.1727511882781982, 'test/num_examples': 10000, 'score': 43730.018189907074, 'total_duration': 48731.59634041786, 'accumulated_submission_time': 43730.018189907074, 'accumulated_eval_time': 4991.971055984497, 'accumulated_logging_time': 4.181777000427246, 'global_step': 95110, 'preemption_count': 0}), (96025, {'train/accuracy': 0.6827734112739563, 'train/loss': 1.2824567556381226, 'validation/accuracy': 0.6331599950790405, 'validation/loss': 1.5004512071609497, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.1757290363311768, 'test/num_examples': 10000, 'score': 44150.107147455215, 'total_duration': 49197.796854019165, 'accumulated_submission_time': 44150.107147455215, 'accumulated_eval_time': 5037.990777015686, 'accumulated_logging_time': 4.222255706787109, 'global_step': 96025, 'preemption_count': 0}), (96938, {'train/accuracy': 0.6849414110183716, 'train/loss': 1.297351360321045, 'validation/accuracy': 0.6337800025939941, 'validation/loss': 1.5296707153320312, 'validation/num_examples': 50000, 'test/accuracy': 0.5078999996185303, 'test/loss': 2.185168504714966, 'test/num_examples': 10000, 'score': 44570.26457285881, 'total_duration': 49665.81526470184, 'accumulated_submission_time': 44570.26457285881, 'accumulated_eval_time': 5085.7554042339325, 'accumulated_logging_time': 4.266266822814941, 'global_step': 96938, 'preemption_count': 0}), (97853, {'train/accuracy': 0.706347644329071, 'train/loss': 1.1973272562026978, 'validation/accuracy': 0.6412000060081482, 'validation/loss': 1.4900233745574951, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.120568037033081, 'test/num_examples': 10000, 'score': 44990.63896560669, 'total_duration': 50136.00769329071, 'accumulated_submission_time': 44990.63896560669, 'accumulated_eval_time': 5135.472050905228, 'accumulated_logging_time': 4.316110610961914, 'global_step': 97853, 'preemption_count': 0}), (98770, {'train/accuracy': 0.6881640553474426, 'train/loss': 1.2870441675186157, 'validation/accuracy': 0.6415799856185913, 'validation/loss': 1.4922298192977905, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.161956548690796, 'test/num_examples': 10000, 'score': 45410.94944810867, 'total_duration': 50603.08851194382, 'accumulated_submission_time': 45410.94944810867, 'accumulated_eval_time': 5182.138915061951, 'accumulated_logging_time': 4.366321802139282, 'global_step': 98770, 'preemption_count': 0}), (99686, {'train/accuracy': 0.6941796541213989, 'train/loss': 1.2387899160385132, 'validation/accuracy': 0.6456999778747559, 'validation/loss': 1.462598204612732, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.105508327484131, 'test/num_examples': 10000, 'score': 45831.12450551987, 'total_duration': 51072.9808216095, 'accumulated_submission_time': 45831.12450551987, 'accumulated_eval_time': 5231.758380651474, 'accumulated_logging_time': 4.412039041519165, 'global_step': 99686, 'preemption_count': 0}), (100594, {'train/accuracy': 0.7018945217132568, 'train/loss': 1.217579960823059, 'validation/accuracy': 0.642799973487854, 'validation/loss': 1.4809935092926025, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.107346296310425, 'test/num_examples': 10000, 'score': 46250.72084593773, 'total_duration': 51538.478660821915, 'accumulated_submission_time': 46250.72084593773, 'accumulated_eval_time': 5277.1856644153595, 'accumulated_logging_time': 4.834028720855713, 'global_step': 100594, 'preemption_count': 0}), (101508, {'train/accuracy': 0.6890038847923279, 'train/loss': 1.2767558097839355, 'validation/accuracy': 0.643839955329895, 'validation/loss': 1.4891250133514404, 'validation/num_examples': 50000, 'test/accuracy': 0.5198000073432922, 'test/loss': 2.138503074645996, 'test/num_examples': 10000, 'score': 46670.82105779648, 'total_duration': 52008.493248701096, 'accumulated_submission_time': 46670.82105779648, 'accumulated_eval_time': 5326.998807668686, 'accumulated_logging_time': 4.8833067417144775, 'global_step': 101508, 'preemption_count': 0}), (102425, {'train/accuracy': 0.7002929449081421, 'train/loss': 1.2077990770339966, 'validation/accuracy': 0.6493200063705444, 'validation/loss': 1.445142149925232, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.082939863204956, 'test/num_examples': 10000, 'score': 47091.16972374916, 'total_duration': 52478.20104813576, 'accumulated_submission_time': 47091.16972374916, 'accumulated_eval_time': 5376.259582996368, 'accumulated_logging_time': 4.928251028060913, 'global_step': 102425, 'preemption_count': 0}), (103341, {'train/accuracy': 0.7048437595367432, 'train/loss': 1.213708758354187, 'validation/accuracy': 0.647599995136261, 'validation/loss': 1.473738670349121, 'validation/num_examples': 50000, 'test/accuracy': 0.5266000032424927, 'test/loss': 2.1160645484924316, 'test/num_examples': 10000, 'score': 47511.17261624336, 'total_duration': 52946.35978603363, 'accumulated_submission_time': 47511.17261624336, 'accumulated_eval_time': 5424.322031259537, 'accumulated_logging_time': 4.9686126708984375, 'global_step': 103341, 'preemption_count': 0}), (104258, {'train/accuracy': 0.6996484398841858, 'train/loss': 1.24612295627594, 'validation/accuracy': 0.6481599807739258, 'validation/loss': 1.4764598608016968, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.1275715827941895, 'test/num_examples': 10000, 'score': 47931.445341825485, 'total_duration': 53416.33458185196, 'accumulated_submission_time': 47931.445341825485, 'accumulated_eval_time': 5473.918003559113, 'accumulated_logging_time': 5.021668195724487, 'global_step': 104258, 'preemption_count': 0}), (105170, {'train/accuracy': 0.701464831829071, 'train/loss': 1.2353618144989014, 'validation/accuracy': 0.6521399617195129, 'validation/loss': 1.4591549634933472, 'validation/num_examples': 50000, 'test/accuracy': 0.5285000205039978, 'test/loss': 2.1071197986602783, 'test/num_examples': 10000, 'score': 48351.39259338379, 'total_duration': 53882.78431844711, 'accumulated_submission_time': 48351.39259338379, 'accumulated_eval_time': 5520.325870513916, 'accumulated_logging_time': 5.064110040664673, 'global_step': 105170, 'preemption_count': 0}), (106082, {'train/accuracy': 0.708984375, 'train/loss': 1.166994333267212, 'validation/accuracy': 0.6547200083732605, 'validation/loss': 1.4128574132919312, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.0650956630706787, 'test/num_examples': 10000, 'score': 48771.495816230774, 'total_duration': 54349.56981277466, 'accumulated_submission_time': 48771.495816230774, 'accumulated_eval_time': 5566.9127151966095, 'accumulated_logging_time': 5.108115911483765, 'global_step': 106082, 'preemption_count': 0}), (106995, {'train/accuracy': 0.7225390672683716, 'train/loss': 1.131512999534607, 'validation/accuracy': 0.6502199769020081, 'validation/loss': 1.4502038955688477, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.100848436355591, 'test/num_examples': 10000, 'score': 49191.43444299698, 'total_duration': 54819.61263132095, 'accumulated_submission_time': 49191.43444299698, 'accumulated_eval_time': 5616.92050743103, 'accumulated_logging_time': 5.1528544425964355, 'global_step': 106995, 'preemption_count': 0}), (107912, {'train/accuracy': 0.7108789086341858, 'train/loss': 1.1651406288146973, 'validation/accuracy': 0.6574999690055847, 'validation/loss': 1.407038688659668, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.0478579998016357, 'test/num_examples': 10000, 'score': 49611.7597925663, 'total_duration': 55291.499467134476, 'accumulated_submission_time': 49611.7597925663, 'accumulated_eval_time': 5668.385055780411, 'accumulated_logging_time': 5.196820497512817, 'global_step': 107912, 'preemption_count': 0}), (108827, {'train/accuracy': 0.7103906273841858, 'train/loss': 1.18244469165802, 'validation/accuracy': 0.6546799540519714, 'validation/loss': 1.4344879388809204, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.086609125137329, 'test/num_examples': 10000, 'score': 50032.11195850372, 'total_duration': 55761.369703531265, 'accumulated_submission_time': 50032.11195850372, 'accumulated_eval_time': 5717.807471752167, 'accumulated_logging_time': 5.240591287612915, 'global_step': 108827, 'preemption_count': 0}), (109742, {'train/accuracy': 0.7229882478713989, 'train/loss': 1.1345579624176025, 'validation/accuracy': 0.6586799621582031, 'validation/loss': 1.426134467124939, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.0645999908447266, 'test/num_examples': 10000, 'score': 50452.293276786804, 'total_duration': 56229.43710780144, 'accumulated_submission_time': 50452.293276786804, 'accumulated_eval_time': 5765.595708608627, 'accumulated_logging_time': 5.286548137664795, 'global_step': 109742, 'preemption_count': 0}), (110658, {'train/accuracy': 0.7133398056030273, 'train/loss': 1.1982223987579346, 'validation/accuracy': 0.6570599675178528, 'validation/loss': 1.440184235572815, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.095458984375, 'test/num_examples': 10000, 'score': 50872.51973128319, 'total_duration': 56695.58960843086, 'accumulated_submission_time': 50872.51973128319, 'accumulated_eval_time': 5811.417835235596, 'accumulated_logging_time': 5.338428258895874, 'global_step': 110658, 'preemption_count': 0}), (111575, {'train/accuracy': 0.7201171517372131, 'train/loss': 1.1232796907424927, 'validation/accuracy': 0.6644399762153625, 'validation/loss': 1.3742200136184692, 'validation/num_examples': 50000, 'test/accuracy': 0.5422000288963318, 'test/loss': 2.019754409790039, 'test/num_examples': 10000, 'score': 51292.659793138504, 'total_duration': 57164.1505010128, 'accumulated_submission_time': 51292.659793138504, 'accumulated_eval_time': 5859.738709926605, 'accumulated_logging_time': 5.3858771324157715, 'global_step': 111575, 'preemption_count': 0}), (112491, {'train/accuracy': 0.7299218773841858, 'train/loss': 1.0928118228912354, 'validation/accuracy': 0.6615599989891052, 'validation/loss': 1.3830986022949219, 'validation/num_examples': 50000, 'test/accuracy': 0.5380000472068787, 'test/loss': 2.0178375244140625, 'test/num_examples': 10000, 'score': 51712.80060315132, 'total_duration': 57629.054327726364, 'accumulated_submission_time': 51712.80060315132, 'accumulated_eval_time': 5904.406593084335, 'accumulated_logging_time': 5.429534912109375, 'global_step': 112491, 'preemption_count': 0}), (113407, {'train/accuracy': 0.7118163704872131, 'train/loss': 1.1474217176437378, 'validation/accuracy': 0.6642599701881409, 'validation/loss': 1.3698629140853882, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 2.007035255432129, 'test/num_examples': 10000, 'score': 52132.85252594948, 'total_duration': 58097.311703681946, 'accumulated_submission_time': 52132.85252594948, 'accumulated_eval_time': 5952.5104813575745, 'accumulated_logging_time': 5.479061126708984, 'global_step': 113407, 'preemption_count': 0}), (114319, {'train/accuracy': 0.7221288681030273, 'train/loss': 1.117608666419983, 'validation/accuracy': 0.6670199632644653, 'validation/loss': 1.3616474866867065, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.0026955604553223, 'test/num_examples': 10000, 'score': 52552.876588106155, 'total_duration': 58562.885924339294, 'accumulated_submission_time': 52552.876588106155, 'accumulated_eval_time': 5997.965879917145, 'accumulated_logging_time': 5.521757125854492, 'global_step': 114319, 'preemption_count': 0}), (115236, {'train/accuracy': 0.7292773127555847, 'train/loss': 1.092387318611145, 'validation/accuracy': 0.6671000123023987, 'validation/loss': 1.3630927801132202, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 2.007647752761841, 'test/num_examples': 10000, 'score': 52973.100821495056, 'total_duration': 59032.51432132721, 'accumulated_submission_time': 52973.100821495056, 'accumulated_eval_time': 6047.270622730255, 'accumulated_logging_time': 5.5683817863464355, 'global_step': 115236, 'preemption_count': 0}), (116150, {'train/accuracy': 0.7247265577316284, 'train/loss': 1.1091890335083008, 'validation/accuracy': 0.6702199578285217, 'validation/loss': 1.354252815246582, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 1.9925272464752197, 'test/num_examples': 10000, 'score': 53393.11542224884, 'total_duration': 59500.606415987015, 'accumulated_submission_time': 53393.11542224884, 'accumulated_eval_time': 6095.24760055542, 'accumulated_logging_time': 5.615738153457642, 'global_step': 116150, 'preemption_count': 0}), (117066, {'train/accuracy': 0.7264843583106995, 'train/loss': 1.096244215965271, 'validation/accuracy': 0.672760009765625, 'validation/loss': 1.340212106704712, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 1.99046790599823, 'test/num_examples': 10000, 'score': 53813.10630583763, 'total_duration': 59967.58908033371, 'accumulated_submission_time': 53813.10630583763, 'accumulated_eval_time': 6142.140088558197, 'accumulated_logging_time': 5.661764621734619, 'global_step': 117066, 'preemption_count': 0}), (117982, {'train/accuracy': 0.7369335889816284, 'train/loss': 1.0518841743469238, 'validation/accuracy': 0.6758599877357483, 'validation/loss': 1.3248143196105957, 'validation/num_examples': 50000, 'test/accuracy': 0.5498000383377075, 'test/loss': 1.9690297842025757, 'test/num_examples': 10000, 'score': 54233.0949075222, 'total_duration': 60437.959518909454, 'accumulated_submission_time': 54233.0949075222, 'accumulated_eval_time': 6192.417934656143, 'accumulated_logging_time': 5.713583707809448, 'global_step': 117982, 'preemption_count': 0}), (118897, {'train/accuracy': 0.7423437237739563, 'train/loss': 1.0445845127105713, 'validation/accuracy': 0.6692599654197693, 'validation/loss': 1.3613452911376953, 'validation/num_examples': 50000, 'test/accuracy': 0.5490000247955322, 'test/loss': 2.0090084075927734, 'test/num_examples': 10000, 'score': 54653.28877854347, 'total_duration': 60906.417788267136, 'accumulated_submission_time': 54653.28877854347, 'accumulated_eval_time': 6240.577670812607, 'accumulated_logging_time': 5.76579213142395, 'global_step': 118897, 'preemption_count': 0}), (119812, {'train/accuracy': 0.7238671779632568, 'train/loss': 1.152235507965088, 'validation/accuracy': 0.6700599789619446, 'validation/loss': 1.3908920288085938, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.030686378479004, 'test/num_examples': 10000, 'score': 55073.62894105911, 'total_duration': 61375.50624871254, 'accumulated_submission_time': 55073.62894105911, 'accumulated_eval_time': 6289.227605819702, 'accumulated_logging_time': 5.812516689300537, 'global_step': 119812, 'preemption_count': 0}), (120727, {'train/accuracy': 0.7369335889816284, 'train/loss': 1.0716278553009033, 'validation/accuracy': 0.6764400005340576, 'validation/loss': 1.331760287284851, 'validation/num_examples': 50000, 'test/accuracy': 0.5505000352859497, 'test/loss': 1.9711410999298096, 'test/num_examples': 10000, 'score': 55493.843186855316, 'total_duration': 61845.34255743027, 'accumulated_submission_time': 55493.843186855316, 'accumulated_eval_time': 6338.752652645111, 'accumulated_logging_time': 5.857455015182495, 'global_step': 120727, 'preemption_count': 0}), (121644, {'train/accuracy': 0.746386706829071, 'train/loss': 1.0214617252349854, 'validation/accuracy': 0.6764199733734131, 'validation/loss': 1.3321937322616577, 'validation/num_examples': 50000, 'test/accuracy': 0.5493000149726868, 'test/loss': 1.9808764457702637, 'test/num_examples': 10000, 'score': 55914.077816963196, 'total_duration': 62315.006165504456, 'accumulated_submission_time': 55914.077816963196, 'accumulated_eval_time': 6388.082646846771, 'accumulated_logging_time': 5.904085636138916, 'global_step': 121644, 'preemption_count': 0}), (122559, {'train/accuracy': 0.7341406345367432, 'train/loss': 1.0811148881912231, 'validation/accuracy': 0.6796999573707581, 'validation/loss': 1.328904390335083, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.9604276418685913, 'test/num_examples': 10000, 'score': 56334.39921450615, 'total_duration': 62786.4237511158, 'accumulated_submission_time': 56334.39921450615, 'accumulated_eval_time': 6439.080575942993, 'accumulated_logging_time': 5.9502387046813965, 'global_step': 122559, 'preemption_count': 0}), (123473, {'train/accuracy': 0.7335742115974426, 'train/loss': 1.078386902809143, 'validation/accuracy': 0.6755599975585938, 'validation/loss': 1.3359389305114746, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 1.9859886169433594, 'test/num_examples': 10000, 'score': 56754.340900182724, 'total_duration': 63255.76044034958, 'accumulated_submission_time': 56754.340900182724, 'accumulated_eval_time': 6488.3691465854645, 'accumulated_logging_time': 6.0046210289001465, 'global_step': 123473, 'preemption_count': 0}), (124389, {'train/accuracy': 0.75, 'train/loss': 0.994757890701294, 'validation/accuracy': 0.6810799837112427, 'validation/loss': 1.293065071105957, 'validation/num_examples': 50000, 'test/accuracy': 0.5511000156402588, 'test/loss': 1.9417612552642822, 'test/num_examples': 10000, 'score': 57174.481586933136, 'total_duration': 63721.19270992279, 'accumulated_submission_time': 57174.481586933136, 'accumulated_eval_time': 6533.5595326423645, 'accumulated_logging_time': 6.052314043045044, 'global_step': 124389, 'preemption_count': 0}), (125305, {'train/accuracy': 0.74378901720047, 'train/loss': 1.0434715747833252, 'validation/accuracy': 0.6869999766349792, 'validation/loss': 1.2920804023742676, 'validation/num_examples': 50000, 'test/accuracy': 0.560200035572052, 'test/loss': 1.9421415328979492, 'test/num_examples': 10000, 'score': 57594.81980538368, 'total_duration': 64192.04764842987, 'accumulated_submission_time': 57594.81980538368, 'accumulated_eval_time': 6583.968408584595, 'accumulated_logging_time': 6.108830213546753, 'global_step': 125305, 'preemption_count': 0}), (126221, {'train/accuracy': 0.7441992163658142, 'train/loss': 1.0305161476135254, 'validation/accuracy': 0.6879000067710876, 'validation/loss': 1.277883529663086, 'validation/num_examples': 50000, 'test/accuracy': 0.5663000345230103, 'test/loss': 1.9110310077667236, 'test/num_examples': 10000, 'score': 58015.106491565704, 'total_duration': 64661.13260555267, 'accumulated_submission_time': 58015.106491565704, 'accumulated_eval_time': 6632.6700274944305, 'accumulated_logging_time': 6.153582334518433, 'global_step': 126221, 'preemption_count': 0}), (127138, {'train/accuracy': 0.7535937428474426, 'train/loss': 0.9919482469558716, 'validation/accuracy': 0.6867799758911133, 'validation/loss': 1.2697197198867798, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.8933143615722656, 'test/num_examples': 10000, 'score': 58435.141280412674, 'total_duration': 65131.23244309425, 'accumulated_submission_time': 58435.141280412674, 'accumulated_eval_time': 6682.635187864304, 'accumulated_logging_time': 6.201958656311035, 'global_step': 127138, 'preemption_count': 0}), (128053, {'train/accuracy': 0.7463476657867432, 'train/loss': 1.0128285884857178, 'validation/accuracy': 0.6887399554252625, 'validation/loss': 1.2626125812530518, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.9025838375091553, 'test/num_examples': 10000, 'score': 58855.1561756134, 'total_duration': 65600.41216874123, 'accumulated_submission_time': 58855.1561756134, 'accumulated_eval_time': 6731.701937913895, 'accumulated_logging_time': 6.2473344802856445, 'global_step': 128053, 'preemption_count': 0}), (128968, {'train/accuracy': 0.7517382502555847, 'train/loss': 0.9961110353469849, 'validation/accuracy': 0.6873199939727783, 'validation/loss': 1.2667717933654785, 'validation/num_examples': 50000, 'test/accuracy': 0.5649000406265259, 'test/loss': 1.8984917402267456, 'test/num_examples': 10000, 'score': 59275.097254276276, 'total_duration': 66071.41319656372, 'accumulated_submission_time': 59275.097254276276, 'accumulated_eval_time': 6782.662457227707, 'accumulated_logging_time': 6.294980049133301, 'global_step': 128968, 'preemption_count': 0}), (129883, {'train/accuracy': 0.7596484422683716, 'train/loss': 0.9546266794204712, 'validation/accuracy': 0.693839967250824, 'validation/loss': 1.2397924661636353, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.8531309366226196, 'test/num_examples': 10000, 'score': 59695.053881406784, 'total_duration': 66539.30385136604, 'accumulated_submission_time': 59695.053881406784, 'accumulated_eval_time': 6830.492385864258, 'accumulated_logging_time': 6.34734582901001, 'global_step': 129883, 'preemption_count': 0}), (130796, {'train/accuracy': 0.764453113079071, 'train/loss': 0.9491795897483826, 'validation/accuracy': 0.691540002822876, 'validation/loss': 1.2591371536254883, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.8973604440689087, 'test/num_examples': 10000, 'score': 60115.20013332367, 'total_duration': 67008.34401488304, 'accumulated_submission_time': 60115.20013332367, 'accumulated_eval_time': 6879.277712106705, 'accumulated_logging_time': 6.404633522033691, 'global_step': 130796, 'preemption_count': 0}), (131711, {'train/accuracy': 0.7549218535423279, 'train/loss': 0.9674057364463806, 'validation/accuracy': 0.6934999823570251, 'validation/loss': 1.2365845441818237, 'validation/num_examples': 50000, 'test/accuracy': 0.5690000057220459, 'test/loss': 1.871274471282959, 'test/num_examples': 10000, 'score': 60535.409593105316, 'total_duration': 67477.44813871384, 'accumulated_submission_time': 60535.409593105316, 'accumulated_eval_time': 6928.066775798798, 'accumulated_logging_time': 6.457538366317749, 'global_step': 131711, 'preemption_count': 0}), (132625, {'train/accuracy': 0.7603515386581421, 'train/loss': 0.9755155444145203, 'validation/accuracy': 0.6967200040817261, 'validation/loss': 1.2530672550201416, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.8836565017700195, 'test/num_examples': 10000, 'score': 60955.45423436165, 'total_duration': 67948.19107365608, 'accumulated_submission_time': 60955.45423436165, 'accumulated_eval_time': 6978.662467956543, 'accumulated_logging_time': 6.508562088012695, 'global_step': 132625, 'preemption_count': 0}), (133540, {'train/accuracy': 0.7734179496765137, 'train/loss': 0.9080670475959778, 'validation/accuracy': 0.6987400054931641, 'validation/loss': 1.2287685871124268, 'validation/num_examples': 50000, 'test/accuracy': 0.5764999985694885, 'test/loss': 1.851581335067749, 'test/num_examples': 10000, 'score': 61375.42639231682, 'total_duration': 68416.36034202576, 'accumulated_submission_time': 61375.42639231682, 'accumulated_eval_time': 7026.758812665939, 'accumulated_logging_time': 6.553529500961304, 'global_step': 133540, 'preemption_count': 0}), (134453, {'train/accuracy': 0.7599413990974426, 'train/loss': 0.9587976336479187, 'validation/accuracy': 0.7029199600219727, 'validation/loss': 1.2122979164123535, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 1.8428810834884644, 'test/num_examples': 10000, 'score': 61795.71336269379, 'total_duration': 68883.31156492233, 'accumulated_submission_time': 61795.71336269379, 'accumulated_eval_time': 7073.3166263103485, 'accumulated_logging_time': 6.606256008148193, 'global_step': 134453, 'preemption_count': 0}), (135369, {'train/accuracy': 0.7627929449081421, 'train/loss': 0.9346864819526672, 'validation/accuracy': 0.7001399993896484, 'validation/loss': 1.2140921354293823, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.8443801403045654, 'test/num_examples': 10000, 'score': 62216.026047468185, 'total_duration': 69354.34955620766, 'accumulated_submission_time': 62216.026047468185, 'accumulated_eval_time': 7123.942526578903, 'accumulated_logging_time': 6.653573513031006, 'global_step': 135369, 'preemption_count': 0}), (136284, {'train/accuracy': 0.7731054425239563, 'train/loss': 0.9033991694450378, 'validation/accuracy': 0.7005999684333801, 'validation/loss': 1.2164199352264404, 'validation/num_examples': 50000, 'test/accuracy': 0.5800999999046326, 'test/loss': 1.845173954963684, 'test/num_examples': 10000, 'score': 62636.19765305519, 'total_duration': 69823.86749219894, 'accumulated_submission_time': 62636.19765305519, 'accumulated_eval_time': 7173.187728404999, 'accumulated_logging_time': 6.702480316162109, 'global_step': 136284, 'preemption_count': 0}), (137195, {'train/accuracy': 0.7671093344688416, 'train/loss': 0.9361597299575806, 'validation/accuracy': 0.7026599645614624, 'validation/loss': 1.2115135192871094, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.8194403648376465, 'test/num_examples': 10000, 'score': 63056.44626426697, 'total_duration': 70293.874396801, 'accumulated_submission_time': 63056.44626426697, 'accumulated_eval_time': 7222.835695266724, 'accumulated_logging_time': 6.760934591293335, 'global_step': 137195, 'preemption_count': 0}), (138106, {'train/accuracy': 0.7684765458106995, 'train/loss': 0.9252225756645203, 'validation/accuracy': 0.7012400031089783, 'validation/loss': 1.2247495651245117, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.8519049882888794, 'test/num_examples': 10000, 'score': 63476.769704818726, 'total_duration': 70763.18441557884, 'accumulated_submission_time': 63476.769704818726, 'accumulated_eval_time': 7271.722208023071, 'accumulated_logging_time': 6.809126853942871, 'global_step': 138106, 'preemption_count': 0}), (139019, {'train/accuracy': 0.7814257740974426, 'train/loss': 0.8607298135757446, 'validation/accuracy': 0.7080000042915344, 'validation/loss': 1.1757205724716187, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 1.8092293739318848, 'test/num_examples': 10000, 'score': 63896.763201236725, 'total_duration': 71234.63944602013, 'accumulated_submission_time': 63896.763201236725, 'accumulated_eval_time': 7323.079668521881, 'accumulated_logging_time': 6.859033823013306, 'global_step': 139019, 'preemption_count': 0}), (139933, {'train/accuracy': 0.7718554735183716, 'train/loss': 0.9126390814781189, 'validation/accuracy': 0.7079600095748901, 'validation/loss': 1.185042142868042, 'validation/num_examples': 50000, 'test/accuracy': 0.5852000117301941, 'test/loss': 1.8195505142211914, 'test/num_examples': 10000, 'score': 64317.06393766403, 'total_duration': 71702.45918250084, 'accumulated_submission_time': 64317.06393766403, 'accumulated_eval_time': 7370.497317314148, 'accumulated_logging_time': 6.90878701210022, 'global_step': 139933, 'preemption_count': 0}), (140847, {'train/accuracy': 0.7747851610183716, 'train/loss': 0.8819167017936707, 'validation/accuracy': 0.7116400003433228, 'validation/loss': 1.1624095439910889, 'validation/num_examples': 50000, 'test/accuracy': 0.5847000479698181, 'test/loss': 1.7930099964141846, 'test/num_examples': 10000, 'score': 64737.026354551315, 'total_duration': 72173.21574926376, 'accumulated_submission_time': 64737.026354551315, 'accumulated_eval_time': 7421.191428661346, 'accumulated_logging_time': 6.957134008407593, 'global_step': 140847, 'preemption_count': 0}), (141762, {'train/accuracy': 0.77978515625, 'train/loss': 0.8617091178894043, 'validation/accuracy': 0.7116599678993225, 'validation/loss': 1.165801763534546, 'validation/num_examples': 50000, 'test/accuracy': 0.5872000455856323, 'test/loss': 1.7895723581314087, 'test/num_examples': 10000, 'score': 65157.04562306404, 'total_duration': 72645.75348472595, 'accumulated_submission_time': 65157.04562306404, 'accumulated_eval_time': 7473.606050014496, 'accumulated_logging_time': 7.00886607170105, 'global_step': 141762, 'preemption_count': 0}), (142676, {'train/accuracy': 0.7821679711341858, 'train/loss': 0.8531965613365173, 'validation/accuracy': 0.7168200016021729, 'validation/loss': 1.1503877639770508, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.76817786693573, 'test/num_examples': 10000, 'score': 65577.20102715492, 'total_duration': 73119.00194859505, 'accumulated_submission_time': 65577.20102715492, 'accumulated_eval_time': 7526.591713428497, 'accumulated_logging_time': 7.064823865890503, 'global_step': 142676, 'preemption_count': 0}), (143588, {'train/accuracy': 0.7820116877555847, 'train/loss': 0.8580734729766846, 'validation/accuracy': 0.7165200114250183, 'validation/loss': 1.1449278593063354, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.7643986940383911, 'test/num_examples': 10000, 'score': 65996.80779743195, 'total_duration': 73590.25830888748, 'accumulated_submission_time': 65996.80779743195, 'accumulated_eval_time': 7577.742676258087, 'accumulated_logging_time': 7.511917352676392, 'global_step': 143588, 'preemption_count': 0}), (144501, {'train/accuracy': 0.7851171493530273, 'train/loss': 0.8476221561431885, 'validation/accuracy': 0.7163400053977966, 'validation/loss': 1.144951581954956, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.7644582986831665, 'test/num_examples': 10000, 'score': 66416.94591355324, 'total_duration': 74057.55110120773, 'accumulated_submission_time': 66416.94591355324, 'accumulated_eval_time': 7624.79746389389, 'accumulated_logging_time': 7.559880018234253, 'global_step': 144501, 'preemption_count': 0}), (145415, {'train/accuracy': 0.7938281297683716, 'train/loss': 0.8047267198562622, 'validation/accuracy': 0.7177599668502808, 'validation/loss': 1.1373757123947144, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.743360996246338, 'test/num_examples': 10000, 'score': 66837.3013036251, 'total_duration': 74521.7456138134, 'accumulated_submission_time': 66837.3013036251, 'accumulated_eval_time': 7668.535885095596, 'accumulated_logging_time': 7.608190298080444, 'global_step': 145415, 'preemption_count': 0}), (146328, {'train/accuracy': 0.7868554592132568, 'train/loss': 0.8384296894073486, 'validation/accuracy': 0.7208999991416931, 'validation/loss': 1.1160486936569214, 'validation/num_examples': 50000, 'test/accuracy': 0.6013000011444092, 'test/loss': 1.727980136871338, 'test/num_examples': 10000, 'score': 67257.48422813416, 'total_duration': 74991.41473031044, 'accumulated_submission_time': 67257.48422813416, 'accumulated_eval_time': 7717.922769069672, 'accumulated_logging_time': 7.655825853347778, 'global_step': 146328, 'preemption_count': 0}), (147243, {'train/accuracy': 0.7939453125, 'train/loss': 0.8069570064544678, 'validation/accuracy': 0.7222599983215332, 'validation/loss': 1.1095448732376099, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.7282333374023438, 'test/num_examples': 10000, 'score': 67677.5356965065, 'total_duration': 75460.78970003128, 'accumulated_submission_time': 67677.5356965065, 'accumulated_eval_time': 7767.141966342926, 'accumulated_logging_time': 7.707298278808594, 'global_step': 147243, 'preemption_count': 0}), (148158, {'train/accuracy': 0.7961523532867432, 'train/loss': 0.7881932854652405, 'validation/accuracy': 0.7221599817276001, 'validation/loss': 1.1152422428131104, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.723463773727417, 'test/num_examples': 10000, 'score': 68097.77912926674, 'total_duration': 75932.03611707687, 'accumulated_submission_time': 68097.77912926674, 'accumulated_eval_time': 7818.040345191956, 'accumulated_logging_time': 7.759216070175171, 'global_step': 148158, 'preemption_count': 0}), (149073, {'train/accuracy': 0.7910937070846558, 'train/loss': 0.8174343109130859, 'validation/accuracy': 0.7259599566459656, 'validation/loss': 1.0968220233917236, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.7236626148223877, 'test/num_examples': 10000, 'score': 68517.94238138199, 'total_duration': 76401.87269186974, 'accumulated_submission_time': 68517.94238138199, 'accumulated_eval_time': 7867.6073389053345, 'accumulated_logging_time': 7.8136961460113525, 'global_step': 149073, 'preemption_count': 0}), (149988, {'train/accuracy': 0.7939257621765137, 'train/loss': 0.8128007054328918, 'validation/accuracy': 0.7231400012969971, 'validation/loss': 1.1107922792434692, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.7529717683792114, 'test/num_examples': 10000, 'score': 68937.99177789688, 'total_duration': 76871.03368258476, 'accumulated_submission_time': 68937.99177789688, 'accumulated_eval_time': 7916.614735364914, 'accumulated_logging_time': 7.865723133087158, 'global_step': 149988, 'preemption_count': 0}), (150900, {'train/accuracy': 0.80189448595047, 'train/loss': 0.7681282758712769, 'validation/accuracy': 0.7280600070953369, 'validation/loss': 1.0890097618103027, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.7106510400772095, 'test/num_examples': 10000, 'score': 69358.29975485802, 'total_duration': 77339.0362174511, 'accumulated_submission_time': 69358.29975485802, 'accumulated_eval_time': 7964.20180106163, 'accumulated_logging_time': 7.918400287628174, 'global_step': 150900, 'preemption_count': 0}), (151815, {'train/accuracy': 0.7974413633346558, 'train/loss': 0.7807225584983826, 'validation/accuracy': 0.7289199829101562, 'validation/loss': 1.0758874416351318, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.681167483329773, 'test/num_examples': 10000, 'score': 69778.54154014587, 'total_duration': 77809.62989974022, 'accumulated_submission_time': 69778.54154014587, 'accumulated_eval_time': 8014.445042133331, 'accumulated_logging_time': 7.975107192993164, 'global_step': 151815, 'preemption_count': 0}), (152729, {'train/accuracy': 0.8052343726158142, 'train/loss': 0.7729941606521606, 'validation/accuracy': 0.7317799925804138, 'validation/loss': 1.0814770460128784, 'validation/num_examples': 50000, 'test/accuracy': 0.6094000339508057, 'test/loss': 1.697064757347107, 'test/num_examples': 10000, 'score': 70198.79391741753, 'total_duration': 78281.05008983612, 'accumulated_submission_time': 70198.79391741753, 'accumulated_eval_time': 8065.495423555374, 'accumulated_logging_time': 8.04086971282959, 'global_step': 152729, 'preemption_count': 0}), (153644, {'train/accuracy': 0.805468738079071, 'train/loss': 0.7481812834739685, 'validation/accuracy': 0.7303999662399292, 'validation/loss': 1.0703809261322021, 'validation/num_examples': 50000, 'test/accuracy': 0.6111000180244446, 'test/loss': 1.6869773864746094, 'test/num_examples': 10000, 'score': 70618.86042189598, 'total_duration': 78747.78933882713, 'accumulated_submission_time': 70618.86042189598, 'accumulated_eval_time': 8112.055419683456, 'accumulated_logging_time': 8.100271940231323, 'global_step': 153644, 'preemption_count': 0}), (154558, {'train/accuracy': 0.806640625, 'train/loss': 0.7597796320915222, 'validation/accuracy': 0.7336199879646301, 'validation/loss': 1.068161129951477, 'validation/num_examples': 50000, 'test/accuracy': 0.6081000566482544, 'test/loss': 1.683420181274414, 'test/num_examples': 10000, 'score': 71038.89610767365, 'total_duration': 79215.44156551361, 'accumulated_submission_time': 71038.89610767365, 'accumulated_eval_time': 8159.562078952789, 'accumulated_logging_time': 8.1575448513031, 'global_step': 154558, 'preemption_count': 0}), (155470, {'train/accuracy': 0.8057226538658142, 'train/loss': 0.7690586447715759, 'validation/accuracy': 0.7343399524688721, 'validation/loss': 1.0750396251678467, 'validation/num_examples': 50000, 'test/accuracy': 0.6116000413894653, 'test/loss': 1.6947035789489746, 'test/num_examples': 10000, 'score': 71458.79195690155, 'total_duration': 79681.98218774796, 'accumulated_submission_time': 71458.79195690155, 'accumulated_eval_time': 8206.094826936722, 'accumulated_logging_time': 8.21754264831543, 'global_step': 155470, 'preemption_count': 0}), (156385, {'train/accuracy': 0.8109179735183716, 'train/loss': 0.7232632040977478, 'validation/accuracy': 0.7350800037384033, 'validation/loss': 1.0466346740722656, 'validation/num_examples': 50000, 'test/accuracy': 0.6154000163078308, 'test/loss': 1.6492758989334106, 'test/num_examples': 10000, 'score': 71878.8363673687, 'total_duration': 80151.62154054642, 'accumulated_submission_time': 71878.8363673687, 'accumulated_eval_time': 8255.587515115738, 'accumulated_logging_time': 8.26708173751831, 'global_step': 156385, 'preemption_count': 0}), (157302, {'train/accuracy': 0.8225781321525574, 'train/loss': 0.6812586188316345, 'validation/accuracy': 0.739139974117279, 'validation/loss': 1.0413532257080078, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.6407500505447388, 'test/num_examples': 10000, 'score': 72298.85396432877, 'total_duration': 80617.73319363594, 'accumulated_submission_time': 72298.85396432877, 'accumulated_eval_time': 8301.570994377136, 'accumulated_logging_time': 8.324889659881592, 'global_step': 157302, 'preemption_count': 0}), (158215, {'train/accuracy': 0.8095898032188416, 'train/loss': 0.7571691870689392, 'validation/accuracy': 0.7379199862480164, 'validation/loss': 1.0595265626907349, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.6860063076019287, 'test/num_examples': 10000, 'score': 72719.15087842941, 'total_duration': 81086.91956949234, 'accumulated_submission_time': 72719.15087842941, 'accumulated_eval_time': 8350.355193376541, 'accumulated_logging_time': 8.37764310836792, 'global_step': 158215, 'preemption_count': 0}), (159131, {'train/accuracy': 0.8150585889816284, 'train/loss': 0.7215979695320129, 'validation/accuracy': 0.7387599945068359, 'validation/loss': 1.0488839149475098, 'validation/num_examples': 50000, 'test/accuracy': 0.6171000003814697, 'test/loss': 1.6728469133377075, 'test/num_examples': 10000, 'score': 73139.33359980583, 'total_duration': 81553.05968117714, 'accumulated_submission_time': 73139.33359980583, 'accumulated_eval_time': 8396.208889245987, 'accumulated_logging_time': 8.428681373596191, 'global_step': 159131, 'preemption_count': 0}), (160046, {'train/accuracy': 0.8247265219688416, 'train/loss': 0.6886405944824219, 'validation/accuracy': 0.7408199906349182, 'validation/loss': 1.0412501096725464, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.6554361581802368, 'test/num_examples': 10000, 'score': 73559.36250901222, 'total_duration': 82023.44992232323, 'accumulated_submission_time': 73559.36250901222, 'accumulated_eval_time': 8446.458607912064, 'accumulated_logging_time': 8.487337112426758, 'global_step': 160046, 'preemption_count': 0}), (160960, {'train/accuracy': 0.8200390338897705, 'train/loss': 0.7016811370849609, 'validation/accuracy': 0.744439959526062, 'validation/loss': 1.0223753452301025, 'validation/num_examples': 50000, 'test/accuracy': 0.6252000331878662, 'test/loss': 1.632118582725525, 'test/num_examples': 10000, 'score': 73979.74830436707, 'total_duration': 82492.14982962608, 'accumulated_submission_time': 73979.74830436707, 'accumulated_eval_time': 8494.669929981232, 'accumulated_logging_time': 8.537365913391113, 'global_step': 160960, 'preemption_count': 0}), (161873, {'train/accuracy': 0.8234765529632568, 'train/loss': 0.682098388671875, 'validation/accuracy': 0.7447400093078613, 'validation/loss': 1.0194628238677979, 'validation/num_examples': 50000, 'test/accuracy': 0.6238000392913818, 'test/loss': 1.6342037916183472, 'test/num_examples': 10000, 'score': 74399.8327550888, 'total_duration': 82959.62244343758, 'accumulated_submission_time': 74399.8327550888, 'accumulated_eval_time': 8541.95247745514, 'accumulated_logging_time': 8.591475486755371, 'global_step': 161873, 'preemption_count': 0}), (162788, {'train/accuracy': 0.8300195336341858, 'train/loss': 0.6636582612991333, 'validation/accuracy': 0.7465400099754333, 'validation/loss': 1.0095263719558716, 'validation/num_examples': 50000, 'test/accuracy': 0.6231000423431396, 'test/loss': 1.6292407512664795, 'test/num_examples': 10000, 'score': 74819.76510477066, 'total_duration': 83429.02394080162, 'accumulated_submission_time': 74819.76510477066, 'accumulated_eval_time': 8591.316792964935, 'accumulated_logging_time': 8.644585847854614, 'global_step': 162788, 'preemption_count': 0}), (163702, {'train/accuracy': 0.8236327767372131, 'train/loss': 0.6771386861801147, 'validation/accuracy': 0.7473399639129639, 'validation/loss': 1.0083914995193481, 'validation/num_examples': 50000, 'test/accuracy': 0.6289000511169434, 'test/loss': 1.6085121631622314, 'test/num_examples': 10000, 'score': 75239.78667020798, 'total_duration': 83894.52511429787, 'accumulated_submission_time': 75239.78667020798, 'accumulated_eval_time': 8636.684086561203, 'accumulated_logging_time': 8.705162763595581, 'global_step': 163702, 'preemption_count': 0}), (164617, {'train/accuracy': 0.8244921565055847, 'train/loss': 0.6821433305740356, 'validation/accuracy': 0.7472999691963196, 'validation/loss': 1.0059750080108643, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.6200848817825317, 'test/num_examples': 10000, 'score': 75659.76452445984, 'total_duration': 84362.22888803482, 'accumulated_submission_time': 75659.76452445984, 'accumulated_eval_time': 8684.300383806229, 'accumulated_logging_time': 8.755192041397095, 'global_step': 164617, 'preemption_count': 0}), (165531, {'train/accuracy': 0.8278319835662842, 'train/loss': 0.6549848914146423, 'validation/accuracy': 0.7478399872779846, 'validation/loss': 0.9953515529632568, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.6096539497375488, 'test/num_examples': 10000, 'score': 76080.02516317368, 'total_duration': 84832.28757619858, 'accumulated_submission_time': 76080.02516317368, 'accumulated_eval_time': 8733.994886159897, 'accumulated_logging_time': 8.806623458862305, 'global_step': 165531, 'preemption_count': 0}), (166447, {'train/accuracy': 0.8307226300239563, 'train/loss': 0.6519699096679688, 'validation/accuracy': 0.7495200037956238, 'validation/loss': 0.9923651218414307, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.6084636449813843, 'test/num_examples': 10000, 'score': 76500.17499065399, 'total_duration': 85297.9245531559, 'accumulated_submission_time': 76500.17499065399, 'accumulated_eval_time': 8779.374361515045, 'accumulated_logging_time': 8.862721681594849, 'global_step': 166447, 'preemption_count': 0}), (167359, {'train/accuracy': 0.8291015625, 'train/loss': 0.6523779034614563, 'validation/accuracy': 0.7506399750709534, 'validation/loss': 0.9864511489868164, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.5886270999908447, 'test/num_examples': 10000, 'score': 76920.16060471535, 'total_duration': 85767.60281062126, 'accumulated_submission_time': 76920.16060471535, 'accumulated_eval_time': 8828.95196390152, 'accumulated_logging_time': 8.925573348999023, 'global_step': 167359, 'preemption_count': 0}), (168272, {'train/accuracy': 0.8313866853713989, 'train/loss': 0.637546181678772, 'validation/accuracy': 0.753600001335144, 'validation/loss': 0.9787457585334778, 'validation/num_examples': 50000, 'test/accuracy': 0.6333000063896179, 'test/loss': 1.587046504020691, 'test/num_examples': 10000, 'score': 77340.48335027695, 'total_duration': 86236.98085284233, 'accumulated_submission_time': 77340.48335027695, 'accumulated_eval_time': 8877.900985002518, 'accumulated_logging_time': 8.980156898498535, 'global_step': 168272, 'preemption_count': 0})], 'global_step': 168670}
I0206 14:33:22.065136 140107197974336 submission_runner.py:586] Timing: 77520.35540795326
I0206 14:33:22.065240 140107197974336 submission_runner.py:588] Total number of evals: 185
I0206 14:33:22.065293 140107197974336 submission_runner.py:589] ====================
I0206 14:33:22.065349 140107197974336 submission_runner.py:542] Using RNG seed 1274177056
I0206 14:33:22.066893 140107197974336 submission_runner.py:551] --- Tuning run 4/5 ---
I0206 14:33:22.067019 140107197974336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_4.
I0206 14:33:22.070209 140107197974336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_4/hparams.json.
I0206 14:33:22.071106 140107197974336 submission_runner.py:206] Initializing dataset.
I0206 14:33:22.084021 140107197974336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0206 14:33:22.097561 140107197974336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 14:33:22.305817 140107197974336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0206 14:33:27.197631 140107197974336 submission_runner.py:213] Initializing model.
I0206 14:33:32.915185 140107197974336 submission_runner.py:255] Initializing optimizer.
I0206 14:33:33.393795 140107197974336 submission_runner.py:262] Initializing metrics bundle.
I0206 14:33:33.393954 140107197974336 submission_runner.py:280] Initializing checkpoint and logger.
I0206 14:33:33.411718 140107197974336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_4 with prefix checkpoint_
I0206 14:33:33.411848 140107197974336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 14:33:50.167858 140107197974336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 14:34:06.735355 140107197974336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_4/flags_0.json.
I0206 14:34:06.740385 140107197974336 submission_runner.py:314] Starting training loop.
I0206 14:34:41.998297 139946372675328 logging_writer.py:48] [0] global_step=0, grad_norm=0.37231191992759705, loss=6.907756328582764
I0206 14:34:42.010688 140107197974336 spec.py:321] Evaluating on the training split.
I0206 14:34:50.497835 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 14:35:09.665975 140107197974336 spec.py:349] Evaluating on the test split.
I0206 14:35:11.263650 140107197974336 submission_runner.py:408] Time since start: 64.52s, 	Step: 1, 	{'train/accuracy': 0.0009765625, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 35.27015542984009, 'total_duration': 64.52321434020996, 'accumulated_submission_time': 35.27015542984009, 'accumulated_eval_time': 29.252927780151367, 'accumulated_logging_time': 0}
I0206 14:35:11.272360 139946381068032 logging_writer.py:48] [1] accumulated_eval_time=29.252928, accumulated_logging_time=0, accumulated_submission_time=35.270155, global_step=1, preemption_count=0, score=35.270155, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=64.523214, train/accuracy=0.000977, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0206 14:36:17.998831 139946414638848 logging_writer.py:48] [100] global_step=100, grad_norm=0.552535355091095, loss=6.823212146759033
I0206 14:37:03.996105 139946397853440 logging_writer.py:48] [200] global_step=200, grad_norm=0.8172916173934937, loss=6.700932502746582
I0206 14:37:52.656075 139946414638848 logging_writer.py:48] [300] global_step=300, grad_norm=0.759077250957489, loss=6.580870628356934
I0206 14:38:40.296773 139946397853440 logging_writer.py:48] [400] global_step=400, grad_norm=0.9970718622207642, loss=6.621547698974609
I0206 14:39:27.352034 139946414638848 logging_writer.py:48] [500] global_step=500, grad_norm=1.045211672782898, loss=6.413815975189209
I0206 14:40:14.293971 139946397853440 logging_writer.py:48] [600] global_step=600, grad_norm=0.7616859674453735, loss=6.386605739593506
I0206 14:41:01.456830 139946414638848 logging_writer.py:48] [700] global_step=700, grad_norm=0.793613851070404, loss=6.253900527954102
I0206 14:41:48.795343 139946397853440 logging_writer.py:48] [800] global_step=800, grad_norm=0.7233229875564575, loss=6.1751227378845215
I0206 14:42:11.700573 140107197974336 spec.py:321] Evaluating on the training split.
I0206 14:42:23.357931 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 14:42:57.935097 140107197974336 spec.py:349] Evaluating on the test split.
I0206 14:42:59.540021 140107197974336 submission_runner.py:408] Time since start: 532.80s, 	Step: 850, 	{'train/accuracy': 0.03515625, 'train/loss': 5.885966777801514, 'validation/accuracy': 0.03391999751329422, 'validation/loss': 5.9119391441345215, 'validation/num_examples': 50000, 'test/accuracy': 0.02680000104010105, 'test/loss': 6.039024353027344, 'test/num_examples': 10000, 'score': 455.6410744190216, 'total_duration': 532.7995707988739, 'accumulated_submission_time': 455.6410744190216, 'accumulated_eval_time': 77.09238171577454, 'accumulated_logging_time': 0.017679691314697266}
I0206 14:42:59.558198 139946414638848 logging_writer.py:48] [850] accumulated_eval_time=77.092382, accumulated_logging_time=0.017680, accumulated_submission_time=455.641074, global_step=850, preemption_count=0, score=455.641074, test/accuracy=0.026800, test/loss=6.039024, test/num_examples=10000, total_duration=532.799571, train/accuracy=0.035156, train/loss=5.885967, validation/accuracy=0.033920, validation/loss=5.911939, validation/num_examples=50000
I0206 14:43:19.617381 139946397853440 logging_writer.py:48] [900] global_step=900, grad_norm=0.9938035011291504, loss=6.347988128662109
I0206 14:44:04.886459 139946414638848 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7472816109657288, loss=6.1767730712890625
I0206 14:44:51.927731 139946397853440 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7672434449195862, loss=6.038355827331543
I0206 14:45:39.096627 139946414638848 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7548049688339233, loss=6.089603424072266
I0206 14:46:26.096020 139946397853440 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7408219575881958, loss=5.972845554351807
I0206 14:47:13.275013 139946414638848 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6293218731880188, loss=5.9969096183776855
I0206 14:48:00.058552 139946397853440 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8321903944015503, loss=6.055375576019287
I0206 14:48:47.277867 139946414638848 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.670006275177002, loss=5.906533241271973
I0206 14:49:34.255995 139946397853440 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7020002603530884, loss=5.9561967849731445
I0206 14:49:59.711080 140107197974336 spec.py:321] Evaluating on the training split.
I0206 14:50:11.243691 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 14:50:48.410496 140107197974336 spec.py:349] Evaluating on the test split.
I0206 14:50:50.010598 140107197974336 submission_runner.py:408] Time since start: 1003.27s, 	Step: 1756, 	{'train/accuracy': 0.07289062440395355, 'train/loss': 5.357734680175781, 'validation/accuracy': 0.0655599981546402, 'validation/loss': 5.43528413772583, 'validation/num_examples': 50000, 'test/accuracy': 0.053300000727176666, 'test/loss': 5.654690742492676, 'test/num_examples': 10000, 'score': 875.7305772304535, 'total_duration': 1003.2701570987701, 'accumulated_submission_time': 875.7305772304535, 'accumulated_eval_time': 127.39190244674683, 'accumulated_logging_time': 0.04587912559509277}
I0206 14:50:50.029316 139946414638848 logging_writer.py:48] [1756] accumulated_eval_time=127.391902, accumulated_logging_time=0.045879, accumulated_submission_time=875.730577, global_step=1756, preemption_count=0, score=875.730577, test/accuracy=0.053300, test/loss=5.654691, test/num_examples=10000, total_duration=1003.270157, train/accuracy=0.072891, train/loss=5.357735, validation/accuracy=0.065560, validation/loss=5.435284, validation/num_examples=50000
I0206 14:51:07.726816 139946397853440 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.625626266002655, loss=6.13046407699585
I0206 14:51:52.432065 139946414638848 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.45741742849349976, loss=6.413178443908691
I0206 14:52:39.454986 139946397853440 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.4981698989868164, loss=5.814477443695068
I0206 14:53:26.261910 139946414638848 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.41845905780792236, loss=6.299509048461914
I0206 14:54:12.960748 139946397853440 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5078954696655273, loss=5.741995811462402
I0206 14:55:00.142496 139946414638848 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5484275817871094, loss=5.616796970367432
I0206 14:55:46.789001 139946397853440 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.4159574806690216, loss=6.419929504394531
I0206 14:56:33.829288 139946414638848 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.45755571126937866, loss=6.563048839569092
I0206 14:57:20.727358 139946397853440 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.45111745595932007, loss=5.512044429779053
I0206 14:57:50.223441 140107197974336 spec.py:321] Evaluating on the training split.
I0206 14:58:01.017676 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 14:58:39.584492 140107197974336 spec.py:349] Evaluating on the test split.
I0206 14:58:41.184235 140107197974336 submission_runner.py:408] Time since start: 1474.44s, 	Step: 2665, 	{'train/accuracy': 0.10384765267372131, 'train/loss': 4.981348991394043, 'validation/accuracy': 0.09711999446153641, 'validation/loss': 5.043646812438965, 'validation/num_examples': 50000, 'test/accuracy': 0.07349999994039536, 'test/loss': 5.327393054962158, 'test/num_examples': 10000, 'score': 1295.8631527423859, 'total_duration': 1474.4437997341156, 'accumulated_submission_time': 1295.8631527423859, 'accumulated_eval_time': 178.35270309448242, 'accumulated_logging_time': 0.07439279556274414}
I0206 14:58:41.203186 139946414638848 logging_writer.py:48] [2665] accumulated_eval_time=178.352703, accumulated_logging_time=0.074393, accumulated_submission_time=1295.863153, global_step=2665, preemption_count=0, score=1295.863153, test/accuracy=0.073500, test/loss=5.327393, test/num_examples=10000, total_duration=1474.443800, train/accuracy=0.103848, train/loss=4.981349, validation/accuracy=0.097120, validation/loss=5.043647, validation/num_examples=50000
I0206 14:58:55.595428 139946397853440 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.4265266954898834, loss=5.464756488800049
I0206 14:59:39.680855 139946414638848 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.45156028866767883, loss=6.440812110900879
I0206 15:00:26.439887 139946397853440 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5267947912216187, loss=5.610732555389404
I0206 15:01:13.422792 139946414638848 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5722423195838928, loss=5.3895134925842285
I0206 15:02:00.004509 139946397853440 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5079779624938965, loss=5.4752607345581055
I0206 15:02:46.549855 139946414638848 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5056959986686707, loss=6.3977742195129395
I0206 15:03:33.470588 139946397853440 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.6071693897247314, loss=5.383882999420166
I0206 15:04:20.184037 139946414638848 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.46557217836380005, loss=5.367530822753906
I0206 15:05:06.944035 139946397853440 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5648674368858337, loss=5.501880168914795
I0206 15:05:41.388456 140107197974336 spec.py:321] Evaluating on the training split.
I0206 15:05:52.714963 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 15:06:29.864191 140107197974336 spec.py:349] Evaluating on the test split.
I0206 15:06:31.473502 140107197974336 submission_runner.py:408] Time since start: 1944.73s, 	Step: 3575, 	{'train/accuracy': 0.1374218761920929, 'train/loss': 4.640011787414551, 'validation/accuracy': 0.12824000418186188, 'validation/loss': 4.715636253356934, 'validation/num_examples': 50000, 'test/accuracy': 0.09840000420808792, 'test/loss': 5.043509006500244, 'test/num_examples': 10000, 'score': 1715.986170053482, 'total_duration': 1944.7330491542816, 'accumulated_submission_time': 1715.986170053482, 'accumulated_eval_time': 228.43773818016052, 'accumulated_logging_time': 0.10317254066467285}
I0206 15:06:31.493605 139946414638848 logging_writer.py:48] [3575] accumulated_eval_time=228.437738, accumulated_logging_time=0.103173, accumulated_submission_time=1715.986170, global_step=3575, preemption_count=0, score=1715.986170, test/accuracy=0.098400, test/loss=5.043509, test/num_examples=10000, total_duration=1944.733049, train/accuracy=0.137422, train/loss=4.640012, validation/accuracy=0.128240, validation/loss=4.715636, validation/num_examples=50000
I0206 15:06:41.725064 139946397853440 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5246768593788147, loss=6.133027076721191
I0206 15:07:25.223959 139946414638848 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6156165599822998, loss=5.290012836456299
I0206 15:08:12.336158 139946397853440 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5154924392700195, loss=5.296802043914795
I0206 15:08:59.407566 139946414638848 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6905669569969177, loss=5.311901092529297
I0206 15:09:46.232743 139946397853440 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7612140774726868, loss=5.217220783233643
I0206 15:10:33.187591 139946414638848 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7050865888595581, loss=5.681087493896484
I0206 15:11:19.991920 139946397853440 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5851338505744934, loss=5.142250061035156
I0206 15:12:06.996246 139946414638848 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6635836362838745, loss=4.993506908416748
I0206 15:12:53.891316 139946397853440 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.45342397689819336, loss=5.918148517608643
I0206 15:13:31.709341 140107197974336 spec.py:321] Evaluating on the training split.
I0206 15:13:42.574225 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 15:14:20.069110 140107197974336 spec.py:349] Evaluating on the test split.
I0206 15:14:21.661365 140107197974336 submission_runner.py:408] Time since start: 2414.92s, 	Step: 4482, 	{'train/accuracy': 0.16847655177116394, 'train/loss': 4.419726848602295, 'validation/accuracy': 0.1536400020122528, 'validation/loss': 4.517857551574707, 'validation/num_examples': 50000, 'test/accuracy': 0.11940000206232071, 'test/loss': 4.872969627380371, 'test/num_examples': 10000, 'score': 2136.13942360878, 'total_duration': 2414.920921087265, 'accumulated_submission_time': 2136.13942360878, 'accumulated_eval_time': 278.38977098464966, 'accumulated_logging_time': 0.13427948951721191}
I0206 15:14:21.677979 139946414638848 logging_writer.py:48] [4482] accumulated_eval_time=278.389771, accumulated_logging_time=0.134279, accumulated_submission_time=2136.139424, global_step=4482, preemption_count=0, score=2136.139424, test/accuracy=0.119400, test/loss=4.872970, test/num_examples=10000, total_duration=2414.920921, train/accuracy=0.168477, train/loss=4.419727, validation/accuracy=0.153640, validation/loss=4.517858, validation/num_examples=50000
I0206 15:14:29.153252 139946397853440 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0700033903121948, loss=5.107685089111328
I0206 15:15:12.182517 139946414638848 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.876014769077301, loss=5.041872024536133
I0206 15:15:58.516504 139946397853440 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9754830598831177, loss=5.029411792755127
I0206 15:16:45.304967 139946414638848 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7014684677124023, loss=4.931554794311523
I0206 15:17:31.951972 139946397853440 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.760244607925415, loss=4.951374530792236
I0206 15:18:18.820188 139946414638848 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7757437229156494, loss=6.153801918029785
I0206 15:19:05.765938 139946397853440 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7630394697189331, loss=5.3966875076293945
I0206 15:19:52.219673 139946414638848 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5982065796852112, loss=5.003382205963135
I0206 15:20:38.800839 139946397853440 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7754955887794495, loss=4.918872833251953
I0206 15:21:22.034685 140107197974336 spec.py:321] Evaluating on the training split.
I0206 15:21:33.206465 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 15:22:12.462064 140107197974336 spec.py:349] Evaluating on the test split.
I0206 15:22:14.056285 140107197974336 submission_runner.py:408] Time since start: 2887.32s, 	Step: 5394, 	{'train/accuracy': 0.18613280355930328, 'train/loss': 4.220674514770508, 'validation/accuracy': 0.17083999514579773, 'validation/loss': 4.3279571533203125, 'validation/num_examples': 50000, 'test/accuracy': 0.1340000033378601, 'test/loss': 4.738316059112549, 'test/num_examples': 10000, 'score': 2556.4348685741425, 'total_duration': 2887.3158445358276, 'accumulated_submission_time': 2556.4348685741425, 'accumulated_eval_time': 330.4113621711731, 'accumulated_logging_time': 0.16042447090148926}
I0206 15:22:14.073808 139946414638848 logging_writer.py:48] [5394] accumulated_eval_time=330.411362, accumulated_logging_time=0.160424, accumulated_submission_time=2556.434869, global_step=5394, preemption_count=0, score=2556.434869, test/accuracy=0.134000, test/loss=4.738316, test/num_examples=10000, total_duration=2887.315845, train/accuracy=0.186133, train/loss=4.220675, validation/accuracy=0.170840, validation/loss=4.327957, validation/num_examples=50000
I0206 15:22:16.835860 139946397853440 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8310753107070923, loss=4.89521598815918
I0206 15:22:58.937200 139946414638848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9779934287071228, loss=5.021838188171387
I0206 15:23:45.310477 139946397853440 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7216486930847168, loss=5.755649566650391
I0206 15:24:32.332046 139946414638848 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.666676938533783, loss=5.021973133087158
I0206 15:25:18.811236 139946397853440 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9091355800628662, loss=4.829041957855225
I0206 15:26:05.338955 139946414638848 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7314351797103882, loss=4.790670394897461
I0206 15:26:51.985725 139946397853440 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6350090503692627, loss=5.186561584472656
I0206 15:27:38.488599 139946414638848 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8145372867584229, loss=5.928159713745117
I0206 15:28:25.060444 139946397853440 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7910816669464111, loss=4.962939262390137
I0206 15:29:12.180596 139946414638848 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8267666697502136, loss=4.749824523925781
I0206 15:29:14.244282 140107197974336 spec.py:321] Evaluating on the training split.
I0206 15:29:25.064673 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 15:30:03.503480 140107197974336 spec.py:349] Evaluating on the test split.
I0206 15:30:05.096451 140107197974336 submission_runner.py:408] Time since start: 3358.36s, 	Step: 6306, 	{'train/accuracy': 0.22626952826976776, 'train/loss': 3.890352487564087, 'validation/accuracy': 0.20678000152111053, 'validation/loss': 3.9983174800872803, 'validation/num_examples': 50000, 'test/accuracy': 0.1624000072479248, 'test/loss': 4.428020477294922, 'test/num_examples': 10000, 'score': 2976.544565677643, 'total_duration': 3358.3559985160828, 'accumulated_submission_time': 2976.544565677643, 'accumulated_eval_time': 381.2634997367859, 'accumulated_logging_time': 0.18742036819458008}
I0206 15:30:05.115338 139946397853440 logging_writer.py:48] [6306] accumulated_eval_time=381.263500, accumulated_logging_time=0.187420, accumulated_submission_time=2976.544566, global_step=6306, preemption_count=0, score=2976.544566, test/accuracy=0.162400, test/loss=4.428020, test/num_examples=10000, total_duration=3358.355999, train/accuracy=0.226270, train/loss=3.890352, validation/accuracy=0.206780, validation/loss=3.998317, validation/num_examples=50000
I0206 15:30:44.584666 139946414638848 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7535220384597778, loss=5.9278082847595215
I0206 15:31:30.947804 139946397853440 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.850472092628479, loss=4.642518043518066
I0206 15:32:17.905596 139946414638848 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.9737026691436768, loss=4.768399238586426
I0206 15:33:04.544028 139946397853440 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7508637309074402, loss=6.15352725982666
I0206 15:33:51.164005 139946414638848 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9290139675140381, loss=5.934276103973389
I0206 15:34:37.956946 139946397853440 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9342639446258545, loss=6.141828536987305
I0206 15:35:24.471938 139946414638848 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6737803816795349, loss=6.025516033172607
I0206 15:36:10.891660 139946397853440 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9789724349975586, loss=4.817017555236816
I0206 15:36:57.510616 139946414638848 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.759197473526001, loss=4.7731614112854
I0206 15:37:05.167828 140107197974336 spec.py:321] Evaluating on the training split.
I0206 15:37:16.281214 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 15:37:53.927285 140107197974336 spec.py:349] Evaluating on the test split.
I0206 15:37:55.537780 140107197974336 submission_runner.py:408] Time since start: 3828.80s, 	Step: 7218, 	{'train/accuracy': 0.232421875, 'train/loss': 3.8077361583709717, 'validation/accuracy': 0.21663999557495117, 'validation/loss': 3.93320369720459, 'validation/num_examples': 50000, 'test/accuracy': 0.16580000519752502, 'test/loss': 4.394979000091553, 'test/num_examples': 10000, 'score': 3396.534752845764, 'total_duration': 3828.797343492508, 'accumulated_submission_time': 3396.534752845764, 'accumulated_eval_time': 431.63345980644226, 'accumulated_logging_time': 0.21734189987182617}
I0206 15:37:55.554242 139946397853440 logging_writer.py:48] [7218] accumulated_eval_time=431.633460, accumulated_logging_time=0.217342, accumulated_submission_time=3396.534753, global_step=7218, preemption_count=0, score=3396.534753, test/accuracy=0.165800, test/loss=4.394979, test/num_examples=10000, total_duration=3828.797343, train/accuracy=0.232422, train/loss=3.807736, validation/accuracy=0.216640, validation/loss=3.933204, validation/num_examples=50000
I0206 15:38:29.566096 139946414638848 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.8692688941955566, loss=4.600008964538574
I0206 15:39:15.841650 139946397853440 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8156469464302063, loss=4.743038654327393
I0206 15:40:02.667750 139946414638848 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.9324722290039062, loss=4.938241004943848
I0206 15:40:49.182985 139946397853440 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9841206073760986, loss=4.712835788726807
I0206 15:41:35.665173 139946414638848 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.0029410123825073, loss=4.712301254272461
I0206 15:42:22.263257 139946397853440 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8063426613807678, loss=4.5578083992004395
I0206 15:43:08.884102 139946414638848 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.0682045221328735, loss=4.659787654876709
I0206 15:43:55.541392 139946397853440 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.0448046922683716, loss=4.5888776779174805
I0206 15:44:42.317043 139946414638848 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8523327112197876, loss=4.919752597808838
I0206 15:44:55.987484 140107197974336 spec.py:321] Evaluating on the training split.
I0206 15:45:07.310383 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 15:45:46.645714 140107197974336 spec.py:349] Evaluating on the test split.
I0206 15:45:48.231760 140107197974336 submission_runner.py:408] Time since start: 4301.49s, 	Step: 8131, 	{'train/accuracy': 0.24320311844348907, 'train/loss': 3.820199728012085, 'validation/accuracy': 0.22071999311447144, 'validation/loss': 3.940143585205078, 'validation/num_examples': 50000, 'test/accuracy': 0.17240001261234283, 'test/loss': 4.411588668823242, 'test/num_examples': 10000, 'score': 3816.9069616794586, 'total_duration': 4301.491326808929, 'accumulated_submission_time': 3816.9069616794586, 'accumulated_eval_time': 483.87773609161377, 'accumulated_logging_time': 0.24266529083251953}
I0206 15:45:48.247813 139946397853440 logging_writer.py:48] [8131] accumulated_eval_time=483.877736, accumulated_logging_time=0.242665, accumulated_submission_time=3816.906962, global_step=8131, preemption_count=0, score=3816.906962, test/accuracy=0.172400, test/loss=4.411589, test/num_examples=10000, total_duration=4301.491327, train/accuracy=0.243203, train/loss=3.820200, validation/accuracy=0.220720, validation/loss=3.940144, validation/num_examples=50000
I0206 15:46:16.685825 139946414638848 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6335272789001465, loss=6.0491766929626465
I0206 15:47:02.765902 139946397853440 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7781188488006592, loss=4.426068305969238
I0206 15:47:49.354816 139946414638848 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7261916995048523, loss=5.687607765197754
I0206 15:48:36.262108 139946397853440 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7849367260932922, loss=4.844865322113037
I0206 15:49:22.727511 139946414638848 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8268066644668579, loss=4.787757873535156
I0206 15:50:09.474913 139946397853440 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.735893964767456, loss=6.026578426361084
I0206 15:50:56.636217 139946414638848 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7909347414970398, loss=4.479436874389648
I0206 15:51:43.273529 139946397853440 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7368624806404114, loss=6.164677619934082
I0206 15:52:29.877607 139946414638848 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7128780484199524, loss=5.504217147827148
I0206 15:52:48.321440 140107197974336 spec.py:321] Evaluating on the training split.
I0206 15:52:59.362383 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 15:53:38.642671 140107197974336 spec.py:349] Evaluating on the test split.
I0206 15:53:40.244386 140107197974336 submission_runner.py:408] Time since start: 4773.50s, 	Step: 9041, 	{'train/accuracy': 0.27070310711860657, 'train/loss': 3.5831527709960938, 'validation/accuracy': 0.2477799952030182, 'validation/loss': 3.710452079772949, 'validation/num_examples': 50000, 'test/accuracy': 0.1932000070810318, 'test/loss': 4.174107074737549, 'test/num_examples': 10000, 'score': 4236.489646196365, 'total_duration': 4773.503950834274, 'accumulated_submission_time': 4236.489646196365, 'accumulated_eval_time': 535.8006870746613, 'accumulated_logging_time': 0.69814133644104}
I0206 15:53:40.265182 139946397853440 logging_writer.py:48] [9041] accumulated_eval_time=535.800687, accumulated_logging_time=0.698141, accumulated_submission_time=4236.489646, global_step=9041, preemption_count=0, score=4236.489646, test/accuracy=0.193200, test/loss=4.174107, test/num_examples=10000, total_duration=4773.503951, train/accuracy=0.270703, train/loss=3.583153, validation/accuracy=0.247780, validation/loss=3.710452, validation/num_examples=50000
I0206 15:54:03.846542 139946414638848 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7265112400054932, loss=4.514786243438721
I0206 15:54:49.991355 139946397853440 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8734747767448425, loss=4.481327056884766
I0206 15:55:36.854550 139946414638848 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8897311091423035, loss=4.533207416534424
I0206 15:56:23.564795 139946397853440 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5684348344802856, loss=5.961467266082764
I0206 15:57:10.119840 139946414638848 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9329115152359009, loss=4.362373352050781
I0206 15:57:56.406250 139946397853440 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7424294948577881, loss=4.357006072998047
I0206 15:58:42.991197 139946414638848 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.8025756478309631, loss=5.390800952911377
I0206 15:59:29.683459 139946397853440 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7571940422058105, loss=5.860698699951172
I0206 16:00:16.444530 139946414638848 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8160873651504517, loss=6.356642723083496
I0206 16:00:40.276216 140107197974336 spec.py:321] Evaluating on the training split.
I0206 16:00:51.196622 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 16:01:29.806306 140107197974336 spec.py:349] Evaluating on the test split.
I0206 16:01:31.410522 140107197974336 submission_runner.py:408] Time since start: 5244.67s, 	Step: 9953, 	{'train/accuracy': 0.2805859446525574, 'train/loss': 3.5102574825286865, 'validation/accuracy': 0.25845998525619507, 'validation/loss': 3.65179705619812, 'validation/num_examples': 50000, 'test/accuracy': 0.1997000128030777, 'test/loss': 4.167118549346924, 'test/num_examples': 10000, 'score': 4656.436341047287, 'total_duration': 5244.670069217682, 'accumulated_submission_time': 4656.436341047287, 'accumulated_eval_time': 586.9349710941315, 'accumulated_logging_time': 0.7309134006500244}
I0206 16:01:31.430265 139946397853440 logging_writer.py:48] [9953] accumulated_eval_time=586.934971, accumulated_logging_time=0.730913, accumulated_submission_time=4656.436341, global_step=9953, preemption_count=0, score=4656.436341, test/accuracy=0.199700, test/loss=4.167119, test/num_examples=10000, total_duration=5244.670069, train/accuracy=0.280586, train/loss=3.510257, validation/accuracy=0.258460, validation/loss=3.651797, validation/num_examples=50000
I0206 16:01:50.284324 139946414638848 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.9409434199333191, loss=4.570841312408447
I0206 16:02:35.334713 139946397853440 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.8423678874969482, loss=4.446596145629883
I0206 16:03:21.840420 139946414638848 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7640108466148376, loss=4.614736557006836
I0206 16:04:08.585694 139946397853440 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.9308807849884033, loss=4.567931652069092
I0206 16:04:55.503201 139946414638848 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.050084114074707, loss=4.396537780761719
I0206 16:05:42.550237 139946397853440 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9377920031547546, loss=4.537100315093994
I0206 16:06:29.528098 139946414638848 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.9160236716270447, loss=4.42886209487915
I0206 16:07:16.159940 139946397853440 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6066267490386963, loss=5.238825798034668
I0206 16:08:02.783092 139946414638848 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.622369110584259, loss=5.632260799407959
I0206 16:08:31.831556 140107197974336 spec.py:321] Evaluating on the training split.
I0206 16:08:42.784662 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 16:09:21.280624 140107197974336 spec.py:349] Evaluating on the test split.
I0206 16:09:22.883028 140107197974336 submission_runner.py:408] Time since start: 5716.14s, 	Step: 10864, 	{'train/accuracy': 0.2991601526737213, 'train/loss': 3.379409074783325, 'validation/accuracy': 0.2633799910545349, 'validation/loss': 3.607219934463501, 'validation/num_examples': 50000, 'test/accuracy': 0.20430001616477966, 'test/loss': 4.103435516357422, 'test/num_examples': 10000, 'score': 5076.775891304016, 'total_duration': 5716.142570257187, 'accumulated_submission_time': 5076.775891304016, 'accumulated_eval_time': 637.9864263534546, 'accumulated_logging_time': 0.7608444690704346}
I0206 16:09:22.907051 139946397853440 logging_writer.py:48] [10864] accumulated_eval_time=637.986426, accumulated_logging_time=0.760844, accumulated_submission_time=5076.775891, global_step=10864, preemption_count=0, score=5076.775891, test/accuracy=0.204300, test/loss=4.103436, test/num_examples=10000, total_duration=5716.142570, train/accuracy=0.299160, train/loss=3.379409, validation/accuracy=0.263380, validation/loss=3.607220, validation/num_examples=50000
I0206 16:09:37.480612 139946414638848 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9116830229759216, loss=4.503401756286621
I0206 16:10:21.950615 139946397853440 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.775082528591156, loss=5.344603061676025
I0206 16:11:08.891081 139946414638848 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.8529629111289978, loss=4.360544204711914
I0206 16:11:55.885149 139946397853440 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.8378530740737915, loss=4.337120056152344
I0206 16:12:42.615114 139946414638848 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9048177599906921, loss=4.3005900382995605
I0206 16:13:29.358696 139946397853440 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.9120157957077026, loss=4.907158374786377
I0206 16:14:16.199350 139946414638848 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.948391318321228, loss=6.202919960021973
I0206 16:15:03.106225 139946397853440 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7218685746192932, loss=6.024743556976318
I0206 16:15:49.911895 139946414638848 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8837530016899109, loss=4.465517997741699
I0206 16:16:23.302895 140107197974336 spec.py:321] Evaluating on the training split.
I0206 16:16:34.299308 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 16:17:13.769623 140107197974336 spec.py:349] Evaluating on the test split.
I0206 16:17:15.392669 140107197974336 submission_runner.py:408] Time since start: 6188.65s, 	Step: 11773, 	{'train/accuracy': 0.2922460734844208, 'train/loss': 3.4271626472473145, 'validation/accuracy': 0.2735399901866913, 'validation/loss': 3.5501279830932617, 'validation/num_examples': 50000, 'test/accuracy': 0.20720000565052032, 'test/loss': 4.052846908569336, 'test/num_examples': 10000, 'score': 5497.108896970749, 'total_duration': 6188.652234077454, 'accumulated_submission_time': 5497.108896970749, 'accumulated_eval_time': 690.076201915741, 'accumulated_logging_time': 0.7959158420562744}
I0206 16:17:15.410801 139946397853440 logging_writer.py:48] [11773] accumulated_eval_time=690.076202, accumulated_logging_time=0.795916, accumulated_submission_time=5497.108897, global_step=11773, preemption_count=0, score=5497.108897, test/accuracy=0.207200, test/loss=4.052847, test/num_examples=10000, total_duration=6188.652234, train/accuracy=0.292246, train/loss=3.427163, validation/accuracy=0.273540, validation/loss=3.550128, validation/num_examples=50000
I0206 16:17:26.412236 139946414638848 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6252618432044983, loss=5.056880474090576
I0206 16:18:10.656075 139946397853440 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.1445897817611694, loss=4.594438552856445
I0206 16:18:57.044687 139946414638848 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9866254329681396, loss=4.195828914642334
I0206 16:19:43.946214 139946397853440 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7158108353614807, loss=5.3242692947387695
I0206 16:20:30.324707 139946414638848 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.3324856758117676, loss=4.317965984344482
I0206 16:21:17.103616 139946397853440 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9633732438087463, loss=4.38894510269165
I0206 16:22:03.822558 139946414638848 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8434965014457703, loss=4.403289794921875
I0206 16:22:50.533838 139946397853440 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9315060377120972, loss=4.5178985595703125
I0206 16:23:37.294652 139946414638848 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6729749441146851, loss=5.909806251525879
I0206 16:24:15.538451 140107197974336 spec.py:321] Evaluating on the training split.
I0206 16:24:26.814157 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 16:25:06.887411 140107197974336 spec.py:349] Evaluating on the test split.
I0206 16:25:08.490698 140107197974336 submission_runner.py:408] Time since start: 6661.75s, 	Step: 12683, 	{'train/accuracy': 0.2971484363079071, 'train/loss': 3.4095776081085205, 'validation/accuracy': 0.27709999680519104, 'validation/loss': 3.530275821685791, 'validation/num_examples': 50000, 'test/accuracy': 0.21390001475811005, 'test/loss': 4.058302879333496, 'test/num_examples': 10000, 'score': 5917.175496578217, 'total_duration': 6661.750261306763, 'accumulated_submission_time': 5917.175496578217, 'accumulated_eval_time': 743.02845287323, 'accumulated_logging_time': 0.8236494064331055}
I0206 16:25:08.509917 139946397853440 logging_writer.py:48] [12683] accumulated_eval_time=743.028453, accumulated_logging_time=0.823649, accumulated_submission_time=5917.175497, global_step=12683, preemption_count=0, score=5917.175497, test/accuracy=0.213900, test/loss=4.058303, test/num_examples=10000, total_duration=6661.750261, train/accuracy=0.297148, train/loss=3.409578, validation/accuracy=0.277100, validation/loss=3.530276, validation/num_examples=50000
I0206 16:25:15.588723 139946414638848 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.033221960067749, loss=4.322516918182373
I0206 16:25:58.534955 139946397853440 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9638100862503052, loss=4.197264671325684
I0206 16:26:45.191563 139946414638848 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9281139969825745, loss=4.401471138000488
I0206 16:27:32.360880 139946397853440 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8131576180458069, loss=4.969634056091309
I0206 16:28:19.049043 139946414638848 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.8697676062583923, loss=4.455694675445557
I0206 16:29:05.755608 139946397853440 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.1326287984848022, loss=4.32933235168457
I0206 16:29:52.534774 139946414638848 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.934657633304596, loss=4.196405410766602
I0206 16:30:39.396456 139946397853440 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.1402742862701416, loss=4.26859712600708
I0206 16:31:26.457135 139946414638848 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.8440373539924622, loss=5.094258785247803
I0206 16:32:08.715120 140107197974336 spec.py:321] Evaluating on the training split.
I0206 16:32:20.056680 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 16:32:57.432582 140107197974336 spec.py:349] Evaluating on the test split.
I0206 16:32:59.029589 140107197974336 submission_runner.py:408] Time since start: 7132.29s, 	Step: 13592, 	{'train/accuracy': 0.32298827171325684, 'train/loss': 3.2224056720733643, 'validation/accuracy': 0.28015998005867004, 'validation/loss': 3.4948790073394775, 'validation/num_examples': 50000, 'test/accuracy': 0.21240000426769257, 'test/loss': 4.011412143707275, 'test/num_examples': 10000, 'score': 6337.318806171417, 'total_duration': 7132.289152383804, 'accumulated_submission_time': 6337.318806171417, 'accumulated_eval_time': 793.3429248332977, 'accumulated_logging_time': 0.8524086475372314}
I0206 16:32:59.047201 139946397853440 logging_writer.py:48] [13592] accumulated_eval_time=793.342925, accumulated_logging_time=0.852409, accumulated_submission_time=6337.318806, global_step=13592, preemption_count=0, score=6337.318806, test/accuracy=0.212400, test/loss=4.011412, test/num_examples=10000, total_duration=7132.289152, train/accuracy=0.322988, train/loss=3.222406, validation/accuracy=0.280160, validation/loss=3.494879, validation/num_examples=50000
I0206 16:33:02.587764 139946414638848 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8013431429862976, loss=4.36096715927124
I0206 16:33:45.160304 139946397853440 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.8852933645248413, loss=4.720954895019531
I0206 16:34:31.966534 139946414638848 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7744684219360352, loss=5.831058502197266
I0206 16:35:19.080230 139946397853440 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.8606531620025635, loss=4.45012092590332
I0206 16:36:06.216091 139946414638848 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.8695196509361267, loss=5.407817840576172
I0206 16:36:53.130218 139946397853440 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.9889578819274902, loss=4.781017303466797
I0206 16:37:39.962183 139946414638848 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5833097696304321, loss=5.823249340057373
I0206 16:38:27.101472 139946397853440 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.8240174651145935, loss=5.8591790199279785
I0206 16:39:13.765043 139946414638848 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7284653782844543, loss=6.131598949432373
I0206 16:39:59.148483 140107197974336 spec.py:321] Evaluating on the training split.
I0206 16:40:10.214774 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 16:40:49.054836 140107197974336 spec.py:349] Evaluating on the test split.
I0206 16:40:50.653681 140107197974336 submission_runner.py:408] Time since start: 7603.91s, 	Step: 14498, 	{'train/accuracy': 0.31587889790534973, 'train/loss': 3.273736000061035, 'validation/accuracy': 0.2939800024032593, 'validation/loss': 3.3951432704925537, 'validation/num_examples': 50000, 'test/accuracy': 0.22270001471042633, 'test/loss': 3.9273860454559326, 'test/num_examples': 10000, 'score': 6757.358831167221, 'total_duration': 7603.913234949112, 'accumulated_submission_time': 6757.358831167221, 'accumulated_eval_time': 844.848123550415, 'accumulated_logging_time': 0.8794562816619873}
I0206 16:40:50.673471 139946397853440 logging_writer.py:48] [14498] accumulated_eval_time=844.848124, accumulated_logging_time=0.879456, accumulated_submission_time=6757.358831, global_step=14498, preemption_count=0, score=6757.358831, test/accuracy=0.222700, test/loss=3.927386, test/num_examples=10000, total_duration=7603.913235, train/accuracy=0.315879, train/loss=3.273736, validation/accuracy=0.293980, validation/loss=3.395143, validation/num_examples=50000
I0206 16:40:51.858231 139946414638848 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9180744886398315, loss=4.248352527618408
I0206 16:41:34.255821 139946397853440 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.9689861536026001, loss=4.207706451416016
I0206 16:42:20.869709 139946414638848 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9145780205726624, loss=4.269009113311768
I0206 16:43:07.835597 139946397853440 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.9147824645042419, loss=4.2811150550842285
I0206 16:43:54.569004 139946414638848 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.2010902166366577, loss=4.100192070007324
I0206 16:44:41.859675 139946397853440 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9406135678291321, loss=4.146902084350586
I0206 16:45:28.520089 139946414638848 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.0268723964691162, loss=4.389989852905273
I0206 16:46:15.574565 139946397853440 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.9244420528411865, loss=4.293464183807373
I0206 16:47:02.459303 139946414638848 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.8813629150390625, loss=4.26347541809082
I0206 16:47:49.513992 139946397853440 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.7635151147842407, loss=5.365284442901611
I0206 16:47:51.129163 140107197974336 spec.py:321] Evaluating on the training split.
I0206 16:48:02.530041 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 16:48:41.540609 140107197974336 spec.py:349] Evaluating on the test split.
I0206 16:48:43.140057 140107197974336 submission_runner.py:408] Time since start: 8076.40s, 	Step: 15405, 	{'train/accuracy': 0.3174218535423279, 'train/loss': 3.3037431240081787, 'validation/accuracy': 0.29363998770713806, 'validation/loss': 3.4267396926879883, 'validation/num_examples': 50000, 'test/accuracy': 0.2321000099182129, 'test/loss': 3.9265310764312744, 'test/num_examples': 10000, 'score': 7177.752962350845, 'total_duration': 8076.399621963501, 'accumulated_submission_time': 7177.752962350845, 'accumulated_eval_time': 896.8590533733368, 'accumulated_logging_time': 0.9088699817657471}
I0206 16:48:43.159142 139946414638848 logging_writer.py:48] [15405] accumulated_eval_time=896.859053, accumulated_logging_time=0.908870, accumulated_submission_time=7177.752962, global_step=15405, preemption_count=0, score=7177.752962, test/accuracy=0.232100, test/loss=3.926531, test/num_examples=10000, total_duration=8076.399622, train/accuracy=0.317422, train/loss=3.303743, validation/accuracy=0.293640, validation/loss=3.426740, validation/num_examples=50000
I0206 16:49:23.215668 139946397853440 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9441314339637756, loss=4.3054680824279785
I0206 16:50:09.661276 139946414638848 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.8932047486305237, loss=4.358602046966553
I0206 16:50:56.292339 139946397853440 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.0722219944000244, loss=4.39903450012207
I0206 16:51:42.982074 139946414638848 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.8416938185691833, loss=4.357017993927002
I0206 16:52:29.821066 139946397853440 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.8513190746307373, loss=4.673804759979248
I0206 16:53:16.352385 139946414638848 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9977200031280518, loss=4.155657768249512
I0206 16:54:03.099925 139946397853440 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.9116882681846619, loss=4.141081809997559
I0206 16:54:50.034455 139946414638848 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7718663215637207, loss=4.4529218673706055
I0206 16:55:36.894460 139946397853440 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.8795629143714905, loss=4.766923904418945
I0206 16:55:43.148722 140107197974336 spec.py:321] Evaluating on the training split.
I0206 16:55:54.091511 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 16:56:32.246365 140107197974336 spec.py:349] Evaluating on the test split.
I0206 16:56:33.848969 140107197974336 submission_runner.py:408] Time since start: 8547.11s, 	Step: 16315, 	{'train/accuracy': 0.3279101550579071, 'train/loss': 3.2197184562683105, 'validation/accuracy': 0.2950599789619446, 'validation/loss': 3.416271448135376, 'validation/num_examples': 50000, 'test/accuracy': 0.22640001773834229, 'test/loss': 3.9574127197265625, 'test/num_examples': 10000, 'score': 7597.680613279343, 'total_duration': 8547.108525753021, 'accumulated_submission_time': 7597.680613279343, 'accumulated_eval_time': 947.5592894554138, 'accumulated_logging_time': 0.9369139671325684}
I0206 16:56:33.870802 139946414638848 logging_writer.py:48] [16315] accumulated_eval_time=947.559289, accumulated_logging_time=0.936914, accumulated_submission_time=7597.680613, global_step=16315, preemption_count=0, score=7597.680613, test/accuracy=0.226400, test/loss=3.957413, test/num_examples=10000, total_duration=8547.108526, train/accuracy=0.327910, train/loss=3.219718, validation/accuracy=0.295060, validation/loss=3.416271, validation/num_examples=50000
I0206 16:57:09.505852 139946397853440 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.8958057165145874, loss=4.232235431671143
I0206 16:57:55.821506 139946414638848 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6998691558837891, loss=5.847572326660156
I0206 16:58:42.580581 139946397853440 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.9837788939476013, loss=4.11180305480957
I0206 16:59:29.200520 139946414638848 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.4039582014083862, loss=4.343307018280029
I0206 17:00:15.987564 139946397853440 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.7561889886856079, loss=5.020389556884766
I0206 17:01:02.405882 139946414638848 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.8891103863716125, loss=4.274223327636719
I0206 17:01:49.164225 139946397853440 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7691993713378906, loss=4.733967304229736
I0206 17:02:35.678135 139946414638848 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9530371427536011, loss=4.4843950271606445
I0206 17:03:22.418724 139946397853440 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.029258370399475, loss=4.0651350021362305
I0206 17:03:34.206931 140107197974336 spec.py:321] Evaluating on the training split.
I0206 17:03:46.481326 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 17:04:25.579116 140107197974336 spec.py:349] Evaluating on the test split.
I0206 17:04:27.176592 140107197974336 submission_runner.py:408] Time since start: 9020.44s, 	Step: 17227, 	{'train/accuracy': 0.30931639671325684, 'train/loss': 3.3662712574005127, 'validation/accuracy': 0.28696000576019287, 'validation/loss': 3.501178741455078, 'validation/num_examples': 50000, 'test/accuracy': 0.21810001134872437, 'test/loss': 4.019433975219727, 'test/num_examples': 10000, 'score': 8017.9552347660065, 'total_duration': 9020.436147928238, 'accumulated_submission_time': 8017.9552347660065, 'accumulated_eval_time': 1000.5289397239685, 'accumulated_logging_time': 0.9675893783569336}
I0206 17:04:27.194462 139946414638848 logging_writer.py:48] [17227] accumulated_eval_time=1000.528940, accumulated_logging_time=0.967589, accumulated_submission_time=8017.955235, global_step=17227, preemption_count=0, score=8017.955235, test/accuracy=0.218100, test/loss=4.019434, test/num_examples=10000, total_duration=9020.436148, train/accuracy=0.309316, train/loss=3.366271, validation/accuracy=0.286960, validation/loss=3.501179, validation/num_examples=50000
I0206 17:04:57.134860 139946397853440 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7535631656646729, loss=4.718058109283447
I0206 17:05:43.405381 139946414638848 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.9963294267654419, loss=5.04459285736084
I0206 17:06:30.604773 139946397853440 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.0105183124542236, loss=4.133669853210449
I0206 17:07:17.745687 139946414638848 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8584474921226501, loss=4.177337646484375
I0206 17:08:04.371831 139946397853440 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9262115359306335, loss=4.190658092498779
I0206 17:08:51.182350 139946414638848 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.1177109479904175, loss=4.109793186187744
I0206 17:09:38.076850 139946397853440 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.8749807476997375, loss=4.264878273010254
I0206 17:10:24.963378 139946414638848 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.2079635858535767, loss=4.227710723876953
I0206 17:11:11.608069 139946397853440 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.0342282056808472, loss=4.028067588806152
I0206 17:11:27.185090 140107197974336 spec.py:321] Evaluating on the training split.
I0206 17:11:38.396428 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 17:12:17.016573 140107197974336 spec.py:349] Evaluating on the test split.
I0206 17:12:18.616284 140107197974336 submission_runner.py:408] Time since start: 9491.88s, 	Step: 18135, 	{'train/accuracy': 0.3327734172344208, 'train/loss': 3.162445545196533, 'validation/accuracy': 0.3094799816608429, 'validation/loss': 3.3020880222320557, 'validation/num_examples': 50000, 'test/accuracy': 0.23330001533031464, 'test/loss': 3.8808505535125732, 'test/num_examples': 10000, 'score': 8437.884447813034, 'total_duration': 9491.875847578049, 'accumulated_submission_time': 8437.884447813034, 'accumulated_eval_time': 1051.960121870041, 'accumulated_logging_time': 0.9948971271514893}
I0206 17:12:18.633975 139946414638848 logging_writer.py:48] [18135] accumulated_eval_time=1051.960122, accumulated_logging_time=0.994897, accumulated_submission_time=8437.884448, global_step=18135, preemption_count=0, score=8437.884448, test/accuracy=0.233300, test/loss=3.880851, test/num_examples=10000, total_duration=9491.875848, train/accuracy=0.332773, train/loss=3.162446, validation/accuracy=0.309480, validation/loss=3.302088, validation/num_examples=50000
I0206 17:12:45.072343 139946397853440 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7019395232200623, loss=4.780555725097656
I0206 17:13:31.530145 139946414638848 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.0155324935913086, loss=4.745821475982666
I0206 17:14:18.305887 139946397853440 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.9082047939300537, loss=4.071837425231934
I0206 17:15:05.416213 139946414638848 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.3138792514801025, loss=4.235968112945557
I0206 17:15:51.945266 139946397853440 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.0126655101776123, loss=4.070059776306152
I0206 17:16:38.714233 139946414638848 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.0938338041305542, loss=4.130833625793457
I0206 17:17:25.729897 139946397853440 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.9040427207946777, loss=5.303956508636475
I0206 17:18:12.585483 139946414638848 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9213787317276001, loss=4.352363586425781
I0206 17:18:59.563309 139946397853440 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8431284427642822, loss=4.035586833953857
I0206 17:19:18.981287 140107197974336 spec.py:321] Evaluating on the training split.
I0206 17:19:30.392506 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 17:20:10.731730 140107197974336 spec.py:349] Evaluating on the test split.
I0206 17:20:12.339212 140107197974336 submission_runner.py:408] Time since start: 9965.60s, 	Step: 19043, 	{'train/accuracy': 0.3293164074420929, 'train/loss': 3.2388904094696045, 'validation/accuracy': 0.30535998940467834, 'validation/loss': 3.392557382583618, 'validation/num_examples': 50000, 'test/accuracy': 0.22960001230239868, 'test/loss': 3.9306869506835938, 'test/num_examples': 10000, 'score': 8858.166803836823, 'total_duration': 9965.598746538162, 'accumulated_submission_time': 8858.166803836823, 'accumulated_eval_time': 1105.318029642105, 'accumulated_logging_time': 1.0265140533447266}
I0206 17:20:12.361190 139946414638848 logging_writer.py:48] [19043] accumulated_eval_time=1105.318030, accumulated_logging_time=1.026514, accumulated_submission_time=8858.166804, global_step=19043, preemption_count=0, score=8858.166804, test/accuracy=0.229600, test/loss=3.930687, test/num_examples=10000, total_duration=9965.598747, train/accuracy=0.329316, train/loss=3.238890, validation/accuracy=0.305360, validation/loss=3.392557, validation/num_examples=50000
I0206 17:20:35.184606 139946397853440 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.7555198073387146, loss=5.979587078094482
I0206 17:21:21.182674 139946414638848 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.0419889688491821, loss=4.131752014160156
I0206 17:22:08.187386 139946397853440 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7778225541114807, loss=4.113953590393066
I0206 17:22:55.014592 139946414638848 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9093911051750183, loss=4.121922969818115
I0206 17:23:41.731958 139946397853440 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7733842730522156, loss=6.0587053298950195
I0206 17:24:28.429681 139946414638848 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.0874505043029785, loss=4.577581405639648
I0206 17:25:15.218519 139946397853440 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.1648081541061401, loss=4.297987461090088
I0206 17:26:02.222489 139946414638848 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8013429045677185, loss=4.456336975097656
I0206 17:26:49.163503 139946397853440 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.4022990465164185, loss=4.1420440673828125
I0206 17:27:12.808120 140107197974336 spec.py:321] Evaluating on the training split.
I0206 17:27:23.780436 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 17:28:06.057579 140107197974336 spec.py:349] Evaluating on the test split.
I0206 17:28:07.655663 140107197974336 submission_runner.py:408] Time since start: 10440.92s, 	Step: 19952, 	{'train/accuracy': 0.33759763836860657, 'train/loss': 3.134190082550049, 'validation/accuracy': 0.31709998846054077, 'validation/loss': 3.262382984161377, 'validation/num_examples': 50000, 'test/accuracy': 0.24770000576972961, 'test/loss': 3.8268377780914307, 'test/num_examples': 10000, 'score': 9278.551815271378, 'total_duration': 10440.915224790573, 'accumulated_submission_time': 9278.551815271378, 'accumulated_eval_time': 1160.165560245514, 'accumulated_logging_time': 1.059199333190918}
I0206 17:28:07.677590 139946414638848 logging_writer.py:48] [19952] accumulated_eval_time=1160.165560, accumulated_logging_time=1.059199, accumulated_submission_time=9278.551815, global_step=19952, preemption_count=0, score=9278.551815, test/accuracy=0.247700, test/loss=3.826838, test/num_examples=10000, total_duration=10440.915225, train/accuracy=0.337598, train/loss=3.134190, validation/accuracy=0.317100, validation/loss=3.262383, validation/num_examples=50000
I0206 17:28:26.942549 139946397853440 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.0065008401870728, loss=4.088773727416992
I0206 17:29:12.234107 139946414638848 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.7840756177902222, loss=5.7399582862854
I0206 17:29:58.888757 139946397853440 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.1594364643096924, loss=4.929996490478516
I0206 17:30:45.936634 139946414638848 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7473399639129639, loss=5.379771709442139
I0206 17:31:32.529644 139946397853440 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6687324047088623, loss=6.106551647186279
I0206 17:32:19.137433 139946414638848 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9121514558792114, loss=4.013835430145264
I0206 17:33:05.948803 139946397853440 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6361659169197083, loss=6.054471015930176
I0206 17:33:52.525311 139946414638848 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.1544256210327148, loss=4.176841735839844
I0206 17:34:39.382509 139946397853440 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.909592866897583, loss=4.125742435455322
I0206 17:35:08.092920 140107197974336 spec.py:321] Evaluating on the training split.
I0206 17:35:19.212844 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 17:35:58.734596 140107197974336 spec.py:349] Evaluating on the test split.
I0206 17:36:00.354145 140107197974336 submission_runner.py:408] Time since start: 10913.61s, 	Step: 20863, 	{'train/accuracy': 0.3479296863079071, 'train/loss': 3.0743730068206787, 'validation/accuracy': 0.32259997725486755, 'validation/loss': 3.2060763835906982, 'validation/num_examples': 50000, 'test/accuracy': 0.24640001356601715, 'test/loss': 3.7816295623779297, 'test/num_examples': 10000, 'score': 9698.902920722961, 'total_duration': 10913.613707065582, 'accumulated_submission_time': 9698.902920722961, 'accumulated_eval_time': 1212.4267747402191, 'accumulated_logging_time': 1.0930509567260742}
I0206 17:36:00.374110 139946414638848 logging_writer.py:48] [20863] accumulated_eval_time=1212.426775, accumulated_logging_time=1.093051, accumulated_submission_time=9698.902921, global_step=20863, preemption_count=0, score=9698.902921, test/accuracy=0.246400, test/loss=3.781630, test/num_examples=10000, total_duration=10913.613707, train/accuracy=0.347930, train/loss=3.074373, validation/accuracy=0.322600, validation/loss=3.206076, validation/num_examples=50000
I0206 17:36:15.328351 139946397853440 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7857122421264648, loss=6.085435390472412
I0206 17:37:00.084154 139946414638848 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8350639343261719, loss=5.5034708976745605
I0206 17:37:47.190484 139946397853440 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.0338635444641113, loss=4.346439361572266
I0206 17:38:34.402329 139946414638848 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.061357021331787, loss=4.21028995513916
I0206 17:39:21.614743 139946397853440 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.8678143620491028, loss=4.0338521003723145
I0206 17:40:08.505954 139946414638848 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7702733278274536, loss=5.774847507476807
I0206 17:40:55.345108 139946397853440 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7830776572227478, loss=4.794985294342041
I0206 17:41:42.153653 139946414638848 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.0270187854766846, loss=4.233233451843262
I0206 17:42:29.111799 139946397853440 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8353691101074219, loss=4.094142436981201
I0206 17:43:00.644597 140107197974336 spec.py:321] Evaluating on the training split.
I0206 17:43:11.824913 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 17:43:49.959926 140107197974336 spec.py:349] Evaluating on the test split.
I0206 17:43:51.557245 140107197974336 submission_runner.py:408] Time since start: 11384.82s, 	Step: 21769, 	{'train/accuracy': 0.3544921875, 'train/loss': 3.012260913848877, 'validation/accuracy': 0.3244200050830841, 'validation/loss': 3.1736767292022705, 'validation/num_examples': 50000, 'test/accuracy': 0.24970000982284546, 'test/loss': 3.733030319213867, 'test/num_examples': 10000, 'score': 10119.1109790802, 'total_duration': 11384.816810846329, 'accumulated_submission_time': 10119.1109790802, 'accumulated_eval_time': 1263.3394284248352, 'accumulated_logging_time': 1.1243548393249512}
I0206 17:43:51.576810 139946414638848 logging_writer.py:48] [21769] accumulated_eval_time=1263.339428, accumulated_logging_time=1.124355, accumulated_submission_time=10119.110979, global_step=21769, preemption_count=0, score=10119.110979, test/accuracy=0.249700, test/loss=3.733030, test/num_examples=10000, total_duration=11384.816811, train/accuracy=0.354492, train/loss=3.012261, validation/accuracy=0.324420, validation/loss=3.173677, validation/num_examples=50000
I0206 17:44:04.154192 139946397853440 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8803374767303467, loss=4.264154434204102
I0206 17:44:48.876196 139946414638848 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.8157326579093933, loss=5.240738868713379
I0206 17:45:35.679972 139946397853440 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.2643022537231445, loss=4.127734661102295
I0206 17:46:22.753586 139946414638848 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9976115822792053, loss=4.197022914886475
I0206 17:47:09.659548 139946397853440 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.8407198786735535, loss=3.9880805015563965
I0206 17:47:56.169043 139946414638848 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.03025484085083, loss=6.072650909423828
I0206 17:48:42.960798 139946397853440 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.9440801739692688, loss=3.9697225093841553
I0206 17:49:29.723215 139946414638848 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.857535183429718, loss=3.929722785949707
I0206 17:50:16.808123 139946397853440 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.0527907609939575, loss=4.098807334899902
I0206 17:50:51.931493 140107197974336 spec.py:321] Evaluating on the training split.
I0206 17:51:02.928821 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 17:51:41.618885 140107197974336 spec.py:349] Evaluating on the test split.
I0206 17:51:43.220730 140107197974336 submission_runner.py:408] Time since start: 11856.48s, 	Step: 22677, 	{'train/accuracy': 0.33763670921325684, 'train/loss': 3.103149652481079, 'validation/accuracy': 0.3175399899482727, 'validation/loss': 3.2267537117004395, 'validation/num_examples': 50000, 'test/accuracy': 0.24650001525878906, 'test/loss': 3.79990553855896, 'test/num_examples': 10000, 'score': 10539.403557777405, 'total_duration': 11856.480294466019, 'accumulated_submission_time': 10539.403557777405, 'accumulated_eval_time': 1314.6286573410034, 'accumulated_logging_time': 1.1545770168304443}
I0206 17:51:43.244780 139946414638848 logging_writer.py:48] [22677] accumulated_eval_time=1314.628657, accumulated_logging_time=1.154577, accumulated_submission_time=10539.403558, global_step=22677, preemption_count=0, score=10539.403558, test/accuracy=0.246500, test/loss=3.799906, test/num_examples=10000, total_duration=11856.480294, train/accuracy=0.337637, train/loss=3.103150, validation/accuracy=0.317540, validation/loss=3.226754, validation/num_examples=50000
I0206 17:51:52.677953 139946397853440 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.91494220495224, loss=4.119546890258789
I0206 17:52:36.178373 139946414638848 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.85107421875, loss=6.004708766937256
I0206 17:53:22.969694 139946397853440 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.9134334921836853, loss=4.09181547164917
I0206 17:54:10.040942 139946414638848 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9341074824333191, loss=3.8729217052459717
I0206 17:54:56.962840 139946397853440 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.1599568128585815, loss=4.3768510818481445
I0206 17:55:43.819883 139946414638848 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.8289096355438232, loss=5.999166011810303
I0206 17:56:30.722566 139946397853440 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8928605914115906, loss=4.389382362365723
I0206 17:57:17.543762 139946414638848 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.9482001662254333, loss=4.1605634689331055
I0206 17:58:04.523633 139946397853440 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.9770469665527344, loss=3.953660488128662
I0206 17:58:43.587130 140107197974336 spec.py:321] Evaluating on the training split.
I0206 17:58:54.514375 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 17:59:33.989370 140107197974336 spec.py:349] Evaluating on the test split.
I0206 17:59:35.584542 140107197974336 submission_runner.py:408] Time since start: 12328.84s, 	Step: 23585, 	{'train/accuracy': 0.35798826813697815, 'train/loss': 3.0236752033233643, 'validation/accuracy': 0.3356199860572815, 'validation/loss': 3.154872179031372, 'validation/num_examples': 50000, 'test/accuracy': 0.2532000243663788, 'test/loss': 3.731189250946045, 'test/num_examples': 10000, 'score': 10959.684936523438, 'total_duration': 12328.844103336334, 'accumulated_submission_time': 10959.684936523438, 'accumulated_eval_time': 1366.626068353653, 'accumulated_logging_time': 1.188666820526123}
I0206 17:59:35.607547 139946414638848 logging_writer.py:48] [23585] accumulated_eval_time=1366.626068, accumulated_logging_time=1.188667, accumulated_submission_time=10959.684937, global_step=23585, preemption_count=0, score=10959.684937, test/accuracy=0.253200, test/loss=3.731189, test/num_examples=10000, total_duration=12328.844103, train/accuracy=0.357988, train/loss=3.023675, validation/accuracy=0.335620, validation/loss=3.154872, validation/num_examples=50000
I0206 17:59:41.903317 139946397853440 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.9768486022949219, loss=6.055978775024414
I0206 18:00:25.294398 139946414638848 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.0429420471191406, loss=4.252252578735352
I0206 18:01:12.081790 139946397853440 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.7583586573600769, loss=4.444021701812744
I0206 18:01:58.889601 139946414638848 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8220386505126953, loss=4.303044319152832
I0206 18:02:45.896255 139946397853440 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.8451390266418457, loss=3.9452874660491943
I0206 18:03:33.141560 139946414638848 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5874053239822388, loss=5.911923408508301
I0206 18:04:19.788169 139946397853440 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.0749841928482056, loss=4.011061668395996
I0206 18:05:07.036866 139946414638848 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.0506446361541748, loss=3.974025249481201
I0206 18:05:53.668680 139946397853440 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.184012532234192, loss=4.034504413604736
I0206 18:06:36.074068 140107197974336 spec.py:321] Evaluating on the training split.
I0206 18:06:46.956227 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 18:07:26.901758 140107197974336 spec.py:349] Evaluating on the test split.
I0206 18:07:28.509241 140107197974336 submission_runner.py:408] Time since start: 12801.77s, 	Step: 24492, 	{'train/accuracy': 0.3534179627895355, 'train/loss': 2.9814937114715576, 'validation/accuracy': 0.3287400007247925, 'validation/loss': 3.1595587730407715, 'validation/num_examples': 50000, 'test/accuracy': 0.25710001587867737, 'test/loss': 3.7189767360687256, 'test/num_examples': 10000, 'score': 11380.087949991226, 'total_duration': 12801.768762588501, 'accumulated_submission_time': 11380.087949991226, 'accumulated_eval_time': 1419.0612137317657, 'accumulated_logging_time': 1.2243428230285645}
I0206 18:07:28.532967 139946414638848 logging_writer.py:48] [24492] accumulated_eval_time=1419.061214, accumulated_logging_time=1.224343, accumulated_submission_time=11380.087950, global_step=24492, preemption_count=0, score=11380.087950, test/accuracy=0.257100, test/loss=3.718977, test/num_examples=10000, total_duration=12801.768763, train/accuracy=0.353418, train/loss=2.981494, validation/accuracy=0.328740, validation/loss=3.159559, validation/num_examples=50000
I0206 18:07:32.068675 139946397853440 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8749316930770874, loss=3.94374942779541
I0206 18:08:14.909009 139946414638848 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.3046436309814453, loss=3.9632935523986816
I0206 18:09:01.587553 139946397853440 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.9659532904624939, loss=4.029597759246826
I0206 18:09:48.315755 139946414638848 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.7745474576950073, loss=5.362950325012207
I0206 18:10:35.243418 139946397853440 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.1065806150436401, loss=4.129644870758057
I0206 18:11:22.132577 139946414638848 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.9340439438819885, loss=4.323594093322754
I0206 18:12:09.311310 139946397853440 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8251593708992004, loss=5.764737606048584
I0206 18:12:56.222447 139946414638848 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.3454539775848389, loss=4.064070701599121
I0206 18:13:43.108473 139946397853440 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.9440050721168518, loss=4.4962310791015625
I0206 18:14:28.591683 140107197974336 spec.py:321] Evaluating on the training split.
I0206 18:14:39.511090 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 18:15:18.143496 140107197974336 spec.py:349] Evaluating on the test split.
I0206 18:15:19.737636 140107197974336 submission_runner.py:408] Time since start: 13273.00s, 	Step: 25398, 	{'train/accuracy': 0.3600195348262787, 'train/loss': 3.08575177192688, 'validation/accuracy': 0.3273800015449524, 'validation/loss': 3.256126880645752, 'validation/num_examples': 50000, 'test/accuracy': 0.24420000612735748, 'test/loss': 3.829700231552124, 'test/num_examples': 10000, 'score': 11800.085270881653, 'total_duration': 13272.997171640396, 'accumulated_submission_time': 11800.085270881653, 'accumulated_eval_time': 1470.207144498825, 'accumulated_logging_time': 1.2579419612884521}
I0206 18:15:19.765917 139946414638848 logging_writer.py:48] [25398] accumulated_eval_time=1470.207144, accumulated_logging_time=1.257942, accumulated_submission_time=11800.085271, global_step=25398, preemption_count=0, score=11800.085271, test/accuracy=0.244200, test/loss=3.829700, test/num_examples=10000, total_duration=13272.997172, train/accuracy=0.360020, train/loss=3.085752, validation/accuracy=0.327380, validation/loss=3.256127, validation/num_examples=50000
I0206 18:15:20.949981 139946397853440 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0171267986297607, loss=4.060245037078857
I0206 18:16:03.523300 139946414638848 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9745265245437622, loss=4.490780830383301
I0206 18:16:50.238936 139946397853440 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0822484493255615, loss=4.827449321746826
I0206 18:17:37.202606 139946414638848 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.2205308675765991, loss=4.0522894859313965
I0206 18:18:24.097302 139946397853440 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.8778769373893738, loss=5.066487789154053
I0206 18:19:10.995826 139946414638848 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.736647367477417, loss=4.852763652801514
I0206 18:19:57.587226 139946397853440 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.9471476078033447, loss=3.935249090194702
I0206 18:20:44.485959 139946414638848 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9743688702583313, loss=3.9830501079559326
I0206 18:21:31.311530 139946397853440 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.9155237674713135, loss=5.773071765899658
I0206 18:22:18.604361 139946414638848 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.9252609610557556, loss=4.066217422485352
I0206 18:22:20.108083 140107197974336 spec.py:321] Evaluating on the training split.
I0206 18:22:30.960371 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 18:23:10.815568 140107197974336 spec.py:349] Evaluating on the test split.
I0206 18:23:12.406568 140107197974336 submission_runner.py:408] Time since start: 13745.67s, 	Step: 26305, 	{'train/accuracy': 0.3636718690395355, 'train/loss': 2.97861909866333, 'validation/accuracy': 0.33987998962402344, 'validation/loss': 3.1036124229431152, 'validation/num_examples': 50000, 'test/accuracy': 0.2581000030040741, 'test/loss': 3.705676317214966, 'test/num_examples': 10000, 'score': 12220.36465883255, 'total_duration': 13745.666129112244, 'accumulated_submission_time': 12220.36465883255, 'accumulated_eval_time': 1522.5056171417236, 'accumulated_logging_time': 1.2966363430023193}
I0206 18:23:12.426980 139946397853440 logging_writer.py:48] [26305] accumulated_eval_time=1522.505617, accumulated_logging_time=1.296636, accumulated_submission_time=12220.364659, global_step=26305, preemption_count=0, score=12220.364659, test/accuracy=0.258100, test/loss=3.705676, test/num_examples=10000, total_duration=13745.666129, train/accuracy=0.363672, train/loss=2.978619, validation/accuracy=0.339880, validation/loss=3.103612, validation/num_examples=50000
I0206 18:23:52.782586 139946414638848 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8380951285362244, loss=4.033481597900391
I0206 18:24:39.898457 139946397853440 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9823744297027588, loss=4.0297017097473145
I0206 18:25:26.912507 139946414638848 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.8408607244491577, loss=4.332581520080566
I0206 18:26:13.961898 139946397853440 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.0955615043640137, loss=3.824326276779175
I0206 18:27:01.002213 139946414638848 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.9659243226051331, loss=4.124419212341309
I0206 18:27:47.994597 139946397853440 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.8068517446517944, loss=4.487802982330322
I0206 18:28:35.018453 139946414638848 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.9448023438453674, loss=3.9661717414855957
I0206 18:29:21.978738 139946397853440 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.9997662305831909, loss=3.8982865810394287
I0206 18:30:09.242157 139946414638848 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.9439563751220703, loss=5.267699241638184
I0206 18:30:12.729617 140107197974336 spec.py:321] Evaluating on the training split.
I0206 18:30:23.748028 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 18:31:02.219850 140107197974336 spec.py:349] Evaluating on the test split.
I0206 18:31:03.813308 140107197974336 submission_runner.py:408] Time since start: 14217.07s, 	Step: 27209, 	{'train/accuracy': 0.37458983063697815, 'train/loss': 2.8777029514312744, 'validation/accuracy': 0.3472599983215332, 'validation/loss': 3.050992727279663, 'validation/num_examples': 50000, 'test/accuracy': 0.2712000012397766, 'test/loss': 3.6168735027313232, 'test/num_examples': 10000, 'score': 12640.606446743011, 'total_duration': 14217.07286643982, 'accumulated_submission_time': 12640.606446743011, 'accumulated_eval_time': 1573.5893032550812, 'accumulated_logging_time': 1.327235221862793}
I0206 18:31:03.832315 139946397853440 logging_writer.py:48] [27209] accumulated_eval_time=1573.589303, accumulated_logging_time=1.327235, accumulated_submission_time=12640.606447, global_step=27209, preemption_count=0, score=12640.606447, test/accuracy=0.271200, test/loss=3.616874, test/num_examples=10000, total_duration=14217.072866, train/accuracy=0.374590, train/loss=2.877703, validation/accuracy=0.347260, validation/loss=3.050993, validation/num_examples=50000
I0206 18:31:42.427531 139946414638848 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.9523766040802002, loss=4.22709321975708
I0206 18:32:29.219058 139946397853440 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.15126371383667, loss=4.031197547912598
I0206 18:33:16.446463 139946414638848 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0032635927200317, loss=4.475654602050781
I0206 18:34:03.824457 139946397853440 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.9462942481040955, loss=4.336034774780273
I0206 18:34:51.471384 139946414638848 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8009623885154724, loss=5.157294273376465
I0206 18:35:38.394633 139946397853440 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.9703594446182251, loss=4.145493984222412
I0206 18:36:25.649459 139946414638848 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.101357102394104, loss=4.143682479858398
I0206 18:37:12.549870 139946397853440 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.0593712329864502, loss=3.800562620162964
I0206 18:38:00.012632 139946414638848 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.0996553897857666, loss=4.0305399894714355
I0206 18:38:03.989254 140107197974336 spec.py:321] Evaluating on the training split.
I0206 18:38:15.247909 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 18:38:53.891413 140107197974336 spec.py:349] Evaluating on the test split.
I0206 18:38:55.487625 140107197974336 submission_runner.py:408] Time since start: 14688.75s, 	Step: 28110, 	{'train/accuracy': 0.4131249785423279, 'train/loss': 2.7041707038879395, 'validation/accuracy': 0.35009998083114624, 'validation/loss': 3.0397725105285645, 'validation/num_examples': 50000, 'test/accuracy': 0.27410000562667847, 'test/loss': 3.6207833290100098, 'test/num_examples': 10000, 'score': 13060.701996088028, 'total_duration': 14688.747186660767, 'accumulated_submission_time': 13060.701996088028, 'accumulated_eval_time': 1625.0876586437225, 'accumulated_logging_time': 1.3568122386932373}
I0206 18:38:55.516163 139946397853440 logging_writer.py:48] [28110] accumulated_eval_time=1625.087659, accumulated_logging_time=1.356812, accumulated_submission_time=13060.701996, global_step=28110, preemption_count=0, score=13060.701996, test/accuracy=0.274100, test/loss=3.620783, test/num_examples=10000, total_duration=14688.747187, train/accuracy=0.413125, train/loss=2.704171, validation/accuracy=0.350100, validation/loss=3.039773, validation/num_examples=50000
I0206 18:39:33.653455 139946414638848 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.8767696022987366, loss=3.8368067741394043
I0206 18:40:20.432093 139946397853440 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.9839492440223694, loss=3.884159564971924
I0206 18:41:07.605424 139946414638848 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.7722253799438477, loss=5.78307580947876
I0206 18:41:54.857319 139946397853440 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.282866358757019, loss=4.02217960357666
I0206 18:42:41.821174 139946414638848 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.7768322825431824, loss=5.751974105834961
I0206 18:43:28.780866 139946397853440 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.6881601810455322, loss=6.030462265014648
I0206 18:44:15.965238 139946414638848 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7922455668449402, loss=5.210574150085449
I0206 18:45:02.870512 139946397853440 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.9484308362007141, loss=3.908370018005371
I0206 18:45:49.839340 139946414638848 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.9569326043128967, loss=4.364666938781738
I0206 18:45:55.640451 140107197974336 spec.py:321] Evaluating on the training split.
I0206 18:46:06.593974 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 18:46:46.446739 140107197974336 spec.py:349] Evaluating on the test split.
I0206 18:46:48.039468 140107197974336 submission_runner.py:408] Time since start: 15161.30s, 	Step: 29014, 	{'train/accuracy': 0.35374999046325684, 'train/loss': 3.0552871227264404, 'validation/accuracy': 0.3319399952888489, 'validation/loss': 3.1854774951934814, 'validation/num_examples': 50000, 'test/accuracy': 0.24890001118183136, 'test/loss': 3.7286272048950195, 'test/num_examples': 10000, 'score': 13480.76611328125, 'total_duration': 15161.299011945724, 'accumulated_submission_time': 13480.76611328125, 'accumulated_eval_time': 1677.4866523742676, 'accumulated_logging_time': 1.3948171138763428}
I0206 18:46:48.065717 139946397853440 logging_writer.py:48] [29014] accumulated_eval_time=1677.486652, accumulated_logging_time=1.394817, accumulated_submission_time=13480.766113, global_step=29014, preemption_count=0, score=13480.766113, test/accuracy=0.248900, test/loss=3.728627, test/num_examples=10000, total_duration=15161.299012, train/accuracy=0.353750, train/loss=3.055287, validation/accuracy=0.331940, validation/loss=3.185477, validation/num_examples=50000
I0206 18:47:24.518389 139946414638848 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.0634242296218872, loss=4.666822910308838
I0206 18:48:11.283682 139946397853440 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6889064908027649, loss=5.701814651489258
I0206 18:48:58.429298 139946414638848 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.3305318355560303, loss=4.152383327484131
I0206 18:49:45.522292 139946397853440 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.9766958355903625, loss=4.14395809173584
I0206 18:50:32.719212 139946414638848 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9538102149963379, loss=5.277126789093018
I0206 18:51:19.741595 139946397853440 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.9539486169815063, loss=3.906339406967163
I0206 18:52:07.014101 139946414638848 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.059267520904541, loss=4.039490699768066
I0206 18:52:54.023417 139946397853440 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.963819146156311, loss=4.030200481414795
I0206 18:53:41.165014 139946414638848 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.8031855225563049, loss=5.2040863037109375
I0206 18:53:48.367189 140107197974336 spec.py:321] Evaluating on the training split.
I0206 18:53:59.231857 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 18:54:40.243489 140107197974336 spec.py:349] Evaluating on the test split.
I0206 18:54:41.841988 140107197974336 submission_runner.py:408] Time since start: 15635.10s, 	Step: 29917, 	{'train/accuracy': 0.3595312535762787, 'train/loss': 3.01259183883667, 'validation/accuracy': 0.3307799994945526, 'validation/loss': 3.17191219329834, 'validation/num_examples': 50000, 'test/accuracy': 0.256600022315979, 'test/loss': 3.744589328765869, 'test/num_examples': 10000, 'score': 13901.006760120392, 'total_duration': 15635.10154223442, 'accumulated_submission_time': 13901.006760120392, 'accumulated_eval_time': 1730.9614639282227, 'accumulated_logging_time': 1.4306964874267578}
I0206 18:54:41.862884 139946397853440 logging_writer.py:48] [29917] accumulated_eval_time=1730.961464, accumulated_logging_time=1.430696, accumulated_submission_time=13901.006760, global_step=29917, preemption_count=0, score=13901.006760, test/accuracy=0.256600, test/loss=3.744589, test/num_examples=10000, total_duration=15635.101542, train/accuracy=0.359531, train/loss=3.012592, validation/accuracy=0.330780, validation/loss=3.171912, validation/num_examples=50000
I0206 18:55:16.902418 139946414638848 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9905756115913391, loss=4.4109907150268555
I0206 18:56:03.830693 139946397853440 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.9382837414741516, loss=4.001231670379639
I0206 18:56:51.033184 139946414638848 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.039909839630127, loss=4.574584007263184
I0206 18:57:37.888621 139946397853440 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.7741954922676086, loss=5.909687042236328
I0206 18:58:25.089629 139946414638848 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.210740089416504, loss=3.9426143169403076
I0206 18:59:12.225657 139946397853440 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.8345718383789062, loss=5.971400260925293
I0206 18:59:59.400397 139946414638848 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.1009167432785034, loss=3.9528114795684814
I0206 19:00:46.508373 139946397853440 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.8628063797950745, loss=5.308150768280029
I0206 19:01:33.466253 139946414638848 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.215505599975586, loss=4.060427188873291
I0206 19:01:42.102648 140107197974336 spec.py:321] Evaluating on the training split.
I0206 19:01:53.058432 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 19:02:34.455918 140107197974336 spec.py:349] Evaluating on the test split.
I0206 19:02:36.056627 140107197974336 submission_runner.py:408] Time since start: 16109.32s, 	Step: 30820, 	{'train/accuracy': 0.38343748450279236, 'train/loss': 2.864189624786377, 'validation/accuracy': 0.33743998408317566, 'validation/loss': 3.1222946643829346, 'validation/num_examples': 50000, 'test/accuracy': 0.25920000672340393, 'test/loss': 3.72937273979187, 'test/num_examples': 10000, 'score': 14321.184031248093, 'total_duration': 16109.316176652908, 'accumulated_submission_time': 14321.184031248093, 'accumulated_eval_time': 1784.9154148101807, 'accumulated_logging_time': 1.4623537063598633}
I0206 19:02:36.083258 139946397853440 logging_writer.py:48] [30820] accumulated_eval_time=1784.915415, accumulated_logging_time=1.462354, accumulated_submission_time=14321.184031, global_step=30820, preemption_count=0, score=14321.184031, test/accuracy=0.259200, test/loss=3.729373, test/num_examples=10000, total_duration=16109.316177, train/accuracy=0.383437, train/loss=2.864190, validation/accuracy=0.337440, validation/loss=3.122295, validation/num_examples=50000
I0206 19:03:09.451545 139946414638848 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.025855541229248, loss=4.550649642944336
I0206 19:03:55.768679 139946397853440 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6607908606529236, loss=5.687608242034912
I0206 19:04:42.875907 139946414638848 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.9500225782394409, loss=4.290513515472412
I0206 19:05:29.761120 139946397853440 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9848862290382385, loss=3.721623182296753
I0206 19:06:16.765839 139946414638848 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.944808840751648, loss=6.088669300079346
I0206 19:07:03.692581 139946397853440 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.8009193539619446, loss=4.255659103393555
I0206 19:07:50.516957 139946414638848 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.9203980565071106, loss=4.128057479858398
I0206 19:08:37.212444 139946397853440 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.0092765092849731, loss=3.9313511848449707
I0206 19:09:23.832792 139946414638848 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.9882780909538269, loss=5.288204669952393
I0206 19:09:36.202169 140107197974336 spec.py:321] Evaluating on the training split.
I0206 19:09:47.181484 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 19:10:29.844744 140107197974336 spec.py:349] Evaluating on the test split.
I0206 19:10:31.448722 140107197974336 submission_runner.py:408] Time since start: 16584.71s, 	Step: 31728, 	{'train/accuracy': 0.3595312535762787, 'train/loss': 3.016347885131836, 'validation/accuracy': 0.3363399803638458, 'validation/loss': 3.149092197418213, 'validation/num_examples': 50000, 'test/accuracy': 0.25920000672340393, 'test/loss': 3.729475259780884, 'test/num_examples': 10000, 'score': 14741.241425275803, 'total_duration': 16584.70828318596, 'accumulated_submission_time': 14741.241425275803, 'accumulated_eval_time': 1840.1619803905487, 'accumulated_logging_time': 1.498626470565796}
I0206 19:10:31.469353 139946397853440 logging_writer.py:48] [31728] accumulated_eval_time=1840.161980, accumulated_logging_time=1.498626, accumulated_submission_time=14741.241425, global_step=31728, preemption_count=0, score=14741.241425, test/accuracy=0.259200, test/loss=3.729475, test/num_examples=10000, total_duration=16584.708283, train/accuracy=0.359531, train/loss=3.016348, validation/accuracy=0.336340, validation/loss=3.149092, validation/num_examples=50000
I0206 19:11:01.110446 139946414638848 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.9960322380065918, loss=4.259068489074707
I0206 19:11:47.717545 139946397853440 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.8418795466423035, loss=4.299113750457764
I0206 19:12:35.094522 139946414638848 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.0296223163604736, loss=3.934751272201538
I0206 19:13:22.074076 139946397853440 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.8450685739517212, loss=4.671248912811279
I0206 19:14:09.149031 139946414638848 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.0246508121490479, loss=3.829047441482544
I0206 19:14:56.278445 139946397853440 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.9394472241401672, loss=3.79184889793396
I0206 19:15:43.535857 139946414638848 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.2011749744415283, loss=4.655233860015869
I0206 19:16:30.346336 139946397853440 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7762193083763123, loss=3.800065040588379
I0206 19:17:17.528892 139946414638848 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0300387144088745, loss=4.386682033538818
I0206 19:17:31.773472 140107197974336 spec.py:321] Evaluating on the training split.
I0206 19:17:42.974198 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 19:18:21.511566 140107197974336 spec.py:349] Evaluating on the test split.
I0206 19:18:23.104335 140107197974336 submission_runner.py:408] Time since start: 17056.36s, 	Step: 32632, 	{'train/accuracy': 0.3854101598262787, 'train/loss': 2.822803497314453, 'validation/accuracy': 0.35711997747421265, 'validation/loss': 2.989459991455078, 'validation/num_examples': 50000, 'test/accuracy': 0.2743000090122223, 'test/loss': 3.608689308166504, 'test/num_examples': 10000, 'score': 15161.485805511475, 'total_duration': 17056.36390018463, 'accumulated_submission_time': 15161.485805511475, 'accumulated_eval_time': 1891.4928452968597, 'accumulated_logging_time': 1.528019905090332}
I0206 19:18:23.126682 139946397853440 logging_writer.py:48] [32632] accumulated_eval_time=1891.492845, accumulated_logging_time=1.528020, accumulated_submission_time=15161.485806, global_step=32632, preemption_count=0, score=15161.485806, test/accuracy=0.274300, test/loss=3.608689, test/num_examples=10000, total_duration=17056.363900, train/accuracy=0.385410, train/loss=2.822803, validation/accuracy=0.357120, validation/loss=2.989460, validation/num_examples=50000
I0206 19:18:50.818269 139946414638848 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9468141794204712, loss=4.851795673370361
I0206 19:19:37.399265 139946397853440 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.6758428812026978, loss=5.78427267074585
I0206 19:20:24.169589 139946414638848 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.711742639541626, loss=5.448973655700684
I0206 19:21:11.084462 139946397853440 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.0149589776992798, loss=4.007339954376221
I0206 19:21:57.888179 139946414638848 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.878923773765564, loss=4.061076641082764
I0206 19:22:44.572142 139946397853440 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.0362364053726196, loss=4.375226020812988
I0206 19:23:31.505436 139946414638848 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.8878955841064453, loss=6.071356296539307
I0206 19:24:18.456220 139946397853440 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.9079207181930542, loss=5.512538433074951
I0206 19:25:05.640893 139946414638848 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.9136543869972229, loss=3.872467041015625
I0206 19:25:23.594568 140107197974336 spec.py:321] Evaluating on the training split.
I0206 19:25:34.744878 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 19:26:13.982738 140107197974336 spec.py:349] Evaluating on the test split.
I0206 19:26:15.585452 140107197974336 submission_runner.py:408] Time since start: 17528.84s, 	Step: 33538, 	{'train/accuracy': 0.3950781226158142, 'train/loss': 2.7515311241149902, 'validation/accuracy': 0.36017999053001404, 'validation/loss': 2.9707324504852295, 'validation/num_examples': 50000, 'test/accuracy': 0.27970001101493835, 'test/loss': 3.5605785846710205, 'test/num_examples': 10000, 'score': 15581.891350269318, 'total_duration': 17528.8449883461, 'accumulated_submission_time': 15581.891350269318, 'accumulated_eval_time': 1943.4836995601654, 'accumulated_logging_time': 1.5617244243621826}
I0206 19:26:15.611659 139946397853440 logging_writer.py:48] [33538] accumulated_eval_time=1943.483700, accumulated_logging_time=1.561724, accumulated_submission_time=15581.891350, global_step=33538, preemption_count=0, score=15581.891350, test/accuracy=0.279700, test/loss=3.560579, test/num_examples=10000, total_duration=17528.844988, train/accuracy=0.395078, train/loss=2.751531, validation/accuracy=0.360180, validation/loss=2.970732, validation/num_examples=50000
I0206 19:26:40.517612 139946414638848 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.7844029068946838, loss=5.80111837387085
I0206 19:27:26.953841 139946397853440 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.1359776258468628, loss=4.055661201477051
I0206 19:28:13.864486 139946414638848 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.8685480356216431, loss=4.650586128234863
I0206 19:29:00.487937 139946397853440 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.9164945483207703, loss=5.32737922668457
I0206 19:29:47.346192 139946414638848 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7420459985733032, loss=5.377742767333984
I0206 19:30:33.834991 139946397853440 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.925091028213501, loss=3.8922367095947266
I0206 19:31:20.432902 139946414638848 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.9948844909667969, loss=4.241026401519775
I0206 19:32:07.408491 139946397853440 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.0554298162460327, loss=3.9577529430389404
I0206 19:32:53.986182 139946414638848 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.0850238800048828, loss=3.960994243621826
I0206 19:33:15.769689 140107197974336 spec.py:321] Evaluating on the training split.
I0206 19:33:26.747528 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 19:34:05.658996 140107197974336 spec.py:349] Evaluating on the test split.
I0206 19:34:07.257367 140107197974336 submission_runner.py:408] Time since start: 18000.52s, 	Step: 34448, 	{'train/accuracy': 0.3739648461341858, 'train/loss': 2.8991634845733643, 'validation/accuracy': 0.3464599847793579, 'validation/loss': 3.0483076572418213, 'validation/num_examples': 50000, 'test/accuracy': 0.26820001006126404, 'test/loss': 3.622101306915283, 'test/num_examples': 10000, 'score': 16001.988507032394, 'total_duration': 18000.516926765442, 'accumulated_submission_time': 16001.988507032394, 'accumulated_eval_time': 1994.9713623523712, 'accumulated_logging_time': 1.596980094909668}
I0206 19:34:07.283279 139946397853440 logging_writer.py:48] [34448] accumulated_eval_time=1994.971362, accumulated_logging_time=1.596980, accumulated_submission_time=16001.988507, global_step=34448, preemption_count=0, score=16001.988507, test/accuracy=0.268200, test/loss=3.622101, test/num_examples=10000, total_duration=18000.516927, train/accuracy=0.373965, train/loss=2.899163, validation/accuracy=0.346460, validation/loss=3.048308, validation/num_examples=50000
I0206 19:34:28.127059 139946414638848 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.0999139547348022, loss=3.89310359954834
I0206 19:35:14.474475 139946397853440 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.8860777616500854, loss=5.237255096435547
I0206 19:36:01.402930 139946414638848 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7313849925994873, loss=6.031336784362793
I0206 19:36:48.682729 139946397853440 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.9839460253715515, loss=3.9952480792999268
I0206 19:37:35.668044 139946414638848 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6864136457443237, loss=4.621043682098389
I0206 19:38:22.604109 139946397853440 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7938877940177917, loss=5.205336570739746
I0206 19:39:09.626619 139946414638848 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.9127383232116699, loss=3.863873243331909
I0206 19:39:56.509200 139946397853440 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.035706877708435, loss=3.976534843444824
I0206 19:40:43.549042 139946414638848 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.8566568493843079, loss=4.237508296966553
I0206 19:41:07.370929 140107197974336 spec.py:321] Evaluating on the training split.
I0206 19:41:18.315812 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 19:41:56.540560 140107197974336 spec.py:349] Evaluating on the test split.
I0206 19:41:58.152240 140107197974336 submission_runner.py:408] Time since start: 18471.41s, 	Step: 35352, 	{'train/accuracy': 0.3836914002895355, 'train/loss': 2.843531370162964, 'validation/accuracy': 0.3626999855041504, 'validation/loss': 2.977720260620117, 'validation/num_examples': 50000, 'test/accuracy': 0.28210002183914185, 'test/loss': 3.5719974040985107, 'test/num_examples': 10000, 'score': 16422.013520240784, 'total_duration': 18471.41180539131, 'accumulated_submission_time': 16422.013520240784, 'accumulated_eval_time': 2045.7526659965515, 'accumulated_logging_time': 1.6344339847564697}
I0206 19:41:58.174089 139946397853440 logging_writer.py:48] [35352] accumulated_eval_time=2045.752666, accumulated_logging_time=1.634434, accumulated_submission_time=16422.013520, global_step=35352, preemption_count=0, score=16422.013520, test/accuracy=0.282100, test/loss=3.571997, test/num_examples=10000, total_duration=18471.411805, train/accuracy=0.383691, train/loss=2.843531, validation/accuracy=0.362700, validation/loss=2.977720, validation/num_examples=50000
I0206 19:42:17.447930 139946414638848 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.992850661277771, loss=4.231997966766357
I0206 19:43:03.458662 139946397853440 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.0157251358032227, loss=4.792582035064697
I0206 19:43:50.253718 139946414638848 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7500510215759277, loss=5.156627178192139
I0206 19:44:37.361053 139946397853440 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.0204839706420898, loss=3.747826337814331
I0206 19:45:23.994426 139946414638848 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.0605037212371826, loss=3.9999608993530273
I0206 19:46:10.926455 139946397853440 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.763827383518219, loss=4.839756011962891
I0206 19:46:57.565393 139946414638848 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.1814337968826294, loss=4.01235294342041
I0206 19:47:44.873591 139946397853440 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.8837268948554993, loss=6.003165245056152
I0206 19:48:32.043259 139946414638848 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.0100247859954834, loss=4.0537872314453125
I0206 19:48:58.363941 140107197974336 spec.py:321] Evaluating on the training split.
I0206 19:49:09.419201 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 19:49:48.928318 140107197974336 spec.py:349] Evaluating on the test split.
I0206 19:49:50.524664 140107197974336 submission_runner.py:408] Time since start: 18943.78s, 	Step: 36258, 	{'train/accuracy': 0.38597655296325684, 'train/loss': 2.8243813514709473, 'validation/accuracy': 0.35651999711990356, 'validation/loss': 3.010390520095825, 'validation/num_examples': 50000, 'test/accuracy': 0.2752000093460083, 'test/loss': 3.615450620651245, 'test/num_examples': 10000, 'score': 16842.142607688904, 'total_duration': 18943.784227132797, 'accumulated_submission_time': 16842.142607688904, 'accumulated_eval_time': 2097.9133784770966, 'accumulated_logging_time': 1.6660008430480957}
I0206 19:49:50.546299 139946397853440 logging_writer.py:48] [36258] accumulated_eval_time=2097.913378, accumulated_logging_time=1.666001, accumulated_submission_time=16842.142608, global_step=36258, preemption_count=0, score=16842.142608, test/accuracy=0.275200, test/loss=3.615451, test/num_examples=10000, total_duration=18943.784227, train/accuracy=0.385977, train/loss=2.824381, validation/accuracy=0.356520, validation/loss=3.010391, validation/num_examples=50000
I0206 19:50:07.450179 139946414638848 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.9538863897323608, loss=3.688910722732544
I0206 19:50:52.826167 139946397853440 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.1207417249679565, loss=3.941673517227173
I0206 19:51:39.577717 139946414638848 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8696394562721252, loss=4.606139659881592
I0206 19:52:26.579373 139946397853440 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.0014710426330566, loss=3.8999171257019043
I0206 19:53:13.564236 139946414638848 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.8900927305221558, loss=4.13112735748291
I0206 19:54:00.338288 139946397853440 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.0494040250778198, loss=3.967472553253174
I0206 19:54:47.616261 139946414638848 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.8565713763237, loss=3.8009328842163086
I0206 19:55:34.553549 139946397853440 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.9265150427818298, loss=5.683596134185791
I0206 19:56:21.298448 139946414638848 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.8277328014373779, loss=5.44779109954834
I0206 19:56:50.527948 140107197974336 spec.py:321] Evaluating on the training split.
I0206 19:57:01.323051 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 19:57:39.663241 140107197974336 spec.py:349] Evaluating on the test split.
I0206 19:57:41.266143 140107197974336 submission_runner.py:408] Time since start: 19414.53s, 	Step: 37164, 	{'train/accuracy': 0.3831445276737213, 'train/loss': 2.815798282623291, 'validation/accuracy': 0.35979998111724854, 'validation/loss': 2.9579362869262695, 'validation/num_examples': 50000, 'test/accuracy': 0.2762000262737274, 'test/loss': 3.5880584716796875, 'test/num_examples': 10000, 'score': 17262.063611745834, 'total_duration': 19414.525707244873, 'accumulated_submission_time': 17262.063611745834, 'accumulated_eval_time': 2148.6515715122223, 'accumulated_logging_time': 1.697392463684082}
I0206 19:57:41.291597 139946397853440 logging_writer.py:48] [37164] accumulated_eval_time=2148.651572, accumulated_logging_time=1.697392, accumulated_submission_time=17262.063612, global_step=37164, preemption_count=0, score=17262.063612, test/accuracy=0.276200, test/loss=3.588058, test/num_examples=10000, total_duration=19414.525707, train/accuracy=0.383145, train/loss=2.815798, validation/accuracy=0.359800, validation/loss=2.957936, validation/num_examples=50000
I0206 19:57:55.842370 139946414638848 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.0562702417373657, loss=4.232691287994385
I0206 19:58:40.869688 139946397853440 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.0587091445922852, loss=3.785740613937378
I0206 19:59:27.829302 139946414638848 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.13021719455719, loss=3.83953595161438
I0206 20:00:14.858916 139946397853440 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.9833232760429382, loss=4.4524970054626465
I0206 20:01:02.014538 139946414638848 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.0221889019012451, loss=3.835108518600464
I0206 20:01:48.847351 139946397853440 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.9655500054359436, loss=3.88407039642334
I0206 20:02:35.717093 139946414638848 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.8258971571922302, loss=5.692392349243164
I0206 20:03:22.579353 139946397853440 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.0293986797332764, loss=3.788285732269287
I0206 20:04:09.576084 139946414638848 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0060278177261353, loss=3.937612533569336
I0206 20:04:41.636719 140107197974336 spec.py:321] Evaluating on the training split.
I0206 20:04:52.538551 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 20:05:33.504028 140107197974336 spec.py:349] Evaluating on the test split.
I0206 20:05:35.104502 140107197974336 submission_runner.py:408] Time since start: 19888.36s, 	Step: 38070, 	{'train/accuracy': 0.37132811546325684, 'train/loss': 2.9289300441741943, 'validation/accuracy': 0.3464599847793579, 'validation/loss': 3.0839309692382812, 'validation/num_examples': 50000, 'test/accuracy': 0.26510000228881836, 'test/loss': 3.700192451477051, 'test/num_examples': 10000, 'score': 17682.347589969635, 'total_duration': 19888.36406493187, 'accumulated_submission_time': 17682.347589969635, 'accumulated_eval_time': 2202.1193537712097, 'accumulated_logging_time': 1.732421636581421}
I0206 20:05:35.133476 139946397853440 logging_writer.py:48] [38070] accumulated_eval_time=2202.119354, accumulated_logging_time=1.732422, accumulated_submission_time=17682.347590, global_step=38070, preemption_count=0, score=17682.347590, test/accuracy=0.265100, test/loss=3.700192, test/num_examples=10000, total_duration=19888.364065, train/accuracy=0.371328, train/loss=2.928930, validation/accuracy=0.346460, validation/loss=3.083931, validation/num_examples=50000
I0206 20:05:47.344042 139946414638848 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.2262262105941772, loss=3.826174259185791
I0206 20:06:31.603896 139946397853440 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.9208452105522156, loss=3.7175304889678955
I0206 20:07:18.281442 139946414638848 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.737867534160614, loss=5.494755744934082
I0206 20:08:05.608270 139946397853440 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.9422961473464966, loss=3.9714858531951904
I0206 20:08:52.531749 139946414638848 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.9379211664199829, loss=3.867582082748413
I0206 20:09:39.405261 139946397853440 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.9855331778526306, loss=4.099194526672363
I0206 20:10:26.493135 139946414638848 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.9180050492286682, loss=4.150157451629639
I0206 20:11:13.532652 139946397853440 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.9525212645530701, loss=3.882734537124634
I0206 20:12:00.515223 139946414638848 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.7652788162231445, loss=5.953298568725586
I0206 20:12:35.424450 140107197974336 spec.py:321] Evaluating on the training split.
I0206 20:12:46.385676 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 20:13:26.680278 140107197974336 spec.py:349] Evaluating on the test split.
I0206 20:13:28.273845 140107197974336 submission_runner.py:408] Time since start: 20361.53s, 	Step: 38976, 	{'train/accuracy': 0.3922656178474426, 'train/loss': 2.8164315223693848, 'validation/accuracy': 0.35637998580932617, 'validation/loss': 3.0107686519622803, 'validation/num_examples': 50000, 'test/accuracy': 0.2685000002384186, 'test/loss': 3.6256322860717773, 'test/num_examples': 10000, 'score': 18102.575630664825, 'total_duration': 20361.533406972885, 'accumulated_submission_time': 18102.575630664825, 'accumulated_eval_time': 2254.9687502384186, 'accumulated_logging_time': 1.7733440399169922}
I0206 20:13:28.311918 139946397853440 logging_writer.py:48] [38976] accumulated_eval_time=2254.968750, accumulated_logging_time=1.773344, accumulated_submission_time=18102.575631, global_step=38976, preemption_count=0, score=18102.575631, test/accuracy=0.268500, test/loss=3.625632, test/num_examples=10000, total_duration=20361.533407, train/accuracy=0.392266, train/loss=2.816432, validation/accuracy=0.356380, validation/loss=3.010769, validation/num_examples=50000
I0206 20:13:38.141254 139946414638848 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.9240778684616089, loss=5.469038486480713
I0206 20:14:22.118754 139946397853440 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.0993406772613525, loss=4.133352756500244
I0206 20:15:08.861931 139946414638848 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.9974581599235535, loss=4.285211086273193
I0206 20:15:55.986732 139946397853440 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.2601872682571411, loss=3.929180860519409
I0206 20:16:42.598105 139946414638848 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9478735327720642, loss=4.541073799133301
I0206 20:17:29.605409 139946397853440 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.0668361186981201, loss=3.9661598205566406
I0206 20:18:16.561685 139946414638848 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.0131162405014038, loss=3.703090190887451
I0206 20:19:03.565151 139946397853440 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.05365788936615, loss=3.8075361251831055
I0206 20:19:50.600865 139946414638848 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.974539041519165, loss=4.3172407150268555
I0206 20:20:28.653785 140107197974336 spec.py:321] Evaluating on the training split.
I0206 20:20:40.650049 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 20:21:17.908838 140107197974336 spec.py:349] Evaluating on the test split.
I0206 20:21:19.505172 140107197974336 submission_runner.py:408] Time since start: 20832.76s, 	Step: 39884, 	{'train/accuracy': 0.3822265565395355, 'train/loss': 2.8847715854644775, 'validation/accuracy': 0.3570599853992462, 'validation/loss': 3.043698787689209, 'validation/num_examples': 50000, 'test/accuracy': 0.2786000072956085, 'test/loss': 3.6124985218048096, 'test/num_examples': 10000, 'score': 18522.856332540512, 'total_duration': 20832.76473426819, 'accumulated_submission_time': 18522.856332540512, 'accumulated_eval_time': 2305.820141553879, 'accumulated_logging_time': 1.8208367824554443}
I0206 20:21:19.528381 139946397853440 logging_writer.py:48] [39884] accumulated_eval_time=2305.820142, accumulated_logging_time=1.820837, accumulated_submission_time=18522.856333, global_step=39884, preemption_count=0, score=18522.856333, test/accuracy=0.278600, test/loss=3.612499, test/num_examples=10000, total_duration=20832.764734, train/accuracy=0.382227, train/loss=2.884772, validation/accuracy=0.357060, validation/loss=3.043699, validation/num_examples=50000
I0206 20:21:26.254100 139946414638848 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.0570566654205322, loss=3.8064281940460205
I0206 20:22:09.544056 139946397853440 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.801794171333313, loss=4.0622148513793945
I0206 20:22:56.194127 139946414638848 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.1287089586257935, loss=3.831228733062744
I0206 20:23:43.019729 139946397853440 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.0289546251296997, loss=3.764417886734009
I0206 20:24:29.913819 139946414638848 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.9814839959144592, loss=3.945277452468872
I0206 20:25:16.920853 139946397853440 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.9860841035842896, loss=3.7329423427581787
I0206 20:26:03.745416 139946414638848 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.8419404625892639, loss=4.325784683227539
I0206 20:26:50.432404 139946397853440 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6825429797172546, loss=5.923278331756592
I0206 20:27:37.289299 139946414638848 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.0369001626968384, loss=3.8494789600372314
I0206 20:28:19.571456 140107197974336 spec.py:321] Evaluating on the training split.
I0206 20:28:30.569562 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 20:29:08.096622 140107197974336 spec.py:349] Evaluating on the test split.
I0206 20:29:09.679083 140107197974336 submission_runner.py:408] Time since start: 21302.94s, 	Step: 40792, 	{'train/accuracy': 0.3929687440395355, 'train/loss': 2.7785167694091797, 'validation/accuracy': 0.36907997727394104, 'validation/loss': 2.914356231689453, 'validation/num_examples': 50000, 'test/accuracy': 0.2778000235557556, 'test/loss': 3.5277481079101562, 'test/num_examples': 10000, 'score': 18942.836881637573, 'total_duration': 21302.938645601273, 'accumulated_submission_time': 18942.836881637573, 'accumulated_eval_time': 2355.927830219269, 'accumulated_logging_time': 1.8549113273620605}
I0206 20:29:09.702292 139946397853440 logging_writer.py:48] [40792] accumulated_eval_time=2355.927830, accumulated_logging_time=1.854911, accumulated_submission_time=18942.836882, global_step=40792, preemption_count=0, score=18942.836882, test/accuracy=0.277800, test/loss=3.527748, test/num_examples=10000, total_duration=21302.938646, train/accuracy=0.392969, train/loss=2.778517, validation/accuracy=0.369080, validation/loss=2.914356, validation/num_examples=50000
I0206 20:29:13.304423 139946414638848 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7010831236839294, loss=5.698411464691162
I0206 20:29:56.375387 139946397853440 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.8761482238769531, loss=4.364450454711914
I0206 20:30:43.246554 139946414638848 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.853614330291748, loss=4.737370014190674
I0206 20:31:30.345815 139946397853440 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.8307971954345703, loss=5.770073890686035
I0206 20:32:17.542362 139946414638848 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.1812713146209717, loss=3.9727797508239746
I0206 20:33:05.039132 139946397853440 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.925929844379425, loss=3.778568744659424
I0206 20:33:51.986890 139946414638848 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.8209595084190369, loss=5.9180426597595215
I0206 20:34:39.373914 139946397853440 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8914209604263306, loss=4.5655198097229
I0206 20:35:26.007130 139946414638848 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.066206932067871, loss=3.8853042125701904
I0206 20:36:09.710862 140107197974336 spec.py:321] Evaluating on the training split.
I0206 20:36:20.910689 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 20:36:58.705469 140107197974336 spec.py:349] Evaluating on the test split.
I0206 20:37:00.309548 140107197974336 submission_runner.py:408] Time since start: 21773.57s, 	Step: 41694, 	{'train/accuracy': 0.3770507872104645, 'train/loss': 2.974250078201294, 'validation/accuracy': 0.3516799807548523, 'validation/loss': 3.125171184539795, 'validation/num_examples': 50000, 'test/accuracy': 0.26820001006126404, 'test/loss': 3.675233840942383, 'test/num_examples': 10000, 'score': 19362.784957170486, 'total_duration': 21773.569100618362, 'accumulated_submission_time': 19362.784957170486, 'accumulated_eval_time': 2406.526533842087, 'accumulated_logging_time': 1.8879799842834473}
I0206 20:37:00.333694 139946397853440 logging_writer.py:48] [41694] accumulated_eval_time=2406.526534, accumulated_logging_time=1.887980, accumulated_submission_time=19362.784957, global_step=41694, preemption_count=0, score=19362.784957, test/accuracy=0.268200, test/loss=3.675234, test/num_examples=10000, total_duration=21773.569101, train/accuracy=0.377051, train/loss=2.974250, validation/accuracy=0.351680, validation/loss=3.125171, validation/num_examples=50000
I0206 20:37:03.110250 139946414638848 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7314242124557495, loss=4.85246467590332
I0206 20:37:45.716155 139946397853440 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.0538761615753174, loss=3.8755486011505127
I0206 20:38:32.215743 139946414638848 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.0714983940124512, loss=4.058923244476318
I0206 20:39:19.334425 139946397853440 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.9021494388580322, loss=3.8546230792999268
I0206 20:40:06.219977 139946414638848 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.0442302227020264, loss=4.617246627807617
I0206 20:40:52.978323 139946397853440 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.9234349131584167, loss=3.8029568195343018
I0206 20:41:39.973372 139946414638848 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.8560387492179871, loss=5.067320346832275
I0206 20:42:27.084433 139946397853440 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.9522567987442017, loss=5.801135540008545
I0206 20:43:14.015593 139946414638848 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.8564926385879517, loss=5.193596363067627
I0206 20:44:00.761915 139946397853440 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.1593997478485107, loss=3.9371049404144287
I0206 20:44:00.774667 140107197974336 spec.py:321] Evaluating on the training split.
I0206 20:44:11.811122 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 20:44:49.538900 140107197974336 spec.py:349] Evaluating on the test split.
I0206 20:44:51.139567 140107197974336 submission_runner.py:408] Time since start: 22244.40s, 	Step: 42601, 	{'train/accuracy': 0.3780468702316284, 'train/loss': 2.9276158809661865, 'validation/accuracy': 0.34797999262809753, 'validation/loss': 3.1044631004333496, 'validation/num_examples': 50000, 'test/accuracy': 0.26570001244544983, 'test/loss': 3.694946765899658, 'test/num_examples': 10000, 'score': 19783.164499998093, 'total_duration': 22244.399122714996, 'accumulated_submission_time': 19783.164499998093, 'accumulated_eval_time': 2456.8914000988007, 'accumulated_logging_time': 1.9217658042907715}
I0206 20:44:51.161892 139946414638848 logging_writer.py:48] [42601] accumulated_eval_time=2456.891400, accumulated_logging_time=1.921766, accumulated_submission_time=19783.164500, global_step=42601, preemption_count=0, score=19783.164500, test/accuracy=0.265700, test/loss=3.694947, test/num_examples=10000, total_duration=22244.399123, train/accuracy=0.378047, train/loss=2.927616, validation/accuracy=0.347980, validation/loss=3.104463, validation/num_examples=50000
I0206 20:45:32.917757 139946397853440 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.9200161099433899, loss=4.116903305053711
I0206 20:46:19.145992 139946414638848 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.0107530355453491, loss=3.7487497329711914
I0206 20:47:05.885630 139946397853440 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.7397500872612, loss=5.780289173126221
I0206 20:47:52.570309 139946414638848 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7041404247283936, loss=5.933290481567383
I0206 20:48:39.107459 139946397853440 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.9035020470619202, loss=4.35895299911499
I0206 20:49:25.793061 139946414638848 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.8671340346336365, loss=4.451418399810791
I0206 20:50:12.240943 139946397853440 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.0323902368545532, loss=5.435698509216309
I0206 20:50:58.625198 139946414638848 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.1060802936553955, loss=3.837820053100586
I0206 20:51:45.227883 139946397853440 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.2484103441238403, loss=3.745150089263916
I0206 20:51:51.440623 140107197974336 spec.py:321] Evaluating on the training split.
I0206 20:52:02.639785 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 20:52:41.928878 140107197974336 spec.py:349] Evaluating on the test split.
I0206 20:52:43.519225 140107197974336 submission_runner.py:408] Time since start: 22716.78s, 	Step: 43515, 	{'train/accuracy': 0.40892577171325684, 'train/loss': 2.723781108856201, 'validation/accuracy': 0.37831997871398926, 'validation/loss': 2.8741984367370605, 'validation/num_examples': 50000, 'test/accuracy': 0.2962000072002411, 'test/loss': 3.4557039737701416, 'test/num_examples': 10000, 'score': 20203.380412817, 'total_duration': 22716.77877688408, 'accumulated_submission_time': 20203.380412817, 'accumulated_eval_time': 2508.969986438751, 'accumulated_logging_time': 1.9536054134368896}
I0206 20:52:43.540992 139946414638848 logging_writer.py:48] [43515] accumulated_eval_time=2508.969986, accumulated_logging_time=1.953605, accumulated_submission_time=20203.380413, global_step=43515, preemption_count=0, score=20203.380413, test/accuracy=0.296200, test/loss=3.455704, test/num_examples=10000, total_duration=22716.778777, train/accuracy=0.408926, train/loss=2.723781, validation/accuracy=0.378320, validation/loss=2.874198, validation/num_examples=50000
I0206 20:53:19.141420 139946397853440 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.0393770933151245, loss=3.7409963607788086
I0206 20:54:05.580573 139946414638848 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.015615463256836, loss=3.760450601577759
I0206 20:54:52.763720 139946397853440 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.0235962867736816, loss=3.7638869285583496
I0206 20:55:39.728208 139946414638848 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7220590114593506, loss=5.813899517059326
I0206 20:56:26.766746 139946397853440 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1133657693862915, loss=3.8587119579315186
I0206 20:57:13.606079 139946414638848 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.9512133002281189, loss=4.026616096496582
I0206 20:58:00.300493 139946397853440 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.027051568031311, loss=3.703479290008545
I0206 20:58:47.310610 139946414638848 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.0517501831054688, loss=3.846731185913086
I0206 20:59:34.319575 139946397853440 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.983400821685791, loss=3.843402147293091
I0206 20:59:43.887444 140107197974336 spec.py:321] Evaluating on the training split.
I0206 20:59:55.010840 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 21:00:32.310926 140107197974336 spec.py:349] Evaluating on the test split.
I0206 21:00:33.911695 140107197974336 submission_runner.py:408] Time since start: 23187.17s, 	Step: 44422, 	{'train/accuracy': 0.3873632848262787, 'train/loss': 2.8844244480133057, 'validation/accuracy': 0.3587599992752075, 'validation/loss': 3.038835287094116, 'validation/num_examples': 50000, 'test/accuracy': 0.27470001578330994, 'test/loss': 3.624547004699707, 'test/num_examples': 10000, 'score': 20623.665967941284, 'total_duration': 23187.171246290207, 'accumulated_submission_time': 20623.665967941284, 'accumulated_eval_time': 2558.9942378997803, 'accumulated_logging_time': 1.9841821193695068}
I0206 21:00:33.934678 139946414638848 logging_writer.py:48] [44422] accumulated_eval_time=2558.994238, accumulated_logging_time=1.984182, accumulated_submission_time=20623.665968, global_step=44422, preemption_count=0, score=20623.665968, test/accuracy=0.274700, test/loss=3.624547, test/num_examples=10000, total_duration=23187.171246, train/accuracy=0.387363, train/loss=2.884424, validation/accuracy=0.358760, validation/loss=3.038835, validation/num_examples=50000
I0206 21:01:06.310610 139946397853440 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.8012191653251648, loss=5.342429161071777
I0206 21:01:52.536829 139946414638848 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.9390559792518616, loss=5.099969863891602
I0206 21:02:39.406586 139946397853440 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.0028892755508423, loss=3.813856363296509
I0206 21:03:26.257710 139946414638848 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.8462844491004944, loss=3.9038357734680176
I0206 21:04:13.067638 139946397853440 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.0680487155914307, loss=3.866575241088867
I0206 21:04:59.954921 139946414638848 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.9724786281585693, loss=3.893861770629883
I0206 21:05:46.705613 139946397853440 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.141015648841858, loss=5.314296722412109
I0206 21:06:33.297853 139946414638848 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.9178935289382935, loss=3.803302526473999
I0206 21:07:20.015282 139946397853440 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.9905579090118408, loss=3.7907941341400146
I0206 21:07:34.083894 140107197974336 spec.py:321] Evaluating on the training split.
I0206 21:07:45.130917 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 21:08:23.054857 140107197974336 spec.py:349] Evaluating on the test split.
I0206 21:08:24.641767 140107197974336 submission_runner.py:408] Time since start: 23657.90s, 	Step: 45332, 	{'train/accuracy': 0.42726561427116394, 'train/loss': 2.607675313949585, 'validation/accuracy': 0.36705997586250305, 'validation/loss': 2.9391558170318604, 'validation/num_examples': 50000, 'test/accuracy': 0.28519999980926514, 'test/loss': 3.533428192138672, 'test/num_examples': 10000, 'score': 21043.75366783142, 'total_duration': 23657.901322603226, 'accumulated_submission_time': 21043.75366783142, 'accumulated_eval_time': 2609.552106142044, 'accumulated_logging_time': 2.0164127349853516}
I0206 21:08:24.665094 139946414638848 logging_writer.py:48] [45332] accumulated_eval_time=2609.552106, accumulated_logging_time=2.016413, accumulated_submission_time=21043.753668, global_step=45332, preemption_count=0, score=21043.753668, test/accuracy=0.285200, test/loss=3.533428, test/num_examples=10000, total_duration=23657.901323, train/accuracy=0.427266, train/loss=2.607675, validation/accuracy=0.367060, validation/loss=2.939156, validation/num_examples=50000
I0206 21:08:52.331793 139946397853440 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.0021247863769531, loss=4.52962589263916
I0206 21:09:38.778733 139946414638848 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8262636065483093, loss=4.741980075836182
I0206 21:10:25.319038 139946397853440 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.0660172700881958, loss=3.649921178817749
I0206 21:11:12.244565 139946414638848 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.0604394674301147, loss=3.7975704669952393
I0206 21:11:58.717624 139946397853440 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.9273540377616882, loss=3.752615213394165
I0206 21:12:45.417500 139946414638848 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.06053626537323, loss=5.840126037597656
I0206 21:13:32.419962 139946397853440 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.0851187705993652, loss=4.040827751159668
I0206 21:14:19.215243 139946414638848 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.113840937614441, loss=3.8527653217315674
I0206 21:15:06.248511 139946397853440 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.9731715321540833, loss=4.486170768737793
I0206 21:15:24.698831 140107197974336 spec.py:321] Evaluating on the training split.
I0206 21:15:35.539070 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 21:16:16.705958 140107197974336 spec.py:349] Evaluating on the test split.
I0206 21:16:18.310174 140107197974336 submission_runner.py:408] Time since start: 24131.57s, 	Step: 46241, 	{'train/accuracy': 0.3944140672683716, 'train/loss': 2.8179240226745605, 'validation/accuracy': 0.3656199872493744, 'validation/loss': 2.9761483669281006, 'validation/num_examples': 50000, 'test/accuracy': 0.28110000491142273, 'test/loss': 3.5713536739349365, 'test/num_examples': 10000, 'score': 21463.72553873062, 'total_duration': 24131.569739818573, 'accumulated_submission_time': 21463.72553873062, 'accumulated_eval_time': 2663.1634533405304, 'accumulated_logging_time': 2.049370527267456}
I0206 21:16:18.336510 139946414638848 logging_writer.py:48] [46241] accumulated_eval_time=2663.163453, accumulated_logging_time=2.049371, accumulated_submission_time=21463.725539, global_step=46241, preemption_count=0, score=21463.725539, test/accuracy=0.281100, test/loss=3.571354, test/num_examples=10000, total_duration=24131.569740, train/accuracy=0.394414, train/loss=2.817924, validation/accuracy=0.365620, validation/loss=2.976148, validation/num_examples=50000
I0206 21:16:41.985399 139946397853440 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.0347938537597656, loss=3.8798439502716064
I0206 21:17:28.407084 139946414638848 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.9535312056541443, loss=3.9506163597106934
I0206 21:18:15.381140 139946397853440 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1740543842315674, loss=3.6621925830841064
I0206 21:19:02.314292 139946414638848 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.0760172605514526, loss=3.747495651245117
I0206 21:19:49.129265 139946397853440 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.8215093612670898, loss=4.470320701599121
I0206 21:20:35.897343 139946414638848 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.0022550821304321, loss=3.8610434532165527
I0206 21:21:22.768810 139946397853440 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.048584222793579, loss=4.476442813873291
I0206 21:22:09.714469 139946414638848 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8943858742713928, loss=5.285696983337402
I0206 21:22:56.555359 139946397853440 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.9233686327934265, loss=4.646639823913574
I0206 21:23:18.751331 140107197974336 spec.py:321] Evaluating on the training split.
I0206 21:23:29.684705 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 21:24:09.426467 140107197974336 spec.py:349] Evaluating on the test split.
I0206 21:24:11.031775 140107197974336 submission_runner.py:408] Time since start: 24604.29s, 	Step: 47149, 	{'train/accuracy': 0.4058789014816284, 'train/loss': 2.753173828125, 'validation/accuracy': 0.374239981174469, 'validation/loss': 2.925992965698242, 'validation/num_examples': 50000, 'test/accuracy': 0.28690001368522644, 'test/loss': 3.5421478748321533, 'test/num_examples': 10000, 'score': 21884.07934308052, 'total_duration': 24604.291342496872, 'accumulated_submission_time': 21884.07934308052, 'accumulated_eval_time': 2715.4439051151276, 'accumulated_logging_time': 2.0853078365325928}
I0206 21:24:11.061850 139946414638848 logging_writer.py:48] [47149] accumulated_eval_time=2715.443905, accumulated_logging_time=2.085308, accumulated_submission_time=21884.079343, global_step=47149, preemption_count=0, score=21884.079343, test/accuracy=0.286900, test/loss=3.542148, test/num_examples=10000, total_duration=24604.291342, train/accuracy=0.405879, train/loss=2.753174, validation/accuracy=0.374240, validation/loss=2.925993, validation/num_examples=50000
I0206 21:24:31.525657 139946397853440 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.1266483068466187, loss=3.701815605163574
I0206 21:25:17.529865 139946414638848 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.2126938104629517, loss=3.804267406463623
I0206 21:26:04.525107 139946397853440 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1553583145141602, loss=3.682194709777832
I0206 21:26:51.438242 139946414638848 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.1065866947174072, loss=3.822812795639038
I0206 21:27:38.391416 139946397853440 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.1281390190124512, loss=4.20961332321167
I0206 21:28:25.180925 139946414638848 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.7757898569107056, loss=4.546614646911621
I0206 21:29:12.126733 139946397853440 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.9843774437904358, loss=3.742164134979248
I0206 21:29:59.102258 139946414638848 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.0105948448181152, loss=3.806702136993408
I0206 21:30:46.014035 139946397853440 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.180197834968567, loss=4.096107006072998
I0206 21:31:11.058259 140107197974336 spec.py:321] Evaluating on the training split.
I0206 21:31:22.054424 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 21:32:01.462402 140107197974336 spec.py:349] Evaluating on the test split.
I0206 21:32:03.064248 140107197974336 submission_runner.py:408] Time since start: 25076.32s, 	Step: 48055, 	{'train/accuracy': 0.42917966842651367, 'train/loss': 2.6209170818328857, 'validation/accuracy': 0.38675999641418457, 'validation/loss': 2.860398054122925, 'validation/num_examples': 50000, 'test/accuracy': 0.29600000381469727, 'test/loss': 3.4566426277160645, 'test/num_examples': 10000, 'score': 22304.013244628906, 'total_duration': 25076.32378435135, 'accumulated_submission_time': 22304.013244628906, 'accumulated_eval_time': 2767.4498670101166, 'accumulated_logging_time': 2.1263139247894287}
I0206 21:32:03.091092 139946414638848 logging_writer.py:48] [48055] accumulated_eval_time=2767.449867, accumulated_logging_time=2.126314, accumulated_submission_time=22304.013245, global_step=48055, preemption_count=0, score=22304.013245, test/accuracy=0.296000, test/loss=3.456643, test/num_examples=10000, total_duration=25076.323784, train/accuracy=0.429180, train/loss=2.620917, validation/accuracy=0.386760, validation/loss=2.860398, validation/num_examples=50000
I0206 21:32:21.175143 139946397853440 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.9642741084098816, loss=3.7842421531677246
I0206 21:33:06.321136 139946414638848 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.9657973051071167, loss=3.787043571472168
I0206 21:33:52.963083 139946397853440 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.0098704099655151, loss=3.7345430850982666
I0206 21:34:40.210222 139946414638848 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.0909147262573242, loss=3.6602749824523926
I0206 21:35:26.918206 139946397853440 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.8736433982849121, loss=5.6108622550964355
I0206 21:36:13.917058 139946414638848 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.866571843624115, loss=4.368331432342529
I0206 21:37:00.942376 139946397853440 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.0379109382629395, loss=5.488003730773926
I0206 21:37:47.918428 139946414638848 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.3872665166854858, loss=3.8471791744232178
I0206 21:38:34.924278 139946397853440 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.2742645740509033, loss=3.682711601257324
I0206 21:39:03.417102 140107197974336 spec.py:321] Evaluating on the training split.
I0206 21:39:14.430233 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 21:39:52.303222 140107197974336 spec.py:349] Evaluating on the test split.
I0206 21:39:53.895951 140107197974336 submission_runner.py:408] Time since start: 25547.16s, 	Step: 48963, 	{'train/accuracy': 0.41550779342651367, 'train/loss': 2.642244815826416, 'validation/accuracy': 0.38763999938964844, 'validation/loss': 2.803826093673706, 'validation/num_examples': 50000, 'test/accuracy': 0.30470001697540283, 'test/loss': 3.414707899093628, 'test/num_examples': 10000, 'score': 22724.277349472046, 'total_duration': 25547.15549659729, 'accumulated_submission_time': 22724.277349472046, 'accumulated_eval_time': 2817.928693294525, 'accumulated_logging_time': 2.163120985031128}
I0206 21:39:53.920221 139946414638848 logging_writer.py:48] [48963] accumulated_eval_time=2817.928693, accumulated_logging_time=2.163121, accumulated_submission_time=22724.277349, global_step=48963, preemption_count=0, score=22724.277349, test/accuracy=0.304700, test/loss=3.414708, test/num_examples=10000, total_duration=25547.155497, train/accuracy=0.415508, train/loss=2.642245, validation/accuracy=0.387640, validation/loss=2.803826, validation/num_examples=50000
I0206 21:40:08.868488 139946397853440 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9890069961547852, loss=5.946447372436523
I0206 21:40:53.725669 139946414638848 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.0908117294311523, loss=3.7435336112976074
I0206 21:41:40.385987 139946397853440 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.9659693837165833, loss=3.743391990661621
I0206 21:42:27.357287 139946414638848 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9626374244689941, loss=4.773425102233887
I0206 21:43:14.014629 139946397853440 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.9491711258888245, loss=3.7378180027008057
I0206 21:44:00.837458 139946414638848 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.922692060470581, loss=3.6726982593536377
I0206 21:44:47.650182 139946397853440 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.0321398973464966, loss=3.8487839698791504
I0206 21:45:34.344983 139946414638848 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.9783081412315369, loss=3.7073752880096436
I0206 21:46:21.331798 139946397853440 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.961020290851593, loss=3.830348253250122
I0206 21:46:54.254253 140107197974336 spec.py:321] Evaluating on the training split.
I0206 21:47:05.417671 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 21:47:44.379176 140107197974336 spec.py:349] Evaluating on the test split.
I0206 21:47:45.980296 140107197974336 submission_runner.py:408] Time since start: 26019.24s, 	Step: 49872, 	{'train/accuracy': 0.4216601550579071, 'train/loss': 2.6226537227630615, 'validation/accuracy': 0.39419999718666077, 'validation/loss': 2.784731388092041, 'validation/num_examples': 50000, 'test/accuracy': 0.30070000886917114, 'test/loss': 3.3947463035583496, 'test/num_examples': 10000, 'score': 23144.547844409943, 'total_duration': 26019.239842414856, 'accumulated_submission_time': 23144.547844409943, 'accumulated_eval_time': 2869.654707431793, 'accumulated_logging_time': 2.1987524032592773}
I0206 21:47:46.004541 139946414638848 logging_writer.py:48] [49872] accumulated_eval_time=2869.654707, accumulated_logging_time=2.198752, accumulated_submission_time=23144.547844, global_step=49872, preemption_count=0, score=23144.547844, test/accuracy=0.300700, test/loss=3.394746, test/num_examples=10000, total_duration=26019.239842, train/accuracy=0.421660, train/loss=2.622654, validation/accuracy=0.394200, validation/loss=2.784731, validation/num_examples=50000
I0206 21:47:57.417379 139946397853440 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.8666424751281738, loss=5.879559516906738
I0206 21:48:41.545355 139946414638848 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.7381235361099243, loss=5.8141889572143555
I0206 21:49:28.304809 139946397853440 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.1558537483215332, loss=3.752424478530884
I0206 21:50:15.317221 139946414638848 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.1465702056884766, loss=3.7566614151000977
I0206 21:51:01.889866 139946397853440 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.3030723333358765, loss=3.780606985092163
I0206 21:51:48.793352 139946414638848 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.784966766834259, loss=5.48996639251709
I0206 21:52:35.438057 139946397853440 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.1433583498001099, loss=3.743922710418701
I0206 21:53:22.027170 139946414638848 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.9758205413818359, loss=3.599656105041504
I0206 21:54:08.689357 139946397853440 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8402177691459656, loss=4.291693687438965
I0206 21:54:46.351107 140107197974336 spec.py:321] Evaluating on the training split.
I0206 21:54:57.178825 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 21:55:37.550021 140107197974336 spec.py:349] Evaluating on the test split.
I0206 21:55:39.151442 140107197974336 submission_runner.py:408] Time since start: 26492.41s, 	Step: 50782, 	{'train/accuracy': 0.431640625, 'train/loss': 2.567415475845337, 'validation/accuracy': 0.3929999768733978, 'validation/loss': 2.7868287563323975, 'validation/num_examples': 50000, 'test/accuracy': 0.30390000343322754, 'test/loss': 3.3923990726470947, 'test/num_examples': 10000, 'score': 23564.829872846603, 'total_duration': 26492.410955429077, 'accumulated_submission_time': 23564.829872846603, 'accumulated_eval_time': 2922.45498919487, 'accumulated_logging_time': 2.235773801803589}
I0206 21:55:39.179756 139946414638848 logging_writer.py:48] [50782] accumulated_eval_time=2922.454989, accumulated_logging_time=2.235774, accumulated_submission_time=23564.829873, global_step=50782, preemption_count=0, score=23564.829873, test/accuracy=0.303900, test/loss=3.392399, test/num_examples=10000, total_duration=26492.410955, train/accuracy=0.431641, train/loss=2.567415, validation/accuracy=0.393000, validation/loss=2.786829, validation/num_examples=50000
I0206 21:55:46.660230 139946397853440 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.8078320026397705, loss=4.914686679840088
I0206 21:56:30.221595 139946414638848 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.0711640119552612, loss=3.8093950748443604
I0206 21:57:16.917050 139946397853440 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.0995124578475952, loss=3.7357568740844727
I0206 21:58:04.965676 139946414638848 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.9636069536209106, loss=6.017130374908447
I0206 21:58:51.478437 139946397853440 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.0096806287765503, loss=3.5845794677734375
I0206 21:59:38.356881 139946414638848 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.9273626804351807, loss=3.7710866928100586
I0206 22:00:25.173436 139946397853440 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.3177595138549805, loss=3.7248239517211914
I0206 22:01:11.935052 139946414638848 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.0215427875518799, loss=3.7648353576660156
I0206 22:01:58.592777 139946397853440 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.8573318719863892, loss=4.0481719970703125
I0206 22:02:39.487381 140107197974336 spec.py:321] Evaluating on the training split.
I0206 22:02:50.582398 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 22:03:30.655526 140107197974336 spec.py:349] Evaluating on the test split.
I0206 22:03:32.248322 140107197974336 submission_runner.py:408] Time since start: 26965.51s, 	Step: 51689, 	{'train/accuracy': 0.4244335889816284, 'train/loss': 2.612565279006958, 'validation/accuracy': 0.3941600024700165, 'validation/loss': 2.762552499771118, 'validation/num_examples': 50000, 'test/accuracy': 0.30320000648498535, 'test/loss': 3.374166250228882, 'test/num_examples': 10000, 'score': 23985.074984312057, 'total_duration': 26965.507806777954, 'accumulated_submission_time': 23985.074984312057, 'accumulated_eval_time': 2975.215850353241, 'accumulated_logging_time': 2.2752928733825684}
I0206 22:03:32.281318 139946414638848 logging_writer.py:48] [51689] accumulated_eval_time=2975.215850, accumulated_logging_time=2.275293, accumulated_submission_time=23985.074984, global_step=51689, preemption_count=0, score=23985.074984, test/accuracy=0.303200, test/loss=3.374166, test/num_examples=10000, total_duration=26965.507807, train/accuracy=0.424434, train/loss=2.612565, validation/accuracy=0.394160, validation/loss=2.762552, validation/num_examples=50000
I0206 22:03:37.012815 139946397853440 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.1728947162628174, loss=3.754836082458496
I0206 22:04:19.932357 139946414638848 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.8663367033004761, loss=5.977884769439697
I0206 22:05:06.720376 139946397853440 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.8983907103538513, loss=4.000353813171387
I0206 22:05:53.579002 139946414638848 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.9794963598251343, loss=4.326684474945068
I0206 22:06:40.229560 139946397853440 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.0648385286331177, loss=3.5822134017944336
I0206 22:07:27.134791 139946414638848 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.8417710661888123, loss=3.613739252090454
I0206 22:08:13.873277 139946397853440 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.9770121574401855, loss=4.572093486785889
I0206 22:09:00.668918 139946414638848 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.2539883852005005, loss=3.724806308746338
I0206 22:09:47.587630 139946397853440 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.1224420070648193, loss=3.6888551712036133
I0206 22:10:32.530746 140107197974336 spec.py:321] Evaluating on the training split.
I0206 22:10:43.668175 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 22:11:21.582290 140107197974336 spec.py:349] Evaluating on the test split.
I0206 22:11:23.177899 140107197974336 submission_runner.py:408] Time since start: 27436.44s, 	Step: 52597, 	{'train/accuracy': 0.4292578101158142, 'train/loss': 2.5953733921051025, 'validation/accuracy': 0.39813998341560364, 'validation/loss': 2.7684292793273926, 'validation/num_examples': 50000, 'test/accuracy': 0.3110000193119049, 'test/loss': 3.386230230331421, 'test/num_examples': 10000, 'score': 24405.261734247208, 'total_duration': 27436.437440156937, 'accumulated_submission_time': 24405.261734247208, 'accumulated_eval_time': 3025.862987279892, 'accumulated_logging_time': 2.3195648193359375}
I0206 22:11:23.206445 139946414638848 logging_writer.py:48] [52597] accumulated_eval_time=3025.862987, accumulated_logging_time=2.319565, accumulated_submission_time=24405.261734, global_step=52597, preemption_count=0, score=24405.261734, test/accuracy=0.311000, test/loss=3.386230, test/num_examples=10000, total_duration=27436.437440, train/accuracy=0.429258, train/loss=2.595373, validation/accuracy=0.398140, validation/loss=2.768429, validation/num_examples=50000
I0206 22:11:24.788017 139946397853440 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.1078860759735107, loss=5.856307029724121
I0206 22:12:07.449052 139946414638848 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.9635106325149536, loss=5.83707332611084
I0206 22:12:54.110800 139946397853440 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.995498538017273, loss=3.778681755065918
I0206 22:13:41.058724 139946414638848 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.8866920471191406, loss=4.156408309936523
I0206 22:14:28.015340 139946397853440 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.267670750617981, loss=3.689215660095215
I0206 22:15:15.306325 139946414638848 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.9953293800354004, loss=3.7029311656951904
I0206 22:16:01.989310 139946397853440 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.0121790170669556, loss=3.614764451980591
I0206 22:16:48.740561 139946414638848 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.1980940103530884, loss=3.776045322418213
I0206 22:17:35.823775 139946397853440 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.2475686073303223, loss=3.6876373291015625
I0206 22:18:22.702974 139946414638848 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.9236134886741638, loss=3.476982355117798
I0206 22:18:23.315514 140107197974336 spec.py:321] Evaluating on the training split.
I0206 22:18:34.148298 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 22:19:13.913807 140107197974336 spec.py:349] Evaluating on the test split.
I0206 22:19:15.519174 140107197974336 submission_runner.py:408] Time since start: 27908.78s, 	Step: 53503, 	{'train/accuracy': 0.4274999797344208, 'train/loss': 2.5975615978240967, 'validation/accuracy': 0.38499999046325684, 'validation/loss': 2.821701765060425, 'validation/num_examples': 50000, 'test/accuracy': 0.29980000853538513, 'test/loss': 3.433039903640747, 'test/num_examples': 10000, 'score': 24825.304622650146, 'total_duration': 27908.778692007065, 'accumulated_submission_time': 24825.304622650146, 'accumulated_eval_time': 3078.066597223282, 'accumulated_logging_time': 2.363283157348633}
I0206 22:19:15.544158 139946397853440 logging_writer.py:48] [53503] accumulated_eval_time=3078.066597, accumulated_logging_time=2.363283, accumulated_submission_time=24825.304623, global_step=53503, preemption_count=0, score=24825.304623, test/accuracy=0.299800, test/loss=3.433040, test/num_examples=10000, total_duration=27908.778692, train/accuracy=0.427500, train/loss=2.597562, validation/accuracy=0.385000, validation/loss=2.821702, validation/num_examples=50000
I0206 22:19:56.703011 139946414638848 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.0429574251174927, loss=3.6945102214813232
I0206 22:20:43.125674 139946397853440 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.1703740358352661, loss=3.6833622455596924
I0206 22:21:29.854955 139946414638848 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.0840468406677246, loss=3.7629971504211426
I0206 22:22:16.635511 139946397853440 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.9258058071136475, loss=3.764538049697876
I0206 22:23:03.336391 139946414638848 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9568544030189514, loss=4.457443714141846
I0206 22:23:50.046769 139946397853440 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.0623058080673218, loss=4.569542407989502
I0206 22:24:36.800609 139946414638848 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.9804080128669739, loss=3.776498794555664
I0206 22:25:23.609362 139946397853440 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9817742109298706, loss=3.927065849304199
I0206 22:26:10.281647 139946414638848 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.9806342720985413, loss=3.753490924835205
I0206 22:26:15.565918 140107197974336 spec.py:321] Evaluating on the training split.
I0206 22:26:26.386996 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 22:27:05.795973 140107197974336 spec.py:349] Evaluating on the test split.
I0206 22:27:07.387841 140107197974336 submission_runner.py:408] Time since start: 28380.65s, 	Step: 54413, 	{'train/accuracy': 0.4327929615974426, 'train/loss': 2.579705238342285, 'validation/accuracy': 0.3990999758243561, 'validation/loss': 2.741082191467285, 'validation/num_examples': 50000, 'test/accuracy': 0.3037000000476837, 'test/loss': 3.3778457641601562, 'test/num_examples': 10000, 'score': 25245.264453172684, 'total_duration': 28380.64738035202, 'accumulated_submission_time': 25245.264453172684, 'accumulated_eval_time': 3129.888496398926, 'accumulated_logging_time': 2.3979485034942627}
I0206 22:27:07.412465 139946397853440 logging_writer.py:48] [54413] accumulated_eval_time=3129.888496, accumulated_logging_time=2.397949, accumulated_submission_time=25245.264453, global_step=54413, preemption_count=0, score=25245.264453, test/accuracy=0.303700, test/loss=3.377846, test/num_examples=10000, total_duration=28380.647380, train/accuracy=0.432793, train/loss=2.579705, validation/accuracy=0.399100, validation/loss=2.741082, validation/num_examples=50000
I0206 22:27:43.870942 139946414638848 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.004284381866455, loss=3.533125400543213
I0206 22:28:30.332519 139946397853440 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.854615330696106, loss=4.359405517578125
I0206 22:29:17.112334 139946414638848 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.0629574060440063, loss=3.612856388092041
I0206 22:30:04.149986 139946397853440 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.1048706769943237, loss=3.8007357120513916
I0206 22:30:50.797333 139946414638848 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.8248329162597656, loss=4.554449558258057
I0206 22:31:37.548338 139946397853440 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.0121042728424072, loss=3.643568992614746
I0206 22:32:24.607319 139946414638848 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0768177509307861, loss=5.763164520263672
I0206 22:33:11.172283 139946397853440 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.8999736905097961, loss=4.311295032501221
I0206 22:33:57.716748 139946414638848 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.8416051268577576, loss=5.769028663635254
I0206 22:34:07.800477 140107197974336 spec.py:321] Evaluating on the training split.
I0206 22:34:18.972003 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 22:34:57.964682 140107197974336 spec.py:349] Evaluating on the test split.
I0206 22:34:59.564621 140107197974336 submission_runner.py:408] Time since start: 28852.82s, 	Step: 55323, 	{'train/accuracy': 0.4314843714237213, 'train/loss': 2.5818235874176025, 'validation/accuracy': 0.4027799963951111, 'validation/loss': 2.7566277980804443, 'validation/num_examples': 50000, 'test/accuracy': 0.31200000643730164, 'test/loss': 3.340991973876953, 'test/num_examples': 10000, 'score': 25665.591471672058, 'total_duration': 28852.82416653633, 'accumulated_submission_time': 25665.591471672058, 'accumulated_eval_time': 3181.6526210308075, 'accumulated_logging_time': 2.4328207969665527}
I0206 22:34:59.589375 139946397853440 logging_writer.py:48] [55323] accumulated_eval_time=3181.652621, accumulated_logging_time=2.432821, accumulated_submission_time=25665.591472, global_step=55323, preemption_count=0, score=25665.591472, test/accuracy=0.312000, test/loss=3.340992, test/num_examples=10000, total_duration=28852.824167, train/accuracy=0.431484, train/loss=2.581824, validation/accuracy=0.402780, validation/loss=2.756628, validation/num_examples=50000
I0206 22:35:31.520688 139946414638848 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.208029866218567, loss=3.726593255996704
I0206 22:36:17.996086 139946397853440 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.7966664433479309, loss=4.5551276206970215
I0206 22:37:04.768517 139946414638848 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.8222529292106628, loss=3.953516721725464
I0206 22:37:51.443266 139946397853440 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.0367792844772339, loss=3.6713125705718994
I0206 22:38:38.179887 139946414638848 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0665608644485474, loss=3.595938205718994
I0206 22:39:24.727827 139946397853440 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.2040278911590576, loss=3.6726908683776855
I0206 22:40:11.604400 139946414638848 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.8622190952301025, loss=4.239028453826904
I0206 22:40:58.268960 139946397853440 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.9939911961555481, loss=3.7535557746887207
I0206 22:41:44.899902 139946414638848 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9187557101249695, loss=3.9264278411865234
I0206 22:41:59.673365 140107197974336 spec.py:321] Evaluating on the training split.
I0206 22:42:10.926183 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 22:42:49.833777 140107197974336 spec.py:349] Evaluating on the test split.
I0206 22:42:51.426295 140107197974336 submission_runner.py:408] Time since start: 29324.69s, 	Step: 56233, 	{'train/accuracy': 0.43990233540534973, 'train/loss': 2.5200986862182617, 'validation/accuracy': 0.407260000705719, 'validation/loss': 2.6993579864501953, 'validation/num_examples': 50000, 'test/accuracy': 0.31220000982284546, 'test/loss': 3.344634532928467, 'test/num_examples': 10000, 'score': 26085.61327648163, 'total_duration': 29324.685839653015, 'accumulated_submission_time': 26085.61327648163, 'accumulated_eval_time': 3233.405528306961, 'accumulated_logging_time': 2.467010736465454}
I0206 22:42:51.451242 139946397853440 logging_writer.py:48] [56233] accumulated_eval_time=3233.405528, accumulated_logging_time=2.467011, accumulated_submission_time=26085.613276, global_step=56233, preemption_count=0, score=26085.613276, test/accuracy=0.312200, test/loss=3.344635, test/num_examples=10000, total_duration=29324.685840, train/accuracy=0.439902, train/loss=2.520099, validation/accuracy=0.407260, validation/loss=2.699358, validation/num_examples=50000
I0206 22:43:18.513753 139946414638848 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.0246747732162476, loss=3.7252302169799805
I0206 22:44:04.724139 139946397853440 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.0848523378372192, loss=3.7138752937316895
I0206 22:44:51.338548 139946414638848 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.0562117099761963, loss=3.624512195587158
I0206 22:45:38.099773 139946397853440 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.8099162578582764, loss=5.290672302246094
I0206 22:46:24.758819 139946414638848 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.995164692401886, loss=3.9504613876342773
I0206 22:47:11.512260 139946397853440 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.0599184036254883, loss=4.130688190460205
I0206 22:47:58.054104 139946414638848 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.8386897444725037, loss=5.7032928466796875
I0206 22:48:44.745295 139946397853440 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.0006183385849, loss=3.6197309494018555
I0206 22:49:31.390727 139946414638848 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.9251898527145386, loss=4.81782865524292
I0206 22:49:51.607287 140107197974336 spec.py:321] Evaluating on the training split.
I0206 22:50:03.167930 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 22:50:41.390355 140107197974336 spec.py:349] Evaluating on the test split.
I0206 22:50:42.987879 140107197974336 submission_runner.py:408] Time since start: 29796.25s, 	Step: 57145, 	{'train/accuracy': 0.4365624785423279, 'train/loss': 2.546628475189209, 'validation/accuracy': 0.40609997510910034, 'validation/loss': 2.6989681720733643, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.323418140411377, 'test/num_examples': 10000, 'score': 26505.705159902573, 'total_duration': 29796.24742078781, 'accumulated_submission_time': 26505.705159902573, 'accumulated_eval_time': 3284.786096572876, 'accumulated_logging_time': 2.5048763751983643}
I0206 22:50:43.015860 139946397853440 logging_writer.py:48] [57145] accumulated_eval_time=3284.786097, accumulated_logging_time=2.504876, accumulated_submission_time=26505.705160, global_step=57145, preemption_count=0, score=26505.705160, test/accuracy=0.311500, test/loss=3.323418, test/num_examples=10000, total_duration=29796.247421, train/accuracy=0.436562, train/loss=2.546628, validation/accuracy=0.406100, validation/loss=2.698968, validation/num_examples=50000
I0206 22:51:05.033133 139946414638848 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.1811497211456299, loss=3.659165382385254
I0206 22:51:50.889214 139946397853440 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.9629001021385193, loss=3.6764843463897705
I0206 22:52:37.553607 139946414638848 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.2019964456558228, loss=3.498267889022827
I0206 22:53:24.354586 139946397853440 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.056138277053833, loss=3.798855781555176
I0206 22:54:11.025886 139946414638848 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9337949156761169, loss=5.725246429443359
I0206 22:54:57.629817 139946397853440 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.0043823719024658, loss=3.636669397354126
I0206 22:55:44.425615 139946414638848 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.0419764518737793, loss=3.609527349472046
I0206 22:56:31.067974 139946397853440 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.8297478556632996, loss=5.131115913391113
I0206 22:57:17.643348 139946414638848 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.7938582301139832, loss=4.876476287841797
I0206 22:57:43.089105 140107197974336 spec.py:321] Evaluating on the training split.
I0206 22:57:54.045031 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 22:58:32.411412 140107197974336 spec.py:349] Evaluating on the test split.
I0206 22:58:34.016012 140107197974336 submission_runner.py:408] Time since start: 30267.28s, 	Step: 58056, 	{'train/accuracy': 0.4286132752895355, 'train/loss': 2.626063108444214, 'validation/accuracy': 0.40685999393463135, 'validation/loss': 2.7548227310180664, 'validation/num_examples': 50000, 'test/accuracy': 0.3158000111579895, 'test/loss': 3.3459408283233643, 'test/num_examples': 10000, 'score': 26925.716034412384, 'total_duration': 30267.275554418564, 'accumulated_submission_time': 26925.716034412384, 'accumulated_eval_time': 3335.712982416153, 'accumulated_logging_time': 2.5432417392730713}
I0206 22:58:34.041851 139946397853440 logging_writer.py:48] [58056] accumulated_eval_time=3335.712982, accumulated_logging_time=2.543242, accumulated_submission_time=26925.716034, global_step=58056, preemption_count=0, score=26925.716034, test/accuracy=0.315800, test/loss=3.345941, test/num_examples=10000, total_duration=30267.275554, train/accuracy=0.428613, train/loss=2.626063, validation/accuracy=0.406860, validation/loss=2.754823, validation/num_examples=50000
I0206 22:58:51.740224 139946414638848 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.0053997039794922, loss=3.6598501205444336
I0206 22:59:37.003627 139946397853440 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.2474833726882935, loss=3.6470682621002197
I0206 23:00:23.651046 139946414638848 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.0219954252243042, loss=3.5461630821228027
I0206 23:01:10.505519 139946397853440 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.0078763961791992, loss=4.818644046783447
I0206 23:01:57.440704 139946414638848 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.9626545906066895, loss=3.4761102199554443
I0206 23:02:44.185925 139946397853440 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.2080029249191284, loss=3.6908047199249268
I0206 23:03:31.036628 139946414638848 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.9613381624221802, loss=4.808357238769531
I0206 23:04:18.020977 139946397853440 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.076690673828125, loss=3.930870532989502
I0206 23:05:04.857269 139946414638848 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.121980905532837, loss=3.6599574089050293
I0206 23:05:34.315262 140107197974336 spec.py:321] Evaluating on the training split.
I0206 23:05:45.338097 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 23:06:25.345862 140107197974336 spec.py:349] Evaluating on the test split.
I0206 23:06:26.943841 140107197974336 submission_runner.py:408] Time since start: 30740.20s, 	Step: 58965, 	{'train/accuracy': 0.4401562511920929, 'train/loss': 2.5463802814483643, 'validation/accuracy': 0.4059799909591675, 'validation/loss': 2.7379801273345947, 'validation/num_examples': 50000, 'test/accuracy': 0.3166000247001648, 'test/loss': 3.3564846515655518, 'test/num_examples': 10000, 'score': 27345.927248716354, 'total_duration': 30740.203387737274, 'accumulated_submission_time': 27345.927248716354, 'accumulated_eval_time': 3388.341548681259, 'accumulated_logging_time': 2.5799450874328613}
I0206 23:06:26.972276 139946397853440 logging_writer.py:48] [58965] accumulated_eval_time=3388.341549, accumulated_logging_time=2.579945, accumulated_submission_time=27345.927249, global_step=58965, preemption_count=0, score=27345.927249, test/accuracy=0.316600, test/loss=3.356485, test/num_examples=10000, total_duration=30740.203388, train/accuracy=0.440156, train/loss=2.546380, validation/accuracy=0.405980, validation/loss=2.737980, validation/num_examples=50000
I0206 23:06:41.120437 139946414638848 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.8988122344017029, loss=4.794464588165283
I0206 23:07:25.930602 139946397853440 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.0313745737075806, loss=5.302815914154053
I0206 23:08:12.591233 139946414638848 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.160736083984375, loss=3.4917895793914795
I0206 23:08:59.436167 139946397853440 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.9616374373435974, loss=6.032341480255127
I0206 23:09:46.155425 139946414638848 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9834349155426025, loss=3.3876841068267822
I0206 23:10:32.773855 139946397853440 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.1699135303497314, loss=3.606365442276001
I0206 23:11:19.495317 139946414638848 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.7971400022506714, loss=5.672363758087158
I0206 23:12:06.355234 139946397853440 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.0945967435836792, loss=3.61468243598938
I0206 23:12:52.864654 139946414638848 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.101356863975525, loss=4.010790824890137
I0206 23:13:27.376340 140107197974336 spec.py:321] Evaluating on the training split.
I0206 23:13:38.539458 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 23:14:20.020736 140107197974336 spec.py:349] Evaluating on the test split.
I0206 23:14:21.618440 140107197974336 submission_runner.py:408] Time since start: 31214.88s, 	Step: 59875, 	{'train/accuracy': 0.4756445288658142, 'train/loss': 2.3494179248809814, 'validation/accuracy': 0.4097599983215332, 'validation/loss': 2.690446376800537, 'validation/num_examples': 50000, 'test/accuracy': 0.3135000169277191, 'test/loss': 3.311406373977661, 'test/num_examples': 10000, 'score': 27766.268231868744, 'total_duration': 31214.877977132797, 'accumulated_submission_time': 27766.268231868744, 'accumulated_eval_time': 3442.583624601364, 'accumulated_logging_time': 2.6201019287109375}
I0206 23:14:21.650169 139946397853440 logging_writer.py:48] [59875] accumulated_eval_time=3442.583625, accumulated_logging_time=2.620102, accumulated_submission_time=27766.268232, global_step=59875, preemption_count=0, score=27766.268232, test/accuracy=0.313500, test/loss=3.311406, test/num_examples=10000, total_duration=31214.877977, train/accuracy=0.475645, train/loss=2.349418, validation/accuracy=0.409760, validation/loss=2.690446, validation/num_examples=50000
I0206 23:14:31.870871 139946414638848 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.9378393292427063, loss=4.839700698852539
I0206 23:15:15.756002 139946397853440 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.1034067869186401, loss=3.6218132972717285
I0206 23:16:02.550007 139946414638848 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.2107141017913818, loss=3.632366180419922
I0206 23:16:49.482253 139946397853440 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.9859821200370789, loss=3.524782419204712
I0206 23:17:36.167330 139946414638848 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.389062523841858, loss=3.5487964153289795
I0206 23:18:23.268851 139946397853440 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.7319480776786804, loss=5.081221580505371
I0206 23:19:09.845414 139946414638848 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.9493938088417053, loss=3.4218688011169434
I0206 23:19:56.529667 139946397853440 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.769481360912323, loss=5.852247714996338
I0206 23:20:43.268535 139946414638848 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.7966985702514648, loss=5.590808868408203
I0206 23:21:21.827929 140107197974336 spec.py:321] Evaluating on the training split.
I0206 23:21:32.859585 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 23:22:12.438403 140107197974336 spec.py:349] Evaluating on the test split.
I0206 23:22:14.044799 140107197974336 submission_runner.py:408] Time since start: 31687.30s, 	Step: 60784, 	{'train/accuracy': 0.440253883600235, 'train/loss': 2.530428647994995, 'validation/accuracy': 0.41533997654914856, 'validation/loss': 2.6711795330047607, 'validation/num_examples': 50000, 'test/accuracy': 0.32510000467300415, 'test/loss': 3.2931430339813232, 'test/num_examples': 10000, 'score': 28186.384392499924, 'total_duration': 31687.30434346199, 'accumulated_submission_time': 28186.384392499924, 'accumulated_eval_time': 3494.8005118370056, 'accumulated_logging_time': 2.661822557449341}
I0206 23:22:14.072059 139946397853440 logging_writer.py:48] [60784] accumulated_eval_time=3494.800512, accumulated_logging_time=2.661823, accumulated_submission_time=28186.384392, global_step=60784, preemption_count=0, score=28186.384392, test/accuracy=0.325100, test/loss=3.293143, test/num_examples=10000, total_duration=31687.304343, train/accuracy=0.440254, train/loss=2.530429, validation/accuracy=0.415340, validation/loss=2.671180, validation/num_examples=50000
I0206 23:22:20.764847 139946414638848 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.8274644017219543, loss=4.831904888153076
I0206 23:23:04.072984 139946397853440 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0436843633651733, loss=3.6862080097198486
I0206 23:23:50.274968 139946414638848 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.0597940683364868, loss=3.544776439666748
I0206 23:24:37.345983 139946397853440 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.0547116994857788, loss=3.493661642074585
I0206 23:25:23.995981 139946414638848 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.0765784978866577, loss=3.5725014209747314
I0206 23:26:10.869777 139946397853440 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.1910673379898071, loss=3.513129711151123
I0206 23:26:57.399564 139946414638848 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.8696190714836121, loss=5.800535202026367
I0206 23:27:44.395265 139946397853440 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.6913130879402161, loss=5.570342540740967
I0206 23:28:31.271919 139946414638848 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.0720958709716797, loss=3.482070207595825
I0206 23:29:14.326134 140107197974336 spec.py:321] Evaluating on the training split.
I0206 23:29:25.431064 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 23:30:05.829894 140107197974336 spec.py:349] Evaluating on the test split.
I0206 23:30:07.421998 140107197974336 submission_runner.py:408] Time since start: 32160.68s, 	Step: 61693, 	{'train/accuracy': 0.4561132788658142, 'train/loss': 2.4191908836364746, 'validation/accuracy': 0.4205799996852875, 'validation/loss': 2.6124274730682373, 'validation/num_examples': 50000, 'test/accuracy': 0.3247000277042389, 'test/loss': 3.2571167945861816, 'test/num_examples': 10000, 'score': 28606.574649333954, 'total_duration': 32160.68154025078, 'accumulated_submission_time': 28606.574649333954, 'accumulated_eval_time': 3547.8963441848755, 'accumulated_logging_time': 2.700721263885498}
I0206 23:30:07.451856 139946397853440 logging_writer.py:48] [61693] accumulated_eval_time=3547.896344, accumulated_logging_time=2.700721, accumulated_submission_time=28606.574649, global_step=61693, preemption_count=0, score=28606.574649, test/accuracy=0.324700, test/loss=3.257117, test/num_examples=10000, total_duration=32160.681540, train/accuracy=0.456113, train/loss=2.419191, validation/accuracy=0.420580, validation/loss=2.612427, validation/num_examples=50000
I0206 23:30:10.611123 139946414638848 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.6777023673057556, loss=5.707884311676025
I0206 23:30:53.154656 139946397853440 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.993394672870636, loss=3.8396193981170654
I0206 23:31:39.489219 139946414638848 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.9066577553749084, loss=4.79280948638916
I0206 23:32:26.111635 139946397853440 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.0289314985275269, loss=3.4401793479919434
I0206 23:33:12.707441 139946414638848 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.991875410079956, loss=3.501492738723755
I0206 23:33:59.084792 139946397853440 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.0342583656311035, loss=3.9207777976989746
I0206 23:34:45.924845 139946414638848 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.8777170777320862, loss=4.866626262664795
I0206 23:35:32.763777 139946397853440 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.8571684956550598, loss=3.976043701171875
I0206 23:36:19.531207 139946414638848 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.036960482597351, loss=4.473592281341553
I0206 23:37:06.415392 139946397853440 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0198208093643188, loss=3.405690908432007
I0206 23:37:07.481962 140107197974336 spec.py:321] Evaluating on the training split.
I0206 23:37:19.650671 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 23:37:59.354010 140107197974336 spec.py:349] Evaluating on the test split.
I0206 23:38:00.949989 140107197974336 submission_runner.py:408] Time since start: 32634.21s, 	Step: 62604, 	{'train/accuracy': 0.4695117175579071, 'train/loss': 2.3920223712921143, 'validation/accuracy': 0.4214800000190735, 'validation/loss': 2.663222074508667, 'validation/num_examples': 50000, 'test/accuracy': 0.31950002908706665, 'test/loss': 3.280048131942749, 'test/num_examples': 10000, 'score': 29026.541818618774, 'total_duration': 32634.209529399872, 'accumulated_submission_time': 29026.541818618774, 'accumulated_eval_time': 3601.3643288612366, 'accumulated_logging_time': 2.742002010345459}
I0206 23:38:00.976534 139946414638848 logging_writer.py:48] [62604] accumulated_eval_time=3601.364329, accumulated_logging_time=2.742002, accumulated_submission_time=29026.541819, global_step=62604, preemption_count=0, score=29026.541819, test/accuracy=0.319500, test/loss=3.280048, test/num_examples=10000, total_duration=32634.209529, train/accuracy=0.469512, train/loss=2.392022, validation/accuracy=0.421480, validation/loss=2.663222, validation/num_examples=50000
I0206 23:38:41.510692 139946397853440 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.859222948551178, loss=5.781612396240234
I0206 23:39:27.821456 139946414638848 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.8783605098724365, loss=3.9518017768859863
I0206 23:40:14.522913 139946397853440 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.1946005821228027, loss=3.509305238723755
I0206 23:41:00.987559 139946414638848 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0147608518600464, loss=3.5786097049713135
I0206 23:41:47.803467 139946397853440 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.1980348825454712, loss=3.645339012145996
I0206 23:42:34.673372 139946414638848 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.0768654346466064, loss=3.5599992275238037
I0206 23:43:21.461786 139946397853440 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0485659837722778, loss=4.174525260925293
I0206 23:44:07.995392 139946414638848 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.7786071300506592, loss=5.73555326461792
I0206 23:44:54.662812 139946397853440 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.1041951179504395, loss=4.925343990325928
I0206 23:45:01.330652 140107197974336 spec.py:321] Evaluating on the training split.
I0206 23:45:12.474956 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 23:45:51.598623 140107197974336 spec.py:349] Evaluating on the test split.
I0206 23:45:53.192977 140107197974336 submission_runner.py:408] Time since start: 33106.45s, 	Step: 63516, 	{'train/accuracy': 0.43568357825279236, 'train/loss': 2.5817770957946777, 'validation/accuracy': 0.41037997603416443, 'validation/loss': 2.7430014610290527, 'validation/num_examples': 50000, 'test/accuracy': 0.3118000030517578, 'test/loss': 3.331474781036377, 'test/num_examples': 10000, 'score': 29446.83342576027, 'total_duration': 33106.452523231506, 'accumulated_submission_time': 29446.83342576027, 'accumulated_eval_time': 3653.226641178131, 'accumulated_logging_time': 2.778369903564453}
I0206 23:45:53.218374 139946414638848 logging_writer.py:48] [63516] accumulated_eval_time=3653.226641, accumulated_logging_time=2.778370, accumulated_submission_time=29446.833426, global_step=63516, preemption_count=0, score=29446.833426, test/accuracy=0.311800, test/loss=3.331475, test/num_examples=10000, total_duration=33106.452523, train/accuracy=0.435684, train/loss=2.581777, validation/accuracy=0.410380, validation/loss=2.743001, validation/num_examples=50000
I0206 23:46:28.393409 139946397853440 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.0729209184646606, loss=3.419118642807007
I0206 23:47:14.936539 139946414638848 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.9090313911437988, loss=4.463162422180176
I0206 23:48:02.159427 139946397853440 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.3228341341018677, loss=3.5426089763641357
I0206 23:48:48.951761 139946414638848 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.839874804019928, loss=5.0706281661987305
I0206 23:49:35.756255 139946397853440 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.147536039352417, loss=3.4740824699401855
I0206 23:50:22.944423 139946414638848 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.9585501551628113, loss=3.7178540229797363
I0206 23:51:09.792792 139946397853440 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.0595391988754272, loss=3.3445510864257812
I0206 23:51:56.657219 139946414638848 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.072799563407898, loss=3.5901944637298584
I0206 23:52:43.648872 139946397853440 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.0421944856643677, loss=3.3769850730895996
I0206 23:52:53.655706 140107197974336 spec.py:321] Evaluating on the training split.
I0206 23:53:05.243912 140107197974336 spec.py:333] Evaluating on the validation split.
I0206 23:53:46.348427 140107197974336 spec.py:349] Evaluating on the test split.
I0206 23:53:47.950580 140107197974336 submission_runner.py:408] Time since start: 33581.21s, 	Step: 64423, 	{'train/accuracy': 0.4575585722923279, 'train/loss': 2.4170541763305664, 'validation/accuracy': 0.42625999450683594, 'validation/loss': 2.5896904468536377, 'validation/num_examples': 50000, 'test/accuracy': 0.33390000462532043, 'test/loss': 3.209775447845459, 'test/num_examples': 10000, 'score': 29867.208926677704, 'total_duration': 33581.21010494232, 'accumulated_submission_time': 29867.208926677704, 'accumulated_eval_time': 3707.521521806717, 'accumulated_logging_time': 2.814645767211914}
I0206 23:53:47.982558 139946414638848 logging_writer.py:48] [64423] accumulated_eval_time=3707.521522, accumulated_logging_time=2.814646, accumulated_submission_time=29867.208927, global_step=64423, preemption_count=0, score=29867.208927, test/accuracy=0.333900, test/loss=3.209775, test/num_examples=10000, total_duration=33581.210105, train/accuracy=0.457559, train/loss=2.417054, validation/accuracy=0.426260, validation/loss=2.589690, validation/num_examples=50000
I0206 23:54:19.906501 139946397853440 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.0363540649414062, loss=3.505051851272583
I0206 23:55:06.365457 139946414638848 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.1137607097625732, loss=3.5342299938201904
I0206 23:55:53.139332 139946397853440 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.9716464281082153, loss=3.4662928581237793
I0206 23:56:39.957409 139946414638848 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.0041452646255493, loss=3.419908046722412
I0206 23:57:26.849009 139946397853440 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.0322991609573364, loss=4.28168249130249
I0206 23:58:13.847078 139946414638848 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.8929400444030762, loss=5.889801979064941
I0206 23:59:00.978834 139946397853440 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.7844119071960449, loss=5.026102542877197
I0206 23:59:47.897883 139946414638848 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.8085566759109497, loss=5.2428178787231445
I0207 00:00:34.799730 139946397853440 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.9396445155143738, loss=3.7776565551757812
I0207 00:00:48.152194 140107197974336 spec.py:321] Evaluating on the training split.
I0207 00:00:59.172955 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 00:01:39.259619 140107197974336 spec.py:349] Evaluating on the test split.
I0207 00:01:40.859854 140107197974336 submission_runner.py:408] Time since start: 34054.12s, 	Step: 65330, 	{'train/accuracy': 0.4688476324081421, 'train/loss': 2.3632867336273193, 'validation/accuracy': 0.42549997568130493, 'validation/loss': 2.597842216491699, 'validation/num_examples': 50000, 'test/accuracy': 0.3272000253200531, 'test/loss': 3.247157573699951, 'test/num_examples': 10000, 'score': 30287.316556453705, 'total_duration': 34054.11939907074, 'accumulated_submission_time': 30287.316556453705, 'accumulated_eval_time': 3760.2291600704193, 'accumulated_logging_time': 2.8578405380249023}
I0207 00:01:40.886088 139946414638848 logging_writer.py:48] [65330] accumulated_eval_time=3760.229160, accumulated_logging_time=2.857841, accumulated_submission_time=30287.316556, global_step=65330, preemption_count=0, score=30287.316556, test/accuracy=0.327200, test/loss=3.247158, test/num_examples=10000, total_duration=34054.119399, train/accuracy=0.468848, train/loss=2.363287, validation/accuracy=0.425500, validation/loss=2.597842, validation/num_examples=50000
I0207 00:02:09.458831 139946397853440 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.151812195777893, loss=3.6638388633728027
I0207 00:02:55.692650 139946414638848 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.123674750328064, loss=3.609254837036133
I0207 00:03:42.616333 139946397853440 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.9152270555496216, loss=3.844604969024658
I0207 00:04:29.535570 139946414638848 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.9859919548034668, loss=3.6503806114196777
I0207 00:05:16.434359 139946397853440 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.1986534595489502, loss=3.623131036758423
I0207 00:06:02.956900 139946414638848 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.0137954950332642, loss=3.4458489418029785
I0207 00:06:49.658552 139946397853440 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.0987789630889893, loss=3.6170248985290527
I0207 00:07:36.321577 139946414638848 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.1568918228149414, loss=3.6224615573883057
I0207 00:08:22.988857 139946397853440 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.9483728408813477, loss=4.492624759674072
I0207 00:08:40.933058 140107197974336 spec.py:321] Evaluating on the training split.
I0207 00:08:51.933551 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 00:09:31.270071 140107197974336 spec.py:349] Evaluating on the test split.
I0207 00:09:32.869694 140107197974336 submission_runner.py:408] Time since start: 34526.13s, 	Step: 66240, 	{'train/accuracy': 0.4552929699420929, 'train/loss': 2.4185917377471924, 'validation/accuracy': 0.4275999963283539, 'validation/loss': 2.574275016784668, 'validation/num_examples': 50000, 'test/accuracy': 0.33240002393722534, 'test/loss': 3.1866111755371094, 'test/num_examples': 10000, 'score': 30707.300862312317, 'total_duration': 34526.12924003601, 'accumulated_submission_time': 30707.300862312317, 'accumulated_eval_time': 3812.165778875351, 'accumulated_logging_time': 2.89416766166687}
I0207 00:09:32.898313 139946414638848 logging_writer.py:48] [66240] accumulated_eval_time=3812.165779, accumulated_logging_time=2.894168, accumulated_submission_time=30707.300862, global_step=66240, preemption_count=0, score=30707.300862, test/accuracy=0.332400, test/loss=3.186611, test/num_examples=10000, total_duration=34526.129240, train/accuracy=0.455293, train/loss=2.418592, validation/accuracy=0.427600, validation/loss=2.574275, validation/num_examples=50000
I0207 00:09:56.987067 139946397853440 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.0454685688018799, loss=3.597966194152832
I0207 00:10:43.384428 139946414638848 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.896558940410614, loss=5.106205463409424
I0207 00:11:30.160849 139946397853440 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.8885314464569092, loss=5.81683349609375
I0207 00:12:17.033949 139946414638848 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.0565450191497803, loss=3.6100997924804688
I0207 00:13:03.799319 139946397853440 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.9705397486686707, loss=5.84014368057251
I0207 00:13:50.421925 139946414638848 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.3390488624572754, loss=3.4535863399505615
I0207 00:14:37.141822 139946397853440 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.3336029052734375, loss=3.837451457977295
I0207 00:15:24.108917 139946414638848 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.0021939277648926, loss=3.833843946456909
I0207 00:16:11.190526 139946397853440 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.9825841784477234, loss=4.990303039550781
I0207 00:16:33.306242 140107197974336 spec.py:321] Evaluating on the training split.
I0207 00:16:44.312140 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 00:17:26.143117 140107197974336 spec.py:349] Evaluating on the test split.
I0207 00:17:27.749085 140107197974336 submission_runner.py:408] Time since start: 35001.01s, 	Step: 67149, 	{'train/accuracy': 0.4733007848262787, 'train/loss': 2.328538417816162, 'validation/accuracy': 0.4402399957180023, 'validation/loss': 2.4984281063079834, 'validation/num_examples': 50000, 'test/accuracy': 0.332800030708313, 'test/loss': 3.1850574016571045, 'test/num_examples': 10000, 'score': 31127.646122694016, 'total_duration': 35001.00862288475, 'accumulated_submission_time': 31127.646122694016, 'accumulated_eval_time': 3866.6085917949677, 'accumulated_logging_time': 2.9339511394500732}
I0207 00:17:27.778653 139946414638848 logging_writer.py:48] [67149] accumulated_eval_time=3866.608592, accumulated_logging_time=2.933951, accumulated_submission_time=31127.646123, global_step=67149, preemption_count=0, score=31127.646123, test/accuracy=0.332800, test/loss=3.185057, test/num_examples=10000, total_duration=35001.008623, train/accuracy=0.473301, train/loss=2.328538, validation/accuracy=0.440240, validation/loss=2.498428, validation/num_examples=50000
I0207 00:17:48.226565 139946397853440 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.8294814825057983, loss=5.779513835906982
I0207 00:18:33.885181 139946414638848 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1585301160812378, loss=3.600249767303467
I0207 00:19:20.765822 139946397853440 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.0451987981796265, loss=3.4313862323760986
I0207 00:20:07.689715 139946414638848 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.0279619693756104, loss=3.357334613800049
I0207 00:20:54.420421 139946397853440 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.8322058320045471, loss=4.587365627288818
I0207 00:21:41.303858 139946414638848 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.9475148320198059, loss=4.32136869430542
I0207 00:22:28.145461 139946397853440 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.9143197536468506, loss=5.177264213562012
I0207 00:23:14.658800 139946414638848 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.9855751991271973, loss=3.419112205505371
I0207 00:24:01.158517 139946397853440 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.0426586866378784, loss=4.0703020095825195
I0207 00:24:27.851864 140107197974336 spec.py:321] Evaluating on the training split.
I0207 00:24:39.002234 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 00:25:19.148068 140107197974336 spec.py:349] Evaluating on the test split.
I0207 00:25:20.744168 140107197974336 submission_runner.py:408] Time since start: 35474.00s, 	Step: 68058, 	{'train/accuracy': 0.4591992199420929, 'train/loss': 2.4289512634277344, 'validation/accuracy': 0.4236399829387665, 'validation/loss': 2.623819351196289, 'validation/num_examples': 50000, 'test/accuracy': 0.33340001106262207, 'test/loss': 3.2271764278411865, 'test/num_examples': 10000, 'score': 31547.656118154526, 'total_duration': 35474.00371336937, 'accumulated_submission_time': 31547.656118154526, 'accumulated_eval_time': 3919.500876426697, 'accumulated_logging_time': 2.9747848510742188}
I0207 00:25:20.773772 139946414638848 logging_writer.py:48] [68058] accumulated_eval_time=3919.500876, accumulated_logging_time=2.974785, accumulated_submission_time=31547.656118, global_step=68058, preemption_count=0, score=31547.656118, test/accuracy=0.333400, test/loss=3.227176, test/num_examples=10000, total_duration=35474.003713, train/accuracy=0.459199, train/loss=2.428951, validation/accuracy=0.423640, validation/loss=2.623819, validation/num_examples=50000
I0207 00:25:37.671804 139946397853440 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.15782630443573, loss=3.3966126441955566
I0207 00:26:22.747230 139946414638848 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.9609248042106628, loss=4.307748317718506
I0207 00:27:09.632014 139946397853440 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.0444996356964111, loss=3.263920545578003
I0207 00:27:56.606953 139946414638848 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.0951430797576904, loss=3.61761736869812
I0207 00:28:43.336037 139946397853440 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.0578762292861938, loss=3.772310733795166
I0207 00:29:30.099737 139946414638848 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.211220383644104, loss=3.9451217651367188
I0207 00:30:16.678533 139946397853440 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.8305859565734863, loss=5.1700520515441895
I0207 00:31:03.545458 139946414638848 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.0215822458267212, loss=3.8171725273132324
I0207 00:31:50.212418 139946397853440 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.7705654501914978, loss=5.7491254806518555
I0207 00:32:20.867822 140107197974336 spec.py:321] Evaluating on the training split.
I0207 00:32:31.936596 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 00:33:11.592823 140107197974336 spec.py:349] Evaluating on the test split.
I0207 00:33:13.190536 140107197974336 submission_runner.py:408] Time since start: 35946.45s, 	Step: 68967, 	{'train/accuracy': 0.46486327052116394, 'train/loss': 2.3892576694488525, 'validation/accuracy': 0.43845999240875244, 'validation/loss': 2.5289251804351807, 'validation/num_examples': 50000, 'test/accuracy': 0.3456000089645386, 'test/loss': 3.1654460430145264, 'test/num_examples': 10000, 'score': 31967.69016432762, 'total_duration': 35946.450082063675, 'accumulated_submission_time': 31967.69016432762, 'accumulated_eval_time': 3971.823595046997, 'accumulated_logging_time': 3.0140380859375}
I0207 00:33:13.221081 139946414638848 logging_writer.py:48] [68967] accumulated_eval_time=3971.823595, accumulated_logging_time=3.014038, accumulated_submission_time=31967.690164, global_step=68967, preemption_count=0, score=31967.690164, test/accuracy=0.345600, test/loss=3.165446, test/num_examples=10000, total_duration=35946.450082, train/accuracy=0.464863, train/loss=2.389258, validation/accuracy=0.438460, validation/loss=2.528925, validation/num_examples=50000
I0207 00:33:26.588412 139946397853440 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.0592501163482666, loss=3.536202907562256
I0207 00:34:11.284313 139946414638848 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.8138489127159119, loss=4.368766784667969
I0207 00:34:58.061602 139946397853440 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.0259554386138916, loss=3.402348041534424
I0207 00:35:45.169141 139946414638848 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2043969631195068, loss=3.5174121856689453
I0207 00:36:32.120728 139946397853440 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.1455708742141724, loss=3.5955464839935303
I0207 00:37:19.019393 139946414638848 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.309683918952942, loss=3.463016986846924
I0207 00:38:05.830232 139946397853440 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.8362669348716736, loss=5.769248962402344
I0207 00:38:52.331873 139946414638848 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.1709163188934326, loss=3.498434543609619
I0207 00:39:39.287977 139946397853440 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.1804331541061401, loss=3.313786506652832
I0207 00:40:13.650367 140107197974336 spec.py:321] Evaluating on the training split.
I0207 00:40:24.552622 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 00:41:06.473794 140107197974336 spec.py:349] Evaluating on the test split.
I0207 00:41:08.068913 140107197974336 submission_runner.py:408] Time since start: 36421.33s, 	Step: 69875, 	{'train/accuracy': 0.46867185831069946, 'train/loss': 2.3740696907043457, 'validation/accuracy': 0.43522000312805176, 'validation/loss': 2.5472748279571533, 'validation/num_examples': 50000, 'test/accuracy': 0.3344000279903412, 'test/loss': 3.193197011947632, 'test/num_examples': 10000, 'score': 32388.05753827095, 'total_duration': 36421.328429460526, 'accumulated_submission_time': 32388.05753827095, 'accumulated_eval_time': 4026.2420842647552, 'accumulated_logging_time': 3.0550498962402344}
I0207 00:41:08.101379 139946414638848 logging_writer.py:48] [69875] accumulated_eval_time=4026.242084, accumulated_logging_time=3.055050, accumulated_submission_time=32388.057538, global_step=69875, preemption_count=0, score=32388.057538, test/accuracy=0.334400, test/loss=3.193197, test/num_examples=10000, total_duration=36421.328429, train/accuracy=0.468672, train/loss=2.374070, validation/accuracy=0.435220, validation/loss=2.547275, validation/num_examples=50000
I0207 00:41:18.380394 139946397853440 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.9793053865432739, loss=3.499519109725952
I0207 00:42:02.303103 139946414638848 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1485518217086792, loss=3.364856719970703
I0207 00:42:49.197520 139946397853440 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.325463056564331, loss=3.7351760864257812
I0207 00:43:36.133693 139946414638848 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.2457817792892456, loss=4.083773136138916
I0207 00:44:22.610486 139946397853440 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1099507808685303, loss=5.484213352203369
I0207 00:45:09.738173 139946414638848 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.0117570161819458, loss=3.5312752723693848
I0207 00:45:56.314357 139946397853440 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.8499919176101685, loss=5.726365089416504
I0207 00:46:43.416686 139946414638848 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.0156537294387817, loss=3.654820442199707
I0207 00:47:30.546386 139946397853440 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.8714815378189087, loss=4.040946006774902
I0207 00:48:08.206048 140107197974336 spec.py:321] Evaluating on the training split.
I0207 00:48:19.524324 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 00:48:59.049518 140107197974336 spec.py:349] Evaluating on the test split.
I0207 00:49:00.645628 140107197974336 submission_runner.py:408] Time since start: 36893.91s, 	Step: 70782, 	{'train/accuracy': 0.4740038812160492, 'train/loss': 2.324805974960327, 'validation/accuracy': 0.4377000033855438, 'validation/loss': 2.5263214111328125, 'validation/num_examples': 50000, 'test/accuracy': 0.3368000090122223, 'test/loss': 3.1847870349884033, 'test/num_examples': 10000, 'score': 32808.10027337074, 'total_duration': 36893.90516161919, 'accumulated_submission_time': 32808.10027337074, 'accumulated_eval_time': 4078.6816279888153, 'accumulated_logging_time': 3.0984416007995605}
I0207 00:49:00.673461 139946414638848 logging_writer.py:48] [70782] accumulated_eval_time=4078.681628, accumulated_logging_time=3.098442, accumulated_submission_time=32808.100273, global_step=70782, preemption_count=0, score=32808.100273, test/accuracy=0.336800, test/loss=3.184787, test/num_examples=10000, total_duration=36893.905162, train/accuracy=0.474004, train/loss=2.324806, validation/accuracy=0.437700, validation/loss=2.526321, validation/num_examples=50000
I0207 00:49:08.149983 139946397853440 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.0612398386001587, loss=3.386038064956665
I0207 00:49:51.346906 139946414638848 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.9517318606376648, loss=3.505361557006836
I0207 00:50:37.978106 139946397853440 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.2887850999832153, loss=4.1775712966918945
I0207 00:51:24.822097 139946414638848 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.8342031240463257, loss=5.235455513000488
I0207 00:52:11.581156 139946397853440 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.398132562637329, loss=3.560276746749878
I0207 00:52:58.326127 139946414638848 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.0659176111221313, loss=3.537260055541992
I0207 00:53:45.119902 139946397853440 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.086039662361145, loss=3.582127571105957
I0207 00:54:32.212358 139946414638848 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.995140016078949, loss=3.4354805946350098
I0207 00:55:18.860109 139946397853440 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.0461630821228027, loss=5.85835599899292
I0207 00:56:00.995639 140107197974336 spec.py:321] Evaluating on the training split.
I0207 00:56:12.234984 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 00:56:52.633062 140107197974336 spec.py:349] Evaluating on the test split.
I0207 00:56:54.227909 140107197974336 submission_runner.py:408] Time since start: 37367.49s, 	Step: 71692, 	{'train/accuracy': 0.4591406285762787, 'train/loss': 2.428839921951294, 'validation/accuracy': 0.42800000309944153, 'validation/loss': 2.5926687717437744, 'validation/num_examples': 50000, 'test/accuracy': 0.33390000462532043, 'test/loss': 3.219151020050049, 'test/num_examples': 10000, 'score': 33228.35967874527, 'total_duration': 37367.487449645996, 'accumulated_submission_time': 33228.35967874527, 'accumulated_eval_time': 4131.9138832092285, 'accumulated_logging_time': 3.1382124423980713}
I0207 00:56:54.258228 139946414638848 logging_writer.py:48] [71692] accumulated_eval_time=4131.913883, accumulated_logging_time=3.138212, accumulated_submission_time=33228.359679, global_step=71692, preemption_count=0, score=33228.359679, test/accuracy=0.333900, test/loss=3.219151, test/num_examples=10000, total_duration=37367.487450, train/accuracy=0.459141, train/loss=2.428840, validation/accuracy=0.428000, validation/loss=2.592669, validation/num_examples=50000
I0207 00:56:57.798836 139946397853440 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.0965898036956787, loss=3.6605336666107178
I0207 00:57:40.430193 139946414638848 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.4029749631881714, loss=3.418501853942871
I0207 00:58:26.994339 139946397853440 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.132154107093811, loss=3.666743755340576
I0207 00:59:13.720857 139946414638848 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.0652462244033813, loss=3.438181161880493
I0207 01:00:00.197691 139946397853440 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.0241985321044922, loss=3.435002088546753
I0207 01:00:47.026142 139946414638848 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.2705258131027222, loss=3.4275999069213867
I0207 01:01:33.812274 139946397853440 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.867707371711731, loss=3.8802425861358643
I0207 01:02:20.992623 139946414638848 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.9623722434043884, loss=5.351529121398926
I0207 01:03:07.753403 139946397853440 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.991239607334137, loss=3.3476858139038086
I0207 01:03:54.378390 139946414638848 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.061655879020691, loss=3.612856388092041
I0207 01:03:54.391784 140107197974336 spec.py:321] Evaluating on the training split.
I0207 01:04:05.398433 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 01:04:47.341257 140107197974336 spec.py:349] Evaluating on the test split.
I0207 01:04:48.944999 140107197974336 submission_runner.py:408] Time since start: 37842.20s, 	Step: 72601, 	{'train/accuracy': 0.4733984172344208, 'train/loss': 2.335913896560669, 'validation/accuracy': 0.442220002412796, 'validation/loss': 2.501166582107544, 'validation/num_examples': 50000, 'test/accuracy': 0.3451000154018402, 'test/loss': 3.1486642360687256, 'test/num_examples': 10000, 'score': 33648.43023991585, 'total_duration': 37842.20454144478, 'accumulated_submission_time': 33648.43023991585, 'accumulated_eval_time': 4186.46707201004, 'accumulated_logging_time': 3.180236339569092}
I0207 01:04:48.972895 139946397853440 logging_writer.py:48] [72601] accumulated_eval_time=4186.467072, accumulated_logging_time=3.180236, accumulated_submission_time=33648.430240, global_step=72601, preemption_count=0, score=33648.430240, test/accuracy=0.345100, test/loss=3.148664, test/num_examples=10000, total_duration=37842.204541, train/accuracy=0.473398, train/loss=2.335914, validation/accuracy=0.442220, validation/loss=2.501167, validation/num_examples=50000
I0207 01:05:30.990369 139946414638848 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.3057540655136108, loss=3.4705870151519775
I0207 01:06:17.431725 139946397853440 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.3831582069396973, loss=3.94000506401062
I0207 01:07:04.334290 139946414638848 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.9731851816177368, loss=3.2913284301757812
I0207 01:07:50.745821 139946397853440 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.1489520072937012, loss=3.601107597351074
I0207 01:08:37.605344 139946414638848 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.8214449286460876, loss=4.236605644226074
I0207 01:09:24.377511 139946397853440 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.079285979270935, loss=5.110080242156982
I0207 01:10:11.010257 139946414638848 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.6904250383377075, loss=5.659577369689941
I0207 01:10:57.540148 139946397853440 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.178100347518921, loss=3.3322300910949707
I0207 01:11:44.323029 139946414638848 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.2475621700286865, loss=3.4053332805633545
I0207 01:11:49.100392 140107197974336 spec.py:321] Evaluating on the training split.
I0207 01:12:00.757443 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 01:12:41.815403 140107197974336 spec.py:349] Evaluating on the test split.
I0207 01:12:43.414065 140107197974336 submission_runner.py:408] Time since start: 38316.67s, 	Step: 73512, 	{'train/accuracy': 0.47892576456069946, 'train/loss': 2.335693359375, 'validation/accuracy': 0.44411998987197876, 'validation/loss': 2.5147647857666016, 'validation/num_examples': 50000, 'test/accuracy': 0.34170001745224, 'test/loss': 3.1700222492218018, 'test/num_examples': 10000, 'score': 34068.49537944794, 'total_duration': 38316.67360329628, 'accumulated_submission_time': 34068.49537944794, 'accumulated_eval_time': 4240.780715942383, 'accumulated_logging_time': 3.2186429500579834}
I0207 01:12:43.444427 139946397853440 logging_writer.py:48] [73512] accumulated_eval_time=4240.780716, accumulated_logging_time=3.218643, accumulated_submission_time=34068.495379, global_step=73512, preemption_count=0, score=34068.495379, test/accuracy=0.341700, test/loss=3.170022, test/num_examples=10000, total_duration=38316.673603, train/accuracy=0.478926, train/loss=2.335693, validation/accuracy=0.444120, validation/loss=2.514765, validation/num_examples=50000
I0207 01:13:20.518097 139946414638848 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.4230035543441772, loss=3.667635917663574
I0207 01:14:07.056006 139946397853440 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.0030686855316162, loss=3.5612382888793945
I0207 01:14:53.844970 139946414638848 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.1793292760849, loss=3.2964892387390137
I0207 01:15:40.411015 139946397853440 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.1530054807662964, loss=3.3820419311523438
I0207 01:16:27.072291 139946414638848 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.5550823211669922, loss=3.40792179107666
I0207 01:17:13.759523 139946397853440 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.0846846103668213, loss=3.5733110904693604
I0207 01:18:00.479058 139946414638848 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.7710095047950745, loss=4.447817802429199
I0207 01:18:47.341023 139946397853440 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.053623080253601, loss=3.8781092166900635
I0207 01:19:34.198236 139946414638848 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.925348162651062, loss=3.6486518383026123
I0207 01:19:43.561177 140107197974336 spec.py:321] Evaluating on the training split.
I0207 01:19:54.812581 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 01:20:32.792262 140107197974336 spec.py:349] Evaluating on the test split.
I0207 01:20:34.396886 140107197974336 submission_runner.py:408] Time since start: 38787.66s, 	Step: 74422, 	{'train/accuracy': 0.514941394329071, 'train/loss': 2.1037821769714355, 'validation/accuracy': 0.456279993057251, 'validation/loss': 2.4261739253997803, 'validation/num_examples': 50000, 'test/accuracy': 0.3525000214576721, 'test/loss': 3.0565521717071533, 'test/num_examples': 10000, 'score': 34488.55130815506, 'total_duration': 38787.65641450882, 'accumulated_submission_time': 34488.55130815506, 'accumulated_eval_time': 4291.616386175156, 'accumulated_logging_time': 3.258612632751465}
I0207 01:20:34.428402 139946397853440 logging_writer.py:48] [74422] accumulated_eval_time=4291.616386, accumulated_logging_time=3.258613, accumulated_submission_time=34488.551308, global_step=74422, preemption_count=0, score=34488.551308, test/accuracy=0.352500, test/loss=3.056552, test/num_examples=10000, total_duration=38787.656415, train/accuracy=0.514941, train/loss=2.103782, validation/accuracy=0.456280, validation/loss=2.426174, validation/num_examples=50000
I0207 01:21:06.869776 139946414638848 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.2003647089004517, loss=3.4750773906707764
I0207 01:21:53.017484 139946397853440 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.247077465057373, loss=3.4957194328308105
I0207 01:22:39.691194 139946414638848 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.1809078454971313, loss=3.5098061561584473
I0207 01:23:26.303895 139946397853440 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.9756389260292053, loss=4.20662784576416
I0207 01:24:13.350233 139946414638848 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1696863174438477, loss=3.2998528480529785
I0207 01:25:00.122622 139946397853440 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.3474892377853394, loss=3.325474739074707
I0207 01:25:47.089468 139946414638848 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.0696744918823242, loss=3.8382763862609863
I0207 01:26:33.796491 139946397853440 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.0668855905532837, loss=3.4758524894714355
I0207 01:27:20.854612 139946414638848 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.0609811544418335, loss=4.1727399826049805
I0207 01:27:34.420926 140107197974336 spec.py:321] Evaluating on the training split.
I0207 01:27:45.657983 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 01:28:23.783781 140107197974336 spec.py:349] Evaluating on the test split.
I0207 01:28:25.390265 140107197974336 submission_runner.py:408] Time since start: 39258.65s, 	Step: 75331, 	{'train/accuracy': 0.4822070300579071, 'train/loss': 2.265528440475464, 'validation/accuracy': 0.4532800018787384, 'validation/loss': 2.43257212638855, 'validation/num_examples': 50000, 'test/accuracy': 0.3541000187397003, 'test/loss': 3.067671537399292, 'test/num_examples': 10000, 'score': 34908.481731414795, 'total_duration': 39258.649804115295, 'accumulated_submission_time': 34908.481731414795, 'accumulated_eval_time': 4342.585709571838, 'accumulated_logging_time': 3.3010001182556152}
I0207 01:28:25.419338 139946397853440 logging_writer.py:48] [75331] accumulated_eval_time=4342.585710, accumulated_logging_time=3.301000, accumulated_submission_time=34908.481731, global_step=75331, preemption_count=0, score=34908.481731, test/accuracy=0.354100, test/loss=3.067672, test/num_examples=10000, total_duration=39258.649804, train/accuracy=0.482207, train/loss=2.265528, validation/accuracy=0.453280, validation/loss=2.432572, validation/num_examples=50000
I0207 01:28:53.620133 139946414638848 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.0590977668762207, loss=5.702570915222168
I0207 01:29:40.206155 139946397853440 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.92500239610672, loss=3.694727897644043
I0207 01:30:27.230045 139946414638848 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.9647438526153564, loss=5.065696716308594
I0207 01:31:13.970219 139946397853440 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.0863662958145142, loss=3.347855567932129
I0207 01:32:00.979931 139946414638848 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.0867830514907837, loss=3.2906084060668945
I0207 01:32:47.636029 139946397853440 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.9363647103309631, loss=4.289510726928711
I0207 01:33:34.620089 139946414638848 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.122847080230713, loss=3.5415940284729004
I0207 01:34:21.576371 139946397853440 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1597377061843872, loss=3.356489658355713
I0207 01:35:08.578887 139946414638848 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.210991382598877, loss=3.375211715698242
I0207 01:35:25.561721 140107197974336 spec.py:321] Evaluating on the training split.
I0207 01:35:36.469422 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 01:36:13.531269 140107197974336 spec.py:349] Evaluating on the test split.
I0207 01:36:15.130897 140107197974336 submission_runner.py:408] Time since start: 39728.39s, 	Step: 76238, 	{'train/accuracy': 0.4829687476158142, 'train/loss': 2.3049960136413574, 'validation/accuracy': 0.44523999094963074, 'validation/loss': 2.4958457946777344, 'validation/num_examples': 50000, 'test/accuracy': 0.34310001134872437, 'test/loss': 3.1404335498809814, 'test/num_examples': 10000, 'score': 35328.56245660782, 'total_duration': 39728.390417575836, 'accumulated_submission_time': 35328.56245660782, 'accumulated_eval_time': 4392.15483045578, 'accumulated_logging_time': 3.3405954837799072}
I0207 01:36:15.160677 139946397853440 logging_writer.py:48] [76238] accumulated_eval_time=4392.154830, accumulated_logging_time=3.340595, accumulated_submission_time=35328.562457, global_step=76238, preemption_count=0, score=35328.562457, test/accuracy=0.343100, test/loss=3.140434, test/num_examples=10000, total_duration=39728.390418, train/accuracy=0.482969, train/loss=2.304996, validation/accuracy=0.445240, validation/loss=2.495846, validation/num_examples=50000
I0207 01:36:40.151753 139946414638848 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.7891835570335388, loss=5.1044511795043945
I0207 01:37:26.893099 139946397853440 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.0530356168746948, loss=3.31626296043396
I0207 01:38:13.540713 139946414638848 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.4180611371994019, loss=3.396348476409912
I0207 01:39:00.603998 139946397853440 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.0779383182525635, loss=4.124650478363037
I0207 01:39:47.246607 139946414638848 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.0932246446609497, loss=3.4249043464660645
I0207 01:40:34.368247 139946397853440 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.0206481218338013, loss=3.760538101196289
I0207 01:41:21.065669 139946414638848 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.9040343165397644, loss=4.837855815887451
I0207 01:42:07.763353 139946397853440 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.140112280845642, loss=3.4885659217834473
I0207 01:42:54.597702 139946414638848 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.142483115196228, loss=3.7693305015563965
I0207 01:43:15.149799 140107197974336 spec.py:321] Evaluating on the training split.
I0207 01:43:26.141215 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 01:44:05.502339 140107197974336 spec.py:349] Evaluating on the test split.
I0207 01:44:07.114812 140107197974336 submission_runner.py:408] Time since start: 40200.37s, 	Step: 77145, 	{'train/accuracy': 0.5133984088897705, 'train/loss': 2.1134865283966064, 'validation/accuracy': 0.45207998156547546, 'validation/loss': 2.433824062347412, 'validation/num_examples': 50000, 'test/accuracy': 0.3474000096321106, 'test/loss': 3.1019248962402344, 'test/num_examples': 10000, 'score': 35748.48908209801, 'total_duration': 40200.3743493557, 'accumulated_submission_time': 35748.48908209801, 'accumulated_eval_time': 4444.11982011795, 'accumulated_logging_time': 3.381718873977661}
I0207 01:44:07.147509 139946397853440 logging_writer.py:48] [77145] accumulated_eval_time=4444.119820, accumulated_logging_time=3.381719, accumulated_submission_time=35748.489082, global_step=77145, preemption_count=0, score=35748.489082, test/accuracy=0.347400, test/loss=3.101925, test/num_examples=10000, total_duration=40200.374349, train/accuracy=0.513398, train/loss=2.113487, validation/accuracy=0.452080, validation/loss=2.433824, validation/num_examples=50000
I0207 01:44:29.154931 139946414638848 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.2473845481872559, loss=3.4659931659698486
I0207 01:45:15.563814 139946397853440 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.0466313362121582, loss=3.5767085552215576
I0207 01:46:02.423733 139946414638848 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.039257526397705, loss=5.749059677124023
I0207 01:46:49.274896 139946397853440 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.9536908864974976, loss=3.5207552909851074
I0207 01:47:36.653959 139946414638848 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.1095079183578491, loss=3.6079554557800293
I0207 01:48:23.425649 139946397853440 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.0343307256698608, loss=3.4022371768951416
I0207 01:49:10.374022 139946414638848 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.9226910471916199, loss=4.941445350646973
I0207 01:49:57.112144 139946397853440 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.9874271750450134, loss=5.6682305335998535
I0207 01:50:43.941125 139946414638848 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.9885112643241882, loss=3.082899808883667
I0207 01:51:07.312271 140107197974336 spec.py:321] Evaluating on the training split.
I0207 01:51:18.319357 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 01:51:57.034910 140107197974336 spec.py:349] Evaluating on the test split.
I0207 01:51:58.629582 140107197974336 submission_runner.py:408] Time since start: 40671.89s, 	Step: 78051, 	{'train/accuracy': 0.4828515648841858, 'train/loss': 2.2945902347564697, 'validation/accuracy': 0.4523399770259857, 'validation/loss': 2.4661505222320557, 'validation/num_examples': 50000, 'test/accuracy': 0.3499000072479248, 'test/loss': 3.089308261871338, 'test/num_examples': 10000, 'score': 36168.59254050255, 'total_duration': 40671.889099121094, 'accumulated_submission_time': 36168.59254050255, 'accumulated_eval_time': 4495.4370748996735, 'accumulated_logging_time': 3.4243533611297607}
I0207 01:51:58.658251 139946397853440 logging_writer.py:48] [78051] accumulated_eval_time=4495.437075, accumulated_logging_time=3.424353, accumulated_submission_time=36168.592541, global_step=78051, preemption_count=0, score=36168.592541, test/accuracy=0.349900, test/loss=3.089308, test/num_examples=10000, total_duration=40671.889099, train/accuracy=0.482852, train/loss=2.294590, validation/accuracy=0.452340, validation/loss=2.466151, validation/num_examples=50000
I0207 01:52:18.323861 139946414638848 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.9310938715934753, loss=4.334726333618164
I0207 01:53:03.843764 139946397853440 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.18559992313385, loss=3.383270263671875
I0207 01:53:50.667685 139946414638848 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.106972098350525, loss=3.3151822090148926
I0207 01:54:37.561133 139946397853440 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.144573450088501, loss=3.2863247394561768
I0207 01:55:24.142796 139946414638848 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.149512767791748, loss=3.2698206901550293
I0207 01:56:10.759114 139946397853440 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.2505213022232056, loss=3.383411169052124
I0207 01:56:57.393729 139946414638848 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.1719337701797485, loss=3.3902764320373535
I0207 01:57:44.110344 139946397853440 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.1993560791015625, loss=3.3655173778533936
I0207 01:58:31.360676 139946414638848 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.9751033782958984, loss=5.680028915405273
I0207 01:58:58.933915 140107197974336 spec.py:321] Evaluating on the training split.
I0207 01:59:10.013951 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 01:59:48.688982 140107197974336 spec.py:349] Evaluating on the test split.
I0207 01:59:50.291342 140107197974336 submission_runner.py:408] Time since start: 41143.55s, 	Step: 78960, 	{'train/accuracy': 0.4942382574081421, 'train/loss': 2.2199559211730957, 'validation/accuracy': 0.45805999636650085, 'validation/loss': 2.4189834594726562, 'validation/num_examples': 50000, 'test/accuracy': 0.34550002217292786, 'test/loss': 3.0912413597106934, 'test/num_examples': 10000, 'score': 36588.80544137955, 'total_duration': 41143.55085515976, 'accumulated_submission_time': 36588.80544137955, 'accumulated_eval_time': 4546.794453859329, 'accumulated_logging_time': 3.4637033939361572}
I0207 01:59:50.327374 139946397853440 logging_writer.py:48] [78960] accumulated_eval_time=4546.794454, accumulated_logging_time=3.463703, accumulated_submission_time=36588.805441, global_step=78960, preemption_count=0, score=36588.805441, test/accuracy=0.345500, test/loss=3.091241, test/num_examples=10000, total_duration=41143.550855, train/accuracy=0.494238, train/loss=2.219956, validation/accuracy=0.458060, validation/loss=2.418983, validation/num_examples=50000
I0207 02:00:06.462635 139946414638848 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.0447694063186646, loss=5.774224281311035
I0207 02:00:51.662091 139946397853440 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.0372475385665894, loss=3.2948384284973145
I0207 02:01:38.792934 139946414638848 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.0444960594177246, loss=3.4932138919830322
I0207 02:02:26.136662 139946397853440 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.785173773765564, loss=5.110824108123779
I0207 02:03:13.134770 139946414638848 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.0742570161819458, loss=3.2522668838500977
I0207 02:04:00.253177 139946397853440 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.0056315660476685, loss=5.7656402587890625
I0207 02:04:47.350993 139946414638848 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.9183864593505859, loss=5.4665374755859375
I0207 02:05:34.318812 139946397853440 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.8970745801925659, loss=5.437752723693848
I0207 02:06:21.301690 139946414638848 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.1229697465896606, loss=3.5248541831970215
I0207 02:06:50.458626 140107197974336 spec.py:321] Evaluating on the training split.
I0207 02:07:01.791593 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 02:07:42.493110 140107197974336 spec.py:349] Evaluating on the test split.
I0207 02:07:44.091817 140107197974336 submission_runner.py:408] Time since start: 41617.35s, 	Step: 79864, 	{'train/accuracy': 0.5013476610183716, 'train/loss': 2.2078988552093506, 'validation/accuracy': 0.4576599895954132, 'validation/loss': 2.443406581878662, 'validation/num_examples': 50000, 'test/accuracy': 0.3482000231742859, 'test/loss': 3.0952043533325195, 'test/num_examples': 10000, 'score': 37008.87369513512, 'total_duration': 41617.35136270523, 'accumulated_submission_time': 37008.87369513512, 'accumulated_eval_time': 4600.427646636963, 'accumulated_logging_time': 3.510913848876953}
I0207 02:07:44.122757 139946397853440 logging_writer.py:48] [79864] accumulated_eval_time=4600.427647, accumulated_logging_time=3.510914, accumulated_submission_time=37008.873695, global_step=79864, preemption_count=0, score=37008.873695, test/accuracy=0.348200, test/loss=3.095204, test/num_examples=10000, total_duration=41617.351363, train/accuracy=0.501348, train/loss=2.207899, validation/accuracy=0.457660, validation/loss=2.443407, validation/num_examples=50000
I0207 02:07:58.672265 139946414638848 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.099128007888794, loss=3.4711785316467285
I0207 02:08:43.416628 139946397853440 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.2510136365890503, loss=3.535560369491577
I0207 02:09:30.304605 139946414638848 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.8591544032096863, loss=4.286446571350098
I0207 02:10:17.285355 139946397853440 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.1672240495681763, loss=3.354522705078125
I0207 02:11:04.102364 139946414638848 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.3313533067703247, loss=3.4535491466522217
I0207 02:11:50.765122 139946397853440 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.1225318908691406, loss=3.663522243499756
I0207 02:12:37.299439 139946414638848 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.9460504055023193, loss=4.698855876922607
I0207 02:13:24.146047 139946397853440 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.039845585823059, loss=3.2588980197906494
I0207 02:14:11.054421 139946414638848 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.27730131149292, loss=3.28257417678833
I0207 02:14:44.424087 140107197974336 spec.py:321] Evaluating on the training split.
I0207 02:14:55.442526 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 02:15:35.805473 140107197974336 spec.py:349] Evaluating on the test split.
I0207 02:15:37.397838 140107197974336 submission_runner.py:408] Time since start: 42090.66s, 	Step: 80773, 	{'train/accuracy': 0.4915429651737213, 'train/loss': 2.2458813190460205, 'validation/accuracy': 0.45997998118400574, 'validation/loss': 2.409734010696411, 'validation/num_examples': 50000, 'test/accuracy': 0.35500001907348633, 'test/loss': 3.052574396133423, 'test/num_examples': 10000, 'score': 37429.113872528076, 'total_duration': 42090.65738582611, 'accumulated_submission_time': 37429.113872528076, 'accumulated_eval_time': 4653.40140414238, 'accumulated_logging_time': 3.551713705062866}
I0207 02:15:37.430041 139946397853440 logging_writer.py:48] [80773] accumulated_eval_time=4653.401404, accumulated_logging_time=3.551714, accumulated_submission_time=37429.113873, global_step=80773, preemption_count=0, score=37429.113873, test/accuracy=0.355000, test/loss=3.052574, test/num_examples=10000, total_duration=42090.657386, train/accuracy=0.491543, train/loss=2.245881, validation/accuracy=0.459980, validation/loss=2.409734, validation/num_examples=50000
I0207 02:15:48.443764 139946414638848 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.9597548246383667, loss=5.1458892822265625
I0207 02:16:32.302015 139946397853440 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.259034514427185, loss=3.367478370666504
I0207 02:17:19.005861 139946414638848 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.0135184526443481, loss=3.610642910003662
I0207 02:18:06.161269 139946397853440 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.9926632642745972, loss=3.6470565795898438
I0207 02:18:52.788116 139946414638848 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.8821156620979309, loss=4.966062545776367
I0207 02:19:39.657707 139946397853440 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.089455485343933, loss=3.2949581146240234
I0207 02:20:26.735059 139946414638848 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.1469464302062988, loss=3.214343547821045
I0207 02:21:13.562208 139946397853440 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.0041756629943848, loss=3.773872137069702
I0207 02:22:00.449769 139946414638848 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.9798232316970825, loss=3.2891478538513184
I0207 02:22:37.666332 140107197974336 spec.py:321] Evaluating on the training split.
I0207 02:22:48.685560 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 02:23:27.991666 140107197974336 spec.py:349] Evaluating on the test split.
I0207 02:23:29.596651 140107197974336 submission_runner.py:408] Time since start: 42562.86s, 	Step: 81681, 	{'train/accuracy': 0.49703124165534973, 'train/loss': 2.2210443019866943, 'validation/accuracy': 0.4611999988555908, 'validation/loss': 2.4077775478363037, 'validation/num_examples': 50000, 'test/accuracy': 0.35860002040863037, 'test/loss': 3.044772148132324, 'test/num_examples': 10000, 'score': 37849.28627562523, 'total_duration': 42562.85619473457, 'accumulated_submission_time': 37849.28627562523, 'accumulated_eval_time': 4705.331708192825, 'accumulated_logging_time': 3.5972063541412354}
I0207 02:23:29.630399 139946397853440 logging_writer.py:48] [81681] accumulated_eval_time=4705.331708, accumulated_logging_time=3.597206, accumulated_submission_time=37849.286276, global_step=81681, preemption_count=0, score=37849.286276, test/accuracy=0.358600, test/loss=3.044772, test/num_examples=10000, total_duration=42562.856195, train/accuracy=0.497031, train/loss=2.221044, validation/accuracy=0.461200, validation/loss=2.407778, validation/num_examples=50000
I0207 02:23:37.493346 139946414638848 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.013852834701538, loss=4.091513633728027
I0207 02:24:21.306301 139946397853440 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.193962574005127, loss=3.3218281269073486
I0207 02:25:08.311743 139946414638848 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.8731449842453003, loss=5.322835922241211
I0207 02:25:55.560678 139946397853440 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.0521140098571777, loss=3.1330575942993164
I0207 02:26:42.368365 139946414638848 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.2053427696228027, loss=3.317290782928467
I0207 02:27:29.058743 139946397853440 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.0824217796325684, loss=3.3161580562591553
I0207 02:28:16.012901 139946414638848 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.0216325521469116, loss=4.368417263031006
I0207 02:29:03.136380 139946397853440 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.0893547534942627, loss=3.3885791301727295
I0207 02:29:50.054277 139946414638848 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.0578948259353638, loss=4.814425468444824
I0207 02:30:29.847626 140107197974336 spec.py:321] Evaluating on the training split.
I0207 02:30:41.051522 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 02:31:18.981044 140107197974336 spec.py:349] Evaluating on the test split.
I0207 02:31:20.585128 140107197974336 submission_runner.py:408] Time since start: 43033.84s, 	Step: 82586, 	{'train/accuracy': 0.5177343487739563, 'train/loss': 2.106281042098999, 'validation/accuracy': 0.47005999088287354, 'validation/loss': 2.3485488891601562, 'validation/num_examples': 50000, 'test/accuracy': 0.3614000082015991, 'test/loss': 3.003753185272217, 'test/num_examples': 10000, 'score': 38269.44111919403, 'total_duration': 43033.84465241432, 'accumulated_submission_time': 38269.44111919403, 'accumulated_eval_time': 4756.069163322449, 'accumulated_logging_time': 3.641781806945801}
I0207 02:31:20.618724 139946397853440 logging_writer.py:48] [82586] accumulated_eval_time=4756.069163, accumulated_logging_time=3.641782, accumulated_submission_time=38269.441119, global_step=82586, preemption_count=0, score=38269.441119, test/accuracy=0.361400, test/loss=3.003753, test/num_examples=10000, total_duration=43033.844652, train/accuracy=0.517734, train/loss=2.106281, validation/accuracy=0.470060, validation/loss=2.348549, validation/num_examples=50000
I0207 02:31:26.515477 139946414638848 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.1872949600219727, loss=3.2960259914398193
I0207 02:32:09.609662 139946397853440 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.0760308504104614, loss=3.8674066066741943
I0207 02:32:55.966825 139946414638848 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.8594584465026855, loss=5.654273986816406
I0207 02:33:42.772945 139946397853440 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.202798843383789, loss=3.6427407264709473
I0207 02:34:29.923329 139946414638848 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.9626985788345337, loss=4.726522922515869
I0207 02:35:16.434057 139946397853440 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.154971718788147, loss=3.3878366947174072
I0207 02:36:03.416983 139946414638848 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.9927664995193481, loss=3.578145742416382
I0207 02:36:49.976409 139946397853440 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.2021865844726562, loss=3.1705751419067383
I0207 02:37:36.499199 139946414638848 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.1577270030975342, loss=3.3430697917938232
I0207 02:38:20.903899 140107197974336 spec.py:321] Evaluating on the training split.
I0207 02:38:31.743010 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 02:39:13.765061 140107197974336 spec.py:349] Evaluating on the test split.
I0207 02:39:15.365808 140107197974336 submission_runner.py:408] Time since start: 43508.63s, 	Step: 83497, 	{'train/accuracy': 0.4992382824420929, 'train/loss': 2.189939260482788, 'validation/accuracy': 0.4672999978065491, 'validation/loss': 2.3583219051361084, 'validation/num_examples': 50000, 'test/accuracy': 0.3677000105381012, 'test/loss': 2.9964702129364014, 'test/num_examples': 10000, 'score': 38689.66334319115, 'total_duration': 43508.62535381317, 'accumulated_submission_time': 38689.66334319115, 'accumulated_eval_time': 4810.5310571193695, 'accumulated_logging_time': 3.6866133213043213}
I0207 02:39:15.395273 139946397853440 logging_writer.py:48] [83497] accumulated_eval_time=4810.531057, accumulated_logging_time=3.686613, accumulated_submission_time=38689.663343, global_step=83497, preemption_count=0, score=38689.663343, test/accuracy=0.367700, test/loss=2.996470, test/num_examples=10000, total_duration=43508.625354, train/accuracy=0.499238, train/loss=2.189939, validation/accuracy=0.467300, validation/loss=2.358322, validation/num_examples=50000
I0207 02:39:16.973497 139946414638848 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.8355960249900818, loss=4.4495744705200195
I0207 02:39:59.709174 139946397853440 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.3389225006103516, loss=3.36568021774292
I0207 02:40:46.727723 139946414638848 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.1090753078460693, loss=3.2303056716918945
I0207 02:41:34.016342 139946397853440 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.8367525339126587, loss=5.221261501312256
I0207 02:42:21.359543 139946414638848 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.9048281908035278, loss=4.183055400848389
I0207 02:43:08.454218 139946397853440 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.0410114526748657, loss=3.395811080932617
I0207 02:43:55.240499 139946414638848 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.9977201819419861, loss=5.643665790557861
I0207 02:44:42.615735 139946397853440 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.1749719381332397, loss=3.2420501708984375
I0207 02:45:29.722279 139946414638848 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.9362785816192627, loss=5.767622947692871
I0207 02:46:15.591413 140107197974336 spec.py:321] Evaluating on the training split.
I0207 02:46:26.713937 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 02:47:05.664963 140107197974336 spec.py:349] Evaluating on the test split.
I0207 02:47:07.272006 140107197974336 submission_runner.py:408] Time since start: 43980.53s, 	Step: 84399, 	{'train/accuracy': 0.5051953196525574, 'train/loss': 2.1985788345336914, 'validation/accuracy': 0.4702000021934509, 'validation/loss': 2.3659684658050537, 'validation/num_examples': 50000, 'test/accuracy': 0.3638000190258026, 'test/loss': 3.0029189586639404, 'test/num_examples': 10000, 'score': 39109.79868769646, 'total_duration': 43980.53152704239, 'accumulated_submission_time': 39109.79868769646, 'accumulated_eval_time': 4862.211612701416, 'accumulated_logging_time': 3.7260732650756836}
I0207 02:47:07.305624 139946397853440 logging_writer.py:48] [84399] accumulated_eval_time=4862.211613, accumulated_logging_time=3.726073, accumulated_submission_time=39109.798688, global_step=84399, preemption_count=0, score=39109.798688, test/accuracy=0.363800, test/loss=3.002919, test/num_examples=10000, total_duration=43980.531527, train/accuracy=0.505195, train/loss=2.198579, validation/accuracy=0.470200, validation/loss=2.365968, validation/num_examples=50000
I0207 02:47:08.101143 139946414638848 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.9357597231864929, loss=3.94783353805542
I0207 02:47:50.149377 139946397853440 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.8962070941925049, loss=5.011300086975098
I0207 02:48:36.609214 139946414638848 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.1360199451446533, loss=3.5386555194854736
I0207 02:49:23.702240 139946397853440 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.9084932208061218, loss=5.262478351593018
I0207 02:50:10.393330 139946414638848 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.0966558456420898, loss=3.4134361743927
I0207 02:50:57.274903 139946397853440 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.012960433959961, loss=3.418484687805176
I0207 02:51:44.074355 139946414638848 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.0901167392730713, loss=3.233640432357788
I0207 02:52:31.076654 139946397853440 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.1176782846450806, loss=3.3140604496002197
I0207 02:53:17.700585 139946414638848 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.0911065340042114, loss=3.8394105434417725
I0207 02:54:04.662922 139946397853440 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.9150516390800476, loss=5.119935989379883
I0207 02:54:07.563858 140107197974336 spec.py:321] Evaluating on the training split.
I0207 02:54:19.609328 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 02:54:59.026584 140107197974336 spec.py:349] Evaluating on the test split.
I0207 02:55:00.626559 140107197974336 submission_runner.py:408] Time since start: 44453.89s, 	Step: 85308, 	{'train/accuracy': 0.505664050579071, 'train/loss': 2.1896252632141113, 'validation/accuracy': 0.4622199833393097, 'validation/loss': 2.4180538654327393, 'validation/num_examples': 50000, 'test/accuracy': 0.35630002617836, 'test/loss': 3.0596253871917725, 'test/num_examples': 10000, 'score': 39529.994701862335, 'total_duration': 44453.88608646393, 'accumulated_submission_time': 39529.994701862335, 'accumulated_eval_time': 4915.274274110794, 'accumulated_logging_time': 3.7699716091156006}
I0207 02:55:00.657016 139946414638848 logging_writer.py:48] [85308] accumulated_eval_time=4915.274274, accumulated_logging_time=3.769972, accumulated_submission_time=39529.994702, global_step=85308, preemption_count=0, score=39529.994702, test/accuracy=0.356300, test/loss=3.059625, test/num_examples=10000, total_duration=44453.886086, train/accuracy=0.505664, train/loss=2.189625, validation/accuracy=0.462220, validation/loss=2.418054, validation/num_examples=50000
I0207 02:55:39.683400 139946397853440 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.0616326332092285, loss=3.8312859535217285
I0207 02:56:26.181644 139946414638848 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.2863595485687256, loss=3.3245327472686768
I0207 02:57:13.344141 139946397853440 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.1129416227340698, loss=3.1495633125305176
I0207 02:58:00.223190 139946414638848 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.1293333768844604, loss=3.1743226051330566
I0207 02:58:47.107500 139946397853440 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.9834311604499817, loss=3.818514347076416
I0207 02:59:33.910006 139946414638848 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.0305604934692383, loss=4.629940986633301
I0207 03:00:21.010962 139946397853440 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.0712298154830933, loss=3.2863609790802
I0207 03:01:07.912202 139946414638848 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.0143848657608032, loss=3.119582176208496
I0207 03:01:54.809696 139946397853440 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.0161545276641846, loss=4.052489280700684
I0207 03:02:01.134364 140107197974336 spec.py:321] Evaluating on the training split.
I0207 03:02:12.208311 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 03:02:50.046061 140107197974336 spec.py:349] Evaluating on the test split.
I0207 03:02:51.644180 140107197974336 submission_runner.py:408] Time since start: 44924.90s, 	Step: 86215, 	{'train/accuracy': 0.49943357706069946, 'train/loss': 2.2269043922424316, 'validation/accuracy': 0.4710399806499481, 'validation/loss': 2.3905842304229736, 'validation/num_examples': 50000, 'test/accuracy': 0.3636000156402588, 'test/loss': 3.021517038345337, 'test/num_examples': 10000, 'score': 39950.410684108734, 'total_duration': 44924.90372800827, 'accumulated_submission_time': 39950.410684108734, 'accumulated_eval_time': 4965.784074783325, 'accumulated_logging_time': 3.8100292682647705}
I0207 03:02:51.684252 139946414638848 logging_writer.py:48] [86215] accumulated_eval_time=4965.784075, accumulated_logging_time=3.810029, accumulated_submission_time=39950.410684, global_step=86215, preemption_count=0, score=39950.410684, test/accuracy=0.363600, test/loss=3.021517, test/num_examples=10000, total_duration=44924.903728, train/accuracy=0.499434, train/loss=2.226904, validation/accuracy=0.471040, validation/loss=2.390584, validation/num_examples=50000
I0207 03:03:27.427441 139946397853440 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.9209608435630798, loss=5.255298614501953
I0207 03:04:14.301837 139946414638848 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.0884329080581665, loss=3.2318944931030273
I0207 03:05:01.762784 139946397853440 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.033667802810669, loss=3.920197010040283
I0207 03:05:48.881224 139946414638848 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.1588060855865479, loss=3.155552387237549
I0207 03:06:36.275813 139946397853440 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.158263921737671, loss=4.07867431640625
I0207 03:07:23.274400 139946414638848 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.2151696681976318, loss=3.235957622528076
I0207 03:08:10.468375 139946397853440 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.1226624250411987, loss=3.2373268604278564
I0207 03:08:57.330027 139946414638848 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.084917664527893, loss=3.1618120670318604
I0207 03:09:44.422025 139946397853440 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.2305834293365479, loss=3.1646993160247803
I0207 03:09:52.048603 140107197974336 spec.py:321] Evaluating on the training split.
I0207 03:10:03.109830 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 03:10:41.988188 140107197974336 spec.py:349] Evaluating on the test split.
I0207 03:10:43.595691 140107197974336 submission_runner.py:408] Time since start: 45396.86s, 	Step: 87118, 	{'train/accuracy': 0.5128515362739563, 'train/loss': 2.143615484237671, 'validation/accuracy': 0.48099997639656067, 'validation/loss': 2.3133881092071533, 'validation/num_examples': 50000, 'test/accuracy': 0.37530001997947693, 'test/loss': 2.977247714996338, 'test/num_examples': 10000, 'score': 40370.71117019653, 'total_duration': 45396.8552069664, 'accumulated_submission_time': 40370.71117019653, 'accumulated_eval_time': 5017.331121683121, 'accumulated_logging_time': 3.8627686500549316}
I0207 03:10:43.635087 139946414638848 logging_writer.py:48] [87118] accumulated_eval_time=5017.331122, accumulated_logging_time=3.862769, accumulated_submission_time=40370.711170, global_step=87118, preemption_count=0, score=40370.711170, test/accuracy=0.375300, test/loss=2.977248, test/num_examples=10000, total_duration=45396.855207, train/accuracy=0.512852, train/loss=2.143615, validation/accuracy=0.481000, validation/loss=2.313388, validation/num_examples=50000
I0207 03:11:17.947891 139946397853440 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.1029225587844849, loss=3.334134340286255
I0207 03:12:04.756994 139946414638848 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.1980034112930298, loss=3.310622215270996
I0207 03:12:52.107432 139946397853440 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.0197839736938477, loss=3.2874293327331543
I0207 03:13:39.144289 139946414638848 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.328466773033142, loss=3.2038967609405518
I0207 03:14:26.369501 139946397853440 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.1247467994689941, loss=3.1872777938842773
I0207 03:15:13.270731 139946414638848 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.1478211879730225, loss=3.3725533485412598
I0207 03:16:00.446151 139946397853440 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.1120811700820923, loss=3.142249822616577
I0207 03:16:47.500596 139946414638848 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.0278195142745972, loss=4.0459699630737305
I0207 03:17:34.667536 139946397853440 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.1015686988830566, loss=3.3888068199157715
I0207 03:17:43.883445 140107197974336 spec.py:321] Evaluating on the training split.
I0207 03:17:55.170406 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 03:18:33.711716 140107197974336 spec.py:349] Evaluating on the test split.
I0207 03:18:35.313951 140107197974336 submission_runner.py:408] Time since start: 45868.57s, 	Step: 88021, 	{'train/accuracy': 0.5203710794448853, 'train/loss': 2.086500883102417, 'validation/accuracy': 0.47773998975753784, 'validation/loss': 2.3107686042785645, 'validation/num_examples': 50000, 'test/accuracy': 0.38050001859664917, 'test/loss': 2.938375234603882, 'test/num_examples': 10000, 'score': 40790.89816689491, 'total_duration': 45868.57350111008, 'accumulated_submission_time': 40790.89816689491, 'accumulated_eval_time': 5068.761607885361, 'accumulated_logging_time': 3.913156747817993}
I0207 03:18:35.344594 139946414638848 logging_writer.py:48] [88021] accumulated_eval_time=5068.761608, accumulated_logging_time=3.913157, accumulated_submission_time=40790.898167, global_step=88021, preemption_count=0, score=40790.898167, test/accuracy=0.380500, test/loss=2.938375, test/num_examples=10000, total_duration=45868.573501, train/accuracy=0.520371, train/loss=2.086501, validation/accuracy=0.477740, validation/loss=2.310769, validation/num_examples=50000
I0207 03:19:08.407121 139946397853440 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.0918939113616943, loss=3.315641403198242
I0207 03:19:55.053685 139946414638848 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.9527696371078491, loss=4.461416721343994
I0207 03:20:41.710630 139946397853440 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.8670450448989868, loss=4.14766788482666
I0207 03:21:28.606486 139946414638848 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.1542047262191772, loss=3.297191619873047
I0207 03:22:15.581529 139946397853440 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.9033170938491821, loss=5.435796737670898
I0207 03:23:02.388844 139946414638848 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.220485806465149, loss=3.234527587890625
I0207 03:23:49.489069 139946397853440 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.199765682220459, loss=3.3946127891540527
I0207 03:24:36.567894 139946414638848 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.3051776885986328, loss=3.0068418979644775
I0207 03:25:23.627871 139946397853440 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.0125489234924316, loss=3.3663272857666016
I0207 03:25:35.459418 140107197974336 spec.py:321] Evaluating on the training split.
I0207 03:25:46.653860 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 03:26:25.085578 140107197974336 spec.py:349] Evaluating on the test split.
I0207 03:26:26.679542 140107197974336 submission_runner.py:408] Time since start: 46339.94s, 	Step: 88927, 	{'train/accuracy': 0.5184179544448853, 'train/loss': 2.0980348587036133, 'validation/accuracy': 0.4792400002479553, 'validation/loss': 2.289841413497925, 'validation/num_examples': 50000, 'test/accuracy': 0.3777000308036804, 'test/loss': 2.9439592361450195, 'test/num_examples': 10000, 'score': 41210.95104074478, 'total_duration': 46339.93909049034, 'accumulated_submission_time': 41210.95104074478, 'accumulated_eval_time': 5119.981722831726, 'accumulated_logging_time': 3.9541375637054443}
I0207 03:26:26.712154 139946414638848 logging_writer.py:48] [88927] accumulated_eval_time=5119.981723, accumulated_logging_time=3.954138, accumulated_submission_time=41210.951041, global_step=88927, preemption_count=0, score=41210.951041, test/accuracy=0.377700, test/loss=2.943959, test/num_examples=10000, total_duration=46339.939090, train/accuracy=0.518418, train/loss=2.098035, validation/accuracy=0.479240, validation/loss=2.289841, validation/num_examples=50000
I0207 03:26:56.650121 139946397853440 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.2267626523971558, loss=3.5614962577819824
I0207 03:27:43.184361 139946414638848 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.9954848289489746, loss=3.5721681118011475
I0207 03:28:29.789096 139946397853440 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.19260835647583, loss=3.3234152793884277
I0207 03:29:16.752833 139946414638848 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.1149475574493408, loss=3.1149559020996094
I0207 03:30:03.613384 139946397853440 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.038132905960083, loss=3.47590970993042
I0207 03:30:50.259436 139946414638848 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.1088398694992065, loss=3.2092864513397217
I0207 03:31:37.196078 139946397853440 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.9426624774932861, loss=5.737181186676025
I0207 03:32:23.958927 139946414638848 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.182268738746643, loss=3.2612767219543457
I0207 03:33:10.922268 139946397853440 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.14360511302948, loss=3.7208869457244873
I0207 03:33:26.971588 140107197974336 spec.py:321] Evaluating on the training split.
I0207 03:33:38.024540 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 03:34:17.210313 140107197974336 spec.py:349] Evaluating on the test split.
I0207 03:34:18.812632 140107197974336 submission_runner.py:408] Time since start: 46812.07s, 	Step: 89836, 	{'train/accuracy': 0.5190234184265137, 'train/loss': 2.101839303970337, 'validation/accuracy': 0.4875999987125397, 'validation/loss': 2.2763702869415283, 'validation/num_examples': 50000, 'test/accuracy': 0.3736000061035156, 'test/loss': 2.944952964782715, 'test/num_examples': 10000, 'score': 41631.14861416817, 'total_duration': 46812.072179317474, 'accumulated_submission_time': 41631.14861416817, 'accumulated_eval_time': 5171.822760105133, 'accumulated_logging_time': 3.9972646236419678}
I0207 03:34:18.842345 139946414638848 logging_writer.py:48] [89836] accumulated_eval_time=5171.822760, accumulated_logging_time=3.997265, accumulated_submission_time=41631.148614, global_step=89836, preemption_count=0, score=41631.148614, test/accuracy=0.373600, test/loss=2.944953, test/num_examples=10000, total_duration=46812.072179, train/accuracy=0.519023, train/loss=2.101839, validation/accuracy=0.487600, validation/loss=2.276370, validation/num_examples=50000
I0207 03:34:44.735619 139946397853440 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.151724100112915, loss=3.3430185317993164
I0207 03:35:31.017041 139946414638848 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.1563234329223633, loss=3.395674467086792
I0207 03:36:18.101701 139946397853440 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.2159785032272339, loss=3.1518969535827637
I0207 03:37:04.888190 139946414638848 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.1449346542358398, loss=3.1739487648010254
I0207 03:37:51.665374 139946397853440 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.1366292238235474, loss=3.1561686992645264
I0207 03:38:38.569612 139946414638848 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.126874566078186, loss=3.245884895324707
I0207 03:39:25.372957 139946397853440 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.1087454557418823, loss=3.1341638565063477
I0207 03:40:12.437745 139946414638848 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.2458271980285645, loss=3.243288993835449
I0207 03:40:59.411500 139946397853440 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.1909120082855225, loss=3.1114790439605713
I0207 03:41:19.026032 140107197974336 spec.py:321] Evaluating on the training split.
I0207 03:41:30.302250 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 03:42:11.242153 140107197974336 spec.py:349] Evaluating on the test split.
I0207 03:42:12.839451 140107197974336 submission_runner.py:408] Time since start: 47286.10s, 	Step: 90743, 	{'train/accuracy': 0.5146093368530273, 'train/loss': 2.1558313369750977, 'validation/accuracy': 0.47551998496055603, 'validation/loss': 2.3405981063842773, 'validation/num_examples': 50000, 'test/accuracy': 0.3727000057697296, 'test/loss': 2.9976208209991455, 'test/num_examples': 10000, 'score': 42051.27122282982, 'total_duration': 47286.098991155624, 'accumulated_submission_time': 42051.27122282982, 'accumulated_eval_time': 5225.636157512665, 'accumulated_logging_time': 4.036552667617798}
I0207 03:42:12.870989 139946414638848 logging_writer.py:48] [90743] accumulated_eval_time=5225.636158, accumulated_logging_time=4.036553, accumulated_submission_time=42051.271223, global_step=90743, preemption_count=0, score=42051.271223, test/accuracy=0.372700, test/loss=2.997621, test/num_examples=10000, total_duration=47286.098991, train/accuracy=0.514609, train/loss=2.155831, validation/accuracy=0.475520, validation/loss=2.340598, validation/num_examples=50000
I0207 03:42:35.668740 139946397853440 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.9250695109367371, loss=4.834938049316406
I0207 03:43:21.842044 139946414638848 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.210862636566162, loss=3.126054286956787
I0207 03:44:08.677165 139946397853440 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.153259038925171, loss=3.7985455989837646
I0207 03:44:55.527981 139946414638848 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.0121870040893555, loss=4.987436294555664
I0207 03:45:42.400493 139946397853440 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.8977569341659546, loss=4.906681537628174
I0207 03:46:29.124557 139946414638848 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.1714954376220703, loss=3.145548105239868
I0207 03:47:16.140928 139946397853440 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.9122212529182434, loss=3.767763614654541
I0207 03:48:02.738770 139946414638848 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.1088382005691528, loss=3.3487391471862793
I0207 03:48:49.626938 139946397853440 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.1448026895523071, loss=3.133427858352661
I0207 03:49:12.904145 140107197974336 spec.py:321] Evaluating on the training split.
I0207 03:49:24.103739 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 03:50:03.109101 140107197974336 spec.py:349] Evaluating on the test split.
I0207 03:50:04.713047 140107197974336 submission_runner.py:408] Time since start: 47757.97s, 	Step: 91651, 	{'train/accuracy': 0.5436718463897705, 'train/loss': 2.0345213413238525, 'validation/accuracy': 0.4791199862957001, 'validation/loss': 2.349999189376831, 'validation/num_examples': 50000, 'test/accuracy': 0.37130001187324524, 'test/loss': 2.993382453918457, 'test/num_examples': 10000, 'score': 42471.242216825485, 'total_duration': 47757.972594976425, 'accumulated_submission_time': 42471.242216825485, 'accumulated_eval_time': 5277.445053577423, 'accumulated_logging_time': 4.078348875045776}
I0207 03:50:04.746567 139946414638848 logging_writer.py:48] [91651] accumulated_eval_time=5277.445054, accumulated_logging_time=4.078349, accumulated_submission_time=42471.242217, global_step=91651, preemption_count=0, score=42471.242217, test/accuracy=0.371300, test/loss=2.993382, test/num_examples=10000, total_duration=47757.972595, train/accuracy=0.543672, train/loss=2.034521, validation/accuracy=0.479120, validation/loss=2.349999, validation/num_examples=50000
I0207 03:50:24.417578 139946397853440 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.1590855121612549, loss=3.2087669372558594
I0207 03:51:10.192064 139946414638848 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.1239509582519531, loss=5.108832836151123
I0207 03:51:57.064602 139946397853440 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.058096170425415, loss=4.0370635986328125
I0207 03:52:44.089991 139946414638848 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.3117221593856812, loss=3.1589653491973877
I0207 03:53:31.039411 139946397853440 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.1302459239959717, loss=3.8353850841522217
I0207 03:54:18.083061 139946414638848 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.051209568977356, loss=4.212486743927002
I0207 03:55:05.576371 139946397853440 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.1030570268630981, loss=3.3158645629882812
I0207 03:55:52.517111 139946414638848 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.3282697200775146, loss=3.360402822494507
I0207 03:56:39.531454 139946397853440 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.09031081199646, loss=3.021843433380127
I0207 03:57:04.973957 140107197974336 spec.py:321] Evaluating on the training split.
I0207 03:57:15.602435 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 03:57:56.457495 140107197974336 spec.py:349] Evaluating on the test split.
I0207 03:57:58.057856 140107197974336 submission_runner.py:408] Time since start: 48231.32s, 	Step: 92556, 	{'train/accuracy': 0.5254101157188416, 'train/loss': 2.0577504634857178, 'validation/accuracy': 0.4925599992275238, 'validation/loss': 2.2484467029571533, 'validation/num_examples': 50000, 'test/accuracy': 0.38120001554489136, 'test/loss': 2.9217004776000977, 'test/num_examples': 10000, 'score': 42891.40844297409, 'total_duration': 48231.31739473343, 'accumulated_submission_time': 42891.40844297409, 'accumulated_eval_time': 5330.52893781662, 'accumulated_logging_time': 4.1219470500946045}
I0207 03:57:58.091205 139946414638848 logging_writer.py:48] [92556] accumulated_eval_time=5330.528938, accumulated_logging_time=4.121947, accumulated_submission_time=42891.408443, global_step=92556, preemption_count=0, score=42891.408443, test/accuracy=0.381200, test/loss=2.921700, test/num_examples=10000, total_duration=48231.317395, train/accuracy=0.525410, train/loss=2.057750, validation/accuracy=0.492560, validation/loss=2.248447, validation/num_examples=50000
I0207 03:58:15.787728 139946397853440 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.1505475044250488, loss=3.176311731338501
I0207 03:59:01.142675 139946414638848 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.2530710697174072, loss=3.1930174827575684
I0207 03:59:47.960484 139946397853440 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.9885684847831726, loss=5.0878520011901855
I0207 04:00:35.010254 139946414638848 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.9557921886444092, loss=4.094104766845703
I0207 04:01:21.630135 139946397853440 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.2620426416397095, loss=3.2126450538635254
I0207 04:02:08.384666 139946414638848 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.08424973487854, loss=3.228008270263672
I0207 04:02:55.176474 139946397853440 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.9532268643379211, loss=5.522850513458252
I0207 04:03:41.959250 139946414638848 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.041477918624878, loss=4.955070495605469
I0207 04:04:29.016438 139946397853440 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.9287232160568237, loss=3.671286106109619
I0207 04:04:58.226930 140107197974336 spec.py:321] Evaluating on the training split.
I0207 04:05:09.662660 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 04:05:49.132670 140107197974336 spec.py:349] Evaluating on the test split.
I0207 04:05:50.725490 140107197974336 submission_runner.py:408] Time since start: 48703.99s, 	Step: 93464, 	{'train/accuracy': 0.5331054329872131, 'train/loss': 2.0151867866516113, 'validation/accuracy': 0.49487999081611633, 'validation/loss': 2.2205684185028076, 'validation/num_examples': 50000, 'test/accuracy': 0.37950003147125244, 'test/loss': 2.9080698490142822, 'test/num_examples': 10000, 'score': 43311.48286437988, 'total_duration': 48703.985027074814, 'accumulated_submission_time': 43311.48286437988, 'accumulated_eval_time': 5383.027472257614, 'accumulated_logging_time': 4.1651999950408936}
I0207 04:05:50.765458 139946414638848 logging_writer.py:48] [93464] accumulated_eval_time=5383.027472, accumulated_logging_time=4.165200, accumulated_submission_time=43311.482864, global_step=93464, preemption_count=0, score=43311.482864, test/accuracy=0.379500, test/loss=2.908070, test/num_examples=10000, total_duration=48703.985027, train/accuracy=0.533105, train/loss=2.015187, validation/accuracy=0.494880, validation/loss=2.220568, validation/num_examples=50000
I0207 04:06:05.316712 139946397853440 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.9406543374061584, loss=4.428635120391846
I0207 04:06:50.189592 139946414638848 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.8894939422607422, loss=5.442005157470703
I0207 04:07:37.057065 139946397853440 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.1550379991531372, loss=3.026797294616699
I0207 04:08:24.260312 139946414638848 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.1879452466964722, loss=3.1029398441314697
I0207 04:09:11.427318 139946397853440 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.0534093379974365, loss=3.2500345706939697
I0207 04:09:57.952855 139946414638848 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.1071335077285767, loss=3.1364524364471436
I0207 04:10:44.907042 139946397853440 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.9181157946586609, loss=4.420047283172607
I0207 04:11:31.965539 139946414638848 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.9573997259140015, loss=5.594669342041016
I0207 04:12:20.019436 139946397853440 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.0761597156524658, loss=3.11387300491333
I0207 04:12:50.949012 140107197974336 spec.py:321] Evaluating on the training split.
I0207 04:13:01.814955 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 04:13:41.789223 140107197974336 spec.py:349] Evaluating on the test split.
I0207 04:13:43.381551 140107197974336 submission_runner.py:408] Time since start: 49176.64s, 	Step: 94368, 	{'train/accuracy': 0.5615038871765137, 'train/loss': 1.8956527709960938, 'validation/accuracy': 0.5003199577331543, 'validation/loss': 2.2095954418182373, 'validation/num_examples': 50000, 'test/accuracy': 0.3873000144958496, 'test/loss': 2.886227607727051, 'test/num_examples': 10000, 'score': 43731.60246658325, 'total_duration': 49176.6410984993, 'accumulated_submission_time': 43731.60246658325, 'accumulated_eval_time': 5435.459993362427, 'accumulated_logging_time': 4.21837306022644}
I0207 04:13:43.417260 139946414638848 logging_writer.py:48] [94368] accumulated_eval_time=5435.459993, accumulated_logging_time=4.218373, accumulated_submission_time=43731.602467, global_step=94368, preemption_count=0, score=43731.602467, test/accuracy=0.387300, test/loss=2.886228, test/num_examples=10000, total_duration=49176.641098, train/accuracy=0.561504, train/loss=1.895653, validation/accuracy=0.500320, validation/loss=2.209595, validation/num_examples=50000
I0207 04:13:56.397849 139946397853440 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.8992617130279541, loss=4.942653656005859
I0207 04:14:41.259483 139946414638848 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.1156511306762695, loss=3.119912624359131
I0207 04:15:27.988725 139946397853440 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.0564844608306885, loss=3.446197271347046
I0207 04:16:14.958653 139946414638848 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.9298753142356873, loss=5.3287458419799805
I0207 04:17:01.871677 139946397853440 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.9676999449729919, loss=5.447328090667725
I0207 04:17:48.832164 139946414638848 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.3005907535552979, loss=2.993741512298584
I0207 04:18:35.849635 139946397853440 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.00947105884552, loss=5.268461227416992
I0207 04:19:23.247007 139946414638848 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.3788572549819946, loss=3.151279926300049
I0207 04:20:10.064357 139946397853440 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.9921186566352844, loss=3.8642144203186035
I0207 04:20:43.516813 140107197974336 spec.py:321] Evaluating on the training split.
I0207 04:20:54.489318 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 04:21:33.302049 140107197974336 spec.py:349] Evaluating on the test split.
I0207 04:21:34.893637 140107197974336 submission_runner.py:408] Time since start: 49648.15s, 	Step: 95273, 	{'train/accuracy': 0.5392187237739563, 'train/loss': 1.992055058479309, 'validation/accuracy': 0.5055800080299377, 'validation/loss': 2.1670339107513428, 'validation/num_examples': 50000, 'test/accuracy': 0.3952000141143799, 'test/loss': 2.8237500190734863, 'test/num_examples': 10000, 'score': 44151.640016794205, 'total_duration': 49648.15318131447, 'accumulated_submission_time': 44151.640016794205, 'accumulated_eval_time': 5486.836786031723, 'accumulated_logging_time': 4.264795303344727}
I0207 04:21:34.926082 139946414638848 logging_writer.py:48] [95273] accumulated_eval_time=5486.836786, accumulated_logging_time=4.264795, accumulated_submission_time=44151.640017, global_step=95273, preemption_count=0, score=44151.640017, test/accuracy=0.395200, test/loss=2.823750, test/num_examples=10000, total_duration=49648.153181, train/accuracy=0.539219, train/loss=1.992055, validation/accuracy=0.505580, validation/loss=2.167034, validation/num_examples=50000
I0207 04:21:46.092587 139946397853440 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.084719181060791, loss=3.3011116981506348
I0207 04:22:30.351419 139946414638848 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.204933524131775, loss=2.982017755508423
I0207 04:23:17.031750 139946397853440 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.0620731115341187, loss=3.00496244430542
I0207 04:24:03.985722 139946414638848 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.9230720400810242, loss=5.315318584442139
I0207 04:24:51.045256 139946397853440 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.1134099960327148, loss=3.038212776184082
I0207 04:25:38.127071 139946414638848 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.2516969442367554, loss=3.0370283126831055
I0207 04:26:24.913567 139946397853440 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.9016225934028625, loss=3.9471681118011475
I0207 04:27:11.975306 139946414638848 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.2091243267059326, loss=3.0681612491607666
I0207 04:27:58.740418 139946397853440 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.0081216096878052, loss=4.967418670654297
I0207 04:28:35.169134 140107197974336 spec.py:321] Evaluating on the training split.
I0207 04:28:46.369001 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 04:29:26.857478 140107197974336 spec.py:349] Evaluating on the test split.
I0207 04:29:28.455622 140107197974336 submission_runner.py:408] Time since start: 50121.72s, 	Step: 96179, 	{'train/accuracy': 0.5384374856948853, 'train/loss': 2.011241912841797, 'validation/accuracy': 0.5003399848937988, 'validation/loss': 2.213174343109131, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.8492650985717773, 'test/num_examples': 10000, 'score': 44571.820883750916, 'total_duration': 50121.71514558792, 'accumulated_submission_time': 44571.820883750916, 'accumulated_eval_time': 5540.123233795166, 'accumulated_logging_time': 4.307686805725098}
I0207 04:29:28.496732 139946414638848 logging_writer.py:48] [96179] accumulated_eval_time=5540.123234, accumulated_logging_time=4.307687, accumulated_submission_time=44571.820884, global_step=96179, preemption_count=0, score=44571.820884, test/accuracy=0.392100, test/loss=2.849265, test/num_examples=10000, total_duration=50121.715146, train/accuracy=0.538437, train/loss=2.011242, validation/accuracy=0.500340, validation/loss=2.213174, validation/num_examples=50000
I0207 04:29:37.147463 139946397853440 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.3612096309661865, loss=3.1827099323272705
I0207 04:30:20.802088 139946414638848 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.2839654684066772, loss=3.187744379043579
I0207 04:31:07.750280 139946397853440 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.0214706659317017, loss=2.9930989742279053
I0207 04:31:54.396728 139946414638848 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.8026859760284424, loss=5.254011154174805
I0207 04:32:41.293572 139946397853440 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.2119272947311401, loss=3.4579966068267822
I0207 04:33:28.180217 139946414638848 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.106092929840088, loss=3.899353504180908
I0207 04:34:15.164709 139946397853440 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.103438138961792, loss=3.962820291519165
I0207 04:35:01.957885 139946414638848 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.1933053731918335, loss=3.072573184967041
I0207 04:35:48.995599 139946397853440 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.9532989859580994, loss=5.314846515655518
I0207 04:36:28.805994 140107197974336 spec.py:321] Evaluating on the training split.
I0207 04:36:39.855811 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 04:37:16.885725 140107197974336 spec.py:349] Evaluating on the test split.
I0207 04:37:18.483175 140107197974336 submission_runner.py:408] Time since start: 50591.74s, 	Step: 97086, 	{'train/accuracy': 0.5535546541213989, 'train/loss': 1.9242347478866577, 'validation/accuracy': 0.5067799687385559, 'validation/loss': 2.171412706375122, 'validation/num_examples': 50000, 'test/accuracy': 0.398000031709671, 'test/loss': 2.8221137523651123, 'test/num_examples': 10000, 'score': 44992.06829333305, 'total_duration': 50591.74269366264, 'accumulated_submission_time': 44992.06829333305, 'accumulated_eval_time': 5589.800358533859, 'accumulated_logging_time': 4.35940146446228}
I0207 04:37:18.516084 139946414638848 logging_writer.py:48] [97086] accumulated_eval_time=5589.800359, accumulated_logging_time=4.359401, accumulated_submission_time=44992.068293, global_step=97086, preemption_count=0, score=44992.068293, test/accuracy=0.398000, test/loss=2.822114, test/num_examples=10000, total_duration=50591.742694, train/accuracy=0.553555, train/loss=1.924235, validation/accuracy=0.506780, validation/loss=2.171413, validation/num_examples=50000
I0207 04:37:24.420465 139946397853440 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.1198325157165527, loss=3.451416254043579
I0207 04:38:07.692183 139946414638848 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.154417872428894, loss=3.3983633518218994
I0207 04:38:54.135047 139946397853440 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.3260990381240845, loss=3.3985650539398193
I0207 04:39:41.240355 139946414638848 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.2167993783950806, loss=3.2416470050811768
I0207 04:40:27.967203 139946397853440 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.2240484952926636, loss=5.352982044219971
I0207 04:41:15.199106 139946414638848 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.998913049697876, loss=5.517081260681152
I0207 04:42:02.040218 139946397853440 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.9440664052963257, loss=4.128697872161865
I0207 04:42:48.892818 139946414638848 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.113966703414917, loss=3.531418561935425
I0207 04:43:35.993577 139946397853440 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.1626298427581787, loss=3.1028263568878174
I0207 04:44:18.654919 140107197974336 spec.py:321] Evaluating on the training split.
I0207 04:44:29.732720 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 04:45:11.477666 140107197974336 spec.py:349] Evaluating on the test split.
I0207 04:45:13.076344 140107197974336 submission_runner.py:408] Time since start: 51066.34s, 	Step: 97993, 	{'train/accuracy': 0.532910168170929, 'train/loss': 2.055387020111084, 'validation/accuracy': 0.4993399977684021, 'validation/loss': 2.2423460483551025, 'validation/num_examples': 50000, 'test/accuracy': 0.38910001516342163, 'test/loss': 2.8874340057373047, 'test/num_examples': 10000, 'score': 45412.143065452576, 'total_duration': 51066.335882902145, 'accumulated_submission_time': 45412.143065452576, 'accumulated_eval_time': 5644.221765995026, 'accumulated_logging_time': 4.40524959564209}
I0207 04:45:13.109209 139946414638848 logging_writer.py:48] [97993] accumulated_eval_time=5644.221766, accumulated_logging_time=4.405250, accumulated_submission_time=45412.143065, global_step=97993, preemption_count=0, score=45412.143065, test/accuracy=0.389100, test/loss=2.887434, test/num_examples=10000, total_duration=51066.335883, train/accuracy=0.532910, train/loss=2.055387, validation/accuracy=0.499340, validation/loss=2.242346, validation/num_examples=50000
I0207 04:45:16.260718 139946397853440 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.190443754196167, loss=3.3225955963134766
I0207 04:45:59.130511 139946414638848 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.9397363662719727, loss=5.2860565185546875
I0207 04:46:45.748126 139946397853440 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.2777431011199951, loss=3.092163562774658
I0207 04:47:32.722863 139946414638848 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.3255985975265503, loss=3.057478189468384
I0207 04:48:19.563363 139946397853440 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.2825636863708496, loss=3.1156492233276367
I0207 04:49:06.612457 139946414638848 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.169766902923584, loss=3.3811826705932617
I0207 04:49:53.559035 139946397853440 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.073677659034729, loss=4.934232711791992
I0207 04:50:40.628612 139946414638848 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.2529231309890747, loss=3.1073460578918457
I0207 04:51:27.535433 139946397853440 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.0363858938217163, loss=3.403931140899658
I0207 04:52:13.215988 140107197974336 spec.py:321] Evaluating on the training split.
I0207 04:52:24.189454 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 04:53:03.045218 140107197974336 spec.py:349] Evaluating on the test split.
I0207 04:53:04.650996 140107197974336 submission_runner.py:408] Time since start: 51537.91s, 	Step: 98898, 	{'train/accuracy': 0.5400781035423279, 'train/loss': 2.028705358505249, 'validation/accuracy': 0.5006399750709534, 'validation/loss': 2.2367770671844482, 'validation/num_examples': 50000, 'test/accuracy': 0.3814000189304352, 'test/loss': 2.900303363800049, 'test/num_examples': 10000, 'score': 45832.18708944321, 'total_duration': 51537.9104912281, 'accumulated_submission_time': 45832.18708944321, 'accumulated_eval_time': 5695.656700849533, 'accumulated_logging_time': 4.4501214027404785}
I0207 04:53:04.693872 139946414638848 logging_writer.py:48] [98898] accumulated_eval_time=5695.656701, accumulated_logging_time=4.450121, accumulated_submission_time=45832.187089, global_step=98898, preemption_count=0, score=45832.187089, test/accuracy=0.381400, test/loss=2.900303, test/num_examples=10000, total_duration=51537.910491, train/accuracy=0.540078, train/loss=2.028705, validation/accuracy=0.500640, validation/loss=2.236777, validation/num_examples=50000
I0207 04:53:05.879506 139946397853440 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.2224818468093872, loss=3.0522878170013428
I0207 04:53:48.241655 139946414638848 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.3125495910644531, loss=3.042858839035034
I0207 04:54:35.105855 139946397853440 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.0330517292022705, loss=5.558308124542236
I0207 04:55:22.033573 139946414638848 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.2895959615707397, loss=3.1439108848571777
I0207 04:56:08.939702 139946397853440 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.515905499458313, loss=3.089470386505127
I0207 04:56:55.766137 139946414638848 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.1186240911483765, loss=3.2970235347747803
I0207 04:57:42.751365 139946397853440 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.1630752086639404, loss=2.983567237854004
I0207 04:58:29.640558 139946414638848 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.249802589416504, loss=3.0847039222717285
I0207 04:59:16.666495 139946397853440 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.190360426902771, loss=3.1366286277770996
I0207 05:00:03.662394 139946414638848 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.3691681623458862, loss=3.0757761001586914
I0207 05:00:04.653556 140107197974336 spec.py:321] Evaluating on the training split.
I0207 05:00:15.747328 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 05:00:56.031372 140107197974336 spec.py:349] Evaluating on the test split.
I0207 05:00:57.632038 140107197974336 submission_runner.py:408] Time since start: 52010.89s, 	Step: 99804, 	{'train/accuracy': 0.5590429306030273, 'train/loss': 1.925033688545227, 'validation/accuracy': 0.5095599889755249, 'validation/loss': 2.1697280406951904, 'validation/num_examples': 50000, 'test/accuracy': 0.39510002732276917, 'test/loss': 2.8197717666625977, 'test/num_examples': 10000, 'score': 46252.08383107185, 'total_duration': 52010.89158630371, 'accumulated_submission_time': 46252.08383107185, 'accumulated_eval_time': 5748.6351981163025, 'accumulated_logging_time': 4.504448413848877}
I0207 05:00:57.663445 139946397853440 logging_writer.py:48] [99804] accumulated_eval_time=5748.635198, accumulated_logging_time=4.504448, accumulated_submission_time=46252.083831, global_step=99804, preemption_count=0, score=46252.083831, test/accuracy=0.395100, test/loss=2.819772, test/num_examples=10000, total_duration=52010.891586, train/accuracy=0.559043, train/loss=1.925034, validation/accuracy=0.509560, validation/loss=2.169728, validation/num_examples=50000
I0207 05:01:38.617770 139946414638848 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.1892882585525513, loss=2.8901522159576416
I0207 05:02:25.123805 139946397853440 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.948310136795044, loss=4.745103359222412
I0207 05:03:12.413824 139946414638848 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.1622732877731323, loss=3.0563161373138428
I0207 05:03:58.884670 139946397853440 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.2106621265411377, loss=2.9490323066711426
I0207 05:04:45.813814 139946414638848 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.1012910604476929, loss=3.0824358463287354
I0207 05:05:32.626796 139946397853440 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.1672226190567017, loss=2.998579502105713
I0207 05:06:19.571041 139946414638848 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.0534727573394775, loss=3.7048254013061523
I0207 05:07:06.406768 139946397853440 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.2134429216384888, loss=2.967156410217285
I0207 05:07:53.341422 139946414638848 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.4257498979568481, loss=3.1217589378356934
I0207 05:07:57.679688 140107197974336 spec.py:321] Evaluating on the training split.
I0207 05:08:08.940199 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 05:08:47.871628 140107197974336 spec.py:349] Evaluating on the test split.
I0207 05:08:49.471800 140107197974336 submission_runner.py:408] Time since start: 52482.73s, 	Step: 100711, 	{'train/accuracy': 0.5338476300239563, 'train/loss': 2.0648865699768066, 'validation/accuracy': 0.4959999918937683, 'validation/loss': 2.255467176437378, 'validation/num_examples': 50000, 'test/accuracy': 0.3897000253200531, 'test/loss': 2.8789451122283936, 'test/num_examples': 10000, 'score': 46672.03926706314, 'total_duration': 52482.73132753372, 'accumulated_submission_time': 46672.03926706314, 'accumulated_eval_time': 5800.427268981934, 'accumulated_logging_time': 4.545479774475098}
I0207 05:08:49.508394 139946397853440 logging_writer.py:48] [100711] accumulated_eval_time=5800.427269, accumulated_logging_time=4.545480, accumulated_submission_time=46672.039267, global_step=100711, preemption_count=0, score=46672.039267, test/accuracy=0.389700, test/loss=2.878945, test/num_examples=10000, total_duration=52482.731328, train/accuracy=0.533848, train/loss=2.064887, validation/accuracy=0.496000, validation/loss=2.255467, validation/num_examples=50000
I0207 05:09:27.137161 139946414638848 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.925797700881958, loss=5.341700553894043
I0207 05:10:13.930409 139946397853440 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.1806056499481201, loss=3.08404278755188
I0207 05:11:01.147722 139946414638848 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.4825177192687988, loss=2.9493069648742676
I0207 05:11:48.103245 139946397853440 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.1227922439575195, loss=3.61283016204834
I0207 05:12:35.127394 139946414638848 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.198418378829956, loss=3.27510929107666
I0207 05:13:22.215070 139946397853440 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.9552901983261108, loss=4.615013599395752
I0207 05:14:09.481709 139946414638848 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.2572020292282104, loss=3.036170721054077
I0207 05:14:56.655240 139946397853440 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.2575783729553223, loss=3.117086410522461
I0207 05:15:43.727869 139946414638848 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.1750378608703613, loss=3.104099750518799
I0207 05:15:49.481917 140107197974336 spec.py:321] Evaluating on the training split.
I0207 05:16:00.346115 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 05:16:39.284651 140107197974336 spec.py:349] Evaluating on the test split.
I0207 05:16:40.882009 140107197974336 submission_runner.py:408] Time since start: 52954.14s, 	Step: 101614, 	{'train/accuracy': 0.5544726252555847, 'train/loss': 1.9149682521820068, 'validation/accuracy': 0.5176399946212769, 'validation/loss': 2.104093313217163, 'validation/num_examples': 50000, 'test/accuracy': 0.40470001101493835, 'test/loss': 2.784193754196167, 'test/num_examples': 10000, 'score': 47091.95157575607, 'total_duration': 52954.14155244827, 'accumulated_submission_time': 47091.95157575607, 'accumulated_eval_time': 5851.827321767807, 'accumulated_logging_time': 4.592525005340576}
I0207 05:16:40.916968 139946397853440 logging_writer.py:48] [101614] accumulated_eval_time=5851.827322, accumulated_logging_time=4.592525, accumulated_submission_time=47091.951576, global_step=101614, preemption_count=0, score=47091.951576, test/accuracy=0.404700, test/loss=2.784194, test/num_examples=10000, total_duration=52954.141552, train/accuracy=0.554473, train/loss=1.914968, validation/accuracy=0.517640, validation/loss=2.104093, validation/num_examples=50000
I0207 05:17:17.213588 139946414638848 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.1782937049865723, loss=2.877551794052124
I0207 05:18:03.747708 139946397853440 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.2432668209075928, loss=3.237969160079956
I0207 05:18:50.532670 139946414638848 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.2419297695159912, loss=3.1548657417297363
I0207 05:19:37.676953 139946397853440 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.2073473930358887, loss=2.997615337371826
I0207 05:20:24.634525 139946414638848 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.065228819847107, loss=4.611900329589844
I0207 05:21:11.726461 139946397853440 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.0139304399490356, loss=3.6579744815826416
I0207 05:21:58.774528 139946414638848 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.2056865692138672, loss=2.9782826900482178
I0207 05:22:45.722949 139946397853440 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.9607089757919312, loss=4.523932933807373
I0207 05:23:33.305161 139946414638848 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.0973701477050781, loss=3.0173535346984863
I0207 05:23:41.133896 140107197974336 spec.py:321] Evaluating on the training split.
I0207 05:23:51.986539 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 05:24:30.695695 140107197974336 spec.py:349] Evaluating on the test split.
I0207 05:24:32.286725 140107197974336 submission_runner.py:408] Time since start: 53425.55s, 	Step: 102519, 	{'train/accuracy': 0.558300793170929, 'train/loss': 1.900124430656433, 'validation/accuracy': 0.5158199667930603, 'validation/loss': 2.1189815998077393, 'validation/num_examples': 50000, 'test/accuracy': 0.4044000208377838, 'test/loss': 2.776407241821289, 'test/num_examples': 10000, 'score': 47512.10585308075, 'total_duration': 53425.546273231506, 'accumulated_submission_time': 47512.10585308075, 'accumulated_eval_time': 5902.980116844177, 'accumulated_logging_time': 4.639246225357056}
I0207 05:24:32.318189 139946397853440 logging_writer.py:48] [102519] accumulated_eval_time=5902.980117, accumulated_logging_time=4.639246, accumulated_submission_time=47512.105853, global_step=102519, preemption_count=0, score=47512.105853, test/accuracy=0.404400, test/loss=2.776407, test/num_examples=10000, total_duration=53425.546273, train/accuracy=0.558301, train/loss=1.900124, validation/accuracy=0.515820, validation/loss=2.118982, validation/num_examples=50000
I0207 05:25:06.419230 139946414638848 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.367355465888977, loss=3.030461549758911
I0207 05:25:52.969810 139946397853440 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.0765671730041504, loss=3.741250514984131
I0207 05:26:39.721323 139946414638848 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.1454799175262451, loss=3.0211265087127686
I0207 05:27:26.709057 139946397853440 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.261292576789856, loss=3.0253915786743164
I0207 05:28:13.746906 139946414638848 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.1439698934555054, loss=2.8948795795440674
I0207 05:29:00.587453 139946397853440 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.151007056236267, loss=2.954603433609009
I0207 05:29:47.539535 139946414638848 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.0763331651687622, loss=4.383577346801758
I0207 05:30:34.391370 139946397853440 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.8966222405433655, loss=4.762319564819336
I0207 05:31:21.080960 139946414638848 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.9872339367866516, loss=4.505414009094238
I0207 05:31:32.500824 140107197974336 spec.py:321] Evaluating on the training split.
I0207 05:31:43.440586 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 05:32:23.347713 140107197974336 spec.py:349] Evaluating on the test split.
I0207 05:32:24.945993 140107197974336 submission_runner.py:408] Time since start: 53898.21s, 	Step: 103426, 	{'train/accuracy': 0.5542968511581421, 'train/loss': 1.9207837581634521, 'validation/accuracy': 0.5221399664878845, 'validation/loss': 2.0911567211151123, 'validation/num_examples': 50000, 'test/accuracy': 0.40730002522468567, 'test/loss': 2.7569947242736816, 'test/num_examples': 10000, 'score': 47932.22675728798, 'total_duration': 53898.2055375576, 'accumulated_submission_time': 47932.22675728798, 'accumulated_eval_time': 5955.4252672195435, 'accumulated_logging_time': 4.68032431602478}
I0207 05:32:24.980188 139946397853440 logging_writer.py:48] [103426] accumulated_eval_time=5955.425267, accumulated_logging_time=4.680324, accumulated_submission_time=47932.226757, global_step=103426, preemption_count=0, score=47932.226757, test/accuracy=0.407300, test/loss=2.756995, test/num_examples=10000, total_duration=53898.205538, train/accuracy=0.554297, train/loss=1.920784, validation/accuracy=0.522140, validation/loss=2.091157, validation/num_examples=50000
I0207 05:32:55.333389 139946414638848 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.0345128774642944, loss=4.6013994216918945
I0207 05:33:41.875422 139946397853440 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.4338632822036743, loss=3.05532169342041
I0207 05:34:29.199993 139946414638848 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.0872251987457275, loss=3.3486292362213135
I0207 05:35:16.342735 139946397853440 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.2256230115890503, loss=2.9113852977752686
I0207 05:36:03.638632 139946414638848 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.0604935884475708, loss=3.7354440689086914
I0207 05:36:50.175767 139946397853440 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.1360381841659546, loss=2.873257875442505
I0207 05:37:37.168757 139946414638848 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.2661631107330322, loss=3.2310848236083984
I0207 05:38:23.867004 139946397853440 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.3343771696090698, loss=2.9050259590148926
I0207 05:39:10.484112 139946414638848 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.9368669986724854, loss=5.3974928855896
I0207 05:39:25.216880 140107197974336 spec.py:321] Evaluating on the training split.
I0207 05:39:36.227394 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 05:40:15.264168 140107197974336 spec.py:349] Evaluating on the test split.
I0207 05:40:16.853317 140107197974336 submission_runner.py:408] Time since start: 54370.11s, 	Step: 104333, 	{'train/accuracy': 0.5587499737739563, 'train/loss': 1.940987467765808, 'validation/accuracy': 0.5200799703598022, 'validation/loss': 2.1409268379211426, 'validation/num_examples': 50000, 'test/accuracy': 0.40380001068115234, 'test/loss': 2.7949371337890625, 'test/num_examples': 10000, 'score': 48352.40204453468, 'total_duration': 54370.112864494324, 'accumulated_submission_time': 48352.40204453468, 'accumulated_eval_time': 6007.061721801758, 'accumulated_logging_time': 4.724331378936768}
I0207 05:40:16.887571 139946397853440 logging_writer.py:48] [104333] accumulated_eval_time=6007.061722, accumulated_logging_time=4.724331, accumulated_submission_time=48352.402045, global_step=104333, preemption_count=0, score=48352.402045, test/accuracy=0.403800, test/loss=2.794937, test/num_examples=10000, total_duration=54370.112864, train/accuracy=0.558750, train/loss=1.940987, validation/accuracy=0.520080, validation/loss=2.140927, validation/num_examples=50000
I0207 05:40:44.146235 139946414638848 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.1515886783599854, loss=2.9522247314453125
I0207 05:41:30.508070 139946397853440 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.2479475736618042, loss=2.9921765327453613
I0207 05:42:17.622556 139946414638848 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.1936088800430298, loss=2.9020371437072754
I0207 05:43:04.361056 139946397853440 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.2587788105010986, loss=3.018143653869629
I0207 05:43:51.011994 139946414638848 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.2202706336975098, loss=3.104983329772949
I0207 05:44:38.254623 139946397853440 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.4877371788024902, loss=2.9646289348602295
I0207 05:45:25.277090 139946414638848 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.9941110014915466, loss=4.2551045417785645
I0207 05:46:12.237338 139946397853440 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.266710877418518, loss=3.019070863723755
I0207 05:46:58.922140 139946414638848 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.2460192441940308, loss=2.9325695037841797
I0207 05:47:17.266870 140107197974336 spec.py:321] Evaluating on the training split.
I0207 05:47:28.248119 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 05:48:09.180963 140107197974336 spec.py:349] Evaluating on the test split.
I0207 05:48:10.784226 140107197974336 submission_runner.py:408] Time since start: 54844.04s, 	Step: 105241, 	{'train/accuracy': 0.560839831829071, 'train/loss': 1.9002279043197632, 'validation/accuracy': 0.514959990978241, 'validation/loss': 2.125225067138672, 'validation/num_examples': 50000, 'test/accuracy': 0.403300017118454, 'test/loss': 2.796470880508423, 'test/num_examples': 10000, 'score': 48772.720823049545, 'total_duration': 54844.04375267029, 'accumulated_submission_time': 48772.720823049545, 'accumulated_eval_time': 6060.579028129578, 'accumulated_logging_time': 4.768460273742676}
I0207 05:48:10.823882 139946397853440 logging_writer.py:48] [105241] accumulated_eval_time=6060.579028, accumulated_logging_time=4.768460, accumulated_submission_time=48772.720823, global_step=105241, preemption_count=0, score=48772.720823, test/accuracy=0.403300, test/loss=2.796471, test/num_examples=10000, total_duration=54844.043753, train/accuracy=0.560840, train/loss=1.900228, validation/accuracy=0.514960, validation/loss=2.125225, validation/num_examples=50000
I0207 05:48:34.453350 139946414638848 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.1645057201385498, loss=3.0132360458374023
I0207 05:49:20.861323 139946397853440 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.2209792137145996, loss=2.882505178451538
I0207 05:50:07.836209 139946414638848 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.2663195133209229, loss=3.0579237937927246
I0207 05:50:54.726460 139946397853440 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.26553475856781, loss=2.8508145809173584
I0207 05:51:41.676932 139946414638848 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.297561526298523, loss=2.9540624618530273
I0207 05:52:28.293842 139946397853440 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.309988260269165, loss=2.9034383296966553
I0207 05:53:15.005389 139946414638848 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.2366106510162354, loss=2.961561441421509
I0207 05:54:01.723563 139946397853440 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.026885747909546, loss=3.7421319484710693
I0207 05:54:48.564595 139946414638848 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.3527586460113525, loss=3.0874900817871094
I0207 05:55:11.167073 140107197974336 spec.py:321] Evaluating on the training split.
I0207 05:55:21.820420 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 05:56:01.303690 140107197974336 spec.py:349] Evaluating on the test split.
I0207 05:56:02.913709 140107197974336 submission_runner.py:408] Time since start: 55316.17s, 	Step: 106150, 	{'train/accuracy': 0.5675585865974426, 'train/loss': 1.8988195657730103, 'validation/accuracy': 0.5279399752616882, 'validation/loss': 2.0975606441497803, 'validation/num_examples': 50000, 'test/accuracy': 0.4099000096321106, 'test/loss': 2.748702049255371, 'test/num_examples': 10000, 'score': 49193.001353263855, 'total_duration': 55316.173254966736, 'accumulated_submission_time': 49193.001353263855, 'accumulated_eval_time': 6112.325652837753, 'accumulated_logging_time': 4.819467544555664}
I0207 05:56:02.951590 139946397853440 logging_writer.py:48] [106150] accumulated_eval_time=6112.325653, accumulated_logging_time=4.819468, accumulated_submission_time=49193.001353, global_step=106150, preemption_count=0, score=49193.001353, test/accuracy=0.409900, test/loss=2.748702, test/num_examples=10000, total_duration=55316.173255, train/accuracy=0.567559, train/loss=1.898820, validation/accuracy=0.527940, validation/loss=2.097561, validation/num_examples=50000
I0207 05:56:22.997351 139946414638848 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.223326325416565, loss=2.950948715209961
I0207 05:57:08.830636 139946397853440 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.0080063343048096, loss=5.213113307952881
I0207 05:57:56.116452 139946414638848 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.2094882726669312, loss=2.9107401371002197
I0207 05:58:43.211715 139946397853440 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.5092862844467163, loss=2.9123799800872803
I0207 05:59:30.154709 139946414638848 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.2513189315795898, loss=2.974318504333496
I0207 06:00:17.096419 139946397853440 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.1845614910125732, loss=2.8661952018737793
I0207 06:01:04.306953 139946414638848 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.220967173576355, loss=5.155597686767578
I0207 06:01:51.133790 139946397853440 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.2718297243118286, loss=3.014589309692383
I0207 06:02:37.915171 139946414638848 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.1734387874603271, loss=3.1516940593719482
I0207 06:03:02.972207 140107197974336 spec.py:321] Evaluating on the training split.
I0207 06:03:13.805772 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 06:03:53.747808 140107197974336 spec.py:349] Evaluating on the test split.
I0207 06:03:55.343345 140107197974336 submission_runner.py:408] Time since start: 55788.60s, 	Step: 107055, 	{'train/accuracy': 0.5690038800239563, 'train/loss': 1.8810782432556152, 'validation/accuracy': 0.5263199806213379, 'validation/loss': 2.085660457611084, 'validation/num_examples': 50000, 'test/accuracy': 0.41220003366470337, 'test/loss': 2.7401907444000244, 'test/num_examples': 10000, 'score': 49612.96082854271, 'total_duration': 55788.6028881073, 'accumulated_submission_time': 49612.96082854271, 'accumulated_eval_time': 6164.696760416031, 'accumulated_logging_time': 4.867316961288452}
I0207 06:03:55.379236 139946397853440 logging_writer.py:48] [107055] accumulated_eval_time=6164.696760, accumulated_logging_time=4.867317, accumulated_submission_time=49612.960829, global_step=107055, preemption_count=0, score=49612.960829, test/accuracy=0.412200, test/loss=2.740191, test/num_examples=10000, total_duration=55788.602888, train/accuracy=0.569004, train/loss=1.881078, validation/accuracy=0.526320, validation/loss=2.085660, validation/num_examples=50000
I0207 06:04:13.477582 139946414638848 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.0657297372817993, loss=4.843117713928223
I0207 06:04:58.996652 139946397853440 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.0652680397033691, loss=3.4242031574249268
I0207 06:05:45.725226 139946414638848 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.2805559635162354, loss=3.241750717163086
I0207 06:06:32.699807 139946397853440 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.2229666709899902, loss=2.9355931282043457
I0207 06:07:19.715106 139946414638848 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.0218762159347534, loss=4.773126602172852
I0207 06:08:06.852023 139946397853440 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.4336137771606445, loss=2.8373336791992188
I0207 06:08:53.562584 139946414638848 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.2739564180374146, loss=2.803589344024658
I0207 06:09:40.682665 139946397853440 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.245829463005066, loss=3.1763904094696045
I0207 06:10:27.826725 139946414638848 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.285555362701416, loss=2.9122042655944824
I0207 06:10:55.681871 140107197974336 spec.py:321] Evaluating on the training split.
I0207 06:11:07.810062 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 06:11:48.231037 140107197974336 spec.py:349] Evaluating on the test split.
I0207 06:11:49.834597 140107197974336 submission_runner.py:408] Time since start: 56263.09s, 	Step: 107961, 	{'train/accuracy': 0.5733398199081421, 'train/loss': 1.872202754020691, 'validation/accuracy': 0.5306199789047241, 'validation/loss': 2.0735056400299072, 'validation/num_examples': 50000, 'test/accuracy': 0.417900025844574, 'test/loss': 2.732545852661133, 'test/num_examples': 10000, 'score': 50033.20030045509, 'total_duration': 56263.09414482117, 'accumulated_submission_time': 50033.20030045509, 'accumulated_eval_time': 6218.849471569061, 'accumulated_logging_time': 4.915220022201538}
I0207 06:11:49.881492 139946397853440 logging_writer.py:48] [107961] accumulated_eval_time=6218.849472, accumulated_logging_time=4.915220, accumulated_submission_time=50033.200300, global_step=107961, preemption_count=0, score=50033.200300, test/accuracy=0.417900, test/loss=2.732546, test/num_examples=10000, total_duration=56263.094145, train/accuracy=0.573340, train/loss=1.872203, validation/accuracy=0.530620, validation/loss=2.073506, validation/num_examples=50000
I0207 06:12:05.638674 139946414638848 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.001081943511963, loss=5.374737739562988
I0207 06:12:50.296684 139946397853440 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.4295389652252197, loss=3.051279306411743
I0207 06:13:36.978550 139946414638848 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.1384080648422241, loss=4.393777847290039
I0207 06:14:23.835458 139946397853440 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.2293158769607544, loss=2.8833048343658447
I0207 06:15:10.475411 139946414638848 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.0113540887832642, loss=5.111142635345459
I0207 06:15:57.103664 139946397853440 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.1711845397949219, loss=3.7155134677886963
I0207 06:16:44.008512 139946414638848 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.2721943855285645, loss=3.04744553565979
I0207 06:17:30.701204 139946397853440 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.3983769416809082, loss=2.9672272205352783
I0207 06:18:17.440243 139946414638848 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.1774133443832397, loss=4.199680805206299
I0207 06:18:50.125134 140107197974336 spec.py:321] Evaluating on the training split.
I0207 06:19:01.421831 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 06:19:40.841268 140107197974336 spec.py:349] Evaluating on the test split.
I0207 06:19:42.439380 140107197974336 submission_runner.py:408] Time since start: 56735.70s, 	Step: 108871, 	{'train/accuracy': 0.6080859303474426, 'train/loss': 1.6695939302444458, 'validation/accuracy': 0.5309799909591675, 'validation/loss': 2.035268545150757, 'validation/num_examples': 50000, 'test/accuracy': 0.41880002617836, 'test/loss': 2.701533317565918, 'test/num_examples': 10000, 'score': 50453.37967920303, 'total_duration': 56735.69892835617, 'accumulated_submission_time': 50453.37967920303, 'accumulated_eval_time': 6271.163687944412, 'accumulated_logging_time': 4.974422931671143}
I0207 06:19:42.479865 139946397853440 logging_writer.py:48] [108871] accumulated_eval_time=6271.163688, accumulated_logging_time=4.974423, accumulated_submission_time=50453.379679, global_step=108871, preemption_count=0, score=50453.379679, test/accuracy=0.418800, test/loss=2.701533, test/num_examples=10000, total_duration=56735.698928, train/accuracy=0.608086, train/loss=1.669594, validation/accuracy=0.530980, validation/loss=2.035269, validation/num_examples=50000
I0207 06:19:54.264580 139946414638848 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.2680590152740479, loss=2.9348065853118896
I0207 06:20:38.695963 139946397853440 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.2080267667770386, loss=2.9573400020599365
I0207 06:21:25.260984 139946414638848 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.9447599649429321, loss=5.386993408203125
I0207 06:22:12.437172 139946397853440 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.0908385515213013, loss=4.434764862060547
I0207 06:22:59.061812 139946414638848 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.3483374118804932, loss=2.9324254989624023
I0207 06:23:45.926693 139946397853440 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.3084732294082642, loss=2.8265466690063477
I0207 06:24:32.928616 139946414638848 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.27322256565094, loss=2.889371633529663
I0207 06:25:19.579107 139946397853440 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.008922815322876, loss=4.072982311248779
I0207 06:26:06.446914 139946414638848 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.2436130046844482, loss=3.1861371994018555
I0207 06:26:42.571544 140107197974336 spec.py:321] Evaluating on the training split.
I0207 06:26:53.576567 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 06:27:34.914374 140107197974336 spec.py:349] Evaluating on the test split.
I0207 06:27:36.516008 140107197974336 submission_runner.py:408] Time since start: 57209.78s, 	Step: 109779, 	{'train/accuracy': 0.5797070264816284, 'train/loss': 1.7871202230453491, 'validation/accuracy': 0.5417999625205994, 'validation/loss': 1.9782638549804688, 'validation/num_examples': 50000, 'test/accuracy': 0.42670002579689026, 'test/loss': 2.641922950744629, 'test/num_examples': 10000, 'score': 50873.40883421898, 'total_duration': 57209.77555155754, 'accumulated_submission_time': 50873.40883421898, 'accumulated_eval_time': 6325.108122825623, 'accumulated_logging_time': 5.025132894515991}
I0207 06:27:36.552470 139946397853440 logging_writer.py:48] [109779] accumulated_eval_time=6325.108123, accumulated_logging_time=5.025133, accumulated_submission_time=50873.408834, global_step=109779, preemption_count=0, score=50873.408834, test/accuracy=0.426700, test/loss=2.641923, test/num_examples=10000, total_duration=57209.775552, train/accuracy=0.579707, train/loss=1.787120, validation/accuracy=0.541800, validation/loss=1.978264, validation/num_examples=50000
I0207 06:27:45.213276 139946414638848 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.0347503423690796, loss=4.844628810882568
I0207 06:28:28.997759 139946397853440 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.3459434509277344, loss=2.858870267868042
I0207 06:29:15.465658 139946414638848 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.2473143339157104, loss=2.907773017883301
I0207 06:30:02.680991 139946397853440 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.3193439245224, loss=3.0359995365142822
I0207 06:30:49.156796 139946414638848 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.0921697616577148, loss=5.076737403869629
I0207 06:31:36.114094 139946397853440 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.0444188117980957, loss=3.744877576828003
I0207 06:32:23.036370 139946414638848 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.9885525703430176, loss=4.836481094360352
I0207 06:33:09.821413 139946397853440 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.341719388961792, loss=2.9341320991516113
I0207 06:33:56.597896 139946414638848 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.0410209894180298, loss=4.783459663391113
I0207 06:34:36.809565 140107197974336 spec.py:321] Evaluating on the training split.
I0207 06:34:47.559663 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 06:35:25.381420 140107197974336 spec.py:349] Evaluating on the test split.
I0207 06:35:26.978498 140107197974336 submission_runner.py:408] Time since start: 57680.24s, 	Step: 110687, 	{'train/accuracy': 0.5723242163658142, 'train/loss': 1.8696619272232056, 'validation/accuracy': 0.5303800106048584, 'validation/loss': 2.0785651206970215, 'validation/num_examples': 50000, 'test/accuracy': 0.41940000653266907, 'test/loss': 2.7238848209381104, 'test/num_examples': 10000, 'score': 51293.603743076324, 'total_duration': 57680.238035440445, 'accumulated_submission_time': 51293.603743076324, 'accumulated_eval_time': 6375.277026414871, 'accumulated_logging_time': 5.072314500808716}
I0207 06:35:27.017040 139946397853440 logging_writer.py:48] [110687] accumulated_eval_time=6375.277026, accumulated_logging_time=5.072315, accumulated_submission_time=51293.603743, global_step=110687, preemption_count=0, score=51293.603743, test/accuracy=0.419400, test/loss=2.723885, test/num_examples=10000, total_duration=57680.238035, train/accuracy=0.572324, train/loss=1.869662, validation/accuracy=0.530380, validation/loss=2.078565, validation/num_examples=50000
I0207 06:35:32.534603 139946414638848 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.1835691928863525, loss=3.526684284210205
I0207 06:36:15.698504 139946397853440 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.175999402999878, loss=4.887520790100098
I0207 06:37:02.215872 139946414638848 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.0164234638214111, loss=4.554049015045166
I0207 06:37:48.767034 139946397853440 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.2619636058807373, loss=2.9475808143615723
I0207 06:38:35.772189 139946414638848 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.2143868207931519, loss=3.6390767097473145
I0207 06:39:22.552873 139946397853440 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.1695150136947632, loss=3.5758581161499023
I0207 06:40:09.394279 139946414638848 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.1430346965789795, loss=3.684631109237671
I0207 06:40:56.294267 139946397853440 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.9895880222320557, loss=5.300169467926025
I0207 06:41:42.820040 139946414638848 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.2221925258636475, loss=2.848560333251953
I0207 06:42:27.242642 140107197974336 spec.py:321] Evaluating on the training split.
I0207 06:42:38.203191 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 06:43:17.784906 140107197974336 spec.py:349] Evaluating on the test split.
I0207 06:43:19.378882 140107197974336 submission_runner.py:408] Time since start: 58152.64s, 	Step: 111597, 	{'train/accuracy': 0.6044335961341858, 'train/loss': 1.6973642110824585, 'validation/accuracy': 0.542639970779419, 'validation/loss': 2.007513999938965, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.6781985759735107, 'test/num_examples': 10000, 'score': 51713.767781972885, 'total_duration': 58152.63842535019, 'accumulated_submission_time': 51713.767781972885, 'accumulated_eval_time': 6427.413257360458, 'accumulated_logging_time': 5.120913028717041}
I0207 06:43:19.412931 139946397853440 logging_writer.py:48] [111597] accumulated_eval_time=6427.413257, accumulated_logging_time=5.120913, accumulated_submission_time=51713.767782, global_step=111597, preemption_count=0, score=51713.767782, test/accuracy=0.423400, test/loss=2.678199, test/num_examples=10000, total_duration=58152.638425, train/accuracy=0.604434, train/loss=1.697364, validation/accuracy=0.542640, validation/loss=2.007514, validation/num_examples=50000
I0207 06:43:20.986224 139946414638848 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.0564044713974, loss=3.5822529792785645
I0207 06:44:03.557926 139946397853440 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.2274065017700195, loss=2.911086082458496
I0207 06:44:50.551191 139946414638848 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.1526843309402466, loss=4.567661285400391
I0207 06:45:37.396327 139946397853440 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.2780309915542603, loss=2.852383613586426
I0207 06:46:24.433598 139946414638848 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.2709779739379883, loss=2.7160985469818115
I0207 06:47:11.555787 139946397853440 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.2582718133926392, loss=2.7932655811309814
I0207 06:47:58.245082 139946414638848 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.0986688137054443, loss=5.339691162109375
I0207 06:48:45.094581 139946397853440 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.0285948514938354, loss=3.883653163909912
I0207 06:49:32.172216 139946414638848 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.1794801950454712, loss=2.930324077606201
I0207 06:50:19.159253 139946397853440 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.3369178771972656, loss=2.8055529594421387
I0207 06:50:19.715020 140107197974336 spec.py:321] Evaluating on the training split.
I0207 06:50:30.561788 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 06:51:11.316663 140107197974336 spec.py:349] Evaluating on the test split.
I0207 06:51:12.917054 140107197974336 submission_runner.py:408] Time since start: 58626.18s, 	Step: 112503, 	{'train/accuracy': 0.5824804306030273, 'train/loss': 1.7816941738128662, 'validation/accuracy': 0.5447999835014343, 'validation/loss': 1.961826205253601, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.6584208011627197, 'test/num_examples': 10000, 'score': 52134.00865268707, 'total_duration': 58626.1765999794, 'accumulated_submission_time': 52134.00865268707, 'accumulated_eval_time': 6480.615281820297, 'accumulated_logging_time': 5.165015459060669}
I0207 06:51:12.950140 139946414638848 logging_writer.py:48] [112503] accumulated_eval_time=6480.615282, accumulated_logging_time=5.165015, accumulated_submission_time=52134.008653, global_step=112503, preemption_count=0, score=52134.008653, test/accuracy=0.431300, test/loss=2.658421, test/num_examples=10000, total_duration=58626.176600, train/accuracy=0.582480, train/loss=1.781694, validation/accuracy=0.544800, validation/loss=1.961826, validation/num_examples=50000
I0207 06:51:54.348517 139946397853440 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.142087459564209, loss=3.3739147186279297
I0207 06:52:40.841379 139946414638848 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.2992860078811646, loss=2.854327440261841
I0207 06:53:27.613896 139946397853440 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.1800463199615479, loss=4.972870349884033
I0207 06:54:14.466848 139946414638848 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.3414278030395508, loss=2.7440595626831055
I0207 06:55:01.808303 139946397853440 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.3696119785308838, loss=2.9423909187316895
I0207 06:55:48.930335 139946414638848 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.2951648235321045, loss=2.7135097980499268
I0207 06:56:35.657466 139946397853440 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.348897099494934, loss=2.914262056350708
I0207 06:57:22.644202 139946414638848 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.2128689289093018, loss=2.7531721591949463
I0207 06:58:09.744450 139946397853440 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.2908879518508911, loss=2.685685873031616
I0207 06:58:13.148244 140107197974336 spec.py:321] Evaluating on the training split.
I0207 06:58:24.334374 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 06:59:03.086944 140107197974336 spec.py:349] Evaluating on the test split.
I0207 06:59:04.691485 140107197974336 submission_runner.py:408] Time since start: 59097.95s, 	Step: 113409, 	{'train/accuracy': 0.5881054401397705, 'train/loss': 1.7460676431655884, 'validation/accuracy': 0.5488399863243103, 'validation/loss': 1.9370585680007935, 'validation/num_examples': 50000, 'test/accuracy': 0.4369000196456909, 'test/loss': 2.59963321685791, 'test/num_examples': 10000, 'score': 52554.144918203354, 'total_duration': 59097.95101642609, 'accumulated_submission_time': 52554.144918203354, 'accumulated_eval_time': 6532.158484697342, 'accumulated_logging_time': 5.209194660186768}
I0207 06:59:04.731116 139946414638848 logging_writer.py:48] [113409] accumulated_eval_time=6532.158485, accumulated_logging_time=5.209195, accumulated_submission_time=52554.144918, global_step=113409, preemption_count=0, score=52554.144918, test/accuracy=0.436900, test/loss=2.599633, test/num_examples=10000, total_duration=59097.951016, train/accuracy=0.588105, train/loss=1.746068, validation/accuracy=0.548840, validation/loss=1.937059, validation/num_examples=50000
I0207 06:59:43.049636 139946397853440 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.358521580696106, loss=2.804218292236328
I0207 07:00:29.554715 139946414638848 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.330843448638916, loss=2.8655035495758057
I0207 07:01:16.691565 139946397853440 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.45156729221344, loss=2.8978190422058105
I0207 07:02:03.428957 139946414638848 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.180151104927063, loss=3.4020724296569824
I0207 07:02:50.508433 139946397853440 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.3363795280456543, loss=2.802488327026367
I0207 07:03:37.218610 139946414638848 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.1053352355957031, loss=4.920315742492676
I0207 07:04:24.511091 139946397853440 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.3665785789489746, loss=5.195666313171387
I0207 07:05:11.461942 139946414638848 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.0233712196350098, loss=4.274752616882324
I0207 07:05:58.189588 139946397853440 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.4454890489578247, loss=2.8755879402160645
I0207 07:06:04.983994 140107197974336 spec.py:321] Evaluating on the training split.
I0207 07:06:16.021071 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 07:06:54.749709 140107197974336 spec.py:349] Evaluating on the test split.
I0207 07:06:56.348598 140107197974336 submission_runner.py:408] Time since start: 59569.61s, 	Step: 114316, 	{'train/accuracy': 0.6094335913658142, 'train/loss': 1.6528574228286743, 'validation/accuracy': 0.5530999898910522, 'validation/loss': 1.9297101497650146, 'validation/num_examples': 50000, 'test/accuracy': 0.4376000165939331, 'test/loss': 2.588284969329834, 'test/num_examples': 10000, 'score': 52974.33516526222, 'total_duration': 59569.60813641548, 'accumulated_submission_time': 52974.33516526222, 'accumulated_eval_time': 6583.523062705994, 'accumulated_logging_time': 5.259409666061401}
I0207 07:06:56.383430 139946414638848 logging_writer.py:48] [114316] accumulated_eval_time=6583.523063, accumulated_logging_time=5.259410, accumulated_submission_time=52974.335165, global_step=114316, preemption_count=0, score=52974.335165, test/accuracy=0.437600, test/loss=2.588285, test/num_examples=10000, total_duration=59569.608136, train/accuracy=0.609434, train/loss=1.652857, validation/accuracy=0.553100, validation/loss=1.929710, validation/num_examples=50000
I0207 07:07:31.581446 139946397853440 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.1857200860977173, loss=5.191205978393555
I0207 07:08:18.170043 139946414638848 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.2400773763656616, loss=2.8812787532806396
I0207 07:09:05.359449 139946397853440 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.3892408609390259, loss=2.850903272628784
I0207 07:09:52.043318 139946414638848 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.2916808128356934, loss=2.825207233428955
I0207 07:10:39.014310 139946397853440 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.3577500581741333, loss=2.901043653488159
I0207 07:11:26.034719 139946414638848 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.349668264389038, loss=2.8356411457061768
I0207 07:12:12.811049 139946397853440 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.0968215465545654, loss=4.538771629333496
I0207 07:12:59.931852 139946414638848 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.2644047737121582, loss=2.843782424926758
I0207 07:13:47.175022 139946397853440 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.128563404083252, loss=4.731785774230957
I0207 07:13:56.704360 140107197974336 spec.py:321] Evaluating on the training split.
I0207 07:14:07.865161 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 07:14:47.283914 140107197974336 spec.py:349] Evaluating on the test split.
I0207 07:14:48.878140 140107197974336 submission_runner.py:408] Time since start: 60042.14s, 	Step: 115222, 	{'train/accuracy': 0.5908203125, 'train/loss': 1.7480441331863403, 'validation/accuracy': 0.5513399839401245, 'validation/loss': 1.947059988975525, 'validation/num_examples': 50000, 'test/accuracy': 0.4300000071525574, 'test/loss': 2.6224279403686523, 'test/num_examples': 10000, 'score': 53394.59422135353, 'total_duration': 60042.13766551018, 'accumulated_submission_time': 53394.59422135353, 'accumulated_eval_time': 6635.696802854538, 'accumulated_logging_time': 5.305138349533081}
I0207 07:14:48.912577 139946414638848 logging_writer.py:48] [115222] accumulated_eval_time=6635.696803, accumulated_logging_time=5.305138, accumulated_submission_time=53394.594221, global_step=115222, preemption_count=0, score=53394.594221, test/accuracy=0.430000, test/loss=2.622428, test/num_examples=10000, total_duration=60042.137666, train/accuracy=0.590820, train/loss=1.748044, validation/accuracy=0.551340, validation/loss=1.947060, validation/num_examples=50000
I0207 07:15:21.815476 139946397853440 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.295926809310913, loss=2.9869191646575928
I0207 07:16:08.304990 139946414638848 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.2456395626068115, loss=3.522223949432373
I0207 07:16:55.133570 139946397853440 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.1703904867172241, loss=3.2490828037261963
I0207 07:17:42.015236 139946414638848 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.1280624866485596, loss=4.131317138671875
I0207 07:18:29.281579 139946397853440 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.2842720746994019, loss=2.6191859245300293
I0207 07:19:16.435866 139946414638848 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.0621569156646729, loss=3.8663203716278076
I0207 07:20:03.605771 139946397853440 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.0644201040267944, loss=4.0579833984375
I0207 07:20:50.646286 139946414638848 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.3720823526382446, loss=2.8542232513427734
I0207 07:21:37.635796 139946397853440 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.1984596252441406, loss=4.039739608764648
I0207 07:21:49.257080 140107197974336 spec.py:321] Evaluating on the training split.
I0207 07:22:00.204461 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 07:22:40.001668 140107197974336 spec.py:349] Evaluating on the test split.
I0207 07:22:41.601857 140107197974336 submission_runner.py:408] Time since start: 60514.86s, 	Step: 116126, 	{'train/accuracy': 0.5944726467132568, 'train/loss': 1.7341322898864746, 'validation/accuracy': 0.5532999634742737, 'validation/loss': 1.9365402460098267, 'validation/num_examples': 50000, 'test/accuracy': 0.4336000084877014, 'test/loss': 2.6125783920288086, 'test/num_examples': 10000, 'score': 53814.87706851959, 'total_duration': 60514.86140489578, 'accumulated_submission_time': 53814.87706851959, 'accumulated_eval_time': 6688.041565418243, 'accumulated_logging_time': 5.350852966308594}
I0207 07:22:41.637130 139946414638848 logging_writer.py:48] [116126] accumulated_eval_time=6688.041565, accumulated_logging_time=5.350853, accumulated_submission_time=53814.877069, global_step=116126, preemption_count=0, score=53814.877069, test/accuracy=0.433600, test/loss=2.612578, test/num_examples=10000, total_duration=60514.861405, train/accuracy=0.594473, train/loss=1.734132, validation/accuracy=0.553300, validation/loss=1.936540, validation/num_examples=50000
I0207 07:23:12.257545 139946397853440 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.4029810428619385, loss=2.7432308197021484
I0207 07:23:58.658749 139946414638848 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.4308223724365234, loss=2.831956148147583
I0207 07:24:46.612476 139946397853440 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.4254523515701294, loss=2.786109685897827
I0207 07:25:33.673404 139946414638848 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.3730348348617554, loss=2.758991241455078
I0207 07:26:20.812071 139946397853440 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.13625967502594, loss=3.754479169845581
I0207 07:27:07.505679 139946414638848 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.2606818675994873, loss=2.782914161682129
I0207 07:27:54.337130 139946397853440 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.6413018703460693, loss=2.668766498565674
I0207 07:28:41.403828 139946414638848 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.3535075187683105, loss=2.6819117069244385
I0207 07:29:28.316130 139946397853440 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.3850300312042236, loss=3.8598179817199707
I0207 07:29:42.187830 140107197974336 spec.py:321] Evaluating on the training split.
I0207 07:29:53.413613 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 07:30:35.088945 140107197974336 spec.py:349] Evaluating on the test split.
I0207 07:30:36.703309 140107197974336 submission_runner.py:408] Time since start: 60989.96s, 	Step: 117031, 	{'train/accuracy': 0.607128918170929, 'train/loss': 1.6637282371520996, 'validation/accuracy': 0.5559200048446655, 'validation/loss': 1.9096101522445679, 'validation/num_examples': 50000, 'test/accuracy': 0.43700000643730164, 'test/loss': 2.60026216506958, 'test/num_examples': 10000, 'score': 54235.36640357971, 'total_duration': 60989.96281218529, 'accumulated_submission_time': 54235.36640357971, 'accumulated_eval_time': 6742.556987047195, 'accumulated_logging_time': 5.396175384521484}
I0207 07:30:36.745904 139946414638848 logging_writer.py:48] [117031] accumulated_eval_time=6742.556987, accumulated_logging_time=5.396175, accumulated_submission_time=54235.366404, global_step=117031, preemption_count=0, score=54235.366404, test/accuracy=0.437000, test/loss=2.600262, test/num_examples=10000, total_duration=60989.962812, train/accuracy=0.607129, train/loss=1.663728, validation/accuracy=0.555920, validation/loss=1.909610, validation/num_examples=50000
I0207 07:31:05.216560 139946397853440 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.3333675861358643, loss=4.95435905456543
I0207 07:31:51.387763 139946414638848 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.1464440822601318, loss=4.897922515869141
I0207 07:32:38.349714 139946397853440 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.3673887252807617, loss=2.8300108909606934
I0207 07:33:25.338305 139946414638848 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.3065944910049438, loss=2.823777675628662
I0207 07:34:12.331568 139946397853440 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.651578426361084, loss=2.7763378620147705
I0207 07:34:59.591544 139946414638848 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.1136287450790405, loss=5.242520809173584
I0207 07:35:46.650705 139946397853440 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.3328500986099243, loss=2.663393259048462
I0207 07:36:33.398303 139946414638848 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.2224130630493164, loss=2.7023608684539795
I0207 07:37:20.431048 139946397853440 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.1882745027542114, loss=5.145473957061768
I0207 07:37:37.039561 140107197974336 spec.py:321] Evaluating on the training split.
I0207 07:37:48.200102 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 07:38:27.219301 140107197974336 spec.py:349] Evaluating on the test split.
I0207 07:38:28.811677 140107197974336 submission_runner.py:408] Time since start: 61462.07s, 	Step: 117937, 	{'train/accuracy': 0.5986914038658142, 'train/loss': 1.7124024629592896, 'validation/accuracy': 0.5584200024604797, 'validation/loss': 1.9031184911727905, 'validation/num_examples': 50000, 'test/accuracy': 0.44120001792907715, 'test/loss': 2.5633251667022705, 'test/num_examples': 10000, 'score': 54655.59771943092, 'total_duration': 61462.07122516632, 'accumulated_submission_time': 54655.59771943092, 'accumulated_eval_time': 6794.329073667526, 'accumulated_logging_time': 5.449033498764038}
I0207 07:38:28.850617 139946414638848 logging_writer.py:48] [117937] accumulated_eval_time=6794.329074, accumulated_logging_time=5.449033, accumulated_submission_time=54655.597719, global_step=117937, preemption_count=0, score=54655.597719, test/accuracy=0.441200, test/loss=2.563325, test/num_examples=10000, total_duration=61462.071225, train/accuracy=0.598691, train/loss=1.712402, validation/accuracy=0.558420, validation/loss=1.903118, validation/num_examples=50000
I0207 07:38:54.322809 139946397853440 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.4077659845352173, loss=2.760938882827759
I0207 07:39:40.845834 139946414638848 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.4712729454040527, loss=2.8404321670532227
I0207 07:40:27.844989 139946397853440 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.3651548624038696, loss=5.4583821296691895
I0207 07:41:14.948647 139946414638848 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.0995995998382568, loss=4.866115570068359
I0207 07:42:01.526989 139946397853440 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.2718777656555176, loss=2.669086217880249
I0207 07:42:48.384328 139946414638848 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.2630698680877686, loss=2.8598272800445557
I0207 07:43:35.127674 139946397853440 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.3793227672576904, loss=2.6624295711517334
I0207 07:44:22.094800 139946414638848 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.308943748474121, loss=2.935110569000244
I0207 07:45:09.301368 139946397853440 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.1780904531478882, loss=5.11739444732666
I0207 07:45:29.290462 140107197974336 spec.py:321] Evaluating on the training split.
I0207 07:45:40.384738 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 07:46:20.825360 140107197974336 spec.py:349] Evaluating on the test split.
I0207 07:46:22.424782 140107197974336 submission_runner.py:408] Time since start: 61935.68s, 	Step: 118844, 	{'train/accuracy': 0.6104296445846558, 'train/loss': 1.6610846519470215, 'validation/accuracy': 0.5698599815368652, 'validation/loss': 1.8717105388641357, 'validation/num_examples': 50000, 'test/accuracy': 0.45280003547668457, 'test/loss': 2.5325958728790283, 'test/num_examples': 10000, 'score': 55075.97532296181, 'total_duration': 61935.68432497978, 'accumulated_submission_time': 55075.97532296181, 'accumulated_eval_time': 6847.463381290436, 'accumulated_logging_time': 5.4985644817352295}
I0207 07:46:22.461030 139946414638848 logging_writer.py:48] [118844] accumulated_eval_time=6847.463381, accumulated_logging_time=5.498564, accumulated_submission_time=55075.975323, global_step=118844, preemption_count=0, score=55075.975323, test/accuracy=0.452800, test/loss=2.532596, test/num_examples=10000, total_duration=61935.684325, train/accuracy=0.610430, train/loss=1.661085, validation/accuracy=0.569860, validation/loss=1.871711, validation/num_examples=50000
I0207 07:46:44.883049 139946397853440 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.3615915775299072, loss=3.2151525020599365
I0207 07:47:31.072217 139946414638848 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.1386041641235352, loss=4.001645088195801
I0207 07:48:18.091201 139946397853440 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.2682809829711914, loss=2.607642650604248
I0207 07:49:05.042308 139946414638848 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.297804594039917, loss=2.780116319656372
I0207 07:49:51.788124 139946397853440 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.362570881843567, loss=2.9900572299957275
I0207 07:50:38.551306 139946414638848 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.4178868532180786, loss=3.2732901573181152
I0207 07:51:25.575344 139946397853440 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.2882927656173706, loss=2.667964220046997
I0207 07:52:12.627859 139946414638848 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.2369922399520874, loss=3.3923544883728027
I0207 07:52:59.356499 139946397853440 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.1024539470672607, loss=4.697945594787598
I0207 07:53:22.590314 140107197974336 spec.py:321] Evaluating on the training split.
I0207 07:53:33.736054 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 07:54:12.434169 140107197974336 spec.py:349] Evaluating on the test split.
I0207 07:54:14.029600 140107197974336 submission_runner.py:408] Time since start: 62407.29s, 	Step: 119751, 	{'train/accuracy': 0.6143554449081421, 'train/loss': 1.6402643918991089, 'validation/accuracy': 0.5634399652481079, 'validation/loss': 1.8867182731628418, 'validation/num_examples': 50000, 'test/accuracy': 0.4409000277519226, 'test/loss': 2.555757999420166, 'test/num_examples': 10000, 'score': 55496.04265832901, 'total_duration': 62407.289147138596, 'accumulated_submission_time': 55496.04265832901, 'accumulated_eval_time': 6898.902683258057, 'accumulated_logging_time': 5.5450968742370605}
I0207 07:54:14.067862 139946414638848 logging_writer.py:48] [119751] accumulated_eval_time=6898.902683, accumulated_logging_time=5.545097, accumulated_submission_time=55496.042658, global_step=119751, preemption_count=0, score=55496.042658, test/accuracy=0.440900, test/loss=2.555758, test/num_examples=10000, total_duration=62407.289147, train/accuracy=0.614355, train/loss=1.640264, validation/accuracy=0.563440, validation/loss=1.886718, validation/num_examples=50000
I0207 07:54:33.744347 139946397853440 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.5158839225769043, loss=2.5914483070373535
I0207 07:55:19.382950 139946414638848 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.3275256156921387, loss=2.8003973960876465
I0207 07:56:06.029210 139946397853440 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.3006311655044556, loss=2.5677435398101807
I0207 07:56:53.078670 139946414638848 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.1824612617492676, loss=3.752185821533203
I0207 07:57:39.714373 139946397853440 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.311737060546875, loss=2.775413990020752
I0207 07:58:26.616435 139946414638848 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.1030020713806152, loss=4.410656452178955
I0207 07:59:13.580168 139946397853440 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.169951319694519, loss=3.5506579875946045
I0207 08:00:00.408094 139946414638848 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.441292643547058, loss=2.613009214401245
I0207 08:00:47.330957 139946397853440 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.2064722776412964, loss=3.58522629737854
I0207 08:01:14.303030 140107197974336 spec.py:321] Evaluating on the training split.
I0207 08:01:25.484065 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 08:02:03.428766 140107197974336 spec.py:349] Evaluating on the test split.
I0207 08:02:05.025276 140107197974336 submission_runner.py:408] Time since start: 62878.28s, 	Step: 120659, 	{'train/accuracy': 0.6095117330551147, 'train/loss': 1.7067832946777344, 'validation/accuracy': 0.5638799667358398, 'validation/loss': 1.9024053812026978, 'validation/num_examples': 50000, 'test/accuracy': 0.44690001010894775, 'test/loss': 2.5595107078552246, 'test/num_examples': 10000, 'score': 55916.214358091354, 'total_duration': 62878.28482103348, 'accumulated_submission_time': 55916.214358091354, 'accumulated_eval_time': 6949.624935626984, 'accumulated_logging_time': 5.5959556102752686}
I0207 08:02:05.060384 139946414638848 logging_writer.py:48] [120659] accumulated_eval_time=6949.624936, accumulated_logging_time=5.595956, accumulated_submission_time=55916.214358, global_step=120659, preemption_count=0, score=55916.214358, test/accuracy=0.446900, test/loss=2.559511, test/num_examples=10000, total_duration=62878.284821, train/accuracy=0.609512, train/loss=1.706783, validation/accuracy=0.563880, validation/loss=1.902405, validation/num_examples=50000
I0207 08:02:21.582821 139946397853440 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.2632640600204468, loss=2.665602207183838
I0207 08:03:06.779934 139946414638848 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.1779067516326904, loss=3.7170796394348145
I0207 08:03:53.530124 139946397853440 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.1276096105575562, loss=5.240555763244629
I0207 08:04:40.610785 139946414638848 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.4611005783081055, loss=2.7145836353302
I0207 08:05:27.316133 139946397853440 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.336197018623352, loss=2.761530876159668
I0207 08:06:14.042347 139946414638848 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.3602046966552734, loss=2.7066030502319336
I0207 08:07:00.966784 139946397853440 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.1772849559783936, loss=3.5760104656219482
I0207 08:07:48.417175 139946414638848 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.459022045135498, loss=2.7577943801879883
I0207 08:08:35.113659 139946397853440 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.413000226020813, loss=2.765592336654663
I0207 08:09:05.272778 140107197974336 spec.py:321] Evaluating on the training split.
I0207 08:09:16.211854 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 08:09:55.056805 140107197974336 spec.py:349] Evaluating on the test split.
I0207 08:09:56.665010 140107197974336 submission_runner.py:408] Time since start: 63349.92s, 	Step: 121566, 	{'train/accuracy': 0.6133007407188416, 'train/loss': 1.6413342952728271, 'validation/accuracy': 0.5678399801254272, 'validation/loss': 1.860566258430481, 'validation/num_examples': 50000, 'test/accuracy': 0.4475000202655792, 'test/loss': 2.534411907196045, 'test/num_examples': 10000, 'score': 56336.366022109985, 'total_duration': 63349.9245557785, 'accumulated_submission_time': 56336.366022109985, 'accumulated_eval_time': 7001.017154455185, 'accumulated_logging_time': 5.640802383422852}
I0207 08:09:56.704141 139946414638848 logging_writer.py:48] [121566] accumulated_eval_time=7001.017154, accumulated_logging_time=5.640802, accumulated_submission_time=56336.366022, global_step=121566, preemption_count=0, score=56336.366022, test/accuracy=0.447500, test/loss=2.534412, test/num_examples=10000, total_duration=63349.924556, train/accuracy=0.613301, train/loss=1.641334, validation/accuracy=0.567840, validation/loss=1.860566, validation/num_examples=50000
I0207 08:10:10.461277 139946397853440 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.160736322402954, loss=4.048856735229492
I0207 08:10:55.088903 139946414638848 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.4142124652862549, loss=2.631646156311035
I0207 08:11:42.003986 139946397853440 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.4552587270736694, loss=2.792266607284546
I0207 08:12:29.063127 139946414638848 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.3407154083251953, loss=4.533779621124268
I0207 08:13:16.007317 139946397853440 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.0956608057022095, loss=5.153512001037598
I0207 08:14:03.062654 139946414638848 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.3547152280807495, loss=2.6383423805236816
I0207 08:14:49.914868 139946397853440 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.2012211084365845, loss=3.1030383110046387
I0207 08:15:37.018391 139946414638848 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.3623089790344238, loss=2.6792984008789062
I0207 08:16:23.857892 139946397853440 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.3611118793487549, loss=3.0770504474639893
I0207 08:16:56.733528 140107197974336 spec.py:321] Evaluating on the training split.
I0207 08:17:07.574756 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 08:17:45.793114 140107197974336 spec.py:349] Evaluating on the test split.
I0207 08:17:47.393110 140107197974336 submission_runner.py:408] Time since start: 63820.65s, 	Step: 122472, 	{'train/accuracy': 0.62255859375, 'train/loss': 1.6071964502334595, 'validation/accuracy': 0.5728999972343445, 'validation/loss': 1.8406982421875, 'validation/num_examples': 50000, 'test/accuracy': 0.45500001311302185, 'test/loss': 2.503061532974243, 'test/num_examples': 10000, 'score': 56756.33172440529, 'total_duration': 63820.65265607834, 'accumulated_submission_time': 56756.33172440529, 'accumulated_eval_time': 7051.67671251297, 'accumulated_logging_time': 5.691583156585693}
I0207 08:17:47.431726 139946414638848 logging_writer.py:48] [122472] accumulated_eval_time=7051.676713, accumulated_logging_time=5.691583, accumulated_submission_time=56756.331724, global_step=122472, preemption_count=0, score=56756.331724, test/accuracy=0.455000, test/loss=2.503062, test/num_examples=10000, total_duration=63820.652656, train/accuracy=0.622559, train/loss=1.607196, validation/accuracy=0.572900, validation/loss=1.840698, validation/num_examples=50000
I0207 08:17:58.837689 139946397853440 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.4936405420303345, loss=2.727045774459839
I0207 08:18:43.445029 139946414638848 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.4088391065597534, loss=2.7604269981384277
I0207 08:19:30.133118 139946397853440 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.6531997919082642, loss=2.589837074279785
I0207 08:20:17.026436 139946414638848 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.3875353336334229, loss=2.7263498306274414
I0207 08:21:03.846066 139946397853440 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.239991545677185, loss=3.1636500358581543
I0207 08:21:50.856646 139946414638848 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.2333054542541504, loss=3.0716516971588135
I0207 08:22:37.910119 139946397853440 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.68182373046875, loss=3.088732957839966
I0207 08:23:25.167060 139946414638848 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.4562472105026245, loss=2.574301242828369
I0207 08:24:11.995935 139946397853440 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.2994149923324585, loss=2.6485109329223633
I0207 08:24:47.791745 140107197974336 spec.py:321] Evaluating on the training split.
I0207 08:24:58.686698 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 08:25:38.705661 140107197974336 spec.py:349] Evaluating on the test split.
I0207 08:25:40.307176 140107197974336 submission_runner.py:408] Time since start: 64293.57s, 	Step: 123377, 	{'train/accuracy': 0.6289257407188416, 'train/loss': 1.570678949356079, 'validation/accuracy': 0.579479992389679, 'validation/loss': 1.805068850517273, 'validation/num_examples': 50000, 'test/accuracy': 0.4579000174999237, 'test/loss': 2.4760637283325195, 'test/num_examples': 10000, 'score': 57176.62945842743, 'total_duration': 64293.56670308113, 'accumulated_submission_time': 57176.62945842743, 'accumulated_eval_time': 7104.192118406296, 'accumulated_logging_time': 5.741584062576294}
I0207 08:25:40.345442 139946414638848 logging_writer.py:48] [123377] accumulated_eval_time=7104.192118, accumulated_logging_time=5.741584, accumulated_submission_time=57176.629458, global_step=123377, preemption_count=0, score=57176.629458, test/accuracy=0.457900, test/loss=2.476064, test/num_examples=10000, total_duration=64293.566703, train/accuracy=0.628926, train/loss=1.570679, validation/accuracy=0.579480, validation/loss=1.805069, validation/num_examples=50000
I0207 08:25:49.780269 139946397853440 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.3596760034561157, loss=2.5453453063964844
I0207 08:26:33.533313 139946414638848 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.48013174533844, loss=2.6884124279022217
I0207 08:27:20.086061 139946397853440 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.3710800409317017, loss=2.8365163803100586
I0207 08:28:07.093674 139946414638848 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.3411352634429932, loss=2.461709976196289
I0207 08:28:53.856806 139946397853440 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.542747974395752, loss=2.7815356254577637
I0207 08:29:40.807126 139946414638848 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.4988652467727661, loss=2.873378276824951
I0207 08:30:27.458030 139946397853440 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.4515767097473145, loss=2.614149570465088
I0207 08:31:14.175554 139946414638848 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.4712855815887451, loss=2.7010750770568848
I0207 08:32:00.899859 139946397853440 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.5323716402053833, loss=2.7000434398651123
I0207 08:32:40.745866 140107197974336 spec.py:321] Evaluating on the training split.
I0207 08:32:51.914685 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 08:33:31.863311 140107197974336 spec.py:349] Evaluating on the test split.
I0207 08:33:33.469602 140107197974336 submission_runner.py:408] Time since start: 64766.73s, 	Step: 124287, 	{'train/accuracy': 0.6289257407188416, 'train/loss': 1.5635874271392822, 'validation/accuracy': 0.5827199816703796, 'validation/loss': 1.768791675567627, 'validation/num_examples': 50000, 'test/accuracy': 0.4643000364303589, 'test/loss': 2.429023504257202, 'test/num_examples': 10000, 'score': 57596.96713638306, 'total_duration': 64766.72912335396, 'accumulated_submission_time': 57596.96713638306, 'accumulated_eval_time': 7156.915832519531, 'accumulated_logging_time': 5.790433645248413}
I0207 08:33:33.515117 139946414638848 logging_writer.py:48] [124287] accumulated_eval_time=7156.915833, accumulated_logging_time=5.790434, accumulated_submission_time=57596.967136, global_step=124287, preemption_count=0, score=57596.967136, test/accuracy=0.464300, test/loss=2.429024, test/num_examples=10000, total_duration=64766.729123, train/accuracy=0.628926, train/loss=1.563587, validation/accuracy=0.582720, validation/loss=1.768792, validation/num_examples=50000
I0207 08:33:39.037622 139946397853440 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.3722158670425415, loss=3.015106678009033
I0207 08:34:22.398205 139946414638848 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.3995224237442017, loss=2.6402149200439453
I0207 08:35:09.000144 139946397853440 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.4626970291137695, loss=3.0163888931274414
I0207 08:35:55.827302 139946414638848 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.1128768920898438, loss=5.1416802406311035
I0207 08:36:42.780000 139946397853440 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.277262806892395, loss=2.939840078353882
I0207 08:37:29.636224 139946414638848 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.567386269569397, loss=2.599259376525879
I0207 08:38:16.778820 139946397853440 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.2541464567184448, loss=3.5908823013305664
I0207 08:39:03.626460 139946414638848 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.4395979642868042, loss=2.6533446311950684
I0207 08:39:50.620233 139946397853440 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.5515639781951904, loss=2.5870518684387207
I0207 08:40:33.691482 140107197974336 spec.py:321] Evaluating on the training split.
I0207 08:40:44.623589 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 08:41:23.950203 140107197974336 spec.py:349] Evaluating on the test split.
I0207 08:41:25.552345 140107197974336 submission_runner.py:408] Time since start: 65238.81s, 	Step: 125193, 	{'train/accuracy': 0.6286327838897705, 'train/loss': 1.5781629085540771, 'validation/accuracy': 0.5806199908256531, 'validation/loss': 1.8158116340637207, 'validation/num_examples': 50000, 'test/accuracy': 0.46160003542900085, 'test/loss': 2.4752357006073, 'test/num_examples': 10000, 'score': 58017.08203911781, 'total_duration': 65238.811891555786, 'accumulated_submission_time': 58017.08203911781, 'accumulated_eval_time': 7208.776687383652, 'accumulated_logging_time': 5.846323251724243}
I0207 08:41:25.588463 139946414638848 logging_writer.py:48] [125193] accumulated_eval_time=7208.776687, accumulated_logging_time=5.846323, accumulated_submission_time=58017.082039, global_step=125193, preemption_count=0, score=58017.082039, test/accuracy=0.461600, test/loss=2.475236, test/num_examples=10000, total_duration=65238.811892, train/accuracy=0.628633, train/loss=1.578163, validation/accuracy=0.580620, validation/loss=1.815812, validation/num_examples=50000
I0207 08:41:28.741613 139946397853440 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.512074589729309, loss=2.805248737335205
I0207 08:42:11.604417 139946414638848 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.3646233081817627, loss=2.6758317947387695
I0207 08:42:58.299620 139946397853440 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.205885887145996, loss=4.9441986083984375
I0207 08:43:45.412149 139946414638848 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.4980530738830566, loss=2.6206860542297363
I0207 08:44:32.368554 139946397853440 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.305153727531433, loss=3.4968929290771484
I0207 08:45:19.257314 139946414638848 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.364677906036377, loss=2.657442092895508
I0207 08:46:05.992385 139946397853440 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.4929472208023071, loss=2.685861825942993
I0207 08:46:52.769052 139946414638848 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.4098873138427734, loss=3.5246522426605225
I0207 08:47:39.673033 139946397853440 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.248557686805725, loss=4.079837322235107
I0207 08:48:25.621452 140107197974336 spec.py:321] Evaluating on the training split.
I0207 08:48:37.020722 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 08:49:14.505288 140107197974336 spec.py:349] Evaluating on the test split.
I0207 08:49:16.105748 140107197974336 submission_runner.py:408] Time since start: 65709.37s, 	Step: 126099, 	{'train/accuracy': 0.6649023294448853, 'train/loss': 1.4432421922683716, 'validation/accuracy': 0.5847399830818176, 'validation/loss': 1.7886931896209717, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.447232484817505, 'test/num_examples': 10000, 'score': 58437.05323433876, 'total_duration': 65709.36529541016, 'accumulated_submission_time': 58437.05323433876, 'accumulated_eval_time': 7259.260968923569, 'accumulated_logging_time': 5.892705202102661}
I0207 08:49:16.143694 139946414638848 logging_writer.py:48] [126099] accumulated_eval_time=7259.260969, accumulated_logging_time=5.892705, accumulated_submission_time=58437.053234, global_step=126099, preemption_count=0, score=58437.053234, test/accuracy=0.472600, test/loss=2.447232, test/num_examples=10000, total_duration=65709.365295, train/accuracy=0.664902, train/loss=1.443242, validation/accuracy=0.584740, validation/loss=1.788693, validation/num_examples=50000
I0207 08:49:16.940361 139946397853440 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.5386031866073608, loss=2.5924696922302246
I0207 08:49:59.253859 139946414638848 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.479650616645813, loss=2.5009615421295166
I0207 08:50:46.034075 139946397853440 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.1210970878601074, loss=4.784209251403809
I0207 08:51:33.279788 139946414638848 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.4019601345062256, loss=3.1197705268859863
I0207 08:52:19.886921 139946397853440 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.5017808675765991, loss=5.279688835144043
I0207 08:53:06.736462 139946414638848 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.2208921909332275, loss=4.190543174743652
I0207 08:53:53.576831 139946397853440 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.3526842594146729, loss=2.6706910133361816
I0207 08:54:40.667050 139946414638848 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.539705514907837, loss=2.8064041137695312
I0207 08:55:27.571517 139946397853440 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.5892382860183716, loss=2.604123830795288
I0207 08:56:14.689847 139946414638848 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.3925691843032837, loss=2.828136444091797
I0207 08:56:16.239621 140107197974336 spec.py:321] Evaluating on the training split.
I0207 08:56:27.240645 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 08:57:05.248843 140107197974336 spec.py:349] Evaluating on the test split.
I0207 08:57:06.847989 140107197974336 submission_runner.py:408] Time since start: 66180.11s, 	Step: 127005, 	{'train/accuracy': 0.6364062428474426, 'train/loss': 1.5266468524932861, 'validation/accuracy': 0.5922799706459045, 'validation/loss': 1.7352849245071411, 'validation/num_examples': 50000, 'test/accuracy': 0.4724000096321106, 'test/loss': 2.40802001953125, 'test/num_examples': 10000, 'score': 58857.087052583694, 'total_duration': 66180.10751318932, 'accumulated_submission_time': 58857.087052583694, 'accumulated_eval_time': 7309.869294166565, 'accumulated_logging_time': 5.941359758377075}
I0207 08:57:06.889916 139946397853440 logging_writer.py:48] [127005] accumulated_eval_time=7309.869294, accumulated_logging_time=5.941360, accumulated_submission_time=58857.087053, global_step=127005, preemption_count=0, score=58857.087053, test/accuracy=0.472400, test/loss=2.408020, test/num_examples=10000, total_duration=66180.107513, train/accuracy=0.636406, train/loss=1.526647, validation/accuracy=0.592280, validation/loss=1.735285, validation/num_examples=50000
I0207 08:57:47.396901 139946414638848 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.2233035564422607, loss=3.8928914070129395
I0207 08:58:33.841460 139946397853440 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.117493987083435, loss=4.965184688568115
I0207 08:59:21.182774 139946414638848 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.3948928117752075, loss=2.5285329818725586
I0207 09:00:08.298381 139946397853440 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.7137054204940796, loss=2.9370248317718506
I0207 09:00:55.116839 139946414638848 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.1624354124069214, loss=4.520604133605957
I0207 09:01:42.208094 139946397853440 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.5990899801254272, loss=2.548171043395996
I0207 09:02:29.395368 139946414638848 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.4888719320297241, loss=3.1594460010528564
I0207 09:03:16.123032 139946397853440 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.3263336420059204, loss=2.866501808166504
I0207 09:04:03.163542 139946414638848 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.5480639934539795, loss=2.5885095596313477
I0207 09:04:07.097059 140107197974336 spec.py:321] Evaluating on the training split.
I0207 09:04:18.366159 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 09:04:58.757027 140107197974336 spec.py:349] Evaluating on the test split.
I0207 09:05:00.357326 140107197974336 submission_runner.py:408] Time since start: 66653.62s, 	Step: 127910, 	{'train/accuracy': 0.6415820121765137, 'train/loss': 1.496817708015442, 'validation/accuracy': 0.5906599760055542, 'validation/loss': 1.746111512184143, 'validation/num_examples': 50000, 'test/accuracy': 0.47380003333091736, 'test/loss': 2.3906781673431396, 'test/num_examples': 10000, 'score': 59277.23089051247, 'total_duration': 66653.61686849594, 'accumulated_submission_time': 59277.23089051247, 'accumulated_eval_time': 7363.129547357559, 'accumulated_logging_time': 5.994652986526489}
I0207 09:05:00.395290 139946397853440 logging_writer.py:48] [127910] accumulated_eval_time=7363.129547, accumulated_logging_time=5.994653, accumulated_submission_time=59277.230891, global_step=127910, preemption_count=0, score=59277.230891, test/accuracy=0.473800, test/loss=2.390678, test/num_examples=10000, total_duration=66653.616868, train/accuracy=0.641582, train/loss=1.496818, validation/accuracy=0.590660, validation/loss=1.746112, validation/num_examples=50000
I0207 09:05:38.889337 139946414638848 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.2300132513046265, loss=4.226471424102783
I0207 09:06:25.644659 139946397853440 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.3219331502914429, loss=3.70304012298584
I0207 09:07:12.733887 139946414638848 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.3154895305633545, loss=4.920029640197754
I0207 09:07:59.654945 139946397853440 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.27786386013031, loss=5.216313362121582
I0207 09:08:46.814548 139946414638848 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.5599473714828491, loss=2.7583770751953125
I0207 09:09:33.687230 139946397853440 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.727640986442566, loss=2.571744918823242
I0207 09:10:20.803849 139946414638848 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.4015833139419556, loss=3.055171012878418
I0207 09:11:07.957103 139946397853440 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.4485104084014893, loss=2.969788074493408
I0207 09:11:55.264525 139946414638848 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.615552306175232, loss=2.635709524154663
I0207 09:12:00.548532 140107197974336 spec.py:321] Evaluating on the training split.
I0207 09:12:11.485590 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 09:12:51.365975 140107197974336 spec.py:349] Evaluating on the test split.
I0207 09:12:52.977732 140107197974336 submission_runner.py:408] Time since start: 67126.24s, 	Step: 128813, 	{'train/accuracy': 0.6640819907188416, 'train/loss': 1.4066020250320435, 'validation/accuracy': 0.5945999622344971, 'validation/loss': 1.7294906377792358, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.3901915550231934, 'test/num_examples': 10000, 'score': 59697.32144474983, 'total_duration': 67126.2372546196, 'accumulated_submission_time': 59697.32144474983, 'accumulated_eval_time': 7415.558722019196, 'accumulated_logging_time': 6.044222116470337}
I0207 09:12:53.026662 139946397853440 logging_writer.py:48] [128813] accumulated_eval_time=7415.558722, accumulated_logging_time=6.044222, accumulated_submission_time=59697.321445, global_step=128813, preemption_count=0, score=59697.321445, test/accuracy=0.475900, test/loss=2.390192, test/num_examples=10000, total_duration=67126.237255, train/accuracy=0.664082, train/loss=1.406602, validation/accuracy=0.594600, validation/loss=1.729491, validation/num_examples=50000
I0207 09:13:29.770087 139946414638848 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.5666996240615845, loss=2.499828815460205
I0207 09:14:16.209670 139946397853440 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.570816159248352, loss=2.4467954635620117
I0207 09:15:03.313593 139946414638848 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.3324373960494995, loss=5.077939033508301
I0207 09:15:50.207862 139946397853440 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.4644356966018677, loss=2.8792786598205566
I0207 09:16:37.126604 139946414638848 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.4233450889587402, loss=3.8854494094848633
I0207 09:17:23.979707 139946397853440 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.5393460988998413, loss=2.559833526611328
I0207 09:18:11.189417 139946414638848 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.295859456062317, loss=4.965053558349609
I0207 09:18:57.925161 139946397853440 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.6350196599960327, loss=2.6211185455322266
I0207 09:19:44.985180 139946414638848 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.6439909934997559, loss=2.612328290939331
I0207 09:19:53.183712 140107197974336 spec.py:321] Evaluating on the training split.
I0207 09:20:04.426475 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 09:20:42.674035 140107197974336 spec.py:349] Evaluating on the test split.
I0207 09:20:44.273973 140107197974336 submission_runner.py:408] Time since start: 67597.53s, 	Step: 129719, 	{'train/accuracy': 0.6342382431030273, 'train/loss': 1.546700358390808, 'validation/accuracy': 0.5904600024223328, 'validation/loss': 1.7574831247329712, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.420825242996216, 'test/num_examples': 10000, 'score': 60117.41744160652, 'total_duration': 67597.53349399567, 'accumulated_submission_time': 60117.41744160652, 'accumulated_eval_time': 7466.6489408016205, 'accumulated_logging_time': 6.103210926055908}
I0207 09:20:44.318002 139946397853440 logging_writer.py:48] [129719] accumulated_eval_time=7466.648941, accumulated_logging_time=6.103211, accumulated_submission_time=60117.417442, global_step=129719, preemption_count=0, score=60117.417442, test/accuracy=0.469500, test/loss=2.420825, test/num_examples=10000, total_duration=67597.533494, train/accuracy=0.634238, train/loss=1.546700, validation/accuracy=0.590460, validation/loss=1.757483, validation/num_examples=50000
I0207 09:21:18.331226 139946414638848 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.5932066440582275, loss=2.7029166221618652
I0207 09:22:04.675726 139946397853440 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.644966959953308, loss=2.53843355178833
I0207 09:22:51.488719 139946414638848 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.4438529014587402, loss=2.6770641803741455
I0207 09:23:38.430078 139946397853440 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.3427170515060425, loss=3.249197483062744
I0207 09:24:25.493669 139946414638848 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.5218480825424194, loss=2.987135887145996
I0207 09:25:12.478855 139946397853440 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.3964385986328125, loss=5.187596321105957
I0207 09:25:59.479228 139946414638848 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.539367914199829, loss=2.5439565181732178
I0207 09:26:46.238776 139946397853440 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.3208568096160889, loss=5.041626930236816
I0207 09:27:33.147044 139946414638848 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.5027401447296143, loss=4.365607738494873
I0207 09:27:44.544330 140107197974336 spec.py:321] Evaluating on the training split.
I0207 09:27:56.622520 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 09:28:33.537883 140107197974336 spec.py:349] Evaluating on the test split.
I0207 09:28:35.139080 140107197974336 submission_runner.py:408] Time since start: 68068.40s, 	Step: 130626, 	{'train/accuracy': 0.6511132717132568, 'train/loss': 1.4745047092437744, 'validation/accuracy': 0.5997399687767029, 'validation/loss': 1.7100039720535278, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.387399196624756, 'test/num_examples': 10000, 'score': 60537.581053733826, 'total_duration': 68068.39860129356, 'accumulated_submission_time': 60537.581053733826, 'accumulated_eval_time': 7517.243643760681, 'accumulated_logging_time': 6.158584356307983}
I0207 09:28:35.183258 139946397853440 logging_writer.py:48] [130626] accumulated_eval_time=7517.243644, accumulated_logging_time=6.158584, accumulated_submission_time=60537.581054, global_step=130626, preemption_count=0, score=60537.581054, test/accuracy=0.478500, test/loss=2.387399, test/num_examples=10000, total_duration=68068.398601, train/accuracy=0.651113, train/loss=1.474505, validation/accuracy=0.599740, validation/loss=1.710004, validation/num_examples=50000
I0207 09:29:05.759459 139946414638848 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.6235815286636353, loss=2.571911096572876
I0207 09:29:52.009881 139946397853440 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.2391061782836914, loss=4.007874965667725
I0207 09:30:38.832753 139946414638848 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.5322076082229614, loss=2.4360711574554443
I0207 09:31:25.586238 139946397853440 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.3440816402435303, loss=3.335651159286499
I0207 09:32:12.292788 139946414638848 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.5867058038711548, loss=2.5011532306671143
I0207 09:32:58.881153 139946397853440 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.7104642391204834, loss=2.5896384716033936
I0207 09:33:45.496929 139946414638848 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.6608859300613403, loss=2.4697275161743164
I0207 09:34:32.759492 139946397853440 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.529348373413086, loss=2.7321341037750244
I0207 09:35:19.687525 139946414638848 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.5610082149505615, loss=2.5219016075134277
I0207 09:35:35.229357 140107197974336 spec.py:321] Evaluating on the training split.
I0207 09:35:46.207444 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 09:36:27.569253 140107197974336 spec.py:349] Evaluating on the test split.
I0207 09:36:29.167895 140107197974336 submission_runner.py:408] Time since start: 68542.43s, 	Step: 131535, 	{'train/accuracy': 0.6660742163658142, 'train/loss': 1.3897231817245483, 'validation/accuracy': 0.6065599918365479, 'validation/loss': 1.6731947660446167, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.348107099533081, 'test/num_examples': 10000, 'score': 60957.56353855133, 'total_duration': 68542.42744445801, 'accumulated_submission_time': 60957.56353855133, 'accumulated_eval_time': 7571.182153224945, 'accumulated_logging_time': 6.214536190032959}
I0207 09:36:29.206166 139946397853440 logging_writer.py:48] [131535] accumulated_eval_time=7571.182153, accumulated_logging_time=6.214536, accumulated_submission_time=60957.563539, global_step=131535, preemption_count=0, score=60957.563539, test/accuracy=0.480000, test/loss=2.348107, test/num_examples=10000, total_duration=68542.427444, train/accuracy=0.666074, train/loss=1.389723, validation/accuracy=0.606560, validation/loss=1.673195, validation/num_examples=50000
I0207 09:36:55.657424 139946414638848 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.2846670150756836, loss=4.881686687469482
I0207 09:37:42.078579 139946397853440 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.2870440483093262, loss=4.885065078735352
I0207 09:38:29.006520 139946414638848 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.2533178329467773, loss=3.7167892456054688
I0207 09:39:15.920635 139946397853440 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.430150032043457, loss=3.2359509468078613
I0207 09:40:03.167951 139946414638848 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.4888836145401, loss=4.315865993499756
I0207 09:40:49.838161 139946397853440 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.5752513408660889, loss=2.3863894939422607
I0207 09:41:36.770632 139946414638848 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.4776499271392822, loss=2.8563661575317383
I0207 09:42:23.710402 139946397853440 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.5631028413772583, loss=4.34718656539917
I0207 09:43:10.630677 139946414638848 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.5191558599472046, loss=2.525099277496338
I0207 09:43:29.400185 140107197974336 spec.py:321] Evaluating on the training split.
I0207 09:43:40.010596 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 09:44:18.578640 140107197974336 spec.py:349] Evaluating on the test split.
I0207 09:44:20.178913 140107197974336 submission_runner.py:408] Time since start: 69013.44s, 	Step: 132442, 	{'train/accuracy': 0.6496874690055847, 'train/loss': 1.4811943769454956, 'validation/accuracy': 0.6088399887084961, 'validation/loss': 1.6800014972686768, 'validation/num_examples': 50000, 'test/accuracy': 0.48130002617836, 'test/loss': 2.3436646461486816, 'test/num_examples': 10000, 'score': 61377.6965944767, 'total_duration': 69013.43845295906, 'accumulated_submission_time': 61377.6965944767, 'accumulated_eval_time': 7621.960869312286, 'accumulated_logging_time': 6.263074159622192}
I0207 09:44:20.216087 139946397853440 logging_writer.py:48] [132442] accumulated_eval_time=7621.960869, accumulated_logging_time=6.263074, accumulated_submission_time=61377.696594, global_step=132442, preemption_count=0, score=61377.696594, test/accuracy=0.481300, test/loss=2.343665, test/num_examples=10000, total_duration=69013.438453, train/accuracy=0.649687, train/loss=1.481194, validation/accuracy=0.608840, validation/loss=1.680001, validation/num_examples=50000
I0207 09:44:43.409261 139946414638848 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.6715009212493896, loss=2.5051424503326416
I0207 09:45:29.919717 139946397853440 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.2918510437011719, loss=4.0207366943359375
I0207 09:46:16.983103 139946414638848 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.6306105852127075, loss=2.714472532272339
I0207 09:47:03.995209 139946397853440 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.810447096824646, loss=2.6464266777038574
I0207 09:47:50.752487 139946414638848 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.7113155126571655, loss=4.854674339294434
I0207 09:48:37.784153 139946397853440 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.5040342807769775, loss=2.5408945083618164
I0207 09:49:24.718201 139946414638848 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.5983611345291138, loss=2.4969711303710938
I0207 09:50:11.467855 139946397853440 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.781156063079834, loss=2.4973607063293457
I0207 09:50:58.065773 139946414638848 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.6347953081130981, loss=2.4896035194396973
I0207 09:51:20.375789 140107197974336 spec.py:321] Evaluating on the training split.
I0207 09:51:31.517447 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 09:52:12.369916 140107197974336 spec.py:349] Evaluating on the test split.
I0207 09:52:13.970796 140107197974336 submission_runner.py:408] Time since start: 69487.23s, 	Step: 133349, 	{'train/accuracy': 0.6616796851158142, 'train/loss': 1.4510056972503662, 'validation/accuracy': 0.6108399629592896, 'validation/loss': 1.6753292083740234, 'validation/num_examples': 50000, 'test/accuracy': 0.4870000183582306, 'test/loss': 2.3509786128997803, 'test/num_examples': 10000, 'score': 61797.793632507324, 'total_duration': 69487.23033475876, 'accumulated_submission_time': 61797.793632507324, 'accumulated_eval_time': 7675.555843830109, 'accumulated_logging_time': 6.311018705368042}
I0207 09:52:14.008533 139946397853440 logging_writer.py:48] [133349] accumulated_eval_time=7675.555844, accumulated_logging_time=6.311019, accumulated_submission_time=61797.793633, global_step=133349, preemption_count=0, score=61797.793633, test/accuracy=0.487000, test/loss=2.350979, test/num_examples=10000, total_duration=69487.230335, train/accuracy=0.661680, train/loss=1.451006, validation/accuracy=0.610840, validation/loss=1.675329, validation/num_examples=50000
I0207 09:52:34.451338 139946414638848 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.6662468910217285, loss=2.6100001335144043
I0207 09:53:20.083796 139946397853440 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.6472339630126953, loss=2.462287664413452
I0207 09:54:07.050852 139946414638848 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.0364599227905273, loss=2.8116090297698975
I0207 09:54:54.127053 139946397853440 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.6348553895950317, loss=2.3804879188537598
I0207 09:55:40.688088 139946414638848 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.4293200969696045, loss=3.8914029598236084
I0207 09:56:27.776787 139946397853440 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.487558126449585, loss=2.729717969894409
I0207 09:57:14.565648 139946414638848 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.5184186697006226, loss=2.4692928791046143
I0207 09:58:01.257740 139946397853440 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.789670467376709, loss=2.5417520999908447
I0207 09:58:47.971812 139946414638848 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.514295220375061, loss=2.8834073543548584
I0207 09:59:13.993106 140107197974336 spec.py:321] Evaluating on the training split.
I0207 09:59:24.683836 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 10:00:06.345478 140107197974336 spec.py:349] Evaluating on the test split.
I0207 10:00:07.937094 140107197974336 submission_runner.py:408] Time since start: 69961.20s, 	Step: 134257, 	{'train/accuracy': 0.6688085794448853, 'train/loss': 1.3909156322479248, 'validation/accuracy': 0.6072199940681458, 'validation/loss': 1.6612194776535034, 'validation/num_examples': 50000, 'test/accuracy': 0.48920002579689026, 'test/loss': 2.3225879669189453, 'test/num_examples': 10000, 'score': 62217.71622133255, 'total_duration': 69961.19663286209, 'accumulated_submission_time': 62217.71622133255, 'accumulated_eval_time': 7729.499813079834, 'accumulated_logging_time': 6.358997106552124}
I0207 10:00:07.974044 139946397853440 logging_writer.py:48] [134257] accumulated_eval_time=7729.499813, accumulated_logging_time=6.358997, accumulated_submission_time=62217.716221, global_step=134257, preemption_count=0, score=62217.716221, test/accuracy=0.489200, test/loss=2.322588, test/num_examples=10000, total_duration=69961.196633, train/accuracy=0.668809, train/loss=1.390916, validation/accuracy=0.607220, validation/loss=1.661219, validation/num_examples=50000
I0207 10:00:25.286755 139946414638848 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.5730576515197754, loss=2.596592903137207
I0207 10:01:10.863788 139946397853440 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.5575282573699951, loss=2.3004677295684814
I0207 10:01:57.728929 139946414638848 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.6359444856643677, loss=2.3007383346557617
I0207 10:02:44.699374 139946397853440 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.5690213441848755, loss=2.4875993728637695
I0207 10:03:31.545080 139946414638848 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.4442230463027954, loss=4.067918300628662
I0207 10:04:18.436251 139946397853440 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.5801455974578857, loss=2.4238319396972656
I0207 10:05:05.414127 139946414638848 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.654045581817627, loss=2.7007181644439697
I0207 10:05:52.357606 139946397853440 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.4891901016235352, loss=3.1831772327423096
I0207 10:06:39.202836 139946414638848 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.5093220472335815, loss=2.679413318634033
I0207 10:07:07.955188 140107197974336 spec.py:321] Evaluating on the training split.
I0207 10:07:19.105990 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 10:08:00.460936 140107197974336 spec.py:349] Evaluating on the test split.
I0207 10:08:02.071042 140107197974336 submission_runner.py:408] Time since start: 70435.33s, 	Step: 135162, 	{'train/accuracy': 0.6540820002555847, 'train/loss': 1.4487427473068237, 'validation/accuracy': 0.6113799810409546, 'validation/loss': 1.649997591972351, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.3041481971740723, 'test/num_examples': 10000, 'score': 62637.634321689606, 'total_duration': 70435.33058691025, 'accumulated_submission_time': 62637.634321689606, 'accumulated_eval_time': 7783.615670204163, 'accumulated_logging_time': 6.407909154891968}
I0207 10:08:02.108727 139946397853440 logging_writer.py:48] [135162] accumulated_eval_time=7783.615670, accumulated_logging_time=6.407909, accumulated_submission_time=62637.634322, global_step=135162, preemption_count=0, score=62637.634322, test/accuracy=0.490700, test/loss=2.304148, test/num_examples=10000, total_duration=70435.330587, train/accuracy=0.654082, train/loss=1.448743, validation/accuracy=0.611380, validation/loss=1.649998, validation/num_examples=50000
I0207 10:08:17.437817 139946414638848 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.5118184089660645, loss=4.337525844573975
I0207 10:09:02.510013 139946397853440 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.7288068532943726, loss=2.4430603981018066
I0207 10:09:49.212015 139946414638848 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.75439453125, loss=2.40071702003479
I0207 10:10:36.495851 139946397853440 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.4531073570251465, loss=4.847916126251221
I0207 10:11:23.386949 139946414638848 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.745904564857483, loss=2.437108039855957
I0207 10:12:10.505119 139946397853440 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.451446294784546, loss=4.6916375160217285
I0207 10:12:57.325221 139946414638848 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.641069769859314, loss=2.7529211044311523
I0207 10:13:44.189479 139946397853440 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.5797184705734253, loss=2.2510664463043213
I0207 10:14:31.309936 139946414638848 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.4521572589874268, loss=3.0178024768829346
I0207 10:15:02.403972 140107197974336 spec.py:321] Evaluating on the training split.
I0207 10:15:13.474934 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 10:15:51.945193 140107197974336 spec.py:349] Evaluating on the test split.
I0207 10:15:53.547339 140107197974336 submission_runner.py:408] Time since start: 70906.81s, 	Step: 136068, 	{'train/accuracy': 0.6649999618530273, 'train/loss': 1.3921610116958618, 'validation/accuracy': 0.6185199618339539, 'validation/loss': 1.6188608407974243, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3076579570770264, 'test/num_examples': 10000, 'score': 63057.867579460144, 'total_duration': 70906.8068845272, 'accumulated_submission_time': 63057.867579460144, 'accumulated_eval_time': 7834.759024858475, 'accumulated_logging_time': 6.456115007400513}
I0207 10:15:53.585619 139946397853440 logging_writer.py:48] [136068] accumulated_eval_time=7834.759025, accumulated_logging_time=6.456115, accumulated_submission_time=63057.867579, global_step=136068, preemption_count=0, score=63057.867579, test/accuracy=0.490100, test/loss=2.307658, test/num_examples=10000, total_duration=70906.806885, train/accuracy=0.665000, train/loss=1.392161, validation/accuracy=0.618520, validation/loss=1.618861, validation/num_examples=50000
I0207 10:16:06.568305 139946414638848 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.688762903213501, loss=2.5310540199279785
I0207 10:16:51.193171 139946397853440 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.3449137210845947, loss=4.031868934631348
I0207 10:17:37.916296 139946414638848 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.4714350700378418, loss=4.859504699707031
I0207 10:18:24.821404 139946397853440 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.4934357404708862, loss=4.920384407043457
I0207 10:19:11.486505 139946414638848 logging_writer.py:48] [136500] global_step=136500, grad_norm=1.7225341796875, loss=2.401068687438965
I0207 10:19:58.382987 139946397853440 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.4728435277938843, loss=2.9864680767059326
I0207 10:20:45.182115 139946414638848 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.4420440196990967, loss=2.6282973289489746
I0207 10:21:32.056083 139946397853440 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.5649948120117188, loss=3.777343511581421
I0207 10:22:19.221796 139946414638848 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.5521036386489868, loss=2.327927350997925
I0207 10:22:54.061321 140107197974336 spec.py:321] Evaluating on the training split.
I0207 10:23:05.175711 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 10:23:43.354423 140107197974336 spec.py:349] Evaluating on the test split.
I0207 10:23:44.951094 140107197974336 submission_runner.py:408] Time since start: 71378.21s, 	Step: 136976, 	{'train/accuracy': 0.6775000095367432, 'train/loss': 1.342071533203125, 'validation/accuracy': 0.6184399724006653, 'validation/loss': 1.606795072555542, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.2707793712615967, 'test/num_examples': 10000, 'score': 63478.281381607056, 'total_duration': 71378.2106127739, 'accumulated_submission_time': 63478.281381607056, 'accumulated_eval_time': 7885.648756742477, 'accumulated_logging_time': 6.504778623580933}
I0207 10:23:44.995016 139946397853440 logging_writer.py:48] [136976] accumulated_eval_time=7885.648757, accumulated_logging_time=6.504779, accumulated_submission_time=63478.281382, global_step=136976, preemption_count=0, score=63478.281382, test/accuracy=0.496800, test/loss=2.270779, test/num_examples=10000, total_duration=71378.210613, train/accuracy=0.677500, train/loss=1.342072, validation/accuracy=0.618440, validation/loss=1.606795, validation/num_examples=50000
I0207 10:23:55.197192 139946414638848 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.50730562210083, loss=4.498345375061035
I0207 10:24:39.192070 139946397853440 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.7072601318359375, loss=2.241586208343506
I0207 10:25:25.773472 139946414638848 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.7198741436004639, loss=2.470951795578003
I0207 10:26:12.609894 139946397853440 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.8678507804870605, loss=4.979664325714111
I0207 10:26:59.164721 139946414638848 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.6807734966278076, loss=2.414762020111084
I0207 10:27:46.423508 139946397853440 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.4928462505340576, loss=3.8181235790252686
I0207 10:28:33.038767 139946414638848 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.5834821462631226, loss=2.7346889972686768
I0207 10:29:20.106299 139946397853440 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.8846732378005981, loss=2.3051810264587402
I0207 10:30:07.121580 139946414638848 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.5406012535095215, loss=2.4563426971435547
I0207 10:30:45.380430 140107197974336 spec.py:321] Evaluating on the training split.
I0207 10:30:56.429277 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 10:31:35.005484 140107197974336 spec.py:349] Evaluating on the test split.
I0207 10:31:36.606932 140107197974336 submission_runner.py:408] Time since start: 71849.87s, 	Step: 137884, 	{'train/accuracy': 0.6682812571525574, 'train/loss': 1.403730034828186, 'validation/accuracy': 0.6224200129508972, 'validation/loss': 1.6151797771453857, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.288226366043091, 'test/num_examples': 10000, 'score': 63898.23468732834, 'total_duration': 71849.86646604538, 'accumulated_submission_time': 63898.23468732834, 'accumulated_eval_time': 7936.875230550766, 'accumulated_logging_time': 6.928737640380859}
I0207 10:31:36.648094 139946397853440 logging_writer.py:48] [137884] accumulated_eval_time=7936.875231, accumulated_logging_time=6.928738, accumulated_submission_time=63898.234687, global_step=137884, preemption_count=0, score=63898.234687, test/accuracy=0.494800, test/loss=2.288226, test/num_examples=10000, total_duration=71849.866466, train/accuracy=0.668281, train/loss=1.403730, validation/accuracy=0.622420, validation/loss=1.615180, validation/num_examples=50000
I0207 10:31:43.336048 139946414638848 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.5624414682388306, loss=2.9087600708007812
I0207 10:32:26.801186 139946397853440 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.3548030853271484, loss=3.5435519218444824
I0207 10:33:13.455221 139946414638848 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.4889808893203735, loss=4.184240818023682
I0207 10:34:00.205383 139946397853440 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.6094069480895996, loss=2.4540481567382812
I0207 10:34:47.150438 139946414638848 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.784036636352539, loss=2.5664138793945312
I0207 10:35:33.927934 139946397853440 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.8855876922607422, loss=2.288640260696411
I0207 10:36:20.884683 139946414638848 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.4225212335586548, loss=4.080185890197754
I0207 10:37:08.040654 139946397853440 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.492788314819336, loss=4.419410228729248
I0207 10:37:54.795043 139946414638848 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.8865625858306885, loss=2.4635252952575684
I0207 10:38:36.769552 140107197974336 spec.py:321] Evaluating on the training split.
I0207 10:38:47.811706 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 10:39:25.641107 140107197974336 spec.py:349] Evaluating on the test split.
I0207 10:39:27.241602 140107197974336 submission_runner.py:408] Time since start: 72320.50s, 	Step: 138791, 	{'train/accuracy': 0.6753710508346558, 'train/loss': 1.351009488105774, 'validation/accuracy': 0.6233800053596497, 'validation/loss': 1.5772992372512817, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.231370687484741, 'test/num_examples': 10000, 'score': 64318.290568351746, 'total_duration': 72320.50111722946, 'accumulated_submission_time': 64318.290568351746, 'accumulated_eval_time': 7987.347238540649, 'accumulated_logging_time': 6.983047246932983}
I0207 10:39:27.292456 139946397853440 logging_writer.py:48] [138791] accumulated_eval_time=7987.347239, accumulated_logging_time=6.983047, accumulated_submission_time=64318.290568, global_step=138791, preemption_count=0, score=64318.290568, test/accuracy=0.499900, test/loss=2.231371, test/num_examples=10000, total_duration=72320.501117, train/accuracy=0.675371, train/loss=1.351009, validation/accuracy=0.623380, validation/loss=1.577299, validation/num_examples=50000
I0207 10:39:31.234996 139946414638848 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.5136111974716187, loss=3.0620288848876953
I0207 10:40:14.454418 139946397853440 logging_writer.py:48] [138900] global_step=138900, grad_norm=1.4284322261810303, loss=4.185046195983887
I0207 10:41:00.917184 139946414638848 logging_writer.py:48] [139000] global_step=139000, grad_norm=1.4391273260116577, loss=2.9482192993164062
I0207 10:41:47.499584 139946397853440 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.692444920539856, loss=2.2970895767211914
I0207 10:42:34.508049 139946414638848 logging_writer.py:48] [139200] global_step=139200, grad_norm=1.720455527305603, loss=2.3481578826904297
I0207 10:43:21.176683 139946397853440 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.545600414276123, loss=2.7781126499176025
I0207 10:44:08.181756 139946414638848 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.824476957321167, loss=2.3685836791992188
I0207 10:44:55.167357 139946397853440 logging_writer.py:48] [139500] global_step=139500, grad_norm=1.825270175933838, loss=2.4011833667755127
I0207 10:45:42.024212 139946414638848 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.4897115230560303, loss=3.706150531768799
I0207 10:46:27.571549 140107197974336 spec.py:321] Evaluating on the training split.
I0207 10:46:38.782032 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 10:47:18.117191 140107197974336 spec.py:349] Evaluating on the test split.
I0207 10:47:19.716255 140107197974336 submission_runner.py:408] Time since start: 72792.98s, 	Step: 139699, 	{'train/accuracy': 0.6874608993530273, 'train/loss': 1.287361741065979, 'validation/accuracy': 0.6303799748420715, 'validation/loss': 1.5501196384429932, 'validation/num_examples': 50000, 'test/accuracy': 0.5088000297546387, 'test/loss': 2.2103207111358643, 'test/num_examples': 10000, 'score': 64738.50710082054, 'total_duration': 72792.97579598427, 'accumulated_submission_time': 64738.50710082054, 'accumulated_eval_time': 8039.49192738533, 'accumulated_logging_time': 7.0454113483428955}
I0207 10:47:19.760472 139946397853440 logging_writer.py:48] [139699] accumulated_eval_time=8039.491927, accumulated_logging_time=7.045411, accumulated_submission_time=64738.507101, global_step=139699, preemption_count=0, score=64738.507101, test/accuracy=0.508800, test/loss=2.210321, test/num_examples=10000, total_duration=72792.975796, train/accuracy=0.687461, train/loss=1.287362, validation/accuracy=0.630380, validation/loss=1.550120, validation/num_examples=50000
I0207 10:47:20.562897 139946414638848 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.834378957748413, loss=2.4346108436584473
I0207 10:48:02.841123 139946397853440 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.7723618745803833, loss=2.327558755874634
I0207 10:48:49.459775 139946414638848 logging_writer.py:48] [139900] global_step=139900, grad_norm=1.7362879514694214, loss=3.233114004135132
I0207 10:49:36.408664 139946397853440 logging_writer.py:48] [140000] global_step=140000, grad_norm=1.7499819993972778, loss=2.1997687816619873
I0207 10:50:23.301335 139946414638848 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.072328567504883, loss=2.259913921356201
I0207 10:51:10.395774 139946397853440 logging_writer.py:48] [140200] global_step=140200, grad_norm=1.6812576055526733, loss=4.804815292358398
I0207 10:51:57.149088 139946414638848 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.7232789993286133, loss=2.385160207748413
I0207 10:52:44.082788 139946397853440 logging_writer.py:48] [140400] global_step=140400, grad_norm=1.666193962097168, loss=3.3779799938201904
I0207 10:53:30.957532 139946414638848 logging_writer.py:48] [140500] global_step=140500, grad_norm=1.7582082748413086, loss=2.3921611309051514
I0207 10:54:17.751367 139946397853440 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.5338574647903442, loss=3.1403017044067383
I0207 10:54:19.763942 140107197974336 spec.py:321] Evaluating on the training split.
I0207 10:54:30.984745 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 10:55:10.739041 140107197974336 spec.py:349] Evaluating on the test split.
I0207 10:55:12.340591 140107197974336 submission_runner.py:408] Time since start: 73265.60s, 	Step: 140606, 	{'train/accuracy': 0.684277355670929, 'train/loss': 1.3224250078201294, 'validation/accuracy': 0.6326999664306641, 'validation/loss': 1.5607450008392334, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.2430734634399414, 'test/num_examples': 10000, 'score': 65158.44564986229, 'total_duration': 73265.60011100769, 'accumulated_submission_time': 65158.44564986229, 'accumulated_eval_time': 8092.068526983261, 'accumulated_logging_time': 7.1019439697265625}
I0207 10:55:12.385470 139946414638848 logging_writer.py:48] [140606] accumulated_eval_time=8092.068527, accumulated_logging_time=7.101944, accumulated_submission_time=65158.445650, global_step=140606, preemption_count=0, score=65158.445650, test/accuracy=0.502900, test/loss=2.243073, test/num_examples=10000, total_duration=73265.600111, train/accuracy=0.684277, train/loss=1.322425, validation/accuracy=0.632700, validation/loss=1.560745, validation/num_examples=50000
I0207 10:55:52.277244 139946397853440 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.4931309223175049, loss=3.3987607955932617
I0207 10:56:38.553143 139946414638848 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.748120665550232, loss=2.2745838165283203
I0207 10:57:25.592353 139946397853440 logging_writer.py:48] [140900] global_step=140900, grad_norm=1.536574125289917, loss=2.98898983001709
I0207 10:58:12.355975 139946414638848 logging_writer.py:48] [141000] global_step=141000, grad_norm=1.7102022171020508, loss=2.3324663639068604
I0207 10:58:59.312332 139946397853440 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.6491875648498535, loss=2.7348499298095703
I0207 10:59:46.108623 139946414638848 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.5599390268325806, loss=3.7906055450439453
I0207 11:00:32.900640 139946397853440 logging_writer.py:48] [141300] global_step=141300, grad_norm=1.8257125616073608, loss=2.4906671047210693
I0207 11:01:20.034967 139946414638848 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.6680026054382324, loss=4.161264419555664
I0207 11:02:06.738107 139946397853440 logging_writer.py:48] [141500] global_step=141500, grad_norm=1.6892961263656616, loss=2.3860044479370117
I0207 11:02:12.521557 140107197974336 spec.py:321] Evaluating on the training split.
I0207 11:02:23.532233 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 11:03:02.547528 140107197974336 spec.py:349] Evaluating on the test split.
I0207 11:03:04.150817 140107197974336 submission_runner.py:408] Time since start: 73737.41s, 	Step: 141514, 	{'train/accuracy': 0.6889257431030273, 'train/loss': 1.2937071323394775, 'validation/accuracy': 0.6418200135231018, 'validation/loss': 1.5113493204116821, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.173495054244995, 'test/num_examples': 10000, 'score': 65578.51907277107, 'total_duration': 73737.41035723686, 'accumulated_submission_time': 65578.51907277107, 'accumulated_eval_time': 8143.69774889946, 'accumulated_logging_time': 7.15775990486145}
I0207 11:03:04.188202 139946414638848 logging_writer.py:48] [141514] accumulated_eval_time=8143.697749, accumulated_logging_time=7.157760, accumulated_submission_time=65578.519073, global_step=141514, preemption_count=0, score=65578.519073, test/accuracy=0.512600, test/loss=2.173495, test/num_examples=10000, total_duration=73737.410357, train/accuracy=0.688926, train/loss=1.293707, validation/accuracy=0.641820, validation/loss=1.511349, validation/num_examples=50000
I0207 11:03:40.258561 139946397853440 logging_writer.py:48] [141600] global_step=141600, grad_norm=1.6152148246765137, loss=4.667246341705322
I0207 11:04:26.698691 139946414638848 logging_writer.py:48] [141700] global_step=141700, grad_norm=1.5853506326675415, loss=4.469728469848633
I0207 11:05:13.710596 139946397853440 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.6884745359420776, loss=2.2104780673980713
I0207 11:06:00.422152 139946414638848 logging_writer.py:48] [141900] global_step=141900, grad_norm=1.9762424230575562, loss=2.3100364208221436
I0207 11:06:47.511646 139946397853440 logging_writer.py:48] [142000] global_step=142000, grad_norm=1.8552703857421875, loss=2.353886365890503
I0207 11:07:34.252896 139946414638848 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.735703468322754, loss=4.395164489746094
I0207 11:08:21.054063 139946397853440 logging_writer.py:48] [142200] global_step=142200, grad_norm=1.8804250955581665, loss=2.17512583732605
I0207 11:09:07.801557 139946414638848 logging_writer.py:48] [142300] global_step=142300, grad_norm=1.6684831380844116, loss=2.2539024353027344
I0207 11:09:55.110822 139946397853440 logging_writer.py:48] [142400] global_step=142400, grad_norm=1.627748966217041, loss=2.840238094329834
I0207 11:10:04.191030 140107197974336 spec.py:321] Evaluating on the training split.
I0207 11:10:15.095964 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 11:10:53.970272 140107197974336 spec.py:349] Evaluating on the test split.
I0207 11:10:55.564348 140107197974336 submission_runner.py:408] Time since start: 74208.82s, 	Step: 142421, 	{'train/accuracy': 0.6948828101158142, 'train/loss': 1.2586766481399536, 'validation/accuracy': 0.6380000114440918, 'validation/loss': 1.5189902782440186, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.185518980026245, 'test/num_examples': 10000, 'score': 65998.46021318436, 'total_duration': 74208.82389330864, 'accumulated_submission_time': 65998.46021318436, 'accumulated_eval_time': 8195.071061849594, 'accumulated_logging_time': 7.205103397369385}
I0207 11:10:55.602465 139946414638848 logging_writer.py:48] [142421] accumulated_eval_time=8195.071062, accumulated_logging_time=7.205103, accumulated_submission_time=65998.460213, global_step=142421, preemption_count=0, score=65998.460213, test/accuracy=0.513700, test/loss=2.185519, test/num_examples=10000, total_duration=74208.823893, train/accuracy=0.694883, train/loss=1.258677, validation/accuracy=0.638000, validation/loss=1.518990, validation/num_examples=50000
I0207 11:11:28.457268 139946397853440 logging_writer.py:48] [142500] global_step=142500, grad_norm=1.8502814769744873, loss=2.3465800285339355
I0207 11:12:15.191946 139946414638848 logging_writer.py:48] [142600] global_step=142600, grad_norm=1.771413803100586, loss=2.70843768119812
I0207 11:13:02.346874 139946397853440 logging_writer.py:48] [142700] global_step=142700, grad_norm=1.6171627044677734, loss=2.8336286544799805
I0207 11:13:49.326159 139946414638848 logging_writer.py:48] [142800] global_step=142800, grad_norm=1.5984309911727905, loss=3.4949474334716797
I0207 11:14:36.529261 139946397853440 logging_writer.py:48] [142900] global_step=142900, grad_norm=1.8386316299438477, loss=2.2230865955352783
I0207 11:15:23.385892 139946414638848 logging_writer.py:48] [143000] global_step=143000, grad_norm=1.7688336372375488, loss=2.2113404273986816
I0207 11:16:10.430922 139946397853440 logging_writer.py:48] [143100] global_step=143100, grad_norm=1.7888884544372559, loss=2.3691325187683105
I0207 11:16:57.334555 139946414638848 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.1424131393432617, loss=2.4229846000671387
I0207 11:17:44.563383 139946397853440 logging_writer.py:48] [143300] global_step=143300, grad_norm=1.7057849168777466, loss=2.833155870437622
I0207 11:17:55.949231 140107197974336 spec.py:321] Evaluating on the training split.
I0207 11:18:07.140181 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 11:18:46.624408 140107197974336 spec.py:349] Evaluating on the test split.
I0207 11:18:48.223816 140107197974336 submission_runner.py:408] Time since start: 74681.48s, 	Step: 143326, 	{'train/accuracy': 0.7229296565055847, 'train/loss': 1.147066593170166, 'validation/accuracy': 0.6485999822616577, 'validation/loss': 1.4839391708374023, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.157728672027588, 'test/num_examples': 10000, 'score': 66418.74589681625, 'total_duration': 74681.48336172104, 'accumulated_submission_time': 66418.74589681625, 'accumulated_eval_time': 8247.3456325531, 'accumulated_logging_time': 7.2528393268585205}
I0207 11:18:48.264548 139946414638848 logging_writer.py:48] [143326] accumulated_eval_time=8247.345633, accumulated_logging_time=7.252839, accumulated_submission_time=66418.745897, global_step=143326, preemption_count=0, score=66418.745897, test/accuracy=0.521800, test/loss=2.157729, test/num_examples=10000, total_duration=74681.483362, train/accuracy=0.722930, train/loss=1.147067, validation/accuracy=0.648600, validation/loss=1.483939, validation/num_examples=50000
I0207 11:19:18.686979 139946397853440 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.7891596555709839, loss=2.885960102081299
I0207 11:20:05.065793 139946414638848 logging_writer.py:48] [143500] global_step=143500, grad_norm=1.771393895149231, loss=2.3540852069854736
I0207 11:20:51.796368 139946397853440 logging_writer.py:48] [143600] global_step=143600, grad_norm=1.9100674390792847, loss=2.3761627674102783
I0207 11:21:38.638415 139946414638848 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.090134620666504, loss=2.3366963863372803
I0207 11:22:25.508479 139946397853440 logging_writer.py:48] [143800] global_step=143800, grad_norm=1.7114660739898682, loss=2.7502248287200928
I0207 11:23:12.735584 139946414638848 logging_writer.py:48] [143900] global_step=143900, grad_norm=1.538264513015747, loss=3.885511875152588
I0207 11:23:59.559346 139946397853440 logging_writer.py:48] [144000] global_step=144000, grad_norm=1.7330100536346436, loss=4.1893696784973145
I0207 11:24:46.191184 139946414638848 logging_writer.py:48] [144100] global_step=144100, grad_norm=1.7034363746643066, loss=3.095019578933716
I0207 11:25:33.154752 139946397853440 logging_writer.py:48] [144200] global_step=144200, grad_norm=1.7945613861083984, loss=2.36079740524292
I0207 11:25:48.680959 140107197974336 spec.py:321] Evaluating on the training split.
I0207 11:25:59.730193 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 11:26:40.705450 140107197974336 spec.py:349] Evaluating on the test split.
I0207 11:26:42.304039 140107197974336 submission_runner.py:408] Time since start: 75155.56s, 	Step: 144235, 	{'train/accuracy': 0.6969531178474426, 'train/loss': 1.259919285774231, 'validation/accuracy': 0.6465399861335754, 'validation/loss': 1.4953464269638062, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.162790298461914, 'test/num_examples': 10000, 'score': 66839.10083460808, 'total_duration': 75155.56358337402, 'accumulated_submission_time': 66839.10083460808, 'accumulated_eval_time': 8300.968694925308, 'accumulated_logging_time': 7.303514003753662}
I0207 11:26:42.344690 139946414638848 logging_writer.py:48] [144235] accumulated_eval_time=8300.968695, accumulated_logging_time=7.303514, accumulated_submission_time=66839.100835, global_step=144235, preemption_count=0, score=66839.100835, test/accuracy=0.518600, test/loss=2.162790, test/num_examples=10000, total_duration=75155.563583, train/accuracy=0.696953, train/loss=1.259919, validation/accuracy=0.646540, validation/loss=1.495346, validation/num_examples=50000
I0207 11:27:08.741285 139946397853440 logging_writer.py:48] [144300] global_step=144300, grad_norm=1.7927989959716797, loss=2.2014315128326416
I0207 11:27:54.822651 139946414638848 logging_writer.py:48] [144400] global_step=144400, grad_norm=1.6759328842163086, loss=4.209196090698242
I0207 11:28:41.696981 139946397853440 logging_writer.py:48] [144500] global_step=144500, grad_norm=1.9116735458374023, loss=2.1886470317840576
I0207 11:29:28.696928 139946414638848 logging_writer.py:48] [144600] global_step=144600, grad_norm=1.6547608375549316, loss=3.661674737930298
I0207 11:30:15.531834 139946397853440 logging_writer.py:48] [144700] global_step=144700, grad_norm=1.7218029499053955, loss=3.303920030593872
I0207 11:31:02.320527 139946414638848 logging_writer.py:48] [144800] global_step=144800, grad_norm=1.6294502019882202, loss=4.8086771965026855
I0207 11:31:49.357475 139946397853440 logging_writer.py:48] [144900] global_step=144900, grad_norm=1.886155128479004, loss=2.2453246116638184
I0207 11:32:36.052610 139946414638848 logging_writer.py:48] [145000] global_step=145000, grad_norm=1.588860034942627, loss=4.788461685180664
I0207 11:33:23.090526 139946397853440 logging_writer.py:48] [145100] global_step=145100, grad_norm=1.7826552391052246, loss=2.092376947402954
I0207 11:33:42.728985 140107197974336 spec.py:321] Evaluating on the training split.
I0207 11:33:53.651379 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 11:34:32.679463 140107197974336 spec.py:349] Evaluating on the test split.
I0207 11:34:34.287265 140107197974336 submission_runner.py:408] Time since start: 75627.55s, 	Step: 145143, 	{'train/accuracy': 0.6981835961341858, 'train/loss': 1.2710505723953247, 'validation/accuracy': 0.6437000036239624, 'validation/loss': 1.523866057395935, 'validation/num_examples': 50000, 'test/accuracy': 0.5236000418663025, 'test/loss': 2.1719541549682617, 'test/num_examples': 10000, 'score': 67259.42416167259, 'total_duration': 75627.54675364494, 'accumulated_submission_time': 67259.42416167259, 'accumulated_eval_time': 8352.526899814606, 'accumulated_logging_time': 7.353893756866455}
I0207 11:34:34.335771 139946414638848 logging_writer.py:48] [145143] accumulated_eval_time=8352.526900, accumulated_logging_time=7.353894, accumulated_submission_time=67259.424162, global_step=145143, preemption_count=0, score=67259.424162, test/accuracy=0.523600, test/loss=2.171954, test/num_examples=10000, total_duration=75627.546754, train/accuracy=0.698184, train/loss=1.271051, validation/accuracy=0.643700, validation/loss=1.523866, validation/num_examples=50000
I0207 11:34:57.380109 139946397853440 logging_writer.py:48] [145200] global_step=145200, grad_norm=1.950713038444519, loss=2.2638261318206787
I0207 11:35:43.595458 139946414638848 logging_writer.py:48] [145300] global_step=145300, grad_norm=1.8783199787139893, loss=2.5330185890197754
I0207 11:36:30.439271 139946397853440 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.119415521621704, loss=2.282541513442993
I0207 11:37:17.334157 139946414638848 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.6126112937927246, loss=3.2009880542755127
I0207 11:38:04.561881 139946397853440 logging_writer.py:48] [145600] global_step=145600, grad_norm=1.6990962028503418, loss=3.8067739009857178
I0207 11:38:51.263030 139946414638848 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.519731044769287, loss=4.974384307861328
I0207 11:39:38.409006 139946397853440 logging_writer.py:48] [145800] global_step=145800, grad_norm=1.9078277349472046, loss=2.268089771270752
I0207 11:40:25.405709 139946414638848 logging_writer.py:48] [145900] global_step=145900, grad_norm=1.923761010169983, loss=2.3587613105773926
I0207 11:41:12.258016 139946397853440 logging_writer.py:48] [146000] global_step=146000, grad_norm=1.892073154449463, loss=2.337052345275879
I0207 11:41:34.494280 140107197974336 spec.py:321] Evaluating on the training split.
I0207 11:41:45.580961 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 11:42:24.553107 140107197974336 spec.py:349] Evaluating on the test split.
I0207 11:42:26.146696 140107197974336 submission_runner.py:408] Time since start: 76099.41s, 	Step: 146049, 	{'train/accuracy': 0.7145116925239563, 'train/loss': 1.1924309730529785, 'validation/accuracy': 0.6484400033950806, 'validation/loss': 1.4934139251708984, 'validation/num_examples': 50000, 'test/accuracy': 0.523300051689148, 'test/loss': 2.157883644104004, 'test/num_examples': 10000, 'score': 67679.52076888084, 'total_duration': 76099.40624260902, 'accumulated_submission_time': 67679.52076888084, 'accumulated_eval_time': 8404.179302930832, 'accumulated_logging_time': 7.413357973098755}
I0207 11:42:26.188736 139946414638848 logging_writer.py:48] [146049] accumulated_eval_time=8404.179303, accumulated_logging_time=7.413358, accumulated_submission_time=67679.520769, global_step=146049, preemption_count=0, score=67679.520769, test/accuracy=0.523300, test/loss=2.157884, test/num_examples=10000, total_duration=76099.406243, train/accuracy=0.714512, train/loss=1.192431, validation/accuracy=0.648440, validation/loss=1.493414, validation/num_examples=50000
I0207 11:42:46.633250 139946397853440 logging_writer.py:48] [146100] global_step=146100, grad_norm=1.826238751411438, loss=2.6746435165405273
I0207 11:43:32.483291 139946414638848 logging_writer.py:48] [146200] global_step=146200, grad_norm=1.6461008787155151, loss=3.3365213871002197
I0207 11:44:19.530043 139946397853440 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.9027369022369385, loss=2.5978007316589355
I0207 11:45:06.974779 139946414638848 logging_writer.py:48] [146400] global_step=146400, grad_norm=1.76836097240448, loss=3.780744791030884
I0207 11:45:53.531705 139946397853440 logging_writer.py:48] [146500] global_step=146500, grad_norm=1.7561687231063843, loss=4.501297473907471
I0207 11:46:39.994379 139946414638848 logging_writer.py:48] [146600] global_step=146600, grad_norm=1.9118449687957764, loss=2.1065895557403564
I0207 11:47:26.772310 139946397853440 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.1700284481048584, loss=2.2803242206573486
I0207 11:48:13.587715 139946414638848 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.0308055877685547, loss=2.2370262145996094
I0207 11:49:00.121549 139946397853440 logging_writer.py:48] [146900] global_step=146900, grad_norm=1.8880513906478882, loss=2.1389379501342773
I0207 11:49:26.518416 140107197974336 spec.py:321] Evaluating on the training split.
I0207 11:49:37.530394 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 11:50:18.292523 140107197974336 spec.py:349] Evaluating on the test split.
I0207 11:50:19.893506 140107197974336 submission_runner.py:408] Time since start: 76573.15s, 	Step: 146958, 	{'train/accuracy': 0.7075585722923279, 'train/loss': 1.1999785900115967, 'validation/accuracy': 0.655239999294281, 'validation/loss': 1.4392666816711426, 'validation/num_examples': 50000, 'test/accuracy': 0.530500054359436, 'test/loss': 2.0883536338806152, 'test/num_examples': 10000, 'score': 68099.78795552254, 'total_duration': 76573.15300369263, 'accumulated_submission_time': 68099.78795552254, 'accumulated_eval_time': 8457.554328680038, 'accumulated_logging_time': 7.466028690338135}
I0207 11:50:19.939169 139946414638848 logging_writer.py:48] [146958] accumulated_eval_time=8457.554329, accumulated_logging_time=7.466029, accumulated_submission_time=68099.787956, global_step=146958, preemption_count=0, score=68099.787956, test/accuracy=0.530500, test/loss=2.088354, test/num_examples=10000, total_duration=76573.153004, train/accuracy=0.707559, train/loss=1.199979, validation/accuracy=0.655240, validation/loss=1.439267, validation/num_examples=50000
I0207 11:50:36.846612 139946397853440 logging_writer.py:48] [147000] global_step=147000, grad_norm=1.6530554294586182, loss=3.357818365097046
I0207 11:51:21.948872 139946414638848 logging_writer.py:48] [147100] global_step=147100, grad_norm=1.6843035221099854, loss=4.7209858894348145
I0207 11:52:08.664252 139946397853440 logging_writer.py:48] [147200] global_step=147200, grad_norm=1.7156745195388794, loss=3.313446283340454
I0207 11:52:55.502950 139946414638848 logging_writer.py:48] [147300] global_step=147300, grad_norm=1.9060256481170654, loss=2.321216106414795
I0207 11:53:42.347381 139946397853440 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.0578744411468506, loss=2.369175910949707
I0207 11:54:29.106116 139946414638848 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.099151372909546, loss=2.248826026916504
I0207 11:55:15.973479 139946397853440 logging_writer.py:48] [147600] global_step=147600, grad_norm=1.9159785509109497, loss=2.247502565383911
I0207 11:56:02.817796 139946414638848 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.123199224472046, loss=2.215019941329956
I0207 11:56:49.783411 139946397853440 logging_writer.py:48] [147800] global_step=147800, grad_norm=1.9791642427444458, loss=2.173316240310669
I0207 11:57:20.104963 140107197974336 spec.py:321] Evaluating on the training split.
I0207 11:57:31.123814 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 11:58:09.727387 140107197974336 spec.py:349] Evaluating on the test split.
I0207 11:58:11.316910 140107197974336 submission_runner.py:408] Time since start: 77044.58s, 	Step: 147867, 	{'train/accuracy': 0.7089257836341858, 'train/loss': 1.1918531656265259, 'validation/accuracy': 0.6549599766731262, 'validation/loss': 1.4504930973052979, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.122317314147949, 'test/num_examples': 10000, 'score': 68519.88999271393, 'total_duration': 77044.57645773888, 'accumulated_submission_time': 68519.88999271393, 'accumulated_eval_time': 8508.766267299652, 'accumulated_logging_time': 7.52376914024353}
I0207 11:58:11.357806 139946414638848 logging_writer.py:48] [147867] accumulated_eval_time=8508.766267, accumulated_logging_time=7.523769, accumulated_submission_time=68519.889993, global_step=147867, preemption_count=0, score=68519.889993, test/accuracy=0.524500, test/loss=2.122317, test/num_examples=10000, total_duration=77044.576458, train/accuracy=0.708926, train/loss=1.191853, validation/accuracy=0.654960, validation/loss=1.450493, validation/num_examples=50000
I0207 11:58:24.766669 139946397853440 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.194328784942627, loss=2.4296321868896484
I0207 11:59:09.212985 139946414638848 logging_writer.py:48] [148000] global_step=148000, grad_norm=1.6693663597106934, loss=4.627696990966797
I0207 11:59:55.697282 139946397853440 logging_writer.py:48] [148100] global_step=148100, grad_norm=1.9041790962219238, loss=2.142787456512451
I0207 12:00:42.887106 139946414638848 logging_writer.py:48] [148200] global_step=148200, grad_norm=1.9465266466140747, loss=2.115111827850342
I0207 12:01:29.670666 139946397853440 logging_writer.py:48] [148300] global_step=148300, grad_norm=1.6971491575241089, loss=3.7053651809692383
I0207 12:02:16.590695 139946414638848 logging_writer.py:48] [148400] global_step=148400, grad_norm=1.999539852142334, loss=2.3706419467926025
I0207 12:03:03.474983 139946397853440 logging_writer.py:48] [148500] global_step=148500, grad_norm=1.9302045106887817, loss=4.643096923828125
I0207 12:03:49.969607 139946414638848 logging_writer.py:48] [148600] global_step=148600, grad_norm=1.8273166418075562, loss=4.697037696838379
I0207 12:04:37.334854 139946397853440 logging_writer.py:48] [148700] global_step=148700, grad_norm=1.7327055931091309, loss=2.939199209213257
I0207 12:05:11.669930 140107197974336 spec.py:321] Evaluating on the training split.
I0207 12:05:22.541224 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 12:06:02.375912 140107197974336 spec.py:349] Evaluating on the test split.
I0207 12:06:03.980309 140107197974336 submission_runner.py:408] Time since start: 77517.24s, 	Step: 148775, 	{'train/accuracy': 0.72328120470047, 'train/loss': 1.1626847982406616, 'validation/accuracy': 0.656279981136322, 'validation/loss': 1.4608744382858276, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.1140568256378174, 'test/num_examples': 10000, 'score': 68940.13805937767, 'total_duration': 77517.2398583889, 'accumulated_submission_time': 68940.13805937767, 'accumulated_eval_time': 8561.076631069183, 'accumulated_logging_time': 7.576759338378906}
I0207 12:06:04.021429 139946414638848 logging_writer.py:48] [148775] accumulated_eval_time=8561.076631, accumulated_logging_time=7.576759, accumulated_submission_time=68940.138059, global_step=148775, preemption_count=0, score=68940.138059, test/accuracy=0.531300, test/loss=2.114057, test/num_examples=10000, total_duration=77517.239858, train/accuracy=0.723281, train/loss=1.162685, validation/accuracy=0.656280, validation/loss=1.460874, validation/num_examples=50000
I0207 12:06:14.246621 139946397853440 logging_writer.py:48] [148800] global_step=148800, grad_norm=1.920993685722351, loss=2.253514528274536
I0207 12:06:58.348479 139946414638848 logging_writer.py:48] [148900] global_step=148900, grad_norm=1.8933706283569336, loss=2.6863889694213867
I0207 12:07:44.961084 139946397853440 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.2740931510925293, loss=2.300272226333618
I0207 12:08:31.778728 139946414638848 logging_writer.py:48] [149100] global_step=149100, grad_norm=1.786656141281128, loss=3.435248613357544
I0207 12:09:18.481221 139946397853440 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.042819023132324, loss=2.2238950729370117
I0207 12:10:05.502459 139946414638848 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.051380157470703, loss=2.1230087280273438
I0207 12:10:52.553728 139946397853440 logging_writer.py:48] [149400] global_step=149400, grad_norm=1.80325448513031, loss=3.8160321712493896
I0207 12:11:39.280086 139946414638848 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.1526038646698, loss=2.0866923332214355
I0207 12:12:26.256937 139946397853440 logging_writer.py:48] [149600] global_step=149600, grad_norm=1.7800780534744263, loss=3.2954580783843994
I0207 12:13:04.098012 140107197974336 spec.py:321] Evaluating on the training split.
I0207 12:13:14.937693 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 12:13:54.018292 140107197974336 spec.py:349] Evaluating on the test split.
I0207 12:13:55.611938 140107197974336 submission_runner.py:408] Time since start: 77988.87s, 	Step: 149682, 	{'train/accuracy': 0.71742182970047, 'train/loss': 1.1706210374832153, 'validation/accuracy': 0.6571800112724304, 'validation/loss': 1.4194693565368652, 'validation/num_examples': 50000, 'test/accuracy': 0.5441000461578369, 'test/loss': 2.061110496520996, 'test/num_examples': 10000, 'score': 69360.1499080658, 'total_duration': 77988.87147164345, 'accumulated_submission_time': 69360.1499080658, 'accumulated_eval_time': 8612.59052824974, 'accumulated_logging_time': 7.63094162940979}
I0207 12:13:55.652774 139946414638848 logging_writer.py:48] [149682] accumulated_eval_time=8612.590528, accumulated_logging_time=7.630942, accumulated_submission_time=69360.149908, global_step=149682, preemption_count=0, score=69360.149908, test/accuracy=0.544100, test/loss=2.061110, test/num_examples=10000, total_duration=77988.871472, train/accuracy=0.717422, train/loss=1.170621, validation/accuracy=0.657180, validation/loss=1.419469, validation/num_examples=50000
I0207 12:14:03.137810 139946397853440 logging_writer.py:48] [149700] global_step=149700, grad_norm=1.7827486991882324, loss=3.580517053604126
I0207 12:14:46.717495 139946414638848 logging_writer.py:48] [149800] global_step=149800, grad_norm=1.7198095321655273, loss=3.87445330619812
I0207 12:15:33.251898 139946397853440 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.222421169281006, loss=4.495020389556885
I0207 12:16:20.240688 139946414638848 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.0949578285217285, loss=2.8228068351745605
I0207 12:17:06.869862 139946397853440 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.2009809017181396, loss=2.1372432708740234
I0207 12:17:53.813621 139946414638848 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.0754196643829346, loss=2.2629010677337646
I0207 12:18:40.703991 139946397853440 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.0907955169677734, loss=4.405740737915039
I0207 12:19:27.986428 139946414638848 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.1583805084228516, loss=2.162642478942871
I0207 12:20:14.613884 139946397853440 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.1210110187530518, loss=2.446720600128174
I0207 12:20:55.613176 140107197974336 spec.py:321] Evaluating on the training split.
I0207 12:21:06.649183 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 12:21:46.014463 140107197974336 spec.py:349] Evaluating on the test split.
I0207 12:21:47.614575 140107197974336 submission_runner.py:408] Time since start: 78460.87s, 	Step: 150589, 	{'train/accuracy': 0.7193945050239563, 'train/loss': 1.145675778388977, 'validation/accuracy': 0.6657599806785583, 'validation/loss': 1.3892215490341187, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.0501999855041504, 'test/num_examples': 10000, 'score': 69780.04849529266, 'total_duration': 78460.87410640717, 'accumulated_submission_time': 69780.04849529266, 'accumulated_eval_time': 8664.59189748764, 'accumulated_logging_time': 7.682143688201904}
I0207 12:21:47.659926 139946414638848 logging_writer.py:48] [150589] accumulated_eval_time=8664.591897, accumulated_logging_time=7.682144, accumulated_submission_time=69780.048495, global_step=150589, preemption_count=0, score=69780.048495, test/accuracy=0.540600, test/loss=2.050200, test/num_examples=10000, total_duration=78460.874106, train/accuracy=0.719395, train/loss=1.145676, validation/accuracy=0.665760, validation/loss=1.389222, validation/num_examples=50000
I0207 12:21:52.386700 139946397853440 logging_writer.py:48] [150600] global_step=150600, grad_norm=1.9836440086364746, loss=2.1145591735839844
I0207 12:22:35.285658 139946414638848 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.3469643592834473, loss=2.276319980621338
I0207 12:23:21.928979 139946397853440 logging_writer.py:48] [150800] global_step=150800, grad_norm=1.9460424184799194, loss=4.0147705078125
I0207 12:24:08.938325 139946414638848 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.0667850971221924, loss=2.2023754119873047
I0207 12:24:55.972044 139946397853440 logging_writer.py:48] [151000] global_step=151000, grad_norm=1.8381307125091553, loss=3.547001361846924
I0207 12:25:42.805444 139946414638848 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.1567044258117676, loss=2.174689769744873
I0207 12:26:29.894526 139946397853440 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.1123130321502686, loss=2.1998977661132812
I0207 12:27:16.819384 139946414638848 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.3220810890197754, loss=4.713413238525391
I0207 12:28:04.079659 139946397853440 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.0364391803741455, loss=2.050577163696289
I0207 12:28:48.005423 140107197974336 spec.py:321] Evaluating on the training split.
I0207 12:28:59.085935 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 12:29:39.108227 140107197974336 spec.py:349] Evaluating on the test split.
I0207 12:29:40.703959 140107197974336 submission_runner.py:408] Time since start: 78933.96s, 	Step: 151496, 	{'train/accuracy': 0.7369726300239563, 'train/loss': 1.0826455354690552, 'validation/accuracy': 0.6676999926567078, 'validation/loss': 1.3798246383666992, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.038510799407959, 'test/num_examples': 10000, 'score': 70200.33002853394, 'total_duration': 78933.96350884438, 'accumulated_submission_time': 70200.33002853394, 'accumulated_eval_time': 8717.290427207947, 'accumulated_logging_time': 7.740187168121338}
I0207 12:29:40.744624 139946414638848 logging_writer.py:48] [151496] accumulated_eval_time=8717.290427, accumulated_logging_time=7.740187, accumulated_submission_time=70200.330029, global_step=151496, preemption_count=0, score=70200.330029, test/accuracy=0.542400, test/loss=2.038511, test/num_examples=10000, total_duration=78933.963509, train/accuracy=0.736973, train/loss=1.082646, validation/accuracy=0.667700, validation/loss=1.379825, validation/num_examples=50000
I0207 12:29:42.712380 139946397853440 logging_writer.py:48] [151500] global_step=151500, grad_norm=1.9394736289978027, loss=2.2615604400634766
I0207 12:30:25.119984 139946414638848 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.0974042415618896, loss=2.0080628395080566
I0207 12:31:11.689161 139946397853440 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.0431034564971924, loss=4.628847599029541
I0207 12:31:58.588918 139946414638848 logging_writer.py:48] [151800] global_step=151800, grad_norm=1.8074873685836792, loss=3.5415823459625244
I0207 12:32:45.560678 139946397853440 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.06968092918396, loss=4.469765663146973
I0207 12:33:32.408615 139946414638848 logging_writer.py:48] [152000] global_step=152000, grad_norm=1.851944088935852, loss=3.687203884124756
I0207 12:34:19.114084 139946397853440 logging_writer.py:48] [152100] global_step=152100, grad_norm=1.8344175815582275, loss=3.1112775802612305
I0207 12:35:06.128134 139946414638848 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.423692226409912, loss=2.086118698120117
I0207 12:35:52.900442 139946397853440 logging_writer.py:48] [152300] global_step=152300, grad_norm=1.8215422630310059, loss=3.294724225997925
I0207 12:36:40.043231 139946414638848 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.173535108566284, loss=2.0998761653900146
I0207 12:36:41.136811 140107197974336 spec.py:321] Evaluating on the training split.
I0207 12:36:52.098354 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 12:37:32.294101 140107197974336 spec.py:349] Evaluating on the test split.
I0207 12:37:33.896478 140107197974336 submission_runner.py:408] Time since start: 79407.16s, 	Step: 152404, 	{'train/accuracy': 0.7234960794448853, 'train/loss': 1.141112208366394, 'validation/accuracy': 0.6663999557495117, 'validation/loss': 1.381853461265564, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.045349597930908, 'test/num_examples': 10000, 'score': 70620.6611096859, 'total_duration': 79407.156021595, 'accumulated_submission_time': 70620.6611096859, 'accumulated_eval_time': 8770.050068855286, 'accumulated_logging_time': 7.791531801223755}
I0207 12:37:33.937797 139946397853440 logging_writer.py:48] [152404] accumulated_eval_time=8770.050069, accumulated_logging_time=7.791532, accumulated_submission_time=70620.661110, global_step=152404, preemption_count=0, score=70620.661110, test/accuracy=0.542300, test/loss=2.045350, test/num_examples=10000, total_duration=79407.156022, train/accuracy=0.723496, train/loss=1.141112, validation/accuracy=0.666400, validation/loss=1.381853, validation/num_examples=50000
I0207 12:38:14.829937 139946414638848 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.152064085006714, loss=2.231494903564453
I0207 12:39:01.007512 139946397853440 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.129814386367798, loss=2.163743019104004
I0207 12:39:48.044347 139946414638848 logging_writer.py:48] [152700] global_step=152700, grad_norm=1.96896493434906, loss=4.016791820526123
I0207 12:40:34.707597 139946397853440 logging_writer.py:48] [152800] global_step=152800, grad_norm=1.8714914321899414, loss=2.9231936931610107
I0207 12:41:21.613323 139946414638848 logging_writer.py:48] [152900] global_step=152900, grad_norm=1.907444953918457, loss=2.809878349304199
I0207 12:42:08.256896 139946397853440 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.1024203300476074, loss=2.035817861557007
I0207 12:42:55.181493 139946414638848 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.1878275871276855, loss=2.1587986946105957
I0207 12:43:41.902841 139946397853440 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.109781503677368, loss=2.2379066944122314
I0207 12:44:29.088029 139946414638848 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.096473455429077, loss=4.200762748718262
I0207 12:44:34.017426 140107197974336 spec.py:321] Evaluating on the training split.
I0207 12:44:46.038821 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 12:45:25.613453 140107197974336 spec.py:349] Evaluating on the test split.
I0207 12:45:27.231070 140107197974336 submission_runner.py:408] Time since start: 79880.49s, 	Step: 153312, 	{'train/accuracy': 0.7329882383346558, 'train/loss': 1.0937864780426025, 'validation/accuracy': 0.6733799576759338, 'validation/loss': 1.3574568033218384, 'validation/num_examples': 50000, 'test/accuracy': 0.5484000444412231, 'test/loss': 2.018724203109741, 'test/num_examples': 10000, 'score': 71040.6793308258, 'total_duration': 79880.49059653282, 'accumulated_submission_time': 71040.6793308258, 'accumulated_eval_time': 8823.263661623001, 'accumulated_logging_time': 7.842597007751465}
I0207 12:45:27.278103 139946397853440 logging_writer.py:48] [153312] accumulated_eval_time=8823.263662, accumulated_logging_time=7.842597, accumulated_submission_time=71040.679331, global_step=153312, preemption_count=0, score=71040.679331, test/accuracy=0.548400, test/loss=2.018724, test/num_examples=10000, total_duration=79880.490597, train/accuracy=0.732988, train/loss=1.093786, validation/accuracy=0.673380, validation/loss=1.357457, validation/num_examples=50000
I0207 12:46:04.322356 139946414638848 logging_writer.py:48] [153400] global_step=153400, grad_norm=1.925706386566162, loss=3.0023908615112305
I0207 12:46:50.675793 139946397853440 logging_writer.py:48] [153500] global_step=153500, grad_norm=1.9803929328918457, loss=3.0667853355407715
I0207 12:47:37.432152 139946414638848 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.017529010772705, loss=2.495131731033325
I0207 12:48:24.496757 139946397853440 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.1179215908050537, loss=2.194384813308716
I0207 12:49:11.532063 139946414638848 logging_writer.py:48] [153800] global_step=153800, grad_norm=1.9241267442703247, loss=3.8999621868133545
I0207 12:49:58.333173 139946397853440 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.0789012908935547, loss=3.0560519695281982
I0207 12:50:45.301009 139946414638848 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.0857629776000977, loss=2.7144179344177246
I0207 12:51:31.769092 139946397853440 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.2524030208587646, loss=2.7153749465942383
I0207 12:52:18.818104 139946414638848 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.3673439025878906, loss=2.0739083290100098
I0207 12:52:27.347801 140107197974336 spec.py:321] Evaluating on the training split.
I0207 12:52:38.607419 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 12:53:16.433723 140107197974336 spec.py:349] Evaluating on the test split.
I0207 12:53:18.035953 140107197974336 submission_runner.py:408] Time since start: 80351.30s, 	Step: 154220, 	{'train/accuracy': 0.7409570217132568, 'train/loss': 1.0447590351104736, 'validation/accuracy': 0.6767799854278564, 'validation/loss': 1.337766408920288, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 1.9991964101791382, 'test/num_examples': 10000, 'score': 71460.68648219109, 'total_duration': 80351.2955019474, 'accumulated_submission_time': 71460.68648219109, 'accumulated_eval_time': 8873.951783180237, 'accumulated_logging_time': 7.901000738143921}
I0207 12:53:18.078795 139946397853440 logging_writer.py:48] [154220] accumulated_eval_time=8873.951783, accumulated_logging_time=7.901001, accumulated_submission_time=71460.686482, global_step=154220, preemption_count=0, score=71460.686482, test/accuracy=0.550900, test/loss=1.999196, test/num_examples=10000, total_duration=80351.295502, train/accuracy=0.740957, train/loss=1.044759, validation/accuracy=0.676780, validation/loss=1.337766, validation/num_examples=50000
I0207 12:53:51.566427 139946414638848 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.15083909034729, loss=4.614452838897705
I0207 12:54:38.129949 139946397853440 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.306703805923462, loss=2.0641117095947266
I0207 12:55:25.100998 139946414638848 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.278844118118286, loss=1.9721758365631104
I0207 12:56:12.072267 139946397853440 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.4733023643493652, loss=2.208000898361206
I0207 12:56:58.705827 139946414638848 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.6916749477386475, loss=1.9838327169418335
I0207 12:57:45.665696 139946397853440 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.300028085708618, loss=2.0676767826080322
I0207 12:58:32.382906 139946414638848 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.466768741607666, loss=4.606698036193848
I0207 12:59:19.275118 139946397853440 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.126910924911499, loss=2.2267727851867676
I0207 13:00:06.028091 139946414638848 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.371943712234497, loss=1.9234129190444946
I0207 13:00:18.079032 140107197974336 spec.py:321] Evaluating on the training split.
I0207 13:00:28.970355 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 13:01:10.705019 140107197974336 spec.py:349] Evaluating on the test split.
I0207 13:01:12.312229 140107197974336 submission_runner.py:408] Time since start: 80825.57s, 	Step: 155127, 	{'train/accuracy': 0.7374804615974426, 'train/loss': 1.0925852060317993, 'validation/accuracy': 0.6778199672698975, 'validation/loss': 1.3446279764175415, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.018172025680542, 'test/num_examples': 10000, 'score': 71880.62465643883, 'total_duration': 80825.57174110413, 'accumulated_submission_time': 71880.62465643883, 'accumulated_eval_time': 8928.184937000275, 'accumulated_logging_time': 7.954378366470337}
I0207 13:01:12.359454 139946397853440 logging_writer.py:48] [155127] accumulated_eval_time=8928.184937, accumulated_logging_time=7.954378, accumulated_submission_time=71880.624656, global_step=155127, preemption_count=0, score=71880.624656, test/accuracy=0.548900, test/loss=2.018172, test/num_examples=10000, total_duration=80825.571741, train/accuracy=0.737480, train/loss=1.092585, validation/accuracy=0.677820, validation/loss=1.344628, validation/num_examples=50000
I0207 13:01:42.473332 139946414638848 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.202077627182007, loss=3.5910425186157227
I0207 13:02:29.197660 139946397853440 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.201000213623047, loss=2.0886731147766113
I0207 13:03:16.108783 139946414638848 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.260486602783203, loss=2.0275464057922363
I0207 13:04:03.399688 139946397853440 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.433448076248169, loss=1.9910094738006592
I0207 13:04:50.452327 139946414638848 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.2594454288482666, loss=2.0349435806274414
I0207 13:05:37.196551 139946397853440 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.226276159286499, loss=2.0712826251983643
I0207 13:06:24.115151 139946414638848 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.402844190597534, loss=1.9233096837997437
I0207 13:07:10.922149 139946397853440 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.064666271209717, loss=2.9028706550598145
I0207 13:07:57.655606 139946414638848 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.1088902950286865, loss=3.809903144836426
I0207 13:08:12.511059 140107197974336 spec.py:321] Evaluating on the training split.
I0207 13:08:23.771111 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 13:09:01.937333 140107197974336 spec.py:349] Evaluating on the test split.
I0207 13:09:03.541185 140107197974336 submission_runner.py:408] Time since start: 81296.80s, 	Step: 156033, 	{'train/accuracy': 0.7378515601158142, 'train/loss': 1.0726988315582275, 'validation/accuracy': 0.6809799671173096, 'validation/loss': 1.3264795541763306, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.985244870185852, 'test/num_examples': 10000, 'score': 72300.71397519112, 'total_duration': 81296.80072975159, 'accumulated_submission_time': 72300.71397519112, 'accumulated_eval_time': 8979.215026378632, 'accumulated_logging_time': 8.012673616409302}
I0207 13:09:03.581065 139946397853440 logging_writer.py:48] [156033] accumulated_eval_time=8979.215026, accumulated_logging_time=8.012674, accumulated_submission_time=72300.713975, global_step=156033, preemption_count=0, score=72300.713975, test/accuracy=0.555000, test/loss=1.985245, test/num_examples=10000, total_duration=81296.800730, train/accuracy=0.737852, train/loss=1.072699, validation/accuracy=0.680980, validation/loss=1.326480, validation/num_examples=50000
I0207 13:09:30.884867 139946414638848 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.0787012577056885, loss=2.3243517875671387
I0207 13:10:17.169210 139946397853440 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.2871057987213135, loss=2.7206344604492188
I0207 13:11:04.119565 139946414638848 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.1931064128875732, loss=1.9354465007781982
I0207 13:11:51.087585 139946397853440 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.320115804672241, loss=2.0360898971557617
I0207 13:12:37.849138 139946414638848 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.295253276824951, loss=2.1090307235717773
I0207 13:13:24.614850 139946397853440 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.331787586212158, loss=1.9296691417694092
I0207 13:14:11.484802 139946414638848 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.390956401824951, loss=4.53237771987915
I0207 13:14:58.063370 139946397853440 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.148515462875366, loss=3.4074788093566895
I0207 13:15:44.916821 139946414638848 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.3046364784240723, loss=1.8111624717712402
I0207 13:16:03.871065 140107197974336 spec.py:321] Evaluating on the training split.
I0207 13:16:14.950387 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 13:16:53.022957 140107197974336 spec.py:349] Evaluating on the test split.
I0207 13:16:54.625570 140107197974336 submission_runner.py:408] Time since start: 81767.89s, 	Step: 156942, 	{'train/accuracy': 0.7446679472923279, 'train/loss': 1.0540313720703125, 'validation/accuracy': 0.6827600002288818, 'validation/loss': 1.3250590562820435, 'validation/num_examples': 50000, 'test/accuracy': 0.5562000274658203, 'test/loss': 1.9805034399032593, 'test/num_examples': 10000, 'score': 72720.9410059452, 'total_duration': 81767.88509273529, 'accumulated_submission_time': 72720.9410059452, 'accumulated_eval_time': 9029.969490528107, 'accumulated_logging_time': 8.063661575317383}
I0207 13:16:54.668169 139946397853440 logging_writer.py:48] [156942] accumulated_eval_time=9029.969491, accumulated_logging_time=8.063662, accumulated_submission_time=72720.941006, global_step=156942, preemption_count=0, score=72720.941006, test/accuracy=0.556200, test/loss=1.980503, test/num_examples=10000, total_duration=81767.885093, train/accuracy=0.744668, train/loss=1.054031, validation/accuracy=0.682760, validation/loss=1.325059, validation/num_examples=50000
I0207 13:17:17.840200 139946414638848 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.4423484802246094, loss=1.9745687246322632
I0207 13:18:04.123320 139946397853440 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.1160852909088135, loss=2.7979612350463867
I0207 13:18:50.793927 139946414638848 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.152704954147339, loss=3.1855435371398926
I0207 13:19:37.535567 139946397853440 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.538050889968872, loss=1.9946852922439575
I0207 13:20:24.166031 139946414638848 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.3587777614593506, loss=1.8876276016235352
I0207 13:21:10.999600 139946397853440 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.5098812580108643, loss=1.8843873739242554
I0207 13:21:57.732792 139946414638848 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.0153229236602783, loss=2.4211597442626953
I0207 13:22:44.974333 139946397853440 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.3247077465057373, loss=1.9003568887710571
I0207 13:23:31.753271 139946414638848 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.5584659576416016, loss=1.9890546798706055
I0207 13:23:54.972278 140107197974336 spec.py:321] Evaluating on the training split.
I0207 13:24:06.100854 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 13:24:47.379007 140107197974336 spec.py:349] Evaluating on the test split.
I0207 13:24:48.982204 140107197974336 submission_runner.py:408] Time since start: 82242.24s, 	Step: 157851, 	{'train/accuracy': 0.7596679329872131, 'train/loss': 0.9815232753753662, 'validation/accuracy': 0.6845600008964539, 'validation/loss': 1.304532527923584, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9559437036514282, 'test/num_examples': 10000, 'score': 73141.18209695816, 'total_duration': 82242.24174976349, 'accumulated_submission_time': 73141.18209695816, 'accumulated_eval_time': 9083.979422330856, 'accumulated_logging_time': 8.117793560028076}
I0207 13:24:49.028048 139946397853440 logging_writer.py:48] [157851] accumulated_eval_time=9083.979422, accumulated_logging_time=8.117794, accumulated_submission_time=73141.182097, global_step=157851, preemption_count=0, score=73141.182097, test/accuracy=0.562100, test/loss=1.955944, test/num_examples=10000, total_duration=82242.241750, train/accuracy=0.759668, train/loss=0.981523, validation/accuracy=0.684560, validation/loss=1.304533, validation/num_examples=50000
I0207 13:25:08.683635 139946414638848 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.547356367111206, loss=4.544624328613281
I0207 13:25:54.564228 139946397853440 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.2248525619506836, loss=2.8227834701538086
I0207 13:26:41.586732 139946414638848 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.2166271209716797, loss=2.136512041091919
I0207 13:27:28.403223 139946397853440 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.1528408527374268, loss=3.2782726287841797
I0207 13:28:15.515569 139946414638848 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.3083395957946777, loss=3.2419700622558594
I0207 13:29:02.451196 139946397853440 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.2164716720581055, loss=1.7766292095184326
I0207 13:29:49.376900 139946414638848 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.4616501331329346, loss=4.488070487976074
I0207 13:30:36.163242 139946397853440 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.4114441871643066, loss=1.999884843826294
I0207 13:31:23.298405 139946414638848 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.407402515411377, loss=1.9069502353668213
I0207 13:31:49.392782 140107197974336 spec.py:321] Evaluating on the training split.
I0207 13:32:00.512956 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 13:32:38.731631 140107197974336 spec.py:349] Evaluating on the test split.
I0207 13:32:40.331533 140107197974336 submission_runner.py:408] Time since start: 82713.59s, 	Step: 158758, 	{'train/accuracy': 0.7476562261581421, 'train/loss': 1.031006932258606, 'validation/accuracy': 0.6883599758148193, 'validation/loss': 1.2879359722137451, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 1.9300532341003418, 'test/num_examples': 10000, 'score': 73561.48490953445, 'total_duration': 82713.59108018875, 'accumulated_submission_time': 73561.48490953445, 'accumulated_eval_time': 9134.91816353798, 'accumulated_logging_time': 8.174100875854492}
I0207 13:32:40.377827 139946397853440 logging_writer.py:48] [158758] accumulated_eval_time=9134.918164, accumulated_logging_time=8.174101, accumulated_submission_time=73561.484910, global_step=158758, preemption_count=0, score=73561.484910, test/accuracy=0.563700, test/loss=1.930053, test/num_examples=10000, total_duration=82713.591080, train/accuracy=0.747656, train/loss=1.031007, validation/accuracy=0.688360, validation/loss=1.287936, validation/num_examples=50000
I0207 13:32:57.271994 139946414638848 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.2419004440307617, loss=1.916795253753662
I0207 13:33:42.431524 139946397853440 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.2719807624816895, loss=2.8993868827819824
I0207 13:34:29.262004 139946414638848 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.3434643745422363, loss=1.980783462524414
I0207 13:35:16.195337 139946397853440 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.438551664352417, loss=1.8622725009918213
I0207 13:36:02.809415 139946414638848 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.2708160877227783, loss=2.605018377304077
I0207 13:36:49.257723 139946397853440 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.2957370281219482, loss=2.546684741973877
I0207 13:37:35.842754 139946414638848 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.3608644008636475, loss=1.971437931060791
I0207 13:38:22.707587 139946397853440 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.4537081718444824, loss=1.8408828973770142
I0207 13:39:09.319280 139946414638848 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.2275240421295166, loss=1.865382194519043
I0207 13:39:40.637475 140107197974336 spec.py:321] Evaluating on the training split.
I0207 13:39:51.340968 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 13:40:29.457009 140107197974336 spec.py:349] Evaluating on the test split.
I0207 13:40:31.052013 140107197974336 submission_runner.py:408] Time since start: 83184.31s, 	Step: 159669, 	{'train/accuracy': 0.7578319907188416, 'train/loss': 0.9961916208267212, 'validation/accuracy': 0.69159996509552, 'validation/loss': 1.2772376537322998, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9215903282165527, 'test/num_examples': 10000, 'score': 73981.68310594559, 'total_duration': 83184.31155920029, 'accumulated_submission_time': 73981.68310594559, 'accumulated_eval_time': 9185.332689285278, 'accumulated_logging_time': 8.230481386184692}
I0207 13:40:31.094774 139946397853440 logging_writer.py:48] [159669] accumulated_eval_time=9185.332689, accumulated_logging_time=8.230481, accumulated_submission_time=73981.683106, global_step=159669, preemption_count=0, score=73981.683106, test/accuracy=0.571100, test/loss=1.921590, test/num_examples=10000, total_duration=83184.311559, train/accuracy=0.757832, train/loss=0.996192, validation/accuracy=0.691600, validation/loss=1.277238, validation/num_examples=50000
I0207 13:40:43.704358 139946414638848 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.2896692752838135, loss=2.153451681137085
I0207 13:41:28.177823 139946397853440 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.5616116523742676, loss=4.453960418701172
I0207 13:42:14.927422 139946414638848 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.551168441772461, loss=1.859727382659912
I0207 13:43:01.972096 139946397853440 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.256242036819458, loss=3.557147979736328
I0207 13:43:48.738022 139946414638848 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.4971022605895996, loss=4.255697727203369
I0207 13:44:35.856623 139946397853440 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.543088436126709, loss=2.0051891803741455
I0207 13:45:22.734698 139946414638848 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.4932570457458496, loss=1.9624426364898682
I0207 13:46:09.713992 139946397853440 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.561492919921875, loss=2.432955503463745
I0207 13:46:56.411415 139946414638848 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.50899338722229, loss=1.7180631160736084
I0207 13:47:31.381896 140107197974336 spec.py:321] Evaluating on the training split.
I0207 13:47:42.462993 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 13:48:24.659884 140107197974336 spec.py:349] Evaluating on the test split.
I0207 13:48:26.253476 140107197974336 submission_runner.py:408] Time since start: 83659.51s, 	Step: 160576, 	{'train/accuracy': 0.7697070240974426, 'train/loss': 0.9384439587593079, 'validation/accuracy': 0.6931599974632263, 'validation/loss': 1.2678637504577637, 'validation/num_examples': 50000, 'test/accuracy': 0.5687000155448914, 'test/loss': 1.9138787984848022, 'test/num_examples': 10000, 'score': 74401.90697979927, 'total_duration': 83659.51302075386, 'accumulated_submission_time': 74401.90697979927, 'accumulated_eval_time': 9240.204246282578, 'accumulated_logging_time': 8.28437876701355}
I0207 13:48:26.298511 139946397853440 logging_writer.py:48] [160576] accumulated_eval_time=9240.204246, accumulated_logging_time=8.284379, accumulated_submission_time=74401.906980, global_step=160576, preemption_count=0, score=74401.906980, test/accuracy=0.568700, test/loss=1.913879, test/num_examples=10000, total_duration=83659.513021, train/accuracy=0.769707, train/loss=0.938444, validation/accuracy=0.693160, validation/loss=1.267864, validation/num_examples=50000
I0207 13:48:36.127294 139946414638848 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.3816256523132324, loss=1.8925936222076416
I0207 13:49:20.073420 139946397853440 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.4937098026275635, loss=1.8741461038589478
I0207 13:50:06.664442 139946414638848 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.577970504760742, loss=2.492941379547119
I0207 13:50:53.206415 139946397853440 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.5244905948638916, loss=2.512126922607422
I0207 13:51:39.888706 139946414638848 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.3637683391571045, loss=2.1841907501220703
I0207 13:52:26.595461 139946397853440 logging_writer.py:48] [161100] global_step=161100, grad_norm=3.0188815593719482, loss=2.014143943786621
I0207 13:53:13.194872 139946414638848 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.460193395614624, loss=1.9446462392807007
I0207 13:54:00.033164 139946397853440 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.4370529651641846, loss=1.9069358110427856
I0207 13:54:47.389764 139946414638848 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.7345170974731445, loss=4.263455390930176
I0207 13:55:26.563046 140107197974336 spec.py:321] Evaluating on the training split.
I0207 13:55:37.662983 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 13:56:16.046376 140107197974336 spec.py:349] Evaluating on the test split.
I0207 13:56:17.652803 140107197974336 submission_runner.py:408] Time since start: 84130.91s, 	Step: 161485, 	{'train/accuracy': 0.7563671469688416, 'train/loss': 0.976298451423645, 'validation/accuracy': 0.6957199573516846, 'validation/loss': 1.2451553344726562, 'validation/num_examples': 50000, 'test/accuracy': 0.5685999989509583, 'test/loss': 1.8960278034210205, 'test/num_examples': 10000, 'score': 74822.10788369179, 'total_duration': 84130.91235041618, 'accumulated_submission_time': 74822.10788369179, 'accumulated_eval_time': 9291.293983697891, 'accumulated_logging_time': 8.341437816619873}
I0207 13:56:17.695341 139946397853440 logging_writer.py:48] [161485] accumulated_eval_time=9291.293984, accumulated_logging_time=8.341438, accumulated_submission_time=74822.107884, global_step=161485, preemption_count=0, score=74822.107884, test/accuracy=0.568600, test/loss=1.896028, test/num_examples=10000, total_duration=84130.912350, train/accuracy=0.756367, train/loss=0.976298, validation/accuracy=0.695720, validation/loss=1.245155, validation/num_examples=50000
I0207 13:56:23.989077 139946414638848 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.263728618621826, loss=2.731311559677124
I0207 13:57:07.101017 139946397853440 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.4443323612213135, loss=2.6130576133728027
I0207 13:57:53.610702 139946414638848 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.5248312950134277, loss=1.806836724281311
I0207 13:58:40.674601 139946397853440 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.54781436920166, loss=4.3571672439575195
I0207 13:59:27.591534 139946414638848 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.4990830421447754, loss=2.9323630332946777
I0207 14:00:14.377593 139946397853440 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.4111013412475586, loss=2.237435817718506
I0207 14:01:00.889447 139946414638848 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.66874098777771, loss=1.8452894687652588
I0207 14:01:47.557170 139946397853440 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.444938898086548, loss=2.818399667739868
I0207 14:02:34.410299 139946414638848 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.415616750717163, loss=1.9081509113311768
I0207 14:03:17.709973 140107197974336 spec.py:321] Evaluating on the training split.
I0207 14:03:28.492676 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 14:04:07.921140 140107197974336 spec.py:349] Evaluating on the test split.
I0207 14:04:09.531024 140107197974336 submission_runner.py:408] Time since start: 84602.79s, 	Step: 162394, 	{'train/accuracy': 0.76527339220047, 'train/loss': 0.9461551308631897, 'validation/accuracy': 0.7001199722290039, 'validation/loss': 1.229008674621582, 'validation/num_examples': 50000, 'test/accuracy': 0.5740000009536743, 'test/loss': 1.8750234842300415, 'test/num_examples': 10000, 'score': 75242.05846524239, 'total_duration': 84602.79054522514, 'accumulated_submission_time': 75242.05846524239, 'accumulated_eval_time': 9343.114990472794, 'accumulated_logging_time': 8.395569086074829}
I0207 14:04:09.579834 139946397853440 logging_writer.py:48] [162394] accumulated_eval_time=9343.114990, accumulated_logging_time=8.395569, accumulated_submission_time=75242.058465, global_step=162394, preemption_count=0, score=75242.058465, test/accuracy=0.574000, test/loss=1.875023, test/num_examples=10000, total_duration=84602.790545, train/accuracy=0.765273, train/loss=0.946155, validation/accuracy=0.700120, validation/loss=1.229009, validation/num_examples=50000
I0207 14:04:12.339419 139946414638848 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.3685855865478516, loss=2.991896867752075
I0207 14:04:55.045449 139946397853440 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.830232858657837, loss=1.9171864986419678
I0207 14:05:41.653450 139946414638848 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.5497398376464844, loss=1.6129283905029297
I0207 14:06:28.636686 139946397853440 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.516990900039673, loss=3.637303590774536
I0207 14:07:15.498698 139946414638848 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.6993765830993652, loss=2.3523635864257812
I0207 14:08:02.298836 139946397853440 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.541450023651123, loss=1.8167390823364258
I0207 14:08:49.397042 139946414638848 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.400468587875366, loss=2.144524574279785
I0207 14:09:36.581978 139946397853440 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.7150826454162598, loss=1.7975211143493652
I0207 14:10:23.293288 139946414638848 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.654682159423828, loss=1.7900582551956177
I0207 14:11:09.538808 140107197974336 spec.py:321] Evaluating on the training split.
I0207 14:11:20.581773 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 14:11:59.638045 140107197974336 spec.py:349] Evaluating on the test split.
I0207 14:12:01.229135 140107197974336 submission_runner.py:408] Time since start: 85074.49s, 	Step: 163300, 	{'train/accuracy': 0.7712695002555847, 'train/loss': 0.9198420643806458, 'validation/accuracy': 0.701259970664978, 'validation/loss': 1.2253583669662476, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 1.875489592552185, 'test/num_examples': 10000, 'score': 75661.95459794998, 'total_duration': 85074.48866915703, 'accumulated_submission_time': 75661.95459794998, 'accumulated_eval_time': 9394.80528140068, 'accumulated_logging_time': 8.456037521362305}
I0207 14:12:01.273145 139946397853440 logging_writer.py:48] [163300] accumulated_eval_time=9394.805281, accumulated_logging_time=8.456038, accumulated_submission_time=75661.954598, global_step=163300, preemption_count=0, score=75661.954598, test/accuracy=0.577100, test/loss=1.875490, test/num_examples=10000, total_duration=85074.488669, train/accuracy=0.771270, train/loss=0.919842, validation/accuracy=0.701260, validation/loss=1.225358, validation/num_examples=50000
I0207 14:12:01.675553 139946414638848 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.79097056388855, loss=1.8260295391082764
I0207 14:12:43.701781 139946397853440 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.6616177558898926, loss=1.8341684341430664
I0207 14:13:30.160780 139946414638848 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.6430647373199463, loss=1.9364582300186157
I0207 14:14:17.078777 139946397853440 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.6972169876098633, loss=2.156479835510254
I0207 14:15:04.068700 139946414638848 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.6426353454589844, loss=1.9271680116653442
I0207 14:15:50.855602 139946397853440 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.4947383403778076, loss=3.506953477859497
I0207 14:16:38.009498 139946414638848 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.8662338256835938, loss=1.950485348701477
I0207 14:17:24.960815 139946397853440 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.612473964691162, loss=1.8718255758285522
I0207 14:18:11.729230 139946414638848 logging_writer.py:48] [164100] global_step=164100, grad_norm=3.1753320693969727, loss=1.8664647340774536
I0207 14:18:58.572770 139946397853440 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.7010338306427, loss=1.8151662349700928
I0207 14:19:01.450322 140107197974336 spec.py:321] Evaluating on the training split.
I0207 14:19:12.800404 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 14:19:52.190124 140107197974336 spec.py:349] Evaluating on the test split.
I0207 14:19:53.777877 140107197974336 submission_runner.py:408] Time since start: 85547.04s, 	Step: 164208, 	{'train/accuracy': 0.7705273032188416, 'train/loss': 0.9230221509933472, 'validation/accuracy': 0.7032399773597717, 'validation/loss': 1.2151079177856445, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.8578152656555176, 'test/num_examples': 10000, 'score': 76082.06962704659, 'total_duration': 85547.03738641739, 'accumulated_submission_time': 76082.06962704659, 'accumulated_eval_time': 9447.132781267166, 'accumulated_logging_time': 8.510085105895996}
I0207 14:19:53.820531 139946414638848 logging_writer.py:48] [164208] accumulated_eval_time=9447.132781, accumulated_logging_time=8.510085, accumulated_submission_time=76082.069627, global_step=164208, preemption_count=0, score=76082.069627, test/accuracy=0.577000, test/loss=1.857815, test/num_examples=10000, total_duration=85547.037386, train/accuracy=0.770527, train/loss=0.923022, validation/accuracy=0.703240, validation/loss=1.215108, validation/num_examples=50000
I0207 14:20:32.631361 139946397853440 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.8579671382904053, loss=1.8139209747314453
I0207 14:21:19.184636 139946414638848 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.803602695465088, loss=1.8379700183868408
I0207 14:22:06.145016 139946397853440 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.900414228439331, loss=1.8730032444000244
I0207 14:22:52.706381 139946414638848 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.806753396987915, loss=1.7760502099990845
I0207 14:23:39.453309 139946397853440 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.551603317260742, loss=2.002943515777588
I0207 14:24:26.261412 139946414638848 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.7185659408569336, loss=1.8436462879180908
I0207 14:25:13.217963 139946397853440 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.521737575531006, loss=3.096017837524414
I0207 14:25:59.965696 139946414638848 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.735703706741333, loss=1.835439682006836
I0207 14:26:46.794153 139946397853440 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.737277030944824, loss=1.8511414527893066
I0207 14:26:53.938208 140107197974336 spec.py:321] Evaluating on the training split.
I0207 14:27:05.113990 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 14:27:45.999746 140107197974336 spec.py:349] Evaluating on the test split.
I0207 14:27:47.604028 140107197974336 submission_runner.py:408] Time since start: 86020.86s, 	Step: 165117, 	{'train/accuracy': 0.7754296660423279, 'train/loss': 0.913085401058197, 'validation/accuracy': 0.7068799734115601, 'validation/loss': 1.2091929912567139, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 1.8517848253250122, 'test/num_examples': 10000, 'score': 76502.12402510643, 'total_duration': 86020.86357402802, 'accumulated_submission_time': 76502.12402510643, 'accumulated_eval_time': 9500.79858636856, 'accumulated_logging_time': 8.563549280166626}
I0207 14:27:47.647548 139946414638848 logging_writer.py:48] [165117] accumulated_eval_time=9500.798586, accumulated_logging_time=8.563549, accumulated_submission_time=76502.124025, global_step=165117, preemption_count=0, score=76502.124025, test/accuracy=0.583400, test/loss=1.851785, test/num_examples=10000, total_duration=86020.863574, train/accuracy=0.775430, train/loss=0.913085, validation/accuracy=0.706880, validation/loss=1.209193, validation/num_examples=50000
I0207 14:28:22.511044 139946397853440 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.735926628112793, loss=2.9084889888763428
I0207 14:29:09.030358 139946414638848 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.808964967727661, loss=4.0597920417785645
I0207 14:29:56.007311 139946397853440 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.705469846725464, loss=1.7771060466766357
I0207 14:30:42.836902 139946414638848 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.8122024536132812, loss=1.7907726764678955
I0207 14:31:29.915996 139946397853440 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.8492231369018555, loss=1.8555079698562622
I0207 14:32:16.830989 139946414638848 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.7697763442993164, loss=1.879370093345642
I0207 14:33:04.238421 139946397853440 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.8571817874908447, loss=1.705812931060791
I0207 14:33:51.102363 139946414638848 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.122035026550293, loss=1.868453025817871
I0207 14:34:38.473665 139946397853440 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.672109603881836, loss=2.123861312866211
I0207 14:34:47.821015 140107197974336 spec.py:321] Evaluating on the training split.
I0207 14:34:59.000013 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 14:35:36.691302 140107197974336 spec.py:349] Evaluating on the test split.
I0207 14:35:38.302088 140107197974336 submission_runner.py:408] Time since start: 86491.56s, 	Step: 166022, 	{'train/accuracy': 0.7830663919448853, 'train/loss': 0.8687943816184998, 'validation/accuracy': 0.7093600034713745, 'validation/loss': 1.190772533416748, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.840895652770996, 'test/num_examples': 10000, 'score': 76922.23614120483, 'total_duration': 86491.56161642075, 'accumulated_submission_time': 76922.23614120483, 'accumulated_eval_time': 9551.27962064743, 'accumulated_logging_time': 8.616734027862549}
I0207 14:35:38.353317 139946414638848 logging_writer.py:48] [166022] accumulated_eval_time=9551.279621, accumulated_logging_time=8.616734, accumulated_submission_time=76922.236141, global_step=166022, preemption_count=0, score=76922.236141, test/accuracy=0.583700, test/loss=1.840896, test/num_examples=10000, total_duration=86491.561616, train/accuracy=0.783066, train/loss=0.868794, validation/accuracy=0.709360, validation/loss=1.190773, validation/num_examples=50000
I0207 14:36:11.029108 139946397853440 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.6617555618286133, loss=2.2985310554504395
I0207 14:36:57.464003 139946414638848 logging_writer.py:48] [166200] global_step=166200, grad_norm=3.0224530696868896, loss=1.6680271625518799
I0207 14:37:44.524114 139946397853440 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.8034298419952393, loss=1.7764908075332642
I0207 14:38:31.645920 139946414638848 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.6622493267059326, loss=1.7306475639343262
I0207 14:39:18.625043 139946397853440 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.6860225200653076, loss=3.689187526702881
I0207 14:40:05.405187 139946414638848 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.8795716762542725, loss=1.717181921005249
I0207 14:40:52.421161 139946397853440 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.5011277198791504, loss=2.468461036682129
I0207 14:41:39.116872 139946414638848 logging_writer.py:48] [166800] global_step=166800, grad_norm=3.1282968521118164, loss=1.7449239492416382
I0207 14:42:26.143551 139946397853440 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.5685245990753174, loss=1.7129263877868652
I0207 14:42:38.501771 140107197974336 spec.py:321] Evaluating on the training split.
I0207 14:42:49.620996 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 14:43:29.095396 140107197974336 spec.py:349] Evaluating on the test split.
I0207 14:43:30.695931 140107197974336 submission_runner.py:408] Time since start: 86963.96s, 	Step: 166928, 	{'train/accuracy': 0.7832421660423279, 'train/loss': 0.871487557888031, 'validation/accuracy': 0.7151399850845337, 'validation/loss': 1.162901520729065, 'validation/num_examples': 50000, 'test/accuracy': 0.5940000414848328, 'test/loss': 1.7914539575576782, 'test/num_examples': 10000, 'score': 77342.32014989853, 'total_duration': 86963.95547676086, 'accumulated_submission_time': 77342.32014989853, 'accumulated_eval_time': 9603.473745822906, 'accumulated_logging_time': 8.681183338165283}
I0207 14:43:30.739879 139946414638848 logging_writer.py:48] [166928] accumulated_eval_time=9603.473746, accumulated_logging_time=8.681183, accumulated_submission_time=77342.320150, global_step=166928, preemption_count=0, score=77342.320150, test/accuracy=0.594000, test/loss=1.791454, test/num_examples=10000, total_duration=86963.955477, train/accuracy=0.783242, train/loss=0.871488, validation/accuracy=0.715140, validation/loss=1.162902, validation/num_examples=50000
I0207 14:44:00.451416 139946397853440 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.6177427768707275, loss=2.9736928939819336
I0207 14:44:47.272249 139946414638848 logging_writer.py:48] [167100] global_step=167100, grad_norm=3.3569672107696533, loss=3.583733320236206
I0207 14:45:34.477422 139946397853440 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.779822826385498, loss=2.204936981201172
I0207 14:46:21.510430 139946414638848 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.6408753395080566, loss=3.658957004547119
I0207 14:46:28.702041 139946397853440 logging_writer.py:48] [167317] global_step=167317, preemption_count=0, score=77520.201707
I0207 14:46:29.270545 140107197974336 checkpoints.py:490] Saving checkpoint at step: 167317
I0207 14:46:30.486837 140107197974336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_4/checkpoint_167317
I0207 14:46:30.510637 140107197974336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_4/checkpoint_167317.
I0207 14:46:31.443562 140107197974336 submission_runner.py:583] Tuning trial 4/5
I0207 14:46:31.443808 140107197974336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0207 14:46:31.457448 140107197974336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009765625, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 35.27015542984009, 'total_duration': 64.52321434020996, 'accumulated_submission_time': 35.27015542984009, 'accumulated_eval_time': 29.252927780151367, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (850, {'train/accuracy': 0.03515625, 'train/loss': 5.885966777801514, 'validation/accuracy': 0.03391999751329422, 'validation/loss': 5.9119391441345215, 'validation/num_examples': 50000, 'test/accuracy': 0.02680000104010105, 'test/loss': 6.039024353027344, 'test/num_examples': 10000, 'score': 455.6410744190216, 'total_duration': 532.7995707988739, 'accumulated_submission_time': 455.6410744190216, 'accumulated_eval_time': 77.09238171577454, 'accumulated_logging_time': 0.017679691314697266, 'global_step': 850, 'preemption_count': 0}), (1756, {'train/accuracy': 0.07289062440395355, 'train/loss': 5.357734680175781, 'validation/accuracy': 0.0655599981546402, 'validation/loss': 5.43528413772583, 'validation/num_examples': 50000, 'test/accuracy': 0.053300000727176666, 'test/loss': 5.654690742492676, 'test/num_examples': 10000, 'score': 875.7305772304535, 'total_duration': 1003.2701570987701, 'accumulated_submission_time': 875.7305772304535, 'accumulated_eval_time': 127.39190244674683, 'accumulated_logging_time': 0.04587912559509277, 'global_step': 1756, 'preemption_count': 0}), (2665, {'train/accuracy': 0.10384765267372131, 'train/loss': 4.981348991394043, 'validation/accuracy': 0.09711999446153641, 'validation/loss': 5.043646812438965, 'validation/num_examples': 50000, 'test/accuracy': 0.07349999994039536, 'test/loss': 5.327393054962158, 'test/num_examples': 10000, 'score': 1295.8631527423859, 'total_duration': 1474.4437997341156, 'accumulated_submission_time': 1295.8631527423859, 'accumulated_eval_time': 178.35270309448242, 'accumulated_logging_time': 0.07439279556274414, 'global_step': 2665, 'preemption_count': 0}), (3575, {'train/accuracy': 0.1374218761920929, 'train/loss': 4.640011787414551, 'validation/accuracy': 0.12824000418186188, 'validation/loss': 4.715636253356934, 'validation/num_examples': 50000, 'test/accuracy': 0.09840000420808792, 'test/loss': 5.043509006500244, 'test/num_examples': 10000, 'score': 1715.986170053482, 'total_duration': 1944.7330491542816, 'accumulated_submission_time': 1715.986170053482, 'accumulated_eval_time': 228.43773818016052, 'accumulated_logging_time': 0.10317254066467285, 'global_step': 3575, 'preemption_count': 0}), (4482, {'train/accuracy': 0.16847655177116394, 'train/loss': 4.419726848602295, 'validation/accuracy': 0.1536400020122528, 'validation/loss': 4.517857551574707, 'validation/num_examples': 50000, 'test/accuracy': 0.11940000206232071, 'test/loss': 4.872969627380371, 'test/num_examples': 10000, 'score': 2136.13942360878, 'total_duration': 2414.920921087265, 'accumulated_submission_time': 2136.13942360878, 'accumulated_eval_time': 278.38977098464966, 'accumulated_logging_time': 0.13427948951721191, 'global_step': 4482, 'preemption_count': 0}), (5394, {'train/accuracy': 0.18613280355930328, 'train/loss': 4.220674514770508, 'validation/accuracy': 0.17083999514579773, 'validation/loss': 4.3279571533203125, 'validation/num_examples': 50000, 'test/accuracy': 0.1340000033378601, 'test/loss': 4.738316059112549, 'test/num_examples': 10000, 'score': 2556.4348685741425, 'total_duration': 2887.3158445358276, 'accumulated_submission_time': 2556.4348685741425, 'accumulated_eval_time': 330.4113621711731, 'accumulated_logging_time': 0.16042447090148926, 'global_step': 5394, 'preemption_count': 0}), (6306, {'train/accuracy': 0.22626952826976776, 'train/loss': 3.890352487564087, 'validation/accuracy': 0.20678000152111053, 'validation/loss': 3.9983174800872803, 'validation/num_examples': 50000, 'test/accuracy': 0.1624000072479248, 'test/loss': 4.428020477294922, 'test/num_examples': 10000, 'score': 2976.544565677643, 'total_duration': 3358.3559985160828, 'accumulated_submission_time': 2976.544565677643, 'accumulated_eval_time': 381.2634997367859, 'accumulated_logging_time': 0.18742036819458008, 'global_step': 6306, 'preemption_count': 0}), (7218, {'train/accuracy': 0.232421875, 'train/loss': 3.8077361583709717, 'validation/accuracy': 0.21663999557495117, 'validation/loss': 3.93320369720459, 'validation/num_examples': 50000, 'test/accuracy': 0.16580000519752502, 'test/loss': 4.394979000091553, 'test/num_examples': 10000, 'score': 3396.534752845764, 'total_duration': 3828.797343492508, 'accumulated_submission_time': 3396.534752845764, 'accumulated_eval_time': 431.63345980644226, 'accumulated_logging_time': 0.21734189987182617, 'global_step': 7218, 'preemption_count': 0}), (8131, {'train/accuracy': 0.24320311844348907, 'train/loss': 3.820199728012085, 'validation/accuracy': 0.22071999311447144, 'validation/loss': 3.940143585205078, 'validation/num_examples': 50000, 'test/accuracy': 0.17240001261234283, 'test/loss': 4.411588668823242, 'test/num_examples': 10000, 'score': 3816.9069616794586, 'total_duration': 4301.491326808929, 'accumulated_submission_time': 3816.9069616794586, 'accumulated_eval_time': 483.87773609161377, 'accumulated_logging_time': 0.24266529083251953, 'global_step': 8131, 'preemption_count': 0}), (9041, {'train/accuracy': 0.27070310711860657, 'train/loss': 3.5831527709960938, 'validation/accuracy': 0.2477799952030182, 'validation/loss': 3.710452079772949, 'validation/num_examples': 50000, 'test/accuracy': 0.1932000070810318, 'test/loss': 4.174107074737549, 'test/num_examples': 10000, 'score': 4236.489646196365, 'total_duration': 4773.503950834274, 'accumulated_submission_time': 4236.489646196365, 'accumulated_eval_time': 535.8006870746613, 'accumulated_logging_time': 0.69814133644104, 'global_step': 9041, 'preemption_count': 0}), (9953, {'train/accuracy': 0.2805859446525574, 'train/loss': 3.5102574825286865, 'validation/accuracy': 0.25845998525619507, 'validation/loss': 3.65179705619812, 'validation/num_examples': 50000, 'test/accuracy': 0.1997000128030777, 'test/loss': 4.167118549346924, 'test/num_examples': 10000, 'score': 4656.436341047287, 'total_duration': 5244.670069217682, 'accumulated_submission_time': 4656.436341047287, 'accumulated_eval_time': 586.9349710941315, 'accumulated_logging_time': 0.7309134006500244, 'global_step': 9953, 'preemption_count': 0}), (10864, {'train/accuracy': 0.2991601526737213, 'train/loss': 3.379409074783325, 'validation/accuracy': 0.2633799910545349, 'validation/loss': 3.607219934463501, 'validation/num_examples': 50000, 'test/accuracy': 0.20430001616477966, 'test/loss': 4.103435516357422, 'test/num_examples': 10000, 'score': 5076.775891304016, 'total_duration': 5716.142570257187, 'accumulated_submission_time': 5076.775891304016, 'accumulated_eval_time': 637.9864263534546, 'accumulated_logging_time': 0.7608444690704346, 'global_step': 10864, 'preemption_count': 0}), (11773, {'train/accuracy': 0.2922460734844208, 'train/loss': 3.4271626472473145, 'validation/accuracy': 0.2735399901866913, 'validation/loss': 3.5501279830932617, 'validation/num_examples': 50000, 'test/accuracy': 0.20720000565052032, 'test/loss': 4.052846908569336, 'test/num_examples': 10000, 'score': 5497.108896970749, 'total_duration': 6188.652234077454, 'accumulated_submission_time': 5497.108896970749, 'accumulated_eval_time': 690.076201915741, 'accumulated_logging_time': 0.7959158420562744, 'global_step': 11773, 'preemption_count': 0}), (12683, {'train/accuracy': 0.2971484363079071, 'train/loss': 3.4095776081085205, 'validation/accuracy': 0.27709999680519104, 'validation/loss': 3.530275821685791, 'validation/num_examples': 50000, 'test/accuracy': 0.21390001475811005, 'test/loss': 4.058302879333496, 'test/num_examples': 10000, 'score': 5917.175496578217, 'total_duration': 6661.750261306763, 'accumulated_submission_time': 5917.175496578217, 'accumulated_eval_time': 743.02845287323, 'accumulated_logging_time': 0.8236494064331055, 'global_step': 12683, 'preemption_count': 0}), (13592, {'train/accuracy': 0.32298827171325684, 'train/loss': 3.2224056720733643, 'validation/accuracy': 0.28015998005867004, 'validation/loss': 3.4948790073394775, 'validation/num_examples': 50000, 'test/accuracy': 0.21240000426769257, 'test/loss': 4.011412143707275, 'test/num_examples': 10000, 'score': 6337.318806171417, 'total_duration': 7132.289152383804, 'accumulated_submission_time': 6337.318806171417, 'accumulated_eval_time': 793.3429248332977, 'accumulated_logging_time': 0.8524086475372314, 'global_step': 13592, 'preemption_count': 0}), (14498, {'train/accuracy': 0.31587889790534973, 'train/loss': 3.273736000061035, 'validation/accuracy': 0.2939800024032593, 'validation/loss': 3.3951432704925537, 'validation/num_examples': 50000, 'test/accuracy': 0.22270001471042633, 'test/loss': 3.9273860454559326, 'test/num_examples': 10000, 'score': 6757.358831167221, 'total_duration': 7603.913234949112, 'accumulated_submission_time': 6757.358831167221, 'accumulated_eval_time': 844.848123550415, 'accumulated_logging_time': 0.8794562816619873, 'global_step': 14498, 'preemption_count': 0}), (15405, {'train/accuracy': 0.3174218535423279, 'train/loss': 3.3037431240081787, 'validation/accuracy': 0.29363998770713806, 'validation/loss': 3.4267396926879883, 'validation/num_examples': 50000, 'test/accuracy': 0.2321000099182129, 'test/loss': 3.9265310764312744, 'test/num_examples': 10000, 'score': 7177.752962350845, 'total_duration': 8076.399621963501, 'accumulated_submission_time': 7177.752962350845, 'accumulated_eval_time': 896.8590533733368, 'accumulated_logging_time': 0.9088699817657471, 'global_step': 15405, 'preemption_count': 0}), (16315, {'train/accuracy': 0.3279101550579071, 'train/loss': 3.2197184562683105, 'validation/accuracy': 0.2950599789619446, 'validation/loss': 3.416271448135376, 'validation/num_examples': 50000, 'test/accuracy': 0.22640001773834229, 'test/loss': 3.9574127197265625, 'test/num_examples': 10000, 'score': 7597.680613279343, 'total_duration': 8547.108525753021, 'accumulated_submission_time': 7597.680613279343, 'accumulated_eval_time': 947.5592894554138, 'accumulated_logging_time': 0.9369139671325684, 'global_step': 16315, 'preemption_count': 0}), (17227, {'train/accuracy': 0.30931639671325684, 'train/loss': 3.3662712574005127, 'validation/accuracy': 0.28696000576019287, 'validation/loss': 3.501178741455078, 'validation/num_examples': 50000, 'test/accuracy': 0.21810001134872437, 'test/loss': 4.019433975219727, 'test/num_examples': 10000, 'score': 8017.9552347660065, 'total_duration': 9020.436147928238, 'accumulated_submission_time': 8017.9552347660065, 'accumulated_eval_time': 1000.5289397239685, 'accumulated_logging_time': 0.9675893783569336, 'global_step': 17227, 'preemption_count': 0}), (18135, {'train/accuracy': 0.3327734172344208, 'train/loss': 3.162445545196533, 'validation/accuracy': 0.3094799816608429, 'validation/loss': 3.3020880222320557, 'validation/num_examples': 50000, 'test/accuracy': 0.23330001533031464, 'test/loss': 3.8808505535125732, 'test/num_examples': 10000, 'score': 8437.884447813034, 'total_duration': 9491.875847578049, 'accumulated_submission_time': 8437.884447813034, 'accumulated_eval_time': 1051.960121870041, 'accumulated_logging_time': 0.9948971271514893, 'global_step': 18135, 'preemption_count': 0}), (19043, {'train/accuracy': 0.3293164074420929, 'train/loss': 3.2388904094696045, 'validation/accuracy': 0.30535998940467834, 'validation/loss': 3.392557382583618, 'validation/num_examples': 50000, 'test/accuracy': 0.22960001230239868, 'test/loss': 3.9306869506835938, 'test/num_examples': 10000, 'score': 8858.166803836823, 'total_duration': 9965.598746538162, 'accumulated_submission_time': 8858.166803836823, 'accumulated_eval_time': 1105.318029642105, 'accumulated_logging_time': 1.0265140533447266, 'global_step': 19043, 'preemption_count': 0}), (19952, {'train/accuracy': 0.33759763836860657, 'train/loss': 3.134190082550049, 'validation/accuracy': 0.31709998846054077, 'validation/loss': 3.262382984161377, 'validation/num_examples': 50000, 'test/accuracy': 0.24770000576972961, 'test/loss': 3.8268377780914307, 'test/num_examples': 10000, 'score': 9278.551815271378, 'total_duration': 10440.915224790573, 'accumulated_submission_time': 9278.551815271378, 'accumulated_eval_time': 1160.165560245514, 'accumulated_logging_time': 1.059199333190918, 'global_step': 19952, 'preemption_count': 0}), (20863, {'train/accuracy': 0.3479296863079071, 'train/loss': 3.0743730068206787, 'validation/accuracy': 0.32259997725486755, 'validation/loss': 3.2060763835906982, 'validation/num_examples': 50000, 'test/accuracy': 0.24640001356601715, 'test/loss': 3.7816295623779297, 'test/num_examples': 10000, 'score': 9698.902920722961, 'total_duration': 10913.613707065582, 'accumulated_submission_time': 9698.902920722961, 'accumulated_eval_time': 1212.4267747402191, 'accumulated_logging_time': 1.0930509567260742, 'global_step': 20863, 'preemption_count': 0}), (21769, {'train/accuracy': 0.3544921875, 'train/loss': 3.012260913848877, 'validation/accuracy': 0.3244200050830841, 'validation/loss': 3.1736767292022705, 'validation/num_examples': 50000, 'test/accuracy': 0.24970000982284546, 'test/loss': 3.733030319213867, 'test/num_examples': 10000, 'score': 10119.1109790802, 'total_duration': 11384.816810846329, 'accumulated_submission_time': 10119.1109790802, 'accumulated_eval_time': 1263.3394284248352, 'accumulated_logging_time': 1.1243548393249512, 'global_step': 21769, 'preemption_count': 0}), (22677, {'train/accuracy': 0.33763670921325684, 'train/loss': 3.103149652481079, 'validation/accuracy': 0.3175399899482727, 'validation/loss': 3.2267537117004395, 'validation/num_examples': 50000, 'test/accuracy': 0.24650001525878906, 'test/loss': 3.79990553855896, 'test/num_examples': 10000, 'score': 10539.403557777405, 'total_duration': 11856.480294466019, 'accumulated_submission_time': 10539.403557777405, 'accumulated_eval_time': 1314.6286573410034, 'accumulated_logging_time': 1.1545770168304443, 'global_step': 22677, 'preemption_count': 0}), (23585, {'train/accuracy': 0.35798826813697815, 'train/loss': 3.0236752033233643, 'validation/accuracy': 0.3356199860572815, 'validation/loss': 3.154872179031372, 'validation/num_examples': 50000, 'test/accuracy': 0.2532000243663788, 'test/loss': 3.731189250946045, 'test/num_examples': 10000, 'score': 10959.684936523438, 'total_duration': 12328.844103336334, 'accumulated_submission_time': 10959.684936523438, 'accumulated_eval_time': 1366.626068353653, 'accumulated_logging_time': 1.188666820526123, 'global_step': 23585, 'preemption_count': 0}), (24492, {'train/accuracy': 0.3534179627895355, 'train/loss': 2.9814937114715576, 'validation/accuracy': 0.3287400007247925, 'validation/loss': 3.1595587730407715, 'validation/num_examples': 50000, 'test/accuracy': 0.25710001587867737, 'test/loss': 3.7189767360687256, 'test/num_examples': 10000, 'score': 11380.087949991226, 'total_duration': 12801.768762588501, 'accumulated_submission_time': 11380.087949991226, 'accumulated_eval_time': 1419.0612137317657, 'accumulated_logging_time': 1.2243428230285645, 'global_step': 24492, 'preemption_count': 0}), (25398, {'train/accuracy': 0.3600195348262787, 'train/loss': 3.08575177192688, 'validation/accuracy': 0.3273800015449524, 'validation/loss': 3.256126880645752, 'validation/num_examples': 50000, 'test/accuracy': 0.24420000612735748, 'test/loss': 3.829700231552124, 'test/num_examples': 10000, 'score': 11800.085270881653, 'total_duration': 13272.997171640396, 'accumulated_submission_time': 11800.085270881653, 'accumulated_eval_time': 1470.207144498825, 'accumulated_logging_time': 1.2579419612884521, 'global_step': 25398, 'preemption_count': 0}), (26305, {'train/accuracy': 0.3636718690395355, 'train/loss': 2.97861909866333, 'validation/accuracy': 0.33987998962402344, 'validation/loss': 3.1036124229431152, 'validation/num_examples': 50000, 'test/accuracy': 0.2581000030040741, 'test/loss': 3.705676317214966, 'test/num_examples': 10000, 'score': 12220.36465883255, 'total_duration': 13745.666129112244, 'accumulated_submission_time': 12220.36465883255, 'accumulated_eval_time': 1522.5056171417236, 'accumulated_logging_time': 1.2966363430023193, 'global_step': 26305, 'preemption_count': 0}), (27209, {'train/accuracy': 0.37458983063697815, 'train/loss': 2.8777029514312744, 'validation/accuracy': 0.3472599983215332, 'validation/loss': 3.050992727279663, 'validation/num_examples': 50000, 'test/accuracy': 0.2712000012397766, 'test/loss': 3.6168735027313232, 'test/num_examples': 10000, 'score': 12640.606446743011, 'total_duration': 14217.07286643982, 'accumulated_submission_time': 12640.606446743011, 'accumulated_eval_time': 1573.5893032550812, 'accumulated_logging_time': 1.327235221862793, 'global_step': 27209, 'preemption_count': 0}), (28110, {'train/accuracy': 0.4131249785423279, 'train/loss': 2.7041707038879395, 'validation/accuracy': 0.35009998083114624, 'validation/loss': 3.0397725105285645, 'validation/num_examples': 50000, 'test/accuracy': 0.27410000562667847, 'test/loss': 3.6207833290100098, 'test/num_examples': 10000, 'score': 13060.701996088028, 'total_duration': 14688.747186660767, 'accumulated_submission_time': 13060.701996088028, 'accumulated_eval_time': 1625.0876586437225, 'accumulated_logging_time': 1.3568122386932373, 'global_step': 28110, 'preemption_count': 0}), (29014, {'train/accuracy': 0.35374999046325684, 'train/loss': 3.0552871227264404, 'validation/accuracy': 0.3319399952888489, 'validation/loss': 3.1854774951934814, 'validation/num_examples': 50000, 'test/accuracy': 0.24890001118183136, 'test/loss': 3.7286272048950195, 'test/num_examples': 10000, 'score': 13480.76611328125, 'total_duration': 15161.299011945724, 'accumulated_submission_time': 13480.76611328125, 'accumulated_eval_time': 1677.4866523742676, 'accumulated_logging_time': 1.3948171138763428, 'global_step': 29014, 'preemption_count': 0}), (29917, {'train/accuracy': 0.3595312535762787, 'train/loss': 3.01259183883667, 'validation/accuracy': 0.3307799994945526, 'validation/loss': 3.17191219329834, 'validation/num_examples': 50000, 'test/accuracy': 0.256600022315979, 'test/loss': 3.744589328765869, 'test/num_examples': 10000, 'score': 13901.006760120392, 'total_duration': 15635.10154223442, 'accumulated_submission_time': 13901.006760120392, 'accumulated_eval_time': 1730.9614639282227, 'accumulated_logging_time': 1.4306964874267578, 'global_step': 29917, 'preemption_count': 0}), (30820, {'train/accuracy': 0.38343748450279236, 'train/loss': 2.864189624786377, 'validation/accuracy': 0.33743998408317566, 'validation/loss': 3.1222946643829346, 'validation/num_examples': 50000, 'test/accuracy': 0.25920000672340393, 'test/loss': 3.72937273979187, 'test/num_examples': 10000, 'score': 14321.184031248093, 'total_duration': 16109.316176652908, 'accumulated_submission_time': 14321.184031248093, 'accumulated_eval_time': 1784.9154148101807, 'accumulated_logging_time': 1.4623537063598633, 'global_step': 30820, 'preemption_count': 0}), (31728, {'train/accuracy': 0.3595312535762787, 'train/loss': 3.016347885131836, 'validation/accuracy': 0.3363399803638458, 'validation/loss': 3.149092197418213, 'validation/num_examples': 50000, 'test/accuracy': 0.25920000672340393, 'test/loss': 3.729475259780884, 'test/num_examples': 10000, 'score': 14741.241425275803, 'total_duration': 16584.70828318596, 'accumulated_submission_time': 14741.241425275803, 'accumulated_eval_time': 1840.1619803905487, 'accumulated_logging_time': 1.498626470565796, 'global_step': 31728, 'preemption_count': 0}), (32632, {'train/accuracy': 0.3854101598262787, 'train/loss': 2.822803497314453, 'validation/accuracy': 0.35711997747421265, 'validation/loss': 2.989459991455078, 'validation/num_examples': 50000, 'test/accuracy': 0.2743000090122223, 'test/loss': 3.608689308166504, 'test/num_examples': 10000, 'score': 15161.485805511475, 'total_duration': 17056.36390018463, 'accumulated_submission_time': 15161.485805511475, 'accumulated_eval_time': 1891.4928452968597, 'accumulated_logging_time': 1.528019905090332, 'global_step': 32632, 'preemption_count': 0}), (33538, {'train/accuracy': 0.3950781226158142, 'train/loss': 2.7515311241149902, 'validation/accuracy': 0.36017999053001404, 'validation/loss': 2.9707324504852295, 'validation/num_examples': 50000, 'test/accuracy': 0.27970001101493835, 'test/loss': 3.5605785846710205, 'test/num_examples': 10000, 'score': 15581.891350269318, 'total_duration': 17528.8449883461, 'accumulated_submission_time': 15581.891350269318, 'accumulated_eval_time': 1943.4836995601654, 'accumulated_logging_time': 1.5617244243621826, 'global_step': 33538, 'preemption_count': 0}), (34448, {'train/accuracy': 0.3739648461341858, 'train/loss': 2.8991634845733643, 'validation/accuracy': 0.3464599847793579, 'validation/loss': 3.0483076572418213, 'validation/num_examples': 50000, 'test/accuracy': 0.26820001006126404, 'test/loss': 3.622101306915283, 'test/num_examples': 10000, 'score': 16001.988507032394, 'total_duration': 18000.516926765442, 'accumulated_submission_time': 16001.988507032394, 'accumulated_eval_time': 1994.9713623523712, 'accumulated_logging_time': 1.596980094909668, 'global_step': 34448, 'preemption_count': 0}), (35352, {'train/accuracy': 0.3836914002895355, 'train/loss': 2.843531370162964, 'validation/accuracy': 0.3626999855041504, 'validation/loss': 2.977720260620117, 'validation/num_examples': 50000, 'test/accuracy': 0.28210002183914185, 'test/loss': 3.5719974040985107, 'test/num_examples': 10000, 'score': 16422.013520240784, 'total_duration': 18471.41180539131, 'accumulated_submission_time': 16422.013520240784, 'accumulated_eval_time': 2045.7526659965515, 'accumulated_logging_time': 1.6344339847564697, 'global_step': 35352, 'preemption_count': 0}), (36258, {'train/accuracy': 0.38597655296325684, 'train/loss': 2.8243813514709473, 'validation/accuracy': 0.35651999711990356, 'validation/loss': 3.010390520095825, 'validation/num_examples': 50000, 'test/accuracy': 0.2752000093460083, 'test/loss': 3.615450620651245, 'test/num_examples': 10000, 'score': 16842.142607688904, 'total_duration': 18943.784227132797, 'accumulated_submission_time': 16842.142607688904, 'accumulated_eval_time': 2097.9133784770966, 'accumulated_logging_time': 1.6660008430480957, 'global_step': 36258, 'preemption_count': 0}), (37164, {'train/accuracy': 0.3831445276737213, 'train/loss': 2.815798282623291, 'validation/accuracy': 0.35979998111724854, 'validation/loss': 2.9579362869262695, 'validation/num_examples': 50000, 'test/accuracy': 0.2762000262737274, 'test/loss': 3.5880584716796875, 'test/num_examples': 10000, 'score': 17262.063611745834, 'total_duration': 19414.525707244873, 'accumulated_submission_time': 17262.063611745834, 'accumulated_eval_time': 2148.6515715122223, 'accumulated_logging_time': 1.697392463684082, 'global_step': 37164, 'preemption_count': 0}), (38070, {'train/accuracy': 0.37132811546325684, 'train/loss': 2.9289300441741943, 'validation/accuracy': 0.3464599847793579, 'validation/loss': 3.0839309692382812, 'validation/num_examples': 50000, 'test/accuracy': 0.26510000228881836, 'test/loss': 3.700192451477051, 'test/num_examples': 10000, 'score': 17682.347589969635, 'total_duration': 19888.36406493187, 'accumulated_submission_time': 17682.347589969635, 'accumulated_eval_time': 2202.1193537712097, 'accumulated_logging_time': 1.732421636581421, 'global_step': 38070, 'preemption_count': 0}), (38976, {'train/accuracy': 0.3922656178474426, 'train/loss': 2.8164315223693848, 'validation/accuracy': 0.35637998580932617, 'validation/loss': 3.0107686519622803, 'validation/num_examples': 50000, 'test/accuracy': 0.2685000002384186, 'test/loss': 3.6256322860717773, 'test/num_examples': 10000, 'score': 18102.575630664825, 'total_duration': 20361.533406972885, 'accumulated_submission_time': 18102.575630664825, 'accumulated_eval_time': 2254.9687502384186, 'accumulated_logging_time': 1.7733440399169922, 'global_step': 38976, 'preemption_count': 0}), (39884, {'train/accuracy': 0.3822265565395355, 'train/loss': 2.8847715854644775, 'validation/accuracy': 0.3570599853992462, 'validation/loss': 3.043698787689209, 'validation/num_examples': 50000, 'test/accuracy': 0.2786000072956085, 'test/loss': 3.6124985218048096, 'test/num_examples': 10000, 'score': 18522.856332540512, 'total_duration': 20832.76473426819, 'accumulated_submission_time': 18522.856332540512, 'accumulated_eval_time': 2305.820141553879, 'accumulated_logging_time': 1.8208367824554443, 'global_step': 39884, 'preemption_count': 0}), (40792, {'train/accuracy': 0.3929687440395355, 'train/loss': 2.7785167694091797, 'validation/accuracy': 0.36907997727394104, 'validation/loss': 2.914356231689453, 'validation/num_examples': 50000, 'test/accuracy': 0.2778000235557556, 'test/loss': 3.5277481079101562, 'test/num_examples': 10000, 'score': 18942.836881637573, 'total_duration': 21302.938645601273, 'accumulated_submission_time': 18942.836881637573, 'accumulated_eval_time': 2355.927830219269, 'accumulated_logging_time': 1.8549113273620605, 'global_step': 40792, 'preemption_count': 0}), (41694, {'train/accuracy': 0.3770507872104645, 'train/loss': 2.974250078201294, 'validation/accuracy': 0.3516799807548523, 'validation/loss': 3.125171184539795, 'validation/num_examples': 50000, 'test/accuracy': 0.26820001006126404, 'test/loss': 3.675233840942383, 'test/num_examples': 10000, 'score': 19362.784957170486, 'total_duration': 21773.569100618362, 'accumulated_submission_time': 19362.784957170486, 'accumulated_eval_time': 2406.526533842087, 'accumulated_logging_time': 1.8879799842834473, 'global_step': 41694, 'preemption_count': 0}), (42601, {'train/accuracy': 0.3780468702316284, 'train/loss': 2.9276158809661865, 'validation/accuracy': 0.34797999262809753, 'validation/loss': 3.1044631004333496, 'validation/num_examples': 50000, 'test/accuracy': 0.26570001244544983, 'test/loss': 3.694946765899658, 'test/num_examples': 10000, 'score': 19783.164499998093, 'total_duration': 22244.399122714996, 'accumulated_submission_time': 19783.164499998093, 'accumulated_eval_time': 2456.8914000988007, 'accumulated_logging_time': 1.9217658042907715, 'global_step': 42601, 'preemption_count': 0}), (43515, {'train/accuracy': 0.40892577171325684, 'train/loss': 2.723781108856201, 'validation/accuracy': 0.37831997871398926, 'validation/loss': 2.8741984367370605, 'validation/num_examples': 50000, 'test/accuracy': 0.2962000072002411, 'test/loss': 3.4557039737701416, 'test/num_examples': 10000, 'score': 20203.380412817, 'total_duration': 22716.77877688408, 'accumulated_submission_time': 20203.380412817, 'accumulated_eval_time': 2508.969986438751, 'accumulated_logging_time': 1.9536054134368896, 'global_step': 43515, 'preemption_count': 0}), (44422, {'train/accuracy': 0.3873632848262787, 'train/loss': 2.8844244480133057, 'validation/accuracy': 0.3587599992752075, 'validation/loss': 3.038835287094116, 'validation/num_examples': 50000, 'test/accuracy': 0.27470001578330994, 'test/loss': 3.624547004699707, 'test/num_examples': 10000, 'score': 20623.665967941284, 'total_duration': 23187.171246290207, 'accumulated_submission_time': 20623.665967941284, 'accumulated_eval_time': 2558.9942378997803, 'accumulated_logging_time': 1.9841821193695068, 'global_step': 44422, 'preemption_count': 0}), (45332, {'train/accuracy': 0.42726561427116394, 'train/loss': 2.607675313949585, 'validation/accuracy': 0.36705997586250305, 'validation/loss': 2.9391558170318604, 'validation/num_examples': 50000, 'test/accuracy': 0.28519999980926514, 'test/loss': 3.533428192138672, 'test/num_examples': 10000, 'score': 21043.75366783142, 'total_duration': 23657.901322603226, 'accumulated_submission_time': 21043.75366783142, 'accumulated_eval_time': 2609.552106142044, 'accumulated_logging_time': 2.0164127349853516, 'global_step': 45332, 'preemption_count': 0}), (46241, {'train/accuracy': 0.3944140672683716, 'train/loss': 2.8179240226745605, 'validation/accuracy': 0.3656199872493744, 'validation/loss': 2.9761483669281006, 'validation/num_examples': 50000, 'test/accuracy': 0.28110000491142273, 'test/loss': 3.5713536739349365, 'test/num_examples': 10000, 'score': 21463.72553873062, 'total_duration': 24131.569739818573, 'accumulated_submission_time': 21463.72553873062, 'accumulated_eval_time': 2663.1634533405304, 'accumulated_logging_time': 2.049370527267456, 'global_step': 46241, 'preemption_count': 0}), (47149, {'train/accuracy': 0.4058789014816284, 'train/loss': 2.753173828125, 'validation/accuracy': 0.374239981174469, 'validation/loss': 2.925992965698242, 'validation/num_examples': 50000, 'test/accuracy': 0.28690001368522644, 'test/loss': 3.5421478748321533, 'test/num_examples': 10000, 'score': 21884.07934308052, 'total_duration': 24604.291342496872, 'accumulated_submission_time': 21884.07934308052, 'accumulated_eval_time': 2715.4439051151276, 'accumulated_logging_time': 2.0853078365325928, 'global_step': 47149, 'preemption_count': 0}), (48055, {'train/accuracy': 0.42917966842651367, 'train/loss': 2.6209170818328857, 'validation/accuracy': 0.38675999641418457, 'validation/loss': 2.860398054122925, 'validation/num_examples': 50000, 'test/accuracy': 0.29600000381469727, 'test/loss': 3.4566426277160645, 'test/num_examples': 10000, 'score': 22304.013244628906, 'total_duration': 25076.32378435135, 'accumulated_submission_time': 22304.013244628906, 'accumulated_eval_time': 2767.4498670101166, 'accumulated_logging_time': 2.1263139247894287, 'global_step': 48055, 'preemption_count': 0}), (48963, {'train/accuracy': 0.41550779342651367, 'train/loss': 2.642244815826416, 'validation/accuracy': 0.38763999938964844, 'validation/loss': 2.803826093673706, 'validation/num_examples': 50000, 'test/accuracy': 0.30470001697540283, 'test/loss': 3.414707899093628, 'test/num_examples': 10000, 'score': 22724.277349472046, 'total_duration': 25547.15549659729, 'accumulated_submission_time': 22724.277349472046, 'accumulated_eval_time': 2817.928693294525, 'accumulated_logging_time': 2.163120985031128, 'global_step': 48963, 'preemption_count': 0}), (49872, {'train/accuracy': 0.4216601550579071, 'train/loss': 2.6226537227630615, 'validation/accuracy': 0.39419999718666077, 'validation/loss': 2.784731388092041, 'validation/num_examples': 50000, 'test/accuracy': 0.30070000886917114, 'test/loss': 3.3947463035583496, 'test/num_examples': 10000, 'score': 23144.547844409943, 'total_duration': 26019.239842414856, 'accumulated_submission_time': 23144.547844409943, 'accumulated_eval_time': 2869.654707431793, 'accumulated_logging_time': 2.1987524032592773, 'global_step': 49872, 'preemption_count': 0}), (50782, {'train/accuracy': 0.431640625, 'train/loss': 2.567415475845337, 'validation/accuracy': 0.3929999768733978, 'validation/loss': 2.7868287563323975, 'validation/num_examples': 50000, 'test/accuracy': 0.30390000343322754, 'test/loss': 3.3923990726470947, 'test/num_examples': 10000, 'score': 23564.829872846603, 'total_duration': 26492.410955429077, 'accumulated_submission_time': 23564.829872846603, 'accumulated_eval_time': 2922.45498919487, 'accumulated_logging_time': 2.235773801803589, 'global_step': 50782, 'preemption_count': 0}), (51689, {'train/accuracy': 0.4244335889816284, 'train/loss': 2.612565279006958, 'validation/accuracy': 0.3941600024700165, 'validation/loss': 2.762552499771118, 'validation/num_examples': 50000, 'test/accuracy': 0.30320000648498535, 'test/loss': 3.374166250228882, 'test/num_examples': 10000, 'score': 23985.074984312057, 'total_duration': 26965.507806777954, 'accumulated_submission_time': 23985.074984312057, 'accumulated_eval_time': 2975.215850353241, 'accumulated_logging_time': 2.2752928733825684, 'global_step': 51689, 'preemption_count': 0}), (52597, {'train/accuracy': 0.4292578101158142, 'train/loss': 2.5953733921051025, 'validation/accuracy': 0.39813998341560364, 'validation/loss': 2.7684292793273926, 'validation/num_examples': 50000, 'test/accuracy': 0.3110000193119049, 'test/loss': 3.386230230331421, 'test/num_examples': 10000, 'score': 24405.261734247208, 'total_duration': 27436.437440156937, 'accumulated_submission_time': 24405.261734247208, 'accumulated_eval_time': 3025.862987279892, 'accumulated_logging_time': 2.3195648193359375, 'global_step': 52597, 'preemption_count': 0}), (53503, {'train/accuracy': 0.4274999797344208, 'train/loss': 2.5975615978240967, 'validation/accuracy': 0.38499999046325684, 'validation/loss': 2.821701765060425, 'validation/num_examples': 50000, 'test/accuracy': 0.29980000853538513, 'test/loss': 3.433039903640747, 'test/num_examples': 10000, 'score': 24825.304622650146, 'total_duration': 27908.778692007065, 'accumulated_submission_time': 24825.304622650146, 'accumulated_eval_time': 3078.066597223282, 'accumulated_logging_time': 2.363283157348633, 'global_step': 53503, 'preemption_count': 0}), (54413, {'train/accuracy': 0.4327929615974426, 'train/loss': 2.579705238342285, 'validation/accuracy': 0.3990999758243561, 'validation/loss': 2.741082191467285, 'validation/num_examples': 50000, 'test/accuracy': 0.3037000000476837, 'test/loss': 3.3778457641601562, 'test/num_examples': 10000, 'score': 25245.264453172684, 'total_duration': 28380.64738035202, 'accumulated_submission_time': 25245.264453172684, 'accumulated_eval_time': 3129.888496398926, 'accumulated_logging_time': 2.3979485034942627, 'global_step': 54413, 'preemption_count': 0}), (55323, {'train/accuracy': 0.4314843714237213, 'train/loss': 2.5818235874176025, 'validation/accuracy': 0.4027799963951111, 'validation/loss': 2.7566277980804443, 'validation/num_examples': 50000, 'test/accuracy': 0.31200000643730164, 'test/loss': 3.340991973876953, 'test/num_examples': 10000, 'score': 25665.591471672058, 'total_duration': 28852.82416653633, 'accumulated_submission_time': 25665.591471672058, 'accumulated_eval_time': 3181.6526210308075, 'accumulated_logging_time': 2.4328207969665527, 'global_step': 55323, 'preemption_count': 0}), (56233, {'train/accuracy': 0.43990233540534973, 'train/loss': 2.5200986862182617, 'validation/accuracy': 0.407260000705719, 'validation/loss': 2.6993579864501953, 'validation/num_examples': 50000, 'test/accuracy': 0.31220000982284546, 'test/loss': 3.344634532928467, 'test/num_examples': 10000, 'score': 26085.61327648163, 'total_duration': 29324.685839653015, 'accumulated_submission_time': 26085.61327648163, 'accumulated_eval_time': 3233.405528306961, 'accumulated_logging_time': 2.467010736465454, 'global_step': 56233, 'preemption_count': 0}), (57145, {'train/accuracy': 0.4365624785423279, 'train/loss': 2.546628475189209, 'validation/accuracy': 0.40609997510910034, 'validation/loss': 2.6989681720733643, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.323418140411377, 'test/num_examples': 10000, 'score': 26505.705159902573, 'total_duration': 29796.24742078781, 'accumulated_submission_time': 26505.705159902573, 'accumulated_eval_time': 3284.786096572876, 'accumulated_logging_time': 2.5048763751983643, 'global_step': 57145, 'preemption_count': 0}), (58056, {'train/accuracy': 0.4286132752895355, 'train/loss': 2.626063108444214, 'validation/accuracy': 0.40685999393463135, 'validation/loss': 2.7548227310180664, 'validation/num_examples': 50000, 'test/accuracy': 0.3158000111579895, 'test/loss': 3.3459408283233643, 'test/num_examples': 10000, 'score': 26925.716034412384, 'total_duration': 30267.275554418564, 'accumulated_submission_time': 26925.716034412384, 'accumulated_eval_time': 3335.712982416153, 'accumulated_logging_time': 2.5432417392730713, 'global_step': 58056, 'preemption_count': 0}), (58965, {'train/accuracy': 0.4401562511920929, 'train/loss': 2.5463802814483643, 'validation/accuracy': 0.4059799909591675, 'validation/loss': 2.7379801273345947, 'validation/num_examples': 50000, 'test/accuracy': 0.3166000247001648, 'test/loss': 3.3564846515655518, 'test/num_examples': 10000, 'score': 27345.927248716354, 'total_duration': 30740.203387737274, 'accumulated_submission_time': 27345.927248716354, 'accumulated_eval_time': 3388.341548681259, 'accumulated_logging_time': 2.5799450874328613, 'global_step': 58965, 'preemption_count': 0}), (59875, {'train/accuracy': 0.4756445288658142, 'train/loss': 2.3494179248809814, 'validation/accuracy': 0.4097599983215332, 'validation/loss': 2.690446376800537, 'validation/num_examples': 50000, 'test/accuracy': 0.3135000169277191, 'test/loss': 3.311406373977661, 'test/num_examples': 10000, 'score': 27766.268231868744, 'total_duration': 31214.877977132797, 'accumulated_submission_time': 27766.268231868744, 'accumulated_eval_time': 3442.583624601364, 'accumulated_logging_time': 2.6201019287109375, 'global_step': 59875, 'preemption_count': 0}), (60784, {'train/accuracy': 0.440253883600235, 'train/loss': 2.530428647994995, 'validation/accuracy': 0.41533997654914856, 'validation/loss': 2.6711795330047607, 'validation/num_examples': 50000, 'test/accuracy': 0.32510000467300415, 'test/loss': 3.2931430339813232, 'test/num_examples': 10000, 'score': 28186.384392499924, 'total_duration': 31687.30434346199, 'accumulated_submission_time': 28186.384392499924, 'accumulated_eval_time': 3494.8005118370056, 'accumulated_logging_time': 2.661822557449341, 'global_step': 60784, 'preemption_count': 0}), (61693, {'train/accuracy': 0.4561132788658142, 'train/loss': 2.4191908836364746, 'validation/accuracy': 0.4205799996852875, 'validation/loss': 2.6124274730682373, 'validation/num_examples': 50000, 'test/accuracy': 0.3247000277042389, 'test/loss': 3.2571167945861816, 'test/num_examples': 10000, 'score': 28606.574649333954, 'total_duration': 32160.68154025078, 'accumulated_submission_time': 28606.574649333954, 'accumulated_eval_time': 3547.8963441848755, 'accumulated_logging_time': 2.700721263885498, 'global_step': 61693, 'preemption_count': 0}), (62604, {'train/accuracy': 0.4695117175579071, 'train/loss': 2.3920223712921143, 'validation/accuracy': 0.4214800000190735, 'validation/loss': 2.663222074508667, 'validation/num_examples': 50000, 'test/accuracy': 0.31950002908706665, 'test/loss': 3.280048131942749, 'test/num_examples': 10000, 'score': 29026.541818618774, 'total_duration': 32634.209529399872, 'accumulated_submission_time': 29026.541818618774, 'accumulated_eval_time': 3601.3643288612366, 'accumulated_logging_time': 2.742002010345459, 'global_step': 62604, 'preemption_count': 0}), (63516, {'train/accuracy': 0.43568357825279236, 'train/loss': 2.5817770957946777, 'validation/accuracy': 0.41037997603416443, 'validation/loss': 2.7430014610290527, 'validation/num_examples': 50000, 'test/accuracy': 0.3118000030517578, 'test/loss': 3.331474781036377, 'test/num_examples': 10000, 'score': 29446.83342576027, 'total_duration': 33106.452523231506, 'accumulated_submission_time': 29446.83342576027, 'accumulated_eval_time': 3653.226641178131, 'accumulated_logging_time': 2.778369903564453, 'global_step': 63516, 'preemption_count': 0}), (64423, {'train/accuracy': 0.4575585722923279, 'train/loss': 2.4170541763305664, 'validation/accuracy': 0.42625999450683594, 'validation/loss': 2.5896904468536377, 'validation/num_examples': 50000, 'test/accuracy': 0.33390000462532043, 'test/loss': 3.209775447845459, 'test/num_examples': 10000, 'score': 29867.208926677704, 'total_duration': 33581.21010494232, 'accumulated_submission_time': 29867.208926677704, 'accumulated_eval_time': 3707.521521806717, 'accumulated_logging_time': 2.814645767211914, 'global_step': 64423, 'preemption_count': 0}), (65330, {'train/accuracy': 0.4688476324081421, 'train/loss': 2.3632867336273193, 'validation/accuracy': 0.42549997568130493, 'validation/loss': 2.597842216491699, 'validation/num_examples': 50000, 'test/accuracy': 0.3272000253200531, 'test/loss': 3.247157573699951, 'test/num_examples': 10000, 'score': 30287.316556453705, 'total_duration': 34054.11939907074, 'accumulated_submission_time': 30287.316556453705, 'accumulated_eval_time': 3760.2291600704193, 'accumulated_logging_time': 2.8578405380249023, 'global_step': 65330, 'preemption_count': 0}), (66240, {'train/accuracy': 0.4552929699420929, 'train/loss': 2.4185917377471924, 'validation/accuracy': 0.4275999963283539, 'validation/loss': 2.574275016784668, 'validation/num_examples': 50000, 'test/accuracy': 0.33240002393722534, 'test/loss': 3.1866111755371094, 'test/num_examples': 10000, 'score': 30707.300862312317, 'total_duration': 34526.12924003601, 'accumulated_submission_time': 30707.300862312317, 'accumulated_eval_time': 3812.165778875351, 'accumulated_logging_time': 2.89416766166687, 'global_step': 66240, 'preemption_count': 0}), (67149, {'train/accuracy': 0.4733007848262787, 'train/loss': 2.328538417816162, 'validation/accuracy': 0.4402399957180023, 'validation/loss': 2.4984281063079834, 'validation/num_examples': 50000, 'test/accuracy': 0.332800030708313, 'test/loss': 3.1850574016571045, 'test/num_examples': 10000, 'score': 31127.646122694016, 'total_duration': 35001.00862288475, 'accumulated_submission_time': 31127.646122694016, 'accumulated_eval_time': 3866.6085917949677, 'accumulated_logging_time': 2.9339511394500732, 'global_step': 67149, 'preemption_count': 0}), (68058, {'train/accuracy': 0.4591992199420929, 'train/loss': 2.4289512634277344, 'validation/accuracy': 0.4236399829387665, 'validation/loss': 2.623819351196289, 'validation/num_examples': 50000, 'test/accuracy': 0.33340001106262207, 'test/loss': 3.2271764278411865, 'test/num_examples': 10000, 'score': 31547.656118154526, 'total_duration': 35474.00371336937, 'accumulated_submission_time': 31547.656118154526, 'accumulated_eval_time': 3919.500876426697, 'accumulated_logging_time': 2.9747848510742188, 'global_step': 68058, 'preemption_count': 0}), (68967, {'train/accuracy': 0.46486327052116394, 'train/loss': 2.3892576694488525, 'validation/accuracy': 0.43845999240875244, 'validation/loss': 2.5289251804351807, 'validation/num_examples': 50000, 'test/accuracy': 0.3456000089645386, 'test/loss': 3.1654460430145264, 'test/num_examples': 10000, 'score': 31967.69016432762, 'total_duration': 35946.450082063675, 'accumulated_submission_time': 31967.69016432762, 'accumulated_eval_time': 3971.823595046997, 'accumulated_logging_time': 3.0140380859375, 'global_step': 68967, 'preemption_count': 0}), (69875, {'train/accuracy': 0.46867185831069946, 'train/loss': 2.3740696907043457, 'validation/accuracy': 0.43522000312805176, 'validation/loss': 2.5472748279571533, 'validation/num_examples': 50000, 'test/accuracy': 0.3344000279903412, 'test/loss': 3.193197011947632, 'test/num_examples': 10000, 'score': 32388.05753827095, 'total_duration': 36421.328429460526, 'accumulated_submission_time': 32388.05753827095, 'accumulated_eval_time': 4026.2420842647552, 'accumulated_logging_time': 3.0550498962402344, 'global_step': 69875, 'preemption_count': 0}), (70782, {'train/accuracy': 0.4740038812160492, 'train/loss': 2.324805974960327, 'validation/accuracy': 0.4377000033855438, 'validation/loss': 2.5263214111328125, 'validation/num_examples': 50000, 'test/accuracy': 0.3368000090122223, 'test/loss': 3.1847870349884033, 'test/num_examples': 10000, 'score': 32808.10027337074, 'total_duration': 36893.90516161919, 'accumulated_submission_time': 32808.10027337074, 'accumulated_eval_time': 4078.6816279888153, 'accumulated_logging_time': 3.0984416007995605, 'global_step': 70782, 'preemption_count': 0}), (71692, {'train/accuracy': 0.4591406285762787, 'train/loss': 2.428839921951294, 'validation/accuracy': 0.42800000309944153, 'validation/loss': 2.5926687717437744, 'validation/num_examples': 50000, 'test/accuracy': 0.33390000462532043, 'test/loss': 3.219151020050049, 'test/num_examples': 10000, 'score': 33228.35967874527, 'total_duration': 37367.487449645996, 'accumulated_submission_time': 33228.35967874527, 'accumulated_eval_time': 4131.9138832092285, 'accumulated_logging_time': 3.1382124423980713, 'global_step': 71692, 'preemption_count': 0}), (72601, {'train/accuracy': 0.4733984172344208, 'train/loss': 2.335913896560669, 'validation/accuracy': 0.442220002412796, 'validation/loss': 2.501166582107544, 'validation/num_examples': 50000, 'test/accuracy': 0.3451000154018402, 'test/loss': 3.1486642360687256, 'test/num_examples': 10000, 'score': 33648.43023991585, 'total_duration': 37842.20454144478, 'accumulated_submission_time': 33648.43023991585, 'accumulated_eval_time': 4186.46707201004, 'accumulated_logging_time': 3.180236339569092, 'global_step': 72601, 'preemption_count': 0}), (73512, {'train/accuracy': 0.47892576456069946, 'train/loss': 2.335693359375, 'validation/accuracy': 0.44411998987197876, 'validation/loss': 2.5147647857666016, 'validation/num_examples': 50000, 'test/accuracy': 0.34170001745224, 'test/loss': 3.1700222492218018, 'test/num_examples': 10000, 'score': 34068.49537944794, 'total_duration': 38316.67360329628, 'accumulated_submission_time': 34068.49537944794, 'accumulated_eval_time': 4240.780715942383, 'accumulated_logging_time': 3.2186429500579834, 'global_step': 73512, 'preemption_count': 0}), (74422, {'train/accuracy': 0.514941394329071, 'train/loss': 2.1037821769714355, 'validation/accuracy': 0.456279993057251, 'validation/loss': 2.4261739253997803, 'validation/num_examples': 50000, 'test/accuracy': 0.3525000214576721, 'test/loss': 3.0565521717071533, 'test/num_examples': 10000, 'score': 34488.55130815506, 'total_duration': 38787.65641450882, 'accumulated_submission_time': 34488.55130815506, 'accumulated_eval_time': 4291.616386175156, 'accumulated_logging_time': 3.258612632751465, 'global_step': 74422, 'preemption_count': 0}), (75331, {'train/accuracy': 0.4822070300579071, 'train/loss': 2.265528440475464, 'validation/accuracy': 0.4532800018787384, 'validation/loss': 2.43257212638855, 'validation/num_examples': 50000, 'test/accuracy': 0.3541000187397003, 'test/loss': 3.067671537399292, 'test/num_examples': 10000, 'score': 34908.481731414795, 'total_duration': 39258.649804115295, 'accumulated_submission_time': 34908.481731414795, 'accumulated_eval_time': 4342.585709571838, 'accumulated_logging_time': 3.3010001182556152, 'global_step': 75331, 'preemption_count': 0}), (76238, {'train/accuracy': 0.4829687476158142, 'train/loss': 2.3049960136413574, 'validation/accuracy': 0.44523999094963074, 'validation/loss': 2.4958457946777344, 'validation/num_examples': 50000, 'test/accuracy': 0.34310001134872437, 'test/loss': 3.1404335498809814, 'test/num_examples': 10000, 'score': 35328.56245660782, 'total_duration': 39728.390417575836, 'accumulated_submission_time': 35328.56245660782, 'accumulated_eval_time': 4392.15483045578, 'accumulated_logging_time': 3.3405954837799072, 'global_step': 76238, 'preemption_count': 0}), (77145, {'train/accuracy': 0.5133984088897705, 'train/loss': 2.1134865283966064, 'validation/accuracy': 0.45207998156547546, 'validation/loss': 2.433824062347412, 'validation/num_examples': 50000, 'test/accuracy': 0.3474000096321106, 'test/loss': 3.1019248962402344, 'test/num_examples': 10000, 'score': 35748.48908209801, 'total_duration': 40200.3743493557, 'accumulated_submission_time': 35748.48908209801, 'accumulated_eval_time': 4444.11982011795, 'accumulated_logging_time': 3.381718873977661, 'global_step': 77145, 'preemption_count': 0}), (78051, {'train/accuracy': 0.4828515648841858, 'train/loss': 2.2945902347564697, 'validation/accuracy': 0.4523399770259857, 'validation/loss': 2.4661505222320557, 'validation/num_examples': 50000, 'test/accuracy': 0.3499000072479248, 'test/loss': 3.089308261871338, 'test/num_examples': 10000, 'score': 36168.59254050255, 'total_duration': 40671.889099121094, 'accumulated_submission_time': 36168.59254050255, 'accumulated_eval_time': 4495.4370748996735, 'accumulated_logging_time': 3.4243533611297607, 'global_step': 78051, 'preemption_count': 0}), (78960, {'train/accuracy': 0.4942382574081421, 'train/loss': 2.2199559211730957, 'validation/accuracy': 0.45805999636650085, 'validation/loss': 2.4189834594726562, 'validation/num_examples': 50000, 'test/accuracy': 0.34550002217292786, 'test/loss': 3.0912413597106934, 'test/num_examples': 10000, 'score': 36588.80544137955, 'total_duration': 41143.55085515976, 'accumulated_submission_time': 36588.80544137955, 'accumulated_eval_time': 4546.794453859329, 'accumulated_logging_time': 3.4637033939361572, 'global_step': 78960, 'preemption_count': 0}), (79864, {'train/accuracy': 0.5013476610183716, 'train/loss': 2.2078988552093506, 'validation/accuracy': 0.4576599895954132, 'validation/loss': 2.443406581878662, 'validation/num_examples': 50000, 'test/accuracy': 0.3482000231742859, 'test/loss': 3.0952043533325195, 'test/num_examples': 10000, 'score': 37008.87369513512, 'total_duration': 41617.35136270523, 'accumulated_submission_time': 37008.87369513512, 'accumulated_eval_time': 4600.427646636963, 'accumulated_logging_time': 3.510913848876953, 'global_step': 79864, 'preemption_count': 0}), (80773, {'train/accuracy': 0.4915429651737213, 'train/loss': 2.2458813190460205, 'validation/accuracy': 0.45997998118400574, 'validation/loss': 2.409734010696411, 'validation/num_examples': 50000, 'test/accuracy': 0.35500001907348633, 'test/loss': 3.052574396133423, 'test/num_examples': 10000, 'score': 37429.113872528076, 'total_duration': 42090.65738582611, 'accumulated_submission_time': 37429.113872528076, 'accumulated_eval_time': 4653.40140414238, 'accumulated_logging_time': 3.551713705062866, 'global_step': 80773, 'preemption_count': 0}), (81681, {'train/accuracy': 0.49703124165534973, 'train/loss': 2.2210443019866943, 'validation/accuracy': 0.4611999988555908, 'validation/loss': 2.4077775478363037, 'validation/num_examples': 50000, 'test/accuracy': 0.35860002040863037, 'test/loss': 3.044772148132324, 'test/num_examples': 10000, 'score': 37849.28627562523, 'total_duration': 42562.85619473457, 'accumulated_submission_time': 37849.28627562523, 'accumulated_eval_time': 4705.331708192825, 'accumulated_logging_time': 3.5972063541412354, 'global_step': 81681, 'preemption_count': 0}), (82586, {'train/accuracy': 0.5177343487739563, 'train/loss': 2.106281042098999, 'validation/accuracy': 0.47005999088287354, 'validation/loss': 2.3485488891601562, 'validation/num_examples': 50000, 'test/accuracy': 0.3614000082015991, 'test/loss': 3.003753185272217, 'test/num_examples': 10000, 'score': 38269.44111919403, 'total_duration': 43033.84465241432, 'accumulated_submission_time': 38269.44111919403, 'accumulated_eval_time': 4756.069163322449, 'accumulated_logging_time': 3.641781806945801, 'global_step': 82586, 'preemption_count': 0}), (83497, {'train/accuracy': 0.4992382824420929, 'train/loss': 2.189939260482788, 'validation/accuracy': 0.4672999978065491, 'validation/loss': 2.3583219051361084, 'validation/num_examples': 50000, 'test/accuracy': 0.3677000105381012, 'test/loss': 2.9964702129364014, 'test/num_examples': 10000, 'score': 38689.66334319115, 'total_duration': 43508.62535381317, 'accumulated_submission_time': 38689.66334319115, 'accumulated_eval_time': 4810.5310571193695, 'accumulated_logging_time': 3.6866133213043213, 'global_step': 83497, 'preemption_count': 0}), (84399, {'train/accuracy': 0.5051953196525574, 'train/loss': 2.1985788345336914, 'validation/accuracy': 0.4702000021934509, 'validation/loss': 2.3659684658050537, 'validation/num_examples': 50000, 'test/accuracy': 0.3638000190258026, 'test/loss': 3.0029189586639404, 'test/num_examples': 10000, 'score': 39109.79868769646, 'total_duration': 43980.53152704239, 'accumulated_submission_time': 39109.79868769646, 'accumulated_eval_time': 4862.211612701416, 'accumulated_logging_time': 3.7260732650756836, 'global_step': 84399, 'preemption_count': 0}), (85308, {'train/accuracy': 0.505664050579071, 'train/loss': 2.1896252632141113, 'validation/accuracy': 0.4622199833393097, 'validation/loss': 2.4180538654327393, 'validation/num_examples': 50000, 'test/accuracy': 0.35630002617836, 'test/loss': 3.0596253871917725, 'test/num_examples': 10000, 'score': 39529.994701862335, 'total_duration': 44453.88608646393, 'accumulated_submission_time': 39529.994701862335, 'accumulated_eval_time': 4915.274274110794, 'accumulated_logging_time': 3.7699716091156006, 'global_step': 85308, 'preemption_count': 0}), (86215, {'train/accuracy': 0.49943357706069946, 'train/loss': 2.2269043922424316, 'validation/accuracy': 0.4710399806499481, 'validation/loss': 2.3905842304229736, 'validation/num_examples': 50000, 'test/accuracy': 0.3636000156402588, 'test/loss': 3.021517038345337, 'test/num_examples': 10000, 'score': 39950.410684108734, 'total_duration': 44924.90372800827, 'accumulated_submission_time': 39950.410684108734, 'accumulated_eval_time': 4965.784074783325, 'accumulated_logging_time': 3.8100292682647705, 'global_step': 86215, 'preemption_count': 0}), (87118, {'train/accuracy': 0.5128515362739563, 'train/loss': 2.143615484237671, 'validation/accuracy': 0.48099997639656067, 'validation/loss': 2.3133881092071533, 'validation/num_examples': 50000, 'test/accuracy': 0.37530001997947693, 'test/loss': 2.977247714996338, 'test/num_examples': 10000, 'score': 40370.71117019653, 'total_duration': 45396.8552069664, 'accumulated_submission_time': 40370.71117019653, 'accumulated_eval_time': 5017.331121683121, 'accumulated_logging_time': 3.8627686500549316, 'global_step': 87118, 'preemption_count': 0}), (88021, {'train/accuracy': 0.5203710794448853, 'train/loss': 2.086500883102417, 'validation/accuracy': 0.47773998975753784, 'validation/loss': 2.3107686042785645, 'validation/num_examples': 50000, 'test/accuracy': 0.38050001859664917, 'test/loss': 2.938375234603882, 'test/num_examples': 10000, 'score': 40790.89816689491, 'total_duration': 45868.57350111008, 'accumulated_submission_time': 40790.89816689491, 'accumulated_eval_time': 5068.761607885361, 'accumulated_logging_time': 3.913156747817993, 'global_step': 88021, 'preemption_count': 0}), (88927, {'train/accuracy': 0.5184179544448853, 'train/loss': 2.0980348587036133, 'validation/accuracy': 0.4792400002479553, 'validation/loss': 2.289841413497925, 'validation/num_examples': 50000, 'test/accuracy': 0.3777000308036804, 'test/loss': 2.9439592361450195, 'test/num_examples': 10000, 'score': 41210.95104074478, 'total_duration': 46339.93909049034, 'accumulated_submission_time': 41210.95104074478, 'accumulated_eval_time': 5119.981722831726, 'accumulated_logging_time': 3.9541375637054443, 'global_step': 88927, 'preemption_count': 0}), (89836, {'train/accuracy': 0.5190234184265137, 'train/loss': 2.101839303970337, 'validation/accuracy': 0.4875999987125397, 'validation/loss': 2.2763702869415283, 'validation/num_examples': 50000, 'test/accuracy': 0.3736000061035156, 'test/loss': 2.944952964782715, 'test/num_examples': 10000, 'score': 41631.14861416817, 'total_duration': 46812.072179317474, 'accumulated_submission_time': 41631.14861416817, 'accumulated_eval_time': 5171.822760105133, 'accumulated_logging_time': 3.9972646236419678, 'global_step': 89836, 'preemption_count': 0}), (90743, {'train/accuracy': 0.5146093368530273, 'train/loss': 2.1558313369750977, 'validation/accuracy': 0.47551998496055603, 'validation/loss': 2.3405981063842773, 'validation/num_examples': 50000, 'test/accuracy': 0.3727000057697296, 'test/loss': 2.9976208209991455, 'test/num_examples': 10000, 'score': 42051.27122282982, 'total_duration': 47286.098991155624, 'accumulated_submission_time': 42051.27122282982, 'accumulated_eval_time': 5225.636157512665, 'accumulated_logging_time': 4.036552667617798, 'global_step': 90743, 'preemption_count': 0}), (91651, {'train/accuracy': 0.5436718463897705, 'train/loss': 2.0345213413238525, 'validation/accuracy': 0.4791199862957001, 'validation/loss': 2.349999189376831, 'validation/num_examples': 50000, 'test/accuracy': 0.37130001187324524, 'test/loss': 2.993382453918457, 'test/num_examples': 10000, 'score': 42471.242216825485, 'total_duration': 47757.972594976425, 'accumulated_submission_time': 42471.242216825485, 'accumulated_eval_time': 5277.445053577423, 'accumulated_logging_time': 4.078348875045776, 'global_step': 91651, 'preemption_count': 0}), (92556, {'train/accuracy': 0.5254101157188416, 'train/loss': 2.0577504634857178, 'validation/accuracy': 0.4925599992275238, 'validation/loss': 2.2484467029571533, 'validation/num_examples': 50000, 'test/accuracy': 0.38120001554489136, 'test/loss': 2.9217004776000977, 'test/num_examples': 10000, 'score': 42891.40844297409, 'total_duration': 48231.31739473343, 'accumulated_submission_time': 42891.40844297409, 'accumulated_eval_time': 5330.52893781662, 'accumulated_logging_time': 4.1219470500946045, 'global_step': 92556, 'preemption_count': 0}), (93464, {'train/accuracy': 0.5331054329872131, 'train/loss': 2.0151867866516113, 'validation/accuracy': 0.49487999081611633, 'validation/loss': 2.2205684185028076, 'validation/num_examples': 50000, 'test/accuracy': 0.37950003147125244, 'test/loss': 2.9080698490142822, 'test/num_examples': 10000, 'score': 43311.48286437988, 'total_duration': 48703.985027074814, 'accumulated_submission_time': 43311.48286437988, 'accumulated_eval_time': 5383.027472257614, 'accumulated_logging_time': 4.1651999950408936, 'global_step': 93464, 'preemption_count': 0}), (94368, {'train/accuracy': 0.5615038871765137, 'train/loss': 1.8956527709960938, 'validation/accuracy': 0.5003199577331543, 'validation/loss': 2.2095954418182373, 'validation/num_examples': 50000, 'test/accuracy': 0.3873000144958496, 'test/loss': 2.886227607727051, 'test/num_examples': 10000, 'score': 43731.60246658325, 'total_duration': 49176.6410984993, 'accumulated_submission_time': 43731.60246658325, 'accumulated_eval_time': 5435.459993362427, 'accumulated_logging_time': 4.21837306022644, 'global_step': 94368, 'preemption_count': 0}), (95273, {'train/accuracy': 0.5392187237739563, 'train/loss': 1.992055058479309, 'validation/accuracy': 0.5055800080299377, 'validation/loss': 2.1670339107513428, 'validation/num_examples': 50000, 'test/accuracy': 0.3952000141143799, 'test/loss': 2.8237500190734863, 'test/num_examples': 10000, 'score': 44151.640016794205, 'total_duration': 49648.15318131447, 'accumulated_submission_time': 44151.640016794205, 'accumulated_eval_time': 5486.836786031723, 'accumulated_logging_time': 4.264795303344727, 'global_step': 95273, 'preemption_count': 0}), (96179, {'train/accuracy': 0.5384374856948853, 'train/loss': 2.011241912841797, 'validation/accuracy': 0.5003399848937988, 'validation/loss': 2.213174343109131, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.8492650985717773, 'test/num_examples': 10000, 'score': 44571.820883750916, 'total_duration': 50121.71514558792, 'accumulated_submission_time': 44571.820883750916, 'accumulated_eval_time': 5540.123233795166, 'accumulated_logging_time': 4.307686805725098, 'global_step': 96179, 'preemption_count': 0}), (97086, {'train/accuracy': 0.5535546541213989, 'train/loss': 1.9242347478866577, 'validation/accuracy': 0.5067799687385559, 'validation/loss': 2.171412706375122, 'validation/num_examples': 50000, 'test/accuracy': 0.398000031709671, 'test/loss': 2.8221137523651123, 'test/num_examples': 10000, 'score': 44992.06829333305, 'total_duration': 50591.74269366264, 'accumulated_submission_time': 44992.06829333305, 'accumulated_eval_time': 5589.800358533859, 'accumulated_logging_time': 4.35940146446228, 'global_step': 97086, 'preemption_count': 0}), (97993, {'train/accuracy': 0.532910168170929, 'train/loss': 2.055387020111084, 'validation/accuracy': 0.4993399977684021, 'validation/loss': 2.2423460483551025, 'validation/num_examples': 50000, 'test/accuracy': 0.38910001516342163, 'test/loss': 2.8874340057373047, 'test/num_examples': 10000, 'score': 45412.143065452576, 'total_duration': 51066.335882902145, 'accumulated_submission_time': 45412.143065452576, 'accumulated_eval_time': 5644.221765995026, 'accumulated_logging_time': 4.40524959564209, 'global_step': 97993, 'preemption_count': 0}), (98898, {'train/accuracy': 0.5400781035423279, 'train/loss': 2.028705358505249, 'validation/accuracy': 0.5006399750709534, 'validation/loss': 2.2367770671844482, 'validation/num_examples': 50000, 'test/accuracy': 0.3814000189304352, 'test/loss': 2.900303363800049, 'test/num_examples': 10000, 'score': 45832.18708944321, 'total_duration': 51537.9104912281, 'accumulated_submission_time': 45832.18708944321, 'accumulated_eval_time': 5695.656700849533, 'accumulated_logging_time': 4.4501214027404785, 'global_step': 98898, 'preemption_count': 0}), (99804, {'train/accuracy': 0.5590429306030273, 'train/loss': 1.925033688545227, 'validation/accuracy': 0.5095599889755249, 'validation/loss': 2.1697280406951904, 'validation/num_examples': 50000, 'test/accuracy': 0.39510002732276917, 'test/loss': 2.8197717666625977, 'test/num_examples': 10000, 'score': 46252.08383107185, 'total_duration': 52010.89158630371, 'accumulated_submission_time': 46252.08383107185, 'accumulated_eval_time': 5748.6351981163025, 'accumulated_logging_time': 4.504448413848877, 'global_step': 99804, 'preemption_count': 0}), (100711, {'train/accuracy': 0.5338476300239563, 'train/loss': 2.0648865699768066, 'validation/accuracy': 0.4959999918937683, 'validation/loss': 2.255467176437378, 'validation/num_examples': 50000, 'test/accuracy': 0.3897000253200531, 'test/loss': 2.8789451122283936, 'test/num_examples': 10000, 'score': 46672.03926706314, 'total_duration': 52482.73132753372, 'accumulated_submission_time': 46672.03926706314, 'accumulated_eval_time': 5800.427268981934, 'accumulated_logging_time': 4.545479774475098, 'global_step': 100711, 'preemption_count': 0}), (101614, {'train/accuracy': 0.5544726252555847, 'train/loss': 1.9149682521820068, 'validation/accuracy': 0.5176399946212769, 'validation/loss': 2.104093313217163, 'validation/num_examples': 50000, 'test/accuracy': 0.40470001101493835, 'test/loss': 2.784193754196167, 'test/num_examples': 10000, 'score': 47091.95157575607, 'total_duration': 52954.14155244827, 'accumulated_submission_time': 47091.95157575607, 'accumulated_eval_time': 5851.827321767807, 'accumulated_logging_time': 4.592525005340576, 'global_step': 101614, 'preemption_count': 0}), (102519, {'train/accuracy': 0.558300793170929, 'train/loss': 1.900124430656433, 'validation/accuracy': 0.5158199667930603, 'validation/loss': 2.1189815998077393, 'validation/num_examples': 50000, 'test/accuracy': 0.4044000208377838, 'test/loss': 2.776407241821289, 'test/num_examples': 10000, 'score': 47512.10585308075, 'total_duration': 53425.546273231506, 'accumulated_submission_time': 47512.10585308075, 'accumulated_eval_time': 5902.980116844177, 'accumulated_logging_time': 4.639246225357056, 'global_step': 102519, 'preemption_count': 0}), (103426, {'train/accuracy': 0.5542968511581421, 'train/loss': 1.9207837581634521, 'validation/accuracy': 0.5221399664878845, 'validation/loss': 2.0911567211151123, 'validation/num_examples': 50000, 'test/accuracy': 0.40730002522468567, 'test/loss': 2.7569947242736816, 'test/num_examples': 10000, 'score': 47932.22675728798, 'total_duration': 53898.2055375576, 'accumulated_submission_time': 47932.22675728798, 'accumulated_eval_time': 5955.4252672195435, 'accumulated_logging_time': 4.68032431602478, 'global_step': 103426, 'preemption_count': 0}), (104333, {'train/accuracy': 0.5587499737739563, 'train/loss': 1.940987467765808, 'validation/accuracy': 0.5200799703598022, 'validation/loss': 2.1409268379211426, 'validation/num_examples': 50000, 'test/accuracy': 0.40380001068115234, 'test/loss': 2.7949371337890625, 'test/num_examples': 10000, 'score': 48352.40204453468, 'total_duration': 54370.112864494324, 'accumulated_submission_time': 48352.40204453468, 'accumulated_eval_time': 6007.061721801758, 'accumulated_logging_time': 4.724331378936768, 'global_step': 104333, 'preemption_count': 0}), (105241, {'train/accuracy': 0.560839831829071, 'train/loss': 1.9002279043197632, 'validation/accuracy': 0.514959990978241, 'validation/loss': 2.125225067138672, 'validation/num_examples': 50000, 'test/accuracy': 0.403300017118454, 'test/loss': 2.796470880508423, 'test/num_examples': 10000, 'score': 48772.720823049545, 'total_duration': 54844.04375267029, 'accumulated_submission_time': 48772.720823049545, 'accumulated_eval_time': 6060.579028129578, 'accumulated_logging_time': 4.768460273742676, 'global_step': 105241, 'preemption_count': 0}), (106150, {'train/accuracy': 0.5675585865974426, 'train/loss': 1.8988195657730103, 'validation/accuracy': 0.5279399752616882, 'validation/loss': 2.0975606441497803, 'validation/num_examples': 50000, 'test/accuracy': 0.4099000096321106, 'test/loss': 2.748702049255371, 'test/num_examples': 10000, 'score': 49193.001353263855, 'total_duration': 55316.173254966736, 'accumulated_submission_time': 49193.001353263855, 'accumulated_eval_time': 6112.325652837753, 'accumulated_logging_time': 4.819467544555664, 'global_step': 106150, 'preemption_count': 0}), (107055, {'train/accuracy': 0.5690038800239563, 'train/loss': 1.8810782432556152, 'validation/accuracy': 0.5263199806213379, 'validation/loss': 2.085660457611084, 'validation/num_examples': 50000, 'test/accuracy': 0.41220003366470337, 'test/loss': 2.7401907444000244, 'test/num_examples': 10000, 'score': 49612.96082854271, 'total_duration': 55788.6028881073, 'accumulated_submission_time': 49612.96082854271, 'accumulated_eval_time': 6164.696760416031, 'accumulated_logging_time': 4.867316961288452, 'global_step': 107055, 'preemption_count': 0}), (107961, {'train/accuracy': 0.5733398199081421, 'train/loss': 1.872202754020691, 'validation/accuracy': 0.5306199789047241, 'validation/loss': 2.0735056400299072, 'validation/num_examples': 50000, 'test/accuracy': 0.417900025844574, 'test/loss': 2.732545852661133, 'test/num_examples': 10000, 'score': 50033.20030045509, 'total_duration': 56263.09414482117, 'accumulated_submission_time': 50033.20030045509, 'accumulated_eval_time': 6218.849471569061, 'accumulated_logging_time': 4.915220022201538, 'global_step': 107961, 'preemption_count': 0}), (108871, {'train/accuracy': 0.6080859303474426, 'train/loss': 1.6695939302444458, 'validation/accuracy': 0.5309799909591675, 'validation/loss': 2.035268545150757, 'validation/num_examples': 50000, 'test/accuracy': 0.41880002617836, 'test/loss': 2.701533317565918, 'test/num_examples': 10000, 'score': 50453.37967920303, 'total_duration': 56735.69892835617, 'accumulated_submission_time': 50453.37967920303, 'accumulated_eval_time': 6271.163687944412, 'accumulated_logging_time': 4.974422931671143, 'global_step': 108871, 'preemption_count': 0}), (109779, {'train/accuracy': 0.5797070264816284, 'train/loss': 1.7871202230453491, 'validation/accuracy': 0.5417999625205994, 'validation/loss': 1.9782638549804688, 'validation/num_examples': 50000, 'test/accuracy': 0.42670002579689026, 'test/loss': 2.641922950744629, 'test/num_examples': 10000, 'score': 50873.40883421898, 'total_duration': 57209.77555155754, 'accumulated_submission_time': 50873.40883421898, 'accumulated_eval_time': 6325.108122825623, 'accumulated_logging_time': 5.025132894515991, 'global_step': 109779, 'preemption_count': 0}), (110687, {'train/accuracy': 0.5723242163658142, 'train/loss': 1.8696619272232056, 'validation/accuracy': 0.5303800106048584, 'validation/loss': 2.0785651206970215, 'validation/num_examples': 50000, 'test/accuracy': 0.41940000653266907, 'test/loss': 2.7238848209381104, 'test/num_examples': 10000, 'score': 51293.603743076324, 'total_duration': 57680.238035440445, 'accumulated_submission_time': 51293.603743076324, 'accumulated_eval_time': 6375.277026414871, 'accumulated_logging_time': 5.072314500808716, 'global_step': 110687, 'preemption_count': 0}), (111597, {'train/accuracy': 0.6044335961341858, 'train/loss': 1.6973642110824585, 'validation/accuracy': 0.542639970779419, 'validation/loss': 2.007513999938965, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.6781985759735107, 'test/num_examples': 10000, 'score': 51713.767781972885, 'total_duration': 58152.63842535019, 'accumulated_submission_time': 51713.767781972885, 'accumulated_eval_time': 6427.413257360458, 'accumulated_logging_time': 5.120913028717041, 'global_step': 111597, 'preemption_count': 0}), (112503, {'train/accuracy': 0.5824804306030273, 'train/loss': 1.7816941738128662, 'validation/accuracy': 0.5447999835014343, 'validation/loss': 1.961826205253601, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.6584208011627197, 'test/num_examples': 10000, 'score': 52134.00865268707, 'total_duration': 58626.1765999794, 'accumulated_submission_time': 52134.00865268707, 'accumulated_eval_time': 6480.615281820297, 'accumulated_logging_time': 5.165015459060669, 'global_step': 112503, 'preemption_count': 0}), (113409, {'train/accuracy': 0.5881054401397705, 'train/loss': 1.7460676431655884, 'validation/accuracy': 0.5488399863243103, 'validation/loss': 1.9370585680007935, 'validation/num_examples': 50000, 'test/accuracy': 0.4369000196456909, 'test/loss': 2.59963321685791, 'test/num_examples': 10000, 'score': 52554.144918203354, 'total_duration': 59097.95101642609, 'accumulated_submission_time': 52554.144918203354, 'accumulated_eval_time': 6532.158484697342, 'accumulated_logging_time': 5.209194660186768, 'global_step': 113409, 'preemption_count': 0}), (114316, {'train/accuracy': 0.6094335913658142, 'train/loss': 1.6528574228286743, 'validation/accuracy': 0.5530999898910522, 'validation/loss': 1.9297101497650146, 'validation/num_examples': 50000, 'test/accuracy': 0.4376000165939331, 'test/loss': 2.588284969329834, 'test/num_examples': 10000, 'score': 52974.33516526222, 'total_duration': 59569.60813641548, 'accumulated_submission_time': 52974.33516526222, 'accumulated_eval_time': 6583.523062705994, 'accumulated_logging_time': 5.259409666061401, 'global_step': 114316, 'preemption_count': 0}), (115222, {'train/accuracy': 0.5908203125, 'train/loss': 1.7480441331863403, 'validation/accuracy': 0.5513399839401245, 'validation/loss': 1.947059988975525, 'validation/num_examples': 50000, 'test/accuracy': 0.4300000071525574, 'test/loss': 2.6224279403686523, 'test/num_examples': 10000, 'score': 53394.59422135353, 'total_duration': 60042.13766551018, 'accumulated_submission_time': 53394.59422135353, 'accumulated_eval_time': 6635.696802854538, 'accumulated_logging_time': 5.305138349533081, 'global_step': 115222, 'preemption_count': 0}), (116126, {'train/accuracy': 0.5944726467132568, 'train/loss': 1.7341322898864746, 'validation/accuracy': 0.5532999634742737, 'validation/loss': 1.9365402460098267, 'validation/num_examples': 50000, 'test/accuracy': 0.4336000084877014, 'test/loss': 2.6125783920288086, 'test/num_examples': 10000, 'score': 53814.87706851959, 'total_duration': 60514.86140489578, 'accumulated_submission_time': 53814.87706851959, 'accumulated_eval_time': 6688.041565418243, 'accumulated_logging_time': 5.350852966308594, 'global_step': 116126, 'preemption_count': 0}), (117031, {'train/accuracy': 0.607128918170929, 'train/loss': 1.6637282371520996, 'validation/accuracy': 0.5559200048446655, 'validation/loss': 1.9096101522445679, 'validation/num_examples': 50000, 'test/accuracy': 0.43700000643730164, 'test/loss': 2.60026216506958, 'test/num_examples': 10000, 'score': 54235.36640357971, 'total_duration': 60989.96281218529, 'accumulated_submission_time': 54235.36640357971, 'accumulated_eval_time': 6742.556987047195, 'accumulated_logging_time': 5.396175384521484, 'global_step': 117031, 'preemption_count': 0}), (117937, {'train/accuracy': 0.5986914038658142, 'train/loss': 1.7124024629592896, 'validation/accuracy': 0.5584200024604797, 'validation/loss': 1.9031184911727905, 'validation/num_examples': 50000, 'test/accuracy': 0.44120001792907715, 'test/loss': 2.5633251667022705, 'test/num_examples': 10000, 'score': 54655.59771943092, 'total_duration': 61462.07122516632, 'accumulated_submission_time': 54655.59771943092, 'accumulated_eval_time': 6794.329073667526, 'accumulated_logging_time': 5.449033498764038, 'global_step': 117937, 'preemption_count': 0}), (118844, {'train/accuracy': 0.6104296445846558, 'train/loss': 1.6610846519470215, 'validation/accuracy': 0.5698599815368652, 'validation/loss': 1.8717105388641357, 'validation/num_examples': 50000, 'test/accuracy': 0.45280003547668457, 'test/loss': 2.5325958728790283, 'test/num_examples': 10000, 'score': 55075.97532296181, 'total_duration': 61935.68432497978, 'accumulated_submission_time': 55075.97532296181, 'accumulated_eval_time': 6847.463381290436, 'accumulated_logging_time': 5.4985644817352295, 'global_step': 118844, 'preemption_count': 0}), (119751, {'train/accuracy': 0.6143554449081421, 'train/loss': 1.6402643918991089, 'validation/accuracy': 0.5634399652481079, 'validation/loss': 1.8867182731628418, 'validation/num_examples': 50000, 'test/accuracy': 0.4409000277519226, 'test/loss': 2.555757999420166, 'test/num_examples': 10000, 'score': 55496.04265832901, 'total_duration': 62407.289147138596, 'accumulated_submission_time': 55496.04265832901, 'accumulated_eval_time': 6898.902683258057, 'accumulated_logging_time': 5.5450968742370605, 'global_step': 119751, 'preemption_count': 0}), (120659, {'train/accuracy': 0.6095117330551147, 'train/loss': 1.7067832946777344, 'validation/accuracy': 0.5638799667358398, 'validation/loss': 1.9024053812026978, 'validation/num_examples': 50000, 'test/accuracy': 0.44690001010894775, 'test/loss': 2.5595107078552246, 'test/num_examples': 10000, 'score': 55916.214358091354, 'total_duration': 62878.28482103348, 'accumulated_submission_time': 55916.214358091354, 'accumulated_eval_time': 6949.624935626984, 'accumulated_logging_time': 5.5959556102752686, 'global_step': 120659, 'preemption_count': 0}), (121566, {'train/accuracy': 0.6133007407188416, 'train/loss': 1.6413342952728271, 'validation/accuracy': 0.5678399801254272, 'validation/loss': 1.860566258430481, 'validation/num_examples': 50000, 'test/accuracy': 0.4475000202655792, 'test/loss': 2.534411907196045, 'test/num_examples': 10000, 'score': 56336.366022109985, 'total_duration': 63349.9245557785, 'accumulated_submission_time': 56336.366022109985, 'accumulated_eval_time': 7001.017154455185, 'accumulated_logging_time': 5.640802383422852, 'global_step': 121566, 'preemption_count': 0}), (122472, {'train/accuracy': 0.62255859375, 'train/loss': 1.6071964502334595, 'validation/accuracy': 0.5728999972343445, 'validation/loss': 1.8406982421875, 'validation/num_examples': 50000, 'test/accuracy': 0.45500001311302185, 'test/loss': 2.503061532974243, 'test/num_examples': 10000, 'score': 56756.33172440529, 'total_duration': 63820.65265607834, 'accumulated_submission_time': 56756.33172440529, 'accumulated_eval_time': 7051.67671251297, 'accumulated_logging_time': 5.691583156585693, 'global_step': 122472, 'preemption_count': 0}), (123377, {'train/accuracy': 0.6289257407188416, 'train/loss': 1.570678949356079, 'validation/accuracy': 0.579479992389679, 'validation/loss': 1.805068850517273, 'validation/num_examples': 50000, 'test/accuracy': 0.4579000174999237, 'test/loss': 2.4760637283325195, 'test/num_examples': 10000, 'score': 57176.62945842743, 'total_duration': 64293.56670308113, 'accumulated_submission_time': 57176.62945842743, 'accumulated_eval_time': 7104.192118406296, 'accumulated_logging_time': 5.741584062576294, 'global_step': 123377, 'preemption_count': 0}), (124287, {'train/accuracy': 0.6289257407188416, 'train/loss': 1.5635874271392822, 'validation/accuracy': 0.5827199816703796, 'validation/loss': 1.768791675567627, 'validation/num_examples': 50000, 'test/accuracy': 0.4643000364303589, 'test/loss': 2.429023504257202, 'test/num_examples': 10000, 'score': 57596.96713638306, 'total_duration': 64766.72912335396, 'accumulated_submission_time': 57596.96713638306, 'accumulated_eval_time': 7156.915832519531, 'accumulated_logging_time': 5.790433645248413, 'global_step': 124287, 'preemption_count': 0}), (125193, {'train/accuracy': 0.6286327838897705, 'train/loss': 1.5781629085540771, 'validation/accuracy': 0.5806199908256531, 'validation/loss': 1.8158116340637207, 'validation/num_examples': 50000, 'test/accuracy': 0.46160003542900085, 'test/loss': 2.4752357006073, 'test/num_examples': 10000, 'score': 58017.08203911781, 'total_duration': 65238.811891555786, 'accumulated_submission_time': 58017.08203911781, 'accumulated_eval_time': 7208.776687383652, 'accumulated_logging_time': 5.846323251724243, 'global_step': 125193, 'preemption_count': 0}), (126099, {'train/accuracy': 0.6649023294448853, 'train/loss': 1.4432421922683716, 'validation/accuracy': 0.5847399830818176, 'validation/loss': 1.7886931896209717, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.447232484817505, 'test/num_examples': 10000, 'score': 58437.05323433876, 'total_duration': 65709.36529541016, 'accumulated_submission_time': 58437.05323433876, 'accumulated_eval_time': 7259.260968923569, 'accumulated_logging_time': 5.892705202102661, 'global_step': 126099, 'preemption_count': 0}), (127005, {'train/accuracy': 0.6364062428474426, 'train/loss': 1.5266468524932861, 'validation/accuracy': 0.5922799706459045, 'validation/loss': 1.7352849245071411, 'validation/num_examples': 50000, 'test/accuracy': 0.4724000096321106, 'test/loss': 2.40802001953125, 'test/num_examples': 10000, 'score': 58857.087052583694, 'total_duration': 66180.10751318932, 'accumulated_submission_time': 58857.087052583694, 'accumulated_eval_time': 7309.869294166565, 'accumulated_logging_time': 5.941359758377075, 'global_step': 127005, 'preemption_count': 0}), (127910, {'train/accuracy': 0.6415820121765137, 'train/loss': 1.496817708015442, 'validation/accuracy': 0.5906599760055542, 'validation/loss': 1.746111512184143, 'validation/num_examples': 50000, 'test/accuracy': 0.47380003333091736, 'test/loss': 2.3906781673431396, 'test/num_examples': 10000, 'score': 59277.23089051247, 'total_duration': 66653.61686849594, 'accumulated_submission_time': 59277.23089051247, 'accumulated_eval_time': 7363.129547357559, 'accumulated_logging_time': 5.994652986526489, 'global_step': 127910, 'preemption_count': 0}), (128813, {'train/accuracy': 0.6640819907188416, 'train/loss': 1.4066020250320435, 'validation/accuracy': 0.5945999622344971, 'validation/loss': 1.7294906377792358, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.3901915550231934, 'test/num_examples': 10000, 'score': 59697.32144474983, 'total_duration': 67126.2372546196, 'accumulated_submission_time': 59697.32144474983, 'accumulated_eval_time': 7415.558722019196, 'accumulated_logging_time': 6.044222116470337, 'global_step': 128813, 'preemption_count': 0}), (129719, {'train/accuracy': 0.6342382431030273, 'train/loss': 1.546700358390808, 'validation/accuracy': 0.5904600024223328, 'validation/loss': 1.7574831247329712, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.420825242996216, 'test/num_examples': 10000, 'score': 60117.41744160652, 'total_duration': 67597.53349399567, 'accumulated_submission_time': 60117.41744160652, 'accumulated_eval_time': 7466.6489408016205, 'accumulated_logging_time': 6.103210926055908, 'global_step': 129719, 'preemption_count': 0}), (130626, {'train/accuracy': 0.6511132717132568, 'train/loss': 1.4745047092437744, 'validation/accuracy': 0.5997399687767029, 'validation/loss': 1.7100039720535278, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.387399196624756, 'test/num_examples': 10000, 'score': 60537.581053733826, 'total_duration': 68068.39860129356, 'accumulated_submission_time': 60537.581053733826, 'accumulated_eval_time': 7517.243643760681, 'accumulated_logging_time': 6.158584356307983, 'global_step': 130626, 'preemption_count': 0}), (131535, {'train/accuracy': 0.6660742163658142, 'train/loss': 1.3897231817245483, 'validation/accuracy': 0.6065599918365479, 'validation/loss': 1.6731947660446167, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.348107099533081, 'test/num_examples': 10000, 'score': 60957.56353855133, 'total_duration': 68542.42744445801, 'accumulated_submission_time': 60957.56353855133, 'accumulated_eval_time': 7571.182153224945, 'accumulated_logging_time': 6.214536190032959, 'global_step': 131535, 'preemption_count': 0}), (132442, {'train/accuracy': 0.6496874690055847, 'train/loss': 1.4811943769454956, 'validation/accuracy': 0.6088399887084961, 'validation/loss': 1.6800014972686768, 'validation/num_examples': 50000, 'test/accuracy': 0.48130002617836, 'test/loss': 2.3436646461486816, 'test/num_examples': 10000, 'score': 61377.6965944767, 'total_duration': 69013.43845295906, 'accumulated_submission_time': 61377.6965944767, 'accumulated_eval_time': 7621.960869312286, 'accumulated_logging_time': 6.263074159622192, 'global_step': 132442, 'preemption_count': 0}), (133349, {'train/accuracy': 0.6616796851158142, 'train/loss': 1.4510056972503662, 'validation/accuracy': 0.6108399629592896, 'validation/loss': 1.6753292083740234, 'validation/num_examples': 50000, 'test/accuracy': 0.4870000183582306, 'test/loss': 2.3509786128997803, 'test/num_examples': 10000, 'score': 61797.793632507324, 'total_duration': 69487.23033475876, 'accumulated_submission_time': 61797.793632507324, 'accumulated_eval_time': 7675.555843830109, 'accumulated_logging_time': 6.311018705368042, 'global_step': 133349, 'preemption_count': 0}), (134257, {'train/accuracy': 0.6688085794448853, 'train/loss': 1.3909156322479248, 'validation/accuracy': 0.6072199940681458, 'validation/loss': 1.6612194776535034, 'validation/num_examples': 50000, 'test/accuracy': 0.48920002579689026, 'test/loss': 2.3225879669189453, 'test/num_examples': 10000, 'score': 62217.71622133255, 'total_duration': 69961.19663286209, 'accumulated_submission_time': 62217.71622133255, 'accumulated_eval_time': 7729.499813079834, 'accumulated_logging_time': 6.358997106552124, 'global_step': 134257, 'preemption_count': 0}), (135162, {'train/accuracy': 0.6540820002555847, 'train/loss': 1.4487427473068237, 'validation/accuracy': 0.6113799810409546, 'validation/loss': 1.649997591972351, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.3041481971740723, 'test/num_examples': 10000, 'score': 62637.634321689606, 'total_duration': 70435.33058691025, 'accumulated_submission_time': 62637.634321689606, 'accumulated_eval_time': 7783.615670204163, 'accumulated_logging_time': 6.407909154891968, 'global_step': 135162, 'preemption_count': 0}), (136068, {'train/accuracy': 0.6649999618530273, 'train/loss': 1.3921610116958618, 'validation/accuracy': 0.6185199618339539, 'validation/loss': 1.6188608407974243, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3076579570770264, 'test/num_examples': 10000, 'score': 63057.867579460144, 'total_duration': 70906.8068845272, 'accumulated_submission_time': 63057.867579460144, 'accumulated_eval_time': 7834.759024858475, 'accumulated_logging_time': 6.456115007400513, 'global_step': 136068, 'preemption_count': 0}), (136976, {'train/accuracy': 0.6775000095367432, 'train/loss': 1.342071533203125, 'validation/accuracy': 0.6184399724006653, 'validation/loss': 1.606795072555542, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.2707793712615967, 'test/num_examples': 10000, 'score': 63478.281381607056, 'total_duration': 71378.2106127739, 'accumulated_submission_time': 63478.281381607056, 'accumulated_eval_time': 7885.648756742477, 'accumulated_logging_time': 6.504778623580933, 'global_step': 136976, 'preemption_count': 0}), (137884, {'train/accuracy': 0.6682812571525574, 'train/loss': 1.403730034828186, 'validation/accuracy': 0.6224200129508972, 'validation/loss': 1.6151797771453857, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.288226366043091, 'test/num_examples': 10000, 'score': 63898.23468732834, 'total_duration': 71849.86646604538, 'accumulated_submission_time': 63898.23468732834, 'accumulated_eval_time': 7936.875230550766, 'accumulated_logging_time': 6.928737640380859, 'global_step': 137884, 'preemption_count': 0}), (138791, {'train/accuracy': 0.6753710508346558, 'train/loss': 1.351009488105774, 'validation/accuracy': 0.6233800053596497, 'validation/loss': 1.5772992372512817, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.231370687484741, 'test/num_examples': 10000, 'score': 64318.290568351746, 'total_duration': 72320.50111722946, 'accumulated_submission_time': 64318.290568351746, 'accumulated_eval_time': 7987.347238540649, 'accumulated_logging_time': 6.983047246932983, 'global_step': 138791, 'preemption_count': 0}), (139699, {'train/accuracy': 0.6874608993530273, 'train/loss': 1.287361741065979, 'validation/accuracy': 0.6303799748420715, 'validation/loss': 1.5501196384429932, 'validation/num_examples': 50000, 'test/accuracy': 0.5088000297546387, 'test/loss': 2.2103207111358643, 'test/num_examples': 10000, 'score': 64738.50710082054, 'total_duration': 72792.97579598427, 'accumulated_submission_time': 64738.50710082054, 'accumulated_eval_time': 8039.49192738533, 'accumulated_logging_time': 7.0454113483428955, 'global_step': 139699, 'preemption_count': 0}), (140606, {'train/accuracy': 0.684277355670929, 'train/loss': 1.3224250078201294, 'validation/accuracy': 0.6326999664306641, 'validation/loss': 1.5607450008392334, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.2430734634399414, 'test/num_examples': 10000, 'score': 65158.44564986229, 'total_duration': 73265.60011100769, 'accumulated_submission_time': 65158.44564986229, 'accumulated_eval_time': 8092.068526983261, 'accumulated_logging_time': 7.1019439697265625, 'global_step': 140606, 'preemption_count': 0}), (141514, {'train/accuracy': 0.6889257431030273, 'train/loss': 1.2937071323394775, 'validation/accuracy': 0.6418200135231018, 'validation/loss': 1.5113493204116821, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.173495054244995, 'test/num_examples': 10000, 'score': 65578.51907277107, 'total_duration': 73737.41035723686, 'accumulated_submission_time': 65578.51907277107, 'accumulated_eval_time': 8143.69774889946, 'accumulated_logging_time': 7.15775990486145, 'global_step': 141514, 'preemption_count': 0}), (142421, {'train/accuracy': 0.6948828101158142, 'train/loss': 1.2586766481399536, 'validation/accuracy': 0.6380000114440918, 'validation/loss': 1.5189902782440186, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.185518980026245, 'test/num_examples': 10000, 'score': 65998.46021318436, 'total_duration': 74208.82389330864, 'accumulated_submission_time': 65998.46021318436, 'accumulated_eval_time': 8195.071061849594, 'accumulated_logging_time': 7.205103397369385, 'global_step': 142421, 'preemption_count': 0}), (143326, {'train/accuracy': 0.7229296565055847, 'train/loss': 1.147066593170166, 'validation/accuracy': 0.6485999822616577, 'validation/loss': 1.4839391708374023, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.157728672027588, 'test/num_examples': 10000, 'score': 66418.74589681625, 'total_duration': 74681.48336172104, 'accumulated_submission_time': 66418.74589681625, 'accumulated_eval_time': 8247.3456325531, 'accumulated_logging_time': 7.2528393268585205, 'global_step': 143326, 'preemption_count': 0}), (144235, {'train/accuracy': 0.6969531178474426, 'train/loss': 1.259919285774231, 'validation/accuracy': 0.6465399861335754, 'validation/loss': 1.4953464269638062, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.162790298461914, 'test/num_examples': 10000, 'score': 66839.10083460808, 'total_duration': 75155.56358337402, 'accumulated_submission_time': 66839.10083460808, 'accumulated_eval_time': 8300.968694925308, 'accumulated_logging_time': 7.303514003753662, 'global_step': 144235, 'preemption_count': 0}), (145143, {'train/accuracy': 0.6981835961341858, 'train/loss': 1.2710505723953247, 'validation/accuracy': 0.6437000036239624, 'validation/loss': 1.523866057395935, 'validation/num_examples': 50000, 'test/accuracy': 0.5236000418663025, 'test/loss': 2.1719541549682617, 'test/num_examples': 10000, 'score': 67259.42416167259, 'total_duration': 75627.54675364494, 'accumulated_submission_time': 67259.42416167259, 'accumulated_eval_time': 8352.526899814606, 'accumulated_logging_time': 7.353893756866455, 'global_step': 145143, 'preemption_count': 0}), (146049, {'train/accuracy': 0.7145116925239563, 'train/loss': 1.1924309730529785, 'validation/accuracy': 0.6484400033950806, 'validation/loss': 1.4934139251708984, 'validation/num_examples': 50000, 'test/accuracy': 0.523300051689148, 'test/loss': 2.157883644104004, 'test/num_examples': 10000, 'score': 67679.52076888084, 'total_duration': 76099.40624260902, 'accumulated_submission_time': 67679.52076888084, 'accumulated_eval_time': 8404.179302930832, 'accumulated_logging_time': 7.413357973098755, 'global_step': 146049, 'preemption_count': 0}), (146958, {'train/accuracy': 0.7075585722923279, 'train/loss': 1.1999785900115967, 'validation/accuracy': 0.655239999294281, 'validation/loss': 1.4392666816711426, 'validation/num_examples': 50000, 'test/accuracy': 0.530500054359436, 'test/loss': 2.0883536338806152, 'test/num_examples': 10000, 'score': 68099.78795552254, 'total_duration': 76573.15300369263, 'accumulated_submission_time': 68099.78795552254, 'accumulated_eval_time': 8457.554328680038, 'accumulated_logging_time': 7.466028690338135, 'global_step': 146958, 'preemption_count': 0}), (147867, {'train/accuracy': 0.7089257836341858, 'train/loss': 1.1918531656265259, 'validation/accuracy': 0.6549599766731262, 'validation/loss': 1.4504930973052979, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.122317314147949, 'test/num_examples': 10000, 'score': 68519.88999271393, 'total_duration': 77044.57645773888, 'accumulated_submission_time': 68519.88999271393, 'accumulated_eval_time': 8508.766267299652, 'accumulated_logging_time': 7.52376914024353, 'global_step': 147867, 'preemption_count': 0}), (148775, {'train/accuracy': 0.72328120470047, 'train/loss': 1.1626847982406616, 'validation/accuracy': 0.656279981136322, 'validation/loss': 1.4608744382858276, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.1140568256378174, 'test/num_examples': 10000, 'score': 68940.13805937767, 'total_duration': 77517.2398583889, 'accumulated_submission_time': 68940.13805937767, 'accumulated_eval_time': 8561.076631069183, 'accumulated_logging_time': 7.576759338378906, 'global_step': 148775, 'preemption_count': 0}), (149682, {'train/accuracy': 0.71742182970047, 'train/loss': 1.1706210374832153, 'validation/accuracy': 0.6571800112724304, 'validation/loss': 1.4194693565368652, 'validation/num_examples': 50000, 'test/accuracy': 0.5441000461578369, 'test/loss': 2.061110496520996, 'test/num_examples': 10000, 'score': 69360.1499080658, 'total_duration': 77988.87147164345, 'accumulated_submission_time': 69360.1499080658, 'accumulated_eval_time': 8612.59052824974, 'accumulated_logging_time': 7.63094162940979, 'global_step': 149682, 'preemption_count': 0}), (150589, {'train/accuracy': 0.7193945050239563, 'train/loss': 1.145675778388977, 'validation/accuracy': 0.6657599806785583, 'validation/loss': 1.3892215490341187, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.0501999855041504, 'test/num_examples': 10000, 'score': 69780.04849529266, 'total_duration': 78460.87410640717, 'accumulated_submission_time': 69780.04849529266, 'accumulated_eval_time': 8664.59189748764, 'accumulated_logging_time': 7.682143688201904, 'global_step': 150589, 'preemption_count': 0}), (151496, {'train/accuracy': 0.7369726300239563, 'train/loss': 1.0826455354690552, 'validation/accuracy': 0.6676999926567078, 'validation/loss': 1.3798246383666992, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.038510799407959, 'test/num_examples': 10000, 'score': 70200.33002853394, 'total_duration': 78933.96350884438, 'accumulated_submission_time': 70200.33002853394, 'accumulated_eval_time': 8717.290427207947, 'accumulated_logging_time': 7.740187168121338, 'global_step': 151496, 'preemption_count': 0}), (152404, {'train/accuracy': 0.7234960794448853, 'train/loss': 1.141112208366394, 'validation/accuracy': 0.6663999557495117, 'validation/loss': 1.381853461265564, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.045349597930908, 'test/num_examples': 10000, 'score': 70620.6611096859, 'total_duration': 79407.156021595, 'accumulated_submission_time': 70620.6611096859, 'accumulated_eval_time': 8770.050068855286, 'accumulated_logging_time': 7.791531801223755, 'global_step': 152404, 'preemption_count': 0}), (153312, {'train/accuracy': 0.7329882383346558, 'train/loss': 1.0937864780426025, 'validation/accuracy': 0.6733799576759338, 'validation/loss': 1.3574568033218384, 'validation/num_examples': 50000, 'test/accuracy': 0.5484000444412231, 'test/loss': 2.018724203109741, 'test/num_examples': 10000, 'score': 71040.6793308258, 'total_duration': 79880.49059653282, 'accumulated_submission_time': 71040.6793308258, 'accumulated_eval_time': 8823.263661623001, 'accumulated_logging_time': 7.842597007751465, 'global_step': 153312, 'preemption_count': 0}), (154220, {'train/accuracy': 0.7409570217132568, 'train/loss': 1.0447590351104736, 'validation/accuracy': 0.6767799854278564, 'validation/loss': 1.337766408920288, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 1.9991964101791382, 'test/num_examples': 10000, 'score': 71460.68648219109, 'total_duration': 80351.2955019474, 'accumulated_submission_time': 71460.68648219109, 'accumulated_eval_time': 8873.951783180237, 'accumulated_logging_time': 7.901000738143921, 'global_step': 154220, 'preemption_count': 0}), (155127, {'train/accuracy': 0.7374804615974426, 'train/loss': 1.0925852060317993, 'validation/accuracy': 0.6778199672698975, 'validation/loss': 1.3446279764175415, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.018172025680542, 'test/num_examples': 10000, 'score': 71880.62465643883, 'total_duration': 80825.57174110413, 'accumulated_submission_time': 71880.62465643883, 'accumulated_eval_time': 8928.184937000275, 'accumulated_logging_time': 7.954378366470337, 'global_step': 155127, 'preemption_count': 0}), (156033, {'train/accuracy': 0.7378515601158142, 'train/loss': 1.0726988315582275, 'validation/accuracy': 0.6809799671173096, 'validation/loss': 1.3264795541763306, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.985244870185852, 'test/num_examples': 10000, 'score': 72300.71397519112, 'total_duration': 81296.80072975159, 'accumulated_submission_time': 72300.71397519112, 'accumulated_eval_time': 8979.215026378632, 'accumulated_logging_time': 8.012673616409302, 'global_step': 156033, 'preemption_count': 0}), (156942, {'train/accuracy': 0.7446679472923279, 'train/loss': 1.0540313720703125, 'validation/accuracy': 0.6827600002288818, 'validation/loss': 1.3250590562820435, 'validation/num_examples': 50000, 'test/accuracy': 0.5562000274658203, 'test/loss': 1.9805034399032593, 'test/num_examples': 10000, 'score': 72720.9410059452, 'total_duration': 81767.88509273529, 'accumulated_submission_time': 72720.9410059452, 'accumulated_eval_time': 9029.969490528107, 'accumulated_logging_time': 8.063661575317383, 'global_step': 156942, 'preemption_count': 0}), (157851, {'train/accuracy': 0.7596679329872131, 'train/loss': 0.9815232753753662, 'validation/accuracy': 0.6845600008964539, 'validation/loss': 1.304532527923584, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9559437036514282, 'test/num_examples': 10000, 'score': 73141.18209695816, 'total_duration': 82242.24174976349, 'accumulated_submission_time': 73141.18209695816, 'accumulated_eval_time': 9083.979422330856, 'accumulated_logging_time': 8.117793560028076, 'global_step': 157851, 'preemption_count': 0}), (158758, {'train/accuracy': 0.7476562261581421, 'train/loss': 1.031006932258606, 'validation/accuracy': 0.6883599758148193, 'validation/loss': 1.2879359722137451, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 1.9300532341003418, 'test/num_examples': 10000, 'score': 73561.48490953445, 'total_duration': 82713.59108018875, 'accumulated_submission_time': 73561.48490953445, 'accumulated_eval_time': 9134.91816353798, 'accumulated_logging_time': 8.174100875854492, 'global_step': 158758, 'preemption_count': 0}), (159669, {'train/accuracy': 0.7578319907188416, 'train/loss': 0.9961916208267212, 'validation/accuracy': 0.69159996509552, 'validation/loss': 1.2772376537322998, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9215903282165527, 'test/num_examples': 10000, 'score': 73981.68310594559, 'total_duration': 83184.31155920029, 'accumulated_submission_time': 73981.68310594559, 'accumulated_eval_time': 9185.332689285278, 'accumulated_logging_time': 8.230481386184692, 'global_step': 159669, 'preemption_count': 0}), (160576, {'train/accuracy': 0.7697070240974426, 'train/loss': 0.9384439587593079, 'validation/accuracy': 0.6931599974632263, 'validation/loss': 1.2678637504577637, 'validation/num_examples': 50000, 'test/accuracy': 0.5687000155448914, 'test/loss': 1.9138787984848022, 'test/num_examples': 10000, 'score': 74401.90697979927, 'total_duration': 83659.51302075386, 'accumulated_submission_time': 74401.90697979927, 'accumulated_eval_time': 9240.204246282578, 'accumulated_logging_time': 8.28437876701355, 'global_step': 160576, 'preemption_count': 0}), (161485, {'train/accuracy': 0.7563671469688416, 'train/loss': 0.976298451423645, 'validation/accuracy': 0.6957199573516846, 'validation/loss': 1.2451553344726562, 'validation/num_examples': 50000, 'test/accuracy': 0.5685999989509583, 'test/loss': 1.8960278034210205, 'test/num_examples': 10000, 'score': 74822.10788369179, 'total_duration': 84130.91235041618, 'accumulated_submission_time': 74822.10788369179, 'accumulated_eval_time': 9291.293983697891, 'accumulated_logging_time': 8.341437816619873, 'global_step': 161485, 'preemption_count': 0}), (162394, {'train/accuracy': 0.76527339220047, 'train/loss': 0.9461551308631897, 'validation/accuracy': 0.7001199722290039, 'validation/loss': 1.229008674621582, 'validation/num_examples': 50000, 'test/accuracy': 0.5740000009536743, 'test/loss': 1.8750234842300415, 'test/num_examples': 10000, 'score': 75242.05846524239, 'total_duration': 84602.79054522514, 'accumulated_submission_time': 75242.05846524239, 'accumulated_eval_time': 9343.114990472794, 'accumulated_logging_time': 8.395569086074829, 'global_step': 162394, 'preemption_count': 0}), (163300, {'train/accuracy': 0.7712695002555847, 'train/loss': 0.9198420643806458, 'validation/accuracy': 0.701259970664978, 'validation/loss': 1.2253583669662476, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 1.875489592552185, 'test/num_examples': 10000, 'score': 75661.95459794998, 'total_duration': 85074.48866915703, 'accumulated_submission_time': 75661.95459794998, 'accumulated_eval_time': 9394.80528140068, 'accumulated_logging_time': 8.456037521362305, 'global_step': 163300, 'preemption_count': 0}), (164208, {'train/accuracy': 0.7705273032188416, 'train/loss': 0.9230221509933472, 'validation/accuracy': 0.7032399773597717, 'validation/loss': 1.2151079177856445, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.8578152656555176, 'test/num_examples': 10000, 'score': 76082.06962704659, 'total_duration': 85547.03738641739, 'accumulated_submission_time': 76082.06962704659, 'accumulated_eval_time': 9447.132781267166, 'accumulated_logging_time': 8.510085105895996, 'global_step': 164208, 'preemption_count': 0}), (165117, {'train/accuracy': 0.7754296660423279, 'train/loss': 0.913085401058197, 'validation/accuracy': 0.7068799734115601, 'validation/loss': 1.2091929912567139, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 1.8517848253250122, 'test/num_examples': 10000, 'score': 76502.12402510643, 'total_duration': 86020.86357402802, 'accumulated_submission_time': 76502.12402510643, 'accumulated_eval_time': 9500.79858636856, 'accumulated_logging_time': 8.563549280166626, 'global_step': 165117, 'preemption_count': 0}), (166022, {'train/accuracy': 0.7830663919448853, 'train/loss': 0.8687943816184998, 'validation/accuracy': 0.7093600034713745, 'validation/loss': 1.190772533416748, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.840895652770996, 'test/num_examples': 10000, 'score': 76922.23614120483, 'total_duration': 86491.56161642075, 'accumulated_submission_time': 76922.23614120483, 'accumulated_eval_time': 9551.27962064743, 'accumulated_logging_time': 8.616734027862549, 'global_step': 166022, 'preemption_count': 0}), (166928, {'train/accuracy': 0.7832421660423279, 'train/loss': 0.871487557888031, 'validation/accuracy': 0.7151399850845337, 'validation/loss': 1.162901520729065, 'validation/num_examples': 50000, 'test/accuracy': 0.5940000414848328, 'test/loss': 1.7914539575576782, 'test/num_examples': 10000, 'score': 77342.32014989853, 'total_duration': 86963.95547676086, 'accumulated_submission_time': 77342.32014989853, 'accumulated_eval_time': 9603.473745822906, 'accumulated_logging_time': 8.681183338165283, 'global_step': 166928, 'preemption_count': 0})], 'global_step': 167317}
I0207 14:46:31.458454 140107197974336 submission_runner.py:586] Timing: 77520.20170688629
I0207 14:46:31.458531 140107197974336 submission_runner.py:588] Total number of evals: 185
I0207 14:46:31.458575 140107197974336 submission_runner.py:589] ====================
I0207 14:46:31.458620 140107197974336 submission_runner.py:542] Using RNG seed 1274177056
I0207 14:46:31.459996 140107197974336 submission_runner.py:551] --- Tuning run 5/5 ---
I0207 14:46:31.460094 140107197974336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_5.
I0207 14:46:31.461519 140107197974336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_5/hparams.json.
I0207 14:46:31.462368 140107197974336 submission_runner.py:206] Initializing dataset.
I0207 14:46:31.472042 140107197974336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0207 14:46:31.483822 140107197974336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0207 14:46:31.676054 140107197974336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0207 14:46:35.881404 140107197974336 submission_runner.py:213] Initializing model.
I0207 14:46:42.360706 140107197974336 submission_runner.py:255] Initializing optimizer.
I0207 14:46:42.839363 140107197974336 submission_runner.py:262] Initializing metrics bundle.
I0207 14:46:42.839536 140107197974336 submission_runner.py:280] Initializing checkpoint and logger.
I0207 14:46:42.857159 140107197974336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_5 with prefix checkpoint_
I0207 14:46:42.857280 140107197974336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0207 14:47:00.056538 140107197974336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0207 14:47:17.050000 140107197974336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_5/flags_0.json.
I0207 14:47:17.054598 140107197974336 submission_runner.py:314] Starting training loop.
I0207 14:47:58.966584 139946372675328 logging_writer.py:48] [0] global_step=0, grad_norm=0.3573084771633148, loss=6.907756328582764
I0207 14:47:58.985612 140107197974336 spec.py:321] Evaluating on the training split.
I0207 14:48:07.484944 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 14:48:28.885365 140107197974336 spec.py:349] Evaluating on the test split.
I0207 14:48:30.470962 140107197974336 submission_runner.py:408] Time since start: 73.42s, 	Step: 1, 	{'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 41.93090534210205, 'total_duration': 73.41631269454956, 'accumulated_submission_time': 41.93090534210205, 'accumulated_eval_time': 31.48529624938965, 'accumulated_logging_time': 0}
I0207 14:48:30.479228 139946381068032 logging_writer.py:48] [1] accumulated_eval_time=31.485296, accumulated_logging_time=0, accumulated_submission_time=41.930905, global_step=1, preemption_count=0, score=41.930905, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=73.416313, train/accuracy=0.000918, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0207 14:49:42.217922 139946414638848 logging_writer.py:48] [100] global_step=100, grad_norm=0.5226721167564392, loss=6.8770036697387695
I0207 14:50:28.629452 139946397853440 logging_writer.py:48] [200] global_step=200, grad_norm=0.5602872371673584, loss=6.794257640838623
I0207 14:51:15.934501 139946414638848 logging_writer.py:48] [300] global_step=300, grad_norm=0.6729114055633545, loss=6.756929397583008
I0207 14:52:03.166493 139946397853440 logging_writer.py:48] [400] global_step=400, grad_norm=0.792780339717865, loss=6.702724933624268
I0207 14:52:50.424392 139946414638848 logging_writer.py:48] [500] global_step=500, grad_norm=1.2728464603424072, loss=6.542254447937012
I0207 14:53:37.644357 139946397853440 logging_writer.py:48] [600] global_step=600, grad_norm=1.1036549806594849, loss=6.445413589477539
I0207 14:54:25.299066 139946414638848 logging_writer.py:48] [700] global_step=700, grad_norm=1.61418616771698, loss=6.398956775665283
I0207 14:55:12.763383 139946397853440 logging_writer.py:48] [800] global_step=800, grad_norm=0.9998824596405029, loss=6.277336120605469
I0207 14:55:30.559035 140107197974336 spec.py:321] Evaluating on the training split.
I0207 14:55:42.268158 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 14:56:20.644664 140107197974336 spec.py:349] Evaluating on the test split.
I0207 14:56:22.243793 140107197974336 submission_runner.py:408] Time since start: 545.19s, 	Step: 839, 	{'train/accuracy': 0.03216796740889549, 'train/loss': 5.976029396057129, 'validation/accuracy': 0.02685999870300293, 'validation/loss': 6.033750534057617, 'validation/num_examples': 50000, 'test/accuracy': 0.02200000174343586, 'test/loss': 6.133785247802734, 'test/num_examples': 10000, 'score': 461.9541461467743, 'total_duration': 545.1891384124756, 'accumulated_submission_time': 461.9541461467743, 'accumulated_eval_time': 83.17005729675293, 'accumulated_logging_time': 0.01717853546142578}
I0207 14:56:22.260434 139946414638848 logging_writer.py:48] [839] accumulated_eval_time=83.170057, accumulated_logging_time=0.017179, accumulated_submission_time=461.954146, global_step=839, preemption_count=0, score=461.954146, test/accuracy=0.022000, test/loss=6.133785, test/num_examples=10000, total_duration=545.189138, train/accuracy=0.032168, train/loss=5.976029, validation/accuracy=0.026860, validation/loss=6.033751, validation/num_examples=50000
I0207 14:56:47.708214 139946397853440 logging_writer.py:48] [900] global_step=900, grad_norm=1.7395799160003662, loss=6.365206241607666
I0207 14:57:33.661754 139946414638848 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9538729786872864, loss=6.180245399475098
I0207 14:58:20.790859 139946397853440 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.198986530303955, loss=6.036252975463867
I0207 14:59:07.818573 139946414638848 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.0497040748596191, loss=6.01330041885376
I0207 14:59:55.099567 139946397853440 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0850151777267456, loss=5.926458835601807
I0207 15:00:42.168387 139946414638848 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.6792569160461426, loss=6.008632659912109
I0207 15:01:29.431109 139946397853440 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.386939525604248, loss=5.971087455749512
I0207 15:02:16.521818 139946414638848 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0625324249267578, loss=5.743690013885498
I0207 15:03:03.791723 139946397853440 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.3455002307891846, loss=5.817805290222168
I0207 15:03:22.323132 140107197974336 spec.py:321] Evaluating on the training split.
I0207 15:03:33.597177 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 15:04:09.941285 140107197974336 spec.py:349] Evaluating on the test split.
I0207 15:04:11.546936 140107197974336 submission_runner.py:408] Time since start: 1014.49s, 	Step: 1741, 	{'train/accuracy': 0.06861328333616257, 'train/loss': 5.289651393890381, 'validation/accuracy': 0.06421999633312225, 'validation/loss': 5.34092903137207, 'validation/num_examples': 50000, 'test/accuracy': 0.04990000277757645, 'test/loss': 5.561094284057617, 'test/num_examples': 10000, 'score': 881.9554603099823, 'total_duration': 1014.4922845363617, 'accumulated_submission_time': 881.9554603099823, 'accumulated_eval_time': 132.39385414123535, 'accumulated_logging_time': 0.043645620346069336}
I0207 15:04:11.563220 139946414638848 logging_writer.py:48] [1741] accumulated_eval_time=132.393854, accumulated_logging_time=0.043646, accumulated_submission_time=881.955460, global_step=1741, preemption_count=0, score=881.955460, test/accuracy=0.049900, test/loss=5.561094, test/num_examples=10000, total_duration=1014.492285, train/accuracy=0.068613, train/loss=5.289651, validation/accuracy=0.064220, validation/loss=5.340929, validation/num_examples=50000
I0207 15:04:36.215025 139946397853440 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1713550090789795, loss=6.045424461364746
I0207 15:05:21.856448 139946414638848 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1000276803970337, loss=6.418310165405273
I0207 15:06:09.108167 139946397853440 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.070594310760498, loss=5.716702938079834
I0207 15:06:55.618657 139946414638848 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8294252753257751, loss=6.271401405334473
I0207 15:07:42.342028 139946397853440 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.1482056379318237, loss=5.6373443603515625
I0207 15:08:29.299537 139946414638848 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9743678569793701, loss=5.459162712097168
I0207 15:09:15.949940 139946397853440 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.6569817066192627, loss=6.392117500305176
I0207 15:10:03.120307 139946414638848 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7263551354408264, loss=6.521825313568115
I0207 15:10:49.646365 139946397853440 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9380325078964233, loss=5.390591621398926
I0207 15:11:11.839056 140107197974336 spec.py:321] Evaluating on the training split.
I0207 15:11:22.996211 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 15:12:01.307084 140107197974336 spec.py:349] Evaluating on the test split.
I0207 15:12:02.921143 140107197974336 submission_runner.py:408] Time since start: 1485.87s, 	Step: 2649, 	{'train/accuracy': 0.11441405862569809, 'train/loss': 4.848740100860596, 'validation/accuracy': 0.10763999819755554, 'validation/loss': 4.910977840423584, 'validation/num_examples': 50000, 'test/accuracy': 0.08380000293254852, 'test/loss': 5.196085453033447, 'test/num_examples': 10000, 'score': 1302.169753074646, 'total_duration': 1485.8664710521698, 'accumulated_submission_time': 1302.169753074646, 'accumulated_eval_time': 183.47590970993042, 'accumulated_logging_time': 0.06981539726257324}
I0207 15:12:02.938970 139946414638848 logging_writer.py:48] [2649] accumulated_eval_time=183.475910, accumulated_logging_time=0.069815, accumulated_submission_time=1302.169753, global_step=2649, preemption_count=0, score=1302.169753, test/accuracy=0.083800, test/loss=5.196085, test/num_examples=10000, total_duration=1485.866471, train/accuracy=0.114414, train/loss=4.848740, validation/accuracy=0.107640, validation/loss=4.910978, validation/num_examples=50000
I0207 15:12:24.294700 139946397853440 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.2610236406326294, loss=5.337978363037109
I0207 15:13:09.943000 139946414638848 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.808780312538147, loss=6.411234378814697
I0207 15:13:56.987393 139946397853440 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9343664646148682, loss=5.515881538391113
I0207 15:14:43.928387 139946414638848 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9011644721031189, loss=5.238052845001221
I0207 15:15:30.742936 139946397853440 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8553482890129089, loss=5.256522178649902
I0207 15:16:17.606269 139946414638848 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0061047077178955, loss=6.395535945892334
I0207 15:17:04.660165 139946397853440 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9631860852241516, loss=5.133425712585449
I0207 15:17:51.308503 139946414638848 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8446731567382812, loss=5.218803882598877
I0207 15:18:38.251907 139946397853440 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9889942407608032, loss=5.324914932250977
I0207 15:19:02.921338 140107197974336 spec.py:321] Evaluating on the training split.
I0207 15:19:14.129903 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 15:19:51.779076 140107197974336 spec.py:349] Evaluating on the test split.
I0207 15:19:53.379329 140107197974336 submission_runner.py:408] Time since start: 1956.32s, 	Step: 3554, 	{'train/accuracy': 0.17302733659744263, 'train/loss': 4.282963752746582, 'validation/accuracy': 0.15741999447345734, 'validation/loss': 4.420511245727539, 'validation/num_examples': 50000, 'test/accuracy': 0.12000000476837158, 'test/loss': 4.79691743850708, 'test/num_examples': 10000, 'score': 1722.0868241786957, 'total_duration': 1956.3246581554413, 'accumulated_submission_time': 1722.0868241786957, 'accumulated_eval_time': 233.9338686466217, 'accumulated_logging_time': 0.10030865669250488}
I0207 15:19:53.398059 139946414638848 logging_writer.py:48] [3554] accumulated_eval_time=233.933869, accumulated_logging_time=0.100309, accumulated_submission_time=1722.086824, global_step=3554, preemption_count=0, score=1722.086824, test/accuracy=0.120000, test/loss=4.796917, test/num_examples=10000, total_duration=1956.324658, train/accuracy=0.173027, train/loss=4.282964, validation/accuracy=0.157420, validation/loss=4.420511, validation/num_examples=50000
I0207 15:20:13.169333 139946397853440 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8092314004898071, loss=6.067872524261475
I0207 15:20:58.133630 139946414638848 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9516377449035645, loss=5.104171276092529
I0207 15:21:46.104625 139946397853440 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.941163182258606, loss=5.093264102935791
I0207 15:22:33.854467 139946414638848 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.1579188108444214, loss=5.039675712585449
I0207 15:23:21.469410 139946397853440 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8179764151573181, loss=4.816235065460205
I0207 15:24:08.951636 139946414638848 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8634806275367737, loss=5.482865810394287
I0207 15:24:56.492140 139946397853440 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8622679114341736, loss=4.8595356941223145
I0207 15:25:43.896444 139946414638848 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8115084767341614, loss=4.642843723297119
I0207 15:26:31.487583 139946397853440 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7373682856559753, loss=5.826535224914551
I0207 15:26:53.529067 140107197974336 spec.py:321] Evaluating on the training split.
I0207 15:27:04.961045 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 15:27:43.414999 140107197974336 spec.py:349] Evaluating on the test split.
I0207 15:27:45.011297 140107197974336 submission_runner.py:408] Time since start: 2427.96s, 	Step: 4448, 	{'train/accuracy': 0.20783202350139618, 'train/loss': 4.076893329620361, 'validation/accuracy': 0.19606000185012817, 'validation/loss': 4.17407751083374, 'validation/num_examples': 50000, 'test/accuracy': 0.14380000531673431, 'test/loss': 4.610869884490967, 'test/num_examples': 10000, 'score': 2141.7024862766266, 'total_duration': 2427.956640481949, 'accumulated_submission_time': 2141.7024862766266, 'accumulated_eval_time': 285.4160861968994, 'accumulated_logging_time': 0.5835461616516113}
I0207 15:27:45.027714 139946414638848 logging_writer.py:48] [4448] accumulated_eval_time=285.416086, accumulated_logging_time=0.583546, accumulated_submission_time=2141.702486, global_step=4448, preemption_count=0, score=2141.702486, test/accuracy=0.143800, test/loss=4.610870, test/num_examples=10000, total_duration=2427.956640, train/accuracy=0.207832, train/loss=4.076893, validation/accuracy=0.196060, validation/loss=4.174078, validation/num_examples=50000
I0207 15:28:06.792674 139946397853440 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.256446361541748, loss=4.757532596588135
I0207 15:28:52.297946 139946414638848 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8982049226760864, loss=4.664127349853516
I0207 15:29:39.281866 139946397853440 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9982736706733704, loss=4.582604885101318
I0207 15:30:26.302309 139946414638848 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8818260431289673, loss=4.557968616485596
I0207 15:31:13.387028 139946397853440 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.357387900352478, loss=4.6766581535339355
I0207 15:32:00.507526 139946414638848 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6420488357543945, loss=6.007911205291748
I0207 15:32:47.743316 139946397853440 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7338641881942749, loss=5.030179500579834
I0207 15:33:35.038771 139946414638848 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.741062581539154, loss=4.648846626281738
I0207 15:34:22.003826 139946397853440 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9773558378219604, loss=4.455676078796387
I0207 15:34:45.036784 140107197974336 spec.py:321] Evaluating on the training split.
I0207 15:34:56.224857 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 15:35:35.226897 140107197974336 spec.py:349] Evaluating on the test split.
I0207 15:35:36.824265 140107197974336 submission_runner.py:408] Time since start: 2899.77s, 	Step: 5350, 	{'train/accuracy': 0.2707226574420929, 'train/loss': 3.5997939109802246, 'validation/accuracy': 0.25001999735832214, 'validation/loss': 3.711319923400879, 'validation/num_examples': 50000, 'test/accuracy': 0.1876000016927719, 'test/loss': 4.200911998748779, 'test/num_examples': 10000, 'score': 2561.6503467559814, 'total_duration': 2899.76961684227, 'accumulated_submission_time': 2561.6503467559814, 'accumulated_eval_time': 337.2035584449768, 'accumulated_logging_time': 0.6109819412231445}
I0207 15:35:36.840787 139946414638848 logging_writer.py:48] [5350] accumulated_eval_time=337.203558, accumulated_logging_time=0.610982, accumulated_submission_time=2561.650347, global_step=5350, preemption_count=0, score=2561.650347, test/accuracy=0.187600, test/loss=4.200912, test/num_examples=10000, total_duration=2899.769617, train/accuracy=0.270723, train/loss=3.599794, validation/accuracy=0.250020, validation/loss=3.711320, validation/num_examples=50000
I0207 15:35:57.790381 139946397853440 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6966047286987305, loss=4.473235130310059
I0207 15:36:43.295847 139946414638848 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7411219477653503, loss=4.533254623413086
I0207 15:37:30.095899 139946397853440 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7594701051712036, loss=5.530184268951416
I0207 15:38:16.885590 139946414638848 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.748586118221283, loss=4.570218563079834
I0207 15:39:03.871763 139946397853440 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.1629382371902466, loss=4.400187969207764
I0207 15:39:50.645231 139946414638848 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8563095927238464, loss=4.302947044372559
I0207 15:40:37.591075 139946397853440 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7772329449653625, loss=4.870467185974121
I0207 15:41:24.319103 139946414638848 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.541083812713623, loss=5.671882152557373
I0207 15:42:10.929699 139946397853440 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7776472568511963, loss=4.5742645263671875
I0207 15:42:37.022083 140107197974336 spec.py:321] Evaluating on the training split.
I0207 15:42:48.189614 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 15:43:26.835610 140107197974336 spec.py:349] Evaluating on the test split.
I0207 15:43:28.425419 140107197974336 submission_runner.py:408] Time since start: 3371.37s, 	Step: 6257, 	{'train/accuracy': 0.31746092438697815, 'train/loss': 3.2546160221099854, 'validation/accuracy': 0.2885800004005432, 'validation/loss': 3.4442403316497803, 'validation/num_examples': 50000, 'test/accuracy': 0.2152000069618225, 'test/loss': 3.982329845428467, 'test/num_examples': 10000, 'score': 2981.769757270813, 'total_duration': 3371.370764017105, 'accumulated_submission_time': 2981.769757270813, 'accumulated_eval_time': 388.6069040298462, 'accumulated_logging_time': 0.6374530792236328}
I0207 15:43:28.441483 139946414638848 logging_writer.py:48] [6257] accumulated_eval_time=388.606904, accumulated_logging_time=0.637453, accumulated_submission_time=2981.769757, global_step=6257, preemption_count=0, score=2981.769757, test/accuracy=0.215200, test/loss=3.982330, test/num_examples=10000, total_duration=3371.370764, train/accuracy=0.317461, train/loss=3.254616, validation/accuracy=0.288580, validation/loss=3.444240, validation/num_examples=50000
I0207 15:43:46.519993 139946397853440 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8051000833511353, loss=4.210726737976074
I0207 15:44:32.030380 139946414638848 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6080120801925659, loss=5.70539665222168
I0207 15:45:18.444758 139946397853440 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.3590497970581055, loss=4.281445026397705
I0207 15:46:05.311077 139946414638848 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.8858688473701477, loss=4.314436912536621
I0207 15:46:51.851797 139946397853440 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6262685060501099, loss=5.980508804321289
I0207 15:47:38.602640 139946414638848 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7578819990158081, loss=5.673569679260254
I0207 15:48:25.618892 139946397853440 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6523457169532776, loss=5.9140400886535645
I0207 15:49:12.303462 139946414638848 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.58986896276474, loss=5.817382335662842
I0207 15:49:58.788048 139946397853440 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9129841923713684, loss=4.182074069976807
I0207 15:50:28.462421 140107197974336 spec.py:321] Evaluating on the training split.
I0207 15:50:39.664058 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 15:51:19.447704 140107197974336 spec.py:349] Evaluating on the test split.
I0207 15:51:21.051139 140107197974336 submission_runner.py:408] Time since start: 3844.00s, 	Step: 7165, 	{'train/accuracy': 0.33455076813697815, 'train/loss': 3.108985424041748, 'validation/accuracy': 0.3141399919986725, 'validation/loss': 3.2446765899658203, 'validation/num_examples': 50000, 'test/accuracy': 0.24170000851154327, 'test/loss': 3.805813789367676, 'test/num_examples': 10000, 'score': 3401.7290391921997, 'total_duration': 3843.9964752197266, 'accumulated_submission_time': 3401.7290391921997, 'accumulated_eval_time': 441.1956124305725, 'accumulated_logging_time': 0.6641831398010254}
I0207 15:51:21.068857 139946414638848 logging_writer.py:48] [7165] accumulated_eval_time=441.195612, accumulated_logging_time=0.664183, accumulated_submission_time=3401.729039, global_step=7165, preemption_count=0, score=3401.729039, test/accuracy=0.241700, test/loss=3.805814, test/num_examples=10000, total_duration=3843.996475, train/accuracy=0.334551, train/loss=3.108985, validation/accuracy=0.314140, validation/loss=3.244677, validation/num_examples=50000
I0207 15:51:35.868059 139946397853440 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.2781397104263306, loss=4.336002349853516
I0207 15:52:20.978649 139946414638848 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.0615038871765137, loss=4.085659027099609
I0207 15:53:07.512297 139946397853440 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7415440678596497, loss=4.242029666900635
I0207 15:53:54.251105 139946414638848 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8267377018928528, loss=4.4425835609436035
I0207 15:54:40.952417 139946397853440 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9658864736557007, loss=4.028860092163086
I0207 15:55:27.589512 139946414638848 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.9663777351379395, loss=4.100191116333008
I0207 15:56:14.402580 139946397853440 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8613532781600952, loss=3.985426425933838
I0207 15:57:00.844116 139946414638848 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8949536085128784, loss=4.019289970397949
I0207 15:57:47.585441 139946397853440 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.015940546989441, loss=4.038150787353516
I0207 15:58:21.098299 140107197974336 spec.py:321] Evaluating on the training split.
I0207 15:58:32.297308 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 15:59:11.102001 140107197974336 spec.py:349] Evaluating on the test split.
I0207 15:59:12.692151 140107197974336 submission_runner.py:408] Time since start: 4315.64s, 	Step: 8073, 	{'train/accuracy': 0.3506445288658142, 'train/loss': 3.0698530673980713, 'validation/accuracy': 0.3208400011062622, 'validation/loss': 3.2362983226776123, 'validation/num_examples': 50000, 'test/accuracy': 0.24710001051425934, 'test/loss': 3.786952495574951, 'test/num_examples': 10000, 'score': 3821.696554660797, 'total_duration': 4315.6374979019165, 'accumulated_submission_time': 3821.696554660797, 'accumulated_eval_time': 492.78946113586426, 'accumulated_logging_time': 0.6920418739318848}
I0207 15:59:12.713255 139946414638848 logging_writer.py:48] [8073] accumulated_eval_time=492.789461, accumulated_logging_time=0.692042, accumulated_submission_time=3821.696555, global_step=8073, preemption_count=0, score=3821.696555, test/accuracy=0.247100, test/loss=3.786952, test/num_examples=10000, total_duration=4315.637498, train/accuracy=0.350645, train/loss=3.069853, validation/accuracy=0.320840, validation/loss=3.236298, validation/num_examples=50000
I0207 15:59:24.203246 139946397853440 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8799631595611572, loss=4.444482803344727
I0207 16:00:08.373319 139946414638848 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.519981861114502, loss=5.838357925415039
I0207 16:00:54.968499 139946397853440 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.0851621627807617, loss=3.937612295150757
I0207 16:01:41.771059 139946414638848 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.6256265044212341, loss=5.363347053527832
I0207 16:02:28.654230 139946397853440 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8586903810501099, loss=4.338015556335449
I0207 16:03:15.582498 139946414638848 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.9097309112548828, loss=4.293172359466553
I0207 16:04:02.179759 139946397853440 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5686704516410828, loss=5.72529411315918
I0207 16:04:49.341821 139946414638848 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.8264224529266357, loss=3.895050048828125
I0207 16:05:36.036347 139946397853440 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7329680323600769, loss=5.972761154174805
I0207 16:06:12.946153 140107197974336 spec.py:321] Evaluating on the training split.
I0207 16:06:24.096329 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 16:07:02.579664 140107197974336 spec.py:349] Evaluating on the test split.
I0207 16:07:04.182497 140107197974336 submission_runner.py:408] Time since start: 4787.13s, 	Step: 8981, 	{'train/accuracy': 0.3797656297683716, 'train/loss': 2.877138614654541, 'validation/accuracy': 0.34685999155044556, 'validation/loss': 3.067584991455078, 'validation/num_examples': 50000, 'test/accuracy': 0.2645000219345093, 'test/loss': 3.641249418258667, 'test/num_examples': 10000, 'score': 4241.8689959049225, 'total_duration': 4787.127847909927, 'accumulated_submission_time': 4241.8689959049225, 'accumulated_eval_time': 544.0257968902588, 'accumulated_logging_time': 0.7220323085784912}
I0207 16:07:04.200143 139946414638848 logging_writer.py:48] [8981] accumulated_eval_time=544.025797, accumulated_logging_time=0.722032, accumulated_submission_time=4241.868996, global_step=8981, preemption_count=0, score=4241.868996, test/accuracy=0.264500, test/loss=3.641249, test/num_examples=10000, total_duration=4787.127848, train/accuracy=0.379766, train/loss=2.877139, validation/accuracy=0.346860, validation/loss=3.067585, validation/num_examples=50000
I0207 16:07:12.413630 139946397853440 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6333958506584167, loss=5.195981502532959
I0207 16:07:56.538957 139946414638848 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9812062382698059, loss=3.8871452808380127
I0207 16:08:43.405005 139946397853440 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8275501132011414, loss=3.8098723888397217
I0207 16:09:30.333317 139946414638848 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8249662518501282, loss=3.80686616897583
I0207 16:10:17.395528 139946397853440 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5647501349449158, loss=5.715555667877197
I0207 16:11:04.366230 139946414638848 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.2751812934875488, loss=3.730502128601074
I0207 16:11:51.244936 139946397853440 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.0108221769332886, loss=3.7285268306732178
I0207 16:12:38.093575 139946414638848 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.670974612236023, loss=5.017762184143066
I0207 16:13:24.999130 139946397853440 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7497415542602539, loss=5.554542541503906
I0207 16:14:04.595494 140107197974336 spec.py:321] Evaluating on the training split.
I0207 16:14:15.512223 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 16:14:57.763625 140107197974336 spec.py:349] Evaluating on the test split.
I0207 16:14:59.362854 140107197974336 submission_runner.py:408] Time since start: 5262.31s, 	Step: 9886, 	{'train/accuracy': 0.4021289050579071, 'train/loss': 2.717653751373291, 'validation/accuracy': 0.3776800036430359, 'validation/loss': 2.877584934234619, 'validation/num_examples': 50000, 'test/accuracy': 0.2873000204563141, 'test/loss': 3.478476047515869, 'test/num_examples': 10000, 'score': 4662.2013692855835, 'total_duration': 5262.30820274353, 'accumulated_submission_time': 4662.2013692855835, 'accumulated_eval_time': 598.793160200119, 'accumulated_logging_time': 0.7506968975067139}
I0207 16:14:59.384194 139946414638848 logging_writer.py:48] [9886] accumulated_eval_time=598.793160, accumulated_logging_time=0.750697, accumulated_submission_time=4662.201369, global_step=9886, preemption_count=0, score=4662.201369, test/accuracy=0.287300, test/loss=3.478476, test/num_examples=10000, total_duration=5262.308203, train/accuracy=0.402129, train/loss=2.717654, validation/accuracy=0.377680, validation/loss=2.877585, validation/num_examples=50000
I0207 16:15:05.551475 139946397853440 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7267822623252869, loss=6.010930061340332
I0207 16:15:49.618737 139946414638848 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8933378458023071, loss=3.8963260650634766
I0207 16:16:36.084176 139946397853440 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.9514715671539307, loss=3.8511593341827393
I0207 16:17:23.087107 139946414638848 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8297324180603027, loss=3.9586374759674072
I0207 16:18:10.019007 139946397853440 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.045589804649353, loss=3.761045217514038
I0207 16:18:56.940138 139946414638848 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9181419014930725, loss=3.651257038116455
I0207 16:19:43.913604 139946397853440 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9014045000076294, loss=3.790931463241577
I0207 16:20:31.030609 139946414638848 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.0733232498168945, loss=3.779095411300659
I0207 16:21:17.978400 139946397853440 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6517542600631714, loss=4.836775779724121
I0207 16:21:59.762915 140107197974336 spec.py:321] Evaluating on the training split.
I0207 16:22:11.123673 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 16:22:49.972039 140107197974336 spec.py:349] Evaluating on the test split.
I0207 16:22:51.576345 140107197974336 submission_runner.py:408] Time since start: 5734.52s, 	Step: 10791, 	{'train/accuracy': 0.42597654461860657, 'train/loss': 2.5819032192230225, 'validation/accuracy': 0.39361998438835144, 'validation/loss': 2.756279706954956, 'validation/num_examples': 50000, 'test/accuracy': 0.29920002818107605, 'test/loss': 3.3826310634613037, 'test/num_examples': 10000, 'score': 5082.514708995819, 'total_duration': 5734.52169752121, 'accumulated_submission_time': 5082.514708995819, 'accumulated_eval_time': 650.6066122055054, 'accumulated_logging_time': 0.7820405960083008}
I0207 16:22:51.595011 139946414638848 logging_writer.py:48] [10791] accumulated_eval_time=650.606612, accumulated_logging_time=0.782041, accumulated_submission_time=5082.514709, global_step=10791, preemption_count=0, score=5082.514709, test/accuracy=0.299200, test/loss=3.382631, test/num_examples=10000, total_duration=5734.521698, train/accuracy=0.425977, train/loss=2.581903, validation/accuracy=0.393620, validation/loss=2.756280, validation/num_examples=50000
I0207 16:22:55.707422 139946397853440 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7886345982551575, loss=5.242043495178223
I0207 16:23:39.321537 139946414638848 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9560277462005615, loss=3.716296672821045
I0207 16:24:26.415264 139946397853440 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7314170598983765, loss=4.879565238952637
I0207 16:25:13.584888 139946414638848 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9063783884048462, loss=3.6595497131347656
I0207 16:26:00.529288 139946397853440 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.8853262662887573, loss=3.5403130054473877
I0207 16:26:47.679739 139946414638848 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.0730100870132446, loss=3.5419745445251465
I0207 16:27:34.596458 139946397853440 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0249193906784058, loss=4.345196723937988
I0207 16:28:21.610889 139946414638848 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7976861000061035, loss=5.828296661376953
I0207 16:29:08.637288 139946397853440 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6433364152908325, loss=5.663671970367432
I0207 16:29:51.944606 140107197974336 spec.py:321] Evaluating on the training split.
I0207 16:30:03.499942 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 16:30:42.755562 140107197974336 spec.py:349] Evaluating on the test split.
I0207 16:30:44.356090 140107197974336 submission_runner.py:408] Time since start: 6207.30s, 	Step: 11694, 	{'train/accuracy': 0.4414648413658142, 'train/loss': 2.500446081161499, 'validation/accuracy': 0.4053199887275696, 'validation/loss': 2.709198474884033, 'validation/num_examples': 50000, 'test/accuracy': 0.31790000200271606, 'test/loss': 3.3103625774383545, 'test/num_examples': 10000, 'score': 5502.804249048233, 'total_duration': 6207.301434278488, 'accumulated_submission_time': 5502.804249048233, 'accumulated_eval_time': 703.0180859565735, 'accumulated_logging_time': 0.8099539279937744}
I0207 16:30:44.374265 139946414638848 logging_writer.py:48] [11694] accumulated_eval_time=703.018086, accumulated_logging_time=0.809954, accumulated_submission_time=5502.804249, global_step=11694, preemption_count=0, score=5502.804249, test/accuracy=0.317900, test/loss=3.310363, test/num_examples=10000, total_duration=6207.301434, train/accuracy=0.441465, train/loss=2.500446, validation/accuracy=0.405320, validation/loss=2.709198, validation/num_examples=50000
I0207 16:30:47.255922 139946397853440 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8708931803703308, loss=3.7746803760528564
I0207 16:31:30.552014 139946414638848 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7068137526512146, loss=4.607446193695068
I0207 16:32:17.259124 139946397853440 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.956012487411499, loss=3.8673760890960693
I0207 16:33:04.090037 139946414638848 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9069573879241943, loss=3.409412384033203
I0207 16:33:50.985724 139946397853440 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7828359603881836, loss=4.902627944946289
I0207 16:34:38.078095 139946414638848 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.0018960237503052, loss=3.433004856109619
I0207 16:35:25.017187 139946397853440 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9185364246368408, loss=3.7837014198303223
I0207 16:36:11.977141 139946414638848 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.9537563920021057, loss=3.5291366577148438
I0207 16:36:58.976103 139946397853440 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8847511410713196, loss=3.85064697265625
I0207 16:37:44.688331 140107197974336 spec.py:321] Evaluating on the training split.
I0207 16:37:55.673871 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 16:38:34.905344 140107197974336 spec.py:349] Evaluating on the test split.
I0207 16:38:36.516130 140107197974336 submission_runner.py:408] Time since start: 6679.46s, 	Step: 12599, 	{'train/accuracy': 0.4473632574081421, 'train/loss': 2.4482831954956055, 'validation/accuracy': 0.41741999983787537, 'validation/loss': 2.6113107204437256, 'validation/num_examples': 50000, 'test/accuracy': 0.3225000202655792, 'test/loss': 3.2534356117248535, 'test/num_examples': 10000, 'score': 5923.055751800537, 'total_duration': 6679.461454868317, 'accumulated_submission_time': 5923.055751800537, 'accumulated_eval_time': 754.8459165096283, 'accumulated_logging_time': 0.8380134105682373}
I0207 16:38:36.536854 139946414638848 logging_writer.py:48] [12599] accumulated_eval_time=754.845917, accumulated_logging_time=0.838013, accumulated_submission_time=5923.055752, global_step=12599, preemption_count=0, score=5923.055752, test/accuracy=0.322500, test/loss=3.253436, test/num_examples=10000, total_duration=6679.461455, train/accuracy=0.447363, train/loss=2.448283, validation/accuracy=0.417420, validation/loss=2.611311, validation/num_examples=50000
I0207 16:38:37.370481 139946397853440 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7182644009590149, loss=5.567219257354736
I0207 16:39:20.729781 139946414638848 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.1598533391952515, loss=3.464055299758911
I0207 16:40:07.777424 139946397853440 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9485012292861938, loss=3.5318987369537354
I0207 16:40:55.126036 139946414638848 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.046000361442566, loss=3.551562786102295
I0207 16:41:42.395598 139946397853440 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8044496774673462, loss=4.372378826141357
I0207 16:42:29.312014 139946414638848 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9411624073982239, loss=3.572969913482666
I0207 16:43:16.454004 139946397853440 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.9032261967658997, loss=3.448660373687744
I0207 16:44:03.377804 139946414638848 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9700310230255127, loss=3.3740429878234863
I0207 16:44:50.554217 139946397853440 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.8982041478157043, loss=3.4315338134765625
I0207 16:45:36.678524 140107197974336 spec.py:321] Evaluating on the training split.
I0207 16:45:47.578865 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 16:46:27.630563 140107197974336 spec.py:349] Evaluating on the test split.
I0207 16:46:29.235532 140107197974336 submission_runner.py:408] Time since start: 7152.18s, 	Step: 13500, 	{'train/accuracy': 0.4481445252895355, 'train/loss': 2.4669337272644043, 'validation/accuracy': 0.41985997557640076, 'validation/loss': 2.638878107070923, 'validation/num_examples': 50000, 'test/accuracy': 0.3232000172138214, 'test/loss': 3.2796595096588135, 'test/num_examples': 10000, 'score': 6343.135094165802, 'total_duration': 7152.180879831314, 'accumulated_submission_time': 6343.135094165802, 'accumulated_eval_time': 807.402941942215, 'accumulated_logging_time': 0.8693232536315918}
I0207 16:46:29.253668 139946414638848 logging_writer.py:48] [13500] accumulated_eval_time=807.402942, accumulated_logging_time=0.869323, accumulated_submission_time=6343.135094, global_step=13500, preemption_count=0, score=6343.135094, test/accuracy=0.323200, test/loss=3.279660, test/num_examples=10000, total_duration=7152.180880, train/accuracy=0.448145, train/loss=2.466934, validation/accuracy=0.419860, validation/loss=2.638878, validation/num_examples=50000
I0207 16:46:29.668852 139946397853440 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.8496569395065308, loss=4.480881690979004
I0207 16:47:12.803664 139946414638848 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9800933599472046, loss=3.617351531982422
I0207 16:47:59.463344 139946397853440 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9601507782936096, loss=4.088095664978027
I0207 16:48:46.915958 139946414638848 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7355682849884033, loss=5.413825988769531
I0207 16:49:33.960269 139946397853440 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0503450632095337, loss=3.707592248916626
I0207 16:50:21.148338 139946414638848 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7036857604980469, loss=4.8374223709106445
I0207 16:51:08.283969 139946397853440 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.8535884022712708, loss=4.105884552001953
I0207 16:51:55.083848 139946414638848 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6560043096542358, loss=5.468390464782715
I0207 16:52:42.353262 139946397853440 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7105317711830139, loss=5.413522243499756
I0207 16:53:29.274136 139946414638848 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.8027764558792114, loss=5.7884111404418945
I0207 16:53:29.287113 140107197974336 spec.py:321] Evaluating on the training split.
I0207 16:53:40.498108 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 16:54:21.479116 140107197974336 spec.py:349] Evaluating on the test split.
I0207 16:54:23.088011 140107197974336 submission_runner.py:408] Time since start: 7626.03s, 	Step: 14401, 	{'train/accuracy': 0.46925780177116394, 'train/loss': 2.391101121902466, 'validation/accuracy': 0.42927998304367065, 'validation/loss': 2.592332601547241, 'validation/num_examples': 50000, 'test/accuracy': 0.32600000500679016, 'test/loss': 3.225304126739502, 'test/num_examples': 10000, 'score': 6763.108623743057, 'total_duration': 7626.033350944519, 'accumulated_submission_time': 6763.108623743057, 'accumulated_eval_time': 861.2038099765778, 'accumulated_logging_time': 0.8967950344085693}
I0207 16:54:23.107232 139946397853440 logging_writer.py:48] [14401] accumulated_eval_time=861.203810, accumulated_logging_time=0.896795, accumulated_submission_time=6763.108624, global_step=14401, preemption_count=0, score=6763.108624, test/accuracy=0.326000, test/loss=3.225304, test/num_examples=10000, total_duration=7626.033351, train/accuracy=0.469258, train/loss=2.391101, validation/accuracy=0.429280, validation/loss=2.592333, validation/num_examples=50000
I0207 16:55:06.255640 139946414638848 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.2177153825759888, loss=3.4469237327575684
I0207 16:55:52.667291 139946397853440 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0108402967453003, loss=3.421412229537964
I0207 16:56:39.611721 139946414638848 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9384087920188904, loss=3.4449384212493896
I0207 16:57:26.549013 139946397853440 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.9534362554550171, loss=3.5483360290527344
I0207 16:58:13.201657 139946414638848 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.0833420753479004, loss=3.155702590942383
I0207 16:59:00.010501 139946397853440 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9692853689193726, loss=3.312007188796997
I0207 16:59:46.558517 139946414638848 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.185926914215088, loss=3.6819019317626953
I0207 17:00:33.608807 139946397853440 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.0088967084884644, loss=3.4010491371154785
I0207 17:01:20.585887 139946414638848 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9727599024772644, loss=3.4957289695739746
I0207 17:01:23.198997 140107197974336 spec.py:321] Evaluating on the training split.
I0207 17:01:34.398012 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 17:02:13.864167 140107197974336 spec.py:349] Evaluating on the test split.
I0207 17:02:15.457566 140107197974336 submission_runner.py:408] Time since start: 8098.40s, 	Step: 15307, 	{'train/accuracy': 0.4732421636581421, 'train/loss': 2.3394665718078613, 'validation/accuracy': 0.4375399947166443, 'validation/loss': 2.532576084136963, 'validation/num_examples': 50000, 'test/accuracy': 0.3416000306606293, 'test/loss': 3.1499056816101074, 'test/num_examples': 10000, 'score': 7183.1378700733185, 'total_duration': 8098.40290927887, 'accumulated_submission_time': 7183.1378700733185, 'accumulated_eval_time': 913.4623625278473, 'accumulated_logging_time': 0.926544189453125}
I0207 17:02:15.475657 139946397853440 logging_writer.py:48] [15307] accumulated_eval_time=913.462363, accumulated_logging_time=0.926544, accumulated_submission_time=7183.137870, global_step=15307, preemption_count=0, score=7183.137870, test/accuracy=0.341600, test/loss=3.149906, test/num_examples=10000, total_duration=8098.402909, train/accuracy=0.473242, train/loss=2.339467, validation/accuracy=0.437540, validation/loss=2.532576, validation/num_examples=50000
I0207 17:02:55.731040 139946414638848 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.8592535853385925, loss=4.8639044761657715
I0207 17:03:42.344264 139946397853440 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9309724569320679, loss=3.3866920471191406
I0207 17:04:29.459708 139946414638848 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.0133063793182373, loss=3.5838255882263184
I0207 17:05:16.537101 139946397853440 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9318260550498962, loss=3.4803338050842285
I0207 17:06:03.526465 139946414638848 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9923470616340637, loss=3.590236186981201
I0207 17:06:50.502335 139946397853440 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.8625031113624573, loss=4.036575794219971
I0207 17:07:37.398165 139946414638848 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9207528233528137, loss=3.237700939178467
I0207 17:08:24.633871 139946397853440 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.9968856573104858, loss=3.22697377204895
I0207 17:09:11.633558 139946414638848 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.854206383228302, loss=3.6647183895111084
I0207 17:09:15.555311 140107197974336 spec.py:321] Evaluating on the training split.
I0207 17:09:26.691974 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 17:10:06.631431 140107197974336 spec.py:349] Evaluating on the test split.
I0207 17:10:08.231625 140107197974336 submission_runner.py:408] Time since start: 8571.18s, 	Step: 16210, 	{'train/accuracy': 0.4846288859844208, 'train/loss': 2.263720750808716, 'validation/accuracy': 0.44745999574661255, 'validation/loss': 2.450514554977417, 'validation/num_examples': 50000, 'test/accuracy': 0.3473000228404999, 'test/loss': 3.0882656574249268, 'test/num_examples': 10000, 'score': 7603.157238006592, 'total_duration': 8571.176964044571, 'accumulated_submission_time': 7603.157238006592, 'accumulated_eval_time': 966.1386594772339, 'accumulated_logging_time': 0.9530913829803467}
I0207 17:10:08.251551 139946397853440 logging_writer.py:48] [16210] accumulated_eval_time=966.138659, accumulated_logging_time=0.953091, accumulated_submission_time=7603.157238, global_step=16210, preemption_count=0, score=7603.157238, test/accuracy=0.347300, test/loss=3.088266, test/num_examples=10000, total_duration=8571.176964, train/accuracy=0.484629, train/loss=2.263721, validation/accuracy=0.447460, validation/loss=2.450515, validation/num_examples=50000
I0207 17:10:47.297593 139946414638848 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.2385817766189575, loss=4.194336414337158
I0207 17:11:34.195522 139946397853440 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.1010854244232178, loss=3.2736778259277344
I0207 17:12:21.330633 139946414638848 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7926634550094604, loss=5.369965076446533
I0207 17:13:08.470713 139946397853440 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0524064302444458, loss=3.2263307571411133
I0207 17:13:55.869399 139946414638848 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.0611305236816406, loss=3.3705921173095703
I0207 17:14:43.259359 139946397853440 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.8291898965835571, loss=4.423790454864502
I0207 17:15:30.491343 139946414638848 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.9558870196342468, loss=3.4795758724212646
I0207 17:16:17.892137 139946397853440 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.8072789907455444, loss=4.068567276000977
I0207 17:17:05.277953 139946414638848 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9207965135574341, loss=3.843872308731079
I0207 17:17:08.236645 140107197974336 spec.py:321] Evaluating on the training split.
I0207 17:17:19.318003 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 17:17:58.463023 140107197974336 spec.py:349] Evaluating on the test split.
I0207 17:18:00.071111 140107197974336 submission_runner.py:408] Time since start: 9043.02s, 	Step: 17108, 	{'train/accuracy': 0.5009179711341858, 'train/loss': 2.1943092346191406, 'validation/accuracy': 0.4619999825954437, 'validation/loss': 2.3972971439361572, 'validation/num_examples': 50000, 'test/accuracy': 0.3574000298976898, 'test/loss': 3.0589606761932373, 'test/num_examples': 10000, 'score': 8023.078520536423, 'total_duration': 9043.016461133957, 'accumulated_submission_time': 8023.078520536423, 'accumulated_eval_time': 1017.9731209278107, 'accumulated_logging_time': 0.9839563369750977}
I0207 17:18:00.091404 139946397853440 logging_writer.py:48] [17108] accumulated_eval_time=1017.973121, accumulated_logging_time=0.983956, accumulated_submission_time=8023.078521, global_step=17108, preemption_count=0, score=8023.078521, test/accuracy=0.357400, test/loss=3.058961, test/num_examples=10000, total_duration=9043.016461, train/accuracy=0.500918, train/loss=2.194309, validation/accuracy=0.462000, validation/loss=2.397297, validation/num_examples=50000
I0207 17:18:39.957937 139946414638848 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0265531539916992, loss=3.306016683578491
I0207 17:19:26.765165 139946397853440 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0952759981155396, loss=4.084996223449707
I0207 17:20:13.964359 139946414638848 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7998870015144348, loss=4.3183674812316895
I0207 17:21:01.240968 139946397853440 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.9648920297622681, loss=3.28143572807312
I0207 17:21:48.375110 139946414638848 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0777982473373413, loss=3.301809072494507
I0207 17:22:35.566021 139946397853440 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.0658138990402222, loss=3.2665414810180664
I0207 17:23:23.049279 139946414638848 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.1453925371170044, loss=3.1842997074127197
I0207 17:24:10.429158 139946397853440 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.9897429347038269, loss=3.442017078399658
I0207 17:24:57.802080 139946414638848 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.2839223146438599, loss=3.3093531131744385
I0207 17:25:00.291362 140107197974336 spec.py:321] Evaluating on the training split.
I0207 17:25:12.013610 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 17:25:52.194772 140107197974336 spec.py:349] Evaluating on the test split.
I0207 17:25:53.794124 140107197974336 submission_runner.py:408] Time since start: 9516.74s, 	Step: 18007, 	{'train/accuracy': 0.5083202719688416, 'train/loss': 2.2235052585601807, 'validation/accuracy': 0.45151999592781067, 'validation/loss': 2.495116710662842, 'validation/num_examples': 50000, 'test/accuracy': 0.35680001974105835, 'test/loss': 3.10998272895813, 'test/num_examples': 10000, 'score': 8443.216924190521, 'total_duration': 9516.7394759655, 'accumulated_submission_time': 8443.216924190521, 'accumulated_eval_time': 1071.4758758544922, 'accumulated_logging_time': 1.0145676136016846}
I0207 17:25:53.812432 139946397853440 logging_writer.py:48] [18007] accumulated_eval_time=1071.475876, accumulated_logging_time=1.014568, accumulated_submission_time=8443.216924, global_step=18007, preemption_count=0, score=8443.216924, test/accuracy=0.356800, test/loss=3.109983, test/num_examples=10000, total_duration=9516.739476, train/accuracy=0.508320, train/loss=2.223505, validation/accuracy=0.451520, validation/loss=2.495117, validation/num_examples=50000
I0207 17:26:34.280326 139946414638848 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.0420804023742676, loss=3.044523000717163
I0207 17:27:21.011799 139946397853440 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.9031155109405518, loss=4.167134761810303
I0207 17:28:08.210678 139946414638848 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.8555116653442383, loss=3.999561071395874
I0207 17:28:55.286565 139946397853440 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.063237190246582, loss=3.1617701053619385
I0207 17:29:42.483761 139946414638848 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0332520008087158, loss=3.213534355163574
I0207 17:30:29.800248 139946397853440 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.0710338354110718, loss=3.165111541748047
I0207 17:31:17.044933 139946414638848 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.2123479843139648, loss=3.271440029144287
I0207 17:32:04.400980 139946397853440 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.780973494052887, loss=4.671172618865967
I0207 17:32:51.470123 139946414638848 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.055478811264038, loss=3.5998048782348633
I0207 17:32:54.058320 140107197974336 spec.py:321] Evaluating on the training split.
I0207 17:33:05.170721 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 17:33:44.591037 140107197974336 spec.py:349] Evaluating on the test split.
I0207 17:33:46.196104 140107197974336 submission_runner.py:408] Time since start: 9989.14s, 	Step: 18907, 	{'train/accuracy': 0.5022070407867432, 'train/loss': 2.1813580989837646, 'validation/accuracy': 0.4710799753665924, 'validation/loss': 2.35910964012146, 'validation/num_examples': 50000, 'test/accuracy': 0.36090001463890076, 'test/loss': 3.0243079662323, 'test/num_examples': 10000, 'score': 8863.401502132416, 'total_duration': 9989.141452550888, 'accumulated_submission_time': 8863.401502132416, 'accumulated_eval_time': 1123.6136424541473, 'accumulated_logging_time': 1.043529987335205}
I0207 17:33:46.214981 139946397853440 logging_writer.py:48] [18907] accumulated_eval_time=1123.613642, accumulated_logging_time=1.043530, accumulated_submission_time=8863.401502, global_step=18907, preemption_count=0, score=8863.401502, test/accuracy=0.360900, test/loss=3.024308, test/num_examples=10000, total_duration=9989.141453, train/accuracy=0.502207, train/loss=2.181358, validation/accuracy=0.471080, validation/loss=2.359110, validation/num_examples=50000
I0207 17:34:27.050289 139946414638848 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9806030988693237, loss=3.1197047233581543
I0207 17:35:14.156335 139946397853440 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.7412588596343994, loss=5.4309186935424805
I0207 17:36:01.153614 139946414638848 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.0312144756317139, loss=3.214015245437622
I0207 17:36:48.514145 139946397853440 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0092589855194092, loss=3.2055857181549072
I0207 17:37:35.845191 139946414638848 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0155547857284546, loss=3.2238001823425293
I0207 17:38:23.256518 139946397853440 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7076925039291382, loss=5.602215766906738
I0207 17:39:10.421756 139946414638848 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.8810884952545166, loss=3.7464096546173096
I0207 17:39:57.417863 139946397853440 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.136431097984314, loss=3.2236764430999756
I0207 17:40:44.876296 139946414638848 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.9946709871292114, loss=3.652970314025879
I0207 17:40:46.297058 140107197974336 spec.py:321] Evaluating on the training split.
I0207 17:40:57.560878 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 17:41:39.556576 140107197974336 spec.py:349] Evaluating on the test split.
I0207 17:41:41.152903 140107197974336 submission_runner.py:408] Time since start: 10464.10s, 	Step: 19804, 	{'train/accuracy': 0.5236718654632568, 'train/loss': 2.076261043548584, 'validation/accuracy': 0.4801599979400635, 'validation/loss': 2.295961618423462, 'validation/num_examples': 50000, 'test/accuracy': 0.37380000948905945, 'test/loss': 2.9481990337371826, 'test/num_examples': 10000, 'score': 9283.423431873322, 'total_duration': 10464.098256111145, 'accumulated_submission_time': 9283.423431873322, 'accumulated_eval_time': 1178.4694809913635, 'accumulated_logging_time': 1.0712995529174805}
I0207 17:41:41.174452 139946397853440 logging_writer.py:48] [19804] accumulated_eval_time=1178.469481, accumulated_logging_time=1.071300, accumulated_submission_time=9283.423432, global_step=19804, preemption_count=0, score=9283.423432, test/accuracy=0.373800, test/loss=2.948199, test/num_examples=10000, total_duration=10464.098256, train/accuracy=0.523672, train/loss=2.076261, validation/accuracy=0.480160, validation/loss=2.295962, validation/num_examples=50000
I0207 17:42:22.879273 139946414638848 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.1943989992141724, loss=3.206791400909424
I0207 17:43:09.627510 139946397853440 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1349964141845703, loss=3.1197657585144043
I0207 17:43:56.978165 139946414638848 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.8048286437988281, loss=5.249316692352295
I0207 17:44:44.281049 139946397853440 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8687237501144409, loss=4.14242696762085
I0207 17:45:31.486616 139946414638848 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8162632584571838, loss=4.7661638259887695
I0207 17:46:18.537942 139946397853440 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7598958611488342, loss=5.647985458374023
I0207 17:47:05.626525 139946414638848 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.0893821716308594, loss=3.1030914783477783
I0207 17:47:52.666137 139946397853440 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7564868927001953, loss=5.66892147064209
I0207 17:48:40.121829 139946414638848 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.4172582626342773, loss=3.229123115539551
I0207 17:48:41.179593 140107197974336 spec.py:321] Evaluating on the training split.
I0207 17:48:52.572287 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 17:49:31.934823 140107197974336 spec.py:349] Evaluating on the test split.
I0207 17:49:33.541719 140107197974336 submission_runner.py:408] Time since start: 10936.49s, 	Step: 20704, 	{'train/accuracy': 0.556835949420929, 'train/loss': 1.9286086559295654, 'validation/accuracy': 0.4835599958896637, 'validation/loss': 2.271160364151001, 'validation/num_examples': 50000, 'test/accuracy': 0.3751000165939331, 'test/loss': 2.9266579151153564, 'test/num_examples': 10000, 'score': 9703.366194963455, 'total_duration': 10936.4870698452, 'accumulated_submission_time': 9703.366194963455, 'accumulated_eval_time': 1230.8315978050232, 'accumulated_logging_time': 1.1028954982757568}
I0207 17:49:33.560720 139946397853440 logging_writer.py:48] [20704] accumulated_eval_time=1230.831598, accumulated_logging_time=1.102895, accumulated_submission_time=9703.366195, global_step=20704, preemption_count=0, score=9703.366195, test/accuracy=0.375100, test/loss=2.926658, test/num_examples=10000, total_duration=10936.487070, train/accuracy=0.556836, train/loss=1.928609, validation/accuracy=0.483560, validation/loss=2.271160, validation/num_examples=50000
I0207 17:50:15.295808 139946414638848 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.150645136833191, loss=3.232943058013916
I0207 17:51:02.041565 139946397853440 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.9126264452934265, loss=5.628522872924805
I0207 17:51:49.146136 139946414638848 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8020036816596985, loss=4.920836925506592
I0207 17:52:36.573787 139946397853440 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.0170785188674927, loss=3.4048964977264404
I0207 17:53:23.588606 139946414638848 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.044058084487915, loss=3.1557364463806152
I0207 17:54:11.179073 139946397853440 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.1642251014709473, loss=3.190028190612793
I0207 17:54:57.896869 139946414638848 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8260117173194885, loss=5.290009021759033
I0207 17:55:44.973586 139946397853440 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8745191693305969, loss=4.045973300933838
I0207 17:56:32.107166 139946414638848 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.1041066646575928, loss=3.211083173751831
I0207 17:56:33.637916 140107197974336 spec.py:321] Evaluating on the training split.
I0207 17:56:44.672302 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 17:57:24.956094 140107197974336 spec.py:349] Evaluating on the test split.
I0207 17:57:26.555704 140107197974336 submission_runner.py:408] Time since start: 11409.50s, 	Step: 21605, 	{'train/accuracy': 0.5281640291213989, 'train/loss': 2.053582191467285, 'validation/accuracy': 0.48861998319625854, 'validation/loss': 2.251695156097412, 'validation/num_examples': 50000, 'test/accuracy': 0.37950003147125244, 'test/loss': 2.898561477661133, 'test/num_examples': 10000, 'score': 10123.38339304924, 'total_duration': 11409.501055002213, 'accumulated_submission_time': 10123.38339304924, 'accumulated_eval_time': 1283.7493817806244, 'accumulated_logging_time': 1.1308624744415283}
I0207 17:57:26.576366 139946397853440 logging_writer.py:48] [21605] accumulated_eval_time=1283.749382, accumulated_logging_time=1.130862, accumulated_submission_time=10123.383393, global_step=21605, preemption_count=0, score=10123.383393, test/accuracy=0.379500, test/loss=2.898561, test/num_examples=10000, total_duration=11409.501055, train/accuracy=0.528164, train/loss=2.053582, validation/accuracy=0.488620, validation/loss=2.251695, validation/num_examples=50000
I0207 17:58:08.088791 139946414638848 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.0458729267120361, loss=3.256964683532715
I0207 17:58:54.949602 139946397853440 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.014199137687683, loss=3.317117214202881
I0207 17:59:42.110713 139946414638848 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.8970286250114441, loss=4.550378322601318
I0207 18:00:29.114539 139946397853440 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.1670153141021729, loss=3.0352206230163574
I0207 18:01:16.362144 139946414638848 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.0259945392608643, loss=3.2833306789398193
I0207 18:02:03.796392 139946397853440 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.2293246984481812, loss=3.130861759185791
I0207 18:02:51.059310 139946414638848 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.1124082803726196, loss=5.521890163421631
I0207 18:03:38.397082 139946397853440 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.2019150257110596, loss=3.0252983570098877
I0207 18:04:25.857495 139946414638848 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.1430763006210327, loss=2.9852347373962402
I0207 18:04:27.202322 140107197974336 spec.py:321] Evaluating on the training split.
I0207 18:04:38.522953 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 18:05:19.363195 140107197974336 spec.py:349] Evaluating on the test split.
I0207 18:05:20.961697 140107197974336 submission_runner.py:408] Time since start: 11883.91s, 	Step: 22504, 	{'train/accuracy': 0.5364062190055847, 'train/loss': 1.9806296825408936, 'validation/accuracy': 0.4958599805831909, 'validation/loss': 2.1945180892944336, 'validation/num_examples': 50000, 'test/accuracy': 0.3874000310897827, 'test/loss': 2.876436948776245, 'test/num_examples': 10000, 'score': 10543.948052406311, 'total_duration': 11883.90704703331, 'accumulated_submission_time': 10543.948052406311, 'accumulated_eval_time': 1337.5087454319, 'accumulated_logging_time': 1.1621778011322021}
I0207 18:05:20.980726 139946397853440 logging_writer.py:48] [22504] accumulated_eval_time=1337.508745, accumulated_logging_time=1.162178, accumulated_submission_time=10543.948052, global_step=22504, preemption_count=0, score=10543.948052, test/accuracy=0.387400, test/loss=2.876437, test/num_examples=10000, total_duration=11883.907047, train/accuracy=0.536406, train/loss=1.980630, validation/accuracy=0.495860, validation/loss=2.194518, validation/num_examples=50000
I0207 18:06:02.909773 139946414638848 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.2388911247253418, loss=3.10168194770813
I0207 18:06:49.700970 139946397853440 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.175140142440796, loss=3.3384053707122803
I0207 18:07:36.996960 139946414638848 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.7363428473472595, loss=5.483615875244141
I0207 18:08:23.988327 139946397853440 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.0365530252456665, loss=3.1481752395629883
I0207 18:09:11.172219 139946414638848 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.3549870252609253, loss=3.0725371837615967
I0207 18:09:58.475413 139946397853440 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.021610975265503, loss=3.5830628871917725
I0207 18:10:45.758934 139946414638848 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.8846447467803955, loss=5.485097885131836
I0207 18:11:33.074214 139946397853440 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.0274639129638672, loss=3.5583691596984863
I0207 18:12:20.250137 139946414638848 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.9727200269699097, loss=3.3259801864624023
I0207 18:12:21.334284 140107197974336 spec.py:321] Evaluating on the training split.
I0207 18:12:32.795570 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 18:13:13.895286 140107197974336 spec.py:349] Evaluating on the test split.
I0207 18:13:15.504672 140107197974336 submission_runner.py:408] Time since start: 12358.45s, 	Step: 23404, 	{'train/accuracy': 0.5655078291893005, 'train/loss': 1.908821702003479, 'validation/accuracy': 0.49917998909950256, 'validation/loss': 2.2292399406433105, 'validation/num_examples': 50000, 'test/accuracy': 0.3874000310897827, 'test/loss': 2.8902664184570312, 'test/num_examples': 10000, 'score': 10964.240460157394, 'total_duration': 12358.45002245903, 'accumulated_submission_time': 10964.240460157394, 'accumulated_eval_time': 1391.6791398525238, 'accumulated_logging_time': 1.190190076828003}
I0207 18:13:15.523304 139946397853440 logging_writer.py:48] [23404] accumulated_eval_time=1391.679140, accumulated_logging_time=1.190190, accumulated_submission_time=10964.240460, global_step=23404, preemption_count=0, score=10964.240460, test/accuracy=0.387400, test/loss=2.890266, test/num_examples=10000, total_duration=12358.450022, train/accuracy=0.565508, train/loss=1.908822, validation/accuracy=0.499180, validation/loss=2.229240, validation/num_examples=50000
I0207 18:13:57.610656 139946414638848 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.1167936325073242, loss=3.1102705001831055
I0207 18:14:45.147471 139946397853440 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.9079052805900574, loss=5.48237419128418
I0207 18:15:32.421941 139946414638848 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.030461072921753, loss=3.2612051963806152
I0207 18:16:20.110083 139946397853440 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.96595299243927, loss=3.717414379119873
I0207 18:17:07.350064 139946414638848 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.9873769879341125, loss=3.5194921493530273
I0207 18:17:54.703856 139946397853440 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.0078874826431274, loss=3.1212849617004395
I0207 18:18:42.107502 139946414638848 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7813823819160461, loss=5.443831920623779
I0207 18:19:29.379602 139946397853440 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.1061489582061768, loss=3.108070135116577
I0207 18:20:15.941862 140107197974336 spec.py:321] Evaluating on the training split.
I0207 18:20:26.816415 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 18:21:05.865630 140107197974336 spec.py:349] Evaluating on the test split.
I0207 18:21:07.453850 140107197974336 submission_runner.py:408] Time since start: 12830.40s, 	Step: 24300, 	{'train/accuracy': 0.5464257597923279, 'train/loss': 1.9774080514907837, 'validation/accuracy': 0.5089799761772156, 'validation/loss': 2.1693193912506104, 'validation/num_examples': 50000, 'test/accuracy': 0.3955000042915344, 'test/loss': 2.8177545070648193, 'test/num_examples': 10000, 'score': 11384.598715543747, 'total_duration': 12830.399197340012, 'accumulated_submission_time': 11384.598715543747, 'accumulated_eval_time': 1443.1911263465881, 'accumulated_logging_time': 1.218409776687622}
I0207 18:21:07.477676 139946414638848 logging_writer.py:48] [24300] accumulated_eval_time=1443.191126, accumulated_logging_time=1.218410, accumulated_submission_time=11384.598716, global_step=24300, preemption_count=0, score=11384.598716, test/accuracy=0.395500, test/loss=2.817755, test/num_examples=10000, total_duration=12830.399197, train/accuracy=0.546426, train/loss=1.977408, validation/accuracy=0.508980, validation/loss=2.169319, validation/num_examples=50000
I0207 18:21:07.896074 139946397853440 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.2391256093978882, loss=2.8912062644958496
I0207 18:21:51.184229 139946414638848 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.342411756515503, loss=3.0611460208892822
I0207 18:22:37.887490 139946397853440 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.268369197845459, loss=3.0228846073150635
I0207 18:23:25.143457 139946414638848 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.0623383522033691, loss=2.892791271209717
I0207 18:24:12.302534 139946397853440 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.043592095375061, loss=3.0572738647460938
I0207 18:24:59.373669 139946414638848 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9664203524589539, loss=4.748579978942871
I0207 18:25:46.663062 139946397853440 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.2580420970916748, loss=3.0722782611846924
I0207 18:26:33.831064 139946414638848 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.0915486812591553, loss=3.4128763675689697
I0207 18:27:21.324885 139946397853440 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.7671716213226318, loss=5.201716899871826
I0207 18:28:07.608718 140107197974336 spec.py:321] Evaluating on the training split.
I0207 18:28:18.517942 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 18:28:58.466742 140107197974336 spec.py:349] Evaluating on the test split.
I0207 18:29:00.061747 140107197974336 submission_runner.py:408] Time since start: 13303.01s, 	Step: 25200, 	{'train/accuracy': 0.5544726252555847, 'train/loss': 1.8977075815200806, 'validation/accuracy': 0.5140599608421326, 'validation/loss': 2.105320930480957, 'validation/num_examples': 50000, 'test/accuracy': 0.4011000096797943, 'test/loss': 2.787417411804199, 'test/num_examples': 10000, 'score': 11804.667268276215, 'total_duration': 13303.007089614868, 'accumulated_submission_time': 11804.667268276215, 'accumulated_eval_time': 1495.6441519260406, 'accumulated_logging_time': 1.2541024684906006}
I0207 18:29:00.085415 139946414638848 logging_writer.py:48] [25200] accumulated_eval_time=1495.644152, accumulated_logging_time=1.254102, accumulated_submission_time=11804.667268, global_step=25200, preemption_count=0, score=11804.667268, test/accuracy=0.401100, test/loss=2.787417, test/num_examples=10000, total_duration=13303.007090, train/accuracy=0.554473, train/loss=1.897708, validation/accuracy=0.514060, validation/loss=2.105321, validation/num_examples=50000
I0207 18:29:00.506435 139946397853440 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.0504364967346191, loss=3.1657094955444336
I0207 18:29:43.789235 139946414638848 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.9675391316413879, loss=3.591602325439453
I0207 18:30:30.555669 139946397853440 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.1665658950805664, loss=3.1357264518737793
I0207 18:31:17.683359 139946414638848 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.0584566593170166, loss=3.5713448524475098
I0207 18:32:05.043723 139946397853440 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8594986200332642, loss=4.015044212341309
I0207 18:32:52.131701 139946414638848 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.1015236377716064, loss=2.9742279052734375
I0207 18:33:39.433724 139946397853440 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.9089968800544739, loss=4.329951763153076
I0207 18:34:26.623796 139946414638848 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.8857734799385071, loss=4.154648780822754
I0207 18:35:13.954705 139946397853440 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.1115471124649048, loss=2.9701950550079346
I0207 18:36:00.369779 140107197974336 spec.py:321] Evaluating on the training split.
I0207 18:36:11.475445 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 18:36:53.532161 140107197974336 spec.py:349] Evaluating on the test split.
I0207 18:36:55.133885 140107197974336 submission_runner.py:408] Time since start: 13778.08s, 	Step: 26100, 	{'train/accuracy': 0.5863476395606995, 'train/loss': 1.7209587097167969, 'validation/accuracy': 0.522059977054596, 'validation/loss': 2.043886423110962, 'validation/num_examples': 50000, 'test/accuracy': 0.40940001606941223, 'test/loss': 2.7280988693237305, 'test/num_examples': 10000, 'score': 12224.890575885773, 'total_duration': 13778.079232692719, 'accumulated_submission_time': 12224.890575885773, 'accumulated_eval_time': 1550.408257484436, 'accumulated_logging_time': 1.28757643699646}
I0207 18:36:55.158939 139946414638848 logging_writer.py:48] [26100] accumulated_eval_time=1550.408257, accumulated_logging_time=1.287576, accumulated_submission_time=12224.890576, global_step=26100, preemption_count=0, score=12224.890576, test/accuracy=0.409400, test/loss=2.728099, test/num_examples=10000, total_duration=13778.079233, train/accuracy=0.586348, train/loss=1.720959, validation/accuracy=0.522060, validation/loss=2.043886, validation/num_examples=50000
I0207 18:36:55.574460 139946397853440 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.2962894439697266, loss=3.0046801567077637
I0207 18:37:38.843270 139946414638848 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8191532492637634, loss=5.106283187866211
I0207 18:38:26.148442 139946397853440 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.0598349571228027, loss=3.2450435161590576
I0207 18:39:13.097199 139946414638848 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.1334246397018433, loss=3.1960229873657227
I0207 18:40:00.138853 139946397853440 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9781649708747864, loss=3.0718815326690674
I0207 18:40:47.227719 139946414638848 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.0738036632537842, loss=3.5864782333374023
I0207 18:41:34.588706 139946397853440 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.1545668840408325, loss=2.8427236080169678
I0207 18:42:21.689119 139946414638848 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.1261032819747925, loss=3.2012431621551514
I0207 18:43:08.977020 139946397853440 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.0889166593551636, loss=3.624685525894165
I0207 18:43:55.431598 140107197974336 spec.py:321] Evaluating on the training split.
I0207 18:44:06.735532 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 18:44:46.004445 140107197974336 spec.py:349] Evaluating on the test split.
I0207 18:44:47.605494 140107197974336 submission_runner.py:408] Time since start: 14250.55s, 	Step: 27000, 	{'train/accuracy': 0.5658202767372131, 'train/loss': 1.8472483158111572, 'validation/accuracy': 0.5233199596405029, 'validation/loss': 2.056349754333496, 'validation/num_examples': 50000, 'test/accuracy': 0.4092000126838684, 'test/loss': 2.723639726638794, 'test/num_examples': 10000, 'score': 12645.100444793701, 'total_duration': 14250.550843000412, 'accumulated_submission_time': 12645.100444793701, 'accumulated_eval_time': 1602.5821635723114, 'accumulated_logging_time': 1.3235411643981934}
I0207 18:44:47.633791 139946414638848 logging_writer.py:48] [27000] accumulated_eval_time=1602.582164, accumulated_logging_time=1.323541, accumulated_submission_time=12645.100445, global_step=27000, preemption_count=0, score=12645.100445, test/accuracy=0.409200, test/loss=2.723640, test/num_examples=10000, total_duration=14250.550843, train/accuracy=0.565820, train/loss=1.847248, validation/accuracy=0.523320, validation/loss=2.056350, validation/num_examples=50000
I0207 18:44:48.055236 139946397853440 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.1282219886779785, loss=2.948094367980957
I0207 18:45:31.347965 139946414638848 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.1179982423782349, loss=2.8740320205688477
I0207 18:46:18.754538 139946397853440 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8778887391090393, loss=4.596615314483643
I0207 18:47:05.838911 139946414638848 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.9390329122543335, loss=3.3538949489593506
I0207 18:47:53.071838 139946397853440 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.1683852672576904, loss=3.0159754753112793
I0207 18:48:40.204471 139946414638848 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.1202051639556885, loss=3.5800867080688477
I0207 18:49:27.650446 139946397853440 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.046237826347351, loss=3.5387024879455566
I0207 18:50:14.774351 139946414638848 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.9227373600006104, loss=4.519606113433838
I0207 18:51:02.229935 139946397853440 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.1051534414291382, loss=3.188950538635254
I0207 18:51:47.723233 140107197974336 spec.py:321] Evaluating on the training split.
I0207 18:51:58.701619 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 18:52:36.730439 140107197974336 spec.py:349] Evaluating on the test split.
I0207 18:52:38.333849 140107197974336 submission_runner.py:408] Time since start: 14721.28s, 	Step: 27898, 	{'train/accuracy': 0.557812511920929, 'train/loss': 1.8963301181793213, 'validation/accuracy': 0.5161399841308594, 'validation/loss': 2.110172748565674, 'validation/num_examples': 50000, 'test/accuracy': 0.4037000238895416, 'test/loss': 2.790687322616577, 'test/num_examples': 10000, 'score': 13065.124905824661, 'total_duration': 14721.279171228409, 'accumulated_submission_time': 13065.124905824661, 'accumulated_eval_time': 1653.192789554596, 'accumulated_logging_time': 1.3615074157714844}
I0207 18:52:38.355142 139946414638848 logging_writer.py:48] [27898] accumulated_eval_time=1653.192790, accumulated_logging_time=1.361507, accumulated_submission_time=13065.124906, global_step=27898, preemption_count=0, score=13065.124906, test/accuracy=0.403700, test/loss=2.790687, test/num_examples=10000, total_duration=14721.279171, train/accuracy=0.557813, train/loss=1.896330, validation/accuracy=0.516140, validation/loss=2.110173, validation/num_examples=50000
I0207 18:52:39.592916 139946397853440 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.0760645866394043, loss=3.0647454261779785
I0207 18:53:23.060681 139946414638848 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.1454757452011108, loss=2.902027130126953
I0207 18:54:10.154670 139946397853440 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.1028399467468262, loss=3.001145601272583
I0207 18:54:57.514195 139946414638848 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.264443278312683, loss=2.824018955230713
I0207 18:55:44.888129 139946397853440 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.2953364849090576, loss=2.890932083129883
I0207 18:56:32.090199 139946414638848 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.826286792755127, loss=5.184814929962158
I0207 18:57:19.470869 139946397853440 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.1126419305801392, loss=2.844228982925415
I0207 18:58:06.844527 139946414638848 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.9398337602615356, loss=5.149852275848389
I0207 18:58:54.126388 139946397853440 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.7682077288627625, loss=5.47515869140625
I0207 18:59:38.643334 140107197974336 spec.py:321] Evaluating on the training split.
I0207 18:59:49.592739 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 19:00:28.866379 140107197974336 spec.py:349] Evaluating on the test split.
I0207 19:00:30.473619 140107197974336 submission_runner.py:408] Time since start: 15193.42s, 	Step: 28795, 	{'train/accuracy': 0.5855664014816284, 'train/loss': 1.7688456773757935, 'validation/accuracy': 0.5272600054740906, 'validation/loss': 2.0590996742248535, 'validation/num_examples': 50000, 'test/accuracy': 0.41620001196861267, 'test/loss': 2.7257065773010254, 'test/num_examples': 10000, 'score': 13485.352035045624, 'total_duration': 15193.418963193893, 'accumulated_submission_time': 13485.352035045624, 'accumulated_eval_time': 1705.023080587387, 'accumulated_logging_time': 1.392759084701538}
I0207 19:00:30.495272 139946414638848 logging_writer.py:48] [28795] accumulated_eval_time=1705.023081, accumulated_logging_time=1.392759, accumulated_submission_time=13485.352035, global_step=28795, preemption_count=0, score=13485.352035, test/accuracy=0.416200, test/loss=2.725707, test/num_examples=10000, total_duration=15193.418963, train/accuracy=0.585566, train/loss=1.768846, validation/accuracy=0.527260, validation/loss=2.059100, validation/num_examples=50000
I0207 19:00:32.964103 139946397853440 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.1516987085342407, loss=4.644382953643799
I0207 19:01:16.626438 139946414638848 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.072430968284607, loss=2.8749349117279053
I0207 19:02:03.590334 139946397853440 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.9110823273658752, loss=3.5216779708862305
I0207 19:02:50.843647 139946414638848 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9340860247612, loss=3.839515209197998
I0207 19:03:37.933394 139946397853440 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.770396888256073, loss=5.075728416442871
I0207 19:04:25.261958 139946414638848 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.365649700164795, loss=3.00968074798584
I0207 19:05:12.764079 139946397853440 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.004462480545044, loss=3.2015342712402344
I0207 19:05:59.834357 139946414638848 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9507962465286255, loss=4.60718297958374
I0207 19:06:47.040313 139946397853440 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.1342700719833374, loss=2.96354603767395
I0207 19:07:30.700573 140107197974336 spec.py:321] Evaluating on the training split.
I0207 19:07:41.678953 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 19:08:21.695025 140107197974336 spec.py:349] Evaluating on the test split.
I0207 19:08:23.296174 140107197974336 submission_runner.py:408] Time since start: 15666.24s, 	Step: 29694, 	{'train/accuracy': 0.5672265291213989, 'train/loss': 1.8366743326187134, 'validation/accuracy': 0.526699960231781, 'validation/loss': 2.0453226566314697, 'validation/num_examples': 50000, 'test/accuracy': 0.4116000235080719, 'test/loss': 2.7133548259735107, 'test/num_examples': 10000, 'score': 13905.495992660522, 'total_duration': 15666.241523504257, 'accumulated_submission_time': 13905.495992660522, 'accumulated_eval_time': 1757.6186830997467, 'accumulated_logging_time': 1.4242591857910156}
I0207 19:08:23.317491 139946414638848 logging_writer.py:48] [29694] accumulated_eval_time=1757.618683, accumulated_logging_time=1.424259, accumulated_submission_time=13905.495993, global_step=29694, preemption_count=0, score=13905.495993, test/accuracy=0.411600, test/loss=2.713355, test/num_examples=10000, total_duration=15666.241524, train/accuracy=0.567227, train/loss=1.836674, validation/accuracy=0.526700, validation/loss=2.045323, validation/num_examples=50000
I0207 19:08:26.205772 139946397853440 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.0719128847122192, loss=2.9937498569488525
I0207 19:09:09.878158 139946414638848 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.30061674118042, loss=2.905777931213379
I0207 19:09:56.732292 139946397853440 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.9387444853782654, loss=4.53102970123291
I0207 19:10:43.736330 139946414638848 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9936628937721252, loss=3.3764290809631348
I0207 19:11:31.025245 139946397853440 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.1021325588226318, loss=3.0158634185791016
I0207 19:12:18.271600 139946414638848 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.9581442475318909, loss=3.697378158569336
I0207 19:13:05.387456 139946397853440 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.8490957021713257, loss=5.328362941741943
I0207 19:13:52.516756 139946414638848 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.3621717691421509, loss=2.8530538082122803
I0207 19:14:39.870912 139946397853440 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.8017171025276184, loss=5.405603885650635
I0207 19:15:23.726049 140107197974336 spec.py:321] Evaluating on the training split.
I0207 19:15:34.824012 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 19:16:14.142726 140107197974336 spec.py:349] Evaluating on the test split.
I0207 19:16:15.747229 140107197974336 submission_runner.py:408] Time since start: 16138.69s, 	Step: 30595, 	{'train/accuracy': 0.5782226324081421, 'train/loss': 1.7867298126220703, 'validation/accuracy': 0.5320000052452087, 'validation/loss': 2.0195980072021484, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.6812961101531982, 'test/num_examples': 10000, 'score': 14325.843662500381, 'total_duration': 16138.692564725876, 'accumulated_submission_time': 14325.843662500381, 'accumulated_eval_time': 1809.6398491859436, 'accumulated_logging_time': 1.4553961753845215}
I0207 19:16:15.769018 139946414638848 logging_writer.py:48] [30595] accumulated_eval_time=1809.639849, accumulated_logging_time=1.455396, accumulated_submission_time=14325.843663, global_step=30595, preemption_count=0, score=14325.843663, test/accuracy=0.423400, test/loss=2.681296, test/num_examples=10000, total_duration=16138.692565, train/accuracy=0.578223, train/loss=1.786730, validation/accuracy=0.532000, validation/loss=2.019598, validation/num_examples=50000
I0207 19:16:18.229970 139946397853440 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.1752852201461792, loss=2.9200494289398193
I0207 19:17:02.001041 139946414638848 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.8598543405532837, loss=4.645158767700195
I0207 19:17:48.901700 139946397853440 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.1795294284820557, loss=2.9830684661865234
I0207 19:18:36.236923 139946414638848 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0007522106170654, loss=3.6200315952301025
I0207 19:19:23.430423 139946397853440 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8304036259651184, loss=5.0355119705200195
I0207 19:20:10.585011 139946414638848 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.064188838005066, loss=3.4223806858062744
I0207 19:20:57.717433 139946397853440 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.1232420206069946, loss=2.74373197555542
I0207 19:21:44.845903 139946414638848 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.0905557870864868, loss=5.4965434074401855
I0207 19:22:32.131762 139946397853440 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.9996398091316223, loss=3.5470657348632812
I0207 19:23:15.769600 140107197974336 spec.py:321] Evaluating on the training split.
I0207 19:23:26.838133 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 19:24:05.786799 140107197974336 spec.py:349] Evaluating on the test split.
I0207 19:24:07.385144 140107197974336 submission_runner.py:408] Time since start: 16610.33s, 	Step: 31494, 	{'train/accuracy': 0.5876171588897705, 'train/loss': 1.7332189083099365, 'validation/accuracy': 0.5371800065040588, 'validation/loss': 1.9984508752822876, 'validation/num_examples': 50000, 'test/accuracy': 0.4247000217437744, 'test/loss': 2.6817445755004883, 'test/num_examples': 10000, 'score': 14745.781195640564, 'total_duration': 16610.330486536026, 'accumulated_submission_time': 14745.781195640564, 'accumulated_eval_time': 1861.2554004192352, 'accumulated_logging_time': 1.4885175228118896}
I0207 19:24:07.408968 139946414638848 logging_writer.py:48] [31494] accumulated_eval_time=1861.255400, accumulated_logging_time=1.488518, accumulated_submission_time=14745.781196, global_step=31494, preemption_count=0, score=14745.781196, test/accuracy=0.424700, test/loss=2.681745, test/num_examples=10000, total_duration=16610.330487, train/accuracy=0.587617, train/loss=1.733219, validation/accuracy=0.537180, validation/loss=1.998451, validation/num_examples=50000
I0207 19:24:10.284341 139946397853440 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.045322060585022, loss=3.2179360389709473
I0207 19:24:53.849645 139946414638848 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.3686054944992065, loss=2.92120361328125
I0207 19:25:40.587915 139946397853440 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.8658943176269531, loss=4.579705238342285
I0207 19:26:27.670046 139946414638848 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.0773255825042725, loss=3.284882068634033
I0207 19:27:14.792515 139946397853440 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.9716240763664246, loss=3.4603304862976074
I0207 19:28:01.457011 139946414638848 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.1921073198318481, loss=2.9573235511779785
I0207 19:28:48.629063 139946397853440 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.9244308471679688, loss=3.8269145488739014
I0207 19:29:35.749395 139946414638848 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.3523045778274536, loss=2.874162197113037
I0207 19:30:22.955244 139946397853440 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.1451382637023926, loss=2.843432664871216
I0207 19:31:07.602292 140107197974336 spec.py:321] Evaluating on the training split.
I0207 19:31:18.558445 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 19:31:57.362126 140107197974336 spec.py:349] Evaluating on the test split.
I0207 19:31:58.954299 140107197974336 submission_runner.py:408] Time since start: 17081.90s, 	Step: 32397, 	{'train/accuracy': 0.572265625, 'train/loss': 1.8312280178070068, 'validation/accuracy': 0.5342599749565125, 'validation/loss': 2.031036615371704, 'validation/num_examples': 50000, 'test/accuracy': 0.4207000136375427, 'test/loss': 2.6954081058502197, 'test/num_examples': 10000, 'score': 15165.913511514664, 'total_duration': 17081.899649858475, 'accumulated_submission_time': 15165.913511514664, 'accumulated_eval_time': 1912.6074118614197, 'accumulated_logging_time': 1.521423101425171}
I0207 19:31:58.980883 139946414638848 logging_writer.py:48] [32397] accumulated_eval_time=1912.607412, accumulated_logging_time=1.521423, accumulated_submission_time=15165.913512, global_step=32397, preemption_count=0, score=15165.913512, test/accuracy=0.420700, test/loss=2.695408, test/num_examples=10000, total_duration=17081.899650, train/accuracy=0.572266, train/loss=1.831228, validation/accuracy=0.534260, validation/loss=2.031037, validation/num_examples=50000
I0207 19:32:00.623509 139946397853440 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.9592000842094421, loss=3.7344226837158203
I0207 19:32:43.919126 139946414638848 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.0758552551269531, loss=2.75346040725708
I0207 19:33:31.043537 139946397853440 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0490995645523071, loss=3.3732800483703613
I0207 19:34:18.052126 139946414638848 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9735098481178284, loss=4.119600772857666
I0207 19:35:05.530223 139946397853440 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.997562825679779, loss=5.248673439025879
I0207 19:35:52.739843 139946414638848 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.9468930959701538, loss=4.774899959564209
I0207 19:36:39.823193 139946397853440 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.2382378578186035, loss=2.839482069015503
I0207 19:37:26.837215 139946414638848 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.2791517972946167, loss=3.0889949798583984
I0207 19:38:13.944474 139946397853440 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.0417718887329102, loss=3.391481637954712
I0207 19:38:59.010048 140107197974336 spec.py:321] Evaluating on the training split.
I0207 19:39:10.228909 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 19:39:49.217461 140107197974336 spec.py:349] Evaluating on the test split.
I0207 19:39:50.823084 140107197974336 submission_runner.py:408] Time since start: 17553.77s, 	Step: 33298, 	{'train/accuracy': 0.5862890481948853, 'train/loss': 1.739125370979309, 'validation/accuracy': 0.5410199761390686, 'validation/loss': 1.962300419807434, 'validation/num_examples': 50000, 'test/accuracy': 0.4212000072002411, 'test/loss': 2.652564764022827, 'test/num_examples': 10000, 'score': 15585.88121342659, 'total_duration': 17553.76842737198, 'accumulated_submission_time': 15585.88121342659, 'accumulated_eval_time': 1964.4204487800598, 'accumulated_logging_time': 1.557866096496582}
I0207 19:39:50.844886 139946414638848 logging_writer.py:48] [33298] accumulated_eval_time=1964.420449, accumulated_logging_time=1.557866, accumulated_submission_time=15585.881213, global_step=33298, preemption_count=0, score=15585.881213, test/accuracy=0.421200, test/loss=2.652565, test/num_examples=10000, total_duration=17553.768427, train/accuracy=0.586289, train/loss=1.739125, validation/accuracy=0.541020, validation/loss=1.962300, validation/num_examples=50000
I0207 19:39:52.086421 139946397853440 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.2553539276123047, loss=5.547767162322998
I0207 19:40:35.046894 139946414638848 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.8475643992424011, loss=4.838603973388672
I0207 19:41:21.513751 139946397853440 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.235873818397522, loss=2.8938846588134766
I0207 19:42:08.750449 139946414638848 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.9213658571243286, loss=5.172557830810547
I0207 19:42:55.607869 139946397853440 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.078534483909607, loss=2.8855786323547363
I0207 19:43:42.761178 139946414638848 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.9314329624176025, loss=3.9122982025146484
I0207 19:44:29.682656 139946397853440 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.9159777164459229, loss=4.5454816818237305
I0207 19:45:16.613494 139946414638848 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.9386526346206665, loss=4.752277374267578
I0207 19:46:03.419251 139946397853440 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.2720450162887573, loss=2.8937199115753174
I0207 19:46:50.199705 139946414638848 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.0147747993469238, loss=3.3951809406280518
I0207 19:46:51.201893 140107197974336 spec.py:321] Evaluating on the training split.
I0207 19:47:02.254948 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 19:47:40.953588 140107197974336 spec.py:349] Evaluating on the test split.
I0207 19:47:42.554163 140107197974336 submission_runner.py:408] Time since start: 18025.50s, 	Step: 34204, 	{'train/accuracy': 0.5970116853713989, 'train/loss': 1.712268590927124, 'validation/accuracy': 0.5445600152015686, 'validation/loss': 1.9671906232833862, 'validation/num_examples': 50000, 'test/accuracy': 0.4263000190258026, 'test/loss': 2.6361184120178223, 'test/num_examples': 10000, 'score': 16006.172748804092, 'total_duration': 18025.4995098114, 'accumulated_submission_time': 16006.172748804092, 'accumulated_eval_time': 2015.7727072238922, 'accumulated_logging_time': 1.5926804542541504}
I0207 19:47:42.574908 139946397853440 logging_writer.py:48] [34204] accumulated_eval_time=2015.772707, accumulated_logging_time=1.592680, accumulated_submission_time=16006.172749, global_step=34204, preemption_count=0, score=16006.172749, test/accuracy=0.426300, test/loss=2.636118, test/num_examples=10000, total_duration=18025.499510, train/accuracy=0.597012, train/loss=1.712269, validation/accuracy=0.544560, validation/loss=1.967191, validation/num_examples=50000
I0207 19:48:24.221097 139946414638848 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.0613577365875244, loss=2.933427572250366
I0207 19:49:10.984851 139946397853440 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.3308130502700806, loss=2.8162763118743896
I0207 19:49:58.156723 139946414638848 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.1524025201797485, loss=2.7991175651550293
I0207 19:50:45.243606 139946397853440 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.9279033541679382, loss=4.49055814743042
I0207 19:51:32.237131 139946414638848 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.8663418292999268, loss=5.450370788574219
I0207 19:52:19.568573 139946397853440 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.0599796772003174, loss=2.976527214050293
I0207 19:53:06.901735 139946414638848 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.9209782481193542, loss=3.9170870780944824
I0207 19:53:53.953318 139946397853440 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.936245322227478, loss=4.501729965209961
I0207 19:54:41.281676 139946414638848 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.2785789966583252, loss=2.914994478225708
I0207 19:54:42.826511 140107197974336 spec.py:321] Evaluating on the training split.
I0207 19:54:53.780535 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 19:55:31.566375 140107197974336 spec.py:349] Evaluating on the test split.
I0207 19:55:33.160810 140107197974336 submission_runner.py:408] Time since start: 18496.11s, 	Step: 35105, 	{'train/accuracy': 0.5822460651397705, 'train/loss': 1.7583211660385132, 'validation/accuracy': 0.5433599948883057, 'validation/loss': 1.9646700620651245, 'validation/num_examples': 50000, 'test/accuracy': 0.42740002274513245, 'test/loss': 2.626077175140381, 'test/num_examples': 10000, 'score': 16426.362616062164, 'total_duration': 18496.106160640717, 'accumulated_submission_time': 16426.362616062164, 'accumulated_eval_time': 2066.1069979667664, 'accumulated_logging_time': 1.6230809688568115}
I0207 19:55:33.184672 139946397853440 logging_writer.py:48] [35105] accumulated_eval_time=2066.106998, accumulated_logging_time=1.623081, accumulated_submission_time=16426.362616, global_step=35105, preemption_count=0, score=16426.362616, test/accuracy=0.427400, test/loss=2.626077, test/num_examples=10000, total_duration=18496.106161, train/accuracy=0.582246, train/loss=1.758321, validation/accuracy=0.543360, validation/loss=1.964670, validation/num_examples=50000
I0207 19:56:14.558216 139946414638848 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.0885471105575562, loss=2.9375061988830566
I0207 19:57:01.405004 139946397853440 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.030527949333191, loss=3.40944766998291
I0207 19:57:48.344131 139946414638848 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.075474500656128, loss=3.319086790084839
I0207 19:58:35.681106 139946397853440 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.0969983339309692, loss=4.026483058929443
I0207 19:59:22.871494 139946414638848 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.9297299981117249, loss=4.460563659667969
I0207 20:00:10.024420 139946397853440 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.374994158744812, loss=2.830824851989746
I0207 20:00:56.872575 139946414638848 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.151949167251587, loss=3.0363683700561523
I0207 20:01:44.115751 139946397853440 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.9784659743309021, loss=4.091699123382568
I0207 20:02:31.350874 139946414638848 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.1775225400924683, loss=2.9331881999969482
I0207 20:02:33.434080 140107197974336 spec.py:321] Evaluating on the training split.
I0207 20:02:44.451295 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 20:03:24.120975 140107197974336 spec.py:349] Evaluating on the test split.
I0207 20:03:25.735332 140107197974336 submission_runner.py:408] Time since start: 18968.68s, 	Step: 36006, 	{'train/accuracy': 0.5903906226158142, 'train/loss': 1.75039541721344, 'validation/accuracy': 0.546999990940094, 'validation/loss': 1.9551115036010742, 'validation/num_examples': 50000, 'test/accuracy': 0.43310001492500305, 'test/loss': 2.605111837387085, 'test/num_examples': 10000, 'score': 16846.550374031067, 'total_duration': 18968.680662870407, 'accumulated_submission_time': 16846.550374031067, 'accumulated_eval_time': 2118.408214569092, 'accumulated_logging_time': 1.6574127674102783}
I0207 20:03:25.760621 139946397853440 logging_writer.py:48] [36006] accumulated_eval_time=2118.408215, accumulated_logging_time=1.657413, accumulated_submission_time=16846.550374, global_step=36006, preemption_count=0, score=16846.550374, test/accuracy=0.433100, test/loss=2.605112, test/num_examples=10000, total_duration=18968.680663, train/accuracy=0.590391, train/loss=1.750395, validation/accuracy=0.547000, validation/loss=1.955112, validation/num_examples=50000
I0207 20:04:06.748288 139946414638848 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.9471927881240845, loss=5.378556728363037
I0207 20:04:53.766546 139946397853440 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.1845980882644653, loss=3.0612852573394775
I0207 20:05:41.272928 139946414638848 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.3205684423446655, loss=2.683464527130127
I0207 20:06:28.460548 139946397853440 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.3222311735153198, loss=2.854905605316162
I0207 20:07:15.617028 139946414638848 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.9338833689689636, loss=3.735288619995117
I0207 20:08:03.090099 139946397853440 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.1806493997573853, loss=2.818167209625244
I0207 20:08:50.148844 139946414638848 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.0229440927505493, loss=3.282198190689087
I0207 20:09:37.485159 139946397853440 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.213365912437439, loss=2.974985361099243
I0207 20:10:24.457434 139946414638848 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.2060362100601196, loss=2.8108320236206055
I0207 20:10:25.998158 140107197974336 spec.py:321] Evaluating on the training split.
I0207 20:10:37.215732 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 20:11:15.978276 140107197974336 spec.py:349] Evaluating on the test split.
I0207 20:11:17.586418 140107197974336 submission_runner.py:408] Time since start: 19440.53s, 	Step: 36905, 	{'train/accuracy': 0.5983593463897705, 'train/loss': 1.7371487617492676, 'validation/accuracy': 0.548259973526001, 'validation/loss': 1.9736733436584473, 'validation/num_examples': 50000, 'test/accuracy': 0.43470001220703125, 'test/loss': 2.613460063934326, 'test/num_examples': 10000, 'score': 17266.725713968277, 'total_duration': 19440.53175663948, 'accumulated_submission_time': 17266.725713968277, 'accumulated_eval_time': 2169.996456384659, 'accumulated_logging_time': 1.6937315464019775}
I0207 20:11:17.611304 139946397853440 logging_writer.py:48] [36905] accumulated_eval_time=2169.996456, accumulated_logging_time=1.693732, accumulated_submission_time=17266.725714, global_step=36905, preemption_count=0, score=17266.725714, test/accuracy=0.434700, test/loss=2.613460, test/num_examples=10000, total_duration=19440.531757, train/accuracy=0.598359, train/loss=1.737149, validation/accuracy=0.548260, validation/loss=1.973673, validation/num_examples=50000
I0207 20:11:58.890344 139946414638848 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0821608304977417, loss=4.998635292053223
I0207 20:12:45.725121 139946397853440 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.859811007976532, loss=4.679815292358398
I0207 20:13:32.737073 139946414638848 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.1845057010650635, loss=3.215463638305664
I0207 20:14:19.954235 139946397853440 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.2007673978805542, loss=2.753997325897217
I0207 20:15:07.194552 139946414638848 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.2236689329147339, loss=2.820852518081665
I0207 20:15:54.340683 139946397853440 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.9541386961936951, loss=3.574875831604004
I0207 20:16:41.541742 139946414638848 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.2028552293777466, loss=2.7944092750549316
I0207 20:17:28.493697 139946397853440 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.093894362449646, loss=2.833116054534912
I0207 20:18:15.548847 139946414638848 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.9214983582496643, loss=5.032599449157715
I0207 20:18:17.609991 140107197974336 spec.py:321] Evaluating on the training split.
I0207 20:18:28.572871 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 20:19:06.866873 140107197974336 spec.py:349] Evaluating on the test split.
I0207 20:19:08.463181 140107197974336 submission_runner.py:408] Time since start: 19911.41s, 	Step: 37806, 	{'train/accuracy': 0.5934765338897705, 'train/loss': 1.7180927991867065, 'validation/accuracy': 0.549560010433197, 'validation/loss': 1.9441777467727661, 'validation/num_examples': 50000, 'test/accuracy': 0.43300002813339233, 'test/loss': 2.5955758094787598, 'test/num_examples': 10000, 'score': 17686.66410136223, 'total_duration': 19911.408517360687, 'accumulated_submission_time': 17686.66410136223, 'accumulated_eval_time': 2220.849625825882, 'accumulated_logging_time': 1.7289478778839111}
I0207 20:19:08.489222 139946397853440 logging_writer.py:48] [37806] accumulated_eval_time=2220.849626, accumulated_logging_time=1.728948, accumulated_submission_time=17686.664101, global_step=37806, preemption_count=0, score=17686.664101, test/accuracy=0.433000, test/loss=2.595576, test/num_examples=10000, total_duration=19911.408517, train/accuracy=0.593477, train/loss=1.718093, validation/accuracy=0.549560, validation/loss=1.944178, validation/num_examples=50000
I0207 20:19:49.334396 139946414638848 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.093432903289795, loss=2.7873849868774414
I0207 20:20:36.149804 139946397853440 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.1540285348892212, loss=2.7305285930633545
I0207 20:21:23.242764 139946414638848 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.334722638130188, loss=2.6705479621887207
I0207 20:22:10.337795 139946397853440 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.1415631771087646, loss=2.6955506801605225
I0207 20:22:57.330552 139946414638848 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.8591643571853638, loss=4.855476379394531
I0207 20:23:44.726296 139946397853440 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.3110710382461548, loss=3.0343222618103027
I0207 20:24:31.860688 139946414638848 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.2030802965164185, loss=2.8570637702941895
I0207 20:25:19.088099 139946397853440 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.0322275161743164, loss=3.07656192779541
I0207 20:26:06.487720 139946414638848 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.0163016319274902, loss=3.2963316440582275
I0207 20:26:08.537580 140107197974336 spec.py:321] Evaluating on the training split.
I0207 20:26:19.941614 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 20:26:58.204544 140107197974336 spec.py:349] Evaluating on the test split.
I0207 20:26:59.806964 140107197974336 submission_runner.py:408] Time since start: 20382.75s, 	Step: 38706, 	{'train/accuracy': 0.5957421660423279, 'train/loss': 1.7233281135559082, 'validation/accuracy': 0.5529400110244751, 'validation/loss': 1.9345680475234985, 'validation/num_examples': 50000, 'test/accuracy': 0.4406000077724457, 'test/loss': 2.6032638549804688, 'test/num_examples': 10000, 'score': 18106.65107870102, 'total_duration': 20382.752317667007, 'accumulated_submission_time': 18106.65107870102, 'accumulated_eval_time': 2272.119005203247, 'accumulated_logging_time': 1.765486717224121}
I0207 20:26:59.832457 139946397853440 logging_writer.py:48] [38706] accumulated_eval_time=2272.119005, accumulated_logging_time=1.765487, accumulated_submission_time=18106.651079, global_step=38706, preemption_count=0, score=18106.651079, test/accuracy=0.440600, test/loss=2.603264, test/num_examples=10000, total_duration=20382.752318, train/accuracy=0.595742, train/loss=1.723328, validation/accuracy=0.552940, validation/loss=1.934568, validation/num_examples=50000
I0207 20:27:40.536592 139946414638848 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.1394166946411133, loss=2.7893459796905518
I0207 20:28:27.230141 139946397853440 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.8347589373588562, loss=5.2569966316223145
I0207 20:29:14.326144 139946414638848 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1613401174545288, loss=4.770230293273926
I0207 20:30:01.182444 139946397853440 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.1785609722137451, loss=3.031248092651367
I0207 20:30:48.206515 139946414638848 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.068413257598877, loss=3.2950589656829834
I0207 20:31:35.451193 139946397853440 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.1815203428268433, loss=2.8357176780700684
I0207 20:32:22.483203 139946414638848 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9383283853530884, loss=3.7555201053619385
I0207 20:33:09.398183 139946397853440 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.2521482706069946, loss=2.9289798736572266
I0207 20:33:56.183295 139946414638848 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.2456809282302856, loss=2.6649155616760254
I0207 20:34:00.145819 140107197974336 spec.py:321] Evaluating on the training split.
I0207 20:34:11.442760 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 20:34:51.543643 140107197974336 spec.py:349] Evaluating on the test split.
I0207 20:34:53.150020 140107197974336 submission_runner.py:408] Time since start: 20856.10s, 	Step: 39610, 	{'train/accuracy': 0.6048241853713989, 'train/loss': 1.6466890573501587, 'validation/accuracy': 0.558139979839325, 'validation/loss': 1.8898195028305054, 'validation/num_examples': 50000, 'test/accuracy': 0.4489000141620636, 'test/loss': 2.53645920753479, 'test/num_examples': 10000, 'score': 18526.90128660202, 'total_duration': 20856.095355033875, 'accumulated_submission_time': 18526.90128660202, 'accumulated_eval_time': 2325.1231849193573, 'accumulated_logging_time': 1.80271577835083}
I0207 20:34:53.180120 139946397853440 logging_writer.py:48] [39610] accumulated_eval_time=2325.123185, accumulated_logging_time=1.802716, accumulated_submission_time=18526.901287, global_step=39610, preemption_count=0, score=18526.901287, test/accuracy=0.448900, test/loss=2.536459, test/num_examples=10000, total_duration=20856.095355, train/accuracy=0.604824, train/loss=1.646689, validation/accuracy=0.558140, validation/loss=1.889820, validation/num_examples=50000
I0207 20:35:32.052856 139946414638848 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.2554417848587036, loss=2.703423261642456
I0207 20:36:19.446588 139946397853440 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.0539900064468384, loss=3.4698102474212646
I0207 20:37:06.225050 139946414638848 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.2246310710906982, loss=2.711698055267334
I0207 20:37:53.067822 139946397853440 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.025117039680481, loss=3.0457887649536133
I0207 20:38:40.352989 139946414638848 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.2360860109329224, loss=2.8228774070739746
I0207 20:39:27.448083 139946397853440 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.2808973789215088, loss=2.7582240104675293
I0207 20:40:14.655543 139946414638848 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.1249645948410034, loss=3.014143705368042
I0207 20:41:01.988130 139946397853440 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.205155372619629, loss=2.6329236030578613
I0207 20:41:49.118220 139946414638848 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.0285637378692627, loss=3.436311960220337
I0207 20:41:53.480476 140107197974336 spec.py:321] Evaluating on the training split.
I0207 20:42:04.667832 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 20:42:43.481441 140107197974336 spec.py:349] Evaluating on the test split.
I0207 20:42:45.083783 140107197974336 submission_runner.py:408] Time since start: 21328.03s, 	Step: 40511, 	{'train/accuracy': 0.6100000143051147, 'train/loss': 1.6908632516860962, 'validation/accuracy': 0.5554800033569336, 'validation/loss': 1.9389760494232178, 'validation/num_examples': 50000, 'test/accuracy': 0.43950003385543823, 'test/loss': 2.585554361343384, 'test/num_examples': 10000, 'score': 18947.140549182892, 'total_duration': 21328.02912712097, 'accumulated_submission_time': 18947.140549182892, 'accumulated_eval_time': 2376.72646856308, 'accumulated_logging_time': 1.8429145812988281}
I0207 20:42:45.106085 139946397853440 logging_writer.py:48] [40511] accumulated_eval_time=2376.726469, accumulated_logging_time=1.842915, accumulated_submission_time=18947.140549, global_step=40511, preemption_count=0, score=18947.140549, test/accuracy=0.439500, test/loss=2.585554, test/num_examples=10000, total_duration=21328.029127, train/accuracy=0.610000, train/loss=1.690863, validation/accuracy=0.555480, validation/loss=1.938976, validation/num_examples=50000
I0207 20:43:23.495903 139946414638848 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.8612350225448608, loss=5.306346416473389
I0207 20:44:10.427351 139946397853440 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.0631465911865234, loss=2.7876970767974854
I0207 20:44:57.694240 139946414638848 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.9065887331962585, loss=5.046290397644043
I0207 20:45:44.698704 139946397853440 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.0462487936019897, loss=3.4487967491149902
I0207 20:46:31.714793 139946414638848 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.9116227030754089, loss=3.998187303543091
I0207 20:47:18.974855 139946397853440 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1280937194824219, loss=5.134641170501709
I0207 20:48:06.296275 139946414638848 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.201603651046753, loss=2.753647804260254
I0207 20:48:53.408765 139946397853440 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.2784639596939087, loss=2.7322962284088135
I0207 20:49:40.504158 139946414638848 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.9629541039466858, loss=5.277754783630371
I0207 20:49:45.398888 140107197974336 spec.py:321] Evaluating on the training split.
I0207 20:49:56.368221 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 20:50:34.646396 140107197974336 spec.py:349] Evaluating on the test split.
I0207 20:50:36.248488 140107197974336 submission_runner.py:408] Time since start: 21799.19s, 	Step: 41412, 	{'train/accuracy': 0.6074023246765137, 'train/loss': 1.6232702732086182, 'validation/accuracy': 0.5635200142860413, 'validation/loss': 1.8514550924301147, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.5395474433898926, 'test/num_examples': 10000, 'score': 19367.371527671814, 'total_duration': 21799.19383573532, 'accumulated_submission_time': 19367.371527671814, 'accumulated_eval_time': 2427.576056241989, 'accumulated_logging_time': 1.8747284412384033}
I0207 20:50:36.272984 139946397853440 logging_writer.py:48] [41412] accumulated_eval_time=2427.576056, accumulated_logging_time=1.874728, accumulated_submission_time=19367.371528, global_step=41412, preemption_count=0, score=19367.371528, test/accuracy=0.446200, test/loss=2.539547, test/num_examples=10000, total_duration=21799.193836, train/accuracy=0.607402, train/loss=1.623270, validation/accuracy=0.563520, validation/loss=1.851455, validation/num_examples=50000
I0207 20:51:14.315953 139946414638848 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.910711407661438, loss=3.7219834327697754
I0207 20:52:01.075760 139946397853440 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.1518027782440186, loss=2.8119335174560547
I0207 20:52:48.252434 139946414638848 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.946563184261322, loss=4.021805286407471
I0207 20:53:35.584420 139946397853440 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.1174036264419556, loss=2.777174234390259
I0207 20:54:22.637270 139946414638848 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.1313586235046387, loss=2.991086006164551
I0207 20:55:09.977609 139946397853440 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.3863351345062256, loss=2.8472633361816406
I0207 20:55:56.972647 139946414638848 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.035323977470398, loss=3.7574496269226074
I0207 20:56:44.014989 139946397853440 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1317229270935059, loss=2.879121780395508
I0207 20:57:31.131513 139946414638848 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.0976439714431763, loss=4.34678316116333
I0207 20:57:36.423267 140107197974336 spec.py:321] Evaluating on the training split.
I0207 20:57:47.421394 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 20:58:29.678488 140107197974336 spec.py:349] Evaluating on the test split.
I0207 20:58:31.288063 140107197974336 submission_runner.py:408] Time since start: 22274.23s, 	Step: 42313, 	{'train/accuracy': 0.610644519329071, 'train/loss': 1.6274819374084473, 'validation/accuracy': 0.561519980430603, 'validation/loss': 1.8759976625442505, 'validation/num_examples': 50000, 'test/accuracy': 0.4504000246524811, 'test/loss': 2.53519868850708, 'test/num_examples': 10000, 'score': 19787.460819482803, 'total_duration': 22274.2334086895, 'accumulated_submission_time': 19787.460819482803, 'accumulated_eval_time': 2482.4408757686615, 'accumulated_logging_time': 1.907930850982666}
I0207 20:58:31.311487 139946397853440 logging_writer.py:48] [42313] accumulated_eval_time=2482.440876, accumulated_logging_time=1.907931, accumulated_submission_time=19787.460819, global_step=42313, preemption_count=0, score=19787.460819, test/accuracy=0.450400, test/loss=2.535199, test/num_examples=10000, total_duration=22274.233409, train/accuracy=0.610645, train/loss=1.627482, validation/accuracy=0.561520, validation/loss=1.875998, validation/num_examples=50000
I0207 20:59:08.759370 139946414638848 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.0492024421691895, loss=5.05057954788208
I0207 20:59:55.303361 139946397853440 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.1159026622772217, loss=4.342532634735107
I0207 21:00:42.497465 139946414638848 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.178063988685608, loss=2.7991926670074463
I0207 21:01:29.455056 139946397853440 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.0902442932128906, loss=3.1404247283935547
I0207 21:02:16.366909 139946414638848 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.2783777713775635, loss=2.7228360176086426
I0207 21:03:03.511885 139946397853440 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.9561731815338135, loss=5.112039566040039
I0207 21:03:50.549139 139946414638848 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9327373504638672, loss=5.304813861846924
I0207 21:04:37.626199 139946397853440 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.027855396270752, loss=3.5389912128448486
I0207 21:05:24.561450 139946414638848 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.0567423105239868, loss=3.6180622577667236
I0207 21:05:31.298194 140107197974336 spec.py:321] Evaluating on the training split.
I0207 21:05:42.339702 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 21:06:23.602946 140107197974336 spec.py:349] Evaluating on the test split.
I0207 21:06:25.205750 140107197974336 submission_runner.py:408] Time since start: 22748.15s, 	Step: 43216, 	{'train/accuracy': 0.6401757597923279, 'train/loss': 1.5094099044799805, 'validation/accuracy': 0.5685200095176697, 'validation/loss': 1.8535585403442383, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.5047597885131836, 'test/num_examples': 10000, 'score': 20207.386869430542, 'total_duration': 22748.151103019714, 'accumulated_submission_time': 20207.386869430542, 'accumulated_eval_time': 2536.3484270572662, 'accumulated_logging_time': 1.941042184829712}
I0207 21:06:25.228347 139946397853440 logging_writer.py:48] [43216] accumulated_eval_time=2536.348427, accumulated_logging_time=1.941042, accumulated_submission_time=20207.386869, global_step=43216, preemption_count=0, score=20207.386869, test/accuracy=0.453300, test/loss=2.504760, test/num_examples=10000, total_duration=22748.151103, train/accuracy=0.640176, train/loss=1.509410, validation/accuracy=0.568520, validation/loss=1.853559, validation/num_examples=50000
I0207 21:07:01.373173 139946414638848 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.964303731918335, loss=4.630033493041992
I0207 21:07:48.104795 139946397853440 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.1259231567382812, loss=2.7223048210144043
I0207 21:08:35.311054 139946414638848 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.1994643211364746, loss=2.6470108032226562
I0207 21:09:22.517031 139946397853440 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.2044682502746582, loss=2.676198959350586
I0207 21:10:09.679045 139946414638848 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.2410622835159302, loss=2.6930594444274902
I0207 21:10:57.048265 139946397853440 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.1110155582427979, loss=2.6035640239715576
I0207 21:11:44.375498 139946414638848 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.8423663377761841, loss=5.133077144622803
I0207 21:12:31.717062 139946397853440 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.2166070938110352, loss=2.725050926208496
I0207 21:13:18.972874 139946414638848 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.121484398841858, loss=3.0380775928497314
I0207 21:13:25.321851 140107197974336 spec.py:321] Evaluating on the training split.
I0207 21:13:36.454951 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 21:14:17.489748 140107197974336 spec.py:349] Evaluating on the test split.
I0207 21:14:19.085690 140107197974336 submission_runner.py:408] Time since start: 23222.03s, 	Step: 44115, 	{'train/accuracy': 0.615527331829071, 'train/loss': 1.6147336959838867, 'validation/accuracy': 0.5694000124931335, 'validation/loss': 1.8382197618484497, 'validation/num_examples': 50000, 'test/accuracy': 0.44780001044273376, 'test/loss': 2.5037691593170166, 'test/num_examples': 10000, 'score': 20627.419096708298, 'total_duration': 23222.031039714813, 'accumulated_submission_time': 20627.419096708298, 'accumulated_eval_time': 2590.112258195877, 'accumulated_logging_time': 1.9733588695526123}
I0207 21:14:19.109059 139946397853440 logging_writer.py:48] [44115] accumulated_eval_time=2590.112258, accumulated_logging_time=1.973359, accumulated_submission_time=20627.419097, global_step=44115, preemption_count=0, score=20627.419097, test/accuracy=0.447800, test/loss=2.503769, test/num_examples=10000, total_duration=23222.031040, train/accuracy=0.615527, train/loss=1.614734, validation/accuracy=0.569400, validation/loss=1.838220, validation/num_examples=50000
I0207 21:14:55.658499 139946414638848 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.0968626737594604, loss=2.6191940307617188
I0207 21:15:42.367371 139946397853440 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.172255516052246, loss=2.7787411212921143
I0207 21:16:29.645493 139946414638848 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.4428389072418213, loss=2.776059627532959
I0207 21:17:16.623122 139946397853440 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.1398111581802368, loss=4.560720443725586
I0207 21:18:03.618359 139946414638848 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.0188236236572266, loss=4.256463050842285
I0207 21:18:50.975058 139946397853440 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.2052322626113892, loss=2.747563362121582
I0207 21:19:38.223503 139946414638848 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.0702239274978638, loss=3.0186665058135986
I0207 21:20:25.390126 139946397853440 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.6027861833572388, loss=2.8420116901397705
I0207 21:21:12.765325 139946414638848 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.1912310123443604, loss=2.85189151763916
I0207 21:21:19.572643 140107197974336 spec.py:321] Evaluating on the training split.
I0207 21:21:30.796323 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 21:22:10.619798 140107197974336 spec.py:349] Evaluating on the test split.
I0207 21:22:12.225535 140107197974336 submission_runner.py:408] Time since start: 23695.17s, 	Step: 45016, 	{'train/accuracy': 0.6134960651397705, 'train/loss': 1.659540057182312, 'validation/accuracy': 0.5658400058746338, 'validation/loss': 1.892349362373352, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.542588472366333, 'test/num_examples': 10000, 'score': 21047.822548627853, 'total_duration': 23695.17087316513, 'accumulated_submission_time': 21047.822548627853, 'accumulated_eval_time': 2642.76513504982, 'accumulated_logging_time': 2.0060572624206543}
I0207 21:22:12.256468 139946397853440 logging_writer.py:48] [45016] accumulated_eval_time=2642.765135, accumulated_logging_time=2.006057, accumulated_submission_time=21047.822549, global_step=45016, preemption_count=0, score=21047.822549, test/accuracy=0.453700, test/loss=2.542588, test/num_examples=10000, total_duration=23695.170873, train/accuracy=0.613496, train/loss=1.659540, validation/accuracy=0.565840, validation/loss=1.892349, validation/num_examples=50000
I0207 21:22:48.583648 139946414638848 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.9640098810195923, loss=4.475193977355957
I0207 21:23:35.570166 139946397853440 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.2559492588043213, loss=2.7906980514526367
I0207 21:24:22.876908 139946414638848 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.1639200448989868, loss=2.7019479274749756
I0207 21:25:10.163790 139946397853440 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.9703524112701416, loss=3.729750633239746
I0207 21:25:57.198612 139946414638848 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.9668784141540527, loss=3.9788689613342285
I0207 21:26:44.413304 139946397853440 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.1341372728347778, loss=2.4707229137420654
I0207 21:27:31.611576 139946414638848 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.275400161743164, loss=2.783993721008301
I0207 21:28:18.890322 139946397853440 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1568560600280762, loss=2.677433967590332
I0207 21:29:06.362714 139946414638848 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.082632064819336, loss=5.054422378540039
I0207 21:29:12.622550 140107197974336 spec.py:321] Evaluating on the training split.
I0207 21:29:23.680441 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 21:30:01.620416 140107197974336 spec.py:349] Evaluating on the test split.
I0207 21:30:03.223373 140107197974336 submission_runner.py:408] Time since start: 24166.17s, 	Step: 45915, 	{'train/accuracy': 0.6446874737739563, 'train/loss': 1.4981642961502075, 'validation/accuracy': 0.5705599784851074, 'validation/loss': 1.8389016389846802, 'validation/num_examples': 50000, 'test/accuracy': 0.4538000226020813, 'test/loss': 2.495013952255249, 'test/num_examples': 10000, 'score': 21468.1283724308, 'total_duration': 24166.16872239113, 'accumulated_submission_time': 21468.1283724308, 'accumulated_eval_time': 2693.3661007881165, 'accumulated_logging_time': 2.0465316772460938}
I0207 21:30:03.253349 139946397853440 logging_writer.py:48] [45915] accumulated_eval_time=2693.366101, accumulated_logging_time=2.046532, accumulated_submission_time=21468.128372, global_step=45915, preemption_count=0, score=21468.128372, test/accuracy=0.453800, test/loss=2.495014, test/num_examples=10000, total_duration=24166.168722, train/accuracy=0.644687, train/loss=1.498164, validation/accuracy=0.570560, validation/loss=1.838902, validation/num_examples=50000
I0207 21:30:40.056693 139946414638848 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.147870659828186, loss=2.946472644805908
I0207 21:31:27.129194 139946397853440 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.2445060014724731, loss=2.8179574012756348
I0207 21:32:14.469631 139946414638848 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.0197848081588745, loss=3.6508052349090576
I0207 21:33:01.841355 139946397853440 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.1012014150619507, loss=2.947417974472046
I0207 21:33:49.234405 139946414638848 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.19044828414917, loss=2.961162567138672
I0207 21:34:36.913234 139946397853440 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1334937810897827, loss=2.6186020374298096
I0207 21:35:24.270209 139946414638848 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.2153712511062622, loss=2.6674492359161377
I0207 21:36:12.599084 139946397853440 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.9417212605476379, loss=3.5643343925476074
I0207 21:36:59.687423 139946414638848 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.1124966144561768, loss=2.7152557373046875
I0207 21:37:03.261405 140107197974336 spec.py:321] Evaluating on the training split.
I0207 21:37:14.079624 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 21:37:54.880210 140107197974336 spec.py:349] Evaluating on the test split.
I0207 21:37:56.469285 140107197974336 submission_runner.py:408] Time since start: 24639.41s, 	Step: 46809, 	{'train/accuracy': 0.6206445097923279, 'train/loss': 1.6042301654815674, 'validation/accuracy': 0.5744799971580505, 'validation/loss': 1.8185001611709595, 'validation/num_examples': 50000, 'test/accuracy': 0.45580002665519714, 'test/loss': 2.4935083389282227, 'test/num_examples': 10000, 'score': 21888.07623887062, 'total_duration': 24639.414632320404, 'accumulated_submission_time': 21888.07623887062, 'accumulated_eval_time': 2746.573971748352, 'accumulated_logging_time': 2.0856590270996094}
I0207 21:37:56.492434 139946397853440 logging_writer.py:48] [46809] accumulated_eval_time=2746.573972, accumulated_logging_time=2.085659, accumulated_submission_time=21888.076239, global_step=46809, preemption_count=0, score=21888.076239, test/accuracy=0.455800, test/loss=2.493508, test/num_examples=10000, total_duration=24639.414632, train/accuracy=0.620645, train/loss=1.604230, validation/accuracy=0.574480, validation/loss=1.818500, validation/num_examples=50000
I0207 21:38:35.806425 139946414638848 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.0778348445892334, loss=3.518754005432129
I0207 21:39:22.468108 139946397853440 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.9758307933807373, loss=4.528424263000488
I0207 21:40:09.658816 139946414638848 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.028478741645813, loss=3.8310484886169434
I0207 21:40:56.489460 139946397853440 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.146975040435791, loss=2.6457345485687256
I0207 21:41:43.654707 139946414638848 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.1793307065963745, loss=2.753777503967285
I0207 21:42:30.833624 139946397853440 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.2357896566390991, loss=2.5992543697357178
I0207 21:43:18.009035 139946414638848 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.1801822185516357, loss=2.7238903045654297
I0207 21:44:05.327477 139946397853440 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.9999719262123108, loss=3.3153555393218994
I0207 21:44:52.409808 139946414638848 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.9643561840057373, loss=3.7340304851531982
I0207 21:44:56.837966 140107197974336 spec.py:321] Evaluating on the training split.
I0207 21:45:07.806049 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 21:45:48.283581 140107197974336 spec.py:349] Evaluating on the test split.
I0207 21:45:49.883178 140107197974336 submission_runner.py:408] Time since start: 25112.83s, 	Step: 47711, 	{'train/accuracy': 0.6209374666213989, 'train/loss': 1.5577152967453003, 'validation/accuracy': 0.5724999904632568, 'validation/loss': 1.797107458114624, 'validation/num_examples': 50000, 'test/accuracy': 0.4577000141143799, 'test/loss': 2.455000400543213, 'test/num_examples': 10000, 'score': 22308.360613822937, 'total_duration': 25112.828466653824, 'accumulated_submission_time': 22308.360613822937, 'accumulated_eval_time': 2799.619107723236, 'accumulated_logging_time': 2.118630886077881}
I0207 21:45:49.908236 139946397853440 logging_writer.py:48] [47711] accumulated_eval_time=2799.619108, accumulated_logging_time=2.118631, accumulated_submission_time=22308.360614, global_step=47711, preemption_count=0, score=22308.360614, test/accuracy=0.457700, test/loss=2.455000, test/num_examples=10000, total_duration=25112.828467, train/accuracy=0.620937, train/loss=1.557715, validation/accuracy=0.572500, validation/loss=1.797107, validation/num_examples=50000
I0207 21:46:28.150053 139946414638848 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.151376485824585, loss=2.770503282546997
I0207 21:47:14.827957 139946397853440 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.1969223022460938, loss=2.6773741245269775
I0207 21:48:01.728312 139946414638848 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0872477293014526, loss=3.0564498901367188
I0207 21:48:48.842889 139946397853440 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.1235873699188232, loss=2.6749770641326904
I0207 21:49:36.088930 139946414638848 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.1568840742111206, loss=2.856353759765625
I0207 21:50:23.257677 139946397853440 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.3784563541412354, loss=2.696610927581787
I0207 21:51:10.264544 139946414638848 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.241030216217041, loss=2.7123656272888184
I0207 21:51:57.364607 139946397853440 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.971095085144043, loss=4.8529462814331055
I0207 21:52:44.337251 139946414638848 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0216538906097412, loss=3.50644850730896
I0207 21:52:50.087499 140107197974336 spec.py:321] Evaluating on the training split.
I0207 21:53:01.058923 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 21:53:41.648808 140107197974336 spec.py:349] Evaluating on the test split.
I0207 21:53:43.257563 140107197974336 submission_runner.py:408] Time since start: 25586.20s, 	Step: 48614, 	{'train/accuracy': 0.6391406059265137, 'train/loss': 1.4967988729476929, 'validation/accuracy': 0.5768799781799316, 'validation/loss': 1.8095238208770752, 'validation/num_examples': 50000, 'test/accuracy': 0.45670002698898315, 'test/loss': 2.475313901901245, 'test/num_examples': 10000, 'score': 22728.47146344185, 'total_duration': 25586.20291495323, 'accumulated_submission_time': 22728.47146344185, 'accumulated_eval_time': 2852.789181947708, 'accumulated_logging_time': 2.153792381286621}
I0207 21:53:43.286505 139946397853440 logging_writer.py:48] [48614] accumulated_eval_time=2852.789182, accumulated_logging_time=2.153792, accumulated_submission_time=22728.471463, global_step=48614, preemption_count=0, score=22728.471463, test/accuracy=0.456700, test/loss=2.475314, test/num_examples=10000, total_duration=25586.202915, train/accuracy=0.639141, train/loss=1.496799, validation/accuracy=0.576880, validation/loss=1.809524, validation/num_examples=50000
I0207 21:54:20.524192 139946414638848 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.923079252243042, loss=4.602804660797119
I0207 21:55:08.076009 139946397853440 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.2707284688949585, loss=2.672471523284912
I0207 21:55:55.330808 139946414638848 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.5797581672668457, loss=2.710017204284668
I0207 21:56:42.587218 139946397853440 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9723964929580688, loss=5.202456951141357
I0207 21:57:29.987269 139946414638848 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.3191286325454712, loss=2.6651129722595215
I0207 21:58:17.383032 139946397853440 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.1473190784454346, loss=2.7146973609924316
I0207 21:59:04.770883 139946414638848 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9346528053283691, loss=4.001682281494141
I0207 21:59:52.135892 139946397853440 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.1128544807434082, loss=2.6968064308166504
I0207 22:00:39.302436 139946414638848 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1413836479187012, loss=2.699526309967041
I0207 22:00:43.702659 140107197974336 spec.py:321] Evaluating on the training split.
I0207 22:00:55.122597 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 22:01:35.380941 140107197974336 spec.py:349] Evaluating on the test split.
I0207 22:01:36.976535 140107197974336 submission_runner.py:408] Time since start: 26059.92s, 	Step: 49511, 	{'train/accuracy': 0.6273437142372131, 'train/loss': 1.5240154266357422, 'validation/accuracy': 0.5815399885177612, 'validation/loss': 1.7533224821090698, 'validation/num_examples': 50000, 'test/accuracy': 0.4651000201702118, 'test/loss': 2.4164364337921143, 'test/num_examples': 10000, 'score': 23148.824804782867, 'total_duration': 26059.92187833786, 'accumulated_submission_time': 23148.824804782867, 'accumulated_eval_time': 2906.063053369522, 'accumulated_logging_time': 2.1936678886413574}
I0207 22:01:37.000720 139946397853440 logging_writer.py:48] [49511] accumulated_eval_time=2906.063053, accumulated_logging_time=2.193668, accumulated_submission_time=23148.824805, global_step=49511, preemption_count=0, score=23148.824805, test/accuracy=0.465100, test/loss=2.416436, test/num_examples=10000, total_duration=26059.921878, train/accuracy=0.627344, train/loss=1.524015, validation/accuracy=0.581540, validation/loss=1.753322, validation/num_examples=50000
I0207 22:02:15.608893 139946414638848 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.225951910018921, loss=2.8291478157043457
I0207 22:03:02.360091 139946397853440 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.2931437492370605, loss=2.587526321411133
I0207 22:03:49.583853 139946414638848 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.3614932298660278, loss=2.7954506874084473
I0207 22:04:36.829083 139946397853440 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.1366450786590576, loss=5.2412848472595215
I0207 22:05:24.059105 139946414638848 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.9665886759757996, loss=5.158940315246582
I0207 22:06:11.733175 139946397853440 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.2646781206130981, loss=2.647282123565674
I0207 22:06:58.906905 139946414638848 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.156856656074524, loss=2.7628908157348633
I0207 22:07:46.367955 139946397853440 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.3946443796157837, loss=2.628679037094116
I0207 22:08:33.901142 139946414638848 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.9038419723510742, loss=4.836564064025879
I0207 22:08:37.365119 140107197974336 spec.py:321] Evaluating on the training split.
I0207 22:08:48.518923 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 22:09:27.302171 140107197974336 spec.py:349] Evaluating on the test split.
I0207 22:09:28.903450 140107197974336 submission_runner.py:408] Time since start: 26531.85s, 	Step: 50409, 	{'train/accuracy': 0.6357030868530273, 'train/loss': 1.4875028133392334, 'validation/accuracy': 0.5874599814414978, 'validation/loss': 1.7306010723114014, 'validation/num_examples': 50000, 'test/accuracy': 0.4692000150680542, 'test/loss': 2.4118294715881348, 'test/num_examples': 10000, 'score': 23569.12908434868, 'total_duration': 26531.848781347275, 'accumulated_submission_time': 23569.12908434868, 'accumulated_eval_time': 2957.601359605789, 'accumulated_logging_time': 2.2275655269622803}
I0207 22:09:28.927601 139946397853440 logging_writer.py:48] [50409] accumulated_eval_time=2957.601360, accumulated_logging_time=2.227566, accumulated_submission_time=23569.129084, global_step=50409, preemption_count=0, score=23569.129084, test/accuracy=0.469200, test/loss=2.411829, test/num_examples=10000, total_duration=26531.848781, train/accuracy=0.635703, train/loss=1.487503, validation/accuracy=0.587460, validation/loss=1.730601, validation/num_examples=50000
I0207 22:10:08.538882 139946414638848 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.212369680404663, loss=2.581465721130371
I0207 22:10:55.597619 139946397853440 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.2023004293441772, loss=2.6066737174987793
I0207 22:11:42.731706 139946414638848 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.0651973485946655, loss=3.4351329803466797
I0207 22:12:29.800402 139946397853440 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.9449126720428467, loss=4.195023059844971
I0207 22:13:16.909510 139946414638848 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.2390600442886353, loss=2.7662665843963623
I0207 22:14:04.120528 139946397853440 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.3321013450622559, loss=2.777867555618286
I0207 22:14:51.479284 139946414638848 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.0246764421463013, loss=5.334641456604004
I0207 22:15:38.833690 139946397853440 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.1935832500457764, loss=2.6661040782928467
I0207 22:16:26.480590 139946414638848 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.1170018911361694, loss=2.8327126502990723
I0207 22:16:29.008524 140107197974336 spec.py:321] Evaluating on the training split.
I0207 22:16:40.099977 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 22:17:21.708958 140107197974336 spec.py:349] Evaluating on the test split.
I0207 22:17:23.305557 140107197974336 submission_runner.py:408] Time since start: 27006.25s, 	Step: 51307, 	{'train/accuracy': 0.6474804282188416, 'train/loss': 1.4440979957580566, 'validation/accuracy': 0.5879999995231628, 'validation/loss': 1.7363550662994385, 'validation/num_examples': 50000, 'test/accuracy': 0.46470001339912415, 'test/loss': 2.418069839477539, 'test/num_examples': 10000, 'score': 23989.14796257019, 'total_duration': 27006.25090765953, 'accumulated_submission_time': 23989.14796257019, 'accumulated_eval_time': 3011.8984134197235, 'accumulated_logging_time': 2.2619385719299316}
I0207 22:17:23.329248 139946397853440 logging_writer.py:48] [51307] accumulated_eval_time=3011.898413, accumulated_logging_time=2.261939, accumulated_submission_time=23989.147963, global_step=51307, preemption_count=0, score=23989.147963, test/accuracy=0.464700, test/loss=2.418070, test/num_examples=10000, total_duration=27006.250908, train/accuracy=0.647480, train/loss=1.444098, validation/accuracy=0.588000, validation/loss=1.736355, validation/num_examples=50000
I0207 22:18:03.804417 139946414638848 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.2079495191574097, loss=2.567025661468506
I0207 22:18:50.730540 139946397853440 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.117537498474121, loss=2.8394150733947754
I0207 22:19:38.122596 139946414638848 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.1195743083953857, loss=3.145120620727539
I0207 22:20:25.281065 139946397853440 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.2883374691009521, loss=2.828115224838257
I0207 22:21:12.861595 139946414638848 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.9958986639976501, loss=5.335365295410156
I0207 22:22:00.174720 139946397853440 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.0858720541000366, loss=3.130833625793457
I0207 22:22:47.331270 139946414638848 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.077608346939087, loss=3.4809622764587402
I0207 22:23:34.481638 139946397853440 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.2171841859817505, loss=2.5778589248657227
I0207 22:24:21.697486 139946414638848 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.0936123132705688, loss=2.611955165863037
I0207 22:24:23.306390 140107197974336 spec.py:321] Evaluating on the training split.
I0207 22:24:34.727490 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 22:25:13.351547 140107197974336 spec.py:349] Evaluating on the test split.
I0207 22:25:14.960987 140107197974336 submission_runner.py:408] Time since start: 27477.91s, 	Step: 52205, 	{'train/accuracy': 0.6319531202316284, 'train/loss': 1.5517568588256836, 'validation/accuracy': 0.5824599862098694, 'validation/loss': 1.791892170906067, 'validation/num_examples': 50000, 'test/accuracy': 0.46140003204345703, 'test/loss': 2.442796230316162, 'test/num_examples': 10000, 'score': 24409.063949108124, 'total_duration': 27477.90633225441, 'accumulated_submission_time': 24409.063949108124, 'accumulated_eval_time': 3063.552988052368, 'accumulated_logging_time': 2.295424699783325}
I0207 22:25:14.990081 139946397853440 logging_writer.py:48] [52205] accumulated_eval_time=3063.552988, accumulated_logging_time=2.295425, accumulated_submission_time=24409.063949, global_step=52205, preemption_count=0, score=24409.063949, test/accuracy=0.461400, test/loss=2.442796, test/num_examples=10000, total_duration=27477.906332, train/accuracy=0.631953, train/loss=1.551757, validation/accuracy=0.582460, validation/loss=1.791892, validation/num_examples=50000
I0207 22:25:56.328011 139946414638848 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.2216717004776, loss=3.7636470794677734
I0207 22:26:43.381515 139946397853440 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.4398330450057983, loss=2.7319037914276123
I0207 22:27:30.429304 139946414638848 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.192007303237915, loss=2.582414150238037
I0207 22:28:17.729697 139946397853440 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.9362385272979736, loss=5.115755081176758
I0207 22:29:04.770861 139946414638848 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.0694626569747925, loss=5.150553226470947
I0207 22:29:51.701488 139946397853440 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.2223113775253296, loss=2.7128729820251465
I0207 22:30:38.585464 139946414638848 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.9688625335693359, loss=3.351017951965332
I0207 22:31:25.914512 139946397853440 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.3515127897262573, loss=2.630858898162842
I0207 22:32:13.003910 139946414638848 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.2488489151000977, loss=2.746746301651001
I0207 22:32:15.053236 140107197974336 spec.py:321] Evaluating on the training split.
I0207 22:32:26.124610 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 22:33:07.165631 140107197974336 spec.py:349] Evaluating on the test split.
I0207 22:33:08.778294 140107197974336 submission_runner.py:408] Time since start: 27951.72s, 	Step: 53106, 	{'train/accuracy': 0.6414257884025574, 'train/loss': 1.4624534845352173, 'validation/accuracy': 0.5923799872398376, 'validation/loss': 1.7016264200210571, 'validation/num_examples': 50000, 'test/accuracy': 0.4686000347137451, 'test/loss': 2.3851122856140137, 'test/num_examples': 10000, 'score': 24829.066326379776, 'total_duration': 27951.723640203476, 'accumulated_submission_time': 24829.066326379776, 'accumulated_eval_time': 3117.2780344486237, 'accumulated_logging_time': 2.3338751792907715}
I0207 22:33:08.803176 139946397853440 logging_writer.py:48] [53106] accumulated_eval_time=3117.278034, accumulated_logging_time=2.333875, accumulated_submission_time=24829.066326, global_step=53106, preemption_count=0, score=24829.066326, test/accuracy=0.468600, test/loss=2.385112, test/num_examples=10000, total_duration=27951.723640, train/accuracy=0.641426, train/loss=1.462453, validation/accuracy=0.592380, validation/loss=1.701626, validation/num_examples=50000
I0207 22:33:49.584920 139946414638848 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2953033447265625, loss=2.6685523986816406
I0207 22:34:36.654666 139946397853440 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.4878818988800049, loss=2.6553544998168945
I0207 22:35:23.639982 139946414638848 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.379689335823059, loss=2.6586530208587646
I0207 22:36:10.728449 139946397853440 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.1855041980743408, loss=2.559882164001465
I0207 22:36:57.839752 139946414638848 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.2067893743515015, loss=2.7502102851867676
I0207 22:37:45.188833 139946397853440 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.2457550764083862, loss=2.6302943229675293
I0207 22:38:32.413999 139946414638848 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.1619250774383545, loss=2.647383213043213
I0207 22:39:19.968218 139946397853440 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.2945711612701416, loss=2.7931785583496094
I0207 22:40:07.313139 139946414638848 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0751909017562866, loss=3.5895068645477295
I0207 22:40:08.858049 140107197974336 spec.py:321] Evaluating on the training split.
I0207 22:40:19.759286 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 22:40:58.853488 140107197974336 spec.py:349] Evaluating on the test split.
I0207 22:41:00.457829 140107197974336 submission_runner.py:408] Time since start: 28423.40s, 	Step: 54005, 	{'train/accuracy': 0.6461132764816284, 'train/loss': 1.4805848598480225, 'validation/accuracy': 0.5896399617195129, 'validation/loss': 1.7537370920181274, 'validation/num_examples': 50000, 'test/accuracy': 0.46390002965927124, 'test/loss': 2.4403724670410156, 'test/num_examples': 10000, 'score': 25249.0579559803, 'total_duration': 28423.403182029724, 'accumulated_submission_time': 25249.0579559803, 'accumulated_eval_time': 3168.877803325653, 'accumulated_logging_time': 2.3705573081970215}
I0207 22:41:00.485296 139946397853440 logging_writer.py:48] [54005] accumulated_eval_time=3168.877803, accumulated_logging_time=2.370557, accumulated_submission_time=25249.057956, global_step=54005, preemption_count=0, score=25249.057956, test/accuracy=0.463900, test/loss=2.440372, test/num_examples=10000, total_duration=28423.403182, train/accuracy=0.646113, train/loss=1.480585, validation/accuracy=0.589640, validation/loss=1.753737, validation/num_examples=50000
I0207 22:41:41.717750 139946414638848 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.9280586838722229, loss=3.746605396270752
I0207 22:42:28.501482 139946397853440 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.2996338605880737, loss=2.7339859008789062
I0207 22:43:15.929469 139946414638848 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.0699107646942139, loss=3.076204538345337
I0207 22:44:03.056684 139946397853440 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.2627273797988892, loss=2.781489849090576
I0207 22:44:50.458758 139946414638848 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2755327224731445, loss=2.4896602630615234
I0207 22:45:37.719234 139946397853440 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.0447394847869873, loss=3.517864465713501
I0207 22:46:24.955620 139946414638848 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.326971173286438, loss=2.639557123184204
I0207 22:47:12.405213 139946397853440 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.2635726928710938, loss=2.676521062850952
I0207 22:47:59.457185 139946414638848 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.1072429418563843, loss=3.7867867946624756
I0207 22:48:00.574037 140107197974336 spec.py:321] Evaluating on the training split.
I0207 22:48:11.844237 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 22:48:49.990440 140107197974336 spec.py:349] Evaluating on the test split.
I0207 22:48:51.596076 140107197974336 submission_runner.py:408] Time since start: 28894.54s, 	Step: 54904, 	{'train/accuracy': 0.6288476586341858, 'train/loss': 1.5620810985565186, 'validation/accuracy': 0.5803399682044983, 'validation/loss': 1.7848657369613647, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4644737243652344, 'test/num_examples': 10000, 'score': 25669.08655667305, 'total_duration': 28894.5414185524, 'accumulated_submission_time': 25669.08655667305, 'accumulated_eval_time': 3219.899830341339, 'accumulated_logging_time': 2.4072391986846924}
I0207 22:48:51.622386 139946397853440 logging_writer.py:48] [54904] accumulated_eval_time=3219.899830, accumulated_logging_time=2.407239, accumulated_submission_time=25669.086557, global_step=54904, preemption_count=0, score=25669.086557, test/accuracy=0.461100, test/loss=2.464474, test/num_examples=10000, total_duration=28894.541419, train/accuracy=0.628848, train/loss=1.562081, validation/accuracy=0.580340, validation/loss=1.784866, validation/num_examples=50000
I0207 22:49:33.355801 139946414638848 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.50713312625885, loss=2.6703717708587646
I0207 22:50:20.394653 139946397853440 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.1232361793518066, loss=5.006618499755859
I0207 22:51:07.638317 139946414638848 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.0775290727615356, loss=3.407771587371826
I0207 22:51:54.623029 139946397853440 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.0335811376571655, loss=5.017676830291748
I0207 22:52:41.767248 139946414638848 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.2559880018234253, loss=2.691361904144287
I0207 22:53:28.819274 139946397853440 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.085065484046936, loss=3.767096757888794
I0207 22:54:16.038259 139946414638848 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.10383939743042, loss=3.0966334342956543
I0207 22:55:03.384067 139946397853440 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.3070722818374634, loss=2.727252960205078
I0207 22:55:50.331350 139946414638848 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.3533604145050049, loss=2.5281717777252197
I0207 22:55:51.940887 140107197974336 spec.py:321] Evaluating on the training split.
I0207 22:56:03.207144 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 22:56:44.433942 140107197974336 spec.py:349] Evaluating on the test split.
I0207 22:56:46.026521 140107197974336 submission_runner.py:408] Time since start: 29368.97s, 	Step: 55805, 	{'train/accuracy': 0.6421093344688416, 'train/loss': 1.4804438352584839, 'validation/accuracy': 0.5927599668502808, 'validation/loss': 1.717319130897522, 'validation/num_examples': 50000, 'test/accuracy': 0.4727000296115875, 'test/loss': 2.3828577995300293, 'test/num_examples': 10000, 'score': 26089.343856811523, 'total_duration': 29368.971861839294, 'accumulated_submission_time': 26089.343856811523, 'accumulated_eval_time': 3273.9854452610016, 'accumulated_logging_time': 2.4438350200653076}
I0207 22:56:46.051577 139946397853440 logging_writer.py:48] [55805] accumulated_eval_time=3273.985445, accumulated_logging_time=2.443835, accumulated_submission_time=26089.343857, global_step=55805, preemption_count=0, score=26089.343857, test/accuracy=0.472700, test/loss=2.382858, test/num_examples=10000, total_duration=29368.971862, train/accuracy=0.642109, train/loss=1.480444, validation/accuracy=0.592760, validation/loss=1.717319, validation/num_examples=50000
I0207 22:57:27.199110 139946414638848 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.3086304664611816, loss=2.60970401763916
I0207 22:58:14.145604 139946397853440 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.0964806079864502, loss=3.473202705383301
I0207 22:59:01.047400 139946414638848 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.084152340888977, loss=2.8367176055908203
I0207 22:59:47.972181 139946397853440 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.0590039491653442, loss=2.969750165939331
I0207 23:00:35.367851 139946414638848 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.227959156036377, loss=2.71193790435791
I0207 23:01:22.603497 139946397853440 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.3511523008346558, loss=2.627230167388916
I0207 23:02:09.843111 139946414638848 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.4640214443206787, loss=2.587958335876465
I0207 23:02:57.079125 139946397853440 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.9646921157836914, loss=4.545494079589844
I0207 23:03:44.197894 139946414638848 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.1412384510040283, loss=3.163410186767578
I0207 23:03:46.145905 140107197974336 spec.py:321] Evaluating on the training split.
I0207 23:03:57.112379 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 23:04:37.426028 140107197974336 spec.py:349] Evaluating on the test split.
I0207 23:04:39.031808 140107197974336 submission_runner.py:408] Time since start: 29841.98s, 	Step: 56706, 	{'train/accuracy': 0.6518945097923279, 'train/loss': 1.4154984951019287, 'validation/accuracy': 0.5967999696731567, 'validation/loss': 1.6895005702972412, 'validation/num_examples': 50000, 'test/accuracy': 0.4781000316143036, 'test/loss': 2.3427188396453857, 'test/num_examples': 10000, 'score': 26509.375621795654, 'total_duration': 29841.977155923843, 'accumulated_submission_time': 26509.375621795654, 'accumulated_eval_time': 3326.8713307380676, 'accumulated_logging_time': 2.479809284210205}
I0207 23:04:39.056754 139946397853440 logging_writer.py:48] [56706] accumulated_eval_time=3326.871331, accumulated_logging_time=2.479809, accumulated_submission_time=26509.375622, global_step=56706, preemption_count=0, score=26509.375622, test/accuracy=0.478100, test/loss=2.342719, test/num_examples=10000, total_duration=29841.977156, train/accuracy=0.651895, train/loss=1.415498, validation/accuracy=0.596800, validation/loss=1.689501, validation/num_examples=50000
I0207 23:05:20.113441 139946414638848 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.1266642808914185, loss=3.246875762939453
I0207 23:06:07.047699 139946397853440 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.0154650211334229, loss=5.026321887969971
I0207 23:06:54.324418 139946414638848 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.2631161212921143, loss=2.628446578979492
I0207 23:07:41.372826 139946397853440 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.070517897605896, loss=4.019304275512695
I0207 23:08:28.656728 139946414638848 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.171325922012329, loss=2.5811660289764404
I0207 23:09:16.122382 139946397853440 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.2163821458816528, loss=2.7264270782470703
I0207 23:10:03.095393 139946414638848 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.1925933361053467, loss=2.5003960132598877
I0207 23:10:50.177098 139946397853440 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.219208002090454, loss=2.757871150970459
I0207 23:11:37.728298 139946414638848 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.0914469957351685, loss=4.959498882293701
I0207 23:11:39.253441 140107197974336 spec.py:321] Evaluating on the training split.
I0207 23:11:50.865683 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 23:12:31.487011 140107197974336 spec.py:349] Evaluating on the test split.
I0207 23:12:33.088821 140107197974336 submission_runner.py:408] Time since start: 30316.03s, 	Step: 57605, 	{'train/accuracy': 0.6357030868530273, 'train/loss': 1.5449306964874268, 'validation/accuracy': 0.5915799736976624, 'validation/loss': 1.7564728260040283, 'validation/num_examples': 50000, 'test/accuracy': 0.4772000312805176, 'test/loss': 2.4073524475097656, 'test/num_examples': 10000, 'score': 26929.512435913086, 'total_duration': 30316.034165859222, 'accumulated_submission_time': 26929.512435913086, 'accumulated_eval_time': 3380.7066905498505, 'accumulated_logging_time': 2.51355242729187}
I0207 23:12:33.115019 139946397853440 logging_writer.py:48] [57605] accumulated_eval_time=3380.706691, accumulated_logging_time=2.513552, accumulated_submission_time=26929.512436, global_step=57605, preemption_count=0, score=26929.512436, test/accuracy=0.477200, test/loss=2.407352, test/num_examples=10000, total_duration=30316.034166, train/accuracy=0.635703, train/loss=1.544931, validation/accuracy=0.591580, validation/loss=1.756473, validation/num_examples=50000
I0207 23:13:14.404731 139946414638848 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.2166790962219238, loss=2.605917453765869
I0207 23:14:01.238984 139946397853440 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.1451855897903442, loss=2.6246562004089355
I0207 23:14:48.666653 139946414638848 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.9762446284294128, loss=4.302562713623047
I0207 23:15:35.799910 139946397853440 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.0186340808868408, loss=4.1801958084106445
I0207 23:16:23.087226 139946414638848 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.3824713230133057, loss=2.7689120769500732
I0207 23:17:10.120811 139946397853440 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.3816958665847778, loss=2.612612724304199
I0207 23:17:57.219844 139946414638848 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.2448607683181763, loss=2.556575298309326
I0207 23:18:44.445966 139946397853440 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.0884668827056885, loss=4.032627582550049
I0207 23:19:31.726199 139946414638848 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1223795413970947, loss=2.5230560302734375
I0207 23:19:33.293034 140107197974336 spec.py:321] Evaluating on the training split.
I0207 23:19:44.835208 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 23:20:25.733219 140107197974336 spec.py:349] Evaluating on the test split.
I0207 23:20:27.335200 140107197974336 submission_runner.py:408] Time since start: 30790.28s, 	Step: 58505, 	{'train/accuracy': 0.6467968821525574, 'train/loss': 1.471572995185852, 'validation/accuracy': 0.5975599884986877, 'validation/loss': 1.7017605304718018, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.3641316890716553, 'test/num_examples': 10000, 'score': 27349.630720853806, 'total_duration': 30790.280519247055, 'accumulated_submission_time': 27349.630720853806, 'accumulated_eval_time': 3434.7488169670105, 'accumulated_logging_time': 2.548457622528076}
I0207 23:20:27.370368 139946397853440 logging_writer.py:48] [58505] accumulated_eval_time=3434.748817, accumulated_logging_time=2.548458, accumulated_submission_time=27349.630721, global_step=58505, preemption_count=0, score=27349.630721, test/accuracy=0.474500, test/loss=2.364132, test/num_examples=10000, total_duration=30790.280519, train/accuracy=0.646797, train/loss=1.471573, validation/accuracy=0.597560, validation/loss=1.701761, validation/num_examples=50000
I0207 23:21:08.515506 139946414638848 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.3280318975448608, loss=2.669161319732666
I0207 23:21:55.246353 139946397853440 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.9909469485282898, loss=3.985825777053833
I0207 23:22:42.651535 139946414638848 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.1146128177642822, loss=3.0235118865966797
I0207 23:23:29.623093 139946397853440 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.2749526500701904, loss=2.5397584438323975
I0207 23:24:16.596970 139946414638848 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.0377233028411865, loss=4.0378642082214355
I0207 23:25:04.051861 139946397853440 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.9583394527435303, loss=4.565620422363281
I0207 23:25:51.086429 139946414638848 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.2141177654266357, loss=2.4387147426605225
I0207 23:26:38.332862 139946397853440 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.1991336345672607, loss=5.270055294036865
I0207 23:27:25.308568 139946414638848 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.4156056642532349, loss=2.3876421451568604
I0207 23:27:27.718267 140107197974336 spec.py:321] Evaluating on the training split.
I0207 23:27:38.849333 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 23:28:17.116092 140107197974336 spec.py:349] Evaluating on the test split.
I0207 23:28:18.717697 140107197974336 submission_runner.py:408] Time since start: 31261.66s, 	Step: 59407, 	{'train/accuracy': 0.6440038681030273, 'train/loss': 1.4910470247268677, 'validation/accuracy': 0.590719997882843, 'validation/loss': 1.7559362649917603, 'validation/num_examples': 50000, 'test/accuracy': 0.4765000343322754, 'test/loss': 2.4175078868865967, 'test/num_examples': 10000, 'score': 27769.91752099991, 'total_duration': 31261.66304731369, 'accumulated_submission_time': 27769.91752099991, 'accumulated_eval_time': 3485.748226881027, 'accumulated_logging_time': 2.594045400619507}
I0207 23:28:18.743563 139946397853440 logging_writer.py:48] [59407] accumulated_eval_time=3485.748227, accumulated_logging_time=2.594045, accumulated_submission_time=27769.917521, global_step=59407, preemption_count=0, score=27769.917521, test/accuracy=0.476500, test/loss=2.417508, test/num_examples=10000, total_duration=31261.663047, train/accuracy=0.644004, train/loss=1.491047, validation/accuracy=0.590720, validation/loss=1.755936, validation/num_examples=50000
I0207 23:28:59.162033 139946414638848 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.2462321519851685, loss=2.6027045249938965
I0207 23:29:46.166596 139946397853440 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.9856470823287964, loss=4.974102973937988
I0207 23:30:33.609968 139946414638848 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.159282922744751, loss=2.4462730884552
I0207 23:31:20.903224 139946397853440 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.2224780321121216, loss=3.1595098972320557
I0207 23:32:08.415603 139946414638848 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.1055214405059814, loss=4.1069746017456055
I0207 23:32:55.905549 139946397853440 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.2279376983642578, loss=2.6137826442718506
I0207 23:33:43.535653 139946414638848 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.3896468877792358, loss=2.5221199989318848
I0207 23:34:31.066296 139946397853440 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.357796549797058, loss=2.5695419311523438
I0207 23:35:18.893727 139946414638848 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.3794001340866089, loss=2.475126266479492
I0207 23:35:18.907018 140107197974336 spec.py:321] Evaluating on the training split.
I0207 23:35:29.871739 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 23:36:08.147078 140107197974336 spec.py:349] Evaluating on the test split.
I0207 23:36:09.748810 140107197974336 submission_runner.py:408] Time since start: 31732.69s, 	Step: 60301, 	{'train/accuracy': 0.6430468559265137, 'train/loss': 1.471490502357483, 'validation/accuracy': 0.5990399718284607, 'validation/loss': 1.6971635818481445, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.3615305423736572, 'test/num_examples': 10000, 'score': 28190.01903152466, 'total_duration': 31732.694142580032, 'accumulated_submission_time': 28190.01903152466, 'accumulated_eval_time': 3536.589988708496, 'accumulated_logging_time': 2.6317689418792725}
I0207 23:36:09.780901 139946397853440 logging_writer.py:48] [60301] accumulated_eval_time=3536.589989, accumulated_logging_time=2.631769, accumulated_submission_time=28190.019032, global_step=60301, preemption_count=0, score=28190.019032, test/accuracy=0.475500, test/loss=2.361531, test/num_examples=10000, total_duration=31732.694143, train/accuracy=0.643047, train/loss=1.471491, validation/accuracy=0.599040, validation/loss=1.697164, validation/num_examples=50000
I0207 23:36:52.749669 139946414638848 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.0716047286987305, loss=4.333805084228516
I0207 23:37:39.667429 139946397853440 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.2541710138320923, loss=2.3572864532470703
I0207 23:38:26.851354 139946414638848 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.0893059968948364, loss=5.165101528167725
I0207 23:39:13.939991 139946397853440 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.9641115069389343, loss=4.816610336303711
I0207 23:40:01.097832 139946414638848 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.0397741794586182, loss=4.033931732177734
I0207 23:40:48.286486 139946397853440 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.2239323854446411, loss=2.6376850605010986
I0207 23:41:35.335737 139946414638848 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.2746580839157104, loss=2.5882484912872314
I0207 23:42:22.790879 139946397853440 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.217573881149292, loss=2.4256486892700195
I0207 23:43:09.799302 139946414638848 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.305756688117981, loss=2.523472785949707
I0207 23:43:09.815578 140107197974336 spec.py:321] Evaluating on the training split.
I0207 23:43:21.010505 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 23:44:00.822390 140107197974336 spec.py:349] Evaluating on the test split.
I0207 23:44:02.423806 140107197974336 submission_runner.py:408] Time since start: 32205.37s, 	Step: 61201, 	{'train/accuracy': 0.6446093320846558, 'train/loss': 1.4742058515548706, 'validation/accuracy': 0.6018999814987183, 'validation/loss': 1.6856762170791626, 'validation/num_examples': 50000, 'test/accuracy': 0.4775000214576721, 'test/loss': 2.361438274383545, 'test/num_examples': 10000, 'score': 28609.990909576416, 'total_duration': 32205.369146585464, 'accumulated_submission_time': 28609.990909576416, 'accumulated_eval_time': 3589.1981959342957, 'accumulated_logging_time': 2.6737277507781982}
I0207 23:44:02.456040 139946397853440 logging_writer.py:48] [61201] accumulated_eval_time=3589.198196, accumulated_logging_time=2.673728, accumulated_submission_time=28609.990910, global_step=61201, preemption_count=0, score=28609.990910, test/accuracy=0.477500, test/loss=2.361438, test/num_examples=10000, total_duration=32205.369147, train/accuracy=0.644609, train/loss=1.474206, validation/accuracy=0.601900, validation/loss=1.685676, validation/num_examples=50000
I0207 23:44:45.975761 139946414638848 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.1912683248519897, loss=2.374349594116211
I0207 23:45:32.679813 139946397853440 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.1907461881637573, loss=5.100758075714111
I0207 23:46:19.810034 139946414638848 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.9983892440795898, loss=4.873844623565674
I0207 23:47:06.758157 139946397853440 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.2650907039642334, loss=2.557663679122925
I0207 23:47:53.919682 139946414638848 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.014872431755066, loss=5.064454555511475
I0207 23:48:41.126129 139946397853440 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.1036041975021362, loss=2.9505257606506348
I0207 23:49:28.184607 139946414638848 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.028264045715332, loss=4.017059326171875
I0207 23:50:15.310740 139946397853440 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.313539743423462, loss=2.501572370529175
I0207 23:51:02.624699 139946414638848 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.2777493000030518, loss=2.5125603675842285
I0207 23:51:02.645878 140107197974336 spec.py:321] Evaluating on the training split.
I0207 23:51:13.699389 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 23:51:55.217104 140107197974336 spec.py:349] Evaluating on the test split.
I0207 23:51:56.809455 140107197974336 submission_runner.py:408] Time since start: 32679.75s, 	Step: 62101, 	{'train/accuracy': 0.6470312476158142, 'train/loss': 1.4702602624893188, 'validation/accuracy': 0.5948399901390076, 'validation/loss': 1.7143629789352417, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.3711555004119873, 'test/num_examples': 10000, 'score': 29030.119871616364, 'total_duration': 32679.75479578972, 'accumulated_submission_time': 29030.119871616364, 'accumulated_eval_time': 3643.3617749214172, 'accumulated_logging_time': 2.71584153175354}
I0207 23:51:56.834728 139946397853440 logging_writer.py:48] [62101] accumulated_eval_time=3643.361775, accumulated_logging_time=2.715842, accumulated_submission_time=29030.119872, global_step=62101, preemption_count=0, score=29030.119872, test/accuracy=0.479200, test/loss=2.371156, test/num_examples=10000, total_duration=32679.754796, train/accuracy=0.647031, train/loss=1.470260, validation/accuracy=0.594840, validation/loss=1.714363, validation/num_examples=50000
I0207 23:52:39.835889 139946414638848 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.071648359298706, loss=3.0956084728240967
I0207 23:53:26.708540 139946397853440 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0159404277801514, loss=4.146517753601074
I0207 23:54:14.182400 139946414638848 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.073311448097229, loss=3.179152011871338
I0207 23:55:01.089617 139946397853440 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.2301392555236816, loss=3.632324695587158
I0207 23:55:48.609602 139946414638848 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.3569598197937012, loss=2.5124704837799072
I0207 23:56:35.925672 139946397853440 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.9991827011108398, loss=5.020974159240723
I0207 23:57:22.962577 139946414638848 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.1083601713180542, loss=3.0908074378967285
I0207 23:58:10.193892 139946397853440 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.3771249055862427, loss=2.532475471496582
I0207 23:58:57.329349 139946414638848 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.3040486574172974, loss=2.6725215911865234
I0207 23:58:57.342591 140107197974336 spec.py:321] Evaluating on the training split.
I0207 23:59:08.739127 140107197974336 spec.py:333] Evaluating on the validation split.
I0207 23:59:48.918900 140107197974336 spec.py:349] Evaluating on the test split.
I0207 23:59:50.516344 140107197974336 submission_runner.py:408] Time since start: 33153.46s, 	Step: 63001, 	{'train/accuracy': 0.6622851490974426, 'train/loss': 1.3750454187393188, 'validation/accuracy': 0.6061999797821045, 'validation/loss': 1.6341488361358643, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.299943447113037, 'test/num_examples': 10000, 'score': 29450.567351818085, 'total_duration': 33153.461698293686, 'accumulated_submission_time': 29450.567351818085, 'accumulated_eval_time': 3696.5355207920074, 'accumulated_logging_time': 2.7506535053253174}
I0207 23:59:50.545417 139946397853440 logging_writer.py:48] [63001] accumulated_eval_time=3696.535521, accumulated_logging_time=2.750654, accumulated_submission_time=29450.567352, global_step=63001, preemption_count=0, score=29450.567352, test/accuracy=0.487900, test/loss=2.299943, test/num_examples=10000, total_duration=33153.461698, train/accuracy=0.662285, train/loss=1.375045, validation/accuracy=0.606200, validation/loss=1.634149, validation/num_examples=50000
I0208 00:00:33.535519 139946414638848 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.1918751001358032, loss=2.5149431228637695
I0208 00:01:20.221940 139946397853440 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.250234842300415, loss=2.480087995529175
I0208 00:02:07.385481 139946414638848 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.2480618953704834, loss=3.4237277507781982
I0208 00:02:54.245654 139946397853440 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.0774065256118774, loss=4.996865749359131
I0208 00:03:41.217073 139946414638848 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.104163408279419, loss=4.101454734802246
I0208 00:04:28.611346 139946397853440 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.2961232662200928, loss=2.4631943702697754
I0208 00:05:15.808225 139946414638848 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.0781971216201782, loss=3.774991273880005
I0208 00:06:03.246887 139946397853440 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.2710700035095215, loss=2.442106246948242
I0208 00:06:50.353186 139946414638848 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.9594029188156128, loss=4.345803260803223
I0208 00:06:50.954878 140107197974336 spec.py:321] Evaluating on the training split.
I0208 00:07:02.091884 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 00:07:41.903231 140107197974336 spec.py:349] Evaluating on the test split.
I0208 00:07:43.499784 140107197974336 submission_runner.py:408] Time since start: 33626.45s, 	Step: 63903, 	{'train/accuracy': 0.6542187333106995, 'train/loss': 1.415706753730774, 'validation/accuracy': 0.6043999791145325, 'validation/loss': 1.652857780456543, 'validation/num_examples': 50000, 'test/accuracy': 0.48170003294944763, 'test/loss': 2.3179423809051514, 'test/num_examples': 10000, 'score': 29870.9140355587, 'total_duration': 33626.445133686066, 'accumulated_submission_time': 29870.9140355587, 'accumulated_eval_time': 3749.0804085731506, 'accumulated_logging_time': 2.791459083557129}
I0208 00:07:43.525185 139946397853440 logging_writer.py:48] [63903] accumulated_eval_time=3749.080409, accumulated_logging_time=2.791459, accumulated_submission_time=29870.914036, global_step=63903, preemption_count=0, score=29870.914036, test/accuracy=0.481700, test/loss=2.317942, test/num_examples=10000, total_duration=33626.445134, train/accuracy=0.654219, train/loss=1.415707, validation/accuracy=0.604400, validation/loss=1.652858, validation/num_examples=50000
I0208 00:08:25.912878 139946414638848 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.4085580110549927, loss=2.35583758354187
I0208 00:09:13.046433 139946397853440 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.2376445531845093, loss=2.774160385131836
I0208 00:10:00.342579 139946414638848 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.3741854429244995, loss=2.416457176208496
I0208 00:10:47.531981 139946397853440 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.517148733139038, loss=2.522676467895508
I0208 00:11:34.892063 139946414638848 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.3436999320983887, loss=2.36869478225708
I0208 00:12:22.200063 139946397853440 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.28000807762146, loss=2.6354076862335205
I0208 00:13:09.428148 139946414638848 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.4044872522354126, loss=2.5652310848236084
I0208 00:13:56.605004 139946397853440 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.2546190023422241, loss=2.545844554901123
I0208 00:14:44.165936 139946414638848 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.3529049158096313, loss=2.467707633972168
I0208 00:14:44.181321 140107197974336 spec.py:321] Evaluating on the training split.
I0208 00:14:55.360416 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 00:15:34.834242 140107197974336 spec.py:349] Evaluating on the test split.
I0208 00:15:36.427848 140107197974336 submission_runner.py:408] Time since start: 34099.37s, 	Step: 64801, 	{'train/accuracy': 0.6602929830551147, 'train/loss': 1.405190348625183, 'validation/accuracy': 0.6033799648284912, 'validation/loss': 1.6764905452728271, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.324260711669922, 'test/num_examples': 10000, 'score': 30291.509792089462, 'total_duration': 34099.37319946289, 'accumulated_submission_time': 30291.509792089462, 'accumulated_eval_time': 3801.3269126415253, 'accumulated_logging_time': 2.8265275955200195}
I0208 00:15:36.454902 139946397853440 logging_writer.py:48] [64801] accumulated_eval_time=3801.326913, accumulated_logging_time=2.826528, accumulated_submission_time=30291.509792, global_step=64801, preemption_count=0, score=30291.509792, test/accuracy=0.480900, test/loss=2.324261, test/num_examples=10000, total_duration=34099.373199, train/accuracy=0.660293, train/loss=1.405190, validation/accuracy=0.603380, validation/loss=1.676491, validation/num_examples=50000
I0208 00:16:19.515944 139946414638848 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.142201542854309, loss=3.3934988975524902
I0208 00:17:06.477778 139946397853440 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.0528481006622314, loss=5.170507907867432
I0208 00:17:53.829280 139946414638848 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.0882612466812134, loss=4.228487014770508
I0208 00:18:40.991522 139946397853440 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.2053898572921753, loss=4.473970413208008
I0208 00:19:28.074370 139946414638848 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1826891899108887, loss=2.9283902645111084
I0208 00:20:15.312881 139946397853440 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.3965333700180054, loss=2.6182544231414795
I0208 00:21:02.336800 139946414638848 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.354904294013977, loss=2.6405863761901855
I0208 00:21:49.257441 139946397853440 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.124029278755188, loss=2.9993269443511963
I0208 00:22:36.246673 139946414638848 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.197937250137329, loss=2.79361891746521
I0208 00:22:36.848382 140107197974336 spec.py:321] Evaluating on the training split.
I0208 00:22:48.351202 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 00:23:25.562272 140107197974336 spec.py:349] Evaluating on the test split.
I0208 00:23:27.161161 140107197974336 submission_runner.py:408] Time since start: 34570.11s, 	Step: 65703, 	{'train/accuracy': 0.6716015338897705, 'train/loss': 1.325235366821289, 'validation/accuracy': 0.6054999828338623, 'validation/loss': 1.634854793548584, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.2978153228759766, 'test/num_examples': 10000, 'score': 30711.840952396393, 'total_duration': 34570.10649561882, 'accumulated_submission_time': 30711.840952396393, 'accumulated_eval_time': 3851.639670610428, 'accumulated_logging_time': 2.863790512084961}
I0208 00:23:27.190143 139946397853440 logging_writer.py:48] [65703] accumulated_eval_time=3851.639671, accumulated_logging_time=2.863791, accumulated_submission_time=30711.840952, global_step=65703, preemption_count=0, score=30711.840952, test/accuracy=0.486200, test/loss=2.297815, test/num_examples=10000, total_duration=34570.106496, train/accuracy=0.671602, train/loss=1.325235, validation/accuracy=0.605500, validation/loss=1.634855, validation/num_examples=50000
I0208 00:24:09.594873 139946414638848 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.5815509557724, loss=2.782313346862793
I0208 00:24:56.390130 139946397853440 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.2541894912719727, loss=2.464590549468994
I0208 00:25:43.833467 139946414638848 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.257691740989685, loss=2.6490204334259033
I0208 00:26:31.308542 139946397853440 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.234756350517273, loss=2.5930745601654053
I0208 00:27:18.842272 139946414638848 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0636677742004395, loss=3.6796915531158447
I0208 00:28:06.679754 139946397853440 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.244207739830017, loss=2.690629720687866
I0208 00:28:53.846636 139946414638848 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0540441274642944, loss=4.378714561462402
I0208 00:29:41.134028 139946397853440 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.9885332584381104, loss=5.020299434661865
I0208 00:30:27.338566 140107197974336 spec.py:321] Evaluating on the training split.
I0208 00:30:38.491141 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 00:31:15.373014 140107197974336 spec.py:349] Evaluating on the test split.
I0208 00:31:16.976043 140107197974336 submission_runner.py:408] Time since start: 35039.92s, 	Step: 66599, 	{'train/accuracy': 0.6601366996765137, 'train/loss': 1.4673115015029907, 'validation/accuracy': 0.6058799624443054, 'validation/loss': 1.716723084449768, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.3495824337005615, 'test/num_examples': 10000, 'score': 31131.927243471146, 'total_duration': 35039.92137694359, 'accumulated_submission_time': 31131.927243471146, 'accumulated_eval_time': 3901.2771389484406, 'accumulated_logging_time': 2.9034883975982666}
I0208 00:31:17.013173 139946414638848 logging_writer.py:48] [66599] accumulated_eval_time=3901.277139, accumulated_logging_time=2.903488, accumulated_submission_time=31131.927243, global_step=66599, preemption_count=0, score=31131.927243, test/accuracy=0.490800, test/loss=2.349582, test/num_examples=10000, total_duration=35039.921377, train/accuracy=0.660137, train/loss=1.467312, validation/accuracy=0.605880, validation/loss=1.716723, validation/num_examples=50000
I0208 00:31:17.845694 139946397853440 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.4184499979019165, loss=2.572105884552002
I0208 00:32:01.029883 139946414638848 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.1255834102630615, loss=5.030598163604736
I0208 00:32:48.018181 139946397853440 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.1580511331558228, loss=2.3846447467803955
I0208 00:33:35.279141 139946414638848 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.0646991729736328, loss=2.855008125305176
I0208 00:34:22.452662 139946397853440 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.1078191995620728, loss=2.9735772609710693
I0208 00:35:09.507219 139946414638848 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.2944519519805908, loss=4.208971977233887
I0208 00:35:56.518426 139946397853440 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.0051286220550537, loss=5.107564926147461
I0208 00:36:43.664592 139946414638848 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.3055944442749023, loss=2.465775966644287
I0208 00:37:31.033421 139946397853440 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.2578707933425903, loss=2.3683090209960938
I0208 00:38:17.408997 140107197974336 spec.py:321] Evaluating on the training split.
I0208 00:38:28.524010 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 00:39:07.357542 140107197974336 spec.py:349] Evaluating on the test split.
I0208 00:39:08.962428 140107197974336 submission_runner.py:408] Time since start: 35511.91s, 	Step: 67499, 	{'train/accuracy': 0.6662890315055847, 'train/loss': 1.3695124387741089, 'validation/accuracy': 0.6140199899673462, 'validation/loss': 1.6112968921661377, 'validation/num_examples': 50000, 'test/accuracy': 0.49320003390312195, 'test/loss': 2.2746410369873047, 'test/num_examples': 10000, 'score': 31552.259560346603, 'total_duration': 35511.90776872635, 'accumulated_submission_time': 31552.259560346603, 'accumulated_eval_time': 3952.83056306839, 'accumulated_logging_time': 2.9529545307159424}
I0208 00:39:08.992665 139946414638848 logging_writer.py:48] [67499] accumulated_eval_time=3952.830563, accumulated_logging_time=2.952955, accumulated_submission_time=31552.259560, global_step=67499, preemption_count=0, score=31552.259560, test/accuracy=0.493200, test/loss=2.274641, test/num_examples=10000, total_duration=35511.907769, train/accuracy=0.666289, train/loss=1.369512, validation/accuracy=0.614020, validation/loss=1.611297, validation/num_examples=50000
I0208 00:39:09.824856 139946397853440 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.2572768926620483, loss=2.3916053771972656
I0208 00:39:53.250070 139946414638848 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.0230286121368408, loss=3.84865140914917
I0208 00:40:40.191266 139946397853440 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1617249250411987, loss=3.583385467529297
I0208 00:41:27.628129 139946414638848 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.0050759315490723, loss=4.371696472167969
I0208 00:42:14.882599 139946397853440 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.3165085315704346, loss=2.516418695449829
I0208 00:43:02.002008 139946414638848 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1347403526306152, loss=3.1872098445892334
I0208 00:43:49.184353 139946397853440 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.4268590211868286, loss=2.4224419593811035
I0208 00:44:36.639247 139946414638848 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1207716464996338, loss=3.5252063274383545
I0208 00:45:23.834319 139946397853440 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.2971603870391846, loss=2.4272656440734863
I0208 00:46:09.312532 140107197974336 spec.py:321] Evaluating on the training split.
I0208 00:46:20.305903 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 00:46:59.192030 140107197974336 spec.py:349] Evaluating on the test split.
I0208 00:47:00.797444 140107197974336 submission_runner.py:408] Time since start: 35983.74s, 	Step: 68398, 	{'train/accuracy': 0.6966406106948853, 'train/loss': 1.2503221035003662, 'validation/accuracy': 0.6096799969673157, 'validation/loss': 1.6365011930465698, 'validation/num_examples': 50000, 'test/accuracy': 0.49330002069473267, 'test/loss': 2.2944016456604004, 'test/num_examples': 10000, 'score': 31972.519397735596, 'total_duration': 35983.74279308319, 'accumulated_submission_time': 31972.519397735596, 'accumulated_eval_time': 4004.3154785633087, 'accumulated_logging_time': 2.992598533630371}
I0208 00:47:00.827961 139946414638848 logging_writer.py:48] [68398] accumulated_eval_time=4004.315479, accumulated_logging_time=2.992599, accumulated_submission_time=31972.519398, global_step=68398, preemption_count=0, score=31972.519398, test/accuracy=0.493300, test/loss=2.294402, test/num_examples=10000, total_duration=35983.742793, train/accuracy=0.696641, train/loss=1.250322, validation/accuracy=0.609680, validation/loss=1.636501, validation/num_examples=50000
I0208 00:47:02.061478 139946397853440 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3391915559768677, loss=2.5740768909454346
I0208 00:47:45.603512 139946414638848 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.2721439599990845, loss=2.8769278526306152
I0208 00:48:32.779153 139946397853440 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.244432806968689, loss=3.1025426387786865
I0208 00:49:20.238923 139946414638848 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.3673889636993408, loss=4.482080936431885
I0208 00:50:07.901693 139946397853440 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.226799488067627, loss=2.9072930812835693
I0208 00:50:55.235329 139946414638848 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.994000256061554, loss=5.027364253997803
I0208 00:51:42.841926 139946397853440 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.2521591186523438, loss=2.6100707054138184
I0208 00:52:30.362913 139946414638848 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.2366585731506348, loss=3.6952803134918213
I0208 00:53:18.224233 139946397853440 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.4207841157913208, loss=2.44968843460083
I0208 00:54:01.147839 140107197974336 spec.py:321] Evaluating on the training split.
I0208 00:54:12.130652 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 00:54:52.239246 140107197974336 spec.py:349] Evaluating on the test split.
I0208 00:54:53.842525 140107197974336 submission_runner.py:408] Time since start: 36456.79s, 	Step: 69292, 	{'train/accuracy': 0.6567773222923279, 'train/loss': 1.425057291984558, 'validation/accuracy': 0.6114400029182434, 'validation/loss': 1.6573946475982666, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.3122198581695557, 'test/num_examples': 10000, 'score': 32392.77770280838, 'total_duration': 36456.787868499756, 'accumulated_submission_time': 32392.77770280838, 'accumulated_eval_time': 4057.0101647377014, 'accumulated_logging_time': 3.033708333969116}
I0208 00:54:53.871045 139946414638848 logging_writer.py:48] [69292] accumulated_eval_time=4057.010165, accumulated_logging_time=3.033708, accumulated_submission_time=32392.777703, global_step=69292, preemption_count=0, score=32392.777703, test/accuracy=0.488600, test/loss=2.312220, test/num_examples=10000, total_duration=36456.787868, train/accuracy=0.656777, train/loss=1.425057, validation/accuracy=0.611440, validation/loss=1.657395, validation/num_examples=50000
I0208 00:54:57.571857 139946397853440 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.3248766660690308, loss=2.4942352771759033
I0208 00:55:41.497387 139946414638848 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.4048030376434326, loss=2.604161262512207
I0208 00:56:28.630149 139946397853440 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.264587163925171, loss=2.411611557006836
I0208 00:57:15.765404 139946414638848 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.057754635810852, loss=4.981083869934082
I0208 00:58:02.976194 139946397853440 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.2164719104766846, loss=2.5087695121765137
I0208 00:58:49.989855 139946414638848 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.3141566514968872, loss=2.3340048789978027
I0208 00:59:37.326454 139946397853440 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3478041887283325, loss=2.6155149936676025
I0208 01:00:24.676506 139946414638848 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.2031632661819458, loss=2.398824453353882
I0208 01:01:12.245569 139946397853440 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.220910668373108, loss=2.7958648204803467
I0208 01:01:54.011128 140107197974336 spec.py:321] Evaluating on the training split.
I0208 01:02:05.314993 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 01:02:43.913446 140107197974336 spec.py:349] Evaluating on the test split.
I0208 01:02:45.517959 140107197974336 submission_runner.py:408] Time since start: 36928.46s, 	Step: 70190, 	{'train/accuracy': 0.6670116782188416, 'train/loss': 1.3967492580413818, 'validation/accuracy': 0.6153599619865417, 'validation/loss': 1.6382001638412476, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.298884153366089, 'test/num_examples': 10000, 'score': 32812.85529613495, 'total_duration': 36928.463305950165, 'accumulated_submission_time': 32812.85529613495, 'accumulated_eval_time': 4108.516996145248, 'accumulated_logging_time': 3.072967052459717}
I0208 01:02:45.545943 139946414638848 logging_writer.py:48] [70190] accumulated_eval_time=4108.516996, accumulated_logging_time=3.072967, accumulated_submission_time=32812.855296, global_step=70190, preemption_count=0, score=32812.855296, test/accuracy=0.488600, test/loss=2.298884, test/num_examples=10000, total_duration=36928.463306, train/accuracy=0.667012, train/loss=1.396749, validation/accuracy=0.615360, validation/loss=1.638200, validation/num_examples=50000
I0208 01:02:50.063736 139946397853440 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.11028254032135, loss=3.1689748764038086
I0208 01:03:34.011126 139946414638848 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.003718614578247, loss=4.603543758392334
I0208 01:04:20.990274 139946397853440 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.441251516342163, loss=2.4474189281463623
I0208 01:05:08.410608 139946414638848 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1343157291412354, loss=5.027849197387695
I0208 01:05:55.672807 139946397853440 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.1514298915863037, loss=2.811415672302246
I0208 01:06:42.848099 139946414638848 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.0874618291854858, loss=3.259791135787964
I0208 01:07:30.216730 139946397853440 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.3635151386260986, loss=2.4673635959625244
I0208 01:08:17.098386 139946414638848 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.1633440256118774, loss=2.6912546157836914
I0208 01:09:04.212698 139946397853440 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.1006196737289429, loss=3.2927424907684326
I0208 01:09:45.766956 140107197974336 spec.py:321] Evaluating on the training split.
I0208 01:09:56.774300 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 01:10:37.437972 140107197974336 spec.py:349] Evaluating on the test split.
I0208 01:10:39.029072 140107197974336 submission_runner.py:408] Time since start: 37401.97s, 	Step: 71090, 	{'train/accuracy': 0.6880077719688416, 'train/loss': 1.30825936794281, 'validation/accuracy': 0.6117199659347534, 'validation/loss': 1.6579543352127075, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.2903990745544434, 'test/num_examples': 10000, 'score': 33233.01276636124, 'total_duration': 37401.97440814972, 'accumulated_submission_time': 33233.01276636124, 'accumulated_eval_time': 4161.779107093811, 'accumulated_logging_time': 3.111844539642334}
I0208 01:10:39.061176 139946414638848 logging_writer.py:48] [71090] accumulated_eval_time=4161.779107, accumulated_logging_time=3.111845, accumulated_submission_time=33233.012766, global_step=71090, preemption_count=0, score=33233.012766, test/accuracy=0.494600, test/loss=2.290399, test/num_examples=10000, total_duration=37401.974408, train/accuracy=0.688008, train/loss=1.308259, validation/accuracy=0.611720, validation/loss=1.657954, validation/num_examples=50000
I0208 01:10:43.594025 139946397853440 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.0615565776824951, loss=4.531987190246582
I0208 01:11:27.385316 139946414638848 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.4223922491073608, loss=2.3832151889801025
I0208 01:12:14.546431 139946397853440 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.4391155242919922, loss=2.5928549766540527
I0208 01:13:02.075659 139946414638848 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.3139466047286987, loss=2.6360371112823486
I0208 01:13:49.212235 139946397853440 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2896865606307983, loss=2.563044309616089
I0208 01:14:36.434807 139946414638848 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.0377265214920044, loss=5.079318046569824
I0208 01:15:23.839391 139946397853440 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.2024123668670654, loss=2.7302401065826416
I0208 01:16:11.158753 139946414638848 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.2247990369796753, loss=2.36570405960083
I0208 01:16:58.356059 139946397853440 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.2249947786331177, loss=2.7485146522521973
I0208 01:17:39.379592 140107197974336 spec.py:321] Evaluating on the training split.
I0208 01:17:50.496407 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 01:18:27.956863 140107197974336 spec.py:349] Evaluating on the test split.
I0208 01:18:29.559746 140107197974336 submission_runner.py:408] Time since start: 37872.51s, 	Step: 71989, 	{'train/accuracy': 0.6581054329872131, 'train/loss': 1.4184592962265015, 'validation/accuracy': 0.613599956035614, 'validation/loss': 1.642994999885559, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.313216209411621, 'test/num_examples': 10000, 'score': 33653.27028799057, 'total_duration': 37872.505085229874, 'accumulated_submission_time': 33653.27028799057, 'accumulated_eval_time': 4211.959260225296, 'accumulated_logging_time': 3.1535627841949463}
I0208 01:18:29.592526 139946414638848 logging_writer.py:48] [71989] accumulated_eval_time=4211.959260, accumulated_logging_time=3.153563, accumulated_submission_time=33653.270288, global_step=71989, preemption_count=0, score=33653.270288, test/accuracy=0.491900, test/loss=2.313216, test/num_examples=10000, total_duration=37872.505085, train/accuracy=0.658105, train/loss=1.418459, validation/accuracy=0.613600, validation/loss=1.642995, validation/num_examples=50000
I0208 01:18:34.534961 139946397853440 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.2314136028289795, loss=2.4653515815734863
I0208 01:19:18.567084 139946414638848 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.3185588121414185, loss=2.508119583129883
I0208 01:20:05.376532 139946397853440 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.5240445137023926, loss=2.417130470275879
I0208 01:20:52.567471 139946414638848 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3576042652130127, loss=3.126387357711792
I0208 01:21:39.897576 139946397853440 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.0478172302246094, loss=4.5666656494140625
I0208 01:22:26.990898 139946414638848 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.3171777725219727, loss=2.4205760955810547
I0208 01:23:14.628755 139946397853440 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.3799583911895752, loss=2.5676093101501465
I0208 01:24:01.803220 139946414638848 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.4176100492477417, loss=2.3632476329803467
I0208 01:24:49.285765 139946397853440 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.3429101705551147, loss=3.0295591354370117
I0208 01:25:29.771034 140107197974336 spec.py:321] Evaluating on the training split.
I0208 01:25:40.818760 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 01:26:20.084608 140107197974336 spec.py:349] Evaluating on the test split.
I0208 01:26:21.691999 140107197974336 submission_runner.py:408] Time since start: 38344.64s, 	Step: 72888, 	{'train/accuracy': 0.6729101538658142, 'train/loss': 1.3513695001602173, 'validation/accuracy': 0.6170799732208252, 'validation/loss': 1.6132991313934326, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.280538320541382, 'test/num_examples': 10000, 'score': 34073.381747722626, 'total_duration': 38344.637328863144, 'accumulated_submission_time': 34073.381747722626, 'accumulated_eval_time': 4263.880227088928, 'accumulated_logging_time': 3.2022414207458496}
I0208 01:26:21.724972 139946414638848 logging_writer.py:48] [72888] accumulated_eval_time=4263.880227, accumulated_logging_time=3.202241, accumulated_submission_time=34073.381748, global_step=72888, preemption_count=0, score=34073.381748, test/accuracy=0.494100, test/loss=2.280538, test/num_examples=10000, total_duration=38344.637329, train/accuracy=0.672910, train/loss=1.351370, validation/accuracy=0.617080, validation/loss=1.613299, validation/num_examples=50000
I0208 01:26:27.068929 139946397853440 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.2844632863998413, loss=2.273808002471924
I0208 01:27:11.276624 139946414638848 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.2223498821258545, loss=2.674267292022705
I0208 01:27:58.397403 139946397853440 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.0409702062606812, loss=3.4834630489349365
I0208 01:28:45.894250 139946414638848 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.1216119527816772, loss=4.361537456512451
I0208 01:29:33.067006 139946397853440 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.033374547958374, loss=4.990095615386963
I0208 01:30:20.345759 139946414638848 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.2351680994033813, loss=2.2936182022094727
I0208 01:31:07.838174 139946397853440 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.3220996856689453, loss=2.37833833694458
I0208 01:31:55.123283 139946414638848 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.2989498376846313, loss=2.673241376876831
I0208 01:32:42.648327 139946397853440 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.2325944900512695, loss=2.688159942626953
I0208 01:33:21.997908 140107197974336 spec.py:321] Evaluating on the training split.
I0208 01:33:33.109303 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 01:34:12.932599 140107197974336 spec.py:349] Evaluating on the test split.
I0208 01:34:14.535311 140107197974336 submission_runner.py:408] Time since start: 38817.48s, 	Step: 73785, 	{'train/accuracy': 0.6938085556030273, 'train/loss': 1.2425113916397095, 'validation/accuracy': 0.6222400069236755, 'validation/loss': 1.575509786605835, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2482941150665283, 'test/num_examples': 10000, 'score': 34493.59297966957, 'total_duration': 38817.48063826561, 'accumulated_submission_time': 34493.59297966957, 'accumulated_eval_time': 4316.417612314224, 'accumulated_logging_time': 3.2457542419433594}
I0208 01:34:14.570773 139946414638848 logging_writer.py:48] [73785] accumulated_eval_time=4316.417612, accumulated_logging_time=3.245754, accumulated_submission_time=34493.592980, global_step=73785, preemption_count=0, score=34493.592980, test/accuracy=0.505900, test/loss=2.248294, test/num_examples=10000, total_duration=38817.480638, train/accuracy=0.693809, train/loss=1.242511, validation/accuracy=0.622240, validation/loss=1.575510, validation/num_examples=50000
I0208 01:34:21.153109 139946397853440 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3198421001434326, loss=2.325192928314209
I0208 01:35:05.305001 139946414638848 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.5863523483276367, loss=2.3772130012512207
I0208 01:35:52.026259 139946397853440 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.2518926858901978, loss=2.402853012084961
I0208 01:36:39.031969 139946414638848 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.174398422241211, loss=2.6312177181243896
I0208 01:37:26.254987 139946397853440 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.1312566995620728, loss=3.7127323150634766
I0208 01:38:13.162263 139946414638848 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.222041130065918, loss=2.974008798599243
I0208 01:39:00.217935 139946397853440 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.1654435396194458, loss=2.864863395690918
I0208 01:39:47.325412 139946414638848 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3609012365341187, loss=2.59275221824646
I0208 01:40:34.377775 139946397853440 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2512962818145752, loss=2.5684046745300293
I0208 01:41:14.732560 140107197974336 spec.py:321] Evaluating on the training split.
I0208 01:41:25.824386 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 01:42:04.610667 140107197974336 spec.py:349] Evaluating on the test split.
I0208 01:42:06.211417 140107197974336 submission_runner.py:408] Time since start: 39289.16s, 	Step: 74687, 	{'train/accuracy': 0.6724804639816284, 'train/loss': 1.328711986541748, 'validation/accuracy': 0.6229999661445618, 'validation/loss': 1.5702327489852905, 'validation/num_examples': 50000, 'test/accuracy': 0.503600001335144, 'test/loss': 2.223655939102173, 'test/num_examples': 10000, 'score': 34913.6925303936, 'total_duration': 39289.15676212311, 'accumulated_submission_time': 34913.6925303936, 'accumulated_eval_time': 4367.896469116211, 'accumulated_logging_time': 3.292013645172119}
I0208 01:42:06.247593 139946414638848 logging_writer.py:48] [74687] accumulated_eval_time=4367.896469, accumulated_logging_time=3.292014, accumulated_submission_time=34913.692530, global_step=74687, preemption_count=0, score=34913.692530, test/accuracy=0.503600, test/loss=2.223656, test/num_examples=10000, total_duration=39289.156762, train/accuracy=0.672480, train/loss=1.328712, validation/accuracy=0.623000, validation/loss=1.570233, validation/num_examples=50000
I0208 01:42:12.000633 139946397853440 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.4564591646194458, loss=2.450458288192749
I0208 01:42:55.950135 139946414638848 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.2249857187271118, loss=3.3850574493408203
I0208 01:43:42.825432 139946397853440 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.3639854192733765, loss=2.3658199310302734
I0208 01:44:30.200080 139946414638848 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.4833565950393677, loss=2.40854811668396
I0208 01:45:17.756041 139946397853440 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.2458157539367676, loss=2.9749579429626465
I0208 01:46:05.114179 139946414638848 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.4320729970932007, loss=2.6354517936706543
I0208 01:46:52.295611 139946397853440 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.1102590560913086, loss=3.3494961261749268
I0208 01:47:39.343366 139946414638848 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.2258795499801636, loss=4.840790748596191
I0208 01:48:26.720958 139946397853440 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.1571576595306396, loss=2.8779332637786865
I0208 01:49:06.588641 140107197974336 spec.py:321] Evaluating on the training split.
I0208 01:49:17.704714 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 01:49:56.471360 140107197974336 spec.py:349] Evaluating on the test split.
I0208 01:49:58.061180 140107197974336 submission_runner.py:408] Time since start: 39761.01s, 	Step: 75586, 	{'train/accuracy': 0.6808202862739563, 'train/loss': 1.340014934539795, 'validation/accuracy': 0.6244800090789795, 'validation/loss': 1.6081055402755737, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.2576117515563965, 'test/num_examples': 10000, 'score': 35333.97060585022, 'total_duration': 39761.00652861595, 'accumulated_submission_time': 35333.97060585022, 'accumulated_eval_time': 4419.369015693665, 'accumulated_logging_time': 3.339527130126953}
I0208 01:49:58.093360 139946414638848 logging_writer.py:48] [75586] accumulated_eval_time=4419.369016, accumulated_logging_time=3.339527, accumulated_submission_time=35333.970606, global_step=75586, preemption_count=0, score=35333.970606, test/accuracy=0.505800, test/loss=2.257612, test/num_examples=10000, total_duration=39761.006529, train/accuracy=0.680820, train/loss=1.340015, validation/accuracy=0.624480, validation/loss=1.608106, validation/num_examples=50000
I0208 01:50:04.251814 139946397853440 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.2086974382400513, loss=4.261096954345703
I0208 01:50:48.240376 139946414638848 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.3060356378555298, loss=2.4133310317993164
I0208 01:51:35.348755 139946397853440 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.41827392578125, loss=2.321113109588623
I0208 01:52:22.506798 139946414638848 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.1440110206604004, loss=3.5695056915283203
I0208 01:53:10.101789 139946397853440 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.3332616090774536, loss=2.5339274406433105
I0208 01:53:57.255103 139946414638848 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.557428002357483, loss=2.3528525829315186
I0208 01:54:44.658083 139946397853440 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.3072558641433716, loss=2.395354747772217
I0208 01:55:31.715174 139946414638848 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.1305211782455444, loss=4.324159622192383
I0208 01:56:19.340726 139946397853440 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.4498400688171387, loss=2.4390878677368164
I0208 01:56:58.070978 140107197974336 spec.py:321] Evaluating on the training split.
I0208 01:57:09.209617 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 01:57:51.378158 140107197974336 spec.py:349] Evaluating on the test split.
I0208 01:57:53.002902 140107197974336 submission_runner.py:408] Time since start: 40235.95s, 	Step: 76484, 	{'train/accuracy': 0.6885741949081421, 'train/loss': 1.2858747243881226, 'validation/accuracy': 0.6215599775314331, 'validation/loss': 1.5913811922073364, 'validation/num_examples': 50000, 'test/accuracy': 0.5015000104904175, 'test/loss': 2.2273242473602295, 'test/num_examples': 10000, 'score': 35753.88782739639, 'total_duration': 40235.9482319355, 'accumulated_submission_time': 35753.88782739639, 'accumulated_eval_time': 4474.300911664963, 'accumulated_logging_time': 3.38096022605896}
I0208 01:57:53.036912 139946414638848 logging_writer.py:48] [76484] accumulated_eval_time=4474.300912, accumulated_logging_time=3.380960, accumulated_submission_time=35753.887827, global_step=76484, preemption_count=0, score=35753.887827, test/accuracy=0.501500, test/loss=2.227324, test/num_examples=10000, total_duration=40235.948232, train/accuracy=0.688574, train/loss=1.285875, validation/accuracy=0.621560, validation/loss=1.591381, validation/num_examples=50000
I0208 01:58:00.030264 139946397853440 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.3615635633468628, loss=2.389665126800537
I0208 01:58:44.357219 139946414638848 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.2263685464859009, loss=3.3742778301239014
I0208 01:59:31.524019 139946397853440 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.4101585149765015, loss=2.4380111694335938
I0208 02:00:18.856005 139946414638848 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.3534287214279175, loss=2.9929051399230957
I0208 02:01:06.535018 139946397853440 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.2278929948806763, loss=4.09497594833374
I0208 02:01:54.058051 139946414638848 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.5403120517730713, loss=2.541964530944824
I0208 02:02:41.824696 139946397853440 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.213927984237671, loss=2.9421908855438232
I0208 02:03:29.579112 139946414638848 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.304684042930603, loss=2.4878077507019043
I0208 02:04:17.114402 139946397853440 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.3723896741867065, loss=2.7488808631896973
I0208 02:04:53.484355 140107197974336 spec.py:321] Evaluating on the training split.
I0208 02:05:04.639824 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 02:05:43.671877 140107197974336 spec.py:349] Evaluating on the test split.
I0208 02:05:45.275448 140107197974336 submission_runner.py:408] Time since start: 40708.22s, 	Step: 77378, 	{'train/accuracy': 0.6756640672683716, 'train/loss': 1.311980962753296, 'validation/accuracy': 0.6242200136184692, 'validation/loss': 1.5544767379760742, 'validation/num_examples': 50000, 'test/accuracy': 0.5027000308036804, 'test/loss': 2.2037034034729004, 'test/num_examples': 10000, 'score': 36174.273703336716, 'total_duration': 40708.220797777176, 'accumulated_submission_time': 36174.273703336716, 'accumulated_eval_time': 4526.091993093491, 'accumulated_logging_time': 3.4254894256591797}
I0208 02:05:45.311423 139946414638848 logging_writer.py:48] [77378] accumulated_eval_time=4526.091993, accumulated_logging_time=3.425489, accumulated_submission_time=36174.273703, global_step=77378, preemption_count=0, score=36174.273703, test/accuracy=0.502700, test/loss=2.203703, test/num_examples=10000, total_duration=40708.220798, train/accuracy=0.675664, train/loss=1.311981, validation/accuracy=0.624220, validation/loss=1.554477, validation/num_examples=50000
I0208 02:05:54.759461 139946397853440 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.5347864627838135, loss=4.98522424697876
I0208 02:06:39.495508 139946414638848 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.3639605045318604, loss=2.5196876525878906
I0208 02:07:26.990954 139946397853440 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.265803575515747, loss=2.7893247604370117
I0208 02:08:14.351094 139946414638848 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.357548713684082, loss=2.599907875061035
I0208 02:09:01.703681 139946397853440 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.1915444135665894, loss=4.206350803375244
I0208 02:09:49.246020 139946414638848 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.243545651435852, loss=4.833109378814697
I0208 02:10:36.892703 139946397853440 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.6198419332504272, loss=2.23476505279541
I0208 02:11:24.047480 139946414638848 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.3480859994888306, loss=3.6355490684509277
I0208 02:12:11.475654 139946397853440 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.3448785543441772, loss=2.3826093673706055
I0208 02:12:45.698354 140107197974336 spec.py:321] Evaluating on the training split.
I0208 02:12:56.853641 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 02:13:38.772634 140107197974336 spec.py:349] Evaluating on the test split.
I0208 02:13:40.364991 140107197974336 submission_runner.py:408] Time since start: 41183.31s, 	Step: 78274, 	{'train/accuracy': 0.6756445169448853, 'train/loss': 1.4200063943862915, 'validation/accuracy': 0.6239799857139587, 'validation/loss': 1.6556744575500488, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.3106143474578857, 'test/num_examples': 10000, 'score': 36594.59930849075, 'total_duration': 41183.310337781906, 'accumulated_submission_time': 36594.59930849075, 'accumulated_eval_time': 4580.758625507355, 'accumulated_logging_time': 3.471120595932007}
I0208 02:13:40.395140 139946414638848 logging_writer.py:48] [78274] accumulated_eval_time=4580.758626, accumulated_logging_time=3.471121, accumulated_submission_time=36594.599308, global_step=78274, preemption_count=0, score=36594.599308, test/accuracy=0.502000, test/loss=2.310614, test/num_examples=10000, total_duration=41183.310338, train/accuracy=0.675645, train/loss=1.420006, validation/accuracy=0.623980, validation/loss=1.655674, validation/num_examples=50000
I0208 02:13:51.494789 139946397853440 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.357538104057312, loss=2.2800164222717285
I0208 02:14:36.646371 139946414638848 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.3272342681884766, loss=2.3552443981170654
I0208 02:15:23.510859 139946397853440 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.401097297668457, loss=2.3865838050842285
I0208 02:16:10.916126 139946414638848 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.3693972826004028, loss=2.3054356575012207
I0208 02:16:57.946882 139946397853440 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.3619976043701172, loss=2.5231292247772217
I0208 02:17:45.526105 139946414638848 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.3882291316986084, loss=2.3870022296905518
I0208 02:18:32.648447 139946397853440 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.3389045000076294, loss=4.954361438751221
I0208 02:19:20.055673 139946414638848 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.1443926095962524, loss=5.019730567932129
I0208 02:20:07.396801 139946397853440 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.2787041664123535, loss=2.445134162902832
I0208 02:20:40.442931 140107197974336 spec.py:321] Evaluating on the training split.
I0208 02:20:51.323753 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 02:21:30.937041 140107197974336 spec.py:349] Evaluating on the test split.
I0208 02:21:32.535678 140107197974336 submission_runner.py:408] Time since start: 41655.48s, 	Step: 79172, 	{'train/accuracy': 0.6913671493530273, 'train/loss': 1.2846978902816772, 'validation/accuracy': 0.6258999705314636, 'validation/loss': 1.5905046463012695, 'validation/num_examples': 50000, 'test/accuracy': 0.49980002641677856, 'test/loss': 2.254966974258423, 'test/num_examples': 10000, 'score': 37014.5864136219, 'total_duration': 41655.481019973755, 'accumulated_submission_time': 37014.5864136219, 'accumulated_eval_time': 4632.851358652115, 'accumulated_logging_time': 3.5113136768341064}
I0208 02:21:32.566678 139946414638848 logging_writer.py:48] [79172] accumulated_eval_time=4632.851359, accumulated_logging_time=3.511314, accumulated_submission_time=37014.586414, global_step=79172, preemption_count=0, score=37014.586414, test/accuracy=0.499800, test/loss=2.254967, test/num_examples=10000, total_duration=41655.481020, train/accuracy=0.691367, train/loss=1.284698, validation/accuracy=0.625900, validation/loss=1.590505, validation/num_examples=50000
I0208 02:21:44.480946 139946397853440 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.348039984703064, loss=2.593831777572632
I0208 02:22:29.443607 139946414638848 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.014993667602539, loss=4.320850849151611
I0208 02:23:16.787298 139946397853440 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.4060702323913574, loss=2.3679792881011963
I0208 02:24:04.296607 139946414638848 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.2188727855682373, loss=5.024477005004883
I0208 02:24:51.684195 139946397853440 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.094795823097229, loss=4.691692352294922
I0208 02:25:38.883379 139946414638848 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.4779037237167358, loss=4.706462860107422
I0208 02:26:26.576977 139946397853440 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.4066011905670166, loss=2.5973639488220215
I0208 02:27:14.016873 139946414638848 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.4645113945007324, loss=2.577472686767578
I0208 02:28:01.190165 139946397853440 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.3942943811416626, loss=2.5565743446350098
I0208 02:28:32.707329 140107197974336 spec.py:321] Evaluating on the training split.
I0208 02:28:43.909310 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 02:29:24.175460 140107197974336 spec.py:349] Evaluating on the test split.
I0208 02:29:25.766926 140107197974336 submission_runner.py:408] Time since start: 42128.71s, 	Step: 80067, 	{'train/accuracy': 0.6728710532188416, 'train/loss': 1.3366504907608032, 'validation/accuracy': 0.6302599906921387, 'validation/loss': 1.55534029006958, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.2068710327148438, 'test/num_examples': 10000, 'score': 37434.66622328758, 'total_duration': 42128.71227145195, 'accumulated_submission_time': 37434.66622328758, 'accumulated_eval_time': 4685.910947561264, 'accumulated_logging_time': 3.5520379543304443}
I0208 02:29:25.800478 139946414638848 logging_writer.py:48] [80067] accumulated_eval_time=4685.910948, accumulated_logging_time=3.552038, accumulated_submission_time=37434.666223, global_step=80067, preemption_count=0, score=37434.666223, test/accuracy=0.505100, test/loss=2.206871, test/num_examples=10000, total_duration=42128.712271, train/accuracy=0.672871, train/loss=1.336650, validation/accuracy=0.630260, validation/loss=1.555340, validation/num_examples=50000
I0208 02:29:39.758997 139946397853440 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.0779651403427124, loss=3.561617851257324
I0208 02:30:24.874865 139946414638848 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.3358135223388672, loss=2.3899385929107666
I0208 02:31:11.698274 139946397853440 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.374436378479004, loss=2.358295440673828
I0208 02:31:58.883196 139946414638848 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.2762160301208496, loss=2.843012571334839
I0208 02:32:46.022371 139946397853440 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.1576170921325684, loss=3.9398033618927
I0208 02:33:33.234271 139946414638848 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.368241786956787, loss=2.3498990535736084
I0208 02:34:20.550473 139946397853440 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.4038633108139038, loss=2.3188118934631348
I0208 02:35:08.028291 139946414638848 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.0838851928710938, loss=4.403185844421387
I0208 02:35:55.135461 139946397853440 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.4242781400680542, loss=2.462369441986084
I0208 02:36:26.083792 140107197974336 spec.py:321] Evaluating on the training split.
I0208 02:36:37.209410 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 02:37:16.451707 140107197974336 spec.py:349] Evaluating on the test split.
I0208 02:37:18.051245 140107197974336 submission_runner.py:408] Time since start: 42601.00s, 	Step: 80967, 	{'train/accuracy': 0.6885156035423279, 'train/loss': 1.2635071277618408, 'validation/accuracy': 0.634660005569458, 'validation/loss': 1.520119309425354, 'validation/num_examples': 50000, 'test/accuracy': 0.5120000243186951, 'test/loss': 2.171586751937866, 'test/num_examples': 10000, 'score': 37854.888414382935, 'total_duration': 42600.99657249451, 'accumulated_submission_time': 37854.888414382935, 'accumulated_eval_time': 4737.878388643265, 'accumulated_logging_time': 3.595428943634033}
I0208 02:37:18.081895 139946414638848 logging_writer.py:48] [80967] accumulated_eval_time=4737.878389, accumulated_logging_time=3.595429, accumulated_submission_time=37854.888414, global_step=80967, preemption_count=0, score=37854.888414, test/accuracy=0.512000, test/loss=2.171587, test/num_examples=10000, total_duration=42600.996572, train/accuracy=0.688516, train/loss=1.263507, validation/accuracy=0.634660, validation/loss=1.520119, validation/num_examples=50000
I0208 02:37:32.054952 139946397853440 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.3028316497802734, loss=2.7867376804351807
I0208 02:38:17.442955 139946414638848 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.347273588180542, loss=2.8855056762695312
I0208 02:39:04.778710 139946397853440 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.093199610710144, loss=4.259963512420654
I0208 02:39:52.554266 139946414638848 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.53804349899292, loss=2.35355544090271
I0208 02:40:40.101057 139946397853440 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.6131588220596313, loss=2.2990870475769043
I0208 02:41:27.436930 139946414638848 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.1284626722335815, loss=3.0596845149993896
I0208 02:42:15.020081 139946397853440 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.3328403234481812, loss=2.368716239929199
I0208 02:43:02.279356 139946414638848 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.2387373447418213, loss=3.3570668697357178
I0208 02:43:49.474928 139946397853440 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.363290548324585, loss=2.3122339248657227
I0208 02:44:18.325522 140107197974336 spec.py:321] Evaluating on the training split.
I0208 02:44:29.620021 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 02:45:08.415328 140107197974336 spec.py:349] Evaluating on the test split.
I0208 02:45:10.016221 140107197974336 submission_runner.py:408] Time since start: 43072.96s, 	Step: 81862, 	{'train/accuracy': 0.6949023008346558, 'train/loss': 1.2488195896148682, 'validation/accuracy': 0.6343599557876587, 'validation/loss': 1.5292257070541382, 'validation/num_examples': 50000, 'test/accuracy': 0.5149000287055969, 'test/loss': 2.177933931350708, 'test/num_examples': 10000, 'score': 38275.072088718414, 'total_duration': 43072.96156978607, 'accumulated_submission_time': 38275.072088718414, 'accumulated_eval_time': 4789.569073200226, 'accumulated_logging_time': 3.6357104778289795}
I0208 02:45:10.048616 139946414638848 logging_writer.py:48] [81862] accumulated_eval_time=4789.569073, accumulated_logging_time=3.635710, accumulated_submission_time=38275.072089, global_step=81862, preemption_count=0, score=38275.072089, test/accuracy=0.514900, test/loss=2.177934, test/num_examples=10000, total_duration=43072.961570, train/accuracy=0.694902, train/loss=1.248820, validation/accuracy=0.634360, validation/loss=1.529226, validation/num_examples=50000
I0208 02:45:26.070831 139946397853440 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.1362290382385254, loss=4.5922322273254395
I0208 02:46:11.563398 139946414638848 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.4969161748886108, loss=2.226475954055786
I0208 02:46:58.380938 139946397853440 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.383323073387146, loss=2.4419846534729004
I0208 02:47:45.437169 139946414638848 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.4419664144515991, loss=2.41447377204895
I0208 02:48:32.790191 139946397853440 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.2586153745651245, loss=3.5796332359313965
I0208 02:49:20.555985 139946414638848 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.5469520092010498, loss=2.4301095008850098
I0208 02:50:08.155884 139946397853440 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.1791073083877563, loss=4.020164489746094
I0208 02:50:55.614665 139946414638848 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.3184051513671875, loss=2.3136086463928223
I0208 02:51:42.738831 139946397853440 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.39582359790802, loss=3.0690317153930664
I0208 02:52:10.371925 140107197974336 spec.py:321] Evaluating on the training split.
I0208 02:52:21.342664 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 02:53:01.889964 140107197974336 spec.py:349] Evaluating on the test split.
I0208 02:53:03.502621 140107197974336 submission_runner.py:408] Time since start: 43546.45s, 	Step: 82760, 	{'train/accuracy': 0.6834570169448853, 'train/loss': 1.3045527935028076, 'validation/accuracy': 0.6303600072860718, 'validation/loss': 1.5471371412277222, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.2118685245513916, 'test/num_examples': 10000, 'score': 38695.33440542221, 'total_duration': 43546.44796872139, 'accumulated_submission_time': 38695.33440542221, 'accumulated_eval_time': 4842.699766159058, 'accumulated_logging_time': 3.677600860595703}
I0208 02:53:03.536956 139946414638848 logging_writer.py:48] [82760] accumulated_eval_time=4842.699766, accumulated_logging_time=3.677601, accumulated_submission_time=38695.334405, global_step=82760, preemption_count=0, score=38695.334405, test/accuracy=0.507400, test/loss=2.211869, test/num_examples=10000, total_duration=43546.447969, train/accuracy=0.683457, train/loss=1.304553, validation/accuracy=0.630360, validation/loss=1.547137, validation/num_examples=50000
I0208 02:53:20.384622 139946397853440 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.13027024269104, loss=4.937075138092041
I0208 02:54:06.410473 139946414638848 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.1326872110366821, loss=2.7943482398986816
I0208 02:54:53.885680 139946397853440 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.198661208152771, loss=3.9620988368988037
I0208 02:55:41.427096 139946414638848 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.305378794670105, loss=2.4311516284942627
I0208 02:56:28.878894 139946397853440 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.2700982093811035, loss=2.731825351715088
I0208 02:57:16.091117 139946414638848 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.2845839262008667, loss=2.1931796073913574
I0208 02:58:03.285443 139946397853440 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.3243203163146973, loss=2.450139045715332
I0208 02:58:50.679320 139946414638848 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.2254530191421509, loss=3.7266595363616943
I0208 02:59:38.261322 139946397853440 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.3790560960769653, loss=2.3521740436553955
I0208 03:00:03.939734 140107197974336 spec.py:321] Evaluating on the training split.
I0208 03:00:14.810627 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 03:00:53.765679 140107197974336 spec.py:349] Evaluating on the test split.
I0208 03:00:55.368639 140107197974336 submission_runner.py:408] Time since start: 44018.31s, 	Step: 83655, 	{'train/accuracy': 0.6850390434265137, 'train/loss': 1.279969573020935, 'validation/accuracy': 0.6342200040817261, 'validation/loss': 1.5213388204574585, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.1926448345184326, 'test/num_examples': 10000, 'score': 39115.676505327225, 'total_duration': 44018.31398630142, 'accumulated_submission_time': 39115.676505327225, 'accumulated_eval_time': 4894.128688812256, 'accumulated_logging_time': 3.7216544151306152}
I0208 03:00:55.398002 139946414638848 logging_writer.py:48] [83655] accumulated_eval_time=4894.128689, accumulated_logging_time=3.721654, accumulated_submission_time=39115.676505, global_step=83655, preemption_count=0, score=39115.676505, test/accuracy=0.512200, test/loss=2.192645, test/num_examples=10000, total_duration=44018.313986, train/accuracy=0.685039, train/loss=1.279970, validation/accuracy=0.634220, validation/loss=1.521339, validation/num_examples=50000
I0208 03:01:14.455421 139946397853440 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.3389149904251099, loss=2.3974204063415527
I0208 03:02:00.312006 139946414638848 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.260959267616272, loss=4.44504451751709
I0208 03:02:48.056993 139946397853440 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.2548754215240479, loss=3.5227715969085693
I0208 03:03:35.253342 139946414638848 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.4007501602172852, loss=2.6379549503326416
I0208 03:04:22.677785 139946397853440 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.1882426738739014, loss=4.8523969650268555
I0208 03:05:09.977418 139946414638848 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.3620195388793945, loss=2.2940621376037598
I0208 03:05:57.321899 139946397853440 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.257106065750122, loss=4.98089599609375
I0208 03:06:44.906807 139946414638848 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.1873935461044312, loss=3.1590301990509033
I0208 03:07:32.074851 139946397853440 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.2565914392471313, loss=4.319100379943848
I0208 03:07:55.678390 140107197974336 spec.py:321] Evaluating on the training split.
I0208 03:08:06.791191 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 03:08:45.936443 140107197974336 spec.py:349] Evaluating on the test split.
I0208 03:08:47.540617 140107197974336 submission_runner.py:408] Time since start: 44490.49s, 	Step: 84551, 	{'train/accuracy': 0.703125, 'train/loss': 1.225046157836914, 'validation/accuracy': 0.6385599970817566, 'validation/loss': 1.5180543661117554, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.160423755645752, 'test/num_examples': 10000, 'score': 39535.89488720894, 'total_duration': 44490.48594856262, 'accumulated_submission_time': 39535.89488720894, 'accumulated_eval_time': 4945.990887403488, 'accumulated_logging_time': 3.761420726776123}
I0208 03:08:47.579070 139946414638848 logging_writer.py:48] [84551] accumulated_eval_time=4945.990887, accumulated_logging_time=3.761421, accumulated_submission_time=39535.894887, global_step=84551, preemption_count=0, score=39535.894887, test/accuracy=0.515900, test/loss=2.160424, test/num_examples=10000, total_duration=44490.485949, train/accuracy=0.703125, train/loss=1.225046, validation/accuracy=0.638560, validation/loss=1.518054, validation/num_examples=50000
I0208 03:09:08.162767 139946397853440 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.2582517862319946, loss=2.6012609004974365
I0208 03:09:54.069759 139946414638848 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.3283615112304688, loss=4.551562309265137
I0208 03:10:41.373020 139946397853440 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.4315372705459595, loss=2.4269323348999023
I0208 03:11:28.672858 139946414638848 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.3252989053726196, loss=2.566579818725586
I0208 03:12:15.901960 139946397853440 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.2915478944778442, loss=2.421922206878662
I0208 03:13:03.344075 139946414638848 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.3757727146148682, loss=2.4156222343444824
I0208 03:13:50.531462 139946397853440 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.224326491355896, loss=3.1029038429260254
I0208 03:14:38.138537 139946414638848 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.0981709957122803, loss=4.293684005737305
I0208 03:15:25.452565 139946397853440 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.371195673942566, loss=3.143061637878418
I0208 03:15:47.811760 140107197974336 spec.py:321] Evaluating on the training split.
I0208 03:15:58.901512 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 03:16:37.331165 140107197974336 spec.py:349] Evaluating on the test split.
I0208 03:16:38.929781 140107197974336 submission_runner.py:408] Time since start: 44961.88s, 	Step: 85449, 	{'train/accuracy': 0.683300793170929, 'train/loss': 1.303541898727417, 'validation/accuracy': 0.6320399641990662, 'validation/loss': 1.532470464706421, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.173837900161743, 'test/num_examples': 10000, 'score': 39956.06528735161, 'total_duration': 44961.87512779236, 'accumulated_submission_time': 39956.06528735161, 'accumulated_eval_time': 4997.108908653259, 'accumulated_logging_time': 3.8109896183013916}
I0208 03:16:38.959319 139946414638848 logging_writer.py:48] [85449] accumulated_eval_time=4997.108909, accumulated_logging_time=3.810990, accumulated_submission_time=39956.065287, global_step=85449, preemption_count=0, score=39956.065287, test/accuracy=0.516100, test/loss=2.173838, test/num_examples=10000, total_duration=44961.875128, train/accuracy=0.683301, train/loss=1.303542, validation/accuracy=0.632040, validation/loss=1.532470, validation/num_examples=50000
I0208 03:17:00.327852 139946397853440 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.4909213781356812, loss=2.3069639205932617
I0208 03:17:46.679729 139946414638848 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.456879734992981, loss=2.154428243637085
I0208 03:18:34.097357 139946397853440 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.4611705541610718, loss=2.3229329586029053
I0208 03:19:21.245796 139946414638848 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.2548248767852783, loss=3.086665630340576
I0208 03:20:08.867013 139946397853440 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.2342374324798584, loss=3.8357439041137695
I0208 03:20:56.311719 139946414638848 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.6054984331130981, loss=2.436720371246338
I0208 03:21:43.825776 139946397853440 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.4397833347320557, loss=2.308347463607788
I0208 03:22:31.439945 139946414638848 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.181725263595581, loss=3.2586798667907715
I0208 03:23:18.710594 139946397853440 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.1542320251464844, loss=4.492956638336182
I0208 03:23:39.070644 140107197974336 spec.py:321] Evaluating on the training split.
I0208 03:23:50.384902 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 03:24:29.412647 140107197974336 spec.py:349] Evaluating on the test split.
I0208 03:24:31.012502 140107197974336 submission_runner.py:408] Time since start: 45433.96s, 	Step: 86344, 	{'train/accuracy': 0.6930859088897705, 'train/loss': 1.2610045671463013, 'validation/accuracy': 0.6387799978256226, 'validation/loss': 1.5271767377853394, 'validation/num_examples': 50000, 'test/accuracy': 0.5146999955177307, 'test/loss': 2.1729276180267334, 'test/num_examples': 10000, 'score': 40376.114825725555, 'total_duration': 45433.95785403252, 'accumulated_submission_time': 40376.114825725555, 'accumulated_eval_time': 5049.05076956749, 'accumulated_logging_time': 3.851077079772949}
I0208 03:24:31.046203 139946414638848 logging_writer.py:48] [86344] accumulated_eval_time=5049.050770, accumulated_logging_time=3.851077, accumulated_submission_time=40376.114826, global_step=86344, preemption_count=0, score=40376.114826, test/accuracy=0.514700, test/loss=2.172928, test/num_examples=10000, total_duration=45433.957854, train/accuracy=0.693086, train/loss=1.261005, validation/accuracy=0.638780, validation/loss=1.527177, validation/num_examples=50000
I0208 03:24:54.469194 139946397853440 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.4259684085845947, loss=2.4211769104003906
I0208 03:25:41.022311 139946414638848 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.1218067407608032, loss=3.1494500637054443
I0208 03:26:28.319470 139946397853440 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.380447506904602, loss=2.1882429122924805
I0208 03:27:15.704969 139946414638848 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.1673098802566528, loss=3.2988152503967285
I0208 03:28:02.996232 139946397853440 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.4399281740188599, loss=2.367959499359131
I0208 03:28:50.287617 139946414638848 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.5361900329589844, loss=2.4067280292510986
I0208 03:29:37.594128 139946397853440 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.465219259262085, loss=2.269225835800171
I0208 03:30:25.199791 139946414638848 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.4820295572280884, loss=2.251201868057251
I0208 03:31:12.508186 139946397853440 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.2841037511825562, loss=2.4730911254882812
I0208 03:31:31.277508 140107197974336 spec.py:321] Evaluating on the training split.
I0208 03:31:42.193684 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 03:32:21.147580 140107197974336 spec.py:349] Evaluating on the test split.
I0208 03:32:22.746004 140107197974336 submission_runner.py:408] Time since start: 45905.69s, 	Step: 87241, 	{'train/accuracy': 0.6998632550239563, 'train/loss': 1.2377312183380127, 'validation/accuracy': 0.638700008392334, 'validation/loss': 1.5107841491699219, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.1511378288269043, 'test/num_examples': 10000, 'score': 40796.28626155853, 'total_duration': 45905.69135284424, 'accumulated_submission_time': 40796.28626155853, 'accumulated_eval_time': 5100.5192720890045, 'accumulated_logging_time': 3.894582748413086}
I0208 03:32:22.779859 139946414638848 logging_writer.py:48] [87241] accumulated_eval_time=5100.519272, accumulated_logging_time=3.894583, accumulated_submission_time=40796.286262, global_step=87241, preemption_count=0, score=40796.286262, test/accuracy=0.519700, test/loss=2.151138, test/num_examples=10000, total_duration=45905.691353, train/accuracy=0.699863, train/loss=1.237731, validation/accuracy=0.638700, validation/loss=1.510784, validation/num_examples=50000
I0208 03:32:47.430027 139946397853440 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.6740806102752686, loss=2.44509220123291
I0208 03:33:35.083754 139946414638848 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.3003051280975342, loss=2.5483791828155518
I0208 03:34:23.013663 139946397853440 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.5115952491760254, loss=2.2644689083099365
I0208 03:35:11.019243 139946414638848 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.3683751821517944, loss=2.272404193878174
I0208 03:35:58.901019 139946397853440 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.5094338655471802, loss=2.517946720123291
I0208 03:36:46.933424 139946414638848 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.456497073173523, loss=2.350612163543701
I0208 03:37:34.992997 139946397853440 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.2864474058151245, loss=3.2504332065582275
I0208 03:38:22.907605 139946414638848 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.28846275806427, loss=2.5912692546844482
I0208 03:39:11.123201 139946397853440 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.2837879657745361, loss=2.2692677974700928
I0208 03:39:22.793734 140107197974336 spec.py:321] Evaluating on the training split.
I0208 03:39:33.915314 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 03:40:14.402224 140107197974336 spec.py:349] Evaluating on the test split.
I0208 03:40:16.010054 140107197974336 submission_runner.py:408] Time since start: 46378.96s, 	Step: 88126, 	{'train/accuracy': 0.6940820217132568, 'train/loss': 1.2332541942596436, 'validation/accuracy': 0.637179970741272, 'validation/loss': 1.494731068611145, 'validation/num_examples': 50000, 'test/accuracy': 0.5146000385284424, 'test/loss': 2.1517715454101562, 'test/num_examples': 10000, 'score': 41216.240828990936, 'total_duration': 46378.955389261246, 'accumulated_submission_time': 41216.240828990936, 'accumulated_eval_time': 5153.7355852127075, 'accumulated_logging_time': 3.9385111331939697}
I0208 03:40:16.043951 139946414638848 logging_writer.py:48] [88126] accumulated_eval_time=5153.735585, accumulated_logging_time=3.938511, accumulated_submission_time=41216.240829, global_step=88126, preemption_count=0, score=41216.240829, test/accuracy=0.514600, test/loss=2.151772, test/num_examples=10000, total_duration=46378.955389, train/accuracy=0.694082, train/loss=1.233254, validation/accuracy=0.637180, validation/loss=1.494731, validation/num_examples=50000
I0208 03:40:47.532707 139946397853440 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.2948369979858398, loss=3.791469097137451
I0208 03:41:34.509696 139946414638848 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.266030192375183, loss=3.3933792114257812
I0208 03:42:21.961392 139946397853440 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.4287407398223877, loss=2.3670945167541504
I0208 03:43:09.099117 139946414638848 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.2701833248138428, loss=4.695628643035889
I0208 03:43:56.478734 139946397853440 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.4016914367675781, loss=2.2866971492767334
I0208 03:44:44.038353 139946414638848 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.4450666904449463, loss=2.442153215408325
I0208 03:45:31.580996 139946397853440 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.4380249977111816, loss=2.0627095699310303
I0208 03:46:40.872938 139946414638848 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.4382489919662476, loss=2.553218126296997
I0208 03:47:16.193952 140107197974336 spec.py:321] Evaluating on the training split.
I0208 03:47:27.209476 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 03:48:06.124566 140107197974336 spec.py:349] Evaluating on the test split.
I0208 03:48:07.726673 140107197974336 submission_runner.py:408] Time since start: 46850.67s, 	Step: 88977, 	{'train/accuracy': 0.6952733993530273, 'train/loss': 1.2344191074371338, 'validation/accuracy': 0.6416999697685242, 'validation/loss': 1.4802577495574951, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.1336536407470703, 'test/num_examples': 10000, 'score': 41636.33084964752, 'total_duration': 46850.67201781273, 'accumulated_submission_time': 41636.33084964752, 'accumulated_eval_time': 5205.26829957962, 'accumulated_logging_time': 3.983673334121704}
I0208 03:48:07.757499 139946397853440 logging_writer.py:48] [88977] accumulated_eval_time=5205.268300, accumulated_logging_time=3.983673, accumulated_submission_time=41636.330850, global_step=88977, preemption_count=0, score=41636.330850, test/accuracy=0.520100, test/loss=2.133654, test/num_examples=10000, total_duration=46850.672018, train/accuracy=0.695273, train/loss=1.234419, validation/accuracy=0.641700, validation/loss=1.480258, validation/num_examples=50000
I0208 03:48:17.611730 139946414638848 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.4492419958114624, loss=2.698513984680176
I0208 03:49:02.511519 139946397853440 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.2358272075653076, loss=2.8504748344421387
I0208 03:49:49.725659 139946414638848 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.4492747783660889, loss=2.4396371841430664
I0208 03:50:37.176235 139946397853440 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.3977432250976562, loss=2.2298734188079834
I0208 03:51:25.806012 139946414638848 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.296803593635559, loss=2.725935220718384
I0208 03:52:13.023033 139946397853440 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.578066349029541, loss=2.304776906967163
I0208 03:53:00.133985 139946414638848 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.2958039045333862, loss=4.921361446380615
I0208 03:53:47.248841 139946397853440 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.5539238452911377, loss=2.4049508571624756
I0208 03:54:34.796779 139946414638848 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.3029545545578003, loss=2.897411823272705
I0208 03:55:07.732172 140107197974336 spec.py:321] Evaluating on the training split.
I0208 03:55:18.781631 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 03:55:59.431858 140107197974336 spec.py:349] Evaluating on the test split.
I0208 03:56:01.035224 140107197974336 submission_runner.py:408] Time since start: 47323.98s, 	Step: 89871, 	{'train/accuracy': 0.704296886920929, 'train/loss': 1.2123621702194214, 'validation/accuracy': 0.6403399705886841, 'validation/loss': 1.5044403076171875, 'validation/num_examples': 50000, 'test/accuracy': 0.516700029373169, 'test/loss': 2.169416666030884, 'test/num_examples': 10000, 'score': 42056.24417757988, 'total_duration': 47323.980553388596, 'accumulated_submission_time': 42056.24417757988, 'accumulated_eval_time': 5258.57132768631, 'accumulated_logging_time': 4.025211334228516}
I0208 03:56:01.070387 139946397853440 logging_writer.py:48] [89871] accumulated_eval_time=5258.571328, accumulated_logging_time=4.025211, accumulated_submission_time=42056.244178, global_step=89871, preemption_count=0, score=42056.244178, test/accuracy=0.516700, test/loss=2.169417, test/num_examples=10000, total_duration=47323.980553, train/accuracy=0.704297, train/loss=1.212362, validation/accuracy=0.640340, validation/loss=1.504440, validation/num_examples=50000
I0208 03:56:13.411985 139946414638848 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.2920501232147217, loss=2.5323305130004883
I0208 03:56:58.248667 139946397853440 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.3933433294296265, loss=2.55832576751709
I0208 03:57:45.563558 139946414638848 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.6776057481765747, loss=2.3243584632873535
I0208 03:58:32.767553 139946397853440 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.646658182144165, loss=2.2319748401641846
I0208 03:59:19.908472 139946414638848 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.5150810480117798, loss=2.289522647857666
I0208 04:00:07.290013 139946397853440 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.4662739038467407, loss=2.4252066612243652
I0208 04:00:54.460300 139946414638848 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.58308744430542, loss=2.340623617172241
I0208 04:01:41.591948 139946397853440 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.5437829494476318, loss=2.327606201171875
I0208 04:02:28.608849 139946414638848 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.3800146579742432, loss=2.1847212314605713
I0208 04:03:01.290372 140107197974336 spec.py:321] Evaluating on the training split.
I0208 04:03:12.551279 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 04:03:51.343325 140107197974336 spec.py:349] Evaluating on the test split.
I0208 04:03:52.946082 140107197974336 submission_runner.py:408] Time since start: 47795.89s, 	Step: 90771, 	{'train/accuracy': 0.6926171779632568, 'train/loss': 1.247671127319336, 'validation/accuracy': 0.6415799856185913, 'validation/loss': 1.4853407144546509, 'validation/num_examples': 50000, 'test/accuracy': 0.5189000368118286, 'test/loss': 2.1496667861938477, 'test/num_examples': 10000, 'score': 42476.40330886841, 'total_duration': 47795.89140582085, 'accumulated_submission_time': 42476.40330886841, 'accumulated_eval_time': 5310.227010965347, 'accumulated_logging_time': 4.070418834686279}
I0208 04:03:52.987519 139946397853440 logging_writer.py:48] [90771] accumulated_eval_time=5310.227011, accumulated_logging_time=4.070419, accumulated_submission_time=42476.403309, global_step=90771, preemption_count=0, score=42476.403309, test/accuracy=0.518900, test/loss=2.149667, test/num_examples=10000, total_duration=47795.891406, train/accuracy=0.692617, train/loss=1.247671, validation/accuracy=0.641580, validation/loss=1.485341, validation/num_examples=50000
I0208 04:04:05.313801 139946414638848 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.1919649839401245, loss=4.053467273712158
I0208 04:04:50.737049 139946397853440 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.44343900680542, loss=2.275953531265259
I0208 04:05:38.147378 139946414638848 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.476137399673462, loss=3.0911478996276855
I0208 04:06:25.790635 139946397853440 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.253007173538208, loss=4.275414943695068
I0208 04:07:13.369103 139946414638848 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.251698613166809, loss=4.146023750305176
I0208 04:08:00.778973 139946397853440 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.647835612297058, loss=2.26086688041687
I0208 04:08:48.686238 139946414638848 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.1945009231567383, loss=3.0103721618652344
I0208 04:09:36.218902 139946397853440 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.4269651174545288, loss=2.431312084197998
I0208 04:10:23.857084 139946414638848 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.4197282791137695, loss=2.2196927070617676
I0208 04:10:53.042652 140107197974336 spec.py:321] Evaluating on the training split.
I0208 04:11:03.955883 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 04:11:43.291469 140107197974336 spec.py:349] Evaluating on the test split.
I0208 04:11:44.886957 140107197974336 submission_runner.py:408] Time since start: 48267.83s, 	Step: 91663, 	{'train/accuracy': 0.7013866901397705, 'train/loss': 1.2053130865097046, 'validation/accuracy': 0.645639955997467, 'validation/loss': 1.4752509593963623, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.1370153427124023, 'test/num_examples': 10000, 'score': 42896.397490262985, 'total_duration': 48267.832307338715, 'accumulated_submission_time': 42896.397490262985, 'accumulated_eval_time': 5362.071304321289, 'accumulated_logging_time': 4.122549295425415}
I0208 04:11:44.918936 139946397853440 logging_writer.py:48] [91663] accumulated_eval_time=5362.071304, accumulated_logging_time=4.122549, accumulated_submission_time=42896.397490, global_step=91663, preemption_count=0, score=42896.397490, test/accuracy=0.519600, test/loss=2.137015, test/num_examples=10000, total_duration=48267.832307, train/accuracy=0.701387, train/loss=1.205313, validation/accuracy=0.645640, validation/loss=1.475251, validation/num_examples=50000
I0208 04:12:00.527911 139946414638848 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.389125943183899, loss=2.2462127208709717
I0208 04:12:46.037885 139946397853440 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.3562054634094238, loss=4.390920162200928
I0208 04:13:33.418820 139946414638848 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.4728282690048218, loss=3.2617435455322266
I0208 04:14:20.966814 139946397853440 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.5071455240249634, loss=2.179218292236328
I0208 04:15:08.769708 139946414638848 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.3302977085113525, loss=3.1588215827941895
I0208 04:15:55.963885 139946397853440 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.1749192476272583, loss=3.5590708255767822
I0208 04:16:43.432528 139946414638848 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.3583956956863403, loss=2.5695393085479736
I0208 04:17:31.143207 139946397853440 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.3133742809295654, loss=2.5693633556365967
I0208 04:18:18.893055 139946414638848 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.454287052154541, loss=2.2228686809539795
I0208 04:18:45.187431 140107197974336 spec.py:321] Evaluating on the training split.
I0208 04:18:56.092354 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 04:19:34.961966 140107197974336 spec.py:349] Evaluating on the test split.
I0208 04:19:36.563727 140107197974336 submission_runner.py:408] Time since start: 48739.51s, 	Step: 92557, 	{'train/accuracy': 0.7112109065055847, 'train/loss': 1.1575263738632202, 'validation/accuracy': 0.6488800048828125, 'validation/loss': 1.4489514827728271, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.1200804710388184, 'test/num_examples': 10000, 'score': 43316.60587668419, 'total_duration': 48739.50898981094, 'accumulated_submission_time': 43316.60587668419, 'accumulated_eval_time': 5413.447510957718, 'accumulated_logging_time': 4.164551734924316}
I0208 04:19:36.599406 139946397853440 logging_writer.py:48] [92557] accumulated_eval_time=5413.447511, accumulated_logging_time=4.164552, accumulated_submission_time=43316.605877, global_step=92557, preemption_count=0, score=43316.605877, test/accuracy=0.525100, test/loss=2.120080, test/num_examples=10000, total_duration=48739.508990, train/accuracy=0.711211, train/loss=1.157526, validation/accuracy=0.648880, validation/loss=1.448951, validation/num_examples=50000
I0208 04:19:54.689393 139946414638848 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.397863745689392, loss=2.2882585525512695
I0208 04:20:40.664201 139946397853440 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.5255762338638306, loss=2.298739433288574
I0208 04:21:27.743901 139946414638848 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.3522586822509766, loss=4.380617141723633
I0208 04:22:15.125844 139946397853440 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.1775633096694946, loss=3.306349039077759
I0208 04:23:02.282368 139946414638848 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.4214041233062744, loss=2.200570821762085
I0208 04:23:49.477416 139946397853440 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.3219763040542603, loss=2.3213157653808594
I0208 04:24:36.967567 139946414638848 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.1844682693481445, loss=4.779038906097412
I0208 04:25:24.246878 139946397853440 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.3989697694778442, loss=4.265352725982666
I0208 04:26:11.798824 139946414638848 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.4229238033294678, loss=3.0361852645874023
I0208 04:26:36.916696 140107197974336 spec.py:321] Evaluating on the training split.
I0208 04:26:48.350632 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 04:27:27.852872 140107197974336 spec.py:349] Evaluating on the test split.
I0208 04:27:29.456997 140107197974336 submission_runner.py:408] Time since start: 49212.40s, 	Step: 93455, 	{'train/accuracy': 0.6953515410423279, 'train/loss': 1.2690379619598389, 'validation/accuracy': 0.6404399871826172, 'validation/loss': 1.5253348350524902, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.171663999557495, 'test/num_examples': 10000, 'score': 43736.8600165844, 'total_duration': 49212.40234661102, 'accumulated_submission_time': 43736.8600165844, 'accumulated_eval_time': 5465.9878051280975, 'accumulated_logging_time': 4.211713075637817}
I0208 04:27:29.489933 139946397853440 logging_writer.py:48] [93455] accumulated_eval_time=5465.987805, accumulated_logging_time=4.211713, accumulated_submission_time=43736.860017, global_step=93455, preemption_count=0, score=43736.860017, test/accuracy=0.520500, test/loss=2.171664, test/num_examples=10000, total_duration=49212.402347, train/accuracy=0.695352, train/loss=1.269038, validation/accuracy=0.640440, validation/loss=1.525335, validation/num_examples=50000
I0208 04:27:48.644309 139946414638848 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.2111977338790894, loss=3.6600818634033203
I0208 04:28:34.429245 139946397853440 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.2368435859680176, loss=4.701106071472168
I0208 04:29:21.503753 139946414638848 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.5054690837860107, loss=2.169687271118164
I0208 04:30:08.915225 139946397853440 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.5132070779800415, loss=2.23372220993042
I0208 04:30:56.369855 139946414638848 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.3951904773712158, loss=2.433826446533203
I0208 04:31:43.539700 139946397853440 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.4166282415390015, loss=2.2402257919311523
I0208 04:32:30.753881 139946414638848 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.1860231161117554, loss=3.693204164505005
I0208 04:33:18.092297 139946397853440 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.2526514530181885, loss=4.838248252868652
I0208 04:34:05.397386 139946414638848 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.5023781061172485, loss=2.313399314880371
I0208 04:34:29.807001 140107197974336 spec.py:321] Evaluating on the training split.
I0208 04:34:40.858542 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 04:35:22.479089 140107197974336 spec.py:349] Evaluating on the test split.
I0208 04:35:24.079333 140107197974336 submission_runner.py:408] Time since start: 49687.02s, 	Step: 94353, 	{'train/accuracy': 0.7071093320846558, 'train/loss': 1.196900486946106, 'validation/accuracy': 0.6487599611282349, 'validation/loss': 1.4550457000732422, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.0961992740631104, 'test/num_examples': 10000, 'score': 44157.11583042145, 'total_duration': 49687.024679899216, 'accumulated_submission_time': 44157.11583042145, 'accumulated_eval_time': 5520.260136604309, 'accumulated_logging_time': 4.254514455795288}
I0208 04:35:24.116203 139946397853440 logging_writer.py:48] [94353] accumulated_eval_time=5520.260137, accumulated_logging_time=4.254514, accumulated_submission_time=44157.115830, global_step=94353, preemption_count=0, score=44157.115830, test/accuracy=0.526900, test/loss=2.096199, test/num_examples=10000, total_duration=49687.024680, train/accuracy=0.707109, train/loss=1.196900, validation/accuracy=0.648760, validation/loss=1.455046, validation/num_examples=50000
I0208 04:35:43.841999 139946414638848 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.135779857635498, loss=4.137462139129639
I0208 04:36:29.905932 139946397853440 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.4391098022460938, loss=2.145117998123169
I0208 04:37:17.358596 139946414638848 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.3290069103240967, loss=2.689488410949707
I0208 04:38:04.788099 139946397853440 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.367321491241455, loss=4.553103923797607
I0208 04:38:52.029314 139946414638848 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.4728621244430542, loss=4.678498268127441
I0208 04:39:39.139779 139946397853440 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.4866617918014526, loss=2.0936169624328613
I0208 04:40:26.490570 139946414638848 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.275754690170288, loss=4.446888446807861
I0208 04:41:14.035691 139946397853440 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.482814073562622, loss=2.3396358489990234
I0208 04:42:01.298485 139946414638848 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.178850769996643, loss=3.1365842819213867
I0208 04:42:24.467138 140107197974336 spec.py:321] Evaluating on the training split.
I0208 04:42:35.651135 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 04:43:15.314690 140107197974336 spec.py:349] Evaluating on the test split.
I0208 04:43:16.911406 140107197974336 submission_runner.py:408] Time since start: 50159.86s, 	Step: 95251, 	{'train/accuracy': 0.7099023461341858, 'train/loss': 1.2014402151107788, 'validation/accuracy': 0.6468999981880188, 'validation/loss': 1.4840155839920044, 'validation/num_examples': 50000, 'test/accuracy': 0.5276000499725342, 'test/loss': 2.12835431098938, 'test/num_examples': 10000, 'score': 44577.40431547165, 'total_duration': 50159.85675024986, 'accumulated_submission_time': 44577.40431547165, 'accumulated_eval_time': 5572.704406738281, 'accumulated_logging_time': 4.302710056304932}
I0208 04:43:16.949467 139946397853440 logging_writer.py:48] [95251] accumulated_eval_time=5572.704407, accumulated_logging_time=4.302710, accumulated_submission_time=44577.404315, global_step=95251, preemption_count=0, score=44577.404315, test/accuracy=0.527600, test/loss=2.128354, test/num_examples=10000, total_duration=50159.856750, train/accuracy=0.709902, train/loss=1.201440, validation/accuracy=0.646900, validation/loss=1.484016, validation/num_examples=50000
I0208 04:43:37.506276 139946414638848 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.3621501922607422, loss=2.552971363067627
I0208 04:44:23.529955 139946397853440 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.49367094039917, loss=2.193575382232666
I0208 04:45:11.239237 139946414638848 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.4918885231018066, loss=2.2191250324249268
I0208 04:45:58.416970 139946397853440 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.2700880765914917, loss=4.5577545166015625
I0208 04:46:45.625220 139946414638848 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.5235196352005005, loss=2.2081427574157715
I0208 04:47:32.766751 139946397853440 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.6837646961212158, loss=2.190932035446167
I0208 04:48:20.138457 139946414638848 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.3172861337661743, loss=3.2503578662872314
I0208 04:49:07.404150 139946397853440 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.5844897031784058, loss=2.2897789478302
I0208 04:49:54.596584 139946414638848 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.2011353969573975, loss=4.2017502784729
I0208 04:50:17.230175 140107197974336 spec.py:321] Evaluating on the training split.
I0208 04:50:28.349677 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 04:51:07.342988 140107197974336 spec.py:349] Evaluating on the test split.
I0208 04:51:08.941428 140107197974336 submission_runner.py:408] Time since start: 50631.89s, 	Step: 96149, 	{'train/accuracy': 0.7105468511581421, 'train/loss': 1.17483651638031, 'validation/accuracy': 0.6487999558448792, 'validation/loss': 1.4631506204605103, 'validation/num_examples': 50000, 'test/accuracy': 0.525700032711029, 'test/loss': 2.12905216217041, 'test/num_examples': 10000, 'score': 44997.624660253525, 'total_duration': 50631.88677740097, 'accumulated_submission_time': 44997.624660253525, 'accumulated_eval_time': 5624.4156901836395, 'accumulated_logging_time': 4.350196361541748}
I0208 04:51:08.975347 139946397853440 logging_writer.py:48] [96149] accumulated_eval_time=5624.415690, accumulated_logging_time=4.350196, accumulated_submission_time=44997.624660, global_step=96149, preemption_count=0, score=44997.624660, test/accuracy=0.525700, test/loss=2.129052, test/num_examples=10000, total_duration=50631.886777, train/accuracy=0.710547, train/loss=1.174837, validation/accuracy=0.648800, validation/loss=1.463151, validation/num_examples=50000
I0208 04:51:30.346041 139946414638848 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.7038788795471191, loss=2.252164363861084
I0208 04:52:16.594249 139946397853440 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.4452404975891113, loss=2.2993388175964355
I0208 04:53:04.115944 139946414638848 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.4692611694335938, loss=2.2506134510040283
I0208 04:53:51.285557 139946397853440 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.4022648334503174, loss=4.552919864654541
I0208 04:54:38.931633 139946414638848 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.43208909034729, loss=2.7303307056427
I0208 04:55:26.257534 139946397853440 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.3862050771713257, loss=3.256788492202759
I0208 04:56:13.896701 139946414638848 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.3292787075042725, loss=3.269965410232544
I0208 04:57:01.362995 139946397853440 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.4717649221420288, loss=2.233490467071533
I0208 04:57:48.775352 139946414638848 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.355736494064331, loss=4.543355941772461
I0208 04:58:09.148696 140107197974336 spec.py:321] Evaluating on the training split.
I0208 04:58:20.069406 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 04:58:58.780010 140107197974336 spec.py:349] Evaluating on the test split.
I0208 04:59:00.382777 140107197974336 submission_runner.py:408] Time since start: 51103.33s, 	Step: 97044, 	{'train/accuracy': 0.7084765434265137, 'train/loss': 1.200149655342102, 'validation/accuracy': 0.650879979133606, 'validation/loss': 1.4600813388824463, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.103090524673462, 'test/num_examples': 10000, 'score': 45417.73803925514, 'total_duration': 51103.32812476158, 'accumulated_submission_time': 45417.73803925514, 'accumulated_eval_time': 5675.649765968323, 'accumulated_logging_time': 4.3937060832977295}
I0208 04:59:00.414164 139946397853440 logging_writer.py:48] [97044] accumulated_eval_time=5675.649766, accumulated_logging_time=4.393706, accumulated_submission_time=45417.738039, global_step=97044, preemption_count=0, score=45417.738039, test/accuracy=0.529300, test/loss=2.103091, test/num_examples=10000, total_duration=51103.328125, train/accuracy=0.708477, train/loss=1.200150, validation/accuracy=0.650880, validation/loss=1.460081, validation/num_examples=50000
I0208 04:59:23.821737 139946414638848 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.4184865951538086, loss=2.690068244934082
I0208 05:00:10.353894 139946397853440 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.430417537689209, loss=2.594630002975464
I0208 05:00:57.620492 139946414638848 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.3757952451705933, loss=2.5747897624969482
I0208 05:01:44.961789 139946397853440 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.6577059030532837, loss=2.38396954536438
I0208 05:02:32.249046 139946414638848 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.4311319589614868, loss=4.527790546417236
I0208 05:03:19.726074 139946397853440 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.2880001068115234, loss=4.709339618682861
I0208 05:04:06.967950 139946414638848 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.2812081575393677, loss=3.477722406387329
I0208 05:04:54.487414 139946397853440 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.3332537412643433, loss=2.816028594970703
I0208 05:05:41.762825 139946414638848 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.4953008890151978, loss=2.2190749645233154
I0208 05:06:00.748740 140107197974336 spec.py:321] Evaluating on the training split.
I0208 05:06:11.933736 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 05:06:52.729932 140107197974336 spec.py:349] Evaluating on the test split.
I0208 05:06:54.354391 140107197974336 submission_runner.py:408] Time since start: 51577.30s, 	Step: 97942, 	{'train/accuracy': 0.7183789014816284, 'train/loss': 1.1449146270751953, 'validation/accuracy': 0.6541199684143066, 'validation/loss': 1.44417405128479, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.0910539627075195, 'test/num_examples': 10000, 'score': 45838.01228451729, 'total_duration': 51577.29973363876, 'accumulated_submission_time': 45838.01228451729, 'accumulated_eval_time': 5729.255409002304, 'accumulated_logging_time': 4.434265851974487}
I0208 05:06:54.390741 139946397853440 logging_writer.py:48] [97942] accumulated_eval_time=5729.255409, accumulated_logging_time=4.434266, accumulated_submission_time=45838.012285, global_step=97942, preemption_count=0, score=45838.012285, test/accuracy=0.530100, test/loss=2.091054, test/num_examples=10000, total_duration=51577.299734, train/accuracy=0.718379, train/loss=1.144915, validation/accuracy=0.654120, validation/loss=1.444174, validation/num_examples=50000
I0208 05:07:18.625461 139946414638848 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.4967963695526123, loss=2.5474889278411865
I0208 05:08:05.231618 139946397853440 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.2072776556015015, loss=4.505827903747559
I0208 05:08:52.239551 139946414638848 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.6660085916519165, loss=2.266167640686035
I0208 05:09:39.600750 139946397853440 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.5352838039398193, loss=2.209839105606079
I0208 05:10:26.727176 139946414638848 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.5449212789535522, loss=2.271224021911621
I0208 05:11:13.972579 139946397853440 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.2740103006362915, loss=2.6062886714935303
I0208 05:12:01.137215 139946414638848 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.2411110401153564, loss=4.19649076461792
I0208 05:12:48.645629 139946397853440 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.620147943496704, loss=2.2510383129119873
I0208 05:13:35.720711 139946414638848 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.392185091972351, loss=2.7250924110412598
I0208 05:13:54.511316 140107197974336 spec.py:321] Evaluating on the training split.
I0208 05:14:05.494876 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 05:14:45.267677 140107197974336 spec.py:349] Evaluating on the test split.
I0208 05:14:46.871453 140107197974336 submission_runner.py:408] Time since start: 52049.82s, 	Step: 98841, 	{'train/accuracy': 0.7341406345367432, 'train/loss': 1.0801435708999634, 'validation/accuracy': 0.6582199931144714, 'validation/loss': 1.4175523519515991, 'validation/num_examples': 50000, 'test/accuracy': 0.530500054359436, 'test/loss': 2.0810186862945557, 'test/num_examples': 10000, 'score': 46258.07188653946, 'total_duration': 52049.81680226326, 'accumulated_submission_time': 46258.07188653946, 'accumulated_eval_time': 5781.6155371665955, 'accumulated_logging_time': 4.480335235595703}
I0208 05:14:46.903671 139946397853440 logging_writer.py:48] [98841] accumulated_eval_time=5781.615537, accumulated_logging_time=4.480335, accumulated_submission_time=46258.071887, global_step=98841, preemption_count=0, score=46258.071887, test/accuracy=0.530500, test/loss=2.081019, test/num_examples=10000, total_duration=52049.816802, train/accuracy=0.734141, train/loss=1.080144, validation/accuracy=0.658220, validation/loss=1.417552, validation/num_examples=50000
I0208 05:15:11.556363 139946414638848 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.4887129068374634, loss=2.2765355110168457
I0208 05:15:58.528315 139946397853440 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.6036739349365234, loss=2.1562271118164062
I0208 05:16:45.949929 139946414638848 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.254469633102417, loss=4.764911651611328
I0208 05:17:33.407256 139946397853440 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.6601688861846924, loss=2.206402063369751
I0208 05:18:21.008104 139946414638848 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.4486175775527954, loss=2.164247989654541
I0208 05:19:08.335086 139946397853440 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.4722453355789185, loss=2.6759438514709473
I0208 05:19:55.853824 139946414638848 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.5210398435592651, loss=2.146714925765991
I0208 05:20:43.205790 139946397853440 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.4523853063583374, loss=2.3035011291503906
I0208 05:21:30.629350 139946414638848 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.4381883144378662, loss=2.260608673095703
I0208 05:21:47.209921 140107197974336 spec.py:321] Evaluating on the training split.
I0208 05:21:58.094645 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 05:22:36.707731 140107197974336 spec.py:349] Evaluating on the test split.
I0208 05:22:38.300108 140107197974336 submission_runner.py:408] Time since start: 52521.25s, 	Step: 99736, 	{'train/accuracy': 0.7138085961341858, 'train/loss': 1.1494009494781494, 'validation/accuracy': 0.6567999720573425, 'validation/loss': 1.4137784242630005, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.062328338623047, 'test/num_examples': 10000, 'score': 46678.317527770996, 'total_duration': 52521.245457172394, 'accumulated_submission_time': 46678.317527770996, 'accumulated_eval_time': 5832.705719232559, 'accumulated_logging_time': 4.522157669067383}
I0208 05:22:38.334182 139946397853440 logging_writer.py:48] [99736] accumulated_eval_time=5832.705719, accumulated_logging_time=4.522158, accumulated_submission_time=46678.317528, global_step=99736, preemption_count=0, score=46678.317528, test/accuracy=0.541100, test/loss=2.062328, test/num_examples=10000, total_duration=52521.245457, train/accuracy=0.713809, train/loss=1.149401, validation/accuracy=0.656800, validation/loss=1.413778, validation/num_examples=50000
I0208 05:23:05.247987 139946414638848 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.5143160820007324, loss=2.1370882987976074
I0208 05:23:52.539740 139946397853440 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.413185954093933, loss=2.1179304122924805
I0208 05:24:40.007112 139946414638848 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.3255878686904907, loss=4.052937030792236
I0208 05:25:27.689583 139946397853440 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.732869029045105, loss=2.2814769744873047
I0208 05:26:15.040501 139946414638848 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.5758334398269653, loss=2.1067276000976562
I0208 05:27:02.511556 139946397853440 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.499889612197876, loss=2.325103998184204
I0208 05:27:50.005791 139946414638848 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.5876519680023193, loss=2.2255828380584717
I0208 05:28:37.581541 139946397853440 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.4608796834945679, loss=3.0019452571868896
I0208 05:29:25.002870 139946414638848 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.6918684244155884, loss=2.1573336124420166
I0208 05:29:38.415184 140107197974336 spec.py:321] Evaluating on the training split.
I0208 05:29:49.431564 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 05:30:28.569367 140107197974336 spec.py:349] Evaluating on the test split.
I0208 05:30:30.165469 140107197974336 submission_runner.py:408] Time since start: 52993.11s, 	Step: 100630, 	{'train/accuracy': 0.71728515625, 'train/loss': 1.1666183471679688, 'validation/accuracy': 0.6571399569511414, 'validation/loss': 1.4454323053359985, 'validation/num_examples': 50000, 'test/accuracy': 0.5279000401496887, 'test/loss': 2.1021368503570557, 'test/num_examples': 10000, 'score': 47098.33820104599, 'total_duration': 52993.11081790924, 'accumulated_submission_time': 47098.33820104599, 'accumulated_eval_time': 5884.45601606369, 'accumulated_logging_time': 4.5659730434417725}
I0208 05:30:30.200247 139946397853440 logging_writer.py:48] [100630] accumulated_eval_time=5884.456016, accumulated_logging_time=4.565973, accumulated_submission_time=47098.338201, global_step=100630, preemption_count=0, score=47098.338201, test/accuracy=0.527900, test/loss=2.102137, test/num_examples=10000, total_duration=52993.110818, train/accuracy=0.717285, train/loss=1.166618, validation/accuracy=0.657140, validation/loss=1.445432, validation/num_examples=50000
I0208 05:30:59.812274 139946414638848 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.5138649940490723, loss=2.2995777130126953
I0208 05:31:46.939135 139946397853440 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.4391717910766602, loss=4.600264072418213
I0208 05:32:34.548456 139946414638848 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.7103053331375122, loss=2.241459608078003
I0208 05:33:22.192290 139946397853440 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.483856201171875, loss=2.1247479915618896
I0208 05:34:09.946405 139946414638848 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.327017903327942, loss=2.908325672149658
I0208 05:34:57.136829 139946397853440 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.3404384851455688, loss=2.6004371643066406
I0208 05:35:44.663531 139946414638848 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.2650158405303955, loss=3.929563522338867
I0208 05:36:32.306514 139946397853440 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.8524175882339478, loss=2.220149040222168
I0208 05:37:19.734119 139946414638848 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.611037254333496, loss=2.288473606109619
I0208 05:37:30.354252 140107197974336 spec.py:321] Evaluating on the training split.
I0208 05:37:41.777832 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 05:38:22.967065 140107197974336 spec.py:349] Evaluating on the test split.
I0208 05:38:24.569883 140107197974336 submission_runner.py:408] Time since start: 53467.52s, 	Step: 101524, 	{'train/accuracy': 0.7432226538658142, 'train/loss': 1.042794108390808, 'validation/accuracy': 0.6620199680328369, 'validation/loss': 1.3963292837142944, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.04892635345459, 'test/num_examples': 10000, 'score': 47518.43212723732, 'total_duration': 53467.515218257904, 'accumulated_submission_time': 47518.43212723732, 'accumulated_eval_time': 5938.6716158390045, 'accumulated_logging_time': 4.610373020172119}
I0208 05:38:24.606113 139946397853440 logging_writer.py:48] [101524] accumulated_eval_time=5938.671616, accumulated_logging_time=4.610373, accumulated_submission_time=47518.432127, global_step=101524, preemption_count=0, score=47518.432127, test/accuracy=0.535600, test/loss=2.048926, test/num_examples=10000, total_duration=53467.515218, train/accuracy=0.743223, train/loss=1.042794, validation/accuracy=0.662020, validation/loss=1.396329, validation/num_examples=50000
I0208 05:38:57.083164 139946414638848 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.4954617023468018, loss=2.3204920291900635
I0208 05:39:44.427472 139946397853440 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.4338783025741577, loss=2.094050884246826
I0208 05:40:32.331636 139946414638848 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.6866819858551025, loss=2.4797165393829346
I0208 05:41:19.830484 139946397853440 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.38137686252594, loss=2.3428854942321777
I0208 05:42:07.323821 139946414638848 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.6698628664016724, loss=2.2121784687042236
I0208 05:42:54.544467 139946397853440 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.3735507726669312, loss=3.9231996536254883
I0208 05:43:41.990423 139946414638848 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.4280266761779785, loss=3.007204055786133
I0208 05:44:29.504187 139946397853440 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.5165985822677612, loss=2.173105478286743
I0208 05:45:16.932061 139946414638848 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.3088750839233398, loss=3.84726619720459
I0208 05:45:24.801568 140107197974336 spec.py:321] Evaluating on the training split.
I0208 05:45:35.851444 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 05:46:14.559828 140107197974336 spec.py:349] Evaluating on the test split.
I0208 05:46:16.164246 140107197974336 submission_runner.py:408] Time since start: 53939.11s, 	Step: 102418, 	{'train/accuracy': 0.7162694931030273, 'train/loss': 1.132684588432312, 'validation/accuracy': 0.6610999703407288, 'validation/loss': 1.4008057117462158, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.0464093685150146, 'test/num_examples': 10000, 'score': 47938.56771445274, 'total_duration': 53939.10957980156, 'accumulated_submission_time': 47938.56771445274, 'accumulated_eval_time': 5990.034274101257, 'accumulated_logging_time': 4.655975341796875}
I0208 05:46:16.200294 139946397853440 logging_writer.py:48] [102418] accumulated_eval_time=5990.034274, accumulated_logging_time=4.655975, accumulated_submission_time=47938.567714, global_step=102418, preemption_count=0, score=47938.567714, test/accuracy=0.539200, test/loss=2.046409, test/num_examples=10000, total_duration=53939.109580, train/accuracy=0.716269, train/loss=1.132685, validation/accuracy=0.661100, validation/loss=1.400806, validation/num_examples=50000
I0208 05:46:51.622028 139946414638848 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.6360176801681519, loss=2.316376209259033
I0208 05:47:38.732230 139946397853440 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.6663581132888794, loss=2.193763017654419
I0208 05:48:26.256735 139946414638848 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.3205028772354126, loss=3.0685343742370605
I0208 05:49:13.448419 139946397853440 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.7532577514648438, loss=2.237494468688965
I0208 05:50:00.985736 139946414638848 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.5344021320343018, loss=2.23759126663208
I0208 05:50:48.474144 139946397853440 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.6707419157028198, loss=2.054051399230957
I0208 05:51:36.150071 139946414638848 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.6804548501968384, loss=2.1345174312591553
I0208 05:52:23.479615 139946397853440 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.329017162322998, loss=3.7626988887786865
I0208 05:53:10.927603 139946414638848 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.1950870752334595, loss=3.953500270843506
I0208 05:53:16.265697 140107197974336 spec.py:321] Evaluating on the training split.
I0208 05:53:27.194336 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 05:54:06.804023 140107197974336 spec.py:349] Evaluating on the test split.
I0208 05:54:08.402649 140107197974336 submission_runner.py:408] Time since start: 54411.35s, 	Step: 103313, 	{'train/accuracy': 0.7245898246765137, 'train/loss': 1.1347663402557373, 'validation/accuracy': 0.6670799851417542, 'validation/loss': 1.4015071392059326, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.047637939453125, 'test/num_examples': 10000, 'score': 48358.572227716446, 'total_duration': 54411.3479924202, 'accumulated_submission_time': 48358.572227716446, 'accumulated_eval_time': 6042.1712164878845, 'accumulated_logging_time': 4.702473402023315}
I0208 05:54:08.439570 139946397853440 logging_writer.py:48] [103313] accumulated_eval_time=6042.171216, accumulated_logging_time=4.702473, accumulated_submission_time=48358.572228, global_step=103313, preemption_count=0, score=48358.572228, test/accuracy=0.541000, test/loss=2.047638, test/num_examples=10000, total_duration=54411.347992, train/accuracy=0.724590, train/loss=1.134766, validation/accuracy=0.667080, validation/loss=1.401507, validation/num_examples=50000
I0208 05:54:46.022314 139946414638848 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.404161810874939, loss=3.7664589881896973
I0208 05:55:33.026630 139946397853440 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.5118358135223389, loss=3.9035422801971436
I0208 05:56:20.713247 139946414638848 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.611968755722046, loss=2.2441840171813965
I0208 05:57:08.116800 139946397853440 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.5461395978927612, loss=2.685380458831787
I0208 05:57:55.270792 139946414638848 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.6135146617889404, loss=2.203291177749634
I0208 05:58:43.151594 139946397853440 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.3840113878250122, loss=2.9838709831237793
I0208 05:59:30.390890 139946414638848 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.6355527639389038, loss=2.1368446350097656
I0208 06:00:18.083441 139946397853440 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.570580244064331, loss=2.568938732147217
I0208 06:01:05.558521 139946414638848 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.706215739250183, loss=2.083606719970703
I0208 06:01:08.587181 140107197974336 spec.py:321] Evaluating on the training split.
I0208 06:01:19.823671 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 06:01:59.942162 140107197974336 spec.py:349] Evaluating on the test split.
I0208 06:02:01.539535 140107197974336 submission_runner.py:408] Time since start: 54884.48s, 	Step: 104208, 	{'train/accuracy': 0.745898425579071, 'train/loss': 1.0389422178268433, 'validation/accuracy': 0.6654399633407593, 'validation/loss': 1.3964356184005737, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.037595748901367, 'test/num_examples': 10000, 'score': 48778.659165382385, 'total_duration': 54884.48488640785, 'accumulated_submission_time': 48778.659165382385, 'accumulated_eval_time': 6095.1235938072205, 'accumulated_logging_time': 4.748953342437744}
I0208 06:02:01.573028 139946397853440 logging_writer.py:48] [104208] accumulated_eval_time=6095.123594, accumulated_logging_time=4.748953, accumulated_submission_time=48778.659165, global_step=104208, preemption_count=0, score=48778.659165, test/accuracy=0.543600, test/loss=2.037596, test/num_examples=10000, total_duration=54884.484886, train/accuracy=0.745898, train/loss=1.038942, validation/accuracy=0.665440, validation/loss=1.396436, validation/num_examples=50000
I0208 06:02:41.657250 139946414638848 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.5637595653533936, loss=4.631303787231445
I0208 06:03:28.596808 139946397853440 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.5333603620529175, loss=2.2332582473754883
I0208 06:04:16.296809 139946414638848 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.5558393001556396, loss=2.2292606830596924
I0208 06:05:03.809232 139946397853440 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.5360814332962036, loss=2.1755928993225098
I0208 06:05:51.020715 139946414638848 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.6384062767028809, loss=2.1092042922973633
I0208 06:06:38.564529 139946397853440 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.6828999519348145, loss=2.4003257751464844
I0208 06:07:25.879459 139946414638848 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.4272714853286743, loss=2.0620405673980713
I0208 06:08:13.273417 139946397853440 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.2854682207107544, loss=3.6182360649108887
I0208 06:09:00.778269 139946414638848 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.4623982906341553, loss=2.194507598876953
I0208 06:09:02.005890 140107197974336 spec.py:321] Evaluating on the training split.
I0208 06:09:13.284163 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 06:09:51.453044 140107197974336 spec.py:349] Evaluating on the test split.
I0208 06:09:53.058681 140107197974336 submission_runner.py:408] Time since start: 55356.00s, 	Step: 105104, 	{'train/accuracy': 0.7211328148841858, 'train/loss': 1.13435959815979, 'validation/accuracy': 0.6629399657249451, 'validation/loss': 1.3976269960403442, 'validation/num_examples': 50000, 'test/accuracy': 0.5414000153541565, 'test/loss': 2.042243719100952, 'test/num_examples': 10000, 'score': 49199.02674174309, 'total_duration': 55356.00403022766, 'accumulated_submission_time': 49199.02674174309, 'accumulated_eval_time': 6146.176388025284, 'accumulated_logging_time': 4.796812057495117}
I0208 06:09:53.095835 139946397853440 logging_writer.py:48] [105104] accumulated_eval_time=6146.176388, accumulated_logging_time=4.796812, accumulated_submission_time=49199.026742, global_step=105104, preemption_count=0, score=49199.026742, test/accuracy=0.541400, test/loss=2.042244, test/num_examples=10000, total_duration=55356.004030, train/accuracy=0.721133, train/loss=1.134360, validation/accuracy=0.662940, validation/loss=1.397627, validation/num_examples=50000
I0208 06:10:34.992407 139946414638848 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.4368034601211548, loss=2.171269178390503
I0208 06:11:21.913046 139946397853440 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.522111415863037, loss=2.096463441848755
I0208 06:12:09.401565 139946414638848 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.5028928518295288, loss=2.099257230758667
I0208 06:12:56.368368 139946414638848 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.4284741878509521, loss=2.3345980644226074
I0208 06:13:43.886650 139946397853440 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.4944796562194824, loss=2.079455614089966
I0208 06:14:31.491191 139946414638848 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.5244756937026978, loss=2.146662712097168
I0208 06:15:19.014383 139946397853440 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.5528424978256226, loss=2.117617130279541
I0208 06:16:06.262325 139946414638848 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.774889349937439, loss=2.196021556854248
I0208 06:16:53.691204 139946397853440 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.4313713312149048, loss=3.141357421875
I0208 06:16:53.704607 140107197974336 spec.py:321] Evaluating on the training split.
I0208 06:17:05.029642 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 06:17:45.696974 140107197974336 spec.py:349] Evaluating on the test split.
I0208 06:17:47.293425 140107197974336 submission_runner.py:408] Time since start: 55830.24s, 	Step: 106001, 	{'train/accuracy': 0.7251952886581421, 'train/loss': 1.1188418865203857, 'validation/accuracy': 0.6670399904251099, 'validation/loss': 1.396875023841858, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.0384507179260254, 'test/num_examples': 10000, 'score': 49619.575608730316, 'total_duration': 55830.23876786232, 'accumulated_submission_time': 49619.575608730316, 'accumulated_eval_time': 6199.765183925629, 'accumulated_logging_time': 4.843739748001099}
I0208 06:17:47.331018 139946414638848 logging_writer.py:48] [106001] accumulated_eval_time=6199.765184, accumulated_logging_time=4.843740, accumulated_submission_time=49619.575609, global_step=106001, preemption_count=0, score=49619.575609, test/accuracy=0.542900, test/loss=2.038451, test/num_examples=10000, total_duration=55830.238768, train/accuracy=0.725195, train/loss=1.118842, validation/accuracy=0.667040, validation/loss=1.396875, validation/num_examples=50000
I0208 06:18:30.336515 139946397853440 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.6202051639556885, loss=2.2713379859924316
I0208 06:19:17.234066 139946414638848 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.5323762893676758, loss=2.166201591491699
I0208 06:20:04.250167 139946397853440 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.4346919059753418, loss=4.4511566162109375
I0208 06:20:51.482171 139946414638848 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.5392228364944458, loss=2.047656536102295
I0208 06:21:38.636120 139946397853440 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.5934431552886963, loss=2.0949931144714355
I0208 06:22:25.900328 139946414638848 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.6500964164733887, loss=2.223604202270508
I0208 06:23:12.825717 139946397853440 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.9415359497070312, loss=2.0474729537963867
I0208 06:24:00.091414 139946414638848 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.4444854259490967, loss=4.382834434509277
I0208 06:24:47.225151 139946397853440 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.5250608921051025, loss=2.3612542152404785
I0208 06:24:47.408696 140107197974336 spec.py:321] Evaluating on the training split.
I0208 06:24:58.650214 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 06:25:39.299084 140107197974336 spec.py:349] Evaluating on the test split.
I0208 06:25:40.898702 140107197974336 submission_runner.py:408] Time since start: 56303.84s, 	Step: 106902, 	{'train/accuracy': 0.7347265481948853, 'train/loss': 1.0637056827545166, 'validation/accuracy': 0.6632599830627441, 'validation/loss': 1.3980926275253296, 'validation/num_examples': 50000, 'test/accuracy': 0.539400041103363, 'test/loss': 2.0443313121795654, 'test/num_examples': 10000, 'score': 50039.59346675873, 'total_duration': 56303.84405493736, 'accumulated_submission_time': 50039.59346675873, 'accumulated_eval_time': 6253.255183458328, 'accumulated_logging_time': 4.89024019241333}
I0208 06:25:40.932046 139946414638848 logging_writer.py:48] [106902] accumulated_eval_time=6253.255183, accumulated_logging_time=4.890240, accumulated_submission_time=50039.593467, global_step=106902, preemption_count=0, score=50039.593467, test/accuracy=0.539400, test/loss=2.044331, test/num_examples=10000, total_duration=56303.844055, train/accuracy=0.734727, train/loss=1.063706, validation/accuracy=0.663260, validation/loss=1.398093, validation/num_examples=50000
I0208 06:26:23.889141 139946397853440 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.327532410621643, loss=2.4822256565093994
I0208 06:27:11.038073 139946414638848 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.3660948276519775, loss=4.127262115478516
I0208 06:27:58.386751 139946397853440 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.3529504537582397, loss=2.762648344039917
I0208 06:28:45.724065 139946414638848 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.5095446109771729, loss=2.5063536167144775
I0208 06:29:33.152510 139946397853440 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.5891929864883423, loss=2.1644794940948486
I0208 06:30:20.600354 139946414638848 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.4146275520324707, loss=4.062602996826172
I0208 06:31:08.069774 139946397853440 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.7625705003738403, loss=2.133894443511963
I0208 06:31:55.361263 139946414638848 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.5335254669189453, loss=1.972385048866272
I0208 06:32:40.995157 140107197974336 spec.py:321] Evaluating on the training split.
I0208 06:32:51.980045 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 06:33:32.569932 140107197974336 spec.py:349] Evaluating on the test split.
I0208 06:33:34.164794 140107197974336 submission_runner.py:408] Time since start: 56777.11s, 	Step: 107798, 	{'train/accuracy': 0.7272851467132568, 'train/loss': 1.1355656385421753, 'validation/accuracy': 0.6676599979400635, 'validation/loss': 1.4018704891204834, 'validation/num_examples': 50000, 'test/accuracy': 0.5404000282287598, 'test/loss': 2.063552141189575, 'test/num_examples': 10000, 'score': 50459.5941298008, 'total_duration': 56777.110137701035, 'accumulated_submission_time': 50459.5941298008, 'accumulated_eval_time': 6306.424833536148, 'accumulated_logging_time': 4.935125350952148}
I0208 06:33:34.199431 139946397853440 logging_writer.py:48] [107798] accumulated_eval_time=6306.424834, accumulated_logging_time=4.935125, accumulated_submission_time=50459.594130, global_step=107798, preemption_count=0, score=50459.594130, test/accuracy=0.540400, test/loss=2.063552, test/num_examples=10000, total_duration=56777.110138, train/accuracy=0.727285, train/loss=1.135566, validation/accuracy=0.667660, validation/loss=1.401870, validation/num_examples=50000
I0208 06:33:35.434783 139946414638848 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.4254794120788574, loss=2.4309606552124023
I0208 06:34:19.025492 139946397853440 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.7545210123062134, loss=2.1488170623779297
I0208 06:35:06.316706 139946414638848 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.4473206996917725, loss=4.644761085510254
I0208 06:35:53.697147 139946397853440 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.6688599586486816, loss=2.137399196624756
I0208 06:36:41.045720 139946414638848 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.4416650533676147, loss=3.73571515083313
I0208 06:37:28.461395 139946397853440 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.8617162704467773, loss=2.1225192546844482
I0208 06:38:15.885297 139946414638848 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.4531619548797607, loss=4.411660194396973
I0208 06:39:03.181880 139946397853440 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.3414610624313354, loss=3.0419280529022217
I0208 06:39:50.903567 139946414638848 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.6690753698349, loss=2.2564597129821777
I0208 06:40:34.183410 140107197974336 spec.py:321] Evaluating on the training split.
I0208 06:40:45.269458 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 06:41:23.867009 140107197974336 spec.py:349] Evaluating on the test split.
I0208 06:41:25.469002 140107197974336 submission_runner.py:408] Time since start: 57248.41s, 	Step: 108692, 	{'train/accuracy': 0.7261914014816284, 'train/loss': 1.173351764678955, 'validation/accuracy': 0.6655200123786926, 'validation/loss': 1.454077959060669, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.09930157661438, 'test/num_examples': 10000, 'score': 50879.518305301666, 'total_duration': 57248.41434073448, 'accumulated_submission_time': 50879.518305301666, 'accumulated_eval_time': 6357.710418224335, 'accumulated_logging_time': 4.979193210601807}
I0208 06:41:25.506243 139946397853440 logging_writer.py:48] [108692] accumulated_eval_time=6357.710418, accumulated_logging_time=4.979193, accumulated_submission_time=50879.518305, global_step=108692, preemption_count=0, score=50879.518305, test/accuracy=0.541600, test/loss=2.099302, test/num_examples=10000, total_duration=57248.414341, train/accuracy=0.726191, train/loss=1.173352, validation/accuracy=0.665520, validation/loss=1.454078, validation/num_examples=50000
I0208 06:41:29.208130 139946414638848 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.0458195209503174, loss=2.1697895526885986
I0208 06:42:13.314418 139946397853440 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.4826340675354004, loss=3.563488006591797
I0208 06:43:00.899468 139946414638848 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.7329462766647339, loss=2.1391115188598633
I0208 06:43:48.858090 139946397853440 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.6113367080688477, loss=2.1451892852783203
I0208 06:44:36.564999 139946414638848 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.368157148361206, loss=4.6646599769592285
I0208 06:45:24.308901 139946397853440 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.3948837518692017, loss=3.715132713317871
I0208 06:46:11.974471 139946414638848 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.6638771295547485, loss=2.145648241043091
I0208 06:46:59.249053 139946397853440 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.6491851806640625, loss=2.0131096839904785
I0208 06:47:47.312327 139946414638848 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.5605984926223755, loss=2.1599555015563965
I0208 06:48:25.543079 140107197974336 spec.py:321] Evaluating on the training split.
I0208 06:48:36.513630 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 06:49:14.875083 140107197974336 spec.py:349] Evaluating on the test split.
I0208 06:49:16.477159 140107197974336 submission_runner.py:408] Time since start: 57719.42s, 	Step: 109582, 	{'train/accuracy': 0.7465234398841858, 'train/loss': 1.0337104797363281, 'validation/accuracy': 0.6727199554443359, 'validation/loss': 1.3676403760910034, 'validation/num_examples': 50000, 'test/accuracy': 0.5498000383377075, 'test/loss': 2.0147860050201416, 'test/num_examples': 10000, 'score': 51299.49491405487, 'total_duration': 57719.42250370979, 'accumulated_submission_time': 51299.49491405487, 'accumulated_eval_time': 6408.644496679306, 'accumulated_logging_time': 5.0263166427612305}
I0208 06:49:16.512018 139946397853440 logging_writer.py:48] [109582] accumulated_eval_time=6408.644497, accumulated_logging_time=5.026317, accumulated_submission_time=51299.494914, global_step=109582, preemption_count=0, score=51299.494914, test/accuracy=0.549800, test/loss=2.014786, test/num_examples=10000, total_duration=57719.422504, train/accuracy=0.746523, train/loss=1.033710, validation/accuracy=0.672720, validation/loss=1.367640, validation/num_examples=50000
I0208 06:49:24.321732 139946414638848 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.3571760654449463, loss=3.4446213245391846
I0208 06:50:09.300317 139946397853440 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.6621397733688354, loss=2.4483819007873535
I0208 06:50:56.629904 139946414638848 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.3305623531341553, loss=4.145011901855469
I0208 06:51:44.450587 139946397853440 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.6325902938842773, loss=2.0817530155181885
I0208 06:52:32.055974 139946414638848 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.7112329006195068, loss=2.111032009124756
I0208 06:53:20.060919 139946397853440 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.6720114946365356, loss=2.166998863220215
I0208 06:54:07.780109 139946414638848 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.4119693040847778, loss=4.3225836753845215
I0208 06:54:55.542158 139946397853440 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.4496742486953735, loss=3.0870370864868164
I0208 06:55:43.377266 139946414638848 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.4778498411178589, loss=4.148140907287598
I0208 06:56:16.886769 140107197974336 spec.py:321] Evaluating on the training split.
I0208 06:56:28.061779 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 06:57:08.156212 140107197974336 spec.py:349] Evaluating on the test split.
I0208 06:57:09.759182 140107197974336 submission_runner.py:408] Time since start: 58192.70s, 	Step: 110472, 	{'train/accuracy': 0.7326952815055847, 'train/loss': 1.1184043884277344, 'validation/accuracy': 0.6693399548530579, 'validation/loss': 1.3929319381713867, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 2.028778553009033, 'test/num_examples': 10000, 'score': 51719.80661559105, 'total_duration': 58192.70450210571, 'accumulated_submission_time': 51719.80661559105, 'accumulated_eval_time': 6461.516880512238, 'accumulated_logging_time': 5.074175596237183}
I0208 06:57:09.800040 139946397853440 logging_writer.py:48] [110472] accumulated_eval_time=6461.516881, accumulated_logging_time=5.074176, accumulated_submission_time=51719.806616, global_step=110472, preemption_count=0, score=51719.806616, test/accuracy=0.548300, test/loss=2.028779, test/num_examples=10000, total_duration=58192.704502, train/accuracy=0.732695, train/loss=1.118404, validation/accuracy=0.669340, validation/loss=1.392932, validation/num_examples=50000
I0208 06:57:21.706376 139946414638848 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.9003747701644897, loss=2.1443560123443604
I0208 06:58:06.797895 139946397853440 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.5187935829162598, loss=4.133164405822754
I0208 06:58:53.891727 139946414638848 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.4786009788513184, loss=2.7884280681610107
I0208 06:59:41.062799 139946397853440 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.584147334098816, loss=4.194411754608154
I0208 07:00:28.405362 139946414638848 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.4785853624343872, loss=3.8897814750671387
I0208 07:01:15.541192 139946397853440 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.5069667100906372, loss=2.2370200157165527
I0208 07:02:03.163886 139946414638848 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.5615047216415405, loss=2.9988317489624023
I0208 07:02:50.301845 139946397853440 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.4511761665344238, loss=2.943270444869995
I0208 07:03:37.859827 139946414638848 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.5026594400405884, loss=3.0392494201660156
I0208 07:04:10.130026 140107197974336 spec.py:321] Evaluating on the training split.
I0208 07:04:21.251410 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 07:05:02.122703 140107197974336 spec.py:349] Evaluating on the test split.
I0208 07:05:03.716370 140107197974336 submission_runner.py:408] Time since start: 58666.66s, 	Step: 111369, 	{'train/accuracy': 0.7405859231948853, 'train/loss': 1.0059020519256592, 'validation/accuracy': 0.678380012512207, 'validation/loss': 1.3027551174163818, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 1.9627418518066406, 'test/num_examples': 10000, 'score': 52140.07601737976, 'total_duration': 58666.6617166996, 'accumulated_submission_time': 52140.07601737976, 'accumulated_eval_time': 6515.1032173633575, 'accumulated_logging_time': 5.12527322769165}
I0208 07:05:03.751139 139946397853440 logging_writer.py:48] [111369] accumulated_eval_time=6515.103217, accumulated_logging_time=5.125273, accumulated_submission_time=52140.076017, global_step=111369, preemption_count=0, score=52140.076017, test/accuracy=0.555100, test/loss=1.962742, test/num_examples=10000, total_duration=58666.661717, train/accuracy=0.740586, train/loss=1.005902, validation/accuracy=0.678380, validation/loss=1.302755, validation/num_examples=50000
I0208 07:05:16.892847 139946414638848 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.4138672351837158, loss=4.561893939971924
I0208 07:06:02.385729 139946397853440 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.6076929569244385, loss=2.0917623043060303
I0208 07:06:49.608613 139946414638848 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.5314301252365112, loss=3.0007219314575195
I0208 07:07:37.073785 139946397853440 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.660144567489624, loss=2.260316848754883
I0208 07:08:24.800919 139946414638848 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.538220763206482, loss=3.8499326705932617
I0208 07:09:12.185454 139946397853440 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.6315257549285889, loss=2.0801172256469727
I0208 07:09:59.544063 139946414638848 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.6394885778427124, loss=1.973436951637268
I0208 07:10:46.856981 139946397853440 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.057260274887085, loss=2.0679166316986084
I0208 07:11:34.321076 139946414638848 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.7498724460601807, loss=4.711250305175781
I0208 07:12:03.785005 140107197974336 spec.py:321] Evaluating on the training split.
I0208 07:12:14.748919 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 07:12:55.204216 140107197974336 spec.py:349] Evaluating on the test split.
I0208 07:12:56.800706 140107197974336 submission_runner.py:408] Time since start: 59139.75s, 	Step: 112264, 	{'train/accuracy': 0.7454296946525574, 'train/loss': 0.9949614405632019, 'validation/accuracy': 0.677619993686676, 'validation/loss': 1.3128902912139893, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 1.9710676670074463, 'test/num_examples': 10000, 'score': 52560.04998207092, 'total_duration': 59139.74605703354, 'accumulated_submission_time': 52560.04998207092, 'accumulated_eval_time': 6568.118919849396, 'accumulated_logging_time': 5.169904947280884}
I0208 07:12:56.838540 139946397853440 logging_writer.py:48] [112264] accumulated_eval_time=6568.118920, accumulated_logging_time=5.169905, accumulated_submission_time=52560.049982, global_step=112264, preemption_count=0, score=52560.049982, test/accuracy=0.550400, test/loss=1.971068, test/num_examples=10000, total_duration=59139.746057, train/accuracy=0.745430, train/loss=0.994961, validation/accuracy=0.677620, validation/loss=1.312890, validation/num_examples=50000
I0208 07:13:12.031069 139946414638848 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.5280004739761353, loss=3.2270421981811523
I0208 07:13:57.900334 139946397853440 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.670588493347168, loss=2.3049399852752686
I0208 07:14:45.665542 139946414638848 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.6655405759811401, loss=2.0476088523864746
I0208 07:15:33.375678 139946397853440 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.6060985326766968, loss=2.732746124267578
I0208 07:16:21.196085 139946414638848 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.6688048839569092, loss=2.064871311187744
I0208 07:17:09.013088 139946397853440 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.4699349403381348, loss=4.290726184844971
I0208 07:17:56.697840 139946414638848 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.6401574611663818, loss=1.9822473526000977
I0208 07:18:44.412610 139946397853440 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.6105525493621826, loss=2.249126434326172
I0208 07:19:32.227437 139946414638848 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.619636058807373, loss=1.987510085105896
I0208 07:19:57.173839 140107197974336 spec.py:321] Evaluating on the training split.
I0208 07:20:08.733126 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 07:20:48.805890 140107197974336 spec.py:349] Evaluating on the test split.
I0208 07:20:50.400149 140107197974336 submission_runner.py:408] Time since start: 59613.35s, 	Step: 113154, 	{'train/accuracy': 0.7370312213897705, 'train/loss': 1.0629130601882935, 'validation/accuracy': 0.674340009689331, 'validation/loss': 1.3487356901168823, 'validation/num_examples': 50000, 'test/accuracy': 0.5530000329017639, 'test/loss': 1.9947872161865234, 'test/num_examples': 10000, 'score': 52980.3245677948, 'total_duration': 59613.3454978466, 'accumulated_submission_time': 52980.3245677948, 'accumulated_eval_time': 6621.345211267471, 'accumulated_logging_time': 5.218159914016724}
I0208 07:20:50.434082 139946397853440 logging_writer.py:48] [113154] accumulated_eval_time=6621.345211, accumulated_logging_time=5.218160, accumulated_submission_time=52980.324568, global_step=113154, preemption_count=0, score=52980.324568, test/accuracy=0.553000, test/loss=1.994787, test/num_examples=10000, total_duration=59613.345498, train/accuracy=0.737031, train/loss=1.062913, validation/accuracy=0.674340, validation/loss=1.348736, validation/num_examples=50000
I0208 07:21:09.723288 139946414638848 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.504988670349121, loss=2.2812881469726562
I0208 07:21:55.754454 139946397853440 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.8638947010040283, loss=2.090765953063965
I0208 07:22:43.257186 139946414638848 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.8171507120132446, loss=1.9156829118728638
I0208 07:23:30.645221 139946397853440 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.8004390001296997, loss=2.0104613304138184
I0208 07:24:18.335792 139946414638848 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.6749919652938843, loss=2.2017509937286377
I0208 07:25:06.003946 139946397853440 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.6630431413650513, loss=2.1409780979156494
I0208 07:25:53.404162 139946414638848 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.6380975246429443, loss=2.757012367248535
I0208 07:26:40.958439 139946397853440 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.6235014200210571, loss=2.058614730834961
I0208 07:27:28.320855 139946414638848 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.40989351272583, loss=4.188564300537109
I0208 07:27:50.807201 140107197974336 spec.py:321] Evaluating on the training split.
I0208 07:28:01.858520 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 07:28:40.409721 140107197974336 spec.py:349] Evaluating on the test split.
I0208 07:28:42.006334 140107197974336 submission_runner.py:408] Time since start: 60084.95s, 	Step: 114049, 	{'train/accuracy': 0.7362695336341858, 'train/loss': 1.1148184537887573, 'validation/accuracy': 0.6717199683189392, 'validation/loss': 1.4028652906417847, 'validation/num_examples': 50000, 'test/accuracy': 0.5480000376701355, 'test/loss': 2.0506300926208496, 'test/num_examples': 10000, 'score': 53400.63660097122, 'total_duration': 60084.95163035393, 'accumulated_submission_time': 53400.63660097122, 'accumulated_eval_time': 6672.544312000275, 'accumulated_logging_time': 5.261758804321289}
I0208 07:28:42.054299 139946397853440 logging_writer.py:48] [114049] accumulated_eval_time=6672.544312, accumulated_logging_time=5.261759, accumulated_submission_time=53400.636601, global_step=114049, preemption_count=0, score=53400.636601, test/accuracy=0.548000, test/loss=2.050630, test/num_examples=10000, total_duration=60084.951630, train/accuracy=0.736270, train/loss=1.114818, validation/accuracy=0.671720, validation/loss=1.402865, validation/num_examples=50000
I0208 07:29:03.424983 139946414638848 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.803404688835144, loss=4.487065315246582
I0208 07:29:50.166820 139946397853440 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.3251116275787354, loss=3.675130605697632
I0208 07:30:38.126539 139946414638848 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.702165126800537, loss=2.145827531814575
I0208 07:31:25.716083 139946397853440 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.4515339136123657, loss=4.380980014801025
I0208 07:32:13.241122 139946414638848 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.6294838190078735, loss=2.097069501876831
I0208 07:33:00.451728 139946397853440 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.754364252090454, loss=2.050816535949707
I0208 07:33:48.212952 139946414638848 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.7509998083114624, loss=2.1566147804260254
I0208 07:34:35.812163 139946397853440 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.7774302959442139, loss=2.1505870819091797
I0208 07:35:23.514698 139946414638848 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.789650559425354, loss=2.101111888885498
I0208 07:35:42.475857 140107197974336 spec.py:321] Evaluating on the training split.
I0208 07:35:53.677127 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 07:36:34.326949 140107197974336 spec.py:349] Evaluating on the test split.
I0208 07:36:35.933730 140107197974336 submission_runner.py:408] Time since start: 60558.88s, 	Step: 114941, 	{'train/accuracy': 0.7517382502555847, 'train/loss': 1.0051695108413696, 'validation/accuracy': 0.6760599613189697, 'validation/loss': 1.3408530950546265, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 1.9954674243927002, 'test/num_examples': 10000, 'score': 53820.99761939049, 'total_duration': 60558.87907385826, 'accumulated_submission_time': 53820.99761939049, 'accumulated_eval_time': 6726.00217795372, 'accumulated_logging_time': 5.320492744445801}
I0208 07:36:35.970954 139946397853440 logging_writer.py:48] [114941] accumulated_eval_time=6726.002178, accumulated_logging_time=5.320493, accumulated_submission_time=53820.997619, global_step=114941, preemption_count=0, score=53820.997619, test/accuracy=0.549900, test/loss=1.995467, test/num_examples=10000, total_duration=60558.879074, train/accuracy=0.751738, train/loss=1.005170, validation/accuracy=0.676060, validation/loss=1.340853, validation/num_examples=50000
I0208 07:37:00.607118 139946414638848 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.4348913431167603, loss=3.869320869445801
I0208 07:37:47.762286 139946397853440 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.694200038909912, loss=2.0840580463409424
I0208 07:38:35.506061 139946414638848 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.5529366731643677, loss=4.036004066467285
I0208 07:39:23.218353 139946397853440 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.9440033435821533, loss=2.2226414680480957
I0208 07:40:10.947973 139946414638848 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.6221238374710083, loss=2.974630355834961
I0208 07:40:58.185176 139946397853440 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.5214637517929077, loss=2.5841567516326904
I0208 07:41:45.532503 139946414638848 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.4671989679336548, loss=3.427462100982666
I0208 07:42:33.226558 139946397853440 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.7101452350616455, loss=1.9655271768569946
I0208 07:43:20.617550 139946414638848 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.579088807106018, loss=3.2640037536621094
I0208 07:43:35.961257 140107197974336 spec.py:321] Evaluating on the training split.
I0208 07:43:47.116537 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 07:44:28.169251 140107197974336 spec.py:349] Evaluating on the test split.
I0208 07:44:29.766921 140107197974336 submission_runner.py:408] Time since start: 61032.71s, 	Step: 115834, 	{'train/accuracy': 0.7432421445846558, 'train/loss': 1.0460532903671265, 'validation/accuracy': 0.6818000078201294, 'validation/loss': 1.3182965517044067, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 1.9709168672561646, 'test/num_examples': 10000, 'score': 54240.927837610245, 'total_duration': 61032.71226191521, 'accumulated_submission_time': 54240.927837610245, 'accumulated_eval_time': 6779.807821750641, 'accumulated_logging_time': 5.36723256111145}
I0208 07:44:29.806137 139946397853440 logging_writer.py:48] [115834] accumulated_eval_time=6779.807822, accumulated_logging_time=5.367233, accumulated_submission_time=54240.927838, global_step=115834, preemption_count=0, score=54240.927838, test/accuracy=0.555800, test/loss=1.970917, test/num_examples=10000, total_duration=61032.712262, train/accuracy=0.743242, train/loss=1.046053, validation/accuracy=0.681800, validation/loss=1.318297, validation/num_examples=50000
I0208 07:44:57.691135 139946414638848 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.389980673789978, loss=3.4775686264038086
I0208 07:45:44.990905 139946397853440 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.8448115587234497, loss=2.0316596031188965
I0208 07:46:32.459209 139946414638848 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.4940977096557617, loss=3.3809313774108887
I0208 07:47:20.354808 139946397853440 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.7502936124801636, loss=1.9991228580474854
I0208 07:48:08.458314 139946414638848 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.0869758129119873, loss=2.074298858642578
I0208 07:48:56.189648 139946397853440 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.688882827758789, loss=2.063124418258667
I0208 07:49:43.966335 139946414638848 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.8122305870056152, loss=2.0723586082458496
I0208 07:50:31.914980 139946397853440 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.529394507408142, loss=3.151672601699829
I0208 07:51:19.610894 139946414638848 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.6722434759140015, loss=2.0888097286224365
I0208 07:51:29.867456 140107197974336 spec.py:321] Evaluating on the training split.
I0208 07:51:40.898044 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 07:52:18.666263 140107197974336 spec.py:349] Evaluating on the test split.
I0208 07:52:20.260570 140107197974336 submission_runner.py:408] Time since start: 61503.21s, 	Step: 116723, 	{'train/accuracy': 0.7484374642372131, 'train/loss': 1.0299557447433472, 'validation/accuracy': 0.6839199662208557, 'validation/loss': 1.325279951095581, 'validation/num_examples': 50000, 'test/accuracy': 0.5578000545501709, 'test/loss': 1.9653598070144653, 'test/num_examples': 10000, 'score': 54660.929174661636, 'total_duration': 61503.20591568947, 'accumulated_submission_time': 54660.929174661636, 'accumulated_eval_time': 6830.200934410095, 'accumulated_logging_time': 5.416102886199951}
I0208 07:52:20.295953 139946397853440 logging_writer.py:48] [116723] accumulated_eval_time=6830.200934, accumulated_logging_time=5.416103, accumulated_submission_time=54660.929175, global_step=116723, preemption_count=0, score=54660.929175, test/accuracy=0.557800, test/loss=1.965360, test/num_examples=10000, total_duration=61503.205916, train/accuracy=0.748437, train/loss=1.029956, validation/accuracy=0.683920, validation/loss=1.325280, validation/num_examples=50000
I0208 07:52:53.507196 139946414638848 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.6594409942626953, loss=1.9738805294036865
I0208 07:53:40.820953 139946397853440 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.7462868690490723, loss=1.988774061203003
I0208 07:54:27.988556 139946414638848 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.6847018003463745, loss=3.239863395690918
I0208 07:55:15.348340 139946397853440 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.5636944770812988, loss=4.242947578430176
I0208 07:56:02.679789 139946414638848 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.5272098779678345, loss=4.149275779724121
I0208 07:56:50.171092 139946397853440 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.923288106918335, loss=2.2214534282684326
I0208 07:57:37.908545 139946414638848 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.8583955764770508, loss=2.0478787422180176
I0208 07:58:25.576534 139946397853440 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.7690091133117676, loss=2.0705573558807373
I0208 07:59:13.303300 139946414638848 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.5300161838531494, loss=4.480445861816406
I0208 07:59:20.622815 140107197974336 spec.py:321] Evaluating on the training split.
I0208 07:59:31.640796 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 08:00:14.377967 140107197974336 spec.py:349] Evaluating on the test split.
I0208 08:00:15.980015 140107197974336 submission_runner.py:408] Time since start: 61978.93s, 	Step: 117617, 	{'train/accuracy': 0.7597265243530273, 'train/loss': 0.9526239633560181, 'validation/accuracy': 0.6856600046157837, 'validation/loss': 1.2929719686508179, 'validation/num_examples': 50000, 'test/accuracy': 0.560699999332428, 'test/loss': 1.9365530014038086, 'test/num_examples': 10000, 'score': 55081.1960170269, 'total_duration': 61978.925362825394, 'accumulated_submission_time': 55081.1960170269, 'accumulated_eval_time': 6885.558126449585, 'accumulated_logging_time': 5.461392164230347}
I0208 08:00:16.021310 139946397853440 logging_writer.py:48] [117617] accumulated_eval_time=6885.558126, accumulated_logging_time=5.461392, accumulated_submission_time=55081.196017, global_step=117617, preemption_count=0, score=55081.196017, test/accuracy=0.560700, test/loss=1.936553, test/num_examples=10000, total_duration=61978.925363, train/accuracy=0.759727, train/loss=0.952624, validation/accuracy=0.685660, validation/loss=1.292972, validation/num_examples=50000
I0208 08:00:51.992746 139946414638848 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.7632609605789185, loss=1.9451783895492554
I0208 08:01:39.427171 139946397853440 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.7133500576019287, loss=1.9832346439361572
I0208 08:02:27.241605 139946414638848 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.5385031700134277, loss=4.427618980407715
I0208 08:03:14.948352 139946397853440 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.117565631866455, loss=2.0655934810638428
I0208 08:04:02.681746 139946414638848 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.6705384254455566, loss=2.210728645324707
I0208 08:04:50.470438 139946397853440 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.8989944458007812, loss=4.7134318351745605
I0208 08:05:38.353210 139946414638848 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.7118619680404663, loss=4.165515422821045
I0208 08:06:26.158724 139946397853440 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.8001679182052612, loss=1.9605777263641357
I0208 08:07:14.278595 139946414638848 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.6006882190704346, loss=2.1568377017974854
I0208 08:07:16.322365 140107197974336 spec.py:321] Evaluating on the training split.
I0208 08:07:27.596740 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 08:08:10.017759 140107197974336 spec.py:349] Evaluating on the test split.
I0208 08:08:11.618992 140107197974336 submission_runner.py:408] Time since start: 62454.56s, 	Step: 118506, 	{'train/accuracy': 0.7503515481948853, 'train/loss': 1.0049922466278076, 'validation/accuracy': 0.6844199895858765, 'validation/loss': 1.30093252658844, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 1.9488836526870728, 'test/num_examples': 10000, 'score': 55501.43515133858, 'total_duration': 62454.564338207245, 'accumulated_submission_time': 55501.43515133858, 'accumulated_eval_time': 6940.854739904404, 'accumulated_logging_time': 5.514406204223633}
I0208 08:08:11.657125 139946397853440 logging_writer.py:48] [118506] accumulated_eval_time=6940.854740, accumulated_logging_time=5.514406, accumulated_submission_time=55501.435151, global_step=118506, preemption_count=0, score=55501.435151, test/accuracy=0.558900, test/loss=1.948884, test/num_examples=10000, total_duration=62454.564338, train/accuracy=0.750352, train/loss=1.004992, validation/accuracy=0.684420, validation/loss=1.300933, validation/num_examples=50000
I0208 08:08:52.926812 139946414638848 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.653464674949646, loss=1.9037829637527466
I0208 08:09:40.261925 139946397853440 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.6728575229644775, loss=2.2879552841186523
I0208 08:10:27.881063 139946414638848 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.6790549755096436, loss=4.398083686828613
I0208 08:11:15.724494 139946397853440 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.2650516033172607, loss=2.637016773223877
I0208 08:12:03.211658 139946414638848 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.8645761013031006, loss=3.3155970573425293
I0208 08:12:50.636655 139946397853440 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.6667641401290894, loss=1.9329451322555542
I0208 08:13:37.963184 139946414638848 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.8261210918426514, loss=2.1503970623016357
I0208 08:14:25.772125 139946397853440 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.656502366065979, loss=2.3867673873901367
I0208 08:15:11.855526 140107197974336 spec.py:321] Evaluating on the training split.
I0208 08:15:22.861245 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 08:16:02.768185 140107197974336 spec.py:349] Evaluating on the test split.
I0208 08:16:04.369614 140107197974336 submission_runner.py:408] Time since start: 62927.31s, 	Step: 119398, 	{'train/accuracy': 0.7482226490974426, 'train/loss': 1.0383548736572266, 'validation/accuracy': 0.6855599880218506, 'validation/loss': 1.3293486833572388, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 1.9782428741455078, 'test/num_examples': 10000, 'score': 55921.57339477539, 'total_duration': 62927.31496787071, 'accumulated_submission_time': 55921.57339477539, 'accumulated_eval_time': 6993.368841648102, 'accumulated_logging_time': 5.561697959899902}
I0208 08:16:04.404997 139946414638848 logging_writer.py:48] [119398] accumulated_eval_time=6993.368842, accumulated_logging_time=5.561698, accumulated_submission_time=55921.573395, global_step=119398, preemption_count=0, score=55921.573395, test/accuracy=0.558400, test/loss=1.978243, test/num_examples=10000, total_duration=62927.314968, train/accuracy=0.748223, train/loss=1.038355, validation/accuracy=0.685560, validation/loss=1.329349, validation/num_examples=50000
I0208 08:16:05.641396 139946397853440 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.5812551975250244, loss=2.5886077880859375
I0208 08:16:49.089795 139946414638848 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.690458059310913, loss=1.9381229877471924
I0208 08:17:36.202419 139946397853440 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.6263141632080078, loss=2.8405559062957764
I0208 08:18:23.803281 139946414638848 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.446694254875183, loss=4.045390605926514
I0208 08:19:11.449793 139946397853440 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.8043829202651978, loss=1.8954415321350098
I0208 08:19:58.953489 139946414638848 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.8603897094726562, loss=2.102485418319702
I0208 08:20:46.507099 139946397853440 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.7365950345993042, loss=1.8755061626434326
I0208 08:21:34.188861 139946414638848 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.5335102081298828, loss=3.262255907058716
I0208 08:22:21.801399 139946397853440 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.6814996004104614, loss=2.061619520187378
I0208 08:23:04.429066 140107197974336 spec.py:321] Evaluating on the training split.
I0208 08:23:15.611803 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 08:23:53.676248 140107197974336 spec.py:349] Evaluating on the test split.
I0208 08:23:55.275110 140107197974336 submission_runner.py:408] Time since start: 63398.22s, 	Step: 120291, 	{'train/accuracy': 0.7626562118530273, 'train/loss': 0.9669126868247986, 'validation/accuracy': 0.6891199946403503, 'validation/loss': 1.2968260049819946, 'validation/num_examples': 50000, 'test/accuracy': 0.5616000294685364, 'test/loss': 1.9497791528701782, 'test/num_examples': 10000, 'score': 56341.536450624466, 'total_duration': 63398.22046136856, 'accumulated_submission_time': 56341.536450624466, 'accumulated_eval_time': 7044.214889287949, 'accumulated_logging_time': 5.607126235961914}
I0208 08:23:55.311302 139946414638848 logging_writer.py:48] [120291] accumulated_eval_time=7044.214889, accumulated_logging_time=5.607126, accumulated_submission_time=56341.536451, global_step=120291, preemption_count=0, score=56341.536451, test/accuracy=0.561600, test/loss=1.949779, test/num_examples=10000, total_duration=63398.220461, train/accuracy=0.762656, train/loss=0.966913, validation/accuracy=0.689120, validation/loss=1.296826, validation/num_examples=50000
I0208 08:23:59.413092 139946397853440 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.8818299770355225, loss=3.799851417541504
I0208 08:24:43.366142 139946414638848 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.7702833414077759, loss=2.977839231491089
I0208 08:25:30.355953 139946397853440 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.7411621809005737, loss=1.986083745956421
I0208 08:26:17.645030 139946414638848 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.6682029962539673, loss=3.0205862522125244
I0208 08:27:05.086243 139946397853440 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.6662107706069946, loss=1.9988268613815308
I0208 08:27:52.429265 139946414638848 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.7506998777389526, loss=3.1248130798339844
I0208 08:28:39.774308 139946397853440 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.6725140810012817, loss=4.560680389404297
I0208 08:29:27.207538 139946414638848 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.8275901079177856, loss=1.9830621480941772
I0208 08:30:14.645318 139946397853440 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.8379744291305542, loss=2.1673946380615234
I0208 08:30:55.622308 140107197974336 spec.py:321] Evaluating on the training split.
I0208 08:31:06.872742 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 08:31:47.402307 140107197974336 spec.py:349] Evaluating on the test split.
I0208 08:31:49.014095 140107197974336 submission_runner.py:408] Time since start: 63871.96s, 	Step: 121188, 	{'train/accuracy': 0.7483984231948853, 'train/loss': 1.023191213607788, 'validation/accuracy': 0.6844800114631653, 'validation/loss': 1.3076735734939575, 'validation/num_examples': 50000, 'test/accuracy': 0.5606000423431396, 'test/loss': 1.9592289924621582, 'test/num_examples': 10000, 'score': 56761.78638911247, 'total_duration': 63871.95942783356, 'accumulated_submission_time': 56761.78638911247, 'accumulated_eval_time': 7097.606656551361, 'accumulated_logging_time': 5.652962684631348}
I0208 08:31:49.055888 139946414638848 logging_writer.py:48] [121188] accumulated_eval_time=7097.606657, accumulated_logging_time=5.652963, accumulated_submission_time=56761.786389, global_step=121188, preemption_count=0, score=56761.786389, test/accuracy=0.560600, test/loss=1.959229, test/num_examples=10000, total_duration=63871.959428, train/accuracy=0.748398, train/loss=1.023191, validation/accuracy=0.684480, validation/loss=1.307674, validation/num_examples=50000
I0208 08:31:54.408486 139946397853440 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.8711942434310913, loss=2.044874668121338
I0208 08:32:38.685199 139946414638848 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.6110813617706299, loss=3.067711353302002
I0208 08:33:26.653019 139946397853440 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.8574252128601074, loss=2.0849640369415283
I0208 08:34:14.386024 139946414638848 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.8444750308990479, loss=2.129342555999756
I0208 08:35:01.919042 139946397853440 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.5968126058578491, loss=3.468207597732544
I0208 08:35:49.736912 139946414638848 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.750247597694397, loss=1.9370087385177612
I0208 08:36:37.692650 139946397853440 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.7983982563018799, loss=2.0137534141540527
I0208 08:37:25.343627 139946414638848 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.5646344423294067, loss=3.904067039489746
I0208 08:38:13.071020 139946397853440 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.590073823928833, loss=4.4817304611206055
I0208 08:38:49.242885 140107197974336 spec.py:321] Evaluating on the training split.
I0208 08:39:00.421456 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 08:39:43.186251 140107197974336 spec.py:349] Evaluating on the test split.
I0208 08:39:44.791350 140107197974336 submission_runner.py:408] Time since start: 64347.74s, 	Step: 122078, 	{'train/accuracy': 0.7550390362739563, 'train/loss': 0.9838645458221436, 'validation/accuracy': 0.689799964427948, 'validation/loss': 1.2810405492782593, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 1.9222257137298584, 'test/num_examples': 10000, 'score': 57181.91209387779, 'total_duration': 64347.73667383194, 'accumulated_submission_time': 57181.91209387779, 'accumulated_eval_time': 7153.155112504959, 'accumulated_logging_time': 5.705412864685059}
I0208 08:39:44.834887 139946414638848 logging_writer.py:48] [122078] accumulated_eval_time=7153.155113, accumulated_logging_time=5.705413, accumulated_submission_time=57181.912094, global_step=122078, preemption_count=0, score=57181.912094, test/accuracy=0.563600, test/loss=1.922226, test/num_examples=10000, total_duration=64347.736674, train/accuracy=0.755039, train/loss=0.983865, validation/accuracy=0.689800, validation/loss=1.281041, validation/num_examples=50000
I0208 08:39:54.280223 139946397853440 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.8639541864395142, loss=2.0109610557556152
I0208 08:40:39.376048 139946414638848 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.6430162191390991, loss=2.5846214294433594
I0208 08:41:26.888687 139946397853440 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.8091200590133667, loss=1.9900270700454712
I0208 08:42:14.624553 139946414638848 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.6336008310317993, loss=2.455125331878662
I0208 08:43:02.067691 139946397853440 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.6921247243881226, loss=2.0052249431610107
I0208 08:43:50.133053 139946414638848 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.7070472240447998, loss=2.063062906265259
I0208 08:44:38.313969 139946397853440 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.8350234031677246, loss=1.9105370044708252
I0208 08:45:26.199869 139946414638848 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.8075048923492432, loss=2.061347246170044
I0208 08:46:13.757865 139946397853440 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.7138044834136963, loss=2.5551655292510986
I0208 08:46:44.959537 140107197974336 spec.py:321] Evaluating on the training split.
I0208 08:46:55.938738 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 08:47:36.844273 140107197974336 spec.py:349] Evaluating on the test split.
I0208 08:47:38.436883 140107197974336 submission_runner.py:408] Time since start: 64821.38s, 	Step: 122967, 	{'train/accuracy': 0.767285168170929, 'train/loss': 0.9140588045120239, 'validation/accuracy': 0.6899399757385254, 'validation/loss': 1.2592631578445435, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 1.895406723022461, 'test/num_examples': 10000, 'score': 57601.97654438019, 'total_duration': 64821.3822324276, 'accumulated_submission_time': 57601.97654438019, 'accumulated_eval_time': 7206.63245177269, 'accumulated_logging_time': 5.758823394775391}
I0208 08:47:38.474995 139946414638848 logging_writer.py:48] [122967] accumulated_eval_time=7206.632452, accumulated_logging_time=5.758823, accumulated_submission_time=57601.976544, global_step=122967, preemption_count=0, score=57601.976544, test/accuracy=0.563700, test/loss=1.895407, test/num_examples=10000, total_duration=64821.382232, train/accuracy=0.767285, train/loss=0.914059, validation/accuracy=0.689940, validation/loss=1.259263, validation/num_examples=50000
I0208 08:47:52.436945 139946397853440 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.7407267093658447, loss=2.5361876487731934
I0208 08:48:38.367236 139946414638848 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.737474799156189, loss=2.5054657459259033
I0208 08:49:25.901337 139946397853440 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.9254062175750732, loss=1.8866807222366333
I0208 08:50:13.856369 139946414638848 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.6568390130996704, loss=1.9960857629776
I0208 08:51:01.716759 139946397853440 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.9790412187576294, loss=1.9135942459106445
I0208 08:51:49.318609 139946414638848 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.9654144048690796, loss=2.0034539699554443
I0208 08:52:37.195423 139946397853440 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.7612879276275635, loss=2.213021993637085
I0208 08:53:25.046654 139946414638848 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.7707654237747192, loss=1.8358309268951416
I0208 08:54:12.872009 139946397853440 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.8659510612487793, loss=2.026899576187134
I0208 08:54:38.693694 140107197974336 spec.py:321] Evaluating on the training split.
I0208 08:54:49.716828 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 08:55:27.643185 140107197974336 spec.py:349] Evaluating on the test split.
I0208 08:55:29.239954 140107197974336 submission_runner.py:408] Time since start: 65292.19s, 	Step: 123855, 	{'train/accuracy': 0.7547070384025574, 'train/loss': 0.9752053618431091, 'validation/accuracy': 0.691819965839386, 'validation/loss': 1.2582887411117554, 'validation/num_examples': 50000, 'test/accuracy': 0.5642000436782837, 'test/loss': 1.9256024360656738, 'test/num_examples': 10000, 'score': 58022.1356446743, 'total_duration': 65292.18530201912, 'accumulated_submission_time': 58022.1356446743, 'accumulated_eval_time': 7257.178694009781, 'accumulated_logging_time': 5.806592226028442}
I0208 08:55:29.279643 139946414638848 logging_writer.py:48] [123855] accumulated_eval_time=7257.178694, accumulated_logging_time=5.806592, accumulated_submission_time=58022.135645, global_step=123855, preemption_count=0, score=58022.135645, test/accuracy=0.564200, test/loss=1.925602, test/num_examples=10000, total_duration=65292.185302, train/accuracy=0.754707, train/loss=0.975205, validation/accuracy=0.691820, validation/loss=1.258289, validation/num_examples=50000
I0208 08:55:48.175716 139946397853440 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.9136886596679688, loss=2.2049434185028076
I0208 08:56:34.640580 139946414638848 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.7601035833358765, loss=1.9998284578323364
I0208 08:57:22.145027 139946397853440 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.9385532140731812, loss=2.0543854236602783
I0208 08:58:09.685322 139946414638848 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.8291817903518677, loss=2.0315144062042236
I0208 08:58:57.034132 139946397853440 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.5562970638275146, loss=2.387305498123169
I0208 08:59:44.494246 139946414638848 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.9067198038101196, loss=1.9741239547729492
I0208 09:00:32.147966 139946397853440 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.7375801801681519, loss=2.3845560550689697
I0208 09:01:19.963195 139946414638848 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.7437260150909424, loss=4.445950508117676
I0208 09:02:07.652502 139946397853440 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.8251655101776123, loss=2.271005153656006
I0208 09:02:29.627794 140107197974336 spec.py:321] Evaluating on the training split.
I0208 09:02:40.949501 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 09:03:20.779124 140107197974336 spec.py:349] Evaluating on the test split.
I0208 09:03:22.374380 140107197974336 submission_runner.py:408] Time since start: 65765.32s, 	Step: 124748, 	{'train/accuracy': 0.7588085532188416, 'train/loss': 0.9643204808235168, 'validation/accuracy': 0.6942799687385559, 'validation/loss': 1.259268045425415, 'validation/num_examples': 50000, 'test/accuracy': 0.5690000057220459, 'test/loss': 1.9072344303131104, 'test/num_examples': 10000, 'score': 58442.4238243103, 'total_duration': 65765.31973147392, 'accumulated_submission_time': 58442.4238243103, 'accumulated_eval_time': 7309.925290107727, 'accumulated_logging_time': 5.855309247970581}
I0208 09:03:22.415450 139946414638848 logging_writer.py:48] [124748] accumulated_eval_time=7309.925290, accumulated_logging_time=5.855309, accumulated_submission_time=58442.423824, global_step=124748, preemption_count=0, score=58442.423824, test/accuracy=0.569000, test/loss=1.907234, test/num_examples=10000, total_duration=65765.319731, train/accuracy=0.758809, train/loss=0.964320, validation/accuracy=0.694280, validation/loss=1.259268, validation/num_examples=50000
I0208 09:03:44.185986 139946397853440 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.8405882120132446, loss=1.9081614017486572
I0208 09:04:31.024989 139946414638848 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.6573810577392578, loss=3.0294599533081055
I0208 09:05:18.727105 139946397853440 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.9770407676696777, loss=1.9613046646118164
I0208 09:06:06.313976 139946414638848 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.0410776138305664, loss=1.9386334419250488
I0208 09:06:53.786218 139946397853440 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.8145140409469604, loss=2.090683937072754
I0208 09:07:40.945873 139946414638848 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.751962661743164, loss=2.018655776977539
I0208 09:08:28.574138 139946397853440 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.609116554260254, loss=4.260492324829102
I0208 09:09:15.998302 139946414638848 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.7645158767700195, loss=1.9729083776474
I0208 09:10:03.545159 139946397853440 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.7528001070022583, loss=2.9280996322631836
I0208 09:10:22.748529 140107197974336 spec.py:321] Evaluating on the training split.
I0208 09:10:34.055486 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 09:11:13.113574 140107197974336 spec.py:349] Evaluating on the test split.
I0208 09:11:14.716903 140107197974336 submission_runner.py:408] Time since start: 66237.66s, 	Step: 125642, 	{'train/accuracy': 0.7699999809265137, 'train/loss': 0.9051749110221863, 'validation/accuracy': 0.6944599747657776, 'validation/loss': 1.2426235675811768, 'validation/num_examples': 50000, 'test/accuracy': 0.5710000395774841, 'test/loss': 1.878885269165039, 'test/num_examples': 10000, 'score': 58862.696983098984, 'total_duration': 66237.66225218773, 'accumulated_submission_time': 58862.696983098984, 'accumulated_eval_time': 7361.893659353256, 'accumulated_logging_time': 5.9061243534088135}
I0208 09:11:14.757048 139946414638848 logging_writer.py:48] [125642] accumulated_eval_time=7361.893659, accumulated_logging_time=5.906124, accumulated_submission_time=58862.696983, global_step=125642, preemption_count=0, score=58862.696983, test/accuracy=0.571000, test/loss=1.878885, test/num_examples=10000, total_duration=66237.662252, train/accuracy=0.770000, train/loss=0.905175, validation/accuracy=0.694460, validation/loss=1.242624, validation/num_examples=50000
I0208 09:11:39.003991 139946397853440 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.7952265739440918, loss=1.9908243417739868
I0208 09:12:26.161008 139946414638848 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.9269040822982788, loss=2.0984692573547363
I0208 09:13:13.775132 139946397853440 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.681930422782898, loss=2.9006528854370117
I0208 09:14:00.813606 139946414638848 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.8632621765136719, loss=3.476179838180542
I0208 09:14:48.404022 139946397853440 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.0178823471069336, loss=1.9884984493255615
I0208 09:15:35.948947 139946414638848 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.7609755992889404, loss=1.8832592964172363
I0208 09:16:23.393908 139946397853440 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.7898544073104858, loss=4.174576759338379
I0208 09:17:11.360771 139946414638848 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.6836671829223633, loss=2.567595958709717
I0208 09:17:58.770724 139946397853440 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.8164156675338745, loss=4.556068420410156
I0208 09:18:14.734535 140107197974336 spec.py:321] Evaluating on the training split.
I0208 09:18:26.436813 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 09:19:04.538811 140107197974336 spec.py:349] Evaluating on the test split.
I0208 09:19:06.138650 140107197974336 submission_runner.py:408] Time since start: 66709.08s, 	Step: 126535, 	{'train/accuracy': 0.7580859065055847, 'train/loss': 0.9454815983772278, 'validation/accuracy': 0.6959999799728394, 'validation/loss': 1.2273520231246948, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.8674311637878418, 'test/num_examples': 10000, 'score': 59282.61265707016, 'total_duration': 66709.08399271965, 'accumulated_submission_time': 59282.61265707016, 'accumulated_eval_time': 7413.29775929451, 'accumulated_logging_time': 5.957125425338745}
I0208 09:19:06.178440 139946414638848 logging_writer.py:48] [126535] accumulated_eval_time=7413.297759, accumulated_logging_time=5.957125, accumulated_submission_time=59282.612657, global_step=126535, preemption_count=0, score=59282.612657, test/accuracy=0.575100, test/loss=1.867431, test/num_examples=10000, total_duration=66709.083993, train/accuracy=0.758086, train/loss=0.945482, validation/accuracy=0.696000, validation/loss=1.227352, validation/num_examples=50000
I0208 09:19:33.643607 139946397853440 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.7489320039749146, loss=3.607520341873169
I0208 09:20:20.823303 139946414638848 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.6889060735702515, loss=2.119694471359253
I0208 09:21:08.271225 139946397853440 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.066649913787842, loss=2.14448618888855
I0208 09:21:55.516426 139946414638848 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.152960777282715, loss=1.9086718559265137
I0208 09:22:43.278148 139946397853440 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.9790071249008179, loss=2.325308322906494
I0208 09:23:31.029028 139946414638848 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.7378077507019043, loss=3.3729076385498047
I0208 09:24:18.488703 139946397853440 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.7975963354110718, loss=4.267704963684082
I0208 09:25:06.109052 139946414638848 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.0083019733428955, loss=1.7996013164520264
I0208 09:25:53.407515 139946397853440 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.6709121465682983, loss=2.323741912841797
I0208 09:26:06.188597 140107197974336 spec.py:321] Evaluating on the training split.
I0208 09:26:17.369462 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 09:26:58.168999 140107197974336 spec.py:349] Evaluating on the test split.
I0208 09:26:59.767505 140107197974336 submission_runner.py:408] Time since start: 67182.71s, 	Step: 127428, 	{'train/accuracy': 0.7644921541213989, 'train/loss': 0.9372758865356445, 'validation/accuracy': 0.6952999830245972, 'validation/loss': 1.2479116916656494, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 1.8793699741363525, 'test/num_examples': 10000, 'score': 59702.56274223328, 'total_duration': 67182.71284723282, 'accumulated_submission_time': 59702.56274223328, 'accumulated_eval_time': 7466.876657009125, 'accumulated_logging_time': 6.0067572593688965}
I0208 09:26:59.806938 139946414638848 logging_writer.py:48] [127428] accumulated_eval_time=7466.876657, accumulated_logging_time=6.006757, accumulated_submission_time=59702.562742, global_step=127428, preemption_count=0, score=59702.562742, test/accuracy=0.573400, test/loss=1.879370, test/num_examples=10000, total_duration=67182.712847, train/accuracy=0.764492, train/loss=0.937276, validation/accuracy=0.695300, validation/loss=1.247912, validation/num_examples=50000
I0208 09:27:30.533739 139946397853440 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.6539273262023926, loss=3.881211757659912
I0208 09:28:18.105121 139946414638848 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.8895814418792725, loss=1.9634623527526855
I0208 09:29:05.820572 139946397853440 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.7129877805709839, loss=2.4958550930023193
I0208 09:29:53.068736 139946414638848 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.00514554977417, loss=2.3104405403137207
I0208 09:30:40.954268 139946397853440 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.8717807531356812, loss=1.9374516010284424
I0208 09:31:28.353090 139946414638848 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.7695727348327637, loss=3.691059112548828
I0208 09:32:15.949572 139946397853440 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.637954831123352, loss=3.1472649574279785
I0208 09:33:03.613360 139946414638848 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.8608081340789795, loss=4.239946365356445
I0208 09:33:50.914203 139946397853440 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.807569980621338, loss=4.506649971008301
I0208 09:34:00.095198 140107197974336 spec.py:321] Evaluating on the training split.
I0208 09:34:11.524324 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 09:34:52.034833 140107197974336 spec.py:349] Evaluating on the test split.
I0208 09:34:53.636771 140107197974336 submission_runner.py:408] Time since start: 67656.58s, 	Step: 128321, 	{'train/accuracy': 0.7732617259025574, 'train/loss': 0.9040424823760986, 'validation/accuracy': 0.696179986000061, 'validation/loss': 1.2386150360107422, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.8976391553878784, 'test/num_examples': 10000, 'score': 60122.79120898247, 'total_duration': 67656.58211922646, 'accumulated_submission_time': 60122.79120898247, 'accumulated_eval_time': 7520.418255567551, 'accumulated_logging_time': 6.055173873901367}
I0208 09:34:53.676403 139946414638848 logging_writer.py:48] [128321] accumulated_eval_time=7520.418256, accumulated_logging_time=6.055174, accumulated_submission_time=60122.791209, global_step=128321, preemption_count=0, score=60122.791209, test/accuracy=0.567800, test/loss=1.897639, test/num_examples=10000, total_duration=67656.582119, train/accuracy=0.773262, train/loss=0.904042, validation/accuracy=0.696180, validation/loss=1.238615, validation/num_examples=50000
I0208 09:35:27.763609 139946397853440 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.1908116340637207, loss=2.1892004013061523
I0208 09:36:14.815367 139946414638848 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.99431312084198, loss=1.830557942390442
I0208 09:37:02.215099 139946397853440 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.9442030191421509, loss=2.5077903270721436
I0208 09:37:49.451872 139946414638848 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.793033242225647, loss=2.424858808517456
I0208 09:38:36.978120 139946397853440 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.0613820552825928, loss=2.036400318145752
I0208 09:39:24.529455 139946414638848 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.0975735187530518, loss=1.8802692890167236
I0208 09:40:11.855038 139946397853440 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.2788734436035156, loss=1.7843526601791382
I0208 09:40:59.109940 139946414638848 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.8955466747283936, loss=4.371849060058594
I0208 09:41:46.493885 139946397853440 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.9807814359664917, loss=2.2416961193084717
I0208 09:41:53.797381 140107197974336 spec.py:321] Evaluating on the training split.
I0208 09:42:05.101432 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 09:42:44.472639 140107197974336 spec.py:349] Evaluating on the test split.
I0208 09:42:46.068125 140107197974336 submission_runner.py:408] Time since start: 68129.01s, 	Step: 129217, 	{'train/accuracy': 0.7625390291213989, 'train/loss': 0.9737145900726318, 'validation/accuracy': 0.6969599723815918, 'validation/loss': 1.2702531814575195, 'validation/num_examples': 50000, 'test/accuracy': 0.572100043296814, 'test/loss': 1.9134632349014282, 'test/num_examples': 10000, 'score': 60542.8531806469, 'total_duration': 68129.01347446442, 'accumulated_submission_time': 60542.8531806469, 'accumulated_eval_time': 7572.68899512291, 'accumulated_logging_time': 6.103700399398804}
I0208 09:42:46.109614 139946414638848 logging_writer.py:48] [129217] accumulated_eval_time=7572.688995, accumulated_logging_time=6.103700, accumulated_submission_time=60542.853181, global_step=129217, preemption_count=0, score=60542.853181, test/accuracy=0.572100, test/loss=1.913463, test/num_examples=10000, total_duration=68129.013474, train/accuracy=0.762539, train/loss=0.973715, validation/accuracy=0.696960, validation/loss=1.270253, validation/num_examples=50000
I0208 09:43:21.830479 139946397853440 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.6893247365951538, loss=3.316586494445801
I0208 09:44:09.085170 139946414638848 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.084723711013794, loss=1.9458242654800415
I0208 09:44:56.400858 139946397853440 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.8068965673446655, loss=4.35275411605835
I0208 09:45:43.637145 139946414638848 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.8592455387115479, loss=1.9848520755767822
I0208 09:46:30.942150 139946397853440 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.0172059535980225, loss=2.0560262203216553
I0208 09:47:18.356667 139946414638848 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.9182343482971191, loss=2.0758562088012695
I0208 09:48:05.655071 139946397853440 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.9322245121002197, loss=1.8930574655532837
I0208 09:48:52.843904 139946414638848 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.8911060094833374, loss=2.0405125617980957
I0208 09:49:40.382752 139946397853440 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.883554220199585, loss=2.7710981369018555
I0208 09:49:46.199576 140107197974336 spec.py:321] Evaluating on the training split.
I0208 09:49:57.366109 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 09:50:37.935371 140107197974336 spec.py:349] Evaluating on the test split.
I0208 09:50:39.532267 140107197974336 submission_runner.py:408] Time since start: 68602.48s, 	Step: 130113, 	{'train/accuracy': 0.7669726610183716, 'train/loss': 0.9456799626350403, 'validation/accuracy': 0.7005999684333801, 'validation/loss': 1.244845986366272, 'validation/num_examples': 50000, 'test/accuracy': 0.5756000280380249, 'test/loss': 1.8801839351654053, 'test/num_examples': 10000, 'score': 60962.88206410408, 'total_duration': 68602.47761464119, 'accumulated_submission_time': 60962.88206410408, 'accumulated_eval_time': 7626.02169251442, 'accumulated_logging_time': 6.154720783233643}
I0208 09:50:39.577211 139946414638848 logging_writer.py:48] [130113] accumulated_eval_time=7626.021693, accumulated_logging_time=6.154721, accumulated_submission_time=60962.882064, global_step=130113, preemption_count=0, score=60962.882064, test/accuracy=0.575600, test/loss=1.880184, test/num_examples=10000, total_duration=68602.477615, train/accuracy=0.766973, train/loss=0.945680, validation/accuracy=0.700600, validation/loss=1.244846, validation/num_examples=50000
I0208 09:51:17.731714 139946397853440 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.8048001527786255, loss=2.3570773601531982
I0208 09:52:05.321015 139946414638848 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.94053316116333, loss=4.514883995056152
I0208 09:52:53.282394 139946397853440 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.8608567714691162, loss=1.9885187149047852
I0208 09:53:40.776248 139946414638848 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.8791667222976685, loss=4.375558376312256
I0208 09:54:29.030521 139946397853440 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.2630417346954346, loss=3.743748664855957
I0208 09:55:16.764132 139946414638848 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.1958632469177246, loss=1.9000320434570312
I0208 09:56:04.709995 139946397853440 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.6951653957366943, loss=3.529895544052124
I0208 09:56:52.684145 139946414638848 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.259881019592285, loss=1.8731695413589478
I0208 09:57:39.838129 140107197974336 spec.py:321] Evaluating on the training split.
I0208 09:57:50.825592 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 09:58:30.739772 140107197974336 spec.py:349] Evaluating on the test split.
I0208 09:58:32.335870 140107197974336 submission_runner.py:408] Time since start: 69075.28s, 	Step: 131000, 	{'train/accuracy': 0.7790820002555847, 'train/loss': 0.8822490572929382, 'validation/accuracy': 0.7021999955177307, 'validation/loss': 1.2245677709579468, 'validation/num_examples': 50000, 'test/accuracy': 0.5766000151634216, 'test/loss': 1.8699551820755005, 'test/num_examples': 10000, 'score': 61383.08276414871, 'total_duration': 69075.28121972084, 'accumulated_submission_time': 61383.08276414871, 'accumulated_eval_time': 7678.519439458847, 'accumulated_logging_time': 6.209751129150391}
I0208 09:58:32.373254 139946397853440 logging_writer.py:48] [131000] accumulated_eval_time=7678.519439, accumulated_logging_time=6.209751, accumulated_submission_time=61383.082764, global_step=131000, preemption_count=0, score=61383.082764, test/accuracy=0.576600, test/loss=1.869955, test/num_examples=10000, total_duration=69075.281220, train/accuracy=0.779082, train/loss=0.882249, validation/accuracy=0.702200, validation/loss=1.224568, validation/num_examples=50000
I0208 09:58:32.798125 139946414638848 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.7281070947647095, loss=2.807715654373169
I0208 09:59:16.232152 139946397853440 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.972921371459961, loss=1.836472749710083
I0208 10:00:03.572540 139946414638848 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.9689316749572754, loss=1.9484257698059082
I0208 10:00:51.238960 139946397853440 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.830106496810913, loss=1.8150941133499146
I0208 10:01:39.017698 139946414638848 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.8955800533294678, loss=2.149320125579834
I0208 10:02:26.604001 139946397853440 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.831516146659851, loss=1.9278274774551392
I0208 10:03:14.162796 139946414638848 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.6933187246322632, loss=4.163810729980469
I0208 10:04:01.595381 139946397853440 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.7458206415176392, loss=4.266140937805176
I0208 10:04:49.307819 139946414638848 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.7781647443771362, loss=3.2110886573791504
I0208 10:05:32.458068 140107197974336 spec.py:321] Evaluating on the training split.
I0208 10:05:43.803629 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 10:06:23.332473 140107197974336 spec.py:349] Evaluating on the test split.
I0208 10:06:24.922134 140107197974336 submission_runner.py:408] Time since start: 69547.87s, 	Step: 131892, 	{'train/accuracy': 0.7725781202316284, 'train/loss': 0.9053280353546143, 'validation/accuracy': 0.7041800022125244, 'validation/loss': 1.2155123949050903, 'validation/num_examples': 50000, 'test/accuracy': 0.575700044631958, 'test/loss': 1.8553532361984253, 'test/num_examples': 10000, 'score': 61803.106755018234, 'total_duration': 69547.86748552322, 'accumulated_submission_time': 61803.106755018234, 'accumulated_eval_time': 7730.9835069179535, 'accumulated_logging_time': 6.25759744644165}
I0208 10:06:24.959597 139946397853440 logging_writer.py:48] [131892] accumulated_eval_time=7730.983507, accumulated_logging_time=6.257597, accumulated_submission_time=61803.106755, global_step=131892, preemption_count=0, score=61803.106755, test/accuracy=0.575700, test/loss=1.855353, test/num_examples=10000, total_duration=69547.867486, train/accuracy=0.772578, train/loss=0.905328, validation/accuracy=0.704180, validation/loss=1.215512, validation/num_examples=50000
I0208 10:06:28.661975 139946414638848 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.7647556066513062, loss=2.655744791030884
I0208 10:07:12.947896 139946397853440 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.8613759279251099, loss=3.711655616760254
I0208 10:08:01.620812 139946414638848 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.8204249143600464, loss=1.8532127141952515
I0208 10:08:48.962161 139946397853440 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.7981520891189575, loss=2.305449962615967
I0208 10:09:36.537892 139946414638848 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.7910445928573608, loss=3.640629291534424
I0208 10:10:24.133636 139946397853440 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.027153968811035, loss=1.9548351764678955
I0208 10:11:11.644459 139946414638848 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.3174495697021484, loss=1.9618957042694092
I0208 10:11:59.025103 139946397853440 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.7621031999588013, loss=3.484011650085449
I0208 10:12:46.841667 139946414638848 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.047253131866455, loss=2.126103639602661
I0208 10:13:25.268404 140107197974336 spec.py:321] Evaluating on the training split.
I0208 10:13:36.449302 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 10:14:16.206760 140107197974336 spec.py:349] Evaluating on the test split.
I0208 10:14:17.804065 140107197974336 submission_runner.py:408] Time since start: 70020.75s, 	Step: 132782, 	{'train/accuracy': 0.7796679735183716, 'train/loss': 0.8771883249282837, 'validation/accuracy': 0.7079199552536011, 'validation/loss': 1.195351243019104, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.8324159383773804, 'test/num_examples': 10000, 'score': 62223.355676651, 'total_duration': 70020.74941420555, 'accumulated_submission_time': 62223.355676651, 'accumulated_eval_time': 7783.519206285477, 'accumulated_logging_time': 6.3048930168151855}
I0208 10:14:17.842566 139946397853440 logging_writer.py:48] [132782] accumulated_eval_time=7783.519206, accumulated_logging_time=6.304893, accumulated_submission_time=62223.355677, global_step=132782, preemption_count=0, score=62223.355677, test/accuracy=0.583300, test/loss=1.832416, test/num_examples=10000, total_duration=70020.749414, train/accuracy=0.779668, train/loss=0.877188, validation/accuracy=0.707920, validation/loss=1.195351, validation/num_examples=50000
I0208 10:14:25.654522 139946414638848 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.1932199001312256, loss=2.040519952774048
I0208 10:15:10.123429 139946397853440 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.1624200344085693, loss=4.222470283508301
I0208 10:15:57.394663 139946414638848 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.9356141090393066, loss=1.8870970010757446
I0208 10:16:44.761589 139946397853440 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.2977182865142822, loss=1.809942364692688
I0208 10:17:32.121268 139946414638848 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.2895238399505615, loss=1.825455665588379
I0208 10:18:19.537250 139946397853440 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.9082750082015991, loss=1.8695776462554932
I0208 10:19:07.016590 139946414638848 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.109818696975708, loss=2.01212739944458
I0208 10:19:54.230178 139946397853440 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.9254523515701294, loss=1.7695107460021973
I0208 10:20:41.611078 139946414638848 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.9624537229537964, loss=2.269507646560669
I0208 10:21:18.289679 140107197974336 spec.py:321] Evaluating on the training split.
I0208 10:21:29.347747 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 10:22:10.113943 140107197974336 spec.py:349] Evaluating on the test split.
I0208 10:22:11.709349 140107197974336 submission_runner.py:408] Time since start: 70494.65s, 	Step: 133679, 	{'train/accuracy': 0.78236323595047, 'train/loss': 0.8421926498413086, 'validation/accuracy': 0.7068600058555603, 'validation/loss': 1.1863993406295776, 'validation/num_examples': 50000, 'test/accuracy': 0.5857000350952148, 'test/loss': 1.8177932500839233, 'test/num_examples': 10000, 'score': 62643.74037742615, 'total_duration': 70494.65468883514, 'accumulated_submission_time': 62643.74037742615, 'accumulated_eval_time': 7836.93887090683, 'accumulated_logging_time': 6.35483980178833}
I0208 10:22:11.747560 139946397853440 logging_writer.py:48] [133679] accumulated_eval_time=7836.938871, accumulated_logging_time=6.354840, accumulated_submission_time=62643.740377, global_step=133679, preemption_count=0, score=62643.740377, test/accuracy=0.585700, test/loss=1.817793, test/num_examples=10000, total_duration=70494.654689, train/accuracy=0.782363, train/loss=0.842193, validation/accuracy=0.706860, validation/loss=1.186399, validation/num_examples=50000
I0208 10:22:20.788623 139946414638848 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.024742841720581, loss=1.88443922996521
I0208 10:23:05.420284 139946397853440 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.7966632843017578, loss=3.362246513366699
I0208 10:23:52.971370 139946414638848 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.9169845581054688, loss=2.2656009197235107
I0208 10:24:40.640078 139946397853440 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.9502202272415161, loss=1.8396921157836914
I0208 10:25:28.085363 139946414638848 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.0500729084014893, loss=1.8658055067062378
I0208 10:26:15.655530 139946397853440 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.8753646612167358, loss=2.375779151916504
I0208 10:27:03.261374 139946414638848 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.9250435829162598, loss=1.998389720916748
I0208 10:27:50.711853 139946397853440 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.9662691354751587, loss=1.6846463680267334
I0208 10:28:38.170063 139946414638848 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.0687310695648193, loss=1.7536870241165161
I0208 10:29:12.047384 140107197974336 spec.py:321] Evaluating on the training split.
I0208 10:29:23.236552 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 10:30:02.140380 140107197974336 spec.py:349] Evaluating on the test split.
I0208 10:30:03.740021 140107197974336 submission_runner.py:408] Time since start: 70966.69s, 	Step: 134573, 	{'train/accuracy': 0.7735351324081421, 'train/loss': 0.902022123336792, 'validation/accuracy': 0.7064799666404724, 'validation/loss': 1.2089617252349854, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.8554986715316772, 'test/num_examples': 10000, 'score': 63063.97910571098, 'total_duration': 70966.68536663055, 'accumulated_submission_time': 63063.97910571098, 'accumulated_eval_time': 7888.631495952606, 'accumulated_logging_time': 6.403504371643066}
I0208 10:30:03.780118 139946397853440 logging_writer.py:48] [134573] accumulated_eval_time=7888.631496, accumulated_logging_time=6.403504, accumulated_submission_time=63063.979106, global_step=134573, preemption_count=0, score=63063.979106, test/accuracy=0.577000, test/loss=1.855499, test/num_examples=10000, total_duration=70966.685367, train/accuracy=0.773535, train/loss=0.902022, validation/accuracy=0.706480, validation/loss=1.208962, validation/num_examples=50000
I0208 10:30:15.283896 139946414638848 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.041778087615967, loss=1.9115662574768066
I0208 10:31:00.447070 139946397853440 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.846799612045288, loss=3.527932643890381
I0208 10:31:47.626246 139946414638848 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.07338809967041, loss=1.8761651515960693
I0208 10:32:35.197185 139946397853440 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.8123611211776733, loss=2.182894229888916
I0208 10:33:22.702569 139946414638848 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.1693873405456543, loss=2.720410108566284
I0208 10:34:10.117901 139946397853440 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.8672329187393188, loss=2.1540133953094482
I0208 10:34:57.926311 139946414638848 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.300309896469116, loss=3.784018039703369
I0208 10:35:45.506758 139946397853440 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.9644091129302979, loss=1.8876925706863403
I0208 10:36:33.195193 139946414638848 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.975511908531189, loss=1.843084454536438
I0208 10:37:03.873512 140107197974336 spec.py:321] Evaluating on the training split.
I0208 10:37:15.177295 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 10:37:53.882169 140107197974336 spec.py:349] Evaluating on the test split.
I0208 10:37:55.478423 140107197974336 submission_runner.py:408] Time since start: 71438.42s, 	Step: 135466, 	{'train/accuracy': 0.7805468440055847, 'train/loss': 0.8645254373550415, 'validation/accuracy': 0.7113199830055237, 'validation/loss': 1.1867696046829224, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 1.8302571773529053, 'test/num_examples': 10000, 'score': 63484.011751413345, 'total_duration': 71438.42376971245, 'accumulated_submission_time': 63484.011751413345, 'accumulated_eval_time': 7940.2364201545715, 'accumulated_logging_time': 6.454331636428833}
I0208 10:37:55.517553 139946397853440 logging_writer.py:48] [135466] accumulated_eval_time=7940.236420, accumulated_logging_time=6.454332, accumulated_submission_time=63484.011751, global_step=135466, preemption_count=0, score=63484.011751, test/accuracy=0.585100, test/loss=1.830257, test/num_examples=10000, total_duration=71438.423770, train/accuracy=0.780547, train/loss=0.864525, validation/accuracy=0.711320, validation/loss=1.186770, validation/num_examples=50000
I0208 10:38:09.886049 139946414638848 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.80539870262146, loss=4.237338542938232
I0208 10:38:55.580355 139946397853440 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.482224464416504, loss=1.912841796875
I0208 10:39:43.093544 139946414638848 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.991274118423462, loss=4.107058048248291
I0208 10:40:30.631438 139946397853440 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.9966391324996948, loss=2.3087615966796875
I0208 10:41:18.220640 139946414638848 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.9215563535690308, loss=1.7424739599227905
I0208 10:42:05.645644 139946397853440 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.8915389776229858, loss=2.4572346210479736
I0208 10:42:53.123253 139946414638848 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.9278532266616821, loss=2.0153658390045166
I0208 10:43:40.792554 139946397853440 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.8389519453048706, loss=3.5300846099853516
I0208 10:44:28.165775 139946414638848 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.9138370752334595, loss=4.202425956726074
I0208 10:44:55.574917 140107197974336 spec.py:321] Evaluating on the training split.
I0208 10:45:06.525727 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 10:45:46.058311 140107197974336 spec.py:349] Evaluating on the test split.
I0208 10:45:47.654403 140107197974336 submission_runner.py:408] Time since start: 71910.60s, 	Step: 136359, 	{'train/accuracy': 0.78480464220047, 'train/loss': 0.8444609045982361, 'validation/accuracy': 0.7076399922370911, 'validation/loss': 1.1892009973526, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.8246159553527832, 'test/num_examples': 10000, 'score': 63904.009852170944, 'total_duration': 71910.59975337982, 'accumulated_submission_time': 63904.009852170944, 'accumulated_eval_time': 7992.3159103393555, 'accumulated_logging_time': 6.5025811195373535}
I0208 10:45:47.693081 139946397853440 logging_writer.py:48] [136359] accumulated_eval_time=7992.315910, accumulated_logging_time=6.502581, accumulated_submission_time=63904.009852, global_step=136359, preemption_count=0, score=63904.009852, test/accuracy=0.581900, test/loss=1.824616, test/num_examples=10000, total_duration=71910.599753, train/accuracy=0.784805, train/loss=0.844461, validation/accuracy=0.707640, validation/loss=1.189201, validation/num_examples=50000
I0208 10:46:04.948070 139946414638848 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.2633841037750244, loss=4.240560531616211
I0208 10:46:50.785731 139946397853440 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.3755390644073486, loss=1.901228666305542
I0208 10:47:38.407461 139946414638848 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.2159008979797363, loss=2.541783094406128
I0208 10:48:25.776683 139946397853440 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.8606529235839844, loss=2.131852149963379
I0208 10:49:13.165242 139946414638848 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.10146164894104, loss=3.3032805919647217
I0208 10:50:00.488446 139946397853440 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.8943167924880981, loss=1.671520709991455
I0208 10:50:48.189898 139946414638848 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.018320083618164, loss=3.90651535987854
I0208 10:51:35.853297 139946397853440 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.249008893966675, loss=1.61872136592865
I0208 10:52:23.314972 139946414638848 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.025808572769165, loss=1.9011616706848145
I0208 10:52:47.662621 140107197974336 spec.py:321] Evaluating on the training split.
I0208 10:52:58.473149 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 10:53:39.749504 140107197974336 spec.py:349] Evaluating on the test split.
I0208 10:53:41.342511 140107197974336 submission_runner.py:408] Time since start: 72384.29s, 	Step: 137253, 	{'train/accuracy': 0.7909960746765137, 'train/loss': 0.8081295490264893, 'validation/accuracy': 0.7122399806976318, 'validation/loss': 1.1592646837234497, 'validation/num_examples': 50000, 'test/accuracy': 0.5891000032424927, 'test/loss': 1.7870514392852783, 'test/num_examples': 10000, 'score': 64323.90890264511, 'total_duration': 72384.28784418106, 'accumulated_submission_time': 64323.90890264511, 'accumulated_eval_time': 8045.995781421661, 'accumulated_logging_time': 6.56099271774292}
I0208 10:53:41.385215 139946397853440 logging_writer.py:48] [137253] accumulated_eval_time=8045.995781, accumulated_logging_time=6.560993, accumulated_submission_time=64323.908903, global_step=137253, preemption_count=0, score=64323.908903, test/accuracy=0.589100, test/loss=1.787051, test/num_examples=10000, total_duration=72384.287844, train/accuracy=0.790996, train/loss=0.808130, validation/accuracy=0.712240, validation/loss=1.159265, validation/num_examples=50000
I0208 10:54:01.089836 139946414638848 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.972757339477539, loss=4.3084611892700195
I0208 10:54:47.672425 139946397853440 logging_writer.py:48] [137400] global_step=137400, grad_norm=2.0868730545043945, loss=1.8325344324111938
I0208 10:55:35.461941 139946414638848 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.8940719366073608, loss=3.315617084503174
I0208 10:56:23.125502 139946397853440 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.182504653930664, loss=2.306886911392212
I0208 10:57:10.947101 139946414638848 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.1825931072235107, loss=1.750673532485962
I0208 10:57:58.499100 139946397853440 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.0504343509674072, loss=1.9191468954086304
I0208 10:58:46.353575 139946414638848 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.9691474437713623, loss=2.4537129402160645
I0208 10:59:34.141828 139946397853440 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.7575936317443848, loss=3.068505048751831
I0208 11:00:21.970057 139946414638848 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.0869505405426025, loss=3.6284074783325195
I0208 11:00:41.748731 140107197974336 spec.py:321] Evaluating on the training split.
I0208 11:00:52.849144 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 11:01:31.585181 140107197974336 spec.py:349] Evaluating on the test split.
I0208 11:01:33.184820 140107197974336 submission_runner.py:408] Time since start: 72856.13s, 	Step: 138143, 	{'train/accuracy': 0.7822265625, 'train/loss': 0.8877792358398438, 'validation/accuracy': 0.7106399536132812, 'validation/loss': 1.2171674966812134, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 1.8493305444717407, 'test/num_examples': 10000, 'score': 64744.211985588074, 'total_duration': 72856.13017225266, 'accumulated_submission_time': 64744.211985588074, 'accumulated_eval_time': 8097.4318726062775, 'accumulated_logging_time': 6.613979339599609}
I0208 11:01:33.224521 139946397853440 logging_writer.py:48] [138143] accumulated_eval_time=8097.431873, accumulated_logging_time=6.613979, accumulated_submission_time=64744.211986, global_step=138143, preemption_count=0, score=64744.211986, test/accuracy=0.583600, test/loss=1.849331, test/num_examples=10000, total_duration=72856.130172, train/accuracy=0.782227, train/loss=0.887779, validation/accuracy=0.710640, validation/loss=1.217167, validation/num_examples=50000
I0208 11:01:57.041976 139946414638848 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.086778402328491, loss=1.9720252752304077
I0208 11:02:43.974837 139946397853440 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.121389627456665, loss=2.018939256668091
I0208 11:03:31.509940 139946414638848 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.297039031982422, loss=1.81874680519104
I0208 11:04:19.282974 139946397853440 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.7807878255844116, loss=3.5630269050598145
I0208 11:05:07.237251 139946414638848 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.135319232940674, loss=3.8619179725646973
I0208 11:05:54.648855 139946397853440 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.8770995140075684, loss=1.8814775943756104
I0208 11:06:42.262342 139946414638848 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.0634567737579346, loss=2.5783281326293945
I0208 11:07:29.910918 139946397853440 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.038815975189209, loss=3.714979648590088
I0208 11:08:17.360025 139946414638848 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.081841230392456, loss=2.489403486251831
I0208 11:08:33.263723 140107197974336 spec.py:321] Evaluating on the training split.
I0208 11:08:44.454496 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 11:09:23.095786 140107197974336 spec.py:349] Evaluating on the test split.
I0208 11:09:24.696268 140107197974336 submission_runner.py:408] Time since start: 73327.64s, 	Step: 139035, 	{'train/accuracy': 0.7908398509025574, 'train/loss': 0.8290248513221741, 'validation/accuracy': 0.7122799754142761, 'validation/loss': 1.171581745147705, 'validation/num_examples': 50000, 'test/accuracy': 0.5884000062942505, 'test/loss': 1.7982066869735718, 'test/num_examples': 10000, 'score': 65164.19147825241, 'total_duration': 73327.64161705971, 'accumulated_submission_time': 65164.19147825241, 'accumulated_eval_time': 8148.8644115924835, 'accumulated_logging_time': 6.663257837295532}
I0208 11:09:24.740690 139946397853440 logging_writer.py:48] [139035] accumulated_eval_time=8148.864412, accumulated_logging_time=6.663258, accumulated_submission_time=65164.191478, global_step=139035, preemption_count=0, score=65164.191478, test/accuracy=0.588400, test/loss=1.798207, test/num_examples=10000, total_duration=73327.641617, train/accuracy=0.790840, train/loss=0.829025, validation/accuracy=0.712280, validation/loss=1.171582, validation/num_examples=50000
I0208 11:09:52.086496 139946414638848 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.8355042934417725, loss=1.6756404638290405
I0208 11:10:39.266999 139946397853440 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.1968812942504883, loss=1.8245470523834229
I0208 11:11:26.744070 139946414638848 logging_writer.py:48] [139300] global_step=139300, grad_norm=2.2709286212921143, loss=2.3419060707092285
I0208 11:12:14.171457 139946397853440 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.036377429962158, loss=1.8392536640167236
I0208 11:13:01.737487 139946414638848 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.1748206615448, loss=1.8301968574523926
I0208 11:13:48.951406 139946397853440 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.225254774093628, loss=3.1864116191864014
I0208 11:14:36.665148 139946414638848 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.976823091506958, loss=1.7971855401992798
I0208 11:15:24.661705 139946397853440 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.391296148300171, loss=1.7303165197372437
I0208 11:16:12.425670 139946414638848 logging_writer.py:48] [139900] global_step=139900, grad_norm=1.9704570770263672, loss=2.688045024871826
I0208 11:16:24.932151 140107197974336 spec.py:321] Evaluating on the training split.
I0208 11:16:36.381702 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 11:17:15.156296 140107197974336 spec.py:349] Evaluating on the test split.
I0208 11:17:16.753144 140107197974336 submission_runner.py:408] Time since start: 73799.70s, 	Step: 139928, 	{'train/accuracy': 0.8020117282867432, 'train/loss': 0.7729999423027039, 'validation/accuracy': 0.7156800031661987, 'validation/loss': 1.1472184658050537, 'validation/num_examples': 50000, 'test/accuracy': 0.5904000401496887, 'test/loss': 1.78292977809906, 'test/num_examples': 10000, 'score': 65584.32089519501, 'total_duration': 73799.69849538803, 'accumulated_submission_time': 65584.32089519501, 'accumulated_eval_time': 8200.685409069061, 'accumulated_logging_time': 6.717529058456421}
I0208 11:17:16.791448 139946397853440 logging_writer.py:48] [139928] accumulated_eval_time=8200.685409, accumulated_logging_time=6.717529, accumulated_submission_time=65584.320895, global_step=139928, preemption_count=0, score=65584.320895, test/accuracy=0.590400, test/loss=1.782930, test/num_examples=10000, total_duration=73799.698495, train/accuracy=0.802012, train/loss=0.773000, validation/accuracy=0.715680, validation/loss=1.147218, validation/num_examples=50000
I0208 11:17:47.379603 139946414638848 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.0649497509002686, loss=1.6584967374801636
I0208 11:18:34.454115 139946397853440 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.03764009475708, loss=1.6544573307037354
I0208 11:19:22.282120 139946414638848 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.209036111831665, loss=4.176397323608398
I0208 11:20:09.348337 139946397853440 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.052961826324463, loss=1.7387878894805908
I0208 11:20:56.910747 139946414638848 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.0928947925567627, loss=2.903172731399536
I0208 11:21:44.333780 139946397853440 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.364976406097412, loss=1.8809442520141602
I0208 11:22:31.886197 139946414638848 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.077883243560791, loss=2.683238983154297
I0208 11:23:19.484915 139946397853440 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.9752085208892822, loss=2.8533363342285156
I0208 11:24:06.843701 139946414638848 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.122849225997925, loss=1.7965939044952393
I0208 11:24:17.014275 140107197974336 spec.py:321] Evaluating on the training split.
I0208 11:24:28.505603 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 11:25:07.768186 140107197974336 spec.py:349] Evaluating on the test split.
I0208 11:25:09.368451 140107197974336 submission_runner.py:408] Time since start: 74272.31s, 	Step: 140823, 	{'train/accuracy': 0.7893944978713989, 'train/loss': 0.811639666557312, 'validation/accuracy': 0.7133199572563171, 'validation/loss': 1.150890588760376, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 1.7908883094787598, 'test/num_examples': 10000, 'score': 66004.48244214058, 'total_duration': 74272.31378889084, 'accumulated_submission_time': 66004.48244214058, 'accumulated_eval_time': 8253.039557218552, 'accumulated_logging_time': 6.766315937042236}
I0208 11:25:09.406909 139946397853440 logging_writer.py:48] [140823] accumulated_eval_time=8253.039557, accumulated_logging_time=6.766316, accumulated_submission_time=66004.482442, global_step=140823, preemption_count=0, score=66004.482442, test/accuracy=0.587900, test/loss=1.790888, test/num_examples=10000, total_duration=74272.313789, train/accuracy=0.789394, train/loss=0.811640, validation/accuracy=0.713320, validation/loss=1.150891, validation/num_examples=50000
I0208 11:25:42.501193 139946414638848 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.292194366455078, loss=2.5102248191833496
I0208 11:26:29.858140 139946397853440 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.3564453125, loss=1.8092964887619019
I0208 11:27:17.334106 139946414638848 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.9860427379608154, loss=2.3183395862579346
I0208 11:28:05.007263 139946397853440 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.188476800918579, loss=3.3190152645111084
I0208 11:28:52.624351 139946414638848 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.345319986343384, loss=1.99502694606781
I0208 11:29:40.558128 139946397853440 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.375298023223877, loss=3.6553399562835693
I0208 11:30:28.404860 139946414638848 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.1339752674102783, loss=1.8995518684387207
I0208 11:31:15.961049 139946397853440 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.190962076187134, loss=4.11322021484375
I0208 11:32:03.645057 139946414638848 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.044567346572876, loss=3.930342197418213
I0208 11:32:09.633940 140107197974336 spec.py:321] Evaluating on the training split.
I0208 11:32:20.673023 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 11:33:01.342849 140107197974336 spec.py:349] Evaluating on the test split.
I0208 11:33:02.951754 140107197974336 submission_runner.py:408] Time since start: 74745.90s, 	Step: 141714, 	{'train/accuracy': 0.7934374809265137, 'train/loss': 0.8436886668205261, 'validation/accuracy': 0.7143200039863586, 'validation/loss': 1.187924861907959, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.8304020166397095, 'test/num_examples': 10000, 'score': 66424.64887809753, 'total_duration': 74745.89710235596, 'accumulated_submission_time': 66424.64887809753, 'accumulated_eval_time': 8306.357367038727, 'accumulated_logging_time': 6.8145668506622314}
I0208 11:33:02.991958 139946397853440 logging_writer.py:48] [141714] accumulated_eval_time=8306.357367, accumulated_logging_time=6.814567, accumulated_submission_time=66424.648878, global_step=141714, preemption_count=0, score=66424.648878, test/accuracy=0.587100, test/loss=1.830402, test/num_examples=10000, total_duration=74745.897102, train/accuracy=0.793437, train/loss=0.843689, validation/accuracy=0.714320, validation/loss=1.187925, validation/num_examples=50000
I0208 11:33:40.331984 139946414638848 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.354013442993164, loss=1.7905746698379517
I0208 11:34:28.026047 139946397853440 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.295344829559326, loss=1.8149774074554443
I0208 11:35:15.830973 139946414638848 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.2303457260131836, loss=1.78465735912323
I0208 11:36:03.294884 139946397853440 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.063004732131958, loss=3.867870569229126
I0208 11:36:51.019531 139946414638848 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.429987668991089, loss=1.7102711200714111
I0208 11:37:38.774949 139946397853440 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.15964412689209, loss=1.7447149753570557
I0208 11:38:27.163743 139946414638848 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.2366998195648193, loss=2.3936045169830322
I0208 11:39:14.658191 139946397853440 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.4735100269317627, loss=1.7924883365631104
I0208 11:40:02.646559 139946414638848 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.049051523208618, loss=2.262169361114502
I0208 11:40:03.261964 140107197974336 spec.py:321] Evaluating on the training split.
I0208 11:40:14.111632 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 11:40:52.743549 140107197974336 spec.py:349] Evaluating on the test split.
I0208 11:40:54.336491 140107197974336 submission_runner.py:408] Time since start: 75217.28s, 	Step: 142603, 	{'train/accuracy': 0.8104296922683716, 'train/loss': 0.7512505054473877, 'validation/accuracy': 0.7206400036811829, 'validation/loss': 1.1383086442947388, 'validation/num_examples': 50000, 'test/accuracy': 0.5949000120162964, 'test/loss': 1.7849997282028198, 'test/num_examples': 10000, 'score': 66844.86011886597, 'total_duration': 75217.28182458878, 'accumulated_submission_time': 66844.86011886597, 'accumulated_eval_time': 8357.431869983673, 'accumulated_logging_time': 6.8636791706085205}
I0208 11:40:54.382797 139946397853440 logging_writer.py:48] [142603] accumulated_eval_time=8357.431870, accumulated_logging_time=6.863679, accumulated_submission_time=66844.860119, global_step=142603, preemption_count=0, score=66844.860119, test/accuracy=0.594900, test/loss=1.785000, test/num_examples=10000, total_duration=75217.281825, train/accuracy=0.810430, train/loss=0.751251, validation/accuracy=0.720640, validation/loss=1.138309, validation/num_examples=50000
I0208 11:41:36.995374 139946414638848 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.1010544300079346, loss=2.4323155879974365
I0208 11:42:24.274637 139946397853440 logging_writer.py:48] [142800] global_step=142800, grad_norm=1.9436020851135254, loss=3.0436148643493652
I0208 11:43:12.315773 139946414638848 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.043886184692383, loss=1.6721858978271484
I0208 11:43:59.756424 139946397853440 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.0827369689941406, loss=1.7064805030822754
I0208 11:44:47.672166 139946414638848 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.1412625312805176, loss=1.7582305669784546
I0208 11:45:35.585626 139946397853440 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.551614761352539, loss=1.8306950330734253
I0208 11:46:23.360842 139946414638848 logging_writer.py:48] [143300] global_step=143300, grad_norm=2.0792109966278076, loss=2.3538711071014404
I0208 11:47:11.131666 139946397853440 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.9359380006790161, loss=2.4272420406341553
I0208 11:47:54.570332 140107197974336 spec.py:321] Evaluating on the training split.
I0208 11:48:05.920197 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 11:48:45.419179 140107197974336 spec.py:349] Evaluating on the test split.
I0208 11:48:47.019036 140107197974336 submission_runner.py:408] Time since start: 75689.96s, 	Step: 143493, 	{'train/accuracy': 0.7852538824081421, 'train/loss': 0.8588006496429443, 'validation/accuracy': 0.713699996471405, 'validation/loss': 1.1768875122070312, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.8258694410324097, 'test/num_examples': 10000, 'score': 67264.9873714447, 'total_duration': 75689.96438384056, 'accumulated_submission_time': 67264.9873714447, 'accumulated_eval_time': 8409.880574703217, 'accumulated_logging_time': 6.9201719760894775}
I0208 11:48:47.066413 139946414638848 logging_writer.py:48] [143493] accumulated_eval_time=8409.880575, accumulated_logging_time=6.920172, accumulated_submission_time=67264.987371, global_step=143493, preemption_count=0, score=67264.987371, test/accuracy=0.590600, test/loss=1.825869, test/num_examples=10000, total_duration=75689.964384, train/accuracy=0.785254, train/loss=0.858801, validation/accuracy=0.713700, validation/loss=1.176888, validation/num_examples=50000
I0208 11:48:50.356505 139946397853440 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.190202474594116, loss=1.7701690196990967
I0208 11:49:34.013370 139946414638848 logging_writer.py:48] [143600] global_step=143600, grad_norm=2.104501724243164, loss=1.799111008644104
I0208 11:50:21.301550 139946397853440 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.3751962184906006, loss=1.867595911026001
I0208 11:51:08.837730 139946414638848 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.118664026260376, loss=2.2830262184143066
I0208 11:51:56.513862 139946397853440 logging_writer.py:48] [143900] global_step=143900, grad_norm=1.899908423423767, loss=3.37520432472229
I0208 11:52:43.733439 139946414638848 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.1138572692871094, loss=3.662822961807251
I0208 11:53:31.258393 139946397853440 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.118337631225586, loss=2.6498947143554688
I0208 11:54:18.539406 139946414638848 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.174626111984253, loss=1.84136962890625
I0208 11:55:06.214754 139946397853440 logging_writer.py:48] [144300] global_step=144300, grad_norm=2.7966954708099365, loss=1.714093565940857
I0208 11:55:47.522277 140107197974336 spec.py:321] Evaluating on the training split.
I0208 11:55:58.866587 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 11:56:38.429757 140107197974336 spec.py:349] Evaluating on the test split.
I0208 11:56:40.037561 140107197974336 submission_runner.py:408] Time since start: 76162.98s, 	Step: 144389, 	{'train/accuracy': 0.8009960651397705, 'train/loss': 0.8065456748008728, 'validation/accuracy': 0.7186799645423889, 'validation/loss': 1.1656146049499512, 'validation/num_examples': 50000, 'test/accuracy': 0.5975000262260437, 'test/loss': 1.7978652715682983, 'test/num_examples': 10000, 'score': 67685.38305974007, 'total_duration': 76162.98290419579, 'accumulated_submission_time': 67685.38305974007, 'accumulated_eval_time': 8462.395847082138, 'accumulated_logging_time': 6.977154016494751}
I0208 11:56:40.077621 139946414638848 logging_writer.py:48] [144389] accumulated_eval_time=8462.395847, accumulated_logging_time=6.977154, accumulated_submission_time=67685.383060, global_step=144389, preemption_count=0, score=67685.383060, test/accuracy=0.597500, test/loss=1.797865, test/num_examples=10000, total_duration=76162.982904, train/accuracy=0.800996, train/loss=0.806546, validation/accuracy=0.718680, validation/loss=1.165615, validation/num_examples=50000
I0208 11:56:45.010161 139946397853440 logging_writer.py:48] [144400] global_step=144400, grad_norm=1.9165037870407104, loss=3.7503695487976074
I0208 11:57:28.952507 139946414638848 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.732419013977051, loss=1.733607530593872
I0208 11:58:16.075820 139946397853440 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.2157087326049805, loss=3.234025239944458
I0208 11:59:03.329846 139946414638848 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.208966016769409, loss=2.837662696838379
I0208 11:59:50.548700 139946397853440 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.1595561504364014, loss=4.265273094177246
I0208 12:00:38.014374 139946414638848 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.172740936279297, loss=1.716421365737915
I0208 12:01:25.375679 139946397853440 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.3948967456817627, loss=4.190609455108643
I0208 12:02:12.710265 139946414638848 logging_writer.py:48] [145100] global_step=145100, grad_norm=2.3236935138702393, loss=1.651578426361084
I0208 12:03:00.369838 139946397853440 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.2850162982940674, loss=1.751733422279358
I0208 12:03:40.280349 140107197974336 spec.py:321] Evaluating on the training split.
I0208 12:03:51.522487 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 12:04:31.052159 140107197974336 spec.py:349] Evaluating on the test split.
I0208 12:04:32.647486 140107197974336 submission_runner.py:408] Time since start: 76635.59s, 	Step: 145286, 	{'train/accuracy': 0.8099414110183716, 'train/loss': 0.7473099231719971, 'validation/accuracy': 0.720579981803894, 'validation/loss': 1.1367037296295166, 'validation/num_examples': 50000, 'test/accuracy': 0.593500018119812, 'test/loss': 1.777706503868103, 'test/num_examples': 10000, 'score': 68105.52442455292, 'total_duration': 76635.59283566475, 'accumulated_submission_time': 68105.52442455292, 'accumulated_eval_time': 8514.762991905212, 'accumulated_logging_time': 7.027865171432495}
I0208 12:04:32.690163 139946414638848 logging_writer.py:48] [145286] accumulated_eval_time=8514.762992, accumulated_logging_time=7.027865, accumulated_submission_time=68105.524425, global_step=145286, preemption_count=0, score=68105.524425, test/accuracy=0.593500, test/loss=1.777707, test/num_examples=10000, total_duration=76635.592836, train/accuracy=0.809941, train/loss=0.747310, validation/accuracy=0.720580, validation/loss=1.136704, validation/num_examples=50000
I0208 12:04:38.852856 139946397853440 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.1729187965393066, loss=2.0344009399414062
I0208 12:05:23.114242 139946414638848 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.2597732543945312, loss=1.7712953090667725
I0208 12:06:10.481422 139946397853440 logging_writer.py:48] [145500] global_step=145500, grad_norm=2.0396790504455566, loss=2.7292888164520264
I0208 12:06:57.852517 139946414638848 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.230908155441284, loss=3.350416898727417
I0208 12:07:45.693072 139946397853440 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.5301032066345215, loss=4.245868682861328
I0208 12:08:33.381672 139946414638848 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.254920721054077, loss=1.7760896682739258
I0208 12:09:20.868207 139946397853440 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.144713878631592, loss=1.828409194946289
I0208 12:10:08.284397 139946414638848 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.2000632286071777, loss=1.9269704818725586
I0208 12:10:55.591954 139946397853440 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.1181459426879883, loss=2.219214916229248
I0208 12:11:32.822393 140107197974336 spec.py:321] Evaluating on the training split.
I0208 12:11:43.967721 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 12:12:24.427889 140107197974336 spec.py:349] Evaluating on the test split.
I0208 12:12:26.026457 140107197974336 submission_runner.py:408] Time since start: 77108.97s, 	Step: 146180, 	{'train/accuracy': 0.7946679592132568, 'train/loss': 0.8052504062652588, 'validation/accuracy': 0.7216199636459351, 'validation/loss': 1.132479190826416, 'validation/num_examples': 50000, 'test/accuracy': 0.5982000231742859, 'test/loss': 1.7780965566635132, 'test/num_examples': 10000, 'score': 68525.59720563889, 'total_duration': 77108.97180509567, 'accumulated_submission_time': 68525.59720563889, 'accumulated_eval_time': 8567.967049360275, 'accumulated_logging_time': 7.0798468589782715}
I0208 12:12:26.066738 139946414638848 logging_writer.py:48] [146180] accumulated_eval_time=8567.967049, accumulated_logging_time=7.079847, accumulated_submission_time=68525.597206, global_step=146180, preemption_count=0, score=68525.597206, test/accuracy=0.598200, test/loss=1.778097, test/num_examples=10000, total_duration=77108.971805, train/accuracy=0.794668, train/loss=0.805250, validation/accuracy=0.721620, validation/loss=1.132479, validation/num_examples=50000
I0208 12:12:34.694852 139946397853440 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.6052303314208984, loss=2.931095600128174
I0208 12:13:19.821016 139946414638848 logging_writer.py:48] [146300] global_step=146300, grad_norm=2.3672492504119873, loss=2.2003939151763916
I0208 12:14:07.473116 139946397853440 logging_writer.py:48] [146400] global_step=146400, grad_norm=2.1668198108673096, loss=3.2707571983337402
I0208 12:14:54.924674 139946414638848 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.0239968299865723, loss=3.9287633895874023
I0208 12:15:42.551478 139946397853440 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.4300601482391357, loss=1.654731273651123
I0208 12:16:30.192257 139946414638848 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.2813363075256348, loss=1.7246750593185425
I0208 12:17:18.191886 139946397853440 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.4077961444854736, loss=1.7599599361419678
I0208 12:18:06.042475 139946414638848 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.3189618587493896, loss=1.6952154636383057
I0208 12:18:53.910672 139946397853440 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.2109286785125732, loss=2.95068097114563
I0208 12:19:26.245721 140107197974336 spec.py:321] Evaluating on the training split.
I0208 12:19:37.515547 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 12:20:16.180077 140107197974336 spec.py:349] Evaluating on the test split.
I0208 12:20:17.783445 140107197974336 submission_runner.py:408] Time since start: 77580.73s, 	Step: 147069, 	{'train/accuracy': 0.8073241710662842, 'train/loss': 0.7616297602653503, 'validation/accuracy': 0.7257999777793884, 'validation/loss': 1.1163734197616577, 'validation/num_examples': 50000, 'test/accuracy': 0.5991000533103943, 'test/loss': 1.7480182647705078, 'test/num_examples': 10000, 'score': 68945.71439909935, 'total_duration': 77580.7287735939, 'accumulated_submission_time': 68945.71439909935, 'accumulated_eval_time': 8619.504743099213, 'accumulated_logging_time': 7.131839036941528}
I0208 12:20:17.831649 139946414638848 logging_writer.py:48] [147069] accumulated_eval_time=8619.504743, accumulated_logging_time=7.131839, accumulated_submission_time=68945.714399, global_step=147069, preemption_count=0, score=68945.714399, test/accuracy=0.599100, test/loss=1.748018, test/num_examples=10000, total_duration=77580.728774, train/accuracy=0.807324, train/loss=0.761630, validation/accuracy=0.725800, validation/loss=1.116373, validation/num_examples=50000
I0208 12:20:30.983890 139946397853440 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.228856086730957, loss=4.26876163482666
I0208 12:21:16.407624 139946414638848 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.544703245162964, loss=2.8792948722839355
I0208 12:22:03.927569 139946397853440 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.481297492980957, loss=1.874333381652832
I0208 12:22:51.311599 139946414638848 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.4995477199554443, loss=1.8868283033370972
I0208 12:23:38.694528 139946397853440 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.2119128704071045, loss=1.7547705173492432
I0208 12:24:26.216166 139946414638848 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.493532657623291, loss=1.7735843658447266
I0208 12:25:14.012101 139946397853440 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.69644832611084, loss=1.6813993453979492
I0208 12:26:01.727230 139946414638848 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.12151837348938, loss=1.7240285873413086
I0208 12:26:49.498537 139946397853440 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.3491621017456055, loss=1.9680861234664917
I0208 12:27:17.883672 140107197974336 spec.py:321] Evaluating on the training split.
I0208 12:27:29.186546 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 12:28:09.343725 140107197974336 spec.py:349] Evaluating on the test split.
I0208 12:28:10.938868 140107197974336 submission_runner.py:408] Time since start: 78053.88s, 	Step: 147961, 	{'train/accuracy': 0.8156249523162842, 'train/loss': 0.7296286821365356, 'validation/accuracy': 0.7245199680328369, 'validation/loss': 1.1153302192687988, 'validation/num_examples': 50000, 'test/accuracy': 0.6000000238418579, 'test/loss': 1.755022406578064, 'test/num_examples': 10000, 'score': 69365.70506572723, 'total_duration': 78053.88421607018, 'accumulated_submission_time': 69365.70506572723, 'accumulated_eval_time': 8672.559925556183, 'accumulated_logging_time': 7.190670490264893}
I0208 12:28:10.978893 139946414638848 logging_writer.py:48] [147961] accumulated_eval_time=8672.559926, accumulated_logging_time=7.190670, accumulated_submission_time=69365.705066, global_step=147961, preemption_count=0, score=69365.705066, test/accuracy=0.600000, test/loss=1.755022, test/num_examples=10000, total_duration=78053.884216, train/accuracy=0.815625, train/loss=0.729629, validation/accuracy=0.724520, validation/loss=1.115330, validation/num_examples=50000
I0208 12:28:27.407063 139946397853440 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.2498679161071777, loss=4.07951545715332
I0208 12:29:13.333510 139946414638848 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.4565672874450684, loss=1.5943703651428223
I0208 12:30:00.884195 139946397853440 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.8006949424743652, loss=1.6765694618225098
I0208 12:30:48.561256 139946414638848 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.2925992012023926, loss=3.3088293075561523
I0208 12:31:36.158928 139946397853440 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.338207960128784, loss=1.9681509733200073
I0208 12:32:23.620790 139946414638848 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.6797966957092285, loss=4.1286773681640625
I0208 12:33:11.330986 139946397853440 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.310438394546509, loss=4.145233631134033
I0208 12:33:59.075414 139946414638848 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.1273114681243896, loss=2.5543508529663086
I0208 12:34:47.005456 139946397853440 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.3156254291534424, loss=1.8126060962677002
I0208 12:35:11.131557 140107197974336 spec.py:321] Evaluating on the training split.
I0208 12:35:22.538550 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 12:36:00.948529 140107197974336 spec.py:349] Evaluating on the test split.
I0208 12:36:02.548150 140107197974336 submission_runner.py:408] Time since start: 78525.49s, 	Step: 148852, 	{'train/accuracy': 0.7974609136581421, 'train/loss': 0.7928144931793213, 'validation/accuracy': 0.7253400087356567, 'validation/loss': 1.1223499774932861, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.7640174627304077, 'test/num_examples': 10000, 'score': 69785.79780435562, 'total_duration': 78525.49350094795, 'accumulated_submission_time': 69785.79780435562, 'accumulated_eval_time': 8723.976521015167, 'accumulated_logging_time': 7.239832878112793}
I0208 12:36:02.592071 139946414638848 logging_writer.py:48] [148852] accumulated_eval_time=8723.976521, accumulated_logging_time=7.239833, accumulated_submission_time=69785.797804, global_step=148852, preemption_count=0, score=69785.797804, test/accuracy=0.601000, test/loss=1.764017, test/num_examples=10000, total_duration=78525.493501, train/accuracy=0.797461, train/loss=0.792814, validation/accuracy=0.725340, validation/loss=1.122350, validation/num_examples=50000
I0208 12:36:22.718590 139946397853440 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.239379644393921, loss=2.3086817264556885
I0208 12:37:09.229351 139946414638848 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.5253732204437256, loss=1.8971868753433228
I0208 12:37:56.655621 139946397853440 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.4802098274230957, loss=3.068356990814209
I0208 12:38:44.122896 139946414638848 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.424910545349121, loss=1.7131608724594116
I0208 12:39:31.818241 139946397853440 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.4730188846588135, loss=1.6645129919052124
I0208 12:40:19.495524 139946414638848 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.609488010406494, loss=3.4454808235168457
I0208 12:41:07.060195 139946397853440 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.474449872970581, loss=1.5818729400634766
I0208 12:41:54.635612 139946414638848 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.1581332683563232, loss=2.887789726257324
I0208 12:42:42.162053 139946397853440 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.316567897796631, loss=3.141456127166748
I0208 12:43:02.838107 140107197974336 spec.py:321] Evaluating on the training split.
I0208 12:43:14.004627 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 12:43:55.839110 140107197974336 spec.py:349] Evaluating on the test split.
I0208 12:43:57.436825 140107197974336 submission_runner.py:408] Time since start: 79000.38s, 	Step: 149745, 	{'train/accuracy': 0.8090038895606995, 'train/loss': 0.7413263320922852, 'validation/accuracy': 0.7287600040435791, 'validation/loss': 1.0949060916900635, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7284026145935059, 'test/num_examples': 10000, 'score': 70205.98230195045, 'total_duration': 79000.38216662407, 'accumulated_submission_time': 70205.98230195045, 'accumulated_eval_time': 8778.575244188309, 'accumulated_logging_time': 7.29470419883728}
I0208 12:43:57.483051 139946414638848 logging_writer.py:48] [149745] accumulated_eval_time=8778.575244, accumulated_logging_time=7.294704, accumulated_submission_time=70205.982302, global_step=149745, preemption_count=0, score=70205.982302, test/accuracy=0.607000, test/loss=1.728403, test/num_examples=10000, total_duration=79000.382167, train/accuracy=0.809004, train/loss=0.741326, validation/accuracy=0.728760, validation/loss=1.094906, validation/num_examples=50000
I0208 12:44:20.485154 139946397853440 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.36928129196167, loss=3.462749481201172
I0208 12:45:07.404656 139946414638848 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.231626033782959, loss=3.9471535682678223
I0208 12:45:54.728474 139946397853440 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.832319974899292, loss=2.3987317085266113
I0208 12:46:41.867130 139946414638848 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.4217000007629395, loss=1.6861578226089478
I0208 12:47:29.749305 139946397853440 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.3591344356536865, loss=1.7727892398834229
I0208 12:48:17.010979 139946414638848 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.6778037548065186, loss=3.84873104095459
I0208 12:49:04.701347 139946397853440 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.4433252811431885, loss=1.7407996654510498
I0208 12:49:51.863019 139946414638848 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.650542974472046, loss=2.0263304710388184
I0208 12:50:39.330496 139946397853440 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.6281614303588867, loss=1.679153561592102
I0208 12:50:57.641535 140107197974336 spec.py:321] Evaluating on the training split.
I0208 12:51:09.006231 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 12:51:48.573900 140107197974336 spec.py:349] Evaluating on the test split.
I0208 12:51:50.167741 140107197974336 submission_runner.py:408] Time since start: 79473.11s, 	Step: 150640, 	{'train/accuracy': 0.8140038847923279, 'train/loss': 0.7625994086265564, 'validation/accuracy': 0.7268999814987183, 'validation/loss': 1.140964388847351, 'validation/num_examples': 50000, 'test/accuracy': 0.5976999998092651, 'test/loss': 1.7766977548599243, 'test/num_examples': 10000, 'score': 70626.07793998718, 'total_duration': 79473.11308956146, 'accumulated_submission_time': 70626.07793998718, 'accumulated_eval_time': 8831.101482391357, 'accumulated_logging_time': 7.352070093154907}
I0208 12:51:50.212628 139946414638848 logging_writer.py:48] [150640] accumulated_eval_time=8831.101482, accumulated_logging_time=7.352070, accumulated_submission_time=70626.077940, global_step=150640, preemption_count=0, score=70626.077940, test/accuracy=0.597700, test/loss=1.776698, test/num_examples=10000, total_duration=79473.113090, train/accuracy=0.814004, train/loss=0.762599, validation/accuracy=0.726900, validation/loss=1.140964, validation/num_examples=50000
I0208 12:52:15.244674 139946397853440 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.53810715675354, loss=1.796168565750122
I0208 12:53:02.415557 139946414638848 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.2429847717285156, loss=3.5702807903289795
I0208 12:53:49.859562 139946397853440 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.4998977184295654, loss=1.8502111434936523
I0208 12:54:37.097500 139946414638848 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.407599687576294, loss=3.1834769248962402
I0208 12:55:24.798216 139946397853440 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.5866308212280273, loss=1.708139419555664
I0208 12:56:12.562954 139946414638848 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.60267972946167, loss=1.7702138423919678
I0208 12:57:00.085627 139946397853440 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.4740116596221924, loss=4.10642147064209
I0208 12:57:48.184457 139946414638848 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.4659619331359863, loss=1.6090493202209473
I0208 12:58:35.622322 139946397853440 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.194603443145752, loss=1.8399955034255981
I0208 12:58:50.457057 140107197974336 spec.py:321] Evaluating on the training split.
I0208 12:59:01.899576 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 12:59:41.019245 140107197974336 spec.py:349] Evaluating on the test split.
I0208 12:59:42.615708 140107197974336 submission_runner.py:408] Time since start: 79945.56s, 	Step: 151533, 	{'train/accuracy': 0.8054101467132568, 'train/loss': 0.7942066192626953, 'validation/accuracy': 0.7263000011444092, 'validation/loss': 1.1323789358139038, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.7719186544418335, 'test/num_examples': 10000, 'score': 71046.26297879219, 'total_duration': 79945.56105446815, 'accumulated_submission_time': 71046.26297879219, 'accumulated_eval_time': 8883.26012802124, 'accumulated_logging_time': 7.406408786773682}
I0208 12:59:42.659831 139946414638848 logging_writer.py:48] [151533] accumulated_eval_time=8883.260128, accumulated_logging_time=7.406409, accumulated_submission_time=71046.262979, global_step=151533, preemption_count=0, score=71046.262979, test/accuracy=0.602500, test/loss=1.771919, test/num_examples=10000, total_duration=79945.561054, train/accuracy=0.805410, train/loss=0.794207, validation/accuracy=0.726300, validation/loss=1.132379, validation/num_examples=50000
I0208 13:00:11.084314 139946397853440 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.2702600955963135, loss=1.6363205909729004
I0208 13:00:58.269752 139946414638848 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.5474979877471924, loss=4.089762210845947
I0208 13:01:46.006508 139946397853440 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.2193970680236816, loss=3.1238632202148438
I0208 13:02:33.524154 139946414638848 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.4428160190582275, loss=3.93645977973938
I0208 13:03:21.516482 139946397853440 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.5011801719665527, loss=3.2521350383758545
I0208 13:04:08.972257 139946414638848 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.2682721614837646, loss=2.6980185508728027
I0208 13:04:56.732698 139946397853440 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.5421454906463623, loss=1.624241828918457
I0208 13:05:44.544120 139946414638848 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.541215419769287, loss=2.914834499359131
I0208 13:06:31.960094 139946397853440 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.501390218734741, loss=1.5815807580947876
I0208 13:06:42.890291 140107197974336 spec.py:321] Evaluating on the training split.
I0208 13:06:54.189211 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 13:07:36.575882 140107197974336 spec.py:349] Evaluating on the test split.
I0208 13:07:38.168586 140107197974336 submission_runner.py:408] Time since start: 80421.11s, 	Step: 152424, 	{'train/accuracy': 0.8122460842132568, 'train/loss': 0.738717794418335, 'validation/accuracy': 0.7300999760627747, 'validation/loss': 1.0963596105575562, 'validation/num_examples': 50000, 'test/accuracy': 0.6144000291824341, 'test/loss': 1.7185935974121094, 'test/num_examples': 10000, 'score': 71466.43354034424, 'total_duration': 80421.11392855644, 'accumulated_submission_time': 71466.43354034424, 'accumulated_eval_time': 8938.53841495514, 'accumulated_logging_time': 7.459969520568848}
I0208 13:07:38.210396 139946414638848 logging_writer.py:48] [152424] accumulated_eval_time=8938.538415, accumulated_logging_time=7.459970, accumulated_submission_time=71466.433540, global_step=152424, preemption_count=0, score=71466.433540, test/accuracy=0.614400, test/loss=1.718594, test/num_examples=10000, total_duration=80421.113929, train/accuracy=0.812246, train/loss=0.738718, validation/accuracy=0.730100, validation/loss=1.096360, validation/num_examples=50000
I0208 13:08:10.797829 139946397853440 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.3556442260742188, loss=1.8263849020004272
I0208 13:08:57.559073 139946414638848 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.4629828929901123, loss=1.7271002531051636
I0208 13:09:45.483992 139946397853440 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.1910812854766846, loss=3.5281829833984375
I0208 13:10:32.805869 139946414638848 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.406017541885376, loss=2.590393543243408
I0208 13:11:20.125744 139946397853440 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.934462070465088, loss=2.4962832927703857
I0208 13:12:07.340625 139946414638848 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.589257001876831, loss=1.6260958909988403
I0208 13:12:54.498582 139946397853440 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.690227508544922, loss=1.7053720951080322
I0208 13:13:41.875456 139946414638848 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.2103686332702637, loss=1.8217788934707642
I0208 13:14:29.353559 139946397853440 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.320701837539673, loss=3.7447433471679688
I0208 13:14:38.555257 140107197974336 spec.py:321] Evaluating on the training split.
I0208 13:14:49.604485 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 13:15:28.402962 140107197974336 spec.py:349] Evaluating on the test split.
I0208 13:15:30.000431 140107197974336 submission_runner.py:408] Time since start: 80892.95s, 	Step: 153321, 	{'train/accuracy': 0.81751948595047, 'train/loss': 0.732594907283783, 'validation/accuracy': 0.7286399602890015, 'validation/loss': 1.1145501136779785, 'validation/num_examples': 50000, 'test/accuracy': 0.6084000468254089, 'test/loss': 1.746617078781128, 'test/num_examples': 10000, 'score': 71886.71799898148, 'total_duration': 80892.94577765465, 'accumulated_submission_time': 71886.71799898148, 'accumulated_eval_time': 8989.983581542969, 'accumulated_logging_time': 7.511559963226318}
I0208 13:15:30.041758 139946414638848 logging_writer.py:48] [153321] accumulated_eval_time=8989.983582, accumulated_logging_time=7.511560, accumulated_submission_time=71886.717999, global_step=153321, preemption_count=0, score=71886.717999, test/accuracy=0.608400, test/loss=1.746617, test/num_examples=10000, total_duration=80892.945778, train/accuracy=0.817519, train/loss=0.732595, validation/accuracy=0.728640, validation/loss=1.114550, validation/num_examples=50000
I0208 13:16:03.990309 139946397853440 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.410876750946045, loss=2.6744179725646973
I0208 13:16:51.167734 139946414638848 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.270848035812378, loss=2.6904969215393066
I0208 13:17:38.585974 139946397853440 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.476560354232788, loss=2.1430399417877197
I0208 13:18:26.003368 139946414638848 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.482671022415161, loss=1.8010597229003906
I0208 13:19:13.848574 139946397853440 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.493532419204712, loss=3.471247673034668
I0208 13:20:01.331572 139946414638848 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.1160693168640137, loss=2.722587823867798
I0208 13:20:48.964274 139946397853440 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.3095946311950684, loss=2.3914873600006104
I0208 13:21:36.619257 139946414638848 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.394643783569336, loss=2.3977293968200684
I0208 13:22:23.870965 139946397853440 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.57051682472229, loss=1.5923480987548828
I0208 13:22:30.103502 140107197974336 spec.py:321] Evaluating on the training split.
I0208 13:22:41.325511 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 13:23:21.607026 140107197974336 spec.py:349] Evaluating on the test split.
I0208 13:23:23.199073 140107197974336 submission_runner.py:408] Time since start: 81366.14s, 	Step: 154215, 	{'train/accuracy': 0.8136718273162842, 'train/loss': 0.7343711853027344, 'validation/accuracy': 0.7323399782180786, 'validation/loss': 1.0908153057098389, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.717441439628601, 'test/num_examples': 10000, 'score': 72306.71941304207, 'total_duration': 81366.14441990852, 'accumulated_submission_time': 72306.71941304207, 'accumulated_eval_time': 9043.079141378403, 'accumulated_logging_time': 7.5628063678741455}
I0208 13:23:23.243104 139946414638848 logging_writer.py:48] [154215] accumulated_eval_time=9043.079141, accumulated_logging_time=7.562806, accumulated_submission_time=72306.719413, global_step=154215, preemption_count=0, score=72306.719413, test/accuracy=0.609600, test/loss=1.717441, test/num_examples=10000, total_duration=81366.144420, train/accuracy=0.813672, train/loss=0.734371, validation/accuracy=0.732340, validation/loss=1.090815, validation/num_examples=50000
I0208 13:23:59.874656 139946397853440 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.734358787536621, loss=4.152400493621826
I0208 13:24:47.129148 139946414638848 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.635132312774658, loss=1.6117926836013794
I0208 13:25:34.507636 139946397853440 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.797692775726318, loss=1.5718752145767212
I0208 13:26:21.843477 139946414638848 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.5508978366851807, loss=1.7311545610427856
I0208 13:27:09.207612 139946397853440 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.494426965713501, loss=1.545573353767395
I0208 13:27:56.446867 139946414638848 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.950941562652588, loss=1.7238810062408447
I0208 13:28:44.040836 139946397853440 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.4715774059295654, loss=4.060495853424072
I0208 13:29:31.511676 139946414638848 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.3349668979644775, loss=1.8655827045440674
I0208 13:30:18.635175 139946397853440 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.311116933822632, loss=1.574690580368042
I0208 13:30:23.579418 140107197974336 spec.py:321] Evaluating on the training split.
I0208 13:30:34.691534 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 13:31:16.128111 140107197974336 spec.py:349] Evaluating on the test split.
I0208 13:31:17.725561 140107197974336 submission_runner.py:408] Time since start: 81840.67s, 	Step: 155112, 	{'train/accuracy': 0.8171679377555847, 'train/loss': 0.7142869234085083, 'validation/accuracy': 0.7331199645996094, 'validation/loss': 1.077955961227417, 'validation/num_examples': 50000, 'test/accuracy': 0.6073000431060791, 'test/loss': 1.7077174186706543, 'test/num_examples': 10000, 'score': 72726.99397587776, 'total_duration': 81840.67090892792, 'accumulated_submission_time': 72726.99397587776, 'accumulated_eval_time': 9097.225280284882, 'accumulated_logging_time': 7.617284536361694}
I0208 13:31:17.768265 139946414638848 logging_writer.py:48] [155112] accumulated_eval_time=9097.225280, accumulated_logging_time=7.617285, accumulated_submission_time=72726.993976, global_step=155112, preemption_count=0, score=72726.993976, test/accuracy=0.607300, test/loss=1.707717, test/num_examples=10000, total_duration=81840.670909, train/accuracy=0.817168, train/loss=0.714287, validation/accuracy=0.733120, validation/loss=1.077956, validation/num_examples=50000
I0208 13:31:56.174473 139946397853440 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.2745625972747803, loss=3.1913912296295166
I0208 13:32:43.430602 139946414638848 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.4225144386291504, loss=1.6772862672805786
I0208 13:33:31.040596 139946397853440 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.519998788833618, loss=1.6479324102401733
I0208 13:34:18.573302 139946414638848 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.502629518508911, loss=1.6061601638793945
I0208 13:35:06.214745 139946397853440 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.75152325630188, loss=1.6402149200439453
I0208 13:35:53.924412 139946414638848 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.4398744106292725, loss=1.6709010601043701
I0208 13:36:41.494280 139946397853440 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.275285482406616, loss=1.479068636894226
I0208 13:37:29.184286 139946414638848 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.415475368499756, loss=2.561025857925415
I0208 13:38:16.825584 139946397853440 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.0130183696746826, loss=3.4818954467773438
I0208 13:38:17.900435 140107197974336 spec.py:321] Evaluating on the training split.
I0208 13:38:29.312711 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 13:39:10.385368 140107197974336 spec.py:349] Evaluating on the test split.
I0208 13:39:11.980345 140107197974336 submission_runner.py:408] Time since start: 82314.93s, 	Step: 156004, 	{'train/accuracy': 0.8233398199081421, 'train/loss': 0.6954863667488098, 'validation/accuracy': 0.735040009021759, 'validation/loss': 1.0758261680603027, 'validation/num_examples': 50000, 'test/accuracy': 0.612000048160553, 'test/loss': 1.6971514225006104, 'test/num_examples': 10000, 'score': 73147.06365466118, 'total_duration': 82314.92569756508, 'accumulated_submission_time': 73147.06365466118, 'accumulated_eval_time': 9151.305181026459, 'accumulated_logging_time': 7.670612573623657}
I0208 13:39:12.020340 139946414638848 logging_writer.py:48] [156004] accumulated_eval_time=9151.305181, accumulated_logging_time=7.670613, accumulated_submission_time=73147.063655, global_step=156004, preemption_count=0, score=73147.063655, test/accuracy=0.612000, test/loss=1.697151, test/num_examples=10000, total_duration=82314.925698, train/accuracy=0.823340, train/loss=0.695486, validation/accuracy=0.735040, validation/loss=1.075826, validation/num_examples=50000
I0208 13:39:53.956671 139946397853440 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.540585994720459, loss=2.0329627990722656
I0208 13:40:41.372139 139946414638848 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.4541091918945312, loss=2.4400973320007324
I0208 13:41:29.001907 139946397853440 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.6120738983154297, loss=1.5658539533615112
I0208 13:42:16.458549 139946414638848 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.550711154937744, loss=1.634209394454956
I0208 13:43:04.176838 139946397853440 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.2940914630889893, loss=1.7209155559539795
I0208 13:43:51.812385 139946414638848 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.44126033782959, loss=1.5436300039291382
I0208 13:44:39.455754 139946397853440 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.7186059951782227, loss=4.1003875732421875
I0208 13:45:26.960301 139946414638848 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.375821113586426, loss=3.0654120445251465
I0208 13:46:12.105129 140107197974336 spec.py:321] Evaluating on the training split.
I0208 13:46:23.132768 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 13:47:01.709694 140107197974336 spec.py:349] Evaluating on the test split.
I0208 13:47:03.319221 140107197974336 submission_runner.py:408] Time since start: 82786.26s, 	Step: 156897, 	{'train/accuracy': 0.8152148127555847, 'train/loss': 0.7171177268028259, 'validation/accuracy': 0.7366399765014648, 'validation/loss': 1.0684080123901367, 'validation/num_examples': 50000, 'test/accuracy': 0.6095000505447388, 'test/loss': 1.7041574716567993, 'test/num_examples': 10000, 'score': 73567.08738541603, 'total_duration': 82786.26455402374, 'accumulated_submission_time': 73567.08738541603, 'accumulated_eval_time': 9202.519262313843, 'accumulated_logging_time': 7.720755577087402}
I0208 13:47:03.365745 139946397853440 logging_writer.py:48] [156897] accumulated_eval_time=9202.519262, accumulated_logging_time=7.720756, accumulated_submission_time=73567.087385, global_step=156897, preemption_count=0, score=73567.087385, test/accuracy=0.609500, test/loss=1.704157, test/num_examples=10000, total_duration=82786.264554, train/accuracy=0.815215, train/loss=0.717118, validation/accuracy=0.736640, validation/loss=1.068408, validation/num_examples=50000
I0208 13:47:05.010831 139946414638848 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.6822001934051514, loss=1.4651036262512207
I0208 13:47:48.875585 139946397853440 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.6335551738739014, loss=1.6141555309295654
I0208 13:48:36.294438 139946414638848 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.2239773273468018, loss=2.4608395099639893
I0208 13:49:24.130972 139946397853440 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.4155290126800537, loss=2.796543598175049
I0208 13:50:11.618851 139946414638848 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.41507625579834, loss=1.5941646099090576
I0208 13:50:59.125552 139946397853440 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.5330376625061035, loss=1.5169979333877563
I0208 13:51:46.832572 139946414638848 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.45804500579834, loss=1.4722487926483154
I0208 13:52:34.479369 139946397853440 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.183791160583496, loss=2.0827386379241943
I0208 13:53:22.348366 139946414638848 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.7347702980041504, loss=1.5036485195159912
I0208 13:54:03.505102 140107197974336 spec.py:321] Evaluating on the training split.
I0208 13:54:14.681139 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 13:54:55.954634 140107197974336 spec.py:349] Evaluating on the test split.
I0208 13:54:57.561571 140107197974336 submission_runner.py:408] Time since start: 83260.51s, 	Step: 157788, 	{'train/accuracy': 0.8219921588897705, 'train/loss': 0.6984254121780396, 'validation/accuracy': 0.7360799908638, 'validation/loss': 1.0686091184616089, 'validation/num_examples': 50000, 'test/accuracy': 0.6110000014305115, 'test/loss': 1.6924991607666016, 'test/num_examples': 10000, 'score': 73987.16595101357, 'total_duration': 83260.5069026947, 'accumulated_submission_time': 73987.16595101357, 'accumulated_eval_time': 9256.575710058212, 'accumulated_logging_time': 7.776738405227661}
I0208 13:54:57.614248 139946397853440 logging_writer.py:48] [157788] accumulated_eval_time=9256.575710, accumulated_logging_time=7.776738, accumulated_submission_time=73987.165951, global_step=157788, preemption_count=0, score=73987.165951, test/accuracy=0.611000, test/loss=1.692499, test/num_examples=10000, total_duration=83260.506903, train/accuracy=0.821992, train/loss=0.698425, validation/accuracy=0.736080, validation/loss=1.068609, validation/num_examples=50000
I0208 13:55:02.954641 139946414638848 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.6018590927124023, loss=1.6355023384094238
I0208 13:55:47.158498 139946397853440 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.6676032543182373, loss=4.06117057800293
I0208 13:56:34.233525 139946414638848 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.5972259044647217, loss=2.529728651046753
I0208 13:57:21.819138 139946397853440 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.3587465286254883, loss=1.8042668104171753
I0208 13:58:09.491495 139946414638848 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.8343989849090576, loss=2.9914565086364746
I0208 13:58:56.734586 139946397853440 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.4931845664978027, loss=2.8608264923095703
I0208 13:59:44.357563 139946414638848 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.306486129760742, loss=1.4476174116134644
I0208 14:00:31.779072 139946397853440 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.656669855117798, loss=4.062558174133301
I0208 14:01:19.294687 139946414638848 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.498892307281494, loss=1.5689060688018799
I0208 14:01:57.831135 140107197974336 spec.py:321] Evaluating on the training split.
I0208 14:02:09.135863 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 14:02:48.601988 140107197974336 spec.py:349] Evaluating on the test split.
I0208 14:02:50.200605 140107197974336 submission_runner.py:408] Time since start: 83733.15s, 	Step: 158683, 	{'train/accuracy': 0.8245312571525574, 'train/loss': 0.7121429443359375, 'validation/accuracy': 0.7342599630355835, 'validation/loss': 1.108445644378662, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.7294844388961792, 'test/num_examples': 10000, 'score': 74407.3215315342, 'total_duration': 83733.14594745636, 'accumulated_submission_time': 74407.3215315342, 'accumulated_eval_time': 9308.945188522339, 'accumulated_logging_time': 7.840383529663086}
I0208 14:02:50.244800 139946397853440 logging_writer.py:48] [158683] accumulated_eval_time=9308.945189, accumulated_logging_time=7.840384, accumulated_submission_time=74407.321532, global_step=158683, preemption_count=0, score=74407.321532, test/accuracy=0.609700, test/loss=1.729484, test/num_examples=10000, total_duration=83733.145947, train/accuracy=0.824531, train/loss=0.712143, validation/accuracy=0.734260, validation/loss=1.108446, validation/num_examples=50000
I0208 14:02:57.635096 139946414638848 logging_writer.py:48] [158700] global_step=158700, grad_norm=3.347503423690796, loss=1.550123691558838
I0208 14:03:41.878781 139946397853440 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.43906569480896, loss=1.4908952713012695
I0208 14:04:29.405458 139946414638848 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.438913345336914, loss=2.57737398147583
I0208 14:05:17.001732 139946397853440 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.593541145324707, loss=1.6049655675888062
I0208 14:06:04.330185 139946414638848 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.532315969467163, loss=1.5138074159622192
I0208 14:06:51.426898 139946397853440 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.6309144496917725, loss=2.2380001544952393
I0208 14:07:38.895351 139946414638848 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.535292863845825, loss=2.257423162460327
I0208 14:08:26.262804 139946397853440 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.6464169025421143, loss=1.6166683435440063
I0208 14:09:13.519218 139946414638848 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.5359513759613037, loss=1.452616572380066
I0208 14:09:50.406806 140107197974336 spec.py:321] Evaluating on the training split.
I0208 14:10:01.760109 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 14:10:42.234142 140107197974336 spec.py:349] Evaluating on the test split.
I0208 14:10:43.830583 140107197974336 submission_runner.py:408] Time since start: 84206.78s, 	Step: 159580, 	{'train/accuracy': 0.8193163871765137, 'train/loss': 0.7136068344116211, 'validation/accuracy': 0.7355799674987793, 'validation/loss': 1.0825302600860596, 'validation/num_examples': 50000, 'test/accuracy': 0.6133000254631042, 'test/loss': 1.709965705871582, 'test/num_examples': 10000, 'score': 74827.42336130142, 'total_duration': 84206.77592754364, 'accumulated_submission_time': 74827.42336130142, 'accumulated_eval_time': 9362.36895108223, 'accumulated_logging_time': 7.894147157669067}
I0208 14:10:43.876306 139946397853440 logging_writer.py:48] [159580] accumulated_eval_time=9362.368951, accumulated_logging_time=7.894147, accumulated_submission_time=74827.423361, global_step=159580, preemption_count=0, score=74827.423361, test/accuracy=0.613300, test/loss=1.709966, test/num_examples=10000, total_duration=84206.775928, train/accuracy=0.819316, train/loss=0.713607, validation/accuracy=0.735580, validation/loss=1.082530, validation/num_examples=50000
I0208 14:10:52.648415 139946414638848 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.4053101539611816, loss=1.5627055168151855
I0208 14:11:37.191537 139946397853440 logging_writer.py:48] [159700] global_step=159700, grad_norm=3.3366129398345947, loss=1.8088138103485107
I0208 14:12:24.468731 139946414638848 logging_writer.py:48] [159800] global_step=159800, grad_norm=3.0236260890960693, loss=4.065405368804932
I0208 14:13:11.877573 139946397853440 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.818209648132324, loss=1.5306832790374756
I0208 14:13:59.359474 139946414638848 logging_writer.py:48] [160000] global_step=160000, grad_norm=3.0125718116760254, loss=3.2148890495300293
I0208 14:14:46.916900 139946397853440 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.6061441898345947, loss=3.8908674716949463
I0208 14:15:34.602558 139946414638848 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.4639430046081543, loss=1.5651291608810425
I0208 14:16:22.084181 139946397853440 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.731194019317627, loss=1.5944015979766846
I0208 14:17:09.768358 139946414638848 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.5588929653167725, loss=2.142712116241455
I0208 14:17:43.966827 140107197974336 spec.py:321] Evaluating on the training split.
I0208 14:17:55.089798 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 14:18:36.182276 140107197974336 spec.py:349] Evaluating on the test split.
I0208 14:18:37.777070 140107197974336 submission_runner.py:408] Time since start: 84680.72s, 	Step: 160474, 	{'train/accuracy': 0.8253905773162842, 'train/loss': 0.6981134414672852, 'validation/accuracy': 0.7397399544715881, 'validation/loss': 1.0691667795181274, 'validation/num_examples': 50000, 'test/accuracy': 0.616100013256073, 'test/loss': 1.6938591003417969, 'test/num_examples': 10000, 'score': 75247.45334744453, 'total_duration': 84680.72242164612, 'accumulated_submission_time': 75247.45334744453, 'accumulated_eval_time': 9416.179197311401, 'accumulated_logging_time': 7.9501307010650635}
I0208 14:18:37.821393 139946397853440 logging_writer.py:48] [160474] accumulated_eval_time=9416.179197, accumulated_logging_time=7.950131, accumulated_submission_time=75247.453347, global_step=160474, preemption_count=0, score=75247.453347, test/accuracy=0.616100, test/loss=1.693859, test/num_examples=10000, total_duration=84680.722422, train/accuracy=0.825391, train/loss=0.698113, validation/accuracy=0.739740, validation/loss=1.069167, validation/num_examples=50000
I0208 14:18:48.905319 139946414638848 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.3478143215179443, loss=1.3679652214050293
I0208 14:19:34.249320 139946397853440 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.4530258178710938, loss=1.5316624641418457
I0208 14:20:21.448756 139946414638848 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.6061489582061768, loss=1.5203288793563843
I0208 14:21:08.793118 139946397853440 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.4478871822357178, loss=2.180121660232544
I0208 14:21:56.326426 139946414638848 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.4073843955993652, loss=2.1857783794403076
I0208 14:22:43.808750 139946397853440 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.853327751159668, loss=1.9059867858886719
I0208 14:23:31.561946 139946414638848 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.931777000427246, loss=1.6761301755905151
I0208 14:24:18.936201 139946397853440 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.656473159790039, loss=1.5758169889450073
I0208 14:25:06.767420 139946414638848 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.321620464324951, loss=1.5802556276321411
I0208 14:25:37.979579 140107197974336 spec.py:321] Evaluating on the training split.
I0208 14:25:49.127903 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 14:26:28.006066 140107197974336 spec.py:349] Evaluating on the test split.
I0208 14:26:29.609903 140107197974336 submission_runner.py:408] Time since start: 85152.56s, 	Step: 161367, 	{'train/accuracy': 0.8282226324081421, 'train/loss': 0.6513522267341614, 'validation/accuracy': 0.7389799952507019, 'validation/loss': 1.0421522855758667, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.6790910959243774, 'test/num_examples': 10000, 'score': 75667.55098891258, 'total_duration': 85152.55523252487, 'accumulated_submission_time': 75667.55098891258, 'accumulated_eval_time': 9467.809502601624, 'accumulated_logging_time': 8.004290342330933}
I0208 14:26:29.661650 139946397853440 logging_writer.py:48] [161367] accumulated_eval_time=9467.809503, accumulated_logging_time=8.004290, accumulated_submission_time=75667.550989, global_step=161367, preemption_count=0, score=75667.550989, test/accuracy=0.617300, test/loss=1.679091, test/num_examples=10000, total_duration=85152.555233, train/accuracy=0.828223, train/loss=0.651352, validation/accuracy=0.738980, validation/loss=1.042152, validation/num_examples=50000
I0208 14:26:43.628689 139946414638848 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.884805917739868, loss=3.837165117263794
I0208 14:27:29.458306 139946397853440 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.570568323135376, loss=2.5018367767333984
I0208 14:28:16.749470 139946414638848 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.6755332946777344, loss=2.3108603954315186
I0208 14:29:04.626767 139946397853440 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.6978759765625, loss=1.4603163003921509
I0208 14:29:51.999605 139946414638848 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.024075508117676, loss=3.983309745788574
I0208 14:30:39.559068 139946397853440 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.422884702682495, loss=2.6728363037109375
I0208 14:31:27.338495 139946414638848 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.547617197036743, loss=1.908766508102417
I0208 14:32:14.574649 139946397853440 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.5611252784729004, loss=1.4777319431304932
I0208 14:33:01.876293 139946414638848 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.7699832916259766, loss=2.5247583389282227
I0208 14:33:29.703444 140107197974336 spec.py:321] Evaluating on the training split.
I0208 14:33:40.893197 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 14:34:19.456157 140107197974336 spec.py:349] Evaluating on the test split.
I0208 14:34:21.057673 140107197974336 submission_runner.py:408] Time since start: 85624.00s, 	Step: 162260, 	{'train/accuracy': 0.8219531178474426, 'train/loss': 0.7046661376953125, 'validation/accuracy': 0.739039957523346, 'validation/loss': 1.0686395168304443, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.6963045597076416, 'test/num_examples': 10000, 'score': 76087.53114652634, 'total_duration': 85624.00301170349, 'accumulated_submission_time': 76087.53114652634, 'accumulated_eval_time': 9519.163709878922, 'accumulated_logging_time': 8.066744565963745}
I0208 14:34:21.107225 139946397853440 logging_writer.py:48] [162260] accumulated_eval_time=9519.163710, accumulated_logging_time=8.066745, accumulated_submission_time=76087.531147, global_step=162260, preemption_count=0, score=76087.531147, test/accuracy=0.616400, test/loss=1.696305, test/num_examples=10000, total_duration=85624.003012, train/accuracy=0.821953, train/loss=0.704666, validation/accuracy=0.739040, validation/loss=1.068640, validation/num_examples=50000
I0208 14:34:37.933373 139946414638848 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.5316598415374756, loss=1.5665929317474365
I0208 14:35:24.441111 139946397853440 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.378551483154297, loss=2.6880040168762207
I0208 14:36:12.058555 139946414638848 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.579465389251709, loss=1.5945103168487549
I0208 14:36:59.712204 139946397853440 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.531419277191162, loss=1.2565832138061523
I0208 14:37:47.636714 139946414638848 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.46718430519104, loss=3.3545639514923096
I0208 14:38:35.258158 139946397853440 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.5860226154327393, loss=2.107229232788086
I0208 14:39:23.008289 139946414638848 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.7557406425476074, loss=1.566990852355957
I0208 14:40:10.791254 139946397853440 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.4432551860809326, loss=1.8893176317214966
I0208 14:40:58.641487 139946414638848 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.8817355632781982, loss=1.5437325239181519
I0208 14:41:21.299276 140107197974336 spec.py:321] Evaluating on the training split.
I0208 14:41:32.641077 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 14:42:14.783423 140107197974336 spec.py:349] Evaluating on the test split.
I0208 14:42:16.380317 140107197974336 submission_runner.py:408] Time since start: 86099.33s, 	Step: 163149, 	{'train/accuracy': 0.8304882645606995, 'train/loss': 0.668041467666626, 'validation/accuracy': 0.7397800087928772, 'validation/loss': 1.0610032081604004, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.6925363540649414, 'test/num_examples': 10000, 'score': 76507.66302323341, 'total_duration': 86099.32566308975, 'accumulated_submission_time': 76507.66302323341, 'accumulated_eval_time': 9574.24474644661, 'accumulated_logging_time': 8.126704692840576}
I0208 14:42:16.423243 139946397853440 logging_writer.py:48] [163149] accumulated_eval_time=9574.244746, accumulated_logging_time=8.126705, accumulated_submission_time=76507.663023, global_step=163149, preemption_count=0, score=76507.663023, test/accuracy=0.615300, test/loss=1.692536, test/num_examples=10000, total_duration=86099.325663, train/accuracy=0.830488, train/loss=0.668041, validation/accuracy=0.739780, validation/loss=1.061003, validation/num_examples=50000
I0208 14:42:37.756667 139946414638848 logging_writer.py:48] [163200] global_step=163200, grad_norm=3.27406644821167, loss=1.4628196954727173
I0208 14:43:24.370492 139946397853440 logging_writer.py:48] [163300] global_step=163300, grad_norm=3.140936851501465, loss=1.5154591798782349
I0208 14:44:12.116082 139946414638848 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.76263689994812, loss=1.535668134689331
I0208 14:44:59.292382 139946397853440 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.8757779598236084, loss=1.601611852645874
I0208 14:45:46.880637 139946414638848 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.771146059036255, loss=1.8909523487091064
I0208 14:46:34.642963 139946397853440 logging_writer.py:48] [163700] global_step=163700, grad_norm=3.0561060905456543, loss=1.60066556930542
I0208 14:47:22.395883 139946414638848 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.7485334873199463, loss=3.261584997177124
I0208 14:48:10.310866 139946397853440 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.8635671138763428, loss=1.6482081413269043
I0208 14:48:58.075948 139946414638848 logging_writer.py:48] [164000] global_step=164000, grad_norm=3.353545904159546, loss=1.5628072023391724
I0208 14:49:16.702653 140107197974336 spec.py:321] Evaluating on the training split.
I0208 14:49:27.902240 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 14:50:06.861250 140107197974336 spec.py:349] Evaluating on the test split.
I0208 14:50:08.457292 140107197974336 submission_runner.py:408] Time since start: 86571.40s, 	Step: 164040, 	{'train/accuracy': 0.8357031345367432, 'train/loss': 0.6520687937736511, 'validation/accuracy': 0.7403199672698975, 'validation/loss': 1.0528931617736816, 'validation/num_examples': 50000, 'test/accuracy': 0.6208000183105469, 'test/loss': 1.6820333003997803, 'test/num_examples': 10000, 'score': 76927.88128042221, 'total_duration': 86571.4026389122, 'accumulated_submission_time': 76927.88128042221, 'accumulated_eval_time': 9625.999377965927, 'accumulated_logging_time': 8.179861307144165}
I0208 14:50:08.507627 139946397853440 logging_writer.py:48] [164040] accumulated_eval_time=9625.999378, accumulated_logging_time=8.179861, accumulated_submission_time=76927.881280, global_step=164040, preemption_count=0, score=76927.881280, test/accuracy=0.620800, test/loss=1.682033, test/num_examples=10000, total_duration=86571.402639, train/accuracy=0.835703, train/loss=0.652069, validation/accuracy=0.740320, validation/loss=1.052893, validation/num_examples=50000
I0208 14:50:33.558177 139946414638848 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.919985771179199, loss=1.5688834190368652
I0208 14:51:20.738652 139946397853440 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.634031295776367, loss=1.462080717086792
I0208 14:52:08.196846 139946414638848 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.704160451889038, loss=1.50507652759552
I0208 14:52:55.270907 139946397853440 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.8826615810394287, loss=1.5338854789733887
I0208 14:53:42.918920 139946414638848 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.8727710247039795, loss=1.487276554107666
I0208 14:54:30.171917 139946397853440 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.6479620933532715, loss=1.485041856765747
I0208 14:55:17.672187 139946414638848 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.5595595836639404, loss=1.7212488651275635
I0208 14:56:05.103499 139946397853440 logging_writer.py:48] [164800] global_step=164800, grad_norm=3.1058590412139893, loss=1.5344688892364502
I0208 14:56:52.176693 139946414638848 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.079501628875732, loss=2.8477349281311035
I0208 14:57:08.817438 140107197974336 spec.py:321] Evaluating on the training split.
I0208 14:57:20.069943 140107197974336 spec.py:333] Evaluating on the validation split.
I0208 14:57:59.988002 140107197974336 spec.py:349] Evaluating on the test split.
I0208 14:58:01.593865 140107197974336 submission_runner.py:408] Time since start: 87044.54s, 	Step: 164936, 	{'train/accuracy': 0.8275195360183716, 'train/loss': 0.6876667737960815, 'validation/accuracy': 0.742859959602356, 'validation/loss': 1.0580904483795166, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.687680959701538, 'test/num_examples': 10000, 'score': 77348.1300997734, 'total_duration': 87044.53920483589, 'accumulated_submission_time': 77348.1300997734, 'accumulated_eval_time': 9678.775826692581, 'accumulated_logging_time': 8.240126609802246}
I0208 14:58:01.657729 139946397853440 logging_writer.py:48] [164936] accumulated_eval_time=9678.775827, accumulated_logging_time=8.240127, accumulated_submission_time=77348.130100, global_step=164936, preemption_count=0, score=77348.130100, test/accuracy=0.617700, test/loss=1.687681, test/num_examples=10000, total_duration=87044.539205, train/accuracy=0.827520, train/loss=0.687667, validation/accuracy=0.742860, validation/loss=1.058090, validation/num_examples=50000
I0208 14:58:28.490811 139946414638848 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.7341389656066895, loss=1.4487801790237427
I0208 14:59:15.610290 139946397853440 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.788184642791748, loss=1.583504557609558
I0208 15:00:03.235740 139946414638848 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.67931866645813, loss=2.6797451972961426
I0208 15:00:50.543862 139946397853440 logging_writer.py:48] [165300] global_step=165300, grad_norm=3.1160948276519775, loss=3.7343950271606445
I0208 15:00:54.000521 139946414638848 logging_writer.py:48] [165309] global_step=165309, preemption_count=0, score=77520.397876
I0208 15:00:54.622883 140107197974336 checkpoints.py:490] Saving checkpoint at step: 165309
I0208 15:00:55.905468 140107197974336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_5/checkpoint_165309
I0208 15:00:55.956586 140107197974336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/imagenet_vit_jax/trial_5/checkpoint_165309.
I0208 15:00:56.821113 140107197974336 submission_runner.py:583] Tuning trial 5/5
I0208 15:00:56.821346 140107197974336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0208 15:00:56.831722 140107197974336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 41.93090534210205, 'total_duration': 73.41631269454956, 'accumulated_submission_time': 41.93090534210205, 'accumulated_eval_time': 31.48529624938965, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (839, {'train/accuracy': 0.03216796740889549, 'train/loss': 5.976029396057129, 'validation/accuracy': 0.02685999870300293, 'validation/loss': 6.033750534057617, 'validation/num_examples': 50000, 'test/accuracy': 0.02200000174343586, 'test/loss': 6.133785247802734, 'test/num_examples': 10000, 'score': 461.9541461467743, 'total_duration': 545.1891384124756, 'accumulated_submission_time': 461.9541461467743, 'accumulated_eval_time': 83.17005729675293, 'accumulated_logging_time': 0.01717853546142578, 'global_step': 839, 'preemption_count': 0}), (1741, {'train/accuracy': 0.06861328333616257, 'train/loss': 5.289651393890381, 'validation/accuracy': 0.06421999633312225, 'validation/loss': 5.34092903137207, 'validation/num_examples': 50000, 'test/accuracy': 0.04990000277757645, 'test/loss': 5.561094284057617, 'test/num_examples': 10000, 'score': 881.9554603099823, 'total_duration': 1014.4922845363617, 'accumulated_submission_time': 881.9554603099823, 'accumulated_eval_time': 132.39385414123535, 'accumulated_logging_time': 0.043645620346069336, 'global_step': 1741, 'preemption_count': 0}), (2649, {'train/accuracy': 0.11441405862569809, 'train/loss': 4.848740100860596, 'validation/accuracy': 0.10763999819755554, 'validation/loss': 4.910977840423584, 'validation/num_examples': 50000, 'test/accuracy': 0.08380000293254852, 'test/loss': 5.196085453033447, 'test/num_examples': 10000, 'score': 1302.169753074646, 'total_duration': 1485.8664710521698, 'accumulated_submission_time': 1302.169753074646, 'accumulated_eval_time': 183.47590970993042, 'accumulated_logging_time': 0.06981539726257324, 'global_step': 2649, 'preemption_count': 0}), (3554, {'train/accuracy': 0.17302733659744263, 'train/loss': 4.282963752746582, 'validation/accuracy': 0.15741999447345734, 'validation/loss': 4.420511245727539, 'validation/num_examples': 50000, 'test/accuracy': 0.12000000476837158, 'test/loss': 4.79691743850708, 'test/num_examples': 10000, 'score': 1722.0868241786957, 'total_duration': 1956.3246581554413, 'accumulated_submission_time': 1722.0868241786957, 'accumulated_eval_time': 233.9338686466217, 'accumulated_logging_time': 0.10030865669250488, 'global_step': 3554, 'preemption_count': 0}), (4448, {'train/accuracy': 0.20783202350139618, 'train/loss': 4.076893329620361, 'validation/accuracy': 0.19606000185012817, 'validation/loss': 4.17407751083374, 'validation/num_examples': 50000, 'test/accuracy': 0.14380000531673431, 'test/loss': 4.610869884490967, 'test/num_examples': 10000, 'score': 2141.7024862766266, 'total_duration': 2427.956640481949, 'accumulated_submission_time': 2141.7024862766266, 'accumulated_eval_time': 285.4160861968994, 'accumulated_logging_time': 0.5835461616516113, 'global_step': 4448, 'preemption_count': 0}), (5350, {'train/accuracy': 0.2707226574420929, 'train/loss': 3.5997939109802246, 'validation/accuracy': 0.25001999735832214, 'validation/loss': 3.711319923400879, 'validation/num_examples': 50000, 'test/accuracy': 0.1876000016927719, 'test/loss': 4.200911998748779, 'test/num_examples': 10000, 'score': 2561.6503467559814, 'total_duration': 2899.76961684227, 'accumulated_submission_time': 2561.6503467559814, 'accumulated_eval_time': 337.2035584449768, 'accumulated_logging_time': 0.6109819412231445, 'global_step': 5350, 'preemption_count': 0}), (6257, {'train/accuracy': 0.31746092438697815, 'train/loss': 3.2546160221099854, 'validation/accuracy': 0.2885800004005432, 'validation/loss': 3.4442403316497803, 'validation/num_examples': 50000, 'test/accuracy': 0.2152000069618225, 'test/loss': 3.982329845428467, 'test/num_examples': 10000, 'score': 2981.769757270813, 'total_duration': 3371.370764017105, 'accumulated_submission_time': 2981.769757270813, 'accumulated_eval_time': 388.6069040298462, 'accumulated_logging_time': 0.6374530792236328, 'global_step': 6257, 'preemption_count': 0}), (7165, {'train/accuracy': 0.33455076813697815, 'train/loss': 3.108985424041748, 'validation/accuracy': 0.3141399919986725, 'validation/loss': 3.2446765899658203, 'validation/num_examples': 50000, 'test/accuracy': 0.24170000851154327, 'test/loss': 3.805813789367676, 'test/num_examples': 10000, 'score': 3401.7290391921997, 'total_duration': 3843.9964752197266, 'accumulated_submission_time': 3401.7290391921997, 'accumulated_eval_time': 441.1956124305725, 'accumulated_logging_time': 0.6641831398010254, 'global_step': 7165, 'preemption_count': 0}), (8073, {'train/accuracy': 0.3506445288658142, 'train/loss': 3.0698530673980713, 'validation/accuracy': 0.3208400011062622, 'validation/loss': 3.2362983226776123, 'validation/num_examples': 50000, 'test/accuracy': 0.24710001051425934, 'test/loss': 3.786952495574951, 'test/num_examples': 10000, 'score': 3821.696554660797, 'total_duration': 4315.6374979019165, 'accumulated_submission_time': 3821.696554660797, 'accumulated_eval_time': 492.78946113586426, 'accumulated_logging_time': 0.6920418739318848, 'global_step': 8073, 'preemption_count': 0}), (8981, {'train/accuracy': 0.3797656297683716, 'train/loss': 2.877138614654541, 'validation/accuracy': 0.34685999155044556, 'validation/loss': 3.067584991455078, 'validation/num_examples': 50000, 'test/accuracy': 0.2645000219345093, 'test/loss': 3.641249418258667, 'test/num_examples': 10000, 'score': 4241.8689959049225, 'total_duration': 4787.127847909927, 'accumulated_submission_time': 4241.8689959049225, 'accumulated_eval_time': 544.0257968902588, 'accumulated_logging_time': 0.7220323085784912, 'global_step': 8981, 'preemption_count': 0}), (9886, {'train/accuracy': 0.4021289050579071, 'train/loss': 2.717653751373291, 'validation/accuracy': 0.3776800036430359, 'validation/loss': 2.877584934234619, 'validation/num_examples': 50000, 'test/accuracy': 0.2873000204563141, 'test/loss': 3.478476047515869, 'test/num_examples': 10000, 'score': 4662.2013692855835, 'total_duration': 5262.30820274353, 'accumulated_submission_time': 4662.2013692855835, 'accumulated_eval_time': 598.793160200119, 'accumulated_logging_time': 0.7506968975067139, 'global_step': 9886, 'preemption_count': 0}), (10791, {'train/accuracy': 0.42597654461860657, 'train/loss': 2.5819032192230225, 'validation/accuracy': 0.39361998438835144, 'validation/loss': 2.756279706954956, 'validation/num_examples': 50000, 'test/accuracy': 0.29920002818107605, 'test/loss': 3.3826310634613037, 'test/num_examples': 10000, 'score': 5082.514708995819, 'total_duration': 5734.52169752121, 'accumulated_submission_time': 5082.514708995819, 'accumulated_eval_time': 650.6066122055054, 'accumulated_logging_time': 0.7820405960083008, 'global_step': 10791, 'preemption_count': 0}), (11694, {'train/accuracy': 0.4414648413658142, 'train/loss': 2.500446081161499, 'validation/accuracy': 0.4053199887275696, 'validation/loss': 2.709198474884033, 'validation/num_examples': 50000, 'test/accuracy': 0.31790000200271606, 'test/loss': 3.3103625774383545, 'test/num_examples': 10000, 'score': 5502.804249048233, 'total_duration': 6207.301434278488, 'accumulated_submission_time': 5502.804249048233, 'accumulated_eval_time': 703.0180859565735, 'accumulated_logging_time': 0.8099539279937744, 'global_step': 11694, 'preemption_count': 0}), (12599, {'train/accuracy': 0.4473632574081421, 'train/loss': 2.4482831954956055, 'validation/accuracy': 0.41741999983787537, 'validation/loss': 2.6113107204437256, 'validation/num_examples': 50000, 'test/accuracy': 0.3225000202655792, 'test/loss': 3.2534356117248535, 'test/num_examples': 10000, 'score': 5923.055751800537, 'total_duration': 6679.461454868317, 'accumulated_submission_time': 5923.055751800537, 'accumulated_eval_time': 754.8459165096283, 'accumulated_logging_time': 0.8380134105682373, 'global_step': 12599, 'preemption_count': 0}), (13500, {'train/accuracy': 0.4481445252895355, 'train/loss': 2.4669337272644043, 'validation/accuracy': 0.41985997557640076, 'validation/loss': 2.638878107070923, 'validation/num_examples': 50000, 'test/accuracy': 0.3232000172138214, 'test/loss': 3.2796595096588135, 'test/num_examples': 10000, 'score': 6343.135094165802, 'total_duration': 7152.180879831314, 'accumulated_submission_time': 6343.135094165802, 'accumulated_eval_time': 807.402941942215, 'accumulated_logging_time': 0.8693232536315918, 'global_step': 13500, 'preemption_count': 0}), (14401, {'train/accuracy': 0.46925780177116394, 'train/loss': 2.391101121902466, 'validation/accuracy': 0.42927998304367065, 'validation/loss': 2.592332601547241, 'validation/num_examples': 50000, 'test/accuracy': 0.32600000500679016, 'test/loss': 3.225304126739502, 'test/num_examples': 10000, 'score': 6763.108623743057, 'total_duration': 7626.033350944519, 'accumulated_submission_time': 6763.108623743057, 'accumulated_eval_time': 861.2038099765778, 'accumulated_logging_time': 0.8967950344085693, 'global_step': 14401, 'preemption_count': 0}), (15307, {'train/accuracy': 0.4732421636581421, 'train/loss': 2.3394665718078613, 'validation/accuracy': 0.4375399947166443, 'validation/loss': 2.532576084136963, 'validation/num_examples': 50000, 'test/accuracy': 0.3416000306606293, 'test/loss': 3.1499056816101074, 'test/num_examples': 10000, 'score': 7183.1378700733185, 'total_duration': 8098.40290927887, 'accumulated_submission_time': 7183.1378700733185, 'accumulated_eval_time': 913.4623625278473, 'accumulated_logging_time': 0.926544189453125, 'global_step': 15307, 'preemption_count': 0}), (16210, {'train/accuracy': 0.4846288859844208, 'train/loss': 2.263720750808716, 'validation/accuracy': 0.44745999574661255, 'validation/loss': 2.450514554977417, 'validation/num_examples': 50000, 'test/accuracy': 0.3473000228404999, 'test/loss': 3.0882656574249268, 'test/num_examples': 10000, 'score': 7603.157238006592, 'total_duration': 8571.176964044571, 'accumulated_submission_time': 7603.157238006592, 'accumulated_eval_time': 966.1386594772339, 'accumulated_logging_time': 0.9530913829803467, 'global_step': 16210, 'preemption_count': 0}), (17108, {'train/accuracy': 0.5009179711341858, 'train/loss': 2.1943092346191406, 'validation/accuracy': 0.4619999825954437, 'validation/loss': 2.3972971439361572, 'validation/num_examples': 50000, 'test/accuracy': 0.3574000298976898, 'test/loss': 3.0589606761932373, 'test/num_examples': 10000, 'score': 8023.078520536423, 'total_duration': 9043.016461133957, 'accumulated_submission_time': 8023.078520536423, 'accumulated_eval_time': 1017.9731209278107, 'accumulated_logging_time': 0.9839563369750977, 'global_step': 17108, 'preemption_count': 0}), (18007, {'train/accuracy': 0.5083202719688416, 'train/loss': 2.2235052585601807, 'validation/accuracy': 0.45151999592781067, 'validation/loss': 2.495116710662842, 'validation/num_examples': 50000, 'test/accuracy': 0.35680001974105835, 'test/loss': 3.10998272895813, 'test/num_examples': 10000, 'score': 8443.216924190521, 'total_duration': 9516.7394759655, 'accumulated_submission_time': 8443.216924190521, 'accumulated_eval_time': 1071.4758758544922, 'accumulated_logging_time': 1.0145676136016846, 'global_step': 18007, 'preemption_count': 0}), (18907, {'train/accuracy': 0.5022070407867432, 'train/loss': 2.1813580989837646, 'validation/accuracy': 0.4710799753665924, 'validation/loss': 2.35910964012146, 'validation/num_examples': 50000, 'test/accuracy': 0.36090001463890076, 'test/loss': 3.0243079662323, 'test/num_examples': 10000, 'score': 8863.401502132416, 'total_duration': 9989.141452550888, 'accumulated_submission_time': 8863.401502132416, 'accumulated_eval_time': 1123.6136424541473, 'accumulated_logging_time': 1.043529987335205, 'global_step': 18907, 'preemption_count': 0}), (19804, {'train/accuracy': 0.5236718654632568, 'train/loss': 2.076261043548584, 'validation/accuracy': 0.4801599979400635, 'validation/loss': 2.295961618423462, 'validation/num_examples': 50000, 'test/accuracy': 0.37380000948905945, 'test/loss': 2.9481990337371826, 'test/num_examples': 10000, 'score': 9283.423431873322, 'total_duration': 10464.098256111145, 'accumulated_submission_time': 9283.423431873322, 'accumulated_eval_time': 1178.4694809913635, 'accumulated_logging_time': 1.0712995529174805, 'global_step': 19804, 'preemption_count': 0}), (20704, {'train/accuracy': 0.556835949420929, 'train/loss': 1.9286086559295654, 'validation/accuracy': 0.4835599958896637, 'validation/loss': 2.271160364151001, 'validation/num_examples': 50000, 'test/accuracy': 0.3751000165939331, 'test/loss': 2.9266579151153564, 'test/num_examples': 10000, 'score': 9703.366194963455, 'total_duration': 10936.4870698452, 'accumulated_submission_time': 9703.366194963455, 'accumulated_eval_time': 1230.8315978050232, 'accumulated_logging_time': 1.1028954982757568, 'global_step': 20704, 'preemption_count': 0}), (21605, {'train/accuracy': 0.5281640291213989, 'train/loss': 2.053582191467285, 'validation/accuracy': 0.48861998319625854, 'validation/loss': 2.251695156097412, 'validation/num_examples': 50000, 'test/accuracy': 0.37950003147125244, 'test/loss': 2.898561477661133, 'test/num_examples': 10000, 'score': 10123.38339304924, 'total_duration': 11409.501055002213, 'accumulated_submission_time': 10123.38339304924, 'accumulated_eval_time': 1283.7493817806244, 'accumulated_logging_time': 1.1308624744415283, 'global_step': 21605, 'preemption_count': 0}), (22504, {'train/accuracy': 0.5364062190055847, 'train/loss': 1.9806296825408936, 'validation/accuracy': 0.4958599805831909, 'validation/loss': 2.1945180892944336, 'validation/num_examples': 50000, 'test/accuracy': 0.3874000310897827, 'test/loss': 2.876436948776245, 'test/num_examples': 10000, 'score': 10543.948052406311, 'total_duration': 11883.90704703331, 'accumulated_submission_time': 10543.948052406311, 'accumulated_eval_time': 1337.5087454319, 'accumulated_logging_time': 1.1621778011322021, 'global_step': 22504, 'preemption_count': 0}), (23404, {'train/accuracy': 0.5655078291893005, 'train/loss': 1.908821702003479, 'validation/accuracy': 0.49917998909950256, 'validation/loss': 2.2292399406433105, 'validation/num_examples': 50000, 'test/accuracy': 0.3874000310897827, 'test/loss': 2.8902664184570312, 'test/num_examples': 10000, 'score': 10964.240460157394, 'total_duration': 12358.45002245903, 'accumulated_submission_time': 10964.240460157394, 'accumulated_eval_time': 1391.6791398525238, 'accumulated_logging_time': 1.190190076828003, 'global_step': 23404, 'preemption_count': 0}), (24300, {'train/accuracy': 0.5464257597923279, 'train/loss': 1.9774080514907837, 'validation/accuracy': 0.5089799761772156, 'validation/loss': 2.1693193912506104, 'validation/num_examples': 50000, 'test/accuracy': 0.3955000042915344, 'test/loss': 2.8177545070648193, 'test/num_examples': 10000, 'score': 11384.598715543747, 'total_duration': 12830.399197340012, 'accumulated_submission_time': 11384.598715543747, 'accumulated_eval_time': 1443.1911263465881, 'accumulated_logging_time': 1.218409776687622, 'global_step': 24300, 'preemption_count': 0}), (25200, {'train/accuracy': 0.5544726252555847, 'train/loss': 1.8977075815200806, 'validation/accuracy': 0.5140599608421326, 'validation/loss': 2.105320930480957, 'validation/num_examples': 50000, 'test/accuracy': 0.4011000096797943, 'test/loss': 2.787417411804199, 'test/num_examples': 10000, 'score': 11804.667268276215, 'total_duration': 13303.007089614868, 'accumulated_submission_time': 11804.667268276215, 'accumulated_eval_time': 1495.6441519260406, 'accumulated_logging_time': 1.2541024684906006, 'global_step': 25200, 'preemption_count': 0}), (26100, {'train/accuracy': 0.5863476395606995, 'train/loss': 1.7209587097167969, 'validation/accuracy': 0.522059977054596, 'validation/loss': 2.043886423110962, 'validation/num_examples': 50000, 'test/accuracy': 0.40940001606941223, 'test/loss': 2.7280988693237305, 'test/num_examples': 10000, 'score': 12224.890575885773, 'total_duration': 13778.079232692719, 'accumulated_submission_time': 12224.890575885773, 'accumulated_eval_time': 1550.408257484436, 'accumulated_logging_time': 1.28757643699646, 'global_step': 26100, 'preemption_count': 0}), (27000, {'train/accuracy': 0.5658202767372131, 'train/loss': 1.8472483158111572, 'validation/accuracy': 0.5233199596405029, 'validation/loss': 2.056349754333496, 'validation/num_examples': 50000, 'test/accuracy': 0.4092000126838684, 'test/loss': 2.723639726638794, 'test/num_examples': 10000, 'score': 12645.100444793701, 'total_duration': 14250.550843000412, 'accumulated_submission_time': 12645.100444793701, 'accumulated_eval_time': 1602.5821635723114, 'accumulated_logging_time': 1.3235411643981934, 'global_step': 27000, 'preemption_count': 0}), (27898, {'train/accuracy': 0.557812511920929, 'train/loss': 1.8963301181793213, 'validation/accuracy': 0.5161399841308594, 'validation/loss': 2.110172748565674, 'validation/num_examples': 50000, 'test/accuracy': 0.4037000238895416, 'test/loss': 2.790687322616577, 'test/num_examples': 10000, 'score': 13065.124905824661, 'total_duration': 14721.279171228409, 'accumulated_submission_time': 13065.124905824661, 'accumulated_eval_time': 1653.192789554596, 'accumulated_logging_time': 1.3615074157714844, 'global_step': 27898, 'preemption_count': 0}), (28795, {'train/accuracy': 0.5855664014816284, 'train/loss': 1.7688456773757935, 'validation/accuracy': 0.5272600054740906, 'validation/loss': 2.0590996742248535, 'validation/num_examples': 50000, 'test/accuracy': 0.41620001196861267, 'test/loss': 2.7257065773010254, 'test/num_examples': 10000, 'score': 13485.352035045624, 'total_duration': 15193.418963193893, 'accumulated_submission_time': 13485.352035045624, 'accumulated_eval_time': 1705.023080587387, 'accumulated_logging_time': 1.392759084701538, 'global_step': 28795, 'preemption_count': 0}), (29694, {'train/accuracy': 0.5672265291213989, 'train/loss': 1.8366743326187134, 'validation/accuracy': 0.526699960231781, 'validation/loss': 2.0453226566314697, 'validation/num_examples': 50000, 'test/accuracy': 0.4116000235080719, 'test/loss': 2.7133548259735107, 'test/num_examples': 10000, 'score': 13905.495992660522, 'total_duration': 15666.241523504257, 'accumulated_submission_time': 13905.495992660522, 'accumulated_eval_time': 1757.6186830997467, 'accumulated_logging_time': 1.4242591857910156, 'global_step': 29694, 'preemption_count': 0}), (30595, {'train/accuracy': 0.5782226324081421, 'train/loss': 1.7867298126220703, 'validation/accuracy': 0.5320000052452087, 'validation/loss': 2.0195980072021484, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.6812961101531982, 'test/num_examples': 10000, 'score': 14325.843662500381, 'total_duration': 16138.692564725876, 'accumulated_submission_time': 14325.843662500381, 'accumulated_eval_time': 1809.6398491859436, 'accumulated_logging_time': 1.4553961753845215, 'global_step': 30595, 'preemption_count': 0}), (31494, {'train/accuracy': 0.5876171588897705, 'train/loss': 1.7332189083099365, 'validation/accuracy': 0.5371800065040588, 'validation/loss': 1.9984508752822876, 'validation/num_examples': 50000, 'test/accuracy': 0.4247000217437744, 'test/loss': 2.6817445755004883, 'test/num_examples': 10000, 'score': 14745.781195640564, 'total_duration': 16610.330486536026, 'accumulated_submission_time': 14745.781195640564, 'accumulated_eval_time': 1861.2554004192352, 'accumulated_logging_time': 1.4885175228118896, 'global_step': 31494, 'preemption_count': 0}), (32397, {'train/accuracy': 0.572265625, 'train/loss': 1.8312280178070068, 'validation/accuracy': 0.5342599749565125, 'validation/loss': 2.031036615371704, 'validation/num_examples': 50000, 'test/accuracy': 0.4207000136375427, 'test/loss': 2.6954081058502197, 'test/num_examples': 10000, 'score': 15165.913511514664, 'total_duration': 17081.899649858475, 'accumulated_submission_time': 15165.913511514664, 'accumulated_eval_time': 1912.6074118614197, 'accumulated_logging_time': 1.521423101425171, 'global_step': 32397, 'preemption_count': 0}), (33298, {'train/accuracy': 0.5862890481948853, 'train/loss': 1.739125370979309, 'validation/accuracy': 0.5410199761390686, 'validation/loss': 1.962300419807434, 'validation/num_examples': 50000, 'test/accuracy': 0.4212000072002411, 'test/loss': 2.652564764022827, 'test/num_examples': 10000, 'score': 15585.88121342659, 'total_duration': 17553.76842737198, 'accumulated_submission_time': 15585.88121342659, 'accumulated_eval_time': 1964.4204487800598, 'accumulated_logging_time': 1.557866096496582, 'global_step': 33298, 'preemption_count': 0}), (34204, {'train/accuracy': 0.5970116853713989, 'train/loss': 1.712268590927124, 'validation/accuracy': 0.5445600152015686, 'validation/loss': 1.9671906232833862, 'validation/num_examples': 50000, 'test/accuracy': 0.4263000190258026, 'test/loss': 2.6361184120178223, 'test/num_examples': 10000, 'score': 16006.172748804092, 'total_duration': 18025.4995098114, 'accumulated_submission_time': 16006.172748804092, 'accumulated_eval_time': 2015.7727072238922, 'accumulated_logging_time': 1.5926804542541504, 'global_step': 34204, 'preemption_count': 0}), (35105, {'train/accuracy': 0.5822460651397705, 'train/loss': 1.7583211660385132, 'validation/accuracy': 0.5433599948883057, 'validation/loss': 1.9646700620651245, 'validation/num_examples': 50000, 'test/accuracy': 0.42740002274513245, 'test/loss': 2.626077175140381, 'test/num_examples': 10000, 'score': 16426.362616062164, 'total_duration': 18496.106160640717, 'accumulated_submission_time': 16426.362616062164, 'accumulated_eval_time': 2066.1069979667664, 'accumulated_logging_time': 1.6230809688568115, 'global_step': 35105, 'preemption_count': 0}), (36006, {'train/accuracy': 0.5903906226158142, 'train/loss': 1.75039541721344, 'validation/accuracy': 0.546999990940094, 'validation/loss': 1.9551115036010742, 'validation/num_examples': 50000, 'test/accuracy': 0.43310001492500305, 'test/loss': 2.605111837387085, 'test/num_examples': 10000, 'score': 16846.550374031067, 'total_duration': 18968.680662870407, 'accumulated_submission_time': 16846.550374031067, 'accumulated_eval_time': 2118.408214569092, 'accumulated_logging_time': 1.6574127674102783, 'global_step': 36006, 'preemption_count': 0}), (36905, {'train/accuracy': 0.5983593463897705, 'train/loss': 1.7371487617492676, 'validation/accuracy': 0.548259973526001, 'validation/loss': 1.9736733436584473, 'validation/num_examples': 50000, 'test/accuracy': 0.43470001220703125, 'test/loss': 2.613460063934326, 'test/num_examples': 10000, 'score': 17266.725713968277, 'total_duration': 19440.53175663948, 'accumulated_submission_time': 17266.725713968277, 'accumulated_eval_time': 2169.996456384659, 'accumulated_logging_time': 1.6937315464019775, 'global_step': 36905, 'preemption_count': 0}), (37806, {'train/accuracy': 0.5934765338897705, 'train/loss': 1.7180927991867065, 'validation/accuracy': 0.549560010433197, 'validation/loss': 1.9441777467727661, 'validation/num_examples': 50000, 'test/accuracy': 0.43300002813339233, 'test/loss': 2.5955758094787598, 'test/num_examples': 10000, 'score': 17686.66410136223, 'total_duration': 19911.408517360687, 'accumulated_submission_time': 17686.66410136223, 'accumulated_eval_time': 2220.849625825882, 'accumulated_logging_time': 1.7289478778839111, 'global_step': 37806, 'preemption_count': 0}), (38706, {'train/accuracy': 0.5957421660423279, 'train/loss': 1.7233281135559082, 'validation/accuracy': 0.5529400110244751, 'validation/loss': 1.9345680475234985, 'validation/num_examples': 50000, 'test/accuracy': 0.4406000077724457, 'test/loss': 2.6032638549804688, 'test/num_examples': 10000, 'score': 18106.65107870102, 'total_duration': 20382.752317667007, 'accumulated_submission_time': 18106.65107870102, 'accumulated_eval_time': 2272.119005203247, 'accumulated_logging_time': 1.765486717224121, 'global_step': 38706, 'preemption_count': 0}), (39610, {'train/accuracy': 0.6048241853713989, 'train/loss': 1.6466890573501587, 'validation/accuracy': 0.558139979839325, 'validation/loss': 1.8898195028305054, 'validation/num_examples': 50000, 'test/accuracy': 0.4489000141620636, 'test/loss': 2.53645920753479, 'test/num_examples': 10000, 'score': 18526.90128660202, 'total_duration': 20856.095355033875, 'accumulated_submission_time': 18526.90128660202, 'accumulated_eval_time': 2325.1231849193573, 'accumulated_logging_time': 1.80271577835083, 'global_step': 39610, 'preemption_count': 0}), (40511, {'train/accuracy': 0.6100000143051147, 'train/loss': 1.6908632516860962, 'validation/accuracy': 0.5554800033569336, 'validation/loss': 1.9389760494232178, 'validation/num_examples': 50000, 'test/accuracy': 0.43950003385543823, 'test/loss': 2.585554361343384, 'test/num_examples': 10000, 'score': 18947.140549182892, 'total_duration': 21328.02912712097, 'accumulated_submission_time': 18947.140549182892, 'accumulated_eval_time': 2376.72646856308, 'accumulated_logging_time': 1.8429145812988281, 'global_step': 40511, 'preemption_count': 0}), (41412, {'train/accuracy': 0.6074023246765137, 'train/loss': 1.6232702732086182, 'validation/accuracy': 0.5635200142860413, 'validation/loss': 1.8514550924301147, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.5395474433898926, 'test/num_examples': 10000, 'score': 19367.371527671814, 'total_duration': 21799.19383573532, 'accumulated_submission_time': 19367.371527671814, 'accumulated_eval_time': 2427.576056241989, 'accumulated_logging_time': 1.8747284412384033, 'global_step': 41412, 'preemption_count': 0}), (42313, {'train/accuracy': 0.610644519329071, 'train/loss': 1.6274819374084473, 'validation/accuracy': 0.561519980430603, 'validation/loss': 1.8759976625442505, 'validation/num_examples': 50000, 'test/accuracy': 0.4504000246524811, 'test/loss': 2.53519868850708, 'test/num_examples': 10000, 'score': 19787.460819482803, 'total_duration': 22274.2334086895, 'accumulated_submission_time': 19787.460819482803, 'accumulated_eval_time': 2482.4408757686615, 'accumulated_logging_time': 1.907930850982666, 'global_step': 42313, 'preemption_count': 0}), (43216, {'train/accuracy': 0.6401757597923279, 'train/loss': 1.5094099044799805, 'validation/accuracy': 0.5685200095176697, 'validation/loss': 1.8535585403442383, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.5047597885131836, 'test/num_examples': 10000, 'score': 20207.386869430542, 'total_duration': 22748.151103019714, 'accumulated_submission_time': 20207.386869430542, 'accumulated_eval_time': 2536.3484270572662, 'accumulated_logging_time': 1.941042184829712, 'global_step': 43216, 'preemption_count': 0}), (44115, {'train/accuracy': 0.615527331829071, 'train/loss': 1.6147336959838867, 'validation/accuracy': 0.5694000124931335, 'validation/loss': 1.8382197618484497, 'validation/num_examples': 50000, 'test/accuracy': 0.44780001044273376, 'test/loss': 2.5037691593170166, 'test/num_examples': 10000, 'score': 20627.419096708298, 'total_duration': 23222.031039714813, 'accumulated_submission_time': 20627.419096708298, 'accumulated_eval_time': 2590.112258195877, 'accumulated_logging_time': 1.9733588695526123, 'global_step': 44115, 'preemption_count': 0}), (45016, {'train/accuracy': 0.6134960651397705, 'train/loss': 1.659540057182312, 'validation/accuracy': 0.5658400058746338, 'validation/loss': 1.892349362373352, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.542588472366333, 'test/num_examples': 10000, 'score': 21047.822548627853, 'total_duration': 23695.17087316513, 'accumulated_submission_time': 21047.822548627853, 'accumulated_eval_time': 2642.76513504982, 'accumulated_logging_time': 2.0060572624206543, 'global_step': 45016, 'preemption_count': 0}), (45915, {'train/accuracy': 0.6446874737739563, 'train/loss': 1.4981642961502075, 'validation/accuracy': 0.5705599784851074, 'validation/loss': 1.8389016389846802, 'validation/num_examples': 50000, 'test/accuracy': 0.4538000226020813, 'test/loss': 2.495013952255249, 'test/num_examples': 10000, 'score': 21468.1283724308, 'total_duration': 24166.16872239113, 'accumulated_submission_time': 21468.1283724308, 'accumulated_eval_time': 2693.3661007881165, 'accumulated_logging_time': 2.0465316772460938, 'global_step': 45915, 'preemption_count': 0}), (46809, {'train/accuracy': 0.6206445097923279, 'train/loss': 1.6042301654815674, 'validation/accuracy': 0.5744799971580505, 'validation/loss': 1.8185001611709595, 'validation/num_examples': 50000, 'test/accuracy': 0.45580002665519714, 'test/loss': 2.4935083389282227, 'test/num_examples': 10000, 'score': 21888.07623887062, 'total_duration': 24639.414632320404, 'accumulated_submission_time': 21888.07623887062, 'accumulated_eval_time': 2746.573971748352, 'accumulated_logging_time': 2.0856590270996094, 'global_step': 46809, 'preemption_count': 0}), (47711, {'train/accuracy': 0.6209374666213989, 'train/loss': 1.5577152967453003, 'validation/accuracy': 0.5724999904632568, 'validation/loss': 1.797107458114624, 'validation/num_examples': 50000, 'test/accuracy': 0.4577000141143799, 'test/loss': 2.455000400543213, 'test/num_examples': 10000, 'score': 22308.360613822937, 'total_duration': 25112.828466653824, 'accumulated_submission_time': 22308.360613822937, 'accumulated_eval_time': 2799.619107723236, 'accumulated_logging_time': 2.118630886077881, 'global_step': 47711, 'preemption_count': 0}), (48614, {'train/accuracy': 0.6391406059265137, 'train/loss': 1.4967988729476929, 'validation/accuracy': 0.5768799781799316, 'validation/loss': 1.8095238208770752, 'validation/num_examples': 50000, 'test/accuracy': 0.45670002698898315, 'test/loss': 2.475313901901245, 'test/num_examples': 10000, 'score': 22728.47146344185, 'total_duration': 25586.20291495323, 'accumulated_submission_time': 22728.47146344185, 'accumulated_eval_time': 2852.789181947708, 'accumulated_logging_time': 2.153792381286621, 'global_step': 48614, 'preemption_count': 0}), (49511, {'train/accuracy': 0.6273437142372131, 'train/loss': 1.5240154266357422, 'validation/accuracy': 0.5815399885177612, 'validation/loss': 1.7533224821090698, 'validation/num_examples': 50000, 'test/accuracy': 0.4651000201702118, 'test/loss': 2.4164364337921143, 'test/num_examples': 10000, 'score': 23148.824804782867, 'total_duration': 26059.92187833786, 'accumulated_submission_time': 23148.824804782867, 'accumulated_eval_time': 2906.063053369522, 'accumulated_logging_time': 2.1936678886413574, 'global_step': 49511, 'preemption_count': 0}), (50409, {'train/accuracy': 0.6357030868530273, 'train/loss': 1.4875028133392334, 'validation/accuracy': 0.5874599814414978, 'validation/loss': 1.7306010723114014, 'validation/num_examples': 50000, 'test/accuracy': 0.4692000150680542, 'test/loss': 2.4118294715881348, 'test/num_examples': 10000, 'score': 23569.12908434868, 'total_duration': 26531.848781347275, 'accumulated_submission_time': 23569.12908434868, 'accumulated_eval_time': 2957.601359605789, 'accumulated_logging_time': 2.2275655269622803, 'global_step': 50409, 'preemption_count': 0}), (51307, {'train/accuracy': 0.6474804282188416, 'train/loss': 1.4440979957580566, 'validation/accuracy': 0.5879999995231628, 'validation/loss': 1.7363550662994385, 'validation/num_examples': 50000, 'test/accuracy': 0.46470001339912415, 'test/loss': 2.418069839477539, 'test/num_examples': 10000, 'score': 23989.14796257019, 'total_duration': 27006.25090765953, 'accumulated_submission_time': 23989.14796257019, 'accumulated_eval_time': 3011.8984134197235, 'accumulated_logging_time': 2.2619385719299316, 'global_step': 51307, 'preemption_count': 0}), (52205, {'train/accuracy': 0.6319531202316284, 'train/loss': 1.5517568588256836, 'validation/accuracy': 0.5824599862098694, 'validation/loss': 1.791892170906067, 'validation/num_examples': 50000, 'test/accuracy': 0.46140003204345703, 'test/loss': 2.442796230316162, 'test/num_examples': 10000, 'score': 24409.063949108124, 'total_duration': 27477.90633225441, 'accumulated_submission_time': 24409.063949108124, 'accumulated_eval_time': 3063.552988052368, 'accumulated_logging_time': 2.295424699783325, 'global_step': 52205, 'preemption_count': 0}), (53106, {'train/accuracy': 0.6414257884025574, 'train/loss': 1.4624534845352173, 'validation/accuracy': 0.5923799872398376, 'validation/loss': 1.7016264200210571, 'validation/num_examples': 50000, 'test/accuracy': 0.4686000347137451, 'test/loss': 2.3851122856140137, 'test/num_examples': 10000, 'score': 24829.066326379776, 'total_duration': 27951.723640203476, 'accumulated_submission_time': 24829.066326379776, 'accumulated_eval_time': 3117.2780344486237, 'accumulated_logging_time': 2.3338751792907715, 'global_step': 53106, 'preemption_count': 0}), (54005, {'train/accuracy': 0.6461132764816284, 'train/loss': 1.4805848598480225, 'validation/accuracy': 0.5896399617195129, 'validation/loss': 1.7537370920181274, 'validation/num_examples': 50000, 'test/accuracy': 0.46390002965927124, 'test/loss': 2.4403724670410156, 'test/num_examples': 10000, 'score': 25249.0579559803, 'total_duration': 28423.403182029724, 'accumulated_submission_time': 25249.0579559803, 'accumulated_eval_time': 3168.877803325653, 'accumulated_logging_time': 2.3705573081970215, 'global_step': 54005, 'preemption_count': 0}), (54904, {'train/accuracy': 0.6288476586341858, 'train/loss': 1.5620810985565186, 'validation/accuracy': 0.5803399682044983, 'validation/loss': 1.7848657369613647, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4644737243652344, 'test/num_examples': 10000, 'score': 25669.08655667305, 'total_duration': 28894.5414185524, 'accumulated_submission_time': 25669.08655667305, 'accumulated_eval_time': 3219.899830341339, 'accumulated_logging_time': 2.4072391986846924, 'global_step': 54904, 'preemption_count': 0}), (55805, {'train/accuracy': 0.6421093344688416, 'train/loss': 1.4804438352584839, 'validation/accuracy': 0.5927599668502808, 'validation/loss': 1.717319130897522, 'validation/num_examples': 50000, 'test/accuracy': 0.4727000296115875, 'test/loss': 2.3828577995300293, 'test/num_examples': 10000, 'score': 26089.343856811523, 'total_duration': 29368.971861839294, 'accumulated_submission_time': 26089.343856811523, 'accumulated_eval_time': 3273.9854452610016, 'accumulated_logging_time': 2.4438350200653076, 'global_step': 55805, 'preemption_count': 0}), (56706, {'train/accuracy': 0.6518945097923279, 'train/loss': 1.4154984951019287, 'validation/accuracy': 0.5967999696731567, 'validation/loss': 1.6895005702972412, 'validation/num_examples': 50000, 'test/accuracy': 0.4781000316143036, 'test/loss': 2.3427188396453857, 'test/num_examples': 10000, 'score': 26509.375621795654, 'total_duration': 29841.977155923843, 'accumulated_submission_time': 26509.375621795654, 'accumulated_eval_time': 3326.8713307380676, 'accumulated_logging_time': 2.479809284210205, 'global_step': 56706, 'preemption_count': 0}), (57605, {'train/accuracy': 0.6357030868530273, 'train/loss': 1.5449306964874268, 'validation/accuracy': 0.5915799736976624, 'validation/loss': 1.7564728260040283, 'validation/num_examples': 50000, 'test/accuracy': 0.4772000312805176, 'test/loss': 2.4073524475097656, 'test/num_examples': 10000, 'score': 26929.512435913086, 'total_duration': 30316.034165859222, 'accumulated_submission_time': 26929.512435913086, 'accumulated_eval_time': 3380.7066905498505, 'accumulated_logging_time': 2.51355242729187, 'global_step': 57605, 'preemption_count': 0}), (58505, {'train/accuracy': 0.6467968821525574, 'train/loss': 1.471572995185852, 'validation/accuracy': 0.5975599884986877, 'validation/loss': 1.7017605304718018, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.3641316890716553, 'test/num_examples': 10000, 'score': 27349.630720853806, 'total_duration': 30790.280519247055, 'accumulated_submission_time': 27349.630720853806, 'accumulated_eval_time': 3434.7488169670105, 'accumulated_logging_time': 2.548457622528076, 'global_step': 58505, 'preemption_count': 0}), (59407, {'train/accuracy': 0.6440038681030273, 'train/loss': 1.4910470247268677, 'validation/accuracy': 0.590719997882843, 'validation/loss': 1.7559362649917603, 'validation/num_examples': 50000, 'test/accuracy': 0.4765000343322754, 'test/loss': 2.4175078868865967, 'test/num_examples': 10000, 'score': 27769.91752099991, 'total_duration': 31261.66304731369, 'accumulated_submission_time': 27769.91752099991, 'accumulated_eval_time': 3485.748226881027, 'accumulated_logging_time': 2.594045400619507, 'global_step': 59407, 'preemption_count': 0}), (60301, {'train/accuracy': 0.6430468559265137, 'train/loss': 1.471490502357483, 'validation/accuracy': 0.5990399718284607, 'validation/loss': 1.6971635818481445, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.3615305423736572, 'test/num_examples': 10000, 'score': 28190.01903152466, 'total_duration': 31732.694142580032, 'accumulated_submission_time': 28190.01903152466, 'accumulated_eval_time': 3536.589988708496, 'accumulated_logging_time': 2.6317689418792725, 'global_step': 60301, 'preemption_count': 0}), (61201, {'train/accuracy': 0.6446093320846558, 'train/loss': 1.4742058515548706, 'validation/accuracy': 0.6018999814987183, 'validation/loss': 1.6856762170791626, 'validation/num_examples': 50000, 'test/accuracy': 0.4775000214576721, 'test/loss': 2.361438274383545, 'test/num_examples': 10000, 'score': 28609.990909576416, 'total_duration': 32205.369146585464, 'accumulated_submission_time': 28609.990909576416, 'accumulated_eval_time': 3589.1981959342957, 'accumulated_logging_time': 2.6737277507781982, 'global_step': 61201, 'preemption_count': 0}), (62101, {'train/accuracy': 0.6470312476158142, 'train/loss': 1.4702602624893188, 'validation/accuracy': 0.5948399901390076, 'validation/loss': 1.7143629789352417, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.3711555004119873, 'test/num_examples': 10000, 'score': 29030.119871616364, 'total_duration': 32679.75479578972, 'accumulated_submission_time': 29030.119871616364, 'accumulated_eval_time': 3643.3617749214172, 'accumulated_logging_time': 2.71584153175354, 'global_step': 62101, 'preemption_count': 0}), (63001, {'train/accuracy': 0.6622851490974426, 'train/loss': 1.3750454187393188, 'validation/accuracy': 0.6061999797821045, 'validation/loss': 1.6341488361358643, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.299943447113037, 'test/num_examples': 10000, 'score': 29450.567351818085, 'total_duration': 33153.461698293686, 'accumulated_submission_time': 29450.567351818085, 'accumulated_eval_time': 3696.5355207920074, 'accumulated_logging_time': 2.7506535053253174, 'global_step': 63001, 'preemption_count': 0}), (63903, {'train/accuracy': 0.6542187333106995, 'train/loss': 1.415706753730774, 'validation/accuracy': 0.6043999791145325, 'validation/loss': 1.652857780456543, 'validation/num_examples': 50000, 'test/accuracy': 0.48170003294944763, 'test/loss': 2.3179423809051514, 'test/num_examples': 10000, 'score': 29870.9140355587, 'total_duration': 33626.445133686066, 'accumulated_submission_time': 29870.9140355587, 'accumulated_eval_time': 3749.0804085731506, 'accumulated_logging_time': 2.791459083557129, 'global_step': 63903, 'preemption_count': 0}), (64801, {'train/accuracy': 0.6602929830551147, 'train/loss': 1.405190348625183, 'validation/accuracy': 0.6033799648284912, 'validation/loss': 1.6764905452728271, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.324260711669922, 'test/num_examples': 10000, 'score': 30291.509792089462, 'total_duration': 34099.37319946289, 'accumulated_submission_time': 30291.509792089462, 'accumulated_eval_time': 3801.3269126415253, 'accumulated_logging_time': 2.8265275955200195, 'global_step': 64801, 'preemption_count': 0}), (65703, {'train/accuracy': 0.6716015338897705, 'train/loss': 1.325235366821289, 'validation/accuracy': 0.6054999828338623, 'validation/loss': 1.634854793548584, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.2978153228759766, 'test/num_examples': 10000, 'score': 30711.840952396393, 'total_duration': 34570.10649561882, 'accumulated_submission_time': 30711.840952396393, 'accumulated_eval_time': 3851.639670610428, 'accumulated_logging_time': 2.863790512084961, 'global_step': 65703, 'preemption_count': 0}), (66599, {'train/accuracy': 0.6601366996765137, 'train/loss': 1.4673115015029907, 'validation/accuracy': 0.6058799624443054, 'validation/loss': 1.716723084449768, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.3495824337005615, 'test/num_examples': 10000, 'score': 31131.927243471146, 'total_duration': 35039.92137694359, 'accumulated_submission_time': 31131.927243471146, 'accumulated_eval_time': 3901.2771389484406, 'accumulated_logging_time': 2.9034883975982666, 'global_step': 66599, 'preemption_count': 0}), (67499, {'train/accuracy': 0.6662890315055847, 'train/loss': 1.3695124387741089, 'validation/accuracy': 0.6140199899673462, 'validation/loss': 1.6112968921661377, 'validation/num_examples': 50000, 'test/accuracy': 0.49320003390312195, 'test/loss': 2.2746410369873047, 'test/num_examples': 10000, 'score': 31552.259560346603, 'total_duration': 35511.90776872635, 'accumulated_submission_time': 31552.259560346603, 'accumulated_eval_time': 3952.83056306839, 'accumulated_logging_time': 2.9529545307159424, 'global_step': 67499, 'preemption_count': 0}), (68398, {'train/accuracy': 0.6966406106948853, 'train/loss': 1.2503221035003662, 'validation/accuracy': 0.6096799969673157, 'validation/loss': 1.6365011930465698, 'validation/num_examples': 50000, 'test/accuracy': 0.49330002069473267, 'test/loss': 2.2944016456604004, 'test/num_examples': 10000, 'score': 31972.519397735596, 'total_duration': 35983.74279308319, 'accumulated_submission_time': 31972.519397735596, 'accumulated_eval_time': 4004.3154785633087, 'accumulated_logging_time': 2.992598533630371, 'global_step': 68398, 'preemption_count': 0}), (69292, {'train/accuracy': 0.6567773222923279, 'train/loss': 1.425057291984558, 'validation/accuracy': 0.6114400029182434, 'validation/loss': 1.6573946475982666, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.3122198581695557, 'test/num_examples': 10000, 'score': 32392.77770280838, 'total_duration': 36456.787868499756, 'accumulated_submission_time': 32392.77770280838, 'accumulated_eval_time': 4057.0101647377014, 'accumulated_logging_time': 3.033708333969116, 'global_step': 69292, 'preemption_count': 0}), (70190, {'train/accuracy': 0.6670116782188416, 'train/loss': 1.3967492580413818, 'validation/accuracy': 0.6153599619865417, 'validation/loss': 1.6382001638412476, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.298884153366089, 'test/num_examples': 10000, 'score': 32812.85529613495, 'total_duration': 36928.463305950165, 'accumulated_submission_time': 32812.85529613495, 'accumulated_eval_time': 4108.516996145248, 'accumulated_logging_time': 3.072967052459717, 'global_step': 70190, 'preemption_count': 0}), (71090, {'train/accuracy': 0.6880077719688416, 'train/loss': 1.30825936794281, 'validation/accuracy': 0.6117199659347534, 'validation/loss': 1.6579543352127075, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.2903990745544434, 'test/num_examples': 10000, 'score': 33233.01276636124, 'total_duration': 37401.97440814972, 'accumulated_submission_time': 33233.01276636124, 'accumulated_eval_time': 4161.779107093811, 'accumulated_logging_time': 3.111844539642334, 'global_step': 71090, 'preemption_count': 0}), (71989, {'train/accuracy': 0.6581054329872131, 'train/loss': 1.4184592962265015, 'validation/accuracy': 0.613599956035614, 'validation/loss': 1.642994999885559, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.313216209411621, 'test/num_examples': 10000, 'score': 33653.27028799057, 'total_duration': 37872.505085229874, 'accumulated_submission_time': 33653.27028799057, 'accumulated_eval_time': 4211.959260225296, 'accumulated_logging_time': 3.1535627841949463, 'global_step': 71989, 'preemption_count': 0}), (72888, {'train/accuracy': 0.6729101538658142, 'train/loss': 1.3513695001602173, 'validation/accuracy': 0.6170799732208252, 'validation/loss': 1.6132991313934326, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.280538320541382, 'test/num_examples': 10000, 'score': 34073.381747722626, 'total_duration': 38344.637328863144, 'accumulated_submission_time': 34073.381747722626, 'accumulated_eval_time': 4263.880227088928, 'accumulated_logging_time': 3.2022414207458496, 'global_step': 72888, 'preemption_count': 0}), (73785, {'train/accuracy': 0.6938085556030273, 'train/loss': 1.2425113916397095, 'validation/accuracy': 0.6222400069236755, 'validation/loss': 1.575509786605835, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2482941150665283, 'test/num_examples': 10000, 'score': 34493.59297966957, 'total_duration': 38817.48063826561, 'accumulated_submission_time': 34493.59297966957, 'accumulated_eval_time': 4316.417612314224, 'accumulated_logging_time': 3.2457542419433594, 'global_step': 73785, 'preemption_count': 0}), (74687, {'train/accuracy': 0.6724804639816284, 'train/loss': 1.328711986541748, 'validation/accuracy': 0.6229999661445618, 'validation/loss': 1.5702327489852905, 'validation/num_examples': 50000, 'test/accuracy': 0.503600001335144, 'test/loss': 2.223655939102173, 'test/num_examples': 10000, 'score': 34913.6925303936, 'total_duration': 39289.15676212311, 'accumulated_submission_time': 34913.6925303936, 'accumulated_eval_time': 4367.896469116211, 'accumulated_logging_time': 3.292013645172119, 'global_step': 74687, 'preemption_count': 0}), (75586, {'train/accuracy': 0.6808202862739563, 'train/loss': 1.340014934539795, 'validation/accuracy': 0.6244800090789795, 'validation/loss': 1.6081055402755737, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.2576117515563965, 'test/num_examples': 10000, 'score': 35333.97060585022, 'total_duration': 39761.00652861595, 'accumulated_submission_time': 35333.97060585022, 'accumulated_eval_time': 4419.369015693665, 'accumulated_logging_time': 3.339527130126953, 'global_step': 75586, 'preemption_count': 0}), (76484, {'train/accuracy': 0.6885741949081421, 'train/loss': 1.2858747243881226, 'validation/accuracy': 0.6215599775314331, 'validation/loss': 1.5913811922073364, 'validation/num_examples': 50000, 'test/accuracy': 0.5015000104904175, 'test/loss': 2.2273242473602295, 'test/num_examples': 10000, 'score': 35753.88782739639, 'total_duration': 40235.9482319355, 'accumulated_submission_time': 35753.88782739639, 'accumulated_eval_time': 4474.300911664963, 'accumulated_logging_time': 3.38096022605896, 'global_step': 76484, 'preemption_count': 0}), (77378, {'train/accuracy': 0.6756640672683716, 'train/loss': 1.311980962753296, 'validation/accuracy': 0.6242200136184692, 'validation/loss': 1.5544767379760742, 'validation/num_examples': 50000, 'test/accuracy': 0.5027000308036804, 'test/loss': 2.2037034034729004, 'test/num_examples': 10000, 'score': 36174.273703336716, 'total_duration': 40708.220797777176, 'accumulated_submission_time': 36174.273703336716, 'accumulated_eval_time': 4526.091993093491, 'accumulated_logging_time': 3.4254894256591797, 'global_step': 77378, 'preemption_count': 0}), (78274, {'train/accuracy': 0.6756445169448853, 'train/loss': 1.4200063943862915, 'validation/accuracy': 0.6239799857139587, 'validation/loss': 1.6556744575500488, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.3106143474578857, 'test/num_examples': 10000, 'score': 36594.59930849075, 'total_duration': 41183.310337781906, 'accumulated_submission_time': 36594.59930849075, 'accumulated_eval_time': 4580.758625507355, 'accumulated_logging_time': 3.471120595932007, 'global_step': 78274, 'preemption_count': 0}), (79172, {'train/accuracy': 0.6913671493530273, 'train/loss': 1.2846978902816772, 'validation/accuracy': 0.6258999705314636, 'validation/loss': 1.5905046463012695, 'validation/num_examples': 50000, 'test/accuracy': 0.49980002641677856, 'test/loss': 2.254966974258423, 'test/num_examples': 10000, 'score': 37014.5864136219, 'total_duration': 41655.481019973755, 'accumulated_submission_time': 37014.5864136219, 'accumulated_eval_time': 4632.851358652115, 'accumulated_logging_time': 3.5113136768341064, 'global_step': 79172, 'preemption_count': 0}), (80067, {'train/accuracy': 0.6728710532188416, 'train/loss': 1.3366504907608032, 'validation/accuracy': 0.6302599906921387, 'validation/loss': 1.55534029006958, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.2068710327148438, 'test/num_examples': 10000, 'score': 37434.66622328758, 'total_duration': 42128.71227145195, 'accumulated_submission_time': 37434.66622328758, 'accumulated_eval_time': 4685.910947561264, 'accumulated_logging_time': 3.5520379543304443, 'global_step': 80067, 'preemption_count': 0}), (80967, {'train/accuracy': 0.6885156035423279, 'train/loss': 1.2635071277618408, 'validation/accuracy': 0.634660005569458, 'validation/loss': 1.520119309425354, 'validation/num_examples': 50000, 'test/accuracy': 0.5120000243186951, 'test/loss': 2.171586751937866, 'test/num_examples': 10000, 'score': 37854.888414382935, 'total_duration': 42600.99657249451, 'accumulated_submission_time': 37854.888414382935, 'accumulated_eval_time': 4737.878388643265, 'accumulated_logging_time': 3.595428943634033, 'global_step': 80967, 'preemption_count': 0}), (81862, {'train/accuracy': 0.6949023008346558, 'train/loss': 1.2488195896148682, 'validation/accuracy': 0.6343599557876587, 'validation/loss': 1.5292257070541382, 'validation/num_examples': 50000, 'test/accuracy': 0.5149000287055969, 'test/loss': 2.177933931350708, 'test/num_examples': 10000, 'score': 38275.072088718414, 'total_duration': 43072.96156978607, 'accumulated_submission_time': 38275.072088718414, 'accumulated_eval_time': 4789.569073200226, 'accumulated_logging_time': 3.6357104778289795, 'global_step': 81862, 'preemption_count': 0}), (82760, {'train/accuracy': 0.6834570169448853, 'train/loss': 1.3045527935028076, 'validation/accuracy': 0.6303600072860718, 'validation/loss': 1.5471371412277222, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.2118685245513916, 'test/num_examples': 10000, 'score': 38695.33440542221, 'total_duration': 43546.44796872139, 'accumulated_submission_time': 38695.33440542221, 'accumulated_eval_time': 4842.699766159058, 'accumulated_logging_time': 3.677600860595703, 'global_step': 82760, 'preemption_count': 0}), (83655, {'train/accuracy': 0.6850390434265137, 'train/loss': 1.279969573020935, 'validation/accuracy': 0.6342200040817261, 'validation/loss': 1.5213388204574585, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.1926448345184326, 'test/num_examples': 10000, 'score': 39115.676505327225, 'total_duration': 44018.31398630142, 'accumulated_submission_time': 39115.676505327225, 'accumulated_eval_time': 4894.128688812256, 'accumulated_logging_time': 3.7216544151306152, 'global_step': 83655, 'preemption_count': 0}), (84551, {'train/accuracy': 0.703125, 'train/loss': 1.225046157836914, 'validation/accuracy': 0.6385599970817566, 'validation/loss': 1.5180543661117554, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.160423755645752, 'test/num_examples': 10000, 'score': 39535.89488720894, 'total_duration': 44490.48594856262, 'accumulated_submission_time': 39535.89488720894, 'accumulated_eval_time': 4945.990887403488, 'accumulated_logging_time': 3.761420726776123, 'global_step': 84551, 'preemption_count': 0}), (85449, {'train/accuracy': 0.683300793170929, 'train/loss': 1.303541898727417, 'validation/accuracy': 0.6320399641990662, 'validation/loss': 1.532470464706421, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.173837900161743, 'test/num_examples': 10000, 'score': 39956.06528735161, 'total_duration': 44961.87512779236, 'accumulated_submission_time': 39956.06528735161, 'accumulated_eval_time': 4997.108908653259, 'accumulated_logging_time': 3.8109896183013916, 'global_step': 85449, 'preemption_count': 0}), (86344, {'train/accuracy': 0.6930859088897705, 'train/loss': 1.2610045671463013, 'validation/accuracy': 0.6387799978256226, 'validation/loss': 1.5271767377853394, 'validation/num_examples': 50000, 'test/accuracy': 0.5146999955177307, 'test/loss': 2.1729276180267334, 'test/num_examples': 10000, 'score': 40376.114825725555, 'total_duration': 45433.95785403252, 'accumulated_submission_time': 40376.114825725555, 'accumulated_eval_time': 5049.05076956749, 'accumulated_logging_time': 3.851077079772949, 'global_step': 86344, 'preemption_count': 0}), (87241, {'train/accuracy': 0.6998632550239563, 'train/loss': 1.2377312183380127, 'validation/accuracy': 0.638700008392334, 'validation/loss': 1.5107841491699219, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.1511378288269043, 'test/num_examples': 10000, 'score': 40796.28626155853, 'total_duration': 45905.69135284424, 'accumulated_submission_time': 40796.28626155853, 'accumulated_eval_time': 5100.5192720890045, 'accumulated_logging_time': 3.894582748413086, 'global_step': 87241, 'preemption_count': 0}), (88126, {'train/accuracy': 0.6940820217132568, 'train/loss': 1.2332541942596436, 'validation/accuracy': 0.637179970741272, 'validation/loss': 1.494731068611145, 'validation/num_examples': 50000, 'test/accuracy': 0.5146000385284424, 'test/loss': 2.1517715454101562, 'test/num_examples': 10000, 'score': 41216.240828990936, 'total_duration': 46378.955389261246, 'accumulated_submission_time': 41216.240828990936, 'accumulated_eval_time': 5153.7355852127075, 'accumulated_logging_time': 3.9385111331939697, 'global_step': 88126, 'preemption_count': 0}), (88977, {'train/accuracy': 0.6952733993530273, 'train/loss': 1.2344191074371338, 'validation/accuracy': 0.6416999697685242, 'validation/loss': 1.4802577495574951, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.1336536407470703, 'test/num_examples': 10000, 'score': 41636.33084964752, 'total_duration': 46850.67201781273, 'accumulated_submission_time': 41636.33084964752, 'accumulated_eval_time': 5205.26829957962, 'accumulated_logging_time': 3.983673334121704, 'global_step': 88977, 'preemption_count': 0}), (89871, {'train/accuracy': 0.704296886920929, 'train/loss': 1.2123621702194214, 'validation/accuracy': 0.6403399705886841, 'validation/loss': 1.5044403076171875, 'validation/num_examples': 50000, 'test/accuracy': 0.516700029373169, 'test/loss': 2.169416666030884, 'test/num_examples': 10000, 'score': 42056.24417757988, 'total_duration': 47323.980553388596, 'accumulated_submission_time': 42056.24417757988, 'accumulated_eval_time': 5258.57132768631, 'accumulated_logging_time': 4.025211334228516, 'global_step': 89871, 'preemption_count': 0}), (90771, {'train/accuracy': 0.6926171779632568, 'train/loss': 1.247671127319336, 'validation/accuracy': 0.6415799856185913, 'validation/loss': 1.4853407144546509, 'validation/num_examples': 50000, 'test/accuracy': 0.5189000368118286, 'test/loss': 2.1496667861938477, 'test/num_examples': 10000, 'score': 42476.40330886841, 'total_duration': 47795.89140582085, 'accumulated_submission_time': 42476.40330886841, 'accumulated_eval_time': 5310.227010965347, 'accumulated_logging_time': 4.070418834686279, 'global_step': 90771, 'preemption_count': 0}), (91663, {'train/accuracy': 0.7013866901397705, 'train/loss': 1.2053130865097046, 'validation/accuracy': 0.645639955997467, 'validation/loss': 1.4752509593963623, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.1370153427124023, 'test/num_examples': 10000, 'score': 42896.397490262985, 'total_duration': 48267.832307338715, 'accumulated_submission_time': 42896.397490262985, 'accumulated_eval_time': 5362.071304321289, 'accumulated_logging_time': 4.122549295425415, 'global_step': 91663, 'preemption_count': 0}), (92557, {'train/accuracy': 0.7112109065055847, 'train/loss': 1.1575263738632202, 'validation/accuracy': 0.6488800048828125, 'validation/loss': 1.4489514827728271, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.1200804710388184, 'test/num_examples': 10000, 'score': 43316.60587668419, 'total_duration': 48739.50898981094, 'accumulated_submission_time': 43316.60587668419, 'accumulated_eval_time': 5413.447510957718, 'accumulated_logging_time': 4.164551734924316, 'global_step': 92557, 'preemption_count': 0}), (93455, {'train/accuracy': 0.6953515410423279, 'train/loss': 1.2690379619598389, 'validation/accuracy': 0.6404399871826172, 'validation/loss': 1.5253348350524902, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.171663999557495, 'test/num_examples': 10000, 'score': 43736.8600165844, 'total_duration': 49212.40234661102, 'accumulated_submission_time': 43736.8600165844, 'accumulated_eval_time': 5465.9878051280975, 'accumulated_logging_time': 4.211713075637817, 'global_step': 93455, 'preemption_count': 0}), (94353, {'train/accuracy': 0.7071093320846558, 'train/loss': 1.196900486946106, 'validation/accuracy': 0.6487599611282349, 'validation/loss': 1.4550457000732422, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.0961992740631104, 'test/num_examples': 10000, 'score': 44157.11583042145, 'total_duration': 49687.024679899216, 'accumulated_submission_time': 44157.11583042145, 'accumulated_eval_time': 5520.260136604309, 'accumulated_logging_time': 4.254514455795288, 'global_step': 94353, 'preemption_count': 0}), (95251, {'train/accuracy': 0.7099023461341858, 'train/loss': 1.2014402151107788, 'validation/accuracy': 0.6468999981880188, 'validation/loss': 1.4840155839920044, 'validation/num_examples': 50000, 'test/accuracy': 0.5276000499725342, 'test/loss': 2.12835431098938, 'test/num_examples': 10000, 'score': 44577.40431547165, 'total_duration': 50159.85675024986, 'accumulated_submission_time': 44577.40431547165, 'accumulated_eval_time': 5572.704406738281, 'accumulated_logging_time': 4.302710056304932, 'global_step': 95251, 'preemption_count': 0}), (96149, {'train/accuracy': 0.7105468511581421, 'train/loss': 1.17483651638031, 'validation/accuracy': 0.6487999558448792, 'validation/loss': 1.4631506204605103, 'validation/num_examples': 50000, 'test/accuracy': 0.525700032711029, 'test/loss': 2.12905216217041, 'test/num_examples': 10000, 'score': 44997.624660253525, 'total_duration': 50631.88677740097, 'accumulated_submission_time': 44997.624660253525, 'accumulated_eval_time': 5624.4156901836395, 'accumulated_logging_time': 4.350196361541748, 'global_step': 96149, 'preemption_count': 0}), (97044, {'train/accuracy': 0.7084765434265137, 'train/loss': 1.200149655342102, 'validation/accuracy': 0.650879979133606, 'validation/loss': 1.4600813388824463, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.103090524673462, 'test/num_examples': 10000, 'score': 45417.73803925514, 'total_duration': 51103.32812476158, 'accumulated_submission_time': 45417.73803925514, 'accumulated_eval_time': 5675.649765968323, 'accumulated_logging_time': 4.3937060832977295, 'global_step': 97044, 'preemption_count': 0}), (97942, {'train/accuracy': 0.7183789014816284, 'train/loss': 1.1449146270751953, 'validation/accuracy': 0.6541199684143066, 'validation/loss': 1.44417405128479, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.0910539627075195, 'test/num_examples': 10000, 'score': 45838.01228451729, 'total_duration': 51577.29973363876, 'accumulated_submission_time': 45838.01228451729, 'accumulated_eval_time': 5729.255409002304, 'accumulated_logging_time': 4.434265851974487, 'global_step': 97942, 'preemption_count': 0}), (98841, {'train/accuracy': 0.7341406345367432, 'train/loss': 1.0801435708999634, 'validation/accuracy': 0.6582199931144714, 'validation/loss': 1.4175523519515991, 'validation/num_examples': 50000, 'test/accuracy': 0.530500054359436, 'test/loss': 2.0810186862945557, 'test/num_examples': 10000, 'score': 46258.07188653946, 'total_duration': 52049.81680226326, 'accumulated_submission_time': 46258.07188653946, 'accumulated_eval_time': 5781.6155371665955, 'accumulated_logging_time': 4.480335235595703, 'global_step': 98841, 'preemption_count': 0}), (99736, {'train/accuracy': 0.7138085961341858, 'train/loss': 1.1494009494781494, 'validation/accuracy': 0.6567999720573425, 'validation/loss': 1.4137784242630005, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.062328338623047, 'test/num_examples': 10000, 'score': 46678.317527770996, 'total_duration': 52521.245457172394, 'accumulated_submission_time': 46678.317527770996, 'accumulated_eval_time': 5832.705719232559, 'accumulated_logging_time': 4.522157669067383, 'global_step': 99736, 'preemption_count': 0}), (100630, {'train/accuracy': 0.71728515625, 'train/loss': 1.1666183471679688, 'validation/accuracy': 0.6571399569511414, 'validation/loss': 1.4454323053359985, 'validation/num_examples': 50000, 'test/accuracy': 0.5279000401496887, 'test/loss': 2.1021368503570557, 'test/num_examples': 10000, 'score': 47098.33820104599, 'total_duration': 52993.11081790924, 'accumulated_submission_time': 47098.33820104599, 'accumulated_eval_time': 5884.45601606369, 'accumulated_logging_time': 4.5659730434417725, 'global_step': 100630, 'preemption_count': 0}), (101524, {'train/accuracy': 0.7432226538658142, 'train/loss': 1.042794108390808, 'validation/accuracy': 0.6620199680328369, 'validation/loss': 1.3963292837142944, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.04892635345459, 'test/num_examples': 10000, 'score': 47518.43212723732, 'total_duration': 53467.515218257904, 'accumulated_submission_time': 47518.43212723732, 'accumulated_eval_time': 5938.6716158390045, 'accumulated_logging_time': 4.610373020172119, 'global_step': 101524, 'preemption_count': 0}), (102418, {'train/accuracy': 0.7162694931030273, 'train/loss': 1.132684588432312, 'validation/accuracy': 0.6610999703407288, 'validation/loss': 1.4008057117462158, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.0464093685150146, 'test/num_examples': 10000, 'score': 47938.56771445274, 'total_duration': 53939.10957980156, 'accumulated_submission_time': 47938.56771445274, 'accumulated_eval_time': 5990.034274101257, 'accumulated_logging_time': 4.655975341796875, 'global_step': 102418, 'preemption_count': 0}), (103313, {'train/accuracy': 0.7245898246765137, 'train/loss': 1.1347663402557373, 'validation/accuracy': 0.6670799851417542, 'validation/loss': 1.4015071392059326, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.047637939453125, 'test/num_examples': 10000, 'score': 48358.572227716446, 'total_duration': 54411.3479924202, 'accumulated_submission_time': 48358.572227716446, 'accumulated_eval_time': 6042.1712164878845, 'accumulated_logging_time': 4.702473402023315, 'global_step': 103313, 'preemption_count': 0}), (104208, {'train/accuracy': 0.745898425579071, 'train/loss': 1.0389422178268433, 'validation/accuracy': 0.6654399633407593, 'validation/loss': 1.3964356184005737, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.037595748901367, 'test/num_examples': 10000, 'score': 48778.659165382385, 'total_duration': 54884.48488640785, 'accumulated_submission_time': 48778.659165382385, 'accumulated_eval_time': 6095.1235938072205, 'accumulated_logging_time': 4.748953342437744, 'global_step': 104208, 'preemption_count': 0}), (105104, {'train/accuracy': 0.7211328148841858, 'train/loss': 1.13435959815979, 'validation/accuracy': 0.6629399657249451, 'validation/loss': 1.3976269960403442, 'validation/num_examples': 50000, 'test/accuracy': 0.5414000153541565, 'test/loss': 2.042243719100952, 'test/num_examples': 10000, 'score': 49199.02674174309, 'total_duration': 55356.00403022766, 'accumulated_submission_time': 49199.02674174309, 'accumulated_eval_time': 6146.176388025284, 'accumulated_logging_time': 4.796812057495117, 'global_step': 105104, 'preemption_count': 0}), (106001, {'train/accuracy': 0.7251952886581421, 'train/loss': 1.1188418865203857, 'validation/accuracy': 0.6670399904251099, 'validation/loss': 1.396875023841858, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.0384507179260254, 'test/num_examples': 10000, 'score': 49619.575608730316, 'total_duration': 55830.23876786232, 'accumulated_submission_time': 49619.575608730316, 'accumulated_eval_time': 6199.765183925629, 'accumulated_logging_time': 4.843739748001099, 'global_step': 106001, 'preemption_count': 0}), (106902, {'train/accuracy': 0.7347265481948853, 'train/loss': 1.0637056827545166, 'validation/accuracy': 0.6632599830627441, 'validation/loss': 1.3980926275253296, 'validation/num_examples': 50000, 'test/accuracy': 0.539400041103363, 'test/loss': 2.0443313121795654, 'test/num_examples': 10000, 'score': 50039.59346675873, 'total_duration': 56303.84405493736, 'accumulated_submission_time': 50039.59346675873, 'accumulated_eval_time': 6253.255183458328, 'accumulated_logging_time': 4.89024019241333, 'global_step': 106902, 'preemption_count': 0}), (107798, {'train/accuracy': 0.7272851467132568, 'train/loss': 1.1355656385421753, 'validation/accuracy': 0.6676599979400635, 'validation/loss': 1.4018704891204834, 'validation/num_examples': 50000, 'test/accuracy': 0.5404000282287598, 'test/loss': 2.063552141189575, 'test/num_examples': 10000, 'score': 50459.5941298008, 'total_duration': 56777.110137701035, 'accumulated_submission_time': 50459.5941298008, 'accumulated_eval_time': 6306.424833536148, 'accumulated_logging_time': 4.935125350952148, 'global_step': 107798, 'preemption_count': 0}), (108692, {'train/accuracy': 0.7261914014816284, 'train/loss': 1.173351764678955, 'validation/accuracy': 0.6655200123786926, 'validation/loss': 1.454077959060669, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.09930157661438, 'test/num_examples': 10000, 'score': 50879.518305301666, 'total_duration': 57248.41434073448, 'accumulated_submission_time': 50879.518305301666, 'accumulated_eval_time': 6357.710418224335, 'accumulated_logging_time': 4.979193210601807, 'global_step': 108692, 'preemption_count': 0}), (109582, {'train/accuracy': 0.7465234398841858, 'train/loss': 1.0337104797363281, 'validation/accuracy': 0.6727199554443359, 'validation/loss': 1.3676403760910034, 'validation/num_examples': 50000, 'test/accuracy': 0.5498000383377075, 'test/loss': 2.0147860050201416, 'test/num_examples': 10000, 'score': 51299.49491405487, 'total_duration': 57719.42250370979, 'accumulated_submission_time': 51299.49491405487, 'accumulated_eval_time': 6408.644496679306, 'accumulated_logging_time': 5.0263166427612305, 'global_step': 109582, 'preemption_count': 0}), (110472, {'train/accuracy': 0.7326952815055847, 'train/loss': 1.1184043884277344, 'validation/accuracy': 0.6693399548530579, 'validation/loss': 1.3929319381713867, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 2.028778553009033, 'test/num_examples': 10000, 'score': 51719.80661559105, 'total_duration': 58192.70450210571, 'accumulated_submission_time': 51719.80661559105, 'accumulated_eval_time': 6461.516880512238, 'accumulated_logging_time': 5.074175596237183, 'global_step': 110472, 'preemption_count': 0}), (111369, {'train/accuracy': 0.7405859231948853, 'train/loss': 1.0059020519256592, 'validation/accuracy': 0.678380012512207, 'validation/loss': 1.3027551174163818, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 1.9627418518066406, 'test/num_examples': 10000, 'score': 52140.07601737976, 'total_duration': 58666.6617166996, 'accumulated_submission_time': 52140.07601737976, 'accumulated_eval_time': 6515.1032173633575, 'accumulated_logging_time': 5.12527322769165, 'global_step': 111369, 'preemption_count': 0}), (112264, {'train/accuracy': 0.7454296946525574, 'train/loss': 0.9949614405632019, 'validation/accuracy': 0.677619993686676, 'validation/loss': 1.3128902912139893, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 1.9710676670074463, 'test/num_examples': 10000, 'score': 52560.04998207092, 'total_duration': 59139.74605703354, 'accumulated_submission_time': 52560.04998207092, 'accumulated_eval_time': 6568.118919849396, 'accumulated_logging_time': 5.169904947280884, 'global_step': 112264, 'preemption_count': 0}), (113154, {'train/accuracy': 0.7370312213897705, 'train/loss': 1.0629130601882935, 'validation/accuracy': 0.674340009689331, 'validation/loss': 1.3487356901168823, 'validation/num_examples': 50000, 'test/accuracy': 0.5530000329017639, 'test/loss': 1.9947872161865234, 'test/num_examples': 10000, 'score': 52980.3245677948, 'total_duration': 59613.3454978466, 'accumulated_submission_time': 52980.3245677948, 'accumulated_eval_time': 6621.345211267471, 'accumulated_logging_time': 5.218159914016724, 'global_step': 113154, 'preemption_count': 0}), (114049, {'train/accuracy': 0.7362695336341858, 'train/loss': 1.1148184537887573, 'validation/accuracy': 0.6717199683189392, 'validation/loss': 1.4028652906417847, 'validation/num_examples': 50000, 'test/accuracy': 0.5480000376701355, 'test/loss': 2.0506300926208496, 'test/num_examples': 10000, 'score': 53400.63660097122, 'total_duration': 60084.95163035393, 'accumulated_submission_time': 53400.63660097122, 'accumulated_eval_time': 6672.544312000275, 'accumulated_logging_time': 5.261758804321289, 'global_step': 114049, 'preemption_count': 0}), (114941, {'train/accuracy': 0.7517382502555847, 'train/loss': 1.0051695108413696, 'validation/accuracy': 0.6760599613189697, 'validation/loss': 1.3408530950546265, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 1.9954674243927002, 'test/num_examples': 10000, 'score': 53820.99761939049, 'total_duration': 60558.87907385826, 'accumulated_submission_time': 53820.99761939049, 'accumulated_eval_time': 6726.00217795372, 'accumulated_logging_time': 5.320492744445801, 'global_step': 114941, 'preemption_count': 0}), (115834, {'train/accuracy': 0.7432421445846558, 'train/loss': 1.0460532903671265, 'validation/accuracy': 0.6818000078201294, 'validation/loss': 1.3182965517044067, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 1.9709168672561646, 'test/num_examples': 10000, 'score': 54240.927837610245, 'total_duration': 61032.71226191521, 'accumulated_submission_time': 54240.927837610245, 'accumulated_eval_time': 6779.807821750641, 'accumulated_logging_time': 5.36723256111145, 'global_step': 115834, 'preemption_count': 0}), (116723, {'train/accuracy': 0.7484374642372131, 'train/loss': 1.0299557447433472, 'validation/accuracy': 0.6839199662208557, 'validation/loss': 1.325279951095581, 'validation/num_examples': 50000, 'test/accuracy': 0.5578000545501709, 'test/loss': 1.9653598070144653, 'test/num_examples': 10000, 'score': 54660.929174661636, 'total_duration': 61503.20591568947, 'accumulated_submission_time': 54660.929174661636, 'accumulated_eval_time': 6830.200934410095, 'accumulated_logging_time': 5.416102886199951, 'global_step': 116723, 'preemption_count': 0}), (117617, {'train/accuracy': 0.7597265243530273, 'train/loss': 0.9526239633560181, 'validation/accuracy': 0.6856600046157837, 'validation/loss': 1.2929719686508179, 'validation/num_examples': 50000, 'test/accuracy': 0.560699999332428, 'test/loss': 1.9365530014038086, 'test/num_examples': 10000, 'score': 55081.1960170269, 'total_duration': 61978.925362825394, 'accumulated_submission_time': 55081.1960170269, 'accumulated_eval_time': 6885.558126449585, 'accumulated_logging_time': 5.461392164230347, 'global_step': 117617, 'preemption_count': 0}), (118506, {'train/accuracy': 0.7503515481948853, 'train/loss': 1.0049922466278076, 'validation/accuracy': 0.6844199895858765, 'validation/loss': 1.30093252658844, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 1.9488836526870728, 'test/num_examples': 10000, 'score': 55501.43515133858, 'total_duration': 62454.564338207245, 'accumulated_submission_time': 55501.43515133858, 'accumulated_eval_time': 6940.854739904404, 'accumulated_logging_time': 5.514406204223633, 'global_step': 118506, 'preemption_count': 0}), (119398, {'train/accuracy': 0.7482226490974426, 'train/loss': 1.0383548736572266, 'validation/accuracy': 0.6855599880218506, 'validation/loss': 1.3293486833572388, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 1.9782428741455078, 'test/num_examples': 10000, 'score': 55921.57339477539, 'total_duration': 62927.31496787071, 'accumulated_submission_time': 55921.57339477539, 'accumulated_eval_time': 6993.368841648102, 'accumulated_logging_time': 5.561697959899902, 'global_step': 119398, 'preemption_count': 0}), (120291, {'train/accuracy': 0.7626562118530273, 'train/loss': 0.9669126868247986, 'validation/accuracy': 0.6891199946403503, 'validation/loss': 1.2968260049819946, 'validation/num_examples': 50000, 'test/accuracy': 0.5616000294685364, 'test/loss': 1.9497791528701782, 'test/num_examples': 10000, 'score': 56341.536450624466, 'total_duration': 63398.22046136856, 'accumulated_submission_time': 56341.536450624466, 'accumulated_eval_time': 7044.214889287949, 'accumulated_logging_time': 5.607126235961914, 'global_step': 120291, 'preemption_count': 0}), (121188, {'train/accuracy': 0.7483984231948853, 'train/loss': 1.023191213607788, 'validation/accuracy': 0.6844800114631653, 'validation/loss': 1.3076735734939575, 'validation/num_examples': 50000, 'test/accuracy': 0.5606000423431396, 'test/loss': 1.9592289924621582, 'test/num_examples': 10000, 'score': 56761.78638911247, 'total_duration': 63871.95942783356, 'accumulated_submission_time': 56761.78638911247, 'accumulated_eval_time': 7097.606656551361, 'accumulated_logging_time': 5.652962684631348, 'global_step': 121188, 'preemption_count': 0}), (122078, {'train/accuracy': 0.7550390362739563, 'train/loss': 0.9838645458221436, 'validation/accuracy': 0.689799964427948, 'validation/loss': 1.2810405492782593, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 1.9222257137298584, 'test/num_examples': 10000, 'score': 57181.91209387779, 'total_duration': 64347.73667383194, 'accumulated_submission_time': 57181.91209387779, 'accumulated_eval_time': 7153.155112504959, 'accumulated_logging_time': 5.705412864685059, 'global_step': 122078, 'preemption_count': 0}), (122967, {'train/accuracy': 0.767285168170929, 'train/loss': 0.9140588045120239, 'validation/accuracy': 0.6899399757385254, 'validation/loss': 1.2592631578445435, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 1.895406723022461, 'test/num_examples': 10000, 'score': 57601.97654438019, 'total_duration': 64821.3822324276, 'accumulated_submission_time': 57601.97654438019, 'accumulated_eval_time': 7206.63245177269, 'accumulated_logging_time': 5.758823394775391, 'global_step': 122967, 'preemption_count': 0}), (123855, {'train/accuracy': 0.7547070384025574, 'train/loss': 0.9752053618431091, 'validation/accuracy': 0.691819965839386, 'validation/loss': 1.2582887411117554, 'validation/num_examples': 50000, 'test/accuracy': 0.5642000436782837, 'test/loss': 1.9256024360656738, 'test/num_examples': 10000, 'score': 58022.1356446743, 'total_duration': 65292.18530201912, 'accumulated_submission_time': 58022.1356446743, 'accumulated_eval_time': 7257.178694009781, 'accumulated_logging_time': 5.806592226028442, 'global_step': 123855, 'preemption_count': 0}), (124748, {'train/accuracy': 0.7588085532188416, 'train/loss': 0.9643204808235168, 'validation/accuracy': 0.6942799687385559, 'validation/loss': 1.259268045425415, 'validation/num_examples': 50000, 'test/accuracy': 0.5690000057220459, 'test/loss': 1.9072344303131104, 'test/num_examples': 10000, 'score': 58442.4238243103, 'total_duration': 65765.31973147392, 'accumulated_submission_time': 58442.4238243103, 'accumulated_eval_time': 7309.925290107727, 'accumulated_logging_time': 5.855309247970581, 'global_step': 124748, 'preemption_count': 0}), (125642, {'train/accuracy': 0.7699999809265137, 'train/loss': 0.9051749110221863, 'validation/accuracy': 0.6944599747657776, 'validation/loss': 1.2426235675811768, 'validation/num_examples': 50000, 'test/accuracy': 0.5710000395774841, 'test/loss': 1.878885269165039, 'test/num_examples': 10000, 'score': 58862.696983098984, 'total_duration': 66237.66225218773, 'accumulated_submission_time': 58862.696983098984, 'accumulated_eval_time': 7361.893659353256, 'accumulated_logging_time': 5.9061243534088135, 'global_step': 125642, 'preemption_count': 0}), (126535, {'train/accuracy': 0.7580859065055847, 'train/loss': 0.9454815983772278, 'validation/accuracy': 0.6959999799728394, 'validation/loss': 1.2273520231246948, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.8674311637878418, 'test/num_examples': 10000, 'score': 59282.61265707016, 'total_duration': 66709.08399271965, 'accumulated_submission_time': 59282.61265707016, 'accumulated_eval_time': 7413.29775929451, 'accumulated_logging_time': 5.957125425338745, 'global_step': 126535, 'preemption_count': 0}), (127428, {'train/accuracy': 0.7644921541213989, 'train/loss': 0.9372758865356445, 'validation/accuracy': 0.6952999830245972, 'validation/loss': 1.2479116916656494, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 1.8793699741363525, 'test/num_examples': 10000, 'score': 59702.56274223328, 'total_duration': 67182.71284723282, 'accumulated_submission_time': 59702.56274223328, 'accumulated_eval_time': 7466.876657009125, 'accumulated_logging_time': 6.0067572593688965, 'global_step': 127428, 'preemption_count': 0}), (128321, {'train/accuracy': 0.7732617259025574, 'train/loss': 0.9040424823760986, 'validation/accuracy': 0.696179986000061, 'validation/loss': 1.2386150360107422, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.8976391553878784, 'test/num_examples': 10000, 'score': 60122.79120898247, 'total_duration': 67656.58211922646, 'accumulated_submission_time': 60122.79120898247, 'accumulated_eval_time': 7520.418255567551, 'accumulated_logging_time': 6.055173873901367, 'global_step': 128321, 'preemption_count': 0}), (129217, {'train/accuracy': 0.7625390291213989, 'train/loss': 0.9737145900726318, 'validation/accuracy': 0.6969599723815918, 'validation/loss': 1.2702531814575195, 'validation/num_examples': 50000, 'test/accuracy': 0.572100043296814, 'test/loss': 1.9134632349014282, 'test/num_examples': 10000, 'score': 60542.8531806469, 'total_duration': 68129.01347446442, 'accumulated_submission_time': 60542.8531806469, 'accumulated_eval_time': 7572.68899512291, 'accumulated_logging_time': 6.103700399398804, 'global_step': 129217, 'preemption_count': 0}), (130113, {'train/accuracy': 0.7669726610183716, 'train/loss': 0.9456799626350403, 'validation/accuracy': 0.7005999684333801, 'validation/loss': 1.244845986366272, 'validation/num_examples': 50000, 'test/accuracy': 0.5756000280380249, 'test/loss': 1.8801839351654053, 'test/num_examples': 10000, 'score': 60962.88206410408, 'total_duration': 68602.47761464119, 'accumulated_submission_time': 60962.88206410408, 'accumulated_eval_time': 7626.02169251442, 'accumulated_logging_time': 6.154720783233643, 'global_step': 130113, 'preemption_count': 0}), (131000, {'train/accuracy': 0.7790820002555847, 'train/loss': 0.8822490572929382, 'validation/accuracy': 0.7021999955177307, 'validation/loss': 1.2245677709579468, 'validation/num_examples': 50000, 'test/accuracy': 0.5766000151634216, 'test/loss': 1.8699551820755005, 'test/num_examples': 10000, 'score': 61383.08276414871, 'total_duration': 69075.28121972084, 'accumulated_submission_time': 61383.08276414871, 'accumulated_eval_time': 7678.519439458847, 'accumulated_logging_time': 6.209751129150391, 'global_step': 131000, 'preemption_count': 0}), (131892, {'train/accuracy': 0.7725781202316284, 'train/loss': 0.9053280353546143, 'validation/accuracy': 0.7041800022125244, 'validation/loss': 1.2155123949050903, 'validation/num_examples': 50000, 'test/accuracy': 0.575700044631958, 'test/loss': 1.8553532361984253, 'test/num_examples': 10000, 'score': 61803.106755018234, 'total_duration': 69547.86748552322, 'accumulated_submission_time': 61803.106755018234, 'accumulated_eval_time': 7730.9835069179535, 'accumulated_logging_time': 6.25759744644165, 'global_step': 131892, 'preemption_count': 0}), (132782, {'train/accuracy': 0.7796679735183716, 'train/loss': 0.8771883249282837, 'validation/accuracy': 0.7079199552536011, 'validation/loss': 1.195351243019104, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.8324159383773804, 'test/num_examples': 10000, 'score': 62223.355676651, 'total_duration': 70020.74941420555, 'accumulated_submission_time': 62223.355676651, 'accumulated_eval_time': 7783.519206285477, 'accumulated_logging_time': 6.3048930168151855, 'global_step': 132782, 'preemption_count': 0}), (133679, {'train/accuracy': 0.78236323595047, 'train/loss': 0.8421926498413086, 'validation/accuracy': 0.7068600058555603, 'validation/loss': 1.1863993406295776, 'validation/num_examples': 50000, 'test/accuracy': 0.5857000350952148, 'test/loss': 1.8177932500839233, 'test/num_examples': 10000, 'score': 62643.74037742615, 'total_duration': 70494.65468883514, 'accumulated_submission_time': 62643.74037742615, 'accumulated_eval_time': 7836.93887090683, 'accumulated_logging_time': 6.35483980178833, 'global_step': 133679, 'preemption_count': 0}), (134573, {'train/accuracy': 0.7735351324081421, 'train/loss': 0.902022123336792, 'validation/accuracy': 0.7064799666404724, 'validation/loss': 1.2089617252349854, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.8554986715316772, 'test/num_examples': 10000, 'score': 63063.97910571098, 'total_duration': 70966.68536663055, 'accumulated_submission_time': 63063.97910571098, 'accumulated_eval_time': 7888.631495952606, 'accumulated_logging_time': 6.403504371643066, 'global_step': 134573, 'preemption_count': 0}), (135466, {'train/accuracy': 0.7805468440055847, 'train/loss': 0.8645254373550415, 'validation/accuracy': 0.7113199830055237, 'validation/loss': 1.1867696046829224, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 1.8302571773529053, 'test/num_examples': 10000, 'score': 63484.011751413345, 'total_duration': 71438.42376971245, 'accumulated_submission_time': 63484.011751413345, 'accumulated_eval_time': 7940.2364201545715, 'accumulated_logging_time': 6.454331636428833, 'global_step': 135466, 'preemption_count': 0}), (136359, {'train/accuracy': 0.78480464220047, 'train/loss': 0.8444609045982361, 'validation/accuracy': 0.7076399922370911, 'validation/loss': 1.1892009973526, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.8246159553527832, 'test/num_examples': 10000, 'score': 63904.009852170944, 'total_duration': 71910.59975337982, 'accumulated_submission_time': 63904.009852170944, 'accumulated_eval_time': 7992.3159103393555, 'accumulated_logging_time': 6.5025811195373535, 'global_step': 136359, 'preemption_count': 0}), (137253, {'train/accuracy': 0.7909960746765137, 'train/loss': 0.8081295490264893, 'validation/accuracy': 0.7122399806976318, 'validation/loss': 1.1592646837234497, 'validation/num_examples': 50000, 'test/accuracy': 0.5891000032424927, 'test/loss': 1.7870514392852783, 'test/num_examples': 10000, 'score': 64323.90890264511, 'total_duration': 72384.28784418106, 'accumulated_submission_time': 64323.90890264511, 'accumulated_eval_time': 8045.995781421661, 'accumulated_logging_time': 6.56099271774292, 'global_step': 137253, 'preemption_count': 0}), (138143, {'train/accuracy': 0.7822265625, 'train/loss': 0.8877792358398438, 'validation/accuracy': 0.7106399536132812, 'validation/loss': 1.2171674966812134, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 1.8493305444717407, 'test/num_examples': 10000, 'score': 64744.211985588074, 'total_duration': 72856.13017225266, 'accumulated_submission_time': 64744.211985588074, 'accumulated_eval_time': 8097.4318726062775, 'accumulated_logging_time': 6.613979339599609, 'global_step': 138143, 'preemption_count': 0}), (139035, {'train/accuracy': 0.7908398509025574, 'train/loss': 0.8290248513221741, 'validation/accuracy': 0.7122799754142761, 'validation/loss': 1.171581745147705, 'validation/num_examples': 50000, 'test/accuracy': 0.5884000062942505, 'test/loss': 1.7982066869735718, 'test/num_examples': 10000, 'score': 65164.19147825241, 'total_duration': 73327.64161705971, 'accumulated_submission_time': 65164.19147825241, 'accumulated_eval_time': 8148.8644115924835, 'accumulated_logging_time': 6.663257837295532, 'global_step': 139035, 'preemption_count': 0}), (139928, {'train/accuracy': 0.8020117282867432, 'train/loss': 0.7729999423027039, 'validation/accuracy': 0.7156800031661987, 'validation/loss': 1.1472184658050537, 'validation/num_examples': 50000, 'test/accuracy': 0.5904000401496887, 'test/loss': 1.78292977809906, 'test/num_examples': 10000, 'score': 65584.32089519501, 'total_duration': 73799.69849538803, 'accumulated_submission_time': 65584.32089519501, 'accumulated_eval_time': 8200.685409069061, 'accumulated_logging_time': 6.717529058456421, 'global_step': 139928, 'preemption_count': 0}), (140823, {'train/accuracy': 0.7893944978713989, 'train/loss': 0.811639666557312, 'validation/accuracy': 0.7133199572563171, 'validation/loss': 1.150890588760376, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 1.7908883094787598, 'test/num_examples': 10000, 'score': 66004.48244214058, 'total_duration': 74272.31378889084, 'accumulated_submission_time': 66004.48244214058, 'accumulated_eval_time': 8253.039557218552, 'accumulated_logging_time': 6.766315937042236, 'global_step': 140823, 'preemption_count': 0}), (141714, {'train/accuracy': 0.7934374809265137, 'train/loss': 0.8436886668205261, 'validation/accuracy': 0.7143200039863586, 'validation/loss': 1.187924861907959, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.8304020166397095, 'test/num_examples': 10000, 'score': 66424.64887809753, 'total_duration': 74745.89710235596, 'accumulated_submission_time': 66424.64887809753, 'accumulated_eval_time': 8306.357367038727, 'accumulated_logging_time': 6.8145668506622314, 'global_step': 141714, 'preemption_count': 0}), (142603, {'train/accuracy': 0.8104296922683716, 'train/loss': 0.7512505054473877, 'validation/accuracy': 0.7206400036811829, 'validation/loss': 1.1383086442947388, 'validation/num_examples': 50000, 'test/accuracy': 0.5949000120162964, 'test/loss': 1.7849997282028198, 'test/num_examples': 10000, 'score': 66844.86011886597, 'total_duration': 75217.28182458878, 'accumulated_submission_time': 66844.86011886597, 'accumulated_eval_time': 8357.431869983673, 'accumulated_logging_time': 6.8636791706085205, 'global_step': 142603, 'preemption_count': 0}), (143493, {'train/accuracy': 0.7852538824081421, 'train/loss': 0.8588006496429443, 'validation/accuracy': 0.713699996471405, 'validation/loss': 1.1768875122070312, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.8258694410324097, 'test/num_examples': 10000, 'score': 67264.9873714447, 'total_duration': 75689.96438384056, 'accumulated_submission_time': 67264.9873714447, 'accumulated_eval_time': 8409.880574703217, 'accumulated_logging_time': 6.9201719760894775, 'global_step': 143493, 'preemption_count': 0}), (144389, {'train/accuracy': 0.8009960651397705, 'train/loss': 0.8065456748008728, 'validation/accuracy': 0.7186799645423889, 'validation/loss': 1.1656146049499512, 'validation/num_examples': 50000, 'test/accuracy': 0.5975000262260437, 'test/loss': 1.7978652715682983, 'test/num_examples': 10000, 'score': 67685.38305974007, 'total_duration': 76162.98290419579, 'accumulated_submission_time': 67685.38305974007, 'accumulated_eval_time': 8462.395847082138, 'accumulated_logging_time': 6.977154016494751, 'global_step': 144389, 'preemption_count': 0}), (145286, {'train/accuracy': 0.8099414110183716, 'train/loss': 0.7473099231719971, 'validation/accuracy': 0.720579981803894, 'validation/loss': 1.1367037296295166, 'validation/num_examples': 50000, 'test/accuracy': 0.593500018119812, 'test/loss': 1.777706503868103, 'test/num_examples': 10000, 'score': 68105.52442455292, 'total_duration': 76635.59283566475, 'accumulated_submission_time': 68105.52442455292, 'accumulated_eval_time': 8514.762991905212, 'accumulated_logging_time': 7.027865171432495, 'global_step': 145286, 'preemption_count': 0}), (146180, {'train/accuracy': 0.7946679592132568, 'train/loss': 0.8052504062652588, 'validation/accuracy': 0.7216199636459351, 'validation/loss': 1.132479190826416, 'validation/num_examples': 50000, 'test/accuracy': 0.5982000231742859, 'test/loss': 1.7780965566635132, 'test/num_examples': 10000, 'score': 68525.59720563889, 'total_duration': 77108.97180509567, 'accumulated_submission_time': 68525.59720563889, 'accumulated_eval_time': 8567.967049360275, 'accumulated_logging_time': 7.0798468589782715, 'global_step': 146180, 'preemption_count': 0}), (147069, {'train/accuracy': 0.8073241710662842, 'train/loss': 0.7616297602653503, 'validation/accuracy': 0.7257999777793884, 'validation/loss': 1.1163734197616577, 'validation/num_examples': 50000, 'test/accuracy': 0.5991000533103943, 'test/loss': 1.7480182647705078, 'test/num_examples': 10000, 'score': 68945.71439909935, 'total_duration': 77580.7287735939, 'accumulated_submission_time': 68945.71439909935, 'accumulated_eval_time': 8619.504743099213, 'accumulated_logging_time': 7.131839036941528, 'global_step': 147069, 'preemption_count': 0}), (147961, {'train/accuracy': 0.8156249523162842, 'train/loss': 0.7296286821365356, 'validation/accuracy': 0.7245199680328369, 'validation/loss': 1.1153302192687988, 'validation/num_examples': 50000, 'test/accuracy': 0.6000000238418579, 'test/loss': 1.755022406578064, 'test/num_examples': 10000, 'score': 69365.70506572723, 'total_duration': 78053.88421607018, 'accumulated_submission_time': 69365.70506572723, 'accumulated_eval_time': 8672.559925556183, 'accumulated_logging_time': 7.190670490264893, 'global_step': 147961, 'preemption_count': 0}), (148852, {'train/accuracy': 0.7974609136581421, 'train/loss': 0.7928144931793213, 'validation/accuracy': 0.7253400087356567, 'validation/loss': 1.1223499774932861, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.7640174627304077, 'test/num_examples': 10000, 'score': 69785.79780435562, 'total_duration': 78525.49350094795, 'accumulated_submission_time': 69785.79780435562, 'accumulated_eval_time': 8723.976521015167, 'accumulated_logging_time': 7.239832878112793, 'global_step': 148852, 'preemption_count': 0}), (149745, {'train/accuracy': 0.8090038895606995, 'train/loss': 0.7413263320922852, 'validation/accuracy': 0.7287600040435791, 'validation/loss': 1.0949060916900635, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7284026145935059, 'test/num_examples': 10000, 'score': 70205.98230195045, 'total_duration': 79000.38216662407, 'accumulated_submission_time': 70205.98230195045, 'accumulated_eval_time': 8778.575244188309, 'accumulated_logging_time': 7.29470419883728, 'global_step': 149745, 'preemption_count': 0}), (150640, {'train/accuracy': 0.8140038847923279, 'train/loss': 0.7625994086265564, 'validation/accuracy': 0.7268999814987183, 'validation/loss': 1.140964388847351, 'validation/num_examples': 50000, 'test/accuracy': 0.5976999998092651, 'test/loss': 1.7766977548599243, 'test/num_examples': 10000, 'score': 70626.07793998718, 'total_duration': 79473.11308956146, 'accumulated_submission_time': 70626.07793998718, 'accumulated_eval_time': 8831.101482391357, 'accumulated_logging_time': 7.352070093154907, 'global_step': 150640, 'preemption_count': 0}), (151533, {'train/accuracy': 0.8054101467132568, 'train/loss': 0.7942066192626953, 'validation/accuracy': 0.7263000011444092, 'validation/loss': 1.1323789358139038, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.7719186544418335, 'test/num_examples': 10000, 'score': 71046.26297879219, 'total_duration': 79945.56105446815, 'accumulated_submission_time': 71046.26297879219, 'accumulated_eval_time': 8883.26012802124, 'accumulated_logging_time': 7.406408786773682, 'global_step': 151533, 'preemption_count': 0}), (152424, {'train/accuracy': 0.8122460842132568, 'train/loss': 0.738717794418335, 'validation/accuracy': 0.7300999760627747, 'validation/loss': 1.0963596105575562, 'validation/num_examples': 50000, 'test/accuracy': 0.6144000291824341, 'test/loss': 1.7185935974121094, 'test/num_examples': 10000, 'score': 71466.43354034424, 'total_duration': 80421.11392855644, 'accumulated_submission_time': 71466.43354034424, 'accumulated_eval_time': 8938.53841495514, 'accumulated_logging_time': 7.459969520568848, 'global_step': 152424, 'preemption_count': 0}), (153321, {'train/accuracy': 0.81751948595047, 'train/loss': 0.732594907283783, 'validation/accuracy': 0.7286399602890015, 'validation/loss': 1.1145501136779785, 'validation/num_examples': 50000, 'test/accuracy': 0.6084000468254089, 'test/loss': 1.746617078781128, 'test/num_examples': 10000, 'score': 71886.71799898148, 'total_duration': 80892.94577765465, 'accumulated_submission_time': 71886.71799898148, 'accumulated_eval_time': 8989.983581542969, 'accumulated_logging_time': 7.511559963226318, 'global_step': 153321, 'preemption_count': 0}), (154215, {'train/accuracy': 0.8136718273162842, 'train/loss': 0.7343711853027344, 'validation/accuracy': 0.7323399782180786, 'validation/loss': 1.0908153057098389, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.717441439628601, 'test/num_examples': 10000, 'score': 72306.71941304207, 'total_duration': 81366.14441990852, 'accumulated_submission_time': 72306.71941304207, 'accumulated_eval_time': 9043.079141378403, 'accumulated_logging_time': 7.5628063678741455, 'global_step': 154215, 'preemption_count': 0}), (155112, {'train/accuracy': 0.8171679377555847, 'train/loss': 0.7142869234085083, 'validation/accuracy': 0.7331199645996094, 'validation/loss': 1.077955961227417, 'validation/num_examples': 50000, 'test/accuracy': 0.6073000431060791, 'test/loss': 1.7077174186706543, 'test/num_examples': 10000, 'score': 72726.99397587776, 'total_duration': 81840.67090892792, 'accumulated_submission_time': 72726.99397587776, 'accumulated_eval_time': 9097.225280284882, 'accumulated_logging_time': 7.617284536361694, 'global_step': 155112, 'preemption_count': 0}), (156004, {'train/accuracy': 0.8233398199081421, 'train/loss': 0.6954863667488098, 'validation/accuracy': 0.735040009021759, 'validation/loss': 1.0758261680603027, 'validation/num_examples': 50000, 'test/accuracy': 0.612000048160553, 'test/loss': 1.6971514225006104, 'test/num_examples': 10000, 'score': 73147.06365466118, 'total_duration': 82314.92569756508, 'accumulated_submission_time': 73147.06365466118, 'accumulated_eval_time': 9151.305181026459, 'accumulated_logging_time': 7.670612573623657, 'global_step': 156004, 'preemption_count': 0}), (156897, {'train/accuracy': 0.8152148127555847, 'train/loss': 0.7171177268028259, 'validation/accuracy': 0.7366399765014648, 'validation/loss': 1.0684080123901367, 'validation/num_examples': 50000, 'test/accuracy': 0.6095000505447388, 'test/loss': 1.7041574716567993, 'test/num_examples': 10000, 'score': 73567.08738541603, 'total_duration': 82786.26455402374, 'accumulated_submission_time': 73567.08738541603, 'accumulated_eval_time': 9202.519262313843, 'accumulated_logging_time': 7.720755577087402, 'global_step': 156897, 'preemption_count': 0}), (157788, {'train/accuracy': 0.8219921588897705, 'train/loss': 0.6984254121780396, 'validation/accuracy': 0.7360799908638, 'validation/loss': 1.0686091184616089, 'validation/num_examples': 50000, 'test/accuracy': 0.6110000014305115, 'test/loss': 1.6924991607666016, 'test/num_examples': 10000, 'score': 73987.16595101357, 'total_duration': 83260.5069026947, 'accumulated_submission_time': 73987.16595101357, 'accumulated_eval_time': 9256.575710058212, 'accumulated_logging_time': 7.776738405227661, 'global_step': 157788, 'preemption_count': 0}), (158683, {'train/accuracy': 0.8245312571525574, 'train/loss': 0.7121429443359375, 'validation/accuracy': 0.7342599630355835, 'validation/loss': 1.108445644378662, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.7294844388961792, 'test/num_examples': 10000, 'score': 74407.3215315342, 'total_duration': 83733.14594745636, 'accumulated_submission_time': 74407.3215315342, 'accumulated_eval_time': 9308.945188522339, 'accumulated_logging_time': 7.840383529663086, 'global_step': 158683, 'preemption_count': 0}), (159580, {'train/accuracy': 0.8193163871765137, 'train/loss': 0.7136068344116211, 'validation/accuracy': 0.7355799674987793, 'validation/loss': 1.0825302600860596, 'validation/num_examples': 50000, 'test/accuracy': 0.6133000254631042, 'test/loss': 1.709965705871582, 'test/num_examples': 10000, 'score': 74827.42336130142, 'total_duration': 84206.77592754364, 'accumulated_submission_time': 74827.42336130142, 'accumulated_eval_time': 9362.36895108223, 'accumulated_logging_time': 7.894147157669067, 'global_step': 159580, 'preemption_count': 0}), (160474, {'train/accuracy': 0.8253905773162842, 'train/loss': 0.6981134414672852, 'validation/accuracy': 0.7397399544715881, 'validation/loss': 1.0691667795181274, 'validation/num_examples': 50000, 'test/accuracy': 0.616100013256073, 'test/loss': 1.6938591003417969, 'test/num_examples': 10000, 'score': 75247.45334744453, 'total_duration': 84680.72242164612, 'accumulated_submission_time': 75247.45334744453, 'accumulated_eval_time': 9416.179197311401, 'accumulated_logging_time': 7.9501307010650635, 'global_step': 160474, 'preemption_count': 0}), (161367, {'train/accuracy': 0.8282226324081421, 'train/loss': 0.6513522267341614, 'validation/accuracy': 0.7389799952507019, 'validation/loss': 1.0421522855758667, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.6790910959243774, 'test/num_examples': 10000, 'score': 75667.55098891258, 'total_duration': 85152.55523252487, 'accumulated_submission_time': 75667.55098891258, 'accumulated_eval_time': 9467.809502601624, 'accumulated_logging_time': 8.004290342330933, 'global_step': 161367, 'preemption_count': 0}), (162260, {'train/accuracy': 0.8219531178474426, 'train/loss': 0.7046661376953125, 'validation/accuracy': 0.739039957523346, 'validation/loss': 1.0686395168304443, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.6963045597076416, 'test/num_examples': 10000, 'score': 76087.53114652634, 'total_duration': 85624.00301170349, 'accumulated_submission_time': 76087.53114652634, 'accumulated_eval_time': 9519.163709878922, 'accumulated_logging_time': 8.066744565963745, 'global_step': 162260, 'preemption_count': 0}), (163149, {'train/accuracy': 0.8304882645606995, 'train/loss': 0.668041467666626, 'validation/accuracy': 0.7397800087928772, 'validation/loss': 1.0610032081604004, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.6925363540649414, 'test/num_examples': 10000, 'score': 76507.66302323341, 'total_duration': 86099.32566308975, 'accumulated_submission_time': 76507.66302323341, 'accumulated_eval_time': 9574.24474644661, 'accumulated_logging_time': 8.126704692840576, 'global_step': 163149, 'preemption_count': 0}), (164040, {'train/accuracy': 0.8357031345367432, 'train/loss': 0.6520687937736511, 'validation/accuracy': 0.7403199672698975, 'validation/loss': 1.0528931617736816, 'validation/num_examples': 50000, 'test/accuracy': 0.6208000183105469, 'test/loss': 1.6820333003997803, 'test/num_examples': 10000, 'score': 76927.88128042221, 'total_duration': 86571.4026389122, 'accumulated_submission_time': 76927.88128042221, 'accumulated_eval_time': 9625.999377965927, 'accumulated_logging_time': 8.179861307144165, 'global_step': 164040, 'preemption_count': 0}), (164936, {'train/accuracy': 0.8275195360183716, 'train/loss': 0.6876667737960815, 'validation/accuracy': 0.742859959602356, 'validation/loss': 1.0580904483795166, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.687680959701538, 'test/num_examples': 10000, 'score': 77348.1300997734, 'total_duration': 87044.53920483589, 'accumulated_submission_time': 77348.1300997734, 'accumulated_eval_time': 9678.775826692581, 'accumulated_logging_time': 8.240126609802246, 'global_step': 164936, 'preemption_count': 0})], 'global_step': 165309}
I0208 15:00:56.832723 140107197974336 submission_runner.py:586] Timing: 77520.39787626266
I0208 15:00:56.832815 140107197974336 submission_runner.py:588] Total number of evals: 185
I0208 15:00:56.832858 140107197974336 submission_runner.py:589] ====================
I0208 15:00:56.835360 140107197974336 submission_runner.py:673] Final imagenet_vit score: 77520.17618727684
