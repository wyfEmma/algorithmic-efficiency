python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_3 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=599091471 --max_global_steps=133333 2>&1 | tee -a /logs/wmt_jax_02-10-2024-14-47-06.log
I0210 14:47:29.915060 140144802662208 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_3/wmt_jax.
I0210 14:47:30.953753 140144802662208 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0210 14:47:30.954532 140144802662208 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0210 14:47:30.954687 140144802662208 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0210 14:47:30.955784 140144802662208 submission_runner.py:542] Using RNG seed 599091471
I0210 14:47:32.115251 140144802662208 submission_runner.py:551] --- Tuning run 1/5 ---
I0210 14:47:32.115497 140144802662208 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_1.
I0210 14:47:32.115719 140144802662208 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_1/hparams.json.
I0210 14:47:32.303259 140144802662208 submission_runner.py:206] Initializing dataset.
I0210 14:47:32.314707 140144802662208 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 14:47:32.318731 140144802662208 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0210 14:47:32.485207 140144802662208 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 14:47:34.662997 140144802662208 submission_runner.py:213] Initializing model.
I0210 14:47:44.509356 140144802662208 submission_runner.py:255] Initializing optimizer.
I0210 14:47:45.680173 140144802662208 submission_runner.py:262] Initializing metrics bundle.
I0210 14:47:45.680447 140144802662208 submission_runner.py:280] Initializing checkpoint and logger.
I0210 14:47:45.681996 140144802662208 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/wmt_jax/trial_1 with prefix checkpoint_
I0210 14:47:45.682172 140144802662208 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_1/meta_data_0.json.
I0210 14:47:45.682430 140144802662208 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0210 14:47:45.682507 140144802662208 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0210 14:47:46.058176 140144802662208 logger_utils.py:220] Unable to record git information. Continuing without it.
I0210 14:47:46.423394 140144802662208 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_1/flags_0.json.
I0210 14:47:46.434691 140144802662208 submission_runner.py:314] Starting training loop.
I0210 14:48:23.807879 139979510503168 logging_writer.py:48] [0] global_step=0, grad_norm=5.227548599243164, loss=10.960338592529297
I0210 14:48:23.824057 140144802662208 spec.py:321] Evaluating on the training split.
I0210 14:48:23.828356 140144802662208 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 14:48:23.831305 140144802662208 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0210 14:48:23.869636 140144802662208 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 14:48:31.498505 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 14:53:26.372305 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 14:53:26.376279 140144802662208 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0210 14:53:26.380351 140144802662208 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0210 14:53:26.419528 140144802662208 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0210 14:53:33.167082 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 14:58:17.902229 140144802662208 spec.py:349] Evaluating on the test split.
I0210 14:58:17.905262 140144802662208 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0210 14:58:17.909169 140144802662208 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0210 14:58:17.949527 140144802662208 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0210 14:58:20.779281 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 15:03:06.360929 140144802662208 submission_runner.py:408] Time since start: 919.93s, 	Step: 1, 	{'train/accuracy': 0.0006382566643878818, 'train/loss': 10.960665702819824, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.38932204246521, 'total_duration': 919.9261367321014, 'accumulated_submission_time': 37.38932204246521, 'accumulated_eval_time': 882.5367727279663, 'accumulated_logging_time': 0}
I0210 15:03:06.387621 139975138969344 logging_writer.py:48] [1] accumulated_eval_time=882.536773, accumulated_logging_time=0, accumulated_submission_time=37.389322, global_step=1, preemption_count=0, score=37.389322, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.966498, test/num_examples=3003, total_duration=919.926137, train/accuracy=0.000638, train/bleu=0.000000, train/loss=10.960666, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.980294, validation/num_examples=3000
I0210 15:03:41.159806 139975130576640 logging_writer.py:48] [100] global_step=100, grad_norm=0.3925250470638275, loss=8.968056678771973
I0210 15:04:15.915627 139975138969344 logging_writer.py:48] [200] global_step=200, grad_norm=0.16082531213760376, loss=8.581539154052734
I0210 15:04:50.719187 139975130576640 logging_writer.py:48] [300] global_step=300, grad_norm=0.17985734343528748, loss=8.335307121276855
I0210 15:05:25.534181 139975138969344 logging_writer.py:48] [400] global_step=400, grad_norm=0.25205564498901367, loss=8.01386547088623
I0210 15:06:00.343320 139975130576640 logging_writer.py:48] [500] global_step=500, grad_norm=0.3576721251010895, loss=7.663034915924072
I0210 15:06:35.189382 139975138969344 logging_writer.py:48] [600] global_step=600, grad_norm=0.5109453797340393, loss=7.411953926086426
I0210 15:07:10.058465 139975130576640 logging_writer.py:48] [700] global_step=700, grad_norm=0.6136749386787415, loss=7.24736213684082
I0210 15:07:44.923220 139975138969344 logging_writer.py:48] [800] global_step=800, grad_norm=0.5663554072380066, loss=6.976061820983887
I0210 15:08:19.840136 139975130576640 logging_writer.py:48] [900] global_step=900, grad_norm=0.579888641834259, loss=6.743558406829834
I0210 15:08:54.755199 139975138969344 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5630159974098206, loss=6.590373992919922
I0210 15:09:29.755689 139975130576640 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.696350634098053, loss=6.362935543060303
I0210 15:10:04.636513 139975138969344 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8131218552589417, loss=6.2095537185668945
I0210 15:10:39.508209 139975130576640 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5065469741821289, loss=6.120990753173828
I0210 15:11:14.413781 139975138969344 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7784177660942078, loss=5.982620716094971
I0210 15:11:49.288156 139975130576640 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6148735284805298, loss=5.829615592956543
I0210 15:12:24.231720 139975138969344 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6450021266937256, loss=5.666720867156982
I0210 15:12:59.158735 139975130576640 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.54518723487854, loss=5.567314624786377
I0210 15:13:34.075694 139975138969344 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.5623139142990112, loss=5.4891357421875
I0210 15:14:08.942723 139975130576640 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0547086000442505, loss=5.3503546714782715
I0210 15:14:43.809026 139975138969344 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6484910845756531, loss=5.307024002075195
I0210 15:15:18.716192 139975130576640 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1187018156051636, loss=5.157747745513916
I0210 15:15:53.602314 139975138969344 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6460539698600769, loss=5.094728946685791
I0210 15:16:28.498494 139975130576640 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7437392473220825, loss=4.9894328117370605
I0210 15:17:03.395400 139975138969344 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.2064570188522339, loss=4.8471760749816895
I0210 15:17:06.605982 140144802662208 spec.py:321] Evaluating on the training split.
I0210 15:17:09.592664 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 15:20:46.672337 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 15:20:49.356288 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 15:24:10.854655 140144802662208 spec.py:349] Evaluating on the test split.
I0210 15:24:13.535611 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 15:27:01.613107 140144802662208 submission_runner.py:408] Time since start: 2355.18s, 	Step: 2411, 	{'train/accuracy': 0.4126627445220947, 'train/loss': 4.002304553985596, 'train/bleu': 14.245377447202788, 'validation/accuracy': 0.3975647985935211, 'validation/loss': 4.118062496185303, 'validation/bleu': 9.613389479466635, 'validation/num_examples': 3000, 'test/accuracy': 0.38110512495040894, 'test/loss': 4.32146692276001, 'test/bleu': 8.015197424590168, 'test/num_examples': 3003, 'score': 877.514372587204, 'total_duration': 2355.1783118247986, 'accumulated_submission_time': 877.514372587204, 'accumulated_eval_time': 1477.5438141822815, 'accumulated_logging_time': 0.03843188285827637}
I0210 15:27:01.639443 139975130576640 logging_writer.py:48] [2411] accumulated_eval_time=1477.543814, accumulated_logging_time=0.038432, accumulated_submission_time=877.514373, global_step=2411, preemption_count=0, score=877.514373, test/accuracy=0.381105, test/bleu=8.015197, test/loss=4.321467, test/num_examples=3003, total_duration=2355.178312, train/accuracy=0.412663, train/bleu=14.245377, train/loss=4.002305, validation/accuracy=0.397565, validation/bleu=9.613389, validation/loss=4.118062, validation/num_examples=3000
I0210 15:27:32.973709 139975138969344 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8057059645652771, loss=4.867135524749756
I0210 15:28:07.809557 139975130576640 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.074629306793213, loss=4.644018173217773
I0210 15:28:42.661449 139975138969344 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.3878788948059082, loss=4.660332202911377
I0210 15:29:17.558048 139975130576640 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.342105746269226, loss=4.604974746704102
I0210 15:29:52.494420 139975138969344 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6634406447410583, loss=4.4743852615356445
I0210 15:30:27.412127 139975130576640 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6725298166275024, loss=4.434315204620361
I0210 15:31:02.306720 139975138969344 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8506165146827698, loss=4.383145809173584
I0210 15:31:37.159545 139975130576640 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7431073784828186, loss=4.345571994781494
I0210 15:32:12.048713 139975138969344 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9651064276695251, loss=4.210877895355225
I0210 15:32:46.895047 139975130576640 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7661617398262024, loss=4.210417747497559
I0210 15:33:21.769389 139975138969344 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7244096398353577, loss=4.14396858215332
I0210 15:33:56.660499 139975130576640 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6636103391647339, loss=4.169451713562012
I0210 15:34:31.581722 139975138969344 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6447309255599976, loss=4.098654747009277
I0210 15:35:06.483609 139975130576640 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6762318015098572, loss=4.060280799865723
I0210 15:35:41.334213 139975138969344 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6723310947418213, loss=4.0118536949157715
I0210 15:36:16.210529 139975130576640 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7229121327400208, loss=4.028074741363525
I0210 15:36:51.093267 139975138969344 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6332935690879822, loss=3.9788341522216797
I0210 15:37:25.983583 139975130576640 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6585279107093811, loss=3.931668758392334
I0210 15:38:00.867208 139975138969344 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.707805335521698, loss=3.8942818641662598
I0210 15:38:35.706233 139975130576640 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5523741841316223, loss=3.8605098724365234
I0210 15:39:10.543043 139975138969344 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5686829686164856, loss=3.8099186420440674
I0210 15:39:45.402911 139975130576640 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5650508403778076, loss=3.8154847621917725
I0210 15:40:20.271372 139975138969344 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5659034252166748, loss=3.7825920581817627
I0210 15:40:55.125857 139975130576640 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5085803866386414, loss=3.7526779174804688
I0210 15:41:01.817202 140144802662208 spec.py:321] Evaluating on the training split.
I0210 15:41:04.797621 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 15:43:52.476085 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 15:43:55.161250 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 15:46:44.855030 140144802662208 spec.py:349] Evaluating on the test split.
I0210 15:46:47.531143 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 15:49:24.124413 140144802662208 submission_runner.py:408] Time since start: 3697.69s, 	Step: 4821, 	{'train/accuracy': 0.5397511124610901, 'train/loss': 2.7535836696624756, 'train/bleu': 24.513584478437387, 'validation/accuracy': 0.5449033379554749, 'validation/loss': 2.7072184085845947, 'validation/bleu': 20.628340353540654, 'validation/num_examples': 3000, 'test/accuracy': 0.5461623668670654, 'test/loss': 2.733055353164673, 'test/bleu': 19.172664785009378, 'test/num_examples': 3003, 'score': 1717.5991599559784, 'total_duration': 3697.6896362304688, 'accumulated_submission_time': 1717.5991599559784, 'accumulated_eval_time': 1979.8509588241577, 'accumulated_logging_time': 0.07812023162841797}
I0210 15:49:24.139668 139975138969344 logging_writer.py:48] [4821] accumulated_eval_time=1979.850959, accumulated_logging_time=0.078120, accumulated_submission_time=1717.599160, global_step=4821, preemption_count=0, score=1717.599160, test/accuracy=0.546162, test/bleu=19.172665, test/loss=2.733055, test/num_examples=3003, total_duration=3697.689636, train/accuracy=0.539751, train/bleu=24.513584, train/loss=2.753584, validation/accuracy=0.544903, validation/bleu=20.628340, validation/loss=2.707218, validation/num_examples=3000
I0210 15:49:51.914129 139975130576640 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5931035280227661, loss=3.7777349948883057
I0210 15:50:26.689330 139975138969344 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5547078251838684, loss=3.7565388679504395
I0210 15:51:01.545074 139975130576640 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.582527756690979, loss=3.70314884185791
I0210 15:51:36.396548 139975138969344 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5369121432304382, loss=3.6902101039886475
I0210 15:52:11.230098 139975130576640 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.4780627191066742, loss=3.64874267578125
I0210 15:52:46.076312 139975138969344 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5045372843742371, loss=3.664782762527466
I0210 15:53:20.957580 139975130576640 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6062529683113098, loss=3.6937451362609863
I0210 15:53:55.788174 139975138969344 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.49247485399246216, loss=3.675201654434204
I0210 15:54:30.646486 139975130576640 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.4781562089920044, loss=3.650358200073242
I0210 15:55:05.483553 139975138969344 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5348918437957764, loss=3.6206936836242676
I0210 15:55:40.312323 139975130576640 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5837454795837402, loss=3.6176536083221436
I0210 15:56:15.140825 139975138969344 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.502457320690155, loss=3.5911879539489746
I0210 15:56:50.011542 139975130576640 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5925126075744629, loss=3.6254005432128906
I0210 15:57:24.854442 139975138969344 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.4726894795894623, loss=3.6206979751586914
I0210 15:57:59.680799 139975130576640 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4330565631389618, loss=3.5316152572631836
I0210 15:58:34.500358 139975138969344 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.46436405181884766, loss=3.550802230834961
I0210 15:59:09.322386 139975130576640 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.42809411883354187, loss=3.5744171142578125
I0210 15:59:44.180893 139975138969344 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5007066130638123, loss=3.6033291816711426
I0210 16:00:18.992472 139975130576640 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.399629145860672, loss=3.534602165222168
I0210 16:00:53.838446 139975138969344 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5077768564224243, loss=3.511838674545288
I0210 16:01:28.684514 139975130576640 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.44078943133354187, loss=3.554338216781616
I0210 16:02:03.501865 139975138969344 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4331341087818146, loss=3.5046873092651367
I0210 16:02:38.322694 139975130576640 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.409318745136261, loss=3.5500969886779785
I0210 16:03:13.155444 139975138969344 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4650873839855194, loss=3.467601776123047
I0210 16:03:24.370124 140144802662208 spec.py:321] Evaluating on the training split.
I0210 16:03:27.356071 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 16:06:16.374204 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 16:06:19.044084 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 16:09:01.561922 140144802662208 spec.py:349] Evaluating on the test split.
I0210 16:09:04.250095 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 16:11:36.545281 140144802662208 submission_runner.py:408] Time since start: 5030.11s, 	Step: 7234, 	{'train/accuracy': 0.5832327604293823, 'train/loss': 2.348278522491455, 'train/bleu': 27.40869814351565, 'validation/accuracy': 0.5864651203155518, 'validation/loss': 2.313688039779663, 'validation/bleu': 23.116123270683246, 'validation/num_examples': 3000, 'test/accuracy': 0.5904944539070129, 'test/loss': 2.3054873943328857, 'test/bleu': 22.381906824340813, 'test/num_examples': 3003, 'score': 2557.73996424675, 'total_duration': 5030.1105189323425, 'accumulated_submission_time': 2557.73996424675, 'accumulated_eval_time': 2472.0260739326477, 'accumulated_logging_time': 0.1051478385925293}
I0210 16:11:36.561593 139975130576640 logging_writer.py:48] [7234] accumulated_eval_time=2472.026074, accumulated_logging_time=0.105148, accumulated_submission_time=2557.739964, global_step=7234, preemption_count=0, score=2557.739964, test/accuracy=0.590494, test/bleu=22.381907, test/loss=2.305487, test/num_examples=3003, total_duration=5030.110519, train/accuracy=0.583233, train/bleu=27.408698, train/loss=2.348279, validation/accuracy=0.586465, validation/bleu=23.116123, validation/loss=2.313688, validation/num_examples=3000
I0210 16:11:59.802799 139975138969344 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.472279816865921, loss=3.4267892837524414
I0210 16:12:34.528587 139975130576640 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.40985485911369324, loss=3.390139102935791
I0210 16:13:09.329184 139975138969344 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.3846465051174164, loss=3.408179521560669
I0210 16:13:44.138423 139975130576640 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5143766403198242, loss=3.3850200176239014
I0210 16:14:18.929900 139975138969344 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4704464375972748, loss=3.4234282970428467
I0210 16:14:53.764932 139975130576640 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.43230873346328735, loss=3.4490268230438232
I0210 16:15:28.597387 139975138969344 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.409260094165802, loss=3.5037918090820312
I0210 16:16:03.409680 139975130576640 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.3558630347251892, loss=3.381349563598633
I0210 16:16:38.215259 139975138969344 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3592727780342102, loss=3.4032180309295654
I0210 16:17:13.048130 139975130576640 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.34859907627105713, loss=3.4089365005493164
I0210 16:17:47.843446 139975138969344 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.41517335176467896, loss=3.373161792755127
I0210 16:18:22.662565 139975130576640 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3799896240234375, loss=3.3937671184539795
I0210 16:18:57.495691 139975138969344 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3320157527923584, loss=3.274843692779541
I0210 16:19:32.289285 139975130576640 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3228687644004822, loss=3.287137746810913
I0210 16:20:07.109597 139975138969344 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3356499969959259, loss=3.3740103244781494
I0210 16:20:41.926412 139975130576640 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.4032892882823944, loss=3.2943270206451416
I0210 16:21:16.726377 139975138969344 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.4150007367134094, loss=3.320326328277588
I0210 16:21:51.509373 139975130576640 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.32754915952682495, loss=3.4274916648864746
I0210 16:22:26.307513 139975138969344 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3552957773208618, loss=3.3640332221984863
I0210 16:23:01.111481 139975130576640 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.348432332277298, loss=3.4304583072662354
I0210 16:23:35.969373 139975138969344 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.34474238753318787, loss=3.32116961479187
I0210 16:24:10.767969 139975130576640 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.32194826006889343, loss=3.377534866333008
I0210 16:24:45.576504 139975138969344 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3061966300010681, loss=3.2913477420806885
I0210 16:25:20.374153 139975130576640 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3259735107421875, loss=3.301790237426758
I0210 16:25:36.817665 140144802662208 spec.py:321] Evaluating on the training split.
I0210 16:25:39.797934 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 16:28:26.196099 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 16:28:28.883295 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 16:30:55.034055 140144802662208 spec.py:349] Evaluating on the test split.
I0210 16:30:57.738665 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 16:33:10.594043 140144802662208 submission_runner.py:408] Time since start: 6324.16s, 	Step: 9649, 	{'train/accuracy': 0.5944892764091492, 'train/loss': 2.2307252883911133, 'train/bleu': 27.885071200396293, 'validation/accuracy': 0.608386754989624, 'validation/loss': 2.125382423400879, 'validation/bleu': 24.95503798977198, 'validation/num_examples': 3000, 'test/accuracy': 0.6160014271736145, 'test/loss': 2.0950565338134766, 'test/bleu': 23.70429726840615, 'test/num_examples': 3003, 'score': 3397.9064087867737, 'total_duration': 6324.159275770187, 'accumulated_submission_time': 3397.9064087867737, 'accumulated_eval_time': 2925.8024010658264, 'accumulated_logging_time': 0.13380646705627441}
I0210 16:33:10.609730 139975138969344 logging_writer.py:48] [9649] accumulated_eval_time=2925.802401, accumulated_logging_time=0.133806, accumulated_submission_time=3397.906409, global_step=9649, preemption_count=0, score=3397.906409, test/accuracy=0.616001, test/bleu=23.704297, test/loss=2.095057, test/num_examples=3003, total_duration=6324.159276, train/accuracy=0.594489, train/bleu=27.885071, train/loss=2.230725, validation/accuracy=0.608387, validation/bleu=24.955038, validation/loss=2.125382, validation/num_examples=3000
I0210 16:33:28.635281 139975130576640 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3052666187286377, loss=3.2355875968933105
I0210 16:34:03.347041 139975138969344 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.2855355143547058, loss=3.300259828567505
I0210 16:34:38.133826 139975130576640 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3014712333679199, loss=3.2819252014160156
I0210 16:35:12.924702 139975138969344 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.30411529541015625, loss=3.2554852962493896
I0210 16:35:47.752317 139975130576640 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.34643349051475525, loss=3.3613510131835938
I0210 16:36:22.539382 139975138969344 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3137511909008026, loss=3.2769975662231445
I0210 16:36:57.356953 139975130576640 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.30692917108535767, loss=3.2743873596191406
I0210 16:37:32.155657 139975138969344 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.30265992879867554, loss=3.2707467079162598
I0210 16:38:06.922544 139975130576640 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2777225077152252, loss=3.360426425933838
I0210 16:38:41.728952 139975138969344 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.2896631062030792, loss=3.28259539604187
I0210 16:39:16.568685 139975130576640 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.2702847421169281, loss=3.3261172771453857
I0210 16:39:51.396616 139975138969344 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.26410922408103943, loss=3.2859439849853516
I0210 16:40:26.196649 139975130576640 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.25657257437705994, loss=3.253636121749878
I0210 16:41:00.995817 139975138969344 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3216131329536438, loss=3.373443365097046
I0210 16:41:35.852100 139975130576640 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.292220801115036, loss=3.193500518798828
I0210 16:42:10.653842 139975138969344 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.26737380027770996, loss=3.2042653560638428
I0210 16:42:45.422984 139975130576640 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.2636148929595947, loss=3.2574470043182373
I0210 16:43:20.210718 139975138969344 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.28549739718437195, loss=3.233229875564575
I0210 16:43:55.048110 139975130576640 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2521972060203552, loss=3.2548015117645264
I0210 16:44:29.879023 139975138969344 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.256357878446579, loss=3.218306064605713
I0210 16:45:04.693742 139975130576640 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.2826078236103058, loss=3.2771666049957275
I0210 16:45:39.496522 139975138969344 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.2684893012046814, loss=3.142275094985962
I0210 16:46:14.288719 139975130576640 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.2725159227848053, loss=3.2387681007385254
I0210 16:46:49.075684 139975138969344 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.33407050371170044, loss=3.270742177963257
I0210 16:47:10.685404 140144802662208 spec.py:321] Evaluating on the training split.
I0210 16:47:13.663213 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 16:49:57.712271 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 16:50:00.398758 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 16:52:37.526511 140144802662208 spec.py:349] Evaluating on the test split.
I0210 16:52:40.211839 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 16:55:00.097934 140144802662208 submission_runner.py:408] Time since start: 7633.66s, 	Step: 12064, 	{'train/accuracy': 0.6005798578262329, 'train/loss': 2.151870012283325, 'train/bleu': 28.89981054140853, 'validation/accuracy': 0.6196823120117188, 'validation/loss': 2.009481906890869, 'validation/bleu': 25.802299412078856, 'validation/num_examples': 3000, 'test/accuracy': 0.6272267699241638, 'test/loss': 1.9692918062210083, 'test/bleu': 24.68109616132204, 'test/num_examples': 3003, 'score': 4237.891711235046, 'total_duration': 7633.663153886795, 'accumulated_submission_time': 4237.891711235046, 'accumulated_eval_time': 3395.214864253998, 'accumulated_logging_time': 0.15960693359375}
I0210 16:55:00.113812 139975130576640 logging_writer.py:48] [12064] accumulated_eval_time=3395.214864, accumulated_logging_time=0.159607, accumulated_submission_time=4237.891711, global_step=12064, preemption_count=0, score=4237.891711, test/accuracy=0.627227, test/bleu=24.681096, test/loss=1.969292, test/num_examples=3003, total_duration=7633.663154, train/accuracy=0.600580, train/bleu=28.899811, train/loss=2.151870, validation/accuracy=0.619682, validation/bleu=25.802299, validation/loss=2.009482, validation/num_examples=3000
I0210 16:55:12.970070 139975138969344 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.26004812121391296, loss=3.1697614192962646
I0210 16:55:47.648840 139975130576640 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.30160561203956604, loss=3.2389421463012695
I0210 16:56:22.444869 139975138969344 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.24179185926914215, loss=3.1535708904266357
I0210 16:56:57.216222 139975130576640 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.25865066051483154, loss=3.1942877769470215
I0210 16:57:32.000925 139975138969344 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.25805747509002686, loss=3.281750440597534
I0210 16:58:06.786110 139975130576640 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.26463544368743896, loss=3.236955404281616
I0210 16:58:41.566275 139975138969344 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.2513737976551056, loss=3.263801336288452
I0210 16:59:16.363541 139975130576640 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.27784860134124756, loss=3.230191469192505
I0210 16:59:51.151843 139975138969344 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.24338778853416443, loss=3.138737201690674
I0210 17:00:25.908264 139975130576640 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.22688056528568268, loss=3.1480605602264404
I0210 17:01:00.692292 139975138969344 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.23473107814788818, loss=3.193376302719116
I0210 17:01:35.485683 139975130576640 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.25882238149642944, loss=3.197476387023926
I0210 17:02:10.264990 139975138969344 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2622393071651459, loss=3.167595148086548
I0210 17:02:45.043154 139975130576640 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.24577999114990234, loss=3.2129838466644287
I0210 17:03:19.832576 139975138969344 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.26636043190956116, loss=3.1724939346313477
I0210 17:03:54.638289 139975130576640 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.2686603367328644, loss=3.165519952774048
I0210 17:04:29.430737 139975138969344 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2696445882320404, loss=3.1810083389282227
I0210 17:05:04.214910 139975130576640 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2514292895793915, loss=3.098299741744995
I0210 17:05:39.135054 139975138969344 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.2572169899940491, loss=3.239983558654785
I0210 17:06:13.966301 139975130576640 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.23973295092582703, loss=3.156761407852173
I0210 17:06:48.741755 139975138969344 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.23575174808502197, loss=3.1682629585266113
I0210 17:07:23.518908 139975130576640 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.26650315523147583, loss=3.167815685272217
I0210 17:07:58.294644 139975138969344 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.28743553161621094, loss=3.100123405456543
I0210 17:08:33.083339 139975130576640 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2676851749420166, loss=3.154449462890625
I0210 17:09:00.311980 140144802662208 spec.py:321] Evaluating on the training split.
I0210 17:09:03.301611 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 17:12:24.277170 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 17:12:26.942650 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 17:14:57.841868 140144802662208 spec.py:349] Evaluating on the test split.
I0210 17:15:00.538060 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 17:17:18.536907 140144802662208 submission_runner.py:408] Time since start: 8972.10s, 	Step: 14480, 	{'train/accuracy': 0.6170651316642761, 'train/loss': 2.030705451965332, 'train/bleu': 29.54870471997391, 'validation/accuracy': 0.630109965801239, 'validation/loss': 1.9172378778457642, 'validation/bleu': 26.213816054497475, 'validation/num_examples': 3000, 'test/accuracy': 0.6392307281494141, 'test/loss': 1.868680477142334, 'test/bleu': 25.582985135267798, 'test/num_examples': 3003, 'score': 5078.001572847366, 'total_duration': 8972.102147102356, 'accumulated_submission_time': 5078.001572847366, 'accumulated_eval_time': 3893.4397599697113, 'accumulated_logging_time': 0.18552017211914062}
I0210 17:17:18.553525 139975138969344 logging_writer.py:48] [14480] accumulated_eval_time=3893.439760, accumulated_logging_time=0.185520, accumulated_submission_time=5078.001573, global_step=14480, preemption_count=0, score=5078.001573, test/accuracy=0.639231, test/bleu=25.582985, test/loss=1.868680, test/num_examples=3003, total_duration=8972.102147, train/accuracy=0.617065, train/bleu=29.548705, train/loss=2.030705, validation/accuracy=0.630110, validation/bleu=26.213816, validation/loss=1.917238, validation/num_examples=3000
I0210 17:17:25.855949 139975130576640 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.25089481472969055, loss=3.090949296951294
I0210 17:18:00.688511 139975138969344 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.2520360052585602, loss=3.1672091484069824
I0210 17:18:35.447478 139975130576640 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.24547214806079865, loss=3.189225673675537
I0210 17:19:10.248030 139975138969344 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.28479665517807007, loss=3.1375935077667236
I0210 17:19:45.068362 139975130576640 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.2624741792678833, loss=3.1697709560394287
I0210 17:20:19.847076 139975138969344 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.27270370721817017, loss=3.1038286685943604
I0210 17:20:54.650151 139975130576640 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.2980414628982544, loss=3.179926872253418
I0210 17:21:29.461036 139975138969344 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.25690561532974243, loss=3.1932265758514404
I0210 17:22:04.281793 139975130576640 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2700249254703522, loss=3.144822120666504
I0210 17:22:39.109668 139975138969344 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.25288763642311096, loss=3.0837624073028564
I0210 17:23:13.921264 139975130576640 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.22979260981082916, loss=3.07589054107666
I0210 17:23:48.692048 139975138969344 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.3434945046901703, loss=3.147327423095703
I0210 17:24:23.484302 139975130576640 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3605646789073944, loss=3.1736257076263428
I0210 17:24:58.281965 139975138969344 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2677477300167084, loss=3.097569704055786
I0210 17:25:33.050967 139975130576640 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.2920781075954437, loss=3.1415483951568604
I0210 17:26:07.842323 139975138969344 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.2988378703594208, loss=3.0200045108795166
I0210 17:26:42.683213 139975130576640 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.2842118740081787, loss=3.131408452987671
I0210 17:27:17.462982 139975138969344 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.280392050743103, loss=3.0481338500976562
I0210 17:27:52.237485 139975130576640 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.25814032554626465, loss=3.069786310195923
I0210 17:28:27.025591 139975138969344 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.27843236923217773, loss=3.0477468967437744
I0210 17:29:01.793690 139975130576640 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.29799267649650574, loss=3.1566927433013916
I0210 17:29:36.579055 139975138969344 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.2867805063724518, loss=3.0136704444885254
I0210 17:30:11.386624 139975130576640 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.350700706243515, loss=3.146448850631714
I0210 17:30:46.306675 139975138969344 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.29503530263900757, loss=3.050377368927002
I0210 17:31:18.792282 140144802662208 spec.py:321] Evaluating on the training split.
I0210 17:31:21.780890 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 17:34:18.171750 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 17:34:20.853183 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 17:37:02.890972 140144802662208 spec.py:349] Evaluating on the test split.
I0210 17:37:05.594062 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 17:39:22.262684 140144802662208 submission_runner.py:408] Time since start: 10295.83s, 	Step: 16895, 	{'train/accuracy': 0.6205698847770691, 'train/loss': 2.0030786991119385, 'train/bleu': 30.48051704857396, 'validation/accuracy': 0.6400168538093567, 'validation/loss': 1.8531957864761353, 'validation/bleu': 26.91149584465031, 'validation/num_examples': 3000, 'test/accuracy': 0.648678183555603, 'test/loss': 1.8056501150131226, 'test/bleu': 26.355332277203157, 'test/num_examples': 3003, 'score': 5918.144634008408, 'total_duration': 10295.827911376953, 'accumulated_submission_time': 5918.144634008408, 'accumulated_eval_time': 4376.910125494003, 'accumulated_logging_time': 0.2147214412689209}
I0210 17:39:22.280887 139975130576640 logging_writer.py:48] [16895] accumulated_eval_time=4376.910125, accumulated_logging_time=0.214721, accumulated_submission_time=5918.144634, global_step=16895, preemption_count=0, score=5918.144634, test/accuracy=0.648678, test/bleu=26.355332, test/loss=1.805650, test/num_examples=3003, total_duration=10295.827911, train/accuracy=0.620570, train/bleu=30.480517, train/loss=2.003079, validation/accuracy=0.640017, validation/bleu=26.911496, validation/loss=1.853196, validation/num_examples=3000
I0210 17:39:24.389758 139975138969344 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.2952533960342407, loss=3.1278254985809326
I0210 17:39:59.036994 139975130576640 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2757393717765808, loss=3.206655740737915
I0210 17:40:33.757600 139975138969344 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2784336805343628, loss=3.1288633346557617
I0210 17:41:08.522321 139975130576640 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.28093650937080383, loss=3.0417184829711914
I0210 17:41:43.319566 139975138969344 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3298672139644623, loss=3.113344430923462
I0210 17:42:18.108201 139975130576640 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.2766452729701996, loss=3.047348976135254
I0210 17:42:52.887874 139975138969344 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.28681668639183044, loss=3.089740514755249
I0210 17:43:27.683043 139975130576640 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.2992739975452423, loss=3.117866039276123
I0210 17:44:02.486875 139975138969344 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.292901873588562, loss=3.0494232177734375
I0210 17:44:37.338262 139975130576640 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.30405786633491516, loss=3.0771732330322266
I0210 17:45:12.124067 139975138969344 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.314297616481781, loss=3.0552914142608643
I0210 17:45:46.934403 139975130576640 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3208718001842499, loss=3.076965808868408
I0210 17:46:21.759362 139975138969344 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.27747634053230286, loss=3.0225138664245605
I0210 17:46:56.594469 139975130576640 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.33193129301071167, loss=3.161677837371826
I0210 17:47:31.354160 139975138969344 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.2787584662437439, loss=3.004356861114502
I0210 17:48:06.118203 139975130576640 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.32286369800567627, loss=3.077329635620117
I0210 17:48:40.901585 139975138969344 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.32039451599121094, loss=3.0780365467071533
I0210 17:49:15.712420 139975130576640 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3162247836589813, loss=3.0876026153564453
I0210 17:49:50.508413 139975138969344 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3585866391658783, loss=3.0108561515808105
I0210 17:50:25.291209 139975130576640 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.33488449454307556, loss=3.093548059463501
I0210 17:51:00.070863 139975138969344 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.329345166683197, loss=2.9899370670318604
I0210 17:51:34.880735 139975130576640 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.38035696744918823, loss=3.0012435913085938
I0210 17:52:09.667480 139975138969344 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3728669583797455, loss=3.0697686672210693
I0210 17:52:44.436708 139975130576640 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3186057209968567, loss=3.0394601821899414
I0210 17:53:19.233521 139975138969344 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3406315743923187, loss=2.9742846488952637
I0210 17:53:22.436645 140144802662208 spec.py:321] Evaluating on the training split.
I0210 17:53:25.413593 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 17:56:18.773709 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 17:56:21.451285 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 17:59:00.702399 140144802662208 spec.py:349] Evaluating on the test split.
I0210 17:59:03.395654 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 18:01:31.414205 140144802662208 submission_runner.py:408] Time since start: 11624.98s, 	Step: 19311, 	{'train/accuracy': 0.6361596584320068, 'train/loss': 1.8797343969345093, 'train/bleu': 30.912875387106396, 'validation/accuracy': 0.6456212401390076, 'validation/loss': 1.8102855682373047, 'validation/bleu': 27.351768905608047, 'validation/num_examples': 3000, 'test/accuracy': 0.6544535756111145, 'test/loss': 1.754570722579956, 'test/bleu': 26.59917892660872, 'test/num_examples': 3003, 'score': 6758.210176944733, 'total_duration': 11624.979398727417, 'accumulated_submission_time': 6758.210176944733, 'accumulated_eval_time': 4865.887587070465, 'accumulated_logging_time': 0.24300289154052734}
I0210 18:01:31.431047 139975130576640 logging_writer.py:48] [19311] accumulated_eval_time=4865.887587, accumulated_logging_time=0.243003, accumulated_submission_time=6758.210177, global_step=19311, preemption_count=0, score=6758.210177, test/accuracy=0.654454, test/bleu=26.599179, test/loss=1.754571, test/num_examples=3003, total_duration=11624.979399, train/accuracy=0.636160, train/bleu=30.912875, train/loss=1.879734, validation/accuracy=0.645621, validation/bleu=27.351769, validation/loss=1.810286, validation/num_examples=3000
I0210 18:02:02.639409 139975138969344 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.36272987723350525, loss=3.0846657752990723
I0210 18:02:37.361096 139975130576640 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.38342776894569397, loss=2.978440761566162
I0210 18:03:12.142845 139975138969344 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3337657153606415, loss=3.1306052207946777
I0210 18:03:46.947759 139975130576640 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3511100113391876, loss=3.0518085956573486
I0210 18:04:21.832202 139975138969344 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.36861222982406616, loss=3.0499439239501953
I0210 18:04:56.627590 139975130576640 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.36824968457221985, loss=3.0933761596679688
I0210 18:05:31.407052 139975138969344 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.31248578429222107, loss=3.041828155517578
I0210 18:06:06.196215 139975130576640 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.3291934132575989, loss=2.972761869430542
I0210 18:06:40.982305 139975138969344 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.4006185829639435, loss=2.998131036758423
I0210 18:07:15.756393 139975130576640 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.42912861704826355, loss=3.0704433917999268
I0210 18:07:50.507195 139975138969344 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.32168665528297424, loss=3.1021728515625
I0210 18:08:25.284890 139975130576640 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3205665349960327, loss=3.0164682865142822
I0210 18:09:00.071300 139975138969344 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3140386939048767, loss=2.9405884742736816
I0210 18:09:34.838516 139975130576640 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.3722720444202423, loss=2.9591891765594482
I0210 18:10:09.619712 139975138969344 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.3093337118625641, loss=3.037590503692627
I0210 18:10:44.404633 139975130576640 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3324505090713501, loss=3.0079171657562256
I0210 18:11:19.223856 139975138969344 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.33961036801338196, loss=3.020125150680542
I0210 18:11:54.027896 139975130576640 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4291062653064728, loss=3.041262626647949
I0210 18:12:28.818810 139975138969344 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.334176629781723, loss=3.0664868354797363
I0210 18:13:03.598230 139975130576640 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.5206797122955322, loss=3.0809640884399414
I0210 18:13:38.361827 139975138969344 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.5673959255218506, loss=5.064516067504883
I0210 18:14:13.094340 139975130576640 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8150994777679443, loss=4.869471073150635
I0210 18:14:47.800952 139975138969344 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.34181827306747437, loss=4.772241115570068
I0210 18:15:22.625629 139975130576640 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5021196007728577, loss=4.753514766693115
I0210 18:15:31.732176 140144802662208 spec.py:321] Evaluating on the training split.
I0210 18:15:34.710275 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 18:18:34.446111 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 18:18:37.138959 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 18:21:32.976630 140144802662208 spec.py:349] Evaluating on the test split.
I0210 18:21:35.671760 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 18:24:27.708869 140144802662208 submission_runner.py:408] Time since start: 13001.27s, 	Step: 21728, 	{'train/accuracy': 0.3390825092792511, 'train/loss': 3.9086546897888184, 'train/bleu': 0.5862118968036852, 'validation/accuracy': 0.3057122528553009, 'validation/loss': 4.292892932891846, 'validation/bleu': 0.1320855284362729, 'validation/num_examples': 3000, 'test/accuracy': 0.2937540113925934, 'test/loss': 4.477366924285889, 'test/bleu': 0.14031335774959658, 'test/num_examples': 3003, 'score': 7598.422609567642, 'total_duration': 13001.2740752697, 'accumulated_submission_time': 7598.422609567642, 'accumulated_eval_time': 5401.86420583725, 'accumulated_logging_time': 0.27010011672973633}
I0210 18:24:27.727099 139975138969344 logging_writer.py:48] [21728] accumulated_eval_time=5401.864206, accumulated_logging_time=0.270100, accumulated_submission_time=7598.422610, global_step=21728, preemption_count=0, score=7598.422610, test/accuracy=0.293754, test/bleu=0.140313, test/loss=4.477367, test/num_examples=3003, total_duration=13001.274075, train/accuracy=0.339083, train/bleu=0.586212, train/loss=3.908655, validation/accuracy=0.305712, validation/bleu=0.132086, validation/loss=4.292893, validation/num_examples=3000
I0210 18:24:52.987517 139975130576640 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.4427677392959595, loss=4.7420172691345215
I0210 18:25:27.589598 139975138969344 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5311369299888611, loss=4.730261325836182
I0210 18:26:02.265113 139975130576640 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7017813920974731, loss=4.792572975158691
I0210 18:26:36.964819 139975138969344 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5649475455284119, loss=4.702534198760986
I0210 18:27:11.685196 139975130576640 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9093417525291443, loss=4.718652248382568
I0210 18:27:46.396805 139975138969344 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6642449498176575, loss=4.624328136444092
I0210 18:28:21.107827 139975130576640 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.412062406539917, loss=4.666037082672119
I0210 18:28:55.858627 139975138969344 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.996959924697876, loss=4.647830963134766
I0210 18:29:30.569389 139975130576640 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.0438146591186523, loss=4.586123943328857
I0210 18:30:05.343854 139975138969344 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.9158036708831787, loss=4.361886024475098
I0210 18:30:40.098262 139975130576640 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6807435750961304, loss=3.3157944679260254
I0210 18:31:14.849268 139975138969344 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3927096128463745, loss=3.1905996799468994
I0210 18:31:49.605571 139975130576640 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.44743654131889343, loss=3.1031675338745117
I0210 18:32:24.396961 139975138969344 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.34519529342651367, loss=3.081324338912964
I0210 18:32:59.159198 139975130576640 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.32571807503700256, loss=3.096550703048706
I0210 18:33:33.906514 139975138969344 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.32447510957717896, loss=3.0638439655303955
I0210 18:34:08.720287 139975130576640 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.33577463030815125, loss=3.1138813495635986
I0210 18:34:43.484048 139975138969344 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.3278729319572449, loss=3.026123046875
I0210 18:35:18.242280 139975130576640 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.3664407730102539, loss=3.1123437881469727
I0210 18:35:52.999731 139975138969344 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.428847998380661, loss=3.0261173248291016
I0210 18:36:27.771865 139975130576640 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.3565795123577118, loss=3.0322511196136475
I0210 18:37:02.523358 139975138969344 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.4516766369342804, loss=3.073529005050659
I0210 18:37:37.278617 139975130576640 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.3207673132419586, loss=3.0370259284973145
I0210 18:38:12.038904 139975138969344 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.35915589332580566, loss=3.0416500568389893
I0210 18:38:27.748029 140144802662208 spec.py:321] Evaluating on the training split.
I0210 18:38:30.720410 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 18:41:23.123985 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 18:41:25.817027 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 18:44:04.371900 140144802662208 spec.py:349] Evaluating on the test split.
I0210 18:44:07.066393 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 18:46:53.722223 140144802662208 submission_runner.py:408] Time since start: 14347.29s, 	Step: 24147, 	{'train/accuracy': 0.6233353614807129, 'train/loss': 1.9733836650848389, 'train/bleu': 30.139503659176032, 'validation/accuracy': 0.6435753703117371, 'validation/loss': 1.819201946258545, 'validation/bleu': 27.412554254769713, 'validation/num_examples': 3000, 'test/accuracy': 0.6543838381767273, 'test/loss': 1.7676335573196411, 'test/bleu': 26.747935955855187, 'test/num_examples': 3003, 'score': 8438.351751565933, 'total_duration': 14347.287384033203, 'accumulated_submission_time': 8438.351751565933, 'accumulated_eval_time': 5907.838281869888, 'accumulated_logging_time': 0.300400972366333}
I0210 18:46:53.743602 139975130576640 logging_writer.py:48] [24147] accumulated_eval_time=5907.838282, accumulated_logging_time=0.300401, accumulated_submission_time=8438.351752, global_step=24147, preemption_count=0, score=8438.351752, test/accuracy=0.654384, test/bleu=26.747936, test/loss=1.767634, test/num_examples=3003, total_duration=14347.287384, train/accuracy=0.623335, train/bleu=30.139504, train/loss=1.973384, validation/accuracy=0.643575, validation/bleu=27.412554, validation/loss=1.819202, validation/num_examples=3000
I0210 18:47:12.466483 139975138969344 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3746841549873352, loss=3.036393880844116
I0210 18:47:47.100630 139975130576640 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.30163559317588806, loss=3.0482370853424072
I0210 18:48:21.831798 139975138969344 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.5027949213981628, loss=2.9839882850646973
I0210 18:48:56.587737 139975130576640 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.36979153752326965, loss=2.979511022567749
I0210 18:49:31.326533 139975138969344 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.32602065801620483, loss=3.0649361610412598
I0210 18:50:06.092317 139975130576640 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.3796490728855133, loss=2.9969263076782227
I0210 18:50:40.912341 139975138969344 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.3398773670196533, loss=3.0049893856048584
I0210 18:51:15.725969 139975130576640 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3307112157344818, loss=3.0664191246032715
I0210 18:51:50.470668 139975138969344 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.37388697266578674, loss=2.958653211593628
I0210 18:52:25.246891 139975130576640 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.41807615756988525, loss=3.0535969734191895
I0210 18:52:59.991516 139975138969344 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.39376401901245117, loss=3.084469795227051
I0210 18:53:34.747655 139975130576640 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.37806153297424316, loss=3.000039577484131
I0210 18:54:09.549721 139975138969344 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.33637502789497375, loss=2.9799251556396484
I0210 18:54:44.303380 139975130576640 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.39228618144989014, loss=2.963064193725586
I0210 18:55:19.074292 139975138969344 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.34207531809806824, loss=2.9999985694885254
I0210 18:55:53.936004 139975130576640 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.4122042655944824, loss=3.002992630004883
I0210 18:56:28.671105 139975138969344 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.33956974744796753, loss=3.0632309913635254
I0210 18:57:03.449877 139975130576640 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.4285835325717926, loss=2.9604740142822266
I0210 18:57:38.238619 139975138969344 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.3986234664916992, loss=3.0237855911254883
I0210 18:58:13.006734 139975130576640 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.42342039942741394, loss=3.047318935394287
I0210 18:58:47.766319 139975138969344 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.3355696499347687, loss=3.0540409088134766
I0210 18:59:22.562598 139975130576640 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.3500496447086334, loss=3.026291847229004
I0210 18:59:57.374612 139975138969344 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.4032098650932312, loss=3.0083587169647217
I0210 19:00:32.167214 139975130576640 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.340937077999115, loss=3.020963668823242
I0210 19:00:53.779014 140144802662208 spec.py:321] Evaluating on the training split.
I0210 19:00:56.749353 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 19:03:50.725595 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 19:03:53.411499 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 19:06:47.930698 140144802662208 spec.py:349] Evaluating on the test split.
I0210 19:06:50.604671 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 19:09:36.978049 140144802662208 submission_runner.py:408] Time since start: 15710.54s, 	Step: 26564, 	{'train/accuracy': 0.6321825981140137, 'train/loss': 1.8988949060440063, 'train/bleu': 30.340070514809657, 'validation/accuracy': 0.648423433303833, 'validation/loss': 1.7781262397766113, 'validation/bleu': 27.29769121173301, 'validation/num_examples': 3000, 'test/accuracy': 0.6584277749061584, 'test/loss': 1.7201982736587524, 'test/bleu': 26.875802443570254, 'test/num_examples': 3003, 'score': 9278.293791770935, 'total_duration': 15710.543276309967, 'accumulated_submission_time': 9278.293791770935, 'accumulated_eval_time': 6431.037258863449, 'accumulated_logging_time': 0.3333091735839844}
I0210 19:09:36.996086 139975138969344 logging_writer.py:48] [26564] accumulated_eval_time=6431.037259, accumulated_logging_time=0.333309, accumulated_submission_time=9278.293792, global_step=26564, preemption_count=0, score=9278.293792, test/accuracy=0.658428, test/bleu=26.875802, test/loss=1.720198, test/num_examples=3003, total_duration=15710.543276, train/accuracy=0.632183, train/bleu=30.340071, train/loss=1.898895, validation/accuracy=0.648423, validation/bleu=27.297691, validation/loss=1.778126, validation/num_examples=3000
I0210 19:09:49.818402 139975130576640 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.41355857253074646, loss=3.0210654735565186
I0210 19:10:24.458879 139975138969344 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3822535276412964, loss=2.9592807292938232
I0210 19:10:59.191246 139975130576640 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.3881177604198456, loss=3.01247239112854
I0210 19:11:33.952851 139975138969344 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.46816015243530273, loss=3.01546573638916
I0210 19:12:08.719982 139975130576640 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.3822689950466156, loss=3.0246071815490723
I0210 19:12:43.466306 139975138969344 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.45034071803092957, loss=2.9664664268493652
I0210 19:13:18.224504 139975130576640 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.36564043164253235, loss=3.0368473529815674
I0210 19:13:52.983107 139975138969344 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.3784796893596649, loss=3.039443016052246
I0210 19:14:27.761103 139975130576640 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.3323136866092682, loss=2.9809513092041016
I0210 19:15:02.558981 139975138969344 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.42185303568840027, loss=3.078864812850952
I0210 19:15:37.360193 139975130576640 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.36728742718696594, loss=3.0374040603637695
I0210 19:16:12.260783 139975138969344 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.3549768626689911, loss=3.076833963394165
I0210 19:16:47.029099 139975130576640 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.42330050468444824, loss=3.003378391265869
I0210 19:17:21.798650 139975138969344 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.3444846570491791, loss=3.0256662368774414
I0210 19:17:56.545669 139975130576640 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4160923659801483, loss=3.04815411567688
I0210 19:18:31.284294 139975138969344 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.41448768973350525, loss=3.045595645904541
I0210 19:19:06.022381 139975130576640 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.47342216968536377, loss=2.9484801292419434
I0210 19:19:40.790526 139975138969344 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.34877461194992065, loss=2.935539484024048
I0210 19:20:15.528588 139975130576640 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.45053625106811523, loss=3.0040676593780518
I0210 19:20:50.312524 139975138969344 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.40586036443710327, loss=3.042361259460449
I0210 19:21:25.083395 139975130576640 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.3361353874206543, loss=2.9363272190093994
I0210 19:21:59.825401 139975138969344 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.3685286343097687, loss=3.0377180576324463
I0210 19:22:34.577721 139975130576640 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3639618754386902, loss=2.9921672344207764
I0210 19:23:09.340780 139975138969344 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.36899110674858093, loss=3.0351171493530273
I0210 19:23:37.219127 140144802662208 spec.py:321] Evaluating on the training split.
I0210 19:23:40.188616 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 19:26:37.264829 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 19:26:39.947227 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 19:29:20.223965 140144802662208 spec.py:349] Evaluating on the test split.
I0210 19:29:22.922880 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 19:31:48.607753 140144802662208 submission_runner.py:408] Time since start: 17042.17s, 	Step: 28982, 	{'train/accuracy': 0.6355407238006592, 'train/loss': 1.8854094743728638, 'train/bleu': 30.347952863442696, 'validation/accuracy': 0.6500105261802673, 'validation/loss': 1.75833261013031, 'validation/bleu': 27.48807983750271, 'validation/num_examples': 3000, 'test/accuracy': 0.6632153987884521, 'test/loss': 1.6976944208145142, 'test/bleu': 27.12441943548814, 'test/num_examples': 3003, 'score': 10118.426603794098, 'total_duration': 17042.172969341278, 'accumulated_submission_time': 10118.426603794098, 'accumulated_eval_time': 6922.425815820694, 'accumulated_logging_time': 0.36264824867248535}
I0210 19:31:48.626454 139975130576640 logging_writer.py:48] [28982] accumulated_eval_time=6922.425816, accumulated_logging_time=0.362648, accumulated_submission_time=10118.426604, global_step=28982, preemption_count=0, score=10118.426604, test/accuracy=0.663215, test/bleu=27.124419, test/loss=1.697694, test/num_examples=3003, total_duration=17042.172969, train/accuracy=0.635541, train/bleu=30.347953, train/loss=1.885409, validation/accuracy=0.650011, validation/bleu=27.488080, validation/loss=1.758333, validation/num_examples=3000
I0210 19:31:55.221441 139975138969344 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.4453940987586975, loss=3.0437183380126953
I0210 19:32:29.855262 139975130576640 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.3980385661125183, loss=2.992387294769287
I0210 19:33:04.549727 139975138969344 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.3904833197593689, loss=3.0154101848602295
I0210 19:33:39.380193 139975130576640 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.35899001359939575, loss=3.034381151199341
I0210 19:34:14.148120 139975138969344 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.3603465259075165, loss=3.007037401199341
I0210 19:34:48.911520 139975130576640 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.31119847297668457, loss=2.9052393436431885
I0210 19:35:23.659058 139975138969344 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.3351239562034607, loss=2.943227529525757
I0210 19:35:58.399965 139975130576640 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.5207687020301819, loss=3.054346799850464
I0210 19:36:33.152853 139975138969344 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.4195081889629364, loss=3.010329484939575
I0210 19:37:07.901480 139975130576640 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.34930068254470825, loss=2.998687744140625
I0210 19:37:42.642374 139975138969344 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.3693409860134125, loss=2.9903695583343506
I0210 19:38:17.407910 139975130576640 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.36686211824417114, loss=3.0013651847839355
I0210 19:38:52.157748 139975138969344 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.38364940881729126, loss=2.9958083629608154
I0210 19:39:26.906594 139975130576640 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.34935688972473145, loss=2.962303400039673
I0210 19:40:01.656294 139975138969344 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.42953354120254517, loss=2.9639432430267334
I0210 19:40:36.412751 139975130576640 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.38933679461479187, loss=2.973226547241211
I0210 19:41:11.163452 139975138969344 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.4793725609779358, loss=3.008068323135376
I0210 19:41:45.923287 139975130576640 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.3428243398666382, loss=2.9780397415161133
I0210 19:42:20.685119 139975138969344 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.36541029810905457, loss=2.944427013397217
I0210 19:42:55.480156 139975130576640 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3781689703464508, loss=2.9478461742401123
I0210 19:43:30.245497 139975138969344 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.32738059759140015, loss=2.9209864139556885
I0210 19:44:05.007507 139975130576640 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.3321775496006012, loss=2.9722671508789062
I0210 19:44:39.809747 139975138969344 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.359745591878891, loss=3.0173227787017822
I0210 19:45:14.573687 139975130576640 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.8918628692626953, loss=3.0426928997039795
I0210 19:45:48.684037 140144802662208 spec.py:321] Evaluating on the training split.
I0210 19:45:51.659215 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 19:48:47.399592 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 19:48:50.073118 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 19:51:59.515934 140144802662208 spec.py:349] Evaluating on the test split.
I0210 19:52:02.205662 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 19:54:44.976260 140144802662208 submission_runner.py:408] Time since start: 18418.54s, 	Step: 31400, 	{'train/accuracy': 0.6615469455718994, 'train/loss': 1.7062214612960815, 'train/bleu': 32.51895289150185, 'validation/accuracy': 0.6527507305145264, 'validation/loss': 1.750123143196106, 'validation/bleu': 27.932306559997773, 'validation/num_examples': 3000, 'test/accuracy': 0.664307713508606, 'test/loss': 1.6905139684677124, 'test/bleu': 27.281990486780842, 'test/num_examples': 3003, 'score': 10958.394407272339, 'total_duration': 18418.541491508484, 'accumulated_submission_time': 10958.394407272339, 'accumulated_eval_time': 7458.717993736267, 'accumulated_logging_time': 0.391817569732666}
I0210 19:54:44.995201 139975138969344 logging_writer.py:48] [31400] accumulated_eval_time=7458.717994, accumulated_logging_time=0.391818, accumulated_submission_time=10958.394407, global_step=31400, preemption_count=0, score=10958.394407, test/accuracy=0.664308, test/bleu=27.281990, test/loss=1.690514, test/num_examples=3003, total_duration=18418.541492, train/accuracy=0.661547, train/bleu=32.518953, train/loss=1.706221, validation/accuracy=0.652751, validation/bleu=27.932307, validation/loss=1.750123, validation/num_examples=3000
I0210 19:54:45.364006 139975130576640 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.4189612567424774, loss=3.032223701477051
I0210 19:55:19.994522 139975138969344 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.2113571166992188, loss=3.044214963912964
I0210 19:55:54.824901 139975130576640 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.3895269632339478, loss=2.966064214706421
I0210 19:56:29.579442 139975138969344 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.49283942580223083, loss=3.04437518119812
I0210 19:57:04.342927 139975130576640 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.3555773198604584, loss=3.0330922603607178
I0210 19:57:39.064565 139975138969344 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.3518662750720978, loss=3.027148485183716
I0210 19:58:13.845749 139975130576640 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.39408037066459656, loss=3.0076420307159424
I0210 19:58:48.572495 139975138969344 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.35631927847862244, loss=3.00575590133667
I0210 19:59:23.332488 139975130576640 logging_writer.py:48] [32200] global_step=32200, grad_norm=4.066104888916016, loss=3.0073280334472656
I0210 19:59:58.080644 139975138969344 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.33170780539512634, loss=2.9187488555908203
I0210 20:00:32.811621 139975130576640 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.3765268921852112, loss=3.0152676105499268
I0210 20:01:07.590492 139975138969344 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.37480029463768005, loss=2.8652827739715576
I0210 20:01:42.472635 139975130576640 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.399077445268631, loss=2.987473726272583
I0210 20:02:17.278854 139975138969344 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3710106909275055, loss=3.0073330402374268
I0210 20:02:52.069847 139975130576640 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.38980185985565186, loss=3.0760817527770996
I0210 20:03:26.861888 139975138969344 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.37257397174835205, loss=2.954683780670166
I0210 20:04:01.616309 139975130576640 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.4190247058868408, loss=2.923246145248413
I0210 20:04:36.370356 139975138969344 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.3466333746910095, loss=2.995711088180542
I0210 20:05:11.121256 139975130576640 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.3755096197128296, loss=2.988585948944092
I0210 20:05:45.877167 139975138969344 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.3425430655479431, loss=2.8929505348205566
I0210 20:06:20.599282 139975130576640 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.45669013261795044, loss=3.0269057750701904
I0210 20:06:55.341565 139975138969344 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.3695048689842224, loss=2.9735846519470215
I0210 20:07:30.107240 139975130576640 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.4535094201564789, loss=3.0310842990875244
I0210 20:08:04.820002 139975138969344 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.37483102083206177, loss=3.022019863128662
I0210 20:08:39.578628 139975130576640 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.371868371963501, loss=2.967590808868408
I0210 20:08:45.217705 140144802662208 spec.py:321] Evaluating on the training split.
I0210 20:08:48.189028 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 20:11:53.272255 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 20:11:55.946562 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 20:14:30.471341 140144802662208 spec.py:349] Evaluating on the test split.
I0210 20:14:33.170475 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 20:16:56.834861 140144802662208 submission_runner.py:408] Time since start: 19750.40s, 	Step: 33818, 	{'train/accuracy': 0.636962890625, 'train/loss': 1.8799915313720703, 'train/bleu': 30.93731275058393, 'validation/accuracy': 0.6577227711677551, 'validation/loss': 1.7262121438980103, 'validation/bleu': 28.16495224589458, 'validation/num_examples': 3000, 'test/accuracy': 0.6667364239692688, 'test/loss': 1.6695778369903564, 'test/bleu': 27.411963702224377, 'test/num_examples': 3003, 'score': 11798.52327799797, 'total_duration': 19750.400065422058, 'accumulated_submission_time': 11798.52327799797, 'accumulated_eval_time': 7950.335062503815, 'accumulated_logging_time': 0.42200541496276855}
I0210 20:16:56.854951 139975138969344 logging_writer.py:48] [33818] accumulated_eval_time=7950.335063, accumulated_logging_time=0.422005, accumulated_submission_time=11798.523278, global_step=33818, preemption_count=0, score=11798.523278, test/accuracy=0.666736, test/bleu=27.411964, test/loss=1.669578, test/num_examples=3003, total_duration=19750.400065, train/accuracy=0.636963, train/bleu=30.937313, train/loss=1.879992, validation/accuracy=0.657723, validation/bleu=28.164952, validation/loss=1.726212, validation/num_examples=3000
I0210 20:17:25.612280 139975130576640 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.47494128346443176, loss=3.0033528804779053
I0210 20:18:00.297840 139975138969344 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.3557429611682892, loss=2.96919846534729
I0210 20:18:35.081683 139975130576640 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.8701068162918091, loss=3.2164387702941895
I0210 20:19:09.890322 139975138969344 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.4113904535770416, loss=3.0251471996307373
I0210 20:19:44.611618 139975130576640 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.3985612392425537, loss=2.9993174076080322
I0210 20:20:19.337379 139975138969344 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.3491533398628235, loss=2.9370367527008057
I0210 20:20:54.097005 139975130576640 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.3375331163406372, loss=2.956265926361084
I0210 20:21:28.842681 139975138969344 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.376032292842865, loss=2.9662742614746094
I0210 20:22:03.620636 139975130576640 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.37138158082962036, loss=2.9510338306427
I0210 20:22:38.360838 139975138969344 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.3603372871875763, loss=2.856416702270508
I0210 20:23:13.095531 139975130576640 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3353974521160126, loss=2.9603261947631836
I0210 20:23:47.871144 139975138969344 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.3390265107154846, loss=2.9234299659729004
I0210 20:24:22.603558 139975130576640 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.34349945187568665, loss=2.9440646171569824
I0210 20:24:57.365293 139975138969344 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.3609969913959503, loss=2.9621453285217285
I0210 20:25:32.114485 139975130576640 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.31451278924942017, loss=2.895585060119629
I0210 20:26:06.858107 139975138969344 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.33623987436294556, loss=2.9882924556732178
I0210 20:26:41.616077 139975130576640 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.3561609387397766, loss=2.9899988174438477
I0210 20:27:16.392050 139975138969344 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.3824460506439209, loss=2.9672982692718506
I0210 20:27:51.147297 139975130576640 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.3654797673225403, loss=2.8926055431365967
I0210 20:28:25.895224 139975138969344 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.3798445165157318, loss=3.0032689571380615
I0210 20:29:00.634133 139975130576640 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.4257570803165436, loss=3.0290348529815674
I0210 20:29:35.387855 139975138969344 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.3880362808704376, loss=2.9183084964752197
I0210 20:30:10.140413 139975130576640 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.3366508185863495, loss=2.948946952819824
I0210 20:30:44.871621 139975138969344 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.34136512875556946, loss=2.955289125442505
I0210 20:30:57.106703 140144802662208 spec.py:321] Evaluating on the training split.
I0210 20:31:00.076263 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 20:33:53.782095 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 20:33:56.464485 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 20:36:27.866515 140144802662208 spec.py:349] Evaluating on the test split.
I0210 20:36:30.555922 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 20:39:00.857814 140144802662208 submission_runner.py:408] Time since start: 21074.42s, 	Step: 36237, 	{'train/accuracy': 0.6327520608901978, 'train/loss': 1.8985131978988647, 'train/bleu': 30.885318130356406, 'validation/accuracy': 0.6562100648880005, 'validation/loss': 1.7289568185806274, 'validation/bleu': 28.097237758642365, 'validation/num_examples': 3000, 'test/accuracy': 0.6673523187637329, 'test/loss': 1.6683061122894287, 'test/bleu': 27.305733465104947, 'test/num_examples': 3003, 'score': 12638.686345100403, 'total_duration': 21074.423023223877, 'accumulated_submission_time': 12638.686345100403, 'accumulated_eval_time': 8434.08610200882, 'accumulated_logging_time': 0.4524543285369873}
I0210 20:39:00.880994 139975130576640 logging_writer.py:48] [36237] accumulated_eval_time=8434.086102, accumulated_logging_time=0.452454, accumulated_submission_time=12638.686345, global_step=36237, preemption_count=0, score=12638.686345, test/accuracy=0.667352, test/bleu=27.305733, test/loss=1.668306, test/num_examples=3003, total_duration=21074.423023, train/accuracy=0.632752, train/bleu=30.885318, train/loss=1.898513, validation/accuracy=0.656210, validation/bleu=28.097238, validation/loss=1.728957, validation/num_examples=3000
I0210 20:39:23.082373 139975138969344 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.37643110752105713, loss=2.9413530826568604
I0210 20:39:57.735664 139975130576640 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.4108891487121582, loss=2.9411098957061768
I0210 20:40:32.477188 139975138969344 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.38095197081565857, loss=2.9245173931121826
I0210 20:41:07.216681 139975130576640 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.3570151627063751, loss=2.9147379398345947
I0210 20:41:41.958959 139975138969344 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.3566676080226898, loss=2.9084277153015137
I0210 20:42:16.727606 139975130576640 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.41275349259376526, loss=2.976186513900757
I0210 20:42:51.478879 139975138969344 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.3515697717666626, loss=3.0275449752807617
I0210 20:43:26.203216 139975130576640 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.34943997859954834, loss=2.9389264583587646
I0210 20:44:00.945086 139975138969344 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3753625750541687, loss=2.9884228706359863
I0210 20:44:35.718247 139975130576640 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.3749074935913086, loss=2.948160171508789
I0210 20:45:10.495813 139975138969344 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.4072926342487335, loss=3.0190491676330566
I0210 20:45:45.248019 139975130576640 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.36025533080101013, loss=2.9480326175689697
I0210 20:46:19.987599 139975138969344 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.40466073155403137, loss=2.964975357055664
I0210 20:46:54.754832 139975130576640 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.34844863414764404, loss=2.975041627883911
I0210 20:47:29.526923 139975138969344 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.35564345121383667, loss=2.914897918701172
I0210 20:48:04.322528 139975130576640 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.34621772170066833, loss=2.898176670074463
I0210 20:48:39.073787 139975138969344 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.4113404452800751, loss=3.0387775897979736
I0210 20:49:13.817353 139975130576640 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.3991624116897583, loss=2.949160575866699
I0210 20:49:48.573339 139975138969344 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.3613168001174927, loss=2.9911916255950928
I0210 20:50:23.316981 139975130576640 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.34250783920288086, loss=2.8964478969573975
I0210 20:50:58.083264 139975138969344 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.3539702296257019, loss=2.9053025245666504
I0210 20:51:32.822509 139975130576640 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.3422158658504486, loss=2.973788022994995
I0210 20:52:07.575378 139975138969344 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.3472400903701782, loss=2.9483978748321533
I0210 20:52:42.326146 139975130576640 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.325923353433609, loss=2.9039838314056396
I0210 20:53:01.149555 140144802662208 spec.py:321] Evaluating on the training split.
I0210 20:53:04.121712 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 20:55:40.926665 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 20:55:43.609228 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 20:58:12.367753 140144802662208 spec.py:349] Evaluating on the test split.
I0210 20:58:15.050334 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 21:00:37.134095 140144802662208 submission_runner.py:408] Time since start: 22370.70s, 	Step: 38656, 	{'train/accuracy': 0.6423766613006592, 'train/loss': 1.8290132284164429, 'train/bleu': 31.39003853798642, 'validation/accuracy': 0.6569168567657471, 'validation/loss': 1.7200067043304443, 'validation/bleu': 27.887943009530343, 'validation/num_examples': 3000, 'test/accuracy': 0.670001745223999, 'test/loss': 1.6540709733963013, 'test/bleu': 27.66217813006899, 'test/num_examples': 3003, 'score': 13478.862303972244, 'total_duration': 22370.699309825897, 'accumulated_submission_time': 13478.862303972244, 'accumulated_eval_time': 8890.070576429367, 'accumulated_logging_time': 0.4886150360107422}
I0210 21:00:37.159368 139975138969344 logging_writer.py:48] [38656] accumulated_eval_time=8890.070576, accumulated_logging_time=0.488615, accumulated_submission_time=13478.862304, global_step=38656, preemption_count=0, score=13478.862304, test/accuracy=0.670002, test/bleu=27.662178, test/loss=1.654071, test/num_examples=3003, total_duration=22370.699310, train/accuracy=0.642377, train/bleu=31.390039, train/loss=1.829013, validation/accuracy=0.656917, validation/bleu=27.887943, validation/loss=1.720007, validation/num_examples=3000
I0210 21:00:52.767831 139975130576640 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.3366447389125824, loss=2.9233651161193848
I0210 21:01:27.457472 139975138969344 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.35938549041748047, loss=2.990485668182373
I0210 21:02:02.187918 139975130576640 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.35490623116493225, loss=2.93734073638916
I0210 21:02:36.929675 139975138969344 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3992502987384796, loss=2.976266860961914
I0210 21:03:11.680101 139975130576640 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.3697102665901184, loss=2.9257736206054688
I0210 21:03:46.420548 139975138969344 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.38146165013313293, loss=2.991037368774414
I0210 21:04:21.186100 139975130576640 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.35041382908821106, loss=2.9098846912384033
I0210 21:04:55.955135 139975138969344 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4623110592365265, loss=2.9331345558166504
I0210 21:05:30.723546 139975130576640 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.3558042347431183, loss=2.970250368118286
I0210 21:06:05.488345 139975138969344 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.3523700535297394, loss=2.946498155593872
I0210 21:06:40.277537 139975130576640 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.36836138367652893, loss=3.0024359226226807
I0210 21:07:15.061894 139975138969344 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.3410980701446533, loss=2.9988842010498047
I0210 21:07:49.819832 139975130576640 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.38898786902427673, loss=2.976029396057129
I0210 21:08:24.559760 139975138969344 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.42474308609962463, loss=2.938983678817749
I0210 21:08:59.300880 139975130576640 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.35976243019104004, loss=2.9229698181152344
I0210 21:09:34.068856 139975138969344 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.41500234603881836, loss=2.994310140609741
I0210 21:10:08.853585 139975130576640 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.3295914828777313, loss=2.909536838531494
I0210 21:10:43.661565 139975138969344 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.3407055139541626, loss=2.9712862968444824
I0210 21:11:18.549710 139975130576640 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.4311979115009308, loss=2.996248245239258
I0210 21:11:53.376806 139975138969344 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3942568004131317, loss=2.940211296081543
I0210 21:12:28.183950 139975130576640 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3227648138999939, loss=2.953248977661133
I0210 21:13:02.970836 139975138969344 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3414575159549713, loss=2.9676685333251953
I0210 21:13:37.774087 139975130576640 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3513709604740143, loss=3.026794195175171
I0210 21:14:12.531733 139975138969344 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.35481274127960205, loss=2.946371078491211
I0210 21:14:37.277955 140144802662208 spec.py:321] Evaluating on the training split.
I0210 21:14:40.253833 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 21:17:31.310126 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 21:17:33.980662 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 21:20:12.865112 140144802662208 spec.py:349] Evaluating on the test split.
I0210 21:20:15.543029 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 21:22:53.187804 140144802662208 submission_runner.py:408] Time since start: 23706.75s, 	Step: 41073, 	{'train/accuracy': 0.6356822848320007, 'train/loss': 1.8719481229782104, 'train/bleu': 31.447752721470277, 'validation/accuracy': 0.6580947637557983, 'validation/loss': 1.707008957862854, 'validation/bleu': 28.085698986814226, 'validation/num_examples': 3000, 'test/accuracy': 0.6714078187942505, 'test/loss': 1.647200584411621, 'test/bleu': 27.96062907699173, 'test/num_examples': 3003, 'score': 14318.886668205261, 'total_duration': 23706.75301337242, 'accumulated_submission_time': 14318.886668205261, 'accumulated_eval_time': 9385.98035311699, 'accumulated_logging_time': 0.5256316661834717}
I0210 21:22:53.208496 139975130576640 logging_writer.py:48] [41073] accumulated_eval_time=9385.980353, accumulated_logging_time=0.525632, accumulated_submission_time=14318.886668, global_step=41073, preemption_count=0, score=14318.886668, test/accuracy=0.671408, test/bleu=27.960629, test/loss=1.647201, test/num_examples=3003, total_duration=23706.753013, train/accuracy=0.635682, train/bleu=31.447753, train/loss=1.871948, validation/accuracy=0.658095, validation/bleu=28.085699, validation/loss=1.707009, validation/num_examples=3000
I0210 21:23:02.924246 139975138969344 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.4085550010204315, loss=2.931896209716797
I0210 21:23:37.554968 139975130576640 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.3762320578098297, loss=2.9764187335968018
I0210 21:24:12.244995 139975138969344 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.3620116114616394, loss=2.9762582778930664
I0210 21:24:47.072945 139975130576640 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.3396834135055542, loss=2.969853639602661
I0210 21:25:21.827822 139975138969344 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.4208475947380066, loss=2.8990349769592285
I0210 21:25:56.536381 139975130576640 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.36130571365356445, loss=3.0465962886810303
I0210 21:26:31.268276 139975138969344 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.3858553171157837, loss=2.946441411972046
I0210 21:27:05.986853 139975130576640 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.3857099115848541, loss=2.972508192062378
I0210 21:27:40.741363 139975138969344 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.37178555130958557, loss=2.9336609840393066
I0210 21:28:15.534827 139975130576640 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.43209534883499146, loss=2.9645142555236816
I0210 21:28:50.284544 139975138969344 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.40119877457618713, loss=2.9073240756988525
I0210 21:29:25.061697 139975130576640 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3733706474304199, loss=2.920074939727783
I0210 21:29:59.825869 139975138969344 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.38527122139930725, loss=2.9096567630767822
I0210 21:30:34.575184 139975130576640 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.5410581827163696, loss=2.929468870162964
I0210 21:31:09.370222 139975138969344 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.37438535690307617, loss=2.9440488815307617
I0210 21:31:44.149082 139975130576640 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.34974437952041626, loss=2.970801830291748
I0210 21:32:18.905975 139975138969344 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.48201438784599304, loss=2.9886608123779297
I0210 21:32:53.617630 139975130576640 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.4220695495605469, loss=2.901099920272827
I0210 21:33:28.352842 139975138969344 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.3707062005996704, loss=2.9420323371887207
I0210 21:34:03.077559 139975130576640 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.37003013491630554, loss=2.9206833839416504
I0210 21:34:37.816787 139975138969344 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.37438639998435974, loss=2.9497997760772705
I0210 21:35:12.537294 139975130576640 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.3537571132183075, loss=2.9321932792663574
I0210 21:35:47.255659 139975138969344 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3776133060455322, loss=3.0165600776672363
I0210 21:36:21.987058 139975130576640 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3632214665412903, loss=2.9257688522338867
I0210 21:36:53.323313 140144802662208 spec.py:321] Evaluating on the training split.
I0210 21:36:56.286910 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 21:40:05.040595 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 21:40:07.729071 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 21:42:40.642948 140144802662208 spec.py:349] Evaluating on the test split.
I0210 21:42:43.343789 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 21:45:22.433113 140144802662208 submission_runner.py:408] Time since start: 25056.00s, 	Step: 43492, 	{'train/accuracy': 0.6358650922775269, 'train/loss': 1.8773494958877563, 'train/bleu': 31.489122957348748, 'validation/accuracy': 0.6598678231239319, 'validation/loss': 1.6993831396102905, 'validation/bleu': 28.275403261768112, 'validation/num_examples': 3000, 'test/accuracy': 0.6728022694587708, 'test/loss': 1.6397759914398193, 'test/bleu': 27.88330925488826, 'test/num_examples': 3003, 'score': 15158.90975689888, 'total_duration': 25055.99835085869, 'accumulated_submission_time': 15158.90975689888, 'accumulated_eval_time': 9895.090104341507, 'accumulated_logging_time': 0.5565388202667236}
I0210 21:45:22.453580 139975138969344 logging_writer.py:48] [43492] accumulated_eval_time=9895.090104, accumulated_logging_time=0.556539, accumulated_submission_time=15158.909757, global_step=43492, preemption_count=0, score=15158.909757, test/accuracy=0.672802, test/bleu=27.883309, test/loss=1.639776, test/num_examples=3003, total_duration=25055.998351, train/accuracy=0.635865, train/bleu=31.489123, train/loss=1.877349, validation/accuracy=0.659868, validation/bleu=28.275403, validation/loss=1.699383, validation/num_examples=3000
I0210 21:45:25.602595 139975130576640 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.3562775254249573, loss=2.890202045440674
I0210 21:46:00.209534 139975138969344 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.34917062520980835, loss=2.940633773803711
I0210 21:46:34.915270 139975130576640 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.34805816411972046, loss=2.9624993801116943
I0210 21:47:09.645311 139975138969344 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.3853919804096222, loss=3.0430233478546143
I0210 21:47:44.414398 139975130576640 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.38057222962379456, loss=2.9468324184417725
I0210 21:48:19.158304 139975138969344 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3784412741661072, loss=2.944268226623535
I0210 21:48:53.914237 139975130576640 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.3344036936759949, loss=2.983062267303467
I0210 21:49:28.678305 139975138969344 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.4000549912452698, loss=2.9622344970703125
I0210 21:50:03.477808 139975130576640 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.3887297809123993, loss=2.9807944297790527
I0210 21:50:38.229999 139975138969344 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.3337101340293884, loss=2.8661155700683594
I0210 21:51:12.992102 139975130576640 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.4160335958003998, loss=2.9576852321624756
I0210 21:51:47.755883 139975138969344 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.4013504683971405, loss=2.852938652038574
I0210 21:52:22.535324 139975130576640 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.41560766100883484, loss=2.963369846343994
I0210 21:52:57.296696 139975138969344 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.36001312732696533, loss=2.9344217777252197
I0210 21:53:32.032198 139975130576640 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.37250733375549316, loss=3.0527243614196777
I0210 21:54:06.796768 139975138969344 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.39140018820762634, loss=2.923112630844116
I0210 21:54:41.571045 139975130576640 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.3825643062591553, loss=2.9285166263580322
I0210 21:55:16.329399 139975138969344 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.33767610788345337, loss=2.894835948944092
I0210 21:55:51.092491 139975130576640 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.3430044651031494, loss=2.909658670425415
I0210 21:56:25.833431 139975138969344 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.4068734049797058, loss=2.9956538677215576
I0210 21:57:00.605677 139975130576640 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.3689272701740265, loss=2.9714651107788086
I0210 21:57:35.386095 139975138969344 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3356039822101593, loss=2.899742841720581
I0210 21:58:10.210877 139975130576640 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.4379980266094208, loss=2.9003679752349854
I0210 21:58:44.985099 139975138969344 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.45088693499565125, loss=2.848618745803833
I0210 21:59:19.748817 139975130576640 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.36905041337013245, loss=2.936345100402832
I0210 21:59:22.606082 140144802662208 spec.py:321] Evaluating on the training split.
I0210 21:59:25.598173 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 22:03:06.144267 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 22:03:08.824697 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 22:05:47.827250 140144802662208 spec.py:349] Evaluating on the test split.
I0210 22:05:50.520634 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 22:08:28.006356 140144802662208 submission_runner.py:408] Time since start: 26441.57s, 	Step: 45910, 	{'train/accuracy': 0.6427233219146729, 'train/loss': 1.8206764459609985, 'train/bleu': 31.07863303548564, 'validation/accuracy': 0.6608349680900574, 'validation/loss': 1.692588448524475, 'validation/bleu': 28.363472882686317, 'validation/num_examples': 3000, 'test/accuracy': 0.6724420785903931, 'test/loss': 1.634313702583313, 'test/bleu': 28.059474468605845, 'test/num_examples': 3003, 'score': 15998.971328496933, 'total_duration': 26441.571583509445, 'accumulated_submission_time': 15998.971328496933, 'accumulated_eval_time': 10440.490324735641, 'accumulated_logging_time': 0.5880370140075684}
I0210 22:08:28.027025 139975138969344 logging_writer.py:48] [45910] accumulated_eval_time=10440.490325, accumulated_logging_time=0.588037, accumulated_submission_time=15998.971328, global_step=45910, preemption_count=0, score=15998.971328, test/accuracy=0.672442, test/bleu=28.059474, test/loss=1.634314, test/num_examples=3003, total_duration=26441.571584, train/accuracy=0.642723, train/bleu=31.078633, train/loss=1.820676, validation/accuracy=0.660835, validation/bleu=28.363473, validation/loss=1.692588, validation/num_examples=3000
I0210 22:08:59.509251 139975130576640 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3691336214542389, loss=2.994511127471924
I0210 22:09:34.162235 139975138969344 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.3989066481590271, loss=2.9042513370513916
I0210 22:10:08.927555 139975130576640 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.355821818113327, loss=2.8889148235321045
I0210 22:10:43.684901 139975138969344 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.3618282079696655, loss=2.980220079421997
I0210 22:11:18.423824 139975130576640 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3745312988758087, loss=2.9112508296966553
I0210 22:11:53.145305 139975138969344 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.3291240632534027, loss=2.941643238067627
I0210 22:12:27.886580 139975130576640 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.3771244287490845, loss=2.9386489391326904
I0210 22:13:02.623852 139975138969344 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.4117438495159149, loss=2.928140640258789
I0210 22:13:37.360558 139975130576640 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.4096846878528595, loss=2.9691123962402344
I0210 22:14:12.125852 139975138969344 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.4448343813419342, loss=2.9537529945373535
I0210 22:14:46.876220 139975130576640 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.3276072144508362, loss=2.9012529850006104
I0210 22:15:21.641163 139975138969344 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.4165685176849365, loss=2.904470682144165
I0210 22:15:56.417870 139975130576640 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.35255053639411926, loss=2.864084005355835
I0210 22:16:31.162482 139975138969344 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.34089136123657227, loss=2.869347095489502
I0210 22:17:05.935636 139975130576640 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3873029053211212, loss=2.939523935317993
I0210 22:17:40.748690 139975138969344 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.3646746277809143, loss=2.946948289871216
I0210 22:18:15.517972 139975130576640 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.3564872443675995, loss=2.869232177734375
I0210 22:18:50.293458 139975138969344 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.40069717168807983, loss=2.978160858154297
I0210 22:19:25.071677 139975130576640 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.3646131753921509, loss=2.963456869125366
I0210 22:19:59.822467 139975138969344 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.35844630002975464, loss=3.0098483562469482
I0210 22:20:34.654477 139975130576640 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.40259626507759094, loss=2.8837623596191406
I0210 22:21:09.399355 139975138969344 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.3411286771297455, loss=2.9109947681427
I0210 22:21:44.176374 139975130576640 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.4035068452358246, loss=2.8875269889831543
I0210 22:22:19.034351 139975138969344 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.3592434823513031, loss=2.9829187393188477
I0210 22:22:28.145535 140144802662208 spec.py:321] Evaluating on the training split.
I0210 22:22:31.117937 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 22:25:24.222695 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 22:25:26.911883 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 22:27:55.849359 140144802662208 spec.py:349] Evaluating on the test split.
I0210 22:27:58.527454 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 22:30:24.407289 140144802662208 submission_runner.py:408] Time since start: 27757.97s, 	Step: 48328, 	{'train/accuracy': 0.6389437913894653, 'train/loss': 1.855763554573059, 'train/bleu': 31.00010065151017, 'validation/accuracy': 0.6613061428070068, 'validation/loss': 1.6884702444076538, 'validation/bleu': 28.22158443610769, 'validation/num_examples': 3000, 'test/accuracy': 0.6750450730323792, 'test/loss': 1.6206564903259277, 'test/bleu': 28.01314804173373, 'test/num_examples': 3003, 'score': 16838.999277830124, 'total_duration': 27757.97251176834, 'accumulated_submission_time': 16838.999277830124, 'accumulated_eval_time': 10916.7520134449, 'accumulated_logging_time': 0.619476318359375}
I0210 22:30:24.428788 139975130576640 logging_writer.py:48] [48328] accumulated_eval_time=10916.752013, accumulated_logging_time=0.619476, accumulated_submission_time=16838.999278, global_step=48328, preemption_count=0, score=16838.999278, test/accuracy=0.675045, test/bleu=28.013148, test/loss=1.620656, test/num_examples=3003, total_duration=27757.972512, train/accuracy=0.638944, train/bleu=31.000101, train/loss=1.855764, validation/accuracy=0.661306, validation/bleu=28.221584, validation/loss=1.688470, validation/num_examples=3000
I0210 22:30:49.691776 139975138969344 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.3565581142902374, loss=2.898013114929199
I0210 22:31:24.347028 139975130576640 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.39032718539237976, loss=3.0128819942474365
I0210 22:31:59.088359 139975138969344 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3771793246269226, loss=2.8810577392578125
I0210 22:32:33.835247 139975130576640 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.35581761598587036, loss=2.920254707336426
I0210 22:33:08.588848 139975138969344 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3512815237045288, loss=2.8912179470062256
I0210 22:33:43.322220 139975130576640 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.3755035996437073, loss=2.9490435123443604
I0210 22:34:18.065811 139975138969344 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3651179373264313, loss=2.948549509048462
I0210 22:34:52.819950 139975130576640 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.3854483366012573, loss=2.903507709503174
I0210 22:35:27.573378 139975138969344 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.40717390179634094, loss=2.9654855728149414
I0210 22:36:02.349011 139975130576640 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3458699882030487, loss=2.9392154216766357
I0210 22:36:37.109195 139975138969344 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.3714669644832611, loss=2.8889107704162598
I0210 22:37:11.856799 139975130576640 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.38541528582572937, loss=2.953561544418335
I0210 22:37:46.621906 139975138969344 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.3961028456687927, loss=2.941235065460205
I0210 22:38:21.400006 139975130576640 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.4063913822174072, loss=2.898927927017212
I0210 22:38:56.176213 139975138969344 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.34917452931404114, loss=2.862203359603882
I0210 22:39:30.948268 139975130576640 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3427446186542511, loss=2.895564556121826
I0210 22:40:05.687937 139975138969344 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3283138871192932, loss=2.9397692680358887
I0210 22:40:40.432082 139975130576640 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.34484803676605225, loss=2.8832619190216064
I0210 22:41:15.186489 139975138969344 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3640757203102112, loss=2.8897018432617188
I0210 22:41:49.941514 139975130576640 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.37165170907974243, loss=2.9235053062438965
I0210 22:42:24.696060 139975138969344 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.3462281823158264, loss=2.9559009075164795
I0210 22:42:59.457512 139975130576640 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3781706988811493, loss=2.9669530391693115
I0210 22:43:34.261687 139975138969344 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.35802891850471497, loss=2.951740264892578
I0210 22:44:09.020600 139975130576640 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.35865744948387146, loss=2.9703962802886963
I0210 22:44:24.725559 140144802662208 spec.py:321] Evaluating on the training split.
I0210 22:44:27.722191 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 22:47:16.400198 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 22:47:19.076261 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 22:49:47.485500 140144802662208 spec.py:349] Evaluating on the test split.
I0210 22:49:50.184423 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 22:52:10.621670 140144802662208 submission_runner.py:408] Time since start: 29064.19s, 	Step: 50747, 	{'train/accuracy': 0.6508919596672058, 'train/loss': 1.7648361921310425, 'train/bleu': 31.782695880760556, 'validation/accuracy': 0.6645422577857971, 'validation/loss': 1.6845265626907349, 'validation/bleu': 28.74312417864181, 'validation/num_examples': 3000, 'test/accuracy': 0.6755331158638, 'test/loss': 1.6183867454528809, 'test/bleu': 28.332459093761646, 'test/num_examples': 3003, 'score': 17679.204163074493, 'total_duration': 29064.18690776825, 'accumulated_submission_time': 17679.204163074493, 'accumulated_eval_time': 11382.648072242737, 'accumulated_logging_time': 0.6527702808380127}
I0210 22:52:10.642963 139975138969344 logging_writer.py:48] [50747] accumulated_eval_time=11382.648072, accumulated_logging_time=0.652770, accumulated_submission_time=17679.204163, global_step=50747, preemption_count=0, score=17679.204163, test/accuracy=0.675533, test/bleu=28.332459, test/loss=1.618387, test/num_examples=3003, total_duration=29064.186908, train/accuracy=0.650892, train/bleu=31.782696, train/loss=1.764836, validation/accuracy=0.664542, validation/bleu=28.743124, validation/loss=1.684527, validation/num_examples=3000
I0210 22:52:29.354419 139975130576640 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.4858730733394623, loss=2.9035532474517822
I0210 22:53:03.970567 139975138969344 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.3651016354560852, loss=2.9568803310394287
I0210 22:53:38.700779 139975130576640 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.3482239544391632, loss=2.8831193447113037
I0210 22:54:13.452204 139975138969344 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.35810673236846924, loss=2.9275619983673096
I0210 22:54:48.185935 139975130576640 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3602573871612549, loss=2.9178574085235596
I0210 22:55:22.909670 139975138969344 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.37713590264320374, loss=2.980250835418701
I0210 22:55:57.647596 139975130576640 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.38531047105789185, loss=2.9135725498199463
I0210 22:56:32.372435 139975138969344 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.36876845359802246, loss=2.91406512260437
I0210 22:57:07.124983 139975130576640 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.37581801414489746, loss=2.8914616107940674
I0210 22:57:41.852167 139975138969344 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.34088146686553955, loss=2.987196683883667
I0210 22:58:16.623816 139975130576640 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.34587201476097107, loss=2.949608087539673
I0210 22:58:51.412343 139975138969344 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3398347795009613, loss=2.850552558898926
I0210 22:59:26.158445 139975130576640 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.3839552104473114, loss=2.9751954078674316
I0210 23:00:00.876974 139975138969344 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.38361355662345886, loss=2.938838243484497
I0210 23:00:35.641502 139975130576640 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.34139758348464966, loss=2.897554636001587
I0210 23:01:10.386604 139975138969344 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.36431658267974854, loss=2.891528367996216
I0210 23:01:45.148838 139975130576640 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.3590144217014313, loss=2.9603071212768555
I0210 23:02:19.906482 139975138969344 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3750784993171692, loss=2.8634586334228516
I0210 23:02:54.627405 139975130576640 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3319995105266571, loss=2.88740873336792
I0210 23:03:29.408075 139975138969344 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.4036286473274231, loss=2.8496358394622803
I0210 23:04:04.193720 139975130576640 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3732466697692871, loss=2.9199607372283936
I0210 23:04:38.936739 139975138969344 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.36463814973831177, loss=2.9629993438720703
I0210 23:05:13.669492 139975130576640 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.36293408274650574, loss=2.860704183578491
I0210 23:05:48.438459 139975138969344 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.36391502618789673, loss=2.9449357986450195
I0210 23:06:10.735131 140144802662208 spec.py:321] Evaluating on the training split.
I0210 23:06:13.706684 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 23:09:22.548175 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 23:09:25.229195 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 23:12:21.758839 140144802662208 spec.py:349] Evaluating on the test split.
I0210 23:12:24.448113 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 23:15:07.258726 140144802662208 submission_runner.py:408] Time since start: 30440.82s, 	Step: 53166, 	{'train/accuracy': 0.6417784690856934, 'train/loss': 1.8273143768310547, 'train/bleu': 31.41106280404022, 'validation/accuracy': 0.664182722568512, 'validation/loss': 1.6704254150390625, 'validation/bleu': 28.515135030428414, 'validation/num_examples': 3000, 'test/accuracy': 0.6751612424850464, 'test/loss': 1.60860013961792, 'test/bleu': 28.214403806713054, 'test/num_examples': 3003, 'score': 18519.205310583115, 'total_duration': 30440.823876857758, 'accumulated_submission_time': 18519.205310583115, 'accumulated_eval_time': 11919.171528339386, 'accumulated_logging_time': 0.6851651668548584}
I0210 23:15:07.283604 139975130576640 logging_writer.py:48] [53166] accumulated_eval_time=11919.171528, accumulated_logging_time=0.685165, accumulated_submission_time=18519.205311, global_step=53166, preemption_count=0, score=18519.205311, test/accuracy=0.675161, test/bleu=28.214404, test/loss=1.608600, test/num_examples=3003, total_duration=30440.823877, train/accuracy=0.641778, train/bleu=31.411063, train/loss=1.827314, validation/accuracy=0.664183, validation/bleu=28.515135, validation/loss=1.670425, validation/num_examples=3000
I0210 23:15:19.432766 139975138969344 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.45260581374168396, loss=2.9386110305786133
I0210 23:15:54.063070 139975130576640 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.37590813636779785, loss=2.8757917881011963
I0210 23:16:28.747207 139975138969344 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.8606134057044983, loss=2.9188833236694336
I0210 23:17:03.487235 139975130576640 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.32981452345848083, loss=2.848825454711914
I0210 23:17:38.229942 139975138969344 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.39117345213890076, loss=2.9218573570251465
I0210 23:18:12.963208 139975130576640 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3665125370025635, loss=2.8157882690429688
I0210 23:18:47.715574 139975138969344 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.4319932758808136, loss=2.9720373153686523
I0210 23:19:22.426775 139975130576640 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.3770095407962799, loss=2.941413164138794
I0210 23:19:57.118820 139975138969344 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3577860891819, loss=2.917057991027832
I0210 23:20:31.851365 139975130576640 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.34923890233039856, loss=2.9437644481658936
I0210 23:21:06.765508 139975138969344 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.38654738664627075, loss=2.938000202178955
I0210 23:21:41.496802 139975130576640 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.36100471019744873, loss=2.9200801849365234
I0210 23:22:16.219403 139975138969344 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.37008970975875854, loss=2.9102096557617188
I0210 23:22:50.967636 139975130576640 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.3628391921520233, loss=3.01997709274292
I0210 23:23:25.745564 139975138969344 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.400686651468277, loss=2.9815855026245117
I0210 23:24:00.521127 139975130576640 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.34171411395072937, loss=2.936460494995117
I0210 23:24:35.306165 139975138969344 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.33732515573501587, loss=2.9115536212921143
I0210 23:25:10.039224 139975130576640 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.35554200410842896, loss=2.8798491954803467
I0210 23:25:44.819988 139975138969344 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.33651527762413025, loss=2.8883864879608154
I0210 23:26:19.579116 139975130576640 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.38466599583625793, loss=2.8649959564208984
I0210 23:26:54.450128 139975138969344 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.3890189826488495, loss=2.8983683586120605
I0210 23:27:29.204114 139975130576640 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.35583850741386414, loss=2.8974015712738037
I0210 23:28:03.944483 139975138969344 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.4064127504825592, loss=2.8784234523773193
I0210 23:28:38.682421 139975130576640 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.34477201104164124, loss=3.0244839191436768
I0210 23:29:07.570299 140144802662208 spec.py:321] Evaluating on the training split.
I0210 23:29:10.541198 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 23:32:43.189262 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 23:32:45.867788 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 23:35:53.197195 140144802662208 spec.py:349] Evaluating on the test split.
I0210 23:35:55.883216 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 23:38:35.565401 140144802662208 submission_runner.py:408] Time since start: 31849.13s, 	Step: 55585, 	{'train/accuracy': 0.6417572498321533, 'train/loss': 1.8483734130859375, 'train/bleu': 31.531738068339457, 'validation/accuracy': 0.665658175945282, 'validation/loss': 1.6686065196990967, 'validation/bleu': 28.607842205522875, 'validation/num_examples': 3000, 'test/accuracy': 0.6774388551712036, 'test/loss': 1.6036652326583862, 'test/bleu': 28.394650410572435, 'test/num_examples': 3003, 'score': 19359.40021085739, 'total_duration': 31849.13062644005, 'accumulated_submission_time': 19359.40021085739, 'accumulated_eval_time': 12487.166585206985, 'accumulated_logging_time': 0.7213225364685059}
I0210 23:38:35.587768 139975138969344 logging_writer.py:48] [55585] accumulated_eval_time=12487.166585, accumulated_logging_time=0.721323, accumulated_submission_time=19359.400211, global_step=55585, preemption_count=0, score=19359.400211, test/accuracy=0.677439, test/bleu=28.394650, test/loss=1.603665, test/num_examples=3003, total_duration=31849.130626, train/accuracy=0.641757, train/bleu=31.531738, train/loss=1.848373, validation/accuracy=0.665658, validation/bleu=28.607842, validation/loss=1.668607, validation/num_examples=3000
I0210 23:38:41.146625 139975130576640 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.3878324031829834, loss=2.910853385925293
I0210 23:39:15.780468 139975138969344 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.34375572204589844, loss=2.9307632446289062
I0210 23:39:50.513725 139975130576640 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.396950900554657, loss=2.9384665489196777
I0210 23:40:25.249768 139975138969344 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.37240496277809143, loss=3.040377378463745
I0210 23:40:59.950099 139975130576640 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.365850031375885, loss=2.9237678050994873
I0210 23:41:34.703772 139975138969344 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.3332739770412445, loss=2.890007734298706
I0210 23:42:09.471124 139975130576640 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.3392930328845978, loss=2.920685052871704
I0210 23:42:44.248790 139975138969344 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.38158950209617615, loss=2.9842545986175537
I0210 23:43:19.031133 139975130576640 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.38164663314819336, loss=2.865856647491455
I0210 23:43:53.809985 139975138969344 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3902043104171753, loss=2.8785665035247803
I0210 23:44:28.565083 139975130576640 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.4195386469364166, loss=2.907533884048462
I0210 23:45:03.296799 139975138969344 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.3789674937725067, loss=2.8937265872955322
I0210 23:45:38.076978 139975130576640 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3830235004425049, loss=2.926589012145996
I0210 23:46:12.859204 139975138969344 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.35885098576545715, loss=2.882885217666626
I0210 23:46:47.584887 139975130576640 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3756328821182251, loss=2.9615426063537598
I0210 23:47:22.326869 139975138969344 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.3704761564731598, loss=2.8744237422943115
I0210 23:47:57.039922 139975130576640 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.3382596969604492, loss=2.960505247116089
I0210 23:48:31.795282 139975138969344 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.37447819113731384, loss=2.8844170570373535
I0210 23:49:06.533400 139975130576640 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3801276385784149, loss=2.9251348972320557
I0210 23:49:41.280770 139975138969344 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3479481041431427, loss=2.9067091941833496
I0210 23:50:16.027466 139975130576640 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3380357623100281, loss=2.9455525875091553
I0210 23:50:50.764392 139975138969344 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.36484792828559875, loss=2.8799805641174316
I0210 23:51:25.526827 139975130576640 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.3182411193847656, loss=2.906658411026001
I0210 23:52:00.252591 139975138969344 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.4274282157421112, loss=2.851635217666626
I0210 23:52:34.985744 139975130576640 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.4158993363380432, loss=2.8971457481384277
I0210 23:52:35.753304 140144802662208 spec.py:321] Evaluating on the training split.
I0210 23:52:38.716806 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 23:55:19.227305 140144802662208 spec.py:333] Evaluating on the validation split.
I0210 23:55:21.922127 140144802662208 workload.py:181] Translating evaluation dataset.
I0210 23:57:52.309859 140144802662208 spec.py:349] Evaluating on the test split.
I0210 23:57:54.988380 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 00:00:19.186553 140144802662208 submission_runner.py:408] Time since start: 33152.75s, 	Step: 58004, 	{'train/accuracy': 0.650646448135376, 'train/loss': 1.7800432443618774, 'train/bleu': 32.27619017894549, 'validation/accuracy': 0.6677040457725525, 'validation/loss': 1.6552205085754395, 'validation/bleu': 28.97028555750232, 'validation/num_examples': 3000, 'test/accuracy': 0.6786706447601318, 'test/loss': 1.5954159498214722, 'test/bleu': 28.438258704322525, 'test/num_examples': 3003, 'score': 20199.471867084503, 'total_duration': 33152.7517850399, 'accumulated_submission_time': 20199.471867084503, 'accumulated_eval_time': 12950.59977388382, 'accumulated_logging_time': 0.7550153732299805}
I0211 00:00:19.208879 139975138969344 logging_writer.py:48] [58004] accumulated_eval_time=12950.599774, accumulated_logging_time=0.755015, accumulated_submission_time=20199.471867, global_step=58004, preemption_count=0, score=20199.471867, test/accuracy=0.678671, test/bleu=28.438259, test/loss=1.595416, test/num_examples=3003, total_duration=33152.751785, train/accuracy=0.650646, train/bleu=32.276190, train/loss=1.780043, validation/accuracy=0.667704, validation/bleu=28.970286, validation/loss=1.655221, validation/num_examples=3000
I0211 00:00:52.788276 139975130576640 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.3727964162826538, loss=2.8675272464752197
I0211 00:01:27.524707 139975138969344 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.34707024693489075, loss=2.9018843173980713
I0211 00:02:02.273430 139975130576640 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3625640273094177, loss=2.8486580848693848
I0211 00:02:37.120648 139975138969344 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.3623446524143219, loss=2.8956897258758545
I0211 00:03:11.868312 139975130576640 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.33852311968803406, loss=2.9236252307891846
I0211 00:03:46.602030 139975138969344 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.3753125071525574, loss=2.872300624847412
I0211 00:04:21.344932 139975130576640 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.39695945382118225, loss=2.9171054363250732
I0211 00:04:56.136418 139975138969344 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3640885055065155, loss=2.819828748703003
I0211 00:05:30.871570 139975130576640 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.38599011301994324, loss=2.872912883758545
I0211 00:06:05.613413 139975138969344 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.3354148268699646, loss=2.864532232284546
I0211 00:06:40.376605 139975130576640 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.35152968764305115, loss=2.9120051860809326
I0211 00:07:15.113960 139975138969344 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.3350776433944702, loss=2.9596784114837646
I0211 00:07:49.897088 139975130576640 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.3383113741874695, loss=2.9641013145446777
I0211 00:08:24.657054 139975138969344 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3941868841648102, loss=2.843153238296509
I0211 00:08:59.409801 139975130576640 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.34710806608200073, loss=2.8883869647979736
I0211 00:09:34.208522 139975138969344 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.3479931056499481, loss=2.869600534439087
I0211 00:10:08.991843 139975130576640 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.41479018330574036, loss=2.939086675643921
I0211 00:10:43.735321 139975138969344 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.37965789437294006, loss=2.912027597427368
I0211 00:11:18.514144 139975130576640 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.3573839068412781, loss=2.8968417644500732
I0211 00:11:53.250030 139975138969344 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.37732407450675964, loss=2.955864191055298
I0211 00:12:28.016781 139975130576640 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.3861495554447174, loss=2.8914670944213867
I0211 00:13:02.769896 139975138969344 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3585907816886902, loss=2.8874330520629883
I0211 00:13:37.516501 139975130576640 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.3861374258995056, loss=2.9483859539031982
I0211 00:14:12.258048 139975138969344 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.4579841196537018, loss=2.940232038497925
I0211 00:14:19.288380 140144802662208 spec.py:321] Evaluating on the training split.
I0211 00:14:22.272878 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 00:17:11.176684 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 00:17:13.854780 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 00:19:47.385250 140144802662208 spec.py:349] Evaluating on the test split.
I0211 00:19:50.069987 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 00:22:34.649213 140144802662208 submission_runner.py:408] Time since start: 34488.21s, 	Step: 60422, 	{'train/accuracy': 0.6451210379600525, 'train/loss': 1.8069937229156494, 'train/bleu': 31.819075921555985, 'validation/accuracy': 0.6668609380722046, 'validation/loss': 1.6589609384536743, 'validation/bleu': 28.74034449614497, 'validation/num_examples': 3000, 'test/accuracy': 0.6806228756904602, 'test/loss': 1.5879414081573486, 'test/bleu': 28.597255906283987, 'test/num_examples': 3003, 'score': 21039.458937883377, 'total_duration': 34488.21444058418, 'accumulated_submission_time': 21039.458937883377, 'accumulated_eval_time': 13445.960545539856, 'accumulated_logging_time': 0.7892227172851562}
I0211 00:22:34.671545 139975130576640 logging_writer.py:48] [60422] accumulated_eval_time=13445.960546, accumulated_logging_time=0.789223, accumulated_submission_time=21039.458938, global_step=60422, preemption_count=0, score=21039.458938, test/accuracy=0.680623, test/bleu=28.597256, test/loss=1.587941, test/num_examples=3003, total_duration=34488.214441, train/accuracy=0.645121, train/bleu=31.819076, train/loss=1.806994, validation/accuracy=0.666861, validation/bleu=28.740344, validation/loss=1.658961, validation/num_examples=3000
I0211 00:23:02.000423 139975138969344 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.37038764357566833, loss=2.9094858169555664
I0211 00:23:36.674660 139975130576640 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.381565123796463, loss=2.853177785873413
I0211 00:24:11.393259 139975138969344 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.4004688262939453, loss=2.976273775100708
I0211 00:24:46.132813 139975130576640 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.40618985891342163, loss=2.97037410736084
I0211 00:25:20.880897 139975138969344 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.34847408533096313, loss=2.906700849533081
I0211 00:25:55.602139 139975130576640 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.3869260251522064, loss=2.978161573410034
I0211 00:26:30.370232 139975138969344 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.33577314019203186, loss=2.885830879211426
I0211 00:27:05.127928 139975130576640 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.3665299415588379, loss=2.883368492126465
I0211 00:27:39.888808 139975138969344 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.35558661818504333, loss=2.841832160949707
I0211 00:28:14.624712 139975130576640 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.36951208114624023, loss=2.89184832572937
I0211 00:28:49.366277 139975138969344 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.36141908168792725, loss=2.9241912364959717
I0211 00:29:24.137099 139975130576640 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.36998462677001953, loss=2.938215732574463
I0211 00:29:58.891463 139975138969344 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3463895320892334, loss=2.8873820304870605
I0211 00:30:33.668981 139975130576640 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3774278461933136, loss=2.9622364044189453
I0211 00:31:08.426065 139975138969344 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.3402271270751953, loss=2.8247573375701904
I0211 00:31:43.182744 139975130576640 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.35798120498657227, loss=2.91907000541687
I0211 00:32:17.944229 139975138969344 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.33115607500076294, loss=2.9365720748901367
I0211 00:32:52.725750 139975130576640 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.3812003433704376, loss=2.846405506134033
I0211 00:33:27.460369 139975138969344 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.4440065920352936, loss=2.8632123470306396
I0211 00:34:02.203082 139975130576640 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.3340953290462494, loss=2.80142879486084
I0211 00:34:36.940831 139975138969344 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.3520571291446686, loss=2.8850879669189453
I0211 00:35:11.711322 139975130576640 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.3808797597885132, loss=2.843496084213257
I0211 00:35:46.430857 139975138969344 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.38482654094696045, loss=2.9052000045776367
I0211 00:36:21.178924 139975130576640 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.3728194832801819, loss=2.893423557281494
I0211 00:36:34.809613 140144802662208 spec.py:321] Evaluating on the training split.
I0211 00:36:37.781311 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 00:39:31.771101 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 00:39:34.457993 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 00:42:05.735324 140144802662208 spec.py:349] Evaluating on the test split.
I0211 00:42:08.423583 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 00:44:39.633563 140144802662208 submission_runner.py:408] Time since start: 35813.20s, 	Step: 62841, 	{'train/accuracy': 0.6688887476921082, 'train/loss': 1.6635355949401855, 'train/bleu': 33.49501242969906, 'validation/accuracy': 0.6675428748130798, 'validation/loss': 1.650787353515625, 'validation/bleu': 28.96451958189699, 'validation/num_examples': 3000, 'test/accuracy': 0.6802510023117065, 'test/loss': 1.5829477310180664, 'test/bleu': 28.315300929416214, 'test/num_examples': 3003, 'score': 21879.506196975708, 'total_duration': 35813.19875717163, 'accumulated_submission_time': 21879.506196975708, 'accumulated_eval_time': 13930.784398078918, 'accumulated_logging_time': 0.8219027519226074}
I0211 00:44:39.656560 139975138969344 logging_writer.py:48] [62841] accumulated_eval_time=13930.784398, accumulated_logging_time=0.821903, accumulated_submission_time=21879.506197, global_step=62841, preemption_count=0, score=21879.506197, test/accuracy=0.680251, test/bleu=28.315301, test/loss=1.582948, test/num_examples=3003, total_duration=35813.198757, train/accuracy=0.668889, train/bleu=33.495012, train/loss=1.663536, validation/accuracy=0.667543, validation/bleu=28.964520, validation/loss=1.650787, validation/num_examples=3000
I0211 00:45:00.469661 139975130576640 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.34163105487823486, loss=2.910236358642578
I0211 00:45:35.191209 139975138969344 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.34185555577278137, loss=2.893566131591797
I0211 00:46:09.955274 139975130576640 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.36438101530075073, loss=2.884448766708374
I0211 00:46:44.696632 139975138969344 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.407156229019165, loss=2.917064666748047
I0211 00:47:19.433696 139975130576640 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.3828997015953064, loss=2.822727918624878
I0211 00:47:54.163890 139975138969344 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.401947021484375, loss=2.9400031566619873
I0211 00:48:28.907974 139975130576640 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3615095019340515, loss=2.869474172592163
I0211 00:49:03.655886 139975138969344 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.36299100518226624, loss=2.8797483444213867
I0211 00:49:38.412254 139975130576640 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.3520923852920532, loss=2.8910574913024902
I0211 00:50:13.185681 139975138969344 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.34263259172439575, loss=2.899064064025879
I0211 00:50:47.944686 139975130576640 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.3520587980747223, loss=2.900507926940918
I0211 00:51:22.667085 139975138969344 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.39552298188209534, loss=2.8437774181365967
I0211 00:51:57.435276 139975130576640 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.3851394057273865, loss=2.8910019397735596
I0211 00:52:32.223884 139975138969344 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.36061468720436096, loss=2.902005672454834
I0211 00:53:06.950086 139975130576640 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.3809013068675995, loss=2.8810107707977295
I0211 00:53:41.668782 139975138969344 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.3680208921432495, loss=2.908046007156372
I0211 00:54:16.402452 139975130576640 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.3639928996562958, loss=2.9342894554138184
I0211 00:54:51.171899 139975138969344 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.36763086915016174, loss=2.8977749347686768
I0211 00:55:25.920235 139975130576640 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3858852684497833, loss=2.9307172298431396
I0211 00:56:00.685294 139975138969344 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.35490846633911133, loss=2.896496534347534
I0211 00:56:35.446385 139975130576640 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.33391812443733215, loss=2.855935573577881
I0211 00:57:10.191967 139975138969344 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3537042438983917, loss=2.863137722015381
I0211 00:57:44.990227 139975130576640 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3884856402873993, loss=2.8926756381988525
I0211 00:58:19.740023 139975138969344 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3700130879878998, loss=2.840921401977539
I0211 00:58:39.958223 140144802662208 spec.py:321] Evaluating on the training split.
I0211 00:58:42.934388 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 01:01:32.657129 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 01:01:35.347958 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 01:04:07.532375 140144802662208 spec.py:349] Evaluating on the test split.
I0211 01:04:10.219449 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 01:06:33.197223 140144802662208 submission_runner.py:408] Time since start: 37126.76s, 	Step: 65260, 	{'train/accuracy': 0.6497659683227539, 'train/loss': 1.7808959484100342, 'train/bleu': 32.40010374364767, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.6441073417663574, 'validation/bleu': 28.94475644983091, 'validation/num_examples': 3000, 'test/accuracy': 0.6811341643333435, 'test/loss': 1.5736756324768066, 'test/bleu': 28.409011933836478, 'test/num_examples': 3003, 'score': 22719.717471837997, 'total_duration': 37126.762442588806, 'accumulated_submission_time': 22719.717471837997, 'accumulated_eval_time': 14404.023327350616, 'accumulated_logging_time': 0.8547759056091309}
I0211 01:06:33.219932 139975130576640 logging_writer.py:48] [65260] accumulated_eval_time=14404.023327, accumulated_logging_time=0.854776, accumulated_submission_time=22719.717472, global_step=65260, preemption_count=0, score=22719.717472, test/accuracy=0.681134, test/bleu=28.409012, test/loss=1.573676, test/num_examples=3003, total_duration=37126.762443, train/accuracy=0.649766, train/bleu=32.400104, train/loss=1.780896, validation/accuracy=0.668783, validation/bleu=28.944756, validation/loss=1.644107, validation/num_examples=3000
I0211 01:06:47.439324 139975138969344 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.37114015221595764, loss=2.8740203380584717
I0211 01:07:22.117615 139975130576640 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.38436359167099, loss=2.917461633682251
I0211 01:07:56.849862 139975138969344 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.34671974182128906, loss=2.8531510829925537
I0211 01:08:31.602619 139975130576640 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3588740825653076, loss=2.916802406311035
I0211 01:09:06.339812 139975138969344 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.3698662221431732, loss=2.9262726306915283
I0211 01:09:41.065672 139975130576640 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.37368255853652954, loss=2.846896171569824
I0211 01:10:15.820002 139975138969344 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.3710727393627167, loss=2.8728041648864746
I0211 01:10:50.564766 139975130576640 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.33971765637397766, loss=2.835028648376465
I0211 01:11:25.308633 139975138969344 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.3657407760620117, loss=2.8840320110321045
I0211 01:12:00.055409 139975130576640 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.35982275009155273, loss=2.888706684112549
I0211 01:12:34.778985 139975138969344 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.3797844648361206, loss=2.9178130626678467
I0211 01:13:09.536134 139975130576640 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.37510398030281067, loss=2.82110595703125
I0211 01:13:44.253606 139975138969344 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.3661774694919586, loss=2.859022378921509
I0211 01:14:19.014442 139975130576640 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.36017340421676636, loss=2.879403829574585
I0211 01:14:53.812907 139975138969344 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.4419693052768707, loss=2.938091516494751
I0211 01:15:28.563307 139975130576640 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.33690670132637024, loss=2.84401798248291
I0211 01:16:03.310634 139975138969344 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.3677331507205963, loss=2.892246723175049
I0211 01:16:38.147645 139975130576640 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.340657114982605, loss=2.8434572219848633
I0211 01:17:12.940838 139975138969344 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3601548373699188, loss=2.906510591506958
I0211 01:17:47.634166 139975130576640 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.35682547092437744, loss=2.838621139526367
I0211 01:18:22.382709 139975138969344 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.36895403265953064, loss=2.893203020095825
I0211 01:18:57.153433 139975130576640 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.36502605676651, loss=2.905402898788452
I0211 01:19:31.910833 139975138969344 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.35332247614860535, loss=2.932079553604126
I0211 01:20:06.679776 139975130576640 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.3653896749019623, loss=2.820197820663452
I0211 01:20:33.523536 140144802662208 spec.py:321] Evaluating on the training split.
I0211 01:20:36.509295 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 01:23:52.957184 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 01:23:55.641031 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 01:26:36.520876 140144802662208 spec.py:349] Evaluating on the test split.
I0211 01:26:39.207732 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 01:29:19.858195 140144802662208 submission_runner.py:408] Time since start: 38493.42s, 	Step: 67679, 	{'train/accuracy': 0.6502645611763, 'train/loss': 1.7860978841781616, 'train/bleu': 31.68374978602823, 'validation/accuracy': 0.6713865995407104, 'validation/loss': 1.6337947845458984, 'validation/bleu': 29.222567907067074, 'validation/num_examples': 3000, 'test/accuracy': 0.6845738291740417, 'test/loss': 1.5635030269622803, 'test/bleu': 29.088031046885384, 'test/num_examples': 3003, 'score': 23559.931114912033, 'total_duration': 38493.42343258858, 'accumulated_submission_time': 23559.931114912033, 'accumulated_eval_time': 14930.35793542862, 'accumulated_logging_time': 0.8874788284301758}
I0211 01:29:19.882161 139975138969344 logging_writer.py:48] [67679] accumulated_eval_time=14930.357935, accumulated_logging_time=0.887479, accumulated_submission_time=23559.931115, global_step=67679, preemption_count=0, score=23559.931115, test/accuracy=0.684574, test/bleu=29.088031, test/loss=1.563503, test/num_examples=3003, total_duration=38493.423433, train/accuracy=0.650265, train/bleu=31.683750, train/loss=1.786098, validation/accuracy=0.671387, validation/bleu=29.222568, validation/loss=1.633795, validation/num_examples=3000
I0211 01:29:27.510222 139975130576640 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3476899564266205, loss=2.8431763648986816
I0211 01:30:02.128215 139975138969344 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3533640503883362, loss=2.8900043964385986
I0211 01:30:36.830678 139975130576640 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.3637946844100952, loss=2.9118764400482178
I0211 01:31:11.649623 139975138969344 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.3818818926811218, loss=2.85843825340271
I0211 01:31:46.466067 139975130576640 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.364311546087265, loss=2.846588611602783
I0211 01:32:21.226642 139975138969344 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.3869923949241638, loss=2.946049451828003
I0211 01:32:56.013210 139975130576640 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3292924165725708, loss=2.821643114089966
I0211 01:33:30.810692 139975138969344 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3668919503688812, loss=2.8323259353637695
I0211 01:34:05.535192 139975130576640 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3659173250198364, loss=2.8849005699157715
I0211 01:34:40.275645 139975138969344 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.34601056575775146, loss=2.834883213043213
I0211 01:35:15.007076 139975130576640 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.3979247510433197, loss=2.9140141010284424
I0211 01:35:49.757985 139975138969344 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.3637126386165619, loss=2.7967145442962646
I0211 01:36:24.503834 139975130576640 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.35031458735466003, loss=2.8812403678894043
I0211 01:36:59.236142 139975138969344 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.34386399388313293, loss=2.8762195110321045
I0211 01:37:33.985052 139975130576640 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.3549096882343292, loss=2.918043851852417
I0211 01:38:08.752347 139975138969344 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.40225088596343994, loss=2.9190921783447266
I0211 01:38:43.480895 139975130576640 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3416622579097748, loss=2.905296802520752
I0211 01:39:18.236866 139975138969344 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.34252673387527466, loss=2.831650495529175
I0211 01:39:53.011812 139975130576640 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.3567074239253998, loss=2.843390464782715
I0211 01:40:27.749498 139975138969344 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3665974736213684, loss=2.838710308074951
I0211 01:41:02.506736 139975130576640 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.3960398733615875, loss=2.9084434509277344
I0211 01:41:37.234706 139975138969344 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.4120461344718933, loss=2.824589967727661
I0211 01:42:11.967697 139975130576640 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.37556374073028564, loss=2.892340898513794
I0211 01:42:46.700661 139975138969344 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3483697474002838, loss=2.8663597106933594
I0211 01:43:20.130578 140144802662208 spec.py:321] Evaluating on the training split.
I0211 01:43:23.102458 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 01:47:03.678304 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 01:47:06.360517 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 01:49:42.471722 140144802662208 spec.py:349] Evaluating on the test split.
I0211 01:49:45.161084 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 01:52:19.522876 140144802662208 submission_runner.py:408] Time since start: 39873.09s, 	Step: 70098, 	{'train/accuracy': 0.6582711338996887, 'train/loss': 1.7285667657852173, 'train/bleu': 32.39691829939411, 'validation/accuracy': 0.671584963798523, 'validation/loss': 1.6267058849334717, 'validation/bleu': 29.149120230108544, 'validation/num_examples': 3000, 'test/accuracy': 0.6856196522712708, 'test/loss': 1.5533530712127686, 'test/bleu': 28.82446304234327, 'test/num_examples': 3003, 'score': 24400.087972164154, 'total_duration': 39873.08808875084, 'accumulated_submission_time': 24400.087972164154, 'accumulated_eval_time': 15469.750158786774, 'accumulated_logging_time': 0.9219081401824951}
I0211 01:52:19.548147 139975130576640 logging_writer.py:48] [70098] accumulated_eval_time=15469.750159, accumulated_logging_time=0.921908, accumulated_submission_time=24400.087972, global_step=70098, preemption_count=0, score=24400.087972, test/accuracy=0.685620, test/bleu=28.824463, test/loss=1.553353, test/num_examples=3003, total_duration=39873.088089, train/accuracy=0.658271, train/bleu=32.396918, train/loss=1.728567, validation/accuracy=0.671585, validation/bleu=29.149120, validation/loss=1.626706, validation/num_examples=3000
I0211 01:52:20.609580 139975138969344 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.38431718945503235, loss=2.8428549766540527
I0211 01:52:55.207584 139975130576640 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.3406656086444855, loss=2.9027857780456543
I0211 01:53:29.892897 139975138969344 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3728577494621277, loss=2.8420279026031494
I0211 01:54:04.637679 139975130576640 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.3653434216976166, loss=2.8018198013305664
I0211 01:54:39.363941 139975138969344 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.36876359581947327, loss=2.8438057899475098
I0211 01:55:14.100296 139975130576640 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.38613054156303406, loss=2.837711811065674
I0211 01:55:48.836387 139975138969344 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.3585345447063446, loss=2.842777967453003
I0211 01:56:23.591093 139975130576640 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.3667052686214447, loss=2.8411190509796143
I0211 01:56:58.350676 139975138969344 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.37430843710899353, loss=2.864048480987549
I0211 01:57:33.058740 139975130576640 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.4076680541038513, loss=2.771554708480835
I0211 01:58:07.810372 139975138969344 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.3614416718482971, loss=2.8495326042175293
I0211 01:58:42.539551 139975130576640 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.3697102963924408, loss=2.928884267807007
I0211 01:59:17.275460 139975138969344 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.37067610025405884, loss=2.856884717941284
I0211 01:59:52.003297 139975130576640 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.35643428564071655, loss=2.855725049972534
I0211 02:00:26.769641 139975138969344 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3378226161003113, loss=2.8681578636169434
I0211 02:01:01.535191 139975130576640 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.3645247220993042, loss=2.845592975616455
I0211 02:01:36.274803 139975138969344 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.39281967282295227, loss=2.8673322200775146
I0211 02:02:11.001689 139975130576640 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.37916299700737, loss=2.8771910667419434
I0211 02:02:45.737288 139975138969344 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.32939857244491577, loss=2.8396551609039307
I0211 02:03:20.486298 139975130576640 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.3680925667285919, loss=2.890812873840332
I0211 02:03:55.236251 139975138969344 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3845524489879608, loss=2.8471527099609375
I0211 02:04:29.987377 139975130576640 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.39625421166419983, loss=2.799652099609375
I0211 02:05:04.726914 139975138969344 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3703228533267975, loss=2.8281147480010986
I0211 02:05:39.493692 139975130576640 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.4019794762134552, loss=2.8968465328216553
I0211 02:06:14.226284 139975138969344 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3867778778076172, loss=2.830470085144043
I0211 02:06:19.857282 140144802662208 spec.py:321] Evaluating on the training split.
I0211 02:06:22.847743 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 02:09:19.056317 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 02:09:21.741117 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 02:12:02.601002 140144802662208 spec.py:349] Evaluating on the test split.
I0211 02:12:05.285821 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 02:14:40.729032 140144802662208 submission_runner.py:408] Time since start: 41214.29s, 	Step: 72518, 	{'train/accuracy': 0.6576229333877563, 'train/loss': 1.7471884489059448, 'train/bleu': 32.81365972041144, 'validation/accuracy': 0.6732960343360901, 'validation/loss': 1.6212689876556396, 'validation/bleu': 29.323903687794544, 'validation/num_examples': 3000, 'test/accuracy': 0.6862123012542725, 'test/loss': 1.5499813556671143, 'test/bleu': 28.91150751075374, 'test/num_examples': 3003, 'score': 25240.307821035385, 'total_duration': 41214.29425621033, 'accumulated_submission_time': 25240.307821035385, 'accumulated_eval_time': 15970.62186050415, 'accumulated_logging_time': 0.957329273223877}
I0211 02:14:40.755498 139975130576640 logging_writer.py:48] [72518] accumulated_eval_time=15970.621861, accumulated_logging_time=0.957329, accumulated_submission_time=25240.307821, global_step=72518, preemption_count=0, score=25240.307821, test/accuracy=0.686212, test/bleu=28.911508, test/loss=1.549981, test/num_examples=3003, total_duration=41214.294256, train/accuracy=0.657623, train/bleu=32.813660, train/loss=1.747188, validation/accuracy=0.673296, validation/bleu=29.323904, validation/loss=1.621269, validation/num_examples=3000
I0211 02:15:09.511510 139975138969344 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.433047890663147, loss=2.8293063640594482
I0211 02:15:44.200932 139975130576640 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.3924673795700073, loss=2.8546664714813232
I0211 02:16:18.957877 139975138969344 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.3486214280128479, loss=2.8717310428619385
I0211 02:16:53.706200 139975130576640 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.3690396249294281, loss=2.8867685794830322
I0211 02:17:28.444610 139975138969344 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.38660183548927307, loss=2.867090940475464
I0211 02:18:03.192251 139975130576640 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3489701747894287, loss=2.8774683475494385
I0211 02:18:37.952504 139975138969344 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3588566780090332, loss=2.8095552921295166
I0211 02:19:12.724074 139975130576640 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.3638390898704529, loss=2.772341728210449
I0211 02:19:47.476099 139975138969344 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3533303141593933, loss=2.791095495223999
I0211 02:20:22.211682 139975130576640 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.3541080057621002, loss=2.848360300064087
I0211 02:20:56.958755 139975138969344 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.37238770723342896, loss=2.911471366882324
I0211 02:21:31.719948 139975130576640 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.38449567556381226, loss=2.8305375576019287
I0211 02:22:06.492418 139975138969344 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.38568297028541565, loss=2.850081205368042
I0211 02:22:41.212489 139975130576640 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.34808099269866943, loss=2.8337771892547607
I0211 02:23:15.969112 139975138969344 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.38755741715431213, loss=2.8912487030029297
I0211 02:23:50.717278 139975130576640 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.38876885175704956, loss=2.8994944095611572
I0211 02:24:25.472178 139975138969344 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.3623087406158447, loss=2.884719133377075
I0211 02:25:00.246703 139975130576640 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.3733859062194824, loss=2.8889503479003906
I0211 02:25:34.987993 139975138969344 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.38848817348480225, loss=2.8293027877807617
I0211 02:26:09.727638 139975130576640 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.38359156250953674, loss=2.771068572998047
I0211 02:26:44.487127 139975138969344 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.3462258279323578, loss=2.7937698364257812
I0211 02:27:19.228286 139975130576640 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.36156389117240906, loss=2.792545795440674
I0211 02:27:53.966518 139975138969344 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.35975417494773865, loss=2.889425039291382
I0211 02:28:28.705663 139975130576640 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.3803251385688782, loss=2.842724084854126
I0211 02:28:40.920755 140144802662208 spec.py:321] Evaluating on the training split.
I0211 02:28:43.900594 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 02:31:38.540802 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 02:31:41.245877 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 02:34:12.604584 140144802662208 spec.py:349] Evaluating on the test split.
I0211 02:34:15.293500 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 02:36:35.602674 140144802662208 submission_runner.py:408] Time since start: 42529.17s, 	Step: 74937, 	{'train/accuracy': 0.6538400053977966, 'train/loss': 1.7619236707687378, 'train/bleu': 32.41990521138852, 'validation/accuracy': 0.6753295063972473, 'validation/loss': 1.6074609756469727, 'validation/bleu': 29.47000517384073, 'validation/num_examples': 3000, 'test/accuracy': 0.6883504986763, 'test/loss': 1.5352879762649536, 'test/bleu': 29.17476942337257, 'test/num_examples': 3003, 'score': 26080.382111549377, 'total_duration': 42529.167917490005, 'accumulated_submission_time': 26080.382111549377, 'accumulated_eval_time': 16445.30373263359, 'accumulated_logging_time': 0.9950253963470459}
I0211 02:36:35.627478 139975138969344 logging_writer.py:48] [74937] accumulated_eval_time=16445.303733, accumulated_logging_time=0.995025, accumulated_submission_time=26080.382112, global_step=74937, preemption_count=0, score=26080.382112, test/accuracy=0.688350, test/bleu=29.174769, test/loss=1.535288, test/num_examples=3003, total_duration=42529.167917, train/accuracy=0.653840, train/bleu=32.419905, train/loss=1.761924, validation/accuracy=0.675330, validation/bleu=29.470005, validation/loss=1.607461, validation/num_examples=3000
I0211 02:36:57.780309 139975130576640 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.36794811487197876, loss=2.813681125640869
I0211 02:37:32.454459 139975138969344 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.3467976748943329, loss=2.7624685764312744
I0211 02:38:07.181188 139975130576640 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3841244876384735, loss=2.821486234664917
I0211 02:38:41.923849 139975138969344 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.3962840139865875, loss=2.8594112396240234
I0211 02:39:16.662642 139975130576640 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.34867623448371887, loss=2.80490779876709
I0211 02:39:51.430513 139975138969344 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.356243371963501, loss=2.855278491973877
I0211 02:40:26.166640 139975130576640 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.37730538845062256, loss=2.8638880252838135
I0211 02:41:00.878469 139975138969344 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.3716140389442444, loss=2.863931655883789
I0211 02:41:35.632061 139975130576640 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.3475792706012726, loss=2.851296901702881
I0211 02:42:10.362241 139975138969344 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.4068402647972107, loss=2.8298985958099365
I0211 02:42:45.111847 139975130576640 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3646553158760071, loss=2.7961578369140625
I0211 02:43:19.836387 139975138969344 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.3625950813293457, loss=2.833228588104248
I0211 02:43:54.574345 139975130576640 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.3574718236923218, loss=2.812213182449341
I0211 02:44:29.301836 139975138969344 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.3850809931755066, loss=2.846182107925415
I0211 02:45:04.074303 139975130576640 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.3626159429550171, loss=2.9233922958374023
I0211 02:45:38.819913 139975138969344 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.37158989906311035, loss=2.825079917907715
I0211 02:46:13.562333 139975130576640 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3730853796005249, loss=2.793164014816284
I0211 02:46:48.318765 139975138969344 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.34687039256095886, loss=2.8542096614837646
I0211 02:47:23.057217 139975130576640 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3669453561306, loss=2.831310272216797
I0211 02:47:57.795965 139975138969344 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.37571030855178833, loss=2.823542356491089
I0211 02:48:32.541829 139975130576640 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.3675709068775177, loss=2.8779091835021973
I0211 02:49:07.269356 139975138969344 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.39055749773979187, loss=2.8205618858337402
I0211 02:49:42.021032 139975130576640 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.3679697811603546, loss=2.8263964653015137
I0211 02:50:16.771539 139975138969344 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.38308900594711304, loss=2.8388850688934326
I0211 02:50:35.942472 140144802662208 spec.py:321] Evaluating on the training split.
I0211 02:50:38.924736 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 02:53:39.109348 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 02:53:41.820656 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 02:56:18.186709 140144802662208 spec.py:349] Evaluating on the test split.
I0211 02:56:20.875630 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 02:58:54.617624 140144802662208 submission_runner.py:408] Time since start: 43868.18s, 	Step: 77357, 	{'train/accuracy': 0.6637595295906067, 'train/loss': 1.6998523473739624, 'train/bleu': 32.32256300712956, 'validation/accuracy': 0.6762842535972595, 'validation/loss': 1.6049737930297852, 'validation/bleu': 29.577603225878857, 'validation/num_examples': 3000, 'test/accuracy': 0.6897449493408203, 'test/loss': 1.5332320928573608, 'test/bleu': 29.111522492945284, 'test/num_examples': 3003, 'score': 26920.607084035873, 'total_duration': 43868.182834625244, 'accumulated_submission_time': 26920.607084035873, 'accumulated_eval_time': 16943.978850841522, 'accumulated_logging_time': 1.0299007892608643}
I0211 02:58:54.644587 139975130576640 logging_writer.py:48] [77357] accumulated_eval_time=16943.978851, accumulated_logging_time=1.029901, accumulated_submission_time=26920.607084, global_step=77357, preemption_count=0, score=26920.607084, test/accuracy=0.689745, test/bleu=29.111522, test/loss=1.533232, test/num_examples=3003, total_duration=43868.182835, train/accuracy=0.663760, train/bleu=32.322563, train/loss=1.699852, validation/accuracy=0.676284, validation/bleu=29.577603, validation/loss=1.604974, validation/num_examples=3000
I0211 02:59:09.869374 139975138969344 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.3687029182910919, loss=2.8411648273468018
I0211 02:59:44.512109 139975130576640 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.4006265699863434, loss=2.844592571258545
I0211 03:00:19.251829 139975138969344 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.39124423265457153, loss=2.8266844749450684
I0211 03:00:53.983830 139975130576640 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.3788607716560364, loss=2.991877555847168
I0211 03:01:28.716880 139975138969344 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.5963325500488281, loss=2.8861992359161377
I0211 03:02:03.438048 139975130576640 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.3810999095439911, loss=2.823634147644043
I0211 03:02:38.162376 139975138969344 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.38765230774879456, loss=2.796070098876953
I0211 03:03:12.893502 139975130576640 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3969780504703522, loss=2.881387710571289
I0211 03:03:47.637692 139975138969344 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.3861624300479889, loss=2.8773953914642334
I0211 03:04:22.411067 139975130576640 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3985893428325653, loss=2.8505706787109375
I0211 03:04:57.151546 139975138969344 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.37096938490867615, loss=2.8148324489593506
I0211 03:05:31.889578 139975130576640 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.36324283480644226, loss=2.8474440574645996
I0211 03:06:06.643438 139975138969344 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3525223135948181, loss=2.839799165725708
I0211 03:06:41.404460 139975130576640 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.3580649793148041, loss=2.8430044651031494
I0211 03:07:16.128419 139975138969344 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3930985629558563, loss=2.771352529525757
I0211 03:07:50.826469 139975130576640 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.4152250289916992, loss=2.810004234313965
I0211 03:08:25.570970 139975138969344 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.38214361667633057, loss=2.814039707183838
I0211 03:09:00.306177 139975130576640 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.36903154850006104, loss=2.870633363723755
I0211 03:09:35.030246 139975138969344 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.3682512640953064, loss=2.8601436614990234
I0211 03:10:09.761370 139975130576640 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.3933565020561218, loss=2.845210313796997
I0211 03:10:44.490832 139975138969344 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.37478724122047424, loss=2.800814628601074
I0211 03:11:19.240025 139975130576640 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.415094792842865, loss=2.904545545578003
I0211 03:11:53.989425 139975138969344 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.3753332197666168, loss=2.8508687019348145
I0211 03:12:28.783842 139975130576640 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.3882572054862976, loss=2.860880136489868
I0211 03:12:54.886270 140144802662208 spec.py:321] Evaluating on the training split.
I0211 03:12:57.849667 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 03:15:48.722013 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 03:15:51.406772 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 03:18:28.981643 140144802662208 spec.py:349] Evaluating on the test split.
I0211 03:18:31.659790 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 03:21:08.224665 140144802662208 submission_runner.py:408] Time since start: 45201.79s, 	Step: 79777, 	{'train/accuracy': 0.6550542116165161, 'train/loss': 1.7492350339889526, 'train/bleu': 32.499716599105, 'validation/accuracy': 0.6763834357261658, 'validation/loss': 1.593187689781189, 'validation/bleu': 29.572896222219057, 'validation/num_examples': 3000, 'test/accuracy': 0.6903492212295532, 'test/loss': 1.5179253816604614, 'test/bleu': 29.494597510156364, 'test/num_examples': 3003, 'score': 27760.757081508636, 'total_duration': 45201.78984117508, 'accumulated_submission_time': 27760.757081508636, 'accumulated_eval_time': 17437.31713628769, 'accumulated_logging_time': 1.0680632591247559}
I0211 03:21:08.255336 139975138969344 logging_writer.py:48] [79777] accumulated_eval_time=17437.317136, accumulated_logging_time=1.068063, accumulated_submission_time=27760.757082, global_step=79777, preemption_count=0, score=27760.757082, test/accuracy=0.690349, test/bleu=29.494598, test/loss=1.517925, test/num_examples=3003, total_duration=45201.789841, train/accuracy=0.655054, train/bleu=32.499717, train/loss=1.749235, validation/accuracy=0.676383, validation/bleu=29.572896, validation/loss=1.593188, validation/num_examples=3000
I0211 03:21:16.672791 139975130576640 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.4002837836742401, loss=2.8441452980041504
I0211 03:21:51.312169 139975138969344 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.3836943805217743, loss=2.8630497455596924
I0211 03:22:26.037093 139975130576640 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.4202423393726349, loss=2.748152494430542
I0211 03:23:00.822660 139975138969344 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.39432254433631897, loss=2.8986001014709473
I0211 03:23:35.555877 139975130576640 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3709912896156311, loss=2.7971630096435547
I0211 03:24:10.270365 139975138969344 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.39267030358314514, loss=2.8675010204315186
I0211 03:24:45.004407 139975130576640 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.37306249141693115, loss=2.8176159858703613
I0211 03:25:19.759544 139975138969344 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.3889355957508087, loss=2.9030568599700928
I0211 03:25:54.488092 139975130576640 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.3583828806877136, loss=2.805004119873047
I0211 03:26:29.240822 139975138969344 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.37066102027893066, loss=2.7921009063720703
I0211 03:27:04.017162 139975130576640 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.3865744471549988, loss=2.8634040355682373
I0211 03:27:38.766381 139975138969344 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.43676602840423584, loss=2.8355159759521484
I0211 03:28:13.499469 139975130576640 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.37165331840515137, loss=2.8681511878967285
I0211 03:28:48.234220 139975138969344 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.3926207423210144, loss=2.790086030960083
I0211 03:29:22.977493 139975130576640 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.3826112151145935, loss=2.8319249153137207
I0211 03:29:57.703924 139975138969344 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.39793288707733154, loss=2.9181137084960938
I0211 03:30:32.439922 139975130576640 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.3791242837905884, loss=2.8129332065582275
I0211 03:31:07.185292 139975138969344 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.37713754177093506, loss=2.75966215133667
I0211 03:31:41.927622 139975130576640 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.4110828936100006, loss=2.8545219898223877
I0211 03:32:16.680135 139975138969344 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.3695835769176483, loss=2.797550916671753
I0211 03:32:51.444218 139975130576640 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.3933188319206238, loss=2.821486234664917
I0211 03:33:26.198534 139975138969344 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.39531081914901733, loss=2.8612968921661377
I0211 03:34:00.934437 139975130576640 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.38747066259384155, loss=2.878809928894043
I0211 03:34:35.690623 139975138969344 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.3860861659049988, loss=2.7629029750823975
I0211 03:35:08.424724 140144802662208 spec.py:321] Evaluating on the training split.
I0211 03:35:11.402775 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 03:38:02.917753 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 03:38:05.605974 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 03:40:44.632656 140144802662208 spec.py:349] Evaluating on the test split.
I0211 03:40:47.316802 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 03:43:22.825273 140144802662208 submission_runner.py:408] Time since start: 46536.39s, 	Step: 82196, 	{'train/accuracy': 0.6707914471626282, 'train/loss': 1.6536064147949219, 'train/bleu': 33.720589661719316, 'validation/accuracy': 0.6779953241348267, 'validation/loss': 1.590328335762024, 'validation/bleu': 29.644675622870558, 'validation/num_examples': 3000, 'test/accuracy': 0.6923130750656128, 'test/loss': 1.5137943029403687, 'test/bleu': 29.512227379029266, 'test/num_examples': 3003, 'score': 28600.833948373795, 'total_duration': 46536.39051222801, 'accumulated_submission_time': 28600.833948373795, 'accumulated_eval_time': 17931.717635393143, 'accumulated_logging_time': 1.1103150844573975}
I0211 03:43:22.851458 139975130576640 logging_writer.py:48] [82196] accumulated_eval_time=17931.717635, accumulated_logging_time=1.110315, accumulated_submission_time=28600.833948, global_step=82196, preemption_count=0, score=28600.833948, test/accuracy=0.692313, test/bleu=29.512227, test/loss=1.513794, test/num_examples=3003, total_duration=46536.390512, train/accuracy=0.670791, train/bleu=33.720590, train/loss=1.653606, validation/accuracy=0.677995, validation/bleu=29.644676, validation/loss=1.590328, validation/num_examples=3000
I0211 03:43:24.595345 139975138969344 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.3671053349971771, loss=2.8694424629211426
I0211 03:43:59.229595 139975130576640 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.396314412355423, loss=2.9044153690338135
I0211 03:44:33.899939 139975138969344 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.35734066367149353, loss=2.8540730476379395
I0211 03:45:08.629603 139975130576640 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.38504743576049805, loss=2.8329455852508545
I0211 03:45:43.374363 139975138969344 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.4161645174026489, loss=2.781384229660034
I0211 03:46:18.107668 139975130576640 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.38556423783302307, loss=2.838972330093384
I0211 03:46:52.875836 139975138969344 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.40427273511886597, loss=2.8264291286468506
I0211 03:47:27.635660 139975130576640 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.37447232007980347, loss=2.8220651149749756
I0211 03:48:02.391170 139975138969344 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.3695664405822754, loss=2.780139446258545
I0211 03:48:37.146031 139975130576640 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.3811858594417572, loss=2.8288958072662354
I0211 03:49:11.877354 139975138969344 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.3804045021533966, loss=2.8806521892547607
I0211 03:49:46.652998 139975130576640 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.37941715121269226, loss=2.766906499862671
I0211 03:50:21.379743 139975138969344 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.4013293981552124, loss=2.823697566986084
I0211 03:50:56.171472 139975130576640 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.4038856029510498, loss=2.8244974613189697
I0211 03:51:30.919858 139975138969344 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.38319531083106995, loss=2.826618194580078
I0211 03:52:05.696892 139975130576640 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.424721360206604, loss=2.784158945083618
I0211 03:52:40.422972 139975138969344 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.39815717935562134, loss=2.75400972366333
I0211 03:53:15.181168 139975130576640 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.3896031677722931, loss=2.8633668422698975
I0211 03:53:49.925550 139975138969344 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.3761278986930847, loss=2.777961254119873
I0211 03:54:24.743845 139975130576640 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.4181777238845825, loss=2.7928826808929443
I0211 03:54:59.496301 139975138969344 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3855832517147064, loss=2.839444637298584
I0211 03:55:34.246973 139975130576640 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.3980123996734619, loss=2.7818727493286133
I0211 03:56:08.988527 139975138969344 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.4351189434528351, loss=2.861405849456787
I0211 03:56:43.716819 139975130576640 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3897326588630676, loss=2.7825729846954346
I0211 03:57:18.467102 139975138969344 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.3736846148967743, loss=2.8146893978118896
I0211 03:57:23.052725 140144802662208 spec.py:321] Evaluating on the training split.
I0211 03:57:26.024591 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 04:00:58.421324 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 04:01:01.101248 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 04:03:33.049479 140144802662208 spec.py:349] Evaluating on the test split.
I0211 04:03:35.744156 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 04:06:02.086175 140144802662208 submission_runner.py:408] Time since start: 47895.65s, 	Step: 84615, 	{'train/accuracy': 0.6634590029716492, 'train/loss': 1.7018529176712036, 'train/bleu': 32.766808653370965, 'validation/accuracy': 0.6799171566963196, 'validation/loss': 1.586037039756775, 'validation/bleu': 29.755778263877012, 'validation/num_examples': 3000, 'test/accuracy': 0.693312406539917, 'test/loss': 1.509398102760315, 'test/bleu': 30.06056970667257, 'test/num_examples': 3003, 'score': 29440.945076942444, 'total_duration': 47895.65140795708, 'accumulated_submission_time': 29440.945076942444, 'accumulated_eval_time': 18450.751024246216, 'accumulated_logging_time': 1.1467373371124268}
I0211 04:06:02.112802 139975130576640 logging_writer.py:48] [84615] accumulated_eval_time=18450.751024, accumulated_logging_time=1.146737, accumulated_submission_time=29440.945077, global_step=84615, preemption_count=0, score=29440.945077, test/accuracy=0.693312, test/bleu=30.060570, test/loss=1.509398, test/num_examples=3003, total_duration=47895.651408, train/accuracy=0.663459, train/bleu=32.766809, train/loss=1.701853, validation/accuracy=0.679917, validation/bleu=29.755778, validation/loss=1.586037, validation/num_examples=3000
I0211 04:06:31.876233 139975138969344 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.38603484630584717, loss=2.800290822982788
I0211 04:07:06.681311 139975130576640 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.406923770904541, loss=2.7562029361724854
I0211 04:07:41.458157 139975138969344 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.37954628467559814, loss=2.8365964889526367
I0211 04:08:16.183981 139975130576640 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.38595861196517944, loss=2.789186716079712
I0211 04:08:50.934861 139975138969344 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.36442482471466064, loss=2.7784805297851562
I0211 04:09:25.699476 139975130576640 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.404991090297699, loss=2.8398687839508057
I0211 04:10:00.472433 139975138969344 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.38696396350860596, loss=2.8256497383117676
I0211 04:10:35.225762 139975130576640 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.38320592045783997, loss=2.809814691543579
I0211 04:11:09.950015 139975138969344 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.4079650938510895, loss=2.870119571685791
I0211 04:11:44.697420 139975130576640 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.437759131193161, loss=2.750066041946411
I0211 04:12:19.464757 139975138969344 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.40073657035827637, loss=2.846881628036499
I0211 04:12:54.224372 139975130576640 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.4058741629123688, loss=2.8926877975463867
I0211 04:13:28.984919 139975138969344 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.46252915263175964, loss=2.88008451461792
I0211 04:14:03.721349 139975130576640 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.39379286766052246, loss=2.8432772159576416
I0211 04:14:38.489049 139975138969344 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.3803946077823639, loss=2.76688551902771
I0211 04:15:13.234454 139975130576640 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.4124826192855835, loss=2.7818822860717773
I0211 04:15:47.965352 139975138969344 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.3968024253845215, loss=2.7759480476379395
I0211 04:16:22.729845 139975130576640 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4081819951534271, loss=2.887720823287964
I0211 04:16:57.464214 139975138969344 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.408660888671875, loss=2.7950732707977295
I0211 04:17:32.228804 139975130576640 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.4120994508266449, loss=2.773103952407837
I0211 04:18:07.121632 139975138969344 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.434670627117157, loss=2.814995765686035
I0211 04:18:41.876282 139975130576640 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.43255987763404846, loss=2.806698799133301
I0211 04:19:16.649783 139975138969344 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4105934500694275, loss=2.784160852432251
I0211 04:19:51.399979 139975130576640 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.39225050806999207, loss=2.767441987991333
I0211 04:20:02.249861 140144802662208 spec.py:321] Evaluating on the training split.
I0211 04:20:05.227694 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 04:23:06.446648 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 04:23:09.135567 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 04:25:47.610310 140144802662208 spec.py:349] Evaluating on the test split.
I0211 04:25:50.287168 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 04:28:30.470897 140144802662208 submission_runner.py:408] Time since start: 49244.04s, 	Step: 87033, 	{'train/accuracy': 0.6625377535820007, 'train/loss': 1.709887146949768, 'train/bleu': 32.974561510453555, 'validation/accuracy': 0.6813058853149414, 'validation/loss': 1.572974443435669, 'validation/bleu': 29.834047284658478, 'validation/num_examples': 3000, 'test/accuracy': 0.6963453888893127, 'test/loss': 1.4961185455322266, 'test/bleu': 29.654388366237686, 'test/num_examples': 3003, 'score': 30280.988560199738, 'total_duration': 49244.036138772964, 'accumulated_submission_time': 30280.988560199738, 'accumulated_eval_time': 18958.97201180458, 'accumulated_logging_time': 1.1856064796447754}
I0211 04:28:30.496912 139975138969344 logging_writer.py:48] [87033] accumulated_eval_time=18958.972012, accumulated_logging_time=1.185606, accumulated_submission_time=30280.988560, global_step=87033, preemption_count=0, score=30280.988560, test/accuracy=0.696345, test/bleu=29.654388, test/loss=1.496119, test/num_examples=3003, total_duration=49244.036139, train/accuracy=0.662538, train/bleu=32.974562, train/loss=1.709887, validation/accuracy=0.681306, validation/bleu=29.834047, validation/loss=1.572974, validation/num_examples=3000
I0211 04:28:54.039395 139975130576640 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.3942829966545105, loss=2.756822347640991
I0211 04:29:28.731591 139975138969344 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.39514243602752686, loss=2.807537317276001
I0211 04:30:03.470411 139975130576640 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.4344634413719177, loss=2.7963104248046875
I0211 04:30:38.227046 139975138969344 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.395718514919281, loss=2.7853434085845947
I0211 04:31:12.970747 139975130576640 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4141945540904999, loss=2.8413219451904297
I0211 04:31:47.709758 139975138969344 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.39557671546936035, loss=2.7613308429718018
I0211 04:32:22.501361 139975130576640 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.39642608165740967, loss=2.7970972061157227
I0211 04:32:57.255250 139975138969344 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.40360766649246216, loss=2.7610387802124023
I0211 04:33:32.000645 139975130576640 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.44016534090042114, loss=2.768803834915161
I0211 04:34:06.755581 139975138969344 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.41231364011764526, loss=2.739504337310791
I0211 04:34:41.512644 139975130576640 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3933945596218109, loss=2.8199193477630615
I0211 04:35:16.263711 139975138969344 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.394370436668396, loss=2.7922518253326416
I0211 04:35:51.019353 139975130576640 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.4120981693267822, loss=2.8298349380493164
I0211 04:36:25.764367 139975138969344 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.4114237129688263, loss=2.8079559803009033
I0211 04:37:00.493085 139975130576640 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.40537989139556885, loss=2.797710657119751
I0211 04:37:35.235930 139975138969344 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.4048508405685425, loss=2.7217445373535156
I0211 04:38:10.003978 139975130576640 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.42744526267051697, loss=2.7876617908477783
I0211 04:38:44.742578 139975138969344 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4037386476993561, loss=2.8261067867279053
I0211 04:39:19.499290 139975130576640 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.42772865295410156, loss=2.8144378662109375
I0211 04:39:54.218062 139975138969344 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.4248846173286438, loss=2.7937943935394287
I0211 04:40:28.994229 139975130576640 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.4143218696117401, loss=2.7967076301574707
I0211 04:41:03.720759 139975138969344 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.44261038303375244, loss=2.8041632175445557
I0211 04:41:38.511015 139975130576640 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.4392099976539612, loss=2.789529323577881
I0211 04:42:13.274639 139975138969344 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.39116090536117554, loss=2.760946273803711
I0211 04:42:30.700731 140144802662208 spec.py:321] Evaluating on the training split.
I0211 04:42:33.678782 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 04:46:03.617780 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 04:46:06.306611 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 04:48:43.274939 140144802662208 spec.py:349] Evaluating on the test split.
I0211 04:48:45.965093 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 04:51:26.241348 140144802662208 submission_runner.py:408] Time since start: 50619.81s, 	Step: 89452, 	{'train/accuracy': 0.6736891269683838, 'train/loss': 1.6366500854492188, 'train/bleu': 33.41154952869727, 'validation/accuracy': 0.6804007291793823, 'validation/loss': 1.5719475746154785, 'validation/bleu': 29.573600778223444, 'validation/num_examples': 3000, 'test/accuracy': 0.6961362361907959, 'test/loss': 1.495225429534912, 'test/bleu': 29.73615868519291, 'test/num_examples': 3003, 'score': 31121.10328722, 'total_duration': 50619.80656862259, 'accumulated_submission_time': 31121.10328722, 'accumulated_eval_time': 19494.512558221817, 'accumulated_logging_time': 1.2216103076934814}
I0211 04:51:26.268528 139975130576640 logging_writer.py:48] [89452] accumulated_eval_time=19494.512558, accumulated_logging_time=1.221610, accumulated_submission_time=31121.103287, global_step=89452, preemption_count=0, score=31121.103287, test/accuracy=0.696136, test/bleu=29.736159, test/loss=1.495225, test/num_examples=3003, total_duration=50619.806569, train/accuracy=0.673689, train/bleu=33.411550, train/loss=1.636650, validation/accuracy=0.680401, validation/bleu=29.573601, validation/loss=1.571948, validation/num_examples=3000
I0211 04:51:43.230536 139975138969344 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.4123077094554901, loss=2.7433419227600098
I0211 04:52:17.897458 139975130576640 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.4228379428386688, loss=2.749244213104248
I0211 04:52:52.722808 139975138969344 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4392889738082886, loss=2.793394088745117
I0211 04:53:27.467099 139975130576640 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.42273715138435364, loss=2.7710044384002686
I0211 04:54:02.218526 139975138969344 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.42200446128845215, loss=2.760887384414673
I0211 04:54:36.979341 139975130576640 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.4374758005142212, loss=2.8503966331481934
I0211 04:55:11.744354 139975138969344 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.43438002467155457, loss=2.8145711421966553
I0211 04:55:46.499321 139975130576640 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.4332541823387146, loss=2.828477382659912
I0211 04:56:21.258903 139975138969344 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.3996865451335907, loss=2.8386926651000977
I0211 04:56:56.008555 139975130576640 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.4617253243923187, loss=2.750314474105835
I0211 04:57:30.760239 139975138969344 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.44548410177230835, loss=2.738225221633911
I0211 04:58:05.547903 139975130576640 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.44134798645973206, loss=2.801163911819458
I0211 04:58:40.297233 139975138969344 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.4174022376537323, loss=2.861436128616333
I0211 04:59:15.021944 139975130576640 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4892069697380066, loss=2.8363261222839355
I0211 04:59:49.758943 139975138969344 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.417949914932251, loss=2.7985455989837646
I0211 05:00:24.494256 139975130576640 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.634194552898407, loss=2.7803359031677246
I0211 05:00:59.218862 139975138969344 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.41187313199043274, loss=2.78983473777771
I0211 05:01:33.961479 139975130576640 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.44845548272132874, loss=2.8236160278320312
I0211 05:02:08.722631 139975138969344 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.44008880853652954, loss=2.8252713680267334
I0211 05:02:43.446452 139975130576640 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.4256579875946045, loss=2.740933656692505
I0211 05:03:18.185113 139975138969344 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.43498778343200684, loss=2.749300718307495
I0211 05:03:52.932921 139975130576640 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.43775343894958496, loss=2.772840738296509
I0211 05:04:27.686089 139975138969344 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.44289320707321167, loss=2.836663007736206
I0211 05:05:02.420857 139975130576640 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.4310888946056366, loss=2.7837915420532227
I0211 05:05:26.453463 140144802662208 spec.py:321] Evaluating on the training split.
I0211 05:05:29.430477 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 05:08:37.322893 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 05:08:40.027715 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 05:11:10.788481 140144802662208 spec.py:349] Evaluating on the test split.
I0211 05:11:13.496716 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 05:13:50.663016 140144802662208 submission_runner.py:408] Time since start: 51964.23s, 	Step: 91871, 	{'train/accuracy': 0.6677938103675842, 'train/loss': 1.668520450592041, 'train/bleu': 33.170331808642466, 'validation/accuracy': 0.6823474168777466, 'validation/loss': 1.5652003288269043, 'validation/bleu': 30.001956328565168, 'validation/num_examples': 3000, 'test/accuracy': 0.6966475248336792, 'test/loss': 1.4875853061676025, 'test/bleu': 29.9217194579761, 'test/num_examples': 3003, 'score': 31961.19503569603, 'total_duration': 51964.228222608566, 'accumulated_submission_time': 31961.19503569603, 'accumulated_eval_time': 19998.722029209137, 'accumulated_logging_time': 1.2600123882293701}
I0211 05:13:50.690881 139975138969344 logging_writer.py:48] [91871] accumulated_eval_time=19998.722029, accumulated_logging_time=1.260012, accumulated_submission_time=31961.195036, global_step=91871, preemption_count=0, score=31961.195036, test/accuracy=0.696648, test/bleu=29.921719, test/loss=1.487585, test/num_examples=3003, total_duration=51964.228223, train/accuracy=0.667794, train/bleu=33.170332, train/loss=1.668520, validation/accuracy=0.682347, validation/bleu=30.001956, validation/loss=1.565200, validation/num_examples=3000
I0211 05:14:01.103023 139975130576640 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.43745923042297363, loss=2.7460997104644775
I0211 05:14:35.746730 139975138969344 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.4492033123970032, loss=2.8301961421966553
I0211 05:15:10.457560 139975130576640 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.431723415851593, loss=2.780719041824341
I0211 05:15:45.197690 139975138969344 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.46225598454475403, loss=2.8691351413726807
I0211 05:16:19.945398 139975130576640 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.4280829131603241, loss=2.7503528594970703
I0211 05:16:54.692614 139975138969344 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.4634414315223694, loss=2.867753744125366
I0211 05:17:29.454013 139975130576640 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.45981788635253906, loss=2.7537789344787598
I0211 05:18:04.195354 139975138969344 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.43900707364082336, loss=2.7231414318084717
I0211 05:18:38.955300 139975130576640 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.42512398958206177, loss=2.805331230163574
I0211 05:19:13.701786 139975138969344 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.44801419973373413, loss=2.8297722339630127
I0211 05:19:48.439212 139975130576640 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.4484519362449646, loss=2.8619308471679688
I0211 05:20:23.204636 139975138969344 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.4174957573413849, loss=2.8273098468780518
I0211 05:20:57.967900 139975130576640 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.45147067308425903, loss=2.7894394397735596
I0211 05:21:32.740123 139975138969344 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.4677262306213379, loss=2.7476534843444824
I0211 05:22:07.511903 139975130576640 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.42892616987228394, loss=2.7240493297576904
I0211 05:22:42.257279 139975138969344 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.4514395594596863, loss=2.7711663246154785
I0211 05:23:17.009384 139975130576640 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.4673295021057129, loss=2.723125696182251
I0211 05:23:51.768234 139975138969344 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.43296176195144653, loss=2.7516093254089355
I0211 05:24:26.543895 139975130576640 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.4501665234565735, loss=2.7964107990264893
I0211 05:25:01.316148 139975138969344 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.4183950126171112, loss=2.7153923511505127
I0211 05:25:36.098752 139975130576640 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.45663848519325256, loss=2.7733712196350098
I0211 05:26:10.834413 139975138969344 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.45792749524116516, loss=2.791313648223877
I0211 05:26:45.597239 139975130576640 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.4488207697868347, loss=2.8559839725494385
I0211 05:27:20.345090 139975138969344 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.457763671875, loss=2.7459676265716553
I0211 05:27:50.972901 140144802662208 spec.py:321] Evaluating on the training split.
I0211 05:27:53.945940 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 05:30:53.113929 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 05:30:55.799840 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 05:33:24.335598 140144802662208 spec.py:349] Evaluating on the test split.
I0211 05:33:27.023518 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 05:35:53.959587 140144802662208 submission_runner.py:408] Time since start: 53287.52s, 	Step: 94290, 	{'train/accuracy': 0.6874837279319763, 'train/loss': 1.560304880142212, 'train/bleu': 34.74978426592824, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.5531262159347534, 'validation/bleu': 30.264827902506898, 'validation/num_examples': 3000, 'test/accuracy': 0.6995874643325806, 'test/loss': 1.4738984107971191, 'test/bleu': 30.343516684026156, 'test/num_examples': 3003, 'score': 32801.38553190231, 'total_duration': 53287.52480864525, 'accumulated_submission_time': 32801.38553190231, 'accumulated_eval_time': 20481.708650112152, 'accumulated_logging_time': 1.298454761505127}
I0211 05:35:53.987301 139975130576640 logging_writer.py:48] [94290] accumulated_eval_time=20481.708650, accumulated_logging_time=1.298455, accumulated_submission_time=32801.385532, global_step=94290, preemption_count=0, score=32801.385532, test/accuracy=0.699587, test/bleu=30.343517, test/loss=1.473898, test/num_examples=3003, total_duration=53287.524809, train/accuracy=0.687484, train/bleu=34.749784, train/loss=1.560305, validation/accuracy=0.684964, validation/bleu=30.264828, validation/loss=1.553126, validation/num_examples=3000
I0211 05:35:57.810619 139975138969344 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.45525819063186646, loss=2.712568521499634
I0211 05:36:32.464254 139975130576640 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.4476432800292969, loss=2.742765426635742
I0211 05:37:07.196927 139975138969344 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4313097298145294, loss=2.7347702980041504
I0211 05:37:41.941417 139975130576640 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.46511003375053406, loss=2.77537202835083
I0211 05:38:16.686371 139975138969344 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.4786888360977173, loss=2.7416019439697266
I0211 05:38:51.455582 139975130576640 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.4649670720100403, loss=2.7565577030181885
I0211 05:39:26.232714 139975138969344 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.4351555109024048, loss=2.8280651569366455
I0211 05:40:01.026476 139975130576640 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.4431900978088379, loss=2.736917018890381
I0211 05:40:35.810163 139975138969344 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.43964850902557373, loss=2.736668348312378
I0211 05:41:10.564428 139975130576640 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.43779486417770386, loss=2.754154682159424
I0211 05:41:45.320984 139975138969344 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.4452337622642517, loss=2.825968027114868
I0211 05:42:20.082016 139975130576640 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.4440062344074249, loss=2.7697854042053223
I0211 05:42:54.825056 139975138969344 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.4864150285720825, loss=2.743811845779419
I0211 05:43:29.582235 139975130576640 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.4340050220489502, loss=2.7626729011535645
I0211 05:44:04.325947 139975138969344 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.4814029932022095, loss=2.8239145278930664
I0211 05:44:39.098838 139975130576640 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.46094080805778503, loss=2.7048254013061523
I0211 05:45:13.847651 139975138969344 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.4643574357032776, loss=2.710969924926758
I0211 05:45:48.633824 139975130576640 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.47049593925476074, loss=2.783371925354004
I0211 05:46:23.370812 139975138969344 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.4530457556247711, loss=2.7798237800598145
I0211 05:46:58.112311 139975130576640 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.46767115592956543, loss=2.6558549404144287
I0211 05:47:32.858135 139975138969344 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.4747313857078552, loss=2.7480101585388184
I0211 05:48:07.611348 139975130576640 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.47122666239738464, loss=2.7279534339904785
I0211 05:48:42.341372 139975138969344 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.45993712544441223, loss=2.756685972213745
I0211 05:49:17.103709 139975130576640 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.4724229872226715, loss=2.8016934394836426
I0211 05:49:51.851191 139975138969344 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.4542865455150604, loss=2.7431299686431885
I0211 05:49:54.005020 140144802662208 spec.py:321] Evaluating on the training split.
I0211 05:49:56.983673 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 05:53:22.764424 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 05:53:25.448863 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 05:56:00.184148 140144802662208 spec.py:349] Evaluating on the test split.
I0211 05:56:02.888758 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 05:58:29.085777 140144802662208 submission_runner.py:408] Time since start: 54642.65s, 	Step: 96708, 	{'train/accuracy': 0.6748186945915222, 'train/loss': 1.623995304107666, 'train/bleu': 33.85164936344683, 'validation/accuracy': 0.6853107810020447, 'validation/loss': 1.5497137308120728, 'validation/bleu': 30.209081609458195, 'validation/num_examples': 3000, 'test/accuracy': 0.700888991355896, 'test/loss': 1.4674443006515503, 'test/bleu': 30.135616820141905, 'test/num_examples': 3003, 'score': 33641.31292676926, 'total_duration': 54642.65100026131, 'accumulated_submission_time': 33641.31292676926, 'accumulated_eval_time': 20996.7893345356, 'accumulated_logging_time': 1.3360624313354492}
I0211 05:58:29.113159 139975130576640 logging_writer.py:48] [96708] accumulated_eval_time=20996.789335, accumulated_logging_time=1.336062, accumulated_submission_time=33641.312927, global_step=96708, preemption_count=0, score=33641.312927, test/accuracy=0.700889, test/bleu=30.135617, test/loss=1.467444, test/num_examples=3003, total_duration=54642.651000, train/accuracy=0.674819, train/bleu=33.851649, train/loss=1.623995, validation/accuracy=0.685311, validation/bleu=30.209082, validation/loss=1.549714, validation/num_examples=3000
I0211 05:59:01.331184 139975138969344 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.4420545995235443, loss=2.761843681335449
I0211 05:59:35.998527 139975130576640 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.4689716100692749, loss=2.7625327110290527
I0211 06:00:10.726412 139975138969344 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.45937034487724304, loss=2.760207414627075
I0211 06:00:45.469977 139975130576640 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.46044591069221497, loss=2.691368341445923
I0211 06:01:20.215930 139975138969344 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.47392573952674866, loss=2.7658584117889404
I0211 06:01:54.940850 139975130576640 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.4651755690574646, loss=2.7942774295806885
I0211 06:02:29.682119 139975138969344 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.45296400785446167, loss=2.730414628982544
I0211 06:03:04.432987 139975130576640 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.4742202162742615, loss=2.7025885581970215
I0211 06:03:39.189889 139975138969344 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.4635511636734009, loss=2.7679107189178467
I0211 06:04:13.946123 139975130576640 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.4903380572795868, loss=2.741945505142212
I0211 06:04:48.732472 139975138969344 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.49221327900886536, loss=2.7749459743499756
I0211 06:05:23.509879 139975130576640 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.46667519211769104, loss=2.7631101608276367
I0211 06:05:58.262530 139975138969344 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.4895700514316559, loss=2.742006540298462
I0211 06:06:33.006779 139975130576640 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.48828327655792236, loss=2.777534008026123
I0211 06:07:07.757760 139975138969344 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.4683261513710022, loss=2.7330968379974365
I0211 06:07:42.475670 139975130576640 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.4786525070667267, loss=2.7202467918395996
I0211 06:08:17.220862 139975138969344 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.49225467443466187, loss=2.702211856842041
I0211 06:08:51.975954 139975130576640 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.47723671793937683, loss=2.738010883331299
I0211 06:09:26.751337 139975138969344 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.4851495325565338, loss=2.7612831592559814
I0211 06:10:01.512099 139975130576640 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.47445255517959595, loss=2.7371203899383545
I0211 06:10:36.264842 139975138969344 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.49756190180778503, loss=2.676790714263916
I0211 06:11:10.996587 139975130576640 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.48282957077026367, loss=2.77211332321167
I0211 06:11:45.737900 139975138969344 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5030858516693115, loss=2.7019333839416504
I0211 06:12:20.508805 139975130576640 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5113677978515625, loss=2.7352840900421143
I0211 06:12:29.266893 140144802662208 spec.py:321] Evaluating on the training split.
I0211 06:12:32.239981 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 06:15:53.722770 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 06:15:56.400396 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 06:18:28.609940 140144802662208 spec.py:349] Evaluating on the test split.
I0211 06:18:31.305391 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 06:21:09.326338 140144802662208 submission_runner.py:408] Time since start: 56002.89s, 	Step: 99127, 	{'train/accuracy': 0.6738321185112, 'train/loss': 1.6364940404891968, 'train/bleu': 33.38403674379716, 'validation/accuracy': 0.6848272085189819, 'validation/loss': 1.545107364654541, 'validation/bleu': 30.294506822664715, 'validation/num_examples': 3000, 'test/accuracy': 0.7012376189231873, 'test/loss': 1.4639596939086914, 'test/bleu': 30.208697155291638, 'test/num_examples': 3003, 'score': 34481.37762069702, 'total_duration': 56002.891575336456, 'accumulated_submission_time': 34481.37762069702, 'accumulated_eval_time': 21516.848722696304, 'accumulated_logging_time': 1.3743152618408203}
I0211 06:21:09.355089 139975138969344 logging_writer.py:48] [99127] accumulated_eval_time=21516.848723, accumulated_logging_time=1.374315, accumulated_submission_time=34481.377621, global_step=99127, preemption_count=0, score=34481.377621, test/accuracy=0.701238, test/bleu=30.208697, test/loss=1.463960, test/num_examples=3003, total_duration=56002.891575, train/accuracy=0.673832, train/bleu=33.384037, train/loss=1.636494, validation/accuracy=0.684827, validation/bleu=30.294507, validation/loss=1.545107, validation/num_examples=3000
I0211 06:21:34.950269 139975130576640 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.4924952983856201, loss=2.6872284412384033
I0211 06:22:09.643125 139975138969344 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.5123789310455322, loss=2.7591030597686768
I0211 06:22:44.362385 139975130576640 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5046500563621521, loss=2.74192214012146
I0211 06:23:19.091756 139975138969344 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.4885141849517822, loss=2.703639030456543
I0211 06:23:53.807919 139975130576640 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.4968789517879486, loss=2.7311999797821045
I0211 06:24:28.544977 139975138969344 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.4988674819469452, loss=2.6896252632141113
I0211 06:25:03.310537 139975130576640 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.48644348978996277, loss=2.6929240226745605
I0211 06:25:38.049477 139975138969344 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.48995551466941833, loss=2.753004550933838
I0211 06:26:12.783865 139975130576640 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.4876081049442291, loss=2.7495126724243164
I0211 06:26:47.558598 139975138969344 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5138220191001892, loss=2.76785945892334
I0211 06:27:22.298416 139975130576640 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5027133226394653, loss=2.742511034011841
I0211 06:27:57.006933 139975138969344 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5058997869491577, loss=2.6981124877929688
I0211 06:28:31.745942 139975130576640 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.4982759654521942, loss=2.7341725826263428
I0211 06:29:06.509395 139975138969344 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.528106689453125, loss=2.7462027072906494
I0211 06:29:41.291651 139975130576640 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.5096952319145203, loss=2.686704397201538
I0211 06:30:16.036347 139975138969344 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5324611663818359, loss=2.7229971885681152
I0211 06:30:50.767112 139975130576640 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5259591341018677, loss=2.730842351913452
I0211 06:31:25.542905 139975138969344 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.48284006118774414, loss=2.7385318279266357
I0211 06:32:00.288359 139975130576640 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5388860106468201, loss=2.7441627979278564
I0211 06:32:35.051058 139975138969344 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.4970210790634155, loss=2.6951112747192383
I0211 06:33:09.779983 139975130576640 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5149274468421936, loss=2.717144727706909
I0211 06:33:44.504688 139975138969344 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.516277551651001, loss=2.6942989826202393
I0211 06:34:19.245220 139975130576640 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.48906129598617554, loss=2.7244815826416016
I0211 06:34:54.002823 139975138969344 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5144104361534119, loss=2.700345039367676
I0211 06:35:09.357165 140144802662208 spec.py:321] Evaluating on the training split.
I0211 06:35:12.339744 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 06:37:46.322586 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 06:37:49.016012 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 06:40:14.820395 140144802662208 spec.py:349] Evaluating on the test split.
I0211 06:40:17.507931 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 06:42:54.419955 140144802662208 submission_runner.py:408] Time since start: 57307.99s, 	Step: 101546, 	{'train/accuracy': 0.6848159432411194, 'train/loss': 1.569870948791504, 'train/bleu': 34.37797153427266, 'validation/accuracy': 0.6859679222106934, 'validation/loss': 1.5355159044265747, 'validation/bleu': 30.165309046410087, 'validation/num_examples': 3000, 'test/accuracy': 0.7024809718132019, 'test/loss': 1.4583240747451782, 'test/bleu': 30.57824113953, 'test/num_examples': 3003, 'score': 35321.28910493851, 'total_duration': 57307.98519515991, 'accumulated_submission_time': 35321.28910493851, 'accumulated_eval_time': 21981.911460876465, 'accumulated_logging_time': 1.413140058517456}
I0211 06:42:54.448445 139975130576640 logging_writer.py:48] [101546] accumulated_eval_time=21981.911461, accumulated_logging_time=1.413140, accumulated_submission_time=35321.289105, global_step=101546, preemption_count=0, score=35321.289105, test/accuracy=0.702481, test/bleu=30.578241, test/loss=1.458324, test/num_examples=3003, total_duration=57307.985195, train/accuracy=0.684816, train/bleu=34.377972, train/loss=1.569871, validation/accuracy=0.685968, validation/bleu=30.165309, validation/loss=1.535516, validation/num_examples=3000
I0211 06:43:13.485655 139975138969344 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.4995958209037781, loss=2.783734083175659
I0211 06:43:48.104599 139975130576640 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.49856331944465637, loss=2.7208595275878906
I0211 06:44:22.831843 139975138969344 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5301160216331482, loss=2.6905274391174316
I0211 06:44:57.581225 139975130576640 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5129609704017639, loss=2.801487445831299
I0211 06:45:32.310325 139975138969344 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5196376442909241, loss=2.700532913208008
I0211 06:46:07.034532 139975130576640 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.5318960547447205, loss=2.671785831451416
I0211 06:46:41.770631 139975138969344 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.503707766532898, loss=2.746131420135498
I0211 06:47:16.547541 139975130576640 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5232406258583069, loss=2.820437431335449
I0211 06:47:51.294853 139975138969344 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5230932235717773, loss=2.7837674617767334
I0211 06:48:26.149107 139975130576640 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.553551435470581, loss=2.703606367111206
I0211 06:49:00.926578 139975138969344 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.48440295457839966, loss=2.6521098613739014
I0211 06:49:35.674391 139975130576640 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5287815928459167, loss=2.678069829940796
I0211 06:50:10.399821 139975138969344 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.5181781649589539, loss=2.676715612411499
I0211 06:50:45.144011 139975130576640 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5089771151542664, loss=2.756180763244629
I0211 06:51:19.915974 139975138969344 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.553922712802887, loss=2.7167203426361084
I0211 06:51:54.704126 139975130576640 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.5084620118141174, loss=2.6713759899139404
I0211 06:52:29.442576 139975138969344 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.5220454335212708, loss=2.61218523979187
I0211 06:53:04.179913 139975130576640 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5124489068984985, loss=2.6458733081817627
I0211 06:53:38.929464 139975138969344 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.520721435546875, loss=2.71718168258667
I0211 06:54:13.704973 139975130576640 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.5335922241210938, loss=2.691736936569214
I0211 06:54:48.466695 139975138969344 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.5526816844940186, loss=2.6887872219085693
I0211 06:55:23.205611 139975130576640 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.5516568422317505, loss=2.7207579612731934
I0211 06:55:57.927122 139975138969344 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.5290833115577698, loss=2.7121083736419678
I0211 06:56:32.661557 139975130576640 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.5716490149497986, loss=2.7697041034698486
I0211 06:56:54.606147 140144802662208 spec.py:321] Evaluating on the training split.
I0211 06:56:57.587569 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 07:00:12.113547 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 07:00:14.796706 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 07:02:38.128332 140144802662208 spec.py:349] Evaluating on the test split.
I0211 07:02:40.815441 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 07:05:03.911838 140144802662208 submission_runner.py:408] Time since start: 58637.48s, 	Step: 103965, 	{'train/accuracy': 0.6798616647720337, 'train/loss': 1.5981993675231934, 'train/bleu': 34.46955027904853, 'validation/accuracy': 0.688063383102417, 'validation/loss': 1.5368680953979492, 'validation/bleu': 30.38069848744398, 'validation/num_examples': 3000, 'test/accuracy': 0.7020859122276306, 'test/loss': 1.455241084098816, 'test/bleu': 30.37345328936431, 'test/num_examples': 3003, 'score': 36161.35554885864, 'total_duration': 58637.47705602646, 'accumulated_submission_time': 36161.35554885864, 'accumulated_eval_time': 22471.217081546783, 'accumulated_logging_time': 1.451249599456787}
I0211 07:05:03.940550 139975138969344 logging_writer.py:48] [103965] accumulated_eval_time=22471.217082, accumulated_logging_time=1.451250, accumulated_submission_time=36161.355549, global_step=103965, preemption_count=0, score=36161.355549, test/accuracy=0.702086, test/bleu=30.373453, test/loss=1.455241, test/num_examples=3003, total_duration=58637.477056, train/accuracy=0.679862, train/bleu=34.469550, train/loss=1.598199, validation/accuracy=0.688063, validation/bleu=30.380698, validation/loss=1.536868, validation/num_examples=3000
I0211 07:05:16.403041 139975130576640 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5541217923164368, loss=2.7173705101013184
I0211 07:05:51.029941 139975138969344 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.5446838140487671, loss=2.7171480655670166
I0211 07:06:25.775437 139975130576640 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.5633269548416138, loss=2.7319350242614746
I0211 07:07:00.549826 139975138969344 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.5421068668365479, loss=2.7570996284484863
I0211 07:07:35.325307 139975130576640 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.5165677070617676, loss=2.668407917022705
I0211 07:08:10.195536 139975138969344 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.5502688884735107, loss=2.723562240600586
I0211 07:08:44.942525 139975130576640 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5283502340316772, loss=2.694075345993042
I0211 07:09:19.693351 139975138969344 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.5485909581184387, loss=2.722501277923584
I0211 07:09:54.446108 139975130576640 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.5503085255622864, loss=2.7605977058410645
I0211 07:10:29.224866 139975138969344 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.5484923124313354, loss=2.708643913269043
I0211 07:11:03.993219 139975130576640 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5571211576461792, loss=2.7489395141601562
I0211 07:11:38.743878 139975138969344 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5487668514251709, loss=2.707444906234741
I0211 07:12:13.488756 139975130576640 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.561536431312561, loss=2.752932071685791
I0211 07:12:48.267400 139975138969344 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.5538227558135986, loss=2.6626811027526855
I0211 07:13:23.053415 139975130576640 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.5396672487258911, loss=2.6685004234313965
I0211 07:13:57.802086 139975138969344 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.5651776194572449, loss=2.6746041774749756
I0211 07:14:32.569570 139975130576640 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.5603107810020447, loss=2.7261641025543213
I0211 07:15:07.331390 139975138969344 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.5234452486038208, loss=2.6925973892211914
I0211 07:15:42.079884 139975130576640 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.5778752565383911, loss=2.7369801998138428
I0211 07:16:16.861312 139975138969344 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.541204035282135, loss=2.704636335372925
I0211 07:16:51.633102 139975130576640 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.5612587928771973, loss=2.6877875328063965
I0211 07:17:26.392592 139975138969344 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.5687614679336548, loss=2.7829177379608154
I0211 07:18:01.126830 139975130576640 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.5320261716842651, loss=2.7252626419067383
I0211 07:18:35.906728 139975138969344 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.547516942024231, loss=2.749983549118042
I0211 07:19:04.127097 140144802662208 spec.py:321] Evaluating on the training split.
I0211 07:19:07.115928 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 07:22:26.381028 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 07:22:29.076819 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 07:24:59.564372 140144802662208 spec.py:349] Evaluating on the test split.
I0211 07:25:02.267931 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 07:27:31.873667 140144802662208 submission_runner.py:408] Time since start: 59985.44s, 	Step: 106383, 	{'train/accuracy': 0.7143574953079224, 'train/loss': 1.4251322746276855, 'train/bleu': 36.8438553370823, 'validation/accuracy': 0.6882121562957764, 'validation/loss': 1.5287905931472778, 'validation/bleu': 30.254751595029035, 'validation/num_examples': 3000, 'test/accuracy': 0.7030503749847412, 'test/loss': 1.4487695693969727, 'test/bleu': 30.489053354388908, 'test/num_examples': 3003, 'score': 37001.44723653793, 'total_duration': 59985.43886613846, 'accumulated_submission_time': 37001.44723653793, 'accumulated_eval_time': 22978.963569164276, 'accumulated_logging_time': 1.4913089275360107}
I0211 07:27:31.902399 139975130576640 logging_writer.py:48] [106383] accumulated_eval_time=22978.963569, accumulated_logging_time=1.491309, accumulated_submission_time=37001.447237, global_step=106383, preemption_count=0, score=37001.447237, test/accuracy=0.703050, test/bleu=30.489053, test/loss=1.448770, test/num_examples=3003, total_duration=59985.438866, train/accuracy=0.714357, train/bleu=36.843855, train/loss=1.425132, validation/accuracy=0.688212, validation/bleu=30.254752, validation/loss=1.528791, validation/num_examples=3000
I0211 07:27:38.158107 139975138969344 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.5800271034240723, loss=2.7074222564697266
I0211 07:28:12.760772 139975130576640 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.6261245608329773, loss=2.7074501514434814
I0211 07:28:47.446919 139975138969344 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.561184823513031, loss=2.659661054611206
I0211 07:29:22.201482 139975130576640 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.5792320370674133, loss=2.687201499938965
I0211 07:29:56.949344 139975138969344 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.5847193598747253, loss=2.7277307510375977
I0211 07:30:31.714809 139975130576640 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.5875439047813416, loss=2.6772711277008057
I0211 07:31:06.455104 139975138969344 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.5824331045150757, loss=2.784168004989624
I0211 07:31:41.213563 139975130576640 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.5592276453971863, loss=2.6475253105163574
I0211 07:32:15.996765 139975138969344 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.5422750115394592, loss=2.6607167720794678
I0211 07:32:50.757939 139975130576640 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.5567176938056946, loss=2.669642686843872
I0211 07:33:25.521666 139975138969344 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.5724276304244995, loss=2.727492332458496
I0211 07:34:00.307331 139975130576640 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.5699822902679443, loss=2.663071393966675
I0211 07:34:35.078305 139975138969344 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.5804151892662048, loss=2.6077189445495605
I0211 07:35:09.864914 139975130576640 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.6032023429870605, loss=2.706359624862671
I0211 07:35:44.653805 139975138969344 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.5791835784912109, loss=2.6602110862731934
I0211 07:36:19.407552 139975130576640 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.5897185206413269, loss=2.6609549522399902
I0211 07:36:54.148497 139975138969344 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.5795257687568665, loss=2.7275948524475098
I0211 07:37:28.949780 139975130576640 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.5819653868675232, loss=2.6954877376556396
I0211 07:38:03.738824 139975138969344 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.6032306551933289, loss=2.7314271926879883
I0211 07:38:38.487436 139975130576640 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.5987319350242615, loss=2.7197840213775635
I0211 07:39:13.250025 139975138969344 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.5965192914009094, loss=2.6525797843933105
I0211 07:39:48.027256 139975130576640 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.5890040397644043, loss=2.6949715614318848
I0211 07:40:22.808196 139975138969344 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.5715120434761047, loss=2.6786813735961914
I0211 07:40:57.566621 139975130576640 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.5626710653305054, loss=2.647894859313965
I0211 07:41:32.346105 139975138969344 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.5837288498878479, loss=2.6288230419158936
I0211 07:41:32.352698 140144802662208 spec.py:321] Evaluating on the training split.
I0211 07:41:35.057178 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 07:44:41.169104 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 07:44:43.867559 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 07:47:20.913836 140144802662208 spec.py:349] Evaluating on the test split.
I0211 07:47:23.611792 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 07:49:58.425399 140144802662208 submission_runner.py:408] Time since start: 61331.99s, 	Step: 108801, 	{'train/accuracy': 0.6898171305656433, 'train/loss': 1.552299976348877, 'train/bleu': 34.717774021813774, 'validation/accuracy': 0.6894644498825073, 'validation/loss': 1.5260159969329834, 'validation/bleu': 30.555977630907655, 'validation/num_examples': 3000, 'test/accuracy': 0.7047237157821655, 'test/loss': 1.4430897235870361, 'test/bleu': 30.474966315456253, 'test/num_examples': 3003, 'score': 37841.80606007576, 'total_duration': 61331.99062085152, 'accumulated_submission_time': 37841.80606007576, 'accumulated_eval_time': 23485.036183595657, 'accumulated_logging_time': 1.5315580368041992}
I0211 07:49:58.455225 139975130576640 logging_writer.py:48] [108801] accumulated_eval_time=23485.036184, accumulated_logging_time=1.531558, accumulated_submission_time=37841.806060, global_step=108801, preemption_count=0, score=37841.806060, test/accuracy=0.704724, test/bleu=30.474966, test/loss=1.443090, test/num_examples=3003, total_duration=61331.990621, train/accuracy=0.689817, train/bleu=34.717774, train/loss=1.552300, validation/accuracy=0.689464, validation/bleu=30.555978, validation/loss=1.526016, validation/num_examples=3000
I0211 07:50:33.053355 139975138969344 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.6013556122779846, loss=2.6423614025115967
I0211 07:51:07.739957 139975130576640 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.5997370481491089, loss=2.732449531555176
I0211 07:51:42.533888 139975138969344 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.5768246650695801, loss=2.65234112739563
I0211 07:52:17.326518 139975130576640 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.5982405543327332, loss=2.6783742904663086
I0211 07:52:52.075960 139975138969344 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.5960463285446167, loss=2.639129161834717
I0211 07:53:26.959190 139975130576640 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.6080130934715271, loss=2.6161530017852783
I0211 07:54:01.730571 139975138969344 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.6376310586929321, loss=2.6991143226623535
I0211 07:54:36.481854 139975130576640 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.6169813275337219, loss=2.6495072841644287
I0211 07:55:11.224077 139975138969344 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.6174927949905396, loss=2.6701574325561523
I0211 07:55:46.021993 139975130576640 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.957398533821106, loss=2.665536642074585
I0211 07:56:20.789944 139975138969344 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.6348278522491455, loss=2.6921918392181396
I0211 07:56:55.547065 139975130576640 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.5978118777275085, loss=2.6148462295532227
I0211 07:57:30.302951 139975138969344 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.6069049835205078, loss=2.6829957962036133
I0211 07:58:05.085284 139975130576640 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.5972442626953125, loss=2.7098891735076904
I0211 07:58:39.824860 139975138969344 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.5909802317619324, loss=2.6251235008239746
I0211 07:59:14.616029 139975130576640 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.6099194884300232, loss=2.649916648864746
I0211 07:59:49.350202 139975138969344 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.62879878282547, loss=2.6571953296661377
I0211 08:00:24.095304 139975130576640 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.6263241767883301, loss=2.596921920776367
I0211 08:00:58.848124 139975138969344 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.6060349941253662, loss=2.6489665508270264
I0211 08:01:33.613932 139975130576640 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.5974387526512146, loss=2.6906256675720215
I0211 08:02:08.350006 139975138969344 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.6080791354179382, loss=2.711845636367798
I0211 08:02:43.096565 139975130576640 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.5992374420166016, loss=2.70564866065979
I0211 08:03:17.847295 139975138969344 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.6191788911819458, loss=2.6980326175689697
I0211 08:03:52.591226 139975130576640 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.6117680072784424, loss=2.6592953205108643
I0211 08:03:58.566649 140144802662208 spec.py:321] Evaluating on the training split.
I0211 08:04:01.553936 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 08:07:02.267629 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 08:07:04.982984 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 08:09:46.065946 140144802662208 spec.py:349] Evaluating on the test split.
I0211 08:09:48.748238 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 08:12:15.643751 140144802662208 submission_runner.py:408] Time since start: 62669.21s, 	Step: 111219, 	{'train/accuracy': 0.6886301040649414, 'train/loss': 1.5508441925048828, 'train/bleu': 34.74471014801158, 'validation/accuracy': 0.6914111375808716, 'validation/loss': 1.5159528255462646, 'validation/bleu': 30.64500096572725, 'validation/num_examples': 3000, 'test/accuracy': 0.7083958387374878, 'test/loss': 1.4284865856170654, 'test/bleu': 30.872641774580426, 'test/num_examples': 3003, 'score': 38681.824439287186, 'total_duration': 62669.208958387375, 'accumulated_submission_time': 38681.824439287186, 'accumulated_eval_time': 23982.11319732666, 'accumulated_logging_time': 1.5726063251495361}
I0211 08:12:15.672925 139975138969344 logging_writer.py:48] [111219] accumulated_eval_time=23982.113197, accumulated_logging_time=1.572606, accumulated_submission_time=38681.824439, global_step=111219, preemption_count=0, score=38681.824439, test/accuracy=0.708396, test/bleu=30.872642, test/loss=1.428487, test/num_examples=3003, total_duration=62669.208958, train/accuracy=0.688630, train/bleu=34.744710, train/loss=1.550844, validation/accuracy=0.691411, validation/bleu=30.645001, validation/loss=1.515953, validation/num_examples=3000
I0211 08:12:44.210268 139975130576640 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.6144291162490845, loss=2.6362764835357666
I0211 08:13:18.932975 139975138969344 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.6485989093780518, loss=2.6846983432769775
I0211 08:13:53.694519 139975130576640 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.6451878547668457, loss=2.6894116401672363
I0211 08:14:28.494169 139975138969344 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.6197229623794556, loss=2.6752126216888428
I0211 08:15:03.263039 139975130576640 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.6602376103401184, loss=2.6780478954315186
I0211 08:15:38.021995 139975138969344 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.6426140666007996, loss=2.6191885471343994
I0211 08:16:12.825456 139975130576640 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.6454641819000244, loss=2.6651861667633057
I0211 08:16:47.617665 139975138969344 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.6437708735466003, loss=2.6021950244903564
I0211 08:17:22.383252 139975130576640 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.6291986107826233, loss=2.621317148208618
I0211 08:17:57.188857 139975138969344 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.6511422991752625, loss=2.7407591342926025
I0211 08:18:32.001146 139975130576640 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.6760209798812866, loss=2.6923229694366455
I0211 08:19:06.792747 139975138969344 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.623359203338623, loss=2.604599714279175
I0211 08:19:41.588509 139975130576640 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.6627195477485657, loss=2.652933359146118
I0211 08:20:16.375638 139975138969344 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.6728960275650024, loss=2.6705338954925537
I0211 08:20:51.169794 139975130576640 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.6162707209587097, loss=2.611499071121216
I0211 08:21:25.976194 139975138969344 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.6291165947914124, loss=2.6406443119049072
I0211 08:22:00.753138 139975130576640 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.6789772510528564, loss=2.6639933586120605
I0211 08:22:35.513983 139975138969344 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.6272211074829102, loss=2.687006950378418
I0211 08:23:10.294076 139975130576640 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.6429402232170105, loss=2.6155197620391846
I0211 08:23:45.093580 139975138969344 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.638579249382019, loss=2.6901230812072754
I0211 08:24:19.869274 139975130576640 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.6552965641021729, loss=2.725614547729492
I0211 08:24:54.642434 139975138969344 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.6527889966964722, loss=2.671360492706299
I0211 08:25:29.429199 139975130576640 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.6268320083618164, loss=2.650308847427368
I0211 08:26:04.239077 139975138969344 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.6340206861495972, loss=2.7008962631225586
I0211 08:26:15.794097 140144802662208 spec.py:321] Evaluating on the training split.
I0211 08:26:18.782655 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 08:29:04.373862 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 08:29:07.068452 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 08:31:35.956126 140144802662208 spec.py:349] Evaluating on the test split.
I0211 08:31:38.641210 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 08:34:05.358799 140144802662208 submission_runner.py:408] Time since start: 63978.92s, 	Step: 113635, 	{'train/accuracy': 0.7003282308578491, 'train/loss': 1.4843988418579102, 'train/bleu': 35.50631142680764, 'validation/accuracy': 0.6920930743217468, 'validation/loss': 1.5145164728164673, 'validation/bleu': 30.741345891342124, 'validation/num_examples': 3000, 'test/accuracy': 0.7083725929260254, 'test/loss': 1.428377389907837, 'test/bleu': 30.95872246263002, 'test/num_examples': 3003, 'score': 39521.848861932755, 'total_duration': 63978.92404127121, 'accumulated_submission_time': 39521.848861932755, 'accumulated_eval_time': 24451.677863121033, 'accumulated_logging_time': 1.6134326457977295}
I0211 08:34:05.388950 139975130576640 logging_writer.py:48] [113635] accumulated_eval_time=24451.677863, accumulated_logging_time=1.613433, accumulated_submission_time=39521.848862, global_step=113635, preemption_count=0, score=39521.848862, test/accuracy=0.708373, test/bleu=30.958722, test/loss=1.428377, test/num_examples=3003, total_duration=63978.924041, train/accuracy=0.700328, train/bleu=35.506311, train/loss=1.484399, validation/accuracy=0.692093, validation/bleu=30.741346, validation/loss=1.514516, validation/num_examples=3000
I0211 08:34:28.262950 139975138969344 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.6674882173538208, loss=2.713104486465454
I0211 08:35:02.923431 139975130576640 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.6510239243507385, loss=2.6351306438446045
I0211 08:35:37.663783 139975138969344 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.6674122214317322, loss=2.7168326377868652
I0211 08:36:12.401189 139975130576640 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.650288999080658, loss=2.609997034072876
I0211 08:36:47.146382 139975138969344 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.6310678720474243, loss=2.589632749557495
I0211 08:37:21.906011 139975130576640 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.6772869229316711, loss=2.596684694290161
I0211 08:37:56.667807 139975138969344 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.6614578366279602, loss=2.6764204502105713
I0211 08:38:31.426034 139975130576640 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.6530629396438599, loss=2.6491122245788574
I0211 08:39:06.160981 139975138969344 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.6478234529495239, loss=2.628636360168457
I0211 08:39:40.925295 139975130576640 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.6442880630493164, loss=2.652606964111328
I0211 08:40:15.677700 139975138969344 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.6788820624351501, loss=2.7040975093841553
I0211 08:40:50.441694 139975130576640 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.6896026730537415, loss=2.6143722534179688
I0211 08:41:25.232771 139975138969344 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.6695559620857239, loss=2.6420302391052246
I0211 08:42:00.046017 139975130576640 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.6582662463188171, loss=2.642272710800171
I0211 08:42:34.818547 139975138969344 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.676045298576355, loss=2.634799003601074
I0211 08:43:09.586390 139975130576640 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.6649565100669861, loss=2.613570213317871
I0211 08:43:44.365813 139975138969344 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.6908288598060608, loss=2.666198968887329
I0211 08:44:19.125782 139975130576640 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.6725761890411377, loss=2.652425527572632
I0211 08:44:53.892440 139975138969344 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.6820173859596252, loss=2.618363380432129
I0211 08:45:28.667390 139975130576640 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.6938309073448181, loss=2.629197835922241
I0211 08:46:03.429264 139975138969344 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.6760430335998535, loss=2.6581919193267822
I0211 08:46:38.210695 139975130576640 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.6723361611366272, loss=2.6130387783050537
I0211 08:47:12.960103 139975138969344 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.6624220609664917, loss=2.6017026901245117
I0211 08:47:47.735213 139975130576640 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.6828877329826355, loss=2.672990560531616
I0211 08:48:05.542378 140144802662208 spec.py:321] Evaluating on the training split.
I0211 08:48:08.517551 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 08:51:02.943279 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 08:51:05.641095 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 08:53:47.012647 140144802662208 spec.py:349] Evaluating on the test split.
I0211 08:53:49.708470 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 08:56:18.861009 140144802662208 submission_runner.py:408] Time since start: 65312.43s, 	Step: 116053, 	{'train/accuracy': 0.6954756379127502, 'train/loss': 1.5078685283660889, 'train/bleu': 35.4937397891447, 'validation/accuracy': 0.6918203234672546, 'validation/loss': 1.5126873254776, 'validation/bleu': 30.846350466007053, 'validation/num_examples': 3000, 'test/accuracy': 0.7091976404190063, 'test/loss': 1.4271239042282104, 'test/bleu': 31.064218560260848, 'test/num_examples': 3003, 'score': 40361.91211247444, 'total_duration': 65312.42619681358, 'accumulated_submission_time': 40361.91211247444, 'accumulated_eval_time': 24944.996393442154, 'accumulated_logging_time': 1.653876543045044}
I0211 08:56:18.897963 139975138969344 logging_writer.py:48] [116053] accumulated_eval_time=24944.996393, accumulated_logging_time=1.653877, accumulated_submission_time=40361.912112, global_step=116053, preemption_count=0, score=40361.912112, test/accuracy=0.709198, test/bleu=31.064219, test/loss=1.427124, test/num_examples=3003, total_duration=65312.426197, train/accuracy=0.695476, train/bleu=35.493740, train/loss=1.507869, validation/accuracy=0.691820, validation/bleu=30.846350, validation/loss=1.512687, validation/num_examples=3000
I0211 08:56:35.548860 139975130576640 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.6605786681175232, loss=2.5934221744537354
I0211 08:57:10.192239 139975138969344 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.6923516392707825, loss=2.634312391281128
I0211 08:57:44.947063 139975130576640 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.6975528001785278, loss=2.582728624343872
I0211 08:58:19.711823 139975138969344 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.6877381801605225, loss=2.599534034729004
I0211 08:58:54.455283 139975130576640 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.6478524804115295, loss=2.6892244815826416
I0211 08:59:29.195533 139975138969344 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.7004785537719727, loss=2.6720495223999023
I0211 09:00:03.960873 139975130576640 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.6560434103012085, loss=2.611241579055786
I0211 09:00:38.702333 139975138969344 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.6843724846839905, loss=2.6971518993377686
I0211 09:01:13.450561 139975130576640 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.6602895259857178, loss=2.678165912628174
I0211 09:01:48.209028 139975138969344 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.6982134580612183, loss=2.6809723377227783
I0211 09:02:22.976578 139975130576640 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.6830813884735107, loss=2.601442813873291
I0211 09:02:57.780066 139975138969344 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.7007237076759338, loss=2.6560373306274414
I0211 09:03:32.675474 139975130576640 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.6980374455451965, loss=2.5793704986572266
I0211 09:04:07.517697 139975138969344 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.6880062818527222, loss=2.712472915649414
I0211 09:04:42.302525 139975130576640 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.6826319098472595, loss=2.6653037071228027
I0211 09:05:17.068241 139975138969344 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.6668294072151184, loss=2.593785524368286
I0211 09:05:51.813832 139975130576640 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.7012579441070557, loss=2.7247235774993896
I0211 09:06:26.607875 139975138969344 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.7063300609588623, loss=2.6522648334503174
I0211 09:07:01.379421 139975130576640 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.7039853930473328, loss=2.5926716327667236
I0211 09:07:36.153867 139975138969344 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.6978278756141663, loss=2.5734570026397705
I0211 09:08:10.915571 139975130576640 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.7003029584884644, loss=2.5791401863098145
I0211 09:08:45.702020 139975138969344 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.709326446056366, loss=2.6222164630889893
I0211 09:09:20.534902 139975130576640 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.6672938466072083, loss=2.6220667362213135
I0211 09:09:55.371938 139975138969344 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.6621264219284058, loss=2.6177778244018555
I0211 09:10:19.085799 140144802662208 spec.py:321] Evaluating on the training split.
I0211 09:10:22.067142 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 09:13:19.602726 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 09:13:22.299397 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 09:16:07.061831 140144802662208 spec.py:349] Evaluating on the test split.
I0211 09:16:09.744242 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 09:18:35.811465 140144802662208 submission_runner.py:408] Time since start: 66649.38s, 	Step: 118470, 	{'train/accuracy': 0.6973755359649658, 'train/loss': 1.5098248720169067, 'train/bleu': 35.351594846978585, 'validation/accuracy': 0.691745936870575, 'validation/loss': 1.5092313289642334, 'validation/bleu': 30.6144301486764, 'validation/num_examples': 3000, 'test/accuracy': 0.7104526162147522, 'test/loss': 1.4224282503128052, 'test/bleu': 30.918699317625673, 'test/num_examples': 3003, 'score': 41202.006234169006, 'total_duration': 66649.37670564651, 'accumulated_submission_time': 41202.006234169006, 'accumulated_eval_time': 25441.722013235092, 'accumulated_logging_time': 1.702117919921875}
I0211 09:18:35.841907 139975130576640 logging_writer.py:48] [118470] accumulated_eval_time=25441.722013, accumulated_logging_time=1.702118, accumulated_submission_time=41202.006234, global_step=118470, preemption_count=0, score=41202.006234, test/accuracy=0.710453, test/bleu=30.918699, test/loss=1.422428, test/num_examples=3003, total_duration=66649.376706, train/accuracy=0.697376, train/bleu=35.351595, train/loss=1.509825, validation/accuracy=0.691746, validation/bleu=30.614430, validation/loss=1.509231, validation/num_examples=3000
I0211 09:18:46.585492 139975138969344 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.6985494494438171, loss=2.6227521896362305
I0211 09:19:21.248755 139975130576640 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.7213995456695557, loss=2.657855987548828
I0211 09:19:55.964543 139975138969344 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.709525465965271, loss=2.685711622238159
I0211 09:20:30.722968 139975130576640 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.6815590858459473, loss=2.61116623878479
I0211 09:21:05.496204 139975138969344 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.6979445815086365, loss=2.629136323928833
I0211 09:21:40.264564 139975130576640 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.7084276080131531, loss=2.6134233474731445
I0211 09:22:14.992859 139975138969344 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.7408649325370789, loss=2.5923120975494385
I0211 09:22:49.742313 139975130576640 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.7085168957710266, loss=2.5680055618286133
I0211 09:23:24.498771 139975138969344 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.710651159286499, loss=2.582749605178833
I0211 09:23:59.245376 139975130576640 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.6891731023788452, loss=2.581948757171631
I0211 09:24:34.019830 139975138969344 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.7076996564865112, loss=2.578352928161621
I0211 09:25:08.788307 139975130576640 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.7186011672019958, loss=2.580263614654541
I0211 09:25:43.541871 139975138969344 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.7368893027305603, loss=2.6415743827819824
I0211 09:26:18.301167 139975130576640 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.689154326915741, loss=2.6063358783721924
I0211 09:26:53.078685 139975138969344 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.720624566078186, loss=2.589353084564209
I0211 09:27:27.838431 139975130576640 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.7224260568618774, loss=2.6042685508728027
I0211 09:28:02.627675 139975138969344 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.7118086218833923, loss=2.674445629119873
I0211 09:28:37.397508 139975130576640 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.7156874537467957, loss=2.6353068351745605
I0211 09:29:12.208293 139975138969344 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.7538852691650391, loss=2.594542980194092
I0211 09:29:46.963605 139975130576640 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.7428133487701416, loss=2.6610851287841797
I0211 09:30:21.700625 139975138969344 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.7237491011619568, loss=2.594515562057495
I0211 09:30:56.486317 139975130576640 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.7181034684181213, loss=2.638866662979126
I0211 09:31:31.275150 139975138969344 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.6964108347892761, loss=2.5667614936828613
I0211 09:32:06.027859 139975130576640 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.7366591095924377, loss=2.566654682159424
I0211 09:32:36.009258 140144802662208 spec.py:321] Evaluating on the training split.
I0211 09:32:38.986487 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 09:35:43.402649 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 09:35:46.099801 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 09:38:23.036757 140144802662208 spec.py:349] Evaluating on the test split.
I0211 09:38:25.741607 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 09:40:57.364330 140144802662208 submission_runner.py:408] Time since start: 67990.93s, 	Step: 120888, 	{'train/accuracy': 0.7043367624282837, 'train/loss': 1.4685285091400146, 'train/bleu': 35.96213979511151, 'validation/accuracy': 0.6929238438606262, 'validation/loss': 1.5071839094161987, 'validation/bleu': 30.69302274081346, 'validation/num_examples': 3000, 'test/accuracy': 0.7091627717018127, 'test/loss': 1.4226619005203247, 'test/bleu': 31.02129557676758, 'test/num_examples': 3003, 'score': 42042.0838637352, 'total_duration': 67990.92956995964, 'accumulated_submission_time': 42042.0838637352, 'accumulated_eval_time': 25943.07703590393, 'accumulated_logging_time': 1.7425308227539062}
I0211 09:40:57.395074 139975138969344 logging_writer.py:48] [120888] accumulated_eval_time=25943.077036, accumulated_logging_time=1.742531, accumulated_submission_time=42042.083864, global_step=120888, preemption_count=0, score=42042.083864, test/accuracy=0.709163, test/bleu=31.021296, test/loss=1.422662, test/num_examples=3003, total_duration=67990.929570, train/accuracy=0.704337, train/bleu=35.962140, train/loss=1.468529, validation/accuracy=0.692924, validation/bleu=30.693023, validation/loss=1.507184, validation/num_examples=3000
I0211 09:41:01.908099 139975130576640 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.7345150709152222, loss=2.592552900314331
I0211 09:41:36.534103 139975138969344 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.7131842970848083, loss=2.5975563526153564
I0211 09:42:11.278160 139975130576640 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.7162164449691772, loss=2.595428705215454
I0211 09:42:46.034306 139975138969344 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.7476246356964111, loss=2.614661455154419
I0211 09:43:20.798701 139975130576640 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.7004334330558777, loss=2.621037483215332
I0211 09:43:55.607304 139975138969344 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.7347797155380249, loss=2.6271908283233643
I0211 09:44:30.491756 139975130576640 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.7310570478439331, loss=2.6233723163604736
I0211 09:45:05.324029 139975138969344 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.7238963842391968, loss=2.6195902824401855
I0211 09:45:40.123090 139975130576640 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.7198082208633423, loss=2.5830328464508057
I0211 09:46:14.943083 139975138969344 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.765560507774353, loss=2.6701016426086426
I0211 09:46:49.721732 139975130576640 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.7241999506950378, loss=2.644768476486206
I0211 09:47:24.475213 139975138969344 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.7183852195739746, loss=2.603691577911377
I0211 09:47:59.218245 139975130576640 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.7306780219078064, loss=2.5871849060058594
I0211 09:48:33.970560 139975138969344 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.7141995429992676, loss=2.6780664920806885
I0211 09:49:08.743215 139975130576640 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.7473200559616089, loss=2.6142349243164062
I0211 09:49:43.528309 139975138969344 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.7385862469673157, loss=2.6097209453582764
I0211 09:50:18.286095 139975130576640 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.7427306175231934, loss=2.5938656330108643
I0211 09:50:53.037383 139975138969344 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.763126790523529, loss=2.611337184906006
I0211 09:51:27.805320 139975130576640 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.7434831261634827, loss=2.6565167903900146
I0211 09:52:02.586463 139975138969344 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.7192605137825012, loss=2.5823447704315186
I0211 09:52:37.334559 139975130576640 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.7598317265510559, loss=2.5604865550994873
I0211 09:53:12.091024 139975138969344 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7322353720664978, loss=2.6215291023254395
I0211 09:53:46.861656 139975130576640 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.733253002166748, loss=2.605726957321167
I0211 09:54:21.617231 139975138969344 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.7505735754966736, loss=2.5591518878936768
I0211 09:54:56.407656 139975130576640 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.7528018355369568, loss=2.5899009704589844
I0211 09:54:57.533212 140144802662208 spec.py:321] Evaluating on the training split.
I0211 09:55:00.529597 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 09:57:53.383654 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 09:57:56.079654 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 10:00:27.020957 140144802662208 spec.py:349] Evaluating on the test split.
I0211 10:00:29.722895 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 10:03:02.418075 140144802662208 submission_runner.py:408] Time since start: 69315.98s, 	Step: 123305, 	{'train/accuracy': 0.7063530087471008, 'train/loss': 1.4596189260482788, 'train/bleu': 36.09334633422933, 'validation/accuracy': 0.6923534870147705, 'validation/loss': 1.504960298538208, 'validation/bleu': 30.758487524049556, 'validation/num_examples': 3000, 'test/accuracy': 0.7106502056121826, 'test/loss': 1.4200072288513184, 'test/bleu': 31.02842778685176, 'test/num_examples': 3003, 'score': 42882.12957930565, 'total_duration': 69315.98330974579, 'accumulated_submission_time': 42882.12957930565, 'accumulated_eval_time': 26427.961848020554, 'accumulated_logging_time': 1.7832961082458496}
I0211 10:03:02.451998 139975138969344 logging_writer.py:48] [123305] accumulated_eval_time=26427.961848, accumulated_logging_time=1.783296, accumulated_submission_time=42882.129579, global_step=123305, preemption_count=0, score=42882.129579, test/accuracy=0.710650, test/bleu=31.028428, test/loss=1.420007, test/num_examples=3003, total_duration=69315.983310, train/accuracy=0.706353, train/bleu=36.093346, train/loss=1.459619, validation/accuracy=0.692353, validation/bleu=30.758488, validation/loss=1.504960, validation/num_examples=3000
I0211 10:03:35.660745 139975130576640 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.7310429215431213, loss=2.63907527923584
I0211 10:04:10.358585 139975138969344 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.763411819934845, loss=2.63155198097229
I0211 10:04:45.109278 139975130576640 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.7211030721664429, loss=2.6393744945526123
I0211 10:05:19.867650 139975138969344 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.7525261640548706, loss=2.6182548999786377
I0211 10:05:54.590614 139975130576640 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.7353394627571106, loss=2.5558018684387207
I0211 10:06:29.333140 139975138969344 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.745060384273529, loss=2.612999200820923
I0211 10:07:04.067025 139975130576640 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.7568570375442505, loss=2.6010165214538574
I0211 10:07:38.793425 139975138969344 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.7609830498695374, loss=2.615886926651001
I0211 10:08:13.542279 139975130576640 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.7404236197471619, loss=2.5881145000457764
I0211 10:08:48.272912 139975138969344 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.7693304419517517, loss=2.613567590713501
I0211 10:09:23.034045 139975130576640 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.750443160533905, loss=2.571847677230835
I0211 10:09:57.805863 139975138969344 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.7332116365432739, loss=2.5756332874298096
I0211 10:10:32.546379 139975130576640 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.7515039443969727, loss=2.624206066131592
I0211 10:11:07.334676 139975138969344 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.7337414622306824, loss=2.597656011581421
I0211 10:11:42.095078 139975130576640 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.7718617916107178, loss=2.575732946395874
I0211 10:12:16.869205 139975138969344 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.7804010510444641, loss=2.601083517074585
I0211 10:12:51.622656 139975130576640 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.7540457248687744, loss=2.6365156173706055
I0211 10:13:26.372867 139975138969344 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.7550545334815979, loss=2.6329004764556885
I0211 10:14:01.144506 139975130576640 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.7550556659698486, loss=2.5730676651000977
I0211 10:14:35.936342 139975138969344 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.7307201623916626, loss=2.534912586212158
I0211 10:15:10.697751 139975130576640 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.7407253980636597, loss=2.6222574710845947
I0211 10:15:45.445209 139975138969344 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.7500357627868652, loss=2.5794944763183594
I0211 10:16:20.201001 139975130576640 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.7505939602851868, loss=2.6312875747680664
I0211 10:16:54.986786 139975138969344 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.7714007496833801, loss=2.5960640907287598
I0211 10:17:02.701313 140144802662208 spec.py:321] Evaluating on the training split.
I0211 10:17:05.688401 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 10:19:58.022291 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 10:20:00.733198 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 10:22:39.114163 140144802662208 spec.py:349] Evaluating on the test split.
I0211 10:22:41.822828 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 10:25:05.813932 140144802662208 submission_runner.py:408] Time since start: 70639.38s, 	Step: 125724, 	{'train/accuracy': 0.7098019123077393, 'train/loss': 1.4383488893508911, 'train/bleu': 36.35349656081629, 'validation/accuracy': 0.6933577656745911, 'validation/loss': 1.502547264099121, 'validation/bleu': 30.86917300683674, 'validation/num_examples': 3000, 'test/accuracy': 0.710615336894989, 'test/loss': 1.4180010557174683, 'test/bleu': 31.023551642698948, 'test/num_examples': 3003, 'score': 43722.28950881958, 'total_duration': 70639.37914276123, 'accumulated_submission_time': 43722.28950881958, 'accumulated_eval_time': 26911.074389457703, 'accumulated_logging_time': 1.8275110721588135}
I0211 10:25:05.846531 139975130576640 logging_writer.py:48] [125724] accumulated_eval_time=26911.074389, accumulated_logging_time=1.827511, accumulated_submission_time=43722.289509, global_step=125724, preemption_count=0, score=43722.289509, test/accuracy=0.710615, test/bleu=31.023552, test/loss=1.418001, test/num_examples=3003, total_duration=70639.379143, train/accuracy=0.709802, train/bleu=36.353497, train/loss=1.438349, validation/accuracy=0.693358, validation/bleu=30.869173, validation/loss=1.502547, validation/num_examples=3000
I0211 10:25:32.539125 139975138969344 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.7545236945152283, loss=2.5999388694763184
I0211 10:26:07.208934 139975130576640 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.7746768593788147, loss=2.562514305114746
I0211 10:26:41.965803 139975138969344 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.7203908562660217, loss=2.5773725509643555
I0211 10:27:16.721629 139975130576640 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.7761037945747375, loss=2.607473373413086
I0211 10:27:51.462237 139975138969344 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.7640630602836609, loss=2.5879950523376465
I0211 10:28:26.220162 139975130576640 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.7392466068267822, loss=2.5558979511260986
I0211 10:29:00.984083 139975138969344 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.7427572011947632, loss=2.5581486225128174
I0211 10:29:35.756988 139975130576640 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.7497156262397766, loss=2.6270763874053955
I0211 10:30:10.549989 139975138969344 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.73710697889328, loss=2.6097283363342285
I0211 10:30:45.338883 139975130576640 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.7054489254951477, loss=2.5398905277252197
I0211 10:31:20.104053 139975138969344 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.755104660987854, loss=2.5716452598571777
I0211 10:31:54.856462 139975130576640 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.7470214366912842, loss=2.559638500213623
I0211 10:32:29.599686 139975138969344 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.7280771136283875, loss=2.5647830963134766
I0211 10:33:04.372626 139975130576640 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.7520414590835571, loss=2.5714361667633057
I0211 10:33:39.120609 139975138969344 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.757919430732727, loss=2.581681489944458
I0211 10:34:13.906200 139975130576640 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.7694215774536133, loss=2.599008083343506
I0211 10:34:48.695873 139975138969344 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.7442244291305542, loss=2.57417368888855
I0211 10:35:23.459958 139975130576640 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.7488650679588318, loss=2.6022415161132812
I0211 10:35:58.195378 139975138969344 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.7512391209602356, loss=2.5520987510681152
I0211 10:36:32.954166 139975130576640 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.7581336498260498, loss=2.6140708923339844
I0211 10:37:07.688020 139975138969344 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.7439743280410767, loss=2.56378173828125
I0211 10:37:42.432234 139975130576640 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.739124059677124, loss=2.5816714763641357
I0211 10:38:17.180892 139975138969344 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.7515736222267151, loss=2.571704864501953
I0211 10:38:51.943087 139975130576640 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.7584065794944763, loss=2.572343349456787
I0211 10:39:05.921783 140144802662208 spec.py:321] Evaluating on the training split.
I0211 10:39:08.895450 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 10:41:57.160199 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 10:41:59.844813 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 10:44:34.232576 140144802662208 spec.py:349] Evaluating on the test split.
I0211 10:44:36.924517 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 10:47:02.483936 140144802662208 submission_runner.py:408] Time since start: 71956.05s, 	Step: 128142, 	{'train/accuracy': 0.7098627090454102, 'train/loss': 1.4390041828155518, 'train/bleu': 36.20999485164631, 'validation/accuracy': 0.6937545537948608, 'validation/loss': 1.502580165863037, 'validation/bleu': 30.961534202035583, 'validation/num_examples': 3000, 'test/accuracy': 0.7110220193862915, 'test/loss': 1.4158810377120972, 'test/bleu': 31.07880207453981, 'test/num_examples': 3003, 'score': 44562.2737493515, 'total_duration': 71956.04916667938, 'accumulated_submission_time': 44562.2737493515, 'accumulated_eval_time': 27387.63648557663, 'accumulated_logging_time': 1.872190237045288}
I0211 10:47:02.516826 139975138969344 logging_writer.py:48] [128142] accumulated_eval_time=27387.636486, accumulated_logging_time=1.872190, accumulated_submission_time=44562.273749, global_step=128142, preemption_count=0, score=44562.273749, test/accuracy=0.711022, test/bleu=31.078802, test/loss=1.415881, test/num_examples=3003, total_duration=71956.049167, train/accuracy=0.709863, train/bleu=36.209995, train/loss=1.439004, validation/accuracy=0.693755, validation/bleu=30.961534, validation/loss=1.502580, validation/num_examples=3000
I0211 10:47:22.938977 139975130576640 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.7113991379737854, loss=2.5722103118896484
I0211 10:47:57.582172 139975138969344 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.7767305374145508, loss=2.5821187496185303
I0211 10:48:32.320508 139975130576640 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.7485705614089966, loss=2.557865858078003
I0211 10:49:07.076052 139975138969344 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.7529439330101013, loss=2.5790743827819824
I0211 10:49:41.862711 139975130576640 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.7076851725578308, loss=2.533083200454712
I0211 10:50:16.596417 139975138969344 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.7634584903717041, loss=2.5360257625579834
I0211 10:50:51.359276 139975130576640 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.7668858170509338, loss=2.6167595386505127
I0211 10:51:26.106159 139975138969344 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.753054141998291, loss=2.5622453689575195
I0211 10:52:00.850518 139975130576640 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.7782588601112366, loss=2.5933361053466797
I0211 10:52:35.604164 139975138969344 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.7309951186180115, loss=2.5878913402557373
I0211 10:53:10.366971 139975130576640 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.739047110080719, loss=2.618027687072754
I0211 10:53:45.133219 139975138969344 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.7424039840698242, loss=2.6100943088531494
I0211 10:54:19.894286 139975130576640 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.749561607837677, loss=2.576965808868408
I0211 10:54:54.680757 139975138969344 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.7517009377479553, loss=2.527024507522583
I0211 10:55:29.448300 139975130576640 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.7276776432991028, loss=2.5938069820404053
I0211 10:56:04.267726 139975138969344 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.7810213565826416, loss=2.6700172424316406
I0211 10:56:39.044672 139975130576640 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.7385932803153992, loss=2.5846192836761475
I0211 10:57:13.801753 139975138969344 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.7669712901115417, loss=2.5311434268951416
I0211 10:57:48.571906 139975130576640 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.7844386100769043, loss=2.600834369659424
I0211 10:58:23.310889 139975138969344 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.760684609413147, loss=2.570592164993286
I0211 10:58:58.043354 139975130576640 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.7744084000587463, loss=2.589036464691162
I0211 10:59:32.792300 139975138969344 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.7422165274620056, loss=2.587865114212036
I0211 11:00:07.587685 139975130576640 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.7457730770111084, loss=2.5805118083953857
I0211 11:00:42.343098 139975138969344 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.7186012268066406, loss=2.574249744415283
I0211 11:01:02.564146 140144802662208 spec.py:321] Evaluating on the training split.
I0211 11:01:05.545512 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:03:51.638006 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 11:03:54.335968 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:06:27.982167 140144802662208 spec.py:349] Evaluating on the test split.
I0211 11:06:30.692246 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:08:59.122717 140144802662208 submission_runner.py:408] Time since start: 73272.69s, 	Step: 130560, 	{'train/accuracy': 0.7109185457229614, 'train/loss': 1.434372901916504, 'train/bleu': 36.323414242246834, 'validation/accuracy': 0.6939157247543335, 'validation/loss': 1.5021934509277344, 'validation/bleu': 30.871359894567888, 'validation/num_examples': 3000, 'test/accuracy': 0.711266040802002, 'test/loss': 1.415252923965454, 'test/bleu': 31.17882674781833, 'test/num_examples': 3003, 'score': 45402.228637218475, 'total_duration': 73272.68795681, 'accumulated_submission_time': 45402.228637218475, 'accumulated_eval_time': 27864.195012569427, 'accumulated_logging_time': 1.9160029888153076}
I0211 11:08:59.155218 139975130576640 logging_writer.py:48] [130560] accumulated_eval_time=27864.195013, accumulated_logging_time=1.916003, accumulated_submission_time=45402.228637, global_step=130560, preemption_count=0, score=45402.228637, test/accuracy=0.711266, test/bleu=31.178827, test/loss=1.415253, test/num_examples=3003, total_duration=73272.687957, train/accuracy=0.710919, train/bleu=36.323414, train/loss=1.434373, validation/accuracy=0.693916, validation/bleu=30.871360, validation/loss=1.502193, validation/num_examples=3000
I0211 11:09:13.351305 139975138969344 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.7696635127067566, loss=2.586352825164795
I0211 11:09:47.966491 139975130576640 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.7429620623588562, loss=2.5936825275421143
I0211 11:10:22.699639 139975138969344 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.7397482991218567, loss=2.622105360031128
I0211 11:10:57.432142 139975130576640 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.741536557674408, loss=2.5468194484710693
I0211 11:11:32.180773 139975138969344 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.7513337135314941, loss=2.5749564170837402
I0211 11:12:06.909342 139975130576640 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.7328603267669678, loss=2.5751445293426514
I0211 11:12:41.641107 139975138969344 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.7526057362556458, loss=2.503641128540039
I0211 11:13:16.412990 139975130576640 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.7640208601951599, loss=2.5732946395874023
I0211 11:13:51.206479 139975138969344 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.7404627203941345, loss=2.5463013648986816
I0211 11:14:25.952536 139975130576640 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.7446333765983582, loss=2.563340902328491
I0211 11:15:00.692651 139975138969344 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.7630705833435059, loss=2.6334729194641113
I0211 11:15:35.493804 139975130576640 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.7515491247177124, loss=2.6247503757476807
I0211 11:16:10.266867 139975138969344 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.7623806595802307, loss=2.628847122192383
I0211 11:16:45.004617 139975130576640 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.7267540097236633, loss=2.5421688556671143
I0211 11:17:19.768376 139975138969344 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.7411047220230103, loss=2.5569543838500977
I0211 11:17:54.515281 139975130576640 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.7501872181892395, loss=2.580157995223999
I0211 11:18:29.251942 139975138969344 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.7554857134819031, loss=2.59100341796875
I0211 11:19:04.011234 139975130576640 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.7486301064491272, loss=2.649790048599243
I0211 11:19:38.790194 139975138969344 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.7498629093170166, loss=2.563368082046509
I0211 11:20:13.576784 139975130576640 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.7542315125465393, loss=2.567450523376465
I0211 11:20:48.354805 139975138969344 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.7321159243583679, loss=2.627107858657837
I0211 11:21:23.140029 139975130576640 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.7514801025390625, loss=2.5352981090545654
I0211 11:21:57.934720 139975138969344 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.7248461842536926, loss=2.492814540863037
I0211 11:22:32.698534 139975130576640 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.7474580407142639, loss=2.5920515060424805
I0211 11:22:59.163631 140144802662208 spec.py:321] Evaluating on the training split.
I0211 11:23:02.157468 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:25:45.371425 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 11:25:48.061144 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:28:19.478298 140144802662208 spec.py:349] Evaluating on the test split.
I0211 11:28:22.169054 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:30:49.532402 140144802662208 submission_runner.py:408] Time since start: 74583.10s, 	Step: 132978, 	{'train/accuracy': 0.7064332365989685, 'train/loss': 1.4602043628692627, 'train/bleu': 36.41087151297877, 'validation/accuracy': 0.6937917470932007, 'validation/loss': 1.502265214920044, 'validation/bleu': 30.904665803245663, 'validation/num_examples': 3000, 'test/accuracy': 0.7113938927650452, 'test/loss': 1.4153876304626465, 'test/bleu': 31.23585328606663, 'test/num_examples': 3003, 'score': 46242.14085435867, 'total_duration': 74583.09764313698, 'accumulated_submission_time': 46242.14085435867, 'accumulated_eval_time': 28334.5637383461, 'accumulated_logging_time': 1.960179090499878}
I0211 11:30:49.566111 139975138969344 logging_writer.py:48] [132978] accumulated_eval_time=28334.563738, accumulated_logging_time=1.960179, accumulated_submission_time=46242.140854, global_step=132978, preemption_count=0, score=46242.140854, test/accuracy=0.711394, test/bleu=31.235853, test/loss=1.415388, test/num_examples=3003, total_duration=74583.097643, train/accuracy=0.706433, train/bleu=36.410872, train/loss=1.460204, validation/accuracy=0.693792, validation/bleu=30.904666, validation/loss=1.502265, validation/num_examples=3000
I0211 11:30:57.551359 139975130576640 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.7591860294342041, loss=2.606795310974121
I0211 11:31:32.186396 139975138969344 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.7346175312995911, loss=2.589456081390381
I0211 11:32:06.879598 139975130576640 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.7430192232131958, loss=2.6335508823394775
I0211 11:32:41.647474 139975138969344 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.7260181903839111, loss=2.5888142585754395
I0211 11:32:52.490228 140144802662208 spec.py:321] Evaluating on the training split.
I0211 11:32:55.471490 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:35:42.337209 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 11:35:45.021419 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:38:16.401455 140144802662208 spec.py:349] Evaluating on the test split.
I0211 11:38:19.097812 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:40:46.673820 140144802662208 submission_runner.py:408] Time since start: 75180.24s, 	Step: 133333, 	{'train/accuracy': 0.7103666067123413, 'train/loss': 1.433834433555603, 'train/bleu': 36.36197193300483, 'validation/accuracy': 0.6937669515609741, 'validation/loss': 1.502273440361023, 'validation/bleu': 30.902044416208398, 'validation/num_examples': 3000, 'test/accuracy': 0.7113706469535828, 'test/loss': 1.415420651435852, 'test/bleu': 31.22193609040821, 'test/num_examples': 3003, 'score': 46365.04322433472, 'total_duration': 75180.2390575409, 'accumulated_submission_time': 46365.04322433472, 'accumulated_eval_time': 28808.747275829315, 'accumulated_logging_time': 2.0037403106689453}
I0211 11:40:46.707645 139975130576640 logging_writer.py:48] [133333] accumulated_eval_time=28808.747276, accumulated_logging_time=2.003740, accumulated_submission_time=46365.043224, global_step=133333, preemption_count=0, score=46365.043224, test/accuracy=0.711371, test/bleu=31.221936, test/loss=1.415421, test/num_examples=3003, total_duration=75180.239058, train/accuracy=0.710367, train/bleu=36.361972, train/loss=1.433834, validation/accuracy=0.693767, validation/bleu=30.902044, validation/loss=1.502273, validation/num_examples=3000
I0211 11:40:46.741491 139975138969344 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46365.043224
I0211 11:40:47.941131 140144802662208 checkpoints.py:490] Saving checkpoint at step: 133333
I0211 11:40:52.000528 140144802662208 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_1/checkpoint_133333
I0211 11:40:52.005574 140144802662208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_1/checkpoint_133333.
I0211 11:40:52.051012 140144802662208 submission_runner.py:583] Tuning trial 1/5
I0211 11:40:52.051199 140144802662208 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0211 11:40:52.054806 140144802662208 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006382566643878818, 'train/loss': 10.960665702819824, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.38932204246521, 'total_duration': 919.9261367321014, 'accumulated_submission_time': 37.38932204246521, 'accumulated_eval_time': 882.5367727279663, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2411, {'train/accuracy': 0.4126627445220947, 'train/loss': 4.002304553985596, 'train/bleu': 14.245377447202788, 'validation/accuracy': 0.3975647985935211, 'validation/loss': 4.118062496185303, 'validation/bleu': 9.613389479466635, 'validation/num_examples': 3000, 'test/accuracy': 0.38110512495040894, 'test/loss': 4.32146692276001, 'test/bleu': 8.015197424590168, 'test/num_examples': 3003, 'score': 877.514372587204, 'total_duration': 2355.1783118247986, 'accumulated_submission_time': 877.514372587204, 'accumulated_eval_time': 1477.5438141822815, 'accumulated_logging_time': 0.03843188285827637, 'global_step': 2411, 'preemption_count': 0}), (4821, {'train/accuracy': 0.5397511124610901, 'train/loss': 2.7535836696624756, 'train/bleu': 24.513584478437387, 'validation/accuracy': 0.5449033379554749, 'validation/loss': 2.7072184085845947, 'validation/bleu': 20.628340353540654, 'validation/num_examples': 3000, 'test/accuracy': 0.5461623668670654, 'test/loss': 2.733055353164673, 'test/bleu': 19.172664785009378, 'test/num_examples': 3003, 'score': 1717.5991599559784, 'total_duration': 3697.6896362304688, 'accumulated_submission_time': 1717.5991599559784, 'accumulated_eval_time': 1979.8509588241577, 'accumulated_logging_time': 0.07812023162841797, 'global_step': 4821, 'preemption_count': 0}), (7234, {'train/accuracy': 0.5832327604293823, 'train/loss': 2.348278522491455, 'train/bleu': 27.40869814351565, 'validation/accuracy': 0.5864651203155518, 'validation/loss': 2.313688039779663, 'validation/bleu': 23.116123270683246, 'validation/num_examples': 3000, 'test/accuracy': 0.5904944539070129, 'test/loss': 2.3054873943328857, 'test/bleu': 22.381906824340813, 'test/num_examples': 3003, 'score': 2557.73996424675, 'total_duration': 5030.1105189323425, 'accumulated_submission_time': 2557.73996424675, 'accumulated_eval_time': 2472.0260739326477, 'accumulated_logging_time': 0.1051478385925293, 'global_step': 7234, 'preemption_count': 0}), (9649, {'train/accuracy': 0.5944892764091492, 'train/loss': 2.2307252883911133, 'train/bleu': 27.885071200396293, 'validation/accuracy': 0.608386754989624, 'validation/loss': 2.125382423400879, 'validation/bleu': 24.95503798977198, 'validation/num_examples': 3000, 'test/accuracy': 0.6160014271736145, 'test/loss': 2.0950565338134766, 'test/bleu': 23.70429726840615, 'test/num_examples': 3003, 'score': 3397.9064087867737, 'total_duration': 6324.159275770187, 'accumulated_submission_time': 3397.9064087867737, 'accumulated_eval_time': 2925.8024010658264, 'accumulated_logging_time': 0.13380646705627441, 'global_step': 9649, 'preemption_count': 0}), (12064, {'train/accuracy': 0.6005798578262329, 'train/loss': 2.151870012283325, 'train/bleu': 28.89981054140853, 'validation/accuracy': 0.6196823120117188, 'validation/loss': 2.009481906890869, 'validation/bleu': 25.802299412078856, 'validation/num_examples': 3000, 'test/accuracy': 0.6272267699241638, 'test/loss': 1.9692918062210083, 'test/bleu': 24.68109616132204, 'test/num_examples': 3003, 'score': 4237.891711235046, 'total_duration': 7633.663153886795, 'accumulated_submission_time': 4237.891711235046, 'accumulated_eval_time': 3395.214864253998, 'accumulated_logging_time': 0.15960693359375, 'global_step': 12064, 'preemption_count': 0}), (14480, {'train/accuracy': 0.6170651316642761, 'train/loss': 2.030705451965332, 'train/bleu': 29.54870471997391, 'validation/accuracy': 0.630109965801239, 'validation/loss': 1.9172378778457642, 'validation/bleu': 26.213816054497475, 'validation/num_examples': 3000, 'test/accuracy': 0.6392307281494141, 'test/loss': 1.868680477142334, 'test/bleu': 25.582985135267798, 'test/num_examples': 3003, 'score': 5078.001572847366, 'total_duration': 8972.102147102356, 'accumulated_submission_time': 5078.001572847366, 'accumulated_eval_time': 3893.4397599697113, 'accumulated_logging_time': 0.18552017211914062, 'global_step': 14480, 'preemption_count': 0}), (16895, {'train/accuracy': 0.6205698847770691, 'train/loss': 2.0030786991119385, 'train/bleu': 30.48051704857396, 'validation/accuracy': 0.6400168538093567, 'validation/loss': 1.8531957864761353, 'validation/bleu': 26.91149584465031, 'validation/num_examples': 3000, 'test/accuracy': 0.648678183555603, 'test/loss': 1.8056501150131226, 'test/bleu': 26.355332277203157, 'test/num_examples': 3003, 'score': 5918.144634008408, 'total_duration': 10295.827911376953, 'accumulated_submission_time': 5918.144634008408, 'accumulated_eval_time': 4376.910125494003, 'accumulated_logging_time': 0.2147214412689209, 'global_step': 16895, 'preemption_count': 0}), (19311, {'train/accuracy': 0.6361596584320068, 'train/loss': 1.8797343969345093, 'train/bleu': 30.912875387106396, 'validation/accuracy': 0.6456212401390076, 'validation/loss': 1.8102855682373047, 'validation/bleu': 27.351768905608047, 'validation/num_examples': 3000, 'test/accuracy': 0.6544535756111145, 'test/loss': 1.754570722579956, 'test/bleu': 26.59917892660872, 'test/num_examples': 3003, 'score': 6758.210176944733, 'total_duration': 11624.979398727417, 'accumulated_submission_time': 6758.210176944733, 'accumulated_eval_time': 4865.887587070465, 'accumulated_logging_time': 0.24300289154052734, 'global_step': 19311, 'preemption_count': 0}), (21728, {'train/accuracy': 0.3390825092792511, 'train/loss': 3.9086546897888184, 'train/bleu': 0.5862118968036852, 'validation/accuracy': 0.3057122528553009, 'validation/loss': 4.292892932891846, 'validation/bleu': 0.1320855284362729, 'validation/num_examples': 3000, 'test/accuracy': 0.2937540113925934, 'test/loss': 4.477366924285889, 'test/bleu': 0.14031335774959658, 'test/num_examples': 3003, 'score': 7598.422609567642, 'total_duration': 13001.2740752697, 'accumulated_submission_time': 7598.422609567642, 'accumulated_eval_time': 5401.86420583725, 'accumulated_logging_time': 0.27010011672973633, 'global_step': 21728, 'preemption_count': 0}), (24147, {'train/accuracy': 0.6233353614807129, 'train/loss': 1.9733836650848389, 'train/bleu': 30.139503659176032, 'validation/accuracy': 0.6435753703117371, 'validation/loss': 1.819201946258545, 'validation/bleu': 27.412554254769713, 'validation/num_examples': 3000, 'test/accuracy': 0.6543838381767273, 'test/loss': 1.7676335573196411, 'test/bleu': 26.747935955855187, 'test/num_examples': 3003, 'score': 8438.351751565933, 'total_duration': 14347.287384033203, 'accumulated_submission_time': 8438.351751565933, 'accumulated_eval_time': 5907.838281869888, 'accumulated_logging_time': 0.300400972366333, 'global_step': 24147, 'preemption_count': 0}), (26564, {'train/accuracy': 0.6321825981140137, 'train/loss': 1.8988949060440063, 'train/bleu': 30.340070514809657, 'validation/accuracy': 0.648423433303833, 'validation/loss': 1.7781262397766113, 'validation/bleu': 27.29769121173301, 'validation/num_examples': 3000, 'test/accuracy': 0.6584277749061584, 'test/loss': 1.7201982736587524, 'test/bleu': 26.875802443570254, 'test/num_examples': 3003, 'score': 9278.293791770935, 'total_duration': 15710.543276309967, 'accumulated_submission_time': 9278.293791770935, 'accumulated_eval_time': 6431.037258863449, 'accumulated_logging_time': 0.3333091735839844, 'global_step': 26564, 'preemption_count': 0}), (28982, {'train/accuracy': 0.6355407238006592, 'train/loss': 1.8854094743728638, 'train/bleu': 30.347952863442696, 'validation/accuracy': 0.6500105261802673, 'validation/loss': 1.75833261013031, 'validation/bleu': 27.48807983750271, 'validation/num_examples': 3000, 'test/accuracy': 0.6632153987884521, 'test/loss': 1.6976944208145142, 'test/bleu': 27.12441943548814, 'test/num_examples': 3003, 'score': 10118.426603794098, 'total_duration': 17042.172969341278, 'accumulated_submission_time': 10118.426603794098, 'accumulated_eval_time': 6922.425815820694, 'accumulated_logging_time': 0.36264824867248535, 'global_step': 28982, 'preemption_count': 0}), (31400, {'train/accuracy': 0.6615469455718994, 'train/loss': 1.7062214612960815, 'train/bleu': 32.51895289150185, 'validation/accuracy': 0.6527507305145264, 'validation/loss': 1.750123143196106, 'validation/bleu': 27.932306559997773, 'validation/num_examples': 3000, 'test/accuracy': 0.664307713508606, 'test/loss': 1.6905139684677124, 'test/bleu': 27.281990486780842, 'test/num_examples': 3003, 'score': 10958.394407272339, 'total_duration': 18418.541491508484, 'accumulated_submission_time': 10958.394407272339, 'accumulated_eval_time': 7458.717993736267, 'accumulated_logging_time': 0.391817569732666, 'global_step': 31400, 'preemption_count': 0}), (33818, {'train/accuracy': 0.636962890625, 'train/loss': 1.8799915313720703, 'train/bleu': 30.93731275058393, 'validation/accuracy': 0.6577227711677551, 'validation/loss': 1.7262121438980103, 'validation/bleu': 28.16495224589458, 'validation/num_examples': 3000, 'test/accuracy': 0.6667364239692688, 'test/loss': 1.6695778369903564, 'test/bleu': 27.411963702224377, 'test/num_examples': 3003, 'score': 11798.52327799797, 'total_duration': 19750.400065422058, 'accumulated_submission_time': 11798.52327799797, 'accumulated_eval_time': 7950.335062503815, 'accumulated_logging_time': 0.42200541496276855, 'global_step': 33818, 'preemption_count': 0}), (36237, {'train/accuracy': 0.6327520608901978, 'train/loss': 1.8985131978988647, 'train/bleu': 30.885318130356406, 'validation/accuracy': 0.6562100648880005, 'validation/loss': 1.7289568185806274, 'validation/bleu': 28.097237758642365, 'validation/num_examples': 3000, 'test/accuracy': 0.6673523187637329, 'test/loss': 1.6683061122894287, 'test/bleu': 27.305733465104947, 'test/num_examples': 3003, 'score': 12638.686345100403, 'total_duration': 21074.423023223877, 'accumulated_submission_time': 12638.686345100403, 'accumulated_eval_time': 8434.08610200882, 'accumulated_logging_time': 0.4524543285369873, 'global_step': 36237, 'preemption_count': 0}), (38656, {'train/accuracy': 0.6423766613006592, 'train/loss': 1.8290132284164429, 'train/bleu': 31.39003853798642, 'validation/accuracy': 0.6569168567657471, 'validation/loss': 1.7200067043304443, 'validation/bleu': 27.887943009530343, 'validation/num_examples': 3000, 'test/accuracy': 0.670001745223999, 'test/loss': 1.6540709733963013, 'test/bleu': 27.66217813006899, 'test/num_examples': 3003, 'score': 13478.862303972244, 'total_duration': 22370.699309825897, 'accumulated_submission_time': 13478.862303972244, 'accumulated_eval_time': 8890.070576429367, 'accumulated_logging_time': 0.4886150360107422, 'global_step': 38656, 'preemption_count': 0}), (41073, {'train/accuracy': 0.6356822848320007, 'train/loss': 1.8719481229782104, 'train/bleu': 31.447752721470277, 'validation/accuracy': 0.6580947637557983, 'validation/loss': 1.707008957862854, 'validation/bleu': 28.085698986814226, 'validation/num_examples': 3000, 'test/accuracy': 0.6714078187942505, 'test/loss': 1.647200584411621, 'test/bleu': 27.96062907699173, 'test/num_examples': 3003, 'score': 14318.886668205261, 'total_duration': 23706.75301337242, 'accumulated_submission_time': 14318.886668205261, 'accumulated_eval_time': 9385.98035311699, 'accumulated_logging_time': 0.5256316661834717, 'global_step': 41073, 'preemption_count': 0}), (43492, {'train/accuracy': 0.6358650922775269, 'train/loss': 1.8773494958877563, 'train/bleu': 31.489122957348748, 'validation/accuracy': 0.6598678231239319, 'validation/loss': 1.6993831396102905, 'validation/bleu': 28.275403261768112, 'validation/num_examples': 3000, 'test/accuracy': 0.6728022694587708, 'test/loss': 1.6397759914398193, 'test/bleu': 27.88330925488826, 'test/num_examples': 3003, 'score': 15158.90975689888, 'total_duration': 25055.99835085869, 'accumulated_submission_time': 15158.90975689888, 'accumulated_eval_time': 9895.090104341507, 'accumulated_logging_time': 0.5565388202667236, 'global_step': 43492, 'preemption_count': 0}), (45910, {'train/accuracy': 0.6427233219146729, 'train/loss': 1.8206764459609985, 'train/bleu': 31.07863303548564, 'validation/accuracy': 0.6608349680900574, 'validation/loss': 1.692588448524475, 'validation/bleu': 28.363472882686317, 'validation/num_examples': 3000, 'test/accuracy': 0.6724420785903931, 'test/loss': 1.634313702583313, 'test/bleu': 28.059474468605845, 'test/num_examples': 3003, 'score': 15998.971328496933, 'total_duration': 26441.571583509445, 'accumulated_submission_time': 15998.971328496933, 'accumulated_eval_time': 10440.490324735641, 'accumulated_logging_time': 0.5880370140075684, 'global_step': 45910, 'preemption_count': 0}), (48328, {'train/accuracy': 0.6389437913894653, 'train/loss': 1.855763554573059, 'train/bleu': 31.00010065151017, 'validation/accuracy': 0.6613061428070068, 'validation/loss': 1.6884702444076538, 'validation/bleu': 28.22158443610769, 'validation/num_examples': 3000, 'test/accuracy': 0.6750450730323792, 'test/loss': 1.6206564903259277, 'test/bleu': 28.01314804173373, 'test/num_examples': 3003, 'score': 16838.999277830124, 'total_duration': 27757.97251176834, 'accumulated_submission_time': 16838.999277830124, 'accumulated_eval_time': 10916.7520134449, 'accumulated_logging_time': 0.619476318359375, 'global_step': 48328, 'preemption_count': 0}), (50747, {'train/accuracy': 0.6508919596672058, 'train/loss': 1.7648361921310425, 'train/bleu': 31.782695880760556, 'validation/accuracy': 0.6645422577857971, 'validation/loss': 1.6845265626907349, 'validation/bleu': 28.74312417864181, 'validation/num_examples': 3000, 'test/accuracy': 0.6755331158638, 'test/loss': 1.6183867454528809, 'test/bleu': 28.332459093761646, 'test/num_examples': 3003, 'score': 17679.204163074493, 'total_duration': 29064.18690776825, 'accumulated_submission_time': 17679.204163074493, 'accumulated_eval_time': 11382.648072242737, 'accumulated_logging_time': 0.6527702808380127, 'global_step': 50747, 'preemption_count': 0}), (53166, {'train/accuracy': 0.6417784690856934, 'train/loss': 1.8273143768310547, 'train/bleu': 31.41106280404022, 'validation/accuracy': 0.664182722568512, 'validation/loss': 1.6704254150390625, 'validation/bleu': 28.515135030428414, 'validation/num_examples': 3000, 'test/accuracy': 0.6751612424850464, 'test/loss': 1.60860013961792, 'test/bleu': 28.214403806713054, 'test/num_examples': 3003, 'score': 18519.205310583115, 'total_duration': 30440.823876857758, 'accumulated_submission_time': 18519.205310583115, 'accumulated_eval_time': 11919.171528339386, 'accumulated_logging_time': 0.6851651668548584, 'global_step': 53166, 'preemption_count': 0}), (55585, {'train/accuracy': 0.6417572498321533, 'train/loss': 1.8483734130859375, 'train/bleu': 31.531738068339457, 'validation/accuracy': 0.665658175945282, 'validation/loss': 1.6686065196990967, 'validation/bleu': 28.607842205522875, 'validation/num_examples': 3000, 'test/accuracy': 0.6774388551712036, 'test/loss': 1.6036652326583862, 'test/bleu': 28.394650410572435, 'test/num_examples': 3003, 'score': 19359.40021085739, 'total_duration': 31849.13062644005, 'accumulated_submission_time': 19359.40021085739, 'accumulated_eval_time': 12487.166585206985, 'accumulated_logging_time': 0.7213225364685059, 'global_step': 55585, 'preemption_count': 0}), (58004, {'train/accuracy': 0.650646448135376, 'train/loss': 1.7800432443618774, 'train/bleu': 32.27619017894549, 'validation/accuracy': 0.6677040457725525, 'validation/loss': 1.6552205085754395, 'validation/bleu': 28.97028555750232, 'validation/num_examples': 3000, 'test/accuracy': 0.6786706447601318, 'test/loss': 1.5954159498214722, 'test/bleu': 28.438258704322525, 'test/num_examples': 3003, 'score': 20199.471867084503, 'total_duration': 33152.7517850399, 'accumulated_submission_time': 20199.471867084503, 'accumulated_eval_time': 12950.59977388382, 'accumulated_logging_time': 0.7550153732299805, 'global_step': 58004, 'preemption_count': 0}), (60422, {'train/accuracy': 0.6451210379600525, 'train/loss': 1.8069937229156494, 'train/bleu': 31.819075921555985, 'validation/accuracy': 0.6668609380722046, 'validation/loss': 1.6589609384536743, 'validation/bleu': 28.74034449614497, 'validation/num_examples': 3000, 'test/accuracy': 0.6806228756904602, 'test/loss': 1.5879414081573486, 'test/bleu': 28.597255906283987, 'test/num_examples': 3003, 'score': 21039.458937883377, 'total_duration': 34488.21444058418, 'accumulated_submission_time': 21039.458937883377, 'accumulated_eval_time': 13445.960545539856, 'accumulated_logging_time': 0.7892227172851562, 'global_step': 60422, 'preemption_count': 0}), (62841, {'train/accuracy': 0.6688887476921082, 'train/loss': 1.6635355949401855, 'train/bleu': 33.49501242969906, 'validation/accuracy': 0.6675428748130798, 'validation/loss': 1.650787353515625, 'validation/bleu': 28.96451958189699, 'validation/num_examples': 3000, 'test/accuracy': 0.6802510023117065, 'test/loss': 1.5829477310180664, 'test/bleu': 28.315300929416214, 'test/num_examples': 3003, 'score': 21879.506196975708, 'total_duration': 35813.19875717163, 'accumulated_submission_time': 21879.506196975708, 'accumulated_eval_time': 13930.784398078918, 'accumulated_logging_time': 0.8219027519226074, 'global_step': 62841, 'preemption_count': 0}), (65260, {'train/accuracy': 0.6497659683227539, 'train/loss': 1.7808959484100342, 'train/bleu': 32.40010374364767, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.6441073417663574, 'validation/bleu': 28.94475644983091, 'validation/num_examples': 3000, 'test/accuracy': 0.6811341643333435, 'test/loss': 1.5736756324768066, 'test/bleu': 28.409011933836478, 'test/num_examples': 3003, 'score': 22719.717471837997, 'total_duration': 37126.762442588806, 'accumulated_submission_time': 22719.717471837997, 'accumulated_eval_time': 14404.023327350616, 'accumulated_logging_time': 0.8547759056091309, 'global_step': 65260, 'preemption_count': 0}), (67679, {'train/accuracy': 0.6502645611763, 'train/loss': 1.7860978841781616, 'train/bleu': 31.68374978602823, 'validation/accuracy': 0.6713865995407104, 'validation/loss': 1.6337947845458984, 'validation/bleu': 29.222567907067074, 'validation/num_examples': 3000, 'test/accuracy': 0.6845738291740417, 'test/loss': 1.5635030269622803, 'test/bleu': 29.088031046885384, 'test/num_examples': 3003, 'score': 23559.931114912033, 'total_duration': 38493.42343258858, 'accumulated_submission_time': 23559.931114912033, 'accumulated_eval_time': 14930.35793542862, 'accumulated_logging_time': 0.8874788284301758, 'global_step': 67679, 'preemption_count': 0}), (70098, {'train/accuracy': 0.6582711338996887, 'train/loss': 1.7285667657852173, 'train/bleu': 32.39691829939411, 'validation/accuracy': 0.671584963798523, 'validation/loss': 1.6267058849334717, 'validation/bleu': 29.149120230108544, 'validation/num_examples': 3000, 'test/accuracy': 0.6856196522712708, 'test/loss': 1.5533530712127686, 'test/bleu': 28.82446304234327, 'test/num_examples': 3003, 'score': 24400.087972164154, 'total_duration': 39873.08808875084, 'accumulated_submission_time': 24400.087972164154, 'accumulated_eval_time': 15469.750158786774, 'accumulated_logging_time': 0.9219081401824951, 'global_step': 70098, 'preemption_count': 0}), (72518, {'train/accuracy': 0.6576229333877563, 'train/loss': 1.7471884489059448, 'train/bleu': 32.81365972041144, 'validation/accuracy': 0.6732960343360901, 'validation/loss': 1.6212689876556396, 'validation/bleu': 29.323903687794544, 'validation/num_examples': 3000, 'test/accuracy': 0.6862123012542725, 'test/loss': 1.5499813556671143, 'test/bleu': 28.91150751075374, 'test/num_examples': 3003, 'score': 25240.307821035385, 'total_duration': 41214.29425621033, 'accumulated_submission_time': 25240.307821035385, 'accumulated_eval_time': 15970.62186050415, 'accumulated_logging_time': 0.957329273223877, 'global_step': 72518, 'preemption_count': 0}), (74937, {'train/accuracy': 0.6538400053977966, 'train/loss': 1.7619236707687378, 'train/bleu': 32.41990521138852, 'validation/accuracy': 0.6753295063972473, 'validation/loss': 1.6074609756469727, 'validation/bleu': 29.47000517384073, 'validation/num_examples': 3000, 'test/accuracy': 0.6883504986763, 'test/loss': 1.5352879762649536, 'test/bleu': 29.17476942337257, 'test/num_examples': 3003, 'score': 26080.382111549377, 'total_duration': 42529.167917490005, 'accumulated_submission_time': 26080.382111549377, 'accumulated_eval_time': 16445.30373263359, 'accumulated_logging_time': 0.9950253963470459, 'global_step': 74937, 'preemption_count': 0}), (77357, {'train/accuracy': 0.6637595295906067, 'train/loss': 1.6998523473739624, 'train/bleu': 32.32256300712956, 'validation/accuracy': 0.6762842535972595, 'validation/loss': 1.6049737930297852, 'validation/bleu': 29.577603225878857, 'validation/num_examples': 3000, 'test/accuracy': 0.6897449493408203, 'test/loss': 1.5332320928573608, 'test/bleu': 29.111522492945284, 'test/num_examples': 3003, 'score': 26920.607084035873, 'total_duration': 43868.182834625244, 'accumulated_submission_time': 26920.607084035873, 'accumulated_eval_time': 16943.978850841522, 'accumulated_logging_time': 1.0299007892608643, 'global_step': 77357, 'preemption_count': 0}), (79777, {'train/accuracy': 0.6550542116165161, 'train/loss': 1.7492350339889526, 'train/bleu': 32.499716599105, 'validation/accuracy': 0.6763834357261658, 'validation/loss': 1.593187689781189, 'validation/bleu': 29.572896222219057, 'validation/num_examples': 3000, 'test/accuracy': 0.6903492212295532, 'test/loss': 1.5179253816604614, 'test/bleu': 29.494597510156364, 'test/num_examples': 3003, 'score': 27760.757081508636, 'total_duration': 45201.78984117508, 'accumulated_submission_time': 27760.757081508636, 'accumulated_eval_time': 17437.31713628769, 'accumulated_logging_time': 1.0680632591247559, 'global_step': 79777, 'preemption_count': 0}), (82196, {'train/accuracy': 0.6707914471626282, 'train/loss': 1.6536064147949219, 'train/bleu': 33.720589661719316, 'validation/accuracy': 0.6779953241348267, 'validation/loss': 1.590328335762024, 'validation/bleu': 29.644675622870558, 'validation/num_examples': 3000, 'test/accuracy': 0.6923130750656128, 'test/loss': 1.5137943029403687, 'test/bleu': 29.512227379029266, 'test/num_examples': 3003, 'score': 28600.833948373795, 'total_duration': 46536.39051222801, 'accumulated_submission_time': 28600.833948373795, 'accumulated_eval_time': 17931.717635393143, 'accumulated_logging_time': 1.1103150844573975, 'global_step': 82196, 'preemption_count': 0}), (84615, {'train/accuracy': 0.6634590029716492, 'train/loss': 1.7018529176712036, 'train/bleu': 32.766808653370965, 'validation/accuracy': 0.6799171566963196, 'validation/loss': 1.586037039756775, 'validation/bleu': 29.755778263877012, 'validation/num_examples': 3000, 'test/accuracy': 0.693312406539917, 'test/loss': 1.509398102760315, 'test/bleu': 30.06056970667257, 'test/num_examples': 3003, 'score': 29440.945076942444, 'total_duration': 47895.65140795708, 'accumulated_submission_time': 29440.945076942444, 'accumulated_eval_time': 18450.751024246216, 'accumulated_logging_time': 1.1467373371124268, 'global_step': 84615, 'preemption_count': 0}), (87033, {'train/accuracy': 0.6625377535820007, 'train/loss': 1.709887146949768, 'train/bleu': 32.974561510453555, 'validation/accuracy': 0.6813058853149414, 'validation/loss': 1.572974443435669, 'validation/bleu': 29.834047284658478, 'validation/num_examples': 3000, 'test/accuracy': 0.6963453888893127, 'test/loss': 1.4961185455322266, 'test/bleu': 29.654388366237686, 'test/num_examples': 3003, 'score': 30280.988560199738, 'total_duration': 49244.036138772964, 'accumulated_submission_time': 30280.988560199738, 'accumulated_eval_time': 18958.97201180458, 'accumulated_logging_time': 1.1856064796447754, 'global_step': 87033, 'preemption_count': 0}), (89452, {'train/accuracy': 0.6736891269683838, 'train/loss': 1.6366500854492188, 'train/bleu': 33.41154952869727, 'validation/accuracy': 0.6804007291793823, 'validation/loss': 1.5719475746154785, 'validation/bleu': 29.573600778223444, 'validation/num_examples': 3000, 'test/accuracy': 0.6961362361907959, 'test/loss': 1.495225429534912, 'test/bleu': 29.73615868519291, 'test/num_examples': 3003, 'score': 31121.10328722, 'total_duration': 50619.80656862259, 'accumulated_submission_time': 31121.10328722, 'accumulated_eval_time': 19494.512558221817, 'accumulated_logging_time': 1.2216103076934814, 'global_step': 89452, 'preemption_count': 0}), (91871, {'train/accuracy': 0.6677938103675842, 'train/loss': 1.668520450592041, 'train/bleu': 33.170331808642466, 'validation/accuracy': 0.6823474168777466, 'validation/loss': 1.5652003288269043, 'validation/bleu': 30.001956328565168, 'validation/num_examples': 3000, 'test/accuracy': 0.6966475248336792, 'test/loss': 1.4875853061676025, 'test/bleu': 29.9217194579761, 'test/num_examples': 3003, 'score': 31961.19503569603, 'total_duration': 51964.228222608566, 'accumulated_submission_time': 31961.19503569603, 'accumulated_eval_time': 19998.722029209137, 'accumulated_logging_time': 1.2600123882293701, 'global_step': 91871, 'preemption_count': 0}), (94290, {'train/accuracy': 0.6874837279319763, 'train/loss': 1.560304880142212, 'train/bleu': 34.74978426592824, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.5531262159347534, 'validation/bleu': 30.264827902506898, 'validation/num_examples': 3000, 'test/accuracy': 0.6995874643325806, 'test/loss': 1.4738984107971191, 'test/bleu': 30.343516684026156, 'test/num_examples': 3003, 'score': 32801.38553190231, 'total_duration': 53287.52480864525, 'accumulated_submission_time': 32801.38553190231, 'accumulated_eval_time': 20481.708650112152, 'accumulated_logging_time': 1.298454761505127, 'global_step': 94290, 'preemption_count': 0}), (96708, {'train/accuracy': 0.6748186945915222, 'train/loss': 1.623995304107666, 'train/bleu': 33.85164936344683, 'validation/accuracy': 0.6853107810020447, 'validation/loss': 1.5497137308120728, 'validation/bleu': 30.209081609458195, 'validation/num_examples': 3000, 'test/accuracy': 0.700888991355896, 'test/loss': 1.4674443006515503, 'test/bleu': 30.135616820141905, 'test/num_examples': 3003, 'score': 33641.31292676926, 'total_duration': 54642.65100026131, 'accumulated_submission_time': 33641.31292676926, 'accumulated_eval_time': 20996.7893345356, 'accumulated_logging_time': 1.3360624313354492, 'global_step': 96708, 'preemption_count': 0}), (99127, {'train/accuracy': 0.6738321185112, 'train/loss': 1.6364940404891968, 'train/bleu': 33.38403674379716, 'validation/accuracy': 0.6848272085189819, 'validation/loss': 1.545107364654541, 'validation/bleu': 30.294506822664715, 'validation/num_examples': 3000, 'test/accuracy': 0.7012376189231873, 'test/loss': 1.4639596939086914, 'test/bleu': 30.208697155291638, 'test/num_examples': 3003, 'score': 34481.37762069702, 'total_duration': 56002.891575336456, 'accumulated_submission_time': 34481.37762069702, 'accumulated_eval_time': 21516.848722696304, 'accumulated_logging_time': 1.3743152618408203, 'global_step': 99127, 'preemption_count': 0}), (101546, {'train/accuracy': 0.6848159432411194, 'train/loss': 1.569870948791504, 'train/bleu': 34.37797153427266, 'validation/accuracy': 0.6859679222106934, 'validation/loss': 1.5355159044265747, 'validation/bleu': 30.165309046410087, 'validation/num_examples': 3000, 'test/accuracy': 0.7024809718132019, 'test/loss': 1.4583240747451782, 'test/bleu': 30.57824113953, 'test/num_examples': 3003, 'score': 35321.28910493851, 'total_duration': 57307.98519515991, 'accumulated_submission_time': 35321.28910493851, 'accumulated_eval_time': 21981.911460876465, 'accumulated_logging_time': 1.413140058517456, 'global_step': 101546, 'preemption_count': 0}), (103965, {'train/accuracy': 0.6798616647720337, 'train/loss': 1.5981993675231934, 'train/bleu': 34.46955027904853, 'validation/accuracy': 0.688063383102417, 'validation/loss': 1.5368680953979492, 'validation/bleu': 30.38069848744398, 'validation/num_examples': 3000, 'test/accuracy': 0.7020859122276306, 'test/loss': 1.455241084098816, 'test/bleu': 30.37345328936431, 'test/num_examples': 3003, 'score': 36161.35554885864, 'total_duration': 58637.47705602646, 'accumulated_submission_time': 36161.35554885864, 'accumulated_eval_time': 22471.217081546783, 'accumulated_logging_time': 1.451249599456787, 'global_step': 103965, 'preemption_count': 0}), (106383, {'train/accuracy': 0.7143574953079224, 'train/loss': 1.4251322746276855, 'train/bleu': 36.8438553370823, 'validation/accuracy': 0.6882121562957764, 'validation/loss': 1.5287905931472778, 'validation/bleu': 30.254751595029035, 'validation/num_examples': 3000, 'test/accuracy': 0.7030503749847412, 'test/loss': 1.4487695693969727, 'test/bleu': 30.489053354388908, 'test/num_examples': 3003, 'score': 37001.44723653793, 'total_duration': 59985.43886613846, 'accumulated_submission_time': 37001.44723653793, 'accumulated_eval_time': 22978.963569164276, 'accumulated_logging_time': 1.4913089275360107, 'global_step': 106383, 'preemption_count': 0}), (108801, {'train/accuracy': 0.6898171305656433, 'train/loss': 1.552299976348877, 'train/bleu': 34.717774021813774, 'validation/accuracy': 0.6894644498825073, 'validation/loss': 1.5260159969329834, 'validation/bleu': 30.555977630907655, 'validation/num_examples': 3000, 'test/accuracy': 0.7047237157821655, 'test/loss': 1.4430897235870361, 'test/bleu': 30.474966315456253, 'test/num_examples': 3003, 'score': 37841.80606007576, 'total_duration': 61331.99062085152, 'accumulated_submission_time': 37841.80606007576, 'accumulated_eval_time': 23485.036183595657, 'accumulated_logging_time': 1.5315580368041992, 'global_step': 108801, 'preemption_count': 0}), (111219, {'train/accuracy': 0.6886301040649414, 'train/loss': 1.5508441925048828, 'train/bleu': 34.74471014801158, 'validation/accuracy': 0.6914111375808716, 'validation/loss': 1.5159528255462646, 'validation/bleu': 30.64500096572725, 'validation/num_examples': 3000, 'test/accuracy': 0.7083958387374878, 'test/loss': 1.4284865856170654, 'test/bleu': 30.872641774580426, 'test/num_examples': 3003, 'score': 38681.824439287186, 'total_duration': 62669.208958387375, 'accumulated_submission_time': 38681.824439287186, 'accumulated_eval_time': 23982.11319732666, 'accumulated_logging_time': 1.5726063251495361, 'global_step': 111219, 'preemption_count': 0}), (113635, {'train/accuracy': 0.7003282308578491, 'train/loss': 1.4843988418579102, 'train/bleu': 35.50631142680764, 'validation/accuracy': 0.6920930743217468, 'validation/loss': 1.5145164728164673, 'validation/bleu': 30.741345891342124, 'validation/num_examples': 3000, 'test/accuracy': 0.7083725929260254, 'test/loss': 1.428377389907837, 'test/bleu': 30.95872246263002, 'test/num_examples': 3003, 'score': 39521.848861932755, 'total_duration': 63978.92404127121, 'accumulated_submission_time': 39521.848861932755, 'accumulated_eval_time': 24451.677863121033, 'accumulated_logging_time': 1.6134326457977295, 'global_step': 113635, 'preemption_count': 0}), (116053, {'train/accuracy': 0.6954756379127502, 'train/loss': 1.5078685283660889, 'train/bleu': 35.4937397891447, 'validation/accuracy': 0.6918203234672546, 'validation/loss': 1.5126873254776, 'validation/bleu': 30.846350466007053, 'validation/num_examples': 3000, 'test/accuracy': 0.7091976404190063, 'test/loss': 1.4271239042282104, 'test/bleu': 31.064218560260848, 'test/num_examples': 3003, 'score': 40361.91211247444, 'total_duration': 65312.42619681358, 'accumulated_submission_time': 40361.91211247444, 'accumulated_eval_time': 24944.996393442154, 'accumulated_logging_time': 1.653876543045044, 'global_step': 116053, 'preemption_count': 0}), (118470, {'train/accuracy': 0.6973755359649658, 'train/loss': 1.5098248720169067, 'train/bleu': 35.351594846978585, 'validation/accuracy': 0.691745936870575, 'validation/loss': 1.5092313289642334, 'validation/bleu': 30.6144301486764, 'validation/num_examples': 3000, 'test/accuracy': 0.7104526162147522, 'test/loss': 1.4224282503128052, 'test/bleu': 30.918699317625673, 'test/num_examples': 3003, 'score': 41202.006234169006, 'total_duration': 66649.37670564651, 'accumulated_submission_time': 41202.006234169006, 'accumulated_eval_time': 25441.722013235092, 'accumulated_logging_time': 1.702117919921875, 'global_step': 118470, 'preemption_count': 0}), (120888, {'train/accuracy': 0.7043367624282837, 'train/loss': 1.4685285091400146, 'train/bleu': 35.96213979511151, 'validation/accuracy': 0.6929238438606262, 'validation/loss': 1.5071839094161987, 'validation/bleu': 30.69302274081346, 'validation/num_examples': 3000, 'test/accuracy': 0.7091627717018127, 'test/loss': 1.4226619005203247, 'test/bleu': 31.02129557676758, 'test/num_examples': 3003, 'score': 42042.0838637352, 'total_duration': 67990.92956995964, 'accumulated_submission_time': 42042.0838637352, 'accumulated_eval_time': 25943.07703590393, 'accumulated_logging_time': 1.7425308227539062, 'global_step': 120888, 'preemption_count': 0}), (123305, {'train/accuracy': 0.7063530087471008, 'train/loss': 1.4596189260482788, 'train/bleu': 36.09334633422933, 'validation/accuracy': 0.6923534870147705, 'validation/loss': 1.504960298538208, 'validation/bleu': 30.758487524049556, 'validation/num_examples': 3000, 'test/accuracy': 0.7106502056121826, 'test/loss': 1.4200072288513184, 'test/bleu': 31.02842778685176, 'test/num_examples': 3003, 'score': 42882.12957930565, 'total_duration': 69315.98330974579, 'accumulated_submission_time': 42882.12957930565, 'accumulated_eval_time': 26427.961848020554, 'accumulated_logging_time': 1.7832961082458496, 'global_step': 123305, 'preemption_count': 0}), (125724, {'train/accuracy': 0.7098019123077393, 'train/loss': 1.4383488893508911, 'train/bleu': 36.35349656081629, 'validation/accuracy': 0.6933577656745911, 'validation/loss': 1.502547264099121, 'validation/bleu': 30.86917300683674, 'validation/num_examples': 3000, 'test/accuracy': 0.710615336894989, 'test/loss': 1.4180010557174683, 'test/bleu': 31.023551642698948, 'test/num_examples': 3003, 'score': 43722.28950881958, 'total_duration': 70639.37914276123, 'accumulated_submission_time': 43722.28950881958, 'accumulated_eval_time': 26911.074389457703, 'accumulated_logging_time': 1.8275110721588135, 'global_step': 125724, 'preemption_count': 0}), (128142, {'train/accuracy': 0.7098627090454102, 'train/loss': 1.4390041828155518, 'train/bleu': 36.20999485164631, 'validation/accuracy': 0.6937545537948608, 'validation/loss': 1.502580165863037, 'validation/bleu': 30.961534202035583, 'validation/num_examples': 3000, 'test/accuracy': 0.7110220193862915, 'test/loss': 1.4158810377120972, 'test/bleu': 31.07880207453981, 'test/num_examples': 3003, 'score': 44562.2737493515, 'total_duration': 71956.04916667938, 'accumulated_submission_time': 44562.2737493515, 'accumulated_eval_time': 27387.63648557663, 'accumulated_logging_time': 1.872190237045288, 'global_step': 128142, 'preemption_count': 0}), (130560, {'train/accuracy': 0.7109185457229614, 'train/loss': 1.434372901916504, 'train/bleu': 36.323414242246834, 'validation/accuracy': 0.6939157247543335, 'validation/loss': 1.5021934509277344, 'validation/bleu': 30.871359894567888, 'validation/num_examples': 3000, 'test/accuracy': 0.711266040802002, 'test/loss': 1.415252923965454, 'test/bleu': 31.17882674781833, 'test/num_examples': 3003, 'score': 45402.228637218475, 'total_duration': 73272.68795681, 'accumulated_submission_time': 45402.228637218475, 'accumulated_eval_time': 27864.195012569427, 'accumulated_logging_time': 1.9160029888153076, 'global_step': 130560, 'preemption_count': 0}), (132978, {'train/accuracy': 0.7064332365989685, 'train/loss': 1.4602043628692627, 'train/bleu': 36.41087151297877, 'validation/accuracy': 0.6937917470932007, 'validation/loss': 1.502265214920044, 'validation/bleu': 30.904665803245663, 'validation/num_examples': 3000, 'test/accuracy': 0.7113938927650452, 'test/loss': 1.4153876304626465, 'test/bleu': 31.23585328606663, 'test/num_examples': 3003, 'score': 46242.14085435867, 'total_duration': 74583.09764313698, 'accumulated_submission_time': 46242.14085435867, 'accumulated_eval_time': 28334.5637383461, 'accumulated_logging_time': 1.960179090499878, 'global_step': 132978, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7103666067123413, 'train/loss': 1.433834433555603, 'train/bleu': 36.36197193300483, 'validation/accuracy': 0.6937669515609741, 'validation/loss': 1.502273440361023, 'validation/bleu': 30.902044416208398, 'validation/num_examples': 3000, 'test/accuracy': 0.7113706469535828, 'test/loss': 1.415420651435852, 'test/bleu': 31.22193609040821, 'test/num_examples': 3003, 'score': 46365.04322433472, 'total_duration': 75180.2390575409, 'accumulated_submission_time': 46365.04322433472, 'accumulated_eval_time': 28808.747275829315, 'accumulated_logging_time': 2.0037403106689453, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0211 11:40:52.055008 140144802662208 submission_runner.py:586] Timing: 46365.04322433472
I0211 11:40:52.055059 140144802662208 submission_runner.py:588] Total number of evals: 57
I0211 11:40:52.055100 140144802662208 submission_runner.py:589] ====================
I0211 11:40:52.055143 140144802662208 submission_runner.py:542] Using RNG seed 599091471
I0211 11:40:52.056802 140144802662208 submission_runner.py:551] --- Tuning run 2/5 ---
I0211 11:40:52.056896 140144802662208 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_2.
I0211 11:40:52.057129 140144802662208 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_2/hparams.json.
I0211 11:40:52.057886 140144802662208 submission_runner.py:206] Initializing dataset.
I0211 11:40:52.060528 140144802662208 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0211 11:40:52.063356 140144802662208 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0211 11:40:52.100250 140144802662208 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0211 11:40:52.728252 140144802662208 submission_runner.py:213] Initializing model.
I0211 11:40:59.243408 140144802662208 submission_runner.py:255] Initializing optimizer.
I0211 11:41:00.046113 140144802662208 submission_runner.py:262] Initializing metrics bundle.
I0211 11:41:00.046298 140144802662208 submission_runner.py:280] Initializing checkpoint and logger.
I0211 11:41:00.047071 140144802662208 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/wmt_jax/trial_2 with prefix checkpoint_
I0211 11:41:00.047222 140144802662208 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_2/meta_data_0.json.
I0211 11:41:00.047433 140144802662208 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0211 11:41:00.047495 140144802662208 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0211 11:41:00.483609 140144802662208 logger_utils.py:220] Unable to record git information. Continuing without it.
I0211 11:41:00.904158 140144802662208 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_2/flags_0.json.
I0211 11:41:00.910941 140144802662208 submission_runner.py:314] Starting training loop.
I0211 11:41:28.914552 139975029864192 logging_writer.py:48] [0] global_step=0, grad_norm=4.7181220054626465, loss=10.972803115844727
I0211 11:41:28.924575 140144802662208 spec.py:321] Evaluating on the training split.
I0211 11:41:31.603807 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:46:17.071356 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 11:46:19.784826 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:51:04.530420 140144802662208 spec.py:349] Evaluating on the test split.
I0211 11:51:07.234156 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 11:55:50.905885 140144802662208 submission_runner.py:408] Time since start: 889.99s, 	Step: 1, 	{'train/accuracy': 0.0006404464365914464, 'train/loss': 10.957476615905762, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 28.013572454452515, 'total_duration': 889.9948675632477, 'accumulated_submission_time': 28.013572454452515, 'accumulated_eval_time': 861.9812302589417, 'accumulated_logging_time': 0}
I0211 11:55:50.915149 139975038256896 logging_writer.py:48] [1] accumulated_eval_time=861.981230, accumulated_logging_time=0, accumulated_submission_time=28.013572, global_step=1, preemption_count=0, score=28.013572, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.966498, test/num_examples=3003, total_duration=889.994868, train/accuracy=0.000640, train/bleu=0.000000, train/loss=10.957477, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.980294, validation/num_examples=3000
I0211 11:56:25.656191 139975029864192 logging_writer.py:48] [100] global_step=100, grad_norm=0.26217153668403625, loss=9.08098316192627
I0211 11:57:00.427306 139975038256896 logging_writer.py:48] [200] global_step=200, grad_norm=0.20238712430000305, loss=8.686631202697754
I0211 11:57:35.220410 139975029864192 logging_writer.py:48] [300] global_step=300, grad_norm=0.8620086312294006, loss=8.317460060119629
I0211 11:58:10.039590 139975038256896 logging_writer.py:48] [400] global_step=400, grad_norm=0.70204097032547, loss=8.102167129516602
I0211 11:58:44.910283 139975029864192 logging_writer.py:48] [500] global_step=500, grad_norm=1.052115559577942, loss=7.861620903015137
I0211 11:59:19.768122 139975038256896 logging_writer.py:48] [600] global_step=600, grad_norm=0.6750308275222778, loss=7.653686046600342
I0211 11:59:54.626684 139975029864192 logging_writer.py:48] [700] global_step=700, grad_norm=0.8003464341163635, loss=7.532662391662598
I0211 12:00:29.485968 139975038256896 logging_writer.py:48] [800] global_step=800, grad_norm=0.6045341491699219, loss=7.298056602478027
I0211 12:01:04.334363 139975029864192 logging_writer.py:48] [900] global_step=900, grad_norm=0.5855745673179626, loss=7.110593318939209
I0211 12:01:39.197337 139975038256896 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7684279084205627, loss=7.008936405181885
I0211 12:02:14.104193 139975029864192 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5497047901153564, loss=6.811478614807129
I0211 12:02:48.954969 139975038256896 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5596245527267456, loss=6.706514358520508
I0211 12:03:23.777524 139975029864192 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5158026218414307, loss=6.641334056854248
I0211 12:03:58.580295 139975038256896 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6539942026138306, loss=6.5464277267456055
I0211 12:04:33.430430 139975029864192 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7065696716308594, loss=6.438429832458496
I0211 12:05:08.322229 139975038256896 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6087493300437927, loss=6.304315090179443
I0211 12:05:43.188894 139975029864192 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6007799506187439, loss=6.214449405670166
I0211 12:06:18.066251 139975038256896 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7954558730125427, loss=6.16202449798584
I0211 12:06:52.887513 139975029864192 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7098702192306519, loss=6.023778915405273
I0211 12:07:27.734073 139975038256896 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6988196969032288, loss=5.972289562225342
I0211 12:08:02.579954 139975029864192 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6375620365142822, loss=5.82429313659668
I0211 12:08:37.420250 139975038256896 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5037304162979126, loss=5.7686285972595215
I0211 12:09:12.253581 139975029864192 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7790490388870239, loss=5.699641227722168
I0211 12:09:47.111572 139975038256896 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.4909802973270416, loss=5.5310773849487305
I0211 12:09:51.023199 140144802662208 spec.py:321] Evaluating on the training split.
I0211 12:09:54.004297 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 12:13:17.336695 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 12:13:20.013643 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 12:16:29.534550 140144802662208 spec.py:349] Evaluating on the test split.
I0211 12:16:32.220065 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 12:19:41.772216 140144802662208 submission_runner.py:408] Time since start: 2320.86s, 	Step: 2413, 	{'train/accuracy': 0.42854857444763184, 'train/loss': 3.919285297393799, 'train/bleu': 15.366745138241077, 'validation/accuracy': 0.41834571957588196, 'validation/loss': 4.008279800415039, 'validation/bleu': 10.708148967332471, 'validation/num_examples': 3000, 'test/accuracy': 0.4059613049030304, 'test/loss': 4.175229549407959, 'test/bleu': 9.371626452081102, 'test/num_examples': 3003, 'score': 868.0313177108765, 'total_duration': 2320.8611991405487, 'accumulated_submission_time': 868.0313177108765, 'accumulated_eval_time': 1452.7302029132843, 'accumulated_logging_time': 0.01902604103088379}
I0211 12:19:41.787648 139975029864192 logging_writer.py:48] [2413] accumulated_eval_time=1452.730203, accumulated_logging_time=0.019026, accumulated_submission_time=868.031318, global_step=2413, preemption_count=0, score=868.031318, test/accuracy=0.405961, test/bleu=9.371626, test/loss=4.175230, test/num_examples=3003, total_duration=2320.861199, train/accuracy=0.428549, train/bleu=15.366745, train/loss=3.919285, validation/accuracy=0.418346, validation/bleu=10.708149, validation/loss=4.008280, validation/num_examples=3000
I0211 12:20:12.325079 139975038256896 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6553287506103516, loss=5.5900654792785645
I0211 12:20:47.151013 139975029864192 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.5439057350158691, loss=5.376059055328369
I0211 12:21:22.043957 139975038256896 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7702192664146423, loss=5.427479267120361
I0211 12:21:56.936504 139975029864192 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7414649128913879, loss=5.3610944747924805
I0211 12:22:31.757230 139975038256896 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5093289017677307, loss=5.2389140129089355
I0211 12:23:06.595007 139975029864192 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5242716073989868, loss=5.214005470275879
I0211 12:23:41.451672 139975038256896 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5353008508682251, loss=5.185771942138672
I0211 12:24:16.300255 139975029864192 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.46857914328575134, loss=5.133233547210693
I0211 12:24:51.145765 139975038256896 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5407776832580566, loss=5.037451267242432
I0211 12:25:25.985146 139975029864192 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.661708652973175, loss=5.083200454711914
I0211 12:26:00.837998 139975038256896 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.4753563702106476, loss=4.98665189743042
I0211 12:26:35.729403 139975029864192 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5021697282791138, loss=5.030281066894531
I0211 12:27:10.586330 139975038256896 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.45826205611228943, loss=4.959399223327637
I0211 12:27:45.567096 139975029864192 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.49132591485977173, loss=4.950759410858154
I0211 12:28:20.429726 139975038256896 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.42912933230400085, loss=4.899035930633545
I0211 12:28:55.309210 139975029864192 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.4088696539402008, loss=4.909001350402832
I0211 12:29:30.176063 139975038256896 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.4435878396034241, loss=4.875472545623779
I0211 12:30:04.995935 139975029864192 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.49942487478256226, loss=4.854422569274902
I0211 12:30:39.826435 139975038256896 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.41098085045814514, loss=4.796177864074707
I0211 12:31:14.679986 139975029864192 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.4251064360141754, loss=4.778979301452637
I0211 12:31:49.501718 139975038256896 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.42866250872612, loss=4.7460784912109375
I0211 12:32:24.361338 139975029864192 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.4168868064880371, loss=4.751546859741211
I0211 12:32:59.184895 139975038256896 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.3653601408004761, loss=4.708460807800293
I0211 12:33:34.032415 139975029864192 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.40667206048965454, loss=4.706079959869385
I0211 12:33:42.109874 140144802662208 spec.py:321] Evaluating on the training split.
I0211 12:33:45.091725 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 12:36:27.736881 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 12:36:30.420480 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 12:39:05.102310 140144802662208 spec.py:349] Evaluating on the test split.
I0211 12:39:07.813754 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 12:41:32.716170 140144802662208 submission_runner.py:408] Time since start: 3631.81s, 	Step: 4825, 	{'train/accuracy': 0.5466793179512024, 'train/loss': 2.79789137840271, 'train/bleu': 24.074602836910767, 'validation/accuracy': 0.5493918061256409, 'validation/loss': 2.760988473892212, 'validation/bleu': 20.896848938078836, 'validation/num_examples': 3000, 'test/accuracy': 0.5490791201591492, 'test/loss': 2.7924811840057373, 'test/bleu': 19.09038173838629, 'test/num_examples': 3003, 'score': 1708.2604315280914, 'total_duration': 3631.8051176071167, 'accumulated_submission_time': 1708.2604315280914, 'accumulated_eval_time': 1923.3364017009735, 'accumulated_logging_time': 0.0459742546081543}
I0211 12:41:32.737786 139975038256896 logging_writer.py:48] [4825] accumulated_eval_time=1923.336402, accumulated_logging_time=0.045974, accumulated_submission_time=1708.260432, global_step=4825, preemption_count=0, score=1708.260432, test/accuracy=0.549079, test/bleu=19.090382, test/loss=2.792481, test/num_examples=3003, total_duration=3631.805118, train/accuracy=0.546679, train/bleu=24.074603, train/loss=2.797891, validation/accuracy=0.549392, validation/bleu=20.896849, validation/loss=2.760988, validation/num_examples=3000
I0211 12:41:59.146510 139975029864192 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3525291383266449, loss=4.700904369354248
I0211 12:42:33.930985 139975038256896 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.36115822196006775, loss=4.694726943969727
I0211 12:43:08.787665 139975029864192 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.3318571150302887, loss=4.643216133117676
I0211 12:43:43.634131 139975038256896 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.317073255777359, loss=4.633093357086182
I0211 12:44:18.461335 139975029864192 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.3473748564720154, loss=4.607985973358154
I0211 12:44:53.282222 139975038256896 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.34914544224739075, loss=4.624570369720459
I0211 12:45:28.094482 139975029864192 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.33914715051651, loss=4.644702911376953
I0211 12:46:02.926376 139975038256896 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.3415662944316864, loss=4.6280317306518555
I0211 12:46:37.781798 139975029864192 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.35512804985046387, loss=4.617014408111572
I0211 12:47:12.600824 139975038256896 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.33258458971977234, loss=4.582523345947266
I0211 12:47:47.409771 139975029864192 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.3171929717063904, loss=4.575563430786133
I0211 12:48:22.219553 139975038256896 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.29526814818382263, loss=4.558787822723389
I0211 12:48:57.043656 139975029864192 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.31876808404922485, loss=4.581782341003418
I0211 12:49:31.879381 139975038256896 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.2787824869155884, loss=4.579863548278809
I0211 12:50:06.711401 139975029864192 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.3035103678703308, loss=4.5092549324035645
I0211 12:50:41.526013 139975038256896 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.26860231161117554, loss=4.512941837310791
I0211 12:51:16.361945 139975029864192 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.258672297000885, loss=4.539217948913574
I0211 12:51:51.242318 139975038256896 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.28254255652427673, loss=4.573749542236328
I0211 12:52:26.064635 139975029864192 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.25596514344215393, loss=4.514712333679199
I0211 12:53:00.891810 139975038256896 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.24393367767333984, loss=4.484755516052246
I0211 12:53:35.712505 139975029864192 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.2925875782966614, loss=4.515305042266846
I0211 12:54:10.521010 139975038256896 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.22852706909179688, loss=4.464118957519531
I0211 12:54:45.334742 139975029864192 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.29699286818504333, loss=4.527080059051514
I0211 12:55:20.162924 139975038256896 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.26832395792007446, loss=4.43571662902832
I0211 12:55:32.780690 140144802662208 spec.py:321] Evaluating on the training split.
I0211 12:55:35.764269 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 12:58:14.041272 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 12:58:16.739370 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 13:00:51.108690 140144802662208 spec.py:349] Evaluating on the test split.
I0211 13:00:53.804537 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 13:03:15.335971 140144802662208 submission_runner.py:408] Time since start: 4934.42s, 	Step: 7238, 	{'train/accuracy': 0.582229733467102, 'train/loss': 2.4481654167175293, 'train/bleu': 25.84377936152035, 'validation/accuracy': 0.5931110382080078, 'validation/loss': 2.375218152999878, 'validation/bleu': 23.345873190842095, 'validation/num_examples': 3000, 'test/accuracy': 0.5952356457710266, 'test/loss': 2.362957000732422, 'test/bleu': 22.041756144935494, 'test/num_examples': 3003, 'score': 2548.210542678833, 'total_duration': 4934.42494559288, 'accumulated_submission_time': 2548.210542678833, 'accumulated_eval_time': 2385.8916296958923, 'accumulated_logging_time': 0.07915925979614258}
I0211 13:03:15.351980 139975029864192 logging_writer.py:48] [7238] accumulated_eval_time=2385.891630, accumulated_logging_time=0.079159, accumulated_submission_time=2548.210543, global_step=7238, preemption_count=0, score=2548.210543, test/accuracy=0.595236, test/bleu=22.041756, test/loss=2.362957, test/num_examples=3003, total_duration=4934.424946, train/accuracy=0.582230, train/bleu=25.843779, train/loss=2.448165, validation/accuracy=0.593111, validation/bleu=23.345873, validation/loss=2.375218, validation/num_examples=3000
I0211 13:03:37.199296 139975038256896 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.2799662947654724, loss=4.411297798156738
I0211 13:04:11.905456 139975029864192 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.22520221769809723, loss=4.365459442138672
I0211 13:04:46.713404 139975038256896 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.22528253495693207, loss=4.382701873779297
I0211 13:05:21.509308 139975029864192 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.226129412651062, loss=4.352102279663086
I0211 13:05:56.325445 139975038256896 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.2301858812570572, loss=4.384777069091797
I0211 13:06:31.139799 139975029864192 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.22474700212478638, loss=4.400541305541992
I0211 13:07:05.936176 139975038256896 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.23131342232227325, loss=4.457603931427002
I0211 13:07:40.733032 139975029864192 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.21210680902004242, loss=4.3471269607543945
I0211 13:08:15.543326 139975038256896 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.2142927199602127, loss=4.369020462036133
I0211 13:08:50.330562 139975029864192 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.20308463275432587, loss=4.370053768157959
I0211 13:09:25.142420 139975038256896 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.22556820511817932, loss=4.340023040771484
I0211 13:09:59.938967 139975029864192 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.2220585197210312, loss=4.361198425292969
I0211 13:10:34.743187 139975038256896 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.18517877161502838, loss=4.257473468780518
I0211 13:11:09.537783 139975029864192 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.2049335390329361, loss=4.2584991455078125
I0211 13:11:44.347937 139975038256896 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.1997867077589035, loss=4.332850933074951
I0211 13:12:19.156243 139975029864192 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.2572048306465149, loss=4.273906230926514
I0211 13:12:53.965092 139975038256896 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.24245460331439972, loss=4.2878594398498535
I0211 13:13:28.755316 139975029864192 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.210484117269516, loss=4.390661716461182
I0211 13:14:03.534433 139975038256896 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.1815466284751892, loss=4.323421478271484
I0211 13:14:38.347278 139975029864192 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.20472028851509094, loss=4.373986721038818
I0211 13:15:13.158751 139975038256896 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.1874176561832428, loss=4.292516231536865
I0211 13:15:47.952115 139975029864192 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.18918277323246002, loss=4.340086460113525
I0211 13:16:22.792412 139975038256896 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.17886783182621002, loss=4.2568278312683105
I0211 13:16:57.643015 139975029864192 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.20181073248386383, loss=4.266094207763672
I0211 13:17:15.482136 140144802662208 spec.py:321] Evaluating on the training split.
I0211 13:17:18.462020 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 13:19:51.901438 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 13:19:54.584842 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 13:22:32.593811 140144802662208 spec.py:349] Evaluating on the test split.
I0211 13:22:35.276412 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 13:24:53.428268 140144802662208 submission_runner.py:408] Time since start: 6232.52s, 	Step: 9653, 	{'train/accuracy': 0.603490948677063, 'train/loss': 2.285142183303833, 'train/bleu': 28.78597901242265, 'validation/accuracy': 0.6179216504096985, 'validation/loss': 2.1735126972198486, 'validation/bleu': 25.328648638725443, 'validation/num_examples': 3000, 'test/accuracy': 0.6251118779182434, 'test/loss': 2.1336803436279297, 'test/bleu': 24.447402386344706, 'test/num_examples': 3003, 'score': 3388.2512698173523, 'total_duration': 6232.517226934433, 'accumulated_submission_time': 3388.2512698173523, 'accumulated_eval_time': 2843.837694168091, 'accumulated_logging_time': 0.10521483421325684}
I0211 13:24:53.444504 139975038256896 logging_writer.py:48] [9653] accumulated_eval_time=2843.837694, accumulated_logging_time=0.105215, accumulated_submission_time=3388.251270, global_step=9653, preemption_count=0, score=3388.251270, test/accuracy=0.625112, test/bleu=24.447402, test/loss=2.133680, test/num_examples=3003, total_duration=6232.517227, train/accuracy=0.603491, train/bleu=28.785979, train/loss=2.285142, validation/accuracy=0.617922, validation/bleu=25.328649, validation/loss=2.173513, validation/num_examples=3000
I0211 13:25:10.090828 139975029864192 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1843535453081131, loss=4.2047119140625
I0211 13:25:44.791647 139975038256896 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.17888818681240082, loss=4.272966384887695
I0211 13:26:19.559364 139975029864192 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.18402956426143646, loss=4.250502586364746
I0211 13:26:54.348643 139975038256896 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.18308284878730774, loss=4.219322681427002
I0211 13:27:29.149011 139975029864192 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.1800011545419693, loss=4.315279483795166
I0211 13:28:03.928219 139975038256896 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.20928020775318146, loss=4.241018772125244
I0211 13:28:38.711622 139975029864192 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.17631655931472778, loss=4.238050937652588
I0211 13:29:13.503556 139975038256896 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.17558616399765015, loss=4.2332072257995605
I0211 13:29:48.346114 139975029864192 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.17358149588108063, loss=4.319881916046143
I0211 13:30:23.146740 139975038256896 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.17791645228862762, loss=4.24631404876709
I0211 13:30:57.927725 139975029864192 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.19412684440612793, loss=4.288228511810303
I0211 13:31:32.733528 139975038256896 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.17048127949237823, loss=4.245293617248535
I0211 13:32:07.535540 139975029864192 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.17777489125728607, loss=4.217604637145996
I0211 13:32:42.332561 139975038256896 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.21955080330371857, loss=4.330517292022705
I0211 13:33:17.174025 139975029864192 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.1767873466014862, loss=4.1672163009643555
I0211 13:33:51.983041 139975038256896 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.16485917568206787, loss=4.172787189483643
I0211 13:34:26.786360 139975029864192 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.1627388745546341, loss=4.2249579429626465
I0211 13:35:01.661690 139975038256896 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.1978987455368042, loss=4.206389904022217
I0211 13:35:36.461430 139975029864192 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.17331962287425995, loss=4.220804691314697
I0211 13:36:11.264327 139975038256896 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.16269564628601074, loss=4.180063724517822
I0211 13:36:46.059176 139975029864192 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.16219517588615417, loss=4.230403423309326
I0211 13:37:20.849403 139975038256896 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.1622757762670517, loss=4.121253490447998
I0211 13:37:55.629499 139975029864192 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.16870899498462677, loss=4.2076616287231445
I0211 13:38:30.419630 139975038256896 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.18081451952457428, loss=4.226919174194336
I0211 13:38:53.431438 140144802662208 spec.py:321] Evaluating on the training split.
I0211 13:38:56.404954 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 13:41:32.977893 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 13:41:35.668872 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 13:44:10.844247 140144802662208 spec.py:349] Evaluating on the test split.
I0211 13:44:13.539561 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 13:46:33.455661 140144802662208 submission_runner.py:408] Time since start: 7532.54s, 	Step: 12068, 	{'train/accuracy': 0.6128459572792053, 'train/loss': 2.18766450881958, 'train/bleu': 29.374546077563878, 'validation/accuracy': 0.6334205269813538, 'validation/loss': 2.038684844970703, 'validation/bleu': 26.087020963661935, 'validation/num_examples': 3000, 'test/accuracy': 0.6416710615158081, 'test/loss': 1.9897395372390747, 'test/bleu': 25.192112223007232, 'test/num_examples': 3003, 'score': 4228.148333311081, 'total_duration': 7532.544618368149, 'accumulated_submission_time': 4228.148333311081, 'accumulated_eval_time': 3303.861836194992, 'accumulated_logging_time': 0.13168787956237793}
I0211 13:46:33.475556 139975029864192 logging_writer.py:48] [12068] accumulated_eval_time=3303.861836, accumulated_logging_time=0.131688, accumulated_submission_time=4228.148333, global_step=12068, preemption_count=0, score=4228.148333, test/accuracy=0.641671, test/bleu=25.192112, test/loss=1.989740, test/num_examples=3003, total_duration=7532.544618, train/accuracy=0.612846, train/bleu=29.374546, train/loss=2.187665, validation/accuracy=0.633421, validation/bleu=26.087021, validation/loss=2.038685, validation/num_examples=3000
I0211 13:46:44.947472 139975038256896 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.16065511107444763, loss=4.146259784698486
I0211 13:47:19.688603 139975029864192 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.15007799863815308, loss=4.195437431335449
I0211 13:47:54.506281 139975038256896 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.19969487190246582, loss=4.122410774230957
I0211 13:48:29.332871 139975029864192 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.17394591867923737, loss=4.162049293518066
I0211 13:49:04.123398 139975038256896 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.1789178103208542, loss=4.238186836242676
I0211 13:49:38.935227 139975029864192 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.17158031463623047, loss=4.198519706726074
I0211 13:50:13.753241 139975038256896 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1667856127023697, loss=4.2260847091674805
I0211 13:50:48.566977 139975029864192 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.1624293327331543, loss=4.192098140716553
I0211 13:51:23.365810 139975038256896 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.16456238925457, loss=4.109089374542236
I0211 13:51:58.146781 139975029864192 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.15449701249599457, loss=4.127838611602783
I0211 13:52:32.913230 139975038256896 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.158062145113945, loss=4.160905838012695
I0211 13:53:07.860682 139975029864192 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.15287883579730988, loss=4.164647102355957
I0211 13:53:42.694353 139975038256896 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.16206802427768707, loss=4.136922836303711
I0211 13:54:17.507520 139975029864192 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.166598841547966, loss=4.172173023223877
I0211 13:54:52.291279 139975038256896 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.16333256661891937, loss=4.137351036071777
I0211 13:55:27.086864 139975029864192 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.1509775072336197, loss=4.129972457885742
I0211 13:56:01.877180 139975038256896 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.15121009945869446, loss=4.146951675415039
I0211 13:56:36.667330 139975029864192 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.15049077570438385, loss=4.078397274017334
I0211 13:57:11.478503 139975038256896 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.1703917682170868, loss=4.211523056030273
I0211 13:57:46.257522 139975029864192 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.14564938843250275, loss=4.129734992980957
I0211 13:58:21.048163 139975038256896 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.15036821365356445, loss=4.138505935668945
I0211 13:58:55.844476 139975029864192 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.1502819061279297, loss=4.142110824584961
I0211 13:59:30.614304 139975038256896 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.17721877992153168, loss=4.078577995300293
I0211 14:00:05.392710 139975029864192 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.14875154197216034, loss=4.131098747253418
I0211 14:00:33.649118 140144802662208 spec.py:321] Evaluating on the training split.
I0211 14:00:36.624943 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 14:03:01.940509 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 14:03:04.629138 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 14:05:38.749582 140144802662208 spec.py:349] Evaluating on the test split.
I0211 14:05:41.451474 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 14:07:52.963571 140144802662208 submission_runner.py:408] Time since start: 8812.05s, 	Step: 14483, 	{'train/accuracy': 0.6248981952667236, 'train/loss': 2.080275535583496, 'train/bleu': 30.089301867317968, 'validation/accuracy': 0.641777515411377, 'validation/loss': 1.9527431726455688, 'validation/bleu': 27.244437164531448, 'validation/num_examples': 3000, 'test/accuracy': 0.6509325504302979, 'test/loss': 1.8961308002471924, 'test/bleu': 26.48782476873385, 'test/num_examples': 3003, 'score': 5068.229043722153, 'total_duration': 8812.05256319046, 'accumulated_submission_time': 5068.229043722153, 'accumulated_eval_time': 3743.176248073578, 'accumulated_logging_time': 0.16280674934387207}
I0211 14:07:52.980576 139975038256896 logging_writer.py:48] [14483] accumulated_eval_time=3743.176248, accumulated_logging_time=0.162807, accumulated_submission_time=5068.229044, global_step=14483, preemption_count=0, score=5068.229044, test/accuracy=0.650933, test/bleu=26.487825, test/loss=1.896131, test/num_examples=3003, total_duration=8812.052563, train/accuracy=0.624898, train/bleu=30.089302, train/loss=2.080276, validation/accuracy=0.641778, validation/bleu=27.244437, validation/loss=1.952743, validation/num_examples=3000
I0211 14:07:59.233813 139975029864192 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.1567518264055252, loss=4.069642543792725
I0211 14:08:33.908932 139975038256896 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.1495925784111023, loss=4.12910270690918
I0211 14:09:08.647007 139975029864192 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.1530865728855133, loss=4.165112018585205
I0211 14:09:43.426533 139975038256896 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.16522042453289032, loss=4.108890056610107
I0211 14:10:18.213230 139975029864192 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.1503639817237854, loss=4.1466875076293945
I0211 14:10:53.006258 139975038256896 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.15610800683498383, loss=4.087738037109375
I0211 14:11:27.787385 139975029864192 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.1635151356458664, loss=4.162906646728516
I0211 14:12:02.592160 139975038256896 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.16186271607875824, loss=4.16875696182251
I0211 14:12:37.405485 139975029864192 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.17980611324310303, loss=4.127966403961182
I0211 14:13:12.194897 139975038256896 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.1497509628534317, loss=4.079938888549805
I0211 14:13:46.959791 139975029864192 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.16608203947544098, loss=4.066045761108398
I0211 14:14:21.719485 139975038256896 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.16030511260032654, loss=4.1397271156311035
I0211 14:14:56.495694 139975029864192 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.18346469104290009, loss=4.15429162979126
I0211 14:15:31.261284 139975038256896 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.17451389133930206, loss=4.08698844909668
I0211 14:16:06.042248 139975029864192 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.15282867848873138, loss=4.11778450012207
I0211 14:16:40.826717 139975038256896 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.1497468203306198, loss=4.006855010986328
I0211 14:17:15.629329 139975029864192 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.1543111801147461, loss=4.113213062286377
I0211 14:17:50.411599 139975038256896 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.1428017020225525, loss=4.037163257598877
I0211 14:18:25.192451 139975029864192 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.1584687978029251, loss=4.070450305938721
I0211 14:18:59.964192 139975038256896 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.1918262541294098, loss=4.049878120422363
I0211 14:19:34.762533 139975029864192 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.15509100258350372, loss=4.129871368408203
I0211 14:20:09.546006 139975038256896 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.16709581017494202, loss=4.009322166442871
I0211 14:20:44.343887 139975029864192 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.1591789275407791, loss=4.130252361297607
I0211 14:21:19.116730 139975038256896 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.15976542234420776, loss=4.04191780090332
I0211 14:21:53.281581 140144802662208 spec.py:321] Evaluating on the training split.
I0211 14:21:56.253336 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 14:24:40.423774 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 14:24:43.124188 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 14:27:17.332600 140144802662208 spec.py:349] Evaluating on the test split.
I0211 14:27:20.025277 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 14:29:40.152233 140144802662208 submission_runner.py:408] Time since start: 10119.24s, 	Step: 16900, 	{'train/accuracy': 0.6296117901802063, 'train/loss': 2.061455726623535, 'train/bleu': 30.56911605973895, 'validation/accuracy': 0.6512380242347717, 'validation/loss': 1.9128708839416504, 'validation/bleu': 27.760250709466792, 'validation/num_examples': 3000, 'test/accuracy': 0.6578467488288879, 'test/loss': 1.8625454902648926, 'test/bleu': 26.744218529681, 'test/num_examples': 3003, 'score': 5908.441868543625, 'total_duration': 10119.24120926857, 'accumulated_submission_time': 5908.441868543625, 'accumulated_eval_time': 4210.046847343445, 'accumulated_logging_time': 0.19021224975585938}
I0211 14:29:40.169387 139975029864192 logging_writer.py:48] [16900] accumulated_eval_time=4210.046847, accumulated_logging_time=0.190212, accumulated_submission_time=5908.441869, global_step=16900, preemption_count=0, score=5908.441869, test/accuracy=0.657847, test/bleu=26.744219, test/loss=1.862545, test/num_examples=3003, total_duration=10119.241209, train/accuracy=0.629612, train/bleu=30.569116, train/loss=2.061456, validation/accuracy=0.651238, validation/bleu=27.760251, validation/loss=1.912871, validation/num_examples=3000
I0211 14:29:40.545969 139975038256896 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.15620912611484528, loss=4.109904766082764
I0211 14:30:15.214292 139975029864192 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.17967310547828674, loss=4.184070110321045
I0211 14:30:49.948036 139975038256896 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.15549516677856445, loss=4.11480188369751
I0211 14:31:24.752170 139975029864192 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.15413103997707367, loss=4.041161060333252
I0211 14:31:59.536332 139975038256896 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.15457016229629517, loss=4.092526435852051
I0211 14:32:34.320591 139975029864192 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.2045086771249771, loss=4.042363166809082
I0211 14:33:09.101973 139975038256896 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.15302011370658875, loss=4.076357364654541
I0211 14:33:43.902198 139975029864192 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.15837916731834412, loss=4.104567527770996
I0211 14:34:18.731627 139975038256896 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.1512085497379303, loss=4.041872024536133
I0211 14:34:53.530033 139975029864192 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.14350613951683044, loss=4.062098979949951
I0211 14:35:28.308346 139975038256896 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.16018696129322052, loss=4.040727138519287
I0211 14:36:03.100749 139975029864192 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.20446158945560455, loss=4.059205532073975
I0211 14:36:37.912568 139975038256896 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.14999064803123474, loss=4.01037073135376
I0211 14:37:12.690383 139975029864192 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.15913444757461548, loss=4.141750335693359
I0211 14:37:47.466513 139975038256896 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.14884328842163086, loss=3.995737075805664
I0211 14:38:22.241335 139975029864192 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.18526151776313782, loss=4.055729866027832
I0211 14:38:57.065731 139975038256896 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.15489009022712708, loss=4.05974006652832
I0211 14:39:31.851407 139975029864192 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.15542007982730865, loss=4.071793556213379
I0211 14:40:06.657330 139975038256896 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.15707461535930634, loss=4.004345893859863
I0211 14:40:41.438647 139975029864192 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.15623825788497925, loss=4.077561378479004
I0211 14:41:16.217394 139975038256896 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.18022990226745605, loss=3.988104820251465
I0211 14:41:51.023244 139975029864192 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.15601994097232819, loss=3.9981741905212402
I0211 14:42:25.818255 139975038256896 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.20442263782024384, loss=4.054588794708252
I0211 14:43:00.587687 139975029864192 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.18113480508327484, loss=4.021943092346191
I0211 14:43:35.419403 139975038256896 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.18115562200546265, loss=3.9700939655303955
I0211 14:43:40.352232 140144802662208 spec.py:321] Evaluating on the training split.
I0211 14:43:43.321091 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 14:46:51.153452 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 14:46:53.829643 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 14:49:32.077832 140144802662208 spec.py:349] Evaluating on the test split.
I0211 14:49:34.755202 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 14:51:56.548871 140144802662208 submission_runner.py:408] Time since start: 11455.64s, 	Step: 19316, 	{'train/accuracy': 0.6452200412750244, 'train/loss': 1.942769169807434, 'train/bleu': 31.575388848299944, 'validation/accuracy': 0.6562596559524536, 'validation/loss': 1.870498538017273, 'validation/bleu': 28.118415670737658, 'validation/num_examples': 3000, 'test/accuracy': 0.6642031669616699, 'test/loss': 1.8098138570785522, 'test/bleu': 27.05667497708177, 'test/num_examples': 3003, 'score': 6748.534591674805, 'total_duration': 11455.637856960297, 'accumulated_submission_time': 6748.534591674805, 'accumulated_eval_time': 4706.243428945541, 'accumulated_logging_time': 0.2173449993133545}
I0211 14:51:56.566119 139975029864192 logging_writer.py:48] [19316] accumulated_eval_time=4706.243429, accumulated_logging_time=0.217345, accumulated_submission_time=6748.534592, global_step=19316, preemption_count=0, score=6748.534592, test/accuracy=0.664203, test/bleu=27.056675, test/loss=1.809814, test/num_examples=3003, total_duration=11455.637857, train/accuracy=0.645220, train/bleu=31.575389, train/loss=1.942769, validation/accuracy=0.656260, validation/bleu=28.118416, validation/loss=1.870499, validation/num_examples=3000
I0211 14:52:26.023961 139975038256896 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.1622275710105896, loss=4.06479024887085
I0211 14:53:00.739408 139975029864192 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.24330440163612366, loss=3.974120855331421
I0211 14:53:35.497971 139975038256896 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.15605084598064423, loss=4.119054317474365
I0211 14:54:10.293196 139975029864192 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.15821640193462372, loss=4.0339035987854
I0211 14:54:45.061891 139975038256896 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.16691185534000397, loss=4.028968334197998
I0211 14:55:19.841902 139975029864192 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.1619047373533249, loss=4.077203273773193
I0211 14:55:54.639204 139975038256896 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.1680479347705841, loss=4.026738166809082
I0211 14:56:29.461948 139975029864192 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.17573562264442444, loss=3.9712202548980713
I0211 14:57:04.305675 139975038256896 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.16088902950286865, loss=3.9843785762786865
I0211 14:57:39.082338 139975029864192 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.16160303354263306, loss=4.048166275024414
I0211 14:58:13.848238 139975038256896 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.16755709052085876, loss=4.080784320831299
I0211 14:58:48.680369 139975029864192 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.18347997963428497, loss=4.0037407875061035
I0211 14:59:23.474179 139975038256896 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.1740548461675644, loss=3.944894790649414
I0211 14:59:58.314183 139975029864192 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.15634340047836304, loss=3.9487826824188232
I0211 15:00:33.154841 139975038256896 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.16712403297424316, loss=4.024500370025635
I0211 15:01:08.061379 139975029864192 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.1630193591117859, loss=3.9947032928466797
I0211 15:01:42.847125 139975038256896 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.15873652696609497, loss=4.0018839836120605
I0211 15:02:17.651694 139975029864192 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.1724437177181244, loss=4.030585765838623
I0211 15:02:52.456441 139975038256896 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.18142862617969513, loss=4.045987129211426
I0211 15:03:27.247059 139975029864192 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.20333947241306305, loss=4.040139675140381
I0211 15:04:02.034409 139975038256896 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.16968584060668945, loss=4.03986930847168
I0211 15:04:36.814798 139975029864192 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.17286626994609833, loss=4.0210981369018555
I0211 15:05:11.592719 139975038256896 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.1672568917274475, loss=3.954937219619751
I0211 15:05:46.341336 139975029864192 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.16540952026844025, loss=3.9329309463500977
I0211 15:05:56.848981 140144802662208 spec.py:321] Evaluating on the training split.
I0211 15:05:59.827565 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 15:08:55.327732 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 15:08:58.019168 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 15:11:30.894775 140144802662208 spec.py:349] Evaluating on the test split.
I0211 15:11:33.588606 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 15:13:52.499412 140144802662208 submission_runner.py:408] Time since start: 12771.59s, 	Step: 21732, 	{'train/accuracy': 0.6420875787734985, 'train/loss': 1.9512648582458496, 'train/bleu': 31.35088749641315, 'validation/accuracy': 0.6596198081970215, 'validation/loss': 1.8318740129470825, 'validation/bleu': 28.56932057810524, 'validation/num_examples': 3000, 'test/accuracy': 0.6690372824668884, 'test/loss': 1.7705334424972534, 'test/bleu': 27.950302449347422, 'test/num_examples': 3003, 'score': 7588.722680091858, 'total_duration': 12771.588398694992, 'accumulated_submission_time': 7588.722680091858, 'accumulated_eval_time': 5181.89380979538, 'accumulated_logging_time': 0.24721455574035645}
I0211 15:13:52.518106 139975038256896 logging_writer.py:48] [21732] accumulated_eval_time=5181.893810, accumulated_logging_time=0.247215, accumulated_submission_time=7588.722680, global_step=21732, preemption_count=0, score=7588.722680, test/accuracy=0.669037, test/bleu=27.950302, test/loss=1.770533, test/num_examples=3003, total_duration=12771.588399, train/accuracy=0.642088, train/bleu=31.350887, train/loss=1.951265, validation/accuracy=0.659620, validation/bleu=28.569321, validation/loss=1.831874, validation/num_examples=3000
I0211 15:14:16.463047 139975029864192 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.17814461886882782, loss=4.0007147789001465
I0211 15:14:51.196771 139975038256896 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.165583074092865, loss=4.058062553405762
I0211 15:15:25.982211 139975029864192 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.1672447919845581, loss=4.119290351867676
I0211 15:16:00.769577 139975038256896 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.17173530161380768, loss=4.065678596496582
I0211 15:16:35.569760 139975029864192 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.16131329536437988, loss=4.0665459632873535
I0211 15:17:10.355598 139975038256896 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.18635116517543793, loss=4.026473522186279
I0211 15:17:45.151906 139975029864192 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.1827288419008255, loss=4.060482501983643
I0211 15:18:19.931457 139975038256896 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.16952703893184662, loss=4.01963472366333
I0211 15:18:54.703754 139975029864192 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.1665915846824646, loss=4.029799461364746
I0211 15:19:29.510701 139975038256896 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.20138497650623322, loss=3.980494499206543
I0211 15:20:04.320143 139975029864192 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.15870346128940582, loss=3.9701695442199707
I0211 15:20:39.107975 139975038256896 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.17538732290267944, loss=4.015294551849365
I0211 15:21:13.887016 139975029864192 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.18117423355579376, loss=3.9861042499542236
I0211 15:21:48.713004 139975038256896 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.21433405578136444, loss=3.97814679145813
I0211 15:22:23.560842 139975029864192 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.20492003858089447, loss=4.016623497009277
I0211 15:22:58.391932 139975038256896 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.16211925446987152, loss=4.000675201416016
I0211 15:23:33.229298 139975029864192 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.1798473447561264, loss=4.049149036407471
I0211 15:24:08.020837 139975038256896 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.17722821235656738, loss=3.9624083042144775
I0211 15:24:42.819010 139975029864192 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.1759849190711975, loss=4.039781093597412
I0211 15:25:17.628057 139975038256896 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.20596031844615936, loss=3.9723589420318604
I0211 15:25:52.424968 139975029864192 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.1718153953552246, loss=3.9695804119110107
I0211 15:26:27.214521 139975038256896 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.19741618633270264, loss=4.015462398529053
I0211 15:27:01.994625 139975029864192 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.1739712804555893, loss=3.9858996868133545
I0211 15:27:36.804847 139975038256896 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.17785201966762543, loss=3.993654727935791
I0211 15:27:52.544975 140144802662208 spec.py:321] Evaluating on the training split.
I0211 15:27:55.519604 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 15:30:46.095625 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 15:30:48.800554 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 15:33:30.334856 140144802662208 spec.py:349] Evaluating on the test split.
I0211 15:33:33.034924 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 15:35:53.174489 140144802662208 submission_runner.py:408] Time since start: 14092.26s, 	Step: 24147, 	{'train/accuracy': 0.6424695253372192, 'train/loss': 1.9587422609329224, 'train/bleu': 31.319786346641795, 'validation/accuracy': 0.6618020534515381, 'validation/loss': 1.8124172687530518, 'validation/bleu': 28.48977382265801, 'validation/num_examples': 3000, 'test/accuracy': 0.6719539761543274, 'test/loss': 1.7517828941345215, 'test/bleu': 28.07511583204494, 'test/num_examples': 3003, 'score': 8428.657732963562, 'total_duration': 14092.263476133347, 'accumulated_submission_time': 8428.657732963562, 'accumulated_eval_time': 5662.523268461227, 'accumulated_logging_time': 0.27626967430114746}
I0211 15:35:53.193458 139975029864192 logging_writer.py:48] [24147] accumulated_eval_time=5662.523268, accumulated_logging_time=0.276270, accumulated_submission_time=8428.657733, global_step=24147, preemption_count=0, score=8428.657733, test/accuracy=0.671954, test/bleu=28.075116, test/loss=1.751783, test/num_examples=3003, total_duration=14092.263476, train/accuracy=0.642470, train/bleu=31.319786, train/loss=1.958742, validation/accuracy=0.661802, validation/bleu=28.489774, validation/loss=1.812417, validation/num_examples=3000
I0211 15:36:11.945335 139975038256896 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.2542358934879303, loss=3.9937727451324463
I0211 15:36:46.691871 139975029864192 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.18481269478797913, loss=4.00394344329834
I0211 15:37:21.458952 139975038256896 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.24886132776737213, loss=3.9337306022644043
I0211 15:37:56.257555 139975029864192 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.16671955585479736, loss=3.935979127883911
I0211 15:38:31.040985 139975038256896 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.15106050670146942, loss=4.014575481414795
I0211 15:39:05.824442 139975029864192 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.16899824142456055, loss=3.9466841220855713
I0211 15:39:40.620270 139975038256896 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.19350190460681915, loss=3.962116241455078
I0211 15:40:15.404884 139975029864192 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.18743766844272614, loss=4.0106964111328125
I0211 15:40:50.201678 139975038256896 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.18107734620571136, loss=3.912822723388672
I0211 15:41:24.997195 139975029864192 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.1824929267168045, loss=4.005850315093994
I0211 15:41:59.781826 139975038256896 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2067788988351822, loss=4.031284809112549
I0211 15:42:34.592240 139975029864192 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.1795629858970642, loss=3.9493260383605957
I0211 15:43:09.400089 139975038256896 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.19388407468795776, loss=3.9441208839416504
I0211 15:43:44.171954 139975029864192 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.21790345013141632, loss=3.920276403427124
I0211 15:44:18.965565 139975038256896 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.18927578628063202, loss=3.956249952316284
I0211 15:44:53.798981 139975029864192 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.17813357710838318, loss=3.9629461765289307
I0211 15:45:28.605525 139975038256896 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.17859520018100739, loss=4.016351222991943
I0211 15:46:03.559828 139975029864192 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.17964914441108704, loss=3.9168519973754883
I0211 15:46:38.415053 139975038256896 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.1791934221982956, loss=3.9708478450775146
I0211 15:47:13.257622 139975029864192 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.19963328540325165, loss=3.999387264251709
I0211 15:47:48.094208 139975038256896 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.18772386014461517, loss=3.997765064239502
I0211 15:48:22.894217 139975029864192 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.1770315170288086, loss=3.971554756164551
I0211 15:48:57.697690 139975038256896 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.1977270245552063, loss=3.9640626907348633
I0211 15:49:32.501557 139975029864192 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.16723045706748962, loss=3.972639799118042
I0211 15:49:53.453033 140144802662208 spec.py:321] Evaluating on the training split.
I0211 15:49:56.429516 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 15:54:15.495466 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 15:54:18.170101 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 15:57:43.818640 140144802662208 spec.py:349] Evaluating on the test split.
I0211 15:57:46.495405 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 16:00:52.102576 140144802662208 submission_runner.py:408] Time since start: 15591.19s, 	Step: 26562, 	{'train/accuracy': 0.6491546630859375, 'train/loss': 1.8978878259658813, 'train/bleu': 32.128709662827625, 'validation/accuracy': 0.6637735366821289, 'validation/loss': 1.7884488105773926, 'validation/bleu': 28.636768559280444, 'validation/num_examples': 3000, 'test/accuracy': 0.6746034622192383, 'test/loss': 1.7212979793548584, 'test/bleu': 28.215674239974483, 'test/num_examples': 3003, 'score': 9268.82431268692, 'total_duration': 15591.191541194916, 'accumulated_submission_time': 9268.82431268692, 'accumulated_eval_time': 6321.172736406326, 'accumulated_logging_time': 0.30533862113952637}
I0211 16:00:52.123990 139975038256896 logging_writer.py:48] [26562] accumulated_eval_time=6321.172736, accumulated_logging_time=0.305339, accumulated_submission_time=9268.824313, global_step=26562, preemption_count=0, score=9268.824313, test/accuracy=0.674603, test/bleu=28.215674, test/loss=1.721298, test/num_examples=3003, total_duration=15591.191541, train/accuracy=0.649155, train/bleu=32.128710, train/loss=1.897888, validation/accuracy=0.663774, validation/bleu=28.636769, validation/loss=1.788449, validation/num_examples=3000
I0211 16:01:05.671031 139975029864192 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.1961497813463211, loss=3.9786674976348877
I0211 16:01:40.353352 139975038256896 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.17298613488674164, loss=3.9195034503936768
I0211 16:02:15.095915 139975029864192 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.18536387383937836, loss=3.9712209701538086
I0211 16:02:49.883244 139975038256896 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.18843616545200348, loss=3.966913938522339
I0211 16:03:24.674609 139975029864192 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.21240316331386566, loss=3.977241039276123
I0211 16:03:59.504235 139975038256896 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.2550937831401825, loss=3.9252235889434814
I0211 16:04:34.278946 139975029864192 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.2035851776599884, loss=3.985929012298584
I0211 16:05:09.062914 139975038256896 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.18008960783481598, loss=3.984457492828369
I0211 16:05:43.869727 139975029864192 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.18607087433338165, loss=3.9502294063568115
I0211 16:06:18.661367 139975038256896 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.263817697763443, loss=4.025935649871826
I0211 16:06:53.440481 139975029864192 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.2698170244693756, loss=3.9802896976470947
I0211 16:07:28.224843 139975038256896 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.20252437889575958, loss=4.019820213317871
I0211 16:08:03.017880 139975029864192 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.19881762564182281, loss=3.9533941745758057
I0211 16:08:37.787743 139975038256896 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.208473339676857, loss=3.9663755893707275
I0211 16:09:12.580408 139975029864192 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.18373729288578033, loss=3.994258165359497
I0211 16:09:47.360379 139975038256896 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.25737449526786804, loss=3.986875534057617
I0211 16:10:22.171460 139975029864192 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.21822920441627502, loss=3.907525062561035
I0211 16:10:57.014167 139975038256896 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.1802339255809784, loss=3.9027607440948486
I0211 16:11:31.777910 139975029864192 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.2901225984096527, loss=3.948960781097412
I0211 16:12:06.584978 139975038256896 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.19037342071533203, loss=3.972600221633911
I0211 16:12:41.423825 139975029864192 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.18156252801418304, loss=3.8906233310699463
I0211 16:13:16.228499 139975038256896 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.20262928307056427, loss=3.982337713241577
I0211 16:13:50.997798 139975029864192 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1866540163755417, loss=3.9447994232177734
I0211 16:14:25.796209 139975038256896 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.19187752902507782, loss=3.986168384552002
I0211 16:14:52.296351 140144802662208 spec.py:321] Evaluating on the training split.
I0211 16:14:55.285528 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 16:17:40.353441 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 16:17:43.038340 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 16:20:27.811905 140144802662208 spec.py:349] Evaluating on the test split.
I0211 16:20:30.507004 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 16:22:56.603625 140144802662208 submission_runner.py:408] Time since start: 16915.69s, 	Step: 28978, 	{'train/accuracy': 0.6483582854270935, 'train/loss': 1.9046748876571655, 'train/bleu': 31.718846363850467, 'validation/accuracy': 0.6666997075080872, 'validation/loss': 1.7674710750579834, 'validation/bleu': 28.82188555062492, 'validation/num_examples': 3000, 'test/accuracy': 0.6767067909240723, 'test/loss': 1.701871633529663, 'test/bleu': 28.105043511042194, 'test/num_examples': 3003, 'score': 10108.904735088348, 'total_duration': 16915.69260573387, 'accumulated_submission_time': 10108.904735088348, 'accumulated_eval_time': 6805.479952096939, 'accumulated_logging_time': 0.3381767272949219}
I0211 16:22:56.622421 139975029864192 logging_writer.py:48] [28978] accumulated_eval_time=6805.479952, accumulated_logging_time=0.338177, accumulated_submission_time=10108.904735, global_step=28978, preemption_count=0, score=10108.904735, test/accuracy=0.676707, test/bleu=28.105044, test/loss=1.701872, test/num_examples=3003, total_duration=16915.692606, train/accuracy=0.648358, train/bleu=31.718846, train/loss=1.904675, validation/accuracy=0.666700, validation/bleu=28.821886, validation/loss=1.767471, validation/num_examples=3000
I0211 16:23:04.614502 139975038256896 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.2251429557800293, loss=3.99021315574646
I0211 16:23:39.315291 139975029864192 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.1892860233783722, loss=3.941861152648926
I0211 16:24:14.082465 139975038256896 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.25481975078582764, loss=3.9689104557037354
I0211 16:24:48.857633 139975029864192 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.2045379877090454, loss=3.9821486473083496
I0211 16:25:23.637372 139975038256896 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.24034962058067322, loss=3.9674739837646484
I0211 16:25:58.410699 139975029864192 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.19078093767166138, loss=3.874190330505371
I0211 16:26:33.206170 139975038256896 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.19544149935245514, loss=3.9066691398620605
I0211 16:27:07.986513 139975029864192 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.23917579650878906, loss=3.999504566192627
I0211 16:27:42.801980 139975038256896 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.1960432380437851, loss=3.967160940170288
I0211 16:28:17.602877 139975029864192 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.19256797432899475, loss=3.950967788696289
I0211 16:28:52.435524 139975038256896 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.2178044319152832, loss=3.939440965652466
I0211 16:29:27.234862 139975029864192 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.19640576839447021, loss=3.9506852626800537
I0211 16:30:02.064942 139975038256896 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.21743658185005188, loss=3.9534103870391846
I0211 16:30:36.845202 139975029864192 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.19227425754070282, loss=3.9200847148895264
I0211 16:31:11.649432 139975038256896 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.19737665355205536, loss=3.929102897644043
I0211 16:31:46.416749 139975029864192 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.20670819282531738, loss=3.9284310340881348
I0211 16:32:21.263813 139975038256896 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.274175763130188, loss=3.95267391204834
I0211 16:32:56.096253 139975029864192 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.1836477369070053, loss=3.933243751525879
I0211 16:33:30.950190 139975038256896 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.20041093230247498, loss=3.903571844100952
I0211 16:34:05.785120 139975029864192 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.2001885026693344, loss=3.9073758125305176
I0211 16:34:40.555063 139975038256896 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.2168428897857666, loss=3.8862903118133545
I0211 16:35:15.329943 139975029864192 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.20309846103191376, loss=3.9345004558563232
I0211 16:35:50.107262 139975038256896 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.2019972950220108, loss=3.970991849899292
I0211 16:36:24.920180 139975029864192 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.19069471955299377, loss=3.9513649940490723
I0211 16:36:56.639436 140144802662208 spec.py:321] Evaluating on the training split.
I0211 16:36:59.623123 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 16:40:20.401957 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 16:40:23.080861 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 16:43:02.207536 140144802662208 spec.py:349] Evaluating on the test split.
I0211 16:43:04.897900 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 16:45:42.656698 140144802662208 submission_runner.py:408] Time since start: 18281.75s, 	Step: 31393, 	{'train/accuracy': 0.6834477782249451, 'train/loss': 1.6875489950180054, 'train/bleu': 34.22825500162002, 'validation/accuracy': 0.6686215996742249, 'validation/loss': 1.760544776916504, 'validation/bleu': 28.802539963238083, 'validation/num_examples': 3000, 'test/accuracy': 0.6804369688034058, 'test/loss': 1.6927775144577026, 'test/bleu': 28.100285054452986, 'test/num_examples': 3003, 'score': 10948.82987356186, 'total_duration': 18281.74568796158, 'accumulated_submission_time': 10948.82987356186, 'accumulated_eval_time': 7331.4971668720245, 'accumulated_logging_time': 0.3668546676635742}
I0211 16:45:42.675732 139975038256896 logging_writer.py:48] [31393] accumulated_eval_time=7331.497167, accumulated_logging_time=0.366855, accumulated_submission_time=10948.829874, global_step=31393, preemption_count=0, score=10948.829874, test/accuracy=0.680437, test/bleu=28.100285, test/loss=1.692778, test/num_examples=3003, total_duration=18281.745688, train/accuracy=0.683448, train/bleu=34.228255, train/loss=1.687549, validation/accuracy=0.668622, validation/bleu=28.802540, validation/loss=1.760545, validation/num_examples=3000
I0211 16:45:45.464844 139975029864192 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.3321651220321655, loss=3.978121757507324
I0211 16:46:20.151169 139975038256896 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.24586082994937897, loss=3.9578957557678223
I0211 16:46:54.916158 139975029864192 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.18730150163173676, loss=3.9044480323791504
I0211 16:47:29.691982 139975038256896 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.25187623500823975, loss=3.9870665073394775
I0211 16:48:04.484718 139975029864192 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.2638745605945587, loss=3.9818813800811768
I0211 16:48:39.257632 139975038256896 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.2050260305404663, loss=3.9824576377868652
I0211 16:49:14.059751 139975029864192 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.2297123819589615, loss=3.9614527225494385
I0211 16:49:48.857914 139975038256896 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.21558694541454315, loss=3.9563417434692383
I0211 16:50:23.663207 139975029864192 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.2646771967411041, loss=3.933986186981201
I0211 16:50:58.479288 139975038256896 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.21775378286838531, loss=3.8754189014434814
I0211 16:51:33.288567 139975029864192 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.21216541528701782, loss=3.9587197303771973
I0211 16:52:08.082373 139975038256896 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.19700214266777039, loss=3.8259787559509277
I0211 16:52:42.924669 139975029864192 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.200142502784729, loss=3.937885284423828
I0211 16:53:17.739911 139975038256896 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.22780902683734894, loss=3.962376356124878
I0211 16:53:52.526200 139975029864192 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.20079804956912994, loss=4.013086318969727
I0211 16:54:27.320649 139975038256896 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.24117282032966614, loss=3.9126055240631104
I0211 16:55:02.118557 139975029864192 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.19597510993480682, loss=3.8813605308532715
I0211 16:55:36.928973 139975038256896 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.22898682951927185, loss=3.9518027305603027
I0211 16:56:11.739482 139975029864192 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.2366001158952713, loss=3.9392237663269043
I0211 16:56:46.512428 139975038256896 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.2130616158246994, loss=3.855494499206543
I0211 16:57:21.329063 139975029864192 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2109416425228119, loss=3.9602324962615967
I0211 16:57:56.120466 139975038256896 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.20440632104873657, loss=3.9193129539489746
I0211 16:58:30.921726 139975029864192 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.21421867609024048, loss=3.9765326976776123
I0211 16:59:05.716168 139975038256896 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.2865982949733734, loss=3.9629831314086914
I0211 16:59:40.543018 139975029864192 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.229283407330513, loss=3.919825792312622
I0211 16:59:42.715589 140144802662208 spec.py:321] Evaluating on the training split.
I0211 16:59:45.694373 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 17:04:16.292093 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 17:04:18.997786 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 17:07:08.614674 140144802662208 spec.py:349] Evaluating on the test split.
I0211 17:07:11.311468 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 17:09:53.126955 140144802662208 submission_runner.py:408] Time since start: 19732.22s, 	Step: 33808, 	{'train/accuracy': 0.6541592478752136, 'train/loss': 1.8681278228759766, 'train/bleu': 31.89343011566094, 'validation/accuracy': 0.6689191460609436, 'validation/loss': 1.7596172094345093, 'validation/bleu': 29.067372362284118, 'validation/num_examples': 3000, 'test/accuracy': 0.681715190410614, 'test/loss': 1.6911182403564453, 'test/bleu': 28.551759458820776, 'test/num_examples': 3003, 'score': 11788.780126810074, 'total_duration': 19732.2159409523, 'accumulated_submission_time': 11788.780126810074, 'accumulated_eval_time': 7941.9084758758545, 'accumulated_logging_time': 0.39618897438049316}
I0211 17:09:53.146757 139975038256896 logging_writer.py:48] [33808] accumulated_eval_time=7941.908476, accumulated_logging_time=0.396189, accumulated_submission_time=11788.780127, global_step=33808, preemption_count=0, score=11788.780127, test/accuracy=0.681715, test/bleu=28.551759, test/loss=1.691118, test/num_examples=3003, total_duration=19732.215941, train/accuracy=0.654159, train/bleu=31.893430, train/loss=1.868128, validation/accuracy=0.668919, validation/bleu=29.067372, validation/loss=1.759617, validation/num_examples=3000
I0211 17:10:25.403312 139975029864192 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.27747842669487, loss=3.955714464187622
I0211 17:11:00.142764 139975038256896 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.2570842504501343, loss=3.911963939666748
I0211 17:11:34.942742 139975029864192 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.22857171297073364, loss=3.9659149646759033
I0211 17:12:09.759426 139975038256896 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.21324129402637482, loss=3.9313595294952393
I0211 17:12:44.558266 139975029864192 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.21902184188365936, loss=3.940436840057373
I0211 17:13:19.370911 139975038256896 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.22441366314888, loss=3.89268159866333
I0211 17:13:54.154263 139975029864192 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.29133841395378113, loss=3.9209494590759277
I0211 17:14:28.965679 139975038256896 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.24490176141262054, loss=3.92516827583313
I0211 17:15:03.759077 139975029864192 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.24945206940174103, loss=3.9194488525390625
I0211 17:15:38.584707 139975038256896 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.21046365797519684, loss=3.8369035720825195
I0211 17:16:13.427123 139975029864192 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.23151886463165283, loss=3.9146194458007812
I0211 17:16:48.350049 139975038256896 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.21064801514148712, loss=3.885758399963379
I0211 17:17:23.170242 139975029864192 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.20176208019256592, loss=3.9104061126708984
I0211 17:17:57.952513 139975038256896 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.23840482532978058, loss=3.9093616008758545
I0211 17:18:32.727678 139975029864192 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.1860208958387375, loss=3.858769416809082
I0211 17:19:07.521216 139975038256896 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.28733405470848083, loss=3.9421300888061523
I0211 17:19:42.298729 139975029864192 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2378818839788437, loss=3.933155059814453
I0211 17:20:17.050173 139975038256896 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.22504408657550812, loss=3.9250853061676025
I0211 17:20:51.802465 139975029864192 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.2115624099969864, loss=3.8494226932525635
I0211 17:21:26.581379 139975038256896 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.21932654082775116, loss=3.949467658996582
I0211 17:22:01.370511 139975029864192 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.251097708940506, loss=3.9670934677124023
I0211 17:22:36.187850 139975038256896 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.2938571870326996, loss=3.87479305267334
I0211 17:23:10.985952 139975029864192 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.20873858034610748, loss=3.8914244174957275
I0211 17:23:45.752232 139975038256896 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.2586238980293274, loss=3.907036304473877
I0211 17:23:53.466671 140144802662208 spec.py:321] Evaluating on the training split.
I0211 17:23:56.441541 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 17:27:31.603127 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 17:27:34.299535 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 17:30:08.353683 140144802662208 spec.py:349] Evaluating on the test split.
I0211 17:30:11.060281 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 17:32:34.016832 140144802662208 submission_runner.py:408] Time since start: 21093.11s, 	Step: 36224, 	{'train/accuracy': 0.654525101184845, 'train/loss': 1.8604371547698975, 'train/bleu': 32.16741371237828, 'validation/accuracy': 0.673159658908844, 'validation/loss': 1.7235437631607056, 'validation/bleu': 29.286452756607737, 'validation/num_examples': 3000, 'test/accuracy': 0.6827842593193054, 'test/loss': 1.6493988037109375, 'test/bleu': 28.751117281827, 'test/num_examples': 3003, 'score': 12629.00738811493, 'total_duration': 21093.10581278801, 'accumulated_submission_time': 12629.00738811493, 'accumulated_eval_time': 8462.458575487137, 'accumulated_logging_time': 0.4259674549102783}
I0211 17:32:34.038679 139975029864192 logging_writer.py:48] [36224] accumulated_eval_time=8462.458575, accumulated_logging_time=0.425967, accumulated_submission_time=12629.007388, global_step=36224, preemption_count=0, score=12629.007388, test/accuracy=0.682784, test/bleu=28.751117, test/loss=1.649399, test/num_examples=3003, total_duration=21093.105813, train/accuracy=0.654525, train/bleu=32.167414, train/loss=1.860437, validation/accuracy=0.673160, validation/bleu=29.286453, validation/loss=1.723544, validation/num_examples=3000
I0211 17:33:00.689630 139975038256896 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.22406551241874695, loss=3.8923723697662354
I0211 17:33:35.392472 139975029864192 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.23255355656147003, loss=3.887465476989746
I0211 17:34:10.166241 139975038256896 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.24052539467811584, loss=3.8742122650146484
I0211 17:34:44.967062 139975029864192 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.22691312432289124, loss=3.867692470550537
I0211 17:35:19.742226 139975038256896 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.28841379284858704, loss=3.859771728515625
I0211 17:35:54.495210 139975029864192 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.30339089035987854, loss=3.9173812866210938
I0211 17:36:29.274609 139975038256896 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.20687401294708252, loss=3.963977575302124
I0211 17:37:04.064520 139975029864192 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.2055480033159256, loss=3.8795952796936035
I0211 17:37:38.834055 139975038256896 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.2576321065425873, loss=3.9258012771606445
I0211 17:38:13.626393 139975029864192 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.24386471509933472, loss=3.8933799266815186
I0211 17:38:48.381398 139975038256896 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.2293287068605423, loss=3.9491255283355713
I0211 17:39:23.163276 139975029864192 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.2087589055299759, loss=3.889927387237549
I0211 17:39:57.955022 139975038256896 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.23195040225982666, loss=3.904663324356079
I0211 17:40:32.715216 139975029864192 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.26861223578453064, loss=3.9184730052948
I0211 17:41:07.490100 139975038256896 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.21357862651348114, loss=3.864029884338379
I0211 17:41:42.304014 139975029864192 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.23704375326633453, loss=3.8486766815185547
I0211 17:42:17.095396 139975038256896 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2142733335494995, loss=3.9712765216827393
I0211 17:42:51.869977 139975029864192 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.2554350197315216, loss=3.8883163928985596
I0211 17:43:26.645766 139975038256896 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.2054048478603363, loss=3.92153000831604
I0211 17:44:01.449747 139975029864192 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.24009212851524353, loss=3.8434934616088867
I0211 17:44:36.246349 139975038256896 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.26928287744522095, loss=3.8578298091888428
I0211 17:45:11.050280 139975029864192 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.2328435629606247, loss=3.918890953063965
I0211 17:45:45.832265 139975038256896 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.22844365239143372, loss=3.894648790359497
I0211 17:46:20.611366 139975029864192 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.2171895056962967, loss=3.8593740463256836
I0211 17:46:34.270602 140144802662208 spec.py:321] Evaluating on the training split.
I0211 17:46:37.255976 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 17:49:15.923284 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 17:49:18.628672 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 17:51:47.348975 140144802662208 spec.py:349] Evaluating on the test split.
I0211 17:51:50.031493 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 17:54:03.307814 140144802662208 submission_runner.py:408] Time since start: 22382.40s, 	Step: 38641, 	{'train/accuracy': 0.6630606055259705, 'train/loss': 1.7945261001586914, 'train/bleu': 32.4857500217436, 'validation/accuracy': 0.6725273132324219, 'validation/loss': 1.7204879522323608, 'validation/bleu': 29.01776935916361, 'validation/num_examples': 3000, 'test/accuracy': 0.6843181848526001, 'test/loss': 1.6504517793655396, 'test/bleu': 28.69606835632274, 'test/num_examples': 3003, 'score': 13469.150447130203, 'total_duration': 22382.396797180176, 'accumulated_submission_time': 13469.150447130203, 'accumulated_eval_time': 8911.495740890503, 'accumulated_logging_time': 0.4579017162322998}
I0211 17:54:03.327511 139975038256896 logging_writer.py:48] [38641] accumulated_eval_time=8911.495741, accumulated_logging_time=0.457902, accumulated_submission_time=13469.150447, global_step=38641, preemption_count=0, score=13469.150447, test/accuracy=0.684318, test/bleu=28.696068, test/loss=1.650452, test/num_examples=3003, total_duration=22382.396797, train/accuracy=0.663061, train/bleu=32.485750, train/loss=1.794526, validation/accuracy=0.672527, validation/bleu=29.017769, validation/loss=1.720488, validation/num_examples=3000
I0211 17:54:24.114293 139975029864192 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.20747733116149902, loss=3.8740131855010986
I0211 17:54:58.837892 139975038256896 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.22634582221508026, loss=3.929669141769409
I0211 17:55:33.626724 139975029864192 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.23182402551174164, loss=3.8884360790252686
I0211 17:56:08.408582 139975038256896 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.2195783108472824, loss=3.9126806259155273
I0211 17:56:43.189065 139975029864192 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.2398780733346939, loss=3.8685479164123535
I0211 17:57:17.979380 139975038256896 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.29187285900115967, loss=3.9282102584838867
I0211 17:57:52.746972 139975029864192 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.25499895215034485, loss=3.860304832458496
I0211 17:58:27.532501 139975038256896 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.21250873804092407, loss=3.87831449508667
I0211 17:59:02.306077 139975029864192 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.25699615478515625, loss=3.9066872596740723
I0211 17:59:37.081743 139975038256896 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.20723474025726318, loss=3.884042978286743
I0211 18:00:11.869343 139975029864192 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.24580547213554382, loss=3.9381561279296875
I0211 18:00:46.640636 139975038256896 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.27526095509529114, loss=3.9416470527648926
I0211 18:01:21.417239 139975029864192 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.28125476837158203, loss=3.918691635131836
I0211 18:01:56.194496 139975038256896 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.25159701704978943, loss=3.875761032104492
I0211 18:02:31.005553 139975029864192 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.22127051651477814, loss=3.8666515350341797
I0211 18:03:05.804099 139975038256896 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.24309620261192322, loss=3.9261646270751953
I0211 18:03:40.590844 139975029864192 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.2845861613750458, loss=3.8511834144592285
I0211 18:04:15.385577 139975038256896 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.22025693953037262, loss=3.9055745601654053
I0211 18:04:50.194231 139975029864192 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.3188624680042267, loss=3.9289650917053223
I0211 18:05:24.989255 139975038256896 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.22659827768802643, loss=3.878685235977173
I0211 18:05:59.759575 139975029864192 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.21649609506130219, loss=3.8919262886047363
I0211 18:06:34.554896 139975038256896 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.23577290773391724, loss=3.9055051803588867
I0211 18:07:09.347130 139975029864192 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.22024138271808624, loss=3.956611156463623
I0211 18:07:44.140994 139975038256896 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.2159104198217392, loss=3.887803792953491
I0211 18:08:03.337811 140144802662208 spec.py:321] Evaluating on the training split.
I0211 18:08:06.313189 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 18:10:57.163371 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 18:10:59.846846 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 18:13:27.919523 140144802662208 spec.py:349] Evaluating on the test split.
I0211 18:13:30.603334 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 18:16:00.521494 140144802662208 submission_runner.py:408] Time since start: 23699.61s, 	Step: 41057, 	{'train/accuracy': 0.6570492386817932, 'train/loss': 1.8250395059585571, 'train/bleu': 32.32277040187047, 'validation/accuracy': 0.6732960343360901, 'validation/loss': 1.7080892324447632, 'validation/bleu': 29.213552506314006, 'validation/num_examples': 3000, 'test/accuracy': 0.6871187090873718, 'test/loss': 1.634048342704773, 'test/bleu': 29.00677069047909, 'test/num_examples': 3003, 'score': 14309.070579051971, 'total_duration': 23699.610480308533, 'accumulated_submission_time': 14309.070579051971, 'accumulated_eval_time': 9388.679366111755, 'accumulated_logging_time': 0.4881284236907959}
I0211 18:16:00.542963 139975029864192 logging_writer.py:48] [41057] accumulated_eval_time=9388.679366, accumulated_logging_time=0.488128, accumulated_submission_time=14309.070579, global_step=41057, preemption_count=0, score=14309.070579, test/accuracy=0.687119, test/bleu=29.006771, test/loss=1.634048, test/num_examples=3003, total_duration=23699.610480, train/accuracy=0.657049, train/bleu=32.322770, train/loss=1.825040, validation/accuracy=0.673296, validation/bleu=29.213553, validation/loss=1.708089, validation/num_examples=3000
I0211 18:16:15.799196 139975038256896 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.22200573980808258, loss=3.8779208660125732
I0211 18:16:50.516736 139975029864192 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.29406818747520447, loss=3.914670467376709
I0211 18:17:25.293499 139975038256896 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.22967958450317383, loss=3.923753261566162
I0211 18:18:00.091900 139975029864192 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.22844474017620087, loss=3.905107021331787
I0211 18:18:34.880299 139975038256896 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.250512033700943, loss=3.849313497543335
I0211 18:19:09.693492 139975029864192 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.23395325243473053, loss=3.972989082336426
I0211 18:19:44.512273 139975038256896 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2611794173717499, loss=3.880964517593384
I0211 18:20:19.377900 139975029864192 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.25012707710266113, loss=3.9106786251068115
I0211 18:20:54.209112 139975038256896 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.2520378530025482, loss=3.8734259605407715
I0211 18:21:29.118561 139975029864192 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.25612327456474304, loss=3.878896951675415
I0211 18:22:03.937091 139975038256896 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.22763825953006744, loss=3.8562395572662354
I0211 18:22:38.724356 139975029864192 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.22757896780967712, loss=3.8661928176879883
I0211 18:23:13.518159 139975038256896 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.23074907064437866, loss=3.8569858074188232
I0211 18:23:48.316261 139975029864192 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.275699257850647, loss=3.872647523880005
I0211 18:24:23.093056 139975038256896 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.24965554475784302, loss=3.8840017318725586
I0211 18:24:57.906550 139975029864192 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.23408633470535278, loss=3.9075684547424316
I0211 18:25:32.743189 139975038256896 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.23828348517417908, loss=3.9161250591278076
I0211 18:26:07.537389 139975029864192 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.29915595054626465, loss=3.841719150543213
I0211 18:26:42.321585 139975038256896 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.25018510222435, loss=3.8824057579040527
I0211 18:27:17.119406 139975029864192 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.25009143352508545, loss=3.8622820377349854
I0211 18:27:51.932140 139975038256896 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.24462422728538513, loss=3.8916842937469482
I0211 18:28:26.695934 139975029864192 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.24372215569019318, loss=3.8728814125061035
I0211 18:29:01.461401 139975038256896 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.2639930248260498, loss=3.9551005363464355
I0211 18:29:36.272346 139975029864192 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.24546535313129425, loss=3.863159656524658
I0211 18:30:00.688654 140144802662208 spec.py:321] Evaluating on the training split.
I0211 18:30:03.692297 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 18:32:50.240024 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 18:32:52.934092 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 18:35:23.258310 140144802662208 spec.py:349] Evaluating on the test split.
I0211 18:35:25.947992 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 18:37:44.624670 140144802662208 submission_runner.py:408] Time since start: 25003.71s, 	Step: 43472, 	{'train/accuracy': 0.6580873727798462, 'train/loss': 1.8422174453735352, 'train/bleu': 32.67865687974197, 'validation/accuracy': 0.6763338446617126, 'validation/loss': 1.7116023302078247, 'validation/bleu': 29.45904070467425, 'validation/num_examples': 3000, 'test/accuracy': 0.6887455582618713, 'test/loss': 1.635982632637024, 'test/bleu': 28.902835204010888, 'test/num_examples': 3003, 'score': 15149.12542128563, 'total_duration': 25003.713657855988, 'accumulated_submission_time': 15149.12542128563, 'accumulated_eval_time': 9852.615337371826, 'accumulated_logging_time': 0.5199141502380371}
I0211 18:37:44.645029 139975038256896 logging_writer.py:48] [43472] accumulated_eval_time=9852.615337, accumulated_logging_time=0.519914, accumulated_submission_time=15149.125421, global_step=43472, preemption_count=0, score=15149.125421, test/accuracy=0.688746, test/bleu=28.902835, test/loss=1.635983, test/num_examples=3003, total_duration=25003.713658, train/accuracy=0.658087, train/bleu=32.678657, train/loss=1.842217, validation/accuracy=0.676334, validation/bleu=29.459041, validation/loss=1.711602, validation/num_examples=3000
I0211 18:37:54.698745 139975029864192 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.23486147820949554, loss=3.835442543029785
I0211 18:38:29.398201 139975038256896 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.26321399211883545, loss=3.8779749870300293
I0211 18:39:04.139331 139975029864192 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.25131648778915405, loss=3.898311138153076
I0211 18:39:38.951074 139975038256896 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.23362956941127777, loss=3.964630126953125
I0211 18:40:13.763485 139975029864192 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.24370305240154266, loss=3.8833119869232178
I0211 18:40:48.581787 139975038256896 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2538607120513916, loss=3.8900809288024902
I0211 18:41:23.393692 139975029864192 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.253011554479599, loss=3.9183993339538574
I0211 18:41:58.194580 139975038256896 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2435522824525833, loss=3.8888585567474365
I0211 18:42:33.008442 139975029864192 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2388310730457306, loss=3.9091691970825195
I0211 18:43:07.818828 139975038256896 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.24175943434238434, loss=3.8174805641174316
I0211 18:43:42.625403 139975029864192 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.2492772936820984, loss=3.8938028812408447
I0211 18:44:17.401875 139975038256896 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.2324252724647522, loss=3.8085501194000244
I0211 18:44:52.209184 139975029864192 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.2557593882083893, loss=3.8994040489196777
I0211 18:45:27.005793 139975038256896 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.23848198354244232, loss=3.876917839050293
I0211 18:46:01.797425 139975029864192 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.28498032689094543, loss=3.98054575920105
I0211 18:46:36.628163 139975038256896 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.25732606649398804, loss=3.865347385406494
I0211 18:47:11.397303 139975029864192 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.2541615664958954, loss=3.86979341506958
I0211 18:47:46.183105 139975038256896 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.2756873667240143, loss=3.840247869491577
I0211 18:48:20.972427 139975029864192 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.22765536606311798, loss=3.8552956581115723
I0211 18:48:55.770007 139975038256896 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.28179535269737244, loss=3.929344415664673
I0211 18:49:30.560207 139975029864192 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2766262888908386, loss=3.903315544128418
I0211 18:50:05.352108 139975038256896 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.24953654408454895, loss=3.847743511199951
I0211 18:50:40.123446 139975029864192 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.23412640392780304, loss=3.840280532836914
I0211 18:51:14.909053 139975038256896 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.24623462557792664, loss=3.8028318881988525
I0211 18:51:44.882222 140144802662208 spec.py:321] Evaluating on the training split.
I0211 18:51:47.852533 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 18:56:13.368578 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 18:56:16.063705 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 18:59:21.304505 140144802662208 spec.py:349] Evaluating on the test split.
I0211 18:59:23.983118 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 19:02:07.714444 140144802662208 submission_runner.py:408] Time since start: 26466.80s, 	Step: 45888, 	{'train/accuracy': 0.6625374555587769, 'train/loss': 1.791016936302185, 'train/bleu': 32.6860518428758, 'validation/accuracy': 0.6753295063972473, 'validation/loss': 1.6926016807556152, 'validation/bleu': 29.099113091513008, 'validation/num_examples': 3000, 'test/accuracy': 0.6890825629234314, 'test/loss': 1.618253231048584, 'test/bleu': 28.803301939041916, 'test/num_examples': 3003, 'score': 15989.273291826248, 'total_duration': 26466.803416490555, 'accumulated_submission_time': 15989.273291826248, 'accumulated_eval_time': 10475.447494745255, 'accumulated_logging_time': 0.5502684116363525}
I0211 19:02:07.735690 139975029864192 logging_writer.py:48] [45888] accumulated_eval_time=10475.447495, accumulated_logging_time=0.550268, accumulated_submission_time=15989.273292, global_step=45888, preemption_count=0, score=15989.273292, test/accuracy=0.689083, test/bleu=28.803302, test/loss=1.618253, test/num_examples=3003, total_duration=26466.803416, train/accuracy=0.662537, train/bleu=32.686052, train/loss=1.791017, validation/accuracy=0.675330, validation/bleu=29.099113, validation/loss=1.692602, validation/num_examples=3000
I0211 19:02:12.266105 139975038256896 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.23112638294696808, loss=3.8757426738739014
I0211 19:02:46.930570 139975029864192 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.28232264518737793, loss=3.9274580478668213
I0211 19:03:21.665551 139975038256896 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.24593213200569153, loss=3.8498635292053223
I0211 19:03:56.442765 139975029864192 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.23085199296474457, loss=3.8341264724731445
I0211 19:04:31.236969 139975038256896 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.26317527890205383, loss=3.913738965988159
I0211 19:05:06.006265 139975029864192 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.23308256268501282, loss=3.8569839000701904
I0211 19:05:40.804527 139975038256896 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.22354552149772644, loss=3.8808982372283936
I0211 19:06:15.594573 139975029864192 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.23127977550029755, loss=3.874117136001587
I0211 19:06:50.394277 139975038256896 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.2432175725698471, loss=3.8642148971557617
I0211 19:07:25.204769 139975029864192 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.28941917419433594, loss=3.897024393081665
I0211 19:08:00.003868 139975038256896 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.32252880930900574, loss=3.8941268920898438
I0211 19:08:34.801149 139975029864192 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.24150021374225616, loss=3.844775915145874
I0211 19:09:09.620071 139975038256896 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.2391696274280548, loss=3.832669258117676
I0211 19:09:44.414410 139975029864192 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2558704912662506, loss=3.8132152557373047
I0211 19:10:19.196073 139975038256896 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.2867598831653595, loss=3.819288492202759
I0211 19:10:53.985573 139975029864192 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.24837590754032135, loss=3.8759922981262207
I0211 19:11:28.771283 139975038256896 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.23811551928520203, loss=3.876608371734619
I0211 19:12:03.581110 139975029864192 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2661394476890564, loss=3.8180551528930664
I0211 19:12:38.403388 139975038256896 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.2488015592098236, loss=3.912930727005005
I0211 19:13:13.199086 139975029864192 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.26408851146698, loss=3.9014854431152344
I0211 19:13:48.016607 139975038256896 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.24127724766731262, loss=3.9368200302124023
I0211 19:14:22.818964 139975029864192 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.23250581324100494, loss=3.8255178928375244
I0211 19:14:57.627895 139975038256896 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.24884039163589478, loss=3.8544745445251465
I0211 19:15:32.416934 139975029864192 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.2630291283130646, loss=3.829019784927368
I0211 19:16:07.197953 139975038256896 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.2534559965133667, loss=3.91377854347229
I0211 19:16:07.974140 140144802662208 spec.py:321] Evaluating on the training split.
I0211 19:16:10.970957 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 19:19:06.585746 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 19:19:09.287898 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 19:21:50.304227 140144802662208 spec.py:349] Evaluating on the test split.
I0211 19:21:52.996019 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 19:24:16.944941 140144802662208 submission_runner.py:408] Time since start: 27796.03s, 	Step: 48304, 	{'train/accuracy': 0.6612311601638794, 'train/loss': 1.8156846761703491, 'train/bleu': 32.39720039613303, 'validation/accuracy': 0.6767057776451111, 'validation/loss': 1.702085256576538, 'validation/bleu': 29.483924220472556, 'validation/num_examples': 3000, 'test/accuracy': 0.6889082789421082, 'test/loss': 1.6244529485702515, 'test/bleu': 29.317962056875775, 'test/num_examples': 3003, 'score': 16829.422029733658, 'total_duration': 27796.0339281559, 'accumulated_submission_time': 16829.422029733658, 'accumulated_eval_time': 10964.418253660202, 'accumulated_logging_time': 0.5813858509063721}
I0211 19:24:16.965941 139975029864192 logging_writer.py:48] [48304] accumulated_eval_time=10964.418254, accumulated_logging_time=0.581386, accumulated_submission_time=16829.422030, global_step=48304, preemption_count=0, score=16829.422030, test/accuracy=0.688908, test/bleu=29.317962, test/loss=1.624453, test/num_examples=3003, total_duration=27796.033928, train/accuracy=0.661231, train/bleu=32.397200, train/loss=1.815685, validation/accuracy=0.676706, validation/bleu=29.483924, validation/loss=1.702085, validation/num_examples=3000
I0211 19:24:50.588970 139975038256896 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.28414037823677063, loss=3.8493542671203613
I0211 19:25:25.316166 139975029864192 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.2477150410413742, loss=3.935622215270996
I0211 19:26:00.099986 139975038256896 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.2785148024559021, loss=3.8303167819976807
I0211 19:26:34.905370 139975029864192 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.29411712288856506, loss=3.865893840789795
I0211 19:27:09.699836 139975038256896 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.24759981036186218, loss=3.8311054706573486
I0211 19:27:44.487449 139975029864192 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2647024393081665, loss=3.8876852989196777
I0211 19:28:19.287356 139975038256896 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.285635769367218, loss=3.8879740238189697
I0211 19:28:54.065821 139975029864192 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.2571132779121399, loss=3.849078416824341
I0211 19:29:28.841726 139975038256896 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.2493714988231659, loss=3.8944344520568848
I0211 19:30:03.639473 139975029864192 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.24706685543060303, loss=3.8731002807617188
I0211 19:30:38.406687 139975038256896 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.24079777300357819, loss=3.8288278579711914
I0211 19:31:13.182305 139975029864192 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.24444285035133362, loss=3.8799357414245605
I0211 19:31:47.940287 139975038256896 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.2998914420604706, loss=3.8764116764068604
I0211 19:32:22.742007 139975029864192 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.24229687452316284, loss=3.838935136795044
I0211 19:32:57.522189 139975038256896 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.24325674772262573, loss=3.8132009506225586
I0211 19:33:32.329772 139975029864192 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.2621128559112549, loss=3.845879554748535
I0211 19:34:07.147170 139975038256896 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.23071756958961487, loss=3.8751211166381836
I0211 19:34:42.061154 139975029864192 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.24453096091747284, loss=3.8316242694854736
I0211 19:35:16.858709 139975038256896 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.2415122240781784, loss=3.841677665710449
I0211 19:35:51.655554 139975029864192 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.25925102829933167, loss=3.862551212310791
I0211 19:36:26.445420 139975038256896 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.28128781914711, loss=3.8916406631469727
I0211 19:37:01.217901 139975029864192 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.25577476620674133, loss=3.8971245288848877
I0211 19:37:36.012927 139975038256896 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.24545685946941376, loss=3.8873579502105713
I0211 19:38:10.805040 139975029864192 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.267994225025177, loss=3.8969857692718506
I0211 19:38:17.141869 140144802662208 spec.py:321] Evaluating on the training split.
I0211 19:38:20.117203 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 19:41:15.249480 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 19:41:17.945533 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 19:43:50.059783 140144802662208 spec.py:349] Evaluating on the test split.
I0211 19:43:52.749865 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 19:46:10.130438 140144802662208 submission_runner.py:408] Time since start: 29109.22s, 	Step: 50720, 	{'train/accuracy': 0.6744092702865601, 'train/loss': 1.718177080154419, 'train/bleu': 33.865250969584935, 'validation/accuracy': 0.6792972087860107, 'validation/loss': 1.6851235628128052, 'validation/bleu': 29.988029477140355, 'validation/num_examples': 3000, 'test/accuracy': 0.6897914409637451, 'test/loss': 1.6089180707931519, 'test/bleu': 29.17607198808368, 'test/num_examples': 3003, 'score': 17669.506113767624, 'total_duration': 29109.219428539276, 'accumulated_submission_time': 17669.506113767624, 'accumulated_eval_time': 11437.406776428223, 'accumulated_logging_time': 0.6123223304748535}
I0211 19:46:10.153049 139975038256896 logging_writer.py:48] [50720] accumulated_eval_time=11437.406776, accumulated_logging_time=0.612322, accumulated_submission_time=17669.506114, global_step=50720, preemption_count=0, score=17669.506114, test/accuracy=0.689791, test/bleu=29.176072, test/loss=1.608918, test/num_examples=3003, total_duration=29109.219429, train/accuracy=0.674409, train/bleu=33.865251, train/loss=1.718177, validation/accuracy=0.679297, validation/bleu=29.988029, validation/loss=1.685124, validation/num_examples=3000
I0211 19:46:38.220371 139975029864192 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.24883073568344116, loss=3.8483219146728516
I0211 19:47:12.929403 139975038256896 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.27350130677223206, loss=3.8857994079589844
I0211 19:47:47.736539 139975029864192 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.2306910902261734, loss=3.8274803161621094
I0211 19:48:22.581764 139975038256896 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3048429787158966, loss=3.8667778968811035
I0211 19:48:57.360733 139975029864192 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.25490280985832214, loss=3.8524720668792725
I0211 19:49:32.150604 139975038256896 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.2580086588859558, loss=3.911923885345459
I0211 19:50:06.914934 139975029864192 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.25362181663513184, loss=3.8479130268096924
I0211 19:50:41.715822 139975038256896 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2585656940937042, loss=3.856837272644043
I0211 19:51:16.541448 139975029864192 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.26386910676956177, loss=3.832113265991211
I0211 19:51:51.309841 139975038256896 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.25142183899879456, loss=3.9125752449035645
I0211 19:52:26.100632 139975029864192 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.24457788467407227, loss=3.8784968852996826
I0211 19:53:00.878292 139975038256896 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.23447348177433014, loss=3.804636001586914
I0211 19:53:35.685062 139975029864192 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.258047878742218, loss=3.9036142826080322
I0211 19:54:10.498565 139975038256896 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.24702560901641846, loss=3.8678550720214844
I0211 19:54:45.353606 139975029864192 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.27951404452323914, loss=3.8406739234924316
I0211 19:55:20.171302 139975038256896 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.26942819356918335, loss=3.840904712677002
I0211 19:55:54.977677 139975029864192 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.35332903265953064, loss=3.893213987350464
I0211 19:56:29.765787 139975038256896 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.274311900138855, loss=3.8051042556762695
I0211 19:57:04.573947 139975029864192 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.26354238390922546, loss=3.829972743988037
I0211 19:57:39.357067 139975038256896 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.25702160596847534, loss=3.7955548763275146
I0211 19:58:14.143570 139975029864192 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.28179216384887695, loss=3.8529155254364014
I0211 19:58:48.918532 139975038256896 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.2571001350879669, loss=3.892754554748535
I0211 19:59:23.735081 139975029864192 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.27252140641212463, loss=3.804138660430908
I0211 19:59:58.515480 139975038256896 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.2535717487335205, loss=3.8739407062530518
I0211 20:00:10.425628 140144802662208 spec.py:321] Evaluating on the training split.
I0211 20:00:13.421349 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 20:03:53.720049 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 20:03:56.410260 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 20:06:40.166151 140144802662208 spec.py:349] Evaluating on the test split.
I0211 20:06:42.856003 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 20:09:20.408990 140144802662208 submission_runner.py:408] Time since start: 30499.50s, 	Step: 53136, 	{'train/accuracy': 0.6658750772476196, 'train/loss': 1.772932529449463, 'train/bleu': 32.865700967124376, 'validation/accuracy': 0.677821695804596, 'validation/loss': 1.6751149892807007, 'validation/bleu': 29.386465831879853, 'validation/num_examples': 3000, 'test/accuracy': 0.6934635043144226, 'test/loss': 1.5972486734390259, 'test/bleu': 29.42710453871561, 'test/num_examples': 3003, 'score': 18509.686230421066, 'total_duration': 30499.497972011566, 'accumulated_submission_time': 18509.686230421066, 'accumulated_eval_time': 11987.390083789825, 'accumulated_logging_time': 0.6450626850128174}
I0211 20:09:20.432185 139975029864192 logging_writer.py:48] [53136] accumulated_eval_time=11987.390084, accumulated_logging_time=0.645063, accumulated_submission_time=18509.686230, global_step=53136, preemption_count=0, score=18509.686230, test/accuracy=0.693464, test/bleu=29.427105, test/loss=1.597249, test/num_examples=3003, total_duration=30499.497972, train/accuracy=0.665875, train/bleu=32.865701, train/loss=1.772933, validation/accuracy=0.677822, validation/bleu=29.386466, validation/loss=1.675115, validation/num_examples=3000
I0211 20:09:42.987423 139975038256896 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.3366338908672333, loss=3.8737282752990723
I0211 20:10:17.683798 139975029864192 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.27263808250427246, loss=3.8236207962036133
I0211 20:10:52.467048 139975038256896 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.24794165790081024, loss=3.848375082015991
I0211 20:11:27.269585 139975029864192 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3387134373188019, loss=3.793726682662964
I0211 20:12:02.063495 139975038256896 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.2507985234260559, loss=3.8555469512939453
I0211 20:12:36.860416 139975029864192 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3537536859512329, loss=3.7672548294067383
I0211 20:13:11.644963 139975038256896 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.2779371738433838, loss=3.9041993618011475
I0211 20:13:46.426523 139975029864192 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.276051789522171, loss=3.886573076248169
I0211 20:14:21.244150 139975038256896 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.2864503860473633, loss=3.852890729904175
I0211 20:14:56.056321 139975029864192 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2733452022075653, loss=3.876875638961792
I0211 20:15:30.842312 139975038256896 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.26536887884140015, loss=3.8667290210723877
I0211 20:16:05.672952 139975029864192 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.24627932906150818, loss=3.8544421195983887
I0211 20:16:40.483371 139975038256896 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.2770085036754608, loss=3.8441827297210693
I0211 20:17:15.272209 139975029864192 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.2919151484966278, loss=3.9419538974761963
I0211 20:17:50.073297 139975038256896 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.279006689786911, loss=3.914726734161377
I0211 20:18:24.842922 139975029864192 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.25256454944610596, loss=3.8728928565979004
I0211 20:18:59.639997 139975038256896 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2500241696834564, loss=3.8461036682128906
I0211 20:19:34.428236 139975029864192 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.28353792428970337, loss=3.825521469116211
I0211 20:20:09.226820 139975038256896 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2792356312274933, loss=3.832392692565918
I0211 20:20:44.032510 139975029864192 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2715744972229004, loss=3.8130452632904053
I0211 20:21:18.829728 139975038256896 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.24170121550559998, loss=3.8369035720825195
I0211 20:21:53.632148 139975029864192 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.26394855976104736, loss=3.842984676361084
I0211 20:22:28.408446 139975038256896 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.278270423412323, loss=3.806633472442627
I0211 20:23:03.180207 139975029864192 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2586306631565094, loss=3.941041946411133
I0211 20:23:20.641089 140144802662208 spec.py:321] Evaluating on the training split.
I0211 20:23:23.619828 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 20:26:02.101154 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 20:26:04.783583 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 20:28:43.114483 140144802662208 spec.py:349] Evaluating on the test split.
I0211 20:28:45.798849 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 20:31:13.530215 140144802662208 submission_runner.py:408] Time since start: 31812.62s, 	Step: 55552, 	{'train/accuracy': 0.6659360527992249, 'train/loss': 1.7834343910217285, 'train/bleu': 32.94168061929005, 'validation/accuracy': 0.6789996027946472, 'validation/loss': 1.6796774864196777, 'validation/bleu': 29.670184999573905, 'validation/num_examples': 3000, 'test/accuracy': 0.6937540173530579, 'test/loss': 1.5995779037475586, 'test/bleu': 29.225783598144606, 'test/num_examples': 3003, 'score': 19349.803040981293, 'total_duration': 31812.61920762062, 'accumulated_submission_time': 19349.803040981293, 'accumulated_eval_time': 12460.27916264534, 'accumulated_logging_time': 0.6809508800506592}
I0211 20:31:13.552737 139975038256896 logging_writer.py:48] [55552] accumulated_eval_time=12460.279163, accumulated_logging_time=0.680951, accumulated_submission_time=19349.803041, global_step=55552, preemption_count=0, score=19349.803041, test/accuracy=0.693754, test/bleu=29.225784, test/loss=1.599578, test/num_examples=3003, total_duration=31812.619208, train/accuracy=0.665936, train/bleu=32.941681, train/loss=1.783434, validation/accuracy=0.679000, validation/bleu=29.670185, validation/loss=1.679677, validation/num_examples=3000
I0211 20:31:30.561665 139975029864192 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.33790919184684753, loss=3.848485231399536
I0211 20:32:05.246490 139975038256896 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.2653084099292755, loss=3.868835687637329
I0211 20:32:39.980714 139975029864192 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2853703200817108, loss=3.8727846145629883
I0211 20:33:14.741966 139975038256896 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.25212761759757996, loss=3.9520044326782227
I0211 20:33:49.533169 139975029864192 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2869112193584442, loss=3.857210636138916
I0211 20:34:24.304042 139975038256896 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.2633526921272278, loss=3.823636054992676
I0211 20:34:59.110118 139975029864192 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.24544790387153625, loss=3.85760498046875
I0211 20:35:33.866857 139975038256896 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.26551321148872375, loss=3.910992383956909
I0211 20:36:08.629477 139975029864192 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.24647003412246704, loss=3.817326307296753
I0211 20:36:43.443573 139975038256896 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.25770455598831177, loss=3.8177688121795654
I0211 20:37:18.234273 139975029864192 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.3170487880706787, loss=3.8456029891967773
I0211 20:37:52.998283 139975038256896 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.25722458958625793, loss=3.8340067863464355
I0211 20:38:27.789924 139975029864192 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.29117831587791443, loss=3.854987382888794
I0211 20:39:02.567091 139975038256896 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2669035792350769, loss=3.8257505893707275
I0211 20:39:37.372756 139975029864192 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.27709928154945374, loss=3.8843765258789062
I0211 20:40:12.165200 139975038256896 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.27379679679870605, loss=3.822247266769409
I0211 20:40:46.943964 139975029864192 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2609997093677521, loss=3.8932063579559326
I0211 20:41:21.716320 139975038256896 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.2812417149543762, loss=3.82372784614563
I0211 20:41:56.516615 139975029864192 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.2838946282863617, loss=3.8606719970703125
I0211 20:42:31.342106 139975038256896 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.2531318664550781, loss=3.843294382095337
I0211 20:43:06.132078 139975029864192 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.26456278562545776, loss=3.8780503273010254
I0211 20:43:40.922784 139975038256896 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.26723504066467285, loss=3.8282251358032227
I0211 20:44:15.692697 139975029864192 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2637070417404175, loss=3.8383965492248535
I0211 20:44:50.487066 139975038256896 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2830711305141449, loss=3.7988369464874268
I0211 20:45:13.530289 140144802662208 spec.py:321] Evaluating on the training split.
I0211 20:45:16.517310 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 20:48:04.137863 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 20:48:06.830166 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 20:50:33.281875 140144802662208 spec.py:349] Evaluating on the test split.
I0211 20:50:35.982179 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 20:52:54.457989 140144802662208 submission_runner.py:408] Time since start: 33113.55s, 	Step: 57968, 	{'train/accuracy': 0.6726042032241821, 'train/loss': 1.7362380027770996, 'train/bleu': 33.86621185465534, 'validation/accuracy': 0.6793344020843506, 'validation/loss': 1.6724979877471924, 'validation/bleu': 29.513959160563886, 'validation/num_examples': 3000, 'test/accuracy': 0.6937307715415955, 'test/loss': 1.5895719528198242, 'test/bleu': 29.51395797552482, 'test/num_examples': 3003, 'score': 20189.688641786575, 'total_duration': 33113.54695224762, 'accumulated_submission_time': 20189.688641786575, 'accumulated_eval_time': 12921.206790924072, 'accumulated_logging_time': 0.71565842628479}
I0211 20:52:54.486060 139975029864192 logging_writer.py:48] [57968] accumulated_eval_time=12921.206791, accumulated_logging_time=0.715658, accumulated_submission_time=20189.688642, global_step=57968, preemption_count=0, score=20189.688642, test/accuracy=0.693731, test/bleu=29.513958, test/loss=1.589572, test/num_examples=3003, total_duration=33113.546952, train/accuracy=0.672604, train/bleu=33.866212, train/loss=1.736238, validation/accuracy=0.679334, validation/bleu=29.513959, validation/loss=1.672498, validation/num_examples=3000
I0211 20:53:05.944641 139975038256896 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.27729061245918274, loss=3.8290598392486572
I0211 20:53:40.656476 139975029864192 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.26147380471229553, loss=3.7988948822021484
I0211 20:54:15.433690 139975038256896 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.3086753785610199, loss=3.8461601734161377
I0211 20:54:50.225929 139975029864192 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.25687962770462036, loss=3.795722484588623
I0211 20:55:25.012862 139975038256896 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.29864323139190674, loss=3.8405916690826416
I0211 20:55:59.869392 139975029864192 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.25974681973457336, loss=3.8585364818573
I0211 20:56:34.729381 139975038256896 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2513887286186218, loss=3.8138210773468018
I0211 20:57:09.532219 139975029864192 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2544611394405365, loss=3.8511626720428467
I0211 20:57:44.390240 139975038256896 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3234122097492218, loss=3.774717330932617
I0211 20:58:19.221072 139975029864192 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.24892500042915344, loss=3.815181255340576
I0211 20:58:54.016559 139975038256896 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.2684750258922577, loss=3.8118739128112793
I0211 20:59:28.791165 139975029864192 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.2545417845249176, loss=3.840930461883545
I0211 21:00:03.580905 139975038256896 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.2873331606388092, loss=3.888592004776001
I0211 21:00:38.362879 139975029864192 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.30615153908729553, loss=3.8863162994384766
I0211 21:01:13.139080 139975038256896 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.2693980038166046, loss=3.7879796028137207
I0211 21:01:47.956658 139975029864192 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.28010135889053345, loss=3.8300297260284424
I0211 21:02:22.746418 139975038256896 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.24955490231513977, loss=3.8118443489074707
I0211 21:02:57.533340 139975029864192 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.27471479773521423, loss=3.8616414070129395
I0211 21:03:32.331430 139975038256896 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.2706976532936096, loss=3.840266227722168
I0211 21:04:07.188739 139975029864192 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.3134155869483948, loss=3.822615385055542
I0211 21:04:41.984414 139975038256896 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.28525441884994507, loss=3.8731110095977783
I0211 21:05:16.768542 139975029864192 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.30052492022514343, loss=3.829106569290161
I0211 21:05:51.551188 139975038256896 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.27816301584243774, loss=3.8291666507720947
I0211 21:06:26.349428 139975029864192 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.27529171109199524, loss=3.87239933013916
I0211 21:06:54.582946 140144802662208 spec.py:321] Evaluating on the training split.
I0211 21:06:57.560508 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 21:09:44.084856 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 21:09:46.764079 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 21:12:14.320291 140144802662208 spec.py:349] Evaluating on the test split.
I0211 21:12:17.012068 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 21:14:37.372380 140144802662208 submission_runner.py:408] Time since start: 34416.46s, 	Step: 60383, 	{'train/accuracy': 0.6659606695175171, 'train/loss': 1.7663089036941528, 'train/bleu': 33.31700121865782, 'validation/accuracy': 0.6792724132537842, 'validation/loss': 1.6618462800979614, 'validation/bleu': 30.042622596801575, 'validation/num_examples': 3000, 'test/accuracy': 0.6942071914672852, 'test/loss': 1.5787086486816406, 'test/bleu': 29.482354140661048, 'test/num_examples': 3003, 'score': 21029.69061565399, 'total_duration': 34416.46136879921, 'accumulated_submission_time': 21029.69061565399, 'accumulated_eval_time': 13383.99617767334, 'accumulated_logging_time': 0.7550392150878906}
I0211 21:14:37.395566 139975038256896 logging_writer.py:48] [60383] accumulated_eval_time=13383.996178, accumulated_logging_time=0.755039, accumulated_submission_time=21029.690616, global_step=60383, preemption_count=0, score=21029.690616, test/accuracy=0.694207, test/bleu=29.482354, test/loss=1.578709, test/num_examples=3003, total_duration=34416.461369, train/accuracy=0.665961, train/bleu=33.317001, train/loss=1.766309, validation/accuracy=0.679272, validation/bleu=30.042623, validation/loss=1.661846, validation/num_examples=3000
I0211 21:14:43.663442 139975029864192 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.35117417573928833, loss=3.8722050189971924
I0211 21:15:18.317623 139975038256896 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.3009366989135742, loss=3.8331668376922607
I0211 21:15:53.050806 139975029864192 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.26975810527801514, loss=3.7919394969940186
I0211 21:16:27.817557 139975038256896 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.2716386318206787, loss=3.9027178287506104
I0211 21:17:02.596459 139975029864192 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.25158509612083435, loss=3.897202968597412
I0211 21:17:37.383214 139975038256896 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.2920988202095032, loss=3.8407652378082275
I0211 21:18:12.175083 139975029864192 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.28167635202407837, loss=3.8966445922851562
I0211 21:18:46.945035 139975038256896 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.2798839211463928, loss=3.8244032859802246
I0211 21:19:21.731695 139975029864192 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.30022314190864563, loss=3.8241078853607178
I0211 21:19:56.508894 139975038256896 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2821061611175537, loss=3.785675287246704
I0211 21:20:31.294826 139975029864192 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.2757543623447418, loss=3.834629535675049
I0211 21:21:06.093729 139975038256896 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.27554839849472046, loss=3.853682041168213
I0211 21:21:40.902065 139975029864192 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.2544932961463928, loss=3.863912343978882
I0211 21:22:15.694958 139975038256896 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.31457412242889404, loss=3.81819224357605
I0211 21:22:50.480041 139975029864192 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.270684152841568, loss=3.8862416744232178
I0211 21:23:25.298211 139975038256896 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.2607613205909729, loss=3.7656004428863525
I0211 21:24:00.098761 139975029864192 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.2897859811782837, loss=3.852609872817993
I0211 21:24:34.895618 139975038256896 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.272807776927948, loss=3.86765456199646
I0211 21:25:09.681592 139975029864192 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.2524667978286743, loss=3.7878596782684326
I0211 21:25:44.485846 139975038256896 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.31155237555503845, loss=3.803429126739502
I0211 21:26:19.272618 139975029864192 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.28338417410850525, loss=3.7522127628326416
I0211 21:26:54.090258 139975038256896 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.26349857449531555, loss=3.8281142711639404
I0211 21:27:28.881043 139975029864192 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2898218035697937, loss=3.7867138385772705
I0211 21:28:03.680917 139975038256896 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.2975870370864868, loss=3.8350470066070557
I0211 21:28:37.506360 140144802662208 spec.py:321] Evaluating on the training split.
I0211 21:28:40.489258 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 21:32:16.848575 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 21:32:19.565423 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 21:35:26.341428 140144802662208 spec.py:349] Evaluating on the test split.
I0211 21:35:29.035498 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 21:38:08.301265 140144802662208 submission_runner.py:408] Time since start: 35827.39s, 	Step: 62799, 	{'train/accuracy': 0.6952691674232483, 'train/loss': 1.5940762758255005, 'train/bleu': 34.85920428088039, 'validation/accuracy': 0.6813182830810547, 'validation/loss': 1.6626694202423096, 'validation/bleu': 30.02150473419823, 'validation/num_examples': 3000, 'test/accuracy': 0.6970425844192505, 'test/loss': 1.576493263244629, 'test/bleu': 29.63074699460067, 'test/num_examples': 3003, 'score': 21869.710822820663, 'total_duration': 35827.3902528286, 'accumulated_submission_time': 21869.710822820663, 'accumulated_eval_time': 13954.791036367416, 'accumulated_logging_time': 0.7893767356872559}
I0211 21:38:08.324623 139975029864192 logging_writer.py:48] [62799] accumulated_eval_time=13954.791036, accumulated_logging_time=0.789377, accumulated_submission_time=21869.710823, global_step=62799, preemption_count=0, score=21869.710823, test/accuracy=0.697043, test/bleu=29.630747, test/loss=1.576493, test/num_examples=3003, total_duration=35827.390253, train/accuracy=0.695269, train/bleu=34.859204, train/loss=1.594076, validation/accuracy=0.681318, validation/bleu=30.021505, validation/loss=1.662669, validation/num_examples=3000
I0211 21:38:09.034084 139975038256896 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.28238049149513245, loss=3.8247745037078857
I0211 21:38:43.695765 139975029864192 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.32137593626976013, loss=3.845921516418457
I0211 21:39:18.415214 139975038256896 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.2925078868865967, loss=3.8300955295562744
I0211 21:39:53.195297 139975029864192 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.26965993642807007, loss=3.8218038082122803
I0211 21:40:27.990432 139975038256896 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2751998007297516, loss=3.8355531692504883
I0211 21:41:02.747999 139975029864192 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.28503865003585815, loss=3.7754056453704834
I0211 21:41:37.543984 139975038256896 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.31910938024520874, loss=3.8600642681121826
I0211 21:42:12.331094 139975029864192 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.2613089084625244, loss=3.8054635524749756
I0211 21:42:47.132367 139975038256896 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.3978828191757202, loss=3.7951273918151855
I0211 21:43:21.924781 139975029864192 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.2746194005012512, loss=3.8290677070617676
I0211 21:43:56.750625 139975038256896 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.27974313497543335, loss=3.834834337234497
I0211 21:44:31.577071 139975029864192 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2826227843761444, loss=3.836176872253418
I0211 21:45:06.372051 139975038256896 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2521299719810486, loss=3.7860195636749268
I0211 21:45:41.157210 139975029864192 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.33906084299087524, loss=3.825894355773926
I0211 21:46:15.952464 139975038256896 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.26341375708580017, loss=3.842219591140747
I0211 21:46:50.753539 139975029864192 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.2795846462249756, loss=3.8124091625213623
I0211 21:47:25.535507 139975038256896 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.2742471992969513, loss=3.839573860168457
I0211 21:48:00.336656 139975029864192 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2706514298915863, loss=3.863330364227295
I0211 21:48:35.156452 139975038256896 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.30662137269973755, loss=3.824603319168091
I0211 21:49:09.986581 139975029864192 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.2954467833042145, loss=3.85880708694458
I0211 21:49:44.822729 139975038256896 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.2880770266056061, loss=3.827598810195923
I0211 21:50:19.665531 139975029864192 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.27014246582984924, loss=3.7924506664276123
I0211 21:50:54.464486 139975038256896 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.26341649889945984, loss=3.799501419067383
I0211 21:51:29.235247 139975029864192 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.26180049777030945, loss=3.8259167671203613
I0211 21:52:04.044830 139975038256896 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.27805596590042114, loss=3.7808170318603516
I0211 21:52:08.640802 140144802662208 spec.py:321] Evaluating on the training split.
I0211 21:52:11.612572 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 21:55:54.336847 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 21:55:57.022493 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 21:58:35.245659 140144802662208 spec.py:349] Evaluating on the test split.
I0211 21:58:37.925891 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 22:01:08.883831 140144802662208 submission_runner.py:408] Time since start: 37207.97s, 	Step: 65215, 	{'train/accuracy': 0.6770080327987671, 'train/loss': 1.7094494104385376, 'train/bleu': 33.7335961330243, 'validation/accuracy': 0.6842692494392395, 'validation/loss': 1.6536860466003418, 'validation/bleu': 30.38194529115567, 'validation/num_examples': 3000, 'test/accuracy': 0.6985416412353516, 'test/loss': 1.5746350288391113, 'test/bleu': 29.660603258547127, 'test/num_examples': 3003, 'score': 22709.933475017548, 'total_duration': 37207.97281885147, 'accumulated_submission_time': 22709.933475017548, 'accumulated_eval_time': 14495.034008741379, 'accumulated_logging_time': 0.8237001895904541}
I0211 22:01:08.906867 139975029864192 logging_writer.py:48] [65215] accumulated_eval_time=14495.034009, accumulated_logging_time=0.823700, accumulated_submission_time=22709.933475, global_step=65215, preemption_count=0, score=22709.933475, test/accuracy=0.698542, test/bleu=29.660603, test/loss=1.574635, test/num_examples=3003, total_duration=37207.972819, train/accuracy=0.677008, train/bleu=33.733596, train/loss=1.709449, validation/accuracy=0.684269, validation/bleu=30.381945, validation/loss=1.653686, validation/num_examples=3000
I0211 22:01:38.710726 139975038256896 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.27422142028808594, loss=3.814521312713623
I0211 22:02:13.403057 139975029864192 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.2702120244503021, loss=3.8507080078125
I0211 22:02:48.212069 139975038256896 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.2509920001029968, loss=3.793118953704834
I0211 22:03:22.986503 139975029864192 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.2780602276325226, loss=3.8460986614227295
I0211 22:03:57.767496 139975038256896 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.2744081914424896, loss=3.8605544567108154
I0211 22:04:32.574250 139975029864192 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.2600763142108917, loss=3.7959489822387695
I0211 22:05:07.354898 139975038256896 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2768978774547577, loss=3.8029391765594482
I0211 22:05:42.173759 139975029864192 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.28391650319099426, loss=3.775533437728882
I0211 22:06:16.969069 139975038256896 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2886661887168884, loss=3.8112683296203613
I0211 22:06:51.752786 139975029864192 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.2748240530490875, loss=3.822354555130005
I0211 22:07:26.588159 139975038256896 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2717037796974182, loss=3.845325231552124
I0211 22:08:01.403322 139975029864192 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.24810078740119934, loss=3.764822006225586
I0211 22:08:36.231491 139975038256896 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.3158886432647705, loss=3.803267002105713
I0211 22:09:11.004653 139975029864192 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.28755903244018555, loss=3.8229806423187256
I0211 22:09:45.851360 139975038256896 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.2845505177974701, loss=3.8584160804748535
I0211 22:10:20.623976 139975029864192 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.29642972350120544, loss=3.787628650665283
I0211 22:10:55.419014 139975038256896 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2971420884132385, loss=3.834925651550293
I0211 22:11:30.211920 139975029864192 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2511339485645294, loss=3.7895898818969727
I0211 22:12:05.033131 139975038256896 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.29124945402145386, loss=3.8389499187469482
I0211 22:12:39.939258 139975029864192 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.2862934470176697, loss=3.7893903255462646
I0211 22:13:14.732289 139975038256896 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.28419390320777893, loss=3.8225083351135254
I0211 22:13:49.526834 139975029864192 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3256374001502991, loss=3.835904359817505
I0211 22:14:24.336326 139975038256896 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.2707603871822357, loss=3.8619801998138428
I0211 22:14:59.114436 139975029864192 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.37747466564178467, loss=3.7599194049835205
I0211 22:15:08.921943 140144802662208 spec.py:321] Evaluating on the training split.
I0211 22:15:11.908356 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 22:18:08.284849 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 22:18:10.981869 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 22:20:51.152577 140144802662208 spec.py:349] Evaluating on the test split.
I0211 22:20:53.843229 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 22:23:22.840258 140144802662208 submission_runner.py:408] Time since start: 38541.93s, 	Step: 67630, 	{'train/accuracy': 0.6708812117576599, 'train/loss': 1.747066617012024, 'train/bleu': 33.23434424702395, 'validation/accuracy': 0.6815910339355469, 'validation/loss': 1.65132737159729, 'validation/bleu': 29.844001559891947, 'validation/num_examples': 3000, 'test/accuracy': 0.6962407827377319, 'test/loss': 1.5660450458526611, 'test/bleu': 29.662369265097585, 'test/num_examples': 3003, 'score': 23549.856046438217, 'total_duration': 38541.929241895676, 'accumulated_submission_time': 23549.856046438217, 'accumulated_eval_time': 14988.952271461487, 'accumulated_logging_time': 0.8565609455108643}
I0211 22:23:22.866559 139975038256896 logging_writer.py:48] [67630] accumulated_eval_time=14988.952271, accumulated_logging_time=0.856561, accumulated_submission_time=23549.856046, global_step=67630, preemption_count=0, score=23549.856046, test/accuracy=0.696241, test/bleu=29.662369, test/loss=1.566045, test/num_examples=3003, total_duration=38541.929242, train/accuracy=0.670881, train/bleu=33.234344, train/loss=1.747067, validation/accuracy=0.681591, validation/bleu=29.844002, validation/loss=1.651327, validation/num_examples=3000
I0211 22:23:47.482318 139975029864192 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.292522668838501, loss=3.7838351726531982
I0211 22:24:22.186892 139975038256896 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.2894834578037262, loss=3.8284287452697754
I0211 22:24:56.999055 139975029864192 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2681778073310852, loss=3.843951940536499
I0211 22:25:31.786283 139975038256896 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.27747344970703125, loss=3.7931556701660156
I0211 22:26:06.588853 139975029864192 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.312895268201828, loss=3.7888989448547363
I0211 22:26:41.421153 139975038256896 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.2915736436843872, loss=3.8663136959075928
I0211 22:27:16.315472 139975029864192 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.26399850845336914, loss=3.7688820362091064
I0211 22:27:51.074579 139975038256896 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3003646731376648, loss=3.772895574569702
I0211 22:28:25.872089 139975029864192 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.2799570858478546, loss=3.8228957653045654
I0211 22:29:00.655835 139975038256896 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.3104006350040436, loss=3.776381015777588
I0211 22:29:35.448866 139975029864192 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.29567310214042664, loss=3.844658374786377
I0211 22:30:10.244827 139975038256896 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.2754606008529663, loss=3.7439041137695312
I0211 22:30:45.084500 139975029864192 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.3433797061443329, loss=3.8145487308502197
I0211 22:31:19.906171 139975038256896 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.28067436814308167, loss=3.8100359439849854
I0211 22:31:54.700734 139975029864192 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.28240200877189636, loss=3.8489177227020264
I0211 22:32:29.509477 139975038256896 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.3165888488292694, loss=3.848292589187622
I0211 22:33:04.322146 139975029864192 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.32100367546081543, loss=3.8390085697174072
I0211 22:33:39.138580 139975038256896 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.288646399974823, loss=3.7679715156555176
I0211 22:34:13.981598 139975029864192 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.3045051097869873, loss=3.7819249629974365
I0211 22:34:48.834891 139975038256896 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3283180892467499, loss=3.7723660469055176
I0211 22:35:23.684676 139975029864192 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2901604175567627, loss=3.836308240890503
I0211 22:35:58.502044 139975038256896 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.34381669759750366, loss=3.7710909843444824
I0211 22:36:33.305687 139975029864192 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.26912277936935425, loss=3.8265130519866943
I0211 22:37:08.112373 139975038256896 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.2716553807258606, loss=3.806488275527954
I0211 22:37:23.160617 140144802662208 spec.py:321] Evaluating on the training split.
I0211 22:37:26.136825 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 22:40:18.403011 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 22:40:21.096682 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 22:42:57.327050 140144802662208 spec.py:349] Evaluating on the test split.
I0211 22:43:00.018168 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 22:45:23.696435 140144802662208 submission_runner.py:408] Time since start: 39862.79s, 	Step: 70045, 	{'train/accuracy': 0.6814951300621033, 'train/loss': 1.6714897155761719, 'train/bleu': 34.03927182611093, 'validation/accuracy': 0.6828433275222778, 'validation/loss': 1.6529614925384521, 'validation/bleu': 30.02545059786362, 'validation/num_examples': 3000, 'test/accuracy': 0.697623610496521, 'test/loss': 1.5698145627975464, 'test/bleu': 29.73855557334434, 'test/num_examples': 3003, 'score': 24390.056773662567, 'total_duration': 39862.7853975296, 'accumulated_submission_time': 24390.056773662567, 'accumulated_eval_time': 15469.488009691238, 'accumulated_logging_time': 0.8931279182434082}
I0211 22:45:23.727530 139975029864192 logging_writer.py:48] [70045] accumulated_eval_time=15469.488010, accumulated_logging_time=0.893128, accumulated_submission_time=24390.056774, global_step=70045, preemption_count=0, score=24390.056774, test/accuracy=0.697624, test/bleu=29.738556, test/loss=1.569815, test/num_examples=3003, total_duration=39862.785398, train/accuracy=0.681495, train/bleu=34.039272, train/loss=1.671490, validation/accuracy=0.682843, validation/bleu=30.025451, validation/loss=1.652961, validation/num_examples=3000
I0211 22:45:43.155086 139975038256896 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.29467278718948364, loss=3.7917368412017822
I0211 22:46:17.882479 139975029864192 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.27084749937057495, loss=3.840583086013794
I0211 22:46:52.663374 139975038256896 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.2909690737724304, loss=3.780590295791626
I0211 22:47:27.446654 139975029864192 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.29147768020629883, loss=3.744596481323242
I0211 22:48:02.230917 139975038256896 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.26690730452537537, loss=3.7777833938598633
I0211 22:48:37.010477 139975029864192 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.2680985629558563, loss=3.777024507522583
I0211 22:49:11.817638 139975038256896 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2867867052555084, loss=3.783308267593384
I0211 22:49:46.614833 139975029864192 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.2907628118991852, loss=3.7824389934539795
I0211 22:50:21.404183 139975038256896 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.29187849164009094, loss=3.806452989578247
I0211 22:50:56.184791 139975029864192 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.26544833183288574, loss=3.726358652114868
I0211 22:51:30.955003 139975038256896 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.27866673469543457, loss=3.795363187789917
I0211 22:52:05.782592 139975029864192 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.28754866123199463, loss=3.859626054763794
I0211 22:52:40.578686 139975038256896 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.318602055311203, loss=3.785385847091675
I0211 22:53:15.384684 139975029864192 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.27788227796554565, loss=3.793856620788574
I0211 22:53:50.159671 139975038256896 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.26739466190338135, loss=3.811833620071411
I0211 22:54:24.958383 139975029864192 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.2684846520423889, loss=3.7854487895965576
I0211 22:54:59.751225 139975038256896 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.2940903902053833, loss=3.7998123168945312
I0211 22:55:34.530099 139975029864192 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.34140029549598694, loss=3.8027844429016113
I0211 22:56:09.319136 139975038256896 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.27572140097618103, loss=3.7847156524658203
I0211 22:56:44.136564 139975029864192 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.3403167128562927, loss=3.826702117919922
I0211 22:57:18.935770 139975038256896 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.2884281277656555, loss=3.78961181640625
I0211 22:57:53.701794 139975029864192 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.27764037251472473, loss=3.747368097305298
I0211 22:58:28.498307 139975038256896 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.28469011187553406, loss=3.765977621078491
I0211 22:59:03.286242 139975029864192 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.2897767722606659, loss=3.832263469696045
I0211 22:59:23.862643 140144802662208 spec.py:321] Evaluating on the training split.
I0211 22:59:26.838410 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 23:02:26.932955 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 23:02:29.624668 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 23:05:01.298236 140144802662208 spec.py:349] Evaluating on the test split.
I0211 23:05:04.013585 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 23:07:28.292358 140144802662208 submission_runner.py:408] Time since start: 41187.38s, 	Step: 72461, 	{'train/accuracy': 0.6753785014152527, 'train/loss': 1.71561861038208, 'train/bleu': 33.9016781817681, 'validation/accuracy': 0.6829301714897156, 'validation/loss': 1.6497820615768433, 'validation/bleu': 30.174211488067066, 'validation/num_examples': 3000, 'test/accuracy': 0.6997618079185486, 'test/loss': 1.559238314628601, 'test/bleu': 29.75574832873671, 'test/num_examples': 3003, 'score': 25230.09949684143, 'total_duration': 41187.3813123703, 'accumulated_submission_time': 25230.09949684143, 'accumulated_eval_time': 15953.917650938034, 'accumulated_logging_time': 0.9359502792358398}
I0211 23:07:28.322611 139975038256896 logging_writer.py:48] [72461] accumulated_eval_time=15953.917651, accumulated_logging_time=0.935950, accumulated_submission_time=25230.099497, global_step=72461, preemption_count=0, score=25230.099497, test/accuracy=0.699762, test/bleu=29.755748, test/loss=1.559238, test/num_examples=3003, total_duration=41187.381312, train/accuracy=0.675379, train/bleu=33.901678, train/loss=1.715619, validation/accuracy=0.682930, validation/bleu=30.174211, validation/loss=1.649782, validation/num_examples=3000
I0211 23:07:42.200750 139975029864192 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.2945965826511383, loss=3.7601523399353027
I0211 23:08:16.917733 139975038256896 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.28152400255203247, loss=3.7687172889709473
I0211 23:08:51.709489 139975029864192 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.27876409888267517, loss=3.7907888889312744
I0211 23:09:26.509427 139975038256896 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.28653451800346375, loss=3.808542013168335
I0211 23:10:01.317966 139975029864192 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2927076518535614, loss=3.823007106781006
I0211 23:10:36.114141 139975038256896 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2887987792491913, loss=3.7980175018310547
I0211 23:11:10.890615 139975029864192 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3051927089691162, loss=3.8154470920562744
I0211 23:11:45.692751 139975038256896 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.28890499472618103, loss=3.7612593173980713
I0211 23:12:20.540909 139975029864192 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.30112528800964355, loss=3.7250053882598877
I0211 23:12:55.363734 139975038256896 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.27214616537094116, loss=3.7317001819610596
I0211 23:13:30.168688 139975029864192 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.2932901382446289, loss=3.790127992630005
I0211 23:14:04.997107 139975038256896 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.29242777824401855, loss=3.8300435543060303
I0211 23:14:39.810005 139975029864192 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.2837314307689667, loss=3.7593204975128174
I0211 23:15:14.622826 139975038256896 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.3065398633480072, loss=3.779938220977783
I0211 23:15:49.418556 139975029864192 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.2767460346221924, loss=3.776085138320923
I0211 23:16:24.212743 139975038256896 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.2766658365726471, loss=3.820608615875244
I0211 23:16:58.994387 139975029864192 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.3142566680908203, loss=3.8248870372772217
I0211 23:17:33.782223 139975038256896 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.30500200390815735, loss=3.814725160598755
I0211 23:18:08.575220 139975029864192 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.3048626482486725, loss=3.82317852973938
I0211 23:18:43.361145 139975038256896 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.295181542634964, loss=3.7669899463653564
I0211 23:19:18.157930 139975029864192 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.27730122208595276, loss=3.721031665802002
I0211 23:19:52.935854 139975038256896 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2699514627456665, loss=3.74784517288208
I0211 23:20:27.708858 139975029864192 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.27267611026763916, loss=3.7340519428253174
I0211 23:21:02.513590 139975038256896 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.2804614007472992, loss=3.8170132637023926
I0211 23:21:28.314843 140144802662208 spec.py:321] Evaluating on the training split.
I0211 23:21:31.289023 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 23:25:30.979840 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 23:25:33.665643 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 23:28:05.680511 140144802662208 spec.py:349] Evaluating on the test split.
I0211 23:28:08.373890 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 23:30:34.655404 140144802662208 submission_runner.py:408] Time since start: 42573.74s, 	Step: 74876, 	{'train/accuracy': 0.6737843155860901, 'train/loss': 1.714847445487976, 'train/bleu': 33.94385616955503, 'validation/accuracy': 0.6842072606086731, 'validation/loss': 1.6327464580535889, 'validation/bleu': 30.115898497553975, 'validation/num_examples': 3000, 'test/accuracy': 0.7012027502059937, 'test/loss': 1.540870189666748, 'test/bleu': 30.20991710316605, 'test/num_examples': 3003, 'score': 26069.99861884117, 'total_duration': 42573.744396448135, 'accumulated_submission_time': 26069.99861884117, 'accumulated_eval_time': 16500.258165597916, 'accumulated_logging_time': 0.977301836013794}
I0211 23:30:34.681663 139975029864192 logging_writer.py:48] [74876] accumulated_eval_time=16500.258166, accumulated_logging_time=0.977302, accumulated_submission_time=26069.998619, global_step=74876, preemption_count=0, score=26069.998619, test/accuracy=0.701203, test/bleu=30.209917, test/loss=1.540870, test/num_examples=3003, total_duration=42573.744396, train/accuracy=0.673784, train/bleu=33.943856, train/loss=1.714847, validation/accuracy=0.684207, validation/bleu=30.115898, validation/loss=1.632746, validation/num_examples=3000
I0211 23:30:43.362137 139975038256896 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.28712520003318787, loss=3.785579204559326
I0211 23:31:18.066144 139975029864192 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.2823893427848816, loss=3.7637031078338623
I0211 23:31:52.800054 139975038256896 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.3141585886478424, loss=3.712869644165039
I0211 23:32:27.589755 139975029864192 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.30371642112731934, loss=3.7602295875549316
I0211 23:33:02.374377 139975038256896 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.29689404368400574, loss=3.7877933979034424
I0211 23:33:37.157139 139975029864192 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.2782234847545624, loss=3.7508468627929688
I0211 23:34:11.940494 139975038256896 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.28554731607437134, loss=3.791635036468506
I0211 23:34:46.729994 139975029864192 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.3004779815673828, loss=3.801255941390991
I0211 23:35:21.551066 139975038256896 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.28039413690567017, loss=3.7960472106933594
I0211 23:35:56.374290 139975029864192 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.28637540340423584, loss=3.789062976837158
I0211 23:36:31.213610 139975038256896 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.2854771018028259, loss=3.7658448219299316
I0211 23:37:05.972709 139975029864192 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3093947172164917, loss=3.7418549060821533
I0211 23:37:40.740475 139975038256896 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.29065272212028503, loss=3.763676404953003
I0211 23:38:15.527264 139975029864192 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.29378148913383484, loss=3.7533321380615234
I0211 23:38:50.303591 139975038256896 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.2843901216983795, loss=3.78861665725708
I0211 23:39:25.065676 139975029864192 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.31024137139320374, loss=3.83266544342041
I0211 23:39:59.875489 139975038256896 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.26611509919166565, loss=3.7617502212524414
I0211 23:40:34.671669 139975029864192 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.27311447262763977, loss=3.7410826683044434
I0211 23:41:09.457911 139975038256896 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.301079660654068, loss=3.784625291824341
I0211 23:41:44.232385 139975029864192 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.28802821040153503, loss=3.773991107940674
I0211 23:42:19.023679 139975038256896 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.27913859486579895, loss=3.764941453933716
I0211 23:42:53.798069 139975029864192 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.299482136964798, loss=3.8112151622772217
I0211 23:43:28.599203 139975038256896 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.28218087553977966, loss=3.763753652572632
I0211 23:44:03.385119 139975029864192 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.2666058838367462, loss=3.764138698577881
I0211 23:44:34.755804 140144802662208 spec.py:321] Evaluating on the training split.
I0211 23:44:37.730324 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 23:48:06.783282 140144802662208 spec.py:333] Evaluating on the validation split.
I0211 23:48:09.471336 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 23:50:42.232570 140144802662208 spec.py:349] Evaluating on the test split.
I0211 23:50:44.924189 140144802662208 workload.py:181] Translating evaluation dataset.
I0211 23:53:18.609238 140144802662208 submission_runner.py:408] Time since start: 43937.70s, 	Step: 77292, 	{'train/accuracy': 0.6831181645393372, 'train/loss': 1.6621711254119873, 'train/bleu': 34.21749717101149, 'validation/accuracy': 0.685459554195404, 'validation/loss': 1.6368142366409302, 'validation/bleu': 30.355440777747113, 'validation/num_examples': 3000, 'test/accuracy': 0.7004590034484863, 'test/loss': 1.5464823246002197, 'test/bleu': 30.350398961195094, 'test/num_examples': 3003, 'score': 26909.980088472366, 'total_duration': 43937.69822263718, 'accumulated_submission_time': 26909.980088472366, 'accumulated_eval_time': 17024.11154460907, 'accumulated_logging_time': 1.0152418613433838}
I0211 23:53:18.635343 139975038256896 logging_writer.py:48] [77292] accumulated_eval_time=17024.111545, accumulated_logging_time=1.015242, accumulated_submission_time=26909.980088, global_step=77292, preemption_count=0, score=26909.980088, test/accuracy=0.700459, test/bleu=30.350399, test/loss=1.546482, test/num_examples=3003, total_duration=43937.698223, train/accuracy=0.683118, train/bleu=34.217497, train/loss=1.662171, validation/accuracy=0.685460, validation/bleu=30.355441, validation/loss=1.636814, validation/num_examples=3000
I0211 23:53:21.771808 139975029864192 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.29272815585136414, loss=3.776076316833496
I0211 23:53:56.405342 139975038256896 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.2957650125026703, loss=3.7714383602142334
I0211 23:54:31.164469 139975029864192 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.2869409918785095, loss=3.778596878051758
I0211 23:55:05.942334 139975038256896 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.29019615054130554, loss=3.765800952911377
I0211 23:55:40.700633 139975029864192 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.31094080209732056, loss=3.895456552505493
I0211 23:56:15.482753 139975038256896 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.29343387484550476, loss=3.8114676475524902
I0211 23:56:50.243807 139975029864192 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.28880295157432556, loss=3.755770206451416
I0211 23:57:25.011926 139975038256896 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.28937605023384094, loss=3.7238128185272217
I0211 23:57:59.805911 139975029864192 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.2790485620498657, loss=3.8119027614593506
I0211 23:58:34.589946 139975038256896 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.2991488575935364, loss=3.791372299194336
I0211 23:59:09.418156 139975029864192 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3043093681335449, loss=3.7851667404174805
I0211 23:59:44.211501 139975038256896 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.2897864580154419, loss=3.754505157470703
I0212 00:00:18.976764 139975029864192 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2973956763744354, loss=3.7843825817108154
I0212 00:00:53.773420 139975038256896 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2810370624065399, loss=3.777130603790283
I0212 00:01:28.603649 139975029864192 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.29588934779167175, loss=3.7844958305358887
I0212 00:02:03.460572 139975038256896 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2933949828147888, loss=3.719392776489258
I0212 00:02:38.247451 139975029864192 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.28588417172431946, loss=3.7519991397857666
I0212 00:03:13.050521 139975038256896 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.31164559721946716, loss=3.757364273071289
I0212 00:03:47.826486 139975029864192 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.2875750660896301, loss=3.8110363483428955
I0212 00:04:22.613578 139975038256896 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2888328731060028, loss=3.7941319942474365
I0212 00:04:57.401576 139975029864192 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.30601513385772705, loss=3.787092924118042
I0212 00:05:32.192401 139975038256896 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.28752028942108154, loss=3.743626356124878
I0212 00:06:06.976320 139975029864192 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.31136777997016907, loss=3.825373411178589
I0212 00:06:41.741913 139975038256896 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.3000738322734833, loss=3.7832064628601074
I0212 00:07:16.515752 139975029864192 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.30877718329429626, loss=3.80167293548584
I0212 00:07:18.680706 140144802662208 spec.py:321] Evaluating on the training split.
I0212 00:07:21.662732 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 00:10:04.747537 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 00:10:07.431388 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 00:12:33.570602 140144802662208 spec.py:349] Evaluating on the test split.
I0212 00:12:36.261644 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 00:14:55.485686 140144802662208 submission_runner.py:408] Time since start: 45234.57s, 	Step: 79708, 	{'train/accuracy': 0.6789937019348145, 'train/loss': 1.6847949028015137, 'train/bleu': 34.13086744144354, 'validation/accuracy': 0.6863647103309631, 'validation/loss': 1.625876784324646, 'validation/bleu': 30.33725457330963, 'validation/num_examples': 3000, 'test/accuracy': 0.7026204466819763, 'test/loss': 1.5370594263076782, 'test/bleu': 30.37668151012431, 'test/num_examples': 3003, 'score': 27749.933834314346, 'total_duration': 45234.57467293739, 'accumulated_submission_time': 27749.933834314346, 'accumulated_eval_time': 17480.916483163834, 'accumulated_logging_time': 1.0527923107147217}
I0212 00:14:55.513439 139975038256896 logging_writer.py:48] [79708] accumulated_eval_time=17480.916483, accumulated_logging_time=1.052792, accumulated_submission_time=27749.933834, global_step=79708, preemption_count=0, score=27749.933834, test/accuracy=0.702620, test/bleu=30.376682, test/loss=1.537059, test/num_examples=3003, total_duration=45234.574673, train/accuracy=0.678994, train/bleu=34.130867, train/loss=1.684795, validation/accuracy=0.686365, validation/bleu=30.337255, validation/loss=1.625877, validation/num_examples=3000
I0212 00:15:27.813286 139975029864192 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3124701678752899, loss=3.779021978378296
I0212 00:16:02.551620 139975038256896 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.2892411947250366, loss=3.7944583892822266
I0212 00:16:37.344958 139975029864192 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.293247789144516, loss=3.695098638534546
I0212 00:17:12.137762 139975038256896 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.32530781626701355, loss=3.8288254737854004
I0212 00:17:46.939340 139975029864192 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.29022979736328125, loss=3.7389755249023438
I0212 00:18:21.714832 139975038256896 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2966334819793701, loss=3.797165632247925
I0212 00:18:56.489456 139975029864192 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.31466537714004517, loss=3.755476236343384
I0212 00:19:31.281598 139975038256896 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.31375181674957275, loss=3.8324530124664307
I0212 00:20:06.072398 139975029864192 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.2973974347114563, loss=3.7524220943450928
I0212 00:20:40.872642 139975038256896 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.28757432103157043, loss=3.734259605407715
I0212 00:21:15.668434 139975029864192 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.28643015027046204, loss=3.794088125228882
I0212 00:21:50.460726 139975038256896 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.2914559543132782, loss=3.7739927768707275
I0212 00:22:25.268705 139975029864192 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.308228075504303, loss=3.800515651702881
I0212 00:23:00.117440 139975038256896 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.3061358332633972, loss=3.7298974990844727
I0212 00:23:34.974651 139975029864192 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.29159101843833923, loss=3.763274908065796
I0212 00:24:09.800940 139975038256896 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.33491161465644836, loss=3.842905044555664
I0212 00:24:44.584896 139975029864192 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.2935982644557953, loss=3.748399257659912
I0212 00:25:19.385360 139975038256896 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.28804153203964233, loss=3.709728717803955
I0212 00:25:54.183364 139975029864192 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3629491627216339, loss=3.7851722240448
I0212 00:26:28.963461 139975038256896 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2936720848083496, loss=3.7414393424987793
I0212 00:27:03.741286 139975029864192 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.3207748532295227, loss=3.7633004188537598
I0212 00:27:38.516668 139975038256896 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.3247648775577545, loss=3.795987606048584
I0212 00:28:13.293629 139975029864192 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.28778135776519775, loss=3.8113739490509033
I0212 00:28:48.083361 139975038256896 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.29380232095718384, loss=3.7128746509552
I0212 00:28:55.798380 140144802662208 spec.py:321] Evaluating on the training split.
I0212 00:28:58.771248 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 00:31:41.796778 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 00:31:44.482888 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 00:34:12.575679 140144802662208 spec.py:349] Evaluating on the test split.
I0212 00:34:15.265521 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 00:36:32.729527 140144802662208 submission_runner.py:408] Time since start: 46531.82s, 	Step: 82124, 	{'train/accuracy': 0.6997047066688538, 'train/loss': 1.573757290840149, 'train/bleu': 35.5428039352093, 'validation/accuracy': 0.6866622567176819, 'validation/loss': 1.6228492259979248, 'validation/bleu': 30.645088176249953, 'validation/num_examples': 3000, 'test/accuracy': 0.7039335370063782, 'test/loss': 1.5355430841445923, 'test/bleu': 30.418042700337793, 'test/num_examples': 3003, 'score': 28590.127032995224, 'total_duration': 46531.81851029396, 'accumulated_submission_time': 28590.127032995224, 'accumulated_eval_time': 17937.84757256508, 'accumulated_logging_time': 1.0906941890716553}
I0212 00:36:32.756614 139975029864192 logging_writer.py:48] [82124] accumulated_eval_time=17937.847573, accumulated_logging_time=1.090694, accumulated_submission_time=28590.127033, global_step=82124, preemption_count=0, score=28590.127033, test/accuracy=0.703934, test/bleu=30.418043, test/loss=1.535543, test/num_examples=3003, total_duration=46531.818510, train/accuracy=0.699705, train/bleu=35.542804, train/loss=1.573757, validation/accuracy=0.686662, validation/bleu=30.645088, validation/loss=1.622849, validation/num_examples=3000
I0212 00:36:59.436461 139975038256896 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.3103305697441101, loss=3.7972025871276855
I0212 00:37:34.152548 139975029864192 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.3201758563518524, loss=3.8286869525909424
I0212 00:38:08.932248 139975038256896 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.29462379217147827, loss=3.78580641746521
I0212 00:38:43.707490 139975029864192 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3019191324710846, loss=3.774021863937378
I0212 00:39:18.501660 139975038256896 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.3091473877429962, loss=3.721741199493408
I0212 00:39:53.294349 139975029864192 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.3127198815345764, loss=3.7685341835021973
I0212 00:40:28.112693 139975038256896 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.29004091024398804, loss=3.7679407596588135
I0212 00:41:02.922653 139975029864192 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.28591448068618774, loss=3.7653298377990723
I0212 00:41:37.751151 139975038256896 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.2837804853916168, loss=3.7250430583953857
I0212 00:42:12.542163 139975029864192 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.29354962706565857, loss=3.772106409072876
I0212 00:42:47.360907 139975038256896 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.2969236373901367, loss=3.792890787124634
I0212 00:43:22.158130 139975029864192 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.29474201798439026, loss=3.7117254734039307
I0212 00:43:56.944288 139975038256896 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.3020687997341156, loss=3.76486873626709
I0212 00:44:31.760248 139975029864192 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.2834891080856323, loss=3.7625908851623535
I0212 00:45:06.567986 139975038256896 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.2869552969932556, loss=3.766143321990967
I0212 00:45:41.381100 139975029864192 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.3030644357204437, loss=3.728675365447998
I0212 00:46:16.174524 139975038256896 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.29775503277778625, loss=3.704759120941162
I0212 00:46:51.012887 139975029864192 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.29927530884742737, loss=3.790501832962036
I0212 00:47:25.831256 139975038256896 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.29611900448799133, loss=3.7243967056274414
I0212 00:48:00.696040 139975029864192 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.3260252773761749, loss=3.7280945777893066
I0212 00:48:35.518532 139975038256896 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.2745240032672882, loss=3.7739205360412598
I0212 00:49:10.375899 139975029864192 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.3235435485839844, loss=3.7285311222076416
I0212 00:49:45.181104 139975038256896 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.3022666275501251, loss=3.782680034637451
I0212 00:50:19.964729 139975029864192 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3063006103038788, loss=3.730635643005371
I0212 00:50:32.920250 140144802662208 spec.py:321] Evaluating on the training split.
I0212 00:50:35.914726 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 00:53:32.044168 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 00:53:34.741604 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 00:56:07.365494 140144802662208 spec.py:349] Evaluating on the test split.
I0212 00:56:10.072124 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 00:58:31.137447 140144802662208 submission_runner.py:408] Time since start: 47850.23s, 	Step: 84539, 	{'train/accuracy': 0.682442307472229, 'train/loss': 1.6693923473358154, 'train/bleu': 34.606211236537106, 'validation/accuracy': 0.688286542892456, 'validation/loss': 1.6197954416275024, 'validation/bleu': 30.211782980971023, 'validation/num_examples': 3000, 'test/accuracy': 0.7041078805923462, 'test/loss': 1.5266790390014648, 'test/bleu': 30.543048679561238, 'test/num_examples': 3003, 'score': 29430.197852134705, 'total_duration': 47850.226432323456, 'accumulated_submission_time': 29430.197852134705, 'accumulated_eval_time': 18416.06471323967, 'accumulated_logging_time': 1.1282691955566406}
I0212 00:58:31.164414 139975038256896 logging_writer.py:48] [84539] accumulated_eval_time=18416.064713, accumulated_logging_time=1.128269, accumulated_submission_time=29430.197852, global_step=84539, preemption_count=0, score=29430.197852, test/accuracy=0.704108, test/bleu=30.543049, test/loss=1.526679, test/num_examples=3003, total_duration=47850.226432, train/accuracy=0.682442, train/bleu=34.606211, train/loss=1.669392, validation/accuracy=0.688287, validation/bleu=30.211783, validation/loss=1.619795, validation/num_examples=3000
I0212 00:58:52.659076 139975029864192 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.2877960503101349, loss=3.751020669937134
I0212 00:59:27.344388 139975038256896 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.28063496947288513, loss=3.743098735809326
I0212 01:00:02.137151 139975029864192 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.2957686185836792, loss=3.700653076171875
I0212 01:00:36.913818 139975038256896 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.3094593584537506, loss=3.775378942489624
I0212 01:01:11.702452 139975029864192 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.3126446008682251, loss=3.7341244220733643
I0212 01:01:46.474386 139975038256896 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.2991553544998169, loss=3.7165720462799072
I0212 01:02:21.249379 139975029864192 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.30290472507476807, loss=3.783698797225952
I0212 01:02:56.030776 139975038256896 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.329038143157959, loss=3.7639474868774414
I0212 01:03:30.805150 139975029864192 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3043774366378784, loss=3.75644588470459
I0212 01:04:05.578319 139975038256896 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.318664014339447, loss=3.784097909927368
I0212 01:04:40.406298 139975029864192 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.3111291527748108, loss=3.700615406036377
I0212 01:05:15.224061 139975038256896 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.31857720017433167, loss=3.7865631580352783
I0212 01:05:50.006411 139975029864192 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.33218875527381897, loss=3.815066337585449
I0212 01:06:24.804492 139975038256896 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.3106857240200043, loss=3.790053367614746
I0212 01:06:59.645010 139975029864192 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.3430626094341278, loss=3.779254198074341
I0212 01:07:34.517200 139975038256896 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.30178794264793396, loss=3.716085910797119
I0212 01:08:09.340428 139975029864192 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.34536460041999817, loss=3.726133108139038
I0212 01:08:44.152524 139975038256896 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.30170586705207825, loss=3.724618911743164
I0212 01:09:18.937261 139975029864192 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.30827897787094116, loss=3.8119606971740723
I0212 01:09:53.736670 139975038256896 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3240554928779602, loss=3.7393906116485596
I0212 01:10:28.520094 139975029864192 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.32766661047935486, loss=3.7136006355285645
I0212 01:11:03.318321 139975038256896 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3270658850669861, loss=3.755509376525879
I0212 01:11:38.083265 139975029864192 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.33007895946502686, loss=3.739332914352417
I0212 01:12:12.856964 139975038256896 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.3141522705554962, loss=3.7253715991973877
I0212 01:12:31.374831 140144802662208 spec.py:321] Evaluating on the training split.
I0212 01:12:34.360080 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 01:15:51.552058 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 01:15:54.241029 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 01:18:27.159339 140144802662208 spec.py:349] Evaluating on the test split.
I0212 01:18:29.854254 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 01:20:57.572067 140144802662208 submission_runner.py:408] Time since start: 49196.66s, 	Step: 86955, 	{'train/accuracy': 0.6825690269470215, 'train/loss': 1.666361689567566, 'train/bleu': 34.71119184261502, 'validation/accuracy': 0.6877037882804871, 'validation/loss': 1.6188981533050537, 'validation/bleu': 30.182893205006323, 'validation/num_examples': 3000, 'test/accuracy': 0.7032247185707092, 'test/loss': 1.5332567691802979, 'test/bleu': 30.316463949991334, 'test/num_examples': 3003, 'score': 30270.317069530487, 'total_duration': 49196.66105890274, 'accumulated_submission_time': 30270.317069530487, 'accumulated_eval_time': 18922.261901140213, 'accumulated_logging_time': 1.1654300689697266}
I0212 01:20:57.598988 139975029864192 logging_writer.py:48] [86955] accumulated_eval_time=18922.261901, accumulated_logging_time=1.165430, accumulated_submission_time=30270.317070, global_step=86955, preemption_count=0, score=30270.317070, test/accuracy=0.703225, test/bleu=30.316464, test/loss=1.533257, test/num_examples=3003, total_duration=49196.661059, train/accuracy=0.682569, train/bleu=34.711192, train/loss=1.666362, validation/accuracy=0.687704, validation/bleu=30.182893, validation/loss=1.618898, validation/num_examples=3000
I0212 01:21:13.580414 139975038256896 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.3008589744567871, loss=3.713202476501465
I0212 01:21:48.278558 139975029864192 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.29890871047973633, loss=3.69856595993042
I0212 01:22:23.048383 139975038256896 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.3269559442996979, loss=3.749973773956299
I0212 01:22:57.861424 139975029864192 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.3467342257499695, loss=3.729008913040161
I0212 01:23:32.669321 139975038256896 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.31690624356269836, loss=3.7262110710144043
I0212 01:24:07.480211 139975029864192 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.3082391619682312, loss=3.772108793258667
I0212 01:24:42.270669 139975038256896 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.33334869146347046, loss=3.7057864665985107
I0212 01:25:17.072723 139975029864192 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.2995190918445587, loss=3.7321934700012207
I0212 01:25:51.858811 139975038256896 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.35049572587013245, loss=3.70798397064209
I0212 01:26:26.646733 139975029864192 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.3416879177093506, loss=3.7167632579803467
I0212 01:27:01.434498 139975038256896 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.318220317363739, loss=3.687619209289551
I0212 01:27:36.224726 139975029864192 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.31860822439193726, loss=3.7487964630126953
I0212 01:28:11.008291 139975038256896 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.3008492588996887, loss=3.740974187850952
I0212 01:28:45.796797 139975029864192 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.30330023169517517, loss=3.7669754028320312
I0212 01:29:20.705514 139975038256896 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.3171027898788452, loss=3.7470290660858154
I0212 01:29:55.535358 139975029864192 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3152610659599304, loss=3.736953020095825
I0212 01:30:30.355188 139975038256896 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.29899662733078003, loss=3.678393840789795
I0212 01:31:05.178623 139975029864192 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.31518739461898804, loss=3.7355830669403076
I0212 01:31:40.001971 139975038256896 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.33105114102363586, loss=3.770538330078125
I0212 01:32:14.802782 139975029864192 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3119567334651947, loss=3.760969877243042
I0212 01:32:49.568574 139975038256896 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.31970423460006714, loss=3.7366199493408203
I0212 01:33:24.367907 139975029864192 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.33730512857437134, loss=3.746063470840454
I0212 01:33:59.178967 139975038256896 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.320474773645401, loss=3.7434046268463135
I0212 01:34:33.953149 139975029864192 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.32403045892715454, loss=3.7303707599639893
I0212 01:34:57.674591 140144802662208 spec.py:321] Evaluating on the training split.
I0212 01:35:00.651309 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 01:37:50.932016 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 01:37:53.621115 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 01:40:24.019767 140144802662208 spec.py:349] Evaluating on the test split.
I0212 01:40:26.707276 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 01:42:45.290608 140144802662208 submission_runner.py:408] Time since start: 50504.38s, 	Step: 89370, 	{'train/accuracy': 0.6950032711029053, 'train/loss': 1.60042405128479, 'train/bleu': 35.154835319669424, 'validation/accuracy': 0.6891793012619019, 'validation/loss': 1.609760046005249, 'validation/bleu': 30.554063056987584, 'validation/num_examples': 3000, 'test/accuracy': 0.705083966255188, 'test/loss': 1.517716407775879, 'test/bleu': 30.645274576054728, 'test/num_examples': 3003, 'score': 31110.298770189285, 'total_duration': 50504.37959980965, 'accumulated_submission_time': 31110.298770189285, 'accumulated_eval_time': 19389.877872228622, 'accumulated_logging_time': 1.2036826610565186}
I0212 01:42:45.318510 139975038256896 logging_writer.py:48] [89370] accumulated_eval_time=19389.877872, accumulated_logging_time=1.203683, accumulated_submission_time=31110.298770, global_step=89370, preemption_count=0, score=31110.298770, test/accuracy=0.705084, test/bleu=30.645275, test/loss=1.517716, test/num_examples=3003, total_duration=50504.379600, train/accuracy=0.695003, train/bleu=35.154835, train/loss=1.600424, validation/accuracy=0.689179, validation/bleu=30.554063, validation/loss=1.609760, validation/num_examples=3000
I0212 01:42:56.100800 139975029864192 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.29410088062286377, loss=3.7051286697387695
I0212 01:43:30.767172 139975038256896 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.3070564568042755, loss=3.6878764629364014
I0212 01:44:05.533136 139975029864192 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.3325732946395874, loss=3.695192337036133
I0212 01:44:40.374034 139975038256896 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3451862633228302, loss=3.7372145652770996
I0212 01:45:15.175742 139975029864192 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.3088529706001282, loss=3.719710350036621
I0212 01:45:50.001205 139975038256896 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.3038657605648041, loss=3.7140390872955322
I0212 01:46:24.831831 139975029864192 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.3310278654098511, loss=3.7883803844451904
I0212 01:46:59.634474 139975038256896 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.3304547965526581, loss=3.75158953666687
I0212 01:47:34.422830 139975029864192 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.31185245513916016, loss=3.762375593185425
I0212 01:48:09.219585 139975038256896 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.31895172595977783, loss=3.77730655670166
I0212 01:48:44.021053 139975029864192 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.32388246059417725, loss=3.6991593837738037
I0212 01:49:18.819786 139975038256896 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.313528835773468, loss=3.674438714981079
I0212 01:49:53.628486 139975029864192 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.346657931804657, loss=3.7368078231811523
I0212 01:50:28.438977 139975038256896 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.35171589255332947, loss=3.7928903102874756
I0212 01:51:03.264921 139975029864192 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.3307960331439972, loss=3.7566347122192383
I0212 01:51:38.056233 139975038256896 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.31294938921928406, loss=3.740689277648926
I0212 01:52:12.858209 139975029864192 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.30352070927619934, loss=3.7250945568084717
I0212 01:52:47.661010 139975038256896 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.31651660799980164, loss=3.726163864135742
I0212 01:53:22.451941 139975029864192 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.368794709444046, loss=3.7651219367980957
I0212 01:53:57.237817 139975038256896 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.3278168737888336, loss=3.767735481262207
I0212 01:54:32.070678 139975029864192 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.3300332725048065, loss=3.694551944732666
I0212 01:55:06.867678 139975038256896 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.31893277168273926, loss=3.703580141067505
I0212 01:55:41.674471 139975029864192 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.32565322518348694, loss=3.713414430618286
I0212 01:56:16.485790 139975038256896 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.35099363327026367, loss=3.7698636054992676
I0212 01:56:45.442571 140144802662208 spec.py:321] Evaluating on the training split.
I0212 01:56:48.423375 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 01:59:29.756212 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 01:59:32.449148 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 02:02:04.812956 140144802662208 spec.py:349] Evaluating on the test split.
I0212 02:02:07.508594 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 02:04:27.162523 140144802662208 submission_runner.py:408] Time since start: 51806.25s, 	Step: 91785, 	{'train/accuracy': 0.6907691955566406, 'train/loss': 1.61209237575531, 'train/bleu': 34.67632533933311, 'validation/accuracy': 0.6889809370040894, 'validation/loss': 1.6088262796401978, 'validation/bleu': 30.479471667183642, 'validation/num_examples': 3000, 'test/accuracy': 0.7055255770683289, 'test/loss': 1.5153443813323975, 'test/bleu': 30.73101495101404, 'test/num_examples': 3003, 'score': 31950.33271098137, 'total_duration': 51806.25148367882, 'accumulated_submission_time': 31950.33271098137, 'accumulated_eval_time': 19851.59775352478, 'accumulated_logging_time': 1.242016077041626}
I0212 02:04:27.191487 139975029864192 logging_writer.py:48] [91785] accumulated_eval_time=19851.597754, accumulated_logging_time=1.242016, accumulated_submission_time=31950.332711, global_step=91785, preemption_count=0, score=31950.332711, test/accuracy=0.705526, test/bleu=30.731015, test/loss=1.515344, test/num_examples=3003, total_duration=51806.251484, train/accuracy=0.690769, train/bleu=34.676325, train/loss=1.612092, validation/accuracy=0.688981, validation/bleu=30.479472, validation/loss=1.608826, validation/num_examples=3000
I0212 02:04:32.764296 139975038256896 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.33304017782211304, loss=3.7287590503692627
I0212 02:05:07.408922 139975029864192 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.30922314524650574, loss=3.695547342300415
I0212 02:05:42.119260 139975038256896 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.3548031449317932, loss=3.766360282897949
I0212 02:06:16.918256 139975029864192 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3086290657520294, loss=3.7259247303009033
I0212 02:06:51.718392 139975038256896 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.3286745846271515, loss=3.789118528366089
I0212 02:07:26.534926 139975029864192 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.31572628021240234, loss=3.6993112564086914
I0212 02:08:01.313591 139975038256896 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.33046475052833557, loss=3.798335552215576
I0212 02:08:36.242035 139975029864192 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.3204519748687744, loss=3.6984643936157227
I0212 02:09:11.055122 139975038256896 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.3159596920013428, loss=3.6699209213256836
I0212 02:09:45.853239 139975029864192 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.33148232102394104, loss=3.747882843017578
I0212 02:10:20.645111 139975038256896 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.31403306126594543, loss=3.7576208114624023
I0212 02:10:55.440589 139975029864192 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.323085755109787, loss=3.7792539596557617
I0212 02:11:30.229088 139975038256896 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.31552401185035706, loss=3.767183780670166
I0212 02:12:05.009021 139975029864192 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.3278051018714905, loss=3.72212815284729
I0212 02:12:39.859100 139975038256896 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.3406131863594055, loss=3.6995460987091064
I0212 02:13:14.719493 139975029864192 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.3313356339931488, loss=3.6735239028930664
I0212 02:13:49.516921 139975038256896 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.32743921875953674, loss=3.7211151123046875
I0212 02:14:24.343707 139975029864192 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.32762497663497925, loss=3.671111583709717
I0212 02:14:59.152322 139975038256896 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.32003486156463623, loss=3.702995538711548
I0212 02:15:33.950575 139975029864192 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.3308877944946289, loss=3.733011245727539
I0212 02:16:08.782042 139975038256896 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.29230204224586487, loss=3.664865732192993
I0212 02:16:43.598465 139975029864192 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.3165232837200165, loss=3.7184181213378906
I0212 02:17:18.428061 139975038256896 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3423212766647339, loss=3.7385551929473877
I0212 02:17:53.258434 139975029864192 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.3327810764312744, loss=3.786238193511963
I0212 02:18:27.463311 140144802662208 spec.py:321] Evaluating on the training split.
I0212 02:18:30.448316 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 02:21:24.535771 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 02:21:27.224134 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 02:23:57.226403 140144802662208 spec.py:349] Evaluating on the test split.
I0212 02:23:59.911400 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 02:26:29.036972 140144802662208 submission_runner.py:408] Time since start: 53128.13s, 	Step: 94200, 	{'train/accuracy': 0.7140809893608093, 'train/loss': 1.4977169036865234, 'train/bleu': 36.6485987570553, 'validation/accuracy': 0.6901712417602539, 'validation/loss': 1.6079293489456177, 'validation/bleu': 30.60468141212876, 'validation/num_examples': 3000, 'test/accuracy': 0.7057114839553833, 'test/loss': 1.514120101928711, 'test/bleu': 30.51289879789388, 'test/num_examples': 3003, 'score': 32790.51075673103, 'total_duration': 53128.12596178055, 'accumulated_submission_time': 32790.51075673103, 'accumulated_eval_time': 20333.17138814926, 'accumulated_logging_time': 1.2815396785736084}
I0212 02:26:29.066216 139975038256896 logging_writer.py:48] [94200] accumulated_eval_time=20333.171388, accumulated_logging_time=1.281540, accumulated_submission_time=32790.510757, global_step=94200, preemption_count=0, score=32790.510757, test/accuracy=0.705711, test/bleu=30.512899, test/loss=1.514120, test/num_examples=3003, total_duration=53128.125962, train/accuracy=0.714081, train/bleu=36.648599, train/loss=1.497717, validation/accuracy=0.690171, validation/bleu=30.604681, validation/loss=1.607929, validation/num_examples=3000
I0212 02:26:29.430073 139975029864192 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.32705503702163696, loss=3.690222978591919
I0212 02:27:04.069743 139975038256896 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.32269489765167236, loss=3.6794490814208984
I0212 02:27:38.822471 139975029864192 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.3346826434135437, loss=3.6884050369262695
I0212 02:28:13.740176 139975038256896 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.3126051425933838, loss=3.691530704498291
I0212 02:28:48.522409 139975029864192 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.3251786530017853, loss=3.7265284061431885
I0212 02:29:23.295614 139975038256896 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.3596704304218292, loss=3.6902997493743896
I0212 02:29:58.073039 139975029864192 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.33185771107673645, loss=3.701444625854492
I0212 02:30:32.882802 139975038256896 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.3265989422798157, loss=3.761704921722412
I0212 02:31:07.677448 139975029864192 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.32639506459236145, loss=3.6880340576171875
I0212 02:31:42.462905 139975038256896 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.33544740080833435, loss=3.689577341079712
I0212 02:32:17.253484 139975029864192 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.3505048453807831, loss=3.6975185871124268
I0212 02:32:52.042161 139975038256896 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.32123756408691406, loss=3.763885259628296
I0212 02:33:26.847790 139975029864192 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.385713130235672, loss=3.708019971847534
I0212 02:34:01.664783 139975038256896 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.33273205161094666, loss=3.6980366706848145
I0212 02:34:36.478139 139975029864192 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.34477874636650085, loss=3.7091903686523438
I0212 02:35:11.254486 139975038256896 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.3319726884365082, loss=3.75504469871521
I0212 02:35:46.077245 139975029864192 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.3155752420425415, loss=3.659816265106201
I0212 02:36:20.894504 139975038256896 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.3327746093273163, loss=3.66100811958313
I0212 02:36:55.689660 139975029864192 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.3416789174079895, loss=3.7231192588806152
I0212 02:37:30.477306 139975038256896 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.3525674641132355, loss=3.7222671508789062
I0212 02:38:05.271724 139975029864192 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.33714067935943604, loss=3.6214816570281982
I0212 02:38:40.063453 139975038256896 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.32671913504600525, loss=3.6959235668182373
I0212 02:39:14.873331 139975029864192 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.32219627499580383, loss=3.6822152137756348
I0212 02:39:49.633780 139975038256896 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.3206808865070343, loss=3.6968190670013428
I0212 02:40:24.410373 139975029864192 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.34082743525505066, loss=3.748016595840454
I0212 02:40:29.349229 140144802662208 spec.py:321] Evaluating on the training split.
I0212 02:40:32.328208 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 02:43:28.050194 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 02:43:30.743626 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 02:45:58.121982 140144802662208 spec.py:349] Evaluating on the test split.
I0212 02:46:00.817053 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 02:48:29.645223 140144802662208 submission_runner.py:408] Time since start: 54448.73s, 	Step: 96616, 	{'train/accuracy': 0.6998093128204346, 'train/loss': 1.5723187923431396, 'train/bleu': 35.22440418347292, 'validation/accuracy': 0.6890801191329956, 'validation/loss': 1.608446478843689, 'validation/bleu': 30.391074376933528, 'validation/num_examples': 3000, 'test/accuracy': 0.7062111496925354, 'test/loss': 1.511307954788208, 'test/bleu': 30.341710528802896, 'test/num_examples': 3003, 'score': 33630.70009255409, 'total_duration': 54448.7342133522, 'accumulated_submission_time': 33630.70009255409, 'accumulated_eval_time': 20813.4673306942, 'accumulated_logging_time': 1.3212535381317139}
I0212 02:48:29.674013 139975038256896 logging_writer.py:48] [96616] accumulated_eval_time=20813.467331, accumulated_logging_time=1.321254, accumulated_submission_time=33630.700093, global_step=96616, preemption_count=0, score=33630.700093, test/accuracy=0.706211, test/bleu=30.341711, test/loss=1.511308, test/num_examples=3003, total_duration=54448.734213, train/accuracy=0.699809, train/bleu=35.224404, train/loss=1.572319, validation/accuracy=0.689080, validation/bleu=30.391074, validation/loss=1.608446, validation/num_examples=3000
I0212 02:48:59.113379 139975029864192 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3216187059879303, loss=3.689965009689331
I0212 02:49:33.824153 139975038256896 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.3395768702030182, loss=3.7131690979003906
I0212 02:50:08.608140 139975029864192 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.3595644235610962, loss=3.7066898345947266
I0212 02:50:43.403006 139975038256896 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3490900993347168, loss=3.708897352218628
I0212 02:51:18.205695 139975029864192 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.3278786242008209, loss=3.650925636291504
I0212 02:51:53.010309 139975038256896 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3388659656047821, loss=3.7130720615386963
I0212 02:52:27.784409 139975029864192 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.35547056794166565, loss=3.728868007659912
I0212 02:53:02.581184 139975038256896 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.32424432039260864, loss=3.684520721435547
I0212 02:53:37.344345 139975029864192 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.35050949454307556, loss=3.6549477577209473
I0212 02:54:12.125010 139975038256896 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.34451353549957275, loss=3.709798574447632
I0212 02:54:46.941453 139975029864192 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.35528019070625305, loss=3.6954503059387207
I0212 02:55:21.747804 139975038256896 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.3590417504310608, loss=3.7184345722198486
I0212 02:55:56.527525 139975029864192 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.3374444246292114, loss=3.6986496448516846
I0212 02:56:31.336192 139975038256896 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.34100663661956787, loss=3.689920425415039
I0212 02:57:06.134613 139975029864192 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.3509548306465149, loss=3.726530075073242
I0212 02:57:40.920986 139975038256896 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.33347809314727783, loss=3.689147710800171
I0212 02:58:15.723085 139975029864192 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.3349279463291168, loss=3.6779425144195557
I0212 02:58:50.524188 139975038256896 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.3276859223842621, loss=3.6624746322631836
I0212 02:59:25.340725 139975029864192 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3279915153980255, loss=3.6868398189544678
I0212 03:00:00.118231 139975038256896 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.35086584091186523, loss=3.7191953659057617
I0212 03:00:34.963828 139975029864192 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.341666579246521, loss=3.697652816772461
I0212 03:01:09.756420 139975038256896 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.33139467239379883, loss=3.641758680343628
I0212 03:01:44.538622 139975029864192 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.3529466688632965, loss=3.7172303199768066
I0212 03:02:19.358530 139975038256896 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.33949291706085205, loss=3.6534576416015625
I0212 03:02:29.894593 140144802662208 spec.py:321] Evaluating on the training split.
I0212 03:02:32.902557 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 03:05:22.784049 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 03:05:25.473183 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 03:07:56.207518 140144802662208 spec.py:349] Evaluating on the test split.
I0212 03:07:58.903987 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 03:10:24.186238 140144802662208 submission_runner.py:408] Time since start: 55763.28s, 	Step: 99032, 	{'train/accuracy': 0.6960033774375916, 'train/loss': 1.58683180809021, 'train/bleu': 35.784215910569934, 'validation/accuracy': 0.6911879777908325, 'validation/loss': 1.6049529314041138, 'validation/bleu': 30.653904813297338, 'validation/num_examples': 3000, 'test/accuracy': 0.706362247467041, 'test/loss': 1.5094925165176392, 'test/bleu': 30.670146910166952, 'test/num_examples': 3003, 'score': 34470.83130598068, 'total_duration': 55763.27522611618, 'accumulated_submission_time': 34470.83130598068, 'accumulated_eval_time': 21287.758934020996, 'accumulated_logging_time': 1.3601250648498535}
I0212 03:10:24.215438 139975029864192 logging_writer.py:48] [99032] accumulated_eval_time=21287.758934, accumulated_logging_time=1.360125, accumulated_submission_time=34470.831306, global_step=99032, preemption_count=0, score=34470.831306, test/accuracy=0.706362, test/bleu=30.670147, test/loss=1.509493, test/num_examples=3003, total_duration=55763.275226, train/accuracy=0.696003, train/bleu=35.784216, train/loss=1.586832, validation/accuracy=0.691188, validation/bleu=30.653905, validation/loss=1.604953, validation/num_examples=3000
I0212 03:10:48.129807 139975038256896 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.3523164391517639, loss=3.690013885498047
I0212 03:11:22.798991 139975029864192 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.32798731327056885, loss=3.6537909507751465
I0212 03:11:57.559793 139975038256896 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.3346540331840515, loss=3.708458662033081
I0212 03:12:32.373031 139975029864192 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.36019688844680786, loss=3.6896934509277344
I0212 03:13:07.178993 139975038256896 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.3362935781478882, loss=3.665344715118408
I0212 03:13:41.978589 139975029864192 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.3306396007537842, loss=3.6775963306427
I0212 03:14:16.773109 139975038256896 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.3302522897720337, loss=3.644228935241699
I0212 03:14:51.597192 139975029864192 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.3261924982070923, loss=3.6479101181030273
I0212 03:15:26.394464 139975038256896 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.3671799898147583, loss=3.696950912475586
I0212 03:16:01.183326 139975029864192 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.34019705653190613, loss=3.6983063220977783
I0212 03:16:36.021061 139975038256896 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.35594555735588074, loss=3.711332082748413
I0212 03:17:10.830404 139975029864192 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.3475220799446106, loss=3.692997455596924
I0212 03:17:45.612791 139975038256896 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.32820817828178406, loss=3.6484899520874023
I0212 03:18:20.399179 139975029864192 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.3509273827075958, loss=3.682934045791626
I0212 03:18:55.204433 139975038256896 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3653532564640045, loss=3.6920907497406006
I0212 03:19:29.998829 139975029864192 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.3375098407268524, loss=3.6478118896484375
I0212 03:20:04.801303 139975038256896 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.3528077006340027, loss=3.6729178428649902
I0212 03:20:39.582723 139975029864192 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.35459914803504944, loss=3.680488109588623
I0212 03:21:14.362555 139975038256896 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.33972272276878357, loss=3.6895337104797363
I0212 03:21:49.149195 139975029864192 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.35185497999191284, loss=3.691431999206543
I0212 03:22:23.975422 139975038256896 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.34181082248687744, loss=3.6579084396362305
I0212 03:22:58.781741 139975029864192 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.3521709144115448, loss=3.6712536811828613
I0212 03:23:33.566126 139975038256896 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.32517462968826294, loss=3.658762216567993
I0212 03:24:08.368463 139975029864192 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.3358782231807709, loss=3.6785175800323486
I0212 03:24:24.456917 140144802662208 spec.py:321] Evaluating on the training split.
I0212 03:24:27.457916 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 03:27:18.398319 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 03:27:21.091313 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 03:29:50.035243 140144802662208 spec.py:349] Evaluating on the test split.
I0212 03:29:52.722331 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 03:32:07.647441 140144802662208 submission_runner.py:408] Time since start: 57066.74s, 	Step: 101448, 	{'train/accuracy': 0.7109413743019104, 'train/loss': 1.5052330493927002, 'train/bleu': 36.498945434107455, 'validation/accuracy': 0.6911631226539612, 'validation/loss': 1.6019834280014038, 'validation/bleu': 30.694062047415066, 'validation/num_examples': 3000, 'test/accuracy': 0.7069781422615051, 'test/loss': 1.5060288906097412, 'test/bleu': 30.958594252207657, 'test/num_examples': 3003, 'score': 35310.982800245285, 'total_duration': 57066.73641419411, 'accumulated_submission_time': 35310.982800245285, 'accumulated_eval_time': 21750.949397325516, 'accumulated_logging_time': 1.3992786407470703}
I0212 03:32:07.676168 139975038256896 logging_writer.py:48] [101448] accumulated_eval_time=21750.949397, accumulated_logging_time=1.399279, accumulated_submission_time=35310.982800, global_step=101448, preemption_count=0, score=35310.982800, test/accuracy=0.706978, test/bleu=30.958594, test/loss=1.506029, test/num_examples=3003, total_duration=57066.736414, train/accuracy=0.710941, train/bleu=36.498945, train/loss=1.505233, validation/accuracy=0.691163, validation/bleu=30.694062, validation/loss=1.601983, validation/num_examples=3000
I0212 03:32:26.057995 139975029864192 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.35055309534072876, loss=3.6611318588256836
I0212 03:33:00.729227 139975038256896 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.35172387957572937, loss=3.7304584980010986
I0212 03:33:35.485707 139975029864192 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.3486301302909851, loss=3.6797008514404297
I0212 03:34:10.256968 139975038256896 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.3476731777191162, loss=3.6510660648345947
I0212 03:34:45.066266 139975029864192 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.36721065640449524, loss=3.744429111480713
I0212 03:35:19.875494 139975038256896 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3444220721721649, loss=3.660754680633545
I0212 03:35:54.684873 139975029864192 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.3392025828361511, loss=3.633927583694458
I0212 03:36:29.484700 139975038256896 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.3455677926540375, loss=3.7059643268585205
I0212 03:37:04.298353 139975029864192 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.3555378317832947, loss=3.762360095977783
I0212 03:37:39.106012 139975038256896 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.36469826102256775, loss=3.723932981491089
I0212 03:38:14.022971 139975029864192 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3834569752216339, loss=3.6674649715423584
I0212 03:38:48.883954 139975038256896 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.3606843948364258, loss=3.6247377395629883
I0212 03:39:23.722127 139975029864192 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.35623636841773987, loss=3.634314775466919
I0212 03:39:58.513148 139975038256896 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.34528419375419617, loss=3.633723497390747
I0212 03:40:33.314552 139975029864192 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.3515860140323639, loss=3.7014718055725098
I0212 03:41:08.102163 139975038256896 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3604491055011749, loss=3.673739433288574
I0212 03:41:42.913711 139975029864192 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.3510607182979584, loss=3.63295578956604
I0212 03:42:17.714024 139975038256896 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.34055712819099426, loss=3.5859768390655518
I0212 03:42:52.516210 139975029864192 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.3365056812763214, loss=3.6199541091918945
I0212 03:43:27.306663 139975038256896 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.3523017466068268, loss=3.673238515853882
I0212 03:44:02.091388 139975029864192 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3449552059173584, loss=3.6559624671936035
I0212 03:44:37.041892 139975038256896 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.36284101009368896, loss=3.653296947479248
I0212 03:45:11.865278 139975029864192 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.35758641362190247, loss=3.6769375801086426
I0212 03:45:46.644738 139975038256896 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.35681137442588806, loss=3.66762375831604
I0212 03:46:07.943717 140144802662208 spec.py:321] Evaluating on the training split.
I0212 03:46:10.930888 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 03:49:00.504615 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 03:49:03.223336 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 03:51:30.886456 140144802662208 spec.py:349] Evaluating on the test split.
I0212 03:51:33.581129 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 03:53:52.186023 140144802662208 submission_runner.py:408] Time since start: 58371.28s, 	Step: 103863, 	{'train/accuracy': 0.6996182203292847, 'train/loss': 1.565675139427185, 'train/bleu': 36.38101795307617, 'validation/accuracy': 0.6910267472267151, 'validation/loss': 1.6026326417922974, 'validation/bleu': 30.47832296179434, 'validation/num_examples': 3000, 'test/accuracy': 0.7081633806228638, 'test/loss': 1.5044052600860596, 'test/bleu': 30.9335101099981, 'test/num_examples': 3003, 'score': 36151.15802383423, 'total_duration': 58371.27501010895, 'accumulated_submission_time': 36151.15802383423, 'accumulated_eval_time': 22215.191648960114, 'accumulated_logging_time': 1.43782639503479}
I0212 03:53:52.215425 139975029864192 logging_writer.py:48] [103863] accumulated_eval_time=22215.191649, accumulated_logging_time=1.437826, accumulated_submission_time=36151.158024, global_step=103863, preemption_count=0, score=36151.158024, test/accuracy=0.708163, test/bleu=30.933510, test/loss=1.504405, test/num_examples=3003, total_duration=58371.275010, train/accuracy=0.699618, train/bleu=36.381018, train/loss=1.565675, validation/accuracy=0.691027, validation/bleu=30.478323, validation/loss=1.602633, validation/num_examples=3000
I0212 03:54:05.408552 139975038256896 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.3586962819099426, loss=3.712955951690674
I0212 03:54:40.094450 139975029864192 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.37462031841278076, loss=3.676192045211792
I0212 03:55:14.859009 139975038256896 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.3773460388183594, loss=3.678281307220459
I0212 03:55:49.643144 139975029864192 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.3598851263523102, loss=3.6872997283935547
I0212 03:56:24.465531 139975038256896 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.36566320061683655, loss=3.7013652324676514
I0212 03:56:59.302892 139975029864192 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.3442537486553192, loss=3.6347601413726807
I0212 03:57:34.090082 139975038256896 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3546195924282074, loss=3.6942386627197266
I0212 03:58:08.913044 139975029864192 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.36096563935279846, loss=3.6522607803344727
I0212 03:58:43.735695 139975038256896 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.3606024980545044, loss=3.677295684814453
I0212 03:59:18.523918 139975029864192 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.3573038876056671, loss=3.6955368518829346
I0212 03:59:53.306522 139975038256896 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.3613652288913727, loss=3.6698005199432373
I0212 04:00:28.106282 139975029864192 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.3743220269680023, loss=3.700798988342285
I0212 04:01:02.905999 139975038256896 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.3458092510700226, loss=3.6694188117980957
I0212 04:01:37.701367 139975029864192 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.3600982427597046, loss=3.704944372177124
I0212 04:02:12.498322 139975038256896 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.3621063530445099, loss=3.6320080757141113
I0212 04:02:47.303421 139975029864192 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.34195148944854736, loss=3.6319832801818848
I0212 04:03:22.123953 139975038256896 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.3466029465198517, loss=3.6399753093719482
I0212 04:03:56.943367 139975029864192 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.38386157155036926, loss=3.684638738632202
I0212 04:04:31.796231 139975038256896 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.3503563106060028, loss=3.6556835174560547
I0212 04:05:06.634940 139975029864192 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.37305790185928345, loss=3.692112445831299
I0212 04:05:41.483247 139975038256896 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.35000666975975037, loss=3.6632187366485596
I0212 04:06:16.290925 139975029864192 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.3383578062057495, loss=3.648942708969116
I0212 04:06:51.107670 139975038256896 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.3647840917110443, loss=3.7275500297546387
I0212 04:07:25.921011 139975029864192 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.3484916090965271, loss=3.6887073516845703
I0212 04:07:52.422006 140144802662208 spec.py:321] Evaluating on the training split.
I0212 04:07:55.401277 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 04:10:52.833758 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 04:10:55.526230 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 04:13:23.072144 140144802662208 spec.py:349] Evaluating on the test split.
I0212 04:13:25.763065 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 04:15:44.293587 140144802662208 submission_runner.py:408] Time since start: 59683.38s, 	Step: 106278, 	{'train/accuracy': 0.7071945071220398, 'train/loss': 1.5247044563293457, 'train/bleu': 35.687987250436876, 'validation/accuracy': 0.6906920075416565, 'validation/loss': 1.6015530824661255, 'validation/bleu': 30.56966933807441, 'validation/num_examples': 3000, 'test/accuracy': 0.7085352540016174, 'test/loss': 1.5015463829040527, 'test/bleu': 30.860647082755722, 'test/num_examples': 3003, 'score': 36991.27319264412, 'total_duration': 59683.38256788254, 'accumulated_submission_time': 36991.27319264412, 'accumulated_eval_time': 22687.063174962997, 'accumulated_logging_time': 1.4773857593536377}
I0212 04:15:44.324113 139975038256896 logging_writer.py:48] [106278] accumulated_eval_time=22687.063175, accumulated_logging_time=1.477386, accumulated_submission_time=36991.273193, global_step=106278, preemption_count=0, score=36991.273193, test/accuracy=0.708535, test/bleu=30.860647, test/loss=1.501546, test/num_examples=3003, total_duration=59683.382568, train/accuracy=0.707195, train/bleu=35.687987, train/loss=1.524704, validation/accuracy=0.690692, validation/bleu=30.569669, validation/loss=1.601553, validation/num_examples=3000
I0212 04:15:52.310687 139975029864192 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.3777695298194885, loss=3.6963534355163574
I0212 04:16:26.993319 139975038256896 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.3625909984111786, loss=3.6661007404327393
I0212 04:17:01.741951 139975029864192 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.37286436557769775, loss=3.6563940048217773
I0212 04:17:36.667650 139975038256896 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.3919318616390228, loss=3.6296164989471436
I0212 04:18:11.492995 139975029864192 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.3737045228481293, loss=3.655233383178711
I0212 04:18:46.297735 139975038256896 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.36124417185783386, loss=3.6867549419403076
I0212 04:19:21.102410 139975029864192 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.37433958053588867, loss=3.640254020690918
I0212 04:19:55.892268 139975038256896 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.36248740553855896, loss=3.722632884979248
I0212 04:20:30.695410 139975029864192 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.3749926686286926, loss=3.621368169784546
I0212 04:21:05.486516 139975038256896 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.35227498412132263, loss=3.628634452819824
I0212 04:21:40.275551 139975029864192 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.35941818356513977, loss=3.6404035091400146
I0212 04:22:15.071223 139975038256896 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.3705650866031647, loss=3.6905245780944824
I0212 04:22:49.861551 139975029864192 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.3822328448295593, loss=3.6371850967407227
I0212 04:23:24.670708 139975038256896 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.35818395018577576, loss=3.5900235176086426
I0212 04:23:59.487819 139975029864192 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.37594884634017944, loss=3.6729817390441895
I0212 04:24:34.307730 139975038256896 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.36394017934799194, loss=3.635715961456299
I0212 04:25:09.134627 139975029864192 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.3662615418434143, loss=3.627218723297119
I0212 04:25:43.938850 139975038256896 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.3668046295642853, loss=3.6915462017059326
I0212 04:26:18.723405 139975029864192 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.3796949088573456, loss=3.6579768657684326
I0212 04:26:53.512460 139975038256896 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.4041399359703064, loss=3.6875972747802734
I0212 04:27:28.303556 139975029864192 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.3568074703216553, loss=3.675389528274536
I0212 04:28:03.102151 139975038256896 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.36858025193214417, loss=3.628817558288574
I0212 04:28:37.899392 139975029864192 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.377296507358551, loss=3.661789655685425
I0212 04:29:12.819986 139975038256896 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.37062785029411316, loss=3.647902488708496
I0212 04:29:44.587021 140144802662208 spec.py:321] Evaluating on the training split.
I0212 04:29:47.580276 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 04:32:41.428437 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 04:32:44.120551 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 04:35:12.101773 140144802662208 spec.py:349] Evaluating on the test split.
I0212 04:35:14.787282 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 04:37:34.708055 140144802662208 submission_runner.py:408] Time since start: 60993.80s, 	Step: 108693, 	{'train/accuracy': 0.7097283601760864, 'train/loss': 1.5101783275604248, 'train/bleu': 36.4322003452034, 'validation/accuracy': 0.691696286201477, 'validation/loss': 1.6009178161621094, 'validation/bleu': 30.72172265147943, 'validation/num_examples': 3000, 'test/accuracy': 0.7084887623786926, 'test/loss': 1.4992235898971558, 'test/bleu': 30.698081478592425, 'test/num_examples': 3003, 'score': 37831.44410777092, 'total_duration': 60993.797033786774, 'accumulated_submission_time': 37831.44410777092, 'accumulated_eval_time': 23157.18415951729, 'accumulated_logging_time': 1.517770767211914}
I0212 04:37:34.738745 139975029864192 logging_writer.py:48] [108693] accumulated_eval_time=23157.184160, accumulated_logging_time=1.517771, accumulated_submission_time=37831.444108, global_step=108693, preemption_count=0, score=37831.444108, test/accuracy=0.708489, test/bleu=30.698081, test/loss=1.499224, test/num_examples=3003, total_duration=60993.797034, train/accuracy=0.709728, train/bleu=36.432200, train/loss=1.510178, validation/accuracy=0.691696, validation/bleu=30.721723, validation/loss=1.600918, validation/num_examples=3000
I0212 04:37:37.537153 139975038256896 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.34975045919418335, loss=3.616518259048462
I0212 04:38:12.205312 139975029864192 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3452339172363281, loss=3.599228858947754
I0212 04:38:46.914016 139975038256896 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.34616655111312866, loss=3.6189966201782227
I0212 04:39:21.686654 139975029864192 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.38966041803359985, loss=3.6964151859283447
I0212 04:39:56.480541 139975038256896 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3683527708053589, loss=3.635101795196533
I0212 04:40:31.261613 139975029864192 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.37794366478919983, loss=3.648739814758301
I0212 04:41:06.057642 139975038256896 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3789966404438019, loss=3.6144471168518066
I0212 04:41:40.862993 139975029864192 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.3661394715309143, loss=3.5853137969970703
I0212 04:42:15.633716 139975038256896 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.38673073053359985, loss=3.6593408584594727
I0212 04:42:50.444519 139975029864192 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.37362799048423767, loss=3.6256892681121826
I0212 04:43:25.236090 139975038256896 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.39946165680885315, loss=3.6405630111694336
I0212 04:44:00.028802 139975029864192 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.40447089076042175, loss=3.6201183795928955
I0212 04:44:34.842325 139975038256896 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.3843892216682434, loss=3.6650640964508057
I0212 04:45:09.623921 139975029864192 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.3548954725265503, loss=3.5969552993774414
I0212 04:45:44.408045 139975038256896 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.36314645409584045, loss=3.653674840927124
I0212 04:46:19.181829 139975029864192 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.3684349060058594, loss=3.67822003364563
I0212 04:46:54.002424 139975038256896 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.38400503993034363, loss=3.604145050048828
I0212 04:47:28.786166 139975029864192 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.35934609174728394, loss=3.631091356277466
I0212 04:48:03.593889 139975038256896 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3629513382911682, loss=3.6328628063201904
I0212 04:48:38.393741 139975029864192 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.3723728358745575, loss=3.5824360847473145
I0212 04:49:13.194226 139975038256896 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.369836688041687, loss=3.621213912963867
I0212 04:49:48.003957 139975029864192 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.3687172830104828, loss=3.659566879272461
I0212 04:50:22.800064 139975038256896 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.3936476707458496, loss=3.6822400093078613
I0212 04:50:57.564505 139975029864192 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.3899359405040741, loss=3.6737537384033203
I0212 04:51:32.369905 139975038256896 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.37522849440574646, loss=3.6681063175201416
I0212 04:51:34.882388 140144802662208 spec.py:321] Evaluating on the training split.
I0212 04:51:37.869162 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 04:54:32.031054 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 04:54:34.743481 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 04:57:02.662847 140144802662208 spec.py:349] Evaluating on the test split.
I0212 04:57:05.384682 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 04:59:24.990152 140144802662208 submission_runner.py:408] Time since start: 62304.08s, 	Step: 111109, 	{'train/accuracy': 0.7096648812294006, 'train/loss': 1.506799578666687, 'train/bleu': 36.62844563646342, 'validation/accuracy': 0.6907168030738831, 'validation/loss': 1.6045445203781128, 'validation/bleu': 30.522907929975723, 'validation/num_examples': 3000, 'test/accuracy': 0.7071989178657532, 'test/loss': 1.5012623071670532, 'test/bleu': 30.854497798748653, 'test/num_examples': 3003, 'score': 38671.495940208435, 'total_duration': 62304.079092502594, 'accumulated_submission_time': 38671.495940208435, 'accumulated_eval_time': 23627.291821718216, 'accumulated_logging_time': 1.559746265411377}
I0212 04:59:25.025863 139975029864192 logging_writer.py:48] [111109] accumulated_eval_time=23627.291822, accumulated_logging_time=1.559746, accumulated_submission_time=38671.495940, global_step=111109, preemption_count=0, score=38671.495940, test/accuracy=0.707199, test/bleu=30.854498, test/loss=1.501262, test/num_examples=3003, total_duration=62304.079093, train/accuracy=0.709665, train/bleu=36.628446, train/loss=1.506800, validation/accuracy=0.690717, validation/bleu=30.522908, validation/loss=1.604545, validation/num_examples=3000
I0212 04:59:56.947537 139975038256896 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.37389466166496277, loss=3.6396331787109375
I0212 05:00:31.680457 139975029864192 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.3708535134792328, loss=3.6148948669433594
I0212 05:01:06.486551 139975038256896 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.39834803342819214, loss=3.65142560005188
I0212 05:01:41.333801 139975029864192 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.3926088511943817, loss=3.6678125858306885
I0212 05:02:16.266178 139975038256896 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.39266565442085266, loss=3.6436305046081543
I0212 05:02:51.112413 139975029864192 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3859667479991913, loss=3.6532182693481445
I0212 05:03:25.935983 139975038256896 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.3616107404232025, loss=3.596621036529541
I0212 05:04:00.717847 139975029864192 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.3688213527202606, loss=3.649876117706299
I0212 05:04:35.519583 139975038256896 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.40112289786338806, loss=3.5917861461639404
I0212 05:05:10.323643 139975029864192 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.39046019315719604, loss=3.6009750366210938
I0212 05:05:45.112526 139975038256896 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.38559091091156006, loss=3.7055091857910156
I0212 05:06:19.922542 139975029864192 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.40800654888153076, loss=3.664607286453247
I0212 05:06:54.757074 139975038256896 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.38119596242904663, loss=3.5974481105804443
I0212 05:07:29.549898 139975029864192 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.3795146644115448, loss=3.6300723552703857
I0212 05:08:04.375159 139975038256896 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.3811201751232147, loss=3.641428232192993
I0212 05:08:39.178297 139975029864192 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.3592619299888611, loss=3.59586238861084
I0212 05:09:14.024039 139975038256896 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.3710399270057678, loss=3.6169512271881104
I0212 05:09:48.856069 139975029864192 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3965224027633667, loss=3.636751174926758
I0212 05:10:23.695524 139975038256896 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.3766348659992218, loss=3.6588521003723145
I0212 05:10:58.484942 139975029864192 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.38493382930755615, loss=3.593501329421997
I0212 05:11:33.292217 139975038256896 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.3746413588523865, loss=3.658694267272949
I0212 05:12:08.104337 139975029864192 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.3941596448421478, loss=3.686702251434326
I0212 05:12:42.919626 139975038256896 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3899572789669037, loss=3.643629789352417
I0212 05:13:17.719175 139975029864192 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.37402161955833435, loss=3.6362600326538086
I0212 05:13:25.089821 140144802662208 spec.py:321] Evaluating on the training split.
I0212 05:13:28.064138 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 05:16:11.373705 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 05:16:14.067662 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 05:18:40.147527 140144802662208 spec.py:349] Evaluating on the test split.
I0212 05:18:42.839428 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 05:21:05.819514 140144802662208 submission_runner.py:408] Time since start: 63604.91s, 	Step: 113523, 	{'train/accuracy': 0.7205490469932556, 'train/loss': 1.4559381008148193, 'train/bleu': 37.3473881454824, 'validation/accuracy': 0.6923410892486572, 'validation/loss': 1.5988622903823853, 'validation/bleu': 30.747762382036655, 'validation/num_examples': 3000, 'test/accuracy': 0.7090232968330383, 'test/loss': 1.4939895868301392, 'test/bleu': 30.819914092900255, 'test/num_examples': 3003, 'score': 39511.46545481682, 'total_duration': 63604.90850496292, 'accumulated_submission_time': 39511.46545481682, 'accumulated_eval_time': 24088.021463871002, 'accumulated_logging_time': 1.606471061706543}
I0212 05:21:05.850201 139975038256896 logging_writer.py:48] [113523] accumulated_eval_time=24088.021464, accumulated_logging_time=1.606471, accumulated_submission_time=39511.465455, global_step=113523, preemption_count=0, score=39511.465455, test/accuracy=0.709023, test/bleu=30.819914, test/loss=1.493990, test/num_examples=3003, total_duration=63604.908505, train/accuracy=0.720549, train/bleu=37.347388, train/loss=1.455938, validation/accuracy=0.692341, validation/bleu=30.747762, validation/loss=1.598862, validation/num_examples=3000
I0212 05:21:32.904954 139975029864192 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.37910524010658264, loss=3.6709794998168945
I0212 05:22:07.615515 139975038256896 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.39345380663871765, loss=3.678063154220581
I0212 05:22:42.427645 139975029864192 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.3822517395019531, loss=3.617246150970459
I0212 05:23:17.240638 139975038256896 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.3804619312286377, loss=3.689146041870117
I0212 05:23:52.030483 139975029864192 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.38395610451698303, loss=3.6051433086395264
I0212 05:24:26.829069 139975038256896 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.37196141481399536, loss=3.5824344158172607
I0212 05:25:01.594015 139975029864192 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.3819085955619812, loss=3.5833096504211426
I0212 05:25:36.362691 139975038256896 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.3999238908290863, loss=3.653311014175415
I0212 05:26:11.172374 139975029864192 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.37626248598098755, loss=3.623565196990967
I0212 05:26:45.973159 139975038256896 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.36821168661117554, loss=3.6184754371643066
I0212 05:27:20.771335 139975029864192 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.3829052746295929, loss=3.637453079223633
I0212 05:27:55.578465 139975038256896 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.39122655987739563, loss=3.672281265258789
I0212 05:28:30.384267 139975029864192 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.3791547417640686, loss=3.5966930389404297
I0212 05:29:05.183056 139975038256896 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.381626695394516, loss=3.628573417663574
I0212 05:29:39.981698 139975029864192 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.3913858234882355, loss=3.62511944770813
I0212 05:30:14.766311 139975038256896 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.3928513824939728, loss=3.6230087280273438
I0212 05:30:49.575773 139975029864192 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.3805292546749115, loss=3.6006054878234863
I0212 05:31:24.370659 139975038256896 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.3926094174385071, loss=3.647052526473999
I0212 05:31:59.190524 139975029864192 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.3760213851928711, loss=3.6318774223327637
I0212 05:32:33.977663 139975038256896 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.38312453031539917, loss=3.6074464321136475
I0212 05:33:08.766928 139975029864192 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.40303540229797363, loss=3.6178841590881348
I0212 05:33:43.568260 139975038256896 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.37824809551239014, loss=3.636631965637207
I0212 05:34:18.353713 139975029864192 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.37962761521339417, loss=3.602203845977783
I0212 05:34:53.170283 139975038256896 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.3724135458469391, loss=3.59498929977417
I0212 05:35:06.114269 140144802662208 spec.py:321] Evaluating on the training split.
I0212 05:35:09.094043 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 05:38:04.155905 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 05:38:06.844379 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 05:40:29.788325 140144802662208 spec.py:349] Evaluating on the test split.
I0212 05:40:32.489882 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 05:42:51.125070 140144802662208 submission_runner.py:408] Time since start: 64910.21s, 	Step: 115939, 	{'train/accuracy': 0.7139232158660889, 'train/loss': 1.487220048904419, 'train/bleu': 36.63052469599194, 'validation/accuracy': 0.6912499666213989, 'validation/loss': 1.601006269454956, 'validation/bleu': 30.716334354820788, 'validation/num_examples': 3000, 'test/accuracy': 0.7080239653587341, 'test/loss': 1.49728524684906, 'test/bleu': 30.801907041652694, 'test/num_examples': 3003, 'score': 40351.63733792305, 'total_duration': 64910.21405529976, 'accumulated_submission_time': 40351.63733792305, 'accumulated_eval_time': 24553.032210111618, 'accumulated_logging_time': 1.6485328674316406}
I0212 05:42:51.156859 139975029864192 logging_writer.py:48] [115939] accumulated_eval_time=24553.032210, accumulated_logging_time=1.648533, accumulated_submission_time=40351.637338, global_step=115939, preemption_count=0, score=40351.637338, test/accuracy=0.708024, test/bleu=30.801907, test/loss=1.497285, test/num_examples=3003, total_duration=64910.214055, train/accuracy=0.713923, train/bleu=36.630525, train/loss=1.487220, validation/accuracy=0.691250, validation/bleu=30.716334, validation/loss=1.601006, validation/num_examples=3000
I0212 05:43:12.680160 139975038256896 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.4005073606967926, loss=3.647921085357666
I0212 05:43:47.387531 139975029864192 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.365621954202652, loss=3.583254098892212
I0212 05:44:22.145597 139975038256896 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.3894137442111969, loss=3.612863779067993
I0212 05:44:56.953164 139975029864192 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.37472766637802124, loss=3.580772876739502
I0212 05:45:31.744000 139975038256896 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.39636391401290894, loss=3.5969858169555664
I0212 05:46:06.546191 139975029864192 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.39687758684158325, loss=3.6649932861328125
I0212 05:46:41.364421 139975038256896 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.39183422923088074, loss=3.6543123722076416
I0212 05:47:16.158004 139975029864192 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.3866269290447235, loss=3.600257635116577
I0212 05:47:50.954015 139975038256896 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.38108620047569275, loss=3.6712002754211426
I0212 05:48:25.754035 139975029864192 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.37723368406295776, loss=3.6609983444213867
I0212 05:49:00.562940 139975038256896 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3877696692943573, loss=3.6608195304870605
I0212 05:49:35.348884 139975029864192 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.3769322633743286, loss=3.598268508911133
I0212 05:50:10.135646 139975038256896 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.4039076864719391, loss=3.6429102420806885
I0212 05:50:44.916200 139975029864192 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.3751085698604584, loss=3.5781071186065674
I0212 05:51:19.715387 139975038256896 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.4021349847316742, loss=3.6883418560028076
I0212 05:51:54.513518 139975029864192 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.4026496112346649, loss=3.648543357849121
I0212 05:52:29.332275 139975038256896 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.3867942988872528, loss=3.5908663272857666
I0212 05:53:04.139068 139975029864192 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.41303664445877075, loss=3.705756187438965
I0212 05:53:38.952749 139975038256896 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.3950292766094208, loss=3.6330392360687256
I0212 05:54:13.725370 139975029864192 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.38167494535446167, loss=3.5922703742980957
I0212 05:54:48.523379 139975038256896 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.38443994522094727, loss=3.5699799060821533
I0212 05:55:23.315624 139975029864192 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.37848126888275146, loss=3.577960968017578
I0212 05:55:58.123381 139975038256896 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.37266483902931213, loss=3.616175651550293
I0212 05:56:32.924798 139975029864192 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.3977440595626831, loss=3.6198480129241943
I0212 05:56:51.432645 140144802662208 spec.py:321] Evaluating on the training split.
I0212 05:56:54.417832 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 05:59:39.751758 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 05:59:42.429336 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 06:02:10.168544 140144802662208 spec.py:349] Evaluating on the test split.
I0212 06:02:12.869784 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 06:04:30.819551 140144802662208 submission_runner.py:408] Time since start: 66209.91s, 	Step: 118355, 	{'train/accuracy': 0.7143579721450806, 'train/loss': 1.484772801399231, 'train/bleu': 37.03462724545019, 'validation/accuracy': 0.6923038959503174, 'validation/loss': 1.6000486612319946, 'validation/bleu': 30.58042947513172, 'validation/num_examples': 3000, 'test/accuracy': 0.7077218294143677, 'test/loss': 1.4967893362045288, 'test/bleu': 30.703935636601802, 'test/num_examples': 3003, 'score': 41191.82350349426, 'total_duration': 66209.90850758553, 'accumulated_submission_time': 41191.82350349426, 'accumulated_eval_time': 25012.41903567314, 'accumulated_logging_time': 1.6906397342681885}
I0212 06:04:30.857058 139975038256896 logging_writer.py:48] [118355] accumulated_eval_time=25012.419036, accumulated_logging_time=1.690640, accumulated_submission_time=41191.823503, global_step=118355, preemption_count=0, score=41191.823503, test/accuracy=0.707722, test/bleu=30.703936, test/loss=1.496789, test/num_examples=3003, total_duration=66209.908508, train/accuracy=0.714358, train/bleu=37.034627, train/loss=1.484773, validation/accuracy=0.692304, validation/bleu=30.580429, validation/loss=1.600049, validation/num_examples=3000
I0212 06:04:46.835341 139975029864192 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.36815693974494934, loss=3.610182523727417
I0212 06:05:21.558496 139975038256896 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.39340728521347046, loss=3.61299467086792
I0212 06:05:56.301860 139975029864192 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.405657023191452, loss=3.649456262588501
I0212 06:06:31.238096 139975038256896 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.4062507152557373, loss=3.670567035675049
I0212 06:07:06.054002 139975029864192 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.4023737609386444, loss=3.599881172180176
I0212 06:07:40.837593 139975038256896 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.38446834683418274, loss=3.6147313117980957
I0212 06:08:15.629860 139975029864192 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3936723470687866, loss=3.6039984226226807
I0212 06:08:50.436154 139975038256896 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.3939928114414215, loss=3.5911478996276855
I0212 06:09:25.231132 139975029864192 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.4106070399284363, loss=3.5751595497131348
I0212 06:10:00.032060 139975038256896 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.3721480965614319, loss=3.580449342727661
I0212 06:10:34.870501 139975029864192 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.40632808208465576, loss=3.5852179527282715
I0212 06:11:09.680267 139975038256896 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.3933352828025818, loss=3.575183868408203
I0212 06:11:44.485193 139975029864192 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.39288052916526794, loss=3.58217453956604
I0212 06:12:19.386204 139975038256896 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.3870195150375366, loss=3.6250503063201904
I0212 06:12:54.167818 139975029864192 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.39742207527160645, loss=3.6000795364379883
I0212 06:13:28.954956 139975038256896 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.40223369002342224, loss=3.584911584854126
I0212 06:14:03.734333 139975029864192 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.38444745540618896, loss=3.601289987564087
I0212 06:14:38.538579 139975038256896 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.40419164299964905, loss=3.6560704708099365
I0212 06:15:13.419669 139975029864192 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.41911211609840393, loss=3.6252474784851074
I0212 06:15:48.230985 139975038256896 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.3874189853668213, loss=3.5959854125976562
I0212 06:16:23.026911 139975029864192 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.4054662585258484, loss=3.6503195762634277
I0212 06:16:57.827878 139975038256896 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.3798835575580597, loss=3.5899248123168945
I0212 06:17:32.643316 139975029864192 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.3902008831501007, loss=3.6282002925872803
I0212 06:18:07.573818 139975038256896 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.36716359853744507, loss=3.5770318508148193
I0212 06:18:30.980076 140144802662208 spec.py:321] Evaluating on the training split.
I0212 06:18:33.970234 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 06:21:32.818543 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 06:21:35.515743 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 06:24:03.624863 140144802662208 spec.py:349] Evaluating on the test split.
I0212 06:24:06.318277 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 06:26:23.394566 140144802662208 submission_runner.py:408] Time since start: 67522.48s, 	Step: 120769, 	{'train/accuracy': 0.7247974872589111, 'train/loss': 1.4364838600158691, 'train/bleu': 37.53693442719608, 'validation/accuracy': 0.6926262378692627, 'validation/loss': 1.5970616340637207, 'validation/bleu': 30.67829971443381, 'validation/num_examples': 3000, 'test/accuracy': 0.7095927000045776, 'test/loss': 1.4932905435562134, 'test/bleu': 30.727894032551443, 'test/num_examples': 3003, 'score': 42031.85128903389, 'total_duration': 67522.48354244232, 'accumulated_submission_time': 42031.85128903389, 'accumulated_eval_time': 25484.83347582817, 'accumulated_logging_time': 1.7393221855163574}
I0212 06:26:23.427086 139975029864192 logging_writer.py:48] [120769] accumulated_eval_time=25484.833476, accumulated_logging_time=1.739322, accumulated_submission_time=42031.851289, global_step=120769, preemption_count=0, score=42031.851289, test/accuracy=0.709593, test/bleu=30.727894, test/loss=1.493291, test/num_examples=3003, total_duration=67522.483542, train/accuracy=0.724797, train/bleu=37.536934, train/loss=1.436484, validation/accuracy=0.692626, validation/bleu=30.678300, validation/loss=1.597062, validation/num_examples=3000
I0212 06:26:34.535994 139975038256896 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.36966952681541443, loss=3.5772457122802734
I0212 06:27:09.241686 139975029864192 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.38760289549827576, loss=3.5901741981506348
I0212 06:27:44.015432 139975038256896 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.3852715790271759, loss=3.589911937713623
I0212 06:28:18.818423 139975029864192 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.41715389490127563, loss=3.5974643230438232
I0212 06:28:53.604806 139975038256896 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.39802417159080505, loss=3.6085216999053955
I0212 06:29:28.420014 139975029864192 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.3715839087963104, loss=3.62448787689209
I0212 06:30:03.231956 139975038256896 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.3934652507305145, loss=3.6242189407348633
I0212 06:30:38.034712 139975029864192 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.3928433358669281, loss=3.631061553955078
I0212 06:31:12.832752 139975038256896 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.37228652834892273, loss=3.615349531173706
I0212 06:31:47.646076 139975029864192 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.3995378613471985, loss=3.5849270820617676
I0212 06:32:22.442673 139975038256896 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.4097172021865845, loss=3.6564157009124756
I0212 06:32:57.226049 139975029864192 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.3970606029033661, loss=3.6406853199005127
I0212 06:33:32.032424 139975038256896 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.391165167093277, loss=3.606565237045288
I0212 06:34:06.845853 139975029864192 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.4003787934780121, loss=3.5972070693969727
I0212 06:34:41.654366 139975038256896 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.41206124424934387, loss=3.6655194759368896
I0212 06:35:16.435675 139975029864192 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.38499516248703003, loss=3.6094086170196533
I0212 06:35:51.240478 139975038256896 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.38066136837005615, loss=3.6127572059631348
I0212 06:36:26.061761 139975029864192 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.4020645022392273, loss=3.594897747039795
I0212 06:37:00.855197 139975038256896 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.40620970726013184, loss=3.6097054481506348
I0212 06:37:35.650735 139975029864192 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.4166083037853241, loss=3.6529581546783447
I0212 06:38:10.453174 139975038256896 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.403615266084671, loss=3.5918285846710205
I0212 06:38:45.232942 139975029864192 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.3819337487220764, loss=3.571624755859375
I0212 06:39:20.027602 139975038256896 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.4035996198654175, loss=3.6116578578948975
I0212 06:39:54.802626 139975029864192 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.3819979727268219, loss=3.6121530532836914
I0212 06:40:23.419521 140144802662208 spec.py:321] Evaluating on the training split.
I0212 06:40:26.408825 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 06:43:18.326092 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 06:43:21.014539 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 06:45:45.180746 140144802662208 spec.py:349] Evaluating on the test split.
I0212 06:45:47.878828 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 06:48:03.782982 140144802662208 submission_runner.py:408] Time since start: 68822.87s, 	Step: 123184, 	{'train/accuracy': 0.7164363265037537, 'train/loss': 1.4766035079956055, 'train/bleu': 37.23399471055593, 'validation/accuracy': 0.6918575167655945, 'validation/loss': 1.5992780923843384, 'validation/bleu': 30.603763521386373, 'validation/num_examples': 3000, 'test/accuracy': 0.7095230221748352, 'test/loss': 1.493468999862671, 'test/bleu': 30.812469473793247, 'test/num_examples': 3003, 'score': 42871.75427532196, 'total_duration': 68822.87196969986, 'accumulated_submission_time': 42871.75427532196, 'accumulated_eval_time': 25945.196885108948, 'accumulated_logging_time': 1.7829713821411133}
I0212 06:48:03.816338 139975038256896 logging_writer.py:48] [123184] accumulated_eval_time=25945.196885, accumulated_logging_time=1.782971, accumulated_submission_time=42871.754275, global_step=123184, preemption_count=0, score=42871.754275, test/accuracy=0.709523, test/bleu=30.812469, test/loss=1.493469, test/num_examples=3003, total_duration=68822.871970, train/accuracy=0.716436, train/bleu=37.233995, train/loss=1.476604, validation/accuracy=0.691858, validation/bleu=30.603764, validation/loss=1.599278, validation/num_examples=3000
I0212 06:48:09.721019 139975029864192 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.39685511589050293, loss=3.5655643939971924
I0212 06:48:44.406091 139975038256896 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.38645488023757935, loss=3.5918753147125244
I0212 06:49:19.155954 139975029864192 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.41454461216926575, loss=3.6394829750061035
I0212 06:49:53.990775 139975038256896 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.3947809338569641, loss=3.62526273727417
I0212 06:50:28.826777 139975029864192 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.3946309983730316, loss=3.6415698528289795
I0212 06:51:03.739179 139975038256896 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.3964863717556, loss=3.620406150817871
I0212 06:51:38.549226 139975029864192 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.3751402795314789, loss=3.5677993297576904
I0212 06:52:13.332597 139975038256896 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.39156317710876465, loss=3.6154744625091553
I0212 06:52:48.149545 139975029864192 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.39677149057388306, loss=3.609747886657715
I0212 06:53:22.992470 139975038256896 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.41275522112846375, loss=3.6189658641815186
I0212 06:53:57.805088 139975029864192 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.39831358194351196, loss=3.592777967453003
I0212 06:54:32.640232 139975038256896 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.4016544818878174, loss=3.6137197017669678
I0212 06:55:07.436784 139975029864192 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.3913886249065399, loss=3.583071708679199
I0212 06:55:42.245917 139975038256896 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.3703889846801758, loss=3.587618350982666
I0212 06:56:17.054750 139975029864192 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.39746031165122986, loss=3.6286590099334717
I0212 06:56:51.938358 139975038256896 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.38332727551460266, loss=3.6002731323242188
I0212 06:57:26.749808 139975029864192 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.40443944931030273, loss=3.581547975540161
I0212 06:58:01.536554 139975038256896 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.4074397087097168, loss=3.6086580753326416
I0212 06:58:36.322713 139975029864192 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.39903783798217773, loss=3.634096622467041
I0212 06:59:11.115564 139975038256896 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.39929723739624023, loss=3.6295969486236572
I0212 06:59:45.974504 139975029864192 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.391624391078949, loss=3.5893261432647705
I0212 07:00:20.805067 139975038256896 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.38100549578666687, loss=3.545964002609253
I0212 07:00:55.592496 139975029864192 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.4028390645980835, loss=3.6193230152130127
I0212 07:01:30.442703 139975038256896 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.3911144733428955, loss=3.5900843143463135
I0212 07:02:03.922068 140144802662208 spec.py:321] Evaluating on the training split.
I0212 07:02:06.895732 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 07:05:01.328862 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 07:05:04.045794 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 07:07:31.057195 140144802662208 spec.py:349] Evaluating on the test split.
I0212 07:07:33.741828 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 07:09:52.161435 140144802662208 submission_runner.py:408] Time since start: 70131.25s, 	Step: 125598, 	{'train/accuracy': 0.7242320775985718, 'train/loss': 1.4370570182800293, 'train/bleu': 37.27877258251524, 'validation/accuracy': 0.6923782825469971, 'validation/loss': 1.598088026046753, 'validation/bleu': 30.46979271110354, 'validation/num_examples': 3000, 'test/accuracy': 0.7091279029846191, 'test/loss': 1.4928165674209595, 'test/bleu': 30.822391857216715, 'test/num_examples': 3003, 'score': 43711.76387667656, 'total_duration': 70131.25042271614, 'accumulated_submission_time': 43711.76387667656, 'accumulated_eval_time': 26413.436207294464, 'accumulated_logging_time': 1.8277215957641602}
I0212 07:09:52.194233 139975029864192 logging_writer.py:48] [125598] accumulated_eval_time=26413.436207, accumulated_logging_time=1.827722, accumulated_submission_time=43711.763877, global_step=125598, preemption_count=0, score=43711.763877, test/accuracy=0.709128, test/bleu=30.822392, test/loss=1.492817, test/num_examples=3003, total_duration=70131.250423, train/accuracy=0.724232, train/bleu=37.278773, train/loss=1.437057, validation/accuracy=0.692378, validation/bleu=30.469793, validation/loss=1.598088, validation/num_examples=3000
I0212 07:09:53.256883 139975038256896 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.40571168065071106, loss=3.6265599727630615
I0212 07:10:27.959333 139975029864192 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.4183514416217804, loss=3.609044313430786
I0212 07:11:02.705995 139975038256896 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.39005759358406067, loss=3.6121697425842285
I0212 07:11:37.515341 139975029864192 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.39412248134613037, loss=3.5726850032806396
I0212 07:12:12.298541 139975038256896 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.3954097330570221, loss=3.5884053707122803
I0212 07:12:47.069963 139975029864192 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.38805946707725525, loss=3.6168012619018555
I0212 07:13:21.865950 139975038256896 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.40578213334083557, loss=3.5983643531799316
I0212 07:13:56.652006 139975029864192 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.388018399477005, loss=3.5625486373901367
I0212 07:14:31.465932 139975038256896 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.3780747354030609, loss=3.576418399810791
I0212 07:15:06.299490 139975029864192 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.4026717245578766, loss=3.624547243118286
I0212 07:15:41.085190 139975038256896 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.40090107917785645, loss=3.6189677715301514
I0212 07:16:15.909982 139975029864192 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.37198880314826965, loss=3.5572972297668457
I0212 07:16:50.852415 139975038256896 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.39322981238365173, loss=3.580907106399536
I0212 07:17:25.714987 139975029864192 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.41095542907714844, loss=3.571047306060791
I0212 07:18:00.559730 139975038256896 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.37980329990386963, loss=3.5846383571624756
I0212 07:18:35.348483 139975029864192 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.3720567524433136, loss=3.5844576358795166
I0212 07:19:10.140130 139975038256896 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.38645070791244507, loss=3.5967559814453125
I0212 07:19:44.966980 139975029864192 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.4050406813621521, loss=3.605093479156494
I0212 07:20:19.777727 139975038256896 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.39460867643356323, loss=3.58866548538208
I0212 07:20:54.572571 139975029864192 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.3916858434677124, loss=3.6172375679016113
I0212 07:21:29.342039 139975038256896 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.4010685682296753, loss=3.565817356109619
I0212 07:22:04.140174 139975029864192 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.392324835062027, loss=3.6234428882598877
I0212 07:22:38.960567 139975038256896 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.376046746969223, loss=3.577803611755371
I0212 07:23:13.745444 139975029864192 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.3698684573173523, loss=3.596165657043457
I0212 07:23:48.522173 139975038256896 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.3901931941509247, loss=3.5803167819976807
I0212 07:23:52.428801 140144802662208 spec.py:321] Evaluating on the training split.
I0212 07:23:55.417028 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 07:26:44.877811 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 07:26:47.567013 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 07:29:16.224374 140144802662208 spec.py:349] Evaluating on the test split.
I0212 07:29:18.938063 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 07:31:35.700460 140144802662208 submission_runner.py:408] Time since start: 71434.79s, 	Step: 128013, 	{'train/accuracy': 0.7244399785995483, 'train/loss': 1.435945749282837, 'train/bleu': 37.314039333533486, 'validation/accuracy': 0.6923162937164307, 'validation/loss': 1.597839117050171, 'validation/bleu': 30.500147410340556, 'validation/num_examples': 3000, 'test/accuracy': 0.7096856832504272, 'test/loss': 1.493368148803711, 'test/bleu': 30.728892446349207, 'test/num_examples': 3003, 'score': 44551.90661859512, 'total_duration': 71434.78941488266, 'accumulated_submission_time': 44551.90661859512, 'accumulated_eval_time': 26876.707780361176, 'accumulated_logging_time': 1.870469331741333}
I0212 07:31:35.741121 139975029864192 logging_writer.py:48] [128013] accumulated_eval_time=26876.707780, accumulated_logging_time=1.870469, accumulated_submission_time=44551.906619, global_step=128013, preemption_count=0, score=44551.906619, test/accuracy=0.709686, test/bleu=30.728892, test/loss=1.493368, test/num_examples=3003, total_duration=71434.789415, train/accuracy=0.724440, train/bleu=37.314039, train/loss=1.435946, validation/accuracy=0.692316, validation/bleu=30.500147, validation/loss=1.597839, validation/num_examples=3000
I0212 07:32:06.278853 139975038256896 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.38851043581962585, loss=3.5860977172851562
I0212 07:32:40.992049 139975029864192 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.3883552849292755, loss=3.5835957527160645
I0212 07:33:15.772714 139975038256896 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.40083861351013184, loss=3.594015598297119
I0212 07:33:50.527613 139975029864192 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.38213881850242615, loss=3.573741912841797
I0212 07:34:25.344735 139975038256896 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.4117651879787445, loss=3.593502998352051
I0212 07:35:00.167624 139975029864192 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.3867640197277069, loss=3.5546212196350098
I0212 07:35:34.999449 139975038256896 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.3896418809890747, loss=3.554577589035034
I0212 07:36:09.897663 139975029864192 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.40917858481407166, loss=3.622202157974243
I0212 07:36:44.712020 139975038256896 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.39272814989089966, loss=3.581735610961914
I0212 07:37:19.528724 139975029864192 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.40304139256477356, loss=3.6053316593170166
I0212 07:37:54.283902 139975038256896 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.39868220686912537, loss=3.594904661178589
I0212 07:38:29.068089 139975029864192 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.3906508982181549, loss=3.623091220855713
I0212 07:39:03.831550 139975038256896 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.3940775394439697, loss=3.619448661804199
I0212 07:39:38.600395 139975029864192 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.4089193344116211, loss=3.5943641662597656
I0212 07:40:13.380522 139975038256896 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.37686288356781006, loss=3.5512211322784424
I0212 07:40:48.151965 139975029864192 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.3808172643184662, loss=3.6023690700531006
I0212 07:41:22.930093 139975038256896 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.41232144832611084, loss=3.671074867248535
I0212 07:41:57.740583 139975029864192 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.4005451202392578, loss=3.598891258239746
I0212 07:42:32.548583 139975038256896 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.3698686361312866, loss=3.5534307956695557
I0212 07:43:07.376163 139975029864192 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.4028565287590027, loss=3.609379529953003
I0212 07:43:42.187934 139975038256896 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.3861849904060364, loss=3.5857975482940674
I0212 07:44:16.969416 139975029864192 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.4064343273639679, loss=3.603532552719116
I0212 07:44:51.770456 139975038256896 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.38997557759284973, loss=3.6052727699279785
I0212 07:45:26.551980 139975029864192 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.3896527886390686, loss=3.5951082706451416
I0212 07:45:36.015279 140144802662208 spec.py:321] Evaluating on the training split.
I0212 07:45:38.989495 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 07:48:28.223270 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 07:48:30.918171 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 07:50:57.133451 140144802662208 spec.py:349] Evaluating on the test split.
I0212 07:50:59.845649 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 07:53:19.625132 140144802662208 submission_runner.py:408] Time since start: 72738.71s, 	Step: 130429, 	{'train/accuracy': 0.7212345004081726, 'train/loss': 1.4529694318771362, 'train/bleu': 37.38313524691233, 'validation/accuracy': 0.6924278736114502, 'validation/loss': 1.598196029663086, 'validation/bleu': 30.587311943434095, 'validation/num_examples': 3000, 'test/accuracy': 0.7091395258903503, 'test/loss': 1.493313193321228, 'test/bleu': 30.76292408930312, 'test/num_examples': 3003, 'score': 45392.08640527725, 'total_duration': 72738.71410131454, 'accumulated_submission_time': 45392.08640527725, 'accumulated_eval_time': 27340.317565202713, 'accumulated_logging_time': 1.9223430156707764}
I0212 07:53:19.659094 139975038256896 logging_writer.py:48] [130429] accumulated_eval_time=27340.317565, accumulated_logging_time=1.922343, accumulated_submission_time=45392.086405, global_step=130429, preemption_count=0, score=45392.086405, test/accuracy=0.709140, test/bleu=30.762924, test/loss=1.493313, test/num_examples=3003, total_duration=72738.714101, train/accuracy=0.721235, train/bleu=37.383135, train/loss=1.452969, validation/accuracy=0.692428, validation/bleu=30.587312, validation/loss=1.598196, validation/num_examples=3000
I0212 07:53:44.653815 139975029864192 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.36642152070999146, loss=3.594416379928589
I0212 07:54:19.371684 139975038256896 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.3898282051086426, loss=3.60105299949646
I0212 07:54:54.162697 139975029864192 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.3912329375743866, loss=3.601858615875244
I0212 07:55:29.096127 139975038256896 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.39752069115638733, loss=3.6362743377685547
I0212 07:56:03.887484 139975029864192 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.381362646818161, loss=3.5649142265319824
I0212 07:56:38.701787 139975038256896 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.38958126306533813, loss=3.5901169776916504
I0212 07:57:13.518819 139975029864192 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.3832450211048126, loss=3.5930705070495605
I0212 07:57:48.355936 139975038256896 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.38925111293792725, loss=3.5313093662261963
I0212 07:58:23.148049 139975029864192 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.4014732837677002, loss=3.5930538177490234
I0212 07:58:57.954194 139975038256896 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.39690670371055603, loss=3.5679469108581543
I0212 07:59:32.765554 139975029864192 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.39102238416671753, loss=3.584653615951538
I0212 08:00:07.566915 139975038256896 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.4416739344596863, loss=3.6414732933044434
I0212 08:00:42.367749 139975029864192 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.415884405374527, loss=3.6286094188690186
I0212 08:01:17.210259 139975038256896 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.3950752913951874, loss=3.6368350982666016
I0212 08:01:52.024441 139975029864192 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.41779306530952454, loss=3.5609169006347656
I0212 08:02:26.842755 139975038256896 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.39074721932411194, loss=3.5778002738952637
I0212 08:03:01.641084 139975029864192 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.3935137391090393, loss=3.5999200344085693
I0212 08:03:36.445986 139975038256896 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.39336323738098145, loss=3.6083016395568848
I0212 08:04:11.256790 139975029864192 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.391020268201828, loss=3.654156446456909
I0212 08:04:46.102751 139975038256896 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.3982287645339966, loss=3.578582763671875
I0212 08:05:20.956401 139975029864192 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.4182889759540558, loss=3.578979253768921
I0212 08:05:55.738879 139975038256896 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.3944147229194641, loss=3.6414809226989746
I0212 08:06:30.527034 139975029864192 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.3845621645450592, loss=3.5589888095855713
I0212 08:07:05.323563 139975038256896 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.3884216845035553, loss=3.52974534034729
I0212 08:07:19.663110 140144802662208 spec.py:321] Evaluating on the training split.
I0212 08:07:22.648528 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:10:12.259042 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 08:10:14.946986 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:12:41.754523 140144802662208 spec.py:349] Evaluating on the test split.
I0212 08:12:44.447246 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:15:11.827296 140144802662208 submission_runner.py:408] Time since start: 74050.92s, 	Step: 132843, 	{'train/accuracy': 0.7237308025360107, 'train/loss': 1.441246747970581, 'train/bleu': 37.43974567291994, 'validation/accuracy': 0.6926634311676025, 'validation/loss': 1.598312497138977, 'validation/bleu': 30.54856858515057, 'validation/num_examples': 3000, 'test/accuracy': 0.7089187502861023, 'test/loss': 1.4930022954940796, 'test/bleu': 30.805018297453067, 'test/num_examples': 3003, 'score': 46231.996900081635, 'total_duration': 74050.91628265381, 'accumulated_submission_time': 46231.996900081635, 'accumulated_eval_time': 27812.481696128845, 'accumulated_logging_time': 1.968170166015625}
I0212 08:15:11.863997 139975029864192 logging_writer.py:48] [132843] accumulated_eval_time=27812.481696, accumulated_logging_time=1.968170, accumulated_submission_time=46231.996900, global_step=132843, preemption_count=0, score=46231.996900, test/accuracy=0.708919, test/bleu=30.805018, test/loss=1.493002, test/num_examples=3003, total_duration=74050.916283, train/accuracy=0.723731, train/bleu=37.439746, train/loss=1.441247, validation/accuracy=0.692663, validation/bleu=30.548569, validation/loss=1.598312, validation/num_examples=3000
I0212 08:15:31.970288 139975038256896 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.4012320041656494, loss=3.6046838760375977
I0212 08:16:06.668131 139975029864192 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.4120580554008484, loss=3.6178135871887207
I0212 08:16:41.424492 139975038256896 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.39118704199790955, loss=3.602708339691162
I0212 08:17:16.186079 139975029864192 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.39902928471565247, loss=3.644510269165039
I0212 08:17:50.962823 139975038256896 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.38537856936454773, loss=3.6015841960906982
I0212 08:18:01.819504 140144802662208 spec.py:321] Evaluating on the training split.
I0212 08:18:04.818377 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:20:51.282243 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 08:20:53.966104 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:23:20.740750 140144802662208 spec.py:349] Evaluating on the test split.
I0212 08:23:23.431568 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:25:50.945725 140144802662208 submission_runner.py:408] Time since start: 74690.03s, 	Step: 133333, 	{'train/accuracy': 0.7221422791481018, 'train/loss': 1.4469060897827148, 'train/bleu': 37.50803900245006, 'validation/accuracy': 0.6926758289337158, 'validation/loss': 1.598305106163025, 'validation/bleu': 30.54901382135056, 'validation/num_examples': 3000, 'test/accuracy': 0.7088838815689087, 'test/loss': 1.4929988384246826, 'test/bleu': 30.78181083377449, 'test/num_examples': 3003, 'score': 46401.92576980591, 'total_duration': 74690.03471302986, 'accumulated_submission_time': 46401.92576980591, 'accumulated_eval_time': 28281.60786294937, 'accumulated_logging_time': 2.0151820182800293}
I0212 08:25:50.980525 139975029864192 logging_writer.py:48] [133333] accumulated_eval_time=28281.607863, accumulated_logging_time=2.015182, accumulated_submission_time=46401.925770, global_step=133333, preemption_count=0, score=46401.925770, test/accuracy=0.708884, test/bleu=30.781811, test/loss=1.492999, test/num_examples=3003, total_duration=74690.034713, train/accuracy=0.722142, train/bleu=37.508039, train/loss=1.446906, validation/accuracy=0.692676, validation/bleu=30.549014, validation/loss=1.598305, validation/num_examples=3000
I0212 08:25:51.013136 139975038256896 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46401.925770
I0212 08:25:52.199338 140144802662208 checkpoints.py:490] Saving checkpoint at step: 133333
I0212 08:25:56.237930 140144802662208 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_2/checkpoint_133333
I0212 08:25:56.242894 140144802662208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_2/checkpoint_133333.
I0212 08:25:56.293320 140144802662208 submission_runner.py:583] Tuning trial 2/5
I0212 08:25:56.293558 140144802662208 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0212 08:25:56.302872 140144802662208 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006404464365914464, 'train/loss': 10.957476615905762, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 28.013572454452515, 'total_duration': 889.9948675632477, 'accumulated_submission_time': 28.013572454452515, 'accumulated_eval_time': 861.9812302589417, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2413, {'train/accuracy': 0.42854857444763184, 'train/loss': 3.919285297393799, 'train/bleu': 15.366745138241077, 'validation/accuracy': 0.41834571957588196, 'validation/loss': 4.008279800415039, 'validation/bleu': 10.708148967332471, 'validation/num_examples': 3000, 'test/accuracy': 0.4059613049030304, 'test/loss': 4.175229549407959, 'test/bleu': 9.371626452081102, 'test/num_examples': 3003, 'score': 868.0313177108765, 'total_duration': 2320.8611991405487, 'accumulated_submission_time': 868.0313177108765, 'accumulated_eval_time': 1452.7302029132843, 'accumulated_logging_time': 0.01902604103088379, 'global_step': 2413, 'preemption_count': 0}), (4825, {'train/accuracy': 0.5466793179512024, 'train/loss': 2.79789137840271, 'train/bleu': 24.074602836910767, 'validation/accuracy': 0.5493918061256409, 'validation/loss': 2.760988473892212, 'validation/bleu': 20.896848938078836, 'validation/num_examples': 3000, 'test/accuracy': 0.5490791201591492, 'test/loss': 2.7924811840057373, 'test/bleu': 19.09038173838629, 'test/num_examples': 3003, 'score': 1708.2604315280914, 'total_duration': 3631.8051176071167, 'accumulated_submission_time': 1708.2604315280914, 'accumulated_eval_time': 1923.3364017009735, 'accumulated_logging_time': 0.0459742546081543, 'global_step': 4825, 'preemption_count': 0}), (7238, {'train/accuracy': 0.582229733467102, 'train/loss': 2.4481654167175293, 'train/bleu': 25.84377936152035, 'validation/accuracy': 0.5931110382080078, 'validation/loss': 2.375218152999878, 'validation/bleu': 23.345873190842095, 'validation/num_examples': 3000, 'test/accuracy': 0.5952356457710266, 'test/loss': 2.362957000732422, 'test/bleu': 22.041756144935494, 'test/num_examples': 3003, 'score': 2548.210542678833, 'total_duration': 4934.42494559288, 'accumulated_submission_time': 2548.210542678833, 'accumulated_eval_time': 2385.8916296958923, 'accumulated_logging_time': 0.07915925979614258, 'global_step': 7238, 'preemption_count': 0}), (9653, {'train/accuracy': 0.603490948677063, 'train/loss': 2.285142183303833, 'train/bleu': 28.78597901242265, 'validation/accuracy': 0.6179216504096985, 'validation/loss': 2.1735126972198486, 'validation/bleu': 25.328648638725443, 'validation/num_examples': 3000, 'test/accuracy': 0.6251118779182434, 'test/loss': 2.1336803436279297, 'test/bleu': 24.447402386344706, 'test/num_examples': 3003, 'score': 3388.2512698173523, 'total_duration': 6232.517226934433, 'accumulated_submission_time': 3388.2512698173523, 'accumulated_eval_time': 2843.837694168091, 'accumulated_logging_time': 0.10521483421325684, 'global_step': 9653, 'preemption_count': 0}), (12068, {'train/accuracy': 0.6128459572792053, 'train/loss': 2.18766450881958, 'train/bleu': 29.374546077563878, 'validation/accuracy': 0.6334205269813538, 'validation/loss': 2.038684844970703, 'validation/bleu': 26.087020963661935, 'validation/num_examples': 3000, 'test/accuracy': 0.6416710615158081, 'test/loss': 1.9897395372390747, 'test/bleu': 25.192112223007232, 'test/num_examples': 3003, 'score': 4228.148333311081, 'total_duration': 7532.544618368149, 'accumulated_submission_time': 4228.148333311081, 'accumulated_eval_time': 3303.861836194992, 'accumulated_logging_time': 0.13168787956237793, 'global_step': 12068, 'preemption_count': 0}), (14483, {'train/accuracy': 0.6248981952667236, 'train/loss': 2.080275535583496, 'train/bleu': 30.089301867317968, 'validation/accuracy': 0.641777515411377, 'validation/loss': 1.9527431726455688, 'validation/bleu': 27.244437164531448, 'validation/num_examples': 3000, 'test/accuracy': 0.6509325504302979, 'test/loss': 1.8961308002471924, 'test/bleu': 26.48782476873385, 'test/num_examples': 3003, 'score': 5068.229043722153, 'total_duration': 8812.05256319046, 'accumulated_submission_time': 5068.229043722153, 'accumulated_eval_time': 3743.176248073578, 'accumulated_logging_time': 0.16280674934387207, 'global_step': 14483, 'preemption_count': 0}), (16900, {'train/accuracy': 0.6296117901802063, 'train/loss': 2.061455726623535, 'train/bleu': 30.56911605973895, 'validation/accuracy': 0.6512380242347717, 'validation/loss': 1.9128708839416504, 'validation/bleu': 27.760250709466792, 'validation/num_examples': 3000, 'test/accuracy': 0.6578467488288879, 'test/loss': 1.8625454902648926, 'test/bleu': 26.744218529681, 'test/num_examples': 3003, 'score': 5908.441868543625, 'total_duration': 10119.24120926857, 'accumulated_submission_time': 5908.441868543625, 'accumulated_eval_time': 4210.046847343445, 'accumulated_logging_time': 0.19021224975585938, 'global_step': 16900, 'preemption_count': 0}), (19316, {'train/accuracy': 0.6452200412750244, 'train/loss': 1.942769169807434, 'train/bleu': 31.575388848299944, 'validation/accuracy': 0.6562596559524536, 'validation/loss': 1.870498538017273, 'validation/bleu': 28.118415670737658, 'validation/num_examples': 3000, 'test/accuracy': 0.6642031669616699, 'test/loss': 1.8098138570785522, 'test/bleu': 27.05667497708177, 'test/num_examples': 3003, 'score': 6748.534591674805, 'total_duration': 11455.637856960297, 'accumulated_submission_time': 6748.534591674805, 'accumulated_eval_time': 4706.243428945541, 'accumulated_logging_time': 0.2173449993133545, 'global_step': 19316, 'preemption_count': 0}), (21732, {'train/accuracy': 0.6420875787734985, 'train/loss': 1.9512648582458496, 'train/bleu': 31.35088749641315, 'validation/accuracy': 0.6596198081970215, 'validation/loss': 1.8318740129470825, 'validation/bleu': 28.56932057810524, 'validation/num_examples': 3000, 'test/accuracy': 0.6690372824668884, 'test/loss': 1.7705334424972534, 'test/bleu': 27.950302449347422, 'test/num_examples': 3003, 'score': 7588.722680091858, 'total_duration': 12771.588398694992, 'accumulated_submission_time': 7588.722680091858, 'accumulated_eval_time': 5181.89380979538, 'accumulated_logging_time': 0.24721455574035645, 'global_step': 21732, 'preemption_count': 0}), (24147, {'train/accuracy': 0.6424695253372192, 'train/loss': 1.9587422609329224, 'train/bleu': 31.319786346641795, 'validation/accuracy': 0.6618020534515381, 'validation/loss': 1.8124172687530518, 'validation/bleu': 28.48977382265801, 'validation/num_examples': 3000, 'test/accuracy': 0.6719539761543274, 'test/loss': 1.7517828941345215, 'test/bleu': 28.07511583204494, 'test/num_examples': 3003, 'score': 8428.657732963562, 'total_duration': 14092.263476133347, 'accumulated_submission_time': 8428.657732963562, 'accumulated_eval_time': 5662.523268461227, 'accumulated_logging_time': 0.27626967430114746, 'global_step': 24147, 'preemption_count': 0}), (26562, {'train/accuracy': 0.6491546630859375, 'train/loss': 1.8978878259658813, 'train/bleu': 32.128709662827625, 'validation/accuracy': 0.6637735366821289, 'validation/loss': 1.7884488105773926, 'validation/bleu': 28.636768559280444, 'validation/num_examples': 3000, 'test/accuracy': 0.6746034622192383, 'test/loss': 1.7212979793548584, 'test/bleu': 28.215674239974483, 'test/num_examples': 3003, 'score': 9268.82431268692, 'total_duration': 15591.191541194916, 'accumulated_submission_time': 9268.82431268692, 'accumulated_eval_time': 6321.172736406326, 'accumulated_logging_time': 0.30533862113952637, 'global_step': 26562, 'preemption_count': 0}), (28978, {'train/accuracy': 0.6483582854270935, 'train/loss': 1.9046748876571655, 'train/bleu': 31.718846363850467, 'validation/accuracy': 0.6666997075080872, 'validation/loss': 1.7674710750579834, 'validation/bleu': 28.82188555062492, 'validation/num_examples': 3000, 'test/accuracy': 0.6767067909240723, 'test/loss': 1.701871633529663, 'test/bleu': 28.105043511042194, 'test/num_examples': 3003, 'score': 10108.904735088348, 'total_duration': 16915.69260573387, 'accumulated_submission_time': 10108.904735088348, 'accumulated_eval_time': 6805.479952096939, 'accumulated_logging_time': 0.3381767272949219, 'global_step': 28978, 'preemption_count': 0}), (31393, {'train/accuracy': 0.6834477782249451, 'train/loss': 1.6875489950180054, 'train/bleu': 34.22825500162002, 'validation/accuracy': 0.6686215996742249, 'validation/loss': 1.760544776916504, 'validation/bleu': 28.802539963238083, 'validation/num_examples': 3000, 'test/accuracy': 0.6804369688034058, 'test/loss': 1.6927775144577026, 'test/bleu': 28.100285054452986, 'test/num_examples': 3003, 'score': 10948.82987356186, 'total_duration': 18281.74568796158, 'accumulated_submission_time': 10948.82987356186, 'accumulated_eval_time': 7331.4971668720245, 'accumulated_logging_time': 0.3668546676635742, 'global_step': 31393, 'preemption_count': 0}), (33808, {'train/accuracy': 0.6541592478752136, 'train/loss': 1.8681278228759766, 'train/bleu': 31.89343011566094, 'validation/accuracy': 0.6689191460609436, 'validation/loss': 1.7596172094345093, 'validation/bleu': 29.067372362284118, 'validation/num_examples': 3000, 'test/accuracy': 0.681715190410614, 'test/loss': 1.6911182403564453, 'test/bleu': 28.551759458820776, 'test/num_examples': 3003, 'score': 11788.780126810074, 'total_duration': 19732.2159409523, 'accumulated_submission_time': 11788.780126810074, 'accumulated_eval_time': 7941.9084758758545, 'accumulated_logging_time': 0.39618897438049316, 'global_step': 33808, 'preemption_count': 0}), (36224, {'train/accuracy': 0.654525101184845, 'train/loss': 1.8604371547698975, 'train/bleu': 32.16741371237828, 'validation/accuracy': 0.673159658908844, 'validation/loss': 1.7235437631607056, 'validation/bleu': 29.286452756607737, 'validation/num_examples': 3000, 'test/accuracy': 0.6827842593193054, 'test/loss': 1.6493988037109375, 'test/bleu': 28.751117281827, 'test/num_examples': 3003, 'score': 12629.00738811493, 'total_duration': 21093.10581278801, 'accumulated_submission_time': 12629.00738811493, 'accumulated_eval_time': 8462.458575487137, 'accumulated_logging_time': 0.4259674549102783, 'global_step': 36224, 'preemption_count': 0}), (38641, {'train/accuracy': 0.6630606055259705, 'train/loss': 1.7945261001586914, 'train/bleu': 32.4857500217436, 'validation/accuracy': 0.6725273132324219, 'validation/loss': 1.7204879522323608, 'validation/bleu': 29.01776935916361, 'validation/num_examples': 3000, 'test/accuracy': 0.6843181848526001, 'test/loss': 1.6504517793655396, 'test/bleu': 28.69606835632274, 'test/num_examples': 3003, 'score': 13469.150447130203, 'total_duration': 22382.396797180176, 'accumulated_submission_time': 13469.150447130203, 'accumulated_eval_time': 8911.495740890503, 'accumulated_logging_time': 0.4579017162322998, 'global_step': 38641, 'preemption_count': 0}), (41057, {'train/accuracy': 0.6570492386817932, 'train/loss': 1.8250395059585571, 'train/bleu': 32.32277040187047, 'validation/accuracy': 0.6732960343360901, 'validation/loss': 1.7080892324447632, 'validation/bleu': 29.213552506314006, 'validation/num_examples': 3000, 'test/accuracy': 0.6871187090873718, 'test/loss': 1.634048342704773, 'test/bleu': 29.00677069047909, 'test/num_examples': 3003, 'score': 14309.070579051971, 'total_duration': 23699.610480308533, 'accumulated_submission_time': 14309.070579051971, 'accumulated_eval_time': 9388.679366111755, 'accumulated_logging_time': 0.4881284236907959, 'global_step': 41057, 'preemption_count': 0}), (43472, {'train/accuracy': 0.6580873727798462, 'train/loss': 1.8422174453735352, 'train/bleu': 32.67865687974197, 'validation/accuracy': 0.6763338446617126, 'validation/loss': 1.7116023302078247, 'validation/bleu': 29.45904070467425, 'validation/num_examples': 3000, 'test/accuracy': 0.6887455582618713, 'test/loss': 1.635982632637024, 'test/bleu': 28.902835204010888, 'test/num_examples': 3003, 'score': 15149.12542128563, 'total_duration': 25003.713657855988, 'accumulated_submission_time': 15149.12542128563, 'accumulated_eval_time': 9852.615337371826, 'accumulated_logging_time': 0.5199141502380371, 'global_step': 43472, 'preemption_count': 0}), (45888, {'train/accuracy': 0.6625374555587769, 'train/loss': 1.791016936302185, 'train/bleu': 32.6860518428758, 'validation/accuracy': 0.6753295063972473, 'validation/loss': 1.6926016807556152, 'validation/bleu': 29.099113091513008, 'validation/num_examples': 3000, 'test/accuracy': 0.6890825629234314, 'test/loss': 1.618253231048584, 'test/bleu': 28.803301939041916, 'test/num_examples': 3003, 'score': 15989.273291826248, 'total_duration': 26466.803416490555, 'accumulated_submission_time': 15989.273291826248, 'accumulated_eval_time': 10475.447494745255, 'accumulated_logging_time': 0.5502684116363525, 'global_step': 45888, 'preemption_count': 0}), (48304, {'train/accuracy': 0.6612311601638794, 'train/loss': 1.8156846761703491, 'train/bleu': 32.39720039613303, 'validation/accuracy': 0.6767057776451111, 'validation/loss': 1.702085256576538, 'validation/bleu': 29.483924220472556, 'validation/num_examples': 3000, 'test/accuracy': 0.6889082789421082, 'test/loss': 1.6244529485702515, 'test/bleu': 29.317962056875775, 'test/num_examples': 3003, 'score': 16829.422029733658, 'total_duration': 27796.0339281559, 'accumulated_submission_time': 16829.422029733658, 'accumulated_eval_time': 10964.418253660202, 'accumulated_logging_time': 0.5813858509063721, 'global_step': 48304, 'preemption_count': 0}), (50720, {'train/accuracy': 0.6744092702865601, 'train/loss': 1.718177080154419, 'train/bleu': 33.865250969584935, 'validation/accuracy': 0.6792972087860107, 'validation/loss': 1.6851235628128052, 'validation/bleu': 29.988029477140355, 'validation/num_examples': 3000, 'test/accuracy': 0.6897914409637451, 'test/loss': 1.6089180707931519, 'test/bleu': 29.17607198808368, 'test/num_examples': 3003, 'score': 17669.506113767624, 'total_duration': 29109.219428539276, 'accumulated_submission_time': 17669.506113767624, 'accumulated_eval_time': 11437.406776428223, 'accumulated_logging_time': 0.6123223304748535, 'global_step': 50720, 'preemption_count': 0}), (53136, {'train/accuracy': 0.6658750772476196, 'train/loss': 1.772932529449463, 'train/bleu': 32.865700967124376, 'validation/accuracy': 0.677821695804596, 'validation/loss': 1.6751149892807007, 'validation/bleu': 29.386465831879853, 'validation/num_examples': 3000, 'test/accuracy': 0.6934635043144226, 'test/loss': 1.5972486734390259, 'test/bleu': 29.42710453871561, 'test/num_examples': 3003, 'score': 18509.686230421066, 'total_duration': 30499.497972011566, 'accumulated_submission_time': 18509.686230421066, 'accumulated_eval_time': 11987.390083789825, 'accumulated_logging_time': 0.6450626850128174, 'global_step': 53136, 'preemption_count': 0}), (55552, {'train/accuracy': 0.6659360527992249, 'train/loss': 1.7834343910217285, 'train/bleu': 32.94168061929005, 'validation/accuracy': 0.6789996027946472, 'validation/loss': 1.6796774864196777, 'validation/bleu': 29.670184999573905, 'validation/num_examples': 3000, 'test/accuracy': 0.6937540173530579, 'test/loss': 1.5995779037475586, 'test/bleu': 29.225783598144606, 'test/num_examples': 3003, 'score': 19349.803040981293, 'total_duration': 31812.61920762062, 'accumulated_submission_time': 19349.803040981293, 'accumulated_eval_time': 12460.27916264534, 'accumulated_logging_time': 0.6809508800506592, 'global_step': 55552, 'preemption_count': 0}), (57968, {'train/accuracy': 0.6726042032241821, 'train/loss': 1.7362380027770996, 'train/bleu': 33.86621185465534, 'validation/accuracy': 0.6793344020843506, 'validation/loss': 1.6724979877471924, 'validation/bleu': 29.513959160563886, 'validation/num_examples': 3000, 'test/accuracy': 0.6937307715415955, 'test/loss': 1.5895719528198242, 'test/bleu': 29.51395797552482, 'test/num_examples': 3003, 'score': 20189.688641786575, 'total_duration': 33113.54695224762, 'accumulated_submission_time': 20189.688641786575, 'accumulated_eval_time': 12921.206790924072, 'accumulated_logging_time': 0.71565842628479, 'global_step': 57968, 'preemption_count': 0}), (60383, {'train/accuracy': 0.6659606695175171, 'train/loss': 1.7663089036941528, 'train/bleu': 33.31700121865782, 'validation/accuracy': 0.6792724132537842, 'validation/loss': 1.6618462800979614, 'validation/bleu': 30.042622596801575, 'validation/num_examples': 3000, 'test/accuracy': 0.6942071914672852, 'test/loss': 1.5787086486816406, 'test/bleu': 29.482354140661048, 'test/num_examples': 3003, 'score': 21029.69061565399, 'total_duration': 34416.46136879921, 'accumulated_submission_time': 21029.69061565399, 'accumulated_eval_time': 13383.99617767334, 'accumulated_logging_time': 0.7550392150878906, 'global_step': 60383, 'preemption_count': 0}), (62799, {'train/accuracy': 0.6952691674232483, 'train/loss': 1.5940762758255005, 'train/bleu': 34.85920428088039, 'validation/accuracy': 0.6813182830810547, 'validation/loss': 1.6626694202423096, 'validation/bleu': 30.02150473419823, 'validation/num_examples': 3000, 'test/accuracy': 0.6970425844192505, 'test/loss': 1.576493263244629, 'test/bleu': 29.63074699460067, 'test/num_examples': 3003, 'score': 21869.710822820663, 'total_duration': 35827.3902528286, 'accumulated_submission_time': 21869.710822820663, 'accumulated_eval_time': 13954.791036367416, 'accumulated_logging_time': 0.7893767356872559, 'global_step': 62799, 'preemption_count': 0}), (65215, {'train/accuracy': 0.6770080327987671, 'train/loss': 1.7094494104385376, 'train/bleu': 33.7335961330243, 'validation/accuracy': 0.6842692494392395, 'validation/loss': 1.6536860466003418, 'validation/bleu': 30.38194529115567, 'validation/num_examples': 3000, 'test/accuracy': 0.6985416412353516, 'test/loss': 1.5746350288391113, 'test/bleu': 29.660603258547127, 'test/num_examples': 3003, 'score': 22709.933475017548, 'total_duration': 37207.97281885147, 'accumulated_submission_time': 22709.933475017548, 'accumulated_eval_time': 14495.034008741379, 'accumulated_logging_time': 0.8237001895904541, 'global_step': 65215, 'preemption_count': 0}), (67630, {'train/accuracy': 0.6708812117576599, 'train/loss': 1.747066617012024, 'train/bleu': 33.23434424702395, 'validation/accuracy': 0.6815910339355469, 'validation/loss': 1.65132737159729, 'validation/bleu': 29.844001559891947, 'validation/num_examples': 3000, 'test/accuracy': 0.6962407827377319, 'test/loss': 1.5660450458526611, 'test/bleu': 29.662369265097585, 'test/num_examples': 3003, 'score': 23549.856046438217, 'total_duration': 38541.929241895676, 'accumulated_submission_time': 23549.856046438217, 'accumulated_eval_time': 14988.952271461487, 'accumulated_logging_time': 0.8565609455108643, 'global_step': 67630, 'preemption_count': 0}), (70045, {'train/accuracy': 0.6814951300621033, 'train/loss': 1.6714897155761719, 'train/bleu': 34.03927182611093, 'validation/accuracy': 0.6828433275222778, 'validation/loss': 1.6529614925384521, 'validation/bleu': 30.02545059786362, 'validation/num_examples': 3000, 'test/accuracy': 0.697623610496521, 'test/loss': 1.5698145627975464, 'test/bleu': 29.73855557334434, 'test/num_examples': 3003, 'score': 24390.056773662567, 'total_duration': 39862.7853975296, 'accumulated_submission_time': 24390.056773662567, 'accumulated_eval_time': 15469.488009691238, 'accumulated_logging_time': 0.8931279182434082, 'global_step': 70045, 'preemption_count': 0}), (72461, {'train/accuracy': 0.6753785014152527, 'train/loss': 1.71561861038208, 'train/bleu': 33.9016781817681, 'validation/accuracy': 0.6829301714897156, 'validation/loss': 1.6497820615768433, 'validation/bleu': 30.174211488067066, 'validation/num_examples': 3000, 'test/accuracy': 0.6997618079185486, 'test/loss': 1.559238314628601, 'test/bleu': 29.75574832873671, 'test/num_examples': 3003, 'score': 25230.09949684143, 'total_duration': 41187.3813123703, 'accumulated_submission_time': 25230.09949684143, 'accumulated_eval_time': 15953.917650938034, 'accumulated_logging_time': 0.9359502792358398, 'global_step': 72461, 'preemption_count': 0}), (74876, {'train/accuracy': 0.6737843155860901, 'train/loss': 1.714847445487976, 'train/bleu': 33.94385616955503, 'validation/accuracy': 0.6842072606086731, 'validation/loss': 1.6327464580535889, 'validation/bleu': 30.115898497553975, 'validation/num_examples': 3000, 'test/accuracy': 0.7012027502059937, 'test/loss': 1.540870189666748, 'test/bleu': 30.20991710316605, 'test/num_examples': 3003, 'score': 26069.99861884117, 'total_duration': 42573.744396448135, 'accumulated_submission_time': 26069.99861884117, 'accumulated_eval_time': 16500.258165597916, 'accumulated_logging_time': 0.977301836013794, 'global_step': 74876, 'preemption_count': 0}), (77292, {'train/accuracy': 0.6831181645393372, 'train/loss': 1.6621711254119873, 'train/bleu': 34.21749717101149, 'validation/accuracy': 0.685459554195404, 'validation/loss': 1.6368142366409302, 'validation/bleu': 30.355440777747113, 'validation/num_examples': 3000, 'test/accuracy': 0.7004590034484863, 'test/loss': 1.5464823246002197, 'test/bleu': 30.350398961195094, 'test/num_examples': 3003, 'score': 26909.980088472366, 'total_duration': 43937.69822263718, 'accumulated_submission_time': 26909.980088472366, 'accumulated_eval_time': 17024.11154460907, 'accumulated_logging_time': 1.0152418613433838, 'global_step': 77292, 'preemption_count': 0}), (79708, {'train/accuracy': 0.6789937019348145, 'train/loss': 1.6847949028015137, 'train/bleu': 34.13086744144354, 'validation/accuracy': 0.6863647103309631, 'validation/loss': 1.625876784324646, 'validation/bleu': 30.33725457330963, 'validation/num_examples': 3000, 'test/accuracy': 0.7026204466819763, 'test/loss': 1.5370594263076782, 'test/bleu': 30.37668151012431, 'test/num_examples': 3003, 'score': 27749.933834314346, 'total_duration': 45234.57467293739, 'accumulated_submission_time': 27749.933834314346, 'accumulated_eval_time': 17480.916483163834, 'accumulated_logging_time': 1.0527923107147217, 'global_step': 79708, 'preemption_count': 0}), (82124, {'train/accuracy': 0.6997047066688538, 'train/loss': 1.573757290840149, 'train/bleu': 35.5428039352093, 'validation/accuracy': 0.6866622567176819, 'validation/loss': 1.6228492259979248, 'validation/bleu': 30.645088176249953, 'validation/num_examples': 3000, 'test/accuracy': 0.7039335370063782, 'test/loss': 1.5355430841445923, 'test/bleu': 30.418042700337793, 'test/num_examples': 3003, 'score': 28590.127032995224, 'total_duration': 46531.81851029396, 'accumulated_submission_time': 28590.127032995224, 'accumulated_eval_time': 17937.84757256508, 'accumulated_logging_time': 1.0906941890716553, 'global_step': 82124, 'preemption_count': 0}), (84539, {'train/accuracy': 0.682442307472229, 'train/loss': 1.6693923473358154, 'train/bleu': 34.606211236537106, 'validation/accuracy': 0.688286542892456, 'validation/loss': 1.6197954416275024, 'validation/bleu': 30.211782980971023, 'validation/num_examples': 3000, 'test/accuracy': 0.7041078805923462, 'test/loss': 1.5266790390014648, 'test/bleu': 30.543048679561238, 'test/num_examples': 3003, 'score': 29430.197852134705, 'total_duration': 47850.226432323456, 'accumulated_submission_time': 29430.197852134705, 'accumulated_eval_time': 18416.06471323967, 'accumulated_logging_time': 1.1282691955566406, 'global_step': 84539, 'preemption_count': 0}), (86955, {'train/accuracy': 0.6825690269470215, 'train/loss': 1.666361689567566, 'train/bleu': 34.71119184261502, 'validation/accuracy': 0.6877037882804871, 'validation/loss': 1.6188981533050537, 'validation/bleu': 30.182893205006323, 'validation/num_examples': 3000, 'test/accuracy': 0.7032247185707092, 'test/loss': 1.5332567691802979, 'test/bleu': 30.316463949991334, 'test/num_examples': 3003, 'score': 30270.317069530487, 'total_duration': 49196.66105890274, 'accumulated_submission_time': 30270.317069530487, 'accumulated_eval_time': 18922.261901140213, 'accumulated_logging_time': 1.1654300689697266, 'global_step': 86955, 'preemption_count': 0}), (89370, {'train/accuracy': 0.6950032711029053, 'train/loss': 1.60042405128479, 'train/bleu': 35.154835319669424, 'validation/accuracy': 0.6891793012619019, 'validation/loss': 1.609760046005249, 'validation/bleu': 30.554063056987584, 'validation/num_examples': 3000, 'test/accuracy': 0.705083966255188, 'test/loss': 1.517716407775879, 'test/bleu': 30.645274576054728, 'test/num_examples': 3003, 'score': 31110.298770189285, 'total_duration': 50504.37959980965, 'accumulated_submission_time': 31110.298770189285, 'accumulated_eval_time': 19389.877872228622, 'accumulated_logging_time': 1.2036826610565186, 'global_step': 89370, 'preemption_count': 0}), (91785, {'train/accuracy': 0.6907691955566406, 'train/loss': 1.61209237575531, 'train/bleu': 34.67632533933311, 'validation/accuracy': 0.6889809370040894, 'validation/loss': 1.6088262796401978, 'validation/bleu': 30.479471667183642, 'validation/num_examples': 3000, 'test/accuracy': 0.7055255770683289, 'test/loss': 1.5153443813323975, 'test/bleu': 30.73101495101404, 'test/num_examples': 3003, 'score': 31950.33271098137, 'total_duration': 51806.25148367882, 'accumulated_submission_time': 31950.33271098137, 'accumulated_eval_time': 19851.59775352478, 'accumulated_logging_time': 1.242016077041626, 'global_step': 91785, 'preemption_count': 0}), (94200, {'train/accuracy': 0.7140809893608093, 'train/loss': 1.4977169036865234, 'train/bleu': 36.6485987570553, 'validation/accuracy': 0.6901712417602539, 'validation/loss': 1.6079293489456177, 'validation/bleu': 30.60468141212876, 'validation/num_examples': 3000, 'test/accuracy': 0.7057114839553833, 'test/loss': 1.514120101928711, 'test/bleu': 30.51289879789388, 'test/num_examples': 3003, 'score': 32790.51075673103, 'total_duration': 53128.12596178055, 'accumulated_submission_time': 32790.51075673103, 'accumulated_eval_time': 20333.17138814926, 'accumulated_logging_time': 1.2815396785736084, 'global_step': 94200, 'preemption_count': 0}), (96616, {'train/accuracy': 0.6998093128204346, 'train/loss': 1.5723187923431396, 'train/bleu': 35.22440418347292, 'validation/accuracy': 0.6890801191329956, 'validation/loss': 1.608446478843689, 'validation/bleu': 30.391074376933528, 'validation/num_examples': 3000, 'test/accuracy': 0.7062111496925354, 'test/loss': 1.511307954788208, 'test/bleu': 30.341710528802896, 'test/num_examples': 3003, 'score': 33630.70009255409, 'total_duration': 54448.7342133522, 'accumulated_submission_time': 33630.70009255409, 'accumulated_eval_time': 20813.4673306942, 'accumulated_logging_time': 1.3212535381317139, 'global_step': 96616, 'preemption_count': 0}), (99032, {'train/accuracy': 0.6960033774375916, 'train/loss': 1.58683180809021, 'train/bleu': 35.784215910569934, 'validation/accuracy': 0.6911879777908325, 'validation/loss': 1.6049529314041138, 'validation/bleu': 30.653904813297338, 'validation/num_examples': 3000, 'test/accuracy': 0.706362247467041, 'test/loss': 1.5094925165176392, 'test/bleu': 30.670146910166952, 'test/num_examples': 3003, 'score': 34470.83130598068, 'total_duration': 55763.27522611618, 'accumulated_submission_time': 34470.83130598068, 'accumulated_eval_time': 21287.758934020996, 'accumulated_logging_time': 1.3601250648498535, 'global_step': 99032, 'preemption_count': 0}), (101448, {'train/accuracy': 0.7109413743019104, 'train/loss': 1.5052330493927002, 'train/bleu': 36.498945434107455, 'validation/accuracy': 0.6911631226539612, 'validation/loss': 1.6019834280014038, 'validation/bleu': 30.694062047415066, 'validation/num_examples': 3000, 'test/accuracy': 0.7069781422615051, 'test/loss': 1.5060288906097412, 'test/bleu': 30.958594252207657, 'test/num_examples': 3003, 'score': 35310.982800245285, 'total_duration': 57066.73641419411, 'accumulated_submission_time': 35310.982800245285, 'accumulated_eval_time': 21750.949397325516, 'accumulated_logging_time': 1.3992786407470703, 'global_step': 101448, 'preemption_count': 0}), (103863, {'train/accuracy': 0.6996182203292847, 'train/loss': 1.565675139427185, 'train/bleu': 36.38101795307617, 'validation/accuracy': 0.6910267472267151, 'validation/loss': 1.6026326417922974, 'validation/bleu': 30.47832296179434, 'validation/num_examples': 3000, 'test/accuracy': 0.7081633806228638, 'test/loss': 1.5044052600860596, 'test/bleu': 30.9335101099981, 'test/num_examples': 3003, 'score': 36151.15802383423, 'total_duration': 58371.27501010895, 'accumulated_submission_time': 36151.15802383423, 'accumulated_eval_time': 22215.191648960114, 'accumulated_logging_time': 1.43782639503479, 'global_step': 103863, 'preemption_count': 0}), (106278, {'train/accuracy': 0.7071945071220398, 'train/loss': 1.5247044563293457, 'train/bleu': 35.687987250436876, 'validation/accuracy': 0.6906920075416565, 'validation/loss': 1.6015530824661255, 'validation/bleu': 30.56966933807441, 'validation/num_examples': 3000, 'test/accuracy': 0.7085352540016174, 'test/loss': 1.5015463829040527, 'test/bleu': 30.860647082755722, 'test/num_examples': 3003, 'score': 36991.27319264412, 'total_duration': 59683.38256788254, 'accumulated_submission_time': 36991.27319264412, 'accumulated_eval_time': 22687.063174962997, 'accumulated_logging_time': 1.4773857593536377, 'global_step': 106278, 'preemption_count': 0}), (108693, {'train/accuracy': 0.7097283601760864, 'train/loss': 1.5101783275604248, 'train/bleu': 36.4322003452034, 'validation/accuracy': 0.691696286201477, 'validation/loss': 1.6009178161621094, 'validation/bleu': 30.72172265147943, 'validation/num_examples': 3000, 'test/accuracy': 0.7084887623786926, 'test/loss': 1.4992235898971558, 'test/bleu': 30.698081478592425, 'test/num_examples': 3003, 'score': 37831.44410777092, 'total_duration': 60993.797033786774, 'accumulated_submission_time': 37831.44410777092, 'accumulated_eval_time': 23157.18415951729, 'accumulated_logging_time': 1.517770767211914, 'global_step': 108693, 'preemption_count': 0}), (111109, {'train/accuracy': 0.7096648812294006, 'train/loss': 1.506799578666687, 'train/bleu': 36.62844563646342, 'validation/accuracy': 0.6907168030738831, 'validation/loss': 1.6045445203781128, 'validation/bleu': 30.522907929975723, 'validation/num_examples': 3000, 'test/accuracy': 0.7071989178657532, 'test/loss': 1.5012623071670532, 'test/bleu': 30.854497798748653, 'test/num_examples': 3003, 'score': 38671.495940208435, 'total_duration': 62304.079092502594, 'accumulated_submission_time': 38671.495940208435, 'accumulated_eval_time': 23627.291821718216, 'accumulated_logging_time': 1.559746265411377, 'global_step': 111109, 'preemption_count': 0}), (113523, {'train/accuracy': 0.7205490469932556, 'train/loss': 1.4559381008148193, 'train/bleu': 37.3473881454824, 'validation/accuracy': 0.6923410892486572, 'validation/loss': 1.5988622903823853, 'validation/bleu': 30.747762382036655, 'validation/num_examples': 3000, 'test/accuracy': 0.7090232968330383, 'test/loss': 1.4939895868301392, 'test/bleu': 30.819914092900255, 'test/num_examples': 3003, 'score': 39511.46545481682, 'total_duration': 63604.90850496292, 'accumulated_submission_time': 39511.46545481682, 'accumulated_eval_time': 24088.021463871002, 'accumulated_logging_time': 1.606471061706543, 'global_step': 113523, 'preemption_count': 0}), (115939, {'train/accuracy': 0.7139232158660889, 'train/loss': 1.487220048904419, 'train/bleu': 36.63052469599194, 'validation/accuracy': 0.6912499666213989, 'validation/loss': 1.601006269454956, 'validation/bleu': 30.716334354820788, 'validation/num_examples': 3000, 'test/accuracy': 0.7080239653587341, 'test/loss': 1.49728524684906, 'test/bleu': 30.801907041652694, 'test/num_examples': 3003, 'score': 40351.63733792305, 'total_duration': 64910.21405529976, 'accumulated_submission_time': 40351.63733792305, 'accumulated_eval_time': 24553.032210111618, 'accumulated_logging_time': 1.6485328674316406, 'global_step': 115939, 'preemption_count': 0}), (118355, {'train/accuracy': 0.7143579721450806, 'train/loss': 1.484772801399231, 'train/bleu': 37.03462724545019, 'validation/accuracy': 0.6923038959503174, 'validation/loss': 1.6000486612319946, 'validation/bleu': 30.58042947513172, 'validation/num_examples': 3000, 'test/accuracy': 0.7077218294143677, 'test/loss': 1.4967893362045288, 'test/bleu': 30.703935636601802, 'test/num_examples': 3003, 'score': 41191.82350349426, 'total_duration': 66209.90850758553, 'accumulated_submission_time': 41191.82350349426, 'accumulated_eval_time': 25012.41903567314, 'accumulated_logging_time': 1.6906397342681885, 'global_step': 118355, 'preemption_count': 0}), (120769, {'train/accuracy': 0.7247974872589111, 'train/loss': 1.4364838600158691, 'train/bleu': 37.53693442719608, 'validation/accuracy': 0.6926262378692627, 'validation/loss': 1.5970616340637207, 'validation/bleu': 30.67829971443381, 'validation/num_examples': 3000, 'test/accuracy': 0.7095927000045776, 'test/loss': 1.4932905435562134, 'test/bleu': 30.727894032551443, 'test/num_examples': 3003, 'score': 42031.85128903389, 'total_duration': 67522.48354244232, 'accumulated_submission_time': 42031.85128903389, 'accumulated_eval_time': 25484.83347582817, 'accumulated_logging_time': 1.7393221855163574, 'global_step': 120769, 'preemption_count': 0}), (123184, {'train/accuracy': 0.7164363265037537, 'train/loss': 1.4766035079956055, 'train/bleu': 37.23399471055593, 'validation/accuracy': 0.6918575167655945, 'validation/loss': 1.5992780923843384, 'validation/bleu': 30.603763521386373, 'validation/num_examples': 3000, 'test/accuracy': 0.7095230221748352, 'test/loss': 1.493468999862671, 'test/bleu': 30.812469473793247, 'test/num_examples': 3003, 'score': 42871.75427532196, 'total_duration': 68822.87196969986, 'accumulated_submission_time': 42871.75427532196, 'accumulated_eval_time': 25945.196885108948, 'accumulated_logging_time': 1.7829713821411133, 'global_step': 123184, 'preemption_count': 0}), (125598, {'train/accuracy': 0.7242320775985718, 'train/loss': 1.4370570182800293, 'train/bleu': 37.27877258251524, 'validation/accuracy': 0.6923782825469971, 'validation/loss': 1.598088026046753, 'validation/bleu': 30.46979271110354, 'validation/num_examples': 3000, 'test/accuracy': 0.7091279029846191, 'test/loss': 1.4928165674209595, 'test/bleu': 30.822391857216715, 'test/num_examples': 3003, 'score': 43711.76387667656, 'total_duration': 70131.25042271614, 'accumulated_submission_time': 43711.76387667656, 'accumulated_eval_time': 26413.436207294464, 'accumulated_logging_time': 1.8277215957641602, 'global_step': 125598, 'preemption_count': 0}), (128013, {'train/accuracy': 0.7244399785995483, 'train/loss': 1.435945749282837, 'train/bleu': 37.314039333533486, 'validation/accuracy': 0.6923162937164307, 'validation/loss': 1.597839117050171, 'validation/bleu': 30.500147410340556, 'validation/num_examples': 3000, 'test/accuracy': 0.7096856832504272, 'test/loss': 1.493368148803711, 'test/bleu': 30.728892446349207, 'test/num_examples': 3003, 'score': 44551.90661859512, 'total_duration': 71434.78941488266, 'accumulated_submission_time': 44551.90661859512, 'accumulated_eval_time': 26876.707780361176, 'accumulated_logging_time': 1.870469331741333, 'global_step': 128013, 'preemption_count': 0}), (130429, {'train/accuracy': 0.7212345004081726, 'train/loss': 1.4529694318771362, 'train/bleu': 37.38313524691233, 'validation/accuracy': 0.6924278736114502, 'validation/loss': 1.598196029663086, 'validation/bleu': 30.587311943434095, 'validation/num_examples': 3000, 'test/accuracy': 0.7091395258903503, 'test/loss': 1.493313193321228, 'test/bleu': 30.76292408930312, 'test/num_examples': 3003, 'score': 45392.08640527725, 'total_duration': 72738.71410131454, 'accumulated_submission_time': 45392.08640527725, 'accumulated_eval_time': 27340.317565202713, 'accumulated_logging_time': 1.9223430156707764, 'global_step': 130429, 'preemption_count': 0}), (132843, {'train/accuracy': 0.7237308025360107, 'train/loss': 1.441246747970581, 'train/bleu': 37.43974567291994, 'validation/accuracy': 0.6926634311676025, 'validation/loss': 1.598312497138977, 'validation/bleu': 30.54856858515057, 'validation/num_examples': 3000, 'test/accuracy': 0.7089187502861023, 'test/loss': 1.4930022954940796, 'test/bleu': 30.805018297453067, 'test/num_examples': 3003, 'score': 46231.996900081635, 'total_duration': 74050.91628265381, 'accumulated_submission_time': 46231.996900081635, 'accumulated_eval_time': 27812.481696128845, 'accumulated_logging_time': 1.968170166015625, 'global_step': 132843, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7221422791481018, 'train/loss': 1.4469060897827148, 'train/bleu': 37.50803900245006, 'validation/accuracy': 0.6926758289337158, 'validation/loss': 1.598305106163025, 'validation/bleu': 30.54901382135056, 'validation/num_examples': 3000, 'test/accuracy': 0.7088838815689087, 'test/loss': 1.4929988384246826, 'test/bleu': 30.78181083377449, 'test/num_examples': 3003, 'score': 46401.92576980591, 'total_duration': 74690.03471302986, 'accumulated_submission_time': 46401.92576980591, 'accumulated_eval_time': 28281.60786294937, 'accumulated_logging_time': 2.0151820182800293, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0212 08:25:56.303169 140144802662208 submission_runner.py:586] Timing: 46401.92576980591
I0212 08:25:56.303232 140144802662208 submission_runner.py:588] Total number of evals: 57
I0212 08:25:56.303286 140144802662208 submission_runner.py:589] ====================
I0212 08:25:56.303337 140144802662208 submission_runner.py:542] Using RNG seed 599091471
I0212 08:25:56.305202 140144802662208 submission_runner.py:551] --- Tuning run 3/5 ---
I0212 08:25:56.305337 140144802662208 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_3.
I0212 08:25:56.305644 140144802662208 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_3/hparams.json.
I0212 08:25:56.306537 140144802662208 submission_runner.py:206] Initializing dataset.
I0212 08:25:56.309655 140144802662208 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0212 08:25:56.312855 140144802662208 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0212 08:25:56.355567 140144802662208 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0212 08:25:57.056080 140144802662208 submission_runner.py:213] Initializing model.
I0212 08:26:04.641512 140144802662208 submission_runner.py:255] Initializing optimizer.
I0212 08:26:05.502526 140144802662208 submission_runner.py:262] Initializing metrics bundle.
I0212 08:26:05.502781 140144802662208 submission_runner.py:280] Initializing checkpoint and logger.
I0212 08:26:05.504061 140144802662208 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/wmt_jax/trial_3 with prefix checkpoint_
I0212 08:26:05.504188 140144802662208 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_3/meta_data_0.json.
I0212 08:26:05.504474 140144802662208 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0212 08:26:05.504546 140144802662208 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0212 08:26:06.016208 140144802662208 logger_utils.py:220] Unable to record git information. Continuing without it.
I0212 08:26:06.497412 140144802662208 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_3/flags_0.json.
I0212 08:26:06.501550 140144802662208 submission_runner.py:314] Starting training loop.
I0212 08:26:36.541116 139975020959488 logging_writer.py:48] [0] global_step=0, grad_norm=5.756457805633545, loss=10.947874069213867
I0212 08:26:36.555753 140144802662208 spec.py:321] Evaluating on the training split.
I0212 08:26:39.243528 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:31:23.448581 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 08:31:26.127254 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:36:10.156793 140144802662208 spec.py:349] Evaluating on the test split.
I0212 08:36:12.840268 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:40:56.433646 140144802662208 submission_runner.py:408] Time since start: 889.93s, 	Step: 1, 	{'train/accuracy': 0.0006362841231748462, 'train/loss': 10.959882736206055, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.05414128303528, 'total_duration': 889.9320287704468, 'accumulated_submission_time': 30.05414128303528, 'accumulated_eval_time': 859.8778374195099, 'accumulated_logging_time': 0}
I0212 08:40:56.443233 139975029352192 logging_writer.py:48] [1] accumulated_eval_time=859.877837, accumulated_logging_time=0, accumulated_submission_time=30.054141, global_step=1, preemption_count=0, score=30.054141, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.966498, test/num_examples=3003, total_duration=889.932029, train/accuracy=0.000636, train/bleu=0.000000, train/loss=10.959883, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.980294, validation/num_examples=3000
I0212 08:41:31.190279 139975020959488 logging_writer.py:48] [100] global_step=100, grad_norm=0.4363662302494049, loss=8.7271146774292
I0212 08:42:05.932816 139975029352192 logging_writer.py:48] [200] global_step=200, grad_norm=0.18352073431015015, loss=8.2994966506958
I0212 08:42:40.723567 139975020959488 logging_writer.py:48] [300] global_step=300, grad_norm=0.2091047614812851, loss=8.015484809875488
I0212 08:43:15.541845 139975029352192 logging_writer.py:48] [400] global_step=400, grad_norm=0.2872457206249237, loss=7.650977611541748
I0212 08:43:50.384079 139975020959488 logging_writer.py:48] [500] global_step=500, grad_norm=0.4089871942996979, loss=7.256438255310059
I0212 08:44:25.240003 139975029352192 logging_writer.py:48] [600] global_step=600, grad_norm=0.6025767922401428, loss=6.973930835723877
I0212 08:45:00.113088 139975020959488 logging_writer.py:48] [700] global_step=700, grad_norm=0.6915962100028992, loss=6.785465717315674
I0212 08:45:34.960482 139975029352192 logging_writer.py:48] [800] global_step=800, grad_norm=0.66336989402771, loss=6.475345134735107
I0212 08:46:09.816901 139975020959488 logging_writer.py:48] [900] global_step=900, grad_norm=0.6709908246994019, loss=6.205613613128662
I0212 08:46:44.646609 139975029352192 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6269343495368958, loss=6.027735710144043
I0212 08:47:19.523381 139975020959488 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.762687623500824, loss=5.762795448303223
I0212 08:47:54.392996 139975029352192 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1794980764389038, loss=5.584896087646484
I0212 08:48:29.236028 139975020959488 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6809054613113403, loss=5.4732489585876465
I0212 08:49:04.082053 139975029352192 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9197639226913452, loss=5.305635452270508
I0212 08:49:38.942619 139975020959488 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6875017881393433, loss=5.122447490692139
I0212 08:50:13.809048 139975029352192 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6898387670516968, loss=4.926460266113281
I0212 08:50:48.666419 139975020959488 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6446742415428162, loss=4.805287837982178
I0212 08:51:23.521743 139975029352192 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.5413042306900024, loss=4.712588310241699
I0212 08:51:58.375007 139975020959488 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.6085511445999146, loss=4.555047035217285
I0212 08:52:33.225529 139975029352192 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7795954942703247, loss=4.495970249176025
I0212 08:53:08.067810 139975020959488 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7782332301139832, loss=4.3071818351745605
I0212 08:53:42.974722 139975029352192 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0662091970443726, loss=4.246205806732178
I0212 08:54:17.829999 139975020959488 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.797031581401825, loss=4.112397193908691
I0212 08:54:52.731593 139975029352192 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.2135579586029053, loss=3.940666437149048
I0212 08:54:56.645777 140144802662208 spec.py:321] Evaluating on the training split.
I0212 08:54:59.619483 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 08:58:34.182721 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 08:58:36.893458 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 09:01:53.720642 140144802662208 spec.py:349] Evaluating on the test split.
I0212 09:01:56.417363 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 09:04:56.030050 140144802662208 submission_runner.py:408] Time since start: 2329.53s, 	Step: 2413, 	{'train/accuracy': 0.4128057062625885, 'train/loss': 3.9366543292999268, 'train/bleu': 14.089845752800661, 'validation/accuracy': 0.3986683189868927, 'validation/loss': 4.049123287200928, 'validation/bleu': 9.633888628769737, 'validation/num_examples': 3000, 'test/accuracy': 0.3826157748699188, 'test/loss': 4.261987686157227, 'test/bleu': 8.030870962674728, 'test/num_examples': 3003, 'score': 870.1676576137543, 'total_duration': 2329.528389930725, 'accumulated_submission_time': 870.1676576137543, 'accumulated_eval_time': 1459.2620244026184, 'accumulated_logging_time': 0.019870281219482422}
I0212 09:04:56.049112 139975020959488 logging_writer.py:48] [2413] accumulated_eval_time=1459.262024, accumulated_logging_time=0.019870, accumulated_submission_time=870.167658, global_step=2413, preemption_count=0, score=870.167658, test/accuracy=0.382616, test/bleu=8.030871, test/loss=4.261988, test/num_examples=3003, total_duration=2329.528390, train/accuracy=0.412806, train/bleu=14.089846, train/loss=3.936654, validation/accuracy=0.398668, validation/bleu=9.633889, validation/loss=4.049123, validation/num_examples=3000
I0212 09:05:26.647312 139975029352192 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8030682802200317, loss=3.963376522064209
I0212 09:06:01.424720 139975020959488 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8338094353675842, loss=3.6881892681121826
I0212 09:06:36.271488 139975029352192 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8842216730117798, loss=3.6935317516326904
I0212 09:07:11.135990 139975020959488 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.7114343643188477, loss=3.6566998958587646
I0212 09:07:46.021848 139975029352192 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.3451462984085083, loss=3.5154201984405518
I0212 09:08:20.878629 139975020959488 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7867220044136047, loss=3.4499194622039795
I0212 09:08:55.723398 139975029352192 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9925693273544312, loss=3.381309747695923
I0212 09:09:30.582989 139975020959488 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7644400000572205, loss=3.340702772140503
I0212 09:10:05.422345 139975029352192 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1004518270492554, loss=3.1798758506774902
I0212 09:10:40.290395 139975020959488 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7188383936882019, loss=3.1770575046539307
I0212 09:11:15.147366 139975029352192 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.911281943321228, loss=3.1030046939849854
I0212 09:11:49.997444 139975020959488 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7860775589942932, loss=3.1351234912872314
I0212 09:12:24.954858 139975029352192 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7345927357673645, loss=3.0406036376953125
I0212 09:12:59.849729 139975020959488 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8236409425735474, loss=3.0017807483673096
I0212 09:13:34.766280 139975029352192 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8518138527870178, loss=2.9508533477783203
I0212 09:14:09.623715 139975020959488 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7886067032814026, loss=2.960103750228882
I0212 09:14:44.484447 139975029352192 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7123783826828003, loss=2.908308267593384
I0212 09:15:19.328914 139975020959488 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8498443365097046, loss=2.8579213619232178
I0212 09:15:54.190191 139975029352192 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6123690009117126, loss=2.796987295150757
I0212 09:16:29.036151 139975020959488 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6769874095916748, loss=2.773632049560547
I0212 09:17:03.872937 139975029352192 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6364309787750244, loss=2.7093915939331055
I0212 09:17:38.694233 139975020959488 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7868551015853882, loss=2.718449831008911
I0212 09:18:13.533006 139975029352192 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.671608030796051, loss=2.670638084411621
I0212 09:18:48.421339 139975020959488 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6380176544189453, loss=2.646653652191162
I0212 09:18:56.153458 140144802662208 spec.py:321] Evaluating on the training split.
I0212 09:18:59.140427 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 09:21:33.067650 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 09:21:35.758079 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 09:24:15.188569 140144802662208 spec.py:349] Evaluating on the test split.
I0212 09:24:17.878529 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 09:26:41.143751 140144802662208 submission_runner.py:408] Time since start: 3634.64s, 	Step: 4824, 	{'train/accuracy': 0.5432161092758179, 'train/loss': 2.668916940689087, 'train/bleu': 24.829318029776022, 'validation/accuracy': 0.5427582859992981, 'validation/loss': 2.6340794563293457, 'validation/bleu': 20.365649432861616, 'validation/num_examples': 3000, 'test/accuracy': 0.5445354580879211, 'test/loss': 2.6518774032592773, 'test/bleu': 19.00498182839845, 'test/num_examples': 3003, 'score': 1710.180258989334, 'total_duration': 3634.642117023468, 'accumulated_submission_time': 1710.180258989334, 'accumulated_eval_time': 1924.2522518634796, 'accumulated_logging_time': 0.05057692527770996}
I0212 09:26:41.159663 139975029352192 logging_writer.py:48] [4824] accumulated_eval_time=1924.252252, accumulated_logging_time=0.050577, accumulated_submission_time=1710.180259, global_step=4824, preemption_count=0, score=1710.180259, test/accuracy=0.544535, test/bleu=19.004982, test/loss=2.651877, test/num_examples=3003, total_duration=3634.642117, train/accuracy=0.543216, train/bleu=24.829318, train/loss=2.668917, validation/accuracy=0.542758, validation/bleu=20.365649, validation/loss=2.634079, validation/num_examples=3000
I0212 09:27:07.916422 139975020959488 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.603691816329956, loss=2.6756231784820557
I0212 09:27:42.682007 139975029352192 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5803073048591614, loss=2.6454412937164307
I0212 09:28:17.524522 139975020959488 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7208337783813477, loss=2.583648204803467
I0212 09:28:52.462254 139975029352192 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7941043376922607, loss=2.5740292072296143
I0212 09:29:27.334458 139975020959488 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6058675050735474, loss=2.5199928283691406
I0212 09:30:02.190677 139975029352192 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5757684111595154, loss=2.5371487140655518
I0212 09:30:37.072597 139975020959488 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6813119649887085, loss=2.5696399211883545
I0212 09:31:11.941622 139975029352192 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6189618706703186, loss=2.5491607189178467
I0212 09:31:46.810812 139975020959488 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6270748972892761, loss=2.5245413780212402
I0212 09:32:21.670274 139975029352192 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5668510794639587, loss=2.4807660579681396
I0212 09:32:56.515468 139975020959488 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5172790884971619, loss=2.4777538776397705
I0212 09:33:31.437665 139975029352192 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5676178932189941, loss=2.4574320316314697
I0212 09:34:06.345134 139975020959488 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5976352691650391, loss=2.4949567317962646
I0212 09:34:41.262181 139975029352192 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5486406087875366, loss=2.4857866764068604
I0212 09:35:16.193583 139975020959488 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6172319650650024, loss=2.393388509750366
I0212 09:35:51.094977 139975029352192 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.49188345670700073, loss=2.4028608798980713
I0212 09:36:25.958007 139975020959488 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5596532225608826, loss=2.4377384185791016
I0212 09:37:00.780941 139975029352192 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5177502036094666, loss=2.470874786376953
I0212 09:37:35.656512 139975020959488 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5402101278305054, loss=2.391751289367676
I0212 09:38:10.506485 139975029352192 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4859856069087982, loss=2.364206552505493
I0212 09:38:45.364662 139975020959488 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6293430924415588, loss=2.426713466644287
I0212 09:39:20.190308 139975029352192 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.46995070576667786, loss=2.358131170272827
I0212 09:39:55.035714 139975020959488 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5290806293487549, loss=2.411918878555298
I0212 09:40:29.883398 139975029352192 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4724552631378174, loss=2.308573007583618
I0212 09:40:41.459934 140144802662208 spec.py:321] Evaluating on the training split.
I0212 09:40:44.436792 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 09:43:20.465399 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 09:43:23.139221 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 09:46:06.375385 140144802662208 spec.py:349] Evaluating on the test split.
I0212 09:46:09.076468 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 09:48:31.971723 140144802662208 submission_runner.py:408] Time since start: 4945.47s, 	Step: 7235, 	{'train/accuracy': 0.581144392490387, 'train/loss': 2.2662315368652344, 'train/bleu': 27.389879639296826, 'validation/accuracy': 0.5851012468338013, 'validation/loss': 2.2258543968200684, 'validation/bleu': 23.189117839268576, 'validation/num_examples': 3000, 'test/accuracy': 0.5854976773262024, 'test/loss': 2.21711802482605, 'test/bleu': 21.73489214068676, 'test/num_examples': 3003, 'score': 2550.3885049819946, 'total_duration': 4945.470089673996, 'accumulated_submission_time': 2550.3885049819946, 'accumulated_eval_time': 2394.7639780044556, 'accumulated_logging_time': 0.07668113708496094}
I0212 09:48:31.987258 139975020959488 logging_writer.py:48] [7235] accumulated_eval_time=2394.763978, accumulated_logging_time=0.076681, accumulated_submission_time=2550.388505, global_step=7235, preemption_count=0, score=2550.388505, test/accuracy=0.585498, test/bleu=21.734892, test/loss=2.217118, test/num_examples=3003, total_duration=4945.470090, train/accuracy=0.581144, train/bleu=27.389880, train/loss=2.266232, validation/accuracy=0.585101, validation/bleu=23.189118, validation/loss=2.225854, validation/num_examples=3000
I0212 09:48:54.895368 139975029352192 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5030933618545532, loss=2.2756900787353516
I0212 09:49:29.622910 139975020959488 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.45214587450027466, loss=2.2318341732025146
I0212 09:50:04.431585 139975029352192 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4611241817474365, loss=2.255495309829712
I0212 09:50:39.264273 139975020959488 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.43591976165771484, loss=2.2207443714141846
I0212 09:51:14.095719 139975029352192 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5104325413703918, loss=2.2665414810180664
I0212 09:51:48.947613 139975020959488 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.487289160490036, loss=2.2979981899261475
I0212 09:52:23.802453 139975029352192 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5745801329612732, loss=2.3686585426330566
I0212 09:52:58.630292 139975020959488 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4523470103740692, loss=2.2264504432678223
I0212 09:53:33.442793 139975029352192 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.45448505878448486, loss=2.258495807647705
I0212 09:54:08.254751 139975020959488 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.41440919041633606, loss=2.2509732246398926
I0212 09:54:43.071591 139975029352192 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.47367388010025024, loss=2.2187674045562744
I0212 09:55:17.895969 139975020959488 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.4960653483867645, loss=2.237891912460327
I0212 09:55:52.712227 139975029352192 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.440532386302948, loss=2.098524332046509
I0212 09:56:27.559725 139975020959488 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.39641931653022766, loss=2.1231467723846436
I0212 09:57:02.414979 139975029352192 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3956741988658905, loss=2.2158432006835938
I0212 09:57:37.275117 139975020959488 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.4383578598499298, loss=2.1237905025482178
I0212 09:58:12.143191 139975029352192 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.4293000102043152, loss=2.148847818374634
I0212 09:58:47.007689 139975020959488 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.4017903804779053, loss=2.275346517562866
I0212 09:59:21.858311 139975029352192 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.4101330041885376, loss=2.2065792083740234
I0212 09:59:56.718810 139975020959488 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.4254838526248932, loss=2.2803986072540283
I0212 10:00:31.544216 139975029352192 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.352723628282547, loss=2.1585304737091064
I0212 10:01:06.372963 139975020959488 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3667811155319214, loss=2.224672794342041
I0212 10:01:41.191796 139975029352192 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.379672646522522, loss=2.112922430038452
I0212 10:02:16.025513 139975020959488 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.39558878540992737, loss=2.129610300064087
I0212 10:02:32.131942 140144802662208 spec.py:321] Evaluating on the training split.
I0212 10:02:35.126734 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 10:05:07.912963 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 10:05:10.603076 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 10:07:37.046576 140144802662208 spec.py:349] Evaluating on the test split.
I0212 10:07:39.729740 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 10:09:49.810960 140144802662208 submission_runner.py:408] Time since start: 6223.31s, 	Step: 9648, 	{'train/accuracy': 0.592995285987854, 'train/loss': 2.1528778076171875, 'train/bleu': 28.409135222092637, 'validation/accuracy': 0.6077048182487488, 'validation/loss': 2.034355640411377, 'validation/bleu': 24.757049137021426, 'validation/num_examples': 3000, 'test/accuracy': 0.6137005686759949, 'test/loss': 1.992855429649353, 'test/bleu': 23.58660872714324, 'test/num_examples': 3003, 'score': 3390.4426593780518, 'total_duration': 6223.30933713913, 'accumulated_submission_time': 3390.4426593780518, 'accumulated_eval_time': 2832.442953109741, 'accumulated_logging_time': 0.10341739654541016}
I0212 10:09:49.828474 139975029352192 logging_writer.py:48] [9648] accumulated_eval_time=2832.442953, accumulated_logging_time=0.103417, accumulated_submission_time=3390.442659, global_step=9648, preemption_count=0, score=3390.442659, test/accuracy=0.613701, test/bleu=23.586609, test/loss=1.992855, test/num_examples=3003, total_duration=6223.309337, train/accuracy=0.592995, train/bleu=28.409135, train/loss=2.152878, validation/accuracy=0.607705, validation/bleu=24.757049, validation/loss=2.034356, validation/num_examples=3000
I0212 10:10:08.218374 139975020959488 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.37233927845954895, loss=2.0608224868774414
I0212 10:10:42.952670 139975029352192 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.35335415601730347, loss=2.133481979370117
I0212 10:11:17.765252 139975020959488 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3451189398765564, loss=2.1142334938049316
I0212 10:11:52.573059 139975029352192 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3656415045261383, loss=2.080944299697876
I0212 10:12:27.401170 139975020959488 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.3538918197154999, loss=2.2102668285369873
I0212 10:13:02.229754 139975029352192 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.38722705841064453, loss=2.1062839031219482
I0212 10:13:37.021724 139975020959488 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.36918213963508606, loss=2.1033577919006348
I0212 10:14:11.803245 139975029352192 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3390131890773773, loss=2.1050939559936523
I0212 10:14:46.596937 139975020959488 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.31663909554481506, loss=2.2113778591156006
I0212 10:15:21.407752 139975029352192 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.337004691362381, loss=2.1184980869293213
I0212 10:15:56.213615 139975020959488 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3265024423599243, loss=2.168458938598633
I0212 10:16:31.022115 139975029352192 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3199106454849243, loss=2.1232657432556152
I0212 10:17:05.827750 139975020959488 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.2991124391555786, loss=2.0882978439331055
I0212 10:17:40.623529 139975029352192 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3758450746536255, loss=2.2188427448272705
I0212 10:18:15.422629 139975020959488 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.30846062302589417, loss=2.018472909927368
I0212 10:18:50.233452 139975029352192 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.34763801097869873, loss=2.027019739151001
I0212 10:19:25.049543 139975020959488 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.3080034852027893, loss=2.0973615646362305
I0212 10:19:59.843317 139975029352192 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.32644274830818176, loss=2.0631964206695557
I0212 10:20:34.662969 139975020959488 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2956233024597168, loss=2.0889155864715576
I0212 10:21:09.464211 139975029352192 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.31514057517051697, loss=2.049464225769043
I0212 10:21:44.262012 139975020959488 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3061824440956116, loss=2.1080641746520996
I0212 10:22:19.046298 139975029352192 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.34339606761932373, loss=1.9583933353424072
I0212 10:22:54.009779 139975020959488 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.3054332137107849, loss=2.073164224624634
I0212 10:23:28.809882 139975029352192 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3815529942512512, loss=2.104215145111084
I0212 10:23:50.092625 140144802662208 spec.py:321] Evaluating on the training split.
I0212 10:23:53.068488 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 10:26:46.804945 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 10:26:49.517987 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 10:29:43.546399 140144802662208 spec.py:349] Evaluating on the test split.
I0212 10:29:46.247325 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 10:32:07.187713 140144802662208 submission_runner.py:408] Time since start: 7560.69s, 	Step: 12063, 	{'train/accuracy': 0.6006006598472595, 'train/loss': 2.061574697494507, 'train/bleu': 28.907384311822263, 'validation/accuracy': 0.6192111372947693, 'validation/loss': 1.9119383096694946, 'validation/bleu': 25.726427060385753, 'validation/num_examples': 3000, 'test/accuracy': 0.6264017224311829, 'test/loss': 1.8653494119644165, 'test/bleu': 24.72895569223176, 'test/num_examples': 3003, 'score': 4230.618235588074, 'total_duration': 7560.686071395874, 'accumulated_submission_time': 4230.618235588074, 'accumulated_eval_time': 3329.5379779338837, 'accumulated_logging_time': 0.13108301162719727}
I0212 10:32:07.207632 139975020959488 logging_writer.py:48] [12063] accumulated_eval_time=3329.537978, accumulated_logging_time=0.131083, accumulated_submission_time=4230.618236, global_step=12063, preemption_count=0, score=4230.618236, test/accuracy=0.626402, test/bleu=24.728956, test/loss=1.865349, test/num_examples=3003, total_duration=7560.686071, train/accuracy=0.600601, train/bleu=28.907384, train/loss=2.061575, validation/accuracy=0.619211, validation/bleu=25.726427, validation/loss=1.911938, validation/num_examples=3000
I0212 10:32:20.359722 139975029352192 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3404427170753479, loss=1.9991934299468994
I0212 10:32:55.008353 139975020959488 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.2963796854019165, loss=2.0632143020629883
I0212 10:33:29.781204 139975029352192 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.3128662407398224, loss=1.9641377925872803
I0212 10:34:04.556044 139975020959488 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3059404194355011, loss=2.0134317874908447
I0212 10:34:39.367514 139975029352192 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.29679635167121887, loss=2.1219489574432373
I0212 10:35:14.196169 139975020959488 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.2926284968852997, loss=2.066492795944214
I0212 10:35:49.000590 139975029352192 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.29704567790031433, loss=2.106855630874634
I0212 10:36:23.807481 139975020959488 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.2866511046886444, loss=2.058661937713623
I0212 10:36:58.599752 139975029352192 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.29184040427207947, loss=1.9567344188690186
I0212 10:37:33.367598 139975020959488 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.28142595291137695, loss=1.9782960414886475
I0212 10:38:08.152417 139975029352192 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2969334125518799, loss=2.018143653869629
I0212 10:38:42.959365 139975020959488 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.2842833399772644, loss=2.021723508834839
I0212 10:39:17.760858 139975029352192 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.27539223432540894, loss=1.9844458103179932
I0212 10:39:52.548828 139975020959488 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.2764124274253845, loss=2.0443875789642334
I0212 10:40:27.342058 139975029352192 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.31941527128219604, loss=1.9893749952316284
I0212 10:41:02.116481 139975020959488 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.31219443678855896, loss=1.9872304201126099
I0212 10:41:36.903898 139975029352192 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2901670038700104, loss=2.0116870403289795
I0212 10:42:11.691947 139975020959488 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2841542065143585, loss=1.9269206523895264
I0212 10:42:46.457879 139975029352192 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.30715686082839966, loss=2.0886662006378174
I0212 10:43:21.234197 139975020959488 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3015325665473938, loss=1.985174298286438
I0212 10:43:56.025717 139975029352192 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.2717624008655548, loss=1.9981071949005127
I0212 10:44:30.823065 139975020959488 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.26448044180870056, loss=1.995057225227356
I0212 10:45:05.606126 139975029352192 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.2750338613986969, loss=1.9168637990951538
I0212 10:45:40.393491 139975020959488 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2541239559650421, loss=1.9876925945281982
I0212 10:46:07.230565 140144802662208 spec.py:321] Evaluating on the training split.
I0212 10:46:10.204996 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 10:49:35.091985 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 10:49:37.796769 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 10:52:05.279171 140144802662208 spec.py:349] Evaluating on the test split.
I0212 10:52:07.986241 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 10:54:20.540903 140144802662208 submission_runner.py:408] Time since start: 8894.04s, 	Step: 14479, 	{'train/accuracy': 0.6118413209915161, 'train/loss': 1.9555613994598389, 'train/bleu': 29.58988159437644, 'validation/accuracy': 0.6300107836723328, 'validation/loss': 1.8212212324142456, 'validation/bleu': 26.352692375606615, 'validation/num_examples': 3000, 'test/accuracy': 0.6380454301834106, 'test/loss': 1.7743580341339111, 'test/bleu': 25.639332507671263, 'test/num_examples': 3003, 'score': 5070.55059671402, 'total_duration': 8894.039239883423, 'accumulated_submission_time': 5070.55059671402, 'accumulated_eval_time': 3822.8482298851013, 'accumulated_logging_time': 0.16233038902282715}
I0212 10:54:20.560598 139975029352192 logging_writer.py:48] [14479] accumulated_eval_time=3822.848230, accumulated_logging_time=0.162330, accumulated_submission_time=5070.550597, global_step=14479, preemption_count=0, score=5070.550597, test/accuracy=0.638045, test/bleu=25.639333, test/loss=1.774358, test/num_examples=3003, total_duration=8894.039240, train/accuracy=0.611841, train/bleu=29.589882, train/loss=1.955561, validation/accuracy=0.630011, validation/bleu=26.352692, validation/loss=1.821221, validation/num_examples=3000
I0212 10:54:28.200695 139975020959488 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.25780415534973145, loss=1.911097526550293
I0212 10:55:02.882216 139975029352192 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.26916933059692383, loss=1.9867944717407227
I0212 10:55:37.604452 139975020959488 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2770979404449463, loss=2.0205113887786865
I0212 10:56:12.398909 139975029352192 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.31872111558914185, loss=1.9654861688613892
I0212 10:56:47.180228 139975020959488 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3001381754875183, loss=2.0038907527923584
I0212 10:57:21.974367 139975029352192 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.29964250326156616, loss=1.9415167570114136
I0212 10:57:56.775020 139975020959488 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.28648391366004944, loss=2.0189123153686523
I0212 10:58:31.569109 139975029352192 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2707538902759552, loss=2.031744956970215
I0212 10:59:06.346722 139975020959488 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.25675374269485474, loss=1.967555046081543
I0212 10:59:41.194420 139975029352192 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.2746454179286957, loss=1.9098224639892578
I0212 11:00:15.997738 139975020959488 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.31614670157432556, loss=1.890059232711792
I0212 11:00:50.797108 139975029352192 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.33389198780059814, loss=1.9842718839645386
I0212 11:01:25.586609 139975020959488 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.34250596165657043, loss=2.0031754970550537
I0212 11:02:00.359128 139975029352192 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3871878385543823, loss=1.9236223697662354
I0212 11:02:35.143980 139975020959488 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.28715255856513977, loss=1.966184377670288
I0212 11:03:09.964372 139975029352192 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3298285901546478, loss=1.83098566532135
I0212 11:03:44.754036 139975020959488 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.30356112122535706, loss=1.9651678800582886
I0212 11:04:19.562188 139975029352192 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.2945398688316345, loss=1.8650944232940674
I0212 11:04:54.404938 139975020959488 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.323068767786026, loss=1.9032065868377686
I0212 11:05:29.184755 139975029352192 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.40280190110206604, loss=1.8703795671463013
I0212 11:06:03.970108 139975020959488 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.364090234041214, loss=1.995100736618042
I0212 11:06:38.753149 139975029352192 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3592236042022705, loss=1.8265341520309448
I0212 11:07:13.542045 139975020959488 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3147846460342407, loss=1.974053144454956
I0212 11:07:48.334134 139975029352192 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.30941158533096313, loss=1.8758370876312256
I0212 11:08:20.762753 140144802662208 spec.py:321] Evaluating on the training split.
I0212 11:08:23.736761 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 11:11:32.595277 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 11:11:35.283166 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 11:14:06.092335 140144802662208 spec.py:349] Evaluating on the test split.
I0212 11:14:08.779298 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 11:16:24.940607 140144802662208 submission_runner.py:408] Time since start: 10218.44s, 	Step: 16895, 	{'train/accuracy': 0.6211254596710205, 'train/loss': 1.907320499420166, 'train/bleu': 29.799160596214755, 'validation/accuracy': 0.6405500173568726, 'validation/loss': 1.7542517185211182, 'validation/bleu': 26.869630513464116, 'validation/num_examples': 3000, 'test/accuracy': 0.6462843418121338, 'test/loss': 1.706995964050293, 'test/bleu': 26.05308990904883, 'test/num_examples': 3003, 'score': 5910.659248828888, 'total_duration': 10218.438910245895, 'accumulated_submission_time': 5910.659248828888, 'accumulated_eval_time': 4307.02596282959, 'accumulated_logging_time': 0.19454002380371094}
I0212 11:16:24.957937 139975020959488 logging_writer.py:48] [16895] accumulated_eval_time=4307.025963, accumulated_logging_time=0.194540, accumulated_submission_time=5910.659249, global_step=16895, preemption_count=0, score=5910.659249, test/accuracy=0.646284, test/bleu=26.053090, test/loss=1.706996, test/num_examples=3003, total_duration=10218.438910, train/accuracy=0.621125, train/bleu=29.799161, train/loss=1.907320, validation/accuracy=0.640550, validation/bleu=26.869631, validation/loss=1.754252, validation/num_examples=3000
I0212 11:16:27.057917 139975029352192 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.2872787117958069, loss=1.9663020372390747
I0212 11:17:01.772775 139975020959488 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3258388936519623, loss=2.0405068397521973
I0212 11:17:36.506525 139975029352192 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.31774240732192993, loss=1.959641695022583
I0212 11:18:11.286925 139975020959488 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3370964229106903, loss=1.8647053241729736
I0212 11:18:46.069118 139975029352192 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.36171436309814453, loss=1.9409843683242798
I0212 11:19:20.872347 139975020959488 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.39216896891593933, loss=1.8742161989212036
I0212 11:19:55.652158 139975029352192 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.31972721219062805, loss=1.9138392210006714
I0212 11:20:30.452925 139975020959488 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.33067283034324646, loss=1.9415616989135742
I0212 11:21:05.239037 139975029352192 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3342888355255127, loss=1.86113703250885
I0212 11:21:40.030811 139975020959488 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2860215902328491, loss=1.8987138271331787
I0212 11:22:14.806577 139975029352192 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.35813888907432556, loss=1.8664909601211548
I0212 11:22:49.610023 139975020959488 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.30896058678627014, loss=1.8958706855773926
I0212 11:23:24.401531 139975029352192 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3168436884880066, loss=1.8288538455963135
I0212 11:23:59.191875 139975020959488 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.34122538566589355, loss=1.9987046718597412
I0212 11:24:34.001257 139975029352192 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.4508105218410492, loss=1.827087640762329
I0212 11:25:08.791303 139975020959488 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2985280752182007, loss=1.8917473554611206
I0212 11:25:43.577416 139975029352192 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.328564316034317, loss=1.9030574560165405
I0212 11:26:18.352992 139975020959488 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.41042718291282654, loss=4.032798767089844
I0212 11:26:53.075250 139975029352192 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.28583070635795593, loss=3.883889675140381
I0212 11:27:27.804414 139975020959488 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.2103265523910522, loss=2.6138408184051514
I0212 11:28:02.606304 139975029352192 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3508596420288086, loss=1.868265151977539
I0212 11:28:37.411241 139975020959488 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.33480119705200195, loss=1.8372139930725098
I0212 11:29:12.204164 139975029352192 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.35584014654159546, loss=1.8933173418045044
I0212 11:29:46.981628 139975020959488 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.29665642976760864, loss=1.8588358163833618
I0212 11:30:21.771635 139975029352192 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.2878319323062897, loss=1.7837055921554565
I0212 11:30:24.977117 140144802662208 spec.py:321] Evaluating on the training split.
I0212 11:30:27.956048 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 11:33:20.594898 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 11:33:23.283481 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 11:36:09.199825 140144802662208 spec.py:349] Evaluating on the test split.
I0212 11:36:11.887492 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 11:38:48.550759 140144802662208 submission_runner.py:408] Time since start: 11562.05s, 	Step: 19311, 	{'train/accuracy': 0.6253435015678406, 'train/loss': 1.8560582399368286, 'train/bleu': 30.119357369554518, 'validation/accuracy': 0.6459808349609375, 'validation/loss': 1.716254472732544, 'validation/bleu': 27.16554742317671, 'validation/num_examples': 3000, 'test/accuracy': 0.6546743512153625, 'test/loss': 1.6618460416793823, 'test/bleu': 26.37794400092443, 'test/num_examples': 3003, 'score': 6750.589684724808, 'total_duration': 11562.049136638641, 'accumulated_submission_time': 6750.589684724808, 'accumulated_eval_time': 4810.599547386169, 'accumulated_logging_time': 0.22215533256530762}
I0212 11:38:48.569042 139975020959488 logging_writer.py:48] [19311] accumulated_eval_time=4810.599547, accumulated_logging_time=0.222155, accumulated_submission_time=6750.589685, global_step=19311, preemption_count=0, score=6750.589685, test/accuracy=0.654674, test/bleu=26.377944, test/loss=1.661846, test/num_examples=3003, total_duration=11562.049137, train/accuracy=0.625344, train/bleu=30.119357, train/loss=1.856058, validation/accuracy=0.645981, validation/bleu=27.165547, validation/loss=1.716254, validation/num_examples=3000
I0212 11:39:19.754343 139975029352192 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.33468326926231384, loss=1.9116549491882324
I0212 11:39:54.494678 139975020959488 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.32171866297721863, loss=1.7840269804000854
I0212 11:40:29.308464 139975029352192 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.336513876914978, loss=1.9717907905578613
I0212 11:41:04.083727 139975020959488 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3716231882572174, loss=1.8641029596328735
I0212 11:41:38.840051 139975029352192 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3829825520515442, loss=1.8655208349227905
I0212 11:42:13.637690 139975020959488 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3263590335845947, loss=1.9222087860107422
I0212 11:42:48.446409 139975029352192 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.34260812401771545, loss=1.8685435056686401
I0212 11:43:23.215659 139975020959488 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.3047375977039337, loss=1.7794495820999146
I0212 11:43:57.995448 139975029352192 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3518327474594116, loss=1.8154127597808838
I0212 11:44:32.813351 139975020959488 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.34703177213668823, loss=1.8951555490493774
I0212 11:45:07.625360 139975029352192 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.3311697244644165, loss=1.935983419418335
I0212 11:45:42.439096 139975020959488 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3381245732307434, loss=1.8432929515838623
I0212 11:46:17.227691 139975029352192 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3462681472301483, loss=1.7527981996536255
I0212 11:46:52.002127 139975020959488 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.3545483350753784, loss=1.7819476127624512
I0212 11:47:26.782685 139975029352192 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.36979496479034424, loss=1.8761197328567505
I0212 11:48:01.565805 139975020959488 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.33791476488113403, loss=1.8327364921569824
I0212 11:48:36.340805 139975029352192 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.3899800777435303, loss=1.8381597995758057
I0212 11:49:11.145236 139975020959488 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.3306359350681305, loss=1.874734878540039
I0212 11:49:45.934220 139975029352192 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.3495471775531769, loss=1.896013617515564
I0212 11:50:20.726757 139975020959488 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.39778944849967957, loss=1.8947808742523193
I0212 11:50:55.509948 139975029352192 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.013167142868042, loss=1.9249744415283203
I0212 11:51:30.280197 139975020959488 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5885605216026306, loss=1.8855596780776978
I0212 11:52:05.061909 139975029352192 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.38333210349082947, loss=1.7793935537338257
I0212 11:52:39.875186 139975020959488 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.37058353424072266, loss=1.7550206184387207
I0212 11:52:48.647962 140144802662208 spec.py:321] Evaluating on the training split.
I0212 11:52:51.640000 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 11:55:43.592992 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 11:55:46.277574 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 11:58:16.327493 140144802662208 spec.py:349] Evaluating on the test split.
I0212 11:58:19.007318 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 12:00:34.506129 140144802662208 submission_runner.py:408] Time since start: 12868.00s, 	Step: 21727, 	{'train/accuracy': 0.625741720199585, 'train/loss': 1.8575769662857056, 'train/bleu': 30.296728445860584, 'validation/accuracy': 0.6483986377716064, 'validation/loss': 1.690137267112732, 'validation/bleu': 27.708869311230746, 'validation/num_examples': 3000, 'test/accuracy': 0.6591482162475586, 'test/loss': 1.6333097219467163, 'test/bleu': 27.06400431392655, 'test/num_examples': 3003, 'score': 7590.577194213867, 'total_duration': 12868.004507541656, 'accumulated_submission_time': 7590.577194213867, 'accumulated_eval_time': 5276.457659244537, 'accumulated_logging_time': 0.25283288955688477}
I0212 12:00:34.524253 139975029352192 logging_writer.py:48] [21727] accumulated_eval_time=5276.457659, accumulated_logging_time=0.252833, accumulated_submission_time=7590.577194, global_step=21727, preemption_count=0, score=7590.577194, test/accuracy=0.659148, test/bleu=27.064004, test/loss=1.633310, test/num_examples=3003, total_duration=12868.004508, train/accuracy=0.625742, train/bleu=30.296728, train/loss=1.857577, validation/accuracy=0.648399, validation/bleu=27.708869, validation/loss=1.690137, validation/num_examples=3000
I0212 12:01:00.208921 139975020959488 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.3648579716682434, loss=1.8378031253814697
I0212 12:01:34.922973 139975029352192 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.3909030556678772, loss=1.9035346508026123
I0212 12:02:09.700864 139975020959488 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.35802900791168213, loss=1.9847480058670044
I0212 12:02:44.522520 139975029352192 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.42715689539909363, loss=1.9250763654708862
I0212 12:03:19.301300 139975020959488 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.3824891149997711, loss=1.924797773361206
I0212 12:03:54.087648 139975029352192 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3361113667488098, loss=1.8853131532669067
I0212 12:04:28.893823 139975020959488 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.4475337862968445, loss=1.9180898666381836
I0212 12:05:03.697612 139975029352192 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3668944537639618, loss=1.8727468252182007
I0212 12:05:38.483550 139975020959488 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3910038471221924, loss=1.8798682689666748
I0212 12:06:13.273627 139975029352192 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.3843393921852112, loss=1.8202074766159058
I0212 12:06:48.117094 139975020959488 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.3803398311138153, loss=1.806331992149353
I0212 12:07:22.971890 139975029352192 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3887810707092285, loss=1.8701472282409668
I0212 12:07:57.795878 139975020959488 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.38192737102508545, loss=1.8357865810394287
I0212 12:08:32.752893 139975029352192 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.34067657589912415, loss=1.823498010635376
I0212 12:09:07.543837 139975020959488 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.4325723946094513, loss=1.874656081199646
I0212 12:09:42.316385 139975029352192 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.4160415828227997, loss=1.8440262079238892
I0212 12:10:17.115885 139975020959488 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.3662043809890747, loss=1.906697392463684
I0212 12:10:51.920432 139975029352192 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.4562883973121643, loss=1.8114655017852783
I0212 12:11:26.766881 139975020959488 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.3665110766887665, loss=1.9009681940078735
I0212 12:12:01.595582 139975029352192 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.44310566782951355, loss=1.806796908378601
I0212 12:12:36.410255 139975020959488 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.34332236647605896, loss=1.8177567720413208
I0212 12:13:11.216629 139975029352192 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.4338340163230896, loss=1.8663750886917114
I0212 12:13:45.989643 139975020959488 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.49332693219184875, loss=1.8422273397445679
I0212 12:14:20.773290 139975029352192 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.3396449387073517, loss=1.8392974138259888
I0212 12:14:34.802342 140144802662208 spec.py:321] Evaluating on the training split.
I0212 12:14:37.774324 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 12:17:53.495277 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 12:17:56.189721 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 12:20:48.761731 140144802662208 spec.py:349] Evaluating on the test split.
I0212 12:20:51.444771 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 12:23:28.110211 140144802662208 submission_runner.py:408] Time since start: 14241.61s, 	Step: 24142, 	{'train/accuracy': 0.6286821961402893, 'train/loss': 1.842318058013916, 'train/bleu': 30.943242981301132, 'validation/accuracy': 0.6500725150108337, 'validation/loss': 1.673073410987854, 'validation/bleu': 27.84176105484295, 'validation/num_examples': 3000, 'test/accuracy': 0.6639590859413147, 'test/loss': 1.6046804189682007, 'test/bleu': 27.245981275229024, 'test/num_examples': 3003, 'score': 8430.762261390686, 'total_duration': 14241.60857963562, 'accumulated_submission_time': 8430.762261390686, 'accumulated_eval_time': 5809.7654638290405, 'accumulated_logging_time': 0.28135228157043457}
I0212 12:23:28.128211 139975020959488 logging_writer.py:48] [24142] accumulated_eval_time=5809.765464, accumulated_logging_time=0.281352, accumulated_submission_time=8430.762261, global_step=24142, preemption_count=0, score=8430.762261, test/accuracy=0.663959, test/bleu=27.245981, test/loss=1.604680, test/num_examples=3003, total_duration=14241.608580, train/accuracy=0.628682, train/bleu=30.943243, train/loss=1.842318, validation/accuracy=0.650073, validation/bleu=27.841761, validation/loss=1.673073, validation/num_examples=3000
I0212 12:23:48.556597 139975029352192 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.39950641989707947, loss=1.830127477645874
I0212 12:24:23.254578 139975020959488 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.36036476492881775, loss=1.8460798263549805
I0212 12:24:58.024298 139975029352192 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4542525112628937, loss=1.765893578529358
I0212 12:25:32.813610 139975020959488 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.34433677792549133, loss=1.7678940296173096
I0212 12:26:07.610029 139975029352192 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.35135418176651, loss=1.8727713823318481
I0212 12:26:42.378478 139975020959488 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.36990001797676086, loss=1.7850046157836914
I0212 12:27:17.187921 139975029352192 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.361635684967041, loss=1.8011118173599243
I0212 12:27:51.973697 139975020959488 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3859986662864685, loss=1.8775088787078857
I0212 12:28:26.738938 139975029352192 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.4082779884338379, loss=1.7461497783660889
I0212 12:29:01.498280 139975020959488 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.36544740200042725, loss=1.8612301349639893
I0212 12:29:36.255657 139975029352192 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.47045230865478516, loss=1.8970599174499512
I0212 12:30:11.034826 139975020959488 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.3890320956707001, loss=1.7853118181228638
I0212 12:30:45.794923 139975029352192 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.45016464591026306, loss=1.7829657793045044
I0212 12:31:20.559226 139975020959488 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.378168523311615, loss=1.7560747861862183
I0212 12:31:55.314242 139975029352192 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.407659649848938, loss=1.8006529808044434
I0212 12:32:30.110236 139975020959488 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.4471966326236725, loss=1.8039684295654297
I0212 12:33:04.933658 139975029352192 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.3790375292301178, loss=1.8782585859298706
I0212 12:33:39.700125 139975020959488 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.35671523213386536, loss=1.7491817474365234
I0212 12:34:14.470315 139975029352192 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.37316930294036865, loss=1.824245572090149
I0212 12:34:49.261080 139975020959488 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.44282224774360657, loss=1.8519489765167236
I0212 12:35:24.094427 139975029352192 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.4201044738292694, loss=1.8641220331192017
I0212 12:35:58.857299 139975020959488 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.4129028916358948, loss=1.821258544921875
I0212 12:36:33.665664 139975029352192 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.4256567060947418, loss=1.8124363422393799
I0212 12:37:08.435906 139975020959488 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.35014328360557556, loss=1.8245822191238403
I0212 12:37:28.333146 140144802662208 spec.py:321] Evaluating on the training split.
I0212 12:37:31.335228 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 12:40:34.981605 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 12:40:37.680639 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 12:43:38.034981 140144802662208 spec.py:349] Evaluating on the test split.
I0212 12:43:40.722246 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 12:46:27.038000 140144802662208 submission_runner.py:408] Time since start: 15620.54s, 	Step: 26559, 	{'train/accuracy': 0.6353502869606018, 'train/loss': 1.772486925125122, 'train/bleu': 30.712247493082852, 'validation/accuracy': 0.6529862880706787, 'validation/loss': 1.6593254804611206, 'validation/bleu': 27.824232335672157, 'validation/num_examples': 3000, 'test/accuracy': 0.6608215570449829, 'test/loss': 1.6026358604431152, 'test/bleu': 26.744297453024025, 'test/num_examples': 3003, 'score': 9270.876539945602, 'total_duration': 15620.536381721497, 'accumulated_submission_time': 9270.876539945602, 'accumulated_eval_time': 6348.470281600952, 'accumulated_logging_time': 0.3095395565032959}
I0212 12:46:27.057438 139975029352192 logging_writer.py:48] [26559] accumulated_eval_time=6348.470282, accumulated_logging_time=0.309540, accumulated_submission_time=9270.876540, global_step=26559, preemption_count=0, score=9270.876540, test/accuracy=0.660822, test/bleu=26.744297, test/loss=1.602636, test/num_examples=3003, total_duration=15620.536382, train/accuracy=0.635350, train/bleu=30.712247, train/loss=1.772487, validation/accuracy=0.652986, validation/bleu=27.824232, validation/loss=1.659325, validation/num_examples=3000
I0212 12:46:41.607341 139975020959488 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.4245719909667969, loss=1.8345270156860352
I0212 12:47:16.263187 139975029352192 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.39803004264831543, loss=1.7600865364074707
I0212 12:47:51.014156 139975020959488 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.418255478143692, loss=1.8194704055786133
I0212 12:48:25.778217 139975029352192 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.39887261390686035, loss=1.813417911529541
I0212 12:49:00.556917 139975020959488 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.3852092921733856, loss=1.8313831090927124
I0212 12:49:35.327835 139975029352192 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.4685125946998596, loss=1.7775006294250488
I0212 12:50:10.108327 139975020959488 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3850567638874054, loss=1.8411643505096436
I0212 12:50:44.896368 139975029352192 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.36595675349235535, loss=1.8471039533615112
I0212 12:51:19.680643 139975020959488 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.3785352408885956, loss=1.7868711948394775
I0212 12:51:54.473009 139975029352192 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.4818664789199829, loss=1.8888436555862427
I0212 12:52:29.254744 139975020959488 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.44124042987823486, loss=1.8388080596923828
I0212 12:53:04.047923 139975029352192 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.399736225605011, loss=1.8831346035003662
I0212 12:53:38.824465 139975020959488 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.4493517577648163, loss=1.8015345335006714
I0212 12:54:13.617088 139975029352192 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.42359134554862976, loss=1.8300576210021973
I0212 12:54:48.456871 139975020959488 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.45502206683158875, loss=1.8555599451065063
I0212 12:55:23.230400 139975029352192 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.40618637204170227, loss=1.8542119264602661
I0212 12:55:58.018491 139975020959488 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6522287726402283, loss=1.7638568878173828
I0212 12:56:32.808718 139975029352192 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.8110136389732361, loss=1.7387096881866455
I0212 12:57:07.586926 139975020959488 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.4410604238510132, loss=1.8088141679763794
I0212 12:57:42.390453 139975029352192 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.42377662658691406, loss=1.8421307802200317
I0212 12:58:17.177533 139975020959488 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.4251185357570648, loss=1.7209529876708984
I0212 12:58:51.986580 139975029352192 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.47475820779800415, loss=1.845016360282898
I0212 12:59:26.793295 139975020959488 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3955661952495575, loss=1.7973054647445679
I0212 13:00:01.583869 139975029352192 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.38031646609306335, loss=1.8558083772659302
I0212 13:00:27.380606 140144802662208 spec.py:321] Evaluating on the training split.
I0212 13:00:30.350188 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 13:03:12.415837 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 13:03:15.098777 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 13:05:42.878070 140144802662208 spec.py:349] Evaluating on the test split.
I0212 13:05:45.554498 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 13:08:05.903028 140144802662208 submission_runner.py:408] Time since start: 16919.40s, 	Step: 28976, 	{'train/accuracy': 0.6322528719902039, 'train/loss': 1.8003666400909424, 'train/bleu': 30.726127530034873, 'validation/accuracy': 0.6520811915397644, 'validation/loss': 1.648480772972107, 'validation/bleu': 27.845179168567565, 'validation/num_examples': 3000, 'test/accuracy': 0.6632967591285706, 'test/loss': 1.5888539552688599, 'test/bleu': 27.30567537970401, 'test/num_examples': 3003, 'score': 10111.109763383865, 'total_duration': 16919.401398181915, 'accumulated_submission_time': 10111.109763383865, 'accumulated_eval_time': 6806.992643594742, 'accumulated_logging_time': 0.33898496627807617}
I0212 13:08:05.921785 139975020959488 logging_writer.py:48] [28976] accumulated_eval_time=6806.992644, accumulated_logging_time=0.338985, accumulated_submission_time=10111.109763, global_step=28976, preemption_count=0, score=10111.109763, test/accuracy=0.663297, test/bleu=27.305675, test/loss=1.588854, test/num_examples=3003, total_duration=16919.401398, train/accuracy=0.632253, train/bleu=30.726128, train/loss=1.800367, validation/accuracy=0.652081, validation/bleu=27.845179, validation/loss=1.648481, validation/num_examples=3000
I0212 13:08:14.599947 139975029352192 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.4543319642543793, loss=1.8418630361557007
I0212 13:08:49.249187 139975020959488 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.43693190813064575, loss=1.798519492149353
I0212 13:09:24.005154 139975029352192 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.4508461356163025, loss=1.8302685022354126
I0212 13:09:58.906183 139975020959488 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.4494117796421051, loss=1.848861813545227
I0212 13:10:33.715417 139975029352192 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.34118229150772095, loss=1.814815878868103
I0212 13:11:08.497386 139975020959488 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.3926108479499817, loss=1.6996816396713257
I0212 13:11:43.275299 139975029352192 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.36912351846694946, loss=1.7403727769851685
I0212 13:12:18.060991 139975020959488 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.47435319423675537, loss=1.8701794147491455
I0212 13:12:52.864500 139975029352192 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.4382390081882477, loss=1.8184597492218018
I0212 13:13:27.679542 139975020959488 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.3985978364944458, loss=1.808051347732544
I0212 13:14:02.483742 139975029352192 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.4706808030605316, loss=1.7973688840866089
I0212 13:14:37.290181 139975020959488 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.38128015398979187, loss=1.8081289529800415
I0212 13:15:12.084726 139975029352192 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.36602479219436646, loss=1.8009717464447021
I0212 13:15:46.884779 139975020959488 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3646116256713867, loss=1.7661134004592896
I0212 13:16:21.675330 139975029352192 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.39365634322166443, loss=1.7733553647994995
I0212 13:16:56.439272 139975020959488 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.4333273470401764, loss=1.7821978330612183
I0212 13:17:31.238795 139975029352192 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.40211012959480286, loss=1.8225324153900146
I0212 13:18:06.020966 139975020959488 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.4396912753582001, loss=1.7872458696365356
I0212 13:18:40.783999 139975029352192 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.43068087100982666, loss=1.7521551847457886
I0212 13:19:15.576821 139975020959488 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3787340223789215, loss=1.7570081949234009
I0212 13:19:50.332295 139975029352192 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.35618117451667786, loss=1.729077935218811
I0212 13:20:25.094172 139975020959488 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.3909781575202942, loss=1.782281517982483
I0212 13:20:59.870510 139975029352192 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.39125049114227295, loss=1.834008812904358
I0212 13:21:34.627046 139975020959488 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.4186858832836151, loss=1.8203564882278442
I0212 13:22:05.993250 140144802662208 spec.py:321] Evaluating on the training split.
I0212 13:22:08.960559 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 13:26:17.435452 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 13:26:20.120094 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 13:28:55.436683 140144802662208 spec.py:349] Evaluating on the test split.
I0212 13:28:58.123622 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 13:31:28.477010 140144802662208 submission_runner.py:408] Time since start: 18321.98s, 	Step: 31392, 	{'train/accuracy': 0.6668255925178528, 'train/loss': 1.5802048444747925, 'train/bleu': 33.53180261019265, 'validation/accuracy': 0.6552553772926331, 'validation/loss': 1.6375457048416138, 'validation/bleu': 27.770932078759643, 'validation/num_examples': 3000, 'test/accuracy': 0.6667131781578064, 'test/loss': 1.5764648914337158, 'test/bleu': 27.34374131007221, 'test/num_examples': 3003, 'score': 10951.089903831482, 'total_duration': 18321.97534775734, 'accumulated_submission_time': 10951.089903831482, 'accumulated_eval_time': 7369.476313352585, 'accumulated_logging_time': 0.36751580238342285}
I0212 13:31:28.499567 139975029352192 logging_writer.py:48] [31392] accumulated_eval_time=7369.476313, accumulated_logging_time=0.367516, accumulated_submission_time=10951.089904, global_step=31392, preemption_count=0, score=10951.089904, test/accuracy=0.666713, test/bleu=27.343741, test/loss=1.576465, test/num_examples=3003, total_duration=18321.975348, train/accuracy=0.666826, train/bleu=33.531803, train/loss=1.580205, validation/accuracy=0.655255, validation/bleu=27.770932, validation/loss=1.637546, validation/num_examples=3000
I0212 13:31:31.639118 139975020959488 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.7515749335289001, loss=1.8469949960708618
I0212 13:32:06.306462 139975029352192 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.4162192642688751, loss=1.822043538093567
I0212 13:32:41.052722 139975020959488 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.38031837344169617, loss=1.7620563507080078
I0212 13:33:15.874321 139975029352192 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.4754049777984619, loss=1.8659790754318237
I0212 13:33:50.673168 139975020959488 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.4206089675426483, loss=1.8528201580047607
I0212 13:34:25.455677 139975029352192 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.4557507038116455, loss=1.8502602577209473
I0212 13:35:00.216255 139975020959488 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.42857882380485535, loss=1.8184700012207031
I0212 13:35:34.987842 139975029352192 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.39797747135162354, loss=1.8320224285125732
I0212 13:36:09.755387 139975020959488 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.41213852167129517, loss=1.7997409105300903
I0212 13:36:44.536345 139975029352192 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.39733171463012695, loss=1.7202329635620117
I0212 13:37:19.329416 139975020959488 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.436315655708313, loss=1.8327724933624268
I0212 13:37:54.126872 139975029352192 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.39236438274383545, loss=1.6559113264083862
I0212 13:38:28.912534 139975020959488 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.4724617898464203, loss=1.8066139221191406
I0212 13:39:03.690027 139975029352192 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.5333961844444275, loss=1.8396342992782593
I0212 13:39:38.461831 139975020959488 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.41292598843574524, loss=1.9068001508712769
I0212 13:40:13.237060 139975029352192 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.3918692469596863, loss=1.7634046077728271
I0212 13:40:48.009953 139975020959488 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.40052470564842224, loss=1.7242885828018188
I0212 13:41:22.779742 139975029352192 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.42198416590690613, loss=1.806735873222351
I0212 13:41:57.574703 139975020959488 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.3955875635147095, loss=1.806432843208313
I0212 13:42:32.386112 139975029352192 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.5686467289924622, loss=1.6942156553268433
I0212 13:43:07.158519 139975020959488 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.4271920919418335, loss=1.8229827880859375
I0212 13:43:41.966134 139975029352192 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.38439059257507324, loss=1.7702527046203613
I0212 13:44:16.757010 139975020959488 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.41849344968795776, loss=1.8530865907669067
I0212 13:44:51.524202 139975029352192 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.43743205070495605, loss=1.8497405052185059
I0212 13:45:26.295176 139975020959488 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.45447465777397156, loss=1.7865787744522095
I0212 13:45:28.804913 140144802662208 spec.py:321] Evaluating on the training split.
I0212 13:45:31.775801 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 13:49:08.777813 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 13:49:11.476029 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 13:51:43.732837 140144802662208 spec.py:349] Evaluating on the test split.
I0212 13:51:46.405049 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 13:54:12.387682 140144802662208 submission_runner.py:408] Time since start: 19685.89s, 	Step: 33809, 	{'train/accuracy': 0.6364154815673828, 'train/loss': 1.7697381973266602, 'train/bleu': 30.883054872066605, 'validation/accuracy': 0.6572515964508057, 'validation/loss': 1.6273834705352783, 'validation/bleu': 28.077985667842412, 'validation/num_examples': 3000, 'test/accuracy': 0.665690541267395, 'test/loss': 1.5672624111175537, 'test/bleu': 27.31780751025964, 'test/num_examples': 3003, 'score': 11791.302706718445, 'total_duration': 19685.88606619835, 'accumulated_submission_time': 11791.302706718445, 'accumulated_eval_time': 7893.05903172493, 'accumulated_logging_time': 0.4018421173095703}
I0212 13:54:12.407307 139975029352192 logging_writer.py:48] [33809] accumulated_eval_time=7893.059032, accumulated_logging_time=0.401842, accumulated_submission_time=11791.302707, global_step=33809, preemption_count=0, score=11791.302707, test/accuracy=0.665691, test/bleu=27.317808, test/loss=1.567262, test/num_examples=3003, total_duration=19685.886066, train/accuracy=0.636415, train/bleu=30.883055, train/loss=1.769738, validation/accuracy=0.657252, validation/bleu=28.077986, validation/loss=1.627383, validation/num_examples=3000
I0212 13:54:44.313889 139975020959488 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5033249258995056, loss=1.8230643272399902
I0212 13:55:19.016921 139975029352192 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.4077298045158386, loss=1.804380178451538
I0212 13:55:53.756242 139975020959488 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.4565862715244293, loss=1.8536750078201294
I0212 13:56:28.485449 139975029352192 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.3871019184589386, loss=1.7958320379257202
I0212 13:57:03.232966 139975020959488 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.3977619707584381, loss=1.7991325855255127
I0212 13:57:38.010458 139975029352192 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.3475136160850525, loss=1.7383553981781006
I0212 13:58:12.737337 139975020959488 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.40149548649787903, loss=1.7677711248397827
I0212 13:58:47.499668 139975029352192 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.3929997682571411, loss=1.7926937341690063
I0212 13:59:22.249539 139975020959488 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.425453245639801, loss=1.7944499254226685
I0212 13:59:57.012519 139975029352192 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.5138201117515564, loss=1.673138976097107
I0212 14:00:31.777972 139975020959488 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.4113244116306305, loss=1.7769839763641357
I0212 14:01:06.506712 139975029352192 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6521703004837036, loss=1.8608055114746094
I0212 14:01:41.264910 139975020959488 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.4438590109348297, loss=1.8041694164276123
I0212 14:02:16.023757 139975029352192 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.42451992630958557, loss=1.7911227941513062
I0212 14:02:50.777961 139975020959488 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.38436850905418396, loss=1.711140513420105
I0212 14:03:25.580703 139975029352192 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.4256408214569092, loss=1.8017816543579102
I0212 14:04:00.359431 139975020959488 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.38335853815078735, loss=1.79396653175354
I0212 14:04:35.138955 139975029352192 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.35353150963783264, loss=1.7642992734909058
I0212 14:05:09.881236 139975020959488 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.34717243909835815, loss=1.7049607038497925
I0212 14:05:44.646234 139975029352192 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.3963303864002228, loss=1.8127570152282715
I0212 14:06:19.411581 139975020959488 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.4858030378818512, loss=1.8312232494354248
I0212 14:06:54.198287 139975029352192 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.4338177442550659, loss=1.7321221828460693
I0212 14:07:28.955713 139975020959488 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.3789335787296295, loss=1.7473981380462646
I0212 14:08:03.713359 139975029352192 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.36653831601142883, loss=1.775048851966858
I0212 14:08:12.477200 140144802662208 spec.py:321] Evaluating on the training split.
I0212 14:08:15.452719 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 14:11:10.258419 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 14:11:12.938925 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 14:13:51.741245 140144802662208 spec.py:349] Evaluating on the test split.
I0212 14:13:54.426169 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 14:16:17.722964 140144802662208 submission_runner.py:408] Time since start: 21011.22s, 	Step: 36227, 	{'train/accuracy': 0.6372659206390381, 'train/loss': 1.7720032930374146, 'train/bleu': 31.08953855694967, 'validation/accuracy': 0.6579459309577942, 'validation/loss': 1.6163133382797241, 'validation/bleu': 28.23293001497443, 'validation/num_examples': 3000, 'test/accuracy': 0.6670966148376465, 'test/loss': 1.5575518608093262, 'test/bleu': 27.478248748542704, 'test/num_examples': 3003, 'score': 12631.284592628479, 'total_duration': 21011.22130537033, 'accumulated_submission_time': 12631.284592628479, 'accumulated_eval_time': 8378.304702997208, 'accumulated_logging_time': 0.43168139457702637}
I0212 14:16:17.747920 139975020959488 logging_writer.py:48] [36227] accumulated_eval_time=8378.304703, accumulated_logging_time=0.431681, accumulated_submission_time=12631.284593, global_step=36227, preemption_count=0, score=12631.284593, test/accuracy=0.667097, test/bleu=27.478249, test/loss=1.557552, test/num_examples=3003, total_duration=21011.221305, train/accuracy=0.637266, train/bleu=31.089539, train/loss=1.772003, validation/accuracy=0.657946, validation/bleu=28.232930, validation/loss=1.616313, validation/num_examples=3000
I0212 14:16:43.422971 139975029352192 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.35586321353912354, loss=1.7403205633163452
I0212 14:17:18.194231 139975020959488 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3779776692390442, loss=1.7425711154937744
I0212 14:17:52.960105 139975029352192 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.39867594838142395, loss=1.727960228919983
I0212 14:18:27.764018 139975020959488 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.36368194222450256, loss=1.7198221683502197
I0212 14:19:02.596259 139975029352192 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.41944420337677, loss=1.7114155292510986
I0212 14:19:37.425110 139975020959488 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.40645116567611694, loss=1.7850265502929688
I0212 14:20:12.275322 139975029352192 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.3737040162086487, loss=1.8415076732635498
I0212 14:20:47.081136 139975020959488 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.30568528175354, loss=1.9459096193313599
I0212 14:21:21.896503 139975029352192 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.42934325337409973, loss=1.8236026763916016
I0212 14:21:56.712124 139975020959488 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.4370429515838623, loss=1.7655912637710571
I0212 14:22:31.515610 139975029352192 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.3776986300945282, loss=1.8348687887191772
I0212 14:23:06.410175 139975020959488 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.44153034687042236, loss=1.7569913864135742
I0212 14:23:41.172385 139975029352192 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.4170284569263458, loss=1.768112301826477
I0212 14:24:15.973023 139975020959488 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.38224083185195923, loss=1.7911489009857178
I0212 14:24:50.753720 139975029352192 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.4372152090072632, loss=1.7075339555740356
I0212 14:25:25.548135 139975020959488 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6052072048187256, loss=1.70505690574646
I0212 14:26:00.326970 139975029352192 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.39585304260253906, loss=1.8585072755813599
I0212 14:26:35.107859 139975020959488 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.40922802686691284, loss=1.757727026939392
I0212 14:27:09.876399 139975029352192 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.453262597322464, loss=1.788291096687317
I0212 14:27:44.628879 139975020959488 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.3624095916748047, loss=1.696938395500183
I0212 14:28:19.400453 139975029352192 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.39709869027137756, loss=1.7148518562316895
I0212 14:28:54.194225 139975020959488 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.34424158930778503, loss=1.7852362394332886
I0212 14:29:28.963906 139975029352192 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.404563844203949, loss=1.757856845855713
I0212 14:30:03.726637 139975020959488 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.32914119958877563, loss=1.709092617034912
I0212 14:30:18.037616 140144802662208 spec.py:321] Evaluating on the training split.
I0212 14:30:21.013663 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 14:33:10.623029 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 14:33:13.332813 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 14:35:41.288717 140144802662208 spec.py:349] Evaluating on the test split.
I0212 14:35:43.985574 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 14:38:00.885277 140144802662208 submission_runner.py:408] Time since start: 22314.38s, 	Step: 38643, 	{'train/accuracy': 0.6398760676383972, 'train/loss': 1.7399176359176636, 'train/bleu': 31.15971808188359, 'validation/accuracy': 0.6560736894607544, 'validation/loss': 1.6214781999588013, 'validation/bleu': 28.02467696226403, 'validation/num_examples': 3000, 'test/accuracy': 0.6660391688346863, 'test/loss': 1.5656033754348755, 'test/bleu': 27.417562134722917, 'test/num_examples': 3003, 'score': 13471.477101564407, 'total_duration': 22314.383660316467, 'accumulated_submission_time': 13471.477101564407, 'accumulated_eval_time': 8841.15231704712, 'accumulated_logging_time': 0.46880340576171875}
I0212 14:38:00.905392 139975029352192 logging_writer.py:48] [38643] accumulated_eval_time=8841.152317, accumulated_logging_time=0.468803, accumulated_submission_time=13471.477102, global_step=38643, preemption_count=0, score=13471.477102, test/accuracy=0.666039, test/bleu=27.417562, test/loss=1.565603, test/num_examples=3003, total_duration=22314.383660, train/accuracy=0.639876, train/bleu=31.159718, train/loss=1.739918, validation/accuracy=0.656074, validation/bleu=28.024677, validation/loss=1.621478, validation/num_examples=3000
I0212 14:38:21.020003 139975020959488 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.36349278688430786, loss=1.7294425964355469
I0212 14:38:55.701563 139975029352192 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.3737606406211853, loss=1.8037954568862915
I0212 14:39:30.477693 139975020959488 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.38823240995407104, loss=1.7499656677246094
I0212 14:40:05.224307 139975029352192 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.42601945996284485, loss=1.7956023216247559
I0212 14:40:39.978958 139975020959488 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.46230632066726685, loss=1.7610676288604736
I0212 14:41:14.733806 139975029352192 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.4352644681930542, loss=1.8105626106262207
I0212 14:41:49.505331 139975020959488 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.37198537588119507, loss=1.712397813796997
I0212 14:42:24.245232 139975029352192 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4029311239719391, loss=1.7536585330963135
I0212 14:42:59.030871 139975020959488 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.40954115986824036, loss=1.7808758020401
I0212 14:43:33.787733 139975029352192 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.37240156531333923, loss=1.753889799118042
I0212 14:44:08.531869 139975020959488 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.4321323335170746, loss=1.827889084815979
I0212 14:44:43.314448 139975029352192 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.3700554072856903, loss=1.815476894378662
I0212 14:45:18.077876 139975020959488 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.37543928623199463, loss=1.7867010831832886
I0212 14:45:52.840269 139975029352192 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.4705478250980377, loss=1.7429254055023193
I0212 14:46:27.616715 139975020959488 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.37453892827033997, loss=1.7313733100891113
I0212 14:47:02.392907 139975029352192 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3943292796611786, loss=1.8092198371887207
I0212 14:47:37.161887 139975020959488 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.37702304124832153, loss=1.7213208675384521
I0212 14:48:11.944740 139975029352192 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.402906209230423, loss=1.7799737453460693
I0212 14:48:46.713660 139975020959488 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.4266606569290161, loss=1.8099900484085083
I0212 14:49:21.487460 139975029352192 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3636108338832855, loss=1.746254563331604
I0212 14:49:56.234158 139975020959488 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.37753117084503174, loss=1.7538610696792603
I0212 14:50:31.006474 139975029352192 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3638823628425598, loss=1.7775020599365234
I0212 14:51:05.769755 139975020959488 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.39364591240882874, loss=1.84275221824646
I0212 14:51:40.523242 139975029352192 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.3817976713180542, loss=1.7535518407821655
I0212 14:52:01.139096 140144802662208 spec.py:321] Evaluating on the training split.
I0212 14:52:04.129749 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 14:55:38.806900 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 14:55:41.478900 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 14:58:50.250419 140144802662208 spec.py:349] Evaluating on the test split.
I0212 14:58:52.935590 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 15:01:42.121424 140144802662208 submission_runner.py:408] Time since start: 23735.62s, 	Step: 41061, 	{'train/accuracy': 0.6372715830802917, 'train/loss': 1.7691075801849365, 'train/bleu': 30.759693329732446, 'validation/accuracy': 0.6587767004966736, 'validation/loss': 1.6140390634536743, 'validation/bleu': 28.07393183728591, 'validation/num_examples': 3000, 'test/accuracy': 0.672163188457489, 'test/loss': 1.5471595525741577, 'test/bleu': 27.999417482086034, 'test/num_examples': 3003, 'score': 14311.619084835052, 'total_duration': 23735.619791984558, 'accumulated_submission_time': 14311.619084835052, 'accumulated_eval_time': 9422.134581565857, 'accumulated_logging_time': 0.5005180835723877}
I0212 15:01:42.141659 139975020959488 logging_writer.py:48] [41061] accumulated_eval_time=9422.134582, accumulated_logging_time=0.500518, accumulated_submission_time=14311.619085, global_step=41061, preemption_count=0, score=14311.619085, test/accuracy=0.672163, test/bleu=27.999417, test/loss=1.547160, test/num_examples=3003, total_duration=23735.619792, train/accuracy=0.637272, train/bleu=30.759693, train/loss=1.769108, validation/accuracy=0.658777, validation/bleu=28.073932, validation/loss=1.614039, validation/num_examples=3000
I0212 15:01:55.993891 139975029352192 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.42847806215286255, loss=1.7499077320098877
I0212 15:02:30.643109 139975020959488 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.4738769233226776, loss=1.8075155019760132
I0212 15:03:05.402484 139975029352192 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.3915834426879883, loss=1.8085283041000366
I0212 15:03:40.180580 139975020959488 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.3864482045173645, loss=1.7836343050003052
I0212 15:04:14.966966 139975029352192 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.43970081210136414, loss=1.71211838722229
I0212 15:04:49.748135 139975020959488 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.43116724491119385, loss=1.869396686553955
I0212 15:05:24.509677 139975029352192 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.42318880558013916, loss=1.7591902017593384
I0212 15:05:59.265231 139975020959488 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.40052103996276855, loss=1.7876310348510742
I0212 15:06:34.018835 139975029352192 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3780169188976288, loss=1.7429437637329102
I0212 15:07:08.767374 139975020959488 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.43086156249046326, loss=1.7522932291030884
I0212 15:07:43.545786 139975029352192 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.37827643752098083, loss=1.7110347747802734
I0212 15:08:18.322697 139975020959488 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3634098768234253, loss=1.722353219985962
I0212 15:08:53.077340 139975029352192 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3708384037017822, loss=1.712721347808838
I0212 15:09:27.863098 139975020959488 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.3857942223548889, loss=1.732474446296692
I0212 15:10:02.646809 139975029352192 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.388656884431839, loss=1.759560465812683
I0212 15:10:37.413571 139975020959488 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.3834773898124695, loss=1.78301203250885
I0212 15:11:12.202263 139975029352192 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.41494858264923096, loss=1.8012971878051758
I0212 15:11:46.964988 139975020959488 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.4755386412143707, loss=1.7031913995742798
I0212 15:12:21.711029 139975029352192 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.3720245361328125, loss=1.7527642250061035
I0212 15:12:56.481674 139975020959488 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.40582937002182007, loss=1.729190707206726
I0212 15:13:31.222169 139975029352192 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.4092119336128235, loss=1.7628108263015747
I0212 15:14:06.005120 139975020959488 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.4184640347957611, loss=1.7499058246612549
I0212 15:14:40.776800 139975029352192 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.4277355372905731, loss=1.8435455560684204
I0212 15:15:15.550153 139975020959488 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.38352933526039124, loss=1.7411954402923584
I0212 15:15:42.389225 140144802662208 spec.py:321] Evaluating on the training split.
I0212 15:15:45.364001 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 15:18:45.426928 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 15:18:48.109327 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 15:21:12.557243 140144802662208 spec.py:349] Evaluating on the test split.
I0212 15:21:15.243734 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 15:23:39.047800 140144802662208 submission_runner.py:408] Time since start: 25052.55s, 	Step: 43479, 	{'train/accuracy': 0.6386000514030457, 'train/loss': 1.760711908340454, 'train/bleu': 31.427426086467367, 'validation/accuracy': 0.6623228192329407, 'validation/loss': 1.5986641645431519, 'validation/bleu': 28.480353182921547, 'validation/num_examples': 3000, 'test/accuracy': 0.6713265180587769, 'test/loss': 1.540757417678833, 'test/bleu': 27.56919141028713, 'test/num_examples': 3003, 'score': 15151.775276899338, 'total_duration': 25052.546183347702, 'accumulated_submission_time': 15151.775276899338, 'accumulated_eval_time': 9898.793123483658, 'accumulated_logging_time': 0.5321898460388184}
I0212 15:23:39.068805 139975029352192 logging_writer.py:48] [43479] accumulated_eval_time=9898.793123, accumulated_logging_time=0.532190, accumulated_submission_time=15151.775277, global_step=43479, preemption_count=0, score=15151.775277, test/accuracy=0.671327, test/bleu=27.569191, test/loss=1.540757, test/num_examples=3003, total_duration=25052.546183, train/accuracy=0.638600, train/bleu=31.427426, train/loss=1.760712, validation/accuracy=0.662323, validation/bleu=28.480353, validation/loss=1.598664, validation/num_examples=3000
I0212 15:23:46.702902 139975020959488 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.4108498692512512, loss=1.6965391635894775
I0212 15:24:21.354714 139975029352192 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.39234688878059387, loss=1.755125641822815
I0212 15:24:56.089119 139975020959488 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.41748398542404175, loss=1.7762572765350342
I0212 15:25:30.853086 139975029352192 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.4327211081981659, loss=1.8595356941223145
I0212 15:26:05.624962 139975020959488 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.40039414167404175, loss=1.7569622993469238
I0212 15:26:40.386981 139975029352192 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3711288869380951, loss=1.7593724727630615
I0212 15:27:15.165691 139975020959488 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.40690359473228455, loss=1.8016263246536255
I0212 15:27:49.909952 139975029352192 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.4399082362651825, loss=1.7728387117385864
I0212 15:28:24.686395 139975020959488 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.407598614692688, loss=1.7963478565216064
I0212 15:28:59.448835 139975029352192 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.3761146366596222, loss=1.6707319021224976
I0212 15:29:34.205314 139975020959488 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.42258021235466003, loss=1.769308090209961
I0212 15:30:08.986242 139975029352192 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.41024982929229736, loss=1.6611579656600952
I0212 15:30:43.742888 139975020959488 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.4203121066093445, loss=1.77340567111969
I0212 15:31:18.504977 139975029352192 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.37476038932800293, loss=1.7420793771743774
I0212 15:31:53.257487 139975020959488 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.39022761583328247, loss=1.8795245885849
I0212 15:32:28.004863 139975029352192 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.40206578373908997, loss=1.736897587776184
I0212 15:33:02.779198 139975020959488 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.4286716878414154, loss=1.7360005378723145
I0212 15:33:37.562425 139975029352192 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.37600794434547424, loss=1.6994844675064087
I0212 15:34:12.337232 139975020959488 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.3922843039035797, loss=1.7205649614334106
I0212 15:34:47.098353 139975029352192 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.38872721791267395, loss=1.8159973621368408
I0212 15:35:21.866088 139975020959488 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.40875008702278137, loss=1.7810052633285522
I0212 15:35:56.652259 139975029352192 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.4549593925476074, loss=1.7108752727508545
I0212 15:36:31.424744 139975020959488 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.4332077205181122, loss=1.7110915184020996
I0212 15:37:06.182586 139975029352192 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.4665306508541107, loss=1.6472431421279907
I0212 15:37:39.320047 140144802662208 spec.py:321] Evaluating on the training split.
I0212 15:37:42.300409 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 15:41:15.840176 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 15:41:18.523408 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 15:44:01.277185 140144802662208 spec.py:349] Evaluating on the test split.
I0212 15:44:03.973432 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 15:46:39.981829 140144802662208 submission_runner.py:408] Time since start: 26433.48s, 	Step: 45897, 	{'train/accuracy': 0.6438037753105164, 'train/loss': 1.7203915119171143, 'train/bleu': 31.366122874842002, 'validation/accuracy': 0.6614301204681396, 'validation/loss': 1.597715973854065, 'validation/bleu': 28.569499214277318, 'validation/num_examples': 3000, 'test/accuracy': 0.6712567806243896, 'test/loss': 1.539738416671753, 'test/bleu': 27.98019850766508, 'test/num_examples': 3003, 'score': 15991.93740272522, 'total_duration': 26433.4802134037, 'accumulated_submission_time': 15991.93740272522, 'accumulated_eval_time': 10439.45487523079, 'accumulated_logging_time': 0.5634627342224121}
I0212 15:46:40.002474 139975020959488 logging_writer.py:48] [45897] accumulated_eval_time=10439.454875, accumulated_logging_time=0.563463, accumulated_submission_time=15991.937403, global_step=45897, preemption_count=0, score=15991.937403, test/accuracy=0.671257, test/bleu=27.980199, test/loss=1.539738, test/num_examples=3003, total_duration=26433.480213, train/accuracy=0.643804, train/bleu=31.366123, train/loss=1.720392, validation/accuracy=0.661430, validation/bleu=28.569499, validation/loss=1.597716, validation/num_examples=3000
I0212 15:46:41.409283 139975029352192 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.3769579529762268, loss=1.7478066682815552
I0212 15:47:16.042343 139975020959488 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.39300206303596497, loss=1.8044599294662476
I0212 15:47:50.713216 139975029352192 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.41516122221946716, loss=1.7048190832138062
I0212 15:48:25.466320 139975020959488 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.4243188500404358, loss=1.6919705867767334
I0212 15:49:00.220290 139975029352192 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.36012041568756104, loss=1.7885382175445557
I0212 15:49:34.983847 139975020959488 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3961092233657837, loss=1.7219910621643066
I0212 15:50:09.738209 139975029352192 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.3616461455821991, loss=1.7497121095657349
I0212 15:50:44.636824 139975020959488 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.37738558650016785, loss=1.7382757663726807
I0212 15:51:19.382658 139975029352192 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.47162964940071106, loss=1.7339917421340942
I0212 15:51:54.154237 139975020959488 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.378773957490921, loss=1.782328486442566
I0212 15:52:28.914195 139975029352192 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.4073410928249359, loss=1.758461356163025
I0212 15:53:03.686758 139975020959488 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.4037454426288605, loss=1.70474374294281
I0212 15:53:38.425579 139975029352192 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.43645527958869934, loss=1.703698754310608
I0212 15:54:13.164228 139975020959488 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.4723040461540222, loss=1.6836968660354614
I0212 15:54:47.983971 139975029352192 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.4185214936733246, loss=1.6813886165618896
I0212 15:55:22.731773 139975020959488 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.45115914940834045, loss=1.7474122047424316
I0212 15:55:57.489873 139975029352192 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.36710482835769653, loss=1.7603620290756226
I0212 15:56:32.306101 139975020959488 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.35429203510284424, loss=1.6724539995193481
I0212 15:57:07.089241 139975029352192 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.3614438772201538, loss=1.7969635725021362
I0212 15:57:41.822064 139975020959488 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.4204917252063751, loss=1.7813704013824463
I0212 15:58:16.587938 139975029352192 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.41208603978157043, loss=1.8328478336334229
I0212 15:58:51.383113 139975020959488 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.41388773918151855, loss=1.6851775646209717
I0212 15:59:26.166061 139975029352192 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.37622132897377014, loss=1.7241830825805664
I0212 16:00:00.933373 139975020959488 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.4153292775154114, loss=1.6858925819396973
I0212 16:00:35.681067 139975029352192 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.4113371968269348, loss=1.7986443042755127
I0212 16:00:40.277246 140144802662208 spec.py:321] Evaluating on the training split.
I0212 16:00:43.252711 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 16:04:09.277872 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 16:04:11.964642 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 16:06:48.065384 140144802662208 spec.py:349] Evaluating on the test split.
I0212 16:06:50.755215 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 16:09:19.868204 140144802662208 submission_runner.py:408] Time since start: 27793.37s, 	Step: 48315, 	{'train/accuracy': 0.6425302028656006, 'train/loss': 1.7314515113830566, 'train/bleu': 31.29823991504049, 'validation/accuracy': 0.6614673137664795, 'validation/loss': 1.5935068130493164, 'validation/bleu': 28.392376189178425, 'validation/num_examples': 3000, 'test/accuracy': 0.6734995245933533, 'test/loss': 1.5197216272354126, 'test/bleu': 28.134675838345206, 'test/num_examples': 3003, 'score': 16832.123507976532, 'total_duration': 27793.366586208344, 'accumulated_submission_time': 16832.123507976532, 'accumulated_eval_time': 10959.045782327652, 'accumulated_logging_time': 0.5941922664642334}
I0212 16:09:19.889845 139975020959488 logging_writer.py:48] [48315] accumulated_eval_time=10959.045782, accumulated_logging_time=0.594192, accumulated_submission_time=16832.123508, global_step=48315, preemption_count=0, score=16832.123508, test/accuracy=0.673500, test/bleu=28.134676, test/loss=1.519722, test/num_examples=3003, total_duration=27793.366586, train/accuracy=0.642530, train/bleu=31.298240, train/loss=1.731452, validation/accuracy=0.661467, validation/bleu=28.392376, validation/loss=1.593507, validation/num_examples=3000
I0212 16:09:49.702962 139975029352192 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.43254485726356506, loss=1.7084087133407593
I0212 16:10:24.399921 139975020959488 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.3965908885002136, loss=1.828625202178955
I0212 16:10:59.168248 139975029352192 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.42679816484451294, loss=1.691910982131958
I0212 16:11:33.929724 139975020959488 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.41734063625335693, loss=1.7348992824554443
I0212 16:12:08.694759 139975029352192 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3808768093585968, loss=1.689414620399475
I0212 16:12:43.482413 139975020959488 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.40089845657348633, loss=1.7599619626998901
I0212 16:13:18.268458 139975029352192 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3640593886375427, loss=1.7601298093795776
I0212 16:13:53.047681 139975020959488 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.43857845664024353, loss=1.7104436159133911
I0212 16:14:27.837806 139975029352192 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.459408164024353, loss=1.7838338613510132
I0212 16:15:02.626217 139975020959488 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.38085827231407166, loss=1.74651038646698
I0212 16:15:37.389749 139975029352192 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.388345867395401, loss=1.6967904567718506
I0212 16:16:12.149788 139975020959488 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.38591432571411133, loss=1.7646923065185547
I0212 16:16:46.905777 139975029352192 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.3888615667819977, loss=1.7528451681137085
I0212 16:17:21.653701 139975020959488 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3723922073841095, loss=1.7101718187332153
I0212 16:17:56.424649 139975029352192 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.40095463395118713, loss=1.6691837310791016
I0212 16:18:31.211401 139975020959488 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3773728609085083, loss=1.7110223770141602
I0212 16:19:05.972479 139975029352192 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.396994948387146, loss=1.7557212114334106
I0212 16:19:40.740244 139975020959488 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.3974553644657135, loss=1.6903072595596313
I0212 16:20:15.497193 139975029352192 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3912258446216583, loss=1.6967910528182983
I0212 16:20:50.253928 139975020959488 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.37488415837287903, loss=1.7283927202224731
I0212 16:21:25.024803 139975029352192 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.37714558839797974, loss=1.7723383903503418
I0212 16:21:59.805900 139975020959488 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3707796335220337, loss=1.777147650718689
I0212 16:22:34.579922 139975029352192 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3947605490684509, loss=1.7646299600601196
I0212 16:23:09.343837 139975020959488 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.39246630668640137, loss=1.7831040620803833
I0212 16:23:20.186843 140144802662208 spec.py:321] Evaluating on the training split.
I0212 16:23:23.162640 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 16:26:08.105703 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 16:26:10.795937 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 16:28:46.156805 140144802662208 spec.py:349] Evaluating on the test split.
I0212 16:28:48.844219 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 16:31:16.099447 140144802662208 submission_runner.py:408] Time since start: 29109.60s, 	Step: 50733, 	{'train/accuracy': 0.6550332307815552, 'train/loss': 1.6446468830108643, 'train/bleu': 31.87787872913857, 'validation/accuracy': 0.664765477180481, 'validation/loss': 1.5815393924713135, 'validation/bleu': 28.684633210753194, 'validation/num_examples': 3000, 'test/accuracy': 0.6736041307449341, 'test/loss': 1.5203142166137695, 'test/bleu': 27.925587206144616, 'test/num_examples': 3003, 'score': 17672.332036733627, 'total_duration': 29109.597824811935, 'accumulated_submission_time': 17672.332036733627, 'accumulated_eval_time': 11434.958333969116, 'accumulated_logging_time': 0.6256530284881592}
I0212 16:31:16.120902 139975029352192 logging_writer.py:48] [50733] accumulated_eval_time=11434.958334, accumulated_logging_time=0.625653, accumulated_submission_time=17672.332037, global_step=50733, preemption_count=0, score=17672.332037, test/accuracy=0.673604, test/bleu=27.925587, test/loss=1.520314, test/num_examples=3003, total_duration=29109.597825, train/accuracy=0.655033, train/bleu=31.877879, train/loss=1.644647, validation/accuracy=0.664765, validation/bleu=28.684633, validation/loss=1.581539, validation/num_examples=3000
I0212 16:31:39.664379 139975020959488 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.44828012585639954, loss=1.7056045532226562
I0212 16:32:14.340907 139975029352192 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.3758913278579712, loss=1.7684270143508911
I0212 16:32:49.117081 139975020959488 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.4367576539516449, loss=1.690640926361084
I0212 16:33:23.886200 139975029352192 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.38831827044487, loss=1.7393115758895874
I0212 16:33:58.678890 139975020959488 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.38184672594070435, loss=1.7217293977737427
I0212 16:34:33.460644 139975029352192 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.38316988945007324, loss=1.801047921180725
I0212 16:35:08.229652 139975020959488 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.39424607157707214, loss=1.7166036367416382
I0212 16:35:43.023989 139975029352192 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.3820604085922241, loss=1.72291898727417
I0212 16:36:17.826648 139975020959488 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.3970976173877716, loss=1.6927396059036255
I0212 16:36:52.622221 139975029352192 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.40457823872566223, loss=1.8078805208206177
I0212 16:37:27.410813 139975020959488 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.5453447699546814, loss=1.7568786144256592
I0212 16:38:02.200887 139975029352192 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3598049283027649, loss=1.6541171073913574
I0212 16:38:36.985312 139975020959488 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.3779057562351227, loss=1.7834810018539429
I0212 16:39:11.759854 139975029352192 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.38556116819381714, loss=1.7567380666732788
I0212 16:39:46.527814 139975020959488 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.373196542263031, loss=1.7056795358657837
I0212 16:40:21.291320 139975029352192 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.35667774081230164, loss=1.6977622509002686
I0212 16:40:56.066776 139975020959488 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.43087127804756165, loss=1.7758703231811523
I0212 16:41:30.850311 139975029352192 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3982105851173401, loss=1.6658523082733154
I0212 16:42:05.658091 139975020959488 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.38938677310943604, loss=1.6984704732894897
I0212 16:42:40.428542 139975029352192 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3767099678516388, loss=1.6463205814361572
I0212 16:43:15.208720 139975020959488 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3672105371952057, loss=1.7315226793289185
I0212 16:43:49.983803 139975029352192 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.41012707352638245, loss=1.7778035402297974
I0212 16:44:24.770322 139975020959488 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.37136536836624146, loss=1.6628601551055908
I0212 16:44:59.563100 139975029352192 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.41926026344299316, loss=1.7567676305770874
I0212 16:45:16.337683 140144802662208 spec.py:321] Evaluating on the training split.
I0212 16:45:19.327206 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 16:49:37.777001 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 16:49:40.470247 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 16:52:26.419780 140144802662208 spec.py:349] Evaluating on the test split.
I0212 16:52:29.117404 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 16:54:48.252642 140144802662208 submission_runner.py:408] Time since start: 30521.75s, 	Step: 53150, 	{'train/accuracy': 0.6440820097923279, 'train/loss': 1.7206270694732666, 'train/bleu': 31.623226056556543, 'validation/accuracy': 0.6641455292701721, 'validation/loss': 1.5732247829437256, 'validation/bleu': 28.549211847348694, 'validation/num_examples': 3000, 'test/accuracy': 0.6757422685623169, 'test/loss': 1.5066756010055542, 'test/bleu': 28.266982534870575, 'test/num_examples': 3003, 'score': 18512.45762705803, 'total_duration': 30521.75099492073, 'accumulated_submission_time': 18512.45762705803, 'accumulated_eval_time': 12006.873229980469, 'accumulated_logging_time': 0.6573050022125244}
I0212 16:54:48.278772 139975020959488 logging_writer.py:48] [53150] accumulated_eval_time=12006.873230, accumulated_logging_time=0.657305, accumulated_submission_time=18512.457627, global_step=53150, preemption_count=0, score=18512.457627, test/accuracy=0.675742, test/bleu=28.266983, test/loss=1.506676, test/num_examples=3003, total_duration=30521.750995, train/accuracy=0.644082, train/bleu=31.623226, train/loss=1.720627, validation/accuracy=0.664146, validation/bleu=28.549212, validation/loss=1.573225, validation/num_examples=3000
I0212 16:55:05.971203 139975029352192 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.42293715476989746, loss=1.7501150369644165
I0212 16:55:40.663810 139975020959488 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.40188443660736084, loss=1.679368257522583
I0212 16:56:15.432831 139975029352192 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.3887324035167694, loss=1.7220627069473267
I0212 16:56:50.271837 139975020959488 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.39837130904197693, loss=1.6501212120056152
I0212 16:57:25.072071 139975029352192 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.36787259578704834, loss=1.7240080833435059
I0212 16:57:59.992533 139975020959488 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.383301705121994, loss=1.6031841039657593
I0212 16:58:34.789702 139975029352192 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.42766422033309937, loss=1.7912198305130005
I0212 16:59:09.542536 139975020959488 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.38430899381637573, loss=1.7632372379302979
I0212 16:59:44.305547 139975029352192 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3730539381504059, loss=1.727279543876648
I0212 17:00:19.108999 139975020959488 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.37401190400123596, loss=1.753954291343689
I0212 17:00:53.911861 139975029352192 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.37686413526535034, loss=1.7382484674453735
I0212 17:01:28.680471 139975020959488 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.42456579208374023, loss=1.7321854829788208
I0212 17:02:03.440397 139975029352192 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.4131927788257599, loss=1.719778060913086
I0212 17:02:38.212272 139975020959488 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.38796761631965637, loss=1.84180748462677
I0212 17:03:12.962443 139975029352192 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.46423548460006714, loss=1.7993955612182617
I0212 17:03:47.790504 139975020959488 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.41681787371635437, loss=1.7448354959487915
I0212 17:04:22.694458 139975029352192 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.3429993689060211, loss=1.7202858924865723
I0212 17:04:57.509755 139975020959488 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.3692280054092407, loss=1.6808682680130005
I0212 17:05:32.277475 139975029352192 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.402781218290329, loss=1.6920288801193237
I0212 17:06:07.050470 139975020959488 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.40779802203178406, loss=1.6672139167785645
I0212 17:06:41.879117 139975029352192 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.3793019950389862, loss=1.7045263051986694
I0212 17:07:16.678662 139975020959488 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.3534787595272064, loss=1.7038917541503906
I0212 17:07:51.428628 139975029352192 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.39275118708610535, loss=1.6777082681655884
I0212 17:08:26.191831 139975020959488 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.3723461329936981, loss=1.8421978950500488
I0212 17:08:48.535091 140144802662208 spec.py:321] Evaluating on the training split.
I0212 17:08:51.503615 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 17:11:45.853823 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 17:11:48.556216 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 17:14:14.778675 140144802662208 spec.py:349] Evaluating on the test split.
I0212 17:14:17.481377 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 17:16:33.769032 140144802662208 submission_runner.py:408] Time since start: 31827.27s, 	Step: 55566, 	{'train/accuracy': 0.6458576917648315, 'train/loss': 1.7048068046569824, 'train/bleu': 31.803183258359116, 'validation/accuracy': 0.6658813953399658, 'validation/loss': 1.5674703121185303, 'validation/bleu': 28.654052868631133, 'validation/num_examples': 3000, 'test/accuracy': 0.6787520051002502, 'test/loss': 1.5035967826843262, 'test/bleu': 28.449937622484967, 'test/num_examples': 3003, 'score': 19352.615475177765, 'total_duration': 31827.267405748367, 'accumulated_submission_time': 19352.615475177765, 'accumulated_eval_time': 12472.107189893723, 'accumulated_logging_time': 0.6961920261383057}
I0212 17:16:33.792698 139975029352192 logging_writer.py:48] [55566] accumulated_eval_time=12472.107190, accumulated_logging_time=0.696192, accumulated_submission_time=19352.615475, global_step=55566, preemption_count=0, score=19352.615475, test/accuracy=0.678752, test/bleu=28.449938, test/loss=1.503597, test/num_examples=3003, total_duration=31827.267406, train/accuracy=0.645858, train/bleu=31.803183, train/loss=1.704807, validation/accuracy=0.665881, validation/bleu=28.654053, validation/loss=1.567470, validation/num_examples=3000
I0212 17:16:45.946868 139975020959488 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.39903998374938965, loss=1.7220438718795776
I0212 17:17:20.600780 139975029352192 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3711267411708832, loss=1.7391809225082397
I0212 17:17:55.358157 139975020959488 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.38893774151802063, loss=1.7518974542617798
I0212 17:18:30.126187 139975029352192 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.39608946442604065, loss=1.8581782579421997
I0212 17:19:04.890483 139975020959488 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.40563836693763733, loss=1.7330570220947266
I0212 17:19:39.719697 139975029352192 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.35714390873908997, loss=1.6940902471542358
I0212 17:20:14.486432 139975020959488 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.3782528042793274, loss=1.7340481281280518
I0212 17:20:49.238966 139975029352192 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.4019401967525482, loss=1.8026401996612549
I0212 17:21:24.023869 139975020959488 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3576951026916504, loss=1.6704264879226685
I0212 17:21:58.798348 139975029352192 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.37488120794296265, loss=1.6912057399749756
I0212 17:22:33.563031 139975020959488 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.4558819532394409, loss=1.7162691354751587
I0212 17:23:08.355551 139975029352192 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.373960018157959, loss=1.6970447301864624
I0212 17:23:43.136746 139975020959488 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3980880379676819, loss=1.729278564453125
I0212 17:24:17.906234 139975029352192 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.3804263472557068, loss=1.68300199508667
I0212 17:24:52.678859 139975020959488 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.4998950958251953, loss=1.7730720043182373
I0212 17:25:27.432149 139975029352192 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.39756110310554504, loss=1.6833479404449463
I0212 17:26:02.208485 139975020959488 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.37728822231292725, loss=1.7731187343597412
I0212 17:26:37.022741 139975029352192 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3943956196308136, loss=1.688167691230774
I0212 17:27:11.831375 139975020959488 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3718924820423126, loss=1.7329241037368774
I0212 17:27:46.620085 139975029352192 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.4117712080478668, loss=1.7205793857574463
I0212 17:28:21.405013 139975020959488 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3678382337093353, loss=1.755220651626587
I0212 17:28:56.202354 139975029352192 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.3825942277908325, loss=1.6828258037567139
I0212 17:29:31.079347 139975020959488 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.41590461134910583, loss=1.7021946907043457
I0212 17:30:05.847656 139975029352192 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.4241989552974701, loss=1.6529874801635742
I0212 17:30:34.059638 140144802662208 spec.py:321] Evaluating on the training split.
I0212 17:30:37.035197 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 17:34:20.499866 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 17:34:23.199127 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 17:36:55.528081 140144802662208 spec.py:349] Evaluating on the test split.
I0212 17:36:58.215204 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 17:39:24.929084 140144802662208 submission_runner.py:408] Time since start: 33198.43s, 	Step: 57983, 	{'train/accuracy': 0.6503816843032837, 'train/loss': 1.6813665628433228, 'train/bleu': 32.32253220630714, 'validation/accuracy': 0.6673941016197205, 'validation/loss': 1.5596922636032104, 'validation/bleu': 28.76944588602514, 'validation/num_examples': 3000, 'test/accuracy': 0.678345263004303, 'test/loss': 1.492568016052246, 'test/bleu': 28.715793486279985, 'test/num_examples': 3003, 'score': 20192.78905391693, 'total_duration': 33198.427434682846, 'accumulated_submission_time': 20192.78905391693, 'accumulated_eval_time': 13002.976558923721, 'accumulated_logging_time': 0.7319936752319336}
I0212 17:39:24.956358 139975020959488 logging_writer.py:48] [57983] accumulated_eval_time=13002.976559, accumulated_logging_time=0.731994, accumulated_submission_time=20192.789054, global_step=57983, preemption_count=0, score=20192.789054, test/accuracy=0.678345, test/bleu=28.715793, test/loss=1.492568, test/num_examples=3003, total_duration=33198.427435, train/accuracy=0.650382, train/bleu=32.322532, train/loss=1.681367, validation/accuracy=0.667394, validation/bleu=28.769446, validation/loss=1.559692, validation/num_examples=3000
I0212 17:39:31.217046 139975029352192 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.3995363414287567, loss=1.7017377614974976
I0212 17:40:05.826436 139975020959488 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.365768700838089, loss=1.6539565324783325
I0212 17:40:40.517678 139975029352192 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.3904266953468323, loss=1.7071126699447632
I0212 17:41:15.272112 139975020959488 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.37227097153663635, loss=1.6464645862579346
I0212 17:41:50.058801 139975029352192 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.37150096893310547, loss=1.6994556188583374
I0212 17:42:24.833966 139975020959488 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.5551869869232178, loss=1.7472617626190186
I0212 17:42:59.615798 139975029352192 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.36644232273101807, loss=1.680554747581482
I0212 17:43:34.391990 139975020959488 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.43771833181381226, loss=1.7207491397857666
I0212 17:44:09.211328 139975029352192 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3822135031223297, loss=1.6167638301849365
I0212 17:44:43.996463 139975020959488 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.36753255128860474, loss=1.6762675046920776
I0212 17:45:18.770348 139975029352192 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.35263755917549133, loss=1.665838360786438
I0212 17:45:53.526030 139975020959488 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.40643471479415894, loss=1.7131935358047485
I0212 17:46:28.292897 139975029352192 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.3902444541454315, loss=1.778914451599121
I0212 17:47:03.103078 139975020959488 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.40233516693115234, loss=1.7682759761810303
I0212 17:47:37.917348 139975029352192 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.37852856516838074, loss=1.6352696418762207
I0212 17:48:12.696016 139975020959488 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.3693366050720215, loss=1.7004947662353516
I0212 17:48:47.513718 139975029352192 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.34220656752586365, loss=1.6725088357925415
I0212 17:49:22.323730 139975020959488 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.44722604751586914, loss=1.7494862079620361
I0212 17:49:57.123060 139975029352192 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.36862310767173767, loss=1.7292898893356323
I0212 17:50:31.901136 139975020959488 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.4615441858768463, loss=1.6966091394424438
I0212 17:51:06.665576 139975029352192 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.45317500829696655, loss=1.7641823291778564
I0212 17:51:41.481454 139975020959488 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.379221647977829, loss=1.6929535865783691
I0212 17:52:16.272198 139975029352192 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3508262634277344, loss=1.6897153854370117
I0212 17:52:51.090406 139975020959488 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.35998424887657166, loss=1.7502822875976562
I0212 17:53:24.939968 140144802662208 spec.py:321] Evaluating on the training split.
I0212 17:53:27.924683 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 17:56:29.579169 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 17:56:32.261956 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 17:59:07.206918 140144802662208 spec.py:349] Evaluating on the test split.
I0212 17:59:09.887607 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 18:01:42.428934 140144802662208 submission_runner.py:408] Time since start: 34535.93s, 	Step: 60399, 	{'train/accuracy': 0.6484251022338867, 'train/loss': 1.6919599771499634, 'train/bleu': 31.836385315191027, 'validation/accuracy': 0.667815625667572, 'validation/loss': 1.5593230724334717, 'validation/bleu': 28.898338564560326, 'validation/num_examples': 3000, 'test/accuracy': 0.6783917546272278, 'test/loss': 1.4889075756072998, 'test/bleu': 28.376137687711587, 'test/num_examples': 3003, 'score': 21032.677373170853, 'total_duration': 34535.92731380463, 'accumulated_submission_time': 21032.677373170853, 'accumulated_eval_time': 13500.465492010117, 'accumulated_logging_time': 0.7707424163818359}
I0212 18:01:42.452141 139975029352192 logging_writer.py:48] [60399] accumulated_eval_time=13500.465492, accumulated_logging_time=0.770742, accumulated_submission_time=21032.677373, global_step=60399, preemption_count=0, score=21032.677373, test/accuracy=0.678392, test/bleu=28.376138, test/loss=1.488908, test/num_examples=3003, total_duration=34535.927314, train/accuracy=0.648425, train/bleu=31.836385, train/loss=1.691960, validation/accuracy=0.667816, validation/bleu=28.898339, validation/loss=1.559323, validation/num_examples=3000
I0212 18:01:43.166610 139975020959488 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.38207244873046875, loss=1.7526466846466064
I0212 18:02:17.782582 139975029352192 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.43857723474502563, loss=1.7010371685028076
I0212 18:02:52.496093 139975020959488 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.3705652952194214, loss=1.6490001678466797
I0212 18:03:27.240797 139975029352192 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.4042871296405792, loss=1.7944997549057007
I0212 18:04:01.983639 139975020959488 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.46381309628486633, loss=1.789644718170166
I0212 18:04:36.763564 139975029352192 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.3739469051361084, loss=1.712205410003662
I0212 18:05:11.493705 139975020959488 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.37329381704330444, loss=1.7876254320144653
I0212 18:05:46.266207 139975029352192 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.38079240918159485, loss=1.6920857429504395
I0212 18:06:21.015717 139975020959488 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.4154762625694275, loss=1.6904356479644775
I0212 18:06:55.753982 139975029352192 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.39254918694496155, loss=1.644373893737793
I0212 18:07:30.508047 139975020959488 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.4130938649177551, loss=1.7020913362503052
I0212 18:08:05.236466 139975029352192 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.3889355957508087, loss=1.7325459718704224
I0212 18:08:39.991785 139975020959488 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.36589112877845764, loss=1.7443222999572754
I0212 18:09:14.749831 139975029352192 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.38340744376182556, loss=1.6952240467071533
I0212 18:09:49.536009 139975020959488 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.4141230285167694, loss=1.7764345407485962
I0212 18:10:24.437507 139975029352192 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.41814762353897095, loss=1.6209511756896973
I0212 18:10:59.223825 139975020959488 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.3720688223838806, loss=1.7342904806137085
I0212 18:11:34.035488 139975029352192 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.3705044388771057, loss=1.7482893466949463
I0212 18:12:08.808229 139975020959488 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.39630863070487976, loss=1.639944076538086
I0212 18:12:43.604989 139975029352192 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.4906160831451416, loss=1.670333743095398
I0212 18:13:18.375242 139975020959488 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.37422680854797363, loss=1.5989488363265991
I0212 18:13:53.124963 139975029352192 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.38751617074012756, loss=1.6915669441223145
I0212 18:14:27.894329 139975020959488 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.37427204847335815, loss=1.6499016284942627
I0212 18:15:02.652353 139975029352192 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.4215611219406128, loss=1.703930139541626
I0212 18:15:37.420059 139975020959488 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.4420345425605774, loss=1.6960638761520386
I0212 18:15:42.726329 140144802662208 spec.py:321] Evaluating on the training split.
I0212 18:15:45.711594 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 18:19:31.960651 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 18:19:34.631410 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 18:23:18.083912 140144802662208 spec.py:349] Evaluating on the test split.
I0212 18:23:20.766318 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 18:26:30.228752 140144802662208 submission_runner.py:408] Time since start: 36023.73s, 	Step: 62817, 	{'train/accuracy': 0.6717912554740906, 'train/loss': 1.5358587503433228, 'train/bleu': 33.683703448514876, 'validation/accuracy': 0.6687455773353577, 'validation/loss': 1.5462676286697388, 'validation/bleu': 28.54941150424726, 'validation/num_examples': 3000, 'test/accuracy': 0.6795189380645752, 'test/loss': 1.4860724210739136, 'test/bleu': 28.410224716126773, 'test/num_examples': 3003, 'score': 21872.860898256302, 'total_duration': 36023.72711586952, 'accumulated_submission_time': 21872.860898256302, 'accumulated_eval_time': 14147.967857837677, 'accumulated_logging_time': 0.803971529006958}
I0212 18:26:30.258568 139975029352192 logging_writer.py:48] [62817] accumulated_eval_time=14147.967858, accumulated_logging_time=0.803972, accumulated_submission_time=21872.860898, global_step=62817, preemption_count=0, score=21872.860898, test/accuracy=0.679519, test/bleu=28.410225, test/loss=1.486072, test/num_examples=3003, total_duration=36023.727116, train/accuracy=0.671791, train/bleu=33.683703, train/loss=1.535859, validation/accuracy=0.668746, validation/bleu=28.549412, validation/loss=1.546268, validation/num_examples=3000
I0212 18:26:59.356511 139975020959488 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.3926600217819214, loss=1.7170178890228271
I0212 18:27:34.018479 139975029352192 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3822060227394104, loss=1.7064818143844604
I0212 18:28:08.760514 139975020959488 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.3881556987762451, loss=1.6879432201385498
I0212 18:28:43.527578 139975029352192 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.3875526189804077, loss=1.7087258100509644
I0212 18:29:18.292713 139975020959488 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.46792933344841003, loss=1.6168769598007202
I0212 18:29:53.033305 139975029352192 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.42101386189460754, loss=1.7443122863769531
I0212 18:30:27.789456 139975020959488 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.35664165019989014, loss=1.6712671518325806
I0212 18:31:02.549271 139975029352192 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.4121478796005249, loss=1.660604476928711
I0212 18:31:37.303873 139975020959488 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.3689277470111847, loss=1.6967841386795044
I0212 18:32:12.046448 139975029352192 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.42710554599761963, loss=1.7021780014038086
I0212 18:32:46.789688 139975020959488 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.4030672013759613, loss=1.7043797969818115
I0212 18:33:21.547232 139975029352192 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.4158746302127838, loss=1.6481730937957764
I0212 18:33:56.325872 139975020959488 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.41553571820259094, loss=1.6961631774902344
I0212 18:34:31.153900 139975029352192 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.38620877265930176, loss=1.7070369720458984
I0212 18:35:05.933326 139975020959488 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.39088985323905945, loss=1.6811518669128418
I0212 18:35:40.702071 139975029352192 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.38503095507621765, loss=1.7129182815551758
I0212 18:36:15.453664 139975020959488 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.3950043022632599, loss=1.7496622800827026
I0212 18:36:50.254693 139975029352192 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.39403289556503296, loss=1.7000696659088135
I0212 18:37:25.028893 139975020959488 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.4268205165863037, loss=1.7344640493392944
I0212 18:37:59.798274 139975029352192 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.4059363305568695, loss=1.7006521224975586
I0212 18:38:34.554419 139975020959488 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3815290331840515, loss=1.649839162826538
I0212 18:39:09.293663 139975029352192 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.35982993245124817, loss=1.6631942987442017
I0212 18:39:44.064006 139975020959488 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3807872235774994, loss=1.6934047937393188
I0212 18:40:18.831233 139975029352192 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.43869802355766296, loss=1.6371833086013794
I0212 18:40:30.460164 140144802662208 spec.py:321] Evaluating on the training split.
I0212 18:40:33.452297 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 18:44:23.267522 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 18:44:25.989017 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 18:47:11.683874 140144802662208 spec.py:349] Evaluating on the test split.
I0212 18:47:14.369564 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 18:49:47.740018 140144802662208 submission_runner.py:408] Time since start: 37421.24s, 	Step: 65235, 	{'train/accuracy': 0.6527928113937378, 'train/loss': 1.659690499305725, 'train/bleu': 32.17369020688684, 'validation/accuracy': 0.6705434322357178, 'validation/loss': 1.5372453927993774, 'validation/bleu': 28.912173341920933, 'validation/num_examples': 3000, 'test/accuracy': 0.6814130544662476, 'test/loss': 1.4726934432983398, 'test/bleu': 28.651393552679146, 'test/num_examples': 3003, 'score': 22712.9723572731, 'total_duration': 37421.238394737244, 'accumulated_submission_time': 22712.9723572731, 'accumulated_eval_time': 14705.24766755104, 'accumulated_logging_time': 0.8449218273162842}
I0212 18:49:47.766388 139975020959488 logging_writer.py:48] [65235] accumulated_eval_time=14705.247668, accumulated_logging_time=0.844922, accumulated_submission_time=22712.972357, global_step=65235, preemption_count=0, score=22712.972357, test/accuracy=0.681413, test/bleu=28.651394, test/loss=1.472693, test/num_examples=3003, total_duration=37421.238395, train/accuracy=0.652793, train/bleu=32.173690, train/loss=1.659690, validation/accuracy=0.670543, validation/bleu=28.912173, validation/loss=1.537245, validation/num_examples=3000
I0212 18:50:10.614686 139975029352192 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.4085080921649933, loss=1.675584316253662
I0212 18:50:45.281215 139975020959488 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.39658108353614807, loss=1.7271205186843872
I0212 18:51:20.050104 139975029352192 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.3624975383281708, loss=1.6492292881011963
I0212 18:51:54.839275 139975020959488 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.41851887106895447, loss=1.7295894622802734
I0212 18:52:29.597299 139975029352192 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.37946072220802307, loss=1.7386468648910522
I0212 18:53:04.353764 139975020959488 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.363740473985672, loss=1.6535917520523071
I0212 18:53:39.116638 139975029352192 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.4538789987564087, loss=1.681078553199768
I0212 18:54:13.903752 139975020959488 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.38521480560302734, loss=1.6290693283081055
I0212 18:54:48.740583 139975029352192 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.40560412406921387, loss=1.6855792999267578
I0212 18:55:23.523826 139975020959488 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.38780874013900757, loss=1.6901309490203857
I0212 18:55:58.299677 139975029352192 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.3680574595928192, loss=1.727489709854126
I0212 18:56:33.080047 139975020959488 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.37063613533973694, loss=1.6164029836654663
I0212 18:57:07.833100 139975029352192 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.4003603458404541, loss=1.658278226852417
I0212 18:57:42.592413 139975020959488 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.42113837599754333, loss=1.6831467151641846
I0212 18:58:17.370781 139975029352192 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.4348680377006531, loss=1.743112325668335
I0212 18:58:52.137646 139975020959488 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3398786783218384, loss=1.6393496990203857
I0212 18:59:26.883997 139975029352192 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.39113280177116394, loss=1.7018102407455444
I0212 19:00:01.657145 139975020959488 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3698054850101471, loss=1.6439443826675415
I0212 19:00:36.464236 139975029352192 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.4003685712814331, loss=1.714614987373352
I0212 19:01:11.280223 139975020959488 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.40326645970344543, loss=1.6431725025177002
I0212 19:01:46.046793 139975029352192 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.41934114694595337, loss=1.6960281133651733
I0212 19:02:20.786661 139975020959488 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.4055401682853699, loss=1.7126259803771973
I0212 19:02:55.538163 139975029352192 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.4049975574016571, loss=1.7438498735427856
I0212 19:03:30.295892 139975020959488 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.42027196288108826, loss=1.6149702072143555
I0212 19:03:47.752339 140144802662208 spec.py:321] Evaluating on the training split.
I0212 19:03:50.722952 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 19:07:37.382809 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 19:07:40.085043 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 19:10:10.161810 140144802662208 spec.py:349] Evaluating on the test split.
I0212 19:10:12.853150 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 19:12:52.944053 140144802662208 submission_runner.py:408] Time since start: 38806.44s, 	Step: 67652, 	{'train/accuracy': 0.6534407138824463, 'train/loss': 1.6673787832260132, 'train/bleu': 32.31266434351574, 'validation/accuracy': 0.6718453764915466, 'validation/loss': 1.5317059755325317, 'validation/bleu': 29.001009543577776, 'validation/num_examples': 3000, 'test/accuracy': 0.6824589371681213, 'test/loss': 1.463875412940979, 'test/bleu': 28.896382659621782, 'test/num_examples': 3003, 'score': 23552.86766433716, 'total_duration': 38806.44243097305, 'accumulated_submission_time': 23552.86766433716, 'accumulated_eval_time': 15250.439332008362, 'accumulated_logging_time': 0.881615400314331}
I0212 19:12:52.969980 139975029352192 logging_writer.py:48] [67652] accumulated_eval_time=15250.439332, accumulated_logging_time=0.881615, accumulated_submission_time=23552.867664, global_step=67652, preemption_count=0, score=23552.867664, test/accuracy=0.682459, test/bleu=28.896383, test/loss=1.463875, test/num_examples=3003, total_duration=38806.442431, train/accuracy=0.653441, train/bleu=32.312664, train/loss=1.667379, validation/accuracy=0.671845, validation/bleu=29.001010, validation/loss=1.531706, validation/num_examples=3000
I0212 19:13:09.952699 139975020959488 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.36251387000083923, loss=1.6364349126815796
I0212 19:13:44.609884 139975029352192 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.4113512933254242, loss=1.7017090320587158
I0212 19:14:19.362187 139975020959488 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.4298435151576996, loss=1.717748761177063
I0212 19:14:54.134953 139975029352192 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.40385136008262634, loss=1.6602543592453003
I0212 19:15:28.894801 139975020959488 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.3922572135925293, loss=1.6483393907546997
I0212 19:16:03.699255 139975029352192 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.4043019711971283, loss=1.7528507709503174
I0212 19:16:38.506603 139975020959488 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3725949823856354, loss=1.622697353363037
I0212 19:17:13.284123 139975029352192 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3761475384235382, loss=1.6335949897766113
I0212 19:17:48.059370 139975020959488 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3800077438354492, loss=1.7025388479232788
I0212 19:18:22.835609 139975029352192 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.39923691749572754, loss=1.6410728693008423
I0212 19:18:57.605861 139975020959488 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.41876742243766785, loss=1.724299669265747
I0212 19:19:32.385386 139975029352192 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.434559166431427, loss=1.593735694885254
I0212 19:20:07.140469 139975020959488 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.3920462727546692, loss=1.6818925142288208
I0212 19:20:41.907602 139975029352192 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.4044421911239624, loss=1.6859161853790283
I0212 19:21:16.684612 139975020959488 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.3751711845397949, loss=1.7311770915985107
I0212 19:21:51.492557 139975029352192 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.4202588200569153, loss=1.7296830415725708
I0212 19:22:26.273962 139975020959488 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3820587396621704, loss=1.7153478860855103
I0212 19:23:01.026636 139975029352192 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.37630534172058105, loss=1.6236066818237305
I0212 19:23:35.807545 139975020959488 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.4311879277229309, loss=1.6454639434814453
I0212 19:24:10.582043 139975029352192 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.4309118390083313, loss=1.6398227214813232
I0212 19:24:45.393518 139975020959488 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.4035045802593231, loss=1.710646152496338
I0212 19:25:20.180192 139975029352192 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.527106523513794, loss=1.6228445768356323
I0212 19:25:54.949491 139975020959488 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.40889111161231995, loss=1.6979855298995972
I0212 19:26:29.698816 139975029352192 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.4091847836971283, loss=1.6696650981903076
I0212 19:26:53.085471 140144802662208 spec.py:321] Evaluating on the training split.
I0212 19:26:56.065222 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 19:30:40.139377 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 19:30:42.826174 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 19:33:39.561219 140144802662208 spec.py:349] Evaluating on the test split.
I0212 19:33:42.242803 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 19:36:41.864466 140144802662208 submission_runner.py:408] Time since start: 40235.36s, 	Step: 70069, 	{'train/accuracy': 0.6589791178703308, 'train/loss': 1.6300126314163208, 'train/bleu': 32.72365867691716, 'validation/accuracy': 0.673568844795227, 'validation/loss': 1.5262335538864136, 'validation/bleu': 29.25410080130436, 'validation/num_examples': 3000, 'test/accuracy': 0.6847829818725586, 'test/loss': 1.4570910930633545, 'test/bleu': 28.990028967412396, 'test/num_examples': 3003, 'score': 24392.89284348488, 'total_duration': 40235.36283278465, 'accumulated_submission_time': 24392.89284348488, 'accumulated_eval_time': 15839.218275308609, 'accumulated_logging_time': 0.917823314666748}
I0212 19:36:41.890833 139975020959488 logging_writer.py:48] [70069] accumulated_eval_time=15839.218275, accumulated_logging_time=0.917823, accumulated_submission_time=24392.892843, global_step=70069, preemption_count=0, score=24392.892843, test/accuracy=0.684783, test/bleu=28.990029, test/loss=1.457091, test/num_examples=3003, total_duration=40235.362833, train/accuracy=0.658979, train/bleu=32.723659, train/loss=1.630013, validation/accuracy=0.673569, validation/bleu=29.254101, validation/loss=1.526234, validation/num_examples=3000
I0212 19:36:53.004258 139975029352192 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.39112189412117004, loss=1.6372445821762085
I0212 19:37:27.667438 139975020959488 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.3999634385108948, loss=1.7172931432724
I0212 19:38:02.379228 139975029352192 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.36821460723876953, loss=1.637486219406128
I0212 19:38:37.191545 139975020959488 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.4263029396533966, loss=1.600755214691162
I0212 19:39:12.011032 139975029352192 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.40623316168785095, loss=1.6435580253601074
I0212 19:39:46.783652 139975020959488 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.37408024072647095, loss=1.6302318572998047
I0212 19:40:21.536628 139975029352192 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.4304296672344208, loss=1.642547607421875
I0212 19:40:56.318902 139975020959488 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.41453519463539124, loss=1.6487232446670532
I0212 19:41:31.089115 139975029352192 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.41689690947532654, loss=1.6738783121109009
I0212 19:42:05.883777 139975020959488 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.39683255553245544, loss=1.5638948678970337
I0212 19:42:40.622169 139975029352192 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.39611536264419556, loss=1.6508846282958984
I0212 19:43:15.405163 139975020959488 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.4055609107017517, loss=1.7391692399978638
I0212 19:43:50.159921 139975029352192 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.41357168555259705, loss=1.6554591655731201
I0212 19:44:24.947418 139975020959488 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.42170289158821106, loss=1.6512672901153564
I0212 19:44:59.766216 139975029352192 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3939724862575531, loss=1.6742496490478516
I0212 19:45:34.587903 139975020959488 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.37647542357444763, loss=1.641461968421936
I0212 19:46:09.321978 139975029352192 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.40084123611450195, loss=1.6595888137817383
I0212 19:46:44.073551 139975020959488 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.41879045963287354, loss=1.67240309715271
I0212 19:47:18.840938 139975029352192 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3980986177921295, loss=1.638331413269043
I0212 19:47:53.591259 139975020959488 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.3838210105895996, loss=1.700653314590454
I0212 19:48:28.384153 139975029352192 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.4095011055469513, loss=1.6478794813156128
I0212 19:49:03.155365 139975020959488 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.4257543683052063, loss=1.5888792276382446
I0212 19:49:37.890656 139975029352192 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3763948976993561, loss=1.6187742948532104
I0212 19:50:12.660653 139975020959488 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.4056600332260132, loss=1.7100330591201782
I0212 19:50:41.935997 140144802662208 spec.py:321] Evaluating on the training split.
I0212 19:50:44.914469 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 19:54:29.702140 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 19:54:32.402969 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 19:57:08.678455 140144802662208 spec.py:349] Evaluating on the test split.
I0212 19:57:11.357432 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 19:59:48.209198 140144802662208 submission_runner.py:408] Time since start: 41621.71s, 	Step: 72486, 	{'train/accuracy': 0.6565991044044495, 'train/loss': 1.6378833055496216, 'train/bleu': 32.39992250394725, 'validation/accuracy': 0.6732092499732971, 'validation/loss': 1.5226255655288696, 'validation/bleu': 29.28347733531719, 'validation/num_examples': 3000, 'test/accuracy': 0.6864911913871765, 'test/loss': 1.4465367794036865, 'test/bleu': 29.154292326536787, 'test/num_examples': 3003, 'score': 25232.844252109528, 'total_duration': 41621.707575798035, 'accumulated_submission_time': 25232.844252109528, 'accumulated_eval_time': 16385.491423606873, 'accumulated_logging_time': 0.9566261768341064}
I0212 19:59:48.235151 139975029352192 logging_writer.py:48] [72486] accumulated_eval_time=16385.491424, accumulated_logging_time=0.956626, accumulated_submission_time=25232.844252, global_step=72486, preemption_count=0, score=25232.844252, test/accuracy=0.686491, test/bleu=29.154292, test/loss=1.446537, test/num_examples=3003, total_duration=41621.707576, train/accuracy=0.656599, train/bleu=32.399923, train/loss=1.637883, validation/accuracy=0.673209, validation/bleu=29.283477, validation/loss=1.522626, validation/num_examples=3000
I0212 19:59:53.446855 139975020959488 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.42541345953941345, loss=1.6161707639694214
I0212 20:00:28.054922 139975029352192 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.38223960995674133, loss=1.629752516746521
I0212 20:01:02.757396 139975020959488 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.39299002289772034, loss=1.6559762954711914
I0212 20:01:37.498270 139975029352192 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.3883943557739258, loss=1.6760401725769043
I0212 20:02:12.263855 139975020959488 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.37791648507118225, loss=1.6937289237976074
I0212 20:02:46.995230 139975029352192 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.4279242157936096, loss=1.6684212684631348
I0212 20:03:21.745110 139975020959488 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3796680271625519, loss=1.6843448877334595
I0212 20:03:56.494183 139975029352192 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.4012971818447113, loss=1.606379508972168
I0212 20:04:31.287783 139975020959488 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.40910881757736206, loss=1.5542353391647339
I0212 20:05:06.079362 139975029352192 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.36977410316467285, loss=1.5808073282241821
I0212 20:05:40.839675 139975020959488 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.3735049068927765, loss=1.6520251035690308
I0212 20:06:15.605180 139975029352192 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.39800477027893066, loss=1.712750792503357
I0212 20:06:50.357487 139975020959488 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.3810659348964691, loss=1.6130396127700806
I0212 20:07:25.159664 139975029352192 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.43332308530807495, loss=1.6456618309020996
I0212 20:07:59.974164 139975020959488 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.3877697288990021, loss=1.629995346069336
I0212 20:08:34.743399 139975029352192 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.41921839118003845, loss=1.6940314769744873
I0212 20:09:09.528187 139975020959488 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.42757371068000793, loss=1.6945265531539917
I0212 20:09:44.327374 139975029352192 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.3826582729816437, loss=1.6841566562652588
I0212 20:10:19.103260 139975020959488 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.41937747597694397, loss=1.7006958723068237
I0212 20:10:53.909507 139975029352192 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.4222979247570038, loss=1.6234302520751953
I0212 20:11:28.731609 139975020959488 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.4108974039554596, loss=1.558564305305481
I0212 20:12:03.517569 139975029352192 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.4327947497367859, loss=1.5857504606246948
I0212 20:12:38.296483 139975020959488 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.36880919337272644, loss=1.5834827423095703
I0212 20:13:13.076439 139975029352192 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.415850430727005, loss=1.6925504207611084
I0212 20:13:47.885338 139975020959488 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.3938632309436798, loss=1.6449739933013916
I0212 20:13:48.314913 140144802662208 spec.py:321] Evaluating on the training split.
I0212 20:13:51.292777 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 20:17:43.632071 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 20:17:46.319018 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 20:20:24.958498 140144802662208 spec.py:349] Evaluating on the test split.
I0212 20:20:27.644846 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 20:22:53.564730 140144802662208 submission_runner.py:408] Time since start: 43007.06s, 	Step: 74903, 	{'train/accuracy': 0.65643310546875, 'train/loss': 1.6501281261444092, 'train/bleu': 32.34811927392111, 'validation/accuracy': 0.6752055287361145, 'validation/loss': 1.5135127305984497, 'validation/bleu': 29.520270208655514, 'validation/num_examples': 3000, 'test/accuracy': 0.6882458925247192, 'test/loss': 1.440338373184204, 'test/bleu': 29.18233746463092, 'test/num_examples': 3003, 'score': 26072.833317756653, 'total_duration': 43007.06311130524, 'accumulated_submission_time': 26072.833317756653, 'accumulated_eval_time': 16930.741188049316, 'accumulated_logging_time': 0.9927854537963867}
I0212 20:22:53.591347 139975029352192 logging_writer.py:48] [74903] accumulated_eval_time=16930.741188, accumulated_logging_time=0.992785, accumulated_submission_time=26072.833318, global_step=74903, preemption_count=0, score=26072.833318, test/accuracy=0.688246, test/bleu=29.182337, test/loss=1.440338, test/num_examples=3003, total_duration=43007.063111, train/accuracy=0.656433, train/bleu=32.348119, train/loss=1.650128, validation/accuracy=0.675206, validation/bleu=29.520270, validation/loss=1.513513, validation/num_examples=3000
I0212 20:23:27.524707 139975020959488 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.39847666025161743, loss=1.6111583709716797
I0212 20:24:02.218140 139975029352192 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.39562487602233887, loss=1.541448950767517
I0212 20:24:36.974045 139975020959488 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.4019707441329956, loss=1.6154875755310059
I0212 20:25:11.739861 139975029352192 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.4772079288959503, loss=1.664951205253601
I0212 20:25:46.496534 139975020959488 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.4074612557888031, loss=1.6053067445755005
I0212 20:26:21.269735 139975029352192 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.380695641040802, loss=1.6528306007385254
I0212 20:26:56.059046 139975020959488 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.3957477807998657, loss=1.6639822721481323
I0212 20:27:30.811697 139975029352192 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.4094071388244629, loss=1.6678588390350342
I0212 20:28:05.565659 139975020959488 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.39453670382499695, loss=1.6541284322738647
I0212 20:28:40.347699 139975029352192 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.39838388562202454, loss=1.6202669143676758
I0212 20:29:15.101740 139975020959488 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.44548842310905457, loss=1.5952529907226562
I0212 20:29:49.869260 139975029352192 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.4317021071910858, loss=1.620901107788086
I0212 20:30:24.637970 139975020959488 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.3848473131656647, loss=1.6080896854400635
I0212 20:30:59.384930 139975029352192 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.3974330723285675, loss=1.6541991233825684
I0212 20:31:34.117416 139975020959488 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.44388970732688904, loss=1.7263822555541992
I0212 20:32:08.882450 139975029352192 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.4094868302345276, loss=1.6165082454681396
I0212 20:32:43.648696 139975020959488 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.4186728894710541, loss=1.5887260437011719
I0212 20:33:18.410349 139975029352192 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.436008483171463, loss=1.655046820640564
I0212 20:33:53.186120 139975020959488 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.947904348373413, loss=1.6340649127960205
I0212 20:34:27.959278 139975029352192 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.4039558470249176, loss=1.6226369142532349
I0212 20:35:02.756747 139975020959488 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.39086586236953735, loss=1.6883273124694824
I0212 20:35:37.507201 139975029352192 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.39380115270614624, loss=1.624468207359314
I0212 20:36:12.267278 139975020959488 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.42265135049819946, loss=1.6171766519546509
I0212 20:36:47.051028 139975029352192 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.41262441873550415, loss=1.6347174644470215
I0212 20:36:53.732143 140144802662208 spec.py:321] Evaluating on the training split.
I0212 20:36:56.704665 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 20:40:41.760942 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 20:40:44.467585 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 20:43:22.294812 140144802662208 spec.py:349] Evaluating on the test split.
I0212 20:43:24.994749 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 20:45:53.363745 140144802662208 submission_runner.py:408] Time since start: 44386.86s, 	Step: 77321, 	{'train/accuracy': 0.6606552600860596, 'train/loss': 1.6113643646240234, 'train/bleu': 33.132585237997496, 'validation/accuracy': 0.6767429709434509, 'validation/loss': 1.5007519721984863, 'validation/bleu': 29.464094676542434, 'validation/num_examples': 3000, 'test/accuracy': 0.6892685294151306, 'test/loss': 1.427660584449768, 'test/bleu': 29.210126730147856, 'test/num_examples': 3003, 'score': 26912.885219335556, 'total_duration': 44386.86210536957, 'accumulated_submission_time': 26912.885219335556, 'accumulated_eval_time': 17470.37271785736, 'accumulated_logging_time': 1.029709815979004}
I0212 20:45:53.389802 139975020959488 logging_writer.py:48] [77321] accumulated_eval_time=17470.372718, accumulated_logging_time=1.029710, accumulated_submission_time=26912.885219, global_step=77321, preemption_count=0, score=26912.885219, test/accuracy=0.689269, test/bleu=29.210127, test/loss=1.427661, test/num_examples=3003, total_duration=44386.862105, train/accuracy=0.660655, train/bleu=33.132585, train/loss=1.611364, validation/accuracy=0.676743, validation/bleu=29.464095, validation/loss=1.500752, validation/num_examples=3000
I0212 20:46:21.138463 139975029352192 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.4173484146595001, loss=1.634400725364685
I0212 20:46:55.867151 139975020959488 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.4015219509601593, loss=1.6390653848648071
I0212 20:47:30.614784 139975029352192 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.4400475025177002, loss=1.6247107982635498
I0212 20:48:05.381607 139975020959488 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.44159746170043945, loss=1.7973271608352661
I0212 20:48:40.185386 139975029352192 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.39379677176475525, loss=1.683740496635437
I0212 20:49:14.925390 139975020959488 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.4145349860191345, loss=1.614088773727417
I0212 20:49:49.670058 139975029352192 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.40658363699913025, loss=1.5794150829315186
I0212 20:50:24.453345 139975020959488 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.449521541595459, loss=1.684979796409607
I0212 20:50:59.336722 139975029352192 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.5508633852005005, loss=1.6641515493392944
I0212 20:51:34.110134 139975020959488 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3888303339481354, loss=1.6410237550735474
I0212 20:52:08.873344 139975029352192 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.3926556408405304, loss=1.6032284498214722
I0212 20:52:43.621126 139975020959488 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3920016884803772, loss=1.6452046632766724
I0212 20:53:18.430914 139975029352192 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3979538381099701, loss=1.6326130628585815
I0212 20:53:53.209031 139975020959488 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.3944266438484192, loss=1.647569179534912
I0212 20:54:27.986203 139975029352192 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3991263806819916, loss=1.5596691370010376
I0212 20:55:02.759406 139975020959488 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.4327550530433655, loss=1.6049227714538574
I0212 20:55:37.529233 139975029352192 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.4113270044326782, loss=1.6076691150665283
I0212 20:56:12.350046 139975020959488 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.41875502467155457, loss=1.6792659759521484
I0212 20:56:47.161139 139975029352192 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.3866857886314392, loss=1.6719175577163696
I0212 20:57:21.986399 139975020959488 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.4155181348323822, loss=1.6445242166519165
I0212 20:57:56.751896 139975029352192 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.40968799591064453, loss=1.5966293811798096
I0212 20:58:31.536364 139975020959488 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.44856756925582886, loss=1.7023667097091675
I0212 20:59:06.315631 139975029352192 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.42262017726898193, loss=1.6501219272613525
I0212 20:59:41.137240 139975020959488 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.4185269773006439, loss=1.658618688583374
I0212 20:59:53.402672 140144802662208 spec.py:321] Evaluating on the training split.
I0212 20:59:56.385836 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 21:03:24.411657 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 21:03:27.079825 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 21:06:05.800933 140144802662208 spec.py:349] Evaluating on the test split.
I0212 21:06:08.483531 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 21:08:28.945153 140144802662208 submission_runner.py:408] Time since start: 45742.44s, 	Step: 79737, 	{'train/accuracy': 0.6597093939781189, 'train/loss': 1.622588872909546, 'train/bleu': 32.721303780072326, 'validation/accuracy': 0.6777225136756897, 'validation/loss': 1.5018718242645264, 'validation/bleu': 29.9094847833621, 'validation/num_examples': 3000, 'test/accuracy': 0.6904189586639404, 'test/loss': 1.426446557044983, 'test/bleu': 29.423481171518826, 'test/num_examples': 3003, 'score': 27752.805153131485, 'total_duration': 45742.4435338974, 'accumulated_submission_time': 27752.805153131485, 'accumulated_eval_time': 17985.91515660286, 'accumulated_logging_time': 1.0653765201568604}
I0212 21:08:28.972591 139975029352192 logging_writer.py:48] [79737] accumulated_eval_time=17985.915157, accumulated_logging_time=1.065377, accumulated_submission_time=27752.805153, global_step=79737, preemption_count=0, score=27752.805153, test/accuracy=0.690419, test/bleu=29.423481, test/loss=1.426447, test/num_examples=3003, total_duration=45742.443534, train/accuracy=0.659709, train/bleu=32.721304, train/loss=1.622589, validation/accuracy=0.677723, validation/bleu=29.909485, validation/loss=1.501872, validation/num_examples=3000
I0212 21:08:51.171873 139975020959488 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.4347386956214905, loss=1.641439437866211
I0212 21:09:25.885292 139975029352192 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.4455200731754303, loss=1.6722233295440674
I0212 21:10:00.684969 139975020959488 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.41572487354278564, loss=1.535555124282837
I0212 21:10:35.468694 139975029352192 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.3933746814727783, loss=1.7082666158676147
I0212 21:11:10.243893 139975020959488 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.40520578622817993, loss=1.5888854265213013
I0212 21:11:45.023952 139975029352192 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.4134991765022278, loss=1.6647160053253174
I0212 21:12:19.789337 139975020959488 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3958408832550049, loss=1.6099348068237305
I0212 21:12:54.560671 139975029352192 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.4028836190700531, loss=1.7182081937789917
I0212 21:13:29.363855 139975020959488 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.3959490954875946, loss=1.605021357536316
I0212 21:14:04.151221 139975029352192 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.4001166820526123, loss=1.5846776962280273
I0212 21:14:38.927272 139975020959488 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.4376290738582611, loss=1.6663341522216797
I0212 21:15:13.696057 139975029352192 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.43413493037223816, loss=1.6363074779510498
I0212 21:15:48.473474 139975020959488 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.41589632630348206, loss=1.6816096305847168
I0212 21:16:23.274263 139975029352192 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.3944620192050934, loss=1.566684603691101
I0212 21:16:58.032664 139975020959488 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.4281063377857208, loss=1.6321632862091064
I0212 21:17:32.816590 139975029352192 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.4298897385597229, loss=1.7190511226654053
I0212 21:18:07.590900 139975020959488 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.4057258665561676, loss=1.605295181274414
I0212 21:18:42.374754 139975029352192 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.4188956320285797, loss=1.555366039276123
I0212 21:19:17.155945 139975020959488 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.4301658868789673, loss=1.6497983932495117
I0212 21:19:51.926768 139975029352192 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.5024467706680298, loss=1.5914802551269531
I0212 21:20:26.714535 139975020959488 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.4181748628616333, loss=1.6178113222122192
I0212 21:21:01.490157 139975029352192 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.40941178798675537, loss=1.6656763553619385
I0212 21:21:36.287670 139975020959488 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.4338647723197937, loss=1.6841480731964111
I0212 21:22:11.047374 139975029352192 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.4473550319671631, loss=1.5518020391464233
I0212 21:22:29.192474 140144802662208 spec.py:321] Evaluating on the training split.
I0212 21:22:32.173856 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 21:25:54.426458 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 21:25:57.096565 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 21:28:40.710682 140144802662208 spec.py:349] Evaluating on the test split.
I0212 21:28:43.380080 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 21:31:13.871867 140144802662208 submission_runner.py:408] Time since start: 47107.37s, 	Step: 82154, 	{'train/accuracy': 0.6717402935028076, 'train/loss': 1.538611888885498, 'train/bleu': 33.79730536923105, 'validation/accuracy': 0.6788632273674011, 'validation/loss': 1.4914655685424805, 'validation/bleu': 29.745504916128827, 'validation/num_examples': 3000, 'test/accuracy': 0.6918599009513855, 'test/loss': 1.4198691844940186, 'test/bleu': 29.724196297362315, 'test/num_examples': 3003, 'score': 28592.93453645706, 'total_duration': 47107.37024998665, 'accumulated_submission_time': 28592.93453645706, 'accumulated_eval_time': 18510.594499349594, 'accumulated_logging_time': 1.1032533645629883}
I0212 21:31:13.898497 139975020959488 logging_writer.py:48] [82154] accumulated_eval_time=18510.594499, accumulated_logging_time=1.103253, accumulated_submission_time=28592.934536, global_step=82154, preemption_count=0, score=28592.934536, test/accuracy=0.691860, test/bleu=29.724196, test/loss=1.419869, test/num_examples=3003, total_duration=47107.370250, train/accuracy=0.671740, train/bleu=33.797305, train/loss=1.538612, validation/accuracy=0.678863, validation/bleu=29.745505, validation/loss=1.491466, validation/num_examples=3000
I0212 21:31:30.185661 139975029352192 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.4169631898403168, loss=1.6688326597213745
I0212 21:32:04.862970 139975020959488 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.42678922414779663, loss=1.707374930381775
I0212 21:32:39.605681 139975029352192 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.4294988214969635, loss=1.659163236618042
I0212 21:33:14.361525 139975020959488 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.4190041720867157, loss=1.6328538656234741
I0212 21:33:49.109708 139975029352192 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.4325597882270813, loss=1.557789921760559
I0212 21:34:23.877734 139975020959488 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.42873743176460266, loss=1.628983974456787
I0212 21:34:58.672525 139975029352192 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.41696012020111084, loss=1.6241353750228882
I0212 21:35:33.446773 139975020959488 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.42150336503982544, loss=1.6204711198806763
I0212 21:36:08.243955 139975029352192 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.41904112696647644, loss=1.5680084228515625
I0212 21:36:43.004696 139975020959488 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.44935524463653564, loss=1.6288307905197144
I0212 21:37:17.759378 139975029352192 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.44006410241127014, loss=1.6715370416641235
I0212 21:37:52.524208 139975020959488 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.4290976822376251, loss=1.5600253343582153
I0212 21:38:27.310227 139975029352192 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.4268501400947571, loss=1.618997573852539
I0212 21:39:02.123996 139975020959488 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.4242696464061737, loss=1.625377893447876
I0212 21:39:36.875971 139975029352192 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.3957361578941345, loss=1.6234402656555176
I0212 21:40:11.632184 139975020959488 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.4376456141471863, loss=1.5704602003097534
I0212 21:40:46.417341 139975029352192 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.42627835273742676, loss=1.5368881225585938
I0212 21:41:21.210760 139975020959488 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.42260971665382385, loss=1.6643527746200562
I0212 21:41:55.984054 139975029352192 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.40750637650489807, loss=1.5797832012176514
I0212 21:42:30.776136 139975020959488 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.4215373396873474, loss=1.5787476301193237
I0212 21:43:05.565715 139975029352192 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.38970479369163513, loss=1.6383956670761108
I0212 21:43:40.349174 139975020959488 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.41327616572380066, loss=1.5697855949401855
I0212 21:44:15.124672 139975029352192 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.44140735268592834, loss=1.6555315256118774
I0212 21:44:49.907717 139975020959488 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.42940133810043335, loss=1.5790512561798096
I0212 21:45:13.952621 140144802662208 spec.py:321] Evaluating on the training split.
I0212 21:45:16.930916 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 21:49:37.019235 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 21:49:39.703949 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 21:52:37.138513 140144802662208 spec.py:349] Evaluating on the test split.
I0212 21:52:39.817222 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 21:55:09.196735 140144802662208 submission_runner.py:408] Time since start: 48542.70s, 	Step: 84571, 	{'train/accuracy': 0.665301501750946, 'train/loss': 1.583433747291565, 'train/bleu': 33.228433223994266, 'validation/accuracy': 0.6785780787467957, 'validation/loss': 1.48270583152771, 'validation/bleu': 29.504855205039437, 'validation/num_examples': 3000, 'test/accuracy': 0.6945209503173828, 'test/loss': 1.4088683128356934, 'test/bleu': 29.945540079129746, 'test/num_examples': 3003, 'score': 29432.89954471588, 'total_duration': 48542.69511389732, 'accumulated_submission_time': 29432.89954471588, 'accumulated_eval_time': 19105.83856487274, 'accumulated_logging_time': 1.1398842334747314}
I0212 21:55:09.226353 139975029352192 logging_writer.py:48] [84571] accumulated_eval_time=19105.838565, accumulated_logging_time=1.139884, accumulated_submission_time=29432.899545, global_step=84571, preemption_count=0, score=29432.899545, test/accuracy=0.694521, test/bleu=29.945540, test/loss=1.408868, test/num_examples=3003, total_duration=48542.695114, train/accuracy=0.665302, train/bleu=33.228433, train/loss=1.583434, validation/accuracy=0.678578, validation/bleu=29.504855, validation/loss=1.482706, validation/num_examples=3000
I0212 21:55:19.612896 139975020959488 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.4203624427318573, loss=1.608038306236267
I0212 21:55:54.264696 139975029352192 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.4155547618865967, loss=1.5951952934265137
I0212 21:56:29.011036 139975020959488 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.44057121872901917, loss=1.5398434400558472
I0212 21:57:03.787574 139975029352192 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4395047128200531, loss=1.6386067867279053
I0212 21:57:38.546047 139975020959488 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.45397743582725525, loss=1.584006667137146
I0212 21:58:13.259822 139975029352192 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.412435382604599, loss=1.563778042793274
I0212 21:58:48.034325 139975020959488 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.43941861391067505, loss=1.64188814163208
I0212 21:59:22.803532 139975029352192 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.44630470871925354, loss=1.628691554069519
I0212 21:59:57.549803 139975020959488 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.44719958305358887, loss=1.6021579504013062
I0212 22:00:32.292028 139975029352192 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.45613524317741394, loss=1.6629246473312378
I0212 22:01:07.073377 139975020959488 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.41043826937675476, loss=1.5378057956695557
I0212 22:01:41.826838 139975029352192 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.43245044350624084, loss=1.64407479763031
I0212 22:02:16.565623 139975020959488 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.45547422766685486, loss=1.692638874053955
I0212 22:02:51.336033 139975029352192 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.48984676599502563, loss=1.6774201393127441
I0212 22:03:26.108715 139975020959488 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.42874079942703247, loss=1.6391443014144897
I0212 22:04:00.878261 139975029352192 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.41795721650123596, loss=1.5624582767486572
I0212 22:04:35.643625 139975020959488 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.4537580907344818, loss=1.5763713121414185
I0212 22:05:10.430394 139975029352192 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.4286377727985382, loss=1.5728565454483032
I0212 22:05:45.209743 139975020959488 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4407098591327667, loss=1.6914118528366089
I0212 22:06:19.988660 139975029352192 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.4317181408405304, loss=1.5887699127197266
I0212 22:06:54.747392 139975020959488 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.454682320356369, loss=1.561415195465088
I0212 22:07:29.499717 139975029352192 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.43389424681663513, loss=1.6109720468521118
I0212 22:08:04.304362 139975020959488 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.4643264412879944, loss=1.6046937704086304
I0212 22:08:39.053780 139975029352192 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4359216094017029, loss=1.5730215311050415
I0212 22:09:09.359665 140144802662208 spec.py:321] Evaluating on the training split.
I0212 22:09:12.332777 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 22:12:56.394254 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 22:12:59.081114 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 22:15:30.656688 140144802662208 spec.py:349] Evaluating on the test split.
I0212 22:15:33.342341 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 22:18:00.257875 140144802662208 submission_runner.py:408] Time since start: 49913.76s, 	Step: 86989, 	{'train/accuracy': 0.6616896986961365, 'train/loss': 1.6091835498809814, 'train/bleu': 33.137917686474665, 'validation/accuracy': 0.6786152720451355, 'validation/loss': 1.482520580291748, 'validation/bleu': 29.751823489337745, 'validation/num_examples': 3000, 'test/accuracy': 0.6953459978103638, 'test/loss': 1.4079169034957886, 'test/bleu': 29.84721352815368, 'test/num_examples': 3003, 'score': 30272.9449737072, 'total_duration': 49913.756229400635, 'accumulated_submission_time': 30272.9449737072, 'accumulated_eval_time': 19636.736701726913, 'accumulated_logging_time': 1.1791932582855225}
I0212 22:18:00.290323 139975020959488 logging_writer.py:48] [86989] accumulated_eval_time=19636.736702, accumulated_logging_time=1.179193, accumulated_submission_time=30272.944974, global_step=86989, preemption_count=0, score=30272.944974, test/accuracy=0.695346, test/bleu=29.847214, test/loss=1.407917, test/num_examples=3003, total_duration=49913.756229, train/accuracy=0.661690, train/bleu=33.137918, train/loss=1.609184, validation/accuracy=0.678615, validation/bleu=29.751823, validation/loss=1.482521, validation/num_examples=3000
I0212 22:18:04.475083 139975029352192 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.4084899127483368, loss=1.5535985231399536
I0212 22:18:39.134297 139975020959488 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.4459291696548462, loss=1.5445233583450317
I0212 22:19:13.825079 139975029352192 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.44000959396362305, loss=1.6032443046569824
I0212 22:19:48.609237 139975020959488 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.45775362849235535, loss=1.5836201906204224
I0212 22:20:23.388178 139975029352192 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.8247045874595642, loss=1.582432508468628
I0212 22:20:58.143025 139975020959488 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.43002891540527344, loss=1.6329480409622192
I0212 22:21:32.878155 139975029352192 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.4268953502178192, loss=1.5545449256896973
I0212 22:22:07.655212 139975020959488 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.43125489354133606, loss=1.5853512287139893
I0212 22:22:42.460907 139975029352192 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.42313000559806824, loss=1.549889326095581
I0212 22:23:17.275332 139975020959488 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.43046116828918457, loss=1.559866189956665
I0212 22:23:52.046804 139975029352192 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.5031412839889526, loss=1.5279501676559448
I0212 22:24:26.812006 139975020959488 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.43996766209602356, loss=1.6090271472930908
I0212 22:25:01.574348 139975029352192 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.4402143061161041, loss=1.5933582782745361
I0212 22:25:36.335495 139975020959488 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.44741180539131165, loss=1.6281535625457764
I0212 22:26:11.104673 139975029352192 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.44445303082466125, loss=1.6035407781600952
I0212 22:26:45.855881 139975020959488 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.43593940138816833, loss=1.5949162244796753
I0212 22:27:20.609697 139975029352192 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.4339074194431305, loss=1.50396728515625
I0212 22:27:55.346950 139975020959488 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.46875691413879395, loss=1.5798652172088623
I0212 22:28:30.129596 139975029352192 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4358825087547302, loss=1.621466875076294
I0212 22:29:04.910383 139975020959488 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.46383237838745117, loss=1.614214539527893
I0212 22:29:39.655332 139975029352192 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.4296363294124603, loss=1.5906809568405151
I0212 22:30:14.410168 139975020959488 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.4380378723144531, loss=1.5951956510543823
I0212 22:30:49.176119 139975029352192 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.43374109268188477, loss=1.5992231369018555
I0212 22:31:23.936820 139975020959488 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.4542142152786255, loss=1.5811158418655396
I0212 22:31:58.693017 139975029352192 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.43017151951789856, loss=1.5487284660339355
I0212 22:32:00.506717 140144802662208 spec.py:321] Evaluating on the training split.
I0212 22:32:03.502492 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 22:36:03.926373 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 22:36:06.591954 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 22:38:46.573752 140144802662208 spec.py:349] Evaluating on the test split.
I0212 22:38:49.256441 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 22:41:05.959215 140144802662208 submission_runner.py:408] Time since start: 51299.46s, 	Step: 89407, 	{'train/accuracy': 0.6713466644287109, 'train/loss': 1.5552958250045776, 'train/bleu': 33.52182294055766, 'validation/accuracy': 0.6820374131202698, 'validation/loss': 1.465645670890808, 'validation/bleu': 29.869141731518045, 'validation/num_examples': 3000, 'test/accuracy': 0.6946952939033508, 'test/loss': 1.3910845518112183, 'test/bleu': 29.631786634944643, 'test/num_examples': 3003, 'score': 31113.069878339767, 'total_duration': 51299.45757818222, 'accumulated_submission_time': 31113.069878339767, 'accumulated_eval_time': 20182.18912935257, 'accumulated_logging_time': 1.2226190567016602}
I0212 22:41:05.988582 139975020959488 logging_writer.py:48] [89407] accumulated_eval_time=20182.189129, accumulated_logging_time=1.222619, accumulated_submission_time=31113.069878, global_step=89407, preemption_count=0, score=31113.069878, test/accuracy=0.694695, test/bleu=29.631787, test/loss=1.391085, test/num_examples=3003, total_duration=51299.457578, train/accuracy=0.671347, train/bleu=33.521823, train/loss=1.555296, validation/accuracy=0.682037, validation/bleu=29.869142, validation/loss=1.465646, validation/num_examples=3000
I0212 22:41:38.567528 139975029352192 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.439337819814682, loss=1.5196466445922852
I0212 22:42:13.240523 139975020959488 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.46485915780067444, loss=1.5355921983718872
I0212 22:42:47.972736 139975029352192 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4848990738391876, loss=1.583860158920288
I0212 22:43:22.753147 139975020959488 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.451570600271225, loss=1.5581825971603394
I0212 22:43:57.553851 139975029352192 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.4289112389087677, loss=1.5547701120376587
I0212 22:44:32.324175 139975020959488 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.4649660289287567, loss=1.6536211967468262
I0212 22:45:07.121519 139975029352192 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.4574662446975708, loss=1.6123369932174683
I0212 22:45:41.950673 139975020959488 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.4361211359500885, loss=1.6271841526031494
I0212 22:46:16.792701 139975029352192 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.43741080164909363, loss=1.6381527185440063
I0212 22:46:51.593076 139975020959488 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.45991072058677673, loss=1.5373929738998413
I0212 22:47:26.367757 139975029352192 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.4605429172515869, loss=1.5150034427642822
I0212 22:48:01.138413 139975020959488 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.4839353859424591, loss=1.592268943786621
I0212 22:48:35.923795 139975029352192 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.46200644969940186, loss=1.6634949445724487
I0212 22:49:10.724059 139975020959488 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4656648337841034, loss=1.6187682151794434
I0212 22:49:45.502633 139975029352192 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.443185955286026, loss=1.594456434249878
I0212 22:50:20.303497 139975020959488 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.4620354473590851, loss=1.5794845819473267
I0212 22:50:55.087569 139975029352192 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.51360023021698, loss=1.5778250694274902
I0212 22:51:29.872640 139975020959488 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.46772393584251404, loss=1.6328226327896118
I0212 22:52:04.680808 139975029352192 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.4637611210346222, loss=1.6254918575286865
I0212 22:52:39.472163 139975020959488 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.4659910202026367, loss=1.5317094326019287
I0212 22:53:14.247223 139975029352192 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.4582071602344513, loss=1.542773962020874
I0212 22:53:49.030103 139975020959488 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.46164512634277344, loss=1.5675852298736572
I0212 22:54:23.842362 139975029352192 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.48476436734199524, loss=1.629556655883789
I0212 22:54:58.638113 139975020959488 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.4477730095386505, loss=1.5806666612625122
I0212 22:55:06.013967 140144802662208 spec.py:321] Evaluating on the training split.
I0212 22:55:08.998194 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 22:58:13.806167 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 22:58:16.506563 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 23:00:48.548610 140144802662208 spec.py:349] Evaluating on the test split.
I0212 23:00:51.238145 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 23:03:15.960898 140144802662208 submission_runner.py:408] Time since start: 52629.46s, 	Step: 91823, 	{'train/accuracy': 0.6688807606697083, 'train/loss': 1.561652421951294, 'train/bleu': 33.646249972575525, 'validation/accuracy': 0.6818886399269104, 'validation/loss': 1.4689723253250122, 'validation/bleu': 29.931857466564306, 'validation/num_examples': 3000, 'test/accuracy': 0.6980187296867371, 'test/loss': 1.3815217018127441, 'test/bleu': 30.353041929433832, 'test/num_examples': 3003, 'score': 31953.005130767822, 'total_duration': 52629.459279060364, 'accumulated_submission_time': 31953.005130767822, 'accumulated_eval_time': 20672.136009454727, 'accumulated_logging_time': 1.2626848220825195}
I0212 23:03:15.989230 139975029352192 logging_writer.py:48] [91823] accumulated_eval_time=20672.136009, accumulated_logging_time=1.262685, accumulated_submission_time=31953.005131, global_step=91823, preemption_count=0, score=31953.005131, test/accuracy=0.698019, test/bleu=30.353042, test/loss=1.381522, test/num_examples=3003, total_duration=52629.459279, train/accuracy=0.668881, train/bleu=33.646250, train/loss=1.561652, validation/accuracy=0.681889, validation/bleu=29.931857, validation/loss=1.468972, validation/num_examples=3000
I0212 23:03:43.012397 139975020959488 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.4560447931289673, loss=1.5298086404800415
I0212 23:04:17.719590 139975029352192 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.46093985438346863, loss=1.6271346807479858
I0212 23:04:52.476675 139975020959488 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.45691588521003723, loss=1.5797942876815796
I0212 23:05:27.211524 139975029352192 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.5100774765014648, loss=1.6577396392822266
I0212 23:06:01.965034 139975020959488 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.4892739951610565, loss=1.5464818477630615
I0212 23:06:36.732601 139975029352192 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.482403039932251, loss=1.6669983863830566
I0212 23:07:11.512412 139975020959488 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.4618024230003357, loss=1.5391522645950317
I0212 23:07:46.280769 139975029352192 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.44588029384613037, loss=1.5018243789672852
I0212 23:08:21.057994 139975020959488 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.43319714069366455, loss=1.5989092588424683
I0212 23:08:55.817607 139975029352192 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.47219032049179077, loss=1.621740460395813
I0212 23:09:30.618269 139975020959488 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.4675843119621277, loss=1.6403003931045532
I0212 23:10:05.443420 139975029352192 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.49037519097328186, loss=1.6236305236816406
I0212 23:10:40.191489 139975020959488 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.4974486231803894, loss=1.5771242380142212
I0212 23:11:14.957080 139975029352192 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.49935948848724365, loss=1.5368472337722778
I0212 23:11:49.737438 139975020959488 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.4769292175769806, loss=1.4981659650802612
I0212 23:12:24.538332 139975029352192 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.48028334975242615, loss=1.561051607131958
I0212 23:12:59.297810 139975020959488 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.46222174167633057, loss=1.5056804418563843
I0212 23:13:34.080835 139975029352192 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.49110376834869385, loss=1.5392787456512451
I0212 23:14:08.836411 139975020959488 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.48907721042633057, loss=1.5829404592514038
I0212 23:14:43.594611 139975029352192 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.47407612204551697, loss=1.5033948421478271
I0212 23:15:18.351322 139975020959488 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.5265394449234009, loss=1.56277596950531
I0212 23:15:53.100490 139975029352192 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.48266446590423584, loss=1.5915383100509644
I0212 23:16:27.856430 139975020959488 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.4770353138446808, loss=1.6483227014541626
I0212 23:17:02.611753 139975029352192 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.4918244183063507, loss=1.5357788801193237
I0212 23:17:16.225161 140144802662208 spec.py:321] Evaluating on the training split.
I0212 23:17:19.200196 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 23:20:44.680769 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 23:20:47.376473 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 23:23:44.999229 140144802662208 spec.py:349] Evaluating on the test split.
I0212 23:23:47.694398 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 23:26:36.358866 140144802662208 submission_runner.py:408] Time since start: 54029.86s, 	Step: 94241, 	{'train/accuracy': 0.6905794739723206, 'train/loss': 1.4270117282867432, 'train/bleu': 34.98234446474538, 'validation/accuracy': 0.6850627660751343, 'validation/loss': 1.4553813934326172, 'validation/bleu': 30.200363953398387, 'validation/num_examples': 3000, 'test/accuracy': 0.699785053730011, 'test/loss': 1.3779276609420776, 'test/bleu': 30.07975448373041, 'test/num_examples': 3003, 'score': 32793.152134656906, 'total_duration': 54029.85724711418, 'accumulated_submission_time': 32793.152134656906, 'accumulated_eval_time': 21232.269668102264, 'accumulated_logging_time': 1.301042079925537}
I0212 23:26:36.389127 139975020959488 logging_writer.py:48] [94241] accumulated_eval_time=21232.269668, accumulated_logging_time=1.301042, accumulated_submission_time=32793.152135, global_step=94241, preemption_count=0, score=32793.152135, test/accuracy=0.699785, test/bleu=30.079754, test/loss=1.377928, test/num_examples=3003, total_duration=54029.857247, train/accuracy=0.690579, train/bleu=34.982344, train/loss=1.427012, validation/accuracy=0.685063, validation/bleu=30.200364, validation/loss=1.455381, validation/num_examples=3000
I0212 23:26:57.176136 139975029352192 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.46287569403648376, loss=1.5000338554382324
I0212 23:27:31.868825 139975020959488 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.47794705629348755, loss=1.530122995376587
I0212 23:28:06.641252 139975029352192 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4732305705547333, loss=1.522990345954895
I0212 23:28:41.425909 139975020959488 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.4865186810493469, loss=1.5637673139572144
I0212 23:29:16.184063 139975029352192 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.4757991135120392, loss=1.531316876411438
I0212 23:29:50.919320 139975020959488 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.5165444016456604, loss=1.5400667190551758
I0212 23:30:25.705473 139975029352192 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.4605301022529602, loss=1.6196634769439697
I0212 23:31:00.455916 139975020959488 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.47082632780075073, loss=1.5208275318145752
I0212 23:31:35.215046 139975029352192 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.48077356815338135, loss=1.5196694135665894
I0212 23:32:09.992748 139975020959488 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.45712509751319885, loss=1.5469915866851807
I0212 23:32:44.772894 139975029352192 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.4723558723926544, loss=1.620188593864441
I0212 23:33:19.522884 139975020959488 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.490711510181427, loss=1.558622121810913
I0212 23:33:54.257857 139975029352192 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.4923837184906006, loss=1.531489372253418
I0212 23:34:29.051370 139975020959488 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.5072701573371887, loss=1.5542513132095337
I0212 23:35:03.813322 139975029352192 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.4817619323730469, loss=1.620696783065796
I0212 23:35:38.636730 139975020959488 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.48065608739852905, loss=1.4836483001708984
I0212 23:36:13.442512 139975029352192 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.48635053634643555, loss=1.4924591779708862
I0212 23:36:48.216220 139975020959488 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.4943975508213043, loss=1.5733532905578613
I0212 23:37:22.989225 139975029352192 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.4964877665042877, loss=1.5735037326812744
I0212 23:37:57.759667 139975020959488 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.49334946274757385, loss=1.4346494674682617
I0212 23:38:32.536857 139975029352192 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.507026195526123, loss=1.540755033493042
I0212 23:39:07.345038 139975020959488 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.48740068078041077, loss=1.51632559299469
I0212 23:39:42.106599 139975029352192 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.516669750213623, loss=1.5429155826568604
I0212 23:40:16.883428 139975020959488 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.4908995032310486, loss=1.5947891473770142
I0212 23:40:36.432425 140144802662208 spec.py:321] Evaluating on the training split.
I0212 23:40:39.416876 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 23:44:27.289438 140144802662208 spec.py:333] Evaluating on the validation split.
I0212 23:44:29.973348 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 23:47:05.424261 140144802662208 spec.py:349] Evaluating on the test split.
I0212 23:47:08.119139 140144802662208 workload.py:181] Translating evaluation dataset.
I0212 23:49:42.588097 140144802662208 submission_runner.py:408] Time since start: 55416.09s, 	Step: 96658, 	{'train/accuracy': 0.6737492084503174, 'train/loss': 1.5275613069534302, 'train/bleu': 34.108211883515416, 'validation/accuracy': 0.6844552159309387, 'validation/loss': 1.4521570205688477, 'validation/bleu': 30.20829723913238, 'validation/num_examples': 3000, 'test/accuracy': 0.6992853283882141, 'test/loss': 1.3757984638214111, 'test/bleu': 30.073980041317707, 'test/num_examples': 3003, 'score': 33633.102585315704, 'total_duration': 55416.08647465706, 'accumulated_submission_time': 33633.102585315704, 'accumulated_eval_time': 21778.425297021866, 'accumulated_logging_time': 1.342395544052124}
I0212 23:49:42.617363 139975029352192 logging_writer.py:48] [96658] accumulated_eval_time=21778.425297, accumulated_logging_time=1.342396, accumulated_submission_time=33633.102585, global_step=96658, preemption_count=0, score=33633.102585, test/accuracy=0.699285, test/bleu=30.073980, test/loss=1.375798, test/num_examples=3003, total_duration=55416.086475, train/accuracy=0.673749, train/bleu=34.108212, train/loss=1.527561, validation/accuracy=0.684455, validation/bleu=30.208297, validation/loss=1.452157, validation/num_examples=3000
I0212 23:49:57.560843 139975020959488 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.49809694290161133, loss=1.5258861780166626
I0212 23:50:32.254204 139975029352192 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.5034236907958984, loss=1.5545612573623657
I0212 23:51:06.993259 139975020959488 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.507289469242096, loss=1.5508277416229248
I0212 23:51:41.816987 139975029352192 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.5187352895736694, loss=1.5524543523788452
I0212 23:52:16.612137 139975020959488 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.519810140132904, loss=1.4744789600372314
I0212 23:52:51.426515 139975029352192 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.5150759220123291, loss=1.5517189502716064
I0212 23:53:26.261172 139975020959488 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.5111708641052246, loss=1.5830730199813843
I0212 23:54:01.024173 139975029352192 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.5195819139480591, loss=1.5132752656936646
I0212 23:54:35.819366 139975020959488 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.4946565628051758, loss=1.4771873950958252
I0212 23:55:10.585886 139975029352192 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.5115466117858887, loss=1.5591890811920166
I0212 23:55:45.364331 139975020959488 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.4980299174785614, loss=1.535025954246521
I0212 23:56:20.120370 139975029352192 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.5115443468093872, loss=1.564406156539917
I0212 23:56:54.908024 139975020959488 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.5149836540222168, loss=1.5430967807769775
I0212 23:57:29.678334 139975029352192 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.5119163393974304, loss=1.5197529792785645
I0212 23:58:04.449788 139975020959488 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.5336775183677673, loss=1.5751301050186157
I0212 23:58:39.217243 139975029352192 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.516582727432251, loss=1.5147401094436646
I0212 23:59:13.983179 139975020959488 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.5188702940940857, loss=1.5125428438186646
I0212 23:59:48.770333 139975029352192 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.48721274733543396, loss=1.4793988466262817
I0213 00:00:23.561498 139975020959488 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5026384592056274, loss=1.5208351612091064
I0213 00:00:58.341928 139975029352192 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.5185776352882385, loss=1.5595097541809082
I0213 00:01:33.138269 139975020959488 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.5075414180755615, loss=1.5361748933792114
I0213 00:02:07.896072 139975029352192 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.5209347009658813, loss=1.4637808799743652
I0213 00:02:42.697990 139975020959488 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5078903436660767, loss=1.5628383159637451
I0213 00:03:17.498415 139975029352192 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5460941791534424, loss=1.4856559038162231
I0213 00:03:42.597613 140144802662208 spec.py:321] Evaluating on the training split.
I0213 00:03:45.569270 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 00:07:41.571091 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 00:07:44.236255 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 00:10:21.280588 140144802662208 spec.py:349] Evaluating on the test split.
I0213 00:10:23.985516 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 00:12:47.263253 140144802662208 submission_runner.py:408] Time since start: 56800.76s, 	Step: 99074, 	{'train/accuracy': 0.6748491525650024, 'train/loss': 1.5263543128967285, 'train/bleu': 33.783884560990835, 'validation/accuracy': 0.6855711340904236, 'validation/loss': 1.4503666162490845, 'validation/bleu': 30.292007534617788, 'validation/num_examples': 3000, 'test/accuracy': 0.7000755667686462, 'test/loss': 1.370320200920105, 'test/bleu': 30.03763911580841, 'test/num_examples': 3003, 'score': 34472.99043941498, 'total_duration': 56800.761588811874, 'accumulated_submission_time': 34472.99043941498, 'accumulated_eval_time': 22323.090844154358, 'accumulated_logging_time': 1.3831169605255127}
I0213 00:12:47.293183 139975020959488 logging_writer.py:48] [99074] accumulated_eval_time=22323.090844, accumulated_logging_time=1.383117, accumulated_submission_time=34472.990439, global_step=99074, preemption_count=0, score=34472.990439, test/accuracy=0.700076, test/bleu=30.037639, test/loss=1.370320, test/num_examples=3003, total_duration=56800.761589, train/accuracy=0.674849, train/bleu=33.783885, train/loss=1.526354, validation/accuracy=0.685571, validation/bleu=30.292008, validation/loss=1.450367, validation/num_examples=3000
I0213 00:12:56.666424 139975029352192 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5358337759971619, loss=1.5228639841079712
I0213 00:13:31.274677 139975020959488 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.5023187398910522, loss=1.4699678421020508
I0213 00:14:05.973120 139975029352192 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.5253044366836548, loss=1.541117548942566
I0213 00:14:40.745630 139975020959488 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5462142825126648, loss=1.5227993726730347
I0213 00:15:15.543462 139975029352192 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.5258642435073853, loss=1.490978717803955
I0213 00:15:50.324452 139975020959488 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.5308812856674194, loss=1.507385492324829
I0213 00:16:25.111208 139975029352192 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.5349686741828918, loss=1.4658682346343994
I0213 00:16:59.926837 139975020959488 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.5210019946098328, loss=1.4749410152435303
I0213 00:17:34.704003 139975029352192 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5585741400718689, loss=1.537685513496399
I0213 00:18:09.489627 139975020959488 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5381402373313904, loss=1.531899333000183
I0213 00:18:44.261510 139975029352192 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5077717900276184, loss=1.556354284286499
I0213 00:19:19.049937 139975020959488 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5230411291122437, loss=1.5214132070541382
I0213 00:19:53.826351 139975029352192 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5243988037109375, loss=1.4703713655471802
I0213 00:20:28.582544 139975020959488 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.5387370586395264, loss=1.5227813720703125
I0213 00:21:03.344590 139975029352192 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5442440509796143, loss=1.5254119634628296
I0213 00:21:38.126166 139975020959488 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.5333535075187683, loss=1.4640140533447266
I0213 00:22:12.894882 139975029352192 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5420975685119629, loss=1.5059494972229004
I0213 00:22:47.676091 139975020959488 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5499293804168701, loss=1.5108611583709717
I0213 00:23:22.440308 139975029352192 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.5268359780311584, loss=1.5252727270126343
I0213 00:23:57.207915 139975020959488 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5328109264373779, loss=1.5278912782669067
I0213 00:24:31.987330 139975029352192 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5264671444892883, loss=1.476696491241455
I0213 00:25:06.765714 139975020959488 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5578922033309937, loss=1.5017162561416626
I0213 00:25:41.526682 139975029352192 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.5362016558647156, loss=1.4770046472549438
I0213 00:26:16.304539 139975020959488 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.5303319096565247, loss=1.504224181175232
I0213 00:26:47.339710 140144802662208 spec.py:321] Evaluating on the training split.
I0213 00:26:50.325941 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 00:30:10.331966 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 00:30:13.041685 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 00:33:02.611234 140144802662208 spec.py:349] Evaluating on the test split.
I0213 00:33:05.326710 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 00:35:46.913436 140144802662208 submission_runner.py:408] Time since start: 58180.41s, 	Step: 101491, 	{'train/accuracy': 0.6866604685783386, 'train/loss': 1.4556916952133179, 'train/bleu': 34.48494577502086, 'validation/accuracy': 0.6869474649429321, 'validation/loss': 1.4450162649154663, 'validation/bleu': 30.187936515973128, 'validation/num_examples': 3000, 'test/accuracy': 0.7018999457359314, 'test/loss': 1.3603651523590088, 'test/bleu': 30.280502251710864, 'test/num_examples': 3003, 'score': 35312.9445669651, 'total_duration': 58180.41181755066, 'accumulated_submission_time': 35312.9445669651, 'accumulated_eval_time': 22862.66453719139, 'accumulated_logging_time': 1.4255762100219727}
I0213 00:35:46.942705 139975029352192 logging_writer.py:48] [101491] accumulated_eval_time=22862.664537, accumulated_logging_time=1.425576, accumulated_submission_time=35312.944567, global_step=101491, preemption_count=0, score=35312.944567, test/accuracy=0.701900, test/bleu=30.280502, test/loss=1.360365, test/num_examples=3003, total_duration=58180.411818, train/accuracy=0.686660, train/bleu=34.484946, train/loss=1.455692, validation/accuracy=0.686947, validation/bleu=30.187937, validation/loss=1.445016, validation/num_examples=3000
I0213 00:35:50.423738 139975020959488 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5516070127487183, loss=1.4808255434036255
I0213 00:36:25.049556 139975029352192 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5409116744995117, loss=1.575895071029663
I0213 00:36:59.733599 139975020959488 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.5422307252883911, loss=1.506709098815918
I0213 00:37:34.474045 139975029352192 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5511966943740845, loss=1.4749679565429688
I0213 00:38:09.224913 139975020959488 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5483843684196472, loss=1.5914610624313354
I0213 00:38:43.956991 139975029352192 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5568134188652039, loss=1.48387610912323
I0213 00:39:18.697226 139975020959488 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.553042471408844, loss=1.4503083229064941
I0213 00:39:53.437129 139975029352192 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.5417041182518005, loss=1.5342098474502563
I0213 00:40:28.195682 139975020959488 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5761054158210754, loss=1.617568016052246
I0213 00:41:02.951393 139975029352192 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5581048727035522, loss=1.5684224367141724
I0213 00:41:37.711399 139975020959488 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5822287201881409, loss=1.4866390228271484
I0213 00:42:12.465074 139975029352192 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5384747385978699, loss=1.4287704229354858
I0213 00:42:47.241277 139975020959488 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5725727081298828, loss=1.4557561874389648
I0213 00:43:22.016060 139975029352192 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.5628259181976318, loss=1.4570173025131226
I0213 00:43:56.782397 139975020959488 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.540924608707428, loss=1.5378960371017456
I0213 00:44:31.572669 139975029352192 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.5561987161636353, loss=1.496675729751587
I0213 00:45:06.338852 139975020959488 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.5696974396705627, loss=1.4511466026306152
I0213 00:45:41.126128 139975029352192 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.5471650958061218, loss=1.3792543411254883
I0213 00:46:15.919729 139975020959488 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5508631467819214, loss=1.4220458269119263
I0213 00:46:50.684831 139975029352192 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5586515665054321, loss=1.5040940046310425
I0213 00:47:25.435124 139975020959488 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.592349112033844, loss=1.4749805927276611
I0213 00:48:00.227209 139975029352192 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.5898489356040955, loss=1.4742815494537354
I0213 00:48:34.977848 139975020959488 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.5859510898590088, loss=1.4996861219406128
I0213 00:49:09.755857 139975029352192 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.5863827466964722, loss=1.4900633096694946
I0213 00:49:44.547684 139975020959488 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.5947692394256592, loss=1.56520414352417
I0213 00:49:47.065908 140144802662208 spec.py:321] Evaluating on the training split.
I0213 00:49:50.052043 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 00:53:57.396217 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 00:54:00.077748 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 00:57:21.680213 140144802662208 spec.py:349] Evaluating on the test split.
I0213 00:57:24.366600 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 01:00:49.647267 140144802662208 submission_runner.py:408] Time since start: 59683.15s, 	Step: 103909, 	{'train/accuracy': 0.6815876960754395, 'train/loss': 1.492857575416565, 'train/bleu': 34.64913971543467, 'validation/accuracy': 0.6869598627090454, 'validation/loss': 1.4377763271331787, 'validation/bleu': 30.17169848257597, 'validation/num_examples': 3000, 'test/accuracy': 0.7025158405303955, 'test/loss': 1.3520406484603882, 'test/bleu': 30.232446000713, 'test/num_examples': 3003, 'score': 36152.97557926178, 'total_duration': 59683.14560890198, 'accumulated_submission_time': 36152.97557926178, 'accumulated_eval_time': 23525.245816469193, 'accumulated_logging_time': 1.4649641513824463}
I0213 01:00:49.676949 139975029352192 logging_writer.py:48] [103909] accumulated_eval_time=23525.245816, accumulated_logging_time=1.464964, accumulated_submission_time=36152.975579, global_step=103909, preemption_count=0, score=36152.975579, test/accuracy=0.702516, test/bleu=30.232446, test/loss=1.352041, test/num_examples=3003, total_duration=59683.145609, train/accuracy=0.681588, train/bleu=34.649140, train/loss=1.492858, validation/accuracy=0.686960, validation/bleu=30.171698, validation/loss=1.437776, validation/num_examples=3000
I0213 01:01:21.513321 139975020959488 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5801482796669006, loss=1.503576397895813
I0213 01:01:56.237746 139975029352192 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.6146481037139893, loss=1.5113953351974487
I0213 01:02:31.016197 139975020959488 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.6163020133972168, loss=1.5212353467941284
I0213 01:03:05.754947 139975029352192 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.5829854607582092, loss=1.539760708808899
I0213 01:03:40.513881 139975020959488 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.5482006072998047, loss=1.4474561214447021
I0213 01:04:15.280604 139975029352192 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.5747339725494385, loss=1.5128872394561768
I0213 01:04:50.031547 139975020959488 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5354934930801392, loss=1.4706766605377197
I0213 01:05:24.775415 139975029352192 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.5749673247337341, loss=1.500654697418213
I0213 01:05:59.555717 139975020959488 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.5888825058937073, loss=1.5341774225234985
I0213 01:06:34.316741 139975029352192 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.6080856919288635, loss=1.4971004724502563
I0213 01:07:09.077982 139975020959488 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5738040208816528, loss=1.5348870754241943
I0213 01:07:43.851000 139975029352192 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5855444073677063, loss=1.4887959957122803
I0213 01:08:18.661089 139975020959488 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.6022738218307495, loss=1.5483170747756958
I0213 01:08:53.431777 139975029352192 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.5881075859069824, loss=1.4372663497924805
I0213 01:09:28.204180 139975020959488 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.5693214535713196, loss=1.442611575126648
I0213 01:10:02.954422 139975029352192 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.588317334651947, loss=1.4553275108337402
I0213 01:10:37.716435 139975020959488 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.6004132628440857, loss=1.5194427967071533
I0213 01:11:12.497251 139975029352192 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.5657470226287842, loss=1.466073989868164
I0213 01:11:47.246104 139975020959488 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.6643443703651428, loss=1.5245012044906616
I0213 01:12:22.004806 139975029352192 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.5896375775337219, loss=1.4887936115264893
I0213 01:12:56.777691 139975020959488 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.5913591980934143, loss=1.4701428413391113
I0213 01:13:31.581152 139975029352192 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.5929738283157349, loss=1.5760232210159302
I0213 01:14:06.382762 139975020959488 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.6141135692596436, loss=1.5103819370269775
I0213 01:14:41.156002 139975029352192 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.5914821028709412, loss=1.5280976295471191
I0213 01:14:49.919933 140144802662208 spec.py:321] Evaluating on the training split.
I0213 01:14:52.911427 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 01:18:48.136804 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 01:18:50.829243 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 01:21:32.017600 140144802662208 spec.py:349] Evaluating on the test split.
I0213 01:21:34.697204 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 01:24:13.914356 140144802662208 submission_runner.py:408] Time since start: 61087.41s, 	Step: 106327, 	{'train/accuracy': 0.6837038993835449, 'train/loss': 1.4776220321655273, 'train/bleu': 35.08743437169032, 'validation/accuracy': 0.688646137714386, 'validation/loss': 1.4313939809799194, 'validation/bleu': 30.522066373997887, 'validation/num_examples': 3000, 'test/accuracy': 0.7030619978904724, 'test/loss': 1.3502763509750366, 'test/bleu': 30.41185965475121, 'test/num_examples': 3003, 'score': 36993.1280477047, 'total_duration': 61087.41273570061, 'accumulated_submission_time': 36993.1280477047, 'accumulated_eval_time': 24089.24018883705, 'accumulated_logging_time': 1.504408836364746}
I0213 01:24:13.944218 139975020959488 logging_writer.py:48] [106327] accumulated_eval_time=24089.240189, accumulated_logging_time=1.504409, accumulated_submission_time=36993.128048, global_step=106327, preemption_count=0, score=36993.128048, test/accuracy=0.703062, test/bleu=30.411860, test/loss=1.350276, test/num_examples=3003, total_duration=61087.412736, train/accuracy=0.683704, train/bleu=35.087434, train/loss=1.477622, validation/accuracy=0.688646, validation/bleu=30.522066, validation/loss=1.431394, validation/num_examples=3000
I0213 01:24:39.606272 139975029352192 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.61366868019104, loss=1.4892240762710571
I0213 01:25:14.288285 139975020959488 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.6837772130966187, loss=1.4842020273208618
I0213 01:25:49.041928 139975029352192 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.6297977566719055, loss=1.43460214138031
I0213 01:26:23.785682 139975020959488 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.6199839115142822, loss=1.4682905673980713
I0213 01:26:58.555143 139975029352192 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.5929911732673645, loss=1.515891671180725
I0213 01:27:33.315643 139975020959488 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.6222861409187317, loss=1.4517097473144531
I0213 01:28:08.081359 139975029352192 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.6102662682533264, loss=1.5580151081085205
I0213 01:28:42.848740 139975020959488 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.5977088212966919, loss=1.4255496263504028
I0213 01:29:17.612578 139975029352192 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.5859838724136353, loss=1.4371709823608398
I0213 01:29:52.391652 139975020959488 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.6012009978294373, loss=1.4486316442489624
I0213 01:30:27.196390 139975029352192 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.6148867607116699, loss=1.516212821006775
I0213 01:31:02.018948 139975020959488 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.6308781504631042, loss=1.4486192464828491
I0213 01:31:36.780163 139975029352192 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.600109338760376, loss=1.3839436769485474
I0213 01:32:11.550941 139975020959488 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.6058205366134644, loss=1.490770697593689
I0213 01:32:46.309576 139975029352192 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.6074698567390442, loss=1.4417259693145752
I0213 01:33:21.083255 139975020959488 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.6443079710006714, loss=1.4424930810928345
I0213 01:33:55.849652 139975029352192 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.6213626265525818, loss=1.5099819898605347
I0213 01:34:30.609845 139975020959488 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.6172678470611572, loss=1.4740952253341675
I0213 01:35:05.402819 139975029352192 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.6603569388389587, loss=1.5227257013320923
I0213 01:35:40.169712 139975020959488 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.6317075490951538, loss=1.5056681632995605
I0213 01:36:14.922842 139975029352192 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.6159219145774841, loss=1.4317786693572998
I0213 01:36:49.677622 139975020959488 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.651013970375061, loss=1.4797916412353516
I0213 01:37:24.445329 139975029352192 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.6073961853981018, loss=1.4538871049880981
I0213 01:37:59.180058 139975020959488 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.5974094271659851, loss=1.4249712228775024
I0213 01:38:14.202149 140144802662208 spec.py:321] Evaluating on the training split.
I0213 01:38:17.191026 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 01:41:47.082404 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 01:41:49.776171 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 01:44:34.353362 140144802662208 spec.py:349] Evaluating on the test split.
I0213 01:44:37.038351 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 01:47:05.873376 140144802662208 submission_runner.py:408] Time since start: 62459.37s, 	Step: 108745, 	{'train/accuracy': 0.6909144520759583, 'train/loss': 1.4369436502456665, 'train/bleu': 34.725908050814276, 'validation/accuracy': 0.6895388960838318, 'validation/loss': 1.42962646484375, 'validation/bleu': 30.422796842264475, 'validation/num_examples': 3000, 'test/accuracy': 0.705420970916748, 'test/loss': 1.344992756843567, 'test/bleu': 30.553237810904655, 'test/num_examples': 3003, 'score': 37833.296102523804, 'total_duration': 62459.371757268906, 'accumulated_submission_time': 37833.296102523804, 'accumulated_eval_time': 24620.91137957573, 'accumulated_logging_time': 1.5444655418395996}
I0213 01:47:05.902914 139975029352192 logging_writer.py:48] [108745] accumulated_eval_time=24620.911380, accumulated_logging_time=1.544466, accumulated_submission_time=37833.296103, global_step=108745, preemption_count=0, score=37833.296103, test/accuracy=0.705421, test/bleu=30.553238, test/loss=1.344993, test/num_examples=3003, total_duration=62459.371757, train/accuracy=0.690914, train/bleu=34.725908, train/loss=1.436944, validation/accuracy=0.689539, validation/bleu=30.422797, validation/loss=1.429626, validation/num_examples=3000
I0213 01:47:25.312518 139975020959488 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.5962685942649841, loss=1.3974123001098633
I0213 01:48:00.103778 139975029352192 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.6350505352020264, loss=1.424831509590149
I0213 01:48:34.820375 139975020959488 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.6431775689125061, loss=1.5204600095748901
I0213 01:49:09.599591 139975029352192 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.6242321729660034, loss=1.4330027103424072
I0213 01:49:44.352824 139975020959488 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.6309258341789246, loss=1.4572819471359253
I0213 01:50:19.107087 139975029352192 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.637061595916748, loss=1.4167577028274536
I0213 01:50:53.856947 139975020959488 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.6543284058570862, loss=1.3780909776687622
I0213 01:51:28.611637 139975029352192 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.631145715713501, loss=1.4768226146697998
I0213 01:52:03.366719 139975020959488 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.6436954736709595, loss=1.4227136373519897
I0213 01:52:38.096930 139975029352192 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.6405729651451111, loss=1.4443187713623047
I0213 01:53:12.866453 139975020959488 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.654480516910553, loss=1.4276870489120483
I0213 01:53:47.676912 139975029352192 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.6696134805679321, loss=1.4810409545898438
I0213 01:54:22.453928 139975020959488 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.623151421546936, loss=1.384401559829712
I0213 01:54:57.252304 139975029352192 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.6468310952186584, loss=1.4615647792816162
I0213 01:55:32.039830 139975020959488 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.6716649532318115, loss=1.4973863363265991
I0213 01:56:06.812738 139975029352192 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.6431851387023926, loss=1.4047037363052368
I0213 01:56:41.583650 139975020959488 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.6427527070045471, loss=1.4265621900558472
I0213 01:57:16.323693 139975029352192 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.671466052532196, loss=1.4376856088638306
I0213 01:57:51.055352 139975020959488 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.6500104069709778, loss=1.3614671230316162
I0213 01:58:25.844782 139975029352192 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.6529066562652588, loss=1.4161055088043213
I0213 01:59:00.608824 139975020959488 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.6346325874328613, loss=1.4718576669692993
I0213 01:59:35.364848 139975029352192 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.6420975923538208, loss=1.4998217821121216
I0213 02:00:10.129056 139975020959488 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.6351681351661682, loss=1.4817625284194946
I0213 02:00:44.939344 139975029352192 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.6448200345039368, loss=1.4862194061279297
I0213 02:01:05.882513 140144802662208 spec.py:321] Evaluating on the training split.
I0213 02:01:08.861020 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 02:04:44.434233 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 02:04:47.113307 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 02:07:31.715956 140144802662208 spec.py:349] Evaluating on the test split.
I0213 02:07:34.407875 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 02:10:03.445993 140144802662208 submission_runner.py:408] Time since start: 63836.94s, 	Step: 111162, 	{'train/accuracy': 0.6862839460372925, 'train/loss': 1.4570157527923584, 'train/bleu': 35.03915497026832, 'validation/accuracy': 0.6900472044944763, 'validation/loss': 1.4253438711166382, 'validation/bleu': 30.634258780744172, 'validation/num_examples': 3000, 'test/accuracy': 0.7066760063171387, 'test/loss': 1.3375072479248047, 'test/bleu': 30.61793987736942, 'test/num_examples': 3003, 'score': 38673.18712234497, 'total_duration': 63836.94434714317, 'accumulated_submission_time': 38673.18712234497, 'accumulated_eval_time': 25158.474792718887, 'accumulated_logging_time': 1.5841474533081055}
I0213 02:10:03.482265 139975020959488 logging_writer.py:48] [111162] accumulated_eval_time=25158.474793, accumulated_logging_time=1.584147, accumulated_submission_time=38673.187122, global_step=111162, preemption_count=0, score=38673.187122, test/accuracy=0.706676, test/bleu=30.617940, test/loss=1.337507, test/num_examples=3003, total_duration=63836.944347, train/accuracy=0.686284, train/bleu=35.039155, train/loss=1.457016, validation/accuracy=0.690047, validation/bleu=30.634259, validation/loss=1.425344, validation/num_examples=3000
I0213 02:10:17.013631 139975029352192 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.6411615014076233, loss=1.4386622905731201
I0213 02:10:51.629746 139975020959488 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.6554515361785889, loss=1.4057366847991943
I0213 02:11:26.391983 139975029352192 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.6956726312637329, loss=1.4655853509902954
I0213 02:12:01.152648 139975020959488 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.6651756763458252, loss=1.4696664810180664
I0213 02:12:35.919303 139975029352192 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.6833778619766235, loss=1.448726773262024
I0213 02:13:10.658534 139975020959488 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.6867081522941589, loss=1.456175684928894
I0213 02:13:45.406383 139975029352192 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.6867735385894775, loss=1.3941795825958252
I0213 02:14:20.170016 139975020959488 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.675603985786438, loss=1.4491608142852783
I0213 02:14:54.959048 139975029352192 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.6842415928840637, loss=1.3752644062042236
I0213 02:15:29.713857 139975020959488 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.7012831568717957, loss=1.3949722051620483
I0213 02:16:04.518809 139975029352192 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.7131091356277466, loss=1.5358190536499023
I0213 02:16:39.292064 139975020959488 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.6740276217460632, loss=1.4763262271881104
I0213 02:17:14.043668 139975029352192 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.6533439755439758, loss=1.3859599828720093
I0213 02:17:48.811906 139975020959488 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.6638275384902954, loss=1.4278597831726074
I0213 02:18:23.548430 139975029352192 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.7146413922309875, loss=1.4455310106277466
I0213 02:18:58.285849 139975020959488 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.6597965359687805, loss=1.3830472230911255
I0213 02:19:33.052028 139975029352192 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.6947926878929138, loss=1.4149972200393677
I0213 02:20:07.809553 139975020959488 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.7138016223907471, loss=1.4373136758804321
I0213 02:20:42.582248 139975029352192 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.6783585548400879, loss=1.474845290184021
I0213 02:21:17.340469 139975020959488 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.6880592107772827, loss=1.377272129058838
I0213 02:21:52.097627 139975029352192 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.6698621511459351, loss=1.4649865627288818
I0213 02:22:26.880175 139975020959488 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.6902087926864624, loss=1.507462739944458
I0213 02:23:01.650242 139975029352192 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.7089334726333618, loss=1.4579790830612183
I0213 02:23:36.404595 139975020959488 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.6747939586639404, loss=1.4358793497085571
I0213 02:24:03.626965 140144802662208 spec.py:321] Evaluating on the training split.
I0213 02:24:06.615442 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 02:27:40.334846 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 02:27:43.042001 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 02:30:33.201076 140144802662208 spec.py:349] Evaluating on the test split.
I0213 02:30:35.896786 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 02:33:17.613149 140144802662208 submission_runner.py:408] Time since start: 65231.11s, 	Step: 113580, 	{'train/accuracy': 0.703749418258667, 'train/loss': 1.3690860271453857, 'train/bleu': 36.01579664231886, 'validation/accuracy': 0.691473126411438, 'validation/loss': 1.4214249849319458, 'validation/bleu': 30.438168648771725, 'validation/num_examples': 3000, 'test/accuracy': 0.7059555053710938, 'test/loss': 1.3358170986175537, 'test/bleu': 30.445572866631903, 'test/num_examples': 3003, 'score': 39513.24196100235, 'total_duration': 65231.111525297165, 'accumulated_submission_time': 39513.24196100235, 'accumulated_eval_time': 25712.46093392372, 'accumulated_logging_time': 1.6317753791809082}
I0213 02:33:17.643313 139975029352192 logging_writer.py:48] [113580] accumulated_eval_time=25712.460934, accumulated_logging_time=1.631775, accumulated_submission_time=39513.241961, global_step=113580, preemption_count=0, score=39513.241961, test/accuracy=0.705956, test/bleu=30.445573, test/loss=1.335817, test/num_examples=3003, total_duration=65231.111525, train/accuracy=0.703749, train/bleu=36.015797, train/loss=1.369086, validation/accuracy=0.691473, validation/bleu=30.438169, validation/loss=1.421425, validation/num_examples=3000
I0213 02:33:24.929547 139975020959488 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.6895034313201904, loss=1.4803513288497925
I0213 02:33:59.528938 139975029352192 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.7303959727287292, loss=1.4969688653945923
I0213 02:34:34.297703 139975020959488 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.7062382698059082, loss=1.4140492677688599
I0213 02:35:09.012831 139975029352192 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.689349353313446, loss=1.5040555000305176
I0213 02:35:43.757255 139975020959488 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.7606878876686096, loss=1.3892813920974731
I0213 02:36:18.501853 139975029352192 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.6905182600021362, loss=1.3601027727127075
I0213 02:36:53.253112 139975020959488 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.7216278314590454, loss=1.362742304801941
I0213 02:37:28.013932 139975029352192 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.7188833355903625, loss=1.4588817358016968
I0213 02:38:02.786326 139975020959488 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.735123336315155, loss=1.4239482879638672
I0213 02:38:37.542363 139975029352192 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.6965378522872925, loss=1.4056859016418457
I0213 02:39:12.322064 139975020959488 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.7330895662307739, loss=1.4350990056991577
I0213 02:39:47.101078 139975029352192 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.7268655896186829, loss=1.4882020950317383
I0213 02:40:21.858559 139975020959488 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.72407466173172, loss=1.3821876049041748
I0213 02:40:56.623478 139975029352192 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.6843135356903076, loss=1.4240285158157349
I0213 02:41:31.410495 139975020959488 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.693061888217926, loss=1.4164304733276367
I0213 02:42:06.206533 139975029352192 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.7226428389549255, loss=1.4095293283462524
I0213 02:42:40.963804 139975020959488 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.7117174863815308, loss=1.3843308687210083
I0213 02:43:15.707906 139975029352192 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.7197505831718445, loss=1.449032187461853
I0213 02:43:50.510555 139975020959488 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.7227246165275574, loss=1.4193217754364014
I0213 02:44:25.289764 139975029352192 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.7223901152610779, loss=1.3936251401901245
I0213 02:45:00.043889 139975020959488 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.7458831071853638, loss=1.4032105207443237
I0213 02:45:34.806442 139975029352192 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.7072479128837585, loss=1.4335224628448486
I0213 02:46:09.559279 139975020959488 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.6940585970878601, loss=1.3771634101867676
I0213 02:46:44.324631 139975029352192 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.7406171560287476, loss=1.3731752634048462
I0213 02:47:17.757702 140144802662208 spec.py:321] Evaluating on the training split.
I0213 02:47:20.730951 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 02:51:22.038695 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 02:51:24.714851 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 02:54:35.275558 140144802662208 spec.py:349] Evaluating on the test split.
I0213 02:54:37.982282 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 02:57:23.341994 140144802662208 submission_runner.py:408] Time since start: 66676.84s, 	Step: 115998, 	{'train/accuracy': 0.6970692873001099, 'train/loss': 1.398681640625, 'train/bleu': 35.804610108981215, 'validation/accuracy': 0.691386342048645, 'validation/loss': 1.418184757232666, 'validation/bleu': 30.566812091793675, 'validation/num_examples': 3000, 'test/accuracy': 0.7085468769073486, 'test/loss': 1.3254882097244263, 'test/bleu': 30.682219921723906, 'test/num_examples': 3003, 'score': 40353.26636815071, 'total_duration': 66676.8403544426, 'accumulated_submission_time': 40353.26636815071, 'accumulated_eval_time': 26318.045152664185, 'accumulated_logging_time': 1.672149419784546}
I0213 02:57:23.373500 139975020959488 logging_writer.py:48] [115998] accumulated_eval_time=26318.045153, accumulated_logging_time=1.672149, accumulated_submission_time=40353.266368, global_step=115998, preemption_count=0, score=40353.266368, test/accuracy=0.708547, test/bleu=30.682220, test/loss=1.325488, test/num_examples=3003, total_duration=66676.840354, train/accuracy=0.697069, train/bleu=35.804610, train/loss=1.398682, validation/accuracy=0.691386, validation/bleu=30.566812, validation/loss=1.418185, validation/num_examples=3000
I0213 02:57:24.441239 139975029352192 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.709679901599884, loss=1.4509496688842773
I0213 02:57:59.043406 139975020959488 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.7081537246704102, loss=1.359898567199707
I0213 02:58:33.740288 139975029352192 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.7300078272819519, loss=1.401255488395691
I0213 02:59:08.498374 139975020959488 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.7188859581947327, loss=1.3565655946731567
I0213 02:59:43.308095 139975029352192 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.7324027419090271, loss=1.3715336322784424
I0213 03:00:18.081462 139975020959488 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.7218706607818604, loss=1.4693408012390137
I0213 03:00:52.861433 139975029352192 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.7712997794151306, loss=1.4490894079208374
I0213 03:01:27.629652 139975020959488 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.7417945861816406, loss=1.381894826889038
I0213 03:02:02.370421 139975029352192 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.740013062953949, loss=1.4745988845825195
I0213 03:02:37.124434 139975020959488 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.7418634295463562, loss=1.458449125289917
I0213 03:03:11.883998 139975029352192 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.7448674440383911, loss=1.4578810930252075
I0213 03:03:46.675251 139975020959488 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.7445728182792664, loss=1.3762797117233276
I0213 03:04:21.492518 139975029352192 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.7540475726127625, loss=1.433368444442749
I0213 03:04:56.284070 139975020959488 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.7284660339355469, loss=1.3441417217254639
I0213 03:05:31.098611 139975029352192 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.7355677485466003, loss=1.4903700351715088
I0213 03:06:05.882432 139975020959488 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.7359992861747742, loss=1.4384554624557495
I0213 03:06:40.650332 139975029352192 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.7277457118034363, loss=1.367089867591858
I0213 03:07:15.465201 139975020959488 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.7575466632843018, loss=1.5084844827651978
I0213 03:07:50.255581 139975029352192 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.7795197367668152, loss=1.4338324069976807
I0213 03:08:25.034864 139975020959488 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.7422415614128113, loss=1.3601255416870117
I0213 03:08:59.798541 139975029352192 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.7443810105323792, loss=1.3420993089675903
I0213 03:09:34.550574 139975020959488 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.7514850497245789, loss=1.3524292707443237
I0213 03:10:09.309872 139975029352192 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.7608524560928345, loss=1.3960918188095093
I0213 03:10:44.073149 139975020959488 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.7239601612091064, loss=1.3998373746871948
I0213 03:11:18.820712 139975029352192 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.750248372554779, loss=1.391135334968567
I0213 03:11:23.420166 140144802662208 spec.py:321] Evaluating on the training split.
I0213 03:11:26.398200 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 03:15:22.284846 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 03:15:24.972328 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 03:18:13.883646 140144802662208 spec.py:349] Evaluating on the test split.
I0213 03:18:16.591819 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 03:20:55.676306 140144802662208 submission_runner.py:408] Time since start: 68089.17s, 	Step: 118415, 	{'train/accuracy': 0.6983740925788879, 'train/loss': 1.3953670263290405, 'train/bleu': 35.78991389340325, 'validation/accuracy': 0.6922046542167664, 'validation/loss': 1.4161663055419922, 'validation/bleu': 30.73623908706971, 'validation/num_examples': 3000, 'test/accuracy': 0.7084422707557678, 'test/loss': 1.3272607326507568, 'test/bleu': 30.917426686390197, 'test/num_examples': 3003, 'score': 41193.21762943268, 'total_duration': 68089.17468738556, 'accumulated_submission_time': 41193.21762943268, 'accumulated_eval_time': 26890.301241874695, 'accumulated_logging_time': 1.715043306350708}
I0213 03:20:55.707963 139975020959488 logging_writer.py:48] [118415] accumulated_eval_time=26890.301242, accumulated_logging_time=1.715043, accumulated_submission_time=41193.217629, global_step=118415, preemption_count=0, score=41193.217629, test/accuracy=0.708442, test/bleu=30.917427, test/loss=1.327261, test/num_examples=3003, total_duration=68089.174687, train/accuracy=0.698374, train/bleu=35.789914, train/loss=1.395367, validation/accuracy=0.692205, validation/bleu=30.736239, validation/loss=1.416166, validation/num_examples=3000
I0213 03:21:25.581895 139975029352192 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.7585025429725647, loss=1.402267575263977
I0213 03:22:00.245883 139975020959488 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.7466006278991699, loss=1.4295716285705566
I0213 03:22:34.982135 139975029352192 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.771793007850647, loss=1.4667701721191406
I0213 03:23:09.746508 139975020959488 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.7357999086380005, loss=1.3821406364440918
I0213 03:23:44.511092 139975029352192 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.7581028938293457, loss=1.3997175693511963
I0213 03:24:19.291612 139975020959488 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.7521147727966309, loss=1.382603645324707
I0213 03:24:54.079097 139975029352192 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.8363485336303711, loss=1.3646042346954346
I0213 03:25:28.862427 139975020959488 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.7591844201087952, loss=1.332980990409851
I0213 03:26:03.661099 139975029352192 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.7558358311653137, loss=1.3493828773498535
I0213 03:26:38.404513 139975020959488 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.7334418892860413, loss=1.3595306873321533
I0213 03:27:13.149742 139975029352192 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.7680086493492126, loss=1.3440097570419312
I0213 03:27:47.873887 139975020959488 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.7754285931587219, loss=1.3472449779510498
I0213 03:28:22.634620 139975029352192 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.7890077829360962, loss=1.4195730686187744
I0213 03:28:57.395747 139975020959488 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.76664137840271, loss=1.3780782222747803
I0213 03:29:32.164938 139975029352192 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.775930643081665, loss=1.3532508611679077
I0213 03:30:06.923705 139975020959488 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.7489684820175171, loss=1.3706855773925781
I0213 03:30:41.659373 139975029352192 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.7636985778808594, loss=1.449944257736206
I0213 03:31:16.418017 139975020959488 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.7745962738990784, loss=1.4045116901397705
I0213 03:31:51.180386 139975029352192 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.7815620303153992, loss=1.3673803806304932
I0213 03:32:25.941520 139975020959488 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.8125041723251343, loss=1.4424339532852173
I0213 03:33:00.690839 139975029352192 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.7835555076599121, loss=1.3616971969604492
I0213 03:33:35.453300 139975020959488 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.7570178508758545, loss=1.4144092798233032
I0213 03:34:10.218492 139975029352192 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.7590363621711731, loss=1.3312065601348877
I0213 03:34:44.999981 139975020959488 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.7439197897911072, loss=1.3348783254623413
I0213 03:34:55.846136 140144802662208 spec.py:321] Evaluating on the training split.
I0213 03:34:58.828438 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 03:38:35.048722 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 03:38:37.724121 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 03:41:19.175066 140144802662208 spec.py:349] Evaluating on the test split.
I0213 03:41:21.883869 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 03:43:56.301391 140144802662208 submission_runner.py:408] Time since start: 69469.80s, 	Step: 120833, 	{'train/accuracy': 0.7079231142997742, 'train/loss': 1.3452441692352295, 'train/bleu': 36.54008424706086, 'validation/accuracy': 0.69246506690979, 'validation/loss': 1.4150506258010864, 'validation/bleu': 30.88375035702032, 'validation/num_examples': 3000, 'test/accuracy': 0.7084422707557678, 'test/loss': 1.324387550354004, 'test/bleu': 30.767148916451248, 'test/num_examples': 3003, 'score': 42033.183654785156, 'total_duration': 69469.79971814156, 'accumulated_submission_time': 42033.183654785156, 'accumulated_eval_time': 27430.756391763687, 'accumulated_logging_time': 1.8395373821258545}
I0213 03:43:56.341938 139975029352192 logging_writer.py:48] [120833] accumulated_eval_time=27430.756392, accumulated_logging_time=1.839537, accumulated_submission_time=42033.183655, global_step=120833, preemption_count=0, score=42033.183655, test/accuracy=0.708442, test/bleu=30.767149, test/loss=1.324388, test/num_examples=3003, total_duration=69469.799718, train/accuracy=0.707923, train/bleu=36.540084, train/loss=1.345244, validation/accuracy=0.692465, validation/bleu=30.883750, validation/loss=1.415051, validation/num_examples=3000
I0213 03:44:19.902626 139975020959488 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.7922665476799011, loss=1.3699395656585693
I0213 03:44:54.561523 139975029352192 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.7769002914428711, loss=1.3642970323562622
I0213 03:45:29.298320 139975020959488 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.7716290354728699, loss=1.3625938892364502
I0213 03:46:04.056985 139975029352192 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.7857236266136169, loss=1.3898457288742065
I0213 03:46:38.807963 139975020959488 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.7597403526306152, loss=1.4052598476409912
I0213 03:47:13.572097 139975029352192 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.8089362978935242, loss=1.4016797542572021
I0213 03:47:48.388023 139975020959488 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.7938926219940186, loss=1.399659276008606
I0213 03:48:23.135268 139975029352192 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.7602003812789917, loss=1.3931835889816284
I0213 03:48:57.908825 139975020959488 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.7877431511878967, loss=1.3549706935882568
I0213 03:49:32.664120 139975029352192 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.7904765009880066, loss=1.4478498697280884
I0213 03:50:07.427085 139975020959488 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.7665984630584717, loss=1.428993582725525
I0213 03:50:42.166866 139975029352192 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.7854304909706116, loss=1.3740699291229248
I0213 03:51:16.946349 139975020959488 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.772514820098877, loss=1.353481650352478
I0213 03:51:51.752407 139975029352192 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.7821942567825317, loss=1.456831693649292
I0213 03:52:26.529660 139975020959488 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.8023245930671692, loss=1.3786622285842896
I0213 03:53:01.311252 139975029352192 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.784522294998169, loss=1.384919285774231
I0213 03:53:36.063719 139975020959488 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.8104151487350464, loss=1.3618124723434448
I0213 03:54:10.823798 139975029352192 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.8117438554763794, loss=1.3835116624832153
I0213 03:54:45.599478 139975020959488 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.8031460046768188, loss=1.4323636293411255
I0213 03:55:20.361672 139975029352192 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.7910184860229492, loss=1.3517658710479736
I0213 03:55:55.093949 139975020959488 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.8118909001350403, loss=1.3239867687225342
I0213 03:56:29.873699 139975029352192 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7788463234901428, loss=1.3933534622192383
I0213 03:57:04.754413 139975020959488 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.7922044992446899, loss=1.38174569606781
I0213 03:57:39.514241 139975029352192 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.8148977756500244, loss=1.3238683938980103
I0213 03:57:56.633890 140144802662208 spec.py:321] Evaluating on the training split.
I0213 03:57:59.633386 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 04:01:36.655886 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 04:01:39.371325 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 04:04:12.558183 140144802662208 spec.py:349] Evaluating on the test split.
I0213 04:04:15.264958 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 04:06:48.877741 140144802662208 submission_runner.py:408] Time since start: 70842.38s, 	Step: 123251, 	{'train/accuracy': 0.7070516347885132, 'train/loss': 1.3467681407928467, 'train/bleu': 36.243165574604014, 'validation/accuracy': 0.6931470036506653, 'validation/loss': 1.4134272336959839, 'validation/bleu': 30.778888404449745, 'validation/num_examples': 3000, 'test/accuracy': 0.7096391916275024, 'test/loss': 1.320741891860962, 'test/bleu': 30.748641590683718, 'test/num_examples': 3003, 'score': 42873.383053064346, 'total_duration': 70842.37611746788, 'accumulated_submission_time': 42873.383053064346, 'accumulated_eval_time': 27963.000198602676, 'accumulated_logging_time': 1.8919637203216553}
I0213 04:06:48.909929 139975020959488 logging_writer.py:48] [123251] accumulated_eval_time=27963.000199, accumulated_logging_time=1.891964, accumulated_submission_time=42873.383053, global_step=123251, preemption_count=0, score=42873.383053, test/accuracy=0.709639, test/bleu=30.748642, test/loss=1.320742, test/num_examples=3003, total_duration=70842.376117, train/accuracy=0.707052, train/bleu=36.243166, train/loss=1.346768, validation/accuracy=0.693147, validation/bleu=30.778888, validation/loss=1.413427, validation/num_examples=3000
I0213 04:07:06.216072 139975029352192 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.7862017750740051, loss=1.3613574504852295
I0213 04:07:40.872839 139975020959488 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.7800718545913696, loss=1.419183373451233
I0213 04:08:15.586607 139975029352192 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.8311448693275452, loss=1.4015867710113525
I0213 04:08:50.367392 139975020959488 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.7790446281433105, loss=1.413881778717041
I0213 04:09:25.140358 139975029352192 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.7797846794128418, loss=1.396077275276184
I0213 04:09:59.895322 139975020959488 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.776310920715332, loss=1.3211138248443604
I0213 04:10:34.701990 139975029352192 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.7892749905586243, loss=1.3883695602416992
I0213 04:11:09.520756 139975020959488 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.818437933921814, loss=1.3734997510910034
I0213 04:11:44.313176 139975029352192 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.8095139861106873, loss=1.3910549879074097
I0213 04:12:19.181639 139975020959488 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.7985261082649231, loss=1.3544301986694336
I0213 04:12:53.915846 139975029352192 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.791986882686615, loss=1.3758563995361328
I0213 04:13:28.681241 139975020959488 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.7977073192596436, loss=1.3398510217666626
I0213 04:14:03.424650 139975029352192 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.7881912589073181, loss=1.3413680791854858
I0213 04:14:38.192592 139975020959488 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.811652660369873, loss=1.4060182571411133
I0213 04:15:12.973037 139975029352192 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.7918804287910461, loss=1.363173007965088
I0213 04:15:47.734257 139975020959488 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.8421834111213684, loss=1.345054268836975
I0213 04:16:22.533844 139975029352192 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.8353496193885803, loss=1.3726294040679932
I0213 04:16:57.296309 139975020959488 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.8140232563018799, loss=1.4176092147827148
I0213 04:17:32.071516 139975029352192 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.7834896445274353, loss=1.401132345199585
I0213 04:18:06.876380 139975020959488 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.8122943043708801, loss=1.3406010866165161
I0213 04:18:41.642956 139975029352192 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.7997777462005615, loss=1.2964924573898315
I0213 04:19:16.415411 139975020959488 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.8104530572891235, loss=1.3927671909332275
I0213 04:19:51.159132 139975029352192 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.7962995767593384, loss=1.3426884412765503
I0213 04:20:25.896057 139975020959488 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.8137235641479492, loss=1.4036352634429932
I0213 04:20:48.911442 140144802662208 spec.py:321] Evaluating on the training split.
I0213 04:20:51.889691 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 04:24:25.898121 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 04:24:28.589154 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 04:27:24.650094 140144802662208 spec.py:349] Evaluating on the test split.
I0213 04:27:27.335954 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 04:30:34.341830 140144802662208 submission_runner.py:408] Time since start: 72267.84s, 	Step: 125668, 	{'train/accuracy': 0.710022509098053, 'train/loss': 1.332497239112854, 'train/bleu': 36.40957170008483, 'validation/accuracy': 0.6937421560287476, 'validation/loss': 1.4104636907577515, 'validation/bleu': 30.8876838523228, 'validation/num_examples': 3000, 'test/accuracy': 0.709337055683136, 'test/loss': 1.3211370706558228, 'test/bleu': 30.857238354469466, 'test/num_examples': 3003, 'score': 43713.294801950455, 'total_duration': 72267.84021043777, 'accumulated_submission_time': 43713.294801950455, 'accumulated_eval_time': 28548.430537700653, 'accumulated_logging_time': 1.933983564376831}
I0213 04:30:34.374670 139975029352192 logging_writer.py:48] [125668] accumulated_eval_time=28548.430538, accumulated_logging_time=1.933984, accumulated_submission_time=43713.294802, global_step=125668, preemption_count=0, score=43713.294802, test/accuracy=0.709337, test/bleu=30.857238, test/loss=1.321137, test/num_examples=3003, total_duration=72267.840210, train/accuracy=0.710023, train/bleu=36.409572, train/loss=1.332497, validation/accuracy=0.693742, validation/bleu=30.887684, validation/loss=1.410464, validation/num_examples=3000
I0213 04:30:45.816859 139975020959488 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.8128581643104553, loss=1.3680590391159058
I0213 04:31:20.473824 139975029352192 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.7991281151771545, loss=1.3741785287857056
I0213 04:31:55.214733 139975020959488 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.8181254863739014, loss=1.327115774154663
I0213 04:32:29.988954 139975029352192 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.7847875952720642, loss=1.3446452617645264
I0213 04:33:04.778860 139975020959488 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.7791725397109985, loss=1.3853306770324707
I0213 04:33:39.622570 139975029352192 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.7931397557258606, loss=1.3532682657241821
I0213 04:34:14.395177 139975020959488 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.7832493782043457, loss=1.3198158740997314
I0213 04:34:49.146903 139975029352192 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.7820371389389038, loss=1.325688362121582
I0213 04:35:23.892007 139975020959488 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.8029128313064575, loss=1.4038339853286743
I0213 04:35:58.641235 139975029352192 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.8110460042953491, loss=1.3823140859603882
I0213 04:36:33.389148 139975020959488 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.7709988951683044, loss=1.308298945426941
I0213 04:37:08.132436 139975029352192 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.8012927770614624, loss=1.3381108045578003
I0213 04:37:42.880394 139975020959488 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.7880598306655884, loss=1.3231827020645142
I0213 04:38:17.639677 139975029352192 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.7843977212905884, loss=1.3334910869598389
I0213 04:38:52.397948 139975020959488 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.8095584511756897, loss=1.3432601690292358
I0213 04:39:27.166321 139975029352192 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.7807256579399109, loss=1.3526606559753418
I0213 04:40:01.933073 139975020959488 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.8226630687713623, loss=1.3639354705810547
I0213 04:40:36.699572 139975029352192 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.7782201766967773, loss=1.3459399938583374
I0213 04:41:11.468814 139975020959488 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.8117186427116394, loss=1.3724955320358276
I0213 04:41:46.230496 139975029352192 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.8075363636016846, loss=1.3163037300109863
I0213 04:42:20.995471 139975020959488 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.819936990737915, loss=1.3909034729003906
I0213 04:42:55.768514 139975029352192 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.8017899394035339, loss=1.329092025756836
I0213 04:43:30.539161 139975020959488 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.7972310781478882, loss=1.3522953987121582
I0213 04:44:05.292178 139975029352192 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.8272135257720947, loss=1.338606595993042
I0213 04:44:34.544763 140144802662208 spec.py:321] Evaluating on the training split.
I0213 04:44:37.522281 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 04:48:11.402168 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 04:48:14.091959 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 04:50:54.606053 140144802662208 spec.py:349] Evaluating on the test split.
I0213 04:50:57.317543 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 04:53:41.565875 140144802662208 submission_runner.py:408] Time since start: 73655.06s, 	Step: 128086, 	{'train/accuracy': 0.7109796404838562, 'train/loss': 1.329494595527649, 'train/bleu': 36.99412188193442, 'validation/accuracy': 0.6932337880134583, 'validation/loss': 1.4097765684127808, 'validation/bleu': 30.99008929284226, 'validation/num_examples': 3000, 'test/accuracy': 0.7093021869659424, 'test/loss': 1.319244623184204, 'test/bleu': 30.86260403130315, 'test/num_examples': 3003, 'score': 44553.37302994728, 'total_duration': 73655.06421208382, 'accumulated_submission_time': 44553.37302994728, 'accumulated_eval_time': 29095.45155930519, 'accumulated_logging_time': 1.9780395030975342}
I0213 04:53:41.607865 139975020959488 logging_writer.py:48] [128086] accumulated_eval_time=29095.451559, accumulated_logging_time=1.978040, accumulated_submission_time=44553.373030, global_step=128086, preemption_count=0, score=44553.373030, test/accuracy=0.709302, test/bleu=30.862604, test/loss=1.319245, test/num_examples=3003, total_duration=73655.064212, train/accuracy=0.710980, train/bleu=36.994122, train/loss=1.329495, validation/accuracy=0.693234, validation/bleu=30.990089, validation/loss=1.409777, validation/num_examples=3000
I0213 04:53:46.815017 139975029352192 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.8074248433113098, loss=1.3363046646118164
I0213 04:54:21.432161 139975020959488 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.7661229968070984, loss=1.3370697498321533
I0213 04:54:56.106839 139975029352192 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.808857262134552, loss=1.3559497594833374
I0213 04:55:30.868525 139975020959488 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.7938884496688843, loss=1.3168646097183228
I0213 04:56:05.594782 139975029352192 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.8052674531936646, loss=1.3486981391906738
I0213 04:56:40.344576 139975020959488 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.7814696431159973, loss=1.2938792705535889
I0213 04:57:15.088986 139975029352192 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.7969872355461121, loss=1.3021568059921265
I0213 04:57:49.830548 139975020959488 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.8063005805015564, loss=1.3916999101638794
I0213 04:58:24.590557 139975029352192 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.7963212728500366, loss=1.3276976346969604
I0213 04:58:59.344424 139975020959488 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.8031867742538452, loss=1.3578256368637085
I0213 04:59:34.101102 139975029352192 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.7837391495704651, loss=1.350124716758728
I0213 05:00:08.859665 139975020959488 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.8195488452911377, loss=1.3920849561691284
I0213 05:00:43.596217 139975029352192 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.7957574725151062, loss=1.3805084228515625
I0213 05:01:18.332472 139975020959488 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.8123946785926819, loss=1.345639944076538
I0213 05:01:53.069912 139975029352192 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.7713550925254822, loss=1.2930347919464111
I0213 05:02:27.832896 139975020959488 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.7949119210243225, loss=1.3641566038131714
I0213 05:03:02.609556 139975029352192 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.8584166169166565, loss=1.4525083303451538
I0213 05:03:37.361985 139975020959488 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.8112217783927917, loss=1.3496023416519165
I0213 05:04:12.147388 139975029352192 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.7868661284446716, loss=1.2909557819366455
I0213 05:04:46.929646 139975020959488 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.8012444376945496, loss=1.3705860376358032
I0213 05:05:21.689303 139975029352192 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.7903707027435303, loss=1.3353607654571533
I0213 05:05:56.454154 139975020959488 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.8199021220207214, loss=1.3590909242630005
I0213 05:06:31.197137 139975029352192 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.8273687958717346, loss=1.358899474143982
I0213 05:07:05.949611 139975020959488 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.803310751914978, loss=1.352763295173645
I0213 05:07:40.716047 139975029352192 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.7905339002609253, loss=1.3451476097106934
I0213 05:07:41.839745 140144802662208 spec.py:321] Evaluating on the training split.
I0213 05:07:44.820235 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:11:18.240304 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 05:11:20.922856 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:13:54.591609 140144802662208 spec.py:349] Evaluating on the test split.
I0213 05:13:57.289307 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:16:49.905649 140144802662208 submission_runner.py:408] Time since start: 75043.40s, 	Step: 130505, 	{'train/accuracy': 0.7105883359909058, 'train/loss': 1.3296940326690674, 'train/bleu': 36.643996629084846, 'validation/accuracy': 0.6933826208114624, 'validation/loss': 1.4100453853607178, 'validation/bleu': 30.929830153579662, 'validation/num_examples': 3000, 'test/accuracy': 0.710220217704773, 'test/loss': 1.3187133073806763, 'test/bleu': 30.82152023947141, 'test/num_examples': 3003, 'score': 45393.514297008514, 'total_duration': 75043.40402579308, 'accumulated_submission_time': 45393.514297008514, 'accumulated_eval_time': 29643.51740694046, 'accumulated_logging_time': 2.0315675735473633}
I0213 05:16:49.939264 139975020959488 logging_writer.py:48] [130505] accumulated_eval_time=29643.517407, accumulated_logging_time=2.031568, accumulated_submission_time=45393.514297, global_step=130505, preemption_count=0, score=45393.514297, test/accuracy=0.710220, test/bleu=30.821520, test/loss=1.318713, test/num_examples=3003, total_duration=75043.404026, train/accuracy=0.710588, train/bleu=36.643997, train/loss=1.329694, validation/accuracy=0.693383, validation/bleu=30.929830, validation/loss=1.410045, validation/num_examples=3000
I0213 05:17:23.154876 139975029352192 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.8107553720474243, loss=1.3578916788101196
I0213 05:17:57.821424 139975020959488 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.8136261105537415, loss=1.3567482233047485
I0213 05:18:32.544971 139975029352192 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.7873310446739197, loss=1.400073766708374
I0213 05:19:07.308857 139975020959488 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.7948977947235107, loss=1.3093847036361694
I0213 05:19:42.035468 139975029352192 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.8256925940513611, loss=1.3451405763626099
I0213 05:20:16.780126 139975020959488 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.7935686111450195, loss=1.3408994674682617
I0213 05:20:51.533263 139975029352192 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.8242329359054565, loss=1.266433835029602
I0213 05:21:26.275389 139975020959488 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.8022050261497498, loss=1.3435001373291016
I0213 05:22:01.043470 139975029352192 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.7910594940185547, loss=1.3050546646118164
I0213 05:22:35.809597 139975020959488 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.792261004447937, loss=1.3300752639770508
I0213 05:23:10.580101 139975029352192 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.8208563327789307, loss=1.4093927145004272
I0213 05:23:45.383441 139975020959488 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.810613214969635, loss=1.3925766944885254
I0213 05:24:20.173908 139975029352192 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.8376151919364929, loss=1.4033138751983643
I0213 05:24:54.948263 139975020959488 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.8035144805908203, loss=1.3022918701171875
I0213 05:25:29.695344 139975029352192 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.797636091709137, loss=1.3186734914779663
I0213 05:26:04.435561 139975020959488 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.8110724687576294, loss=1.3490029573440552
I0213 05:26:39.248028 139975029352192 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.7944236397743225, loss=1.3594735860824585
I0213 05:27:14.042076 139975020959488 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.8028503656387329, loss=1.4264808893203735
I0213 05:27:48.781055 139975029352192 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.790117621421814, loss=1.325565218925476
I0213 05:28:23.541581 139975020959488 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.8170909285545349, loss=1.3323230743408203
I0213 05:28:58.287871 139975029352192 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.7968601584434509, loss=1.405239462852478
I0213 05:29:33.039976 139975020959488 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.8013104200363159, loss=1.2982033491134644
I0213 05:30:07.779032 139975029352192 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.816532552242279, loss=1.2538137435913086
I0213 05:30:42.531082 139975020959488 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.8285872936248779, loss=1.3628512620925903
I0213 05:30:50.243788 140144802662208 spec.py:321] Evaluating on the training split.
I0213 05:30:53.214471 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:34:32.876846 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 05:34:35.555990 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:37:15.375523 140144802662208 spec.py:349] Evaluating on the test split.
I0213 05:37:18.060418 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:40:10.299451 140144802662208 submission_runner.py:408] Time since start: 76443.80s, 	Step: 132924, 	{'train/accuracy': 0.7117727398872375, 'train/loss': 1.324880599975586, 'train/bleu': 36.79409669434138, 'validation/accuracy': 0.6934942007064819, 'validation/loss': 1.4099982976913452, 'validation/bleu': 30.80455651715488, 'validation/num_examples': 3000, 'test/accuracy': 0.7099761962890625, 'test/loss': 1.3190022706985474, 'test/bleu': 30.837535923196857, 'test/num_examples': 3003, 'score': 46233.72875595093, 'total_duration': 76443.79782891273, 'accumulated_submission_time': 46233.72875595093, 'accumulated_eval_time': 30203.573014974594, 'accumulated_logging_time': 2.0750794410705566}
I0213 05:40:10.334566 139975029352192 logging_writer.py:48] [132924] accumulated_eval_time=30203.573015, accumulated_logging_time=2.075079, accumulated_submission_time=46233.728756, global_step=132924, preemption_count=0, score=46233.728756, test/accuracy=0.709976, test/bleu=30.837536, test/loss=1.319002, test/num_examples=3003, total_duration=76443.797829, train/accuracy=0.711773, train/bleu=36.794097, train/loss=1.324881, validation/accuracy=0.693494, validation/bleu=30.804557, validation/loss=1.409998, validation/num_examples=3000
I0213 05:40:36.978884 139975020959488 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.8325353264808655, loss=1.3757930994033813
I0213 05:41:11.658443 139975029352192 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.7910703420639038, loss=1.3590623140335083
I0213 05:41:46.394819 139975020959488 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.8083190321922302, loss=1.4134985208511353
I0213 05:42:21.111913 139975029352192 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.788945198059082, loss=1.3556071519851685
I0213 05:42:31.962321 140144802662208 spec.py:321] Evaluating on the training split.
I0213 05:42:34.941232 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:46:04.977127 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 05:46:07.659689 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:48:45.815926 140144802662208 spec.py:349] Evaluating on the test split.
I0213 05:48:48.498073 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:51:40.256402 140144802662208 submission_runner.py:408] Time since start: 77133.75s, 	Step: 133333, 	{'train/accuracy': 0.7115277051925659, 'train/loss': 1.324217438697815, 'train/bleu': 36.53925289250019, 'validation/accuracy': 0.6935189962387085, 'validation/loss': 1.40999174118042, 'validation/bleu': 30.825316571983876, 'validation/num_examples': 3000, 'test/accuracy': 0.7099761962890625, 'test/loss': 1.318996548652649, 'test/bleu': 30.846443803364846, 'test/num_examples': 3003, 'score': 46375.332560777664, 'total_duration': 77133.75478172302, 'accumulated_submission_time': 46375.332560777664, 'accumulated_eval_time': 30751.867042303085, 'accumulated_logging_time': 2.1205337047576904}
I0213 05:51:40.290755 139975020959488 logging_writer.py:48] [133333] accumulated_eval_time=30751.867042, accumulated_logging_time=2.120534, accumulated_submission_time=46375.332561, global_step=133333, preemption_count=0, score=46375.332561, test/accuracy=0.709976, test/bleu=30.846444, test/loss=1.318997, test/num_examples=3003, total_duration=77133.754782, train/accuracy=0.711528, train/bleu=36.539253, train/loss=1.324217, validation/accuracy=0.693519, validation/bleu=30.825317, validation/loss=1.409992, validation/num_examples=3000
I0213 05:51:40.324180 139975029352192 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46375.332561
I0213 05:51:41.545184 140144802662208 checkpoints.py:490] Saving checkpoint at step: 133333
I0213 05:51:45.603060 140144802662208 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_3/checkpoint_133333
I0213 05:51:45.608530 140144802662208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_3/checkpoint_133333.
I0213 05:51:45.670642 140144802662208 submission_runner.py:583] Tuning trial 3/5
I0213 05:51:45.670878 140144802662208 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0213 05:51:45.680906 140144802662208 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006362841231748462, 'train/loss': 10.959882736206055, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.05414128303528, 'total_duration': 889.9320287704468, 'accumulated_submission_time': 30.05414128303528, 'accumulated_eval_time': 859.8778374195099, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2413, {'train/accuracy': 0.4128057062625885, 'train/loss': 3.9366543292999268, 'train/bleu': 14.089845752800661, 'validation/accuracy': 0.3986683189868927, 'validation/loss': 4.049123287200928, 'validation/bleu': 9.633888628769737, 'validation/num_examples': 3000, 'test/accuracy': 0.3826157748699188, 'test/loss': 4.261987686157227, 'test/bleu': 8.030870962674728, 'test/num_examples': 3003, 'score': 870.1676576137543, 'total_duration': 2329.528389930725, 'accumulated_submission_time': 870.1676576137543, 'accumulated_eval_time': 1459.2620244026184, 'accumulated_logging_time': 0.019870281219482422, 'global_step': 2413, 'preemption_count': 0}), (4824, {'train/accuracy': 0.5432161092758179, 'train/loss': 2.668916940689087, 'train/bleu': 24.829318029776022, 'validation/accuracy': 0.5427582859992981, 'validation/loss': 2.6340794563293457, 'validation/bleu': 20.365649432861616, 'validation/num_examples': 3000, 'test/accuracy': 0.5445354580879211, 'test/loss': 2.6518774032592773, 'test/bleu': 19.00498182839845, 'test/num_examples': 3003, 'score': 1710.180258989334, 'total_duration': 3634.642117023468, 'accumulated_submission_time': 1710.180258989334, 'accumulated_eval_time': 1924.2522518634796, 'accumulated_logging_time': 0.05057692527770996, 'global_step': 4824, 'preemption_count': 0}), (7235, {'train/accuracy': 0.581144392490387, 'train/loss': 2.2662315368652344, 'train/bleu': 27.389879639296826, 'validation/accuracy': 0.5851012468338013, 'validation/loss': 2.2258543968200684, 'validation/bleu': 23.189117839268576, 'validation/num_examples': 3000, 'test/accuracy': 0.5854976773262024, 'test/loss': 2.21711802482605, 'test/bleu': 21.73489214068676, 'test/num_examples': 3003, 'score': 2550.3885049819946, 'total_duration': 4945.470089673996, 'accumulated_submission_time': 2550.3885049819946, 'accumulated_eval_time': 2394.7639780044556, 'accumulated_logging_time': 0.07668113708496094, 'global_step': 7235, 'preemption_count': 0}), (9648, {'train/accuracy': 0.592995285987854, 'train/loss': 2.1528778076171875, 'train/bleu': 28.409135222092637, 'validation/accuracy': 0.6077048182487488, 'validation/loss': 2.034355640411377, 'validation/bleu': 24.757049137021426, 'validation/num_examples': 3000, 'test/accuracy': 0.6137005686759949, 'test/loss': 1.992855429649353, 'test/bleu': 23.58660872714324, 'test/num_examples': 3003, 'score': 3390.4426593780518, 'total_duration': 6223.30933713913, 'accumulated_submission_time': 3390.4426593780518, 'accumulated_eval_time': 2832.442953109741, 'accumulated_logging_time': 0.10341739654541016, 'global_step': 9648, 'preemption_count': 0}), (12063, {'train/accuracy': 0.6006006598472595, 'train/loss': 2.061574697494507, 'train/bleu': 28.907384311822263, 'validation/accuracy': 0.6192111372947693, 'validation/loss': 1.9119383096694946, 'validation/bleu': 25.726427060385753, 'validation/num_examples': 3000, 'test/accuracy': 0.6264017224311829, 'test/loss': 1.8653494119644165, 'test/bleu': 24.72895569223176, 'test/num_examples': 3003, 'score': 4230.618235588074, 'total_duration': 7560.686071395874, 'accumulated_submission_time': 4230.618235588074, 'accumulated_eval_time': 3329.5379779338837, 'accumulated_logging_time': 0.13108301162719727, 'global_step': 12063, 'preemption_count': 0}), (14479, {'train/accuracy': 0.6118413209915161, 'train/loss': 1.9555613994598389, 'train/bleu': 29.58988159437644, 'validation/accuracy': 0.6300107836723328, 'validation/loss': 1.8212212324142456, 'validation/bleu': 26.352692375606615, 'validation/num_examples': 3000, 'test/accuracy': 0.6380454301834106, 'test/loss': 1.7743580341339111, 'test/bleu': 25.639332507671263, 'test/num_examples': 3003, 'score': 5070.55059671402, 'total_duration': 8894.039239883423, 'accumulated_submission_time': 5070.55059671402, 'accumulated_eval_time': 3822.8482298851013, 'accumulated_logging_time': 0.16233038902282715, 'global_step': 14479, 'preemption_count': 0}), (16895, {'train/accuracy': 0.6211254596710205, 'train/loss': 1.907320499420166, 'train/bleu': 29.799160596214755, 'validation/accuracy': 0.6405500173568726, 'validation/loss': 1.7542517185211182, 'validation/bleu': 26.869630513464116, 'validation/num_examples': 3000, 'test/accuracy': 0.6462843418121338, 'test/loss': 1.706995964050293, 'test/bleu': 26.05308990904883, 'test/num_examples': 3003, 'score': 5910.659248828888, 'total_duration': 10218.438910245895, 'accumulated_submission_time': 5910.659248828888, 'accumulated_eval_time': 4307.02596282959, 'accumulated_logging_time': 0.19454002380371094, 'global_step': 16895, 'preemption_count': 0}), (19311, {'train/accuracy': 0.6253435015678406, 'train/loss': 1.8560582399368286, 'train/bleu': 30.119357369554518, 'validation/accuracy': 0.6459808349609375, 'validation/loss': 1.716254472732544, 'validation/bleu': 27.16554742317671, 'validation/num_examples': 3000, 'test/accuracy': 0.6546743512153625, 'test/loss': 1.6618460416793823, 'test/bleu': 26.37794400092443, 'test/num_examples': 3003, 'score': 6750.589684724808, 'total_duration': 11562.049136638641, 'accumulated_submission_time': 6750.589684724808, 'accumulated_eval_time': 4810.599547386169, 'accumulated_logging_time': 0.22215533256530762, 'global_step': 19311, 'preemption_count': 0}), (21727, {'train/accuracy': 0.625741720199585, 'train/loss': 1.8575769662857056, 'train/bleu': 30.296728445860584, 'validation/accuracy': 0.6483986377716064, 'validation/loss': 1.690137267112732, 'validation/bleu': 27.708869311230746, 'validation/num_examples': 3000, 'test/accuracy': 0.6591482162475586, 'test/loss': 1.6333097219467163, 'test/bleu': 27.06400431392655, 'test/num_examples': 3003, 'score': 7590.577194213867, 'total_duration': 12868.004507541656, 'accumulated_submission_time': 7590.577194213867, 'accumulated_eval_time': 5276.457659244537, 'accumulated_logging_time': 0.25283288955688477, 'global_step': 21727, 'preemption_count': 0}), (24142, {'train/accuracy': 0.6286821961402893, 'train/loss': 1.842318058013916, 'train/bleu': 30.943242981301132, 'validation/accuracy': 0.6500725150108337, 'validation/loss': 1.673073410987854, 'validation/bleu': 27.84176105484295, 'validation/num_examples': 3000, 'test/accuracy': 0.6639590859413147, 'test/loss': 1.6046804189682007, 'test/bleu': 27.245981275229024, 'test/num_examples': 3003, 'score': 8430.762261390686, 'total_duration': 14241.60857963562, 'accumulated_submission_time': 8430.762261390686, 'accumulated_eval_time': 5809.7654638290405, 'accumulated_logging_time': 0.28135228157043457, 'global_step': 24142, 'preemption_count': 0}), (26559, {'train/accuracy': 0.6353502869606018, 'train/loss': 1.772486925125122, 'train/bleu': 30.712247493082852, 'validation/accuracy': 0.6529862880706787, 'validation/loss': 1.6593254804611206, 'validation/bleu': 27.824232335672157, 'validation/num_examples': 3000, 'test/accuracy': 0.6608215570449829, 'test/loss': 1.6026358604431152, 'test/bleu': 26.744297453024025, 'test/num_examples': 3003, 'score': 9270.876539945602, 'total_duration': 15620.536381721497, 'accumulated_submission_time': 9270.876539945602, 'accumulated_eval_time': 6348.470281600952, 'accumulated_logging_time': 0.3095395565032959, 'global_step': 26559, 'preemption_count': 0}), (28976, {'train/accuracy': 0.6322528719902039, 'train/loss': 1.8003666400909424, 'train/bleu': 30.726127530034873, 'validation/accuracy': 0.6520811915397644, 'validation/loss': 1.648480772972107, 'validation/bleu': 27.845179168567565, 'validation/num_examples': 3000, 'test/accuracy': 0.6632967591285706, 'test/loss': 1.5888539552688599, 'test/bleu': 27.30567537970401, 'test/num_examples': 3003, 'score': 10111.109763383865, 'total_duration': 16919.401398181915, 'accumulated_submission_time': 10111.109763383865, 'accumulated_eval_time': 6806.992643594742, 'accumulated_logging_time': 0.33898496627807617, 'global_step': 28976, 'preemption_count': 0}), (31392, {'train/accuracy': 0.6668255925178528, 'train/loss': 1.5802048444747925, 'train/bleu': 33.53180261019265, 'validation/accuracy': 0.6552553772926331, 'validation/loss': 1.6375457048416138, 'validation/bleu': 27.770932078759643, 'validation/num_examples': 3000, 'test/accuracy': 0.6667131781578064, 'test/loss': 1.5764648914337158, 'test/bleu': 27.34374131007221, 'test/num_examples': 3003, 'score': 10951.089903831482, 'total_duration': 18321.97534775734, 'accumulated_submission_time': 10951.089903831482, 'accumulated_eval_time': 7369.476313352585, 'accumulated_logging_time': 0.36751580238342285, 'global_step': 31392, 'preemption_count': 0}), (33809, {'train/accuracy': 0.6364154815673828, 'train/loss': 1.7697381973266602, 'train/bleu': 30.883054872066605, 'validation/accuracy': 0.6572515964508057, 'validation/loss': 1.6273834705352783, 'validation/bleu': 28.077985667842412, 'validation/num_examples': 3000, 'test/accuracy': 0.665690541267395, 'test/loss': 1.5672624111175537, 'test/bleu': 27.31780751025964, 'test/num_examples': 3003, 'score': 11791.302706718445, 'total_duration': 19685.88606619835, 'accumulated_submission_time': 11791.302706718445, 'accumulated_eval_time': 7893.05903172493, 'accumulated_logging_time': 0.4018421173095703, 'global_step': 33809, 'preemption_count': 0}), (36227, {'train/accuracy': 0.6372659206390381, 'train/loss': 1.7720032930374146, 'train/bleu': 31.08953855694967, 'validation/accuracy': 0.6579459309577942, 'validation/loss': 1.6163133382797241, 'validation/bleu': 28.23293001497443, 'validation/num_examples': 3000, 'test/accuracy': 0.6670966148376465, 'test/loss': 1.5575518608093262, 'test/bleu': 27.478248748542704, 'test/num_examples': 3003, 'score': 12631.284592628479, 'total_duration': 21011.22130537033, 'accumulated_submission_time': 12631.284592628479, 'accumulated_eval_time': 8378.304702997208, 'accumulated_logging_time': 0.43168139457702637, 'global_step': 36227, 'preemption_count': 0}), (38643, {'train/accuracy': 0.6398760676383972, 'train/loss': 1.7399176359176636, 'train/bleu': 31.15971808188359, 'validation/accuracy': 0.6560736894607544, 'validation/loss': 1.6214781999588013, 'validation/bleu': 28.02467696226403, 'validation/num_examples': 3000, 'test/accuracy': 0.6660391688346863, 'test/loss': 1.5656033754348755, 'test/bleu': 27.417562134722917, 'test/num_examples': 3003, 'score': 13471.477101564407, 'total_duration': 22314.383660316467, 'accumulated_submission_time': 13471.477101564407, 'accumulated_eval_time': 8841.15231704712, 'accumulated_logging_time': 0.46880340576171875, 'global_step': 38643, 'preemption_count': 0}), (41061, {'train/accuracy': 0.6372715830802917, 'train/loss': 1.7691075801849365, 'train/bleu': 30.759693329732446, 'validation/accuracy': 0.6587767004966736, 'validation/loss': 1.6140390634536743, 'validation/bleu': 28.07393183728591, 'validation/num_examples': 3000, 'test/accuracy': 0.672163188457489, 'test/loss': 1.5471595525741577, 'test/bleu': 27.999417482086034, 'test/num_examples': 3003, 'score': 14311.619084835052, 'total_duration': 23735.619791984558, 'accumulated_submission_time': 14311.619084835052, 'accumulated_eval_time': 9422.134581565857, 'accumulated_logging_time': 0.5005180835723877, 'global_step': 41061, 'preemption_count': 0}), (43479, {'train/accuracy': 0.6386000514030457, 'train/loss': 1.760711908340454, 'train/bleu': 31.427426086467367, 'validation/accuracy': 0.6623228192329407, 'validation/loss': 1.5986641645431519, 'validation/bleu': 28.480353182921547, 'validation/num_examples': 3000, 'test/accuracy': 0.6713265180587769, 'test/loss': 1.540757417678833, 'test/bleu': 27.56919141028713, 'test/num_examples': 3003, 'score': 15151.775276899338, 'total_duration': 25052.546183347702, 'accumulated_submission_time': 15151.775276899338, 'accumulated_eval_time': 9898.793123483658, 'accumulated_logging_time': 0.5321898460388184, 'global_step': 43479, 'preemption_count': 0}), (45897, {'train/accuracy': 0.6438037753105164, 'train/loss': 1.7203915119171143, 'train/bleu': 31.366122874842002, 'validation/accuracy': 0.6614301204681396, 'validation/loss': 1.597715973854065, 'validation/bleu': 28.569499214277318, 'validation/num_examples': 3000, 'test/accuracy': 0.6712567806243896, 'test/loss': 1.539738416671753, 'test/bleu': 27.98019850766508, 'test/num_examples': 3003, 'score': 15991.93740272522, 'total_duration': 26433.4802134037, 'accumulated_submission_time': 15991.93740272522, 'accumulated_eval_time': 10439.45487523079, 'accumulated_logging_time': 0.5634627342224121, 'global_step': 45897, 'preemption_count': 0}), (48315, {'train/accuracy': 0.6425302028656006, 'train/loss': 1.7314515113830566, 'train/bleu': 31.29823991504049, 'validation/accuracy': 0.6614673137664795, 'validation/loss': 1.5935068130493164, 'validation/bleu': 28.392376189178425, 'validation/num_examples': 3000, 'test/accuracy': 0.6734995245933533, 'test/loss': 1.5197216272354126, 'test/bleu': 28.134675838345206, 'test/num_examples': 3003, 'score': 16832.123507976532, 'total_duration': 27793.366586208344, 'accumulated_submission_time': 16832.123507976532, 'accumulated_eval_time': 10959.045782327652, 'accumulated_logging_time': 0.5941922664642334, 'global_step': 48315, 'preemption_count': 0}), (50733, {'train/accuracy': 0.6550332307815552, 'train/loss': 1.6446468830108643, 'train/bleu': 31.87787872913857, 'validation/accuracy': 0.664765477180481, 'validation/loss': 1.5815393924713135, 'validation/bleu': 28.684633210753194, 'validation/num_examples': 3000, 'test/accuracy': 0.6736041307449341, 'test/loss': 1.5203142166137695, 'test/bleu': 27.925587206144616, 'test/num_examples': 3003, 'score': 17672.332036733627, 'total_duration': 29109.597824811935, 'accumulated_submission_time': 17672.332036733627, 'accumulated_eval_time': 11434.958333969116, 'accumulated_logging_time': 0.6256530284881592, 'global_step': 50733, 'preemption_count': 0}), (53150, {'train/accuracy': 0.6440820097923279, 'train/loss': 1.7206270694732666, 'train/bleu': 31.623226056556543, 'validation/accuracy': 0.6641455292701721, 'validation/loss': 1.5732247829437256, 'validation/bleu': 28.549211847348694, 'validation/num_examples': 3000, 'test/accuracy': 0.6757422685623169, 'test/loss': 1.5066756010055542, 'test/bleu': 28.266982534870575, 'test/num_examples': 3003, 'score': 18512.45762705803, 'total_duration': 30521.75099492073, 'accumulated_submission_time': 18512.45762705803, 'accumulated_eval_time': 12006.873229980469, 'accumulated_logging_time': 0.6573050022125244, 'global_step': 53150, 'preemption_count': 0}), (55566, {'train/accuracy': 0.6458576917648315, 'train/loss': 1.7048068046569824, 'train/bleu': 31.803183258359116, 'validation/accuracy': 0.6658813953399658, 'validation/loss': 1.5674703121185303, 'validation/bleu': 28.654052868631133, 'validation/num_examples': 3000, 'test/accuracy': 0.6787520051002502, 'test/loss': 1.5035967826843262, 'test/bleu': 28.449937622484967, 'test/num_examples': 3003, 'score': 19352.615475177765, 'total_duration': 31827.267405748367, 'accumulated_submission_time': 19352.615475177765, 'accumulated_eval_time': 12472.107189893723, 'accumulated_logging_time': 0.6961920261383057, 'global_step': 55566, 'preemption_count': 0}), (57983, {'train/accuracy': 0.6503816843032837, 'train/loss': 1.6813665628433228, 'train/bleu': 32.32253220630714, 'validation/accuracy': 0.6673941016197205, 'validation/loss': 1.5596922636032104, 'validation/bleu': 28.76944588602514, 'validation/num_examples': 3000, 'test/accuracy': 0.678345263004303, 'test/loss': 1.492568016052246, 'test/bleu': 28.715793486279985, 'test/num_examples': 3003, 'score': 20192.78905391693, 'total_duration': 33198.427434682846, 'accumulated_submission_time': 20192.78905391693, 'accumulated_eval_time': 13002.976558923721, 'accumulated_logging_time': 0.7319936752319336, 'global_step': 57983, 'preemption_count': 0}), (60399, {'train/accuracy': 0.6484251022338867, 'train/loss': 1.6919599771499634, 'train/bleu': 31.836385315191027, 'validation/accuracy': 0.667815625667572, 'validation/loss': 1.5593230724334717, 'validation/bleu': 28.898338564560326, 'validation/num_examples': 3000, 'test/accuracy': 0.6783917546272278, 'test/loss': 1.4889075756072998, 'test/bleu': 28.376137687711587, 'test/num_examples': 3003, 'score': 21032.677373170853, 'total_duration': 34535.92731380463, 'accumulated_submission_time': 21032.677373170853, 'accumulated_eval_time': 13500.465492010117, 'accumulated_logging_time': 0.7707424163818359, 'global_step': 60399, 'preemption_count': 0}), (62817, {'train/accuracy': 0.6717912554740906, 'train/loss': 1.5358587503433228, 'train/bleu': 33.683703448514876, 'validation/accuracy': 0.6687455773353577, 'validation/loss': 1.5462676286697388, 'validation/bleu': 28.54941150424726, 'validation/num_examples': 3000, 'test/accuracy': 0.6795189380645752, 'test/loss': 1.4860724210739136, 'test/bleu': 28.410224716126773, 'test/num_examples': 3003, 'score': 21872.860898256302, 'total_duration': 36023.72711586952, 'accumulated_submission_time': 21872.860898256302, 'accumulated_eval_time': 14147.967857837677, 'accumulated_logging_time': 0.803971529006958, 'global_step': 62817, 'preemption_count': 0}), (65235, {'train/accuracy': 0.6527928113937378, 'train/loss': 1.659690499305725, 'train/bleu': 32.17369020688684, 'validation/accuracy': 0.6705434322357178, 'validation/loss': 1.5372453927993774, 'validation/bleu': 28.912173341920933, 'validation/num_examples': 3000, 'test/accuracy': 0.6814130544662476, 'test/loss': 1.4726934432983398, 'test/bleu': 28.651393552679146, 'test/num_examples': 3003, 'score': 22712.9723572731, 'total_duration': 37421.238394737244, 'accumulated_submission_time': 22712.9723572731, 'accumulated_eval_time': 14705.24766755104, 'accumulated_logging_time': 0.8449218273162842, 'global_step': 65235, 'preemption_count': 0}), (67652, {'train/accuracy': 0.6534407138824463, 'train/loss': 1.6673787832260132, 'train/bleu': 32.31266434351574, 'validation/accuracy': 0.6718453764915466, 'validation/loss': 1.5317059755325317, 'validation/bleu': 29.001009543577776, 'validation/num_examples': 3000, 'test/accuracy': 0.6824589371681213, 'test/loss': 1.463875412940979, 'test/bleu': 28.896382659621782, 'test/num_examples': 3003, 'score': 23552.86766433716, 'total_duration': 38806.44243097305, 'accumulated_submission_time': 23552.86766433716, 'accumulated_eval_time': 15250.439332008362, 'accumulated_logging_time': 0.881615400314331, 'global_step': 67652, 'preemption_count': 0}), (70069, {'train/accuracy': 0.6589791178703308, 'train/loss': 1.6300126314163208, 'train/bleu': 32.72365867691716, 'validation/accuracy': 0.673568844795227, 'validation/loss': 1.5262335538864136, 'validation/bleu': 29.25410080130436, 'validation/num_examples': 3000, 'test/accuracy': 0.6847829818725586, 'test/loss': 1.4570910930633545, 'test/bleu': 28.990028967412396, 'test/num_examples': 3003, 'score': 24392.89284348488, 'total_duration': 40235.36283278465, 'accumulated_submission_time': 24392.89284348488, 'accumulated_eval_time': 15839.218275308609, 'accumulated_logging_time': 0.917823314666748, 'global_step': 70069, 'preemption_count': 0}), (72486, {'train/accuracy': 0.6565991044044495, 'train/loss': 1.6378833055496216, 'train/bleu': 32.39992250394725, 'validation/accuracy': 0.6732092499732971, 'validation/loss': 1.5226255655288696, 'validation/bleu': 29.28347733531719, 'validation/num_examples': 3000, 'test/accuracy': 0.6864911913871765, 'test/loss': 1.4465367794036865, 'test/bleu': 29.154292326536787, 'test/num_examples': 3003, 'score': 25232.844252109528, 'total_duration': 41621.707575798035, 'accumulated_submission_time': 25232.844252109528, 'accumulated_eval_time': 16385.491423606873, 'accumulated_logging_time': 0.9566261768341064, 'global_step': 72486, 'preemption_count': 0}), (74903, {'train/accuracy': 0.65643310546875, 'train/loss': 1.6501281261444092, 'train/bleu': 32.34811927392111, 'validation/accuracy': 0.6752055287361145, 'validation/loss': 1.5135127305984497, 'validation/bleu': 29.520270208655514, 'validation/num_examples': 3000, 'test/accuracy': 0.6882458925247192, 'test/loss': 1.440338373184204, 'test/bleu': 29.18233746463092, 'test/num_examples': 3003, 'score': 26072.833317756653, 'total_duration': 43007.06311130524, 'accumulated_submission_time': 26072.833317756653, 'accumulated_eval_time': 16930.741188049316, 'accumulated_logging_time': 0.9927854537963867, 'global_step': 74903, 'preemption_count': 0}), (77321, {'train/accuracy': 0.6606552600860596, 'train/loss': 1.6113643646240234, 'train/bleu': 33.132585237997496, 'validation/accuracy': 0.6767429709434509, 'validation/loss': 1.5007519721984863, 'validation/bleu': 29.464094676542434, 'validation/num_examples': 3000, 'test/accuracy': 0.6892685294151306, 'test/loss': 1.427660584449768, 'test/bleu': 29.210126730147856, 'test/num_examples': 3003, 'score': 26912.885219335556, 'total_duration': 44386.86210536957, 'accumulated_submission_time': 26912.885219335556, 'accumulated_eval_time': 17470.37271785736, 'accumulated_logging_time': 1.029709815979004, 'global_step': 77321, 'preemption_count': 0}), (79737, {'train/accuracy': 0.6597093939781189, 'train/loss': 1.622588872909546, 'train/bleu': 32.721303780072326, 'validation/accuracy': 0.6777225136756897, 'validation/loss': 1.5018718242645264, 'validation/bleu': 29.9094847833621, 'validation/num_examples': 3000, 'test/accuracy': 0.6904189586639404, 'test/loss': 1.426446557044983, 'test/bleu': 29.423481171518826, 'test/num_examples': 3003, 'score': 27752.805153131485, 'total_duration': 45742.4435338974, 'accumulated_submission_time': 27752.805153131485, 'accumulated_eval_time': 17985.91515660286, 'accumulated_logging_time': 1.0653765201568604, 'global_step': 79737, 'preemption_count': 0}), (82154, {'train/accuracy': 0.6717402935028076, 'train/loss': 1.538611888885498, 'train/bleu': 33.79730536923105, 'validation/accuracy': 0.6788632273674011, 'validation/loss': 1.4914655685424805, 'validation/bleu': 29.745504916128827, 'validation/num_examples': 3000, 'test/accuracy': 0.6918599009513855, 'test/loss': 1.4198691844940186, 'test/bleu': 29.724196297362315, 'test/num_examples': 3003, 'score': 28592.93453645706, 'total_duration': 47107.37024998665, 'accumulated_submission_time': 28592.93453645706, 'accumulated_eval_time': 18510.594499349594, 'accumulated_logging_time': 1.1032533645629883, 'global_step': 82154, 'preemption_count': 0}), (84571, {'train/accuracy': 0.665301501750946, 'train/loss': 1.583433747291565, 'train/bleu': 33.228433223994266, 'validation/accuracy': 0.6785780787467957, 'validation/loss': 1.48270583152771, 'validation/bleu': 29.504855205039437, 'validation/num_examples': 3000, 'test/accuracy': 0.6945209503173828, 'test/loss': 1.4088683128356934, 'test/bleu': 29.945540079129746, 'test/num_examples': 3003, 'score': 29432.89954471588, 'total_duration': 48542.69511389732, 'accumulated_submission_time': 29432.89954471588, 'accumulated_eval_time': 19105.83856487274, 'accumulated_logging_time': 1.1398842334747314, 'global_step': 84571, 'preemption_count': 0}), (86989, {'train/accuracy': 0.6616896986961365, 'train/loss': 1.6091835498809814, 'train/bleu': 33.137917686474665, 'validation/accuracy': 0.6786152720451355, 'validation/loss': 1.482520580291748, 'validation/bleu': 29.751823489337745, 'validation/num_examples': 3000, 'test/accuracy': 0.6953459978103638, 'test/loss': 1.4079169034957886, 'test/bleu': 29.84721352815368, 'test/num_examples': 3003, 'score': 30272.9449737072, 'total_duration': 49913.756229400635, 'accumulated_submission_time': 30272.9449737072, 'accumulated_eval_time': 19636.736701726913, 'accumulated_logging_time': 1.1791932582855225, 'global_step': 86989, 'preemption_count': 0}), (89407, {'train/accuracy': 0.6713466644287109, 'train/loss': 1.5552958250045776, 'train/bleu': 33.52182294055766, 'validation/accuracy': 0.6820374131202698, 'validation/loss': 1.465645670890808, 'validation/bleu': 29.869141731518045, 'validation/num_examples': 3000, 'test/accuracy': 0.6946952939033508, 'test/loss': 1.3910845518112183, 'test/bleu': 29.631786634944643, 'test/num_examples': 3003, 'score': 31113.069878339767, 'total_duration': 51299.45757818222, 'accumulated_submission_time': 31113.069878339767, 'accumulated_eval_time': 20182.18912935257, 'accumulated_logging_time': 1.2226190567016602, 'global_step': 89407, 'preemption_count': 0}), (91823, {'train/accuracy': 0.6688807606697083, 'train/loss': 1.561652421951294, 'train/bleu': 33.646249972575525, 'validation/accuracy': 0.6818886399269104, 'validation/loss': 1.4689723253250122, 'validation/bleu': 29.931857466564306, 'validation/num_examples': 3000, 'test/accuracy': 0.6980187296867371, 'test/loss': 1.3815217018127441, 'test/bleu': 30.353041929433832, 'test/num_examples': 3003, 'score': 31953.005130767822, 'total_duration': 52629.459279060364, 'accumulated_submission_time': 31953.005130767822, 'accumulated_eval_time': 20672.136009454727, 'accumulated_logging_time': 1.2626848220825195, 'global_step': 91823, 'preemption_count': 0}), (94241, {'train/accuracy': 0.6905794739723206, 'train/loss': 1.4270117282867432, 'train/bleu': 34.98234446474538, 'validation/accuracy': 0.6850627660751343, 'validation/loss': 1.4553813934326172, 'validation/bleu': 30.200363953398387, 'validation/num_examples': 3000, 'test/accuracy': 0.699785053730011, 'test/loss': 1.3779276609420776, 'test/bleu': 30.07975448373041, 'test/num_examples': 3003, 'score': 32793.152134656906, 'total_duration': 54029.85724711418, 'accumulated_submission_time': 32793.152134656906, 'accumulated_eval_time': 21232.269668102264, 'accumulated_logging_time': 1.301042079925537, 'global_step': 94241, 'preemption_count': 0}), (96658, {'train/accuracy': 0.6737492084503174, 'train/loss': 1.5275613069534302, 'train/bleu': 34.108211883515416, 'validation/accuracy': 0.6844552159309387, 'validation/loss': 1.4521570205688477, 'validation/bleu': 30.20829723913238, 'validation/num_examples': 3000, 'test/accuracy': 0.6992853283882141, 'test/loss': 1.3757984638214111, 'test/bleu': 30.073980041317707, 'test/num_examples': 3003, 'score': 33633.102585315704, 'total_duration': 55416.08647465706, 'accumulated_submission_time': 33633.102585315704, 'accumulated_eval_time': 21778.425297021866, 'accumulated_logging_time': 1.342395544052124, 'global_step': 96658, 'preemption_count': 0}), (99074, {'train/accuracy': 0.6748491525650024, 'train/loss': 1.5263543128967285, 'train/bleu': 33.783884560990835, 'validation/accuracy': 0.6855711340904236, 'validation/loss': 1.4503666162490845, 'validation/bleu': 30.292007534617788, 'validation/num_examples': 3000, 'test/accuracy': 0.7000755667686462, 'test/loss': 1.370320200920105, 'test/bleu': 30.03763911580841, 'test/num_examples': 3003, 'score': 34472.99043941498, 'total_duration': 56800.761588811874, 'accumulated_submission_time': 34472.99043941498, 'accumulated_eval_time': 22323.090844154358, 'accumulated_logging_time': 1.3831169605255127, 'global_step': 99074, 'preemption_count': 0}), (101491, {'train/accuracy': 0.6866604685783386, 'train/loss': 1.4556916952133179, 'train/bleu': 34.48494577502086, 'validation/accuracy': 0.6869474649429321, 'validation/loss': 1.4450162649154663, 'validation/bleu': 30.187936515973128, 'validation/num_examples': 3000, 'test/accuracy': 0.7018999457359314, 'test/loss': 1.3603651523590088, 'test/bleu': 30.280502251710864, 'test/num_examples': 3003, 'score': 35312.9445669651, 'total_duration': 58180.41181755066, 'accumulated_submission_time': 35312.9445669651, 'accumulated_eval_time': 22862.66453719139, 'accumulated_logging_time': 1.4255762100219727, 'global_step': 101491, 'preemption_count': 0}), (103909, {'train/accuracy': 0.6815876960754395, 'train/loss': 1.492857575416565, 'train/bleu': 34.64913971543467, 'validation/accuracy': 0.6869598627090454, 'validation/loss': 1.4377763271331787, 'validation/bleu': 30.17169848257597, 'validation/num_examples': 3000, 'test/accuracy': 0.7025158405303955, 'test/loss': 1.3520406484603882, 'test/bleu': 30.232446000713, 'test/num_examples': 3003, 'score': 36152.97557926178, 'total_duration': 59683.14560890198, 'accumulated_submission_time': 36152.97557926178, 'accumulated_eval_time': 23525.245816469193, 'accumulated_logging_time': 1.4649641513824463, 'global_step': 103909, 'preemption_count': 0}), (106327, {'train/accuracy': 0.6837038993835449, 'train/loss': 1.4776220321655273, 'train/bleu': 35.08743437169032, 'validation/accuracy': 0.688646137714386, 'validation/loss': 1.4313939809799194, 'validation/bleu': 30.522066373997887, 'validation/num_examples': 3000, 'test/accuracy': 0.7030619978904724, 'test/loss': 1.3502763509750366, 'test/bleu': 30.41185965475121, 'test/num_examples': 3003, 'score': 36993.1280477047, 'total_duration': 61087.41273570061, 'accumulated_submission_time': 36993.1280477047, 'accumulated_eval_time': 24089.24018883705, 'accumulated_logging_time': 1.504408836364746, 'global_step': 106327, 'preemption_count': 0}), (108745, {'train/accuracy': 0.6909144520759583, 'train/loss': 1.4369436502456665, 'train/bleu': 34.725908050814276, 'validation/accuracy': 0.6895388960838318, 'validation/loss': 1.42962646484375, 'validation/bleu': 30.422796842264475, 'validation/num_examples': 3000, 'test/accuracy': 0.705420970916748, 'test/loss': 1.344992756843567, 'test/bleu': 30.553237810904655, 'test/num_examples': 3003, 'score': 37833.296102523804, 'total_duration': 62459.371757268906, 'accumulated_submission_time': 37833.296102523804, 'accumulated_eval_time': 24620.91137957573, 'accumulated_logging_time': 1.5444655418395996, 'global_step': 108745, 'preemption_count': 0}), (111162, {'train/accuracy': 0.6862839460372925, 'train/loss': 1.4570157527923584, 'train/bleu': 35.03915497026832, 'validation/accuracy': 0.6900472044944763, 'validation/loss': 1.4253438711166382, 'validation/bleu': 30.634258780744172, 'validation/num_examples': 3000, 'test/accuracy': 0.7066760063171387, 'test/loss': 1.3375072479248047, 'test/bleu': 30.61793987736942, 'test/num_examples': 3003, 'score': 38673.18712234497, 'total_duration': 63836.94434714317, 'accumulated_submission_time': 38673.18712234497, 'accumulated_eval_time': 25158.474792718887, 'accumulated_logging_time': 1.5841474533081055, 'global_step': 111162, 'preemption_count': 0}), (113580, {'train/accuracy': 0.703749418258667, 'train/loss': 1.3690860271453857, 'train/bleu': 36.01579664231886, 'validation/accuracy': 0.691473126411438, 'validation/loss': 1.4214249849319458, 'validation/bleu': 30.438168648771725, 'validation/num_examples': 3000, 'test/accuracy': 0.7059555053710938, 'test/loss': 1.3358170986175537, 'test/bleu': 30.445572866631903, 'test/num_examples': 3003, 'score': 39513.24196100235, 'total_duration': 65231.111525297165, 'accumulated_submission_time': 39513.24196100235, 'accumulated_eval_time': 25712.46093392372, 'accumulated_logging_time': 1.6317753791809082, 'global_step': 113580, 'preemption_count': 0}), (115998, {'train/accuracy': 0.6970692873001099, 'train/loss': 1.398681640625, 'train/bleu': 35.804610108981215, 'validation/accuracy': 0.691386342048645, 'validation/loss': 1.418184757232666, 'validation/bleu': 30.566812091793675, 'validation/num_examples': 3000, 'test/accuracy': 0.7085468769073486, 'test/loss': 1.3254882097244263, 'test/bleu': 30.682219921723906, 'test/num_examples': 3003, 'score': 40353.26636815071, 'total_duration': 66676.8403544426, 'accumulated_submission_time': 40353.26636815071, 'accumulated_eval_time': 26318.045152664185, 'accumulated_logging_time': 1.672149419784546, 'global_step': 115998, 'preemption_count': 0}), (118415, {'train/accuracy': 0.6983740925788879, 'train/loss': 1.3953670263290405, 'train/bleu': 35.78991389340325, 'validation/accuracy': 0.6922046542167664, 'validation/loss': 1.4161663055419922, 'validation/bleu': 30.73623908706971, 'validation/num_examples': 3000, 'test/accuracy': 0.7084422707557678, 'test/loss': 1.3272607326507568, 'test/bleu': 30.917426686390197, 'test/num_examples': 3003, 'score': 41193.21762943268, 'total_duration': 68089.17468738556, 'accumulated_submission_time': 41193.21762943268, 'accumulated_eval_time': 26890.301241874695, 'accumulated_logging_time': 1.715043306350708, 'global_step': 118415, 'preemption_count': 0}), (120833, {'train/accuracy': 0.7079231142997742, 'train/loss': 1.3452441692352295, 'train/bleu': 36.54008424706086, 'validation/accuracy': 0.69246506690979, 'validation/loss': 1.4150506258010864, 'validation/bleu': 30.88375035702032, 'validation/num_examples': 3000, 'test/accuracy': 0.7084422707557678, 'test/loss': 1.324387550354004, 'test/bleu': 30.767148916451248, 'test/num_examples': 3003, 'score': 42033.183654785156, 'total_duration': 69469.79971814156, 'accumulated_submission_time': 42033.183654785156, 'accumulated_eval_time': 27430.756391763687, 'accumulated_logging_time': 1.8395373821258545, 'global_step': 120833, 'preemption_count': 0}), (123251, {'train/accuracy': 0.7070516347885132, 'train/loss': 1.3467681407928467, 'train/bleu': 36.243165574604014, 'validation/accuracy': 0.6931470036506653, 'validation/loss': 1.4134272336959839, 'validation/bleu': 30.778888404449745, 'validation/num_examples': 3000, 'test/accuracy': 0.7096391916275024, 'test/loss': 1.320741891860962, 'test/bleu': 30.748641590683718, 'test/num_examples': 3003, 'score': 42873.383053064346, 'total_duration': 70842.37611746788, 'accumulated_submission_time': 42873.383053064346, 'accumulated_eval_time': 27963.000198602676, 'accumulated_logging_time': 1.8919637203216553, 'global_step': 123251, 'preemption_count': 0}), (125668, {'train/accuracy': 0.710022509098053, 'train/loss': 1.332497239112854, 'train/bleu': 36.40957170008483, 'validation/accuracy': 0.6937421560287476, 'validation/loss': 1.4104636907577515, 'validation/bleu': 30.8876838523228, 'validation/num_examples': 3000, 'test/accuracy': 0.709337055683136, 'test/loss': 1.3211370706558228, 'test/bleu': 30.857238354469466, 'test/num_examples': 3003, 'score': 43713.294801950455, 'total_duration': 72267.84021043777, 'accumulated_submission_time': 43713.294801950455, 'accumulated_eval_time': 28548.430537700653, 'accumulated_logging_time': 1.933983564376831, 'global_step': 125668, 'preemption_count': 0}), (128086, {'train/accuracy': 0.7109796404838562, 'train/loss': 1.329494595527649, 'train/bleu': 36.99412188193442, 'validation/accuracy': 0.6932337880134583, 'validation/loss': 1.4097765684127808, 'validation/bleu': 30.99008929284226, 'validation/num_examples': 3000, 'test/accuracy': 0.7093021869659424, 'test/loss': 1.319244623184204, 'test/bleu': 30.86260403130315, 'test/num_examples': 3003, 'score': 44553.37302994728, 'total_duration': 73655.06421208382, 'accumulated_submission_time': 44553.37302994728, 'accumulated_eval_time': 29095.45155930519, 'accumulated_logging_time': 1.9780395030975342, 'global_step': 128086, 'preemption_count': 0}), (130505, {'train/accuracy': 0.7105883359909058, 'train/loss': 1.3296940326690674, 'train/bleu': 36.643996629084846, 'validation/accuracy': 0.6933826208114624, 'validation/loss': 1.4100453853607178, 'validation/bleu': 30.929830153579662, 'validation/num_examples': 3000, 'test/accuracy': 0.710220217704773, 'test/loss': 1.3187133073806763, 'test/bleu': 30.82152023947141, 'test/num_examples': 3003, 'score': 45393.514297008514, 'total_duration': 75043.40402579308, 'accumulated_submission_time': 45393.514297008514, 'accumulated_eval_time': 29643.51740694046, 'accumulated_logging_time': 2.0315675735473633, 'global_step': 130505, 'preemption_count': 0}), (132924, {'train/accuracy': 0.7117727398872375, 'train/loss': 1.324880599975586, 'train/bleu': 36.79409669434138, 'validation/accuracy': 0.6934942007064819, 'validation/loss': 1.4099982976913452, 'validation/bleu': 30.80455651715488, 'validation/num_examples': 3000, 'test/accuracy': 0.7099761962890625, 'test/loss': 1.3190022706985474, 'test/bleu': 30.837535923196857, 'test/num_examples': 3003, 'score': 46233.72875595093, 'total_duration': 76443.79782891273, 'accumulated_submission_time': 46233.72875595093, 'accumulated_eval_time': 30203.573014974594, 'accumulated_logging_time': 2.0750794410705566, 'global_step': 132924, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7115277051925659, 'train/loss': 1.324217438697815, 'train/bleu': 36.53925289250019, 'validation/accuracy': 0.6935189962387085, 'validation/loss': 1.40999174118042, 'validation/bleu': 30.825316571983876, 'validation/num_examples': 3000, 'test/accuracy': 0.7099761962890625, 'test/loss': 1.318996548652649, 'test/bleu': 30.846443803364846, 'test/num_examples': 3003, 'score': 46375.332560777664, 'total_duration': 77133.75478172302, 'accumulated_submission_time': 46375.332560777664, 'accumulated_eval_time': 30751.867042303085, 'accumulated_logging_time': 2.1205337047576904, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0213 05:51:45.681187 140144802662208 submission_runner.py:586] Timing: 46375.332560777664
I0213 05:51:45.681250 140144802662208 submission_runner.py:588] Total number of evals: 57
I0213 05:51:45.681294 140144802662208 submission_runner.py:589] ====================
I0213 05:51:45.681341 140144802662208 submission_runner.py:542] Using RNG seed 599091471
I0213 05:51:45.683187 140144802662208 submission_runner.py:551] --- Tuning run 4/5 ---
I0213 05:51:45.683317 140144802662208 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_4.
I0213 05:51:45.683604 140144802662208 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_4/hparams.json.
I0213 05:51:45.684479 140144802662208 submission_runner.py:206] Initializing dataset.
I0213 05:51:45.687417 140144802662208 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0213 05:51:45.691031 140144802662208 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0213 05:51:45.734313 140144802662208 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0213 05:51:46.482732 140144802662208 submission_runner.py:213] Initializing model.
I0213 05:51:53.703497 140144802662208 submission_runner.py:255] Initializing optimizer.
I0213 05:51:54.511104 140144802662208 submission_runner.py:262] Initializing metrics bundle.
I0213 05:51:54.511374 140144802662208 submission_runner.py:280] Initializing checkpoint and logger.
I0213 05:51:54.512532 140144802662208 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/wmt_jax/trial_4 with prefix checkpoint_
I0213 05:51:54.512656 140144802662208 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_4/meta_data_0.json.
I0213 05:51:54.512929 140144802662208 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0213 05:51:54.512994 140144802662208 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0213 05:51:55.036134 140144802662208 logger_utils.py:220] Unable to record git information. Continuing without it.
I0213 05:51:55.530014 140144802662208 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_4/flags_0.json.
I0213 05:51:55.534011 140144802662208 submission_runner.py:314] Starting training loop.
I0213 05:52:25.409918 139974989014784 logging_writer.py:48] [0] global_step=0, grad_norm=5.756457805633545, loss=10.947874069213867
I0213 05:52:25.423549 140144802662208 spec.py:321] Evaluating on the training split.
I0213 05:52:28.118978 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 05:57:12.440308 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 05:57:15.139461 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 06:01:59.091515 140144802662208 spec.py:349] Evaluating on the test split.
I0213 06:02:01.781844 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 06:06:45.569098 140144802662208 submission_runner.py:408] Time since start: 890.04s, 	Step: 1, 	{'train/accuracy': 0.0005914963549003005, 'train/loss': 10.96237564086914, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 29.889463663101196, 'total_duration': 890.0350096225739, 'accumulated_submission_time': 29.889463663101196, 'accumulated_eval_time': 860.1454901695251, 'accumulated_logging_time': 0}
I0213 06:06:45.578776 139974997407488 logging_writer.py:48] [1] accumulated_eval_time=860.145490, accumulated_logging_time=0, accumulated_submission_time=29.889464, global_step=1, preemption_count=0, score=29.889464, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.966498, test/num_examples=3003, total_duration=890.035010, train/accuracy=0.000591, train/bleu=0.000000, train/loss=10.962376, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.980294, validation/num_examples=3000
I0213 06:07:20.324583 139974989014784 logging_writer.py:48] [100] global_step=100, grad_norm=0.4919303357601166, loss=7.586775779724121
I0213 06:07:55.102652 139974997407488 logging_writer.py:48] [200] global_step=200, grad_norm=0.5085276365280151, loss=6.622316837310791
I0213 06:08:29.926778 139974989014784 logging_writer.py:48] [300] global_step=300, grad_norm=0.5207285284996033, loss=5.844560146331787
I0213 06:09:04.737949 139974997407488 logging_writer.py:48] [400] global_step=400, grad_norm=0.5993211269378662, loss=5.500241756439209
I0213 06:09:39.555808 139974989014784 logging_writer.py:48] [500] global_step=500, grad_norm=0.4084129333496094, loss=5.037432670593262
I0213 06:10:14.382376 139974997407488 logging_writer.py:48] [600] global_step=600, grad_norm=0.5274711847305298, loss=4.751411437988281
I0213 06:10:49.179901 139974989014784 logging_writer.py:48] [700] global_step=700, grad_norm=0.5924687385559082, loss=4.557473659515381
I0213 06:11:23.992670 139974997407488 logging_writer.py:48] [800] global_step=800, grad_norm=0.41844046115875244, loss=4.203516483306885
I0213 06:11:58.796895 139974989014784 logging_writer.py:48] [900] global_step=900, grad_norm=0.42790138721466064, loss=3.9457225799560547
I0213 06:12:33.576106 139974997407488 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.41056984663009644, loss=3.7846806049346924
I0213 06:13:08.402333 139974989014784 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5646035075187683, loss=3.651097536087036
I0213 06:13:43.195358 139974997407488 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3841697871685028, loss=3.5195631980895996
I0213 06:14:17.986382 139974989014784 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.40508928894996643, loss=3.4949188232421875
I0213 06:14:52.793902 139974997407488 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.2758309543132782, loss=3.4144763946533203
I0213 06:15:27.588411 139974989014784 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.29461386799812317, loss=3.212270498275757
I0213 06:16:02.368715 139974997407488 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.22993813455104828, loss=3.1128711700439453
I0213 06:16:37.123578 139974989014784 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.2521751821041107, loss=3.016866445541382
I0213 06:17:11.901620 139974997407488 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.25897544622421265, loss=2.939549446105957
I0213 06:17:46.651839 139974989014784 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.2675936222076416, loss=2.8543148040771484
I0213 06:18:21.440848 139974997407488 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.1849096119403839, loss=2.8232502937316895
I0213 06:18:56.213162 139974989014784 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.2186266928911209, loss=2.7950944900512695
I0213 06:19:31.013309 139974997407488 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.30442336201667786, loss=2.7616143226623535
I0213 06:20:05.834392 139974989014784 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.1724991649389267, loss=2.6637353897094727
I0213 06:20:40.622352 139974997407488 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.2011169195175171, loss=2.583409547805786
I0213 06:20:45.899296 140144802662208 spec.py:321] Evaluating on the training split.
I0213 06:20:48.860955 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 06:23:42.192152 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 06:23:44.862697 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 06:26:25.301346 140144802662208 spec.py:349] Evaluating on the test split.
I0213 06:26:27.969128 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 06:29:02.778307 140144802662208 submission_runner.py:408] Time since start: 2227.24s, 	Step: 2417, 	{'train/accuracy': 0.5345553755760193, 'train/loss': 2.5997135639190674, 'train/bleu': 23.673126618849807, 'validation/accuracy': 0.5361867547035217, 'validation/loss': 2.5623652935028076, 'validation/bleu': 19.77231194625641, 'validation/num_examples': 3000, 'test/accuracy': 0.5345999598503113, 'test/loss': 2.571000099182129, 'test/bleu': 17.927984064231914, 'test/num_examples': 3003, 'score': 870.12095952034, 'total_duration': 2227.2441873550415, 'accumulated_submission_time': 870.12095952034, 'accumulated_eval_time': 1357.0244114398956, 'accumulated_logging_time': 0.019931316375732422}
I0213 06:29:02.796704 139974989014784 logging_writer.py:48] [2417] accumulated_eval_time=1357.024411, accumulated_logging_time=0.019931, accumulated_submission_time=870.120960, global_step=2417, preemption_count=0, score=870.120960, test/accuracy=0.534600, test/bleu=17.927984, test/loss=2.571000, test/num_examples=3003, total_duration=2227.244187, train/accuracy=0.534555, train/bleu=23.673127, train/loss=2.599714, validation/accuracy=0.536187, validation/bleu=19.772312, validation/loss=2.562365, validation/num_examples=3000
I0213 06:29:31.907622 139974997407488 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.20678439736366272, loss=2.6777682304382324
I0213 06:30:06.601711 139974989014784 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.19431602954864502, loss=2.458555221557617
I0213 06:30:41.377338 139974997407488 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.1856725811958313, loss=2.5042996406555176
I0213 06:31:16.173096 139974989014784 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.23129262030124664, loss=2.489593029022217
I0213 06:31:50.932634 139974997407488 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.20998404920101166, loss=2.4309539794921875
I0213 06:32:25.709684 139974989014784 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.27978211641311646, loss=2.442265033721924
I0213 06:33:00.478386 139974997407488 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.24335980415344238, loss=2.4516849517822266
I0213 06:33:35.252408 139974989014784 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.2505995035171509, loss=2.4142744541168213
I0213 06:34:10.023093 139974997407488 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.2198968082666397, loss=2.301503896713257
I0213 06:34:44.806587 139974989014784 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.2295120656490326, loss=2.2986104488372803
I0213 06:35:19.598980 139974997407488 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.3393944799900055, loss=2.3045506477355957
I0213 06:35:54.376682 139974989014784 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.213521346449852, loss=2.373124361038208
I0213 06:36:29.157495 139974997407488 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.42883649468421936, loss=2.326427698135376
I0213 06:37:03.918212 139974989014784 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.3169834613800049, loss=2.3110218048095703
I0213 06:37:38.685643 139974997407488 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.3668576776981354, loss=2.2752838134765625
I0213 06:38:13.466406 139974989014784 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3691127896308899, loss=2.308067798614502
I0213 06:38:48.278734 139974997407488 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.38510754704475403, loss=2.283640146255493
I0213 06:39:23.211414 139974989014784 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.4008280038833618, loss=2.2645998001098633
I0213 06:39:58.089858 139974997407488 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.3305625021457672, loss=2.251291513442993
I0213 06:40:32.938998 139974989014784 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.2748102843761444, loss=2.2266759872436523
I0213 06:41:07.717889 139974997407488 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.266854465007782, loss=2.185310125350952
I0213 06:41:42.482322 139974989014784 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.3117091953754425, loss=2.2257843017578125
I0213 06:42:17.279290 139974997407488 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5115260481834412, loss=2.197021007537842
I0213 06:42:52.077672 139974989014784 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.4230179190635681, loss=2.1973068714141846
I0213 06:43:02.939026 140144802662208 spec.py:321] Evaluating on the training split.
I0213 06:43:05.898277 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 06:45:49.877241 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 06:45:52.544624 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 06:48:29.029133 140144802662208 spec.py:349] Evaluating on the test split.
I0213 06:48:31.711704 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 06:50:43.111896 140144802662208 submission_runner.py:408] Time since start: 3527.58s, 	Step: 4833, 	{'train/accuracy': 0.5767702460289001, 'train/loss': 2.2277464866638184, 'train/bleu': 26.598028617543335, 'validation/accuracy': 0.5945369601249695, 'validation/loss': 2.089860439300537, 'validation/bleu': 23.094498651041594, 'validation/num_examples': 3000, 'test/accuracy': 0.5962698459625244, 'test/loss': 2.0628721714019775, 'test/bleu': 21.65287838064829, 'test/num_examples': 3003, 'score': 1710.169316291809, 'total_duration': 3527.5778126716614, 'accumulated_submission_time': 1710.169316291809, 'accumulated_eval_time': 1817.1972329616547, 'accumulated_logging_time': 0.049460411071777344}
I0213 06:50:43.127020 139974997407488 logging_writer.py:48] [4833] accumulated_eval_time=1817.197233, accumulated_logging_time=0.049460, accumulated_submission_time=1710.169316, global_step=4833, preemption_count=0, score=1710.169316, test/accuracy=0.596270, test/bleu=21.652878, test/loss=2.062872, test/num_examples=3003, total_duration=3527.577813, train/accuracy=0.576770, train/bleu=26.598029, train/loss=2.227746, validation/accuracy=0.594537, validation/bleu=23.094499, validation/loss=2.089860, validation/num_examples=3000
I0213 06:51:06.663424 139974989014784 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3034477233886719, loss=2.2344775199890137
I0213 06:51:41.334327 139974997407488 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.45036545395851135, loss=2.253734588623047
I0213 06:52:16.093468 139974989014784 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.29521414637565613, loss=2.1751089096069336
I0213 06:52:50.903343 139974997407488 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.3732838034629822, loss=2.192638397216797
I0213 06:53:25.672864 139974989014784 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.3029301166534424, loss=2.148247480392456
I0213 06:54:00.428944 139974997407488 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.44118210673332214, loss=2.1837961673736572
I0213 06:54:35.183130 139974989014784 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5849624276161194, loss=2.223381757736206
I0213 06:55:09.945710 139974997407488 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5063647627830505, loss=2.2150871753692627
I0213 06:55:44.707845 139974989014784 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5974105000495911, loss=2.2103216648101807
I0213 06:56:19.498951 139974997407488 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.2955080568790436, loss=2.168224811553955
I0213 06:56:54.249580 139974989014784 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.2861008942127228, loss=2.1843724250793457
I0213 06:57:29.025704 139974997407488 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.40653908252716064, loss=2.177290916442871
I0213 06:58:03.797808 139974989014784 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.29804548621177673, loss=2.2087347507476807
I0213 06:58:38.593447 139974997407488 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5589807033538818, loss=2.2384822368621826
I0213 06:59:13.376873 139975005800192 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.30934271216392517, loss=2.1323554515838623
I0213 06:59:48.116790 139975014192896 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.42201825976371765, loss=2.163304090499878
I0213 07:00:22.895806 139975005800192 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.29153764247894287, loss=2.1925108432769775
I0213 07:00:57.647011 139975014192896 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.35032209753990173, loss=2.2570221424102783
I0213 07:01:32.409703 139975005800192 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.2821163535118103, loss=2.168193817138672
I0213 07:02:07.175861 139975014192896 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.2835829555988312, loss=2.1654088497161865
I0213 07:02:41.945410 139975005800192 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.43637433648109436, loss=2.2221391201019287
I0213 07:03:16.715625 139975014192896 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.3016771972179413, loss=2.1740214824676514
I0213 07:03:51.504350 139975005800192 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6531627774238586, loss=2.255908489227295
I0213 07:04:26.341724 139975014192896 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.32826435565948486, loss=2.1338837146759033
I0213 07:04:43.455670 140144802662208 spec.py:321] Evaluating on the training split.
I0213 07:04:46.427220 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 07:07:37.244995 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 07:07:39.918376 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 07:10:16.475445 140144802662208 spec.py:349] Evaluating on the test split.
I0213 07:10:19.149755 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 07:12:42.419103 140144802662208 submission_runner.py:408] Time since start: 4846.89s, 	Step: 7251, 	{'train/accuracy': 0.5924220681190491, 'train/loss': 2.1205623149871826, 'train/bleu': 27.298196480210727, 'validation/accuracy': 0.6061673164367676, 'validation/loss': 2.006256580352783, 'validation/bleu': 24.03146513922641, 'validation/num_examples': 3000, 'test/accuracy': 0.6099936366081238, 'test/loss': 1.9696370363235474, 'test/bleu': 22.926869041848573, 'test/num_examples': 3003, 'score': 2550.408034324646, 'total_duration': 4846.8850247859955, 'accumulated_submission_time': 2550.408034324646, 'accumulated_eval_time': 2296.1606137752533, 'accumulated_logging_time': 0.07471156120300293}
I0213 07:12:42.434532 139975005800192 logging_writer.py:48] [7251] accumulated_eval_time=2296.160614, accumulated_logging_time=0.074712, accumulated_submission_time=2550.408034, global_step=7251, preemption_count=0, score=2550.408034, test/accuracy=0.609994, test/bleu=22.926869, test/loss=1.969637, test/num_examples=3003, total_duration=4846.885025, train/accuracy=0.592422, train/bleu=27.298196, train/loss=2.120562, validation/accuracy=0.606167, validation/bleu=24.031465, validation/loss=2.006257, validation/num_examples=3000
I0213 07:12:59.762259 139975014192896 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7534749507904053, loss=2.106288194656372
I0213 07:13:34.409788 139975005800192 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.2714790999889374, loss=2.06744647026062
I0213 07:14:09.145781 139975014192896 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5938162207603455, loss=2.1188905239105225
I0213 07:14:43.918992 139975005800192 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.26037734746932983, loss=2.0673348903656006
I0213 07:15:18.673042 139975014192896 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4902655780315399, loss=2.1403138637542725
I0213 07:15:53.441904 139975005800192 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6224454641342163, loss=2.1763803958892822
I0213 07:16:28.203128 139975014192896 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5292606353759766, loss=2.235917806625366
I0213 07:17:02.952396 139975005800192 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.30307695269584656, loss=2.095781087875366
I0213 07:17:37.707851 139975014192896 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.4722161591053009, loss=2.149298906326294
I0213 07:18:12.470425 139975005800192 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3615463674068451, loss=2.138695001602173
I0213 07:18:47.242513 139975014192896 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.3243046998977661, loss=2.1178340911865234
I0213 07:19:22.018323 139975005800192 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.40818676352500916, loss=2.144399404525757
I0213 07:19:56.801450 139975014192896 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4408968687057495, loss=2.0168418884277344
I0213 07:20:31.562264 139975005800192 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.28490814566612244, loss=2.0228240489959717
I0213 07:21:06.323240 139975014192896 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3246215879917145, loss=2.1291720867156982
I0213 07:21:41.087796 139975005800192 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5150879621505737, loss=2.050072193145752
I0213 07:22:15.886840 139975014192896 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3007031977176666, loss=2.0823569297790527
I0213 07:22:50.697159 139975005800192 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7121380567550659, loss=2.218069076538086
I0213 07:23:25.491505 139975014192896 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5146231651306152, loss=2.1316912174224854
I0213 07:24:00.273908 139975005800192 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3042829632759094, loss=2.2044193744659424
I0213 07:24:35.027792 139975014192896 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.35641464591026306, loss=2.111370086669922
I0213 07:25:09.789170 139975005800192 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.30642199516296387, loss=2.176217794418335
I0213 07:25:44.562547 139975014192896 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.31527000665664673, loss=2.0791311264038086
I0213 07:26:19.334642 139975005800192 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.2933911979198456, loss=2.100839614868164
I0213 07:26:42.689155 140144802662208 spec.py:321] Evaluating on the training split.
I0213 07:26:45.653922 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 07:29:46.610999 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 07:29:49.278527 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 07:32:30.115068 140144802662208 spec.py:349] Evaluating on the test split.
I0213 07:32:32.803120 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 07:35:10.791991 140144802662208 submission_runner.py:408] Time since start: 6195.26s, 	Step: 9669, 	{'train/accuracy': 0.5942032337188721, 'train/loss': 2.1156461238861084, 'train/bleu': 27.3323492483322, 'validation/accuracy': 0.6107797622680664, 'validation/loss': 1.9769822359085083, 'validation/bleu': 24.644655251514664, 'validation/num_examples': 3000, 'test/accuracy': 0.6165592074394226, 'test/loss': 1.929946780204773, 'test/bleu': 23.52521008844378, 'test/num_examples': 3003, 'score': 3390.572445869446, 'total_duration': 6195.257897377014, 'accumulated_submission_time': 3390.572445869446, 'accumulated_eval_time': 2804.2633929252625, 'accumulated_logging_time': 0.09983301162719727}
I0213 07:35:10.807687 139975014192896 logging_writer.py:48] [9669] accumulated_eval_time=2804.263393, accumulated_logging_time=0.099833, accumulated_submission_time=3390.572446, global_step=9669, preemption_count=0, score=3390.572446, test/accuracy=0.616559, test/bleu=23.525210, test/loss=1.929947, test/num_examples=3003, total_duration=6195.257897, train/accuracy=0.594203, train/bleu=27.332349, train/loss=2.115646, validation/accuracy=0.610780, validation/bleu=24.644655, validation/loss=1.976982, validation/num_examples=3000
I0213 07:35:21.897431 139975005800192 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.4156002104282379, loss=2.0144262313842773
I0213 07:35:56.495390 139975014192896 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3372372090816498, loss=2.106537103652954
I0213 07:36:31.397099 139975005800192 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5544329285621643, loss=2.0944769382476807
I0213 07:37:06.153990 139975014192896 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.27000439167022705, loss=2.046332836151123
I0213 07:37:40.907685 139975005800192 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5556184649467468, loss=2.19687819480896
I0213 07:38:15.678847 139975014192896 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6925995349884033, loss=2.1004798412323
I0213 07:38:50.470512 139975005800192 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.4021335542201996, loss=2.103677988052368
I0213 07:39:25.271632 139975014192896 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3350040018558502, loss=2.09317946434021
I0213 07:40:00.042052 139975005800192 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.3650229573249817, loss=2.2096569538116455
I0213 07:40:34.828744 139975014192896 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6308872103691101, loss=2.1174280643463135
I0213 07:41:09.605505 139975005800192 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5448846220970154, loss=2.199279308319092
I0213 07:41:44.382640 139975014192896 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.4639641046524048, loss=2.113050699234009
I0213 07:42:19.126603 139975005800192 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6418324112892151, loss=2.107898473739624
I0213 07:42:53.898838 139975014192896 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.869716465473175, loss=2.22859263420105
I0213 07:43:28.694607 139975005800192 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.2780897617340088, loss=2.025336742401123
I0213 07:44:03.449707 139975014192896 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3928472697734833, loss=2.05061411857605
I0213 07:44:38.243679 139975005800192 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.2881445288658142, loss=2.107215642929077
I0213 07:45:13.031307 139975014192896 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.36165934801101685, loss=2.0887300968170166
I0213 07:45:47.812480 139975005800192 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.29565343260765076, loss=2.0977118015289307
I0213 07:46:22.603312 139975014192896 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.3117307424545288, loss=2.0734333992004395
I0213 07:46:57.372237 139975005800192 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3387197256088257, loss=2.140652656555176
I0213 07:47:32.139050 139975014192896 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.47560086846351624, loss=1.9950554370880127
I0213 07:48:06.906362 139975005800192 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.36688706278800964, loss=2.1226677894592285
I0213 07:48:41.714185 139975014192896 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5983918309211731, loss=2.1435348987579346
I0213 07:49:10.974941 140144802662208 spec.py:321] Evaluating on the training split.
I0213 07:49:13.963047 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 07:52:31.297861 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 07:52:33.998867 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 07:55:49.054279 140144802662208 spec.py:349] Evaluating on the test split.
I0213 07:55:51.722781 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 07:58:55.118319 140144802662208 submission_runner.py:408] Time since start: 7619.58s, 	Step: 12086, 	{'train/accuracy': 0.5959599018096924, 'train/loss': 2.1070985794067383, 'train/bleu': 28.17086301658108, 'validation/accuracy': 0.6141027212142944, 'validation/loss': 1.9404385089874268, 'validation/bleu': 24.87685627139683, 'validation/num_examples': 3000, 'test/accuracy': 0.6239033341407776, 'test/loss': 1.8900692462921143, 'test/bleu': 24.094168632677732, 'test/num_examples': 3003, 'score': 4230.6515011787415, 'total_duration': 7619.584226846695, 'accumulated_submission_time': 4230.6515011787415, 'accumulated_eval_time': 3388.406713962555, 'accumulated_logging_time': 0.1255655288696289}
I0213 07:58:55.137042 139975005800192 logging_writer.py:48] [12086] accumulated_eval_time=3388.406714, accumulated_logging_time=0.125566, accumulated_submission_time=4230.651501, global_step=12086, preemption_count=0, score=4230.651501, test/accuracy=0.623903, test/bleu=24.094169, test/loss=1.890069, test/num_examples=3003, total_duration=7619.584227, train/accuracy=0.595960, train/bleu=28.170863, train/loss=2.107099, validation/accuracy=0.614103, validation/bleu=24.876856, validation/loss=1.940439, validation/num_examples=3000
I0213 07:59:00.342559 139975014192896 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.4571816325187683, loss=2.034067153930664
I0213 07:59:34.953510 139975005800192 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.36629733443260193, loss=2.1053953170776367
I0213 08:00:09.677803 139975014192896 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.430728018283844, loss=2.012998580932617
I0213 08:00:44.453483 139975005800192 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5498598217964172, loss=2.0653367042541504
I0213 08:01:19.229406 139975014192896 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5982058048248291, loss=2.1875035762786865
I0213 08:01:53.984762 139975005800192 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.33661898970603943, loss=2.1079351902008057
I0213 08:02:28.759299 139975014192896 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.33918851613998413, loss=2.158596992492676
I0213 08:03:03.544733 139975005800192 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.43533143401145935, loss=2.1202313899993896
I0213 08:03:38.358090 139975014192896 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.35778602957725525, loss=2.023376703262329
I0213 08:04:13.117404 139975005800192 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5308085083961487, loss=2.057863712310791
I0213 08:04:47.909253 139975014192896 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.5937641859054565, loss=2.0909171104431152
I0213 08:05:22.706837 139975005800192 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.29018640518188477, loss=2.0918023586273193
I0213 08:05:57.519979 139975014192896 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6852341890335083, loss=2.0714669227600098
I0213 08:06:32.293469 139975005800192 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.46382588148117065, loss=2.125298261642456
I0213 08:07:07.076946 139975014192896 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6552445888519287, loss=2.0747833251953125
I0213 08:07:41.843883 139975005800192 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.33646005392074585, loss=2.068742036819458
I0213 08:08:16.632757 139975014192896 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5894014239311218, loss=2.10841703414917
I0213 08:08:51.400646 139975005800192 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.3898206055164337, loss=2.006908655166626
I0213 08:09:26.168878 139975014192896 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.3540891706943512, loss=2.1845970153808594
I0213 08:10:00.911027 139975005800192 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.4606407582759857, loss=2.0818052291870117
I0213 08:10:35.676185 139975014192896 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.8261634111404419, loss=2.102440118789673
I0213 08:11:10.443289 139975005800192 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7041885256767273, loss=2.1175081729888916
I0213 08:11:45.216335 139975014192896 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.5473853945732117, loss=2.0179476737976074
I0213 08:12:19.975669 139975005800192 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.30841442942619324, loss=2.088627815246582
I0213 08:12:54.757249 139975014192896 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.2759094834327698, loss=2.0222463607788086
I0213 08:12:55.181841 140144802662208 spec.py:321] Evaluating on the training split.
I0213 08:12:58.151141 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 08:16:42.840123 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 08:16:45.529440 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 08:19:19.872169 140144802662208 spec.py:349] Evaluating on the test split.
I0213 08:19:22.549584 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 08:21:37.807457 140144802662208 submission_runner.py:408] Time since start: 8982.27s, 	Step: 14503, 	{'train/accuracy': 0.6016581058502197, 'train/loss': 2.071608066558838, 'train/bleu': 28.414997950665157, 'validation/accuracy': 0.6191367506980896, 'validation/loss': 1.9150434732437134, 'validation/bleu': 25.375294821756516, 'validation/num_examples': 3000, 'test/accuracy': 0.6236360669136047, 'test/loss': 1.8791215419769287, 'test/bleu': 24.06938099765642, 'test/num_examples': 3003, 'score': 5070.606368780136, 'total_duration': 8982.273378133774, 'accumulated_submission_time': 5070.606368780136, 'accumulated_eval_time': 3911.032277584076, 'accumulated_logging_time': 0.15601110458374023}
I0213 08:21:37.824265 139975005800192 logging_writer.py:48] [14503] accumulated_eval_time=3911.032278, accumulated_logging_time=0.156011, accumulated_submission_time=5070.606369, global_step=14503, preemption_count=0, score=5070.606369, test/accuracy=0.623636, test/bleu=24.069381, test/loss=1.879122, test/num_examples=3003, total_duration=8982.273378, train/accuracy=0.601658, train/bleu=28.414998, train/loss=2.071608, validation/accuracy=0.619137, validation/bleu=25.375295, validation/loss=1.915043, validation/num_examples=3000
I0213 08:22:11.797276 139975014192896 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.33486315608024597, loss=2.097501039505005
I0213 08:22:46.509290 139975005800192 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9375684261322021, loss=2.154831647872925
I0213 08:23:21.303940 139975014192896 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.37796011567115784, loss=2.0690934658050537
I0213 08:23:56.075580 139975005800192 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.30263423919677734, loss=2.1029226779937744
I0213 08:24:30.877695 139975014192896 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.2832165062427521, loss=2.0483546257019043
I0213 08:25:05.642554 139975005800192 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6690593957901001, loss=2.1402323246002197
I0213 08:25:40.407134 139975014192896 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5021425485610962, loss=2.144087553024292
I0213 08:26:15.178864 139975005800192 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.8600654006004333, loss=2.10575270652771
I0213 08:26:49.981502 139975014192896 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.7472630739212036, loss=2.041036605834961
I0213 08:27:24.751426 139975005800192 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.29054543375968933, loss=2.0029001235961914
I0213 08:27:59.586614 139975014192896 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.4402405023574829, loss=2.1108810901641846
I0213 08:28:34.411847 139975005800192 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6476080417633057, loss=2.1608352661132812
I0213 08:29:09.248176 139975014192896 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7671788930892944, loss=2.060598611831665
I0213 08:29:44.065674 139975005800192 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.49785569310188293, loss=2.0993940830230713
I0213 08:30:18.879864 139975014192896 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3235241770744324, loss=1.9721155166625977
I0213 08:30:53.702229 139975005800192 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.4246448874473572, loss=2.0973079204559326
I0213 08:31:28.488282 139975014192896 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3503662049770355, loss=2.0075817108154297
I0213 08:32:03.262024 139975005800192 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3145688772201538, loss=2.0236353874206543
I0213 08:32:38.027490 139975014192896 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2697898745536804, loss=2.007535934448242
I0213 08:33:12.784113 139975005800192 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6083821058273315, loss=2.1411001682281494
I0213 08:33:47.582743 139975014192896 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.41315698623657227, loss=1.9801225662231445
I0213 08:34:22.355533 139975005800192 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5462098717689514, loss=2.142350196838379
I0213 08:34:57.164839 139975014192896 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.35329657793045044, loss=2.026498794555664
I0213 08:35:32.021388 139975005800192 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3944869935512543, loss=2.1192634105682373
I0213 08:35:38.024134 140144802662208 spec.py:321] Evaluating on the training split.
I0213 08:35:41.000074 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 08:39:09.257877 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 08:39:11.925712 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 08:41:56.758438 140144802662208 spec.py:349] Evaluating on the test split.
I0213 08:41:59.441256 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 08:44:29.987559 140144802662208 submission_runner.py:408] Time since start: 10354.45s, 	Step: 16919, 	{'train/accuracy': 0.5999884605407715, 'train/loss': 2.073273181915283, 'train/bleu': 28.36836530080516, 'validation/accuracy': 0.6177108883857727, 'validation/loss': 1.9168777465820312, 'validation/bleu': 25.15550042864411, 'validation/num_examples': 3000, 'test/accuracy': 0.6234152913093567, 'test/loss': 1.8667335510253906, 'test/bleu': 24.182225927018916, 'test/num_examples': 3003, 'score': 5910.714811086655, 'total_duration': 10354.45345211029, 'accumulated_submission_time': 5910.714811086655, 'accumulated_eval_time': 4442.9956386089325, 'accumulated_logging_time': 0.1835007667541504}
I0213 08:44:30.007226 139975014192896 logging_writer.py:48] [16919] accumulated_eval_time=4442.995639, accumulated_logging_time=0.183501, accumulated_submission_time=5910.714811, global_step=16919, preemption_count=0, score=5910.714811, test/accuracy=0.623415, test/bleu=24.182226, test/loss=1.866734, test/num_examples=3003, total_duration=10354.453452, train/accuracy=0.599988, train/bleu=28.368365, train/loss=2.073273, validation/accuracy=0.617711, validation/bleu=25.155500, validation/loss=1.916878, validation/num_examples=3000
I0213 08:44:58.435962 139975005800192 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.582900881767273, loss=2.2097601890563965
I0213 08:45:33.100023 139975014192896 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6786324977874756, loss=2.1293017864227295
I0213 08:46:07.865401 139975005800192 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3465428352355957, loss=2.024731397628784
I0213 08:46:42.642951 139975014192896 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.49707192182540894, loss=2.10064697265625
I0213 08:47:17.410115 139975005800192 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.45463666319847107, loss=2.0235767364501953
I0213 08:47:52.161933 139975014192896 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3400880694389343, loss=2.0744948387145996
I0213 08:48:26.983023 139975005800192 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.2924608886241913, loss=2.1110496520996094
I0213 08:49:01.747307 139975014192896 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.4438633918762207, loss=2.0352094173431396
I0213 08:49:36.525240 139975005800192 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.31427401304244995, loss=2.0470025539398193
I0213 08:50:11.308479 139975014192896 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.48478639125823975, loss=2.022766590118408
I0213 08:50:46.069524 139975005800192 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3023080825805664, loss=2.0568349361419678
I0213 08:51:20.856674 139975014192896 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5089494585990906, loss=2.0069611072540283
I0213 08:51:55.658601 139975005800192 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.46825963258743286, loss=2.1719608306884766
I0213 08:52:30.453278 139975014192896 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.21699059009552002, loss=1.9839147329330444
I0213 08:53:05.210488 139975005800192 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.33166465163230896, loss=2.0546317100524902
I0213 08:53:39.971616 139975014192896 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4387718439102173, loss=2.0720088481903076
I0213 08:54:14.715097 139975005800192 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.7559334635734558, loss=2.0924644470214844
I0213 08:54:49.526801 139975014192896 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.44540107250213623, loss=2.0049564838409424
I0213 08:55:24.275913 139975005800192 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.4494696259498596, loss=2.0981380939483643
I0213 08:55:59.004881 139975014192896 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.2956165671348572, loss=1.977196455001831
I0213 08:56:33.760728 139975005800192 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.4038143754005432, loss=1.9965134859085083
I0213 08:57:08.499358 139975014192896 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.31340542435646057, loss=2.058441638946533
I0213 08:57:43.254624 139975005800192 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5861090421676636, loss=2.038073778152466
I0213 08:58:18.030208 139975014192896 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.27918222546577454, loss=1.962936520576477
I0213 08:58:30.268960 140144802662208 spec.py:321] Evaluating on the training split.
I0213 08:58:33.235828 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 09:01:31.893587 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 09:01:34.576907 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 09:04:30.896094 140144802662208 spec.py:349] Evaluating on the test split.
I0213 09:04:33.573912 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 09:07:18.877968 140144802662208 submission_runner.py:408] Time since start: 11723.34s, 	Step: 19337, 	{'train/accuracy': 0.6061010956764221, 'train/loss': 1.999993920326233, 'train/bleu': 28.632670103777563, 'validation/accuracy': 0.6188639998435974, 'validation/loss': 1.9008632898330688, 'validation/bleu': 24.93843937756793, 'validation/num_examples': 3000, 'test/accuracy': 0.6259601712226868, 'test/loss': 1.8603891134262085, 'test/bleu': 23.853407099789305, 'test/num_examples': 3003, 'score': 6750.888104915619, 'total_duration': 11723.343873977661, 'accumulated_submission_time': 6750.888104915619, 'accumulated_eval_time': 4971.604582309723, 'accumulated_logging_time': 0.21465039253234863}
I0213 09:07:18.896286 139975005800192 logging_writer.py:48] [19337] accumulated_eval_time=4971.604582, accumulated_logging_time=0.214650, accumulated_submission_time=6750.888105, global_step=19337, preemption_count=0, score=6750.888105, test/accuracy=0.625960, test/bleu=23.853407, test/loss=1.860389, test/num_examples=3003, total_duration=11723.343874, train/accuracy=0.606101, train/bleu=28.632670, train/loss=1.999994, validation/accuracy=0.618864, validation/bleu=24.938439, validation/loss=1.900863, validation/num_examples=3000
I0213 09:07:41.085194 139975014192896 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5978598594665527, loss=2.1067466735839844
I0213 09:08:15.753805 139975005800192 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5118564367294312, loss=1.9709324836730957
I0213 09:08:50.512590 139975014192896 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.26323965191841125, loss=2.1528067588806152
I0213 09:09:25.276095 139975005800192 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.4145103693008423, loss=2.0493087768554688
I0213 09:10:00.001108 139975014192896 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.5644285678863525, loss=2.037970542907715
I0213 09:10:34.767384 139975005800192 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3956451416015625, loss=2.1149160861968994
I0213 09:11:09.520859 139975014192896 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.8136594891548157, loss=2.0513217449188232
I0213 09:11:44.328105 139975005800192 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.5771068334579468, loss=1.9654197692871094
I0213 09:12:19.120117 139975014192896 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.2859380543231964, loss=1.9954092502593994
I0213 09:12:53.881390 139975005800192 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.60455721616745, loss=2.097411870956421
I0213 09:13:28.695903 139975014192896 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.694200336933136, loss=2.130758762359619
I0213 09:14:03.496234 139975005800192 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.39545926451683044, loss=2.007111072540283
I0213 09:14:38.285176 139975014192896 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.582935631275177, loss=1.9461085796356201
I0213 09:15:13.048073 139975005800192 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.36901313066482544, loss=1.945069432258606
I0213 09:15:47.824427 139975014192896 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.49970030784606934, loss=2.0548593997955322
I0213 09:16:22.605073 139975005800192 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3266712725162506, loss=2.0239036083221436
I0213 09:16:57.362061 139975014192896 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.26894840598106384, loss=2.0209836959838867
I0213 09:17:32.157569 139975005800192 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4081186056137085, loss=2.0571401119232178
I0213 09:18:06.924880 139975014192896 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.344099760055542, loss=2.092193841934204
I0213 09:18:41.697047 139975005800192 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6059750914573669, loss=2.1012279987335205
I0213 09:19:16.467932 139975014192896 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.44484883546829224, loss=2.09002685546875
I0213 09:19:51.237559 139975005800192 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5621903538703918, loss=2.062021493911743
I0213 09:20:25.989713 139975014192896 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5754510164260864, loss=1.9794964790344238
I0213 09:21:00.742061 139975005800192 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.501415491104126, loss=1.9434771537780762
I0213 09:21:18.897511 140144802662208 spec.py:321] Evaluating on the training split.
I0213 09:21:21.873725 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 09:25:22.013536 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 09:25:24.692141 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 09:28:29.334090 140144802662208 spec.py:349] Evaluating on the test split.
I0213 09:28:32.009081 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 09:31:35.703361 140144802662208 submission_runner.py:408] Time since start: 13180.17s, 	Step: 21754, 	{'train/accuracy': 0.6028353571891785, 'train/loss': 2.0488171577453613, 'train/bleu': 28.765118879859944, 'validation/accuracy': 0.6212570071220398, 'validation/loss': 1.893480896949768, 'validation/bleu': 25.664352122554142, 'validation/num_examples': 3000, 'test/accuracy': 0.6301435232162476, 'test/loss': 1.8423911333084106, 'test/bleu': 24.476060360265514, 'test/num_examples': 3003, 'score': 7590.796914815903, 'total_duration': 13180.169267416, 'accumulated_submission_time': 7590.796914815903, 'accumulated_eval_time': 5588.410370588303, 'accumulated_logging_time': 0.24518036842346191}
I0213 09:31:35.720609 139975014192896 logging_writer.py:48] [21754] accumulated_eval_time=5588.410371, accumulated_logging_time=0.245180, accumulated_submission_time=7590.796915, global_step=21754, preemption_count=0, score=7590.796915, test/accuracy=0.630144, test/bleu=24.476060, test/loss=1.842391, test/num_examples=3003, total_duration=13180.169267, train/accuracy=0.602835, train/bleu=28.765119, train/loss=2.048817, validation/accuracy=0.621257, validation/bleu=25.664352, validation/loss=1.893481, validation/num_examples=3000
I0213 09:31:52.001085 139975005800192 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6314428448677063, loss=2.044769287109375
I0213 09:32:26.651781 139975014192896 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.675927996635437, loss=2.105360507965088
I0213 09:33:01.357049 139975005800192 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.33793389797210693, loss=2.1983773708343506
I0213 09:33:36.096894 139975014192896 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.3891731798648834, loss=2.140331268310547
I0213 09:34:10.830824 139975005800192 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6051576137542725, loss=2.119802713394165
I0213 09:34:45.577764 139975014192896 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.31846657395362854, loss=2.076096773147583
I0213 09:35:20.378104 139975005800192 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.3181591033935547, loss=2.123267650604248
I0213 09:35:55.148140 139975014192896 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.28061196208000183, loss=2.0650877952575684
I0213 09:36:29.953523 139975005800192 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.47498247027397156, loss=2.091275691986084
I0213 09:37:04.722593 139975014192896 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.5466201901435852, loss=2.028778314590454
I0213 09:37:39.496048 139975005800192 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.5121574401855469, loss=2.0057132244110107
I0213 09:38:14.234160 139975014192896 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3919343650341034, loss=2.072674512863159
I0213 09:38:48.976505 139975005800192 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.324957013130188, loss=2.0284178256988525
I0213 09:39:23.748488 139975014192896 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.4932631552219391, loss=2.0247912406921387
I0213 09:39:58.553622 139975005800192 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6816257834434509, loss=2.081815719604492
I0213 09:40:33.339307 139975014192896 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.28765928745269775, loss=2.0343072414398193
I0213 09:41:08.105870 139975005800192 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.3790258467197418, loss=2.0975334644317627
I0213 09:41:42.892015 139975014192896 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.28963497281074524, loss=2.0205531120300293
I0213 09:42:17.636175 139975005800192 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6580408811569214, loss=2.122699499130249
I0213 09:42:52.381338 139975014192896 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.2556631863117218, loss=2.0078325271606445
I0213 09:43:27.168663 139975005800192 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.7358975410461426, loss=2.029905080795288
I0213 09:44:01.922739 139975014192896 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8041979670524597, loss=2.07224178314209
I0213 09:44:36.677897 139975005800192 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2717580497264862, loss=2.0375468730926514
I0213 09:45:11.449800 139975014192896 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.3267843425273895, loss=2.0456295013427734
I0213 09:45:35.853717 140144802662208 spec.py:321] Evaluating on the training split.
I0213 09:45:38.809708 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 09:48:58.502809 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 09:49:01.174401 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 09:52:29.756925 140144802662208 spec.py:349] Evaluating on the test split.
I0213 09:52:32.446584 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 09:55:25.774084 140144802662208 submission_runner.py:408] Time since start: 14610.24s, 	Step: 24172, 	{'train/accuracy': 0.6010239124298096, 'train/loss': 2.058652639389038, 'train/bleu': 28.34594869682518, 'validation/accuracy': 0.6217281818389893, 'validation/loss': 1.8820874691009521, 'validation/bleu': 25.278200789983146, 'validation/num_examples': 3000, 'test/accuracy': 0.626308798789978, 'test/loss': 1.852922797203064, 'test/bleu': 24.137148784396878, 'test/num_examples': 3003, 'score': 8430.839210271835, 'total_duration': 14610.239954471588, 'accumulated_submission_time': 8430.839210271835, 'accumulated_eval_time': 6178.330642223358, 'accumulated_logging_time': 0.27239227294921875}
I0213 09:55:25.795886 139975005800192 logging_writer.py:48] [24172] accumulated_eval_time=6178.330642, accumulated_logging_time=0.272392, accumulated_submission_time=8430.839210, global_step=24172, preemption_count=0, score=8430.839210, test/accuracy=0.626309, test/bleu=24.137149, test/loss=1.852923, test/num_examples=3003, total_duration=14610.239954, train/accuracy=0.601024, train/bleu=28.345949, train/loss=2.058653, validation/accuracy=0.621728, validation/bleu=25.278201, validation/loss=1.882087, validation/num_examples=3000
I0213 09:55:35.844436 139975014192896 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.5107360482215881, loss=2.0497946739196777
I0213 09:56:10.468060 139975005800192 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.47121310234069824, loss=2.0484354496002197
I0213 09:56:45.188822 139975014192896 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4273267984390259, loss=1.9628868103027344
I0213 09:57:19.984096 139975005800192 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3381640613079071, loss=1.9805018901824951
I0213 09:57:54.734508 139975014192896 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5765242576599121, loss=2.0819754600524902
I0213 09:58:29.508082 139975005800192 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.3079897463321686, loss=1.9923427104949951
I0213 09:59:04.298972 139975014192896 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.39369675517082214, loss=2.016113758087158
I0213 09:59:39.074404 139975005800192 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.5542425513267517, loss=2.09993577003479
I0213 10:00:13.854202 139975014192896 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.39926183223724365, loss=1.93975031375885
I0213 10:00:48.629155 139975005800192 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.46491187810897827, loss=2.061262845993042
I0213 10:01:23.377063 139975014192896 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.44497546553611755, loss=2.1014983654022217
I0213 10:01:58.123106 139975005800192 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.45004841685295105, loss=2.0007057189941406
I0213 10:02:32.900815 139975014192896 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2734430730342865, loss=1.9955835342407227
I0213 10:03:07.660416 139975005800192 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.3812960982322693, loss=1.9545514583587646
I0213 10:03:42.434963 139975014192896 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.5638188123703003, loss=2.003572702407837
I0213 10:04:17.218710 139975005800192 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.3239632844924927, loss=2.0163612365722656
I0213 10:04:52.025801 139975014192896 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.3716822862625122, loss=2.086134672164917
I0213 10:05:26.824738 139975005800192 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.7087125182151794, loss=1.9726507663726807
I0213 10:06:01.601290 139975014192896 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.4763982892036438, loss=2.0373027324676514
I0213 10:06:36.354979 139975005800192 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.673903226852417, loss=2.0714900493621826
I0213 10:07:11.113558 139975014192896 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.783788800239563, loss=2.075739622116089
I0213 10:07:45.886615 139975005800192 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6120814085006714, loss=2.0361850261688232
I0213 10:08:20.670272 139975014192896 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.4661593437194824, loss=2.0201289653778076
I0213 10:08:55.432376 139975005800192 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.26631465554237366, loss=2.024348497390747
I0213 10:09:26.095774 140144802662208 spec.py:321] Evaluating on the training split.
I0213 10:09:29.063674 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 10:12:14.625747 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 10:12:17.305646 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 10:14:57.266725 140144802662208 spec.py:349] Evaluating on the test split.
I0213 10:14:59.947454 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 10:17:22.908951 140144802662208 submission_runner.py:408] Time since start: 15927.37s, 	Step: 26590, 	{'train/accuracy': 0.6073386073112488, 'train/loss': 2.0060691833496094, 'train/bleu': 28.424016769886777, 'validation/accuracy': 0.6224597096443176, 'validation/loss': 1.8667932748794556, 'validation/bleu': 25.240027278392578, 'validation/num_examples': 3000, 'test/accuracy': 0.6293068528175354, 'test/loss': 1.8353381156921387, 'test/bleu': 24.118035745694435, 'test/num_examples': 3003, 'score': 9271.049669265747, 'total_duration': 15927.374853849411, 'accumulated_submission_time': 9271.049669265747, 'accumulated_eval_time': 6655.143758058548, 'accumulated_logging_time': 0.3059818744659424}
I0213 10:17:22.928385 139975014192896 logging_writer.py:48] [26590] accumulated_eval_time=6655.143758, accumulated_logging_time=0.305982, accumulated_submission_time=9271.049669, global_step=26590, preemption_count=0, score=9271.049669, test/accuracy=0.629307, test/bleu=24.118036, test/loss=1.835338, test/num_examples=3003, total_duration=15927.374854, train/accuracy=0.607339, train/bleu=28.424017, train/loss=2.006069, validation/accuracy=0.622460, validation/bleu=25.240027, validation/loss=1.866793, validation/num_examples=3000
I0213 10:17:26.768478 139975005800192 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.3679954707622528, loss=2.044342517852783
I0213 10:18:01.429135 139975014192896 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.43001842498779297, loss=1.974010944366455
I0213 10:18:36.148404 139975005800192 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.25691691040992737, loss=2.025502920150757
I0213 10:19:10.901034 139975014192896 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.4685881435871124, loss=2.0318315029144287
I0213 10:19:45.661783 139975005800192 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.42370671033859253, loss=2.035439968109131
I0213 10:20:20.412690 139975014192896 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.41546520590782166, loss=1.9815112352371216
I0213 10:20:55.182481 139975005800192 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3755454421043396, loss=2.049875020980835
I0213 10:21:29.953659 139975014192896 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5871642231941223, loss=2.064699172973633
I0213 10:22:04.726402 139975005800192 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.3550320863723755, loss=1.9954975843429565
I0213 10:22:39.521065 139975014192896 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.35116496682167053, loss=2.1107988357543945
I0213 10:23:14.300798 139975005800192 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.2709612548351288, loss=2.0606794357299805
I0213 10:23:49.058803 139975014192896 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.44706621766090393, loss=2.0988845825195312
I0213 10:24:23.830443 139975005800192 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.47154471278190613, loss=2.017094612121582
I0213 10:24:58.619025 139975014192896 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.4843484163284302, loss=2.066648244857788
I0213 10:25:33.390666 139975005800192 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5806096196174622, loss=2.070730447769165
I0213 10:26:08.154304 139975014192896 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.6558418869972229, loss=2.0578689575195312
I0213 10:26:42.926508 139975005800192 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.32799479365348816, loss=1.9552946090698242
I0213 10:27:17.698160 139975014192896 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.4441787302494049, loss=1.9237054586410522
I0213 10:27:52.448900 139975005800192 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.3794269859790802, loss=2.030338764190674
I0213 10:28:27.230511 139975014192896 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.40205642580986023, loss=2.0420165061950684
I0213 10:29:02.032654 139975005800192 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.42056769132614136, loss=1.9284706115722656
I0213 10:29:36.835840 139975014192896 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.282617449760437, loss=2.0486035346984863
I0213 10:30:11.610686 139975005800192 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.49476784467697144, loss=2.0107319355010986
I0213 10:30:46.404788 139975014192896 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.2971743941307068, loss=2.0598878860473633
I0213 10:31:21.224207 139975005800192 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.5128625631332397, loss=2.0845236778259277
I0213 10:31:23.045884 140144802662208 spec.py:321] Evaluating on the training split.
I0213 10:31:26.022897 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 10:34:23.665543 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 10:34:26.342719 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 10:37:24.430840 140144802662208 spec.py:349] Evaluating on the test split.
I0213 10:37:27.100419 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 10:40:07.151439 140144802662208 submission_runner.py:408] Time since start: 17291.62s, 	Step: 29007, 	{'train/accuracy': 0.601288914680481, 'train/loss': 2.059950113296509, 'train/bleu': 28.236495801272138, 'validation/accuracy': 0.6220133304595947, 'validation/loss': 1.878191590309143, 'validation/bleu': 24.748724901143305, 'validation/num_examples': 3000, 'test/accuracy': 0.6305851340293884, 'test/loss': 1.827478289604187, 'test/bleu': 23.666496475395746, 'test/num_examples': 3003, 'score': 10111.076085329056, 'total_duration': 17291.617335796356, 'accumulated_submission_time': 10111.076085329056, 'accumulated_eval_time': 7179.2492508888245, 'accumulated_logging_time': 0.33559679985046387}
I0213 10:40:07.170700 139975014192896 logging_writer.py:48] [29007] accumulated_eval_time=7179.249251, accumulated_logging_time=0.335597, accumulated_submission_time=10111.076085, global_step=29007, preemption_count=0, score=10111.076085, test/accuracy=0.630585, test/bleu=23.666496, test/loss=1.827478, test/num_examples=3003, total_duration=17291.617336, train/accuracy=0.601289, train/bleu=28.236496, train/loss=2.059950, validation/accuracy=0.622013, validation/bleu=24.748725, validation/loss=1.878192, validation/num_examples=3000
I0213 10:40:39.700422 139975005800192 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.24996498227119446, loss=2.0170178413391113
I0213 10:41:14.393611 139975014192896 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6496458053588867, loss=2.0359764099121094
I0213 10:41:49.122709 139975005800192 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.4580816924571991, loss=2.072026491165161
I0213 10:42:23.854295 139975014192896 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.0089566707611084, loss=2.0499637126922607
I0213 10:42:58.616804 139975005800192 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2947690188884735, loss=1.9115490913391113
I0213 10:43:33.362060 139975014192896 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.3550044596195221, loss=1.9680407047271729
I0213 10:44:08.131644 139975005800192 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.3190264403820038, loss=2.0861165523529053
I0213 10:44:42.890135 139975014192896 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.3000333607196808, loss=2.055846691131592
I0213 10:45:17.639026 139975005800192 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.45201486349105835, loss=2.0337111949920654
I0213 10:45:52.425336 139975014192896 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.4692956805229187, loss=2.020634889602661
I0213 10:46:27.172929 139975005800192 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.4481421113014221, loss=2.0254569053649902
I0213 10:47:01.932602 139975014192896 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.8698949217796326, loss=2.0373945236206055
I0213 10:47:36.695619 139975005800192 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.2625821828842163, loss=1.982025146484375
I0213 10:48:11.447121 139975014192896 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.37917575240135193, loss=1.9897102117538452
I0213 10:48:46.199846 139975005800192 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.507256805896759, loss=2.007388114929199
I0213 10:49:20.911834 139975014192896 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.40041059255599976, loss=2.0433521270751953
I0213 10:49:55.666040 139975005800192 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.2759218215942383, loss=1.9965206384658813
I0213 10:50:30.417279 139975014192896 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.33147966861724854, loss=1.9680269956588745
I0213 10:51:05.191548 139975005800192 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.31084585189819336, loss=1.9668524265289307
I0213 10:51:39.940749 139975014192896 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.24649015069007874, loss=1.9286249876022339
I0213 10:52:14.720689 139975005800192 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.2641066014766693, loss=1.99669349193573
I0213 10:52:49.527887 139975014192896 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.31676673889160156, loss=2.0513384342193604
I0213 10:53:24.277685 139975005800192 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.45001399517059326, loss=2.0575623512268066
I0213 10:53:59.042453 139975014192896 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.2721225321292877, loss=2.0545077323913574
I0213 10:54:07.459603 140144802662208 spec.py:321] Evaluating on the training split.
I0213 10:54:10.427823 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 10:56:49.552951 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 10:56:52.231533 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 10:59:29.531352 140144802662208 spec.py:349] Evaluating on the test split.
I0213 10:59:32.202700 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 11:01:48.343953 140144802662208 submission_runner.py:408] Time since start: 18592.81s, 	Step: 31426, 	{'train/accuracy': 0.6311168670654297, 'train/loss': 1.7990312576293945, 'train/bleu': 29.951347937008773, 'validation/accuracy': 0.6247039437294006, 'validation/loss': 1.8656679391860962, 'validation/bleu': 25.24824983902175, 'validation/num_examples': 3000, 'test/accuracy': 0.6326651573181152, 'test/loss': 1.811199426651001, 'test/bleu': 24.274575200682374, 'test/num_examples': 3003, 'score': 10951.275916576385, 'total_duration': 18592.80987429619, 'accumulated_submission_time': 10951.275916576385, 'accumulated_eval_time': 7640.133558750153, 'accumulated_logging_time': 0.3651926517486572}
I0213 11:01:48.362946 139975005800192 logging_writer.py:48] [31426] accumulated_eval_time=7640.133559, accumulated_logging_time=0.365193, accumulated_submission_time=10951.275917, global_step=31426, preemption_count=0, score=10951.275917, test/accuracy=0.632665, test/bleu=24.274575, test/loss=1.811199, test/num_examples=3003, total_duration=18592.809874, train/accuracy=0.631117, train/bleu=29.951348, train/loss=1.799031, validation/accuracy=0.624704, validation/bleu=25.248250, validation/loss=1.865668, validation/num_examples=3000
I0213 11:02:14.350136 139975014192896 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3197987675666809, loss=2.039975166320801
I0213 11:02:49.025051 139975005800192 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.23338788747787476, loss=1.966153621673584
I0213 11:03:23.774407 139975014192896 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6389555335044861, loss=2.078977108001709
I0213 11:03:58.525927 139975005800192 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.3211933374404907, loss=2.0750253200531006
I0213 11:04:33.319723 139975014192896 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.28926557302474976, loss=2.0664799213409424
I0213 11:05:08.094334 139975005800192 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.31893685460090637, loss=2.043513536453247
I0213 11:05:42.856633 139975014192896 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.3694941997528076, loss=2.057231903076172
I0213 11:06:17.617572 139975005800192 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6795908808708191, loss=2.0170724391937256
I0213 11:06:52.361980 139975014192896 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5546234250068665, loss=1.9285036325454712
I0213 11:07:27.130441 139975005800192 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.38968196511268616, loss=2.0469725131988525
I0213 11:08:01.894580 139975014192896 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.3182673156261444, loss=1.8645286560058594
I0213 11:08:36.650734 139975005800192 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.5846090316772461, loss=2.0340397357940674
I0213 11:09:11.439651 139975014192896 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.4350658655166626, loss=2.0536701679229736
I0213 11:09:46.217596 139975005800192 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.46947404742240906, loss=2.114509105682373
I0213 11:10:20.999717 139975014192896 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6601080894470215, loss=1.9858958721160889
I0213 11:10:55.786900 139975005800192 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.26071104407310486, loss=1.9274787902832031
I0213 11:11:30.681756 139975014192896 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.4032689929008484, loss=2.0287716388702393
I0213 11:12:05.494867 139975005800192 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.4702882170677185, loss=2.0287787914276123
I0213 11:12:40.296079 139975014192896 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.6482404470443726, loss=1.921805500984192
I0213 11:13:15.066666 139975005800192 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.5009099245071411, loss=2.0401337146759033
I0213 11:13:49.835368 139975014192896 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.543433427810669, loss=1.9993066787719727
I0213 11:14:24.604371 139975005800192 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.30956557393074036, loss=2.0750575065612793
I0213 11:14:59.392179 139975014192896 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.6956283450126648, loss=2.0669732093811035
I0213 11:15:34.173695 139975005800192 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.4075416624546051, loss=2.0097429752349854
I0213 11:15:48.510360 140144802662208 spec.py:321] Evaluating on the training split.
I0213 11:15:51.480820 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 11:18:59.814522 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 11:19:02.529042 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 11:21:41.217690 140144802662208 spec.py:349] Evaluating on the test split.
I0213 11:21:43.895229 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 11:24:14.595907 140144802662208 submission_runner.py:408] Time since start: 19939.06s, 	Step: 33843, 	{'train/accuracy': 0.608905017375946, 'train/loss': 2.006382942199707, 'train/bleu': 28.668787281923468, 'validation/accuracy': 0.6290560364723206, 'validation/loss': 1.8475347757339478, 'validation/bleu': 26.221402628623725, 'validation/num_examples': 3000, 'test/accuracy': 0.6364302039146423, 'test/loss': 1.796127200126648, 'test/bleu': 24.618080302001683, 'test/num_examples': 3003, 'score': 11791.334715604782, 'total_duration': 19939.061821222305, 'accumulated_submission_time': 11791.334715604782, 'accumulated_eval_time': 8146.219051361084, 'accumulated_logging_time': 0.3942883014678955}
I0213 11:24:14.616198 139975014192896 logging_writer.py:48] [33843] accumulated_eval_time=8146.219051, accumulated_logging_time=0.394288, accumulated_submission_time=11791.334716, global_step=33843, preemption_count=0, score=11791.334716, test/accuracy=0.636430, test/bleu=24.618080, test/loss=1.796127, test/num_examples=3003, total_duration=19939.061821, train/accuracy=0.608905, train/bleu=28.668787, train/loss=2.006383, validation/accuracy=0.629056, validation/bleu=26.221403, validation/loss=1.847535, validation/num_examples=3000
I0213 11:24:34.712683 139975005800192 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5684729814529419, loss=2.067195415496826
I0213 11:25:09.347289 139975014192896 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.4753338098526001, loss=1.9964288473129272
I0213 11:25:44.094211 139975005800192 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.3657938241958618, loss=2.0725293159484863
I0213 11:26:18.839454 139975014192896 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.31354665756225586, loss=2.0210909843444824
I0213 11:26:53.584397 139975005800192 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.4131146967411041, loss=2.0253047943115234
I0213 11:27:28.328369 139975014192896 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.5411667823791504, loss=1.9626134634017944
I0213 11:28:03.094319 139975005800192 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.28246375918388367, loss=1.9881223440170288
I0213 11:28:37.836691 139975014192896 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.2846281826496124, loss=2.0121119022369385
I0213 11:29:12.596713 139975005800192 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3608456254005432, loss=1.9877785444259644
I0213 11:29:47.381208 139975014192896 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.2694196403026581, loss=1.8700881004333496
I0213 11:30:22.175049 139975005800192 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3670891523361206, loss=1.9922655820846558
I0213 11:30:56.932069 139975014192896 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.3132351040840149, loss=1.9587640762329102
I0213 11:31:31.683172 139975005800192 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.37642550468444824, loss=1.9787635803222656
I0213 11:32:06.451765 139975014192896 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.3384178876876831, loss=2.011303424835205
I0213 11:32:41.217511 139975005800192 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.2637103199958801, loss=1.9184142351150513
I0213 11:33:15.972173 139975014192896 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.2692091763019562, loss=2.0314221382141113
I0213 11:33:50.729226 139975005800192 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.5315358638763428, loss=2.039909601211548
I0213 11:34:25.504456 139975014192896 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.43960505723953247, loss=2.004882574081421
I0213 11:35:00.263051 139975005800192 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.44944652915000916, loss=1.9093071222305298
I0213 11:35:35.011486 139975014192896 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.4411422610282898, loss=2.0467851161956787
I0213 11:36:09.771424 139975005800192 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.4891713559627533, loss=2.0728821754455566
I0213 11:36:44.568472 139975014192896 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6161522269248962, loss=1.9389616250991821
I0213 11:37:19.346259 139975005800192 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.3313063979148865, loss=1.9852170944213867
I0213 11:37:54.107110 139975014192896 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.41024842858314514, loss=1.9834822416305542
I0213 11:38:14.678472 140144802662208 spec.py:321] Evaluating on the training split.
I0213 11:38:17.643960 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 11:41:14.261948 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 11:41:16.930471 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 11:44:16.710758 140144802662208 spec.py:349] Evaluating on the test split.
I0213 11:44:19.383636 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 11:47:07.636525 140144802662208 submission_runner.py:408] Time since start: 21312.10s, 	Step: 36261, 	{'train/accuracy': 0.6050881147384644, 'train/loss': 2.023306131362915, 'train/bleu': 28.62304099873272, 'validation/accuracy': 0.6242327690124512, 'validation/loss': 1.8724839687347412, 'validation/bleu': 25.53469168614256, 'validation/num_examples': 3000, 'test/accuracy': 0.6352681517601013, 'test/loss': 1.8075774908065796, 'test/bleu': 25.250241402682335, 'test/num_examples': 3003, 'score': 12631.307217121124, 'total_duration': 21312.102447271347, 'accumulated_submission_time': 12631.307217121124, 'accumulated_eval_time': 8679.177055120468, 'accumulated_logging_time': 0.4261302947998047}
I0213 11:47:07.655897 139975005800192 logging_writer.py:48] [36261] accumulated_eval_time=8679.177055, accumulated_logging_time=0.426130, accumulated_submission_time=12631.307217, global_step=36261, preemption_count=0, score=12631.307217, test/accuracy=0.635268, test/bleu=25.250241, test/loss=1.807577, test/num_examples=3003, total_duration=21312.102447, train/accuracy=0.605088, train/bleu=28.623041, train/loss=2.023306, validation/accuracy=0.624233, validation/bleu=25.534692, validation/loss=1.872484, validation/num_examples=3000
I0213 11:47:21.512019 139975014192896 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.5071762800216675, loss=1.9701670408248901
I0213 11:47:56.147212 139975005800192 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.26292282342910767, loss=1.9572097063064575
I0213 11:48:30.903629 139975014192896 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.3295641243457794, loss=1.940243124961853
I0213 11:49:05.661458 139975005800192 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.5642104148864746, loss=1.9363890886306763
I0213 11:49:40.421187 139975014192896 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.26541072130203247, loss=1.9252002239227295
I0213 11:50:15.187211 139975005800192 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.3597411513328552, loss=2.012239933013916
I0213 11:50:49.989969 139975014192896 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.9603702425956726, loss=2.0709128379821777
I0213 11:51:24.774435 139975005800192 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.35257527232170105, loss=1.9622466564178467
I0213 11:51:59.563755 139975014192896 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.7986451983451843, loss=2.027374744415283
I0213 11:52:34.496620 139975005800192 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.5103903412818909, loss=1.9804421663284302
I0213 11:53:09.267994 139975014192896 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.30535343289375305, loss=2.0473923683166504
I0213 11:53:44.022009 139975005800192 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.32436493039131165, loss=1.9828954935073853
I0213 11:54:18.799676 139975014192896 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.2457178831100464, loss=1.9893254041671753
I0213 11:54:53.577112 139975005800192 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6444658041000366, loss=2.0072505474090576
I0213 11:55:28.331814 139975014192896 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.5921390652656555, loss=1.938266396522522
I0213 11:56:03.094838 139975005800192 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.3207629323005676, loss=1.9155211448669434
I0213 11:56:37.866412 139975014192896 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.34497207403182983, loss=2.0821523666381836
I0213 11:57:12.658240 139975005800192 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.29381588101387024, loss=1.9763789176940918
I0213 11:57:47.466276 139975014192896 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.44466322660446167, loss=2.0089375972747803
I0213 11:58:22.260221 139975005800192 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.5237789750099182, loss=1.9135762453079224
I0213 11:58:57.067664 139975014192896 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.5979949235916138, loss=1.9313467741012573
I0213 11:59:31.860599 139975005800192 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.32682859897613525, loss=2.0027248859405518
I0213 12:00:06.627985 139975014192896 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.3834305703639984, loss=1.9716864824295044
I0213 12:00:41.407934 139975005800192 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.2410259246826172, loss=1.9053220748901367
I0213 12:01:07.901000 140144802662208 spec.py:321] Evaluating on the training split.
I0213 12:01:10.862561 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 12:04:01.941240 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 12:04:04.628479 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 12:06:44.515274 140144802662208 spec.py:349] Evaluating on the test split.
I0213 12:06:47.197428 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 12:09:26.289571 140144802662208 submission_runner.py:408] Time since start: 22650.76s, 	Step: 38678, 	{'train/accuracy': 0.6133084893226624, 'train/loss': 1.960720181465149, 'train/bleu': 29.088107626615397, 'validation/accuracy': 0.6289196610450745, 'validation/loss': 1.840861201286316, 'validation/bleu': 25.56870350178375, 'validation/num_examples': 3000, 'test/accuracy': 0.6340363621711731, 'test/loss': 1.7975412607192993, 'test/bleu': 24.727151582813146, 'test/num_examples': 3003, 'score': 13471.460614442825, 'total_duration': 22650.755472898483, 'accumulated_submission_time': 13471.460614442825, 'accumulated_eval_time': 9177.565561771393, 'accumulated_logging_time': 0.4557979106903076}
I0213 12:09:26.309878 139975014192896 logging_writer.py:48] [38678] accumulated_eval_time=9177.565562, accumulated_logging_time=0.455798, accumulated_submission_time=13471.460614, global_step=38678, preemption_count=0, score=13471.460614, test/accuracy=0.634036, test/bleu=24.727152, test/loss=1.797541, test/num_examples=3003, total_duration=22650.755473, train/accuracy=0.613308, train/bleu=29.088108, train/loss=1.960720, validation/accuracy=0.628920, validation/bleu=25.568704, validation/loss=1.840861, validation/num_examples=3000
I0213 12:09:34.294787 139975005800192 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.4532955288887024, loss=1.957485318183899
I0213 12:10:08.953573 139975014192896 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.6914609670639038, loss=2.0270512104034424
I0213 12:10:43.687268 139975005800192 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.5364718437194824, loss=1.9681600332260132
I0213 12:11:18.447871 139975014192896 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.2780734896659851, loss=1.9958834648132324
I0213 12:11:53.220492 139975005800192 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.4866805970668793, loss=1.9395129680633545
I0213 12:12:27.953018 139975014192896 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.35427960753440857, loss=2.025301694869995
I0213 12:13:02.748997 139975005800192 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.3429502248764038, loss=1.9436659812927246
I0213 12:13:37.525980 139975014192896 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.6045936346054077, loss=1.9539690017700195
I0213 12:14:12.267686 139975005800192 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.28940120339393616, loss=1.9979113340377808
I0213 12:14:47.010841 139975014192896 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.9005462527275085, loss=1.9850232601165771
I0213 12:15:21.785586 139975005800192 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.4459347128868103, loss=2.050480842590332
I0213 12:15:56.541061 139975014192896 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.38610878586769104, loss=2.037364959716797
I0213 12:16:31.312744 139975005800192 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7740857005119324, loss=2.014890432357788
I0213 12:17:06.063211 139975014192896 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.6519572138786316, loss=1.9688528776168823
I0213 12:17:40.812839 139975005800192 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.38540977239608765, loss=1.9462624788284302
I0213 12:18:15.546414 139975014192896 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3262486755847931, loss=2.0246620178222656
I0213 12:18:50.501780 139975005800192 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.1736239194869995, loss=1.9470919370651245
I0213 12:19:25.286890 139975014192896 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.38023555278778076, loss=1.9962197542190552
I0213 12:20:00.055766 139975005800192 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.1346449851989746, loss=2.045187473297119
I0213 12:20:34.812753 139975014192896 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.33722105622291565, loss=1.9600540399551392
I0213 12:21:09.553793 139975005800192 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.6017665863037109, loss=1.9762214422225952
I0213 12:21:44.295683 139975014192896 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.6223713755607605, loss=1.9954262971878052
I0213 12:22:19.060026 139975005800192 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.6602895855903625, loss=2.074742078781128
I0213 12:22:53.816512 139975014192896 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.42246735095977783, loss=1.9665892124176025
I0213 12:23:26.559695 140144802662208 spec.py:321] Evaluating on the training split.
I0213 12:23:29.524383 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 12:26:31.953130 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 12:26:34.628515 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 12:29:09.321226 140144802662208 spec.py:349] Evaluating on the test split.
I0213 12:29:12.006520 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 12:31:46.766235 140144802662208 submission_runner.py:408] Time since start: 23991.23s, 	Step: 41096, 	{'train/accuracy': 0.6108484864234924, 'train/loss': 1.9878480434417725, 'train/bleu': 28.27439783242662, 'validation/accuracy': 0.6305067539215088, 'validation/loss': 1.8244279623031616, 'validation/bleu': 25.164706094613813, 'validation/num_examples': 3000, 'test/accuracy': 0.6390215754508972, 'test/loss': 1.7725989818572998, 'test/bleu': 24.106862780559368, 'test/num_examples': 3003, 'score': 14311.619910955429, 'total_duration': 23991.232154369354, 'accumulated_submission_time': 14311.619910955429, 'accumulated_eval_time': 9677.77205824852, 'accumulated_logging_time': 0.48833751678466797}
I0213 12:31:46.786469 139975005800192 logging_writer.py:48] [41096] accumulated_eval_time=9677.772058, accumulated_logging_time=0.488338, accumulated_submission_time=14311.619911, global_step=41096, preemption_count=0, score=14311.619911, test/accuracy=0.639022, test/bleu=24.106863, test/loss=1.772599, test/num_examples=3003, total_duration=23991.232154, train/accuracy=0.610848, train/bleu=28.274398, train/loss=1.987848, validation/accuracy=0.630507, validation/bleu=25.164706, validation/loss=1.824428, validation/num_examples=3000
I0213 12:31:48.541659 139975014192896 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5286518931388855, loss=1.9628568887710571
I0213 12:32:23.133279 139975005800192 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.30631136894226074, loss=2.012491464614868
I0213 12:32:57.811578 139975014192896 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.42554590106010437, loss=2.014465808868408
I0213 12:33:32.612988 139975005800192 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.33454838395118713, loss=1.997637152671814
I0213 12:34:07.350682 139975014192896 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8975090980529785, loss=1.923309326171875
I0213 12:34:42.146039 139975005800192 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6858968138694763, loss=2.0953588485717773
I0213 12:35:16.918223 139975014192896 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.385642409324646, loss=1.9688873291015625
I0213 12:35:51.742517 139975005800192 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.49514874815940857, loss=1.9946597814559937
I0213 12:36:26.539571 139975014192896 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3164459466934204, loss=1.9420043230056763
I0213 12:37:01.291217 139975005800192 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.4943602979183197, loss=1.9695104360580444
I0213 12:37:36.091889 139975014192896 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.2941901385784149, loss=1.9215922355651855
I0213 12:38:10.909749 139975005800192 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.33942463994026184, loss=1.938011646270752
I0213 12:38:45.693963 139975014192896 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.2614317536354065, loss=1.919804573059082
I0213 12:39:20.501224 139975005800192 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.3356570303440094, loss=1.9512394666671753
I0213 12:39:55.240591 139975014192896 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.30077460408210754, loss=1.979626178741455
I0213 12:40:29.987578 139975005800192 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.6712267994880676, loss=2.0209970474243164
I0213 12:41:04.775131 139975014192896 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.3858071267604828, loss=2.0316483974456787
I0213 12:41:39.552885 139975005800192 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.4525734484195709, loss=1.927819013595581
I0213 12:42:14.337244 139975014192896 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.33184871077537537, loss=1.962282657623291
I0213 12:42:49.094743 139975005800192 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9731612801551819, loss=1.9488122463226318
I0213 12:43:23.897726 139975014192896 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.7273615598678589, loss=1.9877350330352783
I0213 12:43:58.649151 139975005800192 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.5133412480354309, loss=1.9649543762207031
I0213 12:44:33.413977 139975014192896 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.8448641896247864, loss=2.0746383666992188
I0213 12:45:08.182578 139975005800192 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.2725382447242737, loss=1.9565001726150513
I0213 12:45:42.972044 139975014192896 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.28591102361679077, loss=1.8943923711776733
I0213 12:45:46.876518 140144802662208 spec.py:321] Evaluating on the training split.
I0213 12:45:49.846228 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 12:49:06.818317 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 12:49:09.495828 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 12:52:04.255614 140144802662208 spec.py:349] Evaluating on the test split.
I0213 12:52:06.941951 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 12:54:50.040282 140144802662208 submission_runner.py:408] Time since start: 25374.51s, 	Step: 43513, 	{'train/accuracy': 0.607908308506012, 'train/loss': 2.006985902786255, 'train/bleu': 29.054417956878826, 'validation/accuracy': 0.6317094564437866, 'validation/loss': 1.8237781524658203, 'validation/bleu': 25.956385935198032, 'validation/num_examples': 3000, 'test/accuracy': 0.6409505605697632, 'test/loss': 1.7720146179199219, 'test/bleu': 25.588925953245663, 'test/num_examples': 3003, 'score': 15151.617607355118, 'total_duration': 25374.506201982498, 'accumulated_submission_time': 15151.617607355118, 'accumulated_eval_time': 10220.935767889023, 'accumulated_logging_time': 0.5191919803619385}
I0213 12:54:50.060638 139975005800192 logging_writer.py:48] [43513] accumulated_eval_time=10220.935768, accumulated_logging_time=0.519192, accumulated_submission_time=15151.617607, global_step=43513, preemption_count=0, score=15151.617607, test/accuracy=0.640951, test/bleu=25.588926, test/loss=1.772015, test/num_examples=3003, total_duration=25374.506202, train/accuracy=0.607908, train/bleu=29.054418, train/loss=2.006986, validation/accuracy=0.631709, validation/bleu=25.956386, validation/loss=1.823778, validation/num_examples=3000
I0213 12:55:20.552202 139975014192896 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.3052535951137543, loss=1.9495712518692017
I0213 12:55:55.230299 139975005800192 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.49765631556510925, loss=1.9916296005249023
I0213 12:56:29.980292 139975014192896 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.3487609624862671, loss=2.0973925590515137
I0213 12:57:04.766312 139975005800192 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3115115761756897, loss=1.9762316942214966
I0213 12:57:39.535660 139975014192896 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.4863801896572113, loss=1.970502495765686
I0213 12:58:14.316050 139975005800192 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.4142425060272217, loss=2.026704788208008
I0213 12:58:49.091362 139975014192896 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.6401361227035522, loss=1.998942494392395
I0213 12:59:23.881740 139975005800192 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2822754979133606, loss=2.0267975330352783
I0213 12:59:58.652412 139975014192896 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.3034573793411255, loss=1.8684576749801636
I0213 13:00:33.486200 139975005800192 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.34555259346961975, loss=1.9893702268600464
I0213 13:01:08.286875 139975014192896 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.40377792716026306, loss=1.8668550252914429
I0213 13:01:43.053735 139975005800192 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.5939951539039612, loss=1.9933382272720337
I0213 13:02:17.811830 139975014192896 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.4323733150959015, loss=1.9646350145339966
I0213 13:02:52.593902 139975005800192 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.2883046567440033, loss=2.1046369075775146
I0213 13:03:27.364001 139975014192896 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3832237124443054, loss=1.9376521110534668
I0213 13:04:02.146848 139975005800192 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.36461591720581055, loss=1.9505960941314697
I0213 13:04:36.976635 139975014192896 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.3635592460632324, loss=1.9088844060897827
I0213 13:05:11.782434 139975005800192 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.3932679295539856, loss=1.9364874362945557
I0213 13:05:46.555307 139975014192896 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.38342729210853577, loss=2.0371358394622803
I0213 13:06:21.355491 139975005800192 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2607751190662384, loss=1.9950807094573975
I0213 13:06:56.142589 139975014192896 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.27902570366859436, loss=1.9137028455734253
I0213 13:07:30.906139 139975005800192 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.2902204990386963, loss=1.9129964113235474
I0213 13:08:05.650284 139975014192896 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.22808592021465302, loss=1.8449795246124268
I0213 13:08:40.436145 139975005800192 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.23331700265407562, loss=1.9652718305587769
I0213 13:08:50.242167 140144802662208 spec.py:321] Evaluating on the training split.
I0213 13:08:53.207204 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 13:12:18.251072 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 13:12:20.928611 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 13:15:03.709637 140144802662208 spec.py:349] Evaluating on the test split.
I0213 13:15:06.390794 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 13:17:41.166189 140144802662208 submission_runner.py:408] Time since start: 26745.63s, 	Step: 45930, 	{'train/accuracy': 0.6155939102172852, 'train/loss': 1.9596761465072632, 'train/bleu': 29.238332232257907, 'validation/accuracy': 0.633432924747467, 'validation/loss': 1.8007560968399048, 'validation/bleu': 26.45347452918118, 'validation/num_examples': 3000, 'test/accuracy': 0.6413573026657104, 'test/loss': 1.751164436340332, 'test/bleu': 25.745064110493338, 'test/num_examples': 3003, 'score': 15991.709733009338, 'total_duration': 26745.632090568542, 'accumulated_submission_time': 15991.709733009338, 'accumulated_eval_time': 10751.859727859497, 'accumulated_logging_time': 0.5495162010192871}
I0213 13:17:41.188596 139975014192896 logging_writer.py:48] [45930] accumulated_eval_time=10751.859728, accumulated_logging_time=0.549516, accumulated_submission_time=15991.709733, global_step=45930, preemption_count=0, score=15991.709733, test/accuracy=0.641357, test/bleu=25.745064, test/loss=1.751164, test/num_examples=3003, total_duration=26745.632091, train/accuracy=0.615594, train/bleu=29.238332, train/loss=1.959676, validation/accuracy=0.633433, validation/bleu=26.453475, validation/loss=1.800756, validation/num_examples=3000
I0213 13:18:05.780927 139975005800192 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6173597574234009, loss=2.0208208560943604
I0213 13:18:40.468465 139975014192896 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.4112417697906494, loss=1.9201221466064453
I0213 13:19:15.196066 139975005800192 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.3783748149871826, loss=1.9095227718353271
I0213 13:19:49.934555 139975014192896 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.6978169083595276, loss=2.007659435272217
I0213 13:20:24.702663 139975005800192 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.4158056378364563, loss=1.928865909576416
I0213 13:20:59.516535 139975014192896 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.5954660177230835, loss=1.9673848152160645
I0213 13:21:34.280142 139975005800192 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.35050660371780396, loss=1.9858006238937378
I0213 13:22:09.037566 139975014192896 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3764055371284485, loss=1.9515148401260376
I0213 13:22:43.811876 139975005800192 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.4708878695964813, loss=2.006636142730713
I0213 13:23:18.567296 139975014192896 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.5473122000694275, loss=1.974380612373352
I0213 13:23:53.308373 139975005800192 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7007036805152893, loss=1.9218248128890991
I0213 13:24:28.097172 139975014192896 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.36763399839401245, loss=1.9193108081817627
I0213 13:25:02.906946 139975005800192 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.36440083384513855, loss=1.8681505918502808
I0213 13:25:37.671325 139975014192896 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.5380951762199402, loss=1.8913190364837646
I0213 13:26:12.438348 139975005800192 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.45718181133270264, loss=1.9582793712615967
I0213 13:26:47.209805 139975014192896 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.4648030698299408, loss=1.9712129831314087
I0213 13:27:22.002576 139975005800192 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.33817654848098755, loss=1.884459137916565
I0213 13:27:56.761630 139975014192896 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.5333930850028992, loss=2.0321483612060547
I0213 13:28:31.542313 139975005800192 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.5191154479980469, loss=1.9933547973632812
I0213 13:29:06.281008 139975014192896 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.7109217643737793, loss=2.047454833984375
I0213 13:29:41.054991 139975005800192 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.30554112792015076, loss=1.9114580154418945
I0213 13:30:15.853747 139975014192896 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.5933340191841125, loss=1.9420900344848633
I0213 13:30:50.624272 139975005800192 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.392259418964386, loss=1.8971951007843018
I0213 13:31:25.379095 139975014192896 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.46595531702041626, loss=2.0108113288879395
I0213 13:31:41.448071 140144802662208 spec.py:321] Evaluating on the training split.
I0213 13:31:44.413224 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 13:34:37.695932 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 13:34:40.384066 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 13:37:26.760308 140144802662208 spec.py:349] Evaluating on the test split.
I0213 13:37:29.444834 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 13:40:02.664715 140144802662208 submission_runner.py:408] Time since start: 28087.13s, 	Step: 48348, 	{'train/accuracy': 0.6091585755348206, 'train/loss': 1.986215591430664, 'train/bleu': 28.95863394471144, 'validation/accuracy': 0.6338297128677368, 'validation/loss': 1.803328514099121, 'validation/bleu': 26.30413678744673, 'validation/num_examples': 3000, 'test/accuracy': 0.6401836276054382, 'test/loss': 1.750659704208374, 'test/bleu': 25.13559768501155, 'test/num_examples': 3003, 'score': 16831.879290819168, 'total_duration': 28087.13062429428, 'accumulated_submission_time': 16831.879290819168, 'accumulated_eval_time': 11253.07631278038, 'accumulated_logging_time': 0.5821371078491211}
I0213 13:40:02.686362 139975005800192 logging_writer.py:48] [48348] accumulated_eval_time=11253.076313, accumulated_logging_time=0.582137, accumulated_submission_time=16831.879291, global_step=48348, preemption_count=0, score=16831.879291, test/accuracy=0.640184, test/bleu=25.135598, test/loss=1.750660, test/num_examples=3003, total_duration=28087.130624, train/accuracy=0.609159, train/bleu=28.958634, train/loss=1.986216, validation/accuracy=0.633830, validation/bleu=26.304137, validation/loss=1.803329, validation/num_examples=3000
I0213 13:40:21.053549 139975014192896 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.5933295488357544, loss=1.9164109230041504
I0213 13:40:55.722281 139975005800192 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.28897517919540405, loss=2.048588752746582
I0213 13:41:30.474288 139975014192896 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.2865467071533203, loss=1.9075701236724854
I0213 13:42:05.241587 139975005800192 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.4639386534690857, loss=1.9545739889144897
I0213 13:42:39.980388 139975014192896 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.28425154089927673, loss=1.907231092453003
I0213 13:43:14.723119 139975005800192 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.44530701637268066, loss=1.970932960510254
I0213 13:43:49.499243 139975014192896 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.45407798886299133, loss=1.9732218980789185
I0213 13:44:24.260609 139975005800192 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.3155634105205536, loss=1.9139209985733032
I0213 13:44:59.042733 139975014192896 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.6097907423973083, loss=2.0153682231903076
I0213 13:45:33.798510 139975005800192 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.367449551820755, loss=1.9688929319381714
I0213 13:46:08.544927 139975014192896 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2725878059864044, loss=1.9099183082580566
I0213 13:46:43.302645 139975005800192 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.508687436580658, loss=1.982477068901062
I0213 13:47:18.089412 139975014192896 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.0712226629257202, loss=1.9802309274673462
I0213 13:47:52.886236 139975005800192 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.318557471036911, loss=1.9097274541854858
I0213 13:48:27.667064 139975014192896 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.4042089283466339, loss=1.8774855136871338
I0213 13:49:02.476380 139975005800192 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3966757655143738, loss=1.9271730184555054
I0213 13:49:37.241547 139975014192896 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.5636227130889893, loss=1.9579495191574097
I0213 13:50:12.027683 139975005800192 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.3608168661594391, loss=1.903055191040039
I0213 13:50:46.809925 139975014192896 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.31255054473876953, loss=1.9080711603164673
I0213 13:51:21.592358 139975005800192 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.5696508884429932, loss=1.9366542100906372
I0213 13:51:56.381276 139975014192896 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.25468695163726807, loss=1.9845632314682007
I0213 13:52:31.208773 139975005800192 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.4127849340438843, loss=1.9962148666381836
I0213 13:53:06.003486 139975014192896 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.5366281867027283, loss=1.984267234802246
I0213 13:53:40.802485 139975005800192 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.477622389793396, loss=2.001392364501953
I0213 13:54:02.825960 140144802662208 spec.py:321] Evaluating on the training split.
I0213 13:54:05.809397 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 13:57:52.546619 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 13:57:55.257032 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 14:00:59.832812 140144802662208 spec.py:349] Evaluating on the test split.
I0213 14:01:02.548623 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 14:03:42.368985 140144802662208 submission_runner.py:408] Time since start: 29506.83s, 	Step: 50765, 	{'train/accuracy': 0.6214835047721863, 'train/loss': 1.8999727964401245, 'train/bleu': 29.929704359714087, 'validation/accuracy': 0.6367310881614685, 'validation/loss': 1.78389310836792, 'validation/bleu': 26.166997541679212, 'validation/num_examples': 3000, 'test/accuracy': 0.6443437337875366, 'test/loss': 1.731534719467163, 'test/bleu': 25.595392106041817, 'test/num_examples': 3003, 'score': 17671.927947998047, 'total_duration': 29506.834899187088, 'accumulated_submission_time': 17671.927947998047, 'accumulated_eval_time': 11832.61929321289, 'accumulated_logging_time': 0.6142852306365967}
I0213 14:03:42.391477 139975014192896 logging_writer.py:48] [50765] accumulated_eval_time=11832.619293, accumulated_logging_time=0.614285, accumulated_submission_time=17671.927948, global_step=50765, preemption_count=0, score=17671.927948, test/accuracy=0.644344, test/bleu=25.595392, test/loss=1.731535, test/num_examples=3003, total_duration=29506.834899, train/accuracy=0.621484, train/bleu=29.929704, train/loss=1.899973, validation/accuracy=0.636731, validation/bleu=26.166998, validation/loss=1.783893, validation/num_examples=3000
I0213 14:03:54.883237 139975005800192 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.47975990176200867, loss=1.932997703552246
I0213 14:04:29.606392 139975014192896 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.4914766550064087, loss=1.9841617345809937
I0213 14:05:04.342514 139975005800192 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.28616204857826233, loss=1.906347393989563
I0213 14:05:39.098644 139975014192896 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3639172315597534, loss=1.9488152265548706
I0213 14:06:13.853289 139975005800192 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.5008928775787354, loss=1.9496278762817383
I0213 14:06:48.599874 139975014192896 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.4015325903892517, loss=2.0059287548065186
I0213 14:07:23.364862 139975005800192 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.41270169615745544, loss=1.9297690391540527
I0213 14:07:58.118158 139975014192896 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.4045078754425049, loss=1.9471813440322876
I0213 14:08:32.842550 139975005800192 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.3263806104660034, loss=1.8837614059448242
I0213 14:09:07.586910 139975014192896 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.34190481901168823, loss=2.018226385116577
I0213 14:09:42.366664 139975005800192 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.34118491411209106, loss=1.9652302265167236
I0213 14:10:17.169399 139975014192896 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.6400427222251892, loss=1.8752974271774292
I0213 14:10:51.948179 139975005800192 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.85650235414505, loss=1.9930347204208374
I0213 14:11:26.728402 139975014192896 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.5203120112419128, loss=1.9588209390640259
I0213 14:12:01.489661 139975005800192 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.7684856057167053, loss=1.901615023612976
I0213 14:12:36.268206 139975014192896 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.6779549717903137, loss=1.9155278205871582
I0213 14:13:11.010674 139975005800192 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.9485723972320557, loss=1.9924546480178833
I0213 14:13:45.749047 139975014192896 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3022387623786926, loss=1.8647044897079468
I0213 14:14:20.499305 139975005800192 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.30727002024650574, loss=1.9065685272216797
I0213 14:14:55.240990 139975014192896 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.5620141625404358, loss=1.8558175563812256
I0213 14:15:29.977572 139975005800192 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.25219663977622986, loss=1.9488359689712524
I0213 14:16:04.747843 139975014192896 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3012750446796417, loss=1.985371470451355
I0213 14:16:39.493759 139975005800192 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.3153286874294281, loss=1.8790913820266724
I0213 14:17:14.261479 139975014192896 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.3006439507007599, loss=1.9643887281417847
I0213 14:17:42.489440 140144802662208 spec.py:321] Evaluating on the training split.
I0213 14:17:45.462325 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 14:20:52.769151 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 14:20:55.448676 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 14:23:44.069697 140144802662208 spec.py:349] Evaluating on the test split.
I0213 14:23:46.737170 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 14:26:31.003767 140144802662208 submission_runner.py:408] Time since start: 30875.47s, 	Step: 53183, 	{'train/accuracy': 0.6156458258628845, 'train/loss': 1.9492367506027222, 'train/bleu': 29.42759289127698, 'validation/accuracy': 0.6360987424850464, 'validation/loss': 1.7813689708709717, 'validation/bleu': 26.239123202813865, 'validation/num_examples': 3000, 'test/accuracy': 0.6461565494537354, 'test/loss': 1.7227002382278442, 'test/bleu': 25.632687475591855, 'test/num_examples': 3003, 'score': 18511.936827898026, 'total_duration': 30875.469685792923, 'accumulated_submission_time': 18511.936827898026, 'accumulated_eval_time': 12361.133578777313, 'accumulated_logging_time': 0.6469564437866211}
I0213 14:26:31.025523 139975005800192 logging_writer.py:48] [53183] accumulated_eval_time=12361.133579, accumulated_logging_time=0.646956, accumulated_submission_time=18511.936828, global_step=53183, preemption_count=0, score=18511.936828, test/accuracy=0.646157, test/bleu=25.632687, test/loss=1.722700, test/num_examples=3003, total_duration=30875.469686, train/accuracy=0.615646, train/bleu=29.427593, train/loss=1.949237, validation/accuracy=0.636099, validation/bleu=26.239123, validation/loss=1.781369, validation/num_examples=3000
I0213 14:26:37.271397 139975014192896 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.44966405630111694, loss=1.9670742750167847
I0213 14:27:11.884585 139975005800192 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.4912089705467224, loss=1.8862085342407227
I0213 14:27:46.594228 139975014192896 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.33676475286483765, loss=1.9239779710769653
I0213 14:28:21.358533 139975005800192 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.8834245204925537, loss=1.8497552871704102
I0213 14:28:56.134182 139975014192896 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.6101218461990356, loss=1.9451923370361328
I0213 14:29:30.871867 139975005800192 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.38112908601760864, loss=1.8155752420425415
I0213 14:30:05.635323 139975014192896 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.6048089265823364, loss=2.0011677742004395
I0213 14:30:40.376132 139975005800192 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.8880956768989563, loss=1.9662706851959229
I0213 14:31:15.149865 139975014192896 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.24946211278438568, loss=1.9318931102752686
I0213 14:31:49.954615 139975005800192 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.48137423396110535, loss=1.975460171699524
I0213 14:32:24.718171 139975014192896 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.5065618753433228, loss=1.9578696489334106
I0213 14:32:59.509433 139975005800192 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.5662961602210999, loss=1.957923173904419
I0213 14:33:34.282986 139975014192896 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.31403636932373047, loss=1.9271641969680786
I0213 14:34:09.047833 139975005800192 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.570164680480957, loss=2.063979387283325
I0213 14:34:43.826241 139975014192896 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.40284889936447144, loss=2.0164031982421875
I0213 14:35:18.600271 139975005800192 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.27609580755233765, loss=1.9594224691390991
I0213 14:35:53.513895 139975014192896 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.28040188550949097, loss=1.9207814931869507
I0213 14:36:28.318477 139975005800192 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.2996830940246582, loss=1.889960527420044
I0213 14:37:03.098227 139975014192896 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.6278273463249207, loss=1.8970165252685547
I0213 14:37:37.878364 139975005800192 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.4764201045036316, loss=1.8801268339157104
I0213 14:38:12.656092 139975014192896 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2908328175544739, loss=1.914074420928955
I0213 14:38:47.420230 139975005800192 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.24411629140377045, loss=1.9079792499542236
I0213 14:39:22.224480 139975014192896 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.35217905044555664, loss=1.8716315031051636
I0213 14:39:57.048513 139975005800192 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.31926631927490234, loss=2.0703978538513184
I0213 14:40:31.226706 140144802662208 spec.py:321] Evaluating on the training split.
I0213 14:40:34.218312 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 14:43:27.016489 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 14:43:29.684934 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 14:46:23.324976 140144802662208 spec.py:349] Evaluating on the test split.
I0213 14:46:26.004298 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 14:49:27.233124 140144802662208 submission_runner.py:408] Time since start: 32251.70s, 	Step: 55600, 	{'train/accuracy': 0.6179764866828918, 'train/loss': 1.9319465160369873, 'train/bleu': 28.862571996639158, 'validation/accuracy': 0.64002925157547, 'validation/loss': 1.7602189779281616, 'validation/bleu': 26.359163168974383, 'validation/num_examples': 3000, 'test/accuracy': 0.6483876705169678, 'test/loss': 1.7087100744247437, 'test/bleu': 25.566127278252768, 'test/num_examples': 3003, 'score': 19352.044857025146, 'total_duration': 32251.699042081833, 'accumulated_submission_time': 19352.044857025146, 'accumulated_eval_time': 12897.139957427979, 'accumulated_logging_time': 0.6800875663757324}
I0213 14:49:27.257509 139975014192896 logging_writer.py:48] [55600] accumulated_eval_time=12897.139957, accumulated_logging_time=0.680088, accumulated_submission_time=19352.044857, global_step=55600, preemption_count=0, score=19352.044857, test/accuracy=0.648388, test/bleu=25.566127, test/loss=1.708710, test/num_examples=3003, total_duration=32251.699042, train/accuracy=0.617976, train/bleu=28.862572, train/loss=1.931947, validation/accuracy=0.640029, validation/bleu=26.359163, validation/loss=1.760219, validation/num_examples=3000
I0213 14:49:27.624970 139975005800192 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.3414837419986725, loss=1.9281479120254517
I0213 14:50:02.216274 139975014192896 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.31231260299682617, loss=1.9434531927108765
I0213 14:50:36.939513 139975005800192 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.34070128202438354, loss=1.9530109167099
I0213 14:51:11.715723 139975014192896 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.34217795729637146, loss=2.0822362899780273
I0213 14:51:46.473524 139975005800192 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2342514544725418, loss=1.9471542835235596
I0213 14:52:21.220526 139975014192896 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.6528401374816895, loss=1.915319800376892
I0213 14:52:55.989986 139975005800192 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.35345277190208435, loss=1.9397414922714233
I0213 14:53:30.787612 139975014192896 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.29140356183052063, loss=2.042498826980591
I0213 14:54:05.564048 139975005800192 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.42875513434410095, loss=1.869602084159851
I0213 14:54:40.313529 139975014192896 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.36304035782814026, loss=1.9083929061889648
I0213 14:55:15.067760 139975005800192 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.4852437376976013, loss=1.9238669872283936
I0213 14:55:49.844479 139975014192896 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.5935786366462708, loss=1.9045568704605103
I0213 14:56:24.663493 139975005800192 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3946909010410309, loss=1.9508785009384155
I0213 14:56:59.550791 139975014192896 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.28391799330711365, loss=1.8904565572738647
I0213 14:57:34.312401 139975005800192 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.5188237428665161, loss=1.9865778684616089
I0213 14:58:09.057006 139975014192896 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.3885532021522522, loss=1.8865832090377808
I0213 14:58:43.783403 139975005800192 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.3021235764026642, loss=2.0031909942626953
I0213 14:59:18.545574 139975014192896 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.26217150688171387, loss=1.8948733806610107
I0213 14:59:53.332171 139975005800192 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.2780643701553345, loss=1.9441874027252197
I0213 15:00:28.094386 139975014192896 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.7043691277503967, loss=1.9229258298873901
I0213 15:01:02.879103 139975005800192 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.414438933134079, loss=1.9653403759002686
I0213 15:01:37.660408 139975014192896 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.5701512098312378, loss=1.8946768045425415
I0213 15:02:12.418994 139975005800192 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.29908910393714905, loss=1.9220287799835205
I0213 15:02:47.205716 139975014192896 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.5997841954231262, loss=1.876043677330017
I0213 15:03:21.963216 139975005800192 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.4104442596435547, loss=1.9123566150665283
I0213 15:03:27.262176 140144802662208 spec.py:321] Evaluating on the training split.
I0213 15:03:30.231740 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 15:06:34.994944 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 15:06:37.677891 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 15:09:33.794981 140144802662208 spec.py:349] Evaluating on the test split.
I0213 15:09:36.476203 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 15:12:31.941143 140144802662208 submission_runner.py:408] Time since start: 33636.41s, 	Step: 58017, 	{'train/accuracy': 0.6243790984153748, 'train/loss': 1.8864920139312744, 'train/bleu': 30.10999079467282, 'validation/accuracy': 0.6418147087097168, 'validation/loss': 1.7520946264266968, 'validation/bleu': 26.486150866715008, 'validation/num_examples': 3000, 'test/accuracy': 0.6493057012557983, 'test/loss': 1.694807529449463, 'test/bleu': 26.47097900002062, 'test/num_examples': 3003, 'score': 20191.96054983139, 'total_duration': 33636.40701699257, 'accumulated_submission_time': 20191.96054983139, 'accumulated_eval_time': 13441.818828821182, 'accumulated_logging_time': 0.7141232490539551}
I0213 15:12:31.968362 139975014192896 logging_writer.py:48] [58017] accumulated_eval_time=13441.818829, accumulated_logging_time=0.714123, accumulated_submission_time=20191.960550, global_step=58017, preemption_count=0, score=20191.960550, test/accuracy=0.649306, test/bleu=26.470979, test/loss=1.694808, test/num_examples=3003, total_duration=33636.407017, train/accuracy=0.624379, train/bleu=30.109991, train/loss=1.886492, validation/accuracy=0.641815, validation/bleu=26.486151, validation/loss=1.752095, validation/num_examples=3000
I0213 15:13:01.054468 139975005800192 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.4009724259376526, loss=1.848126769065857
I0213 15:13:35.703544 139975014192896 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.6230782866477966, loss=1.91948664188385
I0213 15:14:10.430464 139975005800192 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.32219481468200684, loss=1.8495479822158813
I0213 15:14:45.203975 139975014192896 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.3234023153781891, loss=1.9117002487182617
I0213 15:15:19.964414 139975005800192 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.296482652425766, loss=1.9446580410003662
I0213 15:15:54.734011 139975014192896 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.5878133773803711, loss=1.886814832687378
I0213 15:16:29.511059 139975005800192 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.4045029878616333, loss=1.935718297958374
I0213 15:17:04.285419 139975014192896 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3118845522403717, loss=1.8029497861862183
I0213 15:17:39.038116 139975005800192 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.30964234471321106, loss=1.8763649463653564
I0213 15:18:13.798119 139975014192896 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.2457694113254547, loss=1.8619885444641113
I0213 15:18:48.544702 139975005800192 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.7763397693634033, loss=1.9251594543457031
I0213 15:19:23.293735 139975014192896 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.304162859916687, loss=1.9810268878936768
I0213 15:19:58.034379 139975005800192 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.6408584117889404, loss=1.9694669246673584
I0213 15:20:32.786294 139975014192896 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3343988358974457, loss=1.8405507802963257
I0213 15:21:07.551229 139975005800192 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.5878413319587708, loss=1.8985463380813599
I0213 15:21:42.332291 139975014192896 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.2973824441432953, loss=1.878126621246338
I0213 15:22:17.114995 139975005800192 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.33702677488327026, loss=1.9558804035186768
I0213 15:22:51.848917 139975014192896 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.6265676617622375, loss=1.9160164594650269
I0213 15:23:26.605196 139975005800192 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.35430634021759033, loss=1.907496690750122
I0213 15:24:01.365637 139975014192896 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.4361344873905182, loss=1.9743537902832031
I0213 15:24:36.126677 139975005800192 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.4863661527633667, loss=1.8913190364837646
I0213 15:25:10.872452 139975014192896 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.36387065052986145, loss=1.8925321102142334
I0213 15:25:45.626681 139975005800192 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.29995065927505493, loss=1.9559297561645508
I0213 15:26:20.369939 139975014192896 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.4991321265697479, loss=1.9577628374099731
I0213 15:26:32.250238 140144802662208 spec.py:321] Evaluating on the training split.
I0213 15:26:35.213071 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 15:29:44.532503 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 15:29:47.199888 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 15:32:36.668670 140144802662208 spec.py:349] Evaluating on the test split.
I0213 15:32:39.330109 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 15:35:36.070726 140144802662208 submission_runner.py:408] Time since start: 35020.54s, 	Step: 60436, 	{'train/accuracy': 0.6193897128105164, 'train/loss': 1.9212905168533325, 'train/bleu': 29.84611340870408, 'validation/accuracy': 0.6405872106552124, 'validation/loss': 1.7477840185165405, 'validation/bleu': 26.8364294960986, 'validation/num_examples': 3000, 'test/accuracy': 0.6511765718460083, 'test/loss': 1.6859363317489624, 'test/bleu': 25.956983521488905, 'test/num_examples': 3003, 'score': 21032.152578353882, 'total_duration': 35020.5366435051, 'accumulated_submission_time': 21032.152578353882, 'accumulated_eval_time': 13985.639262914658, 'accumulated_logging_time': 0.752568244934082}
I0213 15:35:36.093446 139975005800192 logging_writer.py:48] [60436] accumulated_eval_time=13985.639263, accumulated_logging_time=0.752568, accumulated_submission_time=21032.152578, global_step=60436, preemption_count=0, score=21032.152578, test/accuracy=0.651177, test/bleu=25.956984, test/loss=1.685936, test/num_examples=3003, total_duration=35020.536644, train/accuracy=0.619390, train/bleu=29.846113, train/loss=1.921291, validation/accuracy=0.640587, validation/bleu=26.836429, validation/loss=1.747784, validation/num_examples=3000
I0213 15:35:58.595535 139975014192896 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.3079012632369995, loss=1.9111909866333008
I0213 15:36:33.246630 139975005800192 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.32416486740112305, loss=1.8481165170669556
I0213 15:37:07.984762 139975014192896 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.3659045994281769, loss=2.0108885765075684
I0213 15:37:42.746446 139975005800192 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.28612035512924194, loss=1.9893310070037842
I0213 15:38:17.497431 139975014192896 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.5812857151031494, loss=1.9224662780761719
I0213 15:38:52.245209 139975005800192 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.3414647877216339, loss=1.9856716394424438
I0213 15:39:26.984276 139975014192896 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.5735818147659302, loss=1.9131418466567993
I0213 15:40:01.752522 139975005800192 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.39738020300865173, loss=1.8996272087097168
I0213 15:40:36.525494 139975014192896 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.39324018359184265, loss=1.84782874584198
I0213 15:41:11.308028 139975005800192 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.4171733558177948, loss=1.907112717628479
I0213 15:41:46.067331 139975014192896 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.31242606043815613, loss=1.9382888078689575
I0213 15:42:20.836287 139975005800192 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.3432767689228058, loss=1.9493420124053955
I0213 15:42:55.632658 139975014192896 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3653792440891266, loss=1.898783564567566
I0213 15:43:30.448467 139975005800192 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3055856227874756, loss=1.974656105041504
I0213 15:44:05.258345 139975014192896 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.7365569472312927, loss=1.828925371170044
I0213 15:44:40.057752 139975005800192 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.42624107003211975, loss=1.9462049007415771
I0213 15:45:14.807310 139975014192896 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.359262615442276, loss=1.9575265645980835
I0213 15:45:49.547649 139975005800192 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.2829926311969757, loss=1.8351472616195679
I0213 15:46:24.321539 139975014192896 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.808184802532196, loss=1.8782070875167847
I0213 15:46:59.099557 139975005800192 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.5497772097587585, loss=1.7910687923431396
I0213 15:47:33.878797 139975014192896 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.5457958579063416, loss=1.892783522605896
I0213 15:48:08.676913 139975005800192 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.400301992893219, loss=1.8366148471832275
I0213 15:48:43.444335 139975014192896 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.6111034750938416, loss=1.8994836807250977
I0213 15:49:18.201011 139975005800192 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.4832670986652374, loss=1.9016705751419067
I0213 15:49:36.351089 140144802662208 spec.py:321] Evaluating on the training split.
I0213 15:49:39.329009 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 15:52:22.242636 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 15:52:24.936318 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 15:55:24.332141 140144802662208 spec.py:349] Evaluating on the test split.
I0213 15:55:27.019845 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 15:58:26.740096 140144802662208 submission_runner.py:408] Time since start: 36391.21s, 	Step: 62854, 	{'train/accuracy': 0.6402044296264648, 'train/loss': 1.7503422498703003, 'train/bleu': 31.06420597935536, 'validation/accuracy': 0.6426454782485962, 'validation/loss': 1.7346762418746948, 'validation/bleu': 26.713522805537497, 'validation/num_examples': 3000, 'test/accuracy': 0.6542444229125977, 'test/loss': 1.6709492206573486, 'test/bleu': 26.212849654597935, 'test/num_examples': 3003, 'score': 21872.31868505478, 'total_duration': 36391.20599746704, 'accumulated_submission_time': 21872.31868505478, 'accumulated_eval_time': 14516.028207540512, 'accumulated_logging_time': 0.7864320278167725}
I0213 15:58:26.763275 139975014192896 logging_writer.py:48] [62854] accumulated_eval_time=14516.028208, accumulated_logging_time=0.786432, accumulated_submission_time=21872.318685, global_step=62854, preemption_count=0, score=21872.318685, test/accuracy=0.654244, test/bleu=26.212850, test/loss=1.670949, test/num_examples=3003, total_duration=36391.205997, train/accuracy=0.640204, train/bleu=31.064206, train/loss=1.750342, validation/accuracy=0.642645, validation/bleu=26.713523, validation/loss=1.734676, validation/num_examples=3000
I0213 15:58:43.016577 139975005800192 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.31756433844566345, loss=1.9174898862838745
I0213 15:59:17.678832 139975014192896 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.2991679608821869, loss=1.90920889377594
I0213 15:59:52.392983 139975005800192 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.31671154499053955, loss=1.894179344177246
I0213 16:00:27.154375 139975014192896 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.4782539904117584, loss=1.92347252368927
I0213 16:01:01.977105 139975005800192 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.43287596106529236, loss=1.8237816095352173
I0213 16:01:36.715344 139975014192896 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.29455211758613586, loss=1.9506756067276
I0213 16:02:11.463243 139975005800192 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.40298330783843994, loss=1.8764312267303467
I0213 16:02:46.204451 139975014192896 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.5104609131813049, loss=1.8525331020355225
I0213 16:03:20.960377 139975005800192 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.31333407759666443, loss=1.8926641941070557
I0213 16:03:55.711522 139975014192896 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.5601118206977844, loss=1.8985306024551392
I0213 16:04:30.464684 139975005800192 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.38844558596611023, loss=1.903944969177246
I0213 16:05:05.204766 139975014192896 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2822313904762268, loss=1.8401094675064087
I0213 16:05:39.960767 139975005800192 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.555991530418396, loss=1.8855270147323608
I0213 16:06:14.700104 139975014192896 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.28357136249542236, loss=1.905785322189331
I0213 16:06:49.450151 139975005800192 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.3165416419506073, loss=1.8871556520462036
I0213 16:07:24.231815 139975014192896 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.27444493770599365, loss=1.917285680770874
I0213 16:07:58.981360 139975005800192 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.6571779251098633, loss=1.9444551467895508
I0213 16:08:33.732870 139975014192896 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3248327076435089, loss=1.9054467678070068
I0213 16:09:08.482708 139975005800192 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.4102686643600464, loss=1.948683261871338
I0213 16:09:43.233779 139975014192896 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.4059988260269165, loss=1.9058843851089478
I0213 16:10:17.967829 139975005800192 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2790910601615906, loss=1.844224214553833
I0213 16:10:52.702989 139975014192896 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3119310438632965, loss=1.8770513534545898
I0213 16:11:27.430861 139975005800192 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.32922255992889404, loss=1.9148523807525635
I0213 16:12:02.178741 139975014192896 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.709601879119873, loss=1.8362534046173096
I0213 16:12:26.915092 140144802662208 spec.py:321] Evaluating on the training split.
I0213 16:12:29.881177 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 16:16:19.914969 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 16:16:22.603504 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 16:19:21.554397 140144802662208 spec.py:349] Evaluating on the test split.
I0213 16:19:24.254705 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 16:22:12.595044 140144802662208 submission_runner.py:408] Time since start: 37817.06s, 	Step: 65273, 	{'train/accuracy': 0.6218117475509644, 'train/loss': 1.9062598943710327, 'train/bleu': 29.803210788948807, 'validation/accuracy': 0.6440093517303467, 'validation/loss': 1.7179428339004517, 'validation/bleu': 26.75458586334858, 'validation/num_examples': 3000, 'test/accuracy': 0.6531752943992615, 'test/loss': 1.6693758964538574, 'test/bleu': 26.230637257259648, 'test/num_examples': 3003, 'score': 22712.382335186005, 'total_duration': 37817.06096410751, 'accumulated_submission_time': 22712.382335186005, 'accumulated_eval_time': 15101.708109140396, 'accumulated_logging_time': 0.8203840255737305}
I0213 16:22:12.619271 139975005800192 logging_writer.py:48] [65273] accumulated_eval_time=15101.708109, accumulated_logging_time=0.820384, accumulated_submission_time=22712.382335, global_step=65273, preemption_count=0, score=22712.382335, test/accuracy=0.653175, test/bleu=26.230637, test/loss=1.669376, test/num_examples=3003, total_duration=37817.060964, train/accuracy=0.621812, train/bleu=29.803211, train/loss=1.906260, validation/accuracy=0.644009, validation/bleu=26.754586, validation/loss=1.717943, validation/num_examples=3000
I0213 16:22:22.339896 139975014192896 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.34697648882865906, loss=1.8711210489273071
I0213 16:22:56.979474 139975005800192 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.6523271799087524, loss=1.932757019996643
I0213 16:23:31.724712 139975014192896 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.31529608368873596, loss=1.8511078357696533
I0213 16:24:06.520658 139975005800192 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.27935001254081726, loss=1.927986741065979
I0213 16:24:41.297662 139975014192896 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.40423399209976196, loss=1.9402048587799072
I0213 16:25:16.051918 139975005800192 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.3430482745170593, loss=1.852014422416687
I0213 16:25:50.788859 139975014192896 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.5979459881782532, loss=1.8778434991836548
I0213 16:26:25.546184 139975005800192 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.3726111948490143, loss=1.8222575187683105
I0213 16:27:00.299865 139975014192896 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.27349475026130676, loss=1.8810521364212036
I0213 16:27:35.060071 139975005800192 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.3448122441768646, loss=1.883492112159729
I0213 16:28:09.829633 139975014192896 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.4549994468688965, loss=1.93232262134552
I0213 16:28:44.622415 139975005800192 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.40811851620674133, loss=1.7940740585327148
I0213 16:29:19.417287 139975014192896 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.267579048871994, loss=1.8408247232437134
I0213 16:29:54.194312 139975005800192 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.30823296308517456, loss=1.8852179050445557
I0213 16:30:28.952213 139975014192896 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.4828245937824249, loss=1.9430465698242188
I0213 16:31:03.734051 139975005800192 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.6464303135871887, loss=1.836344838142395
I0213 16:31:38.499633 139975014192896 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2888786196708679, loss=1.8969862461090088
I0213 16:32:13.268268 139975005800192 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.28875279426574707, loss=1.842946171760559
I0213 16:32:48.087442 139975014192896 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.31713348627090454, loss=1.9068831205368042
I0213 16:33:23.006246 139975005800192 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.2825187146663666, loss=1.8309245109558105
I0213 16:33:57.805536 139975014192896 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.322176456451416, loss=1.896271824836731
I0213 16:34:32.593806 139975005800192 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.4759332835674286, loss=1.9056288003921509
I0213 16:35:07.406072 139975014192896 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.45833611488342285, loss=1.951255440711975
I0213 16:35:42.217401 139975005800192 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.3308456838130951, loss=1.8127000331878662
I0213 16:36:12.863067 140144802662208 spec.py:321] Evaluating on the training split.
I0213 16:36:15.834353 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 16:40:44.643984 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 16:40:47.330538 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 16:45:14.455618 140144802662208 spec.py:349] Evaluating on the test split.
I0213 16:45:17.151103 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 16:49:37.066663 140144802662208 submission_runner.py:408] Time since start: 39461.53s, 	Step: 67690, 	{'train/accuracy': 0.6277031898498535, 'train/loss': 1.868759274482727, 'train/bleu': 30.25100134313, 'validation/accuracy': 0.6481258869171143, 'validation/loss': 1.7066301107406616, 'validation/bleu': 26.141904725586556, 'validation/num_examples': 3000, 'test/accuracy': 0.6579745411872864, 'test/loss': 1.6437774896621704, 'test/bleu': 26.80649253582942, 'test/num_examples': 3003, 'score': 23552.53147172928, 'total_duration': 39461.53255653381, 'accumulated_submission_time': 23552.53147172928, 'accumulated_eval_time': 15905.911636829376, 'accumulated_logging_time': 0.8546721935272217}
I0213 16:49:37.096887 139975014192896 logging_writer.py:48] [67690] accumulated_eval_time=15905.911637, accumulated_logging_time=0.854672, accumulated_submission_time=23552.531472, global_step=67690, preemption_count=0, score=23552.531472, test/accuracy=0.657975, test/bleu=26.806493, test/loss=1.643777, test/num_examples=3003, total_duration=39461.532557, train/accuracy=0.627703, train/bleu=30.251001, train/loss=1.868759, validation/accuracy=0.648126, validation/bleu=26.141905, validation/loss=1.706630, validation/num_examples=3000
I0213 16:49:40.925195 139975005800192 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3088545501232147, loss=1.8299052715301514
I0213 16:50:15.545835 139975014192896 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.4614294767379761, loss=1.896479845046997
I0213 16:50:50.240525 139975005800192 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.27642035484313965, loss=1.9253802299499512
I0213 16:51:24.990578 139975014192896 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.31430357694625854, loss=1.8510608673095703
I0213 16:51:59.767013 139975005800192 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.2934480309486389, loss=1.8369077444076538
I0213 16:52:34.677507 139975014192896 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.32932496070861816, loss=1.9479905366897583
I0213 16:53:09.501821 139975005800192 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.4744698107242584, loss=1.8186677694320679
I0213 16:53:44.292695 139975014192896 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3433360755443573, loss=1.8244439363479614
I0213 16:54:19.063549 139975005800192 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.5983960032463074, loss=1.8856877088546753
I0213 16:54:53.831055 139975014192896 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.27945858240127563, loss=1.832977056503296
I0213 16:55:28.606134 139975005800192 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.594778299331665, loss=1.9257656335830688
I0213 16:56:03.390081 139975014192896 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.40623369812965393, loss=1.7797337770462036
I0213 16:56:38.117157 139975005800192 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.34797391295433044, loss=1.8854365348815918
I0213 16:57:12.842331 139975014192896 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.5377137660980225, loss=1.8764199018478394
I0213 16:57:47.580923 139975005800192 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.3301638662815094, loss=1.9223105907440186
I0213 16:58:22.336960 139975014192896 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.3189784288406372, loss=1.937135100364685
I0213 16:58:57.137594 139975005800192 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.4732784926891327, loss=1.9138898849487305
I0213 16:59:31.864415 139975014192896 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.40098118782043457, loss=1.8140028715133667
I0213 17:00:06.609829 139975005800192 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.6287033557891846, loss=1.823119878768921
I0213 17:00:41.353215 139975014192896 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.6198287606239319, loss=1.833107352256775
I0213 17:01:16.144448 139975005800192 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.5584871768951416, loss=1.9193235635757446
I0213 17:01:50.872284 139975014192896 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.6325089931488037, loss=1.8193691968917847
I0213 17:02:25.658833 139975005800192 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.2743614912033081, loss=1.8979482650756836
I0213 17:03:00.481240 139975014192896 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.6049676537513733, loss=1.8633570671081543
I0213 17:03:35.269663 139975005800192 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.2638053596019745, loss=1.8313665390014648
I0213 17:03:37.089807 140144802662208 spec.py:321] Evaluating on the training split.
I0213 17:03:40.076451 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 17:06:50.465424 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 17:06:53.143904 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 17:09:36.148100 140144802662208 spec.py:349] Evaluating on the test split.
I0213 17:09:38.838114 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 17:12:31.378051 140144802662208 submission_runner.py:408] Time since start: 40835.84s, 	Step: 70107, 	{'train/accuracy': 0.6321364045143127, 'train/loss': 1.8177008628845215, 'train/bleu': 30.434628193983656, 'validation/accuracy': 0.6484978199005127, 'validation/loss': 1.7004215717315674, 'validation/bleu': 27.146623495404736, 'validation/num_examples': 3000, 'test/accuracy': 0.6586717963218689, 'test/loss': 1.635362982749939, 'test/bleu': 26.89588272520431, 'test/num_examples': 3003, 'score': 24392.43099117279, 'total_duration': 40835.84396624565, 'accumulated_submission_time': 24392.43099117279, 'accumulated_eval_time': 16440.199833631516, 'accumulated_logging_time': 0.8966896533966064}
I0213 17:12:31.404771 139975014192896 logging_writer.py:48] [70107] accumulated_eval_time=16440.199834, accumulated_logging_time=0.896690, accumulated_submission_time=24392.430991, global_step=70107, preemption_count=0, score=24392.430991, test/accuracy=0.658672, test/bleu=26.895883, test/loss=1.635363, test/num_examples=3003, total_duration=40835.843966, train/accuracy=0.632136, train/bleu=30.434628, train/loss=1.817701, validation/accuracy=0.648498, validation/bleu=27.146623, validation/loss=1.700422, validation/num_examples=3000
I0213 17:13:03.976229 139975005800192 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.32649409770965576, loss=1.9032988548278809
I0213 17:13:38.676293 139975014192896 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.4598788321018219, loss=1.8245190382003784
I0213 17:14:13.420033 139975005800192 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.28924477100372314, loss=1.7676039934158325
I0213 17:14:48.180804 139975014192896 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.6492904424667358, loss=1.8335283994674683
I0213 17:15:22.926084 139975005800192 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.37239688634872437, loss=1.8226186037063599
I0213 17:15:57.694118 139975014192896 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.4424978196620941, loss=1.829771637916565
I0213 17:16:32.440973 139975005800192 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.32878825068473816, loss=1.841994285583496
I0213 17:17:07.192742 139975014192896 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.29570329189300537, loss=1.858849287033081
I0213 17:17:41.935246 139975005800192 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.36207178235054016, loss=1.7423205375671387
I0213 17:18:16.710154 139975014192896 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.4991140067577362, loss=1.8413119316101074
I0213 17:18:51.440598 139975005800192 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.38066911697387695, loss=1.9243077039718628
I0213 17:19:26.184874 139975014192896 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.4384351074695587, loss=1.8465806245803833
I0213 17:20:00.944317 139975005800192 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.4161826968193054, loss=1.8510525226593018
I0213 17:20:35.724596 139975014192896 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.49511009454727173, loss=1.8680330514907837
I0213 17:21:10.516839 139975005800192 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.2924859821796417, loss=1.826433777809143
I0213 17:21:45.252483 139975014192896 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.534706711769104, loss=1.857615351676941
I0213 17:22:20.023700 139975005800192 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.3610437214374542, loss=1.869301199913025
I0213 17:22:54.778697 139975014192896 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3124173879623413, loss=1.832842230796814
I0213 17:23:29.525634 139975005800192 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.42729976773262024, loss=1.8872438669204712
I0213 17:24:04.271008 139975014192896 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3548130989074707, loss=1.8500065803527832
I0213 17:24:39.020991 139975005800192 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.5140987634658813, loss=1.7742820978164673
I0213 17:25:13.786478 139975014192896 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.29032668471336365, loss=1.8082077503204346
I0213 17:25:48.668414 139975005800192 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.30958858132362366, loss=1.8905775547027588
I0213 17:26:23.467839 139975014192896 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.5719693303108215, loss=1.8003652095794678
I0213 17:26:31.533157 140144802662208 spec.py:321] Evaluating on the training split.
I0213 17:26:34.498808 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 17:30:22.886249 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 17:30:25.561593 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 17:33:18.882651 140144802662208 spec.py:349] Evaluating on the test split.
I0213 17:33:21.580078 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 17:36:22.284243 140144802662208 submission_runner.py:408] Time since start: 42266.75s, 	Step: 72525, 	{'train/accuracy': 0.6318503618240356, 'train/loss': 1.8400778770446777, 'train/bleu': 30.325490652572917, 'validation/accuracy': 0.6501221060752869, 'validation/loss': 1.6853595972061157, 'validation/bleu': 27.80913835525352, 'validation/num_examples': 3000, 'test/accuracy': 0.6619255542755127, 'test/loss': 1.614753007888794, 'test/bleu': 26.698471399758624, 'test/num_examples': 3003, 'score': 25232.470304965973, 'total_duration': 42266.7501308918, 'accumulated_submission_time': 25232.470304965973, 'accumulated_eval_time': 17030.950835227966, 'accumulated_logging_time': 0.9336686134338379}
I0213 17:36:22.315327 139975005800192 logging_writer.py:48] [72525] accumulated_eval_time=17030.950835, accumulated_logging_time=0.933669, accumulated_submission_time=25232.470305, global_step=72525, preemption_count=0, score=25232.470305, test/accuracy=0.661926, test/bleu=26.698471, test/loss=1.614753, test/num_examples=3003, total_duration=42266.750131, train/accuracy=0.631850, train/bleu=30.325491, train/loss=1.840078, validation/accuracy=0.650122, validation/bleu=27.809138, validation/loss=1.685360, validation/num_examples=3000
I0213 17:36:48.640285 139975014192896 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3923631012439728, loss=1.8141753673553467
I0213 17:37:23.304992 139975005800192 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.3195991814136505, loss=1.8441014289855957
I0213 17:37:58.006464 139975014192896 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.46545225381851196, loss=1.8526523113250732
I0213 17:38:32.728417 139975005800192 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.33234283328056335, loss=1.8746933937072754
I0213 17:39:07.465149 139975014192896 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.4155823886394501, loss=1.8618354797363281
I0213 17:39:42.204941 139975005800192 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.46411293745040894, loss=1.8746824264526367
I0213 17:40:16.948679 139975014192896 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.40494945645332336, loss=1.797583818435669
I0213 17:40:51.692349 139975005800192 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.4590175151824951, loss=1.734754204750061
I0213 17:41:26.495642 139975014192896 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3099042475223541, loss=1.7616275548934937
I0213 17:42:01.255084 139975005800192 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.5270905494689941, loss=1.8340022563934326
I0213 17:42:36.008109 139975014192896 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.348884254693985, loss=1.8990591764450073
I0213 17:43:10.781850 139975005800192 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.4016988277435303, loss=1.7823905944824219
I0213 17:43:45.518547 139975014192896 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.3076595664024353, loss=1.8374147415161133
I0213 17:44:20.257244 139975005800192 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.32606077194213867, loss=1.8219815492630005
I0213 17:44:55.004596 139975014192896 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.5099196434020996, loss=1.874410629272461
I0213 17:45:29.766733 139975005800192 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.2745826840400696, loss=1.8742226362228394
I0213 17:46:04.527153 139975014192896 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.3184325397014618, loss=1.8714802265167236
I0213 17:46:39.265369 139975005800192 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.38061362504959106, loss=1.8800374269485474
I0213 17:47:14.025946 139975014192896 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.30858132243156433, loss=1.8160431385040283
I0213 17:47:48.781475 139975005800192 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.35641416907310486, loss=1.7377482652664185
I0213 17:48:23.561221 139975014192896 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.3520001769065857, loss=1.7697540521621704
I0213 17:48:58.293201 139975005800192 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.5099263191223145, loss=1.7738330364227295
I0213 17:49:33.044683 139975014192896 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.3767954707145691, loss=1.8876527547836304
I0213 17:50:07.783821 139975005800192 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.47086453437805176, loss=1.8303859233856201
I0213 17:50:22.439801 140144802662208 spec.py:321] Evaluating on the training split.
I0213 17:50:25.400934 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 17:54:16.195741 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 17:54:18.872179 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 17:57:31.602961 140144802662208 spec.py:349] Evaluating on the test split.
I0213 17:57:34.283809 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 18:00:41.351351 140144802662208 submission_runner.py:408] Time since start: 43725.82s, 	Step: 74944, 	{'train/accuracy': 0.6331945657730103, 'train/loss': 1.8335165977478027, 'train/bleu': 30.634850980771578, 'validation/accuracy': 0.6535691022872925, 'validation/loss': 1.6698365211486816, 'validation/bleu': 27.427614088747323, 'validation/num_examples': 3000, 'test/accuracy': 0.6597524881362915, 'test/loss': 1.6147087812423706, 'test/bleu': 26.434105724544263, 'test/num_examples': 3003, 'score': 26072.504900217056, 'total_duration': 43725.81726980209, 'accumulated_submission_time': 26072.504900217056, 'accumulated_eval_time': 17649.86233663559, 'accumulated_logging_time': 0.9771442413330078}
I0213 18:00:41.377882 139975014192896 logging_writer.py:48] [74944] accumulated_eval_time=17649.862337, accumulated_logging_time=0.977144, accumulated_submission_time=26072.504900, global_step=74944, preemption_count=0, score=26072.504900, test/accuracy=0.659752, test/bleu=26.434106, test/loss=1.614709, test/num_examples=3003, total_duration=43725.817270, train/accuracy=0.633195, train/bleu=30.634851, train/loss=1.833517, validation/accuracy=0.653569, validation/bleu=27.427614, validation/loss=1.669837, validation/num_examples=3000
I0213 18:01:01.103386 139975005800192 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.310733824968338, loss=1.7851955890655518
I0213 18:01:35.757834 139975014192896 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.5342124700546265, loss=1.715633511543274
I0213 18:02:10.504425 139975005800192 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.2904662489891052, loss=1.7932082414627075
I0213 18:02:45.271515 139975014192896 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.30513453483581543, loss=1.843587040901184
I0213 18:03:20.051826 139975005800192 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.29587849974632263, loss=1.7887030839920044
I0213 18:03:54.937590 139975014192896 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.5234294533729553, loss=1.8296421766281128
I0213 18:04:29.710154 139975005800192 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.4419102072715759, loss=1.852764368057251
I0213 18:05:04.459850 139975014192896 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.5602174997329712, loss=1.8570828437805176
I0213 18:05:39.246475 139975005800192 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.33758091926574707, loss=1.8385847806930542
I0213 18:06:14.034351 139975014192896 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.2818126678466797, loss=1.7999881505966187
I0213 18:06:48.786926 139975005800192 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3172493875026703, loss=1.7782788276672363
I0213 18:07:23.530088 139975014192896 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.6015762686729431, loss=1.8050302267074585
I0213 18:07:58.283578 139975005800192 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.39821234345436096, loss=1.7730389833450317
I0213 18:08:33.044739 139975014192896 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.47457218170166016, loss=1.8356202840805054
I0213 18:09:07.817452 139975005800192 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2945641279220581, loss=1.9188957214355469
I0213 18:09:42.644736 139975014192896 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.32441842555999756, loss=1.8015097379684448
I0213 18:10:17.400187 139975005800192 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.2865425944328308, loss=1.7688261270523071
I0213 18:10:52.152897 139975014192896 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.6025024652481079, loss=1.8408184051513672
I0213 18:11:26.900192 139975005800192 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3468811511993408, loss=1.8101139068603516
I0213 18:12:01.667535 139975014192896 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.2859782874584198, loss=1.815043330192566
I0213 18:12:36.430686 139975005800192 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.37780630588531494, loss=1.8623065948486328
I0213 18:13:11.220623 139975014192896 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.3365648686885834, loss=1.804878830909729
I0213 18:13:45.999366 139975005800192 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.3560142517089844, loss=1.8039405345916748
I0213 18:14:20.778921 139975014192896 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.42681679129600525, loss=1.8127738237380981
I0213 18:14:41.362858 140144802662208 spec.py:321] Evaluating on the training split.
I0213 18:14:44.326843 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 18:17:58.056281 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 18:18:00.732280 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 18:20:37.444192 140144802662208 spec.py:349] Evaluating on the test split.
I0213 18:20:40.129028 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 18:23:16.061182 140144802662208 submission_runner.py:408] Time since start: 45080.53s, 	Step: 77361, 	{'train/accuracy': 0.6339204907417297, 'train/loss': 1.8167153596878052, 'train/bleu': 30.797597115589767, 'validation/accuracy': 0.6549205780029297, 'validation/loss': 1.6654349565505981, 'validation/bleu': 27.880582151012234, 'validation/num_examples': 3000, 'test/accuracy': 0.6661786437034607, 'test/loss': 1.5921257734298706, 'test/bleu': 27.594957648339527, 'test/num_examples': 3003, 'score': 26912.40016245842, 'total_duration': 45080.52706384659, 'accumulated_submission_time': 26912.40016245842, 'accumulated_eval_time': 18164.56058192253, 'accumulated_logging_time': 1.0137157440185547}
I0213 18:23:16.093304 139975005800192 logging_writer.py:48] [77361] accumulated_eval_time=18164.560582, accumulated_logging_time=1.013716, accumulated_submission_time=26912.400162, global_step=77361, preemption_count=0, score=26912.400162, test/accuracy=0.666179, test/bleu=27.594958, test/loss=1.592126, test/num_examples=3003, total_duration=45080.527064, train/accuracy=0.633920, train/bleu=30.797597, train/loss=1.816715, validation/accuracy=0.654921, validation/bleu=27.880582, validation/loss=1.665435, validation/num_examples=3000
I0213 18:23:29.971258 139975014192896 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.5228241086006165, loss=1.8056212663650513
I0213 18:24:04.649456 139975005800192 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.40755948424339294, loss=1.8188856840133667
I0213 18:24:39.370588 139975014192896 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.5108981728553772, loss=1.7996842861175537
I0213 18:25:14.119227 139975005800192 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.41834020614624023, loss=1.9881256818771362
I0213 18:25:48.878298 139975014192896 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.4090716242790222, loss=1.864891529083252
I0213 18:26:23.617744 139975005800192 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.28186899423599243, loss=1.790316104888916
I0213 18:26:58.355788 139975014192896 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.2856232523918152, loss=1.7436631917953491
I0213 18:27:33.110169 139975005800192 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3748231828212738, loss=1.8739010095596313
I0213 18:28:07.863427 139975014192896 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.3027009069919586, loss=1.8387413024902344
I0213 18:28:42.662258 139975005800192 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.4586604833602905, loss=1.8267382383346558
I0213 18:29:17.426719 139975014192896 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.43425044417381287, loss=1.782859206199646
I0213 18:29:52.165400 139975005800192 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3236272931098938, loss=1.817489504814148
I0213 18:30:26.942060 139975014192896 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3001672923564911, loss=1.815623164176941
I0213 18:31:01.691214 139975005800192 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.44267648458480835, loss=1.8195596933364868
I0213 18:31:36.485731 139975014192896 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2910430133342743, loss=1.7427341938018799
I0213 18:32:11.235005 139975005800192 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.46066245436668396, loss=1.7792103290557861
I0213 18:32:46.015050 139975014192896 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.32089880108833313, loss=1.7780776023864746
I0213 18:33:20.814208 139975005800192 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.3056570887565613, loss=1.84592866897583
I0213 18:33:55.595443 139975014192896 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.3576590120792389, loss=1.8472479581832886
I0213 18:34:30.357334 139975005800192 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.3640361428260803, loss=1.8158787488937378
I0213 18:35:05.140471 139975014192896 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3424239754676819, loss=1.775129795074463
I0213 18:35:39.887506 139975005800192 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.38367214798927307, loss=1.8803001642227173
I0213 18:36:14.668758 139975014192896 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.30764296650886536, loss=1.8290770053863525
I0213 18:36:49.438839 139975005800192 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.5199224352836609, loss=1.8421822786331177
I0213 18:37:16.284038 140144802662208 spec.py:321] Evaluating on the training split.
I0213 18:37:19.250347 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 18:40:50.151263 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 18:40:52.831331 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 18:43:41.816845 140144802662208 spec.py:349] Evaluating on the test split.
I0213 18:43:44.504580 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 18:46:19.838624 140144802662208 submission_runner.py:408] Time since start: 46464.30s, 	Step: 79779, 	{'train/accuracy': 0.6340129375457764, 'train/loss': 1.818768858909607, 'train/bleu': 31.369896513488115, 'validation/accuracy': 0.6570656299591064, 'validation/loss': 1.6416860818862915, 'validation/bleu': 28.10779161190765, 'validation/num_examples': 3000, 'test/accuracy': 0.6673523187637329, 'test/loss': 1.5821276903152466, 'test/bleu': 27.324396090936787, 'test/num_examples': 3003, 'score': 27752.500133752823, 'total_duration': 46464.30451631546, 'accumulated_submission_time': 27752.500133752823, 'accumulated_eval_time': 18708.115093946457, 'accumulated_logging_time': 1.0589666366577148}
I0213 18:46:19.870133 139975014192896 logging_writer.py:48] [79779] accumulated_eval_time=18708.115094, accumulated_logging_time=1.058967, accumulated_submission_time=27752.500134, global_step=79779, preemption_count=0, score=27752.500134, test/accuracy=0.667352, test/bleu=27.324396, test/loss=1.582128, test/num_examples=3003, total_duration=46464.304516, train/accuracy=0.634013, train/bleu=31.369897, train/loss=1.818769, validation/accuracy=0.657066, validation/bleu=28.107792, validation/loss=1.641686, validation/num_examples=3000
I0213 18:46:27.513168 139975005800192 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3684123754501343, loss=1.820793867111206
I0213 18:47:02.152668 139975014192896 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.34134116768836975, loss=1.8417338132858276
I0213 18:47:36.859541 139975005800192 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.721634030342102, loss=1.7098326683044434
I0213 18:48:11.628813 139975014192896 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2895004451274872, loss=1.8795000314712524
I0213 18:48:46.410675 139975005800192 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.27831992506980896, loss=1.7578778266906738
I0213 18:49:21.212649 139975014192896 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2882554531097412, loss=1.8503082990646362
I0213 18:49:55.996282 139975005800192 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.32284432649612427, loss=1.8007750511169434
I0213 18:50:30.761477 139975014192896 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.3531126379966736, loss=1.8864213228225708
I0213 18:51:05.527073 139975005800192 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.31948259472846985, loss=1.7860854864120483
I0213 18:51:40.313446 139975014192896 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.29516664147377014, loss=1.7608891725540161
I0213 18:52:15.076645 139975005800192 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.45520395040512085, loss=1.8419108390808105
I0213 18:52:49.864986 139975014192896 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3254038393497467, loss=1.814643383026123
I0213 18:53:24.636027 139975005800192 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.36150121688842773, loss=1.8588134050369263
I0213 18:53:59.399285 139975014192896 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.4307030737400055, loss=1.7486459016799927
I0213 18:54:34.209019 139975005800192 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.3678983151912689, loss=1.790173888206482
I0213 18:55:08.958171 139975014192896 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.27615445852279663, loss=1.888700246810913
I0213 18:55:43.730261 139975005800192 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.31526780128479004, loss=1.771093487739563
I0213 18:56:18.477496 139975014192896 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.29410475492477417, loss=1.7149466276168823
I0213 18:56:53.250910 139975005800192 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3256116807460785, loss=1.8282443284988403
I0213 18:57:28.003441 139975014192896 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.5358450412750244, loss=1.7653213739395142
I0213 18:58:02.769659 139975005800192 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.35642123222351074, loss=1.7877334356307983
I0213 18:58:37.539628 139975014192896 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.5150386691093445, loss=1.839274525642395
I0213 18:59:12.324588 139975005800192 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.38057029247283936, loss=1.8607572317123413
I0213 18:59:47.058938 139975014192896 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.40027546882629395, loss=1.7311244010925293
I0213 19:00:20.135891 140144802662208 spec.py:321] Evaluating on the training split.
I0213 19:00:23.104318 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 19:03:45.843966 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 19:03:48.523061 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 19:06:37.159318 140144802662208 spec.py:349] Evaluating on the test split.
I0213 19:06:39.849129 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 19:09:19.096659 140144802662208 submission_runner.py:408] Time since start: 47843.56s, 	Step: 82197, 	{'train/accuracy': 0.6473284363746643, 'train/loss': 1.7201316356658936, 'train/bleu': 31.41526678659185, 'validation/accuracy': 0.659334659576416, 'validation/loss': 1.6313279867172241, 'validation/bleu': 28.270451851715933, 'validation/num_examples': 3000, 'test/accuracy': 0.6687816381454468, 'test/loss': 1.5679576396942139, 'test/bleu': 27.693554762625116, 'test/num_examples': 3003, 'score': 28592.67398762703, 'total_duration': 47843.56256365776, 'accumulated_submission_time': 28592.67398762703, 'accumulated_eval_time': 19247.07579922676, 'accumulated_logging_time': 1.102534532546997}
I0213 19:09:19.122801 139975005800192 logging_writer.py:48] [82197] accumulated_eval_time=19247.075799, accumulated_logging_time=1.102535, accumulated_submission_time=28592.673988, global_step=82197, preemption_count=0, score=28592.673988, test/accuracy=0.668782, test/bleu=27.693555, test/loss=1.567958, test/num_examples=3003, total_duration=47843.562564, train/accuracy=0.647328, train/bleu=31.415267, train/loss=1.720132, validation/accuracy=0.659335, validation/bleu=28.270452, validation/loss=1.631328, validation/num_examples=3000
I0213 19:09:20.534102 139975014192896 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.28240278363227844, loss=1.8426826000213623
I0213 19:09:55.164330 139975005800192 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.5946868658065796, loss=1.8872392177581787
I0213 19:10:29.862272 139975014192896 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.4781237244606018, loss=1.813059687614441
I0213 19:11:04.591604 139975005800192 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.361713171005249, loss=1.8113608360290527
I0213 19:11:39.371347 139975014192896 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.3876383304595947, loss=1.7381062507629395
I0213 19:12:14.136881 139975005800192 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.3273047208786011, loss=1.798239827156067
I0213 19:12:48.891915 139975014192896 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.31706735491752625, loss=1.7965203523635864
I0213 19:13:23.665205 139975005800192 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.2873251140117645, loss=1.776648998260498
I0213 19:13:58.436694 139975014192896 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.3554478585720062, loss=1.7443630695343018
I0213 19:14:33.202035 139975005800192 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.4346233308315277, loss=1.808032512664795
I0213 19:15:07.968971 139975014192896 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.8189135193824768, loss=1.8456214666366577
I0213 19:15:42.736395 139975005800192 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.48174387216567993, loss=1.7212601900100708
I0213 19:16:17.530447 139975014192896 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.30510297417640686, loss=1.794270396232605
I0213 19:16:52.330181 139975005800192 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3159679174423218, loss=1.788361668586731
I0213 19:17:27.125088 139975014192896 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.3370208144187927, loss=1.792253851890564
I0213 19:18:01.918345 139975005800192 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.2774859070777893, loss=1.751177191734314
I0213 19:18:36.678179 139975014192896 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.42849260568618774, loss=1.703950047492981
I0213 19:19:11.436066 139975005800192 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.3217211365699768, loss=1.8384078741073608
I0213 19:19:46.227461 139975014192896 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.37238577008247375, loss=1.7414222955703735
I0213 19:20:20.976686 139975005800192 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.2959348261356354, loss=1.741794228553772
I0213 19:20:55.719878 139975014192896 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.5041075944900513, loss=1.8150120973587036
I0213 19:21:30.475277 139975005800192 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.39010748267173767, loss=1.7430189847946167
I0213 19:22:05.252094 139975014192896 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.2851067781448364, loss=1.8427594900131226
I0213 19:22:39.994485 139975005800192 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.32865437865257263, loss=1.7596968412399292
I0213 19:23:14.753610 139975014192896 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.4484780728816986, loss=1.7799293994903564
I0213 19:23:19.342357 140144802662208 spec.py:321] Evaluating on the training split.
I0213 19:23:22.321057 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 19:26:46.798763 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 19:26:49.484723 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 19:29:46.539392 140144802662208 spec.py:349] Evaluating on the test split.
I0213 19:29:49.241111 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 19:32:45.862113 140144802662208 submission_runner.py:408] Time since start: 49250.33s, 	Step: 84615, 	{'train/accuracy': 0.6412078142166138, 'train/loss': 1.7685593366622925, 'train/bleu': 30.883998162965828, 'validation/accuracy': 0.6598802208900452, 'validation/loss': 1.6230331659317017, 'validation/bleu': 27.8358965330051, 'validation/num_examples': 3000, 'test/accuracy': 0.6705595254898071, 'test/loss': 1.5564008951187134, 'test/bleu': 27.64755868083939, 'test/num_examples': 3003, 'score': 29432.80449271202, 'total_duration': 49250.32803225517, 'accumulated_submission_time': 29432.80449271202, 'accumulated_eval_time': 19813.59550333023, 'accumulated_logging_time': 1.1386573314666748}
I0213 19:32:45.889355 139975005800192 logging_writer.py:48] [84615] accumulated_eval_time=19813.595503, accumulated_logging_time=1.138657, accumulated_submission_time=29432.804493, global_step=84615, preemption_count=0, score=29432.804493, test/accuracy=0.670560, test/bleu=27.647559, test/loss=1.556401, test/num_examples=3003, total_duration=49250.328032, train/accuracy=0.641208, train/bleu=30.883998, train/loss=1.768559, validation/accuracy=0.659880, validation/bleu=27.835897, validation/loss=1.623033, validation/num_examples=3000
I0213 19:33:15.662932 139975014192896 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.29950061440467834, loss=1.753494381904602
I0213 19:33:50.354152 139975005800192 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.3404693007469177, loss=1.704439640045166
I0213 19:34:25.101152 139975014192896 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4778870940208435, loss=1.810202956199646
I0213 19:34:59.872103 139975005800192 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.33335307240486145, loss=1.7569010257720947
I0213 19:35:34.613068 139975014192896 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.40676620602607727, loss=1.7194817066192627
I0213 19:36:09.369918 139975005800192 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.34464895725250244, loss=1.8037928342819214
I0213 19:36:44.116949 139975014192896 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.38708409667015076, loss=1.798446536064148
I0213 19:37:18.865748 139975005800192 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.37657976150512695, loss=1.7705422639846802
I0213 19:37:53.621508 139975014192896 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.35466909408569336, loss=1.8324910402297974
I0213 19:38:28.422880 139975005800192 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.29846832156181335, loss=1.6940371990203857
I0213 19:39:03.209254 139975014192896 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.30236929655075073, loss=1.8044346570968628
I0213 19:39:37.992903 139975005800192 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.34642505645751953, loss=1.8677418231964111
I0213 19:40:12.757704 139975014192896 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.41681256890296936, loss=1.846387505531311
I0213 19:40:47.517101 139975005800192 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.30722901225090027, loss=1.8044872283935547
I0213 19:41:22.306672 139975014192896 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.32185444235801697, loss=1.7187646627426147
I0213 19:41:57.034334 139975005800192 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.2950018644332886, loss=1.7440588474273682
I0213 19:42:31.784277 139975014192896 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.4637998938560486, loss=1.7423765659332275
I0213 19:43:06.534532 139975005800192 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.3499073088169098, loss=1.8674886226654053
I0213 19:43:41.318685 139975014192896 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.377768337726593, loss=1.7557045221328735
I0213 19:44:16.093229 139975005800192 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.4017815887928009, loss=1.7334191799163818
I0213 19:44:50.869836 139975014192896 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.36363235116004944, loss=1.784513235092163
I0213 19:45:25.662741 139975005800192 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.36018961668014526, loss=1.7722176313400269
I0213 19:46:00.414515 139975014192896 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.3446546792984009, loss=1.7414562702178955
I0213 19:46:35.177716 139975005800192 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.30332428216934204, loss=1.717568278312683
I0213 19:46:46.035343 140144802662208 spec.py:321] Evaluating on the training split.
I0213 19:46:49.006599 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 19:50:21.099394 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 19:50:23.774027 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 19:53:12.629448 140144802662208 spec.py:349] Evaluating on the test split.
I0213 19:53:15.318999 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 19:55:55.077664 140144802662208 submission_runner.py:408] Time since start: 50639.54s, 	Step: 87033, 	{'train/accuracy': 0.6418304443359375, 'train/loss': 1.7551774978637695, 'train/bleu': 31.223073845288226, 'validation/accuracy': 0.6620252728462219, 'validation/loss': 1.6027518510818481, 'validation/bleu': 28.48434794106564, 'validation/num_examples': 3000, 'test/accuracy': 0.6758236289024353, 'test/loss': 1.5261303186416626, 'test/bleu': 28.10549738521797, 'test/num_examples': 3003, 'score': 30272.858829975128, 'total_duration': 50639.543586969376, 'accumulated_submission_time': 30272.858829975128, 'accumulated_eval_time': 20362.637778520584, 'accumulated_logging_time': 1.175865888595581}
I0213 19:55:55.104533 139975014192896 logging_writer.py:48] [87033] accumulated_eval_time=20362.637779, accumulated_logging_time=1.175866, accumulated_submission_time=30272.858830, global_step=87033, preemption_count=0, score=30272.858830, test/accuracy=0.675824, test/bleu=28.105497, test/loss=1.526130, test/num_examples=3003, total_duration=50639.543587, train/accuracy=0.641830, train/bleu=31.223074, train/loss=1.755177, validation/accuracy=0.662025, validation/bleu=28.484348, validation/loss=1.602752, validation/num_examples=3000
I0213 19:56:18.661179 139975005800192 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.30906161665916443, loss=1.7014079093933105
I0213 19:56:53.323419 139975014192896 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.4361996352672577, loss=1.7699488401412964
I0213 19:57:28.051916 139975005800192 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.3926136791706085, loss=1.7443008422851562
I0213 19:58:02.834117 139975014192896 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3282846510410309, loss=1.7431052923202515
I0213 19:58:37.605231 139975005800192 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4015975892543793, loss=1.8114800453186035
I0213 19:59:12.394901 139975014192896 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.3111626207828522, loss=1.7248716354370117
I0213 19:59:47.181222 139975005800192 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.33161312341690063, loss=1.7519217729568481
I0213 20:00:21.952203 139975014192896 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.27378422021865845, loss=1.701534390449524
I0213 20:00:56.843437 139975005800192 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.4917674660682678, loss=1.7289153337478638
I0213 20:01:31.620794 139975014192896 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.3117496967315674, loss=1.6803303956985474
I0213 20:02:06.432986 139975005800192 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.31304261088371277, loss=1.7784030437469482
I0213 20:02:41.240100 139975014192896 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.29547375440597534, loss=1.7451902627944946
I0213 20:03:16.014403 139975005800192 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.4740031361579895, loss=1.8021316528320312
I0213 20:03:50.762634 139975014192896 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.30038532614707947, loss=1.7689971923828125
I0213 20:04:25.522549 139975005800192 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.31603896617889404, loss=1.7713853120803833
I0213 20:05:00.272990 139975014192896 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3237515389919281, loss=1.6508227586746216
I0213 20:05:35.059984 139975005800192 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.36835017800331116, loss=1.7455698251724243
I0213 20:06:09.863870 139975014192896 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3241286277770996, loss=1.8034183979034424
I0213 20:06:44.673184 139975005800192 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3058755397796631, loss=1.7824454307556152
I0213 20:07:19.426147 139975014192896 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.36810457706451416, loss=1.750648856163025
I0213 20:07:54.180074 139975005800192 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.3339769244194031, loss=1.7569864988327026
I0213 20:08:28.982439 139975014192896 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.3100835978984833, loss=1.7631787061691284
I0213 20:09:03.778123 139975005800192 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.30979111790657043, loss=1.7367535829544067
I0213 20:09:38.534594 139975014192896 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.3328884541988373, loss=1.6991292238235474
I0213 20:09:55.297605 140144802662208 spec.py:321] Evaluating on the training split.
I0213 20:09:58.267694 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 20:14:14.366657 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 20:14:17.062330 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 20:17:52.179349 140144802662208 spec.py:349] Evaluating on the test split.
I0213 20:17:54.860043 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 20:21:38.971603 140144802662208 submission_runner.py:408] Time since start: 52183.44s, 	Step: 89450, 	{'train/accuracy': 0.6507663130760193, 'train/loss': 1.6971262693405151, 'train/bleu': 31.590313155448776, 'validation/accuracy': 0.6641083359718323, 'validation/loss': 1.5904189348220825, 'validation/bleu': 28.41043268094581, 'validation/num_examples': 3000, 'test/accuracy': 0.6781593561172485, 'test/loss': 1.5138115882873535, 'test/bleu': 28.19384526539295, 'test/num_examples': 3003, 'score': 31112.956319332123, 'total_duration': 52183.437497615814, 'accumulated_submission_time': 31112.956319332123, 'accumulated_eval_time': 21066.31170296669, 'accumulated_logging_time': 1.2124691009521484}
I0213 20:21:38.999979 139975005800192 logging_writer.py:48] [89450] accumulated_eval_time=21066.311703, accumulated_logging_time=1.212469, accumulated_submission_time=31112.956319, global_step=89450, preemption_count=0, score=31112.956319, test/accuracy=0.678159, test/bleu=28.193845, test/loss=1.513812, test/num_examples=3003, total_duration=52183.437498, train/accuracy=0.650766, train/bleu=31.590313, train/loss=1.697126, validation/accuracy=0.664108, validation/bleu=28.410433, validation/loss=1.590419, validation/num_examples=3000
I0213 20:21:56.646974 139975014192896 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.31263092160224915, loss=1.6792967319488525
I0213 20:22:31.338930 139975005800192 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.312168151140213, loss=1.700437068939209
I0213 20:23:06.125694 139975014192896 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3948420584201813, loss=1.7472200393676758
I0213 20:23:40.909861 139975005800192 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.2938788831233978, loss=1.7181626558303833
I0213 20:24:15.680462 139975014192896 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.38888469338417053, loss=1.7063686847686768
I0213 20:24:50.443992 139975005800192 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.31544366478919983, loss=1.8128944635391235
I0213 20:25:25.252651 139975014192896 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.33081430196762085, loss=1.7885112762451172
I0213 20:26:00.023228 139975005800192 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.32238563895225525, loss=1.7850751876831055
I0213 20:26:34.801949 139975014192896 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.4977531433105469, loss=1.8001419305801392
I0213 20:27:09.580994 139975005800192 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.2953753173351288, loss=1.6974211931228638
I0213 20:27:44.350601 139975014192896 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3348594307899475, loss=1.6673030853271484
I0213 20:28:19.194562 139975005800192 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.35757970809936523, loss=1.7489402294158936
I0213 20:28:54.035003 139975014192896 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.3226872980594635, loss=1.8287074565887451
I0213 20:29:28.810242 139975005800192 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.3374713361263275, loss=1.7765380144119263
I0213 20:30:03.567914 139975014192896 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.5017528533935547, loss=1.7592504024505615
I0213 20:30:38.350428 139975005800192 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.3113382160663605, loss=1.7247318029403687
I0213 20:31:13.127865 139975014192896 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.30306094884872437, loss=1.7397102117538452
I0213 20:31:47.876580 139975005800192 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.4794246554374695, loss=1.7830983400344849
I0213 20:32:22.675415 139975014192896 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.49885329604148865, loss=1.7963937520980835
I0213 20:32:57.448771 139975005800192 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.3308801054954529, loss=1.6822800636291504
I0213 20:33:32.268772 139975014192896 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.3509073853492737, loss=1.6938918828964233
I0213 20:34:07.063353 139975005800192 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.305631548166275, loss=1.7268825769424438
I0213 20:34:41.841226 139975014192896 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.4239908754825592, loss=1.791663646697998
I0213 20:35:16.639620 139975005800192 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.28281259536743164, loss=1.7334914207458496
I0213 20:35:38.983931 140144802662208 spec.py:321] Evaluating on the training split.
I0213 20:35:41.962242 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 20:40:03.708884 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 20:40:06.398291 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 20:43:20.803064 140144802662208 spec.py:349] Evaluating on the test split.
I0213 20:43:23.489657 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 20:46:22.599710 140144802662208 submission_runner.py:408] Time since start: 53667.07s, 	Step: 91866, 	{'train/accuracy': 0.6441076993942261, 'train/loss': 1.7398148775100708, 'train/bleu': 31.650447712787365, 'validation/accuracy': 0.6667988896369934, 'validation/loss': 1.5745916366577148, 'validation/bleu': 28.447228632456838, 'validation/num_examples': 3000, 'test/accuracy': 0.6778688430786133, 'test/loss': 1.5026555061340332, 'test/bleu': 28.213751552293576, 'test/num_examples': 3003, 'score': 31952.85079932213, 'total_duration': 53667.065616846085, 'accumulated_submission_time': 31952.85079932213, 'accumulated_eval_time': 21709.92742419243, 'accumulated_logging_time': 1.2507178783416748}
I0213 20:46:22.627438 139975014192896 logging_writer.py:48] [91866] accumulated_eval_time=21709.927424, accumulated_logging_time=1.250718, accumulated_submission_time=31952.850799, global_step=91866, preemption_count=0, score=31952.850799, test/accuracy=0.677869, test/bleu=28.213752, test/loss=1.502656, test/num_examples=3003, total_duration=53667.065617, train/accuracy=0.644108, train/bleu=31.650448, train/loss=1.739815, validation/accuracy=0.666799, validation/bleu=28.447229, validation/loss=1.574592, validation/num_examples=3000
I0213 20:46:34.746054 139975005800192 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.2877747714519501, loss=1.6816937923431396
I0213 20:47:09.400002 139975014192896 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.31809017062187195, loss=1.781902551651001
I0213 20:47:44.106478 139975005800192 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3844131529331207, loss=1.7272170782089233
I0213 20:48:18.869802 139975014192896 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.32064610719680786, loss=1.8196254968643188
I0213 20:48:53.661260 139975005800192 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.34224602580070496, loss=1.7033236026763916
I0213 20:49:28.461933 139975014192896 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.289858877658844, loss=1.8329899311065674
I0213 20:50:03.230254 139975005800192 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.3703325092792511, loss=1.6906741857528687
I0213 20:50:37.978774 139975014192896 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.32035699486732483, loss=1.648215651512146
I0213 20:51:12.746615 139975005800192 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.3352645933628082, loss=1.7655664682388306
I0213 20:51:47.515233 139975014192896 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.34106889367103577, loss=1.7793617248535156
I0213 20:52:22.274482 139975005800192 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.32514679431915283, loss=1.8059340715408325
I0213 20:52:57.052270 139975014192896 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.30520758032798767, loss=1.7823880910873413
I0213 20:53:31.844419 139975005800192 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.36150413751602173, loss=1.7313287258148193
I0213 20:54:06.612622 139975014192896 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.3700702488422394, loss=1.6900447607040405
I0213 20:54:41.403392 139975005800192 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.34360870718955994, loss=1.6447237730026245
I0213 20:55:16.181046 139975014192896 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.46121639013290405, loss=1.717410922050476
I0213 20:55:50.955712 139975005800192 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.37725937366485596, loss=1.6472781896591187
I0213 20:56:25.745404 139975014192896 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.3726438581943512, loss=1.7012635469436646
I0213 20:57:00.523922 139975005800192 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.34462496638298035, loss=1.7338868379592896
I0213 20:57:35.297660 139975014192896 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.3024623990058899, loss=1.651191234588623
I0213 20:58:10.061852 139975005800192 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.32885506749153137, loss=1.724170207977295
I0213 20:58:44.937349 139975014192896 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.33531254529953003, loss=1.7504806518554688
I0213 20:59:19.738810 139975005800192 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.34984877705574036, loss=1.8152302503585815
I0213 20:59:54.504551 139975014192896 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.34735921025276184, loss=1.6898624897003174
I0213 21:00:22.731709 140144802662208 spec.py:321] Evaluating on the training split.
I0213 21:00:25.711575 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 21:03:35.712420 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 21:03:38.388768 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 21:06:18.563618 140144802662208 spec.py:349] Evaluating on the test split.
I0213 21:06:21.243697 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 21:09:04.147752 140144802662208 submission_runner.py:408] Time since start: 55028.61s, 	Step: 94283, 	{'train/accuracy': 0.6625533699989319, 'train/loss': 1.6080623865127563, 'train/bleu': 33.16702852310345, 'validation/accuracy': 0.6701342463493347, 'validation/loss': 1.558275818824768, 'validation/bleu': 28.759051738058805, 'validation/num_examples': 3000, 'test/accuracy': 0.6807971596717834, 'test/loss': 1.4862122535705566, 'test/bleu': 28.15908149035932, 'test/num_examples': 3003, 'score': 32792.86261463165, 'total_duration': 55028.61365580559, 'accumulated_submission_time': 32792.86261463165, 'accumulated_eval_time': 22231.343421697617, 'accumulated_logging_time': 1.2895457744598389}
I0213 21:09:04.175521 139975005800192 logging_writer.py:48] [94283] accumulated_eval_time=22231.343422, accumulated_logging_time=1.289546, accumulated_submission_time=32792.862615, global_step=94283, preemption_count=0, score=32792.862615, test/accuracy=0.680797, test/bleu=28.159081, test/loss=1.486212, test/num_examples=3003, total_duration=55028.613656, train/accuracy=0.662553, train/bleu=33.167029, train/loss=1.608062, validation/accuracy=0.670134, validation/bleu=28.759052, validation/loss=1.558276, validation/num_examples=3000
I0213 21:09:10.420396 139975014192896 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.3605794906616211, loss=1.6545963287353516
I0213 21:09:45.046267 139975005800192 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.335762619972229, loss=1.6767911911010742
I0213 21:10:19.756141 139975014192896 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.3487527668476105, loss=1.6696631908416748
I0213 21:10:54.491481 139975005800192 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.29235899448394775, loss=1.7142657041549683
I0213 21:11:29.238194 139975014192896 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.3429478108882904, loss=1.6780154705047607
I0213 21:12:03.982461 139975005800192 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.32937487959861755, loss=1.6852774620056152
I0213 21:12:38.782521 139975014192896 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.41412273049354553, loss=1.769911527633667
I0213 21:13:13.564562 139975005800192 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.37061697244644165, loss=1.673203468322754
I0213 21:13:48.365064 139975014192896 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.32970568537712097, loss=1.6709831953048706
I0213 21:14:23.114351 139975005800192 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.391988068819046, loss=1.701854944229126
I0213 21:14:57.868541 139975014192896 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3703886866569519, loss=1.7764283418655396
I0213 21:15:32.635390 139975005800192 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.3091052770614624, loss=1.7023988962173462
I0213 21:16:07.383990 139975014192896 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.31899556517601013, loss=1.679360032081604
I0213 21:16:42.139788 139975005800192 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.3816989064216614, loss=1.718185544013977
I0213 21:17:16.900908 139975014192896 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.309572696685791, loss=1.778762936592102
I0213 21:17:51.659678 139975005800192 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.3488711416721344, loss=1.6291993856430054
I0213 21:18:26.434356 139975014192896 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.3225807547569275, loss=1.648133397102356
I0213 21:19:01.181962 139975005800192 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.41585955023765564, loss=1.7173069715499878
I0213 21:19:35.934672 139975014192896 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.37642353773117065, loss=1.7292746305465698
I0213 21:20:10.677002 139975005800192 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.3159036636352539, loss=1.5727847814559937
I0213 21:20:45.418567 139975014192896 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.3934773802757263, loss=1.6876976490020752
I0213 21:21:20.178380 139975005800192 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.3095768690109253, loss=1.6610194444656372
I0213 21:21:54.936234 139975014192896 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.3594754934310913, loss=1.6858551502227783
I0213 21:22:29.695561 139975005800192 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.33127766847610474, loss=1.7534185647964478
I0213 21:23:04.494950 139975014192896 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3755570352077484, loss=1.6663060188293457
I0213 21:23:04.502985 140144802662208 spec.py:321] Evaluating on the training split.
I0213 21:23:07.194468 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 21:26:51.746404 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 21:26:54.423639 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 21:29:33.677803 140144802662208 spec.py:349] Evaluating on the test split.
I0213 21:29:36.360911 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 21:32:10.942386 140144802662208 submission_runner.py:408] Time since start: 56415.41s, 	Step: 96701, 	{'train/accuracy': 0.6566953063011169, 'train/loss': 1.6568286418914795, 'train/bleu': 31.842143989658336, 'validation/accuracy': 0.670741856098175, 'validation/loss': 1.5458422899246216, 'validation/bleu': 29.138297350353145, 'validation/num_examples': 3000, 'test/accuracy': 0.6864447593688965, 'test/loss': 1.4664220809936523, 'test/bleu': 29.245032393980694, 'test/num_examples': 3003, 'score': 33633.10122871399, 'total_duration': 56415.408299446106, 'accumulated_submission_time': 33633.10122871399, 'accumulated_eval_time': 22777.782760620117, 'accumulated_logging_time': 1.3282885551452637}
I0213 21:32:10.971311 139975005800192 logging_writer.py:48] [96701] accumulated_eval_time=22777.782761, accumulated_logging_time=1.328289, accumulated_submission_time=33633.101229, global_step=96701, preemption_count=0, score=33633.101229, test/accuracy=0.686445, test/bleu=29.245032, test/loss=1.466422, test/num_examples=3003, total_duration=56415.408299, train/accuracy=0.656695, train/bleu=31.842144, train/loss=1.656829, validation/accuracy=0.670742, validation/bleu=29.138297, validation/loss=1.545842, validation/num_examples=3000
I0213 21:32:45.600873 139975014192896 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.3353913128376007, loss=1.6977229118347168
I0213 21:33:20.318261 139975005800192 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.3150971233844757, loss=1.6982991695404053
I0213 21:33:55.065306 139975014192896 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3336534798145294, loss=1.6989082098007202
I0213 21:34:29.851211 139975005800192 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.3272857964038849, loss=1.6096333265304565
I0213 21:35:04.650003 139975014192896 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.34526515007019043, loss=1.700994849205017
I0213 21:35:39.391855 139975005800192 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.36628589034080505, loss=1.7283960580825806
I0213 21:36:14.169808 139975014192896 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.40810340642929077, loss=1.6496262550354004
I0213 21:36:48.961849 139975005800192 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.30648189783096313, loss=1.6169037818908691
I0213 21:37:23.733470 139975014192896 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.3085680305957794, loss=1.7070300579071045
I0213 21:37:58.541417 139975005800192 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.3191620409488678, loss=1.680638074874878
I0213 21:38:33.336862 139975014192896 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.32589513063430786, loss=1.7178410291671753
I0213 21:39:08.124645 139975005800192 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.3155665695667267, loss=1.6835483312606812
I0213 21:39:42.930909 139975014192896 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.29621806740760803, loss=1.658790111541748
I0213 21:40:17.700342 139975005800192 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.3108871281147003, loss=1.7279744148254395
I0213 21:40:52.490373 139975014192896 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.30577176809310913, loss=1.6720192432403564
I0213 21:41:27.293584 139975005800192 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.3127829432487488, loss=1.654556155204773
I0213 21:42:02.070630 139975014192896 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.31109681725502014, loss=1.6263573169708252
I0213 21:42:36.847755 139975005800192 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3692007064819336, loss=1.6600273847579956
I0213 21:43:11.609690 139975014192896 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.31731346249580383, loss=1.702536702156067
I0213 21:43:46.387912 139975005800192 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.4224923551082611, loss=1.6715837717056274
I0213 21:44:21.149360 139975014192896 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.30389803647994995, loss=1.6136021614074707
I0213 21:44:55.936606 139975005800192 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.32212555408477783, loss=1.707303762435913
I0213 21:45:30.732259 139975014192896 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3293532729148865, loss=1.6241940259933472
I0213 21:46:05.510421 139975005800192 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.33742526173591614, loss=1.6670384407043457
I0213 21:46:11.150621 140144802662208 spec.py:321] Evaluating on the training split.
I0213 21:46:14.134095 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 21:49:15.776940 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 21:49:18.462651 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 21:51:57.005146 140144802662208 spec.py:349] Evaluating on the test split.
I0213 21:51:59.682383 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 21:54:36.074184 140144802662208 submission_runner.py:408] Time since start: 57760.54s, 	Step: 99118, 	{'train/accuracy': 0.6576961874961853, 'train/loss': 1.6613869667053223, 'train/bleu': 32.159575954341314, 'validation/accuracy': 0.6734076142311096, 'validation/loss': 1.527236819267273, 'validation/bleu': 29.481519388764976, 'validation/num_examples': 3000, 'test/accuracy': 0.6870489716529846, 'test/loss': 1.4522655010223389, 'test/bleu': 29.00504494574323, 'test/num_examples': 3003, 'score': 34473.18996334076, 'total_duration': 57760.54006314278, 'accumulated_submission_time': 34473.18996334076, 'accumulated_eval_time': 23282.706238031387, 'accumulated_logging_time': 1.3684589862823486}
I0213 21:54:36.102786 139975014192896 logging_writer.py:48] [99118] accumulated_eval_time=23282.706238, accumulated_logging_time=1.368459, accumulated_submission_time=34473.189963, global_step=99118, preemption_count=0, score=34473.189963, test/accuracy=0.687049, test/bleu=29.005045, test/loss=1.452266, test/num_examples=3003, total_duration=57760.540063, train/accuracy=0.657696, train/bleu=32.159576, train/loss=1.661387, validation/accuracy=0.673408, validation/bleu=29.481519, validation/loss=1.527237, validation/num_examples=3000
I0213 21:55:04.853182 139975005800192 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.30501464009284973, loss=1.6148648262023926
I0213 21:55:39.504109 139975014192896 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.3478807806968689, loss=1.6902143955230713
I0213 21:56:14.245750 139975005800192 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.3962819278240204, loss=1.6669946908950806
I0213 21:56:49.028110 139975014192896 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.36620283126831055, loss=1.623297095298767
I0213 21:57:23.783068 139975005800192 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.3198612332344055, loss=1.6469448804855347
I0213 21:57:58.535065 139975014192896 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.30955350399017334, loss=1.5938209295272827
I0213 21:58:33.291087 139975005800192 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.32548293471336365, loss=1.607991337776184
I0213 21:59:08.047562 139975014192896 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.34935352206230164, loss=1.6769824028015137
I0213 21:59:42.808035 139975005800192 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.3076825439929962, loss=1.670943260192871
I0213 22:00:17.602416 139975014192896 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.34967395663261414, loss=1.7058738470077515
I0213 22:00:52.356757 139975005800192 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.34653809666633606, loss=1.6605401039123535
I0213 22:01:27.110619 139975014192896 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.3305831253528595, loss=1.6051181554794312
I0213 22:02:01.863383 139975005800192 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.33236321806907654, loss=1.67253839969635
I0213 22:02:36.631662 139975014192896 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3397039771080017, loss=1.6653345823287964
I0213 22:03:11.392011 139975005800192 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.40777671337127686, loss=1.601786732673645
I0213 22:03:46.134557 139975014192896 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.35774967074394226, loss=1.6447830200195312
I0213 22:04:20.917363 139975005800192 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.36323779821395874, loss=1.651460886001587
I0213 22:04:55.754475 139975014192896 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.3075774908065796, loss=1.6696747541427612
I0213 22:05:30.519994 139975005800192 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.32132765650749207, loss=1.6747745275497437
I0213 22:06:05.273117 139975014192896 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.3528605103492737, loss=1.6181526184082031
I0213 22:06:40.030255 139975005800192 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.3116283416748047, loss=1.643955111503601
I0213 22:07:14.759414 139975014192896 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.30664554238319397, loss=1.6121292114257812
I0213 22:07:49.537082 139975005800192 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.35760068893432617, loss=1.6412464380264282
I0213 22:08:24.323215 139975014192896 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3187587857246399, loss=1.6263059377670288
I0213 22:08:36.207707 140144802662208 spec.py:321] Evaluating on the training split.
I0213 22:08:39.188410 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 22:12:39.986208 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 22:12:42.667709 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 22:15:48.969034 140144802662208 spec.py:349] Evaluating on the test split.
I0213 22:15:51.645170 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 22:18:44.202626 140144802662208 submission_runner.py:408] Time since start: 59208.67s, 	Step: 101536, 	{'train/accuracy': 0.6628363132476807, 'train/loss': 1.61624276638031, 'train/bleu': 33.3643248815988, 'validation/accuracy': 0.6767801642417908, 'validation/loss': 1.5128036737442017, 'validation/bleu': 29.354188999467766, 'validation/num_examples': 3000, 'test/accuracy': 0.6894544363021851, 'test/loss': 1.432341456413269, 'test/bleu': 29.022071865705737, 'test/num_examples': 3003, 'score': 35313.20211029053, 'total_duration': 59208.66853928566, 'accumulated_submission_time': 35313.20211029053, 'accumulated_eval_time': 23890.701112031937, 'accumulated_logging_time': 1.4091482162475586}
I0213 22:18:44.231734 139975005800192 logging_writer.py:48] [101536] accumulated_eval_time=23890.701112, accumulated_logging_time=1.409148, accumulated_submission_time=35313.202110, global_step=101536, preemption_count=0, score=35313.202110, test/accuracy=0.689454, test/bleu=29.022072, test/loss=1.432341, test/num_examples=3003, total_duration=59208.668539, train/accuracy=0.662836, train/bleu=33.364325, train/loss=1.616243, validation/accuracy=0.676780, validation/bleu=29.354189, validation/loss=1.512804, validation/num_examples=3000
I0213 22:19:06.705890 139975014192896 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.322298526763916, loss=1.7112364768981934
I0213 22:19:41.349684 139975005800192 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.3366471230983734, loss=1.6522445678710938
I0213 22:20:16.139016 139975014192896 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.29798465967178345, loss=1.6154730319976807
I0213 22:20:50.873289 139975005800192 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.3480256497859955, loss=1.7339576482772827
I0213 22:21:25.624514 139975014192896 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3666865825653076, loss=1.621525526046753
I0213 22:22:00.361098 139975005800192 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.3173816502094269, loss=1.595664620399475
I0213 22:22:35.144937 139975014192896 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.32910865545272827, loss=1.662650227546692
I0213 22:23:09.908852 139975005800192 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.35313495993614197, loss=1.758935570716858
I0213 22:23:44.652845 139975014192896 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.33787351846694946, loss=1.699729084968567
I0213 22:24:19.411054 139975005800192 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3195940852165222, loss=1.6147488355636597
I0213 22:24:54.212484 139975014192896 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.3254532516002655, loss=1.559380054473877
I0213 22:25:28.977934 139975005800192 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.3128408193588257, loss=1.5874602794647217
I0213 22:26:03.752655 139975014192896 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.3237089216709137, loss=1.572993278503418
I0213 22:26:38.510106 139975005800192 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.3339758515357971, loss=1.6746121644973755
I0213 22:27:13.275507 139975014192896 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.32794642448425293, loss=1.6296470165252686
I0213 22:27:48.065311 139975005800192 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.3063964545726776, loss=1.5725528001785278
I0213 22:28:22.879974 139975014192896 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.308906227350235, loss=1.5121209621429443
I0213 22:28:57.657669 139975005800192 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.3224445581436157, loss=1.5547853708267212
I0213 22:29:32.401419 139975014192896 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.32959726452827454, loss=1.6410140991210938
I0213 22:30:07.170827 139975005800192 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3299504518508911, loss=1.6103155612945557
I0213 22:30:41.933304 139975014192896 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.30085062980651855, loss=1.5985212326049805
I0213 22:31:16.677224 139975005800192 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.31172922253608704, loss=1.6372346878051758
I0213 22:31:51.416626 139975014192896 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.3537643849849701, loss=1.6189323663711548
I0213 22:32:26.172417 139975005800192 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.38180145621299744, loss=1.6955785751342773
I0213 22:32:44.310458 140144802662208 spec.py:321] Evaluating on the training split.
I0213 22:32:47.286090 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 22:36:33.346075 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 22:36:36.035593 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 22:39:13.608202 140144802662208 spec.py:349] Evaluating on the test split.
I0213 22:39:16.309342 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 22:42:20.035470 140144802662208 submission_runner.py:408] Time since start: 60624.50s, 	Step: 103954, 	{'train/accuracy': 0.6624199748039246, 'train/loss': 1.6151397228240967, 'train/bleu': 32.52034005600726, 'validation/accuracy': 0.6787640452384949, 'validation/loss': 1.4980305433273315, 'validation/bleu': 29.72840660397787, 'validation/num_examples': 3000, 'test/accuracy': 0.6900238394737244, 'test/loss': 1.422572374343872, 'test/bleu': 29.251162714857767, 'test/num_examples': 3003, 'score': 36153.19030022621, 'total_duration': 60624.50135970116, 'accumulated_submission_time': 36153.19030022621, 'accumulated_eval_time': 24466.426047563553, 'accumulated_logging_time': 1.4494218826293945}
I0213 22:42:20.072286 139975014192896 logging_writer.py:48] [103954] accumulated_eval_time=24466.426048, accumulated_logging_time=1.449422, accumulated_submission_time=36153.190300, global_step=103954, preemption_count=0, score=36153.190300, test/accuracy=0.690024, test/bleu=29.251163, test/loss=1.422572, test/num_examples=3003, total_duration=60624.501360, train/accuracy=0.662420, train/bleu=32.520340, train/loss=1.615140, validation/accuracy=0.678764, validation/bleu=29.728407, validation/loss=1.498031, validation/num_examples=3000
I0213 22:42:36.335383 139975005800192 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.37530261278152466, loss=1.6400190591812134
I0213 22:43:11.043205 139975014192896 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.34825804829597473, loss=1.634990930557251
I0213 22:43:45.825231 139975005800192 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.33635494112968445, loss=1.6672334671020508
I0213 22:44:20.598106 139975014192896 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.32313257455825806, loss=1.6703635454177856
I0213 22:44:55.362636 139975005800192 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.30945178866386414, loss=1.5740834474563599
I0213 22:45:30.113618 139975014192896 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.31733375787734985, loss=1.6473816633224487
I0213 22:46:04.881348 139975005800192 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.3335844874382019, loss=1.5999044179916382
I0213 22:46:39.639089 139975014192896 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.3739813268184662, loss=1.6357145309448242
I0213 22:47:14.392666 139975005800192 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.3396577835083008, loss=1.6640005111694336
I0213 22:47:49.139913 139975014192896 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.34386059641838074, loss=1.6348744630813599
I0213 22:48:23.916608 139975005800192 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.3287011981010437, loss=1.6607924699783325
I0213 22:48:58.673540 139975014192896 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.3276404142379761, loss=1.6242247819900513
I0213 22:49:33.433739 139975005800192 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.33118799328804016, loss=1.6736100912094116
I0213 22:50:08.187649 139975014192896 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.34574243426322937, loss=1.565940260887146
I0213 22:50:42.961909 139975005800192 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3241349160671234, loss=1.557070255279541
I0213 22:51:17.724897 139975014192896 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.32714635133743286, loss=1.5808089971542358
I0213 22:51:52.477954 139975005800192 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3281995952129364, loss=1.6477025747299194
I0213 22:52:27.241725 139975014192896 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.33335986733436584, loss=1.5893126726150513
I0213 22:53:01.977610 139975005800192 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.3570616841316223, loss=1.653172254562378
I0213 22:53:36.774189 139975014192896 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.3083723783493042, loss=1.6167649030685425
I0213 22:54:11.599007 139975005800192 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.3147081732749939, loss=1.6020913124084473
I0213 22:54:46.379188 139975014192896 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.32527220249176025, loss=1.7048149108886719
I0213 22:55:21.161681 139975005800192 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.38033798336982727, loss=1.6339812278747559
I0213 22:55:55.921616 139975014192896 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.3263424336910248, loss=1.661798119544983
I0213 22:56:20.328490 140144802662208 spec.py:321] Evaluating on the training split.
I0213 22:56:23.304220 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 22:59:59.315645 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 23:00:02.031605 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 23:02:46.513002 140144802662208 spec.py:349] Evaluating on the test split.
I0213 23:02:49.212260 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 23:05:44.495271 140144802662208 submission_runner.py:408] Time since start: 62028.96s, 	Step: 106372, 	{'train/accuracy': 0.7069824934005737, 'train/loss': 1.3659493923187256, 'train/bleu': 36.09946513355275, 'validation/accuracy': 0.6812934875488281, 'validation/loss': 1.4833416938781738, 'validation/bleu': 29.75335012117342, 'validation/num_examples': 3000, 'test/accuracy': 0.6945441961288452, 'test/loss': 1.4052151441574097, 'test/bleu': 29.731491757638405, 'test/num_examples': 3003, 'score': 36993.35413503647, 'total_duration': 62028.96114087105, 'accumulated_submission_time': 36993.35413503647, 'accumulated_eval_time': 25030.5927362442, 'accumulated_logging_time': 1.497650384902954}
I0213 23:05:44.524460 139975005800192 logging_writer.py:48] [106372] accumulated_eval_time=25030.592736, accumulated_logging_time=1.497650, accumulated_submission_time=36993.354135, global_step=106372, preemption_count=0, score=36993.354135, test/accuracy=0.694544, test/bleu=29.731492, test/loss=1.405215, test/num_examples=3003, total_duration=62028.961141, train/accuracy=0.706982, train/bleu=36.099465, train/loss=1.365949, validation/accuracy=0.681293, validation/bleu=29.753350, validation/loss=1.483342, validation/num_examples=3000
I0213 23:05:54.559389 139975014192896 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.3420664370059967, loss=1.614568829536438
I0213 23:06:29.208572 139975005800192 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.3764939606189728, loss=1.6008211374282837
I0213 23:07:03.978799 139975014192896 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.3386727273464203, loss=1.5645265579223633
I0213 23:07:38.721274 139975005800192 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.3389844596385956, loss=1.5968650579452515
I0213 23:08:13.462492 139975014192896 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.33056190609931946, loss=1.639359951019287
I0213 23:08:48.196931 139975005800192 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.33890336751937866, loss=1.5754709243774414
I0213 23:09:22.961027 139975014192896 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.33886125683784485, loss=1.6793078184127808
I0213 23:09:57.715680 139975005800192 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.33054205775260925, loss=1.538469910621643
I0213 23:10:32.469734 139975014192896 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.31803977489471436, loss=1.5486615896224976
I0213 23:11:07.214762 139975005800192 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3194579780101776, loss=1.5658845901489258
I0213 23:11:41.987238 139975014192896 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.3427530527114868, loss=1.6359730958938599
I0213 23:12:16.769449 139975005800192 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.30708128213882446, loss=1.5659173727035522
I0213 23:12:51.518931 139975014192896 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.32675158977508545, loss=1.4943567514419556
I0213 23:13:26.286648 139975005800192 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.34652361273765564, loss=1.6224684715270996
I0213 23:14:01.034265 139975014192896 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.31714141368865967, loss=1.5614203214645386
I0213 23:14:35.807925 139975005800192 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.3585050702095032, loss=1.556179404258728
I0213 23:15:10.560860 139975014192896 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.3303268551826477, loss=1.6402021646499634
I0213 23:15:45.308084 139975005800192 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.36844685673713684, loss=1.6049505472183228
I0213 23:16:20.064314 139975014192896 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.40656790137290955, loss=1.653714656829834
I0213 23:16:54.821655 139975005800192 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.3692249357700348, loss=1.6239430904388428
I0213 23:17:29.577670 139975014192896 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3066381812095642, loss=1.5522453784942627
I0213 23:18:04.357438 139975005800192 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.3275121748447418, loss=1.598196029663086
I0213 23:18:39.121405 139975014192896 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.3182981014251709, loss=1.5830142498016357
I0213 23:19:13.886340 139975005800192 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.32455435395240784, loss=1.536889910697937
I0213 23:19:44.509756 140144802662208 spec.py:321] Evaluating on the training split.
I0213 23:19:47.481877 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 23:23:34.588387 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 23:23:37.280210 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 23:26:26.737490 140144802662208 spec.py:349] Evaluating on the test split.
I0213 23:26:29.422824 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 23:29:18.725508 140144802662208 submission_runner.py:408] Time since start: 63443.19s, 	Step: 108790, 	{'train/accuracy': 0.6711868047714233, 'train/loss': 1.5594106912612915, 'train/bleu': 33.68167779038636, 'validation/accuracy': 0.682409405708313, 'validation/loss': 1.4713393449783325, 'validation/bleu': 29.792658094298666, 'validation/num_examples': 3000, 'test/accuracy': 0.6963105201721191, 'test/loss': 1.3900195360183716, 'test/bleu': 29.78246186358487, 'test/num_examples': 3003, 'score': 37833.25030493736, 'total_duration': 63443.191420555115, 'accumulated_submission_time': 37833.25030493736, 'accumulated_eval_time': 25604.80843281746, 'accumulated_logging_time': 1.5368640422821045}
I0213 23:29:18.755241 139975014192896 logging_writer.py:48] [108790] accumulated_eval_time=25604.808433, accumulated_logging_time=1.536864, accumulated_submission_time=37833.250305, global_step=108790, preemption_count=0, score=37833.250305, test/accuracy=0.696311, test/bleu=29.782462, test/loss=1.390020, test/num_examples=3003, total_duration=63443.191421, train/accuracy=0.671187, train/bleu=33.681678, train/loss=1.559411, validation/accuracy=0.682409, validation/bleu=29.792658, validation/loss=1.471339, validation/num_examples=3000
I0213 23:29:22.578007 139975005800192 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3230619430541992, loss=1.5070768594741821
I0213 23:29:57.161081 139975014192896 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.34294095635414124, loss=1.5336483716964722
I0213 23:30:31.902480 139975005800192 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.3313225507736206, loss=1.6486324071884155
I0213 23:31:06.670278 139975014192896 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3456762433052063, loss=1.549018144607544
I0213 23:31:41.387004 139975005800192 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.3255211412906647, loss=1.5751522779464722
I0213 23:32:16.130406 139975014192896 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3421555161476135, loss=1.5295913219451904
I0213 23:32:50.894397 139975005800192 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.33913347125053406, loss=1.492798924446106
I0213 23:33:25.660526 139975014192896 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.3383401334285736, loss=1.59440279006958
I0213 23:34:00.424097 139975005800192 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.32456740736961365, loss=1.5354042053222656
I0213 23:34:35.172716 139975014192896 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.3668815791606903, loss=1.5629721879959106
I0213 23:35:09.916072 139975005800192 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.34186404943466187, loss=1.54240882396698
I0213 23:35:44.652216 139975014192896 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.3422689437866211, loss=1.6004512310028076
I0213 23:36:19.407913 139975005800192 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.3373136818408966, loss=1.491896152496338
I0213 23:36:54.162626 139975014192896 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.3499677777290344, loss=1.5717328786849976
I0213 23:37:28.915014 139975005800192 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.33912667632102966, loss=1.6144986152648926
I0213 23:38:03.667100 139975014192896 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.33954691886901855, loss=1.5088114738464355
I0213 23:38:38.415365 139975005800192 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3542868196964264, loss=1.5390263795852661
I0213 23:39:13.164110 139975014192896 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3353056311607361, loss=1.5569547414779663
I0213 23:39:48.045835 139975005800192 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.3452148139476776, loss=1.47675621509552
I0213 23:40:22.881962 139975014192896 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.3420509994029999, loss=1.5315344333648682
I0213 23:40:57.717326 139975005800192 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.3449952006340027, loss=1.5970031023025513
I0213 23:41:32.511944 139975014192896 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.32701408863067627, loss=1.6172620058059692
I0213 23:42:07.261640 139975005800192 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.3311527371406555, loss=1.6010680198669434
I0213 23:42:42.024685 139975014192896 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.3467862904071808, loss=1.5942811965942383
I0213 23:43:16.830502 139975005800192 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.33676740527153015, loss=1.5601698160171509
I0213 23:43:18.993593 140144802662208 spec.py:321] Evaluating on the training split.
I0213 23:43:21.965098 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 23:47:41.765273 140144802662208 spec.py:333] Evaluating on the validation split.
I0213 23:47:44.457173 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 23:50:48.583621 140144802662208 spec.py:349] Evaluating on the test split.
I0213 23:50:51.282862 140144802662208 workload.py:181] Translating evaluation dataset.
I0213 23:54:06.054136 140144802662208 submission_runner.py:408] Time since start: 64930.52s, 	Step: 111208, 	{'train/accuracy': 0.6703004240989685, 'train/loss': 1.5756620168685913, 'train/bleu': 33.46816417771401, 'validation/accuracy': 0.683078944683075, 'validation/loss': 1.4614923000335693, 'validation/bleu': 29.834149421480117, 'validation/num_examples': 3000, 'test/accuracy': 0.6995410323143005, 'test/loss': 1.3734357357025146, 'test/bleu': 30.15858276774193, 'test/num_examples': 3003, 'score': 38673.397919654846, 'total_duration': 64930.52002501488, 'accumulated_submission_time': 38673.397919654846, 'accumulated_eval_time': 26251.868897914886, 'accumulated_logging_time': 1.5767569541931152}
I0213 23:54:06.091389 139975014192896 logging_writer.py:48] [111208] accumulated_eval_time=26251.868898, accumulated_logging_time=1.576757, accumulated_submission_time=38673.397920, global_step=111208, preemption_count=0, score=38673.397920, test/accuracy=0.699541, test/bleu=30.158583, test/loss=1.373436, test/num_examples=3003, total_duration=64930.520025, train/accuracy=0.670300, train/bleu=33.468164, train/loss=1.575662, validation/accuracy=0.683079, validation/bleu=29.834149, validation/loss=1.461492, validation/num_examples=3000
I0213 23:54:38.306899 139975005800192 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.342715322971344, loss=1.5154035091400146
I0213 23:55:13.020045 139975014192896 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.34971433877944946, loss=1.580340027809143
I0213 23:55:47.850422 139975005800192 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.33393239974975586, loss=1.5870845317840576
I0213 23:56:22.683965 139975014192896 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3444371223449707, loss=1.5741204023361206
I0213 23:56:57.438889 139975005800192 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.35448771715164185, loss=1.5733968019485474
I0213 23:57:32.189558 139975014192896 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.3394358456134796, loss=1.4975696802139282
I0213 23:58:06.954996 139975005800192 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.3191288113594055, loss=1.5626535415649414
I0213 23:58:41.675390 139975014192896 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3655872046947479, loss=1.4864751100540161
I0213 23:59:16.427145 139975005800192 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.3535778820514679, loss=1.5068591833114624
I0213 23:59:51.187370 139975014192896 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.3731246888637543, loss=1.653937816619873
I0214 00:00:25.943118 139975005800192 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.3496634364128113, loss=1.588747262954712
I0214 00:01:00.719606 139975014192896 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3289726674556732, loss=1.4942735433578491
I0214 00:01:35.494267 139975005800192 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.3477688133716583, loss=1.5423328876495361
I0214 00:02:10.257097 139975014192896 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.3703565299510956, loss=1.5663399696350098
I0214 00:02:45.025322 139975005800192 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.34765177965164185, loss=1.4854276180267334
I0214 00:03:19.776293 139975014192896 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.3482489287853241, loss=1.5232174396514893
I0214 00:03:54.537573 139975005800192 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3586587607860565, loss=1.5559074878692627
I0214 00:04:29.338739 139975014192896 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.37543419003486633, loss=1.5794780254364014
I0214 00:05:04.102385 139975005800192 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.3299887478351593, loss=1.4866517782211304
I0214 00:05:38.868536 139975014192896 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.34790894389152527, loss=1.5719059705734253
I0214 00:06:13.678235 139975005800192 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.37127798795700073, loss=1.6189371347427368
I0214 00:06:48.419450 139975014192896 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.35088974237442017, loss=1.560632348060608
I0214 00:07:23.174699 139975005800192 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.35147595405578613, loss=1.5403715372085571
I0214 00:07:57.920564 139975014192896 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.3689768314361572, loss=1.591944694519043
I0214 00:08:06.339778 140144802662208 spec.py:321] Evaluating on the training split.
I0214 00:08:09.312837 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 00:11:57.838648 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 00:12:00.530344 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 00:14:34.582230 140144802662208 spec.py:349] Evaluating on the test split.
I0214 00:14:37.269783 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 00:17:20.543496 140144802662208 submission_runner.py:408] Time since start: 66325.01s, 	Step: 113626, 	{'train/accuracy': 0.6843309998512268, 'train/loss': 1.4797334671020508, 'train/bleu': 34.52292679167273, 'validation/accuracy': 0.6871954202651978, 'validation/loss': 1.4451779127120972, 'validation/bleu': 30.258902249238222, 'validation/num_examples': 3000, 'test/accuracy': 0.7013537883758545, 'test/loss': 1.3622872829437256, 'test/bleu': 30.219203249138683, 'test/num_examples': 3003, 'score': 39513.55310797691, 'total_duration': 66325.00939846039, 'accumulated_submission_time': 39513.55310797691, 'accumulated_eval_time': 26806.072550058365, 'accumulated_logging_time': 1.625173568725586}
I0214 00:17:20.574089 139975005800192 logging_writer.py:48] [113626] accumulated_eval_time=26806.072550, accumulated_logging_time=1.625174, accumulated_submission_time=39513.553108, global_step=113626, preemption_count=0, score=39513.553108, test/accuracy=0.701354, test/bleu=30.219203, test/loss=1.362287, test/num_examples=3003, total_duration=66325.009398, train/accuracy=0.684331, train/bleu=34.522927, train/loss=1.479733, validation/accuracy=0.687195, validation/bleu=30.258902, validation/loss=1.445178, validation/num_examples=3000
I0214 00:17:46.534047 139975014192896 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.38510170578956604, loss=1.6042075157165527
I0214 00:18:21.196815 139975005800192 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.32037627696990967, loss=1.5150285959243774
I0214 00:18:55.938467 139975014192896 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.3606693148612976, loss=1.6134475469589233
I0214 00:19:30.666026 139975005800192 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.34044113755226135, loss=1.489710807800293
I0214 00:20:05.410335 139975014192896 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.3365314304828644, loss=1.460693120956421
I0214 00:20:40.162764 139975005800192 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.3489801585674286, loss=1.466134786605835
I0214 00:21:14.925009 139975014192896 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.37305402755737305, loss=1.5568004846572876
I0214 00:21:49.664191 139975005800192 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3629971742630005, loss=1.5236502885818481
I0214 00:22:24.411628 139975014192896 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.3472379148006439, loss=1.5073578357696533
I0214 00:22:59.217727 139975005800192 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.36030587553977966, loss=1.5321447849273682
I0214 00:23:33.983316 139975014192896 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.35787835717201233, loss=1.5888383388519287
I0214 00:24:08.749186 139975005800192 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.35527369379997253, loss=1.4868311882019043
I0214 00:24:43.522855 139975014192896 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.350313276052475, loss=1.5235276222229004
I0214 00:25:18.284869 139975005800192 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.3482855260372162, loss=1.5176750421524048
I0214 00:25:53.013789 139975014192896 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.34582123160362244, loss=1.5147864818572998
I0214 00:26:27.758603 139975005800192 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.35614773631095886, loss=1.4821653366088867
I0214 00:27:02.513804 139975014192896 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.3836819529533386, loss=1.5502184629440308
I0214 00:27:37.278841 139975005800192 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.3603372573852539, loss=1.5330594778060913
I0214 00:28:12.055429 139975014192896 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.32131126523017883, loss=1.4864754676818848
I0214 00:28:46.882832 139975005800192 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.3891836106777191, loss=1.5111697912216187
I0214 00:29:21.678016 139975014192896 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.3647124767303467, loss=1.531551480293274
I0214 00:29:56.561387 139975005800192 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.3565569221973419, loss=1.4819798469543457
I0214 00:30:31.346688 139975014192896 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.3343636989593506, loss=1.4796388149261475
I0214 00:31:06.108890 139975005800192 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.3555670976638794, loss=1.5469141006469727
I0214 00:31:20.775934 140144802662208 spec.py:321] Evaluating on the training split.
I0214 00:31:23.742222 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 00:34:53.840766 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 00:34:56.515709 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 00:38:02.186996 140144802662208 spec.py:349] Evaluating on the test split.
I0214 00:38:04.871469 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 00:41:32.838541 140144802662208 submission_runner.py:408] Time since start: 67777.30s, 	Step: 116044, 	{'train/accuracy': 0.677942156791687, 'train/loss': 1.5225650072097778, 'train/bleu': 34.21801980482632, 'validation/accuracy': 0.6876294016838074, 'validation/loss': 1.43970787525177, 'validation/bleu': 30.542062526187728, 'validation/num_examples': 3000, 'test/accuracy': 0.7033408880233765, 'test/loss': 1.350867509841919, 'test/bleu': 30.47281998498783, 'test/num_examples': 3003, 'score': 40353.66239070892, 'total_duration': 67777.30443549156, 'accumulated_submission_time': 40353.66239070892, 'accumulated_eval_time': 27418.13508272171, 'accumulated_logging_time': 1.6672179698944092}
I0214 00:41:32.870952 139975014192896 logging_writer.py:48] [116044] accumulated_eval_time=27418.135083, accumulated_logging_time=1.667218, accumulated_submission_time=40353.662391, global_step=116044, preemption_count=0, score=40353.662391, test/accuracy=0.703341, test/bleu=30.472820, test/loss=1.350868, test/num_examples=3003, total_duration=67777.304435, train/accuracy=0.677942, train/bleu=34.218020, train/loss=1.522565, validation/accuracy=0.687629, validation/bleu=30.542063, validation/loss=1.439708, validation/num_examples=3000
I0214 00:41:52.627055 139975005800192 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.3743387460708618, loss=1.4482063055038452
I0214 00:42:27.278851 139975014192896 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.34316322207450867, loss=1.4973196983337402
I0214 00:43:02.009911 139975005800192 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.3495226800441742, loss=1.440407633781433
I0214 00:43:36.757737 139975014192896 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.34890127182006836, loss=1.4677320718765259
I0214 00:44:11.493103 139975005800192 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3615730106830597, loss=1.574172854423523
I0214 00:44:46.243823 139975014192896 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.36651208996772766, loss=1.5504982471466064
I0214 00:45:21.010526 139975005800192 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.3415304124355316, loss=1.4764063358306885
I0214 00:45:55.928724 139975014192896 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.3602007329463959, loss=1.570054531097412
I0214 00:46:30.687737 139975005800192 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.35555246472358704, loss=1.5498512983322144
I0214 00:47:05.424098 139975014192896 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.35945844650268555, loss=1.561207890510559
I0214 00:47:40.177053 139975005800192 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.35187092423439026, loss=1.4646825790405273
I0214 00:48:14.910546 139975014192896 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.37117451429367065, loss=1.5279403924942017
I0214 00:48:49.689821 139975005800192 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.37590017914772034, loss=1.4378668069839478
I0214 00:49:24.453841 139975014192896 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.37840861082077026, loss=1.6012605428695679
I0214 00:49:59.198036 139975005800192 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.3524685204029083, loss=1.535526156425476
I0214 00:50:33.947545 139975014192896 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.3623744547367096, loss=1.460443139076233
I0214 00:51:08.710741 139975005800192 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.3812660574913025, loss=1.617532730102539
I0214 00:51:43.468967 139975014192896 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.381083220243454, loss=1.5274105072021484
I0214 00:52:18.183063 139975005800192 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.33763444423675537, loss=1.4593024253845215
I0214 00:52:52.937139 139975014192896 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.3499182462692261, loss=1.4340925216674805
I0214 00:53:27.713993 139975005800192 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.3549599051475525, loss=1.4363259077072144
I0214 00:54:02.465439 139975014192896 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.35594624280929565, loss=1.4872217178344727
I0214 00:54:37.216723 139975005800192 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.3661443591117859, loss=1.4849014282226562
I0214 00:55:12.009161 139975014192896 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.3592013418674469, loss=1.4765666723251343
I0214 00:55:32.947725 140144802662208 spec.py:321] Evaluating on the training split.
I0214 00:55:35.924797 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 00:59:26.157106 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 00:59:28.865713 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 01:02:16.788674 140144802662208 spec.py:349] Evaluating on the test split.
I0214 01:02:19.465480 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 01:05:03.106358 140144802662208 submission_runner.py:408] Time since start: 69187.57s, 	Step: 118462, 	{'train/accuracy': 0.6817564964294434, 'train/loss': 1.5030604600906372, 'train/bleu': 34.63627435914792, 'validation/accuracy': 0.6891049146652222, 'validation/loss': 1.4295117855072021, 'validation/bleu': 30.23524154073049, 'validation/num_examples': 3000, 'test/accuracy': 0.705479085445404, 'test/loss': 1.3375895023345947, 'test/bleu': 30.772976650422855, 'test/num_examples': 3003, 'score': 41193.64802837372, 'total_duration': 69187.57227706909, 'accumulated_submission_time': 41193.64802837372, 'accumulated_eval_time': 27988.293677330017, 'accumulated_logging_time': 1.7116749286651611}
I0214 01:05:03.139350 139975005800192 logging_writer.py:48] [118462] accumulated_eval_time=27988.293677, accumulated_logging_time=1.711675, accumulated_submission_time=41193.648028, global_step=118462, preemption_count=0, score=41193.648028, test/accuracy=0.705479, test/bleu=30.772977, test/loss=1.337590, test/num_examples=3003, total_duration=69187.572277, train/accuracy=0.681756, train/bleu=34.636274, train/loss=1.503060, validation/accuracy=0.689105, validation/bleu=30.235242, validation/loss=1.429512, validation/num_examples=3000
I0214 01:05:16.654534 139975014192896 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3997777998447418, loss=1.493548035621643
I0214 01:05:51.260777 139975005800192 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.36242207884788513, loss=1.535283088684082
I0214 01:06:25.988626 139975014192896 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.38131022453308105, loss=1.5662192106246948
I0214 01:07:00.726793 139975005800192 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.3483097553253174, loss=1.4701381921768188
I0214 01:07:35.469798 139975014192896 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.3536178469657898, loss=1.4865949153900146
I0214 01:08:10.191727 139975005800192 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.37308141589164734, loss=1.4683867692947388
I0214 01:08:44.927795 139975014192896 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.3580624759197235, loss=1.4505891799926758
I0214 01:09:19.674163 139975005800192 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.3570426404476166, loss=1.4222768545150757
I0214 01:09:54.427944 139975014192896 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.3679046332836151, loss=1.4384119510650635
I0214 01:10:29.186572 139975005800192 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3535345196723938, loss=1.4343641996383667
I0214 01:11:03.928389 139975014192896 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.3541424870491028, loss=1.4297727346420288
I0214 01:11:38.686703 139975005800192 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.37359142303466797, loss=1.436835527420044
I0214 01:12:13.445874 139975014192896 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.35949161648750305, loss=1.5162231922149658
I0214 01:12:48.182667 139975005800192 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.37552428245544434, loss=1.4660815000534058
I0214 01:13:22.923170 139975014192896 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.3953992426395416, loss=1.4375981092453003
I0214 01:13:57.670205 139975005800192 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.3554406762123108, loss=1.4561136960983276
I0214 01:14:32.466414 139975014192896 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.38429024815559387, loss=1.5424532890319824
I0214 01:15:07.211557 139975005800192 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.3646303713321686, loss=1.4909647703170776
I0214 01:15:41.968330 139975014192896 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.3784323036670685, loss=1.4465899467468262
I0214 01:16:16.710501 139975005800192 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.37926986813545227, loss=1.5319666862487793
I0214 01:16:51.454046 139975014192896 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.37685713171958923, loss=1.4482855796813965
I0214 01:17:26.221327 139975005800192 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.37915554642677307, loss=1.5018484592437744
I0214 01:18:01.014735 139975014192896 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.3668941855430603, loss=1.4101299047470093
I0214 01:18:35.794601 139975005800192 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.35882675647735596, loss=1.4229111671447754
I0214 01:19:03.437779 140144802662208 spec.py:321] Evaluating on the training split.
I0214 01:19:06.426852 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 01:22:54.738039 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 01:22:57.433292 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 01:25:49.578427 140144802662208 spec.py:349] Evaluating on the test split.
I0214 01:25:52.268824 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 01:28:36.481156 140144802662208 submission_runner.py:408] Time since start: 70600.95s, 	Step: 120881, 	{'train/accuracy': 0.6928209066390991, 'train/loss': 1.4292106628417969, 'train/bleu': 35.03023370942456, 'validation/accuracy': 0.6907663941383362, 'validation/loss': 1.4212204217910767, 'validation/bleu': 30.38323877714415, 'validation/num_examples': 3000, 'test/accuracy': 0.7065946459770203, 'test/loss': 1.3328224420547485, 'test/bleu': 30.69767269998512, 'test/num_examples': 3003, 'score': 42033.85468816757, 'total_duration': 70600.94706273079, 'accumulated_submission_time': 42033.85468816757, 'accumulated_eval_time': 28561.33699631691, 'accumulated_logging_time': 1.754795789718628}
I0214 01:28:36.513498 139975014192896 logging_writer.py:48] [120881] accumulated_eval_time=28561.336996, accumulated_logging_time=1.754796, accumulated_submission_time=42033.854688, global_step=120881, preemption_count=0, score=42033.854688, test/accuracy=0.706595, test/bleu=30.697673, test/loss=1.332822, test/num_examples=3003, total_duration=70600.947063, train/accuracy=0.692821, train/bleu=35.030234, train/loss=1.429211, validation/accuracy=0.690766, validation/bleu=30.383239, validation/loss=1.421220, validation/num_examples=3000
I0214 01:28:43.445169 139975005800192 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.37472790479660034, loss=1.4458163976669312
I0214 01:29:18.076399 139975014192896 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.3750896155834198, loss=1.4441498517990112
I0214 01:29:52.749636 139975005800192 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.382443368434906, loss=1.442687749862671
I0214 01:30:27.521214 139975014192896 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.3722175359725952, loss=1.4668961763381958
I0214 01:31:02.286134 139975005800192 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.36092519760131836, loss=1.4827667474746704
I0214 01:31:37.034229 139975014192896 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.3918657600879669, loss=1.4813278913497925
I0214 01:32:11.790806 139975005800192 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.39140844345092773, loss=1.4961321353912354
I0214 01:32:46.551115 139975014192896 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.37840068340301514, loss=1.4765725135803223
I0214 01:33:21.313398 139975005800192 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.3651711344718933, loss=1.4378371238708496
I0214 01:33:56.056716 139975014192896 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.388632208108902, loss=1.5301216840744019
I0214 01:34:30.868679 139975005800192 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.38051560521125793, loss=1.508345365524292
I0214 01:35:05.600825 139975014192896 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.39028143882751465, loss=1.4562124013900757
I0214 01:35:40.342358 139975005800192 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.37710437178611755, loss=1.4421201944351196
I0214 01:36:15.105466 139975014192896 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.3904902935028076, loss=1.5433504581451416
I0214 01:36:49.843290 139975005800192 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.3860507011413574, loss=1.4597269296646118
I0214 01:37:24.606291 139975014192896 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.35899367928504944, loss=1.4609215259552002
I0214 01:37:59.370491 139975005800192 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.385237455368042, loss=1.4372185468673706
I0214 01:38:34.193475 139975014192896 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.399858295917511, loss=1.4681122303009033
I0214 01:39:08.925218 139975005800192 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.3923124372959137, loss=1.517697811126709
I0214 01:39:43.680053 139975014192896 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.3694230020046234, loss=1.4280004501342773
I0214 01:40:18.436841 139975005800192 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.38293758034706116, loss=1.3984652757644653
I0214 01:40:53.183896 139975014192896 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.3898375928401947, loss=1.4677705764770508
I0214 01:41:27.934473 139975005800192 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.3766933083534241, loss=1.4552594423294067
I0214 01:42:02.667454 139975014192896 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.3777763545513153, loss=1.4067115783691406
I0214 01:42:36.788982 140144802662208 spec.py:321] Evaluating on the training split.
I0214 01:42:39.773997 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 01:46:33.539493 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 01:46:36.258873 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 01:49:23.829539 140144802662208 spec.py:349] Evaluating on the test split.
I0214 01:49:26.521382 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 01:52:26.578147 140144802662208 submission_runner.py:408] Time since start: 72031.04s, 	Step: 123300, 	{'train/accuracy': 0.692010223865509, 'train/loss': 1.4411240816116333, 'train/bleu': 35.203263862741345, 'validation/accuracy': 0.6924154758453369, 'validation/loss': 1.4140108823776245, 'validation/bleu': 30.84412796852067, 'validation/num_examples': 3000, 'test/accuracy': 0.7075010538101196, 'test/loss': 1.3267334699630737, 'test/bleu': 30.729561965493104, 'test/num_examples': 3003, 'score': 42874.04085731506, 'total_duration': 72031.04406666756, 'accumulated_submission_time': 42874.04085731506, 'accumulated_eval_time': 29151.126117944717, 'accumulated_logging_time': 1.7970449924468994}
I0214 01:52:26.611649 139975005800192 logging_writer.py:48] [123300] accumulated_eval_time=29151.126118, accumulated_logging_time=1.797045, accumulated_submission_time=42874.040857, global_step=123300, preemption_count=0, score=42874.040857, test/accuracy=0.707501, test/bleu=30.729562, test/loss=1.326733, test/num_examples=3003, total_duration=72031.044067, train/accuracy=0.692010, train/bleu=35.203264, train/loss=1.441124, validation/accuracy=0.692415, validation/bleu=30.844128, validation/loss=1.414011, validation/num_examples=3000
I0214 01:52:26.981816 139975014192896 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.38892558217048645, loss=1.4318053722381592
I0214 01:53:01.588057 139975005800192 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.3733277916908264, loss=1.495367407798767
I0214 01:53:36.260965 139975014192896 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.40560680627822876, loss=1.4840792417526245
I0214 01:54:11.012943 139975005800192 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.3748971223831177, loss=1.4919945001602173
I0214 01:54:45.793086 139975014192896 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.38782575726509094, loss=1.470190167427063
I0214 01:55:20.550143 139975005800192 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.3785390257835388, loss=1.3966833353042603
I0214 01:55:55.295329 139975014192896 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.39609992504119873, loss=1.465890645980835
I0214 01:56:30.169842 139975005800192 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.38290587067604065, loss=1.4429229497909546
I0214 01:57:04.930287 139975014192896 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.3836468458175659, loss=1.4661909341812134
I0214 01:57:39.688173 139975005800192 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.3921310603618622, loss=1.430230736732483
I0214 01:58:14.426137 139975014192896 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.3754113018512726, loss=1.4543893337249756
I0214 01:58:49.181209 139975005800192 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.3904761075973511, loss=1.4180947542190552
I0214 01:59:23.942837 139975014192896 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.37009865045547485, loss=1.4146709442138672
I0214 01:59:58.733939 139975005800192 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.3986932635307312, loss=1.4775220155715942
I0214 02:00:33.484384 139975014192896 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.3940366506576538, loss=1.4422296285629272
I0214 02:01:08.239678 139975005800192 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.3980383276939392, loss=1.4178460836410522
I0214 02:01:43.059363 139975014192896 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.3818502426147461, loss=1.4414074420928955
I0214 02:02:17.816277 139975005800192 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.39660605788230896, loss=1.4848871231079102
I0214 02:02:52.612382 139975014192896 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.37802261114120483, loss=1.4766806364059448
I0214 02:03:27.360106 139975005800192 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.3805799186229706, loss=1.4062608480453491
I0214 02:04:02.092850 139975014192896 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.3918325901031494, loss=1.3685338497161865
I0214 02:04:36.834785 139975005800192 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.39045995473861694, loss=1.4640227556228638
I0214 02:05:11.607183 139975014192896 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.4070698022842407, loss=1.4235129356384277
I0214 02:05:46.349591 139975005800192 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.3718374967575073, loss=1.4701067209243774
I0214 02:06:21.070001 139975014192896 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.3996528685092926, loss=1.43659508228302
I0214 02:06:26.706300 140144802662208 spec.py:321] Evaluating on the training split.
I0214 02:06:29.678859 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 02:10:15.717956 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 02:10:18.413215 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 02:13:14.358239 140144802662208 spec.py:349] Evaluating on the test split.
I0214 02:13:17.064141 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 02:16:17.636293 140144802662208 submission_runner.py:408] Time since start: 73462.10s, 	Step: 125718, 	{'train/accuracy': 0.698666512966156, 'train/loss': 1.3995305299758911, 'train/bleu': 35.56513313304348, 'validation/accuracy': 0.6939033269882202, 'validation/loss': 1.4084949493408203, 'validation/bleu': 30.655571581373597, 'validation/num_examples': 3000, 'test/accuracy': 0.708349347114563, 'test/loss': 1.3209024667739868, 'test/bleu': 30.876623342710356, 'test/num_examples': 3003, 'score': 43714.043221235275, 'total_duration': 73462.10220861435, 'accumulated_submission_time': 43714.043221235275, 'accumulated_eval_time': 29742.056054353714, 'accumulated_logging_time': 1.8420050144195557}
I0214 02:16:17.669245 139975005800192 logging_writer.py:48] [125718] accumulated_eval_time=29742.056054, accumulated_logging_time=1.842005, accumulated_submission_time=43714.043221, global_step=125718, preemption_count=0, score=43714.043221, test/accuracy=0.708349, test/bleu=30.876623, test/loss=1.320902, test/num_examples=3003, total_duration=73462.102209, train/accuracy=0.698667, train/bleu=35.565133, train/loss=1.399531, validation/accuracy=0.693903, validation/bleu=30.655572, validation/loss=1.408495, validation/num_examples=3000
I0214 02:16:46.385139 139975014192896 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.39424648880958557, loss=1.439249873161316
I0214 02:17:21.035015 139975005800192 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.3705947995185852, loss=1.3953078985214233
I0214 02:17:55.765711 139975014192896 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.3837414085865021, loss=1.4105955362319946
I0214 02:18:30.520203 139975005800192 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.40823671221733093, loss=1.45328688621521
I0214 02:19:05.263637 139975014192896 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.3917781412601471, loss=1.4285012483596802
I0214 02:19:40.004550 139975005800192 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.3695719242095947, loss=1.380395770072937
I0214 02:20:14.772583 139975014192896 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.371960312128067, loss=1.3869211673736572
I0214 02:20:49.535346 139975005800192 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.40524473786354065, loss=1.469512701034546
I0214 02:21:24.272544 139975014192896 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.3943270146846771, loss=1.4531166553497314
I0214 02:21:58.999885 139975005800192 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.3748214840888977, loss=1.3687732219696045
I0214 02:22:33.754382 139975014192896 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.3975231945514679, loss=1.3999511003494263
I0214 02:23:08.554604 139975005800192 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.37567147612571716, loss=1.3820204734802246
I0214 02:23:43.321236 139975014192896 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.3639667332172394, loss=1.3954757452011108
I0214 02:24:18.080530 139975005800192 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.3759136497974396, loss=1.4017672538757324
I0214 02:24:52.861573 139975014192896 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.3712194859981537, loss=1.415223479270935
I0214 02:25:27.669765 139975005800192 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.41440388560295105, loss=1.4299757480621338
I0214 02:26:02.449040 139975014192896 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.40054288506507874, loss=1.4047739505767822
I0214 02:26:37.256187 139975005800192 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.3891364634037018, loss=1.4418007135391235
I0214 02:27:12.045397 139975014192896 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.3790441155433655, loss=1.371640682220459
I0214 02:27:46.854759 139975005800192 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.3782402575016022, loss=1.452332615852356
I0214 02:28:21.639009 139975014192896 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.39312297105789185, loss=1.3982959985733032
I0214 02:28:56.423303 139975005800192 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.36406558752059937, loss=1.4155189990997314
I0214 02:29:31.161507 139975014192896 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.3804186284542084, loss=1.4035251140594482
I0214 02:30:05.901974 139975005800192 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.3768986165523529, loss=1.4010372161865234
I0214 02:30:17.788780 140144802662208 spec.py:321] Evaluating on the training split.
I0214 02:30:20.761741 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 02:34:06.207139 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 02:34:08.890689 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 02:36:56.333320 140144802662208 spec.py:349] Evaluating on the test split.
I0214 02:36:59.021458 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 02:40:04.168458 140144802662208 submission_runner.py:408] Time since start: 74888.63s, 	Step: 128136, 	{'train/accuracy': 0.6984717845916748, 'train/loss': 1.4034595489501953, 'train/bleu': 35.613569304448035, 'validation/accuracy': 0.6934818029403687, 'validation/loss': 1.4072446823120117, 'validation/bleu': 30.81339413073557, 'validation/num_examples': 3000, 'test/accuracy': 0.710615336894989, 'test/loss': 1.315718173980713, 'test/bleu': 30.94520762179943, 'test/num_examples': 3003, 'score': 44554.06897211075, 'total_duration': 74888.634370327, 'accumulated_submission_time': 44554.06897211075, 'accumulated_eval_time': 30328.435676574707, 'accumulated_logging_time': 1.886183738708496}
I0214 02:40:04.201444 139975014192896 logging_writer.py:48] [128136] accumulated_eval_time=30328.435677, accumulated_logging_time=1.886184, accumulated_submission_time=44554.068972, global_step=128136, preemption_count=0, score=44554.068972, test/accuracy=0.710615, test/bleu=30.945208, test/loss=1.315718, test/num_examples=3003, total_duration=74888.634370, train/accuracy=0.698472, train/bleu=35.613569, train/loss=1.403460, validation/accuracy=0.693482, validation/bleu=30.813394, validation/loss=1.407245, validation/num_examples=3000
I0214 02:40:26.688272 139975005800192 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.37594521045684814, loss=1.3993526697158813
I0214 02:41:01.327811 139975014192896 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.3974834680557251, loss=1.411542296409607
I0214 02:41:36.054048 139975005800192 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.3884204626083374, loss=1.3858824968338013
I0214 02:42:10.798561 139975014192896 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.39935946464538574, loss=1.4126428365707397
I0214 02:42:45.536135 139975005800192 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.37688910961151123, loss=1.3480325937271118
I0214 02:43:20.283980 139975014192896 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.38046130537986755, loss=1.3565512895584106
I0214 02:43:55.042929 139975005800192 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.3845509886741638, loss=1.4509778022766113
I0214 02:44:29.802000 139975014192896 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.3898344337940216, loss=1.386207103729248
I0214 02:45:04.540648 139975005800192 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.3971242904663086, loss=1.4224852323532104
I0214 02:45:39.272541 139975014192896 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.37276947498321533, loss=1.4125510454177856
I0214 02:46:14.049691 139975005800192 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.384289026260376, loss=1.4548927545547485
I0214 02:46:48.816382 139975014192896 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.3922964930534363, loss=1.4453108310699463
I0214 02:47:23.563642 139975005800192 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.3837052583694458, loss=1.4060992002487183
I0214 02:47:58.303278 139975014192896 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.3752806782722473, loss=1.3476539850234985
I0214 02:48:33.062686 139975005800192 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.36534541845321655, loss=1.4132471084594727
I0214 02:49:07.827670 139975014192896 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.38446518778800964, loss=1.5135555267333984
I0214 02:49:42.582709 139975005800192 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.379163920879364, loss=1.4168399572372437
I0214 02:50:17.327834 139975014192896 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.3681192398071289, loss=1.3490668535232544
I0214 02:50:52.046426 139975005800192 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.36447834968566895, loss=1.4280537366867065
I0214 02:51:26.779559 139975014192896 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.3812369406223297, loss=1.4030951261520386
I0214 02:52:01.595355 139975005800192 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.37790757417678833, loss=1.4220331907272339
I0214 02:52:36.376089 139975014192896 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.3969505727291107, loss=1.421076774597168
I0214 02:53:11.110002 139975005800192 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.384703665971756, loss=1.4090780019760132
I0214 02:53:45.875508 139975014192896 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.36987006664276123, loss=1.4133025407791138
I0214 02:54:04.381545 140144802662208 spec.py:321] Evaluating on the training split.
I0214 02:54:07.371176 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 02:58:17.031207 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 02:58:19.731286 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:01:01.654460 140144802662208 spec.py:349] Evaluating on the test split.
I0214 03:01:04.376907 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:04:05.039037 140144802662208 submission_runner.py:408] Time since start: 76329.50s, 	Step: 130555, 	{'train/accuracy': 0.7004496455192566, 'train/loss': 1.3945019245147705, 'train/bleu': 35.82390045814242, 'validation/accuracy': 0.693221390247345, 'validation/loss': 1.4052176475524902, 'validation/bleu': 30.76973079067687, 'validation/num_examples': 3000, 'test/accuracy': 0.7104758620262146, 'test/loss': 1.3132108449935913, 'test/bleu': 30.946965112698592, 'test/num_examples': 3003, 'score': 45394.16031217575, 'total_duration': 76329.50495123863, 'accumulated_submission_time': 45394.16031217575, 'accumulated_eval_time': 30929.09312582016, 'accumulated_logging_time': 1.9304907321929932}
I0214 03:04:05.072122 139975005800192 logging_writer.py:48] [130555] accumulated_eval_time=30929.093126, accumulated_logging_time=1.930491, accumulated_submission_time=45394.160312, global_step=130555, preemption_count=0, score=45394.160312, test/accuracy=0.710476, test/bleu=30.946965, test/loss=1.313211, test/num_examples=3003, total_duration=76329.504951, train/accuracy=0.700450, train/bleu=35.823900, train/loss=1.394502, validation/accuracy=0.693221, validation/bleu=30.769731, validation/loss=1.405218, validation/num_examples=3000
I0214 03:04:21.010377 139975014192896 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.3880515694618225, loss=1.4169330596923828
I0214 03:04:55.644345 139975005800192 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.39058130979537964, loss=1.420886516571045
I0214 03:05:30.351354 139975014192896 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.3718479871749878, loss=1.4619050025939941
I0214 03:06:05.107057 139975005800192 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.375720739364624, loss=1.3709701299667358
I0214 03:06:39.848418 139975014192896 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.39388108253479004, loss=1.4033488035202026
I0214 03:07:14.609443 139975005800192 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.40026354789733887, loss=1.4004608392715454
I0214 03:07:49.338820 139975014192896 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.3572772443294525, loss=1.3262734413146973
I0214 03:08:24.102064 139975005800192 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.3976134955883026, loss=1.4137957096099854
I0214 03:08:58.905059 139975014192896 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.38481056690216064, loss=1.3645610809326172
I0214 03:09:33.680085 139975005800192 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.3792538642883301, loss=1.3891088962554932
I0214 03:10:08.437194 139975014192896 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.3951358199119568, loss=1.4688700437545776
I0214 03:10:43.190458 139975005800192 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.40590667724609375, loss=1.4561150074005127
I0214 03:11:17.977883 139975014192896 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.39003822207450867, loss=1.4672826528549194
I0214 03:11:52.727656 139975005800192 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.37113603949546814, loss=1.3648842573165894
I0214 03:12:27.485730 139975014192896 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.38654738664627075, loss=1.3823201656341553
I0214 03:13:02.259554 139975005800192 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.4075123071670532, loss=1.4178826808929443
I0214 03:13:37.006905 139975014192896 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.3703153133392334, loss=1.4213193655014038
I0214 03:14:11.795555 139975005800192 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.4100230038166046, loss=1.4823318719863892
I0214 03:14:46.546480 139975014192896 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.40367183089256287, loss=1.3871618509292603
I0214 03:15:21.296903 139975005800192 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.4005877375602722, loss=1.3875073194503784
I0214 03:15:56.044602 139975014192896 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.3816392719745636, loss=1.4642558097839355
I0214 03:16:30.812316 139975005800192 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.3963121175765991, loss=1.347882628440857
I0214 03:17:05.574127 139975014192896 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.3615850508213043, loss=1.305341362953186
I0214 03:17:40.327071 139975005800192 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.36892539262771606, loss=1.4173427820205688
I0214 03:18:05.103544 140144802662208 spec.py:321] Evaluating on the training split.
I0214 03:18:08.104300 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:22:08.360348 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 03:22:11.058583 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:25:04.014497 140144802662208 spec.py:349] Evaluating on the test split.
I0214 03:25:06.727637 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:28:09.866643 140144802662208 submission_runner.py:408] Time since start: 77774.33s, 	Step: 132973, 	{'train/accuracy': 0.7006556391716003, 'train/loss': 1.3924342393875122, 'train/bleu': 35.9691844802764, 'validation/accuracy': 0.6937049627304077, 'validation/loss': 1.4050370454788208, 'validation/bleu': 30.88978323109175, 'validation/num_examples': 3000, 'test/accuracy': 0.7101853489875793, 'test/loss': 1.313064455986023, 'test/bleu': 31.12658905369212, 'test/num_examples': 3003, 'score': 46234.09957194328, 'total_duration': 77774.33254957199, 'accumulated_submission_time': 46234.09957194328, 'accumulated_eval_time': 31533.856184482574, 'accumulated_logging_time': 1.9747204780578613}
I0214 03:28:09.901250 139975014192896 logging_writer.py:48] [132973] accumulated_eval_time=31533.856184, accumulated_logging_time=1.974720, accumulated_submission_time=46234.099572, global_step=132973, preemption_count=0, score=46234.099572, test/accuracy=0.710185, test/bleu=31.126589, test/loss=1.313064, test/num_examples=3003, total_duration=77774.332550, train/accuracy=0.700656, train/bleu=35.969184, train/loss=1.392434, validation/accuracy=0.693705, validation/bleu=30.889783, validation/loss=1.405037, validation/num_examples=3000
I0214 03:28:19.592821 139975005800192 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.3980261981487274, loss=1.4332102537155151
I0214 03:28:54.212641 139975014192896 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.38091209530830383, loss=1.419903039932251
I0214 03:29:28.909840 139975005800192 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.3932545483112335, loss=1.4737883806228638
I0214 03:30:03.640864 139975014192896 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.38796213269233704, loss=1.4189658164978027
I0214 03:30:14.495271 140144802662208 spec.py:321] Evaluating on the training split.
I0214 03:30:17.464949 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:34:05.833947 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 03:34:08.511831 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:36:58.194051 140144802662208 spec.py:349] Evaluating on the test split.
I0214 03:37:00.878017 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:40:03.700383 140144802662208 submission_runner.py:408] Time since start: 78488.17s, 	Step: 133333, 	{'train/accuracy': 0.6965222358703613, 'train/loss': 1.4127204418182373, 'train/bleu': 35.569271445766226, 'validation/accuracy': 0.6937049627304077, 'validation/loss': 1.4050407409667969, 'validation/bleu': 30.89082856801816, 'validation/num_examples': 3000, 'test/accuracy': 0.7102318406105042, 'test/loss': 1.3130912780761719, 'test/bleu': 31.113830544833622, 'test/num_examples': 3003, 'score': 46358.67147278786, 'total_duration': 78488.16630244255, 'accumulated_submission_time': 46358.67147278786, 'accumulated_eval_time': 32123.061244010925, 'accumulated_logging_time': 2.0193135738372803}
I0214 03:40:03.733820 139975005800192 logging_writer.py:48] [133333] accumulated_eval_time=32123.061244, accumulated_logging_time=2.019314, accumulated_submission_time=46358.671473, global_step=133333, preemption_count=0, score=46358.671473, test/accuracy=0.710232, test/bleu=31.113831, test/loss=1.313091, test/num_examples=3003, total_duration=78488.166302, train/accuracy=0.696522, train/bleu=35.569271, train/loss=1.412720, validation/accuracy=0.693705, validation/bleu=30.890829, validation/loss=1.405041, validation/num_examples=3000
I0214 03:40:03.767364 139975014192896 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46358.671473
I0214 03:40:04.974141 140144802662208 checkpoints.py:490] Saving checkpoint at step: 133333
I0214 03:40:08.997618 140144802662208 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_4/checkpoint_133333
I0214 03:40:09.002787 140144802662208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_4/checkpoint_133333.
I0214 03:40:09.062412 140144802662208 submission_runner.py:583] Tuning trial 4/5
I0214 03:40:09.062581 140144802662208 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0214 03:40:09.070214 140144802662208 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005914963549003005, 'train/loss': 10.96237564086914, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 29.889463663101196, 'total_duration': 890.0350096225739, 'accumulated_submission_time': 29.889463663101196, 'accumulated_eval_time': 860.1454901695251, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2417, {'train/accuracy': 0.5345553755760193, 'train/loss': 2.5997135639190674, 'train/bleu': 23.673126618849807, 'validation/accuracy': 0.5361867547035217, 'validation/loss': 2.5623652935028076, 'validation/bleu': 19.77231194625641, 'validation/num_examples': 3000, 'test/accuracy': 0.5345999598503113, 'test/loss': 2.571000099182129, 'test/bleu': 17.927984064231914, 'test/num_examples': 3003, 'score': 870.12095952034, 'total_duration': 2227.2441873550415, 'accumulated_submission_time': 870.12095952034, 'accumulated_eval_time': 1357.0244114398956, 'accumulated_logging_time': 0.019931316375732422, 'global_step': 2417, 'preemption_count': 0}), (4833, {'train/accuracy': 0.5767702460289001, 'train/loss': 2.2277464866638184, 'train/bleu': 26.598028617543335, 'validation/accuracy': 0.5945369601249695, 'validation/loss': 2.089860439300537, 'validation/bleu': 23.094498651041594, 'validation/num_examples': 3000, 'test/accuracy': 0.5962698459625244, 'test/loss': 2.0628721714019775, 'test/bleu': 21.65287838064829, 'test/num_examples': 3003, 'score': 1710.169316291809, 'total_duration': 3527.5778126716614, 'accumulated_submission_time': 1710.169316291809, 'accumulated_eval_time': 1817.1972329616547, 'accumulated_logging_time': 0.049460411071777344, 'global_step': 4833, 'preemption_count': 0}), (7251, {'train/accuracy': 0.5924220681190491, 'train/loss': 2.1205623149871826, 'train/bleu': 27.298196480210727, 'validation/accuracy': 0.6061673164367676, 'validation/loss': 2.006256580352783, 'validation/bleu': 24.03146513922641, 'validation/num_examples': 3000, 'test/accuracy': 0.6099936366081238, 'test/loss': 1.9696370363235474, 'test/bleu': 22.926869041848573, 'test/num_examples': 3003, 'score': 2550.408034324646, 'total_duration': 4846.8850247859955, 'accumulated_submission_time': 2550.408034324646, 'accumulated_eval_time': 2296.1606137752533, 'accumulated_logging_time': 0.07471156120300293, 'global_step': 7251, 'preemption_count': 0}), (9669, {'train/accuracy': 0.5942032337188721, 'train/loss': 2.1156461238861084, 'train/bleu': 27.3323492483322, 'validation/accuracy': 0.6107797622680664, 'validation/loss': 1.9769822359085083, 'validation/bleu': 24.644655251514664, 'validation/num_examples': 3000, 'test/accuracy': 0.6165592074394226, 'test/loss': 1.929946780204773, 'test/bleu': 23.52521008844378, 'test/num_examples': 3003, 'score': 3390.572445869446, 'total_duration': 6195.257897377014, 'accumulated_submission_time': 3390.572445869446, 'accumulated_eval_time': 2804.2633929252625, 'accumulated_logging_time': 0.09983301162719727, 'global_step': 9669, 'preemption_count': 0}), (12086, {'train/accuracy': 0.5959599018096924, 'train/loss': 2.1070985794067383, 'train/bleu': 28.17086301658108, 'validation/accuracy': 0.6141027212142944, 'validation/loss': 1.9404385089874268, 'validation/bleu': 24.87685627139683, 'validation/num_examples': 3000, 'test/accuracy': 0.6239033341407776, 'test/loss': 1.8900692462921143, 'test/bleu': 24.094168632677732, 'test/num_examples': 3003, 'score': 4230.6515011787415, 'total_duration': 7619.584226846695, 'accumulated_submission_time': 4230.6515011787415, 'accumulated_eval_time': 3388.406713962555, 'accumulated_logging_time': 0.1255655288696289, 'global_step': 12086, 'preemption_count': 0}), (14503, {'train/accuracy': 0.6016581058502197, 'train/loss': 2.071608066558838, 'train/bleu': 28.414997950665157, 'validation/accuracy': 0.6191367506980896, 'validation/loss': 1.9150434732437134, 'validation/bleu': 25.375294821756516, 'validation/num_examples': 3000, 'test/accuracy': 0.6236360669136047, 'test/loss': 1.8791215419769287, 'test/bleu': 24.06938099765642, 'test/num_examples': 3003, 'score': 5070.606368780136, 'total_duration': 8982.273378133774, 'accumulated_submission_time': 5070.606368780136, 'accumulated_eval_time': 3911.032277584076, 'accumulated_logging_time': 0.15601110458374023, 'global_step': 14503, 'preemption_count': 0}), (16919, {'train/accuracy': 0.5999884605407715, 'train/loss': 2.073273181915283, 'train/bleu': 28.36836530080516, 'validation/accuracy': 0.6177108883857727, 'validation/loss': 1.9168777465820312, 'validation/bleu': 25.15550042864411, 'validation/num_examples': 3000, 'test/accuracy': 0.6234152913093567, 'test/loss': 1.8667335510253906, 'test/bleu': 24.182225927018916, 'test/num_examples': 3003, 'score': 5910.714811086655, 'total_duration': 10354.45345211029, 'accumulated_submission_time': 5910.714811086655, 'accumulated_eval_time': 4442.9956386089325, 'accumulated_logging_time': 0.1835007667541504, 'global_step': 16919, 'preemption_count': 0}), (19337, {'train/accuracy': 0.6061010956764221, 'train/loss': 1.999993920326233, 'train/bleu': 28.632670103777563, 'validation/accuracy': 0.6188639998435974, 'validation/loss': 1.9008632898330688, 'validation/bleu': 24.93843937756793, 'validation/num_examples': 3000, 'test/accuracy': 0.6259601712226868, 'test/loss': 1.8603891134262085, 'test/bleu': 23.853407099789305, 'test/num_examples': 3003, 'score': 6750.888104915619, 'total_duration': 11723.343873977661, 'accumulated_submission_time': 6750.888104915619, 'accumulated_eval_time': 4971.604582309723, 'accumulated_logging_time': 0.21465039253234863, 'global_step': 19337, 'preemption_count': 0}), (21754, {'train/accuracy': 0.6028353571891785, 'train/loss': 2.0488171577453613, 'train/bleu': 28.765118879859944, 'validation/accuracy': 0.6212570071220398, 'validation/loss': 1.893480896949768, 'validation/bleu': 25.664352122554142, 'validation/num_examples': 3000, 'test/accuracy': 0.6301435232162476, 'test/loss': 1.8423911333084106, 'test/bleu': 24.476060360265514, 'test/num_examples': 3003, 'score': 7590.796914815903, 'total_duration': 13180.169267416, 'accumulated_submission_time': 7590.796914815903, 'accumulated_eval_time': 5588.410370588303, 'accumulated_logging_time': 0.24518036842346191, 'global_step': 21754, 'preemption_count': 0}), (24172, {'train/accuracy': 0.6010239124298096, 'train/loss': 2.058652639389038, 'train/bleu': 28.34594869682518, 'validation/accuracy': 0.6217281818389893, 'validation/loss': 1.8820874691009521, 'validation/bleu': 25.278200789983146, 'validation/num_examples': 3000, 'test/accuracy': 0.626308798789978, 'test/loss': 1.852922797203064, 'test/bleu': 24.137148784396878, 'test/num_examples': 3003, 'score': 8430.839210271835, 'total_duration': 14610.239954471588, 'accumulated_submission_time': 8430.839210271835, 'accumulated_eval_time': 6178.330642223358, 'accumulated_logging_time': 0.27239227294921875, 'global_step': 24172, 'preemption_count': 0}), (26590, {'train/accuracy': 0.6073386073112488, 'train/loss': 2.0060691833496094, 'train/bleu': 28.424016769886777, 'validation/accuracy': 0.6224597096443176, 'validation/loss': 1.8667932748794556, 'validation/bleu': 25.240027278392578, 'validation/num_examples': 3000, 'test/accuracy': 0.6293068528175354, 'test/loss': 1.8353381156921387, 'test/bleu': 24.118035745694435, 'test/num_examples': 3003, 'score': 9271.049669265747, 'total_duration': 15927.374853849411, 'accumulated_submission_time': 9271.049669265747, 'accumulated_eval_time': 6655.143758058548, 'accumulated_logging_time': 0.3059818744659424, 'global_step': 26590, 'preemption_count': 0}), (29007, {'train/accuracy': 0.601288914680481, 'train/loss': 2.059950113296509, 'train/bleu': 28.236495801272138, 'validation/accuracy': 0.6220133304595947, 'validation/loss': 1.878191590309143, 'validation/bleu': 24.748724901143305, 'validation/num_examples': 3000, 'test/accuracy': 0.6305851340293884, 'test/loss': 1.827478289604187, 'test/bleu': 23.666496475395746, 'test/num_examples': 3003, 'score': 10111.076085329056, 'total_duration': 17291.617335796356, 'accumulated_submission_time': 10111.076085329056, 'accumulated_eval_time': 7179.2492508888245, 'accumulated_logging_time': 0.33559679985046387, 'global_step': 29007, 'preemption_count': 0}), (31426, {'train/accuracy': 0.6311168670654297, 'train/loss': 1.7990312576293945, 'train/bleu': 29.951347937008773, 'validation/accuracy': 0.6247039437294006, 'validation/loss': 1.8656679391860962, 'validation/bleu': 25.24824983902175, 'validation/num_examples': 3000, 'test/accuracy': 0.6326651573181152, 'test/loss': 1.811199426651001, 'test/bleu': 24.274575200682374, 'test/num_examples': 3003, 'score': 10951.275916576385, 'total_duration': 18592.80987429619, 'accumulated_submission_time': 10951.275916576385, 'accumulated_eval_time': 7640.133558750153, 'accumulated_logging_time': 0.3651926517486572, 'global_step': 31426, 'preemption_count': 0}), (33843, {'train/accuracy': 0.608905017375946, 'train/loss': 2.006382942199707, 'train/bleu': 28.668787281923468, 'validation/accuracy': 0.6290560364723206, 'validation/loss': 1.8475347757339478, 'validation/bleu': 26.221402628623725, 'validation/num_examples': 3000, 'test/accuracy': 0.6364302039146423, 'test/loss': 1.796127200126648, 'test/bleu': 24.618080302001683, 'test/num_examples': 3003, 'score': 11791.334715604782, 'total_duration': 19939.061821222305, 'accumulated_submission_time': 11791.334715604782, 'accumulated_eval_time': 8146.219051361084, 'accumulated_logging_time': 0.3942883014678955, 'global_step': 33843, 'preemption_count': 0}), (36261, {'train/accuracy': 0.6050881147384644, 'train/loss': 2.023306131362915, 'train/bleu': 28.62304099873272, 'validation/accuracy': 0.6242327690124512, 'validation/loss': 1.8724839687347412, 'validation/bleu': 25.53469168614256, 'validation/num_examples': 3000, 'test/accuracy': 0.6352681517601013, 'test/loss': 1.8075774908065796, 'test/bleu': 25.250241402682335, 'test/num_examples': 3003, 'score': 12631.307217121124, 'total_duration': 21312.102447271347, 'accumulated_submission_time': 12631.307217121124, 'accumulated_eval_time': 8679.177055120468, 'accumulated_logging_time': 0.4261302947998047, 'global_step': 36261, 'preemption_count': 0}), (38678, {'train/accuracy': 0.6133084893226624, 'train/loss': 1.960720181465149, 'train/bleu': 29.088107626615397, 'validation/accuracy': 0.6289196610450745, 'validation/loss': 1.840861201286316, 'validation/bleu': 25.56870350178375, 'validation/num_examples': 3000, 'test/accuracy': 0.6340363621711731, 'test/loss': 1.7975412607192993, 'test/bleu': 24.727151582813146, 'test/num_examples': 3003, 'score': 13471.460614442825, 'total_duration': 22650.755472898483, 'accumulated_submission_time': 13471.460614442825, 'accumulated_eval_time': 9177.565561771393, 'accumulated_logging_time': 0.4557979106903076, 'global_step': 38678, 'preemption_count': 0}), (41096, {'train/accuracy': 0.6108484864234924, 'train/loss': 1.9878480434417725, 'train/bleu': 28.27439783242662, 'validation/accuracy': 0.6305067539215088, 'validation/loss': 1.8244279623031616, 'validation/bleu': 25.164706094613813, 'validation/num_examples': 3000, 'test/accuracy': 0.6390215754508972, 'test/loss': 1.7725989818572998, 'test/bleu': 24.106862780559368, 'test/num_examples': 3003, 'score': 14311.619910955429, 'total_duration': 23991.232154369354, 'accumulated_submission_time': 14311.619910955429, 'accumulated_eval_time': 9677.77205824852, 'accumulated_logging_time': 0.48833751678466797, 'global_step': 41096, 'preemption_count': 0}), (43513, {'train/accuracy': 0.607908308506012, 'train/loss': 2.006985902786255, 'train/bleu': 29.054417956878826, 'validation/accuracy': 0.6317094564437866, 'validation/loss': 1.8237781524658203, 'validation/bleu': 25.956385935198032, 'validation/num_examples': 3000, 'test/accuracy': 0.6409505605697632, 'test/loss': 1.7720146179199219, 'test/bleu': 25.588925953245663, 'test/num_examples': 3003, 'score': 15151.617607355118, 'total_duration': 25374.506201982498, 'accumulated_submission_time': 15151.617607355118, 'accumulated_eval_time': 10220.935767889023, 'accumulated_logging_time': 0.5191919803619385, 'global_step': 43513, 'preemption_count': 0}), (45930, {'train/accuracy': 0.6155939102172852, 'train/loss': 1.9596761465072632, 'train/bleu': 29.238332232257907, 'validation/accuracy': 0.633432924747467, 'validation/loss': 1.8007560968399048, 'validation/bleu': 26.45347452918118, 'validation/num_examples': 3000, 'test/accuracy': 0.6413573026657104, 'test/loss': 1.751164436340332, 'test/bleu': 25.745064110493338, 'test/num_examples': 3003, 'score': 15991.709733009338, 'total_duration': 26745.632090568542, 'accumulated_submission_time': 15991.709733009338, 'accumulated_eval_time': 10751.859727859497, 'accumulated_logging_time': 0.5495162010192871, 'global_step': 45930, 'preemption_count': 0}), (48348, {'train/accuracy': 0.6091585755348206, 'train/loss': 1.986215591430664, 'train/bleu': 28.95863394471144, 'validation/accuracy': 0.6338297128677368, 'validation/loss': 1.803328514099121, 'validation/bleu': 26.30413678744673, 'validation/num_examples': 3000, 'test/accuracy': 0.6401836276054382, 'test/loss': 1.750659704208374, 'test/bleu': 25.13559768501155, 'test/num_examples': 3003, 'score': 16831.879290819168, 'total_duration': 28087.13062429428, 'accumulated_submission_time': 16831.879290819168, 'accumulated_eval_time': 11253.07631278038, 'accumulated_logging_time': 0.5821371078491211, 'global_step': 48348, 'preemption_count': 0}), (50765, {'train/accuracy': 0.6214835047721863, 'train/loss': 1.8999727964401245, 'train/bleu': 29.929704359714087, 'validation/accuracy': 0.6367310881614685, 'validation/loss': 1.78389310836792, 'validation/bleu': 26.166997541679212, 'validation/num_examples': 3000, 'test/accuracy': 0.6443437337875366, 'test/loss': 1.731534719467163, 'test/bleu': 25.595392106041817, 'test/num_examples': 3003, 'score': 17671.927947998047, 'total_duration': 29506.834899187088, 'accumulated_submission_time': 17671.927947998047, 'accumulated_eval_time': 11832.61929321289, 'accumulated_logging_time': 0.6142852306365967, 'global_step': 50765, 'preemption_count': 0}), (53183, {'train/accuracy': 0.6156458258628845, 'train/loss': 1.9492367506027222, 'train/bleu': 29.42759289127698, 'validation/accuracy': 0.6360987424850464, 'validation/loss': 1.7813689708709717, 'validation/bleu': 26.239123202813865, 'validation/num_examples': 3000, 'test/accuracy': 0.6461565494537354, 'test/loss': 1.7227002382278442, 'test/bleu': 25.632687475591855, 'test/num_examples': 3003, 'score': 18511.936827898026, 'total_duration': 30875.469685792923, 'accumulated_submission_time': 18511.936827898026, 'accumulated_eval_time': 12361.133578777313, 'accumulated_logging_time': 0.6469564437866211, 'global_step': 53183, 'preemption_count': 0}), (55600, {'train/accuracy': 0.6179764866828918, 'train/loss': 1.9319465160369873, 'train/bleu': 28.862571996639158, 'validation/accuracy': 0.64002925157547, 'validation/loss': 1.7602189779281616, 'validation/bleu': 26.359163168974383, 'validation/num_examples': 3000, 'test/accuracy': 0.6483876705169678, 'test/loss': 1.7087100744247437, 'test/bleu': 25.566127278252768, 'test/num_examples': 3003, 'score': 19352.044857025146, 'total_duration': 32251.699042081833, 'accumulated_submission_time': 19352.044857025146, 'accumulated_eval_time': 12897.139957427979, 'accumulated_logging_time': 0.6800875663757324, 'global_step': 55600, 'preemption_count': 0}), (58017, {'train/accuracy': 0.6243790984153748, 'train/loss': 1.8864920139312744, 'train/bleu': 30.10999079467282, 'validation/accuracy': 0.6418147087097168, 'validation/loss': 1.7520946264266968, 'validation/bleu': 26.486150866715008, 'validation/num_examples': 3000, 'test/accuracy': 0.6493057012557983, 'test/loss': 1.694807529449463, 'test/bleu': 26.47097900002062, 'test/num_examples': 3003, 'score': 20191.96054983139, 'total_duration': 33636.40701699257, 'accumulated_submission_time': 20191.96054983139, 'accumulated_eval_time': 13441.818828821182, 'accumulated_logging_time': 0.7141232490539551, 'global_step': 58017, 'preemption_count': 0}), (60436, {'train/accuracy': 0.6193897128105164, 'train/loss': 1.9212905168533325, 'train/bleu': 29.84611340870408, 'validation/accuracy': 0.6405872106552124, 'validation/loss': 1.7477840185165405, 'validation/bleu': 26.8364294960986, 'validation/num_examples': 3000, 'test/accuracy': 0.6511765718460083, 'test/loss': 1.6859363317489624, 'test/bleu': 25.956983521488905, 'test/num_examples': 3003, 'score': 21032.152578353882, 'total_duration': 35020.5366435051, 'accumulated_submission_time': 21032.152578353882, 'accumulated_eval_time': 13985.639262914658, 'accumulated_logging_time': 0.752568244934082, 'global_step': 60436, 'preemption_count': 0}), (62854, {'train/accuracy': 0.6402044296264648, 'train/loss': 1.7503422498703003, 'train/bleu': 31.06420597935536, 'validation/accuracy': 0.6426454782485962, 'validation/loss': 1.7346762418746948, 'validation/bleu': 26.713522805537497, 'validation/num_examples': 3000, 'test/accuracy': 0.6542444229125977, 'test/loss': 1.6709492206573486, 'test/bleu': 26.212849654597935, 'test/num_examples': 3003, 'score': 21872.31868505478, 'total_duration': 36391.20599746704, 'accumulated_submission_time': 21872.31868505478, 'accumulated_eval_time': 14516.028207540512, 'accumulated_logging_time': 0.7864320278167725, 'global_step': 62854, 'preemption_count': 0}), (65273, {'train/accuracy': 0.6218117475509644, 'train/loss': 1.9062598943710327, 'train/bleu': 29.803210788948807, 'validation/accuracy': 0.6440093517303467, 'validation/loss': 1.7179428339004517, 'validation/bleu': 26.75458586334858, 'validation/num_examples': 3000, 'test/accuracy': 0.6531752943992615, 'test/loss': 1.6693758964538574, 'test/bleu': 26.230637257259648, 'test/num_examples': 3003, 'score': 22712.382335186005, 'total_duration': 37817.06096410751, 'accumulated_submission_time': 22712.382335186005, 'accumulated_eval_time': 15101.708109140396, 'accumulated_logging_time': 0.8203840255737305, 'global_step': 65273, 'preemption_count': 0}), (67690, {'train/accuracy': 0.6277031898498535, 'train/loss': 1.868759274482727, 'train/bleu': 30.25100134313, 'validation/accuracy': 0.6481258869171143, 'validation/loss': 1.7066301107406616, 'validation/bleu': 26.141904725586556, 'validation/num_examples': 3000, 'test/accuracy': 0.6579745411872864, 'test/loss': 1.6437774896621704, 'test/bleu': 26.80649253582942, 'test/num_examples': 3003, 'score': 23552.53147172928, 'total_duration': 39461.53255653381, 'accumulated_submission_time': 23552.53147172928, 'accumulated_eval_time': 15905.911636829376, 'accumulated_logging_time': 0.8546721935272217, 'global_step': 67690, 'preemption_count': 0}), (70107, {'train/accuracy': 0.6321364045143127, 'train/loss': 1.8177008628845215, 'train/bleu': 30.434628193983656, 'validation/accuracy': 0.6484978199005127, 'validation/loss': 1.7004215717315674, 'validation/bleu': 27.146623495404736, 'validation/num_examples': 3000, 'test/accuracy': 0.6586717963218689, 'test/loss': 1.635362982749939, 'test/bleu': 26.89588272520431, 'test/num_examples': 3003, 'score': 24392.43099117279, 'total_duration': 40835.84396624565, 'accumulated_submission_time': 24392.43099117279, 'accumulated_eval_time': 16440.199833631516, 'accumulated_logging_time': 0.8966896533966064, 'global_step': 70107, 'preemption_count': 0}), (72525, {'train/accuracy': 0.6318503618240356, 'train/loss': 1.8400778770446777, 'train/bleu': 30.325490652572917, 'validation/accuracy': 0.6501221060752869, 'validation/loss': 1.6853595972061157, 'validation/bleu': 27.80913835525352, 'validation/num_examples': 3000, 'test/accuracy': 0.6619255542755127, 'test/loss': 1.614753007888794, 'test/bleu': 26.698471399758624, 'test/num_examples': 3003, 'score': 25232.470304965973, 'total_duration': 42266.7501308918, 'accumulated_submission_time': 25232.470304965973, 'accumulated_eval_time': 17030.950835227966, 'accumulated_logging_time': 0.9336686134338379, 'global_step': 72525, 'preemption_count': 0}), (74944, {'train/accuracy': 0.6331945657730103, 'train/loss': 1.8335165977478027, 'train/bleu': 30.634850980771578, 'validation/accuracy': 0.6535691022872925, 'validation/loss': 1.6698365211486816, 'validation/bleu': 27.427614088747323, 'validation/num_examples': 3000, 'test/accuracy': 0.6597524881362915, 'test/loss': 1.6147087812423706, 'test/bleu': 26.434105724544263, 'test/num_examples': 3003, 'score': 26072.504900217056, 'total_duration': 43725.81726980209, 'accumulated_submission_time': 26072.504900217056, 'accumulated_eval_time': 17649.86233663559, 'accumulated_logging_time': 0.9771442413330078, 'global_step': 74944, 'preemption_count': 0}), (77361, {'train/accuracy': 0.6339204907417297, 'train/loss': 1.8167153596878052, 'train/bleu': 30.797597115589767, 'validation/accuracy': 0.6549205780029297, 'validation/loss': 1.6654349565505981, 'validation/bleu': 27.880582151012234, 'validation/num_examples': 3000, 'test/accuracy': 0.6661786437034607, 'test/loss': 1.5921257734298706, 'test/bleu': 27.594957648339527, 'test/num_examples': 3003, 'score': 26912.40016245842, 'total_duration': 45080.52706384659, 'accumulated_submission_time': 26912.40016245842, 'accumulated_eval_time': 18164.56058192253, 'accumulated_logging_time': 1.0137157440185547, 'global_step': 77361, 'preemption_count': 0}), (79779, {'train/accuracy': 0.6340129375457764, 'train/loss': 1.818768858909607, 'train/bleu': 31.369896513488115, 'validation/accuracy': 0.6570656299591064, 'validation/loss': 1.6416860818862915, 'validation/bleu': 28.10779161190765, 'validation/num_examples': 3000, 'test/accuracy': 0.6673523187637329, 'test/loss': 1.5821276903152466, 'test/bleu': 27.324396090936787, 'test/num_examples': 3003, 'score': 27752.500133752823, 'total_duration': 46464.30451631546, 'accumulated_submission_time': 27752.500133752823, 'accumulated_eval_time': 18708.115093946457, 'accumulated_logging_time': 1.0589666366577148, 'global_step': 79779, 'preemption_count': 0}), (82197, {'train/accuracy': 0.6473284363746643, 'train/loss': 1.7201316356658936, 'train/bleu': 31.41526678659185, 'validation/accuracy': 0.659334659576416, 'validation/loss': 1.6313279867172241, 'validation/bleu': 28.270451851715933, 'validation/num_examples': 3000, 'test/accuracy': 0.6687816381454468, 'test/loss': 1.5679576396942139, 'test/bleu': 27.693554762625116, 'test/num_examples': 3003, 'score': 28592.67398762703, 'total_duration': 47843.56256365776, 'accumulated_submission_time': 28592.67398762703, 'accumulated_eval_time': 19247.07579922676, 'accumulated_logging_time': 1.102534532546997, 'global_step': 82197, 'preemption_count': 0}), (84615, {'train/accuracy': 0.6412078142166138, 'train/loss': 1.7685593366622925, 'train/bleu': 30.883998162965828, 'validation/accuracy': 0.6598802208900452, 'validation/loss': 1.6230331659317017, 'validation/bleu': 27.8358965330051, 'validation/num_examples': 3000, 'test/accuracy': 0.6705595254898071, 'test/loss': 1.5564008951187134, 'test/bleu': 27.64755868083939, 'test/num_examples': 3003, 'score': 29432.80449271202, 'total_duration': 49250.32803225517, 'accumulated_submission_time': 29432.80449271202, 'accumulated_eval_time': 19813.59550333023, 'accumulated_logging_time': 1.1386573314666748, 'global_step': 84615, 'preemption_count': 0}), (87033, {'train/accuracy': 0.6418304443359375, 'train/loss': 1.7551774978637695, 'train/bleu': 31.223073845288226, 'validation/accuracy': 0.6620252728462219, 'validation/loss': 1.6027518510818481, 'validation/bleu': 28.48434794106564, 'validation/num_examples': 3000, 'test/accuracy': 0.6758236289024353, 'test/loss': 1.5261303186416626, 'test/bleu': 28.10549738521797, 'test/num_examples': 3003, 'score': 30272.858829975128, 'total_duration': 50639.543586969376, 'accumulated_submission_time': 30272.858829975128, 'accumulated_eval_time': 20362.637778520584, 'accumulated_logging_time': 1.175865888595581, 'global_step': 87033, 'preemption_count': 0}), (89450, {'train/accuracy': 0.6507663130760193, 'train/loss': 1.6971262693405151, 'train/bleu': 31.590313155448776, 'validation/accuracy': 0.6641083359718323, 'validation/loss': 1.5904189348220825, 'validation/bleu': 28.41043268094581, 'validation/num_examples': 3000, 'test/accuracy': 0.6781593561172485, 'test/loss': 1.5138115882873535, 'test/bleu': 28.19384526539295, 'test/num_examples': 3003, 'score': 31112.956319332123, 'total_duration': 52183.437497615814, 'accumulated_submission_time': 31112.956319332123, 'accumulated_eval_time': 21066.31170296669, 'accumulated_logging_time': 1.2124691009521484, 'global_step': 89450, 'preemption_count': 0}), (91866, {'train/accuracy': 0.6441076993942261, 'train/loss': 1.7398148775100708, 'train/bleu': 31.650447712787365, 'validation/accuracy': 0.6667988896369934, 'validation/loss': 1.5745916366577148, 'validation/bleu': 28.447228632456838, 'validation/num_examples': 3000, 'test/accuracy': 0.6778688430786133, 'test/loss': 1.5026555061340332, 'test/bleu': 28.213751552293576, 'test/num_examples': 3003, 'score': 31952.85079932213, 'total_duration': 53667.065616846085, 'accumulated_submission_time': 31952.85079932213, 'accumulated_eval_time': 21709.92742419243, 'accumulated_logging_time': 1.2507178783416748, 'global_step': 91866, 'preemption_count': 0}), (94283, {'train/accuracy': 0.6625533699989319, 'train/loss': 1.6080623865127563, 'train/bleu': 33.16702852310345, 'validation/accuracy': 0.6701342463493347, 'validation/loss': 1.558275818824768, 'validation/bleu': 28.759051738058805, 'validation/num_examples': 3000, 'test/accuracy': 0.6807971596717834, 'test/loss': 1.4862122535705566, 'test/bleu': 28.15908149035932, 'test/num_examples': 3003, 'score': 32792.86261463165, 'total_duration': 55028.61365580559, 'accumulated_submission_time': 32792.86261463165, 'accumulated_eval_time': 22231.343421697617, 'accumulated_logging_time': 1.2895457744598389, 'global_step': 94283, 'preemption_count': 0}), (96701, {'train/accuracy': 0.6566953063011169, 'train/loss': 1.6568286418914795, 'train/bleu': 31.842143989658336, 'validation/accuracy': 0.670741856098175, 'validation/loss': 1.5458422899246216, 'validation/bleu': 29.138297350353145, 'validation/num_examples': 3000, 'test/accuracy': 0.6864447593688965, 'test/loss': 1.4664220809936523, 'test/bleu': 29.245032393980694, 'test/num_examples': 3003, 'score': 33633.10122871399, 'total_duration': 56415.408299446106, 'accumulated_submission_time': 33633.10122871399, 'accumulated_eval_time': 22777.782760620117, 'accumulated_logging_time': 1.3282885551452637, 'global_step': 96701, 'preemption_count': 0}), (99118, {'train/accuracy': 0.6576961874961853, 'train/loss': 1.6613869667053223, 'train/bleu': 32.159575954341314, 'validation/accuracy': 0.6734076142311096, 'validation/loss': 1.527236819267273, 'validation/bleu': 29.481519388764976, 'validation/num_examples': 3000, 'test/accuracy': 0.6870489716529846, 'test/loss': 1.4522655010223389, 'test/bleu': 29.00504494574323, 'test/num_examples': 3003, 'score': 34473.18996334076, 'total_duration': 57760.54006314278, 'accumulated_submission_time': 34473.18996334076, 'accumulated_eval_time': 23282.706238031387, 'accumulated_logging_time': 1.3684589862823486, 'global_step': 99118, 'preemption_count': 0}), (101536, {'train/accuracy': 0.6628363132476807, 'train/loss': 1.61624276638031, 'train/bleu': 33.3643248815988, 'validation/accuracy': 0.6767801642417908, 'validation/loss': 1.5128036737442017, 'validation/bleu': 29.354188999467766, 'validation/num_examples': 3000, 'test/accuracy': 0.6894544363021851, 'test/loss': 1.432341456413269, 'test/bleu': 29.022071865705737, 'test/num_examples': 3003, 'score': 35313.20211029053, 'total_duration': 59208.66853928566, 'accumulated_submission_time': 35313.20211029053, 'accumulated_eval_time': 23890.701112031937, 'accumulated_logging_time': 1.4091482162475586, 'global_step': 101536, 'preemption_count': 0}), (103954, {'train/accuracy': 0.6624199748039246, 'train/loss': 1.6151397228240967, 'train/bleu': 32.52034005600726, 'validation/accuracy': 0.6787640452384949, 'validation/loss': 1.4980305433273315, 'validation/bleu': 29.72840660397787, 'validation/num_examples': 3000, 'test/accuracy': 0.6900238394737244, 'test/loss': 1.422572374343872, 'test/bleu': 29.251162714857767, 'test/num_examples': 3003, 'score': 36153.19030022621, 'total_duration': 60624.50135970116, 'accumulated_submission_time': 36153.19030022621, 'accumulated_eval_time': 24466.426047563553, 'accumulated_logging_time': 1.4494218826293945, 'global_step': 103954, 'preemption_count': 0}), (106372, {'train/accuracy': 0.7069824934005737, 'train/loss': 1.3659493923187256, 'train/bleu': 36.09946513355275, 'validation/accuracy': 0.6812934875488281, 'validation/loss': 1.4833416938781738, 'validation/bleu': 29.75335012117342, 'validation/num_examples': 3000, 'test/accuracy': 0.6945441961288452, 'test/loss': 1.4052151441574097, 'test/bleu': 29.731491757638405, 'test/num_examples': 3003, 'score': 36993.35413503647, 'total_duration': 62028.96114087105, 'accumulated_submission_time': 36993.35413503647, 'accumulated_eval_time': 25030.5927362442, 'accumulated_logging_time': 1.497650384902954, 'global_step': 106372, 'preemption_count': 0}), (108790, {'train/accuracy': 0.6711868047714233, 'train/loss': 1.5594106912612915, 'train/bleu': 33.68167779038636, 'validation/accuracy': 0.682409405708313, 'validation/loss': 1.4713393449783325, 'validation/bleu': 29.792658094298666, 'validation/num_examples': 3000, 'test/accuracy': 0.6963105201721191, 'test/loss': 1.3900195360183716, 'test/bleu': 29.78246186358487, 'test/num_examples': 3003, 'score': 37833.25030493736, 'total_duration': 63443.191420555115, 'accumulated_submission_time': 37833.25030493736, 'accumulated_eval_time': 25604.80843281746, 'accumulated_logging_time': 1.5368640422821045, 'global_step': 108790, 'preemption_count': 0}), (111208, {'train/accuracy': 0.6703004240989685, 'train/loss': 1.5756620168685913, 'train/bleu': 33.46816417771401, 'validation/accuracy': 0.683078944683075, 'validation/loss': 1.4614923000335693, 'validation/bleu': 29.834149421480117, 'validation/num_examples': 3000, 'test/accuracy': 0.6995410323143005, 'test/loss': 1.3734357357025146, 'test/bleu': 30.15858276774193, 'test/num_examples': 3003, 'score': 38673.397919654846, 'total_duration': 64930.52002501488, 'accumulated_submission_time': 38673.397919654846, 'accumulated_eval_time': 26251.868897914886, 'accumulated_logging_time': 1.5767569541931152, 'global_step': 111208, 'preemption_count': 0}), (113626, {'train/accuracy': 0.6843309998512268, 'train/loss': 1.4797334671020508, 'train/bleu': 34.52292679167273, 'validation/accuracy': 0.6871954202651978, 'validation/loss': 1.4451779127120972, 'validation/bleu': 30.258902249238222, 'validation/num_examples': 3000, 'test/accuracy': 0.7013537883758545, 'test/loss': 1.3622872829437256, 'test/bleu': 30.219203249138683, 'test/num_examples': 3003, 'score': 39513.55310797691, 'total_duration': 66325.00939846039, 'accumulated_submission_time': 39513.55310797691, 'accumulated_eval_time': 26806.072550058365, 'accumulated_logging_time': 1.625173568725586, 'global_step': 113626, 'preemption_count': 0}), (116044, {'train/accuracy': 0.677942156791687, 'train/loss': 1.5225650072097778, 'train/bleu': 34.21801980482632, 'validation/accuracy': 0.6876294016838074, 'validation/loss': 1.43970787525177, 'validation/bleu': 30.542062526187728, 'validation/num_examples': 3000, 'test/accuracy': 0.7033408880233765, 'test/loss': 1.350867509841919, 'test/bleu': 30.47281998498783, 'test/num_examples': 3003, 'score': 40353.66239070892, 'total_duration': 67777.30443549156, 'accumulated_submission_time': 40353.66239070892, 'accumulated_eval_time': 27418.13508272171, 'accumulated_logging_time': 1.6672179698944092, 'global_step': 116044, 'preemption_count': 0}), (118462, {'train/accuracy': 0.6817564964294434, 'train/loss': 1.5030604600906372, 'train/bleu': 34.63627435914792, 'validation/accuracy': 0.6891049146652222, 'validation/loss': 1.4295117855072021, 'validation/bleu': 30.23524154073049, 'validation/num_examples': 3000, 'test/accuracy': 0.705479085445404, 'test/loss': 1.3375895023345947, 'test/bleu': 30.772976650422855, 'test/num_examples': 3003, 'score': 41193.64802837372, 'total_duration': 69187.57227706909, 'accumulated_submission_time': 41193.64802837372, 'accumulated_eval_time': 27988.293677330017, 'accumulated_logging_time': 1.7116749286651611, 'global_step': 118462, 'preemption_count': 0}), (120881, {'train/accuracy': 0.6928209066390991, 'train/loss': 1.4292106628417969, 'train/bleu': 35.03023370942456, 'validation/accuracy': 0.6907663941383362, 'validation/loss': 1.4212204217910767, 'validation/bleu': 30.38323877714415, 'validation/num_examples': 3000, 'test/accuracy': 0.7065946459770203, 'test/loss': 1.3328224420547485, 'test/bleu': 30.69767269998512, 'test/num_examples': 3003, 'score': 42033.85468816757, 'total_duration': 70600.94706273079, 'accumulated_submission_time': 42033.85468816757, 'accumulated_eval_time': 28561.33699631691, 'accumulated_logging_time': 1.754795789718628, 'global_step': 120881, 'preemption_count': 0}), (123300, {'train/accuracy': 0.692010223865509, 'train/loss': 1.4411240816116333, 'train/bleu': 35.203263862741345, 'validation/accuracy': 0.6924154758453369, 'validation/loss': 1.4140108823776245, 'validation/bleu': 30.84412796852067, 'validation/num_examples': 3000, 'test/accuracy': 0.7075010538101196, 'test/loss': 1.3267334699630737, 'test/bleu': 30.729561965493104, 'test/num_examples': 3003, 'score': 42874.04085731506, 'total_duration': 72031.04406666756, 'accumulated_submission_time': 42874.04085731506, 'accumulated_eval_time': 29151.126117944717, 'accumulated_logging_time': 1.7970449924468994, 'global_step': 123300, 'preemption_count': 0}), (125718, {'train/accuracy': 0.698666512966156, 'train/loss': 1.3995305299758911, 'train/bleu': 35.56513313304348, 'validation/accuracy': 0.6939033269882202, 'validation/loss': 1.4084949493408203, 'validation/bleu': 30.655571581373597, 'validation/num_examples': 3000, 'test/accuracy': 0.708349347114563, 'test/loss': 1.3209024667739868, 'test/bleu': 30.876623342710356, 'test/num_examples': 3003, 'score': 43714.043221235275, 'total_duration': 73462.10220861435, 'accumulated_submission_time': 43714.043221235275, 'accumulated_eval_time': 29742.056054353714, 'accumulated_logging_time': 1.8420050144195557, 'global_step': 125718, 'preemption_count': 0}), (128136, {'train/accuracy': 0.6984717845916748, 'train/loss': 1.4034595489501953, 'train/bleu': 35.613569304448035, 'validation/accuracy': 0.6934818029403687, 'validation/loss': 1.4072446823120117, 'validation/bleu': 30.81339413073557, 'validation/num_examples': 3000, 'test/accuracy': 0.710615336894989, 'test/loss': 1.315718173980713, 'test/bleu': 30.94520762179943, 'test/num_examples': 3003, 'score': 44554.06897211075, 'total_duration': 74888.634370327, 'accumulated_submission_time': 44554.06897211075, 'accumulated_eval_time': 30328.435676574707, 'accumulated_logging_time': 1.886183738708496, 'global_step': 128136, 'preemption_count': 0}), (130555, {'train/accuracy': 0.7004496455192566, 'train/loss': 1.3945019245147705, 'train/bleu': 35.82390045814242, 'validation/accuracy': 0.693221390247345, 'validation/loss': 1.4052176475524902, 'validation/bleu': 30.76973079067687, 'validation/num_examples': 3000, 'test/accuracy': 0.7104758620262146, 'test/loss': 1.3132108449935913, 'test/bleu': 30.946965112698592, 'test/num_examples': 3003, 'score': 45394.16031217575, 'total_duration': 76329.50495123863, 'accumulated_submission_time': 45394.16031217575, 'accumulated_eval_time': 30929.09312582016, 'accumulated_logging_time': 1.9304907321929932, 'global_step': 130555, 'preemption_count': 0}), (132973, {'train/accuracy': 0.7006556391716003, 'train/loss': 1.3924342393875122, 'train/bleu': 35.9691844802764, 'validation/accuracy': 0.6937049627304077, 'validation/loss': 1.4050370454788208, 'validation/bleu': 30.88978323109175, 'validation/num_examples': 3000, 'test/accuracy': 0.7101853489875793, 'test/loss': 1.313064455986023, 'test/bleu': 31.12658905369212, 'test/num_examples': 3003, 'score': 46234.09957194328, 'total_duration': 77774.33254957199, 'accumulated_submission_time': 46234.09957194328, 'accumulated_eval_time': 31533.856184482574, 'accumulated_logging_time': 1.9747204780578613, 'global_step': 132973, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6965222358703613, 'train/loss': 1.4127204418182373, 'train/bleu': 35.569271445766226, 'validation/accuracy': 0.6937049627304077, 'validation/loss': 1.4050407409667969, 'validation/bleu': 30.89082856801816, 'validation/num_examples': 3000, 'test/accuracy': 0.7102318406105042, 'test/loss': 1.3130912780761719, 'test/bleu': 31.113830544833622, 'test/num_examples': 3003, 'score': 46358.67147278786, 'total_duration': 78488.16630244255, 'accumulated_submission_time': 46358.67147278786, 'accumulated_eval_time': 32123.061244010925, 'accumulated_logging_time': 2.0193135738372803, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0214 03:40:09.070375 140144802662208 submission_runner.py:586] Timing: 46358.67147278786
I0214 03:40:09.070424 140144802662208 submission_runner.py:588] Total number of evals: 57
I0214 03:40:09.070462 140144802662208 submission_runner.py:589] ====================
I0214 03:40:09.070521 140144802662208 submission_runner.py:542] Using RNG seed 599091471
I0214 03:40:09.072100 140144802662208 submission_runner.py:551] --- Tuning run 5/5 ---
I0214 03:40:09.072222 140144802662208 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_5.
I0214 03:40:09.072480 140144802662208 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_5/hparams.json.
I0214 03:40:09.073285 140144802662208 submission_runner.py:206] Initializing dataset.
I0214 03:40:09.076006 140144802662208 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0214 03:40:09.078930 140144802662208 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0214 03:40:09.117608 140144802662208 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0214 03:40:09.818780 140144802662208 submission_runner.py:213] Initializing model.
I0214 03:40:16.399655 140144802662208 submission_runner.py:255] Initializing optimizer.
I0214 03:40:17.203192 140144802662208 submission_runner.py:262] Initializing metrics bundle.
I0214 03:40:17.203358 140144802662208 submission_runner.py:280] Initializing checkpoint and logger.
I0214 03:40:17.204442 140144802662208 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_3/wmt_jax/trial_5 with prefix checkpoint_
I0214 03:40:17.204566 140144802662208 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_5/meta_data_0.json.
I0214 03:40:17.204782 140144802662208 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0214 03:40:17.204843 140144802662208 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0214 03:40:17.755135 140144802662208 logger_utils.py:220] Unable to record git information. Continuing without it.
I0214 03:40:18.284320 140144802662208 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_5/flags_0.json.
I0214 03:40:18.288028 140144802662208 submission_runner.py:314] Starting training loop.
I0214 03:40:50.085348 139975047763712 logging_writer.py:48] [0] global_step=0, grad_norm=4.866031169891357, loss=10.961175918579102
I0214 03:40:50.099394 140144802662208 spec.py:321] Evaluating on the training split.
I0214 03:40:52.775804 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:45:36.703382 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 03:45:39.391178 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:50:23.480984 140144802662208 spec.py:349] Evaluating on the test split.
I0214 03:50:26.184554 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 03:55:09.821124 140144802662208 submission_runner.py:408] Time since start: 891.53s, 	Step: 1, 	{'train/accuracy': 0.0005722984205931425, 'train/loss': 10.961396217346191, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 31.811314821243286, 'total_duration': 891.5330049991608, 'accumulated_submission_time': 31.811314821243286, 'accumulated_eval_time': 859.7216436862946, 'accumulated_logging_time': 0}
I0214 03:55:09.830673 139975056156416 logging_writer.py:48] [1] accumulated_eval_time=859.721644, accumulated_logging_time=0, accumulated_submission_time=31.811315, global_step=1, preemption_count=0, score=31.811315, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.966498, test/num_examples=3003, total_duration=891.533005, train/accuracy=0.000572, train/bleu=0.000000, train/loss=10.961396, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.980294, validation/num_examples=3000
I0214 03:55:45.380225 139975047763712 logging_writer.py:48] [100] global_step=100, grad_norm=0.1630934625864029, loss=8.263269424438477
I0214 03:56:21.005643 139975056156416 logging_writer.py:48] [200] global_step=200, grad_norm=0.3448466658592224, loss=7.440059185028076
I0214 03:56:56.654834 139975047763712 logging_writer.py:48] [300] global_step=300, grad_norm=0.6337791681289673, loss=6.845795631408691
I0214 03:57:32.341758 139975056156416 logging_writer.py:48] [400] global_step=400, grad_norm=0.3896333575248718, loss=6.3780717849731445
I0214 03:58:08.015865 139975047763712 logging_writer.py:48] [500] global_step=500, grad_norm=0.48827317357063293, loss=5.895190715789795
I0214 03:58:43.646749 139975056156416 logging_writer.py:48] [600] global_step=600, grad_norm=0.6352640986442566, loss=5.568876266479492
I0214 03:59:19.314314 139975047763712 logging_writer.py:48] [700] global_step=700, grad_norm=0.4651747941970825, loss=5.335844993591309
I0214 03:59:54.967000 139975056156416 logging_writer.py:48] [800] global_step=800, grad_norm=0.6650673747062683, loss=5.09985876083374
I0214 04:00:30.619174 139975047763712 logging_writer.py:48] [900] global_step=900, grad_norm=0.5952438712120056, loss=4.8158416748046875
I0214 04:01:06.277769 139975056156416 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.45355114340782166, loss=4.576095104217529
I0214 04:01:42.078282 139975047763712 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7406876683235168, loss=4.288539409637451
I0214 04:02:17.754528 139975056156416 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.4775623381137848, loss=4.07691764831543
I0214 04:02:53.400595 139975047763712 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.46365049481391907, loss=3.9753165245056152
I0214 04:03:29.022639 139975056156416 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.4487914443016052, loss=3.867051362991333
I0214 04:04:04.669575 139975047763712 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5330284833908081, loss=3.6312334537506104
I0214 04:04:40.358855 139975056156416 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.4711976945400238, loss=3.514286756515503
I0214 04:05:16.028203 139975047763712 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5309640169143677, loss=3.4039947986602783
I0214 04:05:51.662035 139975056156416 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.4621942937374115, loss=3.3233065605163574
I0214 04:06:27.292789 139975047763712 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4023077189922333, loss=3.224061965942383
I0214 04:07:02.944382 139975056156416 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.45852112770080566, loss=3.2079708576202393
I0214 04:07:38.564535 139975047763712 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.502906084060669, loss=3.143339157104492
I0214 04:08:14.207819 139975056156416 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.4194009304046631, loss=3.107839584350586
I0214 04:08:49.815254 139975047763712 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.42138832807540894, loss=3.0280933380126953
I0214 04:09:09.843570 140144802662208 spec.py:321] Evaluating on the training split.
I0214 04:09:12.829695 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 04:11:51.401362 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 04:11:54.087792 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 04:14:39.956357 140144802662208 spec.py:349] Evaluating on the test split.
I0214 04:14:42.649801 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 04:17:05.969370 140144802662208 submission_runner.py:408] Time since start: 2207.68s, 	Step: 2358, 	{'train/accuracy': 0.5152115821838379, 'train/loss': 2.8529348373413086, 'train/bleu': 22.559558180281464, 'validation/accuracy': 0.5171913504600525, 'validation/loss': 2.8354642391204834, 'validation/bleu': 18.385519591984245, 'validation/num_examples': 3000, 'test/accuracy': 0.5146359801292419, 'test/loss': 2.8742642402648926, 'test/bleu': 17.058797586743704, 'test/num_examples': 3003, 'score': 871.7362320423126, 'total_duration': 2207.681263446808, 'accumulated_submission_time': 871.7362320423126, 'accumulated_eval_time': 1335.8473892211914, 'accumulated_logging_time': 0.02074408531188965}
I0214 04:17:05.985481 139975056156416 logging_writer.py:48] [2358] accumulated_eval_time=1335.847389, accumulated_logging_time=0.020744, accumulated_submission_time=871.736232, global_step=2358, preemption_count=0, score=871.736232, test/accuracy=0.514636, test/bleu=17.058798, test/loss=2.874264, test/num_examples=3003, total_duration=2207.681263, train/accuracy=0.515212, train/bleu=22.559558, train/loss=2.852935, validation/accuracy=0.517191, validation/bleu=18.385520, validation/loss=2.835464, validation/num_examples=3000
I0214 04:17:21.253784 139975047763712 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.4680982232093811, loss=2.929187297821045
I0214 04:17:56.718367 139975056156416 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.328622967004776, loss=3.0117266178131104
I0214 04:18:32.333287 139975047763712 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2840818762779236, loss=2.7831199169158936
I0214 04:19:07.944565 139975056156416 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.30193978548049927, loss=2.8127126693725586
I0214 04:19:43.560104 139975047763712 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.3594723045825958, loss=2.8076229095458984
I0214 04:20:19.154339 139975056156416 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.3245309293270111, loss=2.731549024581909
I0214 04:20:54.780448 139975047763712 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.3161846697330475, loss=2.7165205478668213
I0214 04:21:30.404056 139975056156416 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.25403133034706116, loss=2.7085886001586914
I0214 04:22:06.080229 139975047763712 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.2260965257883072, loss=2.6690096855163574
I0214 04:22:41.673104 139975056156416 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.25064870715141296, loss=2.55622935295105
I0214 04:23:17.300453 139975047763712 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.2416926473379135, loss=2.566655397415161
I0214 04:23:52.895034 139975056156416 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.2228008210659027, loss=2.5251405239105225
I0214 04:24:28.527663 139975047763712 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.20590896904468536, loss=2.5905818939208984
I0214 04:25:04.149730 139975056156416 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.2323971390724182, loss=2.521678924560547
I0214 04:25:39.785553 139975047763712 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.21177394688129425, loss=2.49629282951355
I0214 04:26:15.425472 139975056156416 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.20283979177474976, loss=2.4602694511413574
I0214 04:26:51.017426 139975047763712 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.20781640708446503, loss=2.484499216079712
I0214 04:27:26.612309 139975056156416 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.22695060074329376, loss=2.458177089691162
I0214 04:28:02.211549 139975047763712 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.20316898822784424, loss=2.4283947944641113
I0214 04:28:37.838068 139975056156416 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.17778632044792175, loss=2.3816006183624268
I0214 04:29:13.459067 139975047763712 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.1804407387971878, loss=2.386629581451416
I0214 04:29:49.065363 139975056156416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.20165826380252838, loss=2.3364925384521484
I0214 04:30:24.659563 139975047763712 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.18896493315696716, loss=2.331134796142578
I0214 04:31:00.298853 139975056156416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.19070664048194885, loss=2.316213369369507
I0214 04:31:06.088501 140144802662208 spec.py:321] Evaluating on the training split.
I0214 04:31:09.062202 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 04:34:21.177087 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 04:34:23.882964 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 04:36:59.077039 140144802662208 spec.py:349] Evaluating on the test split.
I0214 04:37:01.768098 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 04:39:29.672124 140144802662208 submission_runner.py:408] Time since start: 3551.38s, 	Step: 4718, 	{'train/accuracy': 0.5785308480262756, 'train/loss': 2.2483465671539307, 'train/bleu': 27.164030770714678, 'validation/accuracy': 0.5911272168159485, 'validation/loss': 2.150007486343384, 'validation/bleu': 23.69484945016536, 'validation/num_examples': 3000, 'test/accuracy': 0.5932833552360535, 'test/loss': 2.123673677444458, 'test/bleu': 22.21513088925912, 'test/num_examples': 3003, 'score': 1711.7513601779938, 'total_duration': 3551.384021759033, 'accumulated_submission_time': 1711.7513601779938, 'accumulated_eval_time': 1839.4309611320496, 'accumulated_logging_time': 0.04746723175048828}
I0214 04:39:29.686872 139975047763712 logging_writer.py:48] [4718] accumulated_eval_time=1839.430961, accumulated_logging_time=0.047467, accumulated_submission_time=1711.751360, global_step=4718, preemption_count=0, score=1711.751360, test/accuracy=0.593283, test/bleu=22.215131, test/loss=2.123674, test/num_examples=3003, total_duration=3551.384022, train/accuracy=0.578531, train/bleu=27.164031, train/loss=2.248347, validation/accuracy=0.591127, validation/bleu=23.694849, validation/loss=2.150007, validation/num_examples=3000
I0214 04:39:59.147993 139975056156416 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.2091706544160843, loss=2.3020875453948975
I0214 04:40:34.692788 139975047763712 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.18689896166324615, loss=2.3381738662719727
I0214 04:41:10.296042 139975056156416 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.1645199954509735, loss=2.3209056854248047
I0214 04:41:45.959086 139975047763712 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.16782556474208832, loss=2.268517017364502
I0214 04:42:21.575542 139975056156416 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.1513034999370575, loss=2.2577548027038574
I0214 04:42:57.152025 139975047763712 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.16595275700092316, loss=2.238401174545288
I0214 04:43:32.769759 139975056156416 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.15836960077285767, loss=2.2457268238067627
I0214 04:44:08.366114 139975047763712 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.15449373424053192, loss=2.2876853942871094
I0214 04:44:44.035274 139975056156416 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.2127392441034317, loss=2.2747673988342285
I0214 04:45:19.673922 139975047763712 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.17878888547420502, loss=2.2550041675567627
I0214 04:45:55.300164 139975056156416 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.18004843592643738, loss=2.2220423221588135
I0214 04:46:30.953636 139975047763712 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.17789480090141296, loss=2.216279983520508
I0214 04:47:06.598209 139975056156416 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.16484802961349487, loss=2.213916063308716
I0214 04:47:42.308633 139975047763712 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.1659655123949051, loss=2.2321834564208984
I0214 04:48:17.954114 139975056156416 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.1991206854581833, loss=2.2589926719665527
I0214 04:48:53.581331 139975047763712 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.16659614443778992, loss=2.157327890396118
I0214 04:49:29.207786 139975056156416 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.14557570219039917, loss=2.181070566177368
I0214 04:50:04.819315 139975047763712 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.19331945478916168, loss=2.2175257205963135
I0214 04:50:40.472530 139975056156416 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.17526914179325104, loss=2.2600290775299072
I0214 04:51:16.098865 139975047763712 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.14389212429523468, loss=2.1827142238616943
I0214 04:51:51.716178 139975056156416 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.17818093299865723, loss=2.1572937965393066
I0214 04:52:27.374399 139975047763712 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.188208669424057, loss=2.20625901222229
I0214 04:53:03.035829 139975056156416 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.14213484525680542, loss=2.163649797439575
I0214 04:53:29.823597 140144802662208 spec.py:321] Evaluating on the training split.
I0214 04:53:32.793979 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 04:55:56.369914 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 04:55:59.047993 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 04:58:30.500207 140144802662208 spec.py:349] Evaluating on the test split.
I0214 04:58:33.189688 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 05:00:49.657588 140144802662208 submission_runner.py:408] Time since start: 4831.37s, 	Step: 7077, 	{'train/accuracy': 0.6040958762168884, 'train/loss': 2.007240056991577, 'train/bleu': 29.060628161671126, 'validation/accuracy': 0.6170040965080261, 'validation/loss': 1.9235773086547852, 'validation/bleu': 25.445472800205756, 'validation/num_examples': 3000, 'test/accuracy': 0.6244146227836609, 'test/loss': 1.8764768838882446, 'test/bleu': 24.14670156771929, 'test/num_examples': 3003, 'score': 2551.796592235565, 'total_duration': 4831.369480133057, 'accumulated_submission_time': 2551.796592235565, 'accumulated_eval_time': 2279.2648980617523, 'accumulated_logging_time': 0.07353043556213379}
I0214 05:00:49.673974 139975047763712 logging_writer.py:48] [7077] accumulated_eval_time=2279.264898, accumulated_logging_time=0.073530, accumulated_submission_time=2551.796592, global_step=7077, preemption_count=0, score=2551.796592, test/accuracy=0.624415, test/bleu=24.146702, test/loss=1.876477, test/num_examples=3003, total_duration=4831.369480, train/accuracy=0.604096, train/bleu=29.060628, train/loss=2.007240, validation/accuracy=0.617004, validation/bleu=25.445473, validation/loss=1.923577, validation/num_examples=3000
I0214 05:00:58.215414 139975056156416 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.2101561278104782, loss=2.2185349464416504
I0214 05:01:33.718679 139975047763712 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.16041521728038788, loss=2.1273295879364014
I0214 05:02:09.292128 139975056156416 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.19892345368862152, loss=2.0766162872314453
I0214 05:02:44.881658 139975047763712 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.14810876548290253, loss=2.053485631942749
I0214 05:03:20.532876 139975056156416 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.18310077488422394, loss=2.082098960876465
I0214 05:03:56.164943 139975047763712 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.1555284857749939, loss=2.036597728729248
I0214 05:04:31.806898 139975056156416 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.18913744390010834, loss=2.0914158821105957
I0214 05:05:07.418176 139975047763712 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.24312572181224823, loss=2.134563446044922
I0214 05:05:43.041294 139975056156416 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.17842914164066315, loss=2.179497003555298
I0214 05:06:18.696008 139975047763712 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.15164963901042938, loss=2.061892032623291
I0214 05:06:54.396566 139975056156416 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.1736087054014206, loss=2.0813632011413574
I0214 05:07:30.038821 139975047763712 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.1522124856710434, loss=2.092090606689453
I0214 05:08:05.645982 139975056156416 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.20023062825202942, loss=2.0645480155944824
I0214 05:08:41.252566 139975047763712 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.1763761341571808, loss=2.084585666656494
I0214 05:09:17.064596 139975056156416 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.16893206536769867, loss=1.9522544145584106
I0214 05:09:52.726093 139975047763712 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.16594113409519196, loss=1.9766757488250732
I0214 05:10:28.385601 139975056156416 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.17702358961105347, loss=2.059849739074707
I0214 05:11:04.014522 139975047763712 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.181117445230484, loss=1.9731093645095825
I0214 05:11:39.664169 139975056156416 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.1988859325647354, loss=1.9965829849243164
I0214 05:12:15.316922 139975047763712 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.15271402895450592, loss=2.14304518699646
I0214 05:12:50.943232 139975056156416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.25799262523651123, loss=2.054232358932495
I0214 05:13:26.571628 139975047763712 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.16838887333869934, loss=2.1430060863494873
I0214 05:14:02.217783 139975056156416 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.16201797127723694, loss=2.0320706367492676
I0214 05:14:37.853113 139975047763712 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.20565585792064667, loss=2.101919174194336
I0214 05:14:49.686769 140144802662208 spec.py:321] Evaluating on the training split.
I0214 05:14:52.663541 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 05:17:36.991705 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 05:17:39.681152 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 05:20:07.173756 140144802662208 spec.py:349] Evaluating on the test split.
I0214 05:20:09.850452 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 05:22:23.234694 140144802662208 submission_runner.py:408] Time since start: 6124.95s, 	Step: 9435, 	{'train/accuracy': 0.6132701635360718, 'train/loss': 1.9433298110961914, 'train/bleu': 29.630833378883725, 'validation/accuracy': 0.6301347613334656, 'validation/loss': 1.8156956434249878, 'validation/bleu': 26.173138325326224, 'validation/num_examples': 3000, 'test/accuracy': 0.6360118389129639, 'test/loss': 1.7616527080535889, 'test/bleu': 24.85918981164338, 'test/num_examples': 3003, 'score': 3391.7186181545258, 'total_duration': 6124.946580171585, 'accumulated_submission_time': 3391.7186181545258, 'accumulated_eval_time': 2732.8127586841583, 'accumulated_logging_time': 0.10128641128540039}
I0214 05:22:23.250924 139975056156416 logging_writer.py:48] [9435] accumulated_eval_time=2732.812759, accumulated_logging_time=0.101286, accumulated_submission_time=3391.718618, global_step=9435, preemption_count=0, score=3391.718618, test/accuracy=0.636012, test/bleu=24.859190, test/loss=1.761653, test/num_examples=3003, total_duration=6124.946580, train/accuracy=0.613270, train/bleu=29.630833, train/loss=1.943330, validation/accuracy=0.630135, validation/bleu=26.173138, validation/loss=1.815696, validation/num_examples=3000
I0214 05:22:46.635507 139975047763712 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.15386544167995453, loss=1.999233365058899
I0214 05:23:22.182688 139975056156416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.15350881218910217, loss=2.008007764816284
I0214 05:23:57.816631 139975047763712 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.24001027643680573, loss=1.938409447669983
I0214 05:24:33.503781 139975056156416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.16338375210762024, loss=2.006361961364746
I0214 05:25:09.173023 139975047763712 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.17941631376743317, loss=1.9906830787658691
I0214 05:25:44.826056 139975056156416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.19779770076274872, loss=1.964518427848816
I0214 05:26:20.448074 139975047763712 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.2604733109474182, loss=2.0903000831604004
I0214 05:26:56.077100 139975056156416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.28964146971702576, loss=1.9869000911712646
I0214 05:27:31.685633 139975047763712 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.22336836159229279, loss=1.9960156679153442
I0214 05:28:07.335999 139975056156416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.16861651837825775, loss=1.9987190961837769
I0214 05:28:42.977000 139975047763712 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.23260104656219482, loss=2.11038875579834
I0214 05:29:18.629796 139975056156416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.24001042544841766, loss=2.006242513656616
I0214 05:29:54.288591 139975047763712 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.2316485494375229, loss=2.0749754905700684
I0214 05:30:29.930269 139975056156416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.2143714725971222, loss=2.007021427154541
I0214 05:31:05.559212 139975047763712 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.21335631608963013, loss=1.9866209030151367
I0214 05:31:41.170203 139975056156416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.24619898200035095, loss=2.1185836791992188
I0214 05:32:16.793358 139975047763712 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.18509738147258759, loss=1.9145395755767822
I0214 05:32:52.419765 139975056156416 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.21946339309215546, loss=1.9238523244857788
I0214 05:33:28.045636 139975047763712 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.2481270134449005, loss=1.994776725769043
I0214 05:34:03.685971 139975056156416 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.2871457636356354, loss=1.9706138372421265
I0214 05:34:39.317662 139975047763712 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.19422245025634766, loss=1.9800758361816406
I0214 05:35:14.964412 139975056156416 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.20611320436000824, loss=1.9546905755996704
I0214 05:35:50.614853 139975047763712 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.19336780905723572, loss=2.0100860595703125
I0214 05:36:23.492891 140144802662208 spec.py:321] Evaluating on the training split.
I0214 05:36:26.476653 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 05:39:29.815411 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 05:39:32.495189 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 05:41:59.153837 140144802662208 spec.py:349] Evaluating on the test split.
I0214 05:42:01.868007 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 05:44:17.343116 140144802662208 submission_runner.py:408] Time since start: 7439.06s, 	Step: 11794, 	{'train/accuracy': 0.6201885938644409, 'train/loss': 1.886154294013977, 'train/bleu': 29.409620364240315, 'validation/accuracy': 0.6406244039535522, 'validation/loss': 1.745435357093811, 'validation/bleu': 26.7636870644405, 'validation/num_examples': 3000, 'test/accuracy': 0.6483644247055054, 'test/loss': 1.6862653493881226, 'test/bleu': 25.861684393273194, 'test/num_examples': 3003, 'score': 4231.872063875198, 'total_duration': 7439.0550146102905, 'accumulated_submission_time': 4231.872063875198, 'accumulated_eval_time': 3206.6629474163055, 'accumulated_logging_time': 0.12857842445373535}
I0214 05:44:17.360906 139975056156416 logging_writer.py:48] [11794] accumulated_eval_time=3206.662947, accumulated_logging_time=0.128578, accumulated_submission_time=4231.872064, global_step=11794, preemption_count=0, score=4231.872064, test/accuracy=0.648364, test/bleu=25.861684, test/loss=1.686265, test/num_examples=3003, total_duration=7439.055015, train/accuracy=0.620189, train/bleu=29.409620, train/loss=1.886154, validation/accuracy=0.640624, validation/bleu=26.763687, validation/loss=1.745435, validation/num_examples=3000
I0214 05:44:19.855635 139975047763712 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.19061680138111115, loss=1.8808306455612183
I0214 05:44:55.309937 139975056156416 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.2970496714115143, loss=1.9859355688095093
I0214 05:45:30.819656 139975047763712 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.34235215187072754, loss=2.008742094039917
I0214 05:46:06.459048 139975056156416 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.32518231868743896, loss=1.9153833389282227
I0214 05:46:42.070218 139975047763712 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.22169284522533417, loss=1.9798753261566162
I0214 05:47:17.685034 139975056156416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2149191051721573, loss=1.8880876302719116
I0214 05:47:53.299973 139975047763712 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.17589324712753296, loss=1.937804937362671
I0214 05:48:28.946495 139975056156416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.20422805845737457, loss=2.0374045372009277
I0214 05:49:04.584112 139975047763712 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.2060365080833435, loss=1.9780688285827637
I0214 05:49:40.218928 139975056156416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1825559139251709, loss=2.023705244064331
I0214 05:50:15.856221 139975047763712 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.2497275173664093, loss=1.9779059886932373
I0214 05:50:51.482341 139975056156416 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.18204542994499207, loss=1.8767846822738647
I0214 05:51:27.141251 139975047763712 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.19264540076255798, loss=1.8986783027648926
I0214 05:52:02.842955 139975056156416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.274020254611969, loss=1.9476126432418823
I0214 05:52:38.453717 139975047763712 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.21764519810676575, loss=1.9534653425216675
I0214 05:53:14.069663 139975056156416 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2124517560005188, loss=1.9132909774780273
I0214 05:53:49.711207 139975047763712 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.25957047939300537, loss=1.9682315587997437
I0214 05:54:25.315859 139975056156416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2124422788619995, loss=1.9192736148834229
I0214 05:55:00.957814 139975047763712 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.19195400178432465, loss=1.917768955230713
I0214 05:55:36.661558 139975056156416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.23562240600585938, loss=1.9425324201583862
I0214 05:56:12.472212 139975047763712 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.19823840260505676, loss=1.8524765968322754
I0214 05:56:48.137851 139975056156416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.1933792680501938, loss=2.0237762928009033
I0214 05:57:23.798521 139975047763712 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.20508220791816711, loss=1.919573426246643
I0214 05:57:59.423966 139975056156416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.19756647944450378, loss=1.9456586837768555
I0214 05:58:17.665732 140144802662208 spec.py:321] Evaluating on the training split.
I0214 05:58:20.637940 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 06:01:45.285749 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 06:01:47.962717 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 06:04:38.093561 140144802662208 spec.py:349] Evaluating on the test split.
I0214 06:04:40.787596 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 06:07:11.744701 140144802662208 submission_runner.py:408] Time since start: 8813.46s, 	Step: 14153, 	{'train/accuracy': 0.6258756518363953, 'train/loss': 1.830366611480713, 'train/bleu': 30.30475543405984, 'validation/accuracy': 0.6443069577217102, 'validation/loss': 1.7058216333389282, 'validation/bleu': 27.178247847671944, 'validation/num_examples': 3000, 'test/accuracy': 0.6512695550918579, 'test/loss': 1.6413919925689697, 'test/bleu': 25.934985390513155, 'test/num_examples': 3003, 'score': 5072.088208436966, 'total_duration': 8813.45656490326, 'accumulated_submission_time': 5072.088208436966, 'accumulated_eval_time': 3740.741831302643, 'accumulated_logging_time': 0.15647459030151367}
I0214 06:07:11.764717 139975047763712 logging_writer.py:48] [14153] accumulated_eval_time=3740.741831, accumulated_logging_time=0.156475, accumulated_submission_time=5072.088208, global_step=14153, preemption_count=0, score=5072.088208, test/accuracy=0.651270, test/bleu=25.934985, test/loss=1.641392, test/num_examples=3003, total_duration=8813.456565, train/accuracy=0.625876, train/bleu=30.304755, train/loss=1.830367, validation/accuracy=0.644307, validation/bleu=27.178248, validation/loss=1.705822, validation/num_examples=3000
I0214 06:07:28.829248 139975056156416 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.20829357206821442, loss=1.9475775957107544
I0214 06:08:04.365974 139975047763712 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.1965392529964447, loss=1.8561582565307617
I0214 06:08:39.942674 139975056156416 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.19306305050849915, loss=1.934576153755188
I0214 06:09:15.565503 139975047763712 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.24908943474292755, loss=1.8587669134140015
I0214 06:09:51.186735 139975056156416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.17903491854667664, loss=1.9429394006729126
I0214 06:10:26.801214 139975047763712 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.17559528350830078, loss=1.969093918800354
I0214 06:11:02.459416 139975056156416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.21108143031597137, loss=1.9023149013519287
I0214 06:11:38.124369 139975047763712 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.18914374709129333, loss=1.9567850828170776
I0214 06:12:13.751021 139975056156416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.198020339012146, loss=1.8915022611618042
I0214 06:12:49.451286 139975047763712 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.18074096739292145, loss=1.9682540893554688
I0214 06:13:25.082345 139975056156416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2142987698316574, loss=1.9884305000305176
I0214 06:14:00.751224 139975047763712 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.21472853422164917, loss=1.9321510791778564
I0214 06:14:36.439783 139975056156416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3126031756401062, loss=1.8769924640655518
I0214 06:15:12.057064 139975047763712 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.25784236192703247, loss=1.8540019989013672
I0214 06:15:47.656599 139975056156416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.18759126961231232, loss=1.9466214179992676
I0214 06:16:23.265389 139975047763712 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3530009388923645, loss=1.972031593322754
I0214 06:16:58.887236 139975056156416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.30034083127975464, loss=1.889718770980835
I0214 06:17:34.507709 139975047763712 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.21895667910575867, loss=1.9317445755004883
I0214 06:18:10.150402 139975056156416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.1962980180978775, loss=1.8071861267089844
I0214 06:18:45.766540 139975047763712 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.20386049151420593, loss=1.9214168787002563
I0214 06:19:21.376730 139975056156416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.2053455412387848, loss=1.8319464921951294
I0214 06:19:56.993457 139975047763712 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.17931035161018372, loss=1.8703532218933105
I0214 06:20:32.632044 139975056156416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.33911991119384766, loss=1.8476914167404175
I0214 06:21:08.292681 139975047763712 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.27231988310813904, loss=1.9599406719207764
I0214 06:21:11.944197 140144802662208 spec.py:321] Evaluating on the training split.
I0214 06:21:14.919500 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 06:24:07.976696 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 06:24:10.659172 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 06:26:58.934242 140144802662208 spec.py:349] Evaluating on the test split.
I0214 06:27:01.627721 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 06:29:20.952825 140144802662208 submission_runner.py:408] Time since start: 10142.66s, 	Step: 16512, 	{'train/accuracy': 0.6253536343574524, 'train/loss': 1.8386945724487305, 'train/bleu': 30.426984478535974, 'validation/accuracy': 0.6469727754592896, 'validation/loss': 1.6795895099639893, 'validation/bleu': 27.51153504880521, 'validation/num_examples': 3000, 'test/accuracy': 0.6567195653915405, 'test/loss': 1.6103800535202026, 'test/bleu': 26.76462025361148, 'test/num_examples': 3003, 'score': 5912.17545747757, 'total_duration': 10142.664711236954, 'accumulated_submission_time': 5912.17545747757, 'accumulated_eval_time': 4229.750396728516, 'accumulated_logging_time': 0.18865442276000977}
I0214 06:29:20.970925 139975056156416 logging_writer.py:48] [16512] accumulated_eval_time=4229.750397, accumulated_logging_time=0.188654, accumulated_submission_time=5912.175457, global_step=16512, preemption_count=0, score=5912.175457, test/accuracy=0.656720, test/bleu=26.764620, test/loss=1.610380, test/num_examples=3003, total_duration=10142.664711, train/accuracy=0.625354, train/bleu=30.426984, train/loss=1.838695, validation/accuracy=0.646973, validation/bleu=27.511535, validation/loss=1.679590, validation/num_examples=3000
I0214 06:29:52.540989 139975047763712 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.23265960812568665, loss=1.7985063791275024
I0214 06:30:28.089794 139975056156416 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.19525061547756195, loss=1.9495760202407837
I0214 06:31:03.684552 139975047763712 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.1991039663553238, loss=1.840932846069336
I0214 06:31:39.307601 139975056156416 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.27204030752182007, loss=1.939686894416809
I0214 06:32:14.902641 139975047763712 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.22710423171520233, loss=2.015687942504883
I0214 06:32:50.523835 139975056156416 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.19898506999015808, loss=1.9501898288726807
I0214 06:33:26.146577 139975047763712 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.26104483008384705, loss=1.840407133102417
I0214 06:34:01.748130 139975056156416 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2650025188922882, loss=1.9279104471206665
I0214 06:34:37.373680 139975047763712 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.3527575135231018, loss=1.8498533964157104
I0214 06:35:13.025823 139975056156416 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.1914970874786377, loss=1.8982055187225342
I0214 06:35:48.695603 139975047763712 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.18068666756153107, loss=1.9259886741638184
I0214 06:36:24.389378 139975056156416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.19892990589141846, loss=1.8465641736984253
I0214 06:37:00.014044 139975047763712 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.17227298021316528, loss=1.8765138387680054
I0214 06:37:35.676984 139975056156416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.1873738020658493, loss=1.8423975706100464
I0214 06:38:11.444280 139975047763712 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.186864972114563, loss=1.8788740634918213
I0214 06:38:47.090037 139975056156416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.19610121846199036, loss=1.815320372581482
I0214 06:39:22.759928 139975047763712 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1918792575597763, loss=1.972800374031067
I0214 06:39:58.414147 139975056156416 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.1666465848684311, loss=1.8051050901412964
I0214 06:40:34.057290 139975047763712 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2495955526828766, loss=1.880319356918335
I0214 06:41:09.714157 139975056156416 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.22688007354736328, loss=1.8888435363769531
I0214 06:41:45.371509 139975047763712 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.1970498412847519, loss=1.8862210512161255
I0214 06:42:20.999306 139975056156416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.2075187861919403, loss=1.8174821138381958
I0214 06:42:56.607552 139975047763712 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.18843083083629608, loss=1.9075289964675903
I0214 06:43:21.258595 140144802662208 spec.py:321] Evaluating on the training split.
I0214 06:43:24.229530 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 06:47:31.845470 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 06:47:34.525966 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 06:50:27.633196 140144802662208 spec.py:349] Evaluating on the test split.
I0214 06:50:30.308312 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 06:53:08.879064 140144802662208 submission_runner.py:408] Time since start: 11570.59s, 	Step: 18871, 	{'train/accuracy': 0.6545403003692627, 'train/loss': 1.619762897491455, 'train/bleu': 32.29815543479825, 'validation/accuracy': 0.6512380242347717, 'validation/loss': 1.6543937921524048, 'validation/bleu': 27.504371133986982, 'validation/num_examples': 3000, 'test/accuracy': 0.6603451371192932, 'test/loss': 1.5860060453414917, 'test/bleu': 26.534122605706614, 'test/num_examples': 3003, 'score': 6752.3742599487305, 'total_duration': 11570.590962171555, 'accumulated_submission_time': 6752.3742599487305, 'accumulated_eval_time': 4817.370816230774, 'accumulated_logging_time': 0.21682024002075195}
I0214 06:53:08.897068 139975056156416 logging_writer.py:48] [18871] accumulated_eval_time=4817.370816, accumulated_logging_time=0.216820, accumulated_submission_time=6752.374260, global_step=18871, preemption_count=0, score=6752.374260, test/accuracy=0.660345, test/bleu=26.534123, test/loss=1.586006, test/num_examples=3003, total_duration=11570.590962, train/accuracy=0.654540, train/bleu=32.298155, train/loss=1.619763, validation/accuracy=0.651238, validation/bleu=27.504371, validation/loss=1.654394, validation/num_examples=3000
I0214 06:53:19.560346 139975047763712 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.20568011701107025, loss=1.7892082929611206
I0214 06:53:55.111793 139975056156416 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.19560183584690094, loss=1.8096195459365845
I0214 06:54:30.836053 139975047763712 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.2542239725589752, loss=1.8766770362854004
I0214 06:55:06.435134 139975056156416 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.20807966589927673, loss=1.8553109169006348
I0214 06:55:42.072992 139975047763712 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3127308785915375, loss=1.7790062427520752
I0214 06:56:17.692754 139975056156416 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.285238653421402, loss=1.8937060832977295
I0214 06:56:53.332906 139975047763712 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.2756493389606476, loss=1.7796649932861328
I0214 06:57:28.951720 139975056156416 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.19848904013633728, loss=1.9594862461090088
I0214 06:58:04.589941 139975047763712 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.18582794070243835, loss=1.8651283979415894
I0214 06:58:40.252895 139975056156416 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.20540927350521088, loss=1.858135461807251
I0214 06:59:15.902858 139975047763712 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.21003863215446472, loss=1.9102208614349365
I0214 06:59:51.532219 139975056156416 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.20202353596687317, loss=1.8487540483474731
I0214 07:00:27.226585 139975047763712 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.1824304163455963, loss=1.7825136184692383
I0214 07:01:03.006346 139975056156416 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.19149604439735413, loss=1.8059155941009521
I0214 07:01:38.670557 139975047763712 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.2333233803510666, loss=1.8910589218139648
I0214 07:02:14.310693 139975056156416 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.19475844502449036, loss=1.9268977642059326
I0214 07:02:49.984787 139975047763712 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.20131252706050873, loss=1.8334314823150635
I0214 07:03:25.619234 139975056156416 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.20090095698833466, loss=1.7386107444763184
I0214 07:04:01.283478 139975047763712 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.2018539309501648, loss=1.7614102363586426
I0214 07:04:36.936482 139975056156416 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.46194028854370117, loss=1.873502492904663
I0214 07:05:12.617589 139975047763712 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.22844889760017395, loss=1.821155309677124
I0214 07:05:48.269582 139975056156416 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.20889444649219513, loss=1.8347935676574707
I0214 07:06:23.888626 139975047763712 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.18629594147205353, loss=1.8619822263717651
I0214 07:06:59.578412 139975056156416 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.1921454221010208, loss=1.895440697669983
I0214 07:07:08.926331 140144802662208 spec.py:321] Evaluating on the training split.
I0214 07:07:11.902558 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 07:11:02.797988 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 07:11:05.481917 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 07:13:34.336597 140144802662208 spec.py:349] Evaluating on the test split.
I0214 07:13:37.021857 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 07:15:58.866371 140144802662208 submission_runner.py:408] Time since start: 12940.58s, 	Step: 21228, 	{'train/accuracy': 0.6335942149162292, 'train/loss': 1.776694416999817, 'train/bleu': 30.58009748460818, 'validation/accuracy': 0.6522423624992371, 'validation/loss': 1.640693187713623, 'validation/bleu': 27.75300949580246, 'validation/num_examples': 3000, 'test/accuracy': 0.6626227498054504, 'test/loss': 1.5686941146850586, 'test/bleu': 26.675115590587676, 'test/num_examples': 3003, 'score': 7592.311160326004, 'total_duration': 12940.57827091217, 'accumulated_submission_time': 7592.311160326004, 'accumulated_eval_time': 5347.3108031749725, 'accumulated_logging_time': 0.24676942825317383}
I0214 07:15:58.883659 139975047763712 logging_writer.py:48] [21228] accumulated_eval_time=5347.310803, accumulated_logging_time=0.246769, accumulated_submission_time=7592.311160, global_step=21228, preemption_count=0, score=7592.311160, test/accuracy=0.662623, test/bleu=26.675116, test/loss=1.568694, test/num_examples=3003, total_duration=12940.578271, train/accuracy=0.633594, train/bleu=30.580097, train/loss=1.776694, validation/accuracy=0.652242, validation/bleu=27.753009, validation/loss=1.640693, validation/num_examples=3000
I0214 07:16:24.776572 139975056156416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.22432246804237366, loss=1.8865528106689453
I0214 07:17:00.288193 139975047763712 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.31459569931030273, loss=1.8813763856887817
I0214 07:17:35.875648 139975056156416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.21561074256896973, loss=1.8564103841781616
I0214 07:18:11.523256 139975047763712 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.18998652696609497, loss=1.7826423645019531
I0214 07:18:47.153956 139975056156416 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.21394649147987366, loss=1.7443114519119263
I0214 07:19:22.782314 139975047763712 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.21167422831058502, loss=1.8269586563110352
I0214 07:19:58.406918 139975056156416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.1899896264076233, loss=1.905126929283142
I0214 07:20:34.037717 139975047763712 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.20611532032489777, loss=1.9785802364349365
I0214 07:21:09.682523 139975056156416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.2396598607301712, loss=1.9232841730117798
I0214 07:21:45.316971 139975047763712 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.22133280336856842, loss=1.9148902893066406
I0214 07:22:20.933157 139975056156416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.18757504224777222, loss=1.8796075582504272
I0214 07:22:56.533710 139975047763712 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.262024849653244, loss=1.9219636917114258
I0214 07:23:32.188628 139975056156416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.229851633310318, loss=1.8743669986724854
I0214 07:24:07.837620 139975047763712 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.20034785568714142, loss=1.870711088180542
I0214 07:24:43.458754 139975056156416 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.24999099969863892, loss=1.817578911781311
I0214 07:25:19.068156 139975047763712 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.21657924354076385, loss=1.805408239364624
I0214 07:25:54.662027 139975056156416 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.22019509971141815, loss=1.862844705581665
I0214 07:26:30.309921 139975047763712 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.19239607453346252, loss=1.8311513662338257
I0214 07:27:05.920104 139975056156416 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.21068115532398224, loss=1.8183566331863403
I0214 07:27:41.593158 139975047763712 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.2001136988401413, loss=1.8622912168502808
I0214 07:28:17.405398 139975056156416 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.20023944973945618, loss=1.8503661155700684
I0214 07:28:53.102140 139975047763712 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.21330107748508453, loss=1.9021960496902466
I0214 07:29:28.744271 139975056156416 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.196939617395401, loss=1.8018251657485962
I0214 07:29:59.136682 140144802662208 spec.py:321] Evaluating on the training split.
I0214 07:30:02.128632 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 07:34:37.883650 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 07:34:40.565603 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 07:37:28.613136 140144802662208 spec.py:349] Evaluating on the test split.
I0214 07:37:31.303245 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 07:39:45.018537 140144802662208 submission_runner.py:408] Time since start: 14366.73s, 	Step: 23587, 	{'train/accuracy': 0.6337989568710327, 'train/loss': 1.7762765884399414, 'train/bleu': 30.87184258005141, 'validation/accuracy': 0.6548957824707031, 'validation/loss': 1.6313564777374268, 'validation/bleu': 27.84559009417645, 'validation/num_examples': 3000, 'test/accuracy': 0.6640636920928955, 'test/loss': 1.562119483947754, 'test/bleu': 26.70121926683885, 'test/num_examples': 3003, 'score': 8432.473328590393, 'total_duration': 14366.7304251194, 'accumulated_submission_time': 8432.473328590393, 'accumulated_eval_time': 5933.192608118057, 'accumulated_logging_time': 0.27526402473449707}
I0214 07:39:45.037381 139975047763712 logging_writer.py:48] [23587] accumulated_eval_time=5933.192608, accumulated_logging_time=0.275264, accumulated_submission_time=8432.473329, global_step=23587, preemption_count=0, score=8432.473329, test/accuracy=0.664064, test/bleu=26.701219, test/loss=1.562119, test/num_examples=3003, total_duration=14366.730425, train/accuracy=0.633799, train/bleu=30.871843, train/loss=1.776277, validation/accuracy=0.654896, validation/bleu=27.845590, validation/loss=1.631356, validation/num_examples=3000
I0214 07:39:50.018507 139975056156416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.22656168043613434, loss=1.9003037214279175
I0214 07:40:25.519813 139975047763712 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.3306061923503876, loss=1.801974892616272
I0214 07:41:01.057455 139975056156416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.22867777943611145, loss=1.822317123413086
I0214 07:41:36.656909 139975047763712 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.19733716547489166, loss=1.8645321130752563
I0214 07:42:12.273172 139975056156416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2527802288532257, loss=1.8367177248001099
I0214 07:42:47.948408 139975047763712 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.2003951072692871, loss=1.8366451263427734
I0214 07:43:23.570508 139975056156416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.33024168014526367, loss=1.8289201259613037
I0214 07:43:59.198065 139975047763712 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.20330366492271423, loss=1.8534681797027588
I0214 07:44:34.928939 139975056156416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.30720534920692444, loss=1.7697464227676392
I0214 07:45:10.623753 139975047763712 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.24613049626350403, loss=1.785558819770813
I0214 07:45:46.287010 139975056156416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.22799555957317352, loss=1.8783233165740967
I0214 07:46:21.949116 139975047763712 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.2620481848716736, loss=1.7898070812225342
I0214 07:46:57.556035 139975056156416 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.20140792429447174, loss=1.8012686967849731
I0214 07:47:33.186722 139975047763712 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.22865410149097443, loss=1.8735392093658447
I0214 07:48:08.792318 139975056156416 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.20941193401813507, loss=1.7383172512054443
I0214 07:48:44.430735 139975047763712 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2028091698884964, loss=1.8624775409698486
I0214 07:49:20.067767 139975056156416 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2227589339017868, loss=1.897342324256897
I0214 07:49:55.680079 139975047763712 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.21048258244991302, loss=1.7876026630401611
I0214 07:50:31.332921 139975056156416 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.18632110953330994, loss=1.7866497039794922
I0214 07:51:06.952805 139975047763712 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.20036818087100983, loss=1.7646034955978394
I0214 07:51:42.588077 139975056156416 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.24197500944137573, loss=1.8005201816558838
I0214 07:52:18.209449 139975047763712 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.21009612083435059, loss=1.79603910446167
I0214 07:52:53.858331 139975056156416 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.2532353103160858, loss=1.8724461793899536
I0214 07:53:29.466050 139975047763712 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.2932069003582001, loss=1.741722822189331
I0214 07:53:45.234091 140144802662208 spec.py:321] Evaluating on the training split.
I0214 07:53:48.198447 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 07:56:51.579860 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 07:56:54.260390 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 07:59:25.119123 140144802662208 spec.py:349] Evaluating on the test split.
I0214 07:59:27.784327 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 08:01:52.012873 140144802662208 submission_runner.py:408] Time since start: 15693.72s, 	Step: 25946, 	{'train/accuracy': 0.6462305188179016, 'train/loss': 1.6862976551055908, 'train/bleu': 31.452835525252226, 'validation/accuracy': 0.6573259830474854, 'validation/loss': 1.6084153652191162, 'validation/bleu': 27.836506053367742, 'validation/num_examples': 3000, 'test/accuracy': 0.6673174500465393, 'test/loss': 1.539101481437683, 'test/bleu': 27.519325136360557, 'test/num_examples': 3003, 'score': 9272.582363128662, 'total_duration': 15693.724759817123, 'accumulated_submission_time': 9272.582363128662, 'accumulated_eval_time': 6419.971329689026, 'accumulated_logging_time': 0.303957462310791}
I0214 08:01:52.032248 139975056156416 logging_writer.py:48] [25946] accumulated_eval_time=6419.971330, accumulated_logging_time=0.303957, accumulated_submission_time=9272.582363, global_step=25946, preemption_count=0, score=9272.582363, test/accuracy=0.667317, test/bleu=27.519325, test/loss=1.539101, test/num_examples=3003, total_duration=15693.724760, train/accuracy=0.646231, train/bleu=31.452836, train/loss=1.686298, validation/accuracy=0.657326, validation/bleu=27.836506, validation/loss=1.608415, validation/num_examples=3000
I0214 08:02:11.547421 139975047763712 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2210962474346161, loss=1.8341699838638306
I0214 08:02:47.043807 139975056156416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.23932074010372162, loss=1.8580329418182373
I0214 08:03:22.661691 139975047763712 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.18466998636722565, loss=1.855548620223999
I0214 08:03:58.311883 139975056156416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.23819896578788757, loss=1.8243733644485474
I0214 08:04:33.966281 139975047763712 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.2320522516965866, loss=1.816192865371704
I0214 08:05:09.584530 139975056156416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.21470054984092712, loss=1.8351683616638184
I0214 08:05:45.246687 139975047763712 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.19882410764694214, loss=1.8242563009262085
I0214 08:06:20.870025 139975056156416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3305434286594391, loss=1.7601248025894165
I0214 08:06:56.555400 139975047763712 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.19930383563041687, loss=1.8236171007156372
I0214 08:07:32.220429 139975056156416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2126341611146927, loss=1.8311491012573242
I0214 08:08:07.838436 139975047763712 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.2256009429693222, loss=1.8328008651733398
I0214 08:08:43.493819 139975056156416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.26747530698776245, loss=1.7727909088134766
I0214 08:09:19.120196 139975047763712 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.21926642954349518, loss=1.8424100875854492
I0214 08:09:54.783742 139975056156416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.20324809849262238, loss=1.8433908224105835
I0214 08:10:30.450838 139975047763712 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.20144720375537872, loss=1.7996636629104614
I0214 08:11:06.107312 139975056156416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.21231020987033844, loss=1.8974539041519165
I0214 08:11:41.762616 139975047763712 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.18848159909248352, loss=1.8405870199203491
I0214 08:12:17.392549 139975056156416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.19810962677001953, loss=1.8836941719055176
I0214 08:12:53.052270 139975047763712 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.2336890548467636, loss=1.8073841333389282
I0214 08:13:28.715656 139975056156416 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.2493695169687271, loss=1.8354297876358032
I0214 08:14:04.371518 139975047763712 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.22478646039962769, loss=1.8607475757598877
I0214 08:14:40.090252 139975056156416 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.29215744137763977, loss=1.849096417427063
I0214 08:15:15.771494 139975047763712 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.23079338669776917, loss=1.7487926483154297
I0214 08:15:51.455400 139975056156416 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.19591058790683746, loss=1.7512725591659546
I0214 08:15:52.256734 140144802662208 spec.py:321] Evaluating on the training split.
I0214 08:15:55.246060 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 08:18:39.021246 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 08:18:41.722938 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 08:21:05.502006 140144802662208 spec.py:349] Evaluating on the test split.
I0214 08:21:08.223677 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 08:23:33.514324 140144802662208 submission_runner.py:408] Time since start: 16995.23s, 	Step: 28304, 	{'train/accuracy': 0.6390774250030518, 'train/loss': 1.7401479482650757, 'train/bleu': 31.344862714431464, 'validation/accuracy': 0.6581319570541382, 'validation/loss': 1.6097475290298462, 'validation/bleu': 28.05053729965437, 'validation/num_examples': 3000, 'test/accuracy': 0.6687235236167908, 'test/loss': 1.5342345237731934, 'test/bleu': 27.162785054412023, 'test/num_examples': 3003, 'score': 10112.716992616653, 'total_duration': 16995.22621178627, 'accumulated_submission_time': 10112.716992616653, 'accumulated_eval_time': 6881.22886633873, 'accumulated_logging_time': 0.3334319591522217}
I0214 08:23:33.533298 139975047763712 logging_writer.py:48] [28304] accumulated_eval_time=6881.228866, accumulated_logging_time=0.333432, accumulated_submission_time=10112.716993, global_step=28304, preemption_count=0, score=10112.716993, test/accuracy=0.668724, test/bleu=27.162785, test/loss=1.534235, test/num_examples=3003, total_duration=16995.226212, train/accuracy=0.639077, train/bleu=31.344863, train/loss=1.740148, validation/accuracy=0.658132, validation/bleu=28.050537, validation/loss=1.609748, validation/num_examples=3000
I0214 08:24:07.960489 139975056156416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.23018817603588104, loss=1.804750919342041
I0214 08:24:43.516319 139975047763712 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.2547300159931183, loss=1.8307397365570068
I0214 08:25:19.135256 139975056156416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.21905601024627686, loss=1.7229729890823364
I0214 08:25:54.731377 139975047763712 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.25086358189582825, loss=1.8440124988555908
I0214 08:26:30.338362 139975056156416 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1804346889257431, loss=1.7966285943984985
I0214 08:27:06.018629 139975047763712 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.21319058537483215, loss=1.8536854982376099
I0214 08:27:41.687422 139975056156416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.24765293300151825, loss=1.8624680042266846
I0214 08:28:17.355217 139975047763712 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.20258834958076477, loss=1.8064532279968262
I0214 08:28:52.998035 139975056156416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.20004402101039886, loss=1.825122356414795
I0214 08:29:28.589928 139975047763712 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.2112474888563156, loss=1.8491288423538208
I0214 08:30:04.192332 139975056156416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.17882156372070312, loss=1.82645845413208
I0214 08:30:39.822252 139975047763712 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2615625262260437, loss=1.7051351070404053
I0214 08:31:15.480610 139975056156416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.2220490723848343, loss=1.7483634948730469
I0214 08:31:51.177199 139975047763712 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.28862786293029785, loss=1.8827922344207764
I0214 08:32:26.864156 139975056156416 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.238659620285034, loss=1.8267099857330322
I0214 08:33:02.506261 139975047763712 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.1900317370891571, loss=1.8137959241867065
I0214 08:33:38.140224 139975056156416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.2317419797182083, loss=1.79843270778656
I0214 08:34:13.807632 139975047763712 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.20404894649982452, loss=1.8105939626693726
I0214 08:34:49.452144 139975056156416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.2382887452840805, loss=1.8163585662841797
I0214 08:35:25.092471 139975047763712 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.21125611662864685, loss=1.770067811012268
I0214 08:36:00.705486 139975056156416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.2048531323671341, loss=1.7796440124511719
I0214 08:36:36.372548 139975047763712 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.22330321371555328, loss=1.7821085453033447
I0214 08:37:12.041446 139975056156416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.254580557346344, loss=1.8244590759277344
I0214 08:37:33.534013 140144802662208 spec.py:321] Evaluating on the training split.
I0214 08:37:36.513972 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 08:41:08.659427 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 08:41:11.356388 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 08:43:37.170136 140144802662208 spec.py:349] Evaluating on the test split.
I0214 08:43:39.876240 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 08:46:01.577552 140144802662208 submission_runner.py:408] Time since start: 18343.29s, 	Step: 30662, 	{'train/accuracy': 0.63572096824646, 'train/loss': 1.763773798942566, 'train/bleu': 31.040252556013616, 'validation/accuracy': 0.6615169048309326, 'validation/loss': 1.5909042358398438, 'validation/bleu': 28.108661415052204, 'validation/num_examples': 3000, 'test/accuracy': 0.6705711483955383, 'test/loss': 1.5236083269119263, 'test/bleu': 27.320115273429533, 'test/num_examples': 3003, 'score': 10952.624883651733, 'total_duration': 18343.28938150406, 'accumulated_submission_time': 10952.624883651733, 'accumulated_eval_time': 7389.272298574448, 'accumulated_logging_time': 0.3637864589691162}
I0214 08:46:01.608324 139975047763712 logging_writer.py:48] [30662] accumulated_eval_time=7389.272299, accumulated_logging_time=0.363786, accumulated_submission_time=10952.624884, global_step=30662, preemption_count=0, score=10952.624884, test/accuracy=0.670571, test/bleu=27.320115, test/loss=1.523608, test/num_examples=3003, total_duration=18343.289382, train/accuracy=0.635721, train/bleu=31.040253, train/loss=1.763774, validation/accuracy=0.661517, validation/bleu=28.108661, validation/loss=1.590904, validation/num_examples=3000
I0214 08:46:15.488089 139975056156416 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.20937471091747284, loss=1.7943017482757568
I0214 08:46:50.994335 139975047763712 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.22285373508930206, loss=1.7556328773498535
I0214 08:47:26.586533 139975056156416 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.2022542506456375, loss=1.7628337144851685
I0214 08:48:02.216079 139975047763712 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.18818463385105133, loss=1.7290583848953247
I0214 08:48:37.841062 139975056156416 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.19382694363594055, loss=1.7869588136672974
I0214 08:49:13.471342 139975047763712 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.2099892497062683, loss=1.838310718536377
I0214 08:49:49.112580 139975056156416 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.21450680494308472, loss=1.8193618059158325
I0214 08:50:24.756381 139975047763712 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.26166367530822754, loss=1.842669129371643
I0214 08:51:00.348235 139975056156416 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.22231446206569672, loss=1.825907588005066
I0214 08:51:36.009213 139975047763712 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.126169443130493, loss=1.8406516313552856
I0214 08:52:11.672383 139975056156416 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.2802938222885132, loss=1.8709502220153809
I0214 08:52:47.472210 139975047763712 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.2173435389995575, loss=1.8529947996139526
I0214 08:53:23.119816 139975056156416 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.24886777997016907, loss=1.8516407012939453
I0214 08:53:58.776716 139975047763712 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.19723254442214966, loss=1.8246818780899048
I0214 08:54:34.447493 139975056156416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.24339072406291962, loss=1.8232227563858032
I0214 08:55:10.109139 139975047763712 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.24348033964633942, loss=1.8027422428131104
I0214 08:55:45.748801 139975056156416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.2044266313314438, loss=1.7124698162078857
I0214 08:56:21.368544 139975047763712 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.21598295867443085, loss=1.8392776250839233
I0214 08:56:56.983249 139975056156416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.2181740403175354, loss=1.6577059030532837
I0214 08:57:32.625668 139975047763712 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.2027130275964737, loss=1.8093398809432983
I0214 08:58:08.276308 139975056156416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.21148891746997833, loss=1.832420825958252
I0214 08:58:43.949069 139975047763712 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.22771283984184265, loss=1.903938889503479
I0214 08:59:19.564026 139975056156416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.22917623817920685, loss=1.772478461265564
I0214 08:59:55.210347 139975047763712 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.22347129881381989, loss=1.722287654876709
I0214 09:00:01.706379 140144802662208 spec.py:321] Evaluating on the training split.
I0214 09:00:04.679449 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 09:03:53.461904 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 09:03:56.163885 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 09:06:38.295558 140144802662208 spec.py:349] Evaluating on the test split.
I0214 09:06:41.006608 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 09:09:26.027532 140144802662208 submission_runner.py:408] Time since start: 19747.74s, 	Step: 33020, 	{'train/accuracy': 0.6443125009536743, 'train/loss': 1.6968934535980225, 'train/bleu': 31.166148767869927, 'validation/accuracy': 0.6595454216003418, 'validation/loss': 1.5900745391845703, 'validation/bleu': 27.80920383950556, 'validation/num_examples': 3000, 'test/accuracy': 0.6711289286613464, 'test/loss': 1.5155937671661377, 'test/bleu': 27.683319616547642, 'test/num_examples': 3003, 'score': 11792.630442142487, 'total_duration': 19747.73938369751, 'accumulated_submission_time': 11792.630442142487, 'accumulated_eval_time': 7953.593356847763, 'accumulated_logging_time': 0.40709567070007324}
I0214 09:09:26.051923 139975056156416 logging_writer.py:48] [33020] accumulated_eval_time=7953.593357, accumulated_logging_time=0.407096, accumulated_submission_time=11792.630442, global_step=33020, preemption_count=0, score=11792.630442, test/accuracy=0.671129, test/bleu=27.683320, test/loss=1.515594, test/num_examples=3003, total_duration=19747.739384, train/accuracy=0.644313, train/bleu=31.166149, train/loss=1.696893, validation/accuracy=0.659545, validation/bleu=27.809204, validation/loss=1.590075, validation/num_examples=3000
I0214 09:09:54.808897 139975047763712 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.19429560005664825, loss=1.8175384998321533
I0214 09:10:30.393228 139975056156416 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.22374394536018372, loss=1.8136247396469116
I0214 09:11:06.019851 139975047763712 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.21976856887340546, loss=1.7019691467285156
I0214 09:11:41.637782 139975056156416 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.20160150527954102, loss=1.8290445804595947
I0214 09:12:17.302085 139975047763712 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.21493439376354218, loss=1.778467059135437
I0214 09:12:52.944138 139975056156416 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.20653730630874634, loss=1.8526439666748047
I0214 09:13:28.555354 139975047763712 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.20750892162323, loss=1.919468879699707
I0214 09:14:04.172877 139975056156416 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.23562473058700562, loss=1.7931140661239624
I0214 09:14:39.849739 139975047763712 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.2925284802913666, loss=1.8264328241348267
I0214 09:15:15.514278 139975056156416 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5170896053314209, loss=1.7889559268951416
I0214 09:15:51.137437 139975047763712 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.18628859519958496, loss=1.8554643392562866
I0214 09:16:26.802642 139975056156416 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.21034568548202515, loss=1.799195408821106
I0214 09:17:02.472938 139975047763712 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.2041374146938324, loss=1.8079516887664795
I0214 09:17:38.146317 139975056156416 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.19109933078289032, loss=1.7477600574493408
I0214 09:18:13.807900 139975047763712 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.24961000680923462, loss=1.7826558351516724
I0214 09:18:49.486407 139975056156416 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.19621606171131134, loss=1.7952547073364258
I0214 09:19:25.119087 139975047763712 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.19106972217559814, loss=1.7744508981704712
I0214 09:20:00.730768 139975056156416 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.28059983253479004, loss=1.6719000339508057
I0214 09:20:36.369321 139975047763712 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.19412052631378174, loss=1.7789404392242432
I0214 09:21:12.016812 139975056156416 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.19203384220600128, loss=1.7347506284713745
I0214 09:21:47.632755 139975047763712 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2012125700712204, loss=1.7799557447433472
I0214 09:22:23.263222 139975056156416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.21933525800704956, loss=1.782815933227539
I0214 09:22:58.918503 139975047763712 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.21040946245193481, loss=1.7124171257019043
I0214 09:23:26.059435 140144802662208 spec.py:321] Evaluating on the training split.
I0214 09:23:29.027834 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 09:27:31.993171 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 09:27:34.659038 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 09:31:39.999886 140144802662208 spec.py:349] Evaluating on the test split.
I0214 09:31:42.679682 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 09:35:57.664117 140144802662208 submission_runner.py:408] Time since start: 21339.38s, 	Step: 35378, 	{'train/accuracy': 0.6399866938591003, 'train/loss': 1.7375435829162598, 'train/bleu': 31.338208754834255, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.5832661390304565, 'validation/bleu': 28.38960980084261, 'validation/num_examples': 3000, 'test/accuracy': 0.6730231046676636, 'test/loss': 1.5036847591400146, 'test/bleu': 27.782769305202045, 'test/num_examples': 3003, 'score': 12632.544389486313, 'total_duration': 21339.37600016594, 'accumulated_submission_time': 12632.544389486313, 'accumulated_eval_time': 8705.197973966599, 'accumulated_logging_time': 0.4429934024810791}
I0214 09:35:57.683711 139975056156416 logging_writer.py:48] [35378] accumulated_eval_time=8705.197974, accumulated_logging_time=0.442993, accumulated_submission_time=12632.544389, global_step=35378, preemption_count=0, score=12632.544389, test/accuracy=0.673023, test/bleu=27.782769, test/loss=1.503685, test/num_examples=3003, total_duration=21339.376000, train/accuracy=0.639987, train/bleu=31.338209, train/loss=1.737544, validation/accuracy=0.662062, validation/bleu=28.389610, validation/loss=1.583266, validation/num_examples=3000
I0214 09:36:05.846307 139975047763712 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.22539596259593964, loss=1.812599539756775
I0214 09:36:41.304474 139975056156416 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2493351697921753, loss=1.8236998319625854
I0214 09:37:16.852045 139975047763712 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.24923476576805115, loss=1.782592535018921
I0214 09:37:52.469073 139975056156416 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.18886366486549377, loss=1.7032036781311035
I0214 09:38:28.079052 139975047763712 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.21980659663677216, loss=1.8260892629623413
I0214 09:39:03.712569 139975056156416 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.26293569803237915, loss=1.855433464050293
I0214 09:39:39.400271 139975047763712 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.25322583317756653, loss=1.7323660850524902
I0214 09:40:15.081185 139975056156416 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.21830205619335175, loss=1.7596936225891113
I0214 09:40:50.714307 139975047763712 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.2031516283750534, loss=1.769375205039978
I0214 09:41:26.392851 139975056156416 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.22245487570762634, loss=1.7545839548110962
I0214 09:42:02.052588 139975047763712 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.22687330842018127, loss=1.755544900894165
I0214 09:42:37.677598 139975056156416 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.21350409090518951, loss=1.7308518886566162
I0214 09:43:13.310176 139975047763712 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.20174041390419006, loss=1.7179044485092163
I0214 09:43:48.988220 139975056156416 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.19097450375556946, loss=1.7133101224899292
I0214 09:44:24.638450 139975047763712 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.2466472089290619, loss=1.7906389236450195
I0214 09:45:00.361633 139975056156416 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.19220100343227386, loss=1.8492391109466553
I0214 09:45:35.991791 139975047763712 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.1946864128112793, loss=1.745984673500061
I0214 09:46:11.631160 139975056156416 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.2236693650484085, loss=1.8037300109863281
I0214 09:46:47.301548 139975047763712 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.2283088117837906, loss=1.7561023235321045
I0214 09:47:23.011540 139975056156416 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.20481756329536438, loss=1.8328523635864258
I0214 09:47:58.680736 139975047763712 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.23007503151893616, loss=1.7623980045318604
I0214 09:48:34.330268 139975056156416 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.24135364592075348, loss=1.7668970823287964
I0214 09:49:10.040727 139975047763712 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.24974581599235535, loss=1.7879395484924316
I0214 09:49:45.683615 139975056156416 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.2845970392227173, loss=1.713923454284668
I0214 09:49:57.871417 140144802662208 spec.py:321] Evaluating on the training split.
I0214 09:50:00.840565 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 09:54:13.922378 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 09:54:16.593709 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 09:57:06.085937 140144802662208 spec.py:349] Evaluating on the test split.
I0214 09:57:08.762493 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 09:59:47.932012 140144802662208 submission_runner.py:408] Time since start: 22769.64s, 	Step: 37736, 	{'train/accuracy': 0.663627564907074, 'train/loss': 1.5535968542099, 'train/bleu': 32.87796516096618, 'validation/accuracy': 0.6638479232788086, 'validation/loss': 1.5714770555496216, 'validation/bleu': 28.38134980475151, 'validation/num_examples': 3000, 'test/accuracy': 0.6728371381759644, 'test/loss': 1.4999555349349976, 'test/bleu': 27.78853257605112, 'test/num_examples': 3003, 'score': 13472.639306306839, 'total_duration': 22769.643906354904, 'accumulated_submission_time': 13472.639306306839, 'accumulated_eval_time': 9295.258509159088, 'accumulated_logging_time': 0.47411179542541504}
I0214 09:59:47.952538 139975047763712 logging_writer.py:48] [37736] accumulated_eval_time=9295.258509, accumulated_logging_time=0.474112, accumulated_submission_time=13472.639306, global_step=37736, preemption_count=0, score=13472.639306, test/accuracy=0.672837, test/bleu=27.788533, test/loss=1.499956, test/num_examples=3003, total_duration=22769.643906, train/accuracy=0.663628, train/bleu=32.877965, train/loss=1.553597, validation/accuracy=0.663848, validation/bleu=28.381350, validation/loss=1.571477, validation/num_examples=3000
I0214 10:00:11.021963 139975056156416 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.17645686864852905, loss=1.705539584159851
I0214 10:00:46.568393 139975047763712 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.24072934687137604, loss=1.869065523147583
I0214 10:01:22.195365 139975056156416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.22862011194229126, loss=1.7641550302505493
I0214 10:01:57.858077 139975047763712 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.17921936511993408, loss=1.7936931848526
I0214 10:02:33.504147 139975056156416 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.2632918655872345, loss=1.7065937519073486
I0214 10:03:09.197817 139975047763712 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.22598710656166077, loss=1.7522969245910645
I0214 10:03:44.842742 139975056156416 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.20050011575222015, loss=1.8065781593322754
I0214 10:04:20.507101 139975047763712 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.24894826114177704, loss=1.7582931518554688
I0214 10:04:56.163454 139975056156416 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.1874479204416275, loss=1.708314061164856
I0214 10:05:31.793282 139975047763712 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.22008268535137177, loss=1.7369507551193237
I0214 10:06:07.447203 139975056156416 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.21481221914291382, loss=1.8041930198669434
I0214 10:06:43.047606 139975047763712 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.19849717617034912, loss=1.7489794492721558
I0214 10:07:18.703942 139975056156416 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.23435188829898834, loss=1.7816057205200195
I0214 10:07:54.358163 139975047763712 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.20502759516239166, loss=1.7298967838287354
I0214 10:08:29.998008 139975056156416 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.20220151543617249, loss=1.802685260772705
I0214 10:09:05.646903 139975047763712 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.20307287573814392, loss=1.7138099670410156
I0214 10:09:41.334929 139975056156416 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.2340662032365799, loss=1.7376840114593506
I0214 10:10:16.957983 139975047763712 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.19731542468070984, loss=1.7857003211975098
I0214 10:10:52.566624 139975056156416 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.20855270326137543, loss=1.7462270259857178
I0214 10:11:28.191351 139975047763712 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.1901911497116089, loss=1.8267282247543335
I0214 10:12:03.833652 139975056156416 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.20474103093147278, loss=1.8138713836669922
I0214 10:12:39.470315 139975047763712 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.3143906593322754, loss=1.7928615808486938
I0214 10:13:15.148559 139975056156416 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.2321934551000595, loss=1.7550442218780518
I0214 10:13:48.031054 140144802662208 spec.py:321] Evaluating on the training split.
I0214 10:13:51.004628 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 10:18:21.345485 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 10:18:24.026927 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 10:22:08.977847 140144802662208 spec.py:349] Evaluating on the test split.
I0214 10:22:11.667956 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 10:26:14.407773 140144802662208 submission_runner.py:408] Time since start: 24356.12s, 	Step: 40094, 	{'train/accuracy': 0.6476908922195435, 'train/loss': 1.6861525774002075, 'train/bleu': 31.62556446241438, 'validation/accuracy': 0.6630792021751404, 'validation/loss': 1.5618013143539429, 'validation/bleu': 28.31053959711827, 'validation/num_examples': 3000, 'test/accuracy': 0.6751612424850464, 'test/loss': 1.4886205196380615, 'test/bleu': 28.055212927233068, 'test/num_examples': 3003, 'score': 14312.627776622772, 'total_duration': 24356.119666576385, 'accumulated_submission_time': 14312.627776622772, 'accumulated_eval_time': 10041.635178804398, 'accumulated_logging_time': 0.5051698684692383}
I0214 10:26:14.428824 139975047763712 logging_writer.py:48] [40094] accumulated_eval_time=10041.635179, accumulated_logging_time=0.505170, accumulated_submission_time=14312.627777, global_step=40094, preemption_count=0, score=14312.627777, test/accuracy=0.675161, test/bleu=28.055213, test/loss=1.488621, test/num_examples=3003, total_duration=24356.119667, train/accuracy=0.647691, train/bleu=31.625564, train/loss=1.686153, validation/accuracy=0.663079, validation/bleu=28.310540, validation/loss=1.561801, validation/num_examples=3000
I0214 10:26:16.927377 139975056156416 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.2200014889240265, loss=1.736626386642456
I0214 10:26:52.404815 139975047763712 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.19804145395755768, loss=1.810036063194275
I0214 10:27:27.983773 139975056156416 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.20157895982265472, loss=1.7171987295150757
I0214 10:28:03.670540 139975047763712 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.21466009318828583, loss=1.7806167602539062
I0214 10:28:39.306797 139975056156416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.3015381395816803, loss=1.8085840940475464
I0214 10:29:15.014485 139975047763712 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.1916385442018509, loss=1.7488915920257568
I0214 10:29:50.667823 139975056156416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.19013512134552002, loss=1.7516565322875977
I0214 10:30:26.355532 139975047763712 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.20435969531536102, loss=1.7908565998077393
I0214 10:31:02.049311 139975056156416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.41494742035865784, loss=1.8494938611984253
I0214 10:31:37.694116 139975047763712 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.27740439772605896, loss=1.7684893608093262
I0214 10:32:13.385497 139975056156416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.207193985581398, loss=1.7592543363571167
I0214 10:32:48.986542 139975047763712 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.23210032284259796, loss=1.792083501815796
I0214 10:33:24.632728 139975056156416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.1877046823501587, loss=1.8019546270370483
I0214 10:34:00.306679 139975047763712 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.19466297328472137, loss=1.7853233814239502
I0214 10:34:35.966962 139975056156416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.24430447816848755, loss=1.709288477897644
I0214 10:35:11.618725 139975047763712 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.19300466775894165, loss=1.8723702430725098
I0214 10:35:47.261233 139975056156416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.25540316104888916, loss=1.7587900161743164
I0214 10:36:22.901998 139975047763712 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.237201526761055, loss=1.7918943166732788
I0214 10:36:58.533434 139975056156416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.20255547761917114, loss=1.7389754056930542
I0214 10:37:34.167274 139975047763712 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2750185430049896, loss=1.7600971460342407
I0214 10:38:09.831786 139975056156416 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.20147787034511566, loss=1.7184512615203857
I0214 10:38:45.450594 139975047763712 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.21611501276493073, loss=1.7222387790679932
I0214 10:39:21.108635 139975056156416 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.22905948758125305, loss=1.7170931100845337
I0214 10:39:56.726753 139975047763712 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.3329661190509796, loss=1.748657464981079
I0214 10:40:14.628464 140144802662208 spec.py:321] Evaluating on the training split.
I0214 10:40:17.589376 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 10:44:34.482855 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 10:44:37.148219 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 10:47:05.165398 140144802662208 spec.py:349] Evaluating on the test split.
I0214 10:47:07.854946 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 10:49:35.343423 140144802662208 submission_runner.py:408] Time since start: 25757.06s, 	Step: 42452, 	{'train/accuracy': 0.6445221900939941, 'train/loss': 1.700629711151123, 'train/bleu': 31.200524801237783, 'validation/accuracy': 0.6668733358383179, 'validation/loss': 1.5635435581207275, 'validation/bleu': 28.737636815149113, 'validation/num_examples': 3000, 'test/accuracy': 0.6750218272209167, 'test/loss': 1.486008644104004, 'test/bleu': 27.51980545574519, 'test/num_examples': 3003, 'score': 15152.739064216614, 'total_duration': 25757.05531859398, 'accumulated_submission_time': 15152.739064216614, 'accumulated_eval_time': 10602.350082874298, 'accumulated_logging_time': 0.5357956886291504}
I0214 10:49:35.364521 139975056156416 logging_writer.py:48] [42452] accumulated_eval_time=10602.350083, accumulated_logging_time=0.535796, accumulated_submission_time=15152.739064, global_step=42452, preemption_count=0, score=15152.739064, test/accuracy=0.675022, test/bleu=27.519805, test/loss=1.486009, test/num_examples=3003, total_duration=25757.055319, train/accuracy=0.644522, train/bleu=31.200525, train/loss=1.700630, validation/accuracy=0.666873, validation/bleu=28.737637, validation/loss=1.563544, validation/num_examples=3000
I0214 10:49:52.736917 139975047763712 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.1887231469154358, loss=1.7655997276306152
I0214 10:50:28.244845 139975056156416 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.23648081719875336, loss=1.7922354936599731
I0214 10:51:03.835640 139975047763712 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.23703135550022125, loss=1.7996715307235718
I0214 10:51:39.494622 139975056156416 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.0696356296539307, loss=1.735300064086914
I0214 10:52:15.154073 139975047763712 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.20900437235832214, loss=1.748729944229126
I0214 10:52:50.760367 139975056156416 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.20336413383483887, loss=1.7239447832107544
I0214 10:53:26.395894 139975047763712 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2043723464012146, loss=1.7605929374694824
I0214 10:54:02.024282 139975056156416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.19159473478794098, loss=1.738870620727539
I0214 10:54:37.657880 139975047763712 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.2091989517211914, loss=1.8519318103790283
I0214 10:55:13.318197 139975056156416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.23280982673168182, loss=1.7411599159240723
I0214 10:55:48.921967 139975047763712 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.19341956079006195, loss=1.6903011798858643
I0214 10:56:24.556512 139975056156416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.23095828294754028, loss=1.7396680116653442
I0214 10:57:00.155487 139975047763712 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.32326382398605347, loss=1.795998454093933
I0214 10:57:35.790094 139975056156416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.19974006712436676, loss=1.8812898397445679
I0214 10:58:11.411721 139975047763712 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.19973498582839966, loss=1.7644052505493164
I0214 10:58:47.045862 139975056156416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.18902750313282013, loss=1.7569580078125
I0214 10:59:22.655686 139975047763712 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.20754402875900269, loss=1.8013827800750732
I0214 10:59:58.270114 139975056156416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.21717730164527893, loss=1.77219820022583
I0214 11:00:33.890043 139975047763712 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.20368161797523499, loss=1.7985947132110596
I0214 11:01:09.503706 139975056156416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.19932012259960175, loss=1.6602104902267456
I0214 11:01:45.164139 139975047763712 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.19676874577999115, loss=1.7707113027572632
I0214 11:02:20.791403 139975056156416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.19444309175014496, loss=1.654356837272644
I0214 11:02:56.424175 139975047763712 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.20377124845981598, loss=1.770708441734314
I0214 11:03:32.067625 139975056156416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.20687070488929749, loss=1.7484499216079712
I0214 11:03:35.359166 140144802662208 spec.py:321] Evaluating on the training split.
I0214 11:03:38.326422 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 11:07:24.095106 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 11:07:26.762689 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 11:10:10.168323 140144802662208 spec.py:349] Evaluating on the test split.
I0214 11:10:12.850955 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 11:12:51.338996 140144802662208 submission_runner.py:408] Time since start: 27153.05s, 	Step: 44811, 	{'train/accuracy': 0.6530870795249939, 'train/loss': 1.634215235710144, 'train/bleu': 31.688902445897607, 'validation/accuracy': 0.6673692464828491, 'validation/loss': 1.5494558811187744, 'validation/bleu': 28.504146051050036, 'validation/num_examples': 3000, 'test/accuracy': 0.6767880916595459, 'test/loss': 1.4711213111877441, 'test/bleu': 27.938705764575598, 'test/num_examples': 3003, 'score': 15992.64580154419, 'total_duration': 27153.050884485245, 'accumulated_submission_time': 15992.64580154419, 'accumulated_eval_time': 11158.329864501953, 'accumulated_logging_time': 0.566993236541748}
I0214 11:12:51.360504 139975047763712 logging_writer.py:48] [44811] accumulated_eval_time=11158.329865, accumulated_logging_time=0.566993, accumulated_submission_time=15992.645802, global_step=44811, preemption_count=0, score=15992.645802, test/accuracy=0.676788, test/bleu=27.938706, test/loss=1.471121, test/num_examples=3003, total_duration=27153.050884, train/accuracy=0.653087, train/bleu=31.688902, train/loss=1.634215, validation/accuracy=0.667369, validation/bleu=28.504146, validation/loss=1.549456, validation/num_examples=3000
I0214 11:13:23.299600 139975056156416 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.1888156533241272, loss=1.8869695663452148
I0214 11:13:58.854804 139975047763712 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.23615479469299316, loss=1.7292529344558716
I0214 11:14:34.464814 139975056156416 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.19419607520103455, loss=1.7436047792434692
I0214 11:15:10.115840 139975047763712 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.20245614647865295, loss=1.6982934474945068
I0214 11:15:45.758403 139975056156416 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.20092611014842987, loss=1.7216180562973022
I0214 11:16:21.446663 139975047763712 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.21171465516090393, loss=1.8193717002868652
I0214 11:16:57.098849 139975056156416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.24655765295028687, loss=1.7945778369903564
I0214 11:17:32.707819 139975047763712 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.2081434428691864, loss=1.7117347717285156
I0214 11:18:08.315711 139975056156416 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.200489342212677, loss=1.7103383541107178
I0214 11:18:43.930425 139975047763712 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.21159647405147552, loss=1.6433322429656982
I0214 11:19:19.551272 139975056156416 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.19191409647464752, loss=1.750227928161621
I0214 11:19:55.156412 139975047763712 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.18315759301185608, loss=1.809946894645691
I0214 11:20:30.761359 139975056156416 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.20891711115837097, loss=1.704194188117981
I0214 11:21:06.360889 139975047763712 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.21627560257911682, loss=1.7023718357086182
I0214 11:21:42.005986 139975056156416 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.19748584926128387, loss=1.796828031539917
I0214 11:22:17.673526 139975047763712 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.19023947417736053, loss=1.7308064699172974
I0214 11:22:53.350783 139975056156416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.21147161722183228, loss=1.7555533647537231
I0214 11:23:29.087556 139975047763712 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.20112504065036774, loss=1.7545994520187378
I0214 11:24:04.708398 139975056156416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.19448499381542206, loss=1.728243112564087
I0214 11:24:40.360899 139975047763712 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2063094675540924, loss=1.7846633195877075
I0214 11:25:15.996965 139975056156416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.23857548832893372, loss=1.7598493099212646
I0214 11:25:51.640591 139975047763712 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.1966136246919632, loss=1.7134336233139038
I0214 11:26:27.266294 139975056156416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.20250137150287628, loss=1.6983565092086792
I0214 11:26:51.599051 140144802662208 spec.py:321] Evaluating on the training split.
I0214 11:26:54.578155 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 11:29:49.122779 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 11:29:51.792125 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 11:32:26.223101 140144802662208 spec.py:349] Evaluating on the test split.
I0214 11:32:28.900952 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 11:34:56.863383 140144802662208 submission_runner.py:408] Time since start: 28478.58s, 	Step: 47170, 	{'train/accuracy': 0.6481767892837524, 'train/loss': 1.6744226217269897, 'train/bleu': 31.457149841710663, 'validation/accuracy': 0.668088436126709, 'validation/loss': 1.545769214630127, 'validation/bleu': 28.76677074342338, 'validation/num_examples': 3000, 'test/accuracy': 0.6807855367660522, 'test/loss': 1.45988130569458, 'test/bleu': 28.134571784728966, 'test/num_examples': 3003, 'score': 16832.79411482811, 'total_duration': 28478.575281381607, 'accumulated_submission_time': 16832.79411482811, 'accumulated_eval_time': 11643.594151735306, 'accumulated_logging_time': 0.5991125106811523}
I0214 11:34:56.885402 139975047763712 logging_writer.py:48] [47170] accumulated_eval_time=11643.594152, accumulated_logging_time=0.599113, accumulated_submission_time=16832.794115, global_step=47170, preemption_count=0, score=16832.794115, test/accuracy=0.680786, test/bleu=28.134572, test/loss=1.459881, test/num_examples=3003, total_duration=28478.575281, train/accuracy=0.648177, train/bleu=31.457150, train/loss=1.674423, validation/accuracy=0.668088, validation/bleu=28.766771, validation/loss=1.545769, validation/num_examples=3000
I0214 11:35:07.887792 139975056156416 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.19520199298858643, loss=1.6696462631225586
I0214 11:35:43.380890 139975047763712 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.2169024646282196, loss=1.6851718425750732
I0214 11:36:18.933854 139975056156416 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.23105965554714203, loss=1.7656221389770508
I0214 11:36:54.526312 139975047763712 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.17776857316493988, loss=1.7610578536987305
I0214 11:37:30.190034 139975056156416 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2500959038734436, loss=1.6878834962844849
I0214 11:38:05.805223 139975047763712 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.22037845849990845, loss=1.7965693473815918
I0214 11:38:41.431296 139975056156416 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.20023590326309204, loss=1.7800439596176147
I0214 11:39:17.025633 139975047763712 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.22053435444831848, loss=1.8382959365844727
I0214 11:39:52.650138 139975056156416 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.20069555938243866, loss=1.6855319738388062
I0214 11:40:28.238591 139975047763712 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.20138245820999146, loss=1.7293715476989746
I0214 11:41:03.853383 139975056156416 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.23916485905647278, loss=1.6877024173736572
I0214 11:41:39.491893 139975047763712 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.20219115912914276, loss=1.804926872253418
I0214 11:42:15.128369 139975056156416 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.2031852751970291, loss=1.7069289684295654
I0214 11:42:50.750294 139975047763712 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.22358672320842743, loss=1.8232344388961792
I0214 11:43:26.365021 139975056156416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.27206581830978394, loss=1.7016403675079346
I0214 11:44:01.988647 139975047763712 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.21178343892097473, loss=1.7393231391906738
I0214 11:44:37.624872 139975056156416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.2113926112651825, loss=1.7017494440078735
I0214 11:45:13.241248 139975047763712 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.23764492571353912, loss=1.7587223052978516
I0214 11:45:48.857598 139975056156416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.18407313525676727, loss=1.7639249563217163
I0214 11:46:24.478129 139975047763712 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.20616582036018372, loss=1.712247610092163
I0214 11:47:00.118467 139975056156416 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.19377665221691132, loss=1.783908724784851
I0214 11:47:35.798511 139975047763712 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.22096993029117584, loss=1.755608081817627
I0214 11:48:11.435391 139975056156416 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2141006588935852, loss=1.6965376138687134
I0214 11:48:47.056593 139975047763712 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.19850051403045654, loss=1.7581560611724854
I0214 11:48:57.102334 140144802662208 spec.py:321] Evaluating on the training split.
I0214 11:49:00.071173 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 11:53:06.101883 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 11:53:08.780059 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 11:55:41.571014 140144802662208 spec.py:349] Evaluating on the test split.
I0214 11:55:44.256046 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 11:58:15.889612 140144802662208 submission_runner.py:408] Time since start: 29877.60s, 	Step: 49530, 	{'train/accuracy': 0.6455420851707458, 'train/loss': 1.7034410238265991, 'train/bleu': 31.773293987179354, 'validation/accuracy': 0.666240930557251, 'validation/loss': 1.5409092903137207, 'validation/bleu': 28.787587888853047, 'validation/num_examples': 3000, 'test/accuracy': 0.681494414806366, 'test/loss': 1.4549793004989624, 'test/bleu': 28.547696719561916, 'test/num_examples': 3003, 'score': 17672.922026872635, 'total_duration': 29877.601472377777, 'accumulated_submission_time': 17672.922026872635, 'accumulated_eval_time': 12202.3813393116, 'accumulated_logging_time': 0.6326305866241455}
I0214 11:58:15.911304 139975056156416 logging_writer.py:48] [49530] accumulated_eval_time=12202.381339, accumulated_logging_time=0.632631, accumulated_submission_time=17672.922027, global_step=49530, preemption_count=0, score=17672.922027, test/accuracy=0.681494, test/bleu=28.547697, test/loss=1.454979, test/num_examples=3003, total_duration=29877.601472, train/accuracy=0.645542, train/bleu=31.773294, train/loss=1.703441, validation/accuracy=0.666241, validation/bleu=28.787588, validation/loss=1.540909, validation/num_examples=3000
I0214 11:58:41.104358 139975047763712 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.21704384684562683, loss=1.7562042474746704
I0214 11:59:16.589223 139975056156416 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.19662027060985565, loss=1.7029743194580078
I0214 11:59:52.141464 139975047763712 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.19564354419708252, loss=1.6680080890655518
I0214 12:00:27.735914 139975056156416 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3221370279788971, loss=1.7232489585876465
I0214 12:01:03.331388 139975047763712 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.17381839454174042, loss=1.7549744844436646
I0214 12:01:38.941749 139975056156416 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.20426949858665466, loss=1.6924957036972046
I0214 12:02:14.550959 139975047763712 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.21375992894172668, loss=1.7079412937164307
I0214 12:02:50.168060 139975056156416 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.2072998434305191, loss=1.7391316890716553
I0214 12:03:25.800529 139975047763712 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.19183041155338287, loss=1.777485966682434
I0214 12:04:01.450700 139975056156416 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.19041430950164795, loss=1.7879775762557983
I0214 12:04:37.090959 139975047763712 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.20452405512332916, loss=1.7694473266601562
I0214 12:05:12.701889 139975056156416 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.21633346378803253, loss=1.7930759191513062
I0214 12:05:48.347734 139975047763712 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.24379748106002808, loss=1.714317798614502
I0214 12:06:23.993093 139975056156416 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.20702219009399414, loss=1.7681578397750854
I0214 12:06:59.596053 139975047763712 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.19648036360740662, loss=1.6962904930114746
I0214 12:07:35.228407 139975056156416 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.20705309510231018, loss=1.7312198877334595
I0214 12:08:10.869560 139975047763712 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.19271278381347656, loss=1.7211006879806519
I0214 12:08:46.500646 139975056156416 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.20566146075725555, loss=1.7993104457855225
I0214 12:09:22.148142 139975047763712 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.19778606295585632, loss=1.7173579931259155
I0214 12:09:57.791494 139975056156416 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.22322314977645874, loss=1.73232102394104
I0214 12:10:33.411633 139975047763712 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.2039925456047058, loss=1.6902451515197754
I0214 12:11:09.088148 139975056156416 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2026335448026657, loss=1.8011223077774048
I0214 12:11:44.727623 139975047763712 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.19191347062587738, loss=1.7607563734054565
I0214 12:12:16.170903 140144802662208 spec.py:321] Evaluating on the training split.
I0214 12:12:19.163782 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 12:16:55.916959 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 12:16:58.628399 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 12:20:20.580946 140144802662208 spec.py:349] Evaluating on the test split.
I0214 12:20:23.249732 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 12:23:35.540423 140144802662208 submission_runner.py:408] Time since start: 31397.25s, 	Step: 51890, 	{'train/accuracy': 0.652571439743042, 'train/loss': 1.6386526823043823, 'train/bleu': 32.39050117798098, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.5333878993988037, 'validation/bleu': 28.55614230670583, 'validation/num_examples': 3000, 'test/accuracy': 0.6804369688034058, 'test/loss': 1.4555186033248901, 'test/bleu': 28.056858321750056, 'test/num_examples': 3003, 'score': 18513.092685222626, 'total_duration': 31397.252323150635, 'accumulated_submission_time': 18513.092685222626, 'accumulated_eval_time': 12881.750809669495, 'accumulated_logging_time': 0.6645634174346924}
I0214 12:23:35.562364 139975056156416 logging_writer.py:48] [51890] accumulated_eval_time=12881.750810, accumulated_logging_time=0.664563, accumulated_submission_time=18513.092685, global_step=51890, preemption_count=0, score=18513.092685, test/accuracy=0.680437, test/bleu=28.056858, test/loss=1.455519, test/num_examples=3003, total_duration=31397.252323, train/accuracy=0.652571, train/bleu=32.390501, train/loss=1.638653, validation/accuracy=0.668783, validation/bleu=28.556142, validation/loss=1.533388, validation/num_examples=3000
I0214 12:23:39.478713 139975047763712 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.19898702204227448, loss=1.661658763885498
I0214 12:24:14.967252 139975056156416 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.19565793871879578, loss=1.7894846200942993
I0214 12:24:50.530963 139975047763712 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.20379424095153809, loss=1.7464524507522583
I0214 12:25:26.126498 139975056156416 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.19148851931095123, loss=1.7013181447982788
I0214 12:26:01.757039 139975047763712 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.18688137829303741, loss=1.7084068059921265
I0214 12:26:37.420040 139975056156416 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.21724626421928406, loss=1.7794444561004639
I0214 12:27:13.021833 139975047763712 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.2008778601884842, loss=1.662042498588562
I0214 12:27:48.645319 139975056156416 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.20903469622135162, loss=1.7013002634048462
I0214 12:28:24.268115 139975047763712 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.1973014622926712, loss=1.6534712314605713
I0214 12:28:59.908052 139975056156416 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.2664850056171417, loss=1.7386338710784912
I0214 12:29:35.571309 139975047763712 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.18742084503173828, loss=1.7803176641464233
I0214 12:30:11.222689 139975056156416 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.22774171829223633, loss=1.6743789911270142
I0214 12:30:46.891522 139975047763712 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.1904352903366089, loss=1.7554796934127808
I0214 12:31:22.515329 139975056156416 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.20975729823112488, loss=1.7546665668487549
I0214 12:31:58.130113 139975047763712 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.21185402572155, loss=1.677284598350525
I0214 12:32:33.749332 139975056156416 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.19235806167125702, loss=1.7123944759368896
I0214 12:33:09.401651 139975047763712 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.20110663771629333, loss=1.6442278623580933
I0214 12:33:45.057016 139975056156416 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.2637370824813843, loss=1.7366230487823486
I0214 12:34:20.706427 139975047763712 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.26133134961128235, loss=1.6144375801086426
I0214 12:34:56.326395 139975056156416 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.2102818340063095, loss=1.7829911708831787
I0214 12:35:31.933203 139975047763712 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.22184257209300995, loss=1.7578699588775635
I0214 12:36:07.553259 139975056156416 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.19651344418525696, loss=1.7211815118789673
I0214 12:36:43.152913 139975047763712 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.24243032932281494, loss=1.765423059463501
I0214 12:37:18.788295 139975056156416 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.21200226247310638, loss=1.747273325920105
I0214 12:37:35.617970 140144802662208 spec.py:321] Evaluating on the training split.
I0214 12:37:38.586226 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 12:41:41.065436 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 12:41:43.767931 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 12:45:16.555189 140144802662208 spec.py:349] Evaluating on the test split.
I0214 12:45:19.234536 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 12:48:52.991707 140144802662208 submission_runner.py:408] Time since start: 32914.70s, 	Step: 54249, 	{'train/accuracy': 0.6511799693107605, 'train/loss': 1.6562837362289429, 'train/bleu': 31.79512948258729, 'validation/accuracy': 0.6702210903167725, 'validation/loss': 1.527982473373413, 'validation/bleu': 29.006077664411652, 'validation/num_examples': 3000, 'test/accuracy': 0.6831910014152527, 'test/loss': 1.4422683715820312, 'test/bleu': 28.418639510555767, 'test/num_examples': 3003, 'score': 19353.05917453766, 'total_duration': 32914.703605651855, 'accumulated_submission_time': 19353.05917453766, 'accumulated_eval_time': 13559.124497413635, 'accumulated_logging_time': 0.6962974071502686}
I0214 12:48:53.013735 139975047763712 logging_writer.py:48] [54249] accumulated_eval_time=13559.124497, accumulated_logging_time=0.696297, accumulated_submission_time=19353.059175, global_step=54249, preemption_count=0, score=19353.059175, test/accuracy=0.683191, test/bleu=28.418640, test/loss=1.442268, test/num_examples=3003, total_duration=32914.703606, train/accuracy=0.651180, train/bleu=31.795129, train/loss=1.656284, validation/accuracy=0.670221, validation/bleu=29.006078, validation/loss=1.527982, validation/num_examples=3000
I0214 12:49:11.446174 139975056156416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.21898271143436432, loss=1.7361406087875366
I0214 12:49:46.929820 139975047763712 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.19893676042556763, loss=1.7194737195968628
I0214 12:50:22.512995 139975056156416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.21733082830905914, loss=1.8411277532577515
I0214 12:50:58.182392 139975047763712 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.20728476345539093, loss=1.8027422428131104
I0214 12:51:33.808819 139975056156416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.21373629570007324, loss=1.7440390586853027
I0214 12:52:09.446629 139975047763712 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.20529599487781525, loss=1.7236076593399048
I0214 12:52:45.067294 139975056156416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.21656787395477295, loss=1.7001546621322632
I0214 12:53:20.715717 139975047763712 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2036714106798172, loss=1.7000871896743774
I0214 12:53:56.309026 139975056156416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2710292935371399, loss=1.6742688417434692
I0214 12:54:31.941866 139975047763712 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2237691730260849, loss=1.7088239192962646
I0214 12:55:07.575275 139975056156416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.20959880948066711, loss=1.7025219202041626
I0214 12:55:43.211545 139975047763712 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.21390952169895172, loss=1.6691851615905762
I0214 12:56:18.859664 139975056156416 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2104567587375641, loss=1.8492088317871094
I0214 12:56:54.477237 139975047763712 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.22569751739501953, loss=1.7152156829833984
I0214 12:57:30.084272 139975056156416 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1976517140865326, loss=1.7398802042007446
I0214 12:58:05.697373 139975047763712 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2021263986825943, loss=1.7517461776733398
I0214 12:58:41.314473 139975056156416 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.20483265817165375, loss=1.8584462404251099
I0214 12:59:16.911263 139975047763712 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.214286670088768, loss=1.738784909248352
I0214 12:59:52.556565 139975056156416 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.19944700598716736, loss=1.694264531135559
I0214 13:00:28.208207 139975047763712 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.20017702877521515, loss=1.7331050634384155
I0214 13:01:03.817153 139975056156416 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.19763875007629395, loss=1.8011513948440552
I0214 13:01:39.449319 139975047763712 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.2123301923274994, loss=1.6805068254470825
I0214 13:02:15.065123 139975056156416 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.1982330083847046, loss=1.6935521364212036
I0214 13:02:50.689614 139975047763712 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.34685778617858887, loss=1.7154539823532104
I0214 13:02:53.266910 140144802662208 spec.py:321] Evaluating on the training split.
I0214 13:02:56.233708 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 13:06:59.345537 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 13:07:02.058340 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 13:10:08.305810 140144802662208 spec.py:349] Evaluating on the test split.
I0214 13:10:10.998832 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 13:13:26.551757 140144802662208 submission_runner.py:408] Time since start: 34388.26s, 	Step: 56609, 	{'train/accuracy': 0.6711536049842834, 'train/loss': 1.5183404684066772, 'train/bleu': 33.20045640260435, 'validation/accuracy': 0.6715229749679565, 'validation/loss': 1.5183759927749634, 'validation/bleu': 29.055875248804714, 'validation/num_examples': 3000, 'test/accuracy': 0.6822032332420349, 'test/loss': 1.4344021081924438, 'test/bleu': 28.28308659347989, 'test/num_examples': 3003, 'score': 20193.22402882576, 'total_duration': 34388.2636551857, 'accumulated_submission_time': 20193.22402882576, 'accumulated_eval_time': 14192.409289360046, 'accumulated_logging_time': 0.7298541069030762}
I0214 13:13:26.574293 139975056156416 logging_writer.py:48] [56609] accumulated_eval_time=14192.409289, accumulated_logging_time=0.729854, accumulated_submission_time=20193.224029, global_step=56609, preemption_count=0, score=20193.224029, test/accuracy=0.682203, test/bleu=28.283087, test/loss=1.434402, test/num_examples=3003, total_duration=34388.263655, train/accuracy=0.671154, train/bleu=33.200456, train/loss=1.518340, validation/accuracy=0.671523, validation/bleu=29.055875, validation/loss=1.518376, validation/num_examples=3000
I0214 13:13:59.174037 139975047763712 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.21785680949687958, loss=1.6956685781478882
I0214 13:14:34.722691 139975056156416 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.23315247893333435, loss=1.7284337282180786
I0214 13:15:10.339941 139975047763712 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.19018426537513733, loss=1.6884573698043823
I0214 13:15:45.930244 139975056156416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.2424260377883911, loss=1.7703487873077393
I0214 13:16:21.698619 139975047763712 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.19704478979110718, loss=1.6908990144729614
I0214 13:16:57.380339 139975056156416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.20651811361312866, loss=1.782724380493164
I0214 13:17:33.058933 139975047763712 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.21363729238510132, loss=1.6858611106872559
I0214 13:18:08.728685 139975056156416 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.19614803791046143, loss=1.7361376285552979
I0214 13:18:44.366419 139975047763712 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.20838333666324615, loss=1.7232551574707031
I0214 13:19:20.043613 139975056156416 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.17968735098838806, loss=1.7542309761047363
I0214 13:19:55.684784 139975047763712 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.35415226221084595, loss=1.699660062789917
I0214 13:20:31.346742 139975056156416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.19317571818828583, loss=1.7206026315689087
I0214 13:21:06.969651 139975047763712 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2067287713289261, loss=1.653799295425415
I0214 13:21:42.628018 139975056156416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2140088528394699, loss=1.7002118825912476
I0214 13:22:18.270626 139975047763712 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.7999464869499207, loss=1.6439141035079956
I0214 13:22:53.935057 139975056156416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.2410164624452591, loss=1.706109642982483
I0214 13:23:29.542545 139975047763712 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.19726940989494324, loss=1.6488652229309082
I0214 13:24:05.149653 139975056156416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.18373528122901917, loss=1.6977473497390747
I0214 13:24:40.772593 139975047763712 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.19598527252674103, loss=1.730176568031311
I0214 13:25:16.378797 139975056156416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.19538173079490662, loss=1.6693495512008667
I0214 13:25:51.986660 139975047763712 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.25280436873435974, loss=1.7412358522415161
I0214 13:26:27.641182 139975056156416 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.20685936510562897, loss=1.6131446361541748
I0214 13:27:03.282554 139975047763712 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.2089688926935196, loss=1.676737666130066
I0214 13:27:26.853715 140144802662208 spec.py:321] Evaluating on the training split.
I0214 13:27:29.823497 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 13:32:06.813357 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 13:32:09.495083 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 13:36:13.384692 140144802662208 spec.py:349] Evaluating on the test split.
I0214 13:36:16.062162 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 13:40:25.737035 140144802662208 submission_runner.py:408] Time since start: 36007.45s, 	Step: 58968, 	{'train/accuracy': 0.6562167406082153, 'train/loss': 1.6146293878555298, 'train/bleu': 32.00557628346472, 'validation/accuracy': 0.6700970530509949, 'validation/loss': 1.5160971879959106, 'validation/bleu': 28.6046113423605, 'validation/num_examples': 3000, 'test/accuracy': 0.6840625405311584, 'test/loss': 1.4318151473999023, 'test/bleu': 28.442912978921708, 'test/num_examples': 3003, 'score': 21033.412729263306, 'total_duration': 36007.44893741608, 'accumulated_submission_time': 21033.412729263306, 'accumulated_eval_time': 14971.292563676834, 'accumulated_logging_time': 0.7623600959777832}
I0214 13:40:25.761499 139975056156416 logging_writer.py:48] [58968] accumulated_eval_time=14971.292564, accumulated_logging_time=0.762360, accumulated_submission_time=21033.412729, global_step=58968, preemption_count=0, score=21033.412729, test/accuracy=0.684063, test/bleu=28.442913, test/loss=1.431815, test/num_examples=3003, total_duration=36007.448937, train/accuracy=0.656217, train/bleu=32.005576, train/loss=1.614629, validation/accuracy=0.670097, validation/bleu=28.604611, validation/loss=1.516097, validation/num_examples=3000
I0214 13:40:37.489290 139975047763712 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.19167733192443848, loss=1.6670151948928833
I0214 13:41:12.983423 139975056156416 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19875428080558777, loss=1.7145441770553589
I0214 13:41:48.543945 139975047763712 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.20749633014202118, loss=1.7747669219970703
I0214 13:42:24.158600 139975056156416 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.19414068758487701, loss=1.77041757106781
I0214 13:42:59.770440 139975047763712 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.20519302785396576, loss=1.6430003643035889
I0214 13:43:35.400703 139975056156416 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.19911979138851166, loss=1.6983928680419922
I0214 13:44:11.006849 139975047763712 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.19905886054039001, loss=1.6781584024429321
I0214 13:44:46.620522 139975056156416 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.23094050586223602, loss=1.7492138147354126
I0214 13:45:22.206810 139975047763712 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.20458602905273438, loss=1.714928388595581
I0214 13:45:57.822213 139975056156416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.23733989894390106, loss=1.701204776763916
I0214 13:46:33.438112 139975047763712 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.21546374261379242, loss=1.7599103450775146
I0214 13:47:09.089840 139975056156416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.2197413593530655, loss=1.6931953430175781
I0214 13:47:44.726489 139975047763712 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.21207521855831146, loss=1.705887794494629
I0214 13:48:20.359522 139975056156416 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.2130449116230011, loss=1.7531557083129883
I0214 13:48:56.013816 139975047763712 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.2112952321767807, loss=1.7379786968231201
I0214 13:49:31.704695 139975056156416 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.20086455345153809, loss=1.7077655792236328
I0214 13:50:07.335028 139975047763712 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.22189708054065704, loss=1.644079327583313
I0214 13:50:42.944670 139975056156416 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.2298593819141388, loss=1.7934327125549316
I0214 13:51:18.556621 139975047763712 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.1836501806974411, loss=1.7891467809677124
I0214 13:51:54.182821 139975056156416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.20407457649707794, loss=1.7184809446334839
I0214 13:52:29.820617 139975047763712 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.19571588933467865, loss=1.7839462757110596
I0214 13:53:05.464614 139975056156416 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.20927086472511292, loss=1.691754698753357
I0214 13:53:41.095492 139975047763712 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.22452396154403687, loss=1.6989468336105347
I0214 13:54:16.751989 139975056156416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.26896485686302185, loss=1.6486049890518188
I0214 13:54:25.769826 140144802662208 spec.py:321] Evaluating on the training split.
I0214 13:54:28.746350 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 13:58:15.994874 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 13:58:18.663587 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 14:00:40.004824 140144802662208 spec.py:349] Evaluating on the test split.
I0214 14:00:42.680768 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 14:03:33.444408 140144802662208 submission_runner.py:408] Time since start: 37395.16s, 	Step: 61327, 	{'train/accuracy': 0.6536709666252136, 'train/loss': 1.6435863971710205, 'train/bleu': 32.201144941719825, 'validation/accuracy': 0.67365562915802, 'validation/loss': 1.5085053443908691, 'validation/bleu': 29.024937312260363, 'validation/num_examples': 3000, 'test/accuracy': 0.6860031485557556, 'test/loss': 1.4202231168746948, 'test/bleu': 28.753225893997975, 'test/num_examples': 3003, 'score': 21873.331847190857, 'total_duration': 37395.15630316734, 'accumulated_submission_time': 21873.331847190857, 'accumulated_eval_time': 15518.967089891434, 'accumulated_logging_time': 0.7969238758087158}
I0214 14:03:33.467774 139975047763712 logging_writer.py:48] [61327] accumulated_eval_time=15518.967090, accumulated_logging_time=0.796924, accumulated_submission_time=21873.331847, global_step=61327, preemption_count=0, score=21873.331847, test/accuracy=0.686003, test/bleu=28.753226, test/loss=1.420223, test/num_examples=3003, total_duration=37395.156303, train/accuracy=0.653671, train/bleu=32.201145, train/loss=1.643586, validation/accuracy=0.673656, validation/bleu=29.024937, validation/loss=1.508505, validation/num_examples=3000
I0214 14:03:59.805714 139975056156416 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.22628922760486603, loss=1.707133412361145
I0214 14:04:35.319760 139975047763712 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.20665888488292694, loss=1.7329760789871216
I0214 14:05:10.927245 139975056156416 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.20927570760250092, loss=1.7473945617675781
I0214 14:05:46.531744 139975047763712 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.20065779983997345, loss=1.6940723657608032
I0214 14:06:22.130561 139975056156416 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.20908379554748535, loss=1.7771703004837036
I0214 14:06:57.762196 139975047763712 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.20243863761425018, loss=1.618268370628357
I0214 14:07:33.392411 139975056156416 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.223493292927742, loss=1.726973056793213
I0214 14:08:08.995789 139975047763712 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.2296493798494339, loss=1.756898045539856
I0214 14:08:44.631868 139975056156416 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.19845451414585114, loss=1.6404696702957153
I0214 14:09:20.248030 139975047763712 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.23883508145809174, loss=1.6692523956298828
I0214 14:09:55.890355 139975056156416 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.19116273522377014, loss=1.5972754955291748
I0214 14:10:31.496725 139975047763712 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.20674008131027222, loss=1.696853756904602
I0214 14:11:07.118362 139975056156416 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.19908416271209717, loss=1.6382664442062378
I0214 14:11:42.802763 139975047763712 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.2535982131958008, loss=1.7056326866149902
I0214 14:12:18.509616 139975056156416 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.21961010992527008, loss=1.6976350545883179
I0214 14:12:54.158835 139975047763712 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.20541469752788544, loss=1.7212601900100708
I0214 14:13:29.799746 139975056156416 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.2015371322631836, loss=1.7046294212341309
I0214 14:14:05.430264 139975047763712 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.20195843279361725, loss=1.6914350986480713
I0214 14:14:41.090651 139975056156416 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.20172718167304993, loss=1.7204958200454712
I0214 14:15:16.728672 139975047763712 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.20370326936244965, loss=1.6267106533050537
I0214 14:15:52.354614 139975056156416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.2429959774017334, loss=1.7486745119094849
I0214 14:16:27.999408 139975047763712 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.19028641283512115, loss=1.6780776977539062
I0214 14:17:03.616968 139975056156416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.40159812569618225, loss=1.645896077156067
I0214 14:17:33.629942 140144802662208 spec.py:321] Evaluating on the training split.
I0214 14:17:36.600882 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 14:20:56.014218 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 14:20:58.689587 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 14:24:01.183722 140144802662208 spec.py:349] Evaluating on the test split.
I0214 14:24:03.883806 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 14:27:09.946422 140144802662208 submission_runner.py:408] Time since start: 38811.66s, 	Step: 63686, 	{'train/accuracy': 0.6629753708839417, 'train/loss': 1.568068265914917, 'train/bleu': 33.01746848462258, 'validation/accuracy': 0.6728744506835938, 'validation/loss': 1.503533124923706, 'validation/bleu': 28.890694782084704, 'validation/num_examples': 3000, 'test/accuracy': 0.686851441860199, 'test/loss': 1.4139506816864014, 'test/bleu': 29.019938516980574, 'test/num_examples': 3003, 'score': 22713.31698822975, 'total_duration': 38811.658296108246, 'accumulated_submission_time': 22713.31698822975, 'accumulated_eval_time': 16095.283498048782, 'accumulated_logging_time': 0.9187502861022949}
I0214 14:27:09.975428 139975047763712 logging_writer.py:48] [63686] accumulated_eval_time=16095.283498, accumulated_logging_time=0.918750, accumulated_submission_time=22713.316988, global_step=63686, preemption_count=0, score=22713.316988, test/accuracy=0.686851, test/bleu=29.019939, test/loss=1.413951, test/num_examples=3003, total_duration=38811.658296, train/accuracy=0.662975, train/bleu=33.017468, train/loss=1.568068, validation/accuracy=0.672874, validation/bleu=28.890695, validation/loss=1.503533, validation/num_examples=3000
I0214 14:27:15.316932 139975056156416 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.199768528342247, loss=1.702147126197815
I0214 14:27:50.781976 139975047763712 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.20139473676681519, loss=1.7009063959121704
I0214 14:28:26.341976 139975056156416 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.20454208552837372, loss=1.705636978149414
I0214 14:29:01.951874 139975047763712 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2115357220172882, loss=1.6549667119979858
I0214 14:29:37.548217 139975056156416 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.20193637907505035, loss=1.7010505199432373
I0214 14:30:13.182765 139975047763712 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.20681214332580566, loss=1.7121299505233765
I0214 14:30:48.821917 139975056156416 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.2010682225227356, loss=1.6760631799697876
I0214 14:31:24.474431 139975047763712 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.20348896086215973, loss=1.7191452980041504
I0214 14:32:00.283011 139975056156416 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2424570918083191, loss=1.7458925247192383
I0214 14:32:35.924300 139975047763712 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2329837828874588, loss=1.693561315536499
I0214 14:33:11.550529 139975056156416 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.19692760705947876, loss=1.7401918172836304
I0214 14:33:47.175511 139975047763712 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.22426092624664307, loss=1.7157057523727417
I0214 14:34:22.794400 139975056156416 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.1944873034954071, loss=1.6446137428283691
I0214 14:34:58.423836 139975047763712 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.21756203472614288, loss=1.6662954092025757
I0214 14:35:34.035329 139975056156416 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.19972464442253113, loss=1.7024579048156738
I0214 14:36:09.648218 139975047763712 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.2076234519481659, loss=1.6406917572021484
I0214 14:36:45.281340 139975056156416 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.20025570690631866, loss=1.6791634559631348
I0214 14:37:20.895120 139975047763712 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.1942204087972641, loss=1.7307809591293335
I0214 14:37:56.518627 139975056156416 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.22082407772541046, loss=1.660902976989746
I0214 14:38:32.202398 139975047763712 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.2022697925567627, loss=1.7241301536560059
I0214 14:39:07.854263 139975056156416 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.20640119910240173, loss=1.7382718324661255
I0214 14:39:43.522336 139975047763712 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.21288976073265076, loss=1.6640011072158813
I0214 14:40:19.130030 139975056156416 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.20286986231803894, loss=1.6729239225387573
I0214 14:40:54.740044 139975047763712 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.2078833431005478, loss=1.6247118711471558
I0214 14:41:10.128944 140144802662208 spec.py:321] Evaluating on the training split.
I0214 14:41:13.094540 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 14:44:39.830816 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 14:44:42.514475 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 14:47:10.093929 140144802662208 spec.py:349] Evaluating on the test split.
I0214 14:47:12.779353 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 14:49:58.822845 140144802662208 submission_runner.py:408] Time since start: 40180.53s, 	Step: 66045, 	{'train/accuracy': 0.6583556532859802, 'train/loss': 1.5996129512786865, 'train/bleu': 32.41859989742564, 'validation/accuracy': 0.6759990453720093, 'validation/loss': 1.4937245845794678, 'validation/bleu': 29.610730501242646, 'validation/num_examples': 3000, 'test/accuracy': 0.6908256411552429, 'test/loss': 1.3972442150115967, 'test/bleu': 29.21380668381085, 'test/num_examples': 3003, 'score': 23553.378241539, 'total_duration': 40180.534745931625, 'accumulated_submission_time': 23553.378241539, 'accumulated_eval_time': 16623.977352142334, 'accumulated_logging_time': 0.9587399959564209}
I0214 14:49:58.848746 139975056156416 logging_writer.py:48] [66045] accumulated_eval_time=16623.977352, accumulated_logging_time=0.958740, accumulated_submission_time=23553.378242, global_step=66045, preemption_count=0, score=23553.378242, test/accuracy=0.690826, test/bleu=29.213807, test/loss=1.397244, test/num_examples=3003, total_duration=40180.534746, train/accuracy=0.658356, train/bleu=32.418600, train/loss=1.599613, validation/accuracy=0.675999, validation/bleu=29.610731, validation/loss=1.493725, validation/num_examples=3000
I0214 14:50:18.730270 139975047763712 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2259625345468521, loss=1.6857411861419678
I0214 14:50:54.223502 139975056156416 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.2032550424337387, loss=1.6888610124588013
I0214 14:51:29.833350 139975047763712 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.21267271041870117, loss=1.7284539937973022
I0214 14:52:05.504375 139975056156416 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.197394460439682, loss=1.6109797954559326
I0214 14:52:41.214442 139975047763712 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.21052499115467072, loss=1.6644960641860962
I0214 14:53:17.012193 139975056156416 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.20275181531906128, loss=1.6869218349456787
I0214 14:53:52.636723 139975047763712 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.21793313324451447, loss=1.7422285079956055
I0214 14:54:28.267374 139975056156416 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.18964096903800964, loss=1.6369982957839966
I0214 14:55:03.892609 139975047763712 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2655235826969147, loss=1.7028757333755493
I0214 14:55:39.493637 139975056156416 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.20993179082870483, loss=1.6473299264907837
I0214 14:56:15.099579 139975047763712 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.22605352103710175, loss=1.711180329322815
I0214 14:56:50.758650 139975056156416 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.20538190007209778, loss=1.6470927000045776
I0214 14:57:26.445411 139975047763712 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.19613207876682281, loss=1.7006665468215942
I0214 14:58:02.090405 139975056156416 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.2106931060552597, loss=1.7105505466461182
I0214 14:58:37.713771 139975047763712 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.2103845179080963, loss=1.7601072788238525
I0214 14:59:13.304538 139975056156416 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2069341093301773, loss=1.6082037687301636
I0214 14:59:48.987733 139975047763712 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3478984832763672, loss=1.6381034851074219
I0214 15:00:24.628386 139975056156416 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.2212456315755844, loss=1.6994978189468384
I0214 15:01:00.308293 139975047763712 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2140842080116272, loss=1.7129188776016235
I0214 15:01:35.967011 139975056156416 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.33633938431739807, loss=1.6544924974441528
I0214 15:02:11.600782 139975047763712 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.207926407456398, loss=1.6434831619262695
I0214 15:02:47.195761 139975056156416 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.23180252313613892, loss=1.7546597719192505
I0214 15:03:22.843626 139975047763712 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.18349917232990265, loss=1.621519923210144
I0214 15:03:58.504575 139975056156416 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.21081946790218353, loss=1.6282039880752563
I0214 15:03:58.950410 140144802662208 spec.py:321] Evaluating on the training split.
I0214 15:04:01.955874 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 15:07:47.498878 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 15:07:50.181678 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 15:10:37.592324 140144802662208 spec.py:349] Evaluating on the test split.
I0214 15:10:40.272570 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 15:13:06.223466 140144802662208 submission_runner.py:408] Time since start: 41567.94s, 	Step: 68403, 	{'train/accuracy': 0.6556589603424072, 'train/loss': 1.61857008934021, 'train/bleu': 32.53331232232554, 'validation/accuracy': 0.675317108631134, 'validation/loss': 1.4912937879562378, 'validation/bleu': 29.188595260579557, 'validation/num_examples': 3000, 'test/accuracy': 0.6906862258911133, 'test/loss': 1.391940951347351, 'test/bleu': 28.9712420024266, 'test/num_examples': 3003, 'score': 24393.38697886467, 'total_duration': 41567.93536186218, 'accumulated_submission_time': 24393.38697886467, 'accumulated_eval_time': 17171.25036072731, 'accumulated_logging_time': 0.9950568675994873}
I0214 15:13:06.249915 139975047763712 logging_writer.py:48] [68403] accumulated_eval_time=17171.250361, accumulated_logging_time=0.995057, accumulated_submission_time=24393.386979, global_step=68403, preemption_count=0, score=24393.386979, test/accuracy=0.690686, test/bleu=28.971242, test/loss=1.391941, test/num_examples=3003, total_duration=41567.935362, train/accuracy=0.655659, train/bleu=32.533312, train/loss=1.618570, validation/accuracy=0.675317, validation/bleu=29.188595, validation/loss=1.491294, validation/num_examples=3000
I0214 15:13:41.011121 139975056156416 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.250333696603775, loss=1.6940701007843018
I0214 15:14:16.575784 139975047763712 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.21482087671756744, loss=1.6372172832489014
I0214 15:14:52.212474 139975056156416 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.2257596254348755, loss=1.721699833869934
I0214 15:15:27.871707 139975047763712 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.2225247472524643, loss=1.5878318548202515
I0214 15:16:03.500751 139975056156416 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.20667783915996552, loss=1.6825652122497559
I0214 15:16:39.112171 139975047763712 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.20153768360614777, loss=1.674025297164917
I0214 15:17:14.798574 139975056156416 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.18560031056404114, loss=1.7313278913497925
I0214 15:17:50.415173 139975047763712 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.2152942717075348, loss=1.7327179908752441
I0214 15:18:26.039221 139975056156416 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.20144113898277283, loss=1.7089329957962036
I0214 15:19:01.658273 139975047763712 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.20253150165081024, loss=1.6221222877502441
I0214 15:19:37.291924 139975056156416 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.20602253079414368, loss=1.6324902772903442
I0214 15:20:12.929600 139975047763712 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.20580942928791046, loss=1.6343501806259155
I0214 15:20:48.562548 139975056156416 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.21193058788776398, loss=1.7138408422470093
I0214 15:21:24.211660 139975047763712 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.2300441861152649, loss=1.6301995515823364
I0214 15:21:59.834614 139975056156416 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.2082255780696869, loss=1.702933669090271
I0214 15:22:35.460632 139975047763712 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.195247620344162, loss=1.6696679592132568
I0214 15:23:11.135579 139975056156416 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.22907404601573944, loss=1.6540862321853638
I0214 15:23:46.819665 139975047763712 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.2016347050666809, loss=1.7101181745529175
I0214 15:24:22.452943 139975056156416 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.2002333551645279, loss=1.6414350271224976
I0214 15:24:58.094030 139975047763712 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.2188100814819336, loss=1.5934319496154785
I0214 15:25:33.712500 139975056156416 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.21102066338062286, loss=1.6469852924346924
I0214 15:26:09.377264 139975047763712 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.22376449406147003, loss=1.6397364139556885
I0214 15:26:44.995477 139975056156416 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2073826640844345, loss=1.642852544784546
I0214 15:27:06.466776 140144802662208 spec.py:321] Evaluating on the training split.
I0214 15:27:09.438337 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 15:31:20.181453 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 15:31:22.855647 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 15:34:59.330950 140144802662208 spec.py:349] Evaluating on the test split.
I0214 15:35:02.030994 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 15:38:49.882415 140144802662208 submission_runner.py:408] Time since start: 43111.59s, 	Step: 70762, 	{'train/accuracy': 0.6649782657623291, 'train/loss': 1.5656242370605469, 'train/bleu': 33.27862937609742, 'validation/accuracy': 0.6770901679992676, 'validation/loss': 1.4805506467819214, 'validation/bleu': 29.460682932058393, 'validation/num_examples': 3000, 'test/accuracy': 0.6927662491798401, 'test/loss': 1.3877900838851929, 'test/bleu': 29.393405939683753, 'test/num_examples': 3003, 'score': 25233.515276670456, 'total_duration': 43111.594311475754, 'accumulated_submission_time': 25233.515276670456, 'accumulated_eval_time': 17874.665945768356, 'accumulated_logging_time': 1.0316176414489746}
I0214 15:38:49.907492 139975047763712 logging_writer.py:48] [70762] accumulated_eval_time=17874.665946, accumulated_logging_time=1.031618, accumulated_submission_time=25233.515277, global_step=70762, preemption_count=0, score=25233.515277, test/accuracy=0.692766, test/bleu=29.393406, test/loss=1.387790, test/num_examples=3003, total_duration=43111.594311, train/accuracy=0.664978, train/bleu=33.278629, train/loss=1.565624, validation/accuracy=0.677090, validation/bleu=29.460683, validation/loss=1.480551, validation/num_examples=3000
I0214 15:39:03.755941 139975056156416 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.22461992502212524, loss=1.6372963190078735
I0214 15:39:39.237505 139975047763712 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.2085740864276886, loss=1.6736446619033813
I0214 15:40:14.861280 139975056156416 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20607393980026245, loss=1.5595896244049072
I0214 15:40:50.497953 139975047763712 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.20582188665866852, loss=1.6532078981399536
I0214 15:41:26.147387 139975056156416 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.22236037254333496, loss=1.738372564315796
I0214 15:42:01.789617 139975047763712 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.1998380571603775, loss=1.6571601629257202
I0214 15:42:37.405618 139975056156416 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.22415953874588013, loss=1.665221095085144
I0214 15:43:13.047056 139975047763712 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.20006215572357178, loss=1.6791647672653198
I0214 15:43:48.679153 139975056156416 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.2030986249446869, loss=1.6505194902420044
I0214 15:44:24.346375 139975047763712 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.2192608118057251, loss=1.6671555042266846
I0214 15:44:59.995564 139975056156416 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.22168691456317902, loss=1.6780509948730469
I0214 15:45:35.659507 139975047763712 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.2015099972486496, loss=1.64590585231781
I0214 15:46:11.271145 139975056156416 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.2112918198108673, loss=1.6995559930801392
I0214 15:46:46.921218 139975047763712 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.20106931030750275, loss=1.6532608270645142
I0214 15:47:22.594935 139975056156416 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.220020592212677, loss=1.5996567010879517
I0214 15:47:58.223795 139975047763712 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.19938446581363678, loss=1.6245051622390747
I0214 15:48:33.868696 139975056156416 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.21835680305957794, loss=1.7135536670684814
I0214 15:49:09.498480 139975047763712 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.26758015155792236, loss=1.6077622175216675
I0214 15:49:45.112700 139975056156416 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.1933145821094513, loss=1.6282316446304321
I0214 15:50:20.755366 139975047763712 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.19222749769687653, loss=1.6545847654342651
I0214 15:50:56.405596 139975056156416 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.19240079820156097, loss=1.672598123550415
I0214 15:51:32.053854 139975047763712 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.20412497222423553, loss=1.6838243007659912
I0214 15:52:07.707888 139975056156416 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2277846336364746, loss=1.6733778715133667
I0214 15:52:43.366343 139975047763712 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.21405328810214996, loss=1.6922011375427246
I0214 15:52:50.236637 140144802662208 spec.py:321] Evaluating on the training split.
I0214 15:52:53.219414 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 15:56:55.720705 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 15:56:58.409726 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 15:59:40.267969 140144802662208 spec.py:349] Evaluating on the test split.
I0214 15:59:42.935547 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 16:02:32.057385 140144802662208 submission_runner.py:408] Time since start: 44533.77s, 	Step: 73121, 	{'train/accuracy': 0.6601581573486328, 'train/loss': 1.5850191116333008, 'train/bleu': 33.035396256855584, 'validation/accuracy': 0.6781564950942993, 'validation/loss': 1.4745867252349854, 'validation/bleu': 29.27033968859275, 'validation/num_examples': 3000, 'test/accuracy': 0.6933588981628418, 'test/loss': 1.3788319826126099, 'test/bleu': 29.37666021325784, 'test/num_examples': 3003, 'score': 26073.755289793015, 'total_duration': 44533.76924061775, 'accumulated_submission_time': 26073.755289793015, 'accumulated_eval_time': 18456.486599206924, 'accumulated_logging_time': 1.0666627883911133}
I0214 16:02:32.088983 139975056156416 logging_writer.py:48] [73121] accumulated_eval_time=18456.486599, accumulated_logging_time=1.066663, accumulated_submission_time=26073.755290, global_step=73121, preemption_count=0, score=26073.755290, test/accuracy=0.693359, test/bleu=29.376660, test/loss=1.378832, test/num_examples=3003, total_duration=44533.769241, train/accuracy=0.660158, train/bleu=33.035396, train/loss=1.585019, validation/accuracy=0.678156, validation/bleu=29.270340, validation/loss=1.474587, validation/num_examples=3000
I0214 16:03:00.477632 139975047763712 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.20318053662776947, loss=1.6106290817260742
I0214 16:03:36.019249 139975056156416 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.1915614753961563, loss=1.5665559768676758
I0214 16:04:11.637806 139975047763712 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.21410199999809265, loss=1.5846915245056152
I0214 16:04:47.295590 139975056156416 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.1934785097837448, loss=1.6552414894104004
I0214 16:05:22.913927 139975047763712 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.23397710919380188, loss=1.7070046663284302
I0214 16:05:58.538161 139975056156416 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.21487537026405334, loss=1.6142605543136597
I0214 16:06:34.171386 139975047763712 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.21034793555736542, loss=1.6624847650527954
I0214 16:07:09.807224 139975056156416 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.19787907600402832, loss=1.6360892057418823
I0214 16:07:45.420367 139975047763712 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.20303089916706085, loss=1.690509557723999
I0214 16:08:21.083599 139975056156416 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.21859721839427948, loss=1.6984782218933105
I0214 16:08:56.739293 139975047763712 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.21953710913658142, loss=1.7030959129333496
I0214 16:09:32.374902 139975056156416 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.27017760276794434, loss=1.7057121992111206
I0214 16:10:07.988275 139975047763712 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.24389751255512238, loss=1.617377519607544
I0214 16:10:43.618343 139975056156416 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.1936727911233902, loss=1.556052565574646
I0214 16:11:19.258599 139975047763712 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2188955694437027, loss=1.5918058156967163
I0214 16:11:54.935418 139975056156416 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.23350462317466736, loss=1.5952205657958984
I0214 16:12:30.603134 139975047763712 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.20779025554656982, loss=1.7028933763504028
I0214 16:13:06.217836 139975056156416 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.20282740890979767, loss=1.6431461572647095
I0214 16:13:41.848984 139975047763712 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.1917242854833603, loss=1.6148560047149658
I0214 16:14:17.507277 139975056156416 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.20231948792934418, loss=1.544396996498108
I0214 16:14:53.133042 139975047763712 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.21825408935546875, loss=1.62189781665802
I0214 16:15:28.754392 139975056156416 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.22107292711734772, loss=1.6676498651504517
I0214 16:16:04.391589 139975047763712 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.1863371729850769, loss=1.607365608215332
I0214 16:16:32.240763 140144802662208 spec.py:321] Evaluating on the training split.
I0214 16:16:35.214132 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 16:20:47.279805 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 16:20:49.942852 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 16:23:54.642372 140144802662208 spec.py:349] Evaluating on the test split.
I0214 16:23:57.336172 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 16:27:05.733505 140144802662208 submission_runner.py:408] Time since start: 46007.45s, 	Step: 75480, 	{'train/accuracy': 0.6792148947715759, 'train/loss': 1.4641389846801758, 'train/bleu': 33.80764561643805, 'validation/accuracy': 0.6794211864471436, 'validation/loss': 1.4671216011047363, 'validation/bleu': 29.555809985213795, 'validation/num_examples': 3000, 'test/accuracy': 0.6943350434303284, 'test/loss': 1.3725438117980957, 'test/bleu': 29.297592249645188, 'test/num_examples': 3003, 'score': 26913.816690921783, 'total_duration': 46007.44537067413, 'accumulated_submission_time': 26913.816690921783, 'accumulated_eval_time': 19089.9792573452, 'accumulated_logging_time': 1.1101996898651123}
I0214 16:27:05.764788 139975056156416 logging_writer.py:48] [75480] accumulated_eval_time=19089.979257, accumulated_logging_time=1.110200, accumulated_submission_time=26913.816691, global_step=75480, preemption_count=0, score=26913.816691, test/accuracy=0.694335, test/bleu=29.297592, test/loss=1.372544, test/num_examples=3003, total_duration=46007.445371, train/accuracy=0.679215, train/bleu=33.807646, train/loss=1.464139, validation/accuracy=0.679421, validation/bleu=29.555810, validation/loss=1.467122, validation/num_examples=3000
I0214 16:27:13.255097 139975047763712 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.2026921957731247, loss=1.649977445602417
I0214 16:27:48.787840 139975056156416 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.2539091408252716, loss=1.6722171306610107
I0214 16:28:24.379645 139975047763712 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.20972801744937897, loss=1.66452157497406
I0214 16:28:59.989605 139975056156416 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.19803909957408905, loss=1.653673529624939
I0214 16:29:35.631007 139975047763712 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.20256882905960083, loss=1.6247738599777222
I0214 16:30:11.270645 139975056156416 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.1919429749250412, loss=1.5973821878433228
I0214 16:30:46.902071 139975047763712 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.1956489533185959, loss=1.6227041482925415
I0214 16:31:22.586019 139975056156416 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.2758807837963104, loss=1.6105620861053467
I0214 16:31:58.296007 139975047763712 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.19753529131412506, loss=1.6549867391586304
I0214 16:32:33.991164 139975056156416 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.20585276186466217, loss=1.7272281646728516
I0214 16:33:09.638067 139975047763712 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.1996009796857834, loss=1.6214048862457275
I0214 16:33:45.260277 139975056156416 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.2156420350074768, loss=1.5933600664138794
I0214 16:34:20.898350 139975047763712 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.2189551442861557, loss=1.6498854160308838
I0214 16:34:56.541628 139975056156416 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.1951592117547989, loss=1.6358708143234253
I0214 16:35:32.155808 139975047763712 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.2061573565006256, loss=1.6264961957931519
I0214 16:36:07.759693 139975056156416 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.20076732337474823, loss=1.6850454807281494
I0214 16:36:43.404847 139975047763712 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.2113114297389984, loss=1.6235800981521606
I0214 16:37:19.064654 139975056156416 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.20919443666934967, loss=1.6264561414718628
I0214 16:37:54.689096 139975047763712 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.19282034039497375, loss=1.6402864456176758
I0214 16:38:30.300293 139975056156416 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.22327010333538055, loss=1.6316633224487305
I0214 16:39:05.929002 139975047763712 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.21599730849266052, loss=1.6548188924789429
I0214 16:39:41.557899 139975056156416 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.21625955402851105, loss=1.6289297342300415
I0214 16:40:17.245869 139975047763712 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.20403485000133514, loss=1.7951713800430298
I0214 16:40:52.862614 139975056156416 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.20990429818630219, loss=1.693577766418457
I0214 16:41:05.761855 140144802662208 spec.py:321] Evaluating on the training split.
I0214 16:41:08.729074 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 16:45:38.900838 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 16:45:41.568229 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 16:49:54.301479 140144802662208 spec.py:349] Evaluating on the test split.
I0214 16:49:56.985326 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 16:54:22.164377 140144802662208 submission_runner.py:408] Time since start: 47643.88s, 	Step: 77838, 	{'train/accuracy': 0.6665636897087097, 'train/loss': 1.5482903718948364, 'train/bleu': 33.07441180399439, 'validation/accuracy': 0.68016517162323, 'validation/loss': 1.459350347518921, 'validation/bleu': 29.50371177493752, 'validation/num_examples': 3000, 'test/accuracy': 0.6929754614830017, 'test/loss': 1.3658453226089478, 'test/bleu': 29.21646204970246, 'test/num_examples': 3003, 'score': 27753.721145629883, 'total_duration': 47643.87626647949, 'accumulated_submission_time': 27753.721145629883, 'accumulated_eval_time': 19886.381717681885, 'accumulated_logging_time': 1.1533830165863037}
I0214 16:54:22.192006 139975047763712 logging_writer.py:48] [77838] accumulated_eval_time=19886.381718, accumulated_logging_time=1.153383, accumulated_submission_time=27753.721146, global_step=77838, preemption_count=0, score=27753.721146, test/accuracy=0.692975, test/bleu=29.216462, test/loss=1.365845, test/num_examples=3003, total_duration=47643.876266, train/accuracy=0.666564, train/bleu=33.074412, train/loss=1.548290, validation/accuracy=0.680165, validation/bleu=29.503712, validation/loss=1.459350, validation/num_examples=3000
I0214 16:54:44.520946 139975056156416 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.21195626258850098, loss=1.613901138305664
I0214 16:55:20.029917 139975047763712 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.19325889647006989, loss=1.579179286956787
I0214 16:55:55.639616 139975056156416 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.21058586239814758, loss=1.6931337118148804
I0214 16:56:31.279653 139975047763712 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.2288709282875061, loss=1.6783626079559326
I0214 16:57:06.884574 139975056156416 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.20633605122566223, loss=1.6476786136627197
I0214 16:57:42.484752 139975047763712 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.18808139860630035, loss=1.6148570775985718
I0214 16:58:18.116829 139975056156416 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2048538774251938, loss=1.6472578048706055
I0214 16:58:53.738673 139975047763712 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.19914625585079193, loss=1.6385729312896729
I0214 16:59:29.394551 139975056156416 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.21323144435882568, loss=1.6445709466934204
I0214 17:00:05.026213 139975047763712 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2002234011888504, loss=1.560865879058838
I0214 17:00:40.667337 139975056156416 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.2001674771308899, loss=1.5944299697875977
I0214 17:01:16.267441 139975047763712 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.20650912821292877, loss=1.616523265838623
I0214 17:01:51.928376 139975056156416 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.19859899580478668, loss=1.6760241985321045
I0214 17:02:27.613354 139975047763712 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2155180722475052, loss=1.6756888628005981
I0214 17:03:03.295416 139975056156416 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.20451977849006653, loss=1.6435868740081787
I0214 17:03:38.932540 139975047763712 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.24513521790504456, loss=1.6160725355148315
I0214 17:04:14.550822 139975056156416 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.21476620435714722, loss=1.7125242948532104
I0214 17:04:50.195616 139975047763712 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.2102789580821991, loss=1.6484590768814087
I0214 17:05:25.828502 139975056156416 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.19925691187381744, loss=1.6677123308181763
I0214 17:06:01.464213 139975047763712 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.21119588613510132, loss=1.6533740758895874
I0214 17:06:37.103337 139975056156416 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.21351028978824615, loss=1.6685714721679688
I0214 17:07:12.717300 139975047763712 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.21478939056396484, loss=1.5365509986877441
I0214 17:07:48.336637 139975056156416 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2115197479724884, loss=1.7084561586380005
I0214 17:08:22.301766 140144802662208 spec.py:321] Evaluating on the training split.
I0214 17:08:25.284723 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 17:12:50.473287 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 17:12:53.181713 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 17:16:23.258184 140144802662208 spec.py:349] Evaluating on the test split.
I0214 17:16:25.948187 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 17:19:48.448292 140144802662208 submission_runner.py:408] Time since start: 49170.16s, 	Step: 80197, 	{'train/accuracy': 0.664623498916626, 'train/loss': 1.563930869102478, 'train/bleu': 33.14200471817913, 'validation/accuracy': 0.6821985840797424, 'validation/loss': 1.4505184888839722, 'validation/bleu': 29.735981389350844, 'validation/num_examples': 3000, 'test/accuracy': 0.6975771188735962, 'test/loss': 1.3530352115631104, 'test/bleu': 29.435464433153378, 'test/num_examples': 3003, 'score': 28593.740771770477, 'total_duration': 49170.16017580032, 'accumulated_submission_time': 28593.740771770477, 'accumulated_eval_time': 20572.528188943863, 'accumulated_logging_time': 1.1913011074066162}
I0214 17:19:48.474913 139975047763712 logging_writer.py:48] [80197] accumulated_eval_time=20572.528189, accumulated_logging_time=1.191301, accumulated_submission_time=28593.740772, global_step=80197, preemption_count=0, score=28593.740772, test/accuracy=0.697577, test/bleu=29.435464, test/loss=1.353035, test/num_examples=3003, total_duration=49170.160176, train/accuracy=0.664623, train/bleu=33.142005, train/loss=1.563931, validation/accuracy=0.682199, validation/bleu=29.735981, validation/loss=1.450518, validation/num_examples=3000
I0214 17:19:49.915248 139975056156416 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.1992863118648529, loss=1.5878158807754517
I0214 17:20:25.350414 139975047763712 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.21306684613227844, loss=1.6820741891860962
I0214 17:21:00.894022 139975056156416 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.20672234892845154, loss=1.6123768091201782
I0214 17:21:36.513132 139975047763712 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.21906828880310059, loss=1.7207459211349487
I0214 17:22:12.090768 139975056156416 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.20027944445610046, loss=1.6012300252914429
I0214 17:22:47.676925 139975047763712 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.19914528727531433, loss=1.5912388563156128
I0214 17:23:23.306102 139975056156416 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.22467483580112457, loss=1.6696467399597168
I0214 17:23:58.914266 139975047763712 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.5030671954154968, loss=1.6377979516983032
I0214 17:24:34.525245 139975056156416 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.21218685805797577, loss=1.6815333366394043
I0214 17:25:10.174466 139975047763712 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.2009141743183136, loss=1.5752270221710205
I0214 17:25:45.812430 139975056156416 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.20878222584724426, loss=1.6246099472045898
I0214 17:26:21.481797 139975047763712 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.20740029215812683, loss=1.720682144165039
I0214 17:26:57.086428 139975056156416 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.21018542349338531, loss=1.6017310619354248
I0214 17:27:32.705351 139975047763712 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.1921827346086502, loss=1.5520999431610107
I0214 17:28:08.310157 139975056156416 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.21657748520374298, loss=1.653923749923706
I0214 17:28:43.922560 139975047763712 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.20143699645996094, loss=1.5993449687957764
I0214 17:29:19.518179 139975056156416 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.2364656925201416, loss=1.6180779933929443
I0214 17:29:55.108687 139975047763712 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.2070518285036087, loss=1.6626741886138916
I0214 17:30:30.686392 139975056156416 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.20885255932807922, loss=1.6854329109191895
I0214 17:31:06.314243 139975047763712 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2239096462726593, loss=1.5667140483856201
I0214 17:31:41.937467 139975056156416 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.19800785183906555, loss=1.6706428527832031
I0214 17:32:17.566606 139975047763712 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.20063216984272003, loss=1.7175041437149048
I0214 17:32:53.182478 139975056156416 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.20633885264396667, loss=1.6549724340438843
I0214 17:33:28.805143 139975047763712 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.20335647463798523, loss=1.6450165510177612
I0214 17:33:48.479713 140144802662208 spec.py:321] Evaluating on the training split.
I0214 17:33:51.456091 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 17:38:17.365919 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 17:38:20.058351 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 17:41:52.997921 140144802662208 spec.py:349] Evaluating on the test split.
I0214 17:41:55.686441 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 17:46:05.961294 140144802662208 submission_runner.py:408] Time since start: 50747.67s, 	Step: 82557, 	{'train/accuracy': 0.6770812273025513, 'train/loss': 1.4800242185592651, 'train/bleu': 34.02139815328523, 'validation/accuracy': 0.6815042495727539, 'validation/loss': 1.4466660022735596, 'validation/bleu': 29.459093235760516, 'validation/num_examples': 3000, 'test/accuracy': 0.6959037780761719, 'test/loss': 1.3536893129348755, 'test/bleu': 29.501150496830565, 'test/num_examples': 3003, 'score': 29433.655618667603, 'total_duration': 50747.67319107056, 'accumulated_submission_time': 29433.655618667603, 'accumulated_eval_time': 21310.009718179703, 'accumulated_logging_time': 1.229506015777588}
I0214 17:46:05.987345 139975056156416 logging_writer.py:48] [82557] accumulated_eval_time=21310.009718, accumulated_logging_time=1.229506, accumulated_submission_time=29433.655619, global_step=82557, preemption_count=0, score=29433.655619, test/accuracy=0.695904, test/bleu=29.501150, test/loss=1.353689, test/num_examples=3003, total_duration=50747.673191, train/accuracy=0.677081, train/bleu=34.021398, train/loss=1.480024, validation/accuracy=0.681504, validation/bleu=29.459093, validation/loss=1.446666, validation/num_examples=3000
I0214 17:46:21.602737 139975047763712 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.23650330305099487, loss=1.5650688409805298
I0214 17:46:57.086212 139975056156416 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.22640305757522583, loss=1.6304575204849243
I0214 17:47:32.660637 139975047763712 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.2026015967130661, loss=1.6361713409423828
I0214 17:48:08.259943 139975056156416 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.21002833545207977, loss=1.6229032278060913
I0214 17:48:43.888225 139975047763712 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.1978604942560196, loss=1.5734882354736328
I0214 17:49:19.504486 139975056156416 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.1983138471841812, loss=1.6289846897125244
I0214 17:49:55.115287 139975047763712 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.20781157910823822, loss=1.6624913215637207
I0214 17:50:30.798400 139975056156416 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.2052174061536789, loss=1.5580570697784424
I0214 17:51:06.447318 139975047763712 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.22021198272705078, loss=1.622937798500061
I0214 17:51:42.071620 139975056156416 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.20794549584388733, loss=1.6346824169158936
I0214 17:52:17.712965 139975047763712 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.22174212336540222, loss=1.6276648044586182
I0214 17:52:53.370476 139975056156416 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.21839047968387604, loss=1.5818036794662476
I0214 17:53:29.038118 139975047763712 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.195420503616333, loss=1.5488638877868652
I0214 17:54:04.686507 139975056156416 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.20409135520458221, loss=1.6620068550109863
I0214 17:54:40.352419 139975047763712 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.21702826023101807, loss=1.5780383348464966
I0214 17:55:16.019522 139975056156416 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.23145045340061188, loss=1.5836260318756104
I0214 17:55:51.644318 139975047763712 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.1944328248500824, loss=1.6399091482162476
I0214 17:56:27.293508 139975056156416 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.20531880855560303, loss=1.5839195251464844
I0214 17:57:02.931080 139975047763712 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.20209568738937378, loss=1.6601197719573975
I0214 17:57:38.604571 139975056156416 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.21609912812709808, loss=1.5975557565689087
I0214 17:58:14.239860 139975047763712 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.20692500472068787, loss=1.615095853805542
I0214 17:58:49.885752 139975056156416 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.2199956476688385, loss=1.5934828519821167
I0214 17:59:25.482425 139975047763712 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.19609324634075165, loss=1.5431166887283325
I0214 18:00:01.101295 139975056156416 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.20431160926818848, loss=1.6439505815505981
I0214 18:00:06.184580 140144802662208 spec.py:321] Evaluating on the training split.
I0214 18:00:09.153368 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 18:04:21.698321 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 18:04:24.404258 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 18:08:06.023544 140144802662208 spec.py:349] Evaluating on the test split.
I0214 18:08:08.716766 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 18:12:16.239468 140144802662208 submission_runner.py:408] Time since start: 52317.95s, 	Step: 84916, 	{'train/accuracy': 0.6718239784240723, 'train/loss': 1.5177863836288452, 'train/bleu': 33.53921780192777, 'validation/accuracy': 0.6841204762458801, 'validation/loss': 1.4370832443237305, 'validation/bleu': 29.954040391928764, 'validation/num_examples': 3000, 'test/accuracy': 0.698843777179718, 'test/loss': 1.3375470638275146, 'test/bleu': 29.460929968466225, 'test/num_examples': 3003, 'score': 30273.7618765831, 'total_duration': 52317.95136976242, 'accumulated_submission_time': 30273.7618765831, 'accumulated_eval_time': 22040.064561843872, 'accumulated_logging_time': 1.2669031620025635}
I0214 18:12:16.266671 139975047763712 logging_writer.py:48] [84916] accumulated_eval_time=22040.064562, accumulated_logging_time=1.266903, accumulated_submission_time=30273.761877, global_step=84916, preemption_count=0, score=30273.761877, test/accuracy=0.698844, test/bleu=29.460930, test/loss=1.337547, test/num_examples=3003, total_duration=52317.951370, train/accuracy=0.671824, train/bleu=33.539218, train/loss=1.517786, validation/accuracy=0.684120, validation/bleu=29.954040, validation/loss=1.437083, validation/num_examples=3000
I0214 18:12:46.401481 139975056156416 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.20557107031345367, loss=1.593446135520935
I0214 18:13:21.930027 139975047763712 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.19104933738708496, loss=1.564587116241455
I0214 18:13:57.566951 139975056156416 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.23556895554065704, loss=1.6565531492233276
I0214 18:14:33.190748 139975047763712 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.22654792666435242, loss=1.634645700454712
I0214 18:15:08.840116 139975056156416 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.20042799413204193, loss=1.6072663068771362
I0214 18:15:44.473488 139975047763712 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.22590351104736328, loss=1.6649807691574097
I0214 18:16:20.102366 139975056156416 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.20922398567199707, loss=1.5448027849197388
I0214 18:16:55.724479 139975047763712 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.20002351701259613, loss=1.6473428010940552
I0214 18:17:31.328497 139975056156416 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2110779881477356, loss=1.7032201290130615
I0214 18:18:06.964089 139975047763712 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.23758180439472198, loss=1.68478262424469
I0214 18:18:42.614303 139975056156416 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.20363742113113403, loss=1.6497206687927246
I0214 18:19:18.251803 139975047763712 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.21168239414691925, loss=1.5701043605804443
I0214 18:19:53.888205 139975056156416 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.21868416666984558, loss=1.5862252712249756
I0214 18:20:29.528334 139975047763712 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.20440860092639923, loss=1.5818198919296265
I0214 18:21:05.146148 139975056156416 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.21449533104896545, loss=1.7067363262176514
I0214 18:21:40.736895 139975047763712 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.21152471005916595, loss=1.6001185178756714
I0214 18:22:16.351940 139975056156416 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.2191438525915146, loss=1.5682052373886108
I0214 18:22:52.099744 139975047763712 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.23712262511253357, loss=1.619201421737671
I0214 18:23:27.742978 139975056156416 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.22329425811767578, loss=1.6034430265426636
I0214 18:24:03.401705 139975047763712 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.21228618919849396, loss=1.5774002075195312
I0214 18:24:39.075726 139975056156416 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.19650527834892273, loss=1.5635590553283691
I0214 18:25:14.710603 139975047763712 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.20277021825313568, loss=1.5531320571899414
I0214 18:25:50.313246 139975056156416 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.2204626500606537, loss=1.6141408681869507
I0214 18:26:16.399640 140144802662208 spec.py:321] Evaluating on the training split.
I0214 18:26:19.375330 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 18:30:20.339241 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 18:30:23.018302 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 18:33:26.501907 140144802662208 spec.py:349] Evaluating on the test split.
I0214 18:33:29.191874 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 18:35:47.204034 140144802662208 submission_runner.py:408] Time since start: 53728.92s, 	Step: 87275, 	{'train/accuracy': 0.6702315211296082, 'train/loss': 1.5227426290512085, 'train/bleu': 33.57381970171942, 'validation/accuracy': 0.6851620078086853, 'validation/loss': 1.4330618381500244, 'validation/bleu': 30.223012227748367, 'validation/num_examples': 3000, 'test/accuracy': 0.699785053730011, 'test/loss': 1.3343571424484253, 'test/bleu': 29.740632616160205, 'test/num_examples': 3003, 'score': 31113.805472373962, 'total_duration': 53728.91593122482, 'accumulated_submission_time': 31113.805472373962, 'accumulated_eval_time': 22610.868903636932, 'accumulated_logging_time': 1.3044085502624512}
I0214 18:35:47.230894 139975047763712 logging_writer.py:48] [87275] accumulated_eval_time=22610.868904, accumulated_logging_time=1.304409, accumulated_submission_time=31113.805472, global_step=87275, preemption_count=0, score=31113.805472, test/accuracy=0.699785, test/bleu=29.740633, test/loss=1.334357, test/num_examples=3003, total_duration=53728.915931, train/accuracy=0.670232, train/bleu=33.573820, train/loss=1.522743, validation/accuracy=0.685162, validation/bleu=30.223012, validation/loss=1.433062, validation/num_examples=3000
I0214 18:35:56.486824 139975056156416 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.320983350276947, loss=1.5852268934249878
I0214 18:36:32.010824 139975047763712 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.20910799503326416, loss=1.5842406749725342
I0214 18:37:07.637888 139975056156416 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.22774863243103027, loss=1.6524410247802734
I0214 18:37:43.298846 139975047763712 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.22072815895080566, loss=1.5624876022338867
I0214 18:38:18.898639 139975056156416 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.21386858820915222, loss=1.5957103967666626
I0214 18:38:54.593712 139975047763712 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.20128792524337769, loss=1.561124563217163
I0214 18:39:30.230152 139975056156416 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.21547017991542816, loss=1.5663769245147705
I0214 18:40:05.836139 139975047763712 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.2138759046792984, loss=1.5340157747268677
I0214 18:40:41.483320 139975056156416 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.2149287462234497, loss=1.620890736579895
I0214 18:41:17.141178 139975047763712 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.20678438246250153, loss=1.5983365774154663
I0214 18:41:52.820991 139975056156416 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.22027213871479034, loss=1.640704870223999
I0214 18:42:28.458224 139975047763712 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.21631690859794617, loss=1.6120858192443848
I0214 18:43:04.053355 139975056156416 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.2104133516550064, loss=1.6077319383621216
I0214 18:43:39.698827 139975047763712 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.19732047617435455, loss=1.510579228401184
I0214 18:44:15.327733 139975056156416 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.21943449974060059, loss=1.5948632955551147
I0214 18:44:50.986540 139975047763712 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.22040468454360962, loss=1.6360164880752563
I0214 18:45:26.600027 139975056156416 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.2138873040676117, loss=1.6305011510849
I0214 18:46:02.240998 139975047763712 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.21688681840896606, loss=1.600816249847412
I0214 18:46:37.860290 139975056156416 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.2273702174425125, loss=1.6055923700332642
I0214 18:47:13.524770 139975047763712 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.21295322477817535, loss=1.610914707183838
I0214 18:47:49.177119 139975056156416 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.23470987379550934, loss=1.5865241289138794
I0214 18:48:24.851222 139975047763712 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.20925359427928925, loss=1.559996485710144
I0214 18:49:00.519732 139975056156416 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.2123694121837616, loss=1.5358448028564453
I0214 18:49:36.134818 139975047763712 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.23767684400081635, loss=1.5494673252105713
I0214 18:49:47.252379 140144802662208 spec.py:321] Evaluating on the training split.
I0214 18:49:50.220800 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 18:54:12.165825 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 18:54:14.844026 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 18:57:17.020542 140144802662208 spec.py:349] Evaluating on the test split.
I0214 18:57:19.701874 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 19:00:09.187681 140144802662208 submission_runner.py:408] Time since start: 55190.90s, 	Step: 89633, 	{'train/accuracy': 0.6788753867149353, 'train/loss': 1.4781488180160522, 'train/bleu': 34.11023781864019, 'validation/accuracy': 0.6842444539070129, 'validation/loss': 1.4293118715286255, 'validation/bleu': 30.039313991534137, 'validation/num_examples': 3000, 'test/accuracy': 0.7009587287902832, 'test/loss': 1.3281270265579224, 'test/bleu': 29.580239725008756, 'test/num_examples': 3003, 'score': 31953.73739719391, 'total_duration': 55190.8995449543, 'accumulated_submission_time': 31953.73739719391, 'accumulated_eval_time': 23232.804119586945, 'accumulated_logging_time': 1.3434412479400635}
I0214 19:00:09.221575 139975056156416 logging_writer.py:48] [89633] accumulated_eval_time=23232.804120, accumulated_logging_time=1.343441, accumulated_submission_time=31953.737397, global_step=89633, preemption_count=0, score=31953.737397, test/accuracy=0.700959, test/bleu=29.580240, test/loss=1.328127, test/num_examples=3003, total_duration=55190.899545, train/accuracy=0.678875, train/bleu=34.110238, train/loss=1.478149, validation/accuracy=0.684244, validation/bleu=30.039314, validation/loss=1.429312, validation/num_examples=3000
I0214 19:00:33.356762 139975047763712 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.27144908905029297, loss=1.612043857574463
I0214 19:01:08.849162 139975056156416 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.20804016292095184, loss=1.5692201852798462
I0214 19:01:44.478304 139975047763712 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.20118281245231628, loss=1.5586093664169312
I0214 19:02:20.086288 139975056156416 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.22605495154857635, loss=1.6649863719940186
I0214 19:02:55.712504 139975047763712 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.2347458302974701, loss=1.6165075302124023
I0214 19:03:31.377364 139975056156416 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.20745038986206055, loss=1.640568733215332
I0214 19:04:07.016089 139975047763712 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.2073320895433426, loss=1.6438703536987305
I0214 19:04:42.642076 139975056156416 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.21694496273994446, loss=1.546789526939392
I0214 19:05:18.275322 139975047763712 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.22030310332775116, loss=1.5225919485092163
I0214 19:05:53.930904 139975056156416 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.24242889881134033, loss=1.6002708673477173
I0214 19:06:29.566036 139975047763712 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.20629264414310455, loss=1.675990343093872
I0214 19:07:05.214401 139975056156416 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.2449713498353958, loss=1.617110252380371
I0214 19:07:40.845781 139975047763712 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.20458541810512543, loss=1.6122533082962036
I0214 19:08:16.477850 139975056156416 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.20707394182682037, loss=1.5835976600646973
I0214 19:08:52.137542 139975047763712 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.209201842546463, loss=1.5909329652786255
I0214 19:09:27.799587 139975056156416 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.23694777488708496, loss=1.6304103136062622
I0214 19:10:03.469024 139975047763712 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.2135588526725769, loss=1.643308401107788
I0214 19:10:39.104846 139975056156416 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.20776790380477905, loss=1.5361380577087402
I0214 19:11:14.759798 139975047763712 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.21011458337306976, loss=1.551192045211792
I0214 19:11:50.385129 139975056156416 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.20817089080810547, loss=1.5808292627334595
I0214 19:12:26.024928 139975047763712 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.23164230585098267, loss=1.6449509859085083
I0214 19:13:01.623865 139975056156416 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.22847427427768707, loss=1.5895682573318481
I0214 19:13:37.228277 139975047763712 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.21444647014141083, loss=1.5422688722610474
I0214 19:14:09.365187 140144802662208 spec.py:321] Evaluating on the training split.
I0214 19:14:12.335991 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 19:18:39.258282 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 19:18:41.937171 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 19:21:33.872562 140144802662208 spec.py:349] Evaluating on the test split.
I0214 19:21:36.560894 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 19:24:19.987036 140144802662208 submission_runner.py:408] Time since start: 56641.70s, 	Step: 91992, 	{'train/accuracy': 0.674981951713562, 'train/loss': 1.4979826211929321, 'train/bleu': 33.107271415973, 'validation/accuracy': 0.6863523125648499, 'validation/loss': 1.421521544456482, 'validation/bleu': 29.94934206485915, 'validation/num_examples': 3000, 'test/accuracy': 0.7017953991889954, 'test/loss': 1.3203881978988647, 'test/bleu': 30.07968705955243, 'test/num_examples': 3003, 'score': 32793.78981876373, 'total_duration': 56641.69893527031, 'accumulated_submission_time': 32793.78981876373, 'accumulated_eval_time': 23843.425915002823, 'accumulated_logging_time': 1.3887052536010742}
I0214 19:24:20.015276 139975056156416 logging_writer.py:48] [91992] accumulated_eval_time=23843.425915, accumulated_logging_time=1.388705, accumulated_submission_time=32793.789819, global_step=91992, preemption_count=0, score=32793.789819, test/accuracy=0.701795, test/bleu=30.079687, test/loss=1.320388, test/num_examples=3003, total_duration=56641.698935, train/accuracy=0.674982, train/bleu=33.107271, train/loss=1.497983, validation/accuracy=0.686352, validation/bleu=29.949342, validation/loss=1.421522, validation/num_examples=3000
I0214 19:24:23.219036 139975047763712 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.21774201095104218, loss=1.6358245611190796
I0214 19:24:58.710558 139975056156416 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.2121850997209549, loss=1.588313102722168
I0214 19:25:34.286596 139975047763712 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.3920176923274994, loss=1.6798229217529297
I0214 19:26:09.885621 139975056156416 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21365921199321747, loss=1.5619120597839355
I0214 19:26:45.491649 139975047763712 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.21970990300178528, loss=1.679800271987915
I0214 19:27:21.123891 139975056156416 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.2219519466161728, loss=1.5499614477157593
I0214 19:27:56.758072 139975047763712 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.20000220835208893, loss=1.5112369060516357
I0214 19:28:32.391019 139975056156416 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.22040480375289917, loss=1.6100208759307861
I0214 19:29:08.002881 139975047763712 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.23476453125476837, loss=1.642615795135498
I0214 19:29:43.620832 139975056156416 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.2336726188659668, loss=1.6449253559112549
I0214 19:30:19.238116 139975047763712 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.32548800110816956, loss=1.6395809650421143
I0214 19:30:54.904403 139975056156416 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.21565133333206177, loss=1.587515950202942
I0214 19:31:30.518043 139975047763712 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.2199176549911499, loss=1.556565284729004
I0214 19:32:06.145544 139975056156416 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.211434006690979, loss=1.5121686458587646
I0214 19:32:41.806641 139975047763712 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.3247166872024536, loss=1.5921841859817505
I0214 19:33:17.442029 139975056156416 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.2107064574956894, loss=1.5114697217941284
I0214 19:33:53.095898 139975047763712 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.21328409016132355, loss=1.5500240325927734
I0214 19:34:28.736184 139975056156416 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.2065339833498001, loss=1.6034471988677979
I0214 19:35:04.363760 139975047763712 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.21548838913440704, loss=1.5130926370620728
I0214 19:35:40.034532 139975056156416 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.23891353607177734, loss=1.578769564628601
I0214 19:36:15.713190 139975047763712 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.21548157930374146, loss=1.6024963855743408
I0214 19:36:51.347625 139975056156416 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.21324948966503143, loss=1.6723583936691284
I0214 19:37:27.014446 139975047763712 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.22398045659065247, loss=1.5483288764953613
I0214 19:38:02.616394 139975056156416 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.20104004442691803, loss=1.5172290802001953
I0214 19:38:20.159139 140144802662208 spec.py:321] Evaluating on the training split.
I0214 19:38:23.134123 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 19:42:43.248572 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 19:42:45.932126 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 19:45:22.903625 140144802662208 spec.py:349] Evaluating on the test split.
I0214 19:45:25.581171 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 19:48:22.826616 140144802662208 submission_runner.py:408] Time since start: 58084.54s, 	Step: 94351, 	{'train/accuracy': 0.6841570734977722, 'train/loss': 1.4367806911468506, 'train/bleu': 34.501937700227685, 'validation/accuracy': 0.6876170039176941, 'validation/loss': 1.4109599590301514, 'validation/bleu': 30.217930488187925, 'validation/num_examples': 3000, 'test/accuracy': 0.704317033290863, 'test/loss': 1.3113782405853271, 'test/bleu': 30.00001747016476, 'test/num_examples': 3003, 'score': 33633.84631562233, 'total_duration': 58084.538499593735, 'accumulated_submission_time': 33633.84631562233, 'accumulated_eval_time': 24446.093323946, 'accumulated_logging_time': 1.426877498626709}
I0214 19:48:22.855432 139975047763712 logging_writer.py:48] [94351] accumulated_eval_time=24446.093324, accumulated_logging_time=1.426877, accumulated_submission_time=33633.846316, global_step=94351, preemption_count=0, score=33633.846316, test/accuracy=0.704317, test/bleu=30.000017, test/loss=1.311378, test/num_examples=3003, total_duration=58084.538500, train/accuracy=0.684157, train/bleu=34.501938, train/loss=1.436781, validation/accuracy=0.687617, validation/bleu=30.217930, validation/loss=1.410960, validation/num_examples=3000
I0214 19:48:40.576761 139975056156416 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.21529699862003326, loss=1.5442132949829102
I0214 19:49:16.065710 139975047763712 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.2124004065990448, loss=1.5353081226348877
I0214 19:49:51.623502 139975056156416 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.23971886932849884, loss=1.5863088369369507
I0214 19:50:27.276452 139975047763712 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.22621877491474152, loss=1.5315134525299072
I0214 19:51:02.865568 139975056156416 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.21952137351036072, loss=1.5559360980987549
I0214 19:51:38.467527 139975047763712 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.2102375030517578, loss=1.636401891708374
I0214 19:52:14.080359 139975056156416 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.21497978270053864, loss=1.5343902111053467
I0214 19:52:49.714063 139975047763712 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.22056780755519867, loss=1.5300909280776978
I0214 19:53:25.365838 139975056156416 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.21842670440673828, loss=1.5601359605789185
I0214 19:54:01.021258 139975047763712 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.23205316066741943, loss=1.6392707824707031
I0214 19:54:36.661568 139975056156416 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.2179080992937088, loss=1.5734491348266602
I0214 19:55:12.278739 139975047763712 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.20753072202205658, loss=1.5468497276306152
I0214 19:55:47.913196 139975056156416 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.22496278584003448, loss=1.582384467124939
I0214 19:56:23.542260 139975047763712 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.23472484946250916, loss=1.6339839696884155
I0214 19:56:59.197482 139975056156416 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.21189889311790466, loss=1.4898110628128052
I0214 19:57:34.839059 139975047763712 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.214897021651268, loss=1.513148546218872
I0214 19:58:10.482252 139975056156416 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.22948071360588074, loss=1.5887863636016846
I0214 19:58:46.098712 139975047763712 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.2186092585325241, loss=1.5866012573242188
I0214 19:59:21.718484 139975056156416 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.2186850607395172, loss=1.4470759630203247
I0214 19:59:57.365427 139975047763712 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.2184222936630249, loss=1.5502424240112305
I0214 20:00:33.058059 139975056156416 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.2074880450963974, loss=1.53145170211792
I0214 20:01:08.664893 139975047763712 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.21254029870033264, loss=1.5543992519378662
I0214 20:01:44.284183 139975056156416 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.21263864636421204, loss=1.6098600625991821
I0214 20:02:19.926776 139975047763712 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.23040859401226044, loss=1.538842797279358
I0214 20:02:22.864178 140144802662208 spec.py:321] Evaluating on the training split.
I0214 20:02:25.833192 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 20:06:53.689498 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 20:06:56.391716 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 20:10:15.008885 140144802662208 spec.py:349] Evaluating on the test split.
I0214 20:10:17.698428 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 20:13:38.342807 140144802662208 submission_runner.py:408] Time since start: 59600.05s, 	Step: 96710, 	{'train/accuracy': 0.6783720254898071, 'train/loss': 1.4733608961105347, 'train/bleu': 34.322575911748935, 'validation/accuracy': 0.6874558329582214, 'validation/loss': 1.4094278812408447, 'validation/bleu': 30.34860323432693, 'validation/num_examples': 3000, 'test/accuracy': 0.7056649923324585, 'test/loss': 1.3058409690856934, 'test/bleu': 30.114120610001066, 'test/num_examples': 3003, 'score': 34473.7677295208, 'total_duration': 59600.05470824242, 'accumulated_submission_time': 34473.7677295208, 'accumulated_eval_time': 25121.571900844574, 'accumulated_logging_time': 1.4660224914550781}
I0214 20:13:38.371381 139975056156416 logging_writer.py:48] [96710] accumulated_eval_time=25121.571901, accumulated_logging_time=1.466022, accumulated_submission_time=34473.767730, global_step=96710, preemption_count=0, score=34473.767730, test/accuracy=0.705665, test/bleu=30.114121, test/loss=1.305841, test/num_examples=3003, total_duration=59600.054708, train/accuracy=0.678372, train/bleu=34.322576, train/loss=1.473361, validation/accuracy=0.687456, validation/bleu=30.348603, validation/loss=1.409428, validation/num_examples=3000
I0214 20:14:10.596694 139975047763712 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.22621659934520721, loss=1.5765691995620728
I0214 20:14:46.169029 139975056156416 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.2181723564863205, loss=1.5697681903839111
I0214 20:15:21.798342 139975047763712 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.22776836156845093, loss=1.5628641843795776
I0214 20:15:57.404129 139975056156416 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.21622852981090546, loss=1.478654384613037
I0214 20:16:32.998220 139975047763712 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.22252507507801056, loss=1.576218605041504
I0214 20:17:08.621438 139975056156416 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.2643265724182129, loss=1.6059105396270752
I0214 20:17:44.236201 139975047763712 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.21007241308689117, loss=1.5250898599624634
I0214 20:18:19.887508 139975056156416 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.21971029043197632, loss=1.5022202730178833
I0214 20:18:55.505603 139975047763712 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.21914492547512054, loss=1.5768592357635498
I0214 20:19:31.148073 139975056156416 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.2290298193693161, loss=1.557493805885315
I0214 20:20:06.791702 139975047763712 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.2164975255727768, loss=1.5872905254364014
I0214 20:20:42.390645 139975056156416 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.22578065097332, loss=1.5557092428207397
I0214 20:21:18.009024 139975047763712 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.20861268043518066, loss=1.5383583307266235
I0214 20:21:53.618781 139975056156416 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.2310032993555069, loss=1.597435474395752
I0214 20:22:29.232443 139975047763712 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.21511045098304749, loss=1.5435831546783447
I0214 20:23:04.916113 139975056156416 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.22512926161289215, loss=1.534480094909668
I0214 20:23:40.592580 139975047763712 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.20543716847896576, loss=1.5010573863983154
I0214 20:24:16.209701 139975056156416 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.2138112634420395, loss=1.5368356704711914
I0214 20:24:51.842316 139975047763712 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.254865825176239, loss=1.5777199268341064
I0214 20:25:27.458447 139975056156416 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.21867381036281586, loss=1.5538438558578491
I0214 20:26:03.091829 139975047763712 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.21273228526115417, loss=1.4802110195159912
I0214 20:26:38.689106 139975056156416 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.22600863873958588, loss=1.5803648233413696
I0214 20:27:14.332364 139975047763712 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.22512845695018768, loss=1.5034366846084595
I0214 20:27:38.636719 140144802662208 spec.py:321] Evaluating on the training split.
I0214 20:27:41.607002 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 20:31:48.990017 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 20:31:51.665817 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 20:34:19.432092 140144802662208 spec.py:349] Evaluating on the test split.
I0214 20:34:22.124274 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 20:36:56.020629 140144802662208 submission_runner.py:408] Time since start: 60997.73s, 	Step: 99070, 	{'train/accuracy': 0.6781623959541321, 'train/loss': 1.4802968502044678, 'train/bleu': 33.968795052975345, 'validation/accuracy': 0.6889064908027649, 'validation/loss': 1.4011088609695435, 'validation/bleu': 30.316761453508192, 'validation/num_examples': 3000, 'test/accuracy': 0.7047004699707031, 'test/loss': 1.3031493425369263, 'test/bleu': 30.178915366389266, 'test/num_examples': 3003, 'score': 35313.945922613144, 'total_duration': 60997.73252558708, 'accumulated_submission_time': 35313.945922613144, 'accumulated_eval_time': 25678.955759763718, 'accumulated_logging_time': 1.5046508312225342}
I0214 20:36:56.052568 139975056156416 logging_writer.py:48] [99070] accumulated_eval_time=25678.955760, accumulated_logging_time=1.504651, accumulated_submission_time=35313.945923, global_step=99070, preemption_count=0, score=35313.945923, test/accuracy=0.704700, test/bleu=30.178915, test/loss=1.303149, test/num_examples=3003, total_duration=60997.732526, train/accuracy=0.678162, train/bleu=33.968795, train/loss=1.480297, validation/accuracy=0.688906, validation/bleu=30.316761, validation/loss=1.401109, validation/num_examples=3000
I0214 20:37:07.056488 139975047763712 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.23119691014289856, loss=1.5539624691009521
I0214 20:37:42.526102 139975056156416 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.23277783393859863, loss=1.4920635223388672
I0214 20:38:18.102956 139975047763712 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.22584238648414612, loss=1.5611698627471924
I0214 20:38:53.725467 139975056156416 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.24064509570598602, loss=1.5318070650100708
I0214 20:39:29.350541 139975047763712 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.33418792486190796, loss=1.5099115371704102
I0214 20:40:05.003101 139975056156416 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.22003334760665894, loss=1.5337923765182495
I0214 20:40:40.621081 139975047763712 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.22528468072414398, loss=1.4859275817871094
I0214 20:41:16.257058 139975056156416 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.2267066240310669, loss=1.4970412254333496
I0214 20:41:51.899822 139975047763712 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.2385488599538803, loss=1.56563138961792
I0214 20:42:27.554656 139975056156416 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.21496932208538055, loss=1.557597041130066
I0214 20:43:03.176872 139975047763712 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.22658155858516693, loss=1.5752036571502686
I0214 20:43:38.771614 139975056156416 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.22448541224002838, loss=1.5448487997055054
I0214 20:44:14.366452 139975047763712 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.21681037545204163, loss=1.492982029914856
I0214 20:44:49.994899 139975056156416 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.2284633219242096, loss=1.5428951978683472
I0214 20:45:25.635832 139975047763712 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.23982135951519012, loss=1.5492827892303467
I0214 20:46:01.248509 139975056156416 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.20751138031482697, loss=1.489035964012146
I0214 20:46:36.867983 139975047763712 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.2160295695066452, loss=1.522428035736084
I0214 20:47:12.460116 139975056156416 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.24179215729236603, loss=1.5435893535614014
I0214 20:47:48.045823 139975047763712 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.21784450113773346, loss=1.5445231199264526
I0214 20:48:23.662243 139975056156416 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.23756523430347443, loss=1.5594520568847656
I0214 20:48:59.276190 139975047763712 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.21826264262199402, loss=1.4996026754379272
I0214 20:49:34.905299 139975056156416 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.22892658412456512, loss=1.52572500705719
I0214 20:50:10.521594 139975047763712 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.21412979066371918, loss=1.5037697553634644
I0214 20:50:46.137581 139975056156416 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.21199391782283783, loss=1.525443196296692
I0214 20:50:56.188698 140144802662208 spec.py:321] Evaluating on the training split.
I0214 20:50:59.162662 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 20:55:10.092026 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 20:55:12.766316 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 20:58:39.100360 140144802662208 spec.py:349] Evaluating on the test split.
I0214 20:58:41.795874 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 21:01:49.581964 140144802662208 submission_runner.py:408] Time since start: 62491.29s, 	Step: 101430, 	{'train/accuracy': 0.6870272159576416, 'train/loss': 1.419952392578125, 'train/bleu': 34.51947111345885, 'validation/accuracy': 0.6896132826805115, 'validation/loss': 1.3957315683364868, 'validation/bleu': 30.235381427345153, 'validation/num_examples': 3000, 'test/accuracy': 0.7055255770683289, 'test/loss': 1.2928892374038696, 'test/bleu': 30.34395915705975, 'test/num_examples': 3003, 'score': 36153.992918252945, 'total_duration': 62491.293865680695, 'accumulated_submission_time': 36153.992918252945, 'accumulated_eval_time': 26332.348979234695, 'accumulated_logging_time': 1.5485038757324219}
I0214 21:01:49.610754 139975047763712 logging_writer.py:48] [101430] accumulated_eval_time=26332.348979, accumulated_logging_time=1.548504, accumulated_submission_time=36153.992918, global_step=101430, preemption_count=0, score=36153.992918, test/accuracy=0.705526, test/bleu=30.343959, test/loss=1.292889, test/num_examples=3003, total_duration=62491.293866, train/accuracy=0.687027, train/bleu=34.519471, train/loss=1.419952, validation/accuracy=0.689613, validation/bleu=30.235381, validation/loss=1.395732, validation/num_examples=3000
I0214 21:02:14.772187 139975056156416 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.22658249735832214, loss=1.5101890563964844
I0214 21:02:50.264398 139975047763712 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.21615830063819885, loss=1.6018198728561401
I0214 21:03:25.843574 139975056156416 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.2376132309436798, loss=1.522221326828003
I0214 21:04:01.502477 139975047763712 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.22786694765090942, loss=1.5068732500076294
I0214 21:04:37.134264 139975056156416 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.2267853170633316, loss=1.6176230907440186
I0214 21:05:12.788798 139975047763712 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.21896618604660034, loss=1.51659095287323
I0214 21:05:48.457796 139975056156416 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.2493438422679901, loss=1.4873346090316772
I0214 21:06:24.101384 139975047763712 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.218610942363739, loss=1.5548179149627686
I0214 21:06:59.747241 139975056156416 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.22711795568466187, loss=1.6460204124450684
I0214 21:07:35.381914 139975047763712 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.21949662268161774, loss=1.5919326543807983
I0214 21:08:10.997317 139975056156416 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.23634746670722961, loss=1.5072640180587769
I0214 21:08:46.631837 139975047763712 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.21253514289855957, loss=1.4532567262649536
I0214 21:09:22.267223 139975056156416 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.23036538064479828, loss=1.4803427457809448
I0214 21:09:57.893863 139975047763712 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.23212234675884247, loss=1.4750819206237793
I0214 21:10:33.564693 139975056156416 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.21592485904693604, loss=1.567801833152771
I0214 21:11:09.226935 139975047763712 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.23431552946567535, loss=1.5212658643722534
I0214 21:11:44.915079 139975056156416 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.22685950994491577, loss=1.4712632894515991
I0214 21:12:20.563982 139975047763712 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.20790277421474457, loss=1.4081813097000122
I0214 21:12:56.198541 139975056156416 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.20878201723098755, loss=1.4443038702011108
I0214 21:13:31.819653 139975047763712 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.21643999218940735, loss=1.5362458229064941
I0214 21:14:07.460126 139975056156416 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.23402263224124908, loss=1.5015759468078613
I0214 21:14:43.239392 139975047763712 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.2185763567686081, loss=1.5062752962112427
I0214 21:15:18.887469 139975056156416 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.2414335310459137, loss=1.5391520261764526
I0214 21:15:49.595478 140144802662208 spec.py:321] Evaluating on the training split.
I0214 21:15:52.580505 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 21:19:42.579941 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 21:19:45.281329 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 21:22:45.373333 140144802662208 spec.py:349] Evaluating on the test split.
I0214 21:22:48.079412 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 21:25:54.853948 140144802662208 submission_runner.py:408] Time since start: 63936.57s, 	Step: 103788, 	{'train/accuracy': 0.6809467077255249, 'train/loss': 1.462726354598999, 'train/bleu': 34.7810708399674, 'validation/accuracy': 0.6901588439941406, 'validation/loss': 1.3913047313690186, 'validation/bleu': 30.359409154734198, 'validation/num_examples': 3000, 'test/accuracy': 0.7094416618347168, 'test/loss': 1.2849253416061401, 'test/bleu': 30.334914627920924, 'test/num_examples': 3003, 'score': 36993.887571811676, 'total_duration': 63936.5658159256, 'accumulated_submission_time': 36993.887571811676, 'accumulated_eval_time': 26937.60739517212, 'accumulated_logging_time': 1.5876469612121582}
I0214 21:25:54.894820 139975047763712 logging_writer.py:48] [103788] accumulated_eval_time=26937.607395, accumulated_logging_time=1.587647, accumulated_submission_time=36993.887572, global_step=103788, preemption_count=0, score=36993.887572, test/accuracy=0.709442, test/bleu=30.334915, test/loss=1.284925, test/num_examples=3003, total_duration=63936.565816, train/accuracy=0.680947, train/bleu=34.781071, train/loss=1.462726, validation/accuracy=0.690159, validation/bleu=30.359409, validation/loss=1.391305, validation/num_examples=3000
I0214 21:25:59.528278 139975056156416 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22357037663459778, loss=1.5242249965667725
I0214 21:26:35.011345 139975047763712 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.2344549596309662, loss=1.5877357721328735
I0214 21:27:10.576687 139975056156416 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.23900443315505981, loss=1.5346577167510986
I0214 21:27:46.147941 139975047763712 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.2197597771883011, loss=1.535330891609192
I0214 21:28:21.767806 139975056156416 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.23600943386554718, loss=1.5608422756195068
I0214 21:28:57.369739 139975047763712 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.8027412295341492, loss=1.5728520154953003
I0214 21:29:33.012321 139975056156416 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.20882189273834229, loss=1.4769315719604492
I0214 21:30:08.636432 139975047763712 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.297670841217041, loss=1.5498278141021729
I0214 21:30:44.296157 139975056156416 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.25176918506622314, loss=1.5032641887664795
I0214 21:31:20.112690 139975047763712 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.22265399992465973, loss=1.5353236198425293
I0214 21:31:55.733407 139975056156416 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.2234693467617035, loss=1.5661894083023071
I0214 21:32:31.354397 139975047763712 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.22948198020458221, loss=1.533923625946045
I0214 21:33:07.018419 139975056156416 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.22548717260360718, loss=1.576576828956604
I0214 21:33:42.646388 139975047763712 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.22800394892692566, loss=1.5291781425476074
I0214 21:34:18.276997 139975056156416 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.2363845854997635, loss=1.576893925666809
I0214 21:34:53.905385 139975047763712 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.22705887258052826, loss=1.4705451726913452
I0214 21:35:29.521333 139975056156416 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.21176448464393616, loss=1.4608705043792725
I0214 21:36:05.164090 139975047763712 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.23679278790950775, loss=1.4931087493896484
I0214 21:36:40.772191 139975056156416 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.23192860186100006, loss=1.5534398555755615
I0214 21:37:16.423428 139975047763712 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.20968382060527802, loss=1.5017321109771729
I0214 21:37:52.042134 139975056156416 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.2328394651412964, loss=1.553636908531189
I0214 21:38:27.662653 139975047763712 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.21905618906021118, loss=1.5198543071746826
I0214 21:39:03.269460 139975056156416 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.22591067850589752, loss=1.5136420726776123
I0214 21:39:38.921569 139975047763712 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.22582656145095825, loss=1.6123323440551758
I0214 21:39:55.034607 140144802662208 spec.py:321] Evaluating on the training split.
I0214 21:39:58.019436 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 21:43:56.949935 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 21:43:59.651431 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 21:46:50.694702 140144802662208 spec.py:349] Evaluating on the test split.
I0214 21:46:53.389913 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 21:49:22.516303 140144802662208 submission_runner.py:408] Time since start: 65344.23s, 	Step: 106147, 	{'train/accuracy': 0.687261164188385, 'train/loss': 1.4270676374435425, 'train/bleu': 34.439128262689984, 'validation/accuracy': 0.6915971040725708, 'validation/loss': 1.3868253231048584, 'validation/bleu': 30.57444599493856, 'validation/num_examples': 3000, 'test/accuracy': 0.7098251581192017, 'test/loss': 1.2795196771621704, 'test/bleu': 30.52362190743765, 'test/num_examples': 3003, 'score': 37833.93421292305, 'total_duration': 65344.22820472717, 'accumulated_submission_time': 37833.93421292305, 'accumulated_eval_time': 27505.089040517807, 'accumulated_logging_time': 1.6410527229309082}
I0214 21:49:22.546444 139975056156416 logging_writer.py:48] [106147] accumulated_eval_time=27505.089041, accumulated_logging_time=1.641053, accumulated_submission_time=37833.934213, global_step=106147, preemption_count=0, score=37833.934213, test/accuracy=0.709825, test/bleu=30.523622, test/loss=1.279520, test/num_examples=3003, total_duration=65344.228205, train/accuracy=0.687261, train/bleu=34.439128, train/loss=1.427068, validation/accuracy=0.691597, validation/bleu=30.574446, validation/loss=1.386825, validation/num_examples=3000
I0214 21:49:41.705263 139975047763712 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.21733756363391876, loss=1.5503870248794556
I0214 21:50:17.211555 139975056156416 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.23216545581817627, loss=1.5693796873092651
I0214 21:50:52.803731 139975047763712 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.22844262421131134, loss=1.5284805297851562
I0214 21:51:28.416404 139975056156416 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.2410893440246582, loss=1.515183448791504
I0214 21:52:04.033444 139975047763712 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.21739868819713593, loss=1.4699292182922363
I0214 21:52:39.652412 139975056156416 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.2368319034576416, loss=1.4996315240859985
I0214 21:53:15.285993 139975047763712 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.22445599734783173, loss=1.5519734621047974
I0214 21:53:50.913645 139975056156416 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.21769124269485474, loss=1.482853651046753
I0214 21:54:26.542802 139975047763712 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.2346930354833603, loss=1.59628164768219
I0214 21:55:02.204262 139975056156416 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.22963666915893555, loss=1.4593102931976318
I0214 21:55:37.854665 139975047763712 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.2160632163286209, loss=1.4719306230545044
I0214 21:56:13.486265 139975056156416 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.23063147068023682, loss=1.4854992628097534
I0214 21:56:49.127650 139975047763712 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.23177236318588257, loss=1.5445948839187622
I0214 21:57:24.777239 139975056156416 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.22577226161956787, loss=1.4802587032318115
I0214 21:58:00.400203 139975047763712 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.2242831289768219, loss=1.4215468168258667
I0214 21:58:36.073844 139975056156416 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.23546475172042847, loss=1.5383684635162354
I0214 21:59:11.754903 139975047763712 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.24092203378677368, loss=1.470814824104309
I0214 21:59:47.388780 139975056156416 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.228067085146904, loss=1.4651437997817993
I0214 22:00:23.030727 139975047763712 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.2316174954175949, loss=1.554287075996399
I0214 22:00:58.635041 139975056156416 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.2213810682296753, loss=1.5140318870544434
I0214 22:01:34.263856 139975047763712 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.24346642196178436, loss=1.5629361867904663
I0214 22:02:09.893692 139975056156416 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.23128767311573029, loss=1.5422121286392212
I0214 22:02:45.516169 139975047763712 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.23341882228851318, loss=1.4692027568817139
I0214 22:03:21.142197 139975056156416 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.25135916471481323, loss=1.5186399221420288
I0214 22:03:22.653610 140144802662208 spec.py:321] Evaluating on the training split.
I0214 22:03:25.636406 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 22:07:51.405892 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 22:07:54.097269 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 22:10:58.033757 140144802662208 spec.py:349] Evaluating on the test split.
I0214 22:11:00.719362 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 22:14:07.009294 140144802662208 submission_runner.py:408] Time since start: 66828.72s, 	Step: 108506, 	{'train/accuracy': 0.6925578117370605, 'train/loss': 1.3921180963516235, 'train/bleu': 35.037549363106514, 'validation/accuracy': 0.6933826208114624, 'validation/loss': 1.3761391639709473, 'validation/bleu': 30.82730396732934, 'validation/num_examples': 3000, 'test/accuracy': 0.7099180817604065, 'test/loss': 1.2736002206802368, 'test/bleu': 30.546075277934015, 'test/num_examples': 3003, 'score': 38673.95379757881, 'total_duration': 66828.72117614746, 'accumulated_submission_time': 38673.95379757881, 'accumulated_eval_time': 28149.444666147232, 'accumulated_logging_time': 1.6813023090362549}
I0214 22:14:07.040440 139975047763712 logging_writer.py:48] [108506] accumulated_eval_time=28149.444666, accumulated_logging_time=1.681302, accumulated_submission_time=38673.953798, global_step=108506, preemption_count=0, score=38673.953798, test/accuracy=0.709918, test/bleu=30.546075, test/loss=1.273600, test/num_examples=3003, total_duration=66828.721176, train/accuracy=0.692558, train/bleu=35.037549, train/loss=1.392118, validation/accuracy=0.693383, validation/bleu=30.827304, validation/loss=1.376139, validation/num_examples=3000
I0214 22:14:40.718149 139975056156416 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.22173337638378143, loss=1.4934231042861938
I0214 22:15:16.292759 139975047763712 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.22718608379364014, loss=1.4567923545837402
I0214 22:15:51.911463 139975056156416 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.22620633244514465, loss=1.4322806596755981
I0214 22:16:27.554974 139975047763712 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.22286492586135864, loss=1.4606878757476807
I0214 22:17:03.189440 139975056156416 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.23233801126480103, loss=1.5615047216415405
I0214 22:17:38.803587 139975047763712 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.22908471524715424, loss=1.4801477193832397
I0214 22:18:14.437729 139975056156416 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.2281854897737503, loss=1.4935685396194458
I0214 22:18:50.068818 139975047763712 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.22543840110301971, loss=1.4580470323562622
I0214 22:19:25.712884 139975056156416 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.22852997481822968, loss=1.4113911390304565
I0214 22:20:01.319847 139975047763712 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.22563982009887695, loss=1.519403338432312
I0214 22:20:36.933425 139975056156416 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.22594986855983734, loss=1.4673569202423096
I0214 22:21:12.547274 139975047763712 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.22267232835292816, loss=1.4882879257202148
I0214 22:21:48.207310 139975056156416 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.2296309769153595, loss=1.4603263139724731
I0214 22:22:23.808556 139975047763712 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.2373042106628418, loss=1.5220133066177368
I0214 22:22:59.453139 139975056156416 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.21572060883045197, loss=1.4277148246765137
I0214 22:23:35.080487 139975047763712 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.22681817412376404, loss=1.505111813545227
I0214 22:24:10.739065 139975056156416 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.23747920989990234, loss=1.536393404006958
I0214 22:24:46.370497 139975047763712 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.22077494859695435, loss=1.4385486841201782
I0214 22:25:21.991342 139975056156416 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.23089240491390228, loss=1.4766682386398315
I0214 22:25:57.622911 139975047763712 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.23118269443511963, loss=1.489396095275879
I0214 22:26:33.237742 139975056156416 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.23205755650997162, loss=1.4126060009002686
I0214 22:27:08.894441 139975047763712 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.23294247686862946, loss=1.465057611465454
I0214 22:27:44.542647 139975056156416 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.23456302285194397, loss=1.5298690795898438
I0214 22:28:07.092455 140144802662208 spec.py:321] Evaluating on the training split.
I0214 22:28:10.093248 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 22:32:25.471780 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 22:32:28.163371 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 22:35:23.823777 140144802662208 spec.py:349] Evaluating on the test split.
I0214 22:35:26.523991 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 22:38:04.960392 140144802662208 submission_runner.py:408] Time since start: 68266.67s, 	Step: 110865, 	{'train/accuracy': 0.6871196031570435, 'train/loss': 1.4239205121994019, 'train/bleu': 34.8735757618847, 'validation/accuracy': 0.6927750110626221, 'validation/loss': 1.379041075706482, 'validation/bleu': 30.659466710577252, 'validation/num_examples': 3000, 'test/accuracy': 0.7104293704032898, 'test/loss': 1.2698314189910889, 'test/bleu': 30.823263329626307, 'test/num_examples': 3003, 'score': 39513.917432546616, 'total_duration': 68266.67229032516, 'accumulated_submission_time': 39513.917432546616, 'accumulated_eval_time': 28747.312561511993, 'accumulated_logging_time': 1.722498893737793}
I0214 22:38:04.991112 139975047763712 logging_writer.py:48] [110865] accumulated_eval_time=28747.312562, accumulated_logging_time=1.722499, accumulated_submission_time=39513.917433, global_step=110865, preemption_count=0, score=39513.917433, test/accuracy=0.710429, test/bleu=30.823263, test/loss=1.269831, test/num_examples=3003, total_duration=68266.672290, train/accuracy=0.687120, train/bleu=34.873576, train/loss=1.423921, validation/accuracy=0.692775, validation/bleu=30.659467, validation/loss=1.379041, validation/num_examples=3000
I0214 22:38:17.788802 139975056156416 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.22480222582817078, loss=1.5480268001556396
I0214 22:38:53.308410 139975047763712 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.2188621163368225, loss=1.542702317237854
I0214 22:39:28.912416 139975056156416 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.2241748720407486, loss=1.5310264825820923
I0214 22:40:04.558294 139975047763712 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.21794341504573822, loss=1.481537103652954
I0214 22:40:40.171420 139975056156416 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.22413894534111023, loss=1.4521859884262085
I0214 22:41:15.805520 139975047763712 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.22876016795635223, loss=1.5153578519821167
I0214 22:41:51.426926 139975056156416 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.23490525782108307, loss=1.5226627588272095
I0214 22:42:27.072497 139975047763712 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.24277932941913605, loss=1.5048961639404297
I0214 22:43:02.673318 139975056156416 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.2379220873117447, loss=1.504521369934082
I0214 22:43:38.287737 139975047763712 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.22905686497688293, loss=1.4311429262161255
I0214 22:44:13.924230 139975056156416 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.23594199120998383, loss=1.4972279071807861
I0214 22:44:49.555019 139975047763712 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.24628432095050812, loss=1.4262744188308716
I0214 22:45:25.388016 139975056156416 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.23243050277233124, loss=1.4370862245559692
I0214 22:46:01.082633 139975047763712 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.2360668182373047, loss=1.5903818607330322
I0214 22:46:36.749133 139975056156416 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.2516607642173767, loss=1.5260066986083984
I0214 22:47:12.373517 139975047763712 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.22513256967067719, loss=1.4283853769302368
I0214 22:47:48.032315 139975056156416 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.22389547526836395, loss=1.4808188676834106
I0214 22:48:23.704260 139975047763712 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.24402153491973877, loss=1.501397728919983
I0214 22:48:59.377002 139975056156416 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.2104976624250412, loss=1.431806206703186
I0214 22:49:35.000533 139975047763712 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.2292754203081131, loss=1.4622234106063843
I0214 22:50:10.632403 139975056156416 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.25242316722869873, loss=1.4935272932052612
I0214 22:50:46.313387 139975047763712 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.22716595232486725, loss=1.5335612297058105
I0214 22:51:22.116669 139975056156416 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.22428472340106964, loss=1.42258882522583
I0214 22:51:57.777158 139975047763712 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.23489375412464142, loss=1.515428900718689
I0214 22:52:04.987337 140144802662208 spec.py:321] Evaluating on the training split.
I0214 22:52:07.964810 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 22:56:13.790208 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 22:56:16.483229 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 22:59:22.116243 140144802662208 spec.py:349] Evaluating on the test split.
I0214 22:59:24.813466 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 23:02:53.445315 140144802662208 submission_runner.py:408] Time since start: 69755.16s, 	Step: 113222, 	{'train/accuracy': 0.6949504017829895, 'train/loss': 1.3666104078292847, 'train/bleu': 35.29129947522768, 'validation/accuracy': 0.6940769553184509, 'validation/loss': 1.373827338218689, 'validation/bleu': 30.822025096723628, 'validation/num_examples': 3000, 'test/accuracy': 0.7110685110092163, 'test/loss': 1.2664237022399902, 'test/bleu': 30.57820511062706, 'test/num_examples': 3003, 'score': 40353.82215619087, 'total_duration': 69755.15716338158, 'accumulated_submission_time': 40353.82215619087, 'accumulated_eval_time': 29395.77043747902, 'accumulated_logging_time': 1.7632851600646973}
I0214 23:02:53.484884 139975056156416 logging_writer.py:48] [113222] accumulated_eval_time=29395.770437, accumulated_logging_time=1.763285, accumulated_submission_time=40353.822156, global_step=113222, preemption_count=0, score=40353.822156, test/accuracy=0.711069, test/bleu=30.578205, test/loss=1.266424, test/num_examples=3003, total_duration=69755.157163, train/accuracy=0.694950, train/bleu=35.291299, train/loss=1.366610, validation/accuracy=0.694077, validation/bleu=30.822025, validation/loss=1.373827, validation/num_examples=3000
I0214 23:03:21.482642 139975047763712 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.24144603312015533, loss=1.5614008903503418
I0214 23:03:57.002789 139975056156416 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.22980175912380219, loss=1.5037143230438232
I0214 23:04:32.628204 139975047763712 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.22877931594848633, loss=1.4874866008758545
I0214 23:05:08.241419 139975056156416 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.23076942563056946, loss=1.5385175943374634
I0214 23:05:43.859511 139975047763712 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.23944242298603058, loss=1.5545198917388916
I0214 23:06:19.531396 139975056156416 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.23610350489616394, loss=1.4709776639938354
I0214 23:06:55.190148 139975047763712 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.24017278850078583, loss=1.5610356330871582
I0214 23:07:30.827211 139975056156416 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.23799404501914978, loss=1.4458673000335693
I0214 23:08:06.464578 139975047763712 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.22642244398593903, loss=1.4131230115890503
I0214 23:08:42.204613 139975056156416 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.2462209165096283, loss=1.4221312999725342
I0214 23:09:17.824093 139975047763712 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.2270117700099945, loss=1.506934404373169
I0214 23:09:53.417186 139975056156416 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.2251405119895935, loss=1.475223183631897
I0214 23:10:29.024429 139975047763712 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.22876331210136414, loss=1.4621262550354004
I0214 23:11:04.649145 139975056156416 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.23982927203178406, loss=1.4836448431015015
I0214 23:11:40.264060 139975047763712 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.24169425666332245, loss=1.5386435985565186
I0214 23:12:16.006011 139975056156416 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.2315240502357483, loss=1.4414710998535156
I0214 23:12:51.668839 139975047763712 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.27248552441596985, loss=1.4830328226089478
I0214 23:13:27.316509 139975056156416 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.23061268031597137, loss=1.4673919677734375
I0214 23:14:02.953763 139975047763712 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.33157879114151, loss=1.4698586463928223
I0214 23:14:38.660916 139975056156416 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.2608349621295929, loss=1.4301011562347412
I0214 23:15:14.277659 139975047763712 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.2445596158504486, loss=1.5067812204360962
I0214 23:15:49.900863 139975056156416 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.2354084551334381, loss=1.482018232345581
I0214 23:16:25.525480 139975047763712 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.25438642501831055, loss=1.4490307569503784
I0214 23:16:53.716831 140144802662208 spec.py:321] Evaluating on the training split.
I0214 23:16:56.686393 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 23:21:25.670134 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 23:21:28.348775 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 23:24:40.873082 140144802662208 spec.py:349] Evaluating on the test split.
I0214 23:24:43.546934 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 23:27:24.583674 140144802662208 submission_runner.py:408] Time since start: 71226.30s, 	Step: 115581, 	{'train/accuracy': 0.692932665348053, 'train/loss': 1.3845998048782349, 'train/bleu': 34.87558627803715, 'validation/accuracy': 0.6947588920593262, 'validation/loss': 1.3719394207000732, 'validation/bleu': 30.69022831385768, 'validation/num_examples': 3000, 'test/accuracy': 0.7114287614822388, 'test/loss': 1.2623103857040405, 'test/bleu': 30.915548870642528, 'test/num_examples': 3003, 'score': 41193.96239209175, 'total_duration': 71226.29557180405, 'accumulated_submission_time': 41193.96239209175, 'accumulated_eval_time': 30026.637234210968, 'accumulated_logging_time': 1.8142666816711426}
I0214 23:27:24.616132 139975056156416 logging_writer.py:48] [115581] accumulated_eval_time=30026.637234, accumulated_logging_time=1.814267, accumulated_submission_time=41193.962392, global_step=115581, preemption_count=0, score=41193.962392, test/accuracy=0.711429, test/bleu=30.915549, test/loss=1.262310, test/num_examples=3003, total_duration=71226.295572, train/accuracy=0.692933, train/bleu=34.875586, train/loss=1.384600, validation/accuracy=0.694759, validation/bleu=30.690228, validation/loss=1.371939, validation/num_examples=3000
I0214 23:27:31.719171 139975047763712 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.2401283085346222, loss=1.473568320274353
I0214 23:28:07.201761 139975056156416 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.2400922030210495, loss=1.4935424327850342
I0214 23:28:42.778927 139975047763712 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.24224838614463806, loss=1.4401613473892212
I0214 23:29:18.386819 139975056156416 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.233895406126976, loss=1.4358946084976196
I0214 23:29:54.024884 139975047763712 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.23173768818378448, loss=1.5015194416046143
I0214 23:30:29.653851 139975056156416 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.21680642664432526, loss=1.4143444299697876
I0214 23:31:05.255879 139975047763712 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.2252437174320221, loss=1.4584838151931763
I0214 23:31:40.917412 139975056156416 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.23266160488128662, loss=1.4103052616119385
I0214 23:32:16.538529 139975047763712 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.24096961319446564, loss=1.4335854053497314
I0214 23:32:52.190838 139975056156416 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.2267068773508072, loss=1.5348148345947266
I0214 23:33:27.839586 139975047763712 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.23576503992080688, loss=1.5081478357315063
I0214 23:34:03.466948 139975056156416 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.22503246366977692, loss=1.4386260509490967
I0214 23:34:39.058628 139975047763712 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.23430109024047852, loss=1.5349920988082886
I0214 23:35:14.676352 139975056156416 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.22651058435440063, loss=1.5220057964324951
I0214 23:35:50.293397 139975047763712 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.23848047852516174, loss=1.5247219800949097
I0214 23:36:25.956087 139975056156416 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.23464739322662354, loss=1.4365756511688232
I0214 23:37:01.589259 139975047763712 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.24441194534301758, loss=1.5035070180892944
I0214 23:37:37.205855 139975056156416 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.23138181865215302, loss=1.4113786220550537
I0214 23:38:12.866625 139975047763712 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.2458329051733017, loss=1.5670615434646606
I0214 23:38:48.500761 139975056156416 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.23067477345466614, loss=1.5036004781723022
I0214 23:39:24.164825 139975047763712 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.227178156375885, loss=1.4333428144454956
I0214 23:39:59.811219 139975056156416 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.25240832567214966, loss=1.5835020542144775
I0214 23:40:35.462952 139975047763712 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.23847785592079163, loss=1.4945194721221924
I0214 23:41:11.077208 139975056156416 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.23063872754573822, loss=1.4301127195358276
I0214 23:41:24.719190 140144802662208 spec.py:321] Evaluating on the training split.
I0214 23:41:27.715643 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 23:45:24.361965 140144802662208 spec.py:333] Evaluating on the validation split.
I0214 23:45:27.061111 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 23:48:00.647270 140144802662208 spec.py:349] Evaluating on the test split.
I0214 23:48:03.350146 140144802662208 workload.py:181] Translating evaluation dataset.
I0214 23:50:33.726632 140144802662208 submission_runner.py:408] Time since start: 72615.44s, 	Step: 117940, 	{'train/accuracy': 0.6903288960456848, 'train/loss': 1.406310796737671, 'train/bleu': 35.18875162329316, 'validation/accuracy': 0.6950441002845764, 'validation/loss': 1.3678932189941406, 'validation/bleu': 30.774742153044187, 'validation/num_examples': 3000, 'test/accuracy': 0.7126837372779846, 'test/loss': 1.2599252462387085, 'test/bleu': 31.04379746971764, 'test/num_examples': 3003, 'score': 42033.97531580925, 'total_duration': 72615.43852734566, 'accumulated_submission_time': 42033.97531580925, 'accumulated_eval_time': 30575.644649982452, 'accumulated_logging_time': 1.8567523956298828}
I0214 23:50:33.758146 139975047763712 logging_writer.py:48] [117940] accumulated_eval_time=30575.644650, accumulated_logging_time=1.856752, accumulated_submission_time=42033.975316, global_step=117940, preemption_count=0, score=42033.975316, test/accuracy=0.712684, test/bleu=31.043797, test/loss=1.259925, test/num_examples=3003, total_duration=72615.438527, train/accuracy=0.690329, train/bleu=35.188752, train/loss=1.406311, validation/accuracy=0.695044, validation/bleu=30.774742, validation/loss=1.367893, validation/num_examples=3000
I0214 23:50:55.394157 139975056156416 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.23446275293827057, loss=1.408803939819336
I0214 23:51:30.891914 139975047763712 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.23138946294784546, loss=1.4115465879440308
I0214 23:52:06.489000 139975056156416 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.23733744025230408, loss=1.4620217084884644
I0214 23:52:42.131802 139975047763712 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.22361566126346588, loss=1.460584282875061
I0214 23:53:17.739882 139975056156416 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.2409079223871231, loss=1.455029010772705
I0214 23:53:53.381522 139975047763712 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.24267145991325378, loss=1.4646153450012207
I0214 23:54:29.039019 139975056156416 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.236134335398674, loss=1.5067620277404785
I0214 23:55:04.673640 139975047763712 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.24237191677093506, loss=1.5412707328796387
I0214 23:55:40.303300 139975056156416 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.22902831435203552, loss=1.4447345733642578
I0214 23:56:15.934909 139975047763712 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.233662948012352, loss=1.467577338218689
I0214 23:56:51.559542 139975056156416 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.2318296581506729, loss=1.4453555345535278
I0214 23:57:27.174545 139975047763712 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.23822881281375885, loss=1.4298509359359741
I0214 23:58:02.809291 139975056156416 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.23426446318626404, loss=1.4036164283752441
I0214 23:58:38.425708 139975047763712 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.2291877120733261, loss=1.4192492961883545
I0214 23:59:14.035956 139975056156416 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.22533951699733734, loss=1.4128302335739136
I0214 23:59:49.770100 139975047763712 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.23001070320606232, loss=1.4055088758468628
I0215 00:00:25.369954 139975056156416 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.22414715588092804, loss=1.4196239709854126
I0215 00:01:00.985245 139975047763712 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.23010484874248505, loss=1.4922863245010376
I0215 00:01:36.646291 139975056156416 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.23487742245197296, loss=1.4546993970870972
I0215 00:02:12.310829 139975047763712 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.23200871050357819, loss=1.418859601020813
I0215 00:02:47.955438 139975056156416 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.23400583863258362, loss=1.4364969730377197
I0215 00:03:23.579846 139975047763712 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.2319883555173874, loss=1.5208204984664917
I0215 00:03:59.173173 139975056156416 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.23628133535385132, loss=1.473084807395935
I0215 00:04:33.824379 140144802662208 spec.py:321] Evaluating on the training split.
I0215 00:04:36.804389 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 00:08:29.583184 140144802662208 spec.py:333] Evaluating on the validation split.
I0215 00:08:32.269595 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 00:11:21.595760 140144802662208 spec.py:349] Evaluating on the test split.
I0215 00:11:24.284973 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 00:13:56.010097 140144802662208 submission_runner.py:408] Time since start: 74017.72s, 	Step: 120299, 	{'train/accuracy': 0.6964380145072937, 'train/loss': 1.3709895610809326, 'train/bleu': 35.66247077415662, 'validation/accuracy': 0.6952300667762756, 'validation/loss': 1.3651654720306396, 'validation/bleu': 30.875901980810767, 'validation/num_examples': 3000, 'test/accuracy': 0.7123816609382629, 'test/loss': 1.2575608491897583, 'test/bleu': 30.803521074857283, 'test/num_examples': 3003, 'score': 42873.95320272446, 'total_duration': 74017.72199606895, 'accumulated_submission_time': 42873.95320272446, 'accumulated_eval_time': 31137.830321788788, 'accumulated_logging_time': 1.8995048999786377}
I0215 00:13:56.042755 139975047763712 logging_writer.py:48] [120299] accumulated_eval_time=31137.830322, accumulated_logging_time=1.899505, accumulated_submission_time=42873.953203, global_step=120299, preemption_count=0, score=42873.953203, test/accuracy=0.712382, test/bleu=30.803521, test/loss=1.257561, test/num_examples=3003, total_duration=74017.721996, train/accuracy=0.696438, train/bleu=35.662471, train/loss=1.370990, validation/accuracy=0.695230, validation/bleu=30.875902, validation/loss=1.365165, validation/num_examples=3000
I0215 00:13:56.771334 139975056156416 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.2398366928100586, loss=1.4347695112228394
I0215 00:14:32.241555 139975047763712 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.24346213042736053, loss=1.5134001970291138
I0215 00:15:07.828964 139975056156416 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.23444147408008575, loss=1.4334747791290283
I0215 00:15:43.453016 139975047763712 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.23725448548793793, loss=1.4848525524139404
I0215 00:16:19.038720 139975056156416 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.22475390136241913, loss=1.4006282091140747
I0215 00:16:54.638538 139975047763712 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.2367480993270874, loss=1.4093550443649292
I0215 00:17:30.240111 139975056156416 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.24992862343788147, loss=1.4383878707885742
I0215 00:18:05.840160 139975047763712 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.2421494871377945, loss=1.4328638315200806
I0215 00:18:41.464017 139975056156416 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.2236883044242859, loss=1.4294049739837646
I0215 00:19:17.079595 139975047763712 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.24018870294094086, loss=1.4555871486663818
I0215 00:19:52.694378 139975056156416 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.2262054681777954, loss=1.4850444793701172
I0215 00:20:28.324709 139975047763712 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.24587921798229218, loss=1.4634504318237305
I0215 00:21:03.939692 139975056156416 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.24143891036510468, loss=1.4806891679763794
I0215 00:21:39.619576 139975047763712 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.23810985684394836, loss=1.4731824398040771
I0215 00:22:15.232652 139975056156416 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.23664440214633942, loss=1.4365758895874023
I0215 00:22:50.839772 139975047763712 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.2502376139163971, loss=1.5287832021713257
I0215 00:23:26.476205 139975056156416 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.22835394740104675, loss=1.5071150064468384
I0215 00:24:02.083686 139975047763712 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.23150335252285004, loss=1.4554694890975952
I0215 00:24:37.706509 139975056156416 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.2285042107105255, loss=1.4377243518829346
I0215 00:25:13.339053 139975047763712 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.24471648037433624, loss=1.5403103828430176
I0215 00:25:48.937598 139975056156416 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.23195868730545044, loss=1.4557981491088867
I0215 00:26:24.545603 139975047763712 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.22744235396385193, loss=1.4631264209747314
I0215 00:27:00.139615 139975056156416 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.2311207503080368, loss=1.4441442489624023
I0215 00:27:35.775136 139975047763712 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.24282440543174744, loss=1.4648547172546387
I0215 00:27:56.156696 140144802662208 spec.py:321] Evaluating on the training split.
I0215 00:27:59.125270 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 00:32:04.581202 140144802662208 spec.py:333] Evaluating on the validation split.
I0215 00:32:07.272321 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 00:35:00.990156 140144802662208 spec.py:349] Evaluating on the test split.
I0215 00:35:03.698423 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 00:37:53.114040 140144802662208 submission_runner.py:408] Time since start: 75454.83s, 	Step: 122659, 	{'train/accuracy': 0.6943261623382568, 'train/loss': 1.382710337638855, 'train/bleu': 35.10050834963898, 'validation/accuracy': 0.6955276131629944, 'validation/loss': 1.3625982999801636, 'validation/bleu': 31.04729680749209, 'validation/num_examples': 3000, 'test/accuracy': 0.7139387726783752, 'test/loss': 1.2523609399795532, 'test/bleu': 30.83641365004823, 'test/num_examples': 3003, 'score': 43713.98013854027, 'total_duration': 75454.82591438293, 'accumulated_submission_time': 43713.98013854027, 'accumulated_eval_time': 31734.78758907318, 'accumulated_logging_time': 1.942683458328247}
I0215 00:37:53.152106 139975056156416 logging_writer.py:48] [122659] accumulated_eval_time=31734.787589, accumulated_logging_time=1.942683, accumulated_submission_time=43713.980139, global_step=122659, preemption_count=0, score=43713.980139, test/accuracy=0.713939, test/bleu=30.836414, test/loss=1.252361, test/num_examples=3003, total_duration=75454.825914, train/accuracy=0.694326, train/bleu=35.100508, train/loss=1.382710, validation/accuracy=0.695528, validation/bleu=31.047297, validation/loss=1.362598, validation/num_examples=3000
I0215 00:38:08.027764 139975047763712 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.24951504170894623, loss=1.5124982595443726
I0215 00:38:43.499171 139975056156416 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.231549933552742, loss=1.4307336807250977
I0215 00:39:19.062074 139975047763712 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.2399989515542984, loss=1.3974175453186035
I0215 00:39:54.650094 139975056156416 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.23947639763355255, loss=1.4721494913101196
I0215 00:40:30.274897 139975047763712 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.22791334986686707, loss=1.4630563259124756
I0215 00:41:05.860300 139975056156416 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.23781003057956696, loss=1.4070205688476562
I0215 00:41:41.466011 139975047763712 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.22378940880298615, loss=1.425279140472412
I0215 00:42:17.088355 139975056156416 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.23742495477199554, loss=1.4988726377487183
I0215 00:42:52.706642 139975047763712 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.24227601289749146, loss=1.487999677658081
I0215 00:43:28.330741 139975056156416 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.24133314192295074, loss=1.5020782947540283
I0215 00:44:03.953502 139975047763712 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.239669069647789, loss=1.4747283458709717
I0215 00:44:39.571305 139975056156416 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.23980775475502014, loss=1.399639368057251
I0215 00:45:15.175018 139975047763712 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.23418596386909485, loss=1.4693037271499634
I0215 00:45:50.799486 139975056156416 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.2280118465423584, loss=1.454103708267212
I0215 00:46:26.422653 139975047763712 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.24155199527740479, loss=1.4790109395980835
I0215 00:47:02.033053 139975056156416 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.23752889037132263, loss=1.441962480545044
I0215 00:47:37.645521 139975047763712 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.23916757106781006, loss=1.4673036336898804
I0215 00:48:13.246312 139975056156416 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.24007096886634827, loss=1.42322838306427
I0215 00:48:48.897064 139975047763712 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.23155733942985535, loss=1.4237093925476074
I0215 00:49:24.532157 139975056156416 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.24481597542762756, loss=1.488701581954956
I0215 00:50:00.144181 139975047763712 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.23777541518211365, loss=1.4467447996139526
I0215 00:50:35.781523 139975056156416 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.24092666804790497, loss=1.430260181427002
I0215 00:51:11.404517 139975047763712 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.24353966116905212, loss=1.4570204019546509
I0215 00:51:47.013855 139975056156416 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.23413124680519104, loss=1.4976736307144165
I0215 00:51:53.153095 140144802662208 spec.py:321] Evaluating on the training split.
I0215 00:51:56.124629 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 00:56:02.443146 140144802662208 spec.py:333] Evaluating on the validation split.
I0215 00:56:05.128443 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 00:58:51.298128 140144802662208 spec.py:349] Evaluating on the test split.
I0215 00:58:54.002901 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 01:01:28.532554 140144802662208 submission_runner.py:408] Time since start: 76870.24s, 	Step: 125019, 	{'train/accuracy': 0.6947394609451294, 'train/loss': 1.377518892288208, 'train/bleu': 35.855026133964195, 'validation/accuracy': 0.6967179775238037, 'validation/loss': 1.361031174659729, 'validation/bleu': 30.876074128853972, 'validation/num_examples': 3000, 'test/accuracy': 0.713927149772644, 'test/loss': 1.2523263692855835, 'test/bleu': 30.98107247465367, 'test/num_examples': 3003, 'score': 44553.89228606224, 'total_duration': 76870.24442744255, 'accumulated_submission_time': 44553.89228606224, 'accumulated_eval_time': 32310.166967868805, 'accumulated_logging_time': 1.9930167198181152}
I0215 01:01:28.565502 139975047763712 logging_writer.py:48] [125019] accumulated_eval_time=32310.166968, accumulated_logging_time=1.993017, accumulated_submission_time=44553.892286, global_step=125019, preemption_count=0, score=44553.892286, test/accuracy=0.713927, test/bleu=30.981072, test/loss=1.252326, test/num_examples=3003, total_duration=76870.244427, train/accuracy=0.694739, train/bleu=35.855026, train/loss=1.377519, validation/accuracy=0.696718, validation/bleu=30.876074, validation/loss=1.361031, validation/num_examples=3000
I0215 01:01:57.617372 139975056156416 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.2396596074104309, loss=1.4926838874816895
I0215 01:02:33.130123 139975047763712 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.23850217461585999, loss=1.429498314857483
I0215 01:03:08.742535 139975056156416 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.23392461240291595, loss=1.3847637176513672
I0215 01:03:44.334112 139975047763712 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.23023411631584167, loss=1.4691890478134155
I0215 01:04:19.958752 139975056156416 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.23275324702262878, loss=1.4286938905715942
I0215 01:04:55.591310 139975047763712 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.23690922558307648, loss=1.4839367866516113
I0215 01:05:31.212364 139975056156416 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.23986247181892395, loss=1.4522093534469604
I0215 01:06:06.846349 139975047763712 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.23376043140888214, loss=1.4573688507080078
I0215 01:06:42.442032 139975056156416 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.254324734210968, loss=1.4131280183792114
I0215 01:07:18.042409 139975047763712 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.23846596479415894, loss=1.4286085367202759
I0215 01:07:53.664943 139975056156416 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.23116633296012878, loss=1.4720954895019531
I0215 01:08:29.293609 139975047763712 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.23967519402503967, loss=1.4392462968826294
I0215 01:09:04.916172 139975056156416 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.22619964182376862, loss=1.3984898328781128
I0215 01:09:40.563100 139975047763712 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.21728676557540894, loss=1.4085144996643066
I0215 01:10:16.187196 139975056156416 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.24066241085529327, loss=1.4841411113739014
I0215 01:10:51.863898 139975047763712 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.23274679481983185, loss=1.4690340757369995
I0215 01:11:27.487753 139975056156416 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.23013935983181, loss=1.382540225982666
I0215 01:12:03.123267 139975047763712 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.2408612072467804, loss=1.4228947162628174
I0215 01:12:38.864313 139975056156416 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.23247991502285004, loss=1.4050813913345337
I0215 01:13:14.473453 139975047763712 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.22600437700748444, loss=1.4215232133865356
I0215 01:13:50.079329 139975056156416 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.24249686300754547, loss=1.4226605892181396
I0215 01:14:25.698907 139975047763712 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.2373676896095276, loss=1.4356375932693481
I0215 01:15:01.322640 139975056156416 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.23790034651756287, loss=1.454749345779419
I0215 01:15:28.812668 140144802662208 spec.py:321] Evaluating on the training split.
I0215 01:15:31.787619 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 01:19:44.227192 140144802662208 spec.py:333] Evaluating on the validation split.
I0215 01:19:46.908878 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 01:22:32.190772 140144802662208 spec.py:349] Evaluating on the test split.
I0215 01:22:34.871457 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 01:25:23.218558 140144802662208 submission_runner.py:408] Time since start: 78304.93s, 	Step: 127379, 	{'train/accuracy': 0.6961644291877747, 'train/loss': 1.3683280944824219, 'train/bleu': 35.79156931045429, 'validation/accuracy': 0.6965195536613464, 'validation/loss': 1.3605613708496094, 'validation/bleu': 30.945679584384134, 'validation/num_examples': 3000, 'test/accuracy': 0.7140550017356873, 'test/loss': 1.2519234418869019, 'test/bleu': 30.88051741175642, 'test/num_examples': 3003, 'score': 45394.04832172394, 'total_duration': 78304.93042588234, 'accumulated_submission_time': 45394.04832172394, 'accumulated_eval_time': 32904.57277917862, 'accumulated_logging_time': 2.0373668670654297}
I0215 01:25:23.257499 139975047763712 logging_writer.py:48] [127379] accumulated_eval_time=32904.572779, accumulated_logging_time=2.037367, accumulated_submission_time=45394.048322, global_step=127379, preemption_count=0, score=45394.048322, test/accuracy=0.714055, test/bleu=30.880517, test/loss=1.251923, test/num_examples=3003, total_duration=78304.930426, train/accuracy=0.696164, train/bleu=35.791569, train/loss=1.368328, validation/accuracy=0.696520, validation/bleu=30.945680, validation/loss=1.360561, validation/num_examples=3000
I0215 01:25:31.060431 139975056156416 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.22919276356697083, loss=1.4334194660186768
I0215 01:26:06.492027 139975047763712 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.24090036749839783, loss=1.4662978649139404
I0215 01:26:42.053412 139975056156416 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.23329377174377441, loss=1.4000616073608398
I0215 01:27:17.655067 139975047763712 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.2309318482875824, loss=1.4795552492141724
I0215 01:27:53.259427 139975056156416 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.22412237524986267, loss=1.4200348854064941
I0215 01:28:28.922158 139975047763712 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.2292715460062027, loss=1.4410303831100464
I0215 01:29:04.537631 139975056156416 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.23397691547870636, loss=1.4314721822738647
I0215 01:29:40.141973 139975047763712 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.22732023894786835, loss=1.4349483251571655
I0215 01:30:15.754163 139975056156416 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.23050257563591003, loss=1.418735384941101
I0215 01:30:51.398775 139975047763712 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.23112136125564575, loss=1.4396154880523682
I0215 01:31:27.060479 139975056156416 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.23456628620624542, loss=1.40890634059906
I0215 01:32:02.672231 139975047763712 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.24249722063541412, loss=1.4327350854873657
I0215 01:32:38.300817 139975056156416 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.22684648633003235, loss=1.3851182460784912
I0215 01:33:13.928917 139975047763712 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.23412686586380005, loss=1.389767050743103
I0215 01:33:49.555999 139975056156416 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.23715636134147644, loss=1.4850552082061768
I0215 01:34:25.193869 139975047763712 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.24588643014431, loss=1.417175531387329
I0215 01:35:00.822730 139975056156416 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.2464265674352646, loss=1.4537858963012695
I0215 01:35:36.455057 139975047763712 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.22405453026294708, loss=1.444532871246338
I0215 01:36:12.055061 139975056156416 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.238952174782753, loss=1.4859727621078491
I0215 01:36:47.688530 139975047763712 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.24214346706867218, loss=1.4696403741836548
I0215 01:37:23.354932 139975056156416 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.23418469727039337, loss=1.4407861232757568
I0215 01:37:58.961870 139975047763712 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.235805943608284, loss=1.3801876306533813
I0215 01:38:34.584446 139975056156416 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.22822779417037964, loss=1.4409475326538086
I0215 01:39:10.200657 139975047763712 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.2338399738073349, loss=1.543164849281311
I0215 01:39:23.465722 140144802662208 spec.py:321] Evaluating on the training split.
I0215 01:39:26.438693 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 01:43:32.267119 140144802662208 spec.py:333] Evaluating on the validation split.
I0215 01:43:34.946986 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 01:46:20.871496 140144802662208 spec.py:349] Evaluating on the test split.
I0215 01:46:23.561906 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 01:49:02.325749 140144802662208 submission_runner.py:408] Time since start: 79724.04s, 	Step: 129739, 	{'train/accuracy': 0.6961685419082642, 'train/loss': 1.3700485229492188, 'train/bleu': 35.88213839829184, 'validation/accuracy': 0.6964327692985535, 'validation/loss': 1.36008882522583, 'validation/bleu': 30.981903451783634, 'validation/num_examples': 3000, 'test/accuracy': 0.7140898704528809, 'test/loss': 1.2510143518447876, 'test/bleu': 30.877321535382997, 'test/num_examples': 3003, 'score': 46234.166835308075, 'total_duration': 79724.03761744499, 'accumulated_submission_time': 46234.166835308075, 'accumulated_eval_time': 33483.432725191116, 'accumulated_logging_time': 2.08833909034729}
I0215 01:49:02.363539 139975056156416 logging_writer.py:48] [129739] accumulated_eval_time=33483.432725, accumulated_logging_time=2.088339, accumulated_submission_time=46234.166835, global_step=129739, preemption_count=0, score=46234.166835, test/accuracy=0.714090, test/bleu=30.877322, test/loss=1.251014, test/num_examples=3003, total_duration=79724.037617, train/accuracy=0.696169, train/bleu=35.882138, train/loss=1.370049, validation/accuracy=0.696433, validation/bleu=30.981903, validation/loss=1.360089, validation/num_examples=3000
I0215 01:49:24.326391 139975047763712 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.23785540461540222, loss=1.4452418088912964
I0215 01:49:59.842383 139975056156416 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.2239983081817627, loss=1.3824023008346558
I0215 01:50:35.444766 139975047763712 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.23492500185966492, loss=1.4600473642349243
I0215 01:51:11.073372 139975056156416 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.23684974014759064, loss=1.4343899488449097
I0215 01:51:46.713696 139975047763712 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.23786118626594543, loss=1.4562971591949463
I0215 01:52:22.335728 139975056156416 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.23402439057826996, loss=1.4539194107055664
I0215 01:52:57.912427 139975047763712 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.22890757024288177, loss=1.4410388469696045
I0215 01:53:33.515869 139975056156416 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.22748783230781555, loss=1.4434682130813599
I0215 01:54:09.091371 139975047763712 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.23085558414459229, loss=1.4457486867904663
I0215 01:54:44.728908 139975056156416 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.23538145422935486, loss=1.4555236101150513
I0215 01:55:20.339532 139975047763712 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.23718972504138947, loss=1.4936802387237549
I0215 01:55:55.946481 139975056156416 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.23331432044506073, loss=1.4109058380126953
I0215 01:56:31.592381 139975047763712 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.23291365802288055, loss=1.433208703994751
I0215 01:57:07.207989 139975056156416 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.23575329780578613, loss=1.4345778226852417
I0215 01:57:42.807543 139975047763712 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.22671079635620117, loss=1.3623765707015991
I0215 01:58:18.421904 139975056156416 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.24103474617004395, loss=1.444190263748169
I0215 01:58:54.023004 139975047763712 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.22531253099441528, loss=1.3975729942321777
I0215 01:59:29.617257 139975056156416 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.23457057774066925, loss=1.4335448741912842
I0215 02:00:05.226362 139975047763712 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.23746202886104584, loss=1.501158595085144
I0215 02:00:40.859951 139975056156416 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.24317608773708344, loss=1.4937797784805298
I0215 02:01:16.507904 139975047763712 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.23326364159584045, loss=1.5022687911987305
I0215 02:01:52.122455 139975056156416 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.2300477772951126, loss=1.395443320274353
I0215 02:02:27.737112 139975047763712 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.23039239645004272, loss=1.4160051345825195
I0215 02:03:02.372971 140144802662208 spec.py:321] Evaluating on the training split.
I0215 02:03:05.343869 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 02:07:22.712382 140144802662208 spec.py:333] Evaluating on the validation split.
I0215 02:07:25.389833 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 02:10:11.195106 140144802662208 spec.py:349] Evaluating on the test split.
I0215 02:10:13.873604 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 02:12:57.700892 140144802662208 submission_runner.py:408] Time since start: 81159.41s, 	Step: 132099, 	{'train/accuracy': 0.6940845251083374, 'train/loss': 1.3850692510604858, 'train/bleu': 35.896859808845385, 'validation/accuracy': 0.6964203715324402, 'validation/loss': 1.359937071800232, 'validation/bleu': 30.921410299106665, 'validation/num_examples': 3000, 'test/accuracy': 0.7141827940940857, 'test/loss': 1.2505288124084473, 'test/bleu': 30.886567051931777, 'test/num_examples': 3003, 'score': 47074.08837556839, 'total_duration': 81159.41278767586, 'accumulated_submission_time': 47074.08837556839, 'accumulated_eval_time': 34078.76060009003, 'accumulated_logging_time': 2.137282371520996}
I0215 02:12:57.735120 139975056156416 logging_writer.py:48] [132099] accumulated_eval_time=34078.760600, accumulated_logging_time=2.137282, accumulated_submission_time=47074.088376, global_step=132099, preemption_count=0, score=47074.088376, test/accuracy=0.714183, test/bleu=30.886567, test/loss=1.250529, test/num_examples=3003, total_duration=81159.412788, train/accuracy=0.694085, train/bleu=35.896860, train/loss=1.385069, validation/accuracy=0.696420, validation/bleu=30.921410, validation/loss=1.359937, validation/num_examples=3000
I0215 02:12:58.471519 139975047763712 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.2381984144449234, loss=1.4552638530731201
I0215 02:13:33.936561 139975056156416 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.2324846237897873, loss=1.458692193031311
I0215 02:14:09.498019 139975047763712 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.23876790702342987, loss=1.5190489292144775
I0215 02:14:45.098562 139975056156416 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.23352113366127014, loss=1.4173598289489746
I0215 02:15:20.717450 139975047763712 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.24274830520153046, loss=1.4283396005630493
I0215 02:15:56.322018 139975056156416 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.2277388870716095, loss=1.5001872777938843
I0215 02:16:31.932151 139975047763712 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.23215678334236145, loss=1.3867994546890259
I0215 02:17:07.559742 139975056156416 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.2309715449810028, loss=1.3392466306686401
I0215 02:17:43.166481 139975047763712 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.2269025444984436, loss=1.4488834142684937
I0215 02:18:18.781251 139975056156416 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.24937620759010315, loss=1.474068522453308
I0215 02:18:54.405955 139975047763712 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.2266293615102768, loss=1.4496012926101685
I0215 02:19:30.054095 139975056156416 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.24106059968471527, loss=1.5091477632522583
I0215 02:20:05.682307 139975047763712 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.2285490334033966, loss=1.460647702217102
I0215 02:20:16.806425 140144802662208 spec.py:321] Evaluating on the training split.
I0215 02:20:19.779624 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 02:24:17.226362 140144802662208 spec.py:333] Evaluating on the validation split.
I0215 02:24:19.905689 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 02:27:06.432373 140144802662208 spec.py:349] Evaluating on the test split.
I0215 02:27:09.113188 140144802662208 workload.py:181] Translating evaluation dataset.
I0215 02:29:47.749191 140144802662208 submission_runner.py:408] Time since start: 82169.46s, 	Step: 133333, 	{'train/accuracy': 0.6966027021408081, 'train/loss': 1.3738631010055542, 'train/bleu': 35.481369630996994, 'validation/accuracy': 0.6963583827018738, 'validation/loss': 1.360085368156433, 'validation/bleu': 30.92441491532882, 'validation/num_examples': 3000, 'test/accuracy': 0.7141595482826233, 'test/loss': 1.2506054639816284, 'test/bleu': 30.925173570681565, 'test/num_examples': 3003, 'score': 47513.109431266785, 'total_duration': 82169.46109032631, 'accumulated_submission_time': 47513.109431266785, 'accumulated_eval_time': 34649.70331478119, 'accumulated_logging_time': 2.181474447250366}
I0215 02:29:47.782975 139975056156416 logging_writer.py:48] [133333] accumulated_eval_time=34649.703315, accumulated_logging_time=2.181474, accumulated_submission_time=47513.109431, global_step=133333, preemption_count=0, score=47513.109431, test/accuracy=0.714160, test/bleu=30.925174, test/loss=1.250605, test/num_examples=3003, total_duration=82169.461090, train/accuracy=0.696603, train/bleu=35.481370, train/loss=1.373863, validation/accuracy=0.696358, validation/bleu=30.924415, validation/loss=1.360085, validation/num_examples=3000
I0215 02:29:47.817475 139975047763712 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=47513.109431
I0215 02:29:49.010187 140144802662208 checkpoints.py:490] Saving checkpoint at step: 133333
I0215 02:29:53.091181 140144802662208 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_3/wmt_jax/trial_5/checkpoint_133333
I0215 02:29:53.096117 140144802662208 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_3/wmt_jax/trial_5/checkpoint_133333.
I0215 02:29:53.161165 140144802662208 submission_runner.py:583] Tuning trial 5/5
I0215 02:29:53.161320 140144802662208 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0215 02:29:53.193556 140144802662208 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005722984205931425, 'train/loss': 10.961396217346191, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.980294227600098, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.966498374938965, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 31.811314821243286, 'total_duration': 891.5330049991608, 'accumulated_submission_time': 31.811314821243286, 'accumulated_eval_time': 859.7216436862946, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2358, {'train/accuracy': 0.5152115821838379, 'train/loss': 2.8529348373413086, 'train/bleu': 22.559558180281464, 'validation/accuracy': 0.5171913504600525, 'validation/loss': 2.8354642391204834, 'validation/bleu': 18.385519591984245, 'validation/num_examples': 3000, 'test/accuracy': 0.5146359801292419, 'test/loss': 2.8742642402648926, 'test/bleu': 17.058797586743704, 'test/num_examples': 3003, 'score': 871.7362320423126, 'total_duration': 2207.681263446808, 'accumulated_submission_time': 871.7362320423126, 'accumulated_eval_time': 1335.8473892211914, 'accumulated_logging_time': 0.02074408531188965, 'global_step': 2358, 'preemption_count': 0}), (4718, {'train/accuracy': 0.5785308480262756, 'train/loss': 2.2483465671539307, 'train/bleu': 27.164030770714678, 'validation/accuracy': 0.5911272168159485, 'validation/loss': 2.150007486343384, 'validation/bleu': 23.69484945016536, 'validation/num_examples': 3000, 'test/accuracy': 0.5932833552360535, 'test/loss': 2.123673677444458, 'test/bleu': 22.21513088925912, 'test/num_examples': 3003, 'score': 1711.7513601779938, 'total_duration': 3551.384021759033, 'accumulated_submission_time': 1711.7513601779938, 'accumulated_eval_time': 1839.4309611320496, 'accumulated_logging_time': 0.04746723175048828, 'global_step': 4718, 'preemption_count': 0}), (7077, {'train/accuracy': 0.6040958762168884, 'train/loss': 2.007240056991577, 'train/bleu': 29.060628161671126, 'validation/accuracy': 0.6170040965080261, 'validation/loss': 1.9235773086547852, 'validation/bleu': 25.445472800205756, 'validation/num_examples': 3000, 'test/accuracy': 0.6244146227836609, 'test/loss': 1.8764768838882446, 'test/bleu': 24.14670156771929, 'test/num_examples': 3003, 'score': 2551.796592235565, 'total_duration': 4831.369480133057, 'accumulated_submission_time': 2551.796592235565, 'accumulated_eval_time': 2279.2648980617523, 'accumulated_logging_time': 0.07353043556213379, 'global_step': 7077, 'preemption_count': 0}), (9435, {'train/accuracy': 0.6132701635360718, 'train/loss': 1.9433298110961914, 'train/bleu': 29.630833378883725, 'validation/accuracy': 0.6301347613334656, 'validation/loss': 1.8156956434249878, 'validation/bleu': 26.173138325326224, 'validation/num_examples': 3000, 'test/accuracy': 0.6360118389129639, 'test/loss': 1.7616527080535889, 'test/bleu': 24.85918981164338, 'test/num_examples': 3003, 'score': 3391.7186181545258, 'total_duration': 6124.946580171585, 'accumulated_submission_time': 3391.7186181545258, 'accumulated_eval_time': 2732.8127586841583, 'accumulated_logging_time': 0.10128641128540039, 'global_step': 9435, 'preemption_count': 0}), (11794, {'train/accuracy': 0.6201885938644409, 'train/loss': 1.886154294013977, 'train/bleu': 29.409620364240315, 'validation/accuracy': 0.6406244039535522, 'validation/loss': 1.745435357093811, 'validation/bleu': 26.7636870644405, 'validation/num_examples': 3000, 'test/accuracy': 0.6483644247055054, 'test/loss': 1.6862653493881226, 'test/bleu': 25.861684393273194, 'test/num_examples': 3003, 'score': 4231.872063875198, 'total_duration': 7439.0550146102905, 'accumulated_submission_time': 4231.872063875198, 'accumulated_eval_time': 3206.6629474163055, 'accumulated_logging_time': 0.12857842445373535, 'global_step': 11794, 'preemption_count': 0}), (14153, {'train/accuracy': 0.6258756518363953, 'train/loss': 1.830366611480713, 'train/bleu': 30.30475543405984, 'validation/accuracy': 0.6443069577217102, 'validation/loss': 1.7058216333389282, 'validation/bleu': 27.178247847671944, 'validation/num_examples': 3000, 'test/accuracy': 0.6512695550918579, 'test/loss': 1.6413919925689697, 'test/bleu': 25.934985390513155, 'test/num_examples': 3003, 'score': 5072.088208436966, 'total_duration': 8813.45656490326, 'accumulated_submission_time': 5072.088208436966, 'accumulated_eval_time': 3740.741831302643, 'accumulated_logging_time': 0.15647459030151367, 'global_step': 14153, 'preemption_count': 0}), (16512, {'train/accuracy': 0.6253536343574524, 'train/loss': 1.8386945724487305, 'train/bleu': 30.426984478535974, 'validation/accuracy': 0.6469727754592896, 'validation/loss': 1.6795895099639893, 'validation/bleu': 27.51153504880521, 'validation/num_examples': 3000, 'test/accuracy': 0.6567195653915405, 'test/loss': 1.6103800535202026, 'test/bleu': 26.76462025361148, 'test/num_examples': 3003, 'score': 5912.17545747757, 'total_duration': 10142.664711236954, 'accumulated_submission_time': 5912.17545747757, 'accumulated_eval_time': 4229.750396728516, 'accumulated_logging_time': 0.18865442276000977, 'global_step': 16512, 'preemption_count': 0}), (18871, {'train/accuracy': 0.6545403003692627, 'train/loss': 1.619762897491455, 'train/bleu': 32.29815543479825, 'validation/accuracy': 0.6512380242347717, 'validation/loss': 1.6543937921524048, 'validation/bleu': 27.504371133986982, 'validation/num_examples': 3000, 'test/accuracy': 0.6603451371192932, 'test/loss': 1.5860060453414917, 'test/bleu': 26.534122605706614, 'test/num_examples': 3003, 'score': 6752.3742599487305, 'total_duration': 11570.590962171555, 'accumulated_submission_time': 6752.3742599487305, 'accumulated_eval_time': 4817.370816230774, 'accumulated_logging_time': 0.21682024002075195, 'global_step': 18871, 'preemption_count': 0}), (21228, {'train/accuracy': 0.6335942149162292, 'train/loss': 1.776694416999817, 'train/bleu': 30.58009748460818, 'validation/accuracy': 0.6522423624992371, 'validation/loss': 1.640693187713623, 'validation/bleu': 27.75300949580246, 'validation/num_examples': 3000, 'test/accuracy': 0.6626227498054504, 'test/loss': 1.5686941146850586, 'test/bleu': 26.675115590587676, 'test/num_examples': 3003, 'score': 7592.311160326004, 'total_duration': 12940.57827091217, 'accumulated_submission_time': 7592.311160326004, 'accumulated_eval_time': 5347.3108031749725, 'accumulated_logging_time': 0.24676942825317383, 'global_step': 21228, 'preemption_count': 0}), (23587, {'train/accuracy': 0.6337989568710327, 'train/loss': 1.7762765884399414, 'train/bleu': 30.87184258005141, 'validation/accuracy': 0.6548957824707031, 'validation/loss': 1.6313564777374268, 'validation/bleu': 27.84559009417645, 'validation/num_examples': 3000, 'test/accuracy': 0.6640636920928955, 'test/loss': 1.562119483947754, 'test/bleu': 26.70121926683885, 'test/num_examples': 3003, 'score': 8432.473328590393, 'total_duration': 14366.7304251194, 'accumulated_submission_time': 8432.473328590393, 'accumulated_eval_time': 5933.192608118057, 'accumulated_logging_time': 0.27526402473449707, 'global_step': 23587, 'preemption_count': 0}), (25946, {'train/accuracy': 0.6462305188179016, 'train/loss': 1.6862976551055908, 'train/bleu': 31.452835525252226, 'validation/accuracy': 0.6573259830474854, 'validation/loss': 1.6084153652191162, 'validation/bleu': 27.836506053367742, 'validation/num_examples': 3000, 'test/accuracy': 0.6673174500465393, 'test/loss': 1.539101481437683, 'test/bleu': 27.519325136360557, 'test/num_examples': 3003, 'score': 9272.582363128662, 'total_duration': 15693.724759817123, 'accumulated_submission_time': 9272.582363128662, 'accumulated_eval_time': 6419.971329689026, 'accumulated_logging_time': 0.303957462310791, 'global_step': 25946, 'preemption_count': 0}), (28304, {'train/accuracy': 0.6390774250030518, 'train/loss': 1.7401479482650757, 'train/bleu': 31.344862714431464, 'validation/accuracy': 0.6581319570541382, 'validation/loss': 1.6097475290298462, 'validation/bleu': 28.05053729965437, 'validation/num_examples': 3000, 'test/accuracy': 0.6687235236167908, 'test/loss': 1.5342345237731934, 'test/bleu': 27.162785054412023, 'test/num_examples': 3003, 'score': 10112.716992616653, 'total_duration': 16995.22621178627, 'accumulated_submission_time': 10112.716992616653, 'accumulated_eval_time': 6881.22886633873, 'accumulated_logging_time': 0.3334319591522217, 'global_step': 28304, 'preemption_count': 0}), (30662, {'train/accuracy': 0.63572096824646, 'train/loss': 1.763773798942566, 'train/bleu': 31.040252556013616, 'validation/accuracy': 0.6615169048309326, 'validation/loss': 1.5909042358398438, 'validation/bleu': 28.108661415052204, 'validation/num_examples': 3000, 'test/accuracy': 0.6705711483955383, 'test/loss': 1.5236083269119263, 'test/bleu': 27.320115273429533, 'test/num_examples': 3003, 'score': 10952.624883651733, 'total_duration': 18343.28938150406, 'accumulated_submission_time': 10952.624883651733, 'accumulated_eval_time': 7389.272298574448, 'accumulated_logging_time': 0.3637864589691162, 'global_step': 30662, 'preemption_count': 0}), (33020, {'train/accuracy': 0.6443125009536743, 'train/loss': 1.6968934535980225, 'train/bleu': 31.166148767869927, 'validation/accuracy': 0.6595454216003418, 'validation/loss': 1.5900745391845703, 'validation/bleu': 27.80920383950556, 'validation/num_examples': 3000, 'test/accuracy': 0.6711289286613464, 'test/loss': 1.5155937671661377, 'test/bleu': 27.683319616547642, 'test/num_examples': 3003, 'score': 11792.630442142487, 'total_duration': 19747.73938369751, 'accumulated_submission_time': 11792.630442142487, 'accumulated_eval_time': 7953.593356847763, 'accumulated_logging_time': 0.40709567070007324, 'global_step': 33020, 'preemption_count': 0}), (35378, {'train/accuracy': 0.6399866938591003, 'train/loss': 1.7375435829162598, 'train/bleu': 31.338208754834255, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.5832661390304565, 'validation/bleu': 28.38960980084261, 'validation/num_examples': 3000, 'test/accuracy': 0.6730231046676636, 'test/loss': 1.5036847591400146, 'test/bleu': 27.782769305202045, 'test/num_examples': 3003, 'score': 12632.544389486313, 'total_duration': 21339.37600016594, 'accumulated_submission_time': 12632.544389486313, 'accumulated_eval_time': 8705.197973966599, 'accumulated_logging_time': 0.4429934024810791, 'global_step': 35378, 'preemption_count': 0}), (37736, {'train/accuracy': 0.663627564907074, 'train/loss': 1.5535968542099, 'train/bleu': 32.87796516096618, 'validation/accuracy': 0.6638479232788086, 'validation/loss': 1.5714770555496216, 'validation/bleu': 28.38134980475151, 'validation/num_examples': 3000, 'test/accuracy': 0.6728371381759644, 'test/loss': 1.4999555349349976, 'test/bleu': 27.78853257605112, 'test/num_examples': 3003, 'score': 13472.639306306839, 'total_duration': 22769.643906354904, 'accumulated_submission_time': 13472.639306306839, 'accumulated_eval_time': 9295.258509159088, 'accumulated_logging_time': 0.47411179542541504, 'global_step': 37736, 'preemption_count': 0}), (40094, {'train/accuracy': 0.6476908922195435, 'train/loss': 1.6861525774002075, 'train/bleu': 31.62556446241438, 'validation/accuracy': 0.6630792021751404, 'validation/loss': 1.5618013143539429, 'validation/bleu': 28.31053959711827, 'validation/num_examples': 3000, 'test/accuracy': 0.6751612424850464, 'test/loss': 1.4886205196380615, 'test/bleu': 28.055212927233068, 'test/num_examples': 3003, 'score': 14312.627776622772, 'total_duration': 24356.119666576385, 'accumulated_submission_time': 14312.627776622772, 'accumulated_eval_time': 10041.635178804398, 'accumulated_logging_time': 0.5051698684692383, 'global_step': 40094, 'preemption_count': 0}), (42452, {'train/accuracy': 0.6445221900939941, 'train/loss': 1.700629711151123, 'train/bleu': 31.200524801237783, 'validation/accuracy': 0.6668733358383179, 'validation/loss': 1.5635435581207275, 'validation/bleu': 28.737636815149113, 'validation/num_examples': 3000, 'test/accuracy': 0.6750218272209167, 'test/loss': 1.486008644104004, 'test/bleu': 27.51980545574519, 'test/num_examples': 3003, 'score': 15152.739064216614, 'total_duration': 25757.05531859398, 'accumulated_submission_time': 15152.739064216614, 'accumulated_eval_time': 10602.350082874298, 'accumulated_logging_time': 0.5357956886291504, 'global_step': 42452, 'preemption_count': 0}), (44811, {'train/accuracy': 0.6530870795249939, 'train/loss': 1.634215235710144, 'train/bleu': 31.688902445897607, 'validation/accuracy': 0.6673692464828491, 'validation/loss': 1.5494558811187744, 'validation/bleu': 28.504146051050036, 'validation/num_examples': 3000, 'test/accuracy': 0.6767880916595459, 'test/loss': 1.4711213111877441, 'test/bleu': 27.938705764575598, 'test/num_examples': 3003, 'score': 15992.64580154419, 'total_duration': 27153.050884485245, 'accumulated_submission_time': 15992.64580154419, 'accumulated_eval_time': 11158.329864501953, 'accumulated_logging_time': 0.566993236541748, 'global_step': 44811, 'preemption_count': 0}), (47170, {'train/accuracy': 0.6481767892837524, 'train/loss': 1.6744226217269897, 'train/bleu': 31.457149841710663, 'validation/accuracy': 0.668088436126709, 'validation/loss': 1.545769214630127, 'validation/bleu': 28.76677074342338, 'validation/num_examples': 3000, 'test/accuracy': 0.6807855367660522, 'test/loss': 1.45988130569458, 'test/bleu': 28.134571784728966, 'test/num_examples': 3003, 'score': 16832.79411482811, 'total_duration': 28478.575281381607, 'accumulated_submission_time': 16832.79411482811, 'accumulated_eval_time': 11643.594151735306, 'accumulated_logging_time': 0.5991125106811523, 'global_step': 47170, 'preemption_count': 0}), (49530, {'train/accuracy': 0.6455420851707458, 'train/loss': 1.7034410238265991, 'train/bleu': 31.773293987179354, 'validation/accuracy': 0.666240930557251, 'validation/loss': 1.5409092903137207, 'validation/bleu': 28.787587888853047, 'validation/num_examples': 3000, 'test/accuracy': 0.681494414806366, 'test/loss': 1.4549793004989624, 'test/bleu': 28.547696719561916, 'test/num_examples': 3003, 'score': 17672.922026872635, 'total_duration': 29877.601472377777, 'accumulated_submission_time': 17672.922026872635, 'accumulated_eval_time': 12202.3813393116, 'accumulated_logging_time': 0.6326305866241455, 'global_step': 49530, 'preemption_count': 0}), (51890, {'train/accuracy': 0.652571439743042, 'train/loss': 1.6386526823043823, 'train/bleu': 32.39050117798098, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.5333878993988037, 'validation/bleu': 28.55614230670583, 'validation/num_examples': 3000, 'test/accuracy': 0.6804369688034058, 'test/loss': 1.4555186033248901, 'test/bleu': 28.056858321750056, 'test/num_examples': 3003, 'score': 18513.092685222626, 'total_duration': 31397.252323150635, 'accumulated_submission_time': 18513.092685222626, 'accumulated_eval_time': 12881.750809669495, 'accumulated_logging_time': 0.6645634174346924, 'global_step': 51890, 'preemption_count': 0}), (54249, {'train/accuracy': 0.6511799693107605, 'train/loss': 1.6562837362289429, 'train/bleu': 31.79512948258729, 'validation/accuracy': 0.6702210903167725, 'validation/loss': 1.527982473373413, 'validation/bleu': 29.006077664411652, 'validation/num_examples': 3000, 'test/accuracy': 0.6831910014152527, 'test/loss': 1.4422683715820312, 'test/bleu': 28.418639510555767, 'test/num_examples': 3003, 'score': 19353.05917453766, 'total_duration': 32914.703605651855, 'accumulated_submission_time': 19353.05917453766, 'accumulated_eval_time': 13559.124497413635, 'accumulated_logging_time': 0.6962974071502686, 'global_step': 54249, 'preemption_count': 0}), (56609, {'train/accuracy': 0.6711536049842834, 'train/loss': 1.5183404684066772, 'train/bleu': 33.20045640260435, 'validation/accuracy': 0.6715229749679565, 'validation/loss': 1.5183759927749634, 'validation/bleu': 29.055875248804714, 'validation/num_examples': 3000, 'test/accuracy': 0.6822032332420349, 'test/loss': 1.4344021081924438, 'test/bleu': 28.28308659347989, 'test/num_examples': 3003, 'score': 20193.22402882576, 'total_duration': 34388.2636551857, 'accumulated_submission_time': 20193.22402882576, 'accumulated_eval_time': 14192.409289360046, 'accumulated_logging_time': 0.7298541069030762, 'global_step': 56609, 'preemption_count': 0}), (58968, {'train/accuracy': 0.6562167406082153, 'train/loss': 1.6146293878555298, 'train/bleu': 32.00557628346472, 'validation/accuracy': 0.6700970530509949, 'validation/loss': 1.5160971879959106, 'validation/bleu': 28.6046113423605, 'validation/num_examples': 3000, 'test/accuracy': 0.6840625405311584, 'test/loss': 1.4318151473999023, 'test/bleu': 28.442912978921708, 'test/num_examples': 3003, 'score': 21033.412729263306, 'total_duration': 36007.44893741608, 'accumulated_submission_time': 21033.412729263306, 'accumulated_eval_time': 14971.292563676834, 'accumulated_logging_time': 0.7623600959777832, 'global_step': 58968, 'preemption_count': 0}), (61327, {'train/accuracy': 0.6536709666252136, 'train/loss': 1.6435863971710205, 'train/bleu': 32.201144941719825, 'validation/accuracy': 0.67365562915802, 'validation/loss': 1.5085053443908691, 'validation/bleu': 29.024937312260363, 'validation/num_examples': 3000, 'test/accuracy': 0.6860031485557556, 'test/loss': 1.4202231168746948, 'test/bleu': 28.753225893997975, 'test/num_examples': 3003, 'score': 21873.331847190857, 'total_duration': 37395.15630316734, 'accumulated_submission_time': 21873.331847190857, 'accumulated_eval_time': 15518.967089891434, 'accumulated_logging_time': 0.7969238758087158, 'global_step': 61327, 'preemption_count': 0}), (63686, {'train/accuracy': 0.6629753708839417, 'train/loss': 1.568068265914917, 'train/bleu': 33.01746848462258, 'validation/accuracy': 0.6728744506835938, 'validation/loss': 1.503533124923706, 'validation/bleu': 28.890694782084704, 'validation/num_examples': 3000, 'test/accuracy': 0.686851441860199, 'test/loss': 1.4139506816864014, 'test/bleu': 29.019938516980574, 'test/num_examples': 3003, 'score': 22713.31698822975, 'total_duration': 38811.658296108246, 'accumulated_submission_time': 22713.31698822975, 'accumulated_eval_time': 16095.283498048782, 'accumulated_logging_time': 0.9187502861022949, 'global_step': 63686, 'preemption_count': 0}), (66045, {'train/accuracy': 0.6583556532859802, 'train/loss': 1.5996129512786865, 'train/bleu': 32.41859989742564, 'validation/accuracy': 0.6759990453720093, 'validation/loss': 1.4937245845794678, 'validation/bleu': 29.610730501242646, 'validation/num_examples': 3000, 'test/accuracy': 0.6908256411552429, 'test/loss': 1.3972442150115967, 'test/bleu': 29.21380668381085, 'test/num_examples': 3003, 'score': 23553.378241539, 'total_duration': 40180.534745931625, 'accumulated_submission_time': 23553.378241539, 'accumulated_eval_time': 16623.977352142334, 'accumulated_logging_time': 0.9587399959564209, 'global_step': 66045, 'preemption_count': 0}), (68403, {'train/accuracy': 0.6556589603424072, 'train/loss': 1.61857008934021, 'train/bleu': 32.53331232232554, 'validation/accuracy': 0.675317108631134, 'validation/loss': 1.4912937879562378, 'validation/bleu': 29.188595260579557, 'validation/num_examples': 3000, 'test/accuracy': 0.6906862258911133, 'test/loss': 1.391940951347351, 'test/bleu': 28.9712420024266, 'test/num_examples': 3003, 'score': 24393.38697886467, 'total_duration': 41567.93536186218, 'accumulated_submission_time': 24393.38697886467, 'accumulated_eval_time': 17171.25036072731, 'accumulated_logging_time': 0.9950568675994873, 'global_step': 68403, 'preemption_count': 0}), (70762, {'train/accuracy': 0.6649782657623291, 'train/loss': 1.5656242370605469, 'train/bleu': 33.27862937609742, 'validation/accuracy': 0.6770901679992676, 'validation/loss': 1.4805506467819214, 'validation/bleu': 29.460682932058393, 'validation/num_examples': 3000, 'test/accuracy': 0.6927662491798401, 'test/loss': 1.3877900838851929, 'test/bleu': 29.393405939683753, 'test/num_examples': 3003, 'score': 25233.515276670456, 'total_duration': 43111.594311475754, 'accumulated_submission_time': 25233.515276670456, 'accumulated_eval_time': 17874.665945768356, 'accumulated_logging_time': 1.0316176414489746, 'global_step': 70762, 'preemption_count': 0}), (73121, {'train/accuracy': 0.6601581573486328, 'train/loss': 1.5850191116333008, 'train/bleu': 33.035396256855584, 'validation/accuracy': 0.6781564950942993, 'validation/loss': 1.4745867252349854, 'validation/bleu': 29.27033968859275, 'validation/num_examples': 3000, 'test/accuracy': 0.6933588981628418, 'test/loss': 1.3788319826126099, 'test/bleu': 29.37666021325784, 'test/num_examples': 3003, 'score': 26073.755289793015, 'total_duration': 44533.76924061775, 'accumulated_submission_time': 26073.755289793015, 'accumulated_eval_time': 18456.486599206924, 'accumulated_logging_time': 1.0666627883911133, 'global_step': 73121, 'preemption_count': 0}), (75480, {'train/accuracy': 0.6792148947715759, 'train/loss': 1.4641389846801758, 'train/bleu': 33.80764561643805, 'validation/accuracy': 0.6794211864471436, 'validation/loss': 1.4671216011047363, 'validation/bleu': 29.555809985213795, 'validation/num_examples': 3000, 'test/accuracy': 0.6943350434303284, 'test/loss': 1.3725438117980957, 'test/bleu': 29.297592249645188, 'test/num_examples': 3003, 'score': 26913.816690921783, 'total_duration': 46007.44537067413, 'accumulated_submission_time': 26913.816690921783, 'accumulated_eval_time': 19089.9792573452, 'accumulated_logging_time': 1.1101996898651123, 'global_step': 75480, 'preemption_count': 0}), (77838, {'train/accuracy': 0.6665636897087097, 'train/loss': 1.5482903718948364, 'train/bleu': 33.07441180399439, 'validation/accuracy': 0.68016517162323, 'validation/loss': 1.459350347518921, 'validation/bleu': 29.50371177493752, 'validation/num_examples': 3000, 'test/accuracy': 0.6929754614830017, 'test/loss': 1.3658453226089478, 'test/bleu': 29.21646204970246, 'test/num_examples': 3003, 'score': 27753.721145629883, 'total_duration': 47643.87626647949, 'accumulated_submission_time': 27753.721145629883, 'accumulated_eval_time': 19886.381717681885, 'accumulated_logging_time': 1.1533830165863037, 'global_step': 77838, 'preemption_count': 0}), (80197, {'train/accuracy': 0.664623498916626, 'train/loss': 1.563930869102478, 'train/bleu': 33.14200471817913, 'validation/accuracy': 0.6821985840797424, 'validation/loss': 1.4505184888839722, 'validation/bleu': 29.735981389350844, 'validation/num_examples': 3000, 'test/accuracy': 0.6975771188735962, 'test/loss': 1.3530352115631104, 'test/bleu': 29.435464433153378, 'test/num_examples': 3003, 'score': 28593.740771770477, 'total_duration': 49170.16017580032, 'accumulated_submission_time': 28593.740771770477, 'accumulated_eval_time': 20572.528188943863, 'accumulated_logging_time': 1.1913011074066162, 'global_step': 80197, 'preemption_count': 0}), (82557, {'train/accuracy': 0.6770812273025513, 'train/loss': 1.4800242185592651, 'train/bleu': 34.02139815328523, 'validation/accuracy': 0.6815042495727539, 'validation/loss': 1.4466660022735596, 'validation/bleu': 29.459093235760516, 'validation/num_examples': 3000, 'test/accuracy': 0.6959037780761719, 'test/loss': 1.3536893129348755, 'test/bleu': 29.501150496830565, 'test/num_examples': 3003, 'score': 29433.655618667603, 'total_duration': 50747.67319107056, 'accumulated_submission_time': 29433.655618667603, 'accumulated_eval_time': 21310.009718179703, 'accumulated_logging_time': 1.229506015777588, 'global_step': 82557, 'preemption_count': 0}), (84916, {'train/accuracy': 0.6718239784240723, 'train/loss': 1.5177863836288452, 'train/bleu': 33.53921780192777, 'validation/accuracy': 0.6841204762458801, 'validation/loss': 1.4370832443237305, 'validation/bleu': 29.954040391928764, 'validation/num_examples': 3000, 'test/accuracy': 0.698843777179718, 'test/loss': 1.3375470638275146, 'test/bleu': 29.460929968466225, 'test/num_examples': 3003, 'score': 30273.7618765831, 'total_duration': 52317.95136976242, 'accumulated_submission_time': 30273.7618765831, 'accumulated_eval_time': 22040.064561843872, 'accumulated_logging_time': 1.2669031620025635, 'global_step': 84916, 'preemption_count': 0}), (87275, {'train/accuracy': 0.6702315211296082, 'train/loss': 1.5227426290512085, 'train/bleu': 33.57381970171942, 'validation/accuracy': 0.6851620078086853, 'validation/loss': 1.4330618381500244, 'validation/bleu': 30.223012227748367, 'validation/num_examples': 3000, 'test/accuracy': 0.699785053730011, 'test/loss': 1.3343571424484253, 'test/bleu': 29.740632616160205, 'test/num_examples': 3003, 'score': 31113.805472373962, 'total_duration': 53728.91593122482, 'accumulated_submission_time': 31113.805472373962, 'accumulated_eval_time': 22610.868903636932, 'accumulated_logging_time': 1.3044085502624512, 'global_step': 87275, 'preemption_count': 0}), (89633, {'train/accuracy': 0.6788753867149353, 'train/loss': 1.4781488180160522, 'train/bleu': 34.11023781864019, 'validation/accuracy': 0.6842444539070129, 'validation/loss': 1.4293118715286255, 'validation/bleu': 30.039313991534137, 'validation/num_examples': 3000, 'test/accuracy': 0.7009587287902832, 'test/loss': 1.3281270265579224, 'test/bleu': 29.580239725008756, 'test/num_examples': 3003, 'score': 31953.73739719391, 'total_duration': 55190.8995449543, 'accumulated_submission_time': 31953.73739719391, 'accumulated_eval_time': 23232.804119586945, 'accumulated_logging_time': 1.3434412479400635, 'global_step': 89633, 'preemption_count': 0}), (91992, {'train/accuracy': 0.674981951713562, 'train/loss': 1.4979826211929321, 'train/bleu': 33.107271415973, 'validation/accuracy': 0.6863523125648499, 'validation/loss': 1.421521544456482, 'validation/bleu': 29.94934206485915, 'validation/num_examples': 3000, 'test/accuracy': 0.7017953991889954, 'test/loss': 1.3203881978988647, 'test/bleu': 30.07968705955243, 'test/num_examples': 3003, 'score': 32793.78981876373, 'total_duration': 56641.69893527031, 'accumulated_submission_time': 32793.78981876373, 'accumulated_eval_time': 23843.425915002823, 'accumulated_logging_time': 1.3887052536010742, 'global_step': 91992, 'preemption_count': 0}), (94351, {'train/accuracy': 0.6841570734977722, 'train/loss': 1.4367806911468506, 'train/bleu': 34.501937700227685, 'validation/accuracy': 0.6876170039176941, 'validation/loss': 1.4109599590301514, 'validation/bleu': 30.217930488187925, 'validation/num_examples': 3000, 'test/accuracy': 0.704317033290863, 'test/loss': 1.3113782405853271, 'test/bleu': 30.00001747016476, 'test/num_examples': 3003, 'score': 33633.84631562233, 'total_duration': 58084.538499593735, 'accumulated_submission_time': 33633.84631562233, 'accumulated_eval_time': 24446.093323946, 'accumulated_logging_time': 1.426877498626709, 'global_step': 94351, 'preemption_count': 0}), (96710, {'train/accuracy': 0.6783720254898071, 'train/loss': 1.4733608961105347, 'train/bleu': 34.322575911748935, 'validation/accuracy': 0.6874558329582214, 'validation/loss': 1.4094278812408447, 'validation/bleu': 30.34860323432693, 'validation/num_examples': 3000, 'test/accuracy': 0.7056649923324585, 'test/loss': 1.3058409690856934, 'test/bleu': 30.114120610001066, 'test/num_examples': 3003, 'score': 34473.7677295208, 'total_duration': 59600.05470824242, 'accumulated_submission_time': 34473.7677295208, 'accumulated_eval_time': 25121.571900844574, 'accumulated_logging_time': 1.4660224914550781, 'global_step': 96710, 'preemption_count': 0}), (99070, {'train/accuracy': 0.6781623959541321, 'train/loss': 1.4802968502044678, 'train/bleu': 33.968795052975345, 'validation/accuracy': 0.6889064908027649, 'validation/loss': 1.4011088609695435, 'validation/bleu': 30.316761453508192, 'validation/num_examples': 3000, 'test/accuracy': 0.7047004699707031, 'test/loss': 1.3031493425369263, 'test/bleu': 30.178915366389266, 'test/num_examples': 3003, 'score': 35313.945922613144, 'total_duration': 60997.73252558708, 'accumulated_submission_time': 35313.945922613144, 'accumulated_eval_time': 25678.955759763718, 'accumulated_logging_time': 1.5046508312225342, 'global_step': 99070, 'preemption_count': 0}), (101430, {'train/accuracy': 0.6870272159576416, 'train/loss': 1.419952392578125, 'train/bleu': 34.51947111345885, 'validation/accuracy': 0.6896132826805115, 'validation/loss': 1.3957315683364868, 'validation/bleu': 30.235381427345153, 'validation/num_examples': 3000, 'test/accuracy': 0.7055255770683289, 'test/loss': 1.2928892374038696, 'test/bleu': 30.34395915705975, 'test/num_examples': 3003, 'score': 36153.992918252945, 'total_duration': 62491.293865680695, 'accumulated_submission_time': 36153.992918252945, 'accumulated_eval_time': 26332.348979234695, 'accumulated_logging_time': 1.5485038757324219, 'global_step': 101430, 'preemption_count': 0}), (103788, {'train/accuracy': 0.6809467077255249, 'train/loss': 1.462726354598999, 'train/bleu': 34.7810708399674, 'validation/accuracy': 0.6901588439941406, 'validation/loss': 1.3913047313690186, 'validation/bleu': 30.359409154734198, 'validation/num_examples': 3000, 'test/accuracy': 0.7094416618347168, 'test/loss': 1.2849253416061401, 'test/bleu': 30.334914627920924, 'test/num_examples': 3003, 'score': 36993.887571811676, 'total_duration': 63936.5658159256, 'accumulated_submission_time': 36993.887571811676, 'accumulated_eval_time': 26937.60739517212, 'accumulated_logging_time': 1.5876469612121582, 'global_step': 103788, 'preemption_count': 0}), (106147, {'train/accuracy': 0.687261164188385, 'train/loss': 1.4270676374435425, 'train/bleu': 34.439128262689984, 'validation/accuracy': 0.6915971040725708, 'validation/loss': 1.3868253231048584, 'validation/bleu': 30.57444599493856, 'validation/num_examples': 3000, 'test/accuracy': 0.7098251581192017, 'test/loss': 1.2795196771621704, 'test/bleu': 30.52362190743765, 'test/num_examples': 3003, 'score': 37833.93421292305, 'total_duration': 65344.22820472717, 'accumulated_submission_time': 37833.93421292305, 'accumulated_eval_time': 27505.089040517807, 'accumulated_logging_time': 1.6410527229309082, 'global_step': 106147, 'preemption_count': 0}), (108506, {'train/accuracy': 0.6925578117370605, 'train/loss': 1.3921180963516235, 'train/bleu': 35.037549363106514, 'validation/accuracy': 0.6933826208114624, 'validation/loss': 1.3761391639709473, 'validation/bleu': 30.82730396732934, 'validation/num_examples': 3000, 'test/accuracy': 0.7099180817604065, 'test/loss': 1.2736002206802368, 'test/bleu': 30.546075277934015, 'test/num_examples': 3003, 'score': 38673.95379757881, 'total_duration': 66828.72117614746, 'accumulated_submission_time': 38673.95379757881, 'accumulated_eval_time': 28149.444666147232, 'accumulated_logging_time': 1.6813023090362549, 'global_step': 108506, 'preemption_count': 0}), (110865, {'train/accuracy': 0.6871196031570435, 'train/loss': 1.4239205121994019, 'train/bleu': 34.8735757618847, 'validation/accuracy': 0.6927750110626221, 'validation/loss': 1.379041075706482, 'validation/bleu': 30.659466710577252, 'validation/num_examples': 3000, 'test/accuracy': 0.7104293704032898, 'test/loss': 1.2698314189910889, 'test/bleu': 30.823263329626307, 'test/num_examples': 3003, 'score': 39513.917432546616, 'total_duration': 68266.67229032516, 'accumulated_submission_time': 39513.917432546616, 'accumulated_eval_time': 28747.312561511993, 'accumulated_logging_time': 1.722498893737793, 'global_step': 110865, 'preemption_count': 0}), (113222, {'train/accuracy': 0.6949504017829895, 'train/loss': 1.3666104078292847, 'train/bleu': 35.29129947522768, 'validation/accuracy': 0.6940769553184509, 'validation/loss': 1.373827338218689, 'validation/bleu': 30.822025096723628, 'validation/num_examples': 3000, 'test/accuracy': 0.7110685110092163, 'test/loss': 1.2664237022399902, 'test/bleu': 30.57820511062706, 'test/num_examples': 3003, 'score': 40353.82215619087, 'total_duration': 69755.15716338158, 'accumulated_submission_time': 40353.82215619087, 'accumulated_eval_time': 29395.77043747902, 'accumulated_logging_time': 1.7632851600646973, 'global_step': 113222, 'preemption_count': 0}), (115581, {'train/accuracy': 0.692932665348053, 'train/loss': 1.3845998048782349, 'train/bleu': 34.87558627803715, 'validation/accuracy': 0.6947588920593262, 'validation/loss': 1.3719394207000732, 'validation/bleu': 30.69022831385768, 'validation/num_examples': 3000, 'test/accuracy': 0.7114287614822388, 'test/loss': 1.2623103857040405, 'test/bleu': 30.915548870642528, 'test/num_examples': 3003, 'score': 41193.96239209175, 'total_duration': 71226.29557180405, 'accumulated_submission_time': 41193.96239209175, 'accumulated_eval_time': 30026.637234210968, 'accumulated_logging_time': 1.8142666816711426, 'global_step': 115581, 'preemption_count': 0}), (117940, {'train/accuracy': 0.6903288960456848, 'train/loss': 1.406310796737671, 'train/bleu': 35.18875162329316, 'validation/accuracy': 0.6950441002845764, 'validation/loss': 1.3678932189941406, 'validation/bleu': 30.774742153044187, 'validation/num_examples': 3000, 'test/accuracy': 0.7126837372779846, 'test/loss': 1.2599252462387085, 'test/bleu': 31.04379746971764, 'test/num_examples': 3003, 'score': 42033.97531580925, 'total_duration': 72615.43852734566, 'accumulated_submission_time': 42033.97531580925, 'accumulated_eval_time': 30575.644649982452, 'accumulated_logging_time': 1.8567523956298828, 'global_step': 117940, 'preemption_count': 0}), (120299, {'train/accuracy': 0.6964380145072937, 'train/loss': 1.3709895610809326, 'train/bleu': 35.66247077415662, 'validation/accuracy': 0.6952300667762756, 'validation/loss': 1.3651654720306396, 'validation/bleu': 30.875901980810767, 'validation/num_examples': 3000, 'test/accuracy': 0.7123816609382629, 'test/loss': 1.2575608491897583, 'test/bleu': 30.803521074857283, 'test/num_examples': 3003, 'score': 42873.95320272446, 'total_duration': 74017.72199606895, 'accumulated_submission_time': 42873.95320272446, 'accumulated_eval_time': 31137.830321788788, 'accumulated_logging_time': 1.8995048999786377, 'global_step': 120299, 'preemption_count': 0}), (122659, {'train/accuracy': 0.6943261623382568, 'train/loss': 1.382710337638855, 'train/bleu': 35.10050834963898, 'validation/accuracy': 0.6955276131629944, 'validation/loss': 1.3625982999801636, 'validation/bleu': 31.04729680749209, 'validation/num_examples': 3000, 'test/accuracy': 0.7139387726783752, 'test/loss': 1.2523609399795532, 'test/bleu': 30.83641365004823, 'test/num_examples': 3003, 'score': 43713.98013854027, 'total_duration': 75454.82591438293, 'accumulated_submission_time': 43713.98013854027, 'accumulated_eval_time': 31734.78758907318, 'accumulated_logging_time': 1.942683458328247, 'global_step': 122659, 'preemption_count': 0}), (125019, {'train/accuracy': 0.6947394609451294, 'train/loss': 1.377518892288208, 'train/bleu': 35.855026133964195, 'validation/accuracy': 0.6967179775238037, 'validation/loss': 1.361031174659729, 'validation/bleu': 30.876074128853972, 'validation/num_examples': 3000, 'test/accuracy': 0.713927149772644, 'test/loss': 1.2523263692855835, 'test/bleu': 30.98107247465367, 'test/num_examples': 3003, 'score': 44553.89228606224, 'total_duration': 76870.24442744255, 'accumulated_submission_time': 44553.89228606224, 'accumulated_eval_time': 32310.166967868805, 'accumulated_logging_time': 1.9930167198181152, 'global_step': 125019, 'preemption_count': 0}), (127379, {'train/accuracy': 0.6961644291877747, 'train/loss': 1.3683280944824219, 'train/bleu': 35.79156931045429, 'validation/accuracy': 0.6965195536613464, 'validation/loss': 1.3605613708496094, 'validation/bleu': 30.945679584384134, 'validation/num_examples': 3000, 'test/accuracy': 0.7140550017356873, 'test/loss': 1.2519234418869019, 'test/bleu': 30.88051741175642, 'test/num_examples': 3003, 'score': 45394.04832172394, 'total_duration': 78304.93042588234, 'accumulated_submission_time': 45394.04832172394, 'accumulated_eval_time': 32904.57277917862, 'accumulated_logging_time': 2.0373668670654297, 'global_step': 127379, 'preemption_count': 0}), (129739, {'train/accuracy': 0.6961685419082642, 'train/loss': 1.3700485229492188, 'train/bleu': 35.88213839829184, 'validation/accuracy': 0.6964327692985535, 'validation/loss': 1.36008882522583, 'validation/bleu': 30.981903451783634, 'validation/num_examples': 3000, 'test/accuracy': 0.7140898704528809, 'test/loss': 1.2510143518447876, 'test/bleu': 30.877321535382997, 'test/num_examples': 3003, 'score': 46234.166835308075, 'total_duration': 79724.03761744499, 'accumulated_submission_time': 46234.166835308075, 'accumulated_eval_time': 33483.432725191116, 'accumulated_logging_time': 2.08833909034729, 'global_step': 129739, 'preemption_count': 0}), (132099, {'train/accuracy': 0.6940845251083374, 'train/loss': 1.3850692510604858, 'train/bleu': 35.896859808845385, 'validation/accuracy': 0.6964203715324402, 'validation/loss': 1.359937071800232, 'validation/bleu': 30.921410299106665, 'validation/num_examples': 3000, 'test/accuracy': 0.7141827940940857, 'test/loss': 1.2505288124084473, 'test/bleu': 30.886567051931777, 'test/num_examples': 3003, 'score': 47074.08837556839, 'total_duration': 81159.41278767586, 'accumulated_submission_time': 47074.08837556839, 'accumulated_eval_time': 34078.76060009003, 'accumulated_logging_time': 2.137282371520996, 'global_step': 132099, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6966027021408081, 'train/loss': 1.3738631010055542, 'train/bleu': 35.481369630996994, 'validation/accuracy': 0.6963583827018738, 'validation/loss': 1.360085368156433, 'validation/bleu': 30.92441491532882, 'validation/num_examples': 3000, 'test/accuracy': 0.7141595482826233, 'test/loss': 1.2506054639816284, 'test/bleu': 30.925173570681565, 'test/num_examples': 3003, 'score': 47513.109431266785, 'total_duration': 82169.46109032631, 'accumulated_submission_time': 47513.109431266785, 'accumulated_eval_time': 34649.70331478119, 'accumulated_logging_time': 2.181474447250366, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0215 02:29:53.193797 140144802662208 submission_runner.py:586] Timing: 47513.109431266785
I0215 02:29:53.193861 140144802662208 submission_runner.py:588] Total number of evals: 58
I0215 02:29:53.193911 140144802662208 submission_runner.py:589] ====================
I0215 02:29:53.194834 140144802662208 submission_runner.py:673] Final wmt score: 46358.67147278786
